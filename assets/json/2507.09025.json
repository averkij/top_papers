{
    "paper_title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "authors": [
        "Chien Van Nguyen",
        "Ruiyi Zhang",
        "Hanieh Deilamsalehy",
        "Puneet Mathur",
        "Viet Dac Lai",
        "Haoliang Wang",
        "Jayakumar Subramanian",
        "Ryan A. Rossi",
        "Trung Bui",
        "Nikos Vlassis",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 2 0 9 0 . 7 0 5 2 : r Lizard: An Efficient Linearization Framework for Large Language Models Chien Van Nguyen1 Ruiyi Zhang2 Hanieh Deilamsalehy2 Puneet Mathur2 Viet Dac Lai2 Haoliang Wang Jayakumar Subramanian2 Ryan A. Rossi2 Trung Bui2 Nikos Vlassis2 Franck Dernoncourt2, Thien Huu Nguyen1, 1University of Oregon 2Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "We propose Lizard, linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizardincorporates gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizardcombines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizardachieves near-lossless recovery of the teacher models performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizardimproves over prior models by 18 points and shows significant improvements on associative recall tasks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) based on Transformer architectures have achieved impressive advances in natural language processing across various tasks [Grattafiori et al., 2024, Mann et al., 2020, Achiam et al., 2023]. However, their reliance on softmax attention with quadratic time and memory complexity poses significant limitations for long-context applications, both during training and inference. In particular, the softmax attention computation scales quadratically with the sequence length, while the key-value (KV) cache grows linearly during generation, resulting in significant computational and memory overhead for long-context sequences [Adnan et al., 2024]. Corresponding author: {chienn, thienn}@uoregon.edu, dernonco@adobe.com Preprint. Under review. Recent work has proposed linear and subquadratic alternatives to softmax attention [Yang et al., 2023, Gu and Dao, 2023, Peng et al., 2023b, Wang et al., 2020, Dao and Gu, 2024], enabling linear-time training and constant-memory inference. However, despite these efficiency benefits, pretraining LLMs with such architectures from scratch requires massive training budgets, often involving trillions of tokens. More importantly, models trained with linear attention mechanisms from scratch consistently underperform on tasks that require in-context learning and retrieval. For example, Transformer models significantly outperform both Mamba and Mamba-2 by up to 15 points on the 5-shot MMLU benchmark when all are pretrained under the same settings [Waleffe et al.]. Another promising direction involves linearizing pretrained Transformer-based LLMs by replacing their softmax attention modules with subquadratic alternatives [Zhang et al., Mercat et al., 2024, Zhang et al., 2024, Wang et al., 2024]. Instead of training new models from scratch with linear attention, this approach leverages the capabilities of openly available, large-scale pretrained models typically trained on trillions of tokens, and retrofits them with more efficient attention mechanisms. By starting from high-quality pretrained models, linearized models can better preserve task performance and recover softmax-like spiky attention patterns, thereby enhancing retrieval performance in in-context learning tasks [Zhang et al., 2024]. However, prior work either fails to fully exploit the strengths of linear attention mechanisms or still significantly underperforms compared to softmax-based attention. For example, LoLCATs [Zhang et al., 2024], recent state-of-the-art linearization method, shows promising results and cost-effectiveness. However, it still relies on predefined positional embedding patterns for positional encoding, thereby struggling with length generalization and failing to extrapolate beyond the trained context length. Moreover, due to architectural constraints and limited flexibility, it lacks an efficient memory update mechanism, which significantly affects its performance on recall-intensive tasks. On the other hand, models such as MambaInLlama [Wang et al., 2024] show better extrapolation to longer sequences, but still suffer from substantial performance degradation. Specifically, on the 5-shot MMLU benchmark, LoLCATs lags behind its teacher model by 13.8 points, while MambaInLLaMA exhibits 23.4 point drop. In this paper, we introduce Lizard (Linearizing Softmax Attention with Recurrent Gate Dynamics), an efficient framework for linearizing LLMs. Unlike prior approaches that constrain themselves to reusing only the pretrained models existing parameters and fixed architecture [Mercat et al., 2024, Zhang et al., 2024], we introduce additional learnable components in the form of gating module. The gating module serves two primary purposes. First, it acts as data-dependent alternative to predefined positional embedding methods, such as Rotary Positional Embeddings (RoPE) [Heo et al., 2024], allowing the model to learn implicit relative positional information through adaptive decay patterns, enhancing length generalization. Second, the gated recurrent structure provides dynamic memory management mechanism, enabling the model to control the retention and forgetting of past tokens during generation, thereby improving associative recall. To further recover the expressiveness of softmax attention, we combine the output of the gated linear attention with sliding window attention mechanism augmented with meta memory. While the gated component captures global context in compressed form, the sliding window attention preserves local precision and expressivity within the local context. This hybrid attention framework provides high-quality approximation to softmax attention. We train our model in two stages: (1) an attention approximation stage where the subquadratic modules are trained to mimic softmax attention outputs, and (2) fine-tuning stage where the linearized model is adapted to downstream language modeling objectives. Our contributions can be summarized as follows: We propose Lizard, linearization framework that adapts pretrained Transformer-based LLMs into gated recurrent architectures. This adaptation enables highly efficient, subquadratic attention mechanism with constant-memory inference and strong length generalization. Lizard achieves near-lossless recovery of the original models performance across major benchmarks and outperforms prior linearization methods by 818 points on the 5-shot MMLU benchmark. We conduct extensive empirical studies to analyze architectural design choices within Lizardand evaluate which state-of-the-art recurrent architectures are most effective, across diverse benchmarks. We present hardware-aware implementation that improves the training throughput by up to 24%, enabling faster optimization."
        },
        {
            "title": "2 Preliminary",
            "content": "To motivate Lizard, we first review the core components of Causal Softmax Attention, Gated Linear Attention, and techniques for approximating softmax attention. We then briefly discuss related work on linearizing Transformers and adapting pretrained LLMs for efficient long-context modeling. Causal Softmax Attention In modern Transformer architectures [Touvron et al., 2023, Jiang et al.], for query vector at position i, Causal Softmax Attention with RoPE produces the output yi as: yi = (cid:88) t=1 (cid:16) exp φRoPE(qi)φRoPE(kt)/ (cid:17) (cid:80)i j=1 exp (cid:16) φRoPE(qi)φRoPE(kj)/ (cid:17) vt, where φRoPE(qi) and φRoPE(ki) denote RoPE-transformed query and key vectors, respectively. The causal softmax attention with RoPE can be expressed in matrix multiplication form as below: Ysoftmax = softmax (cid:16) QRoPEK RoPE/ (cid:17) where is the context length, QRoPE, KRoPE RLd are the RoPE-transformed query and key matrices, RLL is the lower-triangular causal mask (Mij = 1 if j, and Mij = if < j), RLd is the value matrix, denotes element-wise multiplication. Gated Linear Attention Linear attention mechanisms [Katharopoulos et al., 2020] approximate softmax attention by replacing the exponential similarity function with kernel function k(q, k) = ϕ(q)ϕ(k), where ϕ is feature map. Gated Linear Attention (GLA), introduced by [Yang et al., 2023], is variant of linear attention that incorporates learnable decay term. The output of gated linear attention ˆyi is computed as: ˆyi = ϕq(qi) (cid:80)i ϕq(qi) (cid:80)i t=1 j=1 (cid:16)(cid:81)i l=t+1 Γl (cid:17) (cid:16)(cid:81)i l=j+1 Γl ϕk(kt)vt (cid:17) ϕk(kj) , where Γi = sigmoid(Wγxi) is data-dependent gating factor with Wγ Rdd. The term (cid:81)i l=t+1 Γl represents decaying factor applied to the contribution from position t. Critically, this formulation enables recurrent structure that allows constant memory inference: Si = ΓtSi1 + ϕk(ki)v , ˆyi = Siϕq(qi) Let ct = (cid:81)t output can then be expressed in matrix form as: j=1 Γt and define RLd as the matrix obtained by stacking the vectors ct. The GLA (cid:32)(cid:32) ˆYgate = (ϕq(Q) C) (cid:18) ϕk(K) (cid:19)(cid:33) (cid:33) (1) Linearizing Transformers To combine efficiency with quality, [Zhang et al.] proposes learnable feature map that preserves the spiky and monotonic properties of softmax attention, crucial for selective focus, while maintaining linear time complexity. The feature map Hedgehog is defined as: ϕ(x) = [exp(xW ) exp(xW )] , where is the concatenation operation. Hedgehog demonstrated that the symmetric exponential form enables low-entropy, high-selectivity attention patterns. Prior methods have exploited Hedgehog as feature map to linearize Transformer-based LLMs [Zhang et al., 2024]. 3 Figure 1: Overview of the Lizardtraining pipeline. The model is trained in two stages. Stage 1: The pretrained model is frozen, and the added modules are trained to approximate both the positional encoding patterns and the softmax attention outputs. Stage 2: The softmax attention is replaced with the trained hybrid modules, then fine-tuned on the next-word prediction objective."
        },
        {
            "title": "3 Lizard: Subquadratic Attention for Infinite Context",
            "content": "To enable infinite-context generation, we replace softmax attention with subquadratic mechanism by introducing learnable modules that approximate both attention weights and RoPE patterns. This enables constant-memory inference while preserving generalization to longer sequences. Prior linearization approaches typically retain the original architecture of pretrained models and avoid adding new parameters [Mercat et al., 2024, Zhang et al., 2024], which limits their flexibility and prevents the use of more expressive designs. As result, they inherit architectural constraints that hinder performance and generalization. In contrast, we introduce learnable gating module inspired by recent advances in gated recurrent architectures [Yang et al., 2023]. This design choice allows for adaptive memory control, stronger length generalization, and more effective recovery of the original models capabilities. To recover the performance of softmax attention, we adopt two-stage training procedure (Figure 1). In the First Stage, we remove RoPE and replace softmax attention with hybrid subquadratic mechanism. The added modules are trained to mimic softmax behavior, including both attention weights and RoPE-induced positional patterns, enabling generalization to longer contexts. In the Second Stage, we replace the original attention layers with the trained subquadratic modules and employ fine-tuning to align the them with downstream language modeling objectives. 3.1 First Stage: Approximating Softmax Attention for Unbounded Context In the first stage, we introduce three new components: two Hedgehog feature maps, ϕq and ϕk, and learnable gating module parameterized by Wγ. These are integrated into the original attention mechanism to form Gated Linear Attention (GLA), which serves as subquadratic approximation of the original softmax attention. We approximate the causal softmax attention with RoPE with GLA: (cid:16) exp φRoPE(qi)φRoPE(kt)/ (cid:17) yi = (cid:88) t=1 (cid:80)i j=1 exp (cid:16) φRoPE(qi)φRoPE(kj)/ vt (cid:17) , ˆyi = ϕq(qi) (cid:80)i ϕq(qi) (cid:80)i t= j=1 (cid:16)(cid:81)i l=t+1 Γl (cid:17) (cid:16)(cid:81)i l=j+1 Γl ϕk(kt)vt (cid:17) ϕk(kj) where Γi = sigmoid(Wγxi) is data-dependent gating factor with Wγ Rdd The gating mechanism plays dual role in the attention transformation. First, it implicitly captures relative positional information by controlling the decay of past contributions. Unlike RoPE, which relies on predefined sinusoidal patterns, the data-adaptive gating factors γi enable better generalization across context lengths. Second, the gating mechanism supports recurrent formulation that allows constant-memory inference. The accumulated hidden state Si incrementally summarizes historical 4 Figure 2: Visualize attention map of Lizard, as combination of Gated Linear Attention and Sliding Window Attention with Meta Memory (with = 3 and = 2) information up to position using gated updates, removing the need to store the full key-value sequence. Hardware-aware implementation The matrix form of gated linear attention in Eq. 1 is not numerically stable, as the cumulative product of gating values ct = (cid:81)t j=1 Γj can become extremely small, leading to underflow and instability during training. Prior work mitigates this by operating in log-space and introducing secondary-level chunking mechanism to regain compatibility with tensor core matrix multiplications [Yang et al., 2023]. However, this method still requires full-precision computation within chunks, limiting overall efficiency. To address this, we propose more hardware-efficient factorization that directly incorporates the log-gating values into the query and key projections. Specifically, we reparameterize the attention computation by absorbing the cumulative gates into the queries and keys as: (cid:101)Q = ϕq(Q) = ϕq(Q) exp(log C) (cid:101)K = ϕk(K) = ϕk(K) exp(log C) Substituting these into Eq. 1, the gated attention becomes: Ygate = (cid:16)(cid:16) (cid:101)Q (cid:101)K (cid:17) (cid:17) V. This reparameterization is only numerically stable when the feature maps ϕq and ϕk are nonnegative. In our case, we leverage the non-negativity of the Hedgehog feature map, enabling us to express the gated projections compactly as: (cid:101)Q = [exp(QW + log C) exp(QW + log C)] , (cid:101)K = [exp(KW log C) exp(KW log C)] . By shifting the gating contributions into the feature space, our formulation enables stable and parallelizable matrix operations compatible with tensor core units. This allows for an efficient chunkwise GPU implementation with no need for full-precision fallback. Empirically, our method yields stable training dynamics and achieves up to 32% speedup compared to prior gated attention implementations2 [Yang et al., 2023]. Sliding Window Attention with Meta Memory Inspired by prior work demonstrating that hybrid combinations of attention layers and linear attention mechanisms can improve model quality [Arora et al., 2024, Munkhdalai et al., 2024], we incorporate short sliding window attention (SWA) module combined with meta-memory mechanism. While gated linear attention captures compressed global context, SWA preserves the expressive nonlinearity of softmax attention, enabling effective modeling of fine-grained local dependencies. Specifically, for window size w, each query vector qi attends only to the most recent keys {kiw+1, . . . , ki}, along with small set of prepended special \"meta\" tokens {k0, . . . , km1 }, which serve as compressed global context representations. Formally, the output of the sliding window attention at position is computed as: (cid:80) ˆyi = t{0,...,m1}{iw+1,...,i} exp(q (cid:80) t{0,...,m1}{iw+1,...,i} exp(q kt/ kt/ d)vt d) 2https://github.com/fla-org/flash-linear-attention In matrix multiplication form, we express SWA as: Yswa = softmax where Mswa is mask matrix enforcing the sliding window and the meta-token attention pattern. Mswa QK/ (cid:16) (cid:17) These special meta-tokens act as compressed representations that all tokens can attend to globally. Although these meta-tokens are not semantically meaningful, they receive high attention weights from other tokens in the sequence, as also observed in previous studies [Lin et al., 2024, Xiao et al., 2023]. Our experiments show that these meta-tokens significantly enhance the models recall capability. We illustrate the attention pattern of Lizardin Figure 2. This formulation enables O(L(w + m)d) time and space complexity, where is the sequence length, is the window size and is the number of meta-tokens. Importantly, it also enables constant-memory usage during inference by maintaining only fixed size + key-value cache. Attention Approximation and Training Objective We approximate the full softmax attention output Yattn by combining the outputs of gated linear attention and sliding window attention: ˆYattn ˆYgate + α ˆYSW The parameters ϕq, ϕk, Wγ, and α are optimized via the following mean squared error loss, minimizing the discrepancy between the true and approximated attention outputs: (cid:88) LMSE(ϕq, ϕk, Wγ, α) = (cid:13) (cid:13)Yl (cid:13) where is the number of attention layers. Overall, Lizardachieves an O(cid:0)L(w + m)d + Ld2(cid:1) time and space complexity. For generation, Lizardrequires only O((w + m)2d + d2) time and space for every token. gate + α ˆYl (cid:17)(cid:13) 2 (cid:13) (cid:13) attn (cid:16) ˆYl 1 SW l=1 3.2 Second Stage: Aligning Subquadratic Attention with Language Modeling While the first stage focuses on minimizing the approximation error between the hybrid subquadratic attention and full softmax attention, it does not directly optimize for the final language modeling performance. To bridge the gap between architectural approximation and practical performance, we fine-tune the model on autoregressive language modeling objectives to align the approximated attention mechanism with downstream performance. The loss function used during this stage is the standard autoregressive language modeling objective: LLM(θ) = (cid:80)L i=1 log (xi x1:i)"
        },
        {
            "title": "4 Experimental Study",
            "content": "In this section, we present our experimental results, focusing on three key aspects: 1. Language Modeling Benchmarks: We evaluate Lizardon standard language modeling datasets and compare its performance against existing subquadratic alternatives and linearizations. Our results indicate that Lizardmatches the average performance of the teacher model and significantly outperforms other baselines by large margin. 2. Language Modeling Recall: To evaluate memory retrieval capabilities, we conduct experiments on the Needle-in-a-Haystack test, which assesses the models ability to retrieve specific information over long sequences. Lizardsignificantly outperforms existing linearized LLMs on associative recall tasks. Furthermore, by retaining small number of full attention layers, Lizardclosely approximates the teacher models performance in the 5-shot MMLU benchmark. 3. Generation Efficiency: We compare the generation throughput of Lizardwith the teacher model equipped with FlashAttention-2. While the teacher model quickly runs out of memory at sequence length of 32K, Lizardmaintains constant memory usage and throughput, enabling efficient generation with infinite context. 4. Architectural and Ablation Analysis: We conduct detailed analysis of Lizards design choices, including the structure of the gating module and the contribution of each component. These studies help identify the most effective configurations for performance and efficiency. 72.3 72.4 72.0 64.7 69.4 64.1 63.8 65.8 64.0 70.7 70.7 61.9 72.2 72.4 Avg. Table 1: Performance comparison of Lizardand existing 7B-size subquadratic LLMs. Linearized models are categorized as Bounded (limited to context length) or Unbounded (capable of extrapolating to longer sequences). Training PiQA ARC-e ARC-c Hella. Wino. MMLU Tokens (B) acc acc_norm acc_norm acc (5-shot) (no MMLU) acc Avg. Avg. Model Transformer Gemma-7B Mistral-7B LLaMA-3-8B Subquadratic Mamba-7B RWKV-6-v2.1-7B TransNormerLLM-7B Hawk-7B Griffin-7B Linearized (Bounded) Mistral-7B-SUPRA Mistral-7B-LoLCATs LLaMA-3-8B-LoLCATs Linearized (Unbounded) Mamba2-LLaMA-3-8B Mistral-7B-Lizard(Ours) LLaMA-3-8B-Lizard(Ours) 6000 8000* 15000 1200 1420 1400 300 100 0.04 0.04 20 0.04 0.04 81.9 82.1 79.9 81.0 78.7 80.1 80.0 81.0 80.4 81.5 80.9 76.8 81.8 81.1 80.9 80.1 77.5 76.8 75.4 74.4 75.4 75.9 81.7 81.7 74.1 83.2 83.5 53.2 53.8 53.3 46.7 46.3 44.4 45.9 47. 45.8 54.9 54.9 48.0 55.8 56.7 80.7 81.0 79.1 77.9 75.1 75.2 77.6 78.6 77.1 80.7 79.7 70.8 79.8 79. 73.7 74.0 73.1 71.8 70.0 66.1 69.9 72.6 70.3 74.0 74.1 58.6 72.0 71.7 62.9 62.4 66.6 33.3 43.1 35.0 39. 34.2 51.4 52.8 43.2 60.8 61.2 74.1 74.4 73.1 71.0 69.4 68.2 69.6 71.1 69.9 74.5 74.2 65.6 74.5 74. Table 2: Comparison of hybrid softmax models on language understanding benchmarks. Model Training PiQA ARC-e ARC-c Hella. Wino. MMLU Tokens (B) acc acc_norm acc_norm acc (5-shot) (no MMLU) Avg. acc LLaMA-3-8B 15000 79.9 80.1 53.3 73. 79.1 66.6 73.1 72.0 Hybrid Softmax StripedHyena-Nous-7B Zamba-7B 78.8 81.4 77.2 74.5 Linearized (Keep 50% Full Attn.) Mamba2-LLaMA-3 LLaMA-3-8B-Lizard(Ours) 20 0.04 81.5 82.2 78.8 83. 40.0 46.6 58.2 55.9 66.4 76.4 71.5 73.6 76.4 80.2 79.5 81. 26.0 57.7 56.7 65.1 67.8 71.8 73.9 75.2 60.8 69.5 71.0 73. Experimental Setup: We conduct our experiments using two widely used Transformer-based LLMs: Mistral-7B [Jiang et al.] and Llama-3-8B [Grattafiori et al., 2024] as teacher models. For training, we utilize curated subset of 50K high-quality examples cleaned Alpaca dataset 3 [Taori et al., 2023]. , γi = σ(Wγxi), with γi and By default, we use scalar gating structure, where Γi = γi1 Wγ Rd1. We also explore various gating module designs, which are discussed in Section 4.4. In the First Stage, we train the feature maps ϕq, ϕk, and the gating parameter matrix Wγ jointly to approximate full softmax attention and RoPE patterns. For the sliding window attention module, we use small window size of = 128 and = 4 meta tokens. In the Second Stage, we employ Low-Rank Adaptation (LoRA) [Hu et al., 2022] for parameter-efficient fine-tuning. Specifically, LoRA is applied to the projection matrices WQ, WK, and WV , with default rank = 8 and scaling factor α = 16. Both stages are trained for 2 epochs, corresponding to 20M tokens per stage. We use the AdamW optimizer and cosine learning rate schedule with 10% linear warmup. Training is performed using Fully Sharded Data Parallelism (FSDP-2) [Zhao et al., 2023] across 8A100 80GB GPUs. The peak learning rate is set to 1 103 for the first stage and 5 104 for the second stage. We adopt global batch size of 8, with each example having maximum sequence length of 2048 tokens. 7 Figure 4: Needle-in-a-Haystack evaluation. Each cell shows retrieval accuracy by sequence length (X-axis) and target distance (Y-axis). Green indicates success; red indicates failure. The white dashed line marks the max training length. 4.1 Language Modeling Benchmarks We evaluate Lizardon six popular language understanding benchmarks from the LM Evaluation Harness (LM Eval) 4 [Gao et al., 2024], including PiQA [Bisk et al., 2020], ARC-easy (ARC-e) and ARC-challenge (ARC-c) [Clark et al., 2018], HellaSwag (Hella.) [Zellers et al., 2019], WinoGrande (Wino.) [Sakaguchi et al., 2021], and MMLU [Hendrycks et al., 2020]. Notably, Lizardis able to closely recover the performance of the teacher model and achieves near-lossless accuracy across tasks in average, demonstrating that it preserves the original models language understanding capabilities. We compare Lizardagainst two groups of baselines. The first group, presented in Table 1, consists of subquadratic LLMs, including models pre-trained from scratch with linear or subquadratic attention mechanisms, such as Mamba [Gu and Dao, 2023], RWKV-6 [Peng et al., 2023a], TransNormerLLM7B [Qin et al.], Hawk and Griffin [De et al.], as well as linearized variants such as SUPRA [Mercat et al., 2024], LoLCATs [Zhang et al., 2024], and Mamba2LLaMA [Wang et al., 2024]. Lizardconsistently outperforms prior approaches, particularly on the 5-shot MMLU benchmark, where it achieves an 18% improvement over previous methods with similar extrapolation capabilities. Compared to LoLCATs, which does not generalize beyond training context, Lizardscores 8 points higher on the 5-shot MMLU. The second group shown in Table 2, Hybrid Softmax Architectures includes models that combine full softmax with subquadratic attention layers. We compare with models such as StripedHyenaNous-7B [Poli et al., 2023] and Zamba-7B [Glorioso et al., 2024]. Following the same configuration of Mamba2-LLaMA-3-8B [Wang et al., 2024], which retains 50% softmax layers. On 5-shot MMLU, Lizardscore 65.1, closely matching the 66.1 score of the original LLaMA-3-8B teacher model, while outperforming all hybrid baselines. 4.2 Recall Evaluations To evaluate our models performance on associative recall tasks, where the goal is to retrieve specific information from long context, we use the Needle-in-a-Haystack setup. To better assess retrieval capabilities, we design synthetic passkey-retrieval dataset tailored for this purpose. As illustrated in Figure 3, each input sequence contains five randomly generated passkeys, each of length 5-8 tokens, inserted at random positions within long sequence. At test time, the model is prompted to retrieve one selected passkey from the five embedded within the sequence. We generate 10,000 synthetic examples, train the model on sequences of length 2048, and evaluate its performance on longer Figure 3: Example from the synthetic passkey retrieval dataset. 3https://huggingface.co/datasets/yahma/alpaca-cleaned 4https://github.com/EleutherAI/lm-evaluation-harness Table 3: Performance comparison of different gating designs and their parameterizations. Model Gating Parameterization Lizard (Ours) Mamba-2 [Dao and Gu, 2024] GLA [Yang et al., 2023] 1D-Pooling Γi = γi1 Γi = γi1 , γi = σ(Wγxi) , γi = exp ( softplus(xiWγ) exp(a)) Γi = σ (xiWγ1 Wγ2 ) Γi = σ(P ooling(kt)) Learnable Parameters Wγ Rd1 Wγ Rd1, Wγ1 Rd16, Wγ2 R16d N/A MMLU 5-shot 61.2 57.6 53.5 44.1 sequences ranging from 2048 to 8192 tokens to assess its generalization and recall capabilities in long-context settings. We evaluate Lizardand compare its performance against the teacher model and LoLCATs [Zhang et al., 2024], recent state-of-the-art linearization method. Figure 4 reports the results of all three models on the associative recall test set. We find that Lizardsignificantly outperforms LoLCATs in both associative recall accuracy and length generalization. Notably, Lizardis able to perfectly retrieve the correct passkey across various context lengths, while LoLCATs fails when the sequence length exceeds the training window. This highlights the strength of the gated recurrent structures, which effectively compresses global context and does not rely solely on the expressiveness of local sliding window attention. 4.3 Generation Efficiency We assess the efficiency of Lizardby comparing its throughput and memory usage to that of the teacher model across input sequence lengths from 1K to 32K, using batch size of 16. As shown in Figure 5, the teacher model with FlashAttention-2 [Dao, 2023] encounters outof-memory (OOM) issues at longer sequence lengths. In contrast, Lizardmaintains constant memory consumption and stable throughput throughout. All experiments were conducted on an NVIDIA A100 80GB GPU. Figure 5: Throughput and memory comparison. 4.4 Ablation and Architectural Analysis Gated Structures Design Table 3 presents comparison of different gating designs and parameterizations based on recent architectural advances. We experiment with multiple formulations, ranging from minimal scalar gates to more expressive multi-layer projections. Our results show that the Lizard parameterization achieves the highest performance on the 5-shot MMLU benchmark. While complex gated recurrent structures offer greater modeling flexibility, we observe that their effectiveness is limited by the need to initialize these modules from scratch. Heavier parameterizations can lead to overfitting or instability during fine-tuning, ultimately degrading performance. In contrast, lightweight designs with minimal additional parameters are easier to train and generalize better, resulting in stronger overall performance. Additionally, we evaluate pooling-based variant where the gating values are derived from 1D pooling over key vectors, eliminating the need for any learnable parameters. However, this configuration results in significant drop in performance. This suggests that having learnable gating mechanism, even with minimal parameters, is crucial for capturing meaningful temporal patterns and maintaining strong performance on downstream tasks. Table 4: 5-shot MMLU performance with varying window and meta token sizes Effect of window and meta memory size Table 4 presents an ablation study evaluating the impact of varying the local attention window size (w) and the number of meta tokens (m) on 5shot MMLU performance. While increasing the window size can improve performance, it does not guarantee consistent gains. For instance, performance peaks at = 128 for = 4, but drops notably at = 256. This indicates that simply enlarging the local attention span is not always beneficial. = 32 = 64 = 128 = 256 = 2 = 4 = 6 42.2 44.6 43.8 58.6 61.2 60. 51.3 52.4 52.4 54.1 57.6 57."
        },
        {
            "title": "5 Conclusion",
            "content": "We present Lizard, linearization framework that transforms pretrained Transformers into scalable, subquadratic architectures. Our method achieves near-parity with the teacher model on standard language modeling benchmarks, including 5-shot MMLU, while enabling constant-memory inference and strong length generalization. Through extensive experiments and ablations, we demonstrate that lightweight gating designs and hybrid attention improve both performance and efficiency, making Lizarda practical solution for long-context language modeling."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. URL https://arxiv. org/abs/2402.19427, page 50. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. AQ Jiang, Sablayrolles, Mensch, Bamford, DS Chaplot, de Las Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b, arxiv abs/2310.06825 (2023). URL: https://api. semanticscholar. org/CorpusID, 263830494. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. Bin Lin, Chen Zhang, Tao Peng, Hanyu Zhao, Wencong Xiao, Minmin Sun, Anmin Liu, Zhipeng Infinite-llm: Efficient llm service for long context with Zhang, Lanbo Li, Xiafei Qiu, et al. distattention and distributed kvcache. arXiv preprint arXiv:2401.02669, 2024. Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1:3, 2020. Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models. arXiv preprint arXiv:2405.06640, 2024. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1404814077, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.936. URL https://aclanthology.org/ 2023.findings-emnlp.936/. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023b. Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. Stripedhyena: Moving beyond transformers with hybrid signal processing models. GitHub repository, 12, 2023. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, et al. Transnormerllm: faster and better large language model with improved transnormer, 2024. URL https://arxiv. org/abs/2307.14995. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 11 Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models, 2024. URL https://arxiv. org/abs/2406.07887. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37:6243262457, 2024. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800. Association for Computational Linguistics, 2019. URL https://aclanthology. org/P19-1472. Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Ré. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry, 2024b. URL https://arxiv. org/abs/2402.04347. Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254, 2024. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Despite the promising performance and efficiency gains demonstrated by Lizard, our approach has two key limitations. First, Lizardstill relies on strong pretrained backbone to achieve high quality. As with many recent distillation-based or hybrid architectures, the success of our method depends heavily on the expressiveness and generalization capacity of the teacher model. Without access to high-quality pretrained model (e.g., Llama-3-8B), the performance of Lizardmay degrade significantly, especially on complex reasoning and multilingual tasks. Second, Lizardinherits the inherent tradeoffs present in linear attention mechanisms. While our design enables constant-time and constant-memory inference with infinite context length, it still exhibits recall-memory tradeoff. That is, models with fixed-size state representations, such as our gated linear attention, may underperform in recall-intensive tasks compared to full attention models, which maintain growing key-value cache. This aligns with recent findings that efficient alternatives to attention often struggle to retain long-range information critical for grounding generations in earlier context. As result, while Lizardexpands the throughput-recall Pareto frontier, it does not eliminate the tradeoff entirely."
        },
        {
            "title": "B Inference Efficiency",
            "content": "Figure 6: Inference speed comparison between GLA and Lizard kernel.. Hardware-aware GLA in Lizard. We benchmark the Lizard kernel under BF16 precision with batch size = 16, sequence length = 2048, number of heads = 32, and head dimension Dhead = 128. As shown in Figure 6, our hardware-aware implementation of GLA achieves 3.25 ms per forward pass, representing 32% reduction in inference time compared to the original Gated Linear Attention 5 kernel (4.30 ms). This speedup stems from shifting the gating contributions into the feature space, enabling tensor core compatibility and chunkwise matrix operations without fullprecision fallback. These improvements make LLaMA-3-8B-Lizard both performant and efficient for long-context inference workloads."
        },
        {
            "title": "C Ablation Study",
            "content": "Impact of Architectural Components We conduct an ablation study to evaluate the contribution of each module in Lizard. As shown in Table 5, removing the sliding window attention (SWA), gated module, or attention approximation significantly degrades performance on both the 5-shot MMLU benchmark and the average across other language understanding tasks. Interestingly, replacing LoRA with full fine-tuning results in similar performance, indicating the effectiveness of parameter-efficient adaptation. 5https://github.com/fla-org/flash-linear-attention 13 Table 5: Ablation results on MMLU (5-shot) and average performance across other tasks. We report the impact of removing individual components from Lizard. Model MMLU (5-shot) Avg. (no MMLU) LLaMA-3-8B-Lizard - SWA - Gated Module - Full FT (no LoRA) - w/o Attention Approximation 61.2 39.7 42.2 61.4 50.8 74.6 72.9 70.1 74.6 71.2 Effect of LoRA Rank We study how varying the LoRA rank impacts model performance across downstream tasks. As shown in Table 6, we find that LoRA rank of 8 is sufficient to achieve strong performance, matching or even surpassing full fine-tuning on both 5-shot MMLU and the average across other tasks. Table 6: Effect of LoRA Rank. LoRA Rank MMLU (5-shot) Avg. (no MMLU) 4 8 16 32 64 59.7 61.2 60.6 61 59.2 74.1 74.6 73.3 74."
        },
        {
            "title": "D Experimental Details",
            "content": "Table 7: Hyperparameters for the experiments. Resources Distributed Setup 8xA100 80GB Fully Sharded Data Parallel (FSDP-2) Model Precision Sequence length Hedgehog Feature Dimension Hedgehog Feature Activation bfloat16 2048 128 Softmax Optimizer and LR Schedule Optimizer (β1, β2), ϵ Learning Rate min ratio Global batch size Micro batch size Gradient Clipping Learning rate Rchedule AdamW (0.9, 0.99), 1.0e-8 0.1 8 1 1.0 Cosine Annealing LR Stage 1: Attention Approximation Number of tokens Peak Learning Rate 20M 1.0 103 Stage 2: Fine-tuning Number of tokens Peak Learning Rate LoRA rank and alpha LoRA dropout LoRA projections 20M 5.0 104 = 8, α = 16 0.0 Wq, Wk, Wv Hyperparameters All model and training hyperparameters are summarized in Table 7. For the learning rate, we performed an initial sweep over {1e-2, 5e-3, 1e-3, 5e-4, 1e-4}. We did not tune the batch size. For the other designs, we adopted the default values used by prior work [Zhang et al., 2024]. Evaluation on small-size LLMs Table 8: Evaluation results of small-size LLMs and their variants across multiple benchmarks. Lizard consistently outperforms LoLCATs and closely matches the performance of the teacher models. Model PiQA ARC-e ARC-c acc_norm acc_norm Hella. Wino. MMLU (5-shot) acc Avg. (no MMLU) 36.4 35.1 36.5 46.0 45.2 63.8 63.7 64. 73.6 74.3 60.0 61.5 59.7 69.9 69.3 31.0 27.3 29.8 56.2 53.4 59.9 59.6 60. 68.1 68.2 Avg. 55.1 54.2 55.1 66.1 65.7 Llama-3.2-1B LoLCATS Lizard Llama-3.2-3B Lizard acc 74.1 74.6 74.8 76.4 76.8 acc 65.4 63.0 65.6 74.7 75."
        },
        {
            "title": "F Sample Generations",
            "content": "Prompt Why did we linearize the chungus among us? LoLCATs Response [Zhang et al., 2024] Linearizing the chungus among us is process of organizing and arranging the information in logical and coherent manner. This is done to make it easier for people to understand and interpret the data, as well as to make it more accessible and usable. By linearizing the data, we are able to extract meaningful insights and patterns from the information, which can then be used to make informed decisions and take action. Lizard Response Im sorry, but dont have any information about linearizing the \"Chungus Among Us.\" Could you please provide more context or clarify what you mean by \"linearize the chungus among us\"? Softmax Attention Im sorry, but cannot respond to this question as do not have the necessary information to provide meaningful response. Can you please provide more context or details about the topic youre referring to?"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Oregon"
    ]
}