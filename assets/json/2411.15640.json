{
    "paper_title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset",
    "authors": [
        "Tobi Olatunji",
        "Charles Nimo",
        "Abraham Owodunni",
        "Tassallah Abdullahi",
        "Emmanuel Ayodele",
        "Mardhiyah Sanni",
        "Chinemelu Aka",
        "Folafunmi Omofoye",
        "Foutse Yuehgoh",
        "Timothy Faniran",
        "Bonaventure F. P. Dossou",
        "Moshood Yekini",
        "Jonas Kemp",
        "Katherine Heller",
        "Jude Chidubem Omeke",
        "Chidi Asuzu MD",
        "Naome A. Etori",
        "Aimérou Ndiaye",
        "Ifeoma Okoh",
        "Evans Doe Ocansey",
        "Wendy Kinara",
        "Michael Best",
        "Irfan Essa",
        "Stephen Edward Moore",
        "Chris Fourie",
        "Mercy Nyamewaa Asiedu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 2 0 4 6 5 1 . 1 1 4 2 : r AfriMed-QA: Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset Tobi Olatunji1,2,4, Charles Nimo2, Abraham Owodunni1,3, Tassallah Abdullahi4,12, Emmanuel Ayodele1, Mardhiyah Sanni1,4, Chinemelu Aka1, Folafunmi Omofoye4, Foutse Yuehgoh4, Timothy Faniran4, Bonaventure F. P. Dossou4,10, Moshood Yekini4, Jonas Kemp6, Katherine Heller6, Jude Chidubem Omeke4, Chidi Asuzu MD4, Naome A. Etori4,11, Aimérou Ndiaye5, Ifeoma Okoh5, Evans Doe Ocansey5, Wendy Kinara7, Michael Best2, Irfan Essa2,6, Stephen Edward Moore8, Chris Fourie9, Mercy Nyamewaa Asiedu6 1Intron, 2Georgia Institute of Technology, 3The Ohio State University, 4BioRAMP, 5Masakhane, 6Google Research, 7Kenyatta University, 8University of Cape Coast, 9SisonkeBiotik, 10MILA Quebec, 11University of Minnesota, 12Brown University tobi@intron.io"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large language model (LLM) performance on medical multiplechoice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-andmiddle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA , the first largescale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve passing score. Interestingly, human evaluations show consistent consumer preference for LLM answers and explanations when compared with clinician answers."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have gained popularity in specialized domains such as finance (Wu et al., 2023b), medicine (Singhal et al., 2022b), and climate (Thulke et al., 2024). In the medical field, LLMs like Med-PaLM (Singhal et al., 1Authors marked with are senior authors. 2Our dataset is available at: https://huggingface.co/datasets/intronhealth/afrimedqa_v2 1 2022b), PMC-LLaMA (Wu et al., 2023a), and GPT4 (Achiam et al., 2023) have shown impressive performance in tasks such as summarizing clinical notes and answering medical questions with high accuracy (Liu et al., 2024c; Eriksen et al., 2023; Abdullahi et al., 2024; Liu et al., 2024a). Especially in low-resource settings, these models have the potential to improve clinician productivity, accessibility, operational efficiency, and enable multilingual clinical decision support (Yang et al., 2023; Gangavarapu, 2023). Despite their success on existing medical benchmarks, it is uncertain whether these models generalize to tasks involving linguistic variations, even within English, localized cultural contexts, and region-specific medical knowledge, highlighting the need for more diverse benchmark datasets. Current evaluations rely on publicly available digital resources (Kung et al., 2023; Jin et al., 2019, 2021), but these may not translate to out-of-distribution datasets, such as those from African countries. To address this gap, we introduce AfriMed-QA, dataset of 15,275 English, clinically diverse questions and answers, 4,000+ expert multiple-choice questions (MCQs) with answers, over 1,200 openended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ), to rigorously assess LLM performance for correctness and demographic bias. We evaluate 30 large, small, open, closed, biomedical, and general LLMs using quantitative and qualitative approaches. While the development of the dataset is still in progress, this work establishes foundation for acquiring diverse and representative health benchmark datasets across LMICs. Figure 1: Methodology Overview Table 1: Comparative analysis of the AfriMed-QA dataset with health datasets. Feature AfriMed-QA BioASQ (Krithara et al., 2023) MedQA-SWE (Hertzberg and Lokrantz, 2024) MedQA (Jin et al., 2020) MedMCQA (Pal et al., 2022b) PubMedQA (Jin et al., 2019) MMLU (Hendrycks et al., 2020) HealthSearchQA (Singhal et al., 2022a) Dataset Size Question Types Clinical Scenarios Answer Options Correct Answers Answer Rationale Question Source 15,275 MCQ, SAQ, Consumer Queries 2-5 (MCQs) 4,721 yes/no, Factoid, List, Summary 3,180 MCQ 5 (MCQs) 12,723 MCQ 4 (MCQs) 193,155 MCQ 4 (MCQs) 1,000 Yes/No/maybe, Factoid, List 15,908 MCQ 4 (MCQs) 3,173 Consumer Queries"
        },
        {
            "title": "LLMs",
            "content": "The Open Medical LLM Leaderboard (Pal et al., 2024) tracks, ranks, and evaluates LLM performance on medical question-answering tasks across diverse datasets, including MedQA (USMLE) (Jin et al., 2021), PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022a), and subsets of MMLU (Hendrycks et al., 2020). These datasets assess various medical domains, such as clinical knowledge, genetics, and anatomy, through multiple-choice and open-ended questions. CMExam (Liu et al., 2023) from the Chinese National Medical Licensing Examination offers MCQs with detailed annotations, while Q-Pain (Loge et al., 2021), Medication QA (Abacha et al., 2019), LiveQA (Liu et al., 2020), MultiMedQA (Singhal et al., 2022a), and EquityMedQA (Pfohl et al., 2024) cover various medical QA challenges. Table 1 compares AfriMed-QA to other medical QA benchmarks."
        },
        {
            "title": "There have been several studies evaluating LLMs\nfor medical standardized exams and for various",
            "content": "clinical tasks using approaches like zero-shot, finetuning, developing benchmarking metrics and running human evalutations (Jahan et al., 2024; Singhal et al., 2022b; Fleming et al., 2024; Umapathi et al., 2023; Chen et al., 2024). These studies underscore the importance of diverse, high-quality datasets and human assessments to capture the nuances of real-world medical applications. Reddy et al. (Reddy, 2023) proposed the TEHAI framework to assess the translational and governance aspects of LLMs in healthcare, emphasizing contextual relevance, safety, ethical considerations, and efficiency. Our evaluation extends TEHAI and work from Singhal et al. (Singhal et al., 2022b), and Pfohl et al. (Pfohl et al., 2024) by incorporating specialty and region-specific dimensions, ensuring comprehensive assessment of LLMs. Metrics such as local expertise, harmfulness, and bias are included to cover ethical dimensions and governance. By addressing correctness, omission, hallucination, and reasonableness, we thoroughly evaluate AI systems in healthcare. The AfriMed-QA dataset aims to (1) integrate geo-culturally diverse datasets, specifically those from African LMICs that have historically relied on paper-based records, and local health data and 2 are underrepresented in LLM training and evaluation; and (2) expand healthcare LLM benchmark datasets to include African consumer/patient-based queries. This enables LLM training and evaluation on broad spectrum of medical data, creating more robust, inclusive, and practical AI solutions for Africa-centric applications."
        },
        {
            "title": "3 AfriMed-QA Dataset",
            "content": "We introduce AfriMed-QA, the first large-scale Pan-African multispecialty medical QuestionAnswer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. Figure 1 details our methodology and data collection process. Question Tier Type Count Total Expert MCQ SAQ Crowdsourced MCQ SAQ CQ 3,910 359 129 877 10,000 Total 4, 11,006 15,275 Table 2: Dataset statistics Table 2 and 9 shows dataset statistics. Multiplechoice (MCQ) professional medical exam questions include question, 2-5 alternative answer options, the correct answer(s), and the rationale for the correct answer (answer option count distribution is shown in Appendix Tab 8). Open-ended short answer questions (SAQ) require short essay answers, usually 1-3 paragraphs long. Consumer queries (CQs) deepen our understanding of LLM response quality to consumer queries. To maximize the diversity of consumer questions, we leveraged curated list of 472 medical conditions, symptoms, and patient complaints common in African communities across 32 specialties, to create culturally appropriate prompts that help elicit diverse questions from clinical and non-clinical crowd workers. Example MCQ, SAQ, and CQ questions, as well as CQ human prompt templates are shown in Figure 2. This dataset was crowd-sourced from 621 contributors (Female 55.56%, Male 44.44%) from over 60 medical schools across 16 countries  (Table 9)  , covering 32 medical specialties including Obstetrics & Gynecology, Neurosurgery, Internal Medicine, Emergency Medicine, Medical genetics, Infectious Disease, and others. Appendix Table 11 Figure 2: Question Samples for multiple choice questions (MCQs), Short Answer Questions (SAQs), and Consumer Queries shows the Specialty distribution. Human answers and explanations are provided for 5,444 questions. During human evaluation, 379 raters (58 clinicians, 321 non-clinicians) contributed 37,435 model ratings."
        },
        {
            "title": "3.1 Data Collection",
            "content": "We adapted web-based platform previously developed by Intron Health 1 to crowd-source accented and multilingual clinical speech data at scale across Africa (Olatunji et al., 2023). Custom user interfaces were developed to collect each question type, for quality reviews, and for blind human evaluation of LLM responses. The MCQ User Interface and other details about the data collection tool can be 1Intron Healths biomedical crowd-sourcing platform https://speech.intron.health found in Appendix Figure 13. 3.2 Contributor Recruitment and Instructions for Data Collection and Evaluation Contributors: Medical trainees and clinicians were recruited to contribute questions through referrals from existing Intron Healths web-based platform contributors, medical associations, and via social media. Experts (Professors) were recruited from Medical Schools in 5 countries as shown in Appendix Table 10a. Recruitment efforts were targeted at African clinicians from subSaharan African countries prioritized by population size. To maximize geographic representation, each contributor was limited to maximum of 300 questions and answers. Contributions were paid at $5 to $100/hr based on task difficulty and expertise. Instructions: For MCQs and SAQs, clinician contributors were instructed to 1) input question, answer options, correct answer(s), rationale/explanation for correct answer, and question metadata into the interface. Experts provided MCQ answers with no explanations. 2) prioritize questions relevant to African healthcare, from African sources alone (e.g. no USMLE prep questions). For CQs, all contributors (clinicians and non-clinicians) were granted access to the CQ interface, where healthrelated questions were provided in response to prompts. CQ Contributors were instructed to ask one question per prompt, draw from practical community experience about the condition, and assume questions were directed at their local physician. Clinician contributors were then granted privileged access to dedicated interface to provide human answers to consumer queries. Human Evaluations: Consumers provided ratings for LLM responses to CQs on relevance, helpfulness, and local context, but NOT correctness. Due of the higher expertise required, only confirmed clinicians were granted access to projects rating MCQ, SAQ, and CQ answers. Clinician status was confirmed through submitted credentials and background checks reviewing publicly available information about them. Raters were randomly assigned (double-blind) to the answer source (human or LLM). More details on human evaluations are provided in Section 4.3 Quality Review: We utilized the contributor quality review process described in (Olatunji et al., 2023). Contributors were rigorously evaluated by team of clinicians. Question quality, answer quality, and rationale were cross-checked against authoritative clinical reference material. Only contributors with 80% or higher positive ratings were granted access to contribute to the dataset."
        },
        {
            "title": "4 Approach",
            "content": "4.1 LLM Selection We evaluate 30 LLMs  (Table 3)  including opensource and proprietary LLMs, general-purpose and biomedical LLMs, mixture-of-experts, and models of varying sizes from 3B to over 540B parameters (Chowdhery et al., 2023)). Of these, 13 are proprietary while 17 are open-source. We evaluate LLMs like Phi-3 (Abdin et al., 2024), GPT4 (Achiam et al., 2023), MedLM, Claude 3 (Anthropic, 2023), OpenBioLLM (Pal and Sankarasubbu, 2024), Gemini (Team et al., 2023), Meditron (Chen et al., 2023), and MedLlama (Wu et al., 2023a)."
        },
        {
            "title": "4.2 Quantitative Evaluation",
            "content": "For multiple-choice tasks, accuracy is measured by comparing LLMs single-letter answer choice [A,B,C,D,E] with the reference. For open-ended questions, semantic similarity is measured using BertScore, comparing the generated response from the language model against reference answer (Zhang* et al., 2020)."
        },
        {
            "title": "Evaluation Axes",
            "content": "LLM responses to fixed subset of questions (n=3000, randomly sampled) were sent out for human evaluation on the Intron Health crowdsourcing platform. Adapting the evaluation axes in (Singhal et al., 2023), we collected human evaluations in two categories: (1) Non-clinicians were instructed to provide ratings to CQ LLM responses to determine if answers were relevant, helpful, and localized; (2) Clinicians were instructed to provide ratings to the LLMs MCQ, SAQ, and CQ responses to determine if answers were correct and localized, if omissions or hallucinations were present, and if potential for harm exists. Evaluation axes and exact instructions are detailed in Appendix section A.1. Ratings were on 5-point scale representing the extent to which the criteria were met. 1 represents No\" or completely absent\", and 5 represents Yes\" or Absolutely present\". Raters were blinded to the answer source (model name or human). Each 4 Figure 3: AfriMedQA: Expert-MCQ accuracy, SAQ, and CQ Bertscore (a) Top: MedQA vs AfriMedQA MCQ Accuracy. Bottom: Effect of Explanations (b) MCQ accuracy by specialty Figure 4: Breakdown of MCQ accuracy by specialty and dataset rater evaluated multiple LLMs or human answers in random blind sequence."
        },
        {
            "title": "4.4 Experiment Setup",
            "content": "Checkpoints for open-source models were sourced from HuggingFace and Googles Vertex AI Studio, while proprietary models were accessed through developers API using default hyperparameters. More details about the hyper-parameters used in this study are available in the Appendix in Table 14. Aligning with our community participatory design approach, experiments by multiple collaborators ran on various hardware types including NVIDIA L4, NVIDIA T4, and A100. 5 and medium models (11B-70B) scoring between 60 and 75% (details in Appendix Table 3). As the Gemma-2, Claude-3, Phi-3, and Mistral model families results show, models of different sizes trained on similar datasets demonstrate better generalization capabilities with increasing parameter count. Mixture-of-Expert (MoE) models Mistral-8x7B outperforms its biomedical and general 7B variants by wide-margin further confirming the correlation between model capacity and performance. Blind clinician evaluation of LLM answers to open and closed-ended questions (Figure 6) are consistent with this trend, showing larger models are more correct and less susceptible to hallucinations and omissions than small models. This trend may be unfavorable to low-resource settings where on-prem or edge deployments with smaller specialized models are preferred."
        },
        {
            "title": "6.2 Localization is still a challenge",
            "content": "Figure 4a (top left) reveals an unmistakable performance gap between USMLE (MedQA) (orange line) and AfriMed-QA Expert MCQs (blue line), with proprietary GPT-4o, Claude-3.5-sonnet and Gemma-2B showing an 8.86, 5.57, and 15.5 point drop in performance (Appendix Table 4) which may indicate bias attributable to their training data distribution. Figure 4a further shows differences in question difficulty by source. Since MCQs that mention African cities or locations (e.g. 50-year old patient with recent travel to Uganda...) were mostly contributed by Trainees (green line), relatively higher LLM accuracy on this subset (for small and large LLMs) suggests they were relatively easier for LLMs when compared with Expert MCQs (blue line) and MCQs that did not mention African geographies (red line). This finding is counter-intuitive and requires further investigation. Figure 5: MCQ accuracy by country"
        },
        {
            "title": "5.1 Benchmark evaluations",
            "content": "AfriMed-QA MCQ, SAQ, and CQ evaluation results are shown in Figure 3. Accuracy ranges from 0.17 (Gemma-2B, the smallest LLM in this study) to 0.79 (GPT-4o). GPT-4o, Claude-3.5-sonnet and Llama3-405b are the top 3 most accurate LLMs (granular details are provided in Appendix Table 3). Using Base Prompts (Appendix 7), we assess LLM performance by country, (Fig 5), specialty (Fig 4b), data subset (Fig 4a), and with or without explanations (Fig 4a)."
        },
        {
            "title": "5.2 Human evaluations",
            "content": "Figure 6 shows results from clinician and nonclinician human evaluators on the dataset across various axes for LLM and Human responses. Figure 6e shows non-clinician ratings of responses to consumer queries from LLMs and humans (blinded). Overall we find that LLMs are rated better on CQ responses and with less variability compared to humans under blinded settings."
        },
        {
            "title": "6 Discussion",
            "content": "The experimental results of our study on AfriMedQA reveal several key insights and trends."
        },
        {
            "title": "6.1 The dominance of large models may be",
            "content": "unfavorable for low-resource settings Figure 3 shows wide range of LLM accuracies on Expert MCQs with the best models (large, 100B2T, proprietary) scoring over 75%, smaller models (under 10B) clustering in the 40-60% range,"
        },
        {
            "title": "6.3 Evidence of progress in LLM reasoning",
            "content": "abilities Performance of the GPT model series (3.5, 4, and 4o in Fig 3) show an interesting temporal trend with newer models scoring significantly higher on both MedQA and AfriMed-QA showing strong evidence of progress with LLM reasoning abilities not attributable to question memorization since the novel AfriMed-QA questions were not part of any GPT versions training data. 6 (a) Correctness (b) Irrelevant information (c) Omission of relevant information (d) Possibility of harm Figure 6: Clinician and consumer blind evaluations of human and LLM answers showing mean ratings and confidence intervals across various axes. (e) Consumer ratings of CQs 7 6.4 Domain-specific biomedical LLMs still struggle Figure 3 shows general models outperform and generalize better than biomedical models of similar size (8B and 70B). This counter-intuitive result could be due to the size limitations of open biomedical models in our study or it could indicate LLMs overfit the specific biases and nuances of their training data, making them less adaptable to the unique characteristics of the AfriMed-QA dataset. This supports the hypothesis that large language models (LLMs) may carry inherent biases based on their training data, but seem especially profound when finetuned for specific tasks. 6.5 Specialties must select LLMs with caution Figure 4b shows clear top-down, left-right trend indicating which LLMs are more reliable in certain specialties. Several small to medium LLMs in the right half of the graph are clearly less adapted to African healthcare settings. While larger closed LLMs seem to generalize across specialties, our results reveal that LLMs perform better on Medical Specialties (Rheumatology, Nephrology, Gastroenterology, Endocrinology, Pulmonary, etc) when compared with other specialties like Surgery, Pathology, Pediatrics, Infectious Diseases, and Obstetrics & Gynecology that are very important in LMICs, striking trend that requires further investigation."
        },
        {
            "title": "6.6 Performance variation by country",
            "content": "Figure 5 shows clear difference in difficulty of expert question from South Africa and Nigeria for LLMs. This requires further investigation but could be result of differences in specialty distributions of expert MCQs per country. For example, expert questions from South Africa are dominated by Pediatrics, difficult specialty for LLMs as shown in Figure 4b while significant number of Pathology and Obstetrics and Gynecology questions come from Nigeria. Country-Specialty counts are detailed in Appendix Tables 7, 6, and 10."
        },
        {
            "title": "6.7 Positive and negative insights from LLM",
            "content": "explanations We investigated the effect of generating explanations on LLM accuracy (Fig 4a bottom left) and found that, contrary to the general notion that model explanations improve LLM accuracy (Wei et al., 2022), in the context of MCQs, postprocessing (regex or pattern matching) challenges with automatically extracting the answer option selected from the explanation led to subpar results and LLMs scored higher without explanations (Appendix 12). For example, Claude Opus generally struggled with pattern consistency, producing variations like The most appropriate ... <option>\" or The doctor should respond ... <option>\" instead of simply providing its answer Option B\" followed by its rationale. This highlights challenges with the ability of LLMs to produce consistent or structured outputs in response to instructions (Liu et al., 2024b). 6.8 Consumers prefer LLM answers Consumer and clinician human evaluation of LLM answers to CQs (Fig 6e) revealed an overwhelming preference for LLM responses as they were consistently rated to be more complete, informative, and relevant when compared with clinican answer brevity (Fig 2). Clinician answers to consumer queries were rated highest on omission of relevant information."
        },
        {
            "title": "6.9 Potential for harm, omissions, and",
            "content": "hallucinations still persist Figure 6e showed that smaller open general and biomedical LLMs like Llama-3-8b and JSL-Medllama-8b had the highest count of answers with hallucinations, omissions, and the potential for harm in MCQ, open-ended SAQ, and CQ answers. Small and Medium biomedical LLMs like OpenBioLLM (8B and 70B) also had higher tendency to hallucinate and omit important information. Smaller LLMs also had notable difficulty with questions that require selecting the \"most likely\" clinical management step, intervention, or \"most common\" diagnosis, particularly evident in epidemiologicrelated questions, highlighting the cultural and geographic variability inherent in the practice of medicine around the world further substantiating the importance of datasets like AfriMed-QA."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce AfriMed-QA, the first large-scale multi-specialty Pan-African medical Question-Answer (QA) dataset comprised of 15k MCQs, SAQs and CQs, designed to evaluate and develop equitable and effective LLMs for African healthcare. We quantitatively and qualitatively evaluate thirty open and closed-source LLMs demonstrating performance variability across specialties, and geographies."
        },
        {
            "title": "8 Limitations and Future Work",
            "content": "large-scale, multiAlthough this is the first specialty, indigenously sourced Pan-African dataset of its kind, it is by no means complete. Over 60% of the expert MCQ questions came from West Africa. We are already working to expand representation from more African regions and the Global South. We also recognize that medicine is inherently multilingual and multimodal and we plan to expand beyond English-only text-based question answering to non-English official and native languages as well as incorporate multimodal (e.g. visual and audio) question answering. Despite these limitations, our rigorous contributor screening, quality assurance protocols, and successful track record with crowd-sourced datasets give us confidence in AfriMed-QAs value for LLM development. We recommend its use for benchmarking and finetuning, recognizing its potential to drive advancements in medical LLMs that are culturally attuned to the unique needs of African populations and other regions in the Global South."
        },
        {
            "title": "Acknowledgements",
            "content": "Support for this project was provided by Google Research, PATH, and the Bill & Melinda Gates Foundation [Gates INV-068056]. We appreciate the excellent execution by the Intron Health Data Team who managed this pan-African data collection effort including recruiting contributors, developing custom UIs, quality reviews, and multi-currency contributor payments. We appreciate the invaluable support provided by the BioRAMP researchers, whose collaboration and insights have been fundamental to our research. We also extend our thanks to the Masakhane Community for their expert contributions, the Sisonke Biotik community for their enthusiastic participation and commitment, and the Federation of African Medical Students Associations [FAMSA] for their relentless recruitment and community engagement. These partnerships have been instrumental in enhancing the scope and impact of our work, and we appreciate their dedication to advancing knowledge in our field."
        },
        {
            "title": "References",
            "content": "Asma Ben Abacha, Yassine Mrabet, Mark E. Sharp, Travis R. Goodwin, Sonya E. Shooshan, and Dina Demner-Fushman. 2019. Bridging the gap between consumers medication questions and trusted answers. Studies in health technology and informatics, 264:2529. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Tassallah Abdullahi, Ritambhara Singh, Carsten Eickhoff, et al. 2024. Learning to make rare and complex diagnoses with generative ai assistance: Qualitative study of popular large language models. JMIR Medical Education, 10(1):e51391. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anthropic. 2023. Introducing claude. Accessed: 202405-31. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2024. Benchmarking large language models on answering and explaining challenging medical questions. arXiv preprint arXiv:2402.18060. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113. Alexander Eriksen, Sören Möller, and Jesper Ryg. 2023. Use of gpt-4 to diagnose complex clinical cases. Scott L. Fleming, Alejandro Lozano, William J. Haberkorn, Jenelle A. Jindal, Eduardo Reis, Rahul Thapa, Louis Blankemeier, Julian Z. Genkins, Ethan Steinberg, Ashwin Nayak, Birju Patel, Chia-Chun Chiang, Alison Callahan, Zepeng Huo, Sergios Gatidis, Scott Adams, Oluseyi Fayanju, Shreya J. Shah, Thomas Savage, Ethan Goh, Akshay S. Chaudhari, Nima Aghaeepour, Christopher Sharp, Michael A. Pfeffer, Percy Liang, Jonathan H. Chen, Keith E. Morse, Emma P. Brunskill, Jason A. Fries, and Nigam H. Shah. 2024. Medalign: clinician-generated dataset for instruction following with electronic medical records. Proceedings of the AAAI Conference on Artificial Intelligence, 38(20):2202122030. Agasthya Gangavarapu. 2023. Llms: promising new tool for improving healthcare in low-resource nations. 9 In 2023 IEEE Global Humanitarian Technology Conference (GHTC), pages 252255. IEEE. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300. Niclas Hertzberg and Anna Lokrantz. 2024. MedQASWE - clinical question & answer dataset for In Proceedings of the 2024 Joint InSwedish. ternational Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1117811186, Torino, Italia. ELRA and ICCL. Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, and Jimmy Xiangji Huang. 2024. comprehensive evaluation of large language models on benchmark biomedical text processing tasks. Computers in Biology and Medicine, 171:108189. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, et al. 2024a. Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Advances in Neural Information Processing Systems, 36. Michael Xieyang Liu, Frederick Liu, Alexander Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie Cai. 2024b. \" we need structured output\": Towards user-centered constraints on large language model output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 19. Qianying Liu, Sicong Jiang, Yizhong Wang, and Sujian Li. 2020. LiveQA: question answering dataset over sports live. In Proceedings of the 19th Chinese National Conference on Computational Linguistics, pages 10571067, Haikou, China. Chinese Information Processing Society of China. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What disease does this patient have? large-scale open domain question answering dataset from medical exams. ArXiv, abs/2009.13081. Yong Liu, Shenggen Ju, and Junfeng Wang. 2024c. Exploring the potential of chatgpt in medical dialogue summarization: study on consistency with human preferences. BMC Medical Informatics and Decision Making, 24(1):75. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567 2577, Hong Kong, China. Association for Computational Linguistics. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023. Bioasqqa: manually curated corpus for biomedical question answering. Scientific Data, 10:170. Tiffany Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel DiazCandido, James Maningo, et al. 2023. Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198. Cecile Loge, Emily L. Ross, David Yaw Amoah Dadey, Saahil Jain, Adriel Saporta, Andrew Y. Ng, and Pranav Rajpurkar. 2021. Q-pain: question answering dataset to measure social bias in pain management. ArXiv, abs/2108.01764. Tobi Olatunji, Tejumade Afonja, Aditya Yadavalli, Chris Chinenye Emezue, Sahib Singh, Bonaventure FP Dossou, Joanne Osuchukwu, Salomey Osei, Atnafu Lambebo Tonja, Naome Etori, et al. 2023. Afrispeech-200: Pan-african accented speech dataset for clinical and general domain asr. Transactions of the Association for Computational Linguistics, 11:16691685. Ankit Pal, Pasquale Minervini, Geert Motzfeldt, Aryo and Beatrice Alex. 2024. dreas Gema, lifescienceai/open_medical_llm_leaderboard. https://huggingface.co/spaces/ openlifescienceai/open_medical_llm_ leaderboard. AnPradipta openAdvancing open-source Ankit Pal and Malaikannan Sankarasubbu. 2024. large Openbiollms: language models for healthcare and life scihttps://huggingface.co/aaditya/ ences. OpenBioLLM-Llama3-70B. Hugging Face repository. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, LEI ZHU, and Michael Lingzhi Li. 2023. Benchmarking large language models on cmexam - comprehensive chinese medical exam dataset. In Advances in Neural Information Processing Systems, volume 36, pages 5243052452. Curran Associates, Inc. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022a. Medmcqa : large-scale multi-subject multi-choice dataset for medical domain question answering. In ACM Conference on Health, Inference, and Learning. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022b. Medmcqa: large-scale 10 multi-subject multi-choice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248260. PMLR. Stephen Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh, et al. 2024. toolbox for surfacing health equity harms and biases in large language models. arXiv preprint arXiv:2403.12025. Sandeep Reddy. 2023. Evaluating large language models for use in healthcare: framework for translaInformatics in Medicine tional value assessment. Unlocked, 41:101304. K. Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J. ColeLewis, Stephen J. Pfohl, Payne, Martin G. Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P. A. Mansfield, Blaise Agüera Arcas, Dale R. Webster, Greg S. Corrado, Yossi Matias, Katherine Hui-Ling Chou, Juraj Gottweis, Nenad Tomaševic, Yun Liu, Alvin Rajkomar, Joëlle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2022a. Large language models encode clinical knowledge. Nature, 620:172180. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022b. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. 2023. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian van Wyk, Abdallah Nasir, Hayden Goldstein, et al. 2024. Climategpt: Towards ai synthesizing interdisciplinary research on climate change. arXiv preprint arXiv:2401.09646. Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination test for large language models. arXiv preprint arXiv:2307.15343. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023a. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023b. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Rui Yang, Ting Fang Tan, Wei Lu, Arun James Thirunavukarasu, Daniel Shu Wei Ting, and Nan Liu. 2023. Large language models in health care: Development, applications, and challenges. Health Care Science, 2(4):255263. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models are not robust multiple choice selectors. In International Conference on Learning Representations."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Human Evaluation Axes we collected human evaluations in two categories: Non-clinicians: were instructed to provide ratings to LLM responses to determine if answers were relevant (Answer relates to question content), helpful (Answer is helpful and informative), and localized (Answer reflects African local context). These represent the axes of relevance, helpfulness, and bias. Clinicians: were instructed to provide ratings to the LLM responses using the following criteria: (a) Correctness: (Correct and consistent with scientific consensus) (b) Harm: (Possibility of harm), (c) Omission: Omission of relevant info, (d)Hallucination: Includes irrelevant, wrong, or extraneous information, (e) African: Requires African local expertise A.2 Samples of Prompts Used for the Model We show the output of our post-processing pipeline and how the texts are formatting for several question types. Our prompt design and figure formatting were inspired by (Zheng et al., 2024) 11 Base MCQ prompt: The following are multiple choice questions (MCQs). You should directly answer the question by choosing the correct option and then provide rationale for your answer. Instruction-tuning MCQ prompt: You are skilled doctor with years of experience in the medical field in Africa, working in hospital setting. Your expertise spans range of conditions from common ailments to complex diseases. As part of your commitment to ongoing medical education, you are evaluating set of multiple choice questions (MCQs) designed for medical students. Carefully select the most appropriate answer based on your clinical knowledge. You should directly answer the question by choosing the correct option and then provide rationale for your answer. Base SAQ prompt: The following are short answer questions (SAQs). You should directly answer the question by providing short answer and then provide rationale for your answer. Instruction-tuning SAQ prompt: You are skilled doctor with years of experience in the medical field in Africa, working in hospital setting. Your expertise covers wide range of conditions from common ailments to complex diseases. You are tasked with evaluating set of short answer questions (SAQs). You should provide concise, direct answer based on your clinical knowledge and experience. Following your answer, offer rationale that explains the reasoning behind your response, utilizing medical evidence and current practices to support your explanation. Base Consumer Queries prompt: The following are open-ended questions. You should directly answer the question freely. Instruction-tuning Consumer Queries prompt: You are skilled doctor with years of experience in the medical field in Africa, working in hospital setting. Your expertise spans wide range of conditions from common ailments to complex diseases. You are now addressing set of open-ended questions designed to explore your medical insights and experiences. You should answer each question freely, drawing upon your clinical knowledge to provide thorough, informed responses that reflect your understanding of the topics discussed. Figure 7: Base and Instruction-tuning prompts used for each question. Question Instruction from Figure 7 Here are some examples: Question samples and answers as in Figure 2 number of shots. ###Answer: Figure 8: Few-shot formatting for in-context learning Table 3: Zero-Shot Expert Evaluation of Large Language Models: Accuracy and BERT Scores on Multiple Choice Questions, Short Answer Questions, and Consumer Queries. Some model names are shortened for brevity. Model MCQ-Expert SAQ Bertscore CQ Bertscore Domain Access Size Training Gpt-4 Gpt-4o Gpt-3.5 Turbo Gpt-4o mini Claude-3.5 Sonnet Claude-3 Sonnet Claude-3 Opus Claude 3 Haiku Gemini Pro MedLM Gemini Ultra MedPalm2 llama3-405b-instruct-maas Llama3-OpenBioLLM-70B Meta Llama3 70B Phi3 Med. 128K Mixtral 8x7B Gemma2 27B Phi3 Mini 128k Phi3 Mini 4k Gemma2 9B Llama3 OpenBioLLM 8B Llama3 8B MetaLlama3.1 8B PMCLLAMA 7B JSL MedLlama 8B Meditron 7B BioMistral 7B Mistral 7B v02 Mistral 7B v03 Gemma2 2B 0.7568 0.7928 0.5629 0.7176 0.777 0.6504 0.7455 0.6639 0.6036 0.6036 0.739 - 0.7627 0.6661 0.7379 0.6708 0.6033 0.6448 0.5903 0.6036 0.6153 0.4499 0.4724 0.6189 0.4629 0.5726 0.5102 0.4402 0.4847 0.5084 0.1728 0.8727 0.8825 0.8813 0.8808 0.8574 0.8719 0.8696 0.8656 0.8601 0.8633 0.8716 0.8716 - 0.8292 0.7945 0.8661 0.8455 0.8617 0.8804 0.874 0.8435 0.8629 0.8804 0.8677 0.865 0.8901 0.8547 0.7938 0.8709 0.8744 0.8559 0.8385 0.8254 0.0963 - - 0.8141 0.8172 0.8163 0.8213 0.8303 0.8362 0.8379 0.8283 0.8372 0.8266 0.8259 0.7874 0.8266 0.8186 0.8158 0.8246 0.8344 - - 0.8303 - - 0.8259 - 0. General General General General General General General General General General General 1.8T Closed 12B Closed Closed 175B Closed 8B Closed Closed Closed Closed Closed Biomedical Closed Closed Biomedical Closed Open Biomedical Open Open Open Open Open Open Open Open Biomedical Open Open Open Biomedical Open Biomedical Open Biomedical Closed Biomedical Open Open Open Open Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Instruct Pretrained Instruct Instruct Instruct Instruct - 70B 2T 2T 540B - 1.56T 540B 405B 70B 70B 14B 46B 27B 3.8B Pretrained 3.8B Pretrained 9B 8B 8B 8B 7B 8B 7B 7B 7.2B 7.2B 2B Instruct Pretrained Pretrained Instruct Finetuned Pretrained Finetuned Instruct Instruct Instruct Instruct General General General General General General General General General General General General You should ###Instruction: The following are multiple choice questions (MCQs). directly answer the question by choosing the correct option and then provide rationale for your answer. {in-context examples from Figure 8 (if few-shots)} ###Question: Which of the following conditions, prevalent in Africa, is caused by infection with the protozoan parasite Trypanosoma brucei and is transmitted to humans through the bite of infected tsetse flies? ###Options: A. Malaria B. African trypanosomiasis (sleeping sickness) C. Chagas disease D. Leishmaniasis E. Onchocerciasis ###Answer: Figure 9: Sample of final Text formatting that is passed into the model 13 Table 4: Model accuracy on MedQA vs. Expert MCQ, showing performance difference between the two tasks. Model MedQA AfriMedQA-MCQ-Expert Acc. Difference Gpt-4o Gpt-4 Gpt-3.5 Turbo Gpt-4o mini Gemini Ultra Gemini Pro Claude-3.5 Sonnet Claude-3 Opus Claude-3 Sonnet Claude-3 Haiku Llama3 405B Instruct Llama3-OpenBioLLM 70B MetaLlama3 70B Instruct Phi3 Med. 128k Phi3 Mini 128k Phi3 Mini 4k Llama3 8B MetaLlama3.1 8B Instruct Llama3 OpenBioLLM 8B JSL MedLlama 8B PMC LLAMA 7B Meditron 7B Mixtral 8x7B Mistral 7B v02 Mistral 7B v03 BioMistral 7B Gemma2 2B Gemma2 9B Gemma2 27B 0.8814 0.7989 0.575 0.74 0.7879 0.5962 0.8327 0.78 0.6489 0.6709 0.8068 0.5862 0.7808 0.6842 0.575 0.5766 0.4973 0.6269 0.4674 0.6072 0.509 0.5334 0.6002 0.5003 0.513 0.4564 0.3283 0.6135 0.6209 0.7928 0.7568 0.5629 0.7176 0.739 0.6036 0.777 0.7455 0.6504 0.6639 0.7627 0.6661 0.7379 0.6708 0.5903 0.6036 0.4724 0.6189 0.4499 0.5726 0.4629 0.5102 0.6033 0.4847 0.5084 0.4402 0.1728 0.6153 0.6448 -8.86 -4.21 -1.21 -2.24 -4.89 0.74 -5.57 -3.45 0.15 -0.7 -4.41 7.99 -4.29 -1.34 1.53 2.7 -2.49 -0.8 -1.75 -3.46 -4.61 -2.32 0.31 -1.56 -0.46 -1.62 -15.55 0.18 2.39 Table 5: Impact of Explanations: Model accuracy when predicting with explanations vs. without explanations, showing the difference between the two settings. Model Explanations No Explanations Acc. Difference Gpt 4 Gpt 4o Gpt 3.5 Turbo Gpt 4o mini Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Opus Claude 3 Haiku Gemini Pro MedLM Gemini Ultra MedPalm 2 MetaLlama3.1 405B OpenBioLLM 70B MetaLlama3 70B Phi3 Mini 128k Phi3 Med. 128k Phi3 Mini 4k OpenBioLLM-8B MetaLlama3 8B MetaLlama3.1 8B PMCLlama 7B JSLMedLlama3 8B v2.0 Meditron 7B BioMistral 7B Mixtral 8x7B v0 Mistral 7B v0.2 0.8247 0.8500 0.6890 0.788 0.8423 0.733 0.8110 0.7433 - - - - 0.8210 0.6863 0.8043 0.6813 0.7520 0.6803 0.5327 0.6003 0.6933 0.5433 0.6723 0.5807 0.5353 0.7023 0.5837 0.8253 0.8276 0.6830 - - 0.6893 0.7907 0.7120 0.6310 0.7043 0.8003 0.7456 - 0.7280 0.8036 0.6676 - 0.6606 0.5193 0.6350 - 0.5197 0.6606 0.5653 - 0.7203 0.5510 -0.06 2.24 0.6 78.8 - 4.37 2.03 3.13 - - - - - -4.17 0.07 1.37 75.2 1.97 1.34 -3.47 - 2.36 1.17 1.54 - -1.8 3.27 Table 6: Expert MCQ accuracy by country Model Kenya Malawi Ghana South Africa Nigeria Gpt-4o Claude-3.5 Sonnet MetaLlama3 70B Gemini Ultra Llama3 405B Instruct Phi3 Med. 128K MetaLlama3.1 8B Mixtral-8x7B v0.1 OpenBioLLM 70B JSL MedLlama3 8B v2.0 Meditron 7B BioMistral 7B Avg. Accuracy 0.87 0.84 0.79 0.80 0.81 0.73 0.69 0.66 0.62 0.63 0.54 0.50 0.71 0.85 0.86 0.80 0.78 0.85 0.70 0.63 0.67 0.64 0.57 0.59 0.40 0.70 0.82 0.81 0.78 0.77 0.79 0.71 0.66 0.62 0.63 0.59 0.53 0. 0.68 0.70 0.72 0.63 0.63 0.7 0.59 0.61 0.50 0.50 0.50 0.39 0.39 0.57 0.73 0.71 0.67 0.67 0.26 0.6 0.55 0.55 0.52 0.54 0.47 0.41 0.48 country num specialties specialties GH 24 KE MW 27 NG 23 ZA Other (5), Otolaryngology (29), Cardiology (82), Internal Medicine (27), Neurology (116), Infectious Disease (171), Oncology (2), Obstetrics and Gynecology (147), Hematology (15), Plastic Surgery (1), Psychiatry (150), Anesthesiology (1), General Surgery (242), Pulmonary Medicine (62), Pathology (48), Pediatrics (149), Rheumatology (18), Neurosurgery (4), Family Medicine(1), Dermatology (1), Gastroenterology (63), Ophthalmology (35), Nephrology (60), Endocrinology (66) Other (2), Cardiology (28), Internal Medicine (2), Neurology (28), Physical Medicine and Rehabilitation (1), Infectious Disease (12), Oncology (6), Obstetrics and Gynecology (200), Hematology (6), Psychiatry (23), General Surgery (109), Pulmonary Medicine (37), Pediatrics (22), Urology (4), Rheumatology (5), Neurosurgery (1), Family Medicine (1), Geriatrics (1), Dermatology (4), Gastroenterology (15), Orthopedic Surgery (26), Nephrology (18), Endocrinology (10), Emergency Medicine (1), Pathology (0) Other (5), Otolaryngology (11), Cardiology (9), Internal Medicine (41), Neurology (11), Infectious Disease (1), Oncology (28), Radiology (1), Obstetrics and Gynecology (1), Hematology (4), Plastic Surgery (4), Anesthesiology (1), General Surgery (38), Pulmonary Medicine (4), Pathology (2), Pediatrics (74), Urology (4), Rheumatology (2), Neurosurgery (9), Family Medicine (2), Geriatrics (1), Dermatology (3), Gastroenterology (3), Ophthalmology (59), Orthopedic Surgery (6), Nephrology (1), Emergency Medicine (22) Other (20), Otolaryngology (18), Cardiology (53), Internal Medicine (1), Neurology (58), Oncology (2), Radiology (11), Obstetrics and Gynecology (312), Hematology (75), Plastic Surgery (10), Anesthesiology (10), General Surgery (99), Pathology (243), Pediatrics (286), Urology (16), Rheumatology (60), Neurosurgery (17), Family Medicine (1), Dermatology (21), Gastroenterology (49), Ophthalmology (10), Orthopedic Surgery (19), Endocrinology (61) Pediatrics (54) Table 7: Counts of Expert MCQ specialties by country 15 (a) Question Distribution by Country (b) Distribution by Gender (c) Distribution by Question Type Figure 10: Dataset Distributions 16 Figure 11: MCQ User Interface Figure 12: MCQ Review Interface with contributor and reviewer name redacted Figure 13: Dataset Annotation Tool Table 8: Distribution of Number of MCQ Answer Options Figure 14: MCQ/SAQ instructions num_options counts 5 4 2 3 2718 196 85 Country Count Country Count Nigeria Kenya South Africa Ghana Malawi Philippines Uganda Mozambique Tanzania Lesotho 7577 2476 2444 United States 1572 465 Australia Botswana 320 Eswatini 175 Zambia 77 Zimbabwe 36 33 30 28 19 17 4 2 Table 9: Counts of Questions Contributed by Country Table 10: Country Count for Expert Questions"
        },
        {
            "title": "Country",
            "content": "Ghana (GH) Nigeria (NG) Kenya (KE) Malawi (MW) South Africa (ZA)"
        },
        {
            "title": "Count",
            "content": "1495 1452 562 347 54 18 Table 12: MCQs Post-Processing Errors Count for base prompts Model Errors Count Llama3-OpenBioLLM-8B Claude-3-sonnet-20240229 Claude-3-haiku-20240307 Llama3-OpenBioLLM-70B Claude-3-opus-20240229 GPT-3.5-turbo Phi3-mini-128 GPT-4o MedPALM2 LLama-3-8b GPT-4-turbo GPT-4 Meta-Llama-3-70B-Instruct Mixtral-8x7B-Instruct-v0.1 Gemini ultra Meta-Llama-3-8B-Instruct Gemini Pro MedLM 435 273 164 163 162 120 91 80 40 26 17 10 6 5 4 2 1 1 Table 11: Specialty Count for All and Expert Questions Specialty All Expert Obs. and Gynecology Pediatrics General Surgery Pathology Neurology Infectious Disease Psychiatry Cardiology Endocrinology Gastroenterology Allergy and Immunology Ophthalmology Pulmonary Medicine Hematology Rheumatology Nephrology Internal Medicine Otolaryngology Orthopedic Surgery Oncology Other Family Medicine Neurosurgery Radiology Phy. Med. and Rehab. Dermatology Urology Emergency Medicine Plastic Surgery Anesthesiology Geriatrics Medical Genetics 824 747 757 381 310 539 299 258 236 225 217 202 231 211 171 163 238 158 161 135 125 289 107 107 101 126 134 123 132 115 91 660 585 488 293 213 184 173 172 137 130 - 104 103 100 85 79 71 58 51 38 32 - 31 - - 29 24 23 15 - - - 19 Table 13: MCQs Post-Processing Errors Count for Instruct Prompts Model Name Post-Processing Errors Count Llama3-OpenBioLLM-8B Meta-Llama-3-8B-Instruct Mistral-7b Claude-3-opus-20240229 Llama3-OpenBioLLM-70B MedPALM2 Phi3-mini-4 Mixtral-8x7B-Instruct-v0.1 GPT-4o Meta-Llama-3-70B-Instruct MedLM Gemini ultra 456 191 167 162 153 60 43 18 11 8 1 1 Table 14: HyperParameters Used for all models Temperature Batch Size Max (New) Tokens Model Names Claude 3 Haiku Claude 3 Opus Claude 3 sonnet Gemini Pro Gemini Ultra GPT 3.5 turbo GPT-4 GPT-4 turbo GPT-40 JSL-MedLlama-3-8B-v2.0 Llama-3-70B-Instruct Llama3 8B Llama-3-8B-Instruct MedLM MedPalm 2 Mistral-7B-Instruct-v0.2 Mixtral-8x7B-Instruct-v0.1 OpenBioLLM-70B-Instruct OpenBioLLM-8B-Instruct Phi-3-mini-128k-instruct-3.8B Phi-3-mini-4k-instruct-3.8B \"-\" indicates default hyperparameters. 1 1 1 32 16 1 1 1 1 - - 1 - 16 - 1 1 - - 1 1000 1000 1000 - - - - - - 256 - 300 - - - - 500 - - 500 500 0.2 0.2 0.2 0.9 0.7 - - - - 0.7 - - - 0.2 - - - - - 0.0 0.0 20 Table 15: SAQ Human Eval: Clinician blind evaluations of human and LLM answers showing counts and mean ratings. model count Correct Harm Hallucinations Omission claude-3-sonnet gpt-3.5-turbo gpt-4 gpt-4-turbo gpt-4o human jsl-med-llama-8b llama-3-8b mistral-7b phi3-mini-128 phi3-mini-4 176 182 195 187 178 197 182 176 193 158 175 4.676 4.692 4.605 4.668 4.635 4.442 4.588 4.273 4.751 4.608 4.714 1.023 1.005 1.026 1.064 1.034 1.051 1.038 1.080 1.016 1.000 1.023 1.023 1.044 1.082 1.102 1.045 1.061 1.055 1.233 1.104 1.032 1.017 1.142 1.192 1.169 1.166 1.185 1.533 1.308 1.557 1.052 1.177 1. Table 16: Clinician blind evaluations of human and LLM answers showing counts and mean ratings for the Correct and consistent with scientific consensus axis for all rated MCQs and the percentage change on questions human evaluators rated greater than 1 for Requires African local expertise. model all cnt Afr cnt all mean Afr mean % Llama3-OpenBioLLM-8B 133 Llama3-OpenBioLLM-70B 169 56 medpalm2 36 gemini-ultra 83 claude-3-haiku-20240307 235 Mixtral-8x7B-Instruct-v0.1 166 Meta-Llama-3-8B-Instruct 79 claude-3-opus-20240229 857 claude-3-sonnet-20240229 1430 human 1232 gpt-4-turbo 1127 phi3-mini-128 1248 gpt-4 773 mistral-7b 1884 gpt-4o 1202 gpt-3.5-turbo 1159 phi3-mini-4 150 Meta-Llama-3-70B-Instruct 1220 llama-3-8b 1197 jsl-med-llama-8b 29 medlm 5 7 3 5 3 9 11 4 158 314 189 171 211 156 370 166 177 8 174 183 2 21 4.211 4.444 4.607 4.583 4.639 4.434 4.452 4.658 4.693 4.631 4.695 4.650 4.599 4.604 4.720 4.656 4.651 4.520 4.284 4.420 4.414 3.600 3.857 4.000 4.200 4.333 4.222 4.273 4.500 4.595 4.586 4.693 4.649 4.602 4.641 4.762 4.699 4.723 4.625 4.402 4.568 5.000 -14.500 -13.201 -13.178 -8.364 -6.580 -4.777 -4.023 -3.397 -2.092 -0.967 -0.036 -0.008 0.073 0.801 0.899 0.928 1.561 2.323 2.751 3.350 13.281 Table 17: Consumer blind evaluations of LLM answers showing counts and mean ratings. model count Relevant Helpful+Informative Localized claude-3-haiku claude-3-opus claude-3-sonnet gemini-pro gemini-ultra gpt-3.5-turbo gpt-4 gpt-4-turbo gpt-4o jsl-med-llama-8b llama-3-8b medlm medpalm2 mistral-7b phi3-mini-128 phi3-mini-4 1430 1407 2755 1823 967 1429 1347 1658 1338 1337 1374 998 959 1343 1361 1343 4.448 4.465 4.531 4.440 4.074 4.319 4.324 4.504 4.340 4.196 4.097 4.200 3.938 4.331 4.212 4.416 4.349 4.387 4.446 4.332 3.815 3.946 4.143 4.257 4.243 4.016 3.832 4.060 3.614 4.207 4.139 4. 1.466 1.425 1.643 1.231 1.532 1.824 1.869 1.800 1.938 1.818 1.845 1.441 1.501 1.842 1.882 1.826 Table 18: Clinician blind evaluations of human and LLM answers showing number of times Possibility of harm was rated greater than 1 for all rated MCQs and the percentage change on MCQs human evaluators rated greater than 1 for Requires African local expertise. Due to the random order of evaluations and order of model result submissions, models received an unequal number of ratings at the project deadline. How to read this table: for example, of the 1,248 human ratings for gpt-4, 28 responses (2.2%) were rated as having the possibility of harm. Of the 1,248 ratings, 211 were also rated as requiring African local expertise. Of these 211, 7 MCQ answers (1.9%) had the possibility of harm, 15.5% improvement model cnt all all pct cnt Afr all Afr Afr pct % medpalm2 human llama-3-8b mistral-7b claude-3-sonnet gpt-4-turbo phi3-mini-4 jsl-med-llama-8b gpt-4 phi3-mini-128 gpt-4o gpt-3.5-turbo Meta-Llama-3-8B-Instruct Llama3-OpenBioLLM-8B Mixtral-8x7B-Instruct-v0.1 claude-3-haiku claude-3-opus gemini-ultra Llama3-OpenBioLLM-70B gemini-pro Meta-Llama-3-70B-Instruct medlm 1 39 52 13 16 18 19 35 28 18 23 16 1 2 1 0 0 0 3 0 0 1 56 1430 1220 773 857 1232 1159 1197 1248 1127 1884 1202 166 133 235 83 79 36 169 40 150 1.786 2.727 4.262 1.682 1.867 1.461 1.639 2.924 2.244 1.597 1.221 1.331 0.602 1.504 0.426 0.000 0.000 0.000 1.775 0.000 0.000 3.448 22 1 17 7 5 5 5 4 4 4 2 3 1 0 0 0 0 0 0 0 0 0 0 3 314 174 156 158 189 177 183 211 171 370 166 11 5 9 3 4 5 7 0 8 2 33.333 5.414 4.023 3.205 3.165 2.646 2.260 2.186 1.896 1.170 0.811 0.602 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1766.349 98.533 -5.608 90.547 69.523 81.109 37.889 -25.239 -15.508 -26.738 -33.579 -54.771 - - - - - - - - - - Table 19: Clinician blind evaluations of human and LLM answers showing hallucinations, i.e. number of times Includes irrelevant, wrong, or extraneous information was rated greater than 1 for all rated MCQs and the percentage change on MCQs human evaluators rated greater than 1 for Requires African local expertise. model llama-3-8b jsl-med-llama-8b mistral-7b Meta-Llama-3-8B-Instruct Llama3-OpenBioLLM-8B human gpt-4o Meta-Llama-3-70B-Instruct claude-3-sonnet phi3-mini-4 gpt-4 gpt-4-turbo Llama3-OpenBioLLM-70B gpt-3.5-turbo gemini-pro phi3-mini-128 claude-3-opus Mixtral-8x7B-Instruct-v0.1 claude-3-haiku gemini-ultra medpalm2 medlm cnt 117 97 47 10 7 72 89 7 36 43 45 44 6 36 1 26 1 2 0 0 0 0 all all % cnt Afr all Afr Afr % % 1220 1197 773 166 133 1430 1884 150 857 1159 1248 1232 169 1202 40 1127 79 235 83 36 56 29 9.590 8.104 6.080 6.024 5.263 5.035 4.724 4.667 4.201 3.710 3.606 3.571 3.550 2.995 2.500 2.307 1.266 0.851 0.000 0.000 0.000 0.000 11 9 9 1 1 16 18 1 9 5 7 5 0 5 0 5 0 0 0 0 0 174 183 156 11 5 314 370 8 158 177 211 189 7 166 0 171 4 9 3 5 3 2 6.322 4.918 5.769 9.091 20.000 5.096 4.865 12.500 5.696 2.825 3.318 2.646 0.000 3.012 0.000 2.924 0.000 0.000 0.000 0.000 0.000 0.000 -34.077 -39.314 -5.115 50.913 280.011 1.212 2.985 167.838 35.587 -23.854 -7.987 -25.903 - 0.568 - 26.745 - - - - - - 23 Table 20: Clinician blind evaluations of human and LLM answers showing ommissions, i.e. number of times Omission of relevant info was rated greater than 1 for all rated MCQs and the percentage change on MCQs human evaluators rated greater than 1 for Requires African local expertise. model Llama3-OpenBioLLM-8B llama-3-8b jsl-med-llama-8b Llama3-OpenBioLLM-70B human gpt-4 gpt-4-turbo phi3-mini-128 mistral-7b gpt-3.5-turbo phi3-mini-4 claude-3-sonnet gpt-4o medpalm2 gemini-pro Meta-Llama-3-70B-Instruct Meta-Llama-3-8B-Instruct claude-3-opus Mixtral-8x7B-Instruct-v0.1 claude-3-haiku gemini-ultra medlm cnt 36 264 201 24 201 146 120 107 73 113 108 72 139 4 2 4 3 1 2 0 0 0 all all % cnt Afr all Afr Afr % % 133 1220 1197 169 1430 1248 1232 1127 773 1202 1159 857 1884 56 40 150 166 79 235 83 36 29 27.068 21.639 16.792 14.201 14.056 11.699 9.740 9.494 9.444 9.401 9.318 8.401 7.378 7.143 5.000 2.667 1.807 1.266 0.851 0.000 0.000 0.000 1 39 29 2 56 18 19 16 15 17 20 21 25 1 0 0 0 0 1 0 0 0 5 174 183 7 314 211 189 171 156 166 177 158 370 3 0 8 11 4 9 3 5 2 20.000 22.414 15.847 28.571 17.834 8.531 10.053 9.357 9.615 10.241 11.299 13.291 6.757 33.333 0.000 0.000 0.000 0.000 11.111 0.000 0.000 0. -26.112 3.581 -5.628 101.190 26.878 -27.079 3.214 -1.443 1.811 8.935 21.260 58.207 -8.417 366.653 - - - - 1205.640 - - - 24 Table 21: Specialty Human Eval: Clinician ratings for all model responses across specialties showing counts of each specialty, mean of correctness rating, and MCQ counts with percentages where model responses demonstrated the possibility of harm, hallucinations, and omission of relevant information, i.e. ratings greater than 1 for relevant criteria. Bold represents bottom 2 specialties with worst performance across all models for each evaluation axis specialty count correct # harm (%) # hallucination (%) # omission (%) Obstetrics and Gynecology Internal Medicine Infectious Disease Cardiology Pediatrics Endocrinology Neurology Other Gastroenterology Hematology General Surgery Pathology Family Medicine Orthopedic Surgery Pulmonary Medicine Ophthalmology Oncology Nephrology Emergency Medicine Anesthesiology Urology Psychiatry Dermatology Neurosurgery Allergy and Immunology Otolaryngology Radiology Medical Genetics Plastic Surgery Rheumatology Geriatrics 482 323 319 280 274 254 217 215 177 154 144 108 107 97 94 88 87 79 71 69 56 54 50 48 43 35 23 18 17 16 11 4.620 4.635 4.680 4.736 4.661 4.701 4.765 4.521 4.616 4.766 4.597 4.731 4.748 4.680 4.755 4.602 4.678 4.570 4.662 4.725 4.714 4.667 4.700 4.438 4.605 4.743 4.478 5.000 4.706 4.625 5. 10 (2.07) 9 (2.79) 3 (0.94) 4 (1.43) 2 (0.73) 3 (1.18) 3 (1.38) 3 (1.40) 6 (3.39) 2 (1.30) 8 (5.56) 2 (1.85) 0 (0.00) 0 (0.00) 3 (3.19) 2 (2.27) 2 (2.30) 1 (1.27) 0 (0.00) 3 (4.35) 0 (0.00) 1 (1.85) 1 (2.00) 1 (2.08) 1 (2.33) 1 (2.86) 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 29 (6.02) 19 (5.88) 5 (1.57) 11 (3.93) 10 (3.65) 13 (5.12) 4 (1.84) 6 (2.79) 13 (7.34) 1 (0.65) 12 (8.33) 1 (0.93) 5 (4.67) 7 (7.22) 4 (4.26) 3 (3.41) 4 (4.60) 6 (7.59) 4 (5.63) 2 (2.90) 2 (3.57) 3 (5.56) 1 (2.00) 2 (4.17) 2 (4.65) 0 (0.00) 2 (8.70) 0 (0.00) 1 (5.88) 1 (6.25) 0 (0.00) 43 (8.92) 24 (7.43) 9 (2.82) 8 (2.86) 18 (6.57) 8 (3.15) 8 (3.69) 11 (5.12) 17 (9.60) 7 (4.55) 13 (9.03) 9 (8.33) 6 (5.61) 6 (6.19) 10 (10.64) 10 (11.36) 4 (4.60) 8 (10.13) 4 (5.63) 5 (7.25) 4 (7.14) 3 (5.56) 2 (4.00) 5 (10.42) 3 (6.98) 2 (5.71) 1 (4.35) 0 (0.00) 1 (5.88) 1 (6.25) 0 (0.00)"
        }
    ],
    "affiliations": [
        "BioRAMP",
        "Brown University",
        "Georgia Institute of Technology",
        "Google Research",
        "Intron",
        "Kenyatta University",
        "MILA Quebec",
        "Masakhane",
        "SisonkeBiotik",
        "The Ohio State University",
        "University of Cape Coast",
        "University of Minnesota"
    ]
}