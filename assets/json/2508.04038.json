{
    "paper_title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents",
    "authors": [
        "Zechen Li",
        "Baiyu Chen",
        "Hao Xue",
        "Flora D. Salim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA."
        },
        {
            "title": "Start",
            "content": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents Zechen Li, Baiyu Chen, Hao Xue, Flora D. Salim University of New South Wales, Sydney {zechen.li, hao.xue1, flora.salim}@unsw.edu.au, breeze.chen@student.unsw.edu.au 5 2 0 2 6 ] . [ 1 8 3 0 4 0 . 8 0 5 2 : r Abstract Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, multisensor retrieval module that surfaces relevant evidence, and hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53 in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as promising step toward trustworthy, plugand-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA. Introduction Human activity recognition (HAR) from on-body motion sensors underpins wide range of ubiquitous computing applications, from digital health and sports analytics to adaptive user interfaces. However, most existing HAR systems remain heavily reliant on task-specific deep neural networks, such as DeepConvLSTM (Ordonez and Roggen 2016), Attend (Abedin et al. 2021), and Transformer (Vaswani et al. 2017) variants, carefully trained for fixed set of sensors and and predefined activity classes. Although these models achieve strong in-distribution accuracy, their practical deployment remains limited. In particular, existing HAR methods (see Figure 1) face three critical limitations. Poor Generalisation. Introducing new wearable devices or sensor setups usually requires costly retraining or fine-tuning, indicating limited generalizability across hardware and environmental variations. Limited Zero-Shot Capability. Foundation encoders such as Figure 1: Method Families for Zero-Shot Human Activity Recognition. Moment (Goswami et al. 2024), Mantis (Feofanov et al. 2025) provide robust transferable representations yet still require task-specific classifiers, while contrastive-based pretrained models like UniMTS (Zhang et al. 2024) remove the classifier step but perform poorly in zero-shot recognition. Lack of Interpretability. Current HAR approaches yield only categorical predictions without transparent, interpretable reasoning, severely limiting trust and applicability, particularly in safety-critical scenarios. Meanwhile, large language models (LLMs) enhanced with retrieval-augmented generation (RAG) have achieved groundbreaking zero-shot reasoning capabilities in vision and NLP tasks. However, sensor-based HAR has yet to benefit from these advances. Early attempts to apply LLMs to HAR, typically by converting multichannel sensor signals into images or lengthy token sequences, have resulted in excessive token usage, significant information loss, and mediocre accuracy despite high computational costs. We argue that LLMs fall short because they lack structured, sensor-specific knowledge. When equipped with (i) discriminative motion features and (ii) retrieval mechanism for relevant evidence, an LLM can reason effectively about unseen activities. Providing this domain knowledge and retrieval capability is therefore critical to plug-and-play, interpretable, zero-shot HAR. 1 Motivated by this insight, we introduce ZARA, novel framework for zero-shot motion time-series analysis. ZARA unifies three components. Domain-Knowledge Injection: we automatically build general-purpose knowledge base that stores discriminative feature profiles for every activity pair, spanning time-domain, frequency-domain, and crosschannel statistics, so it remains valid no matter which activities appear at inference time. Class-Wise Multi-Sensor Retrieval: pre-trained encoder produces embeddings for each sensor channel and, within every candidate class, retrieves the top-k nearest evidence, then integrate their retrieval results via Reciprocal Rank Fusion (Cormack, Clarke, and Buettcher 2009). Retrieval is performed independently within each candidate activity class to give balanced recall even for long-tail activities. Hierarchical Multi-Agent Reasoning: four LLM agents work in sequence, initial feature selection, evidence retrieval and candidate pruning, refined feature selection, and final retrieval plus classification, gradually narrowing the label set while generating human-readable explanations that cite the chosen features and retrieved evidence. Notably, ZARA operates entirely through prompting an off-the-shelf LLM, without fine-tuning or external classifiers, thereby enabling flexible, interpretable, and highly effective zero-shot HAR. We benchmark ZARA against 10 widely used baselines across 8 public HAR datasets covering diverse sensor configurations and activity categories. By fusing structured sensor knowledge with LLM-based reasoning, ZARA delivers plug-and-play alternative to todays retraining-heavy pipelines and produces interpretable predictions that are ready for real-world use. Concretely, this work contributes: Pair-Wise Knowledge Base. Built automatically from labeled datasets, the knowledge base eliminates manual effort. Its pairwise structure decouples domain knowledge from any fixed label set, enabling plug-and-play generalization to new activity combinations. RAG-driven, Agent-based HAR. ZARA is the first system that classifies multi-sensor motion time-series while simultaneously generating concise, verifiable rationales. Classifier-Free Generalization with SOTA Performance. ZARA achieves 2.53 average improvement in zero-shot macro F1 over the strongest baseline across 8 benchmarks, demonstrating its robust performance under the Classifier-Free Generalization (CFG) setting. Related Work Conventional HAR Methods. Classical machine learning approaches on HAR utilized handcrafted features, with Kwapisz, Weiss, and Moore 2011 applying decision trees and MLPs, and Haresamudram, Anderson, and Plotz 2019 demonstrating that optimized feature extraction within the Activity Recognition Chain can rival end-to-end deep learning. With the rise of deep learning, convolutional and recurrent architectures such as DeepConvLSTM (Ordonez and Roggen 2016), DeepConvLSTMAttn (Murahari and Plotz 2018) and Attend (Abedin et al. 2021) have shown strong performance under in-distribution conditions. However, these models require extensive retraining to adapt to new activities, sensor configurations, or user populations, severely limiting their scalability and generalisation. Foundation Models. Recent advances in foundation models for time series have aimed to improve generalization across domains and tasks. Chronos (Ansari et al. 2024) tokenizes time-series (TS) through scaling and quantization into fixed vocabulary, enabling the use of text-style encoderdecoder architectures for training. Moment (Goswami et al. 2024) adopts masked TS modeling approach, pre-training transformer to predict missing values. Mantis (Feofanov et al. 2025) proposes contrastively pre-trained Vision Transformer (Dosovitskiy et al. 2020) architecture specifically tailored for TS classification. While these models learn general-purpose representations, they still rely on fine-tuning or training task-specific classifiers for zero-shot TS classification task. UniMTS (Zhang et al. 2024), in contrast, eliminates the need for downstream classifiers by aligning synthetic skeleton-based motion time series with text embeddings generated by LLMs, using spatiotemporal graph networks to enable cross-location generalization. However, it exhibits poor zero-shot performance when confronted with entirely unseen activity classes. Cross-modal and LLM-based HAR. Motivated by the success of LLMs in vision and language domains, recent studies have begun exploring their potential for HAR from motion TS. One line of research leverages large vision-language models (VLMs) to construct joint embedding spaces across modalities. ImageBind (Girdhar et al. 2023) and IMU2CLIP (Moon et al. 2023) employ pretrained VLMs (Radford et al. 2021) to align motion signals with text. However, both methods are trained on head-mounted sensor data, raising concerns about their generalisation to other sensor placements. SensorLLM (Li et al. 2025) combines pretrained TS encoder with LLM to align sensor data with natural language, enabling cross-sensor generalisation and generating human-readable outputs, but still requires training task-specific classifier for recognition. COMODO (Chen et al. 2025) leverages cross-modal selfsupervision to distill semantic knowledge from videos into IMU signals, but depends on paired multi-modal data and cannot perform true zero-shot recognition. Another line of work applies LLMs or cross-modal models directly to raw motion time series. HARGPT (Ji, Zheng, and Wu 2024) uses chain-of-thought prompting to process sensor signals with LLMs. Yoon et al. 2024 convert sensor data into images to enable visual prompting with MLLMs and reduce token cost. However, these methods often depend on prompt engineering, external context, or paired modalities that may be unavailable in practice. Moreover, LLMs and crossmodal models are not inherently suited for raw time series, limiting their utility in complex or zero-shot HAR. ZeroHAR (Chowdhury et al. 2025) improves zero-shot activity recognition using spatial and biomechanical metadata but lacks interpretability for real-world use. SensorLM (Zhang et al. 2025) uses hierarchical captioning to aid LLM interaction with sensor data but is limited by its fixed 26dimensional features, hindering generalization. 2 Figure 2: Overall architecture of ZARA, motion TS analysis agent augmented with prior knowledge and retrieval. Methodology Figure 2 sketches the ZARA pipeline. We first detail the construction of domain knowledge, then the multi-sensor retrieval and reranking backbone, and finally the agent workflow that delivers zero-shot HAR. Domain-Knowledge Generation. To equip the LLM with structured, sensor-specific priors, we automatically construct Activity-Pair Feature Importance Knowledge Base offline. Each wearable unit streams six raw channels, threeaxis accelerometer (ax, ay, az) and three-axis gyroscope (gx, gy, gz). For every labelled window xa RT of activity (T time steps, channels) we derive feature pool comprising low-cost, human-interpretable statistics: timedomain measures (mean, variance, RMS, signal-magnitude frequency-domain descriptors (dominant frearea, etc.), quency, spectral entropy, band energy, etc.), and crosschannel indicators (channel correlations, tilt angle, etc.). For each ordered activity pair (ai, aj) we estimate an importance score s[f, (ai, aj)] for every using AutoGluons (Erickson et al. 2020) permutation-based feature ranking with cross-validation; fold-weighted averaging yields robust, dataset-agnostic estimates. All featurescore tuples are stored as K[(ai, aj)] = [(f1, s1), (f2, s2), . . . , (fP , sP )]. Because is organized pair-wise, it is label-inventory agnostic: adding new activity requires only O(F) statistic updates against existing classes, with no retraining of inference models or manual curation. This fully automated knowledge base provides plug-and-play priors that guide downstream reasoning at inference. Placement-specific Vector Databases. To ensure the retrieved evidence matches querys wearing position, we maintain set of placement-specific vector stores {Dloc}, where loc denotes the sensor placement (e.g., wrist, thigh, ankle). Each database indexes historical motion windows, six raw channels from three-axis accelerometer and three-axis gyroscope, together with activity labels and sensor metadata. Every window is first embedded by frozen time-series foundation encoder g() Mantis (Feofanov et al. 2025) by default. The resulting vectors are L2-normalized, making inner-product search equivalent to cosine similarity, and are stored in FAISS IndexFlatIP structure (Douze et al. 2025) within the corresponding placement shard. This configuration enables exact, brute-force nearest-neighbour retrieval inside each body-location database. For query embedding = g(x) and stored vector v, similarity is simply cos(u, v) = uv, with u2 = v2 = 1, enabling precise cosine retrieval with negligible indexing overhead. Class-Wise Multi-Sensor Retrieval. Given query window with sensor placement tag loc and candidate activities = {a1, . . . , aM }, we first obtain its normalized embedding and score it against all vectors vd Dloc . For every activity am, this produces similarity-sorted list Lloc ). If the query contains additional placements, the same scoring is performed on each extra database. We then fuse the lists with reciprocal-rank fusion (RRF) (Cormack, Clarke, and Buettcher 2009): (cid:88) m, . . . , dDloc m = (d1 RRF(d) = krrf = 60 1 krrf + rloc(d) , loc where rloc(d) {0, 1, . . . , K} is the 0-based rank of document in Lloc . Because identical indices correspond to the same time window across sensors, RRF aligns and jointly reranks them. After fusion, we retain the top-k windows (k=100 by default) for each activity, yielding balanced evidence set even when the underlying class distribution is highly uneven. Hierarchical Multi-Agent Reasoning. ZARA employs three LLM agent types executed in four stages (Figure 3). First, Feature Selector agent consults the pair-wise knowledge base together with the activity candidate set 3 Figure 3: ZARAs hierarchical multi-agent workflow with placement-specific, class-wise evidence retrieval and rank fusion. and returns highly discriminative features. Next, an Evidence Pruning agent aggregates the class-wise evidence lists {Na(x)}, builds structured feature-statistics markdown table (query value, class means, class standard deviations) of those features, and removes activities whose distributions differ markedly from the query, yielding narrowed set A. The Feature Selector is then reused on to select finergrained features, giving the LLM greater granularity to distinguish closely related activities. Finally, Decision Insight agent receives an updated statistics table and outputs both the final label and an explicit natural-language rationale grounded in the selected features and the retrieved evidence. All agents share the same frozen LLM Gemini2.0-flash (DeepMind 2025). Experiments Datasets. We benchmark ZARA on 8 publicly available motion time-series (TS) datasets that collectively span wide variety of activities and placements. We partition them into three difficulty levels: (i) Easy: Opportunity (Roggen et al. 2010), UCI-HAR (Anguita et al. 2013), and Shoaib (Shoaib et al. 2014); (ii) Medium: PAMAP2 (Reiss and Stricker 2012), USC-HAD (Zhang and Sawchuk 2012), and MHealth (Banos et al. 2014); and (iii) Hard: WISDM (Weiss 2019) and DSADS (Altun, Barshan, and Tuncel 2010). Table 1 lists the number of activity classes and sensor channels for each dataset. Baselines. We benchmark ZARA against 10 representative zero-shot activity recognition baselines grouped into 3 families: (i) Text-based LLMs: HARGPT Text (Ji, Zheng, and Wu 2024), Gemini Text and Gemini Table, which are directly prompted with numerical sensor data in structured text form; (ii) Multimodal LLMs: HARGPT Plot and Gemini Plot, which use plotted sensor signals as visual prompts for activity prediction; (iii) Pretrained HAR Models: ImageBind (Girdhar et al. 2023), IMU2CLIP (Moon et al. 2023), NormWear (Luo et al. 2024), and UniMTS (Zhang et al. 2024), which learn modality-aligned embeddings for zeroshot transfer. IMUGPT (Leng, Kwon, and Ploetz 2023) difFigure 4: Subject split and data flow. ZARA builds its vector database and domain knowledge from seen subjects and tests on held-out subjects. Baselines may load pretrained weights and, when needed, train classifier head on the database split before evaluating on the same held-out subjects. fers by pretraining on task-specific virtual motion data for downstream use. All Gemini-based models are evaluated using the Gemini2.0-Flash (DeepMind 2025). Both HARGPT and IMUGPT are run on GPT-4o-mini (OpenAI 2024). Gemini Table follows the structured input described in (Fang et al. 2024), where TS data is encoded as Markdown table. Experimental Settings. To rigorously evaluate ZARAs generalization ability, we use the subject-hold-out protocol illustrated in Figure 4. Every inference window is drawn from participants excluded from both the knowledge-base construction and the vector-database indexing stages. For each dataset, we sample class-balanced test split, guaranteeing that (i) every activity is equally represented and (ii) each held-out user contributes the same number of windows per class. And we report both macro F1 and accuracy as evaluation metrics. This setting simulates challenging user-generalization scenario and mirrors the dynamics of cross-dataset transfer, where the model must reason over sensor patterns from entirely unseen individuals, an essential property for real-world deployment. To further showcase ZARAs flexibility in open-set zero-shot conditions, on the larger WISDM and DSADS benchmarks we omit preDataset Metrics Number of Classes Number of Channels Level HARGPT Text Gemini Text Gemini Table HARGPT Plot Gemini Plot ImageBind IMU2CLIP NormWear IMUGPT UniMTS ZARA Acc F1 Acc F1 Acc Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc Acc F1 Acc F"
        },
        {
            "title": "Opportunity",
            "content": "UCI-HAR 4 30 21.0 19.2 26.5 19.8 29.0 22. 21.5 15.6 23.5 21.3 35.5 30.0 36.5 34.4 23.0 23.8 38.5 28. 33.5 24.8 92.5 92.5 6 6 Easy 29.6 17. 24.2 13.0 21.3 9.8 28.3 15.7 31.7 20.6 28.8 19.9 33.3 22. 17.9 11.4 32.5 21.6 37.1 23.9 90.0 90."
        },
        {
            "title": "Shoaib",
            "content": "7 30 27.1 19.2 27.1 17.6 27.6 18.7 24.3 14. 31.4 24.1 36.7 30.2 39.5 34.5 15.2 11.7 26.7 15.2 51.9 40. 97.1 97.1 PAMAP2 USC-HAD"
        },
        {
            "title": "Average",
            "content": "12 18 12.1 6.2 15.0 10.2 11.7 7.2 10.0 6. 10.4 6.9 18.8 10.2 15.8 11.6 9.2 2.7 12.9 3.8 32.9 29. 76.7 76.9 12 6 Medium 13.8 7.3 14.2 5. 17.1 9.7 14.6 8.7 10.8 5.3 7.9 1.8 16.3 10.5 10.0 5. 2.9 1.9 29.6 24.2 60.0 60.1 12 15 12.1 6. 25.4 20.8 22.9 18.3 15.0 11.0 19.2 17.4 17.9 11.1 16.3 14. 8.3 2.2 8.3 2.8 65.4 58.8 86.3 86.1 Hard 6 5.6 1.8 11.1 7.6 10.1 7.8 5.9 2.8 9.4 7. 8.0 4.7 10.1 5.9 4.2 1.4 5.9 2.1 30.2 28.5 65.6 64. 19 30 10.5 6.2 13.2 8.6 16.3 10.3 7.9 4. 10.0 4.8 10.5 5.7 13.7 9.2 3.7 2.2 7.4 3.6 34.7 27. 84.2 84.4 Avg 16.5 10.4 19.6 12.9 19.5 13.0 15.9 9. 18.3 13.5 20.5 14.2 22.7 17.9 11.4 7.7 16.9 10.0 39.4 32. 81.6 81.4 Table 1: Zero-shot performance: ZARA vs. 10 baselines from three method families. Best scores are shown in bold; second-best are underlined. defined candidate lists and instead apply RAG to select the top-10 most similar activity classes per query, significantly reducing token usage when the class set is large. All LLM queries are issued with temperature 0, yielding deterministic, fully reproducible outputs. Further details on datasets, preprocessing, baselines, and other experimental settings are provided in the Appendix. Results. Table 1 reports zero-shot performance across all 8 benchmarks. ZARA outperforms all 10 baselines, consistently exceeding the strongest baseline UniMTS, with an average 2.07 improvement in accuracy and 2.53 in F1 score. IMUGPT, though pre-trained on virtual motion data, performs poorly on real-world benchmarks and requires separate training for each downstream task. Contrastive approaches like ImageBind and IMU2CLIP, limited to single-placement sensors, underperform even when their best-performing placements are used. While UniMTS and NormWear accept multi-sensor input, their performance degrades sharply on unseen activities, revealing strong dependence on label exposure. Critically, none of these models provide verifiable reasoning. Prompting general-purpose LLMs (e.g., HARGPT and Gemini) with raw, structured, or visualized motion data yields poor zero-shot performance, underscoring their difficulty in distinguishing unseen activities without domain guidance. Notably, ZARA achieves F1 scores closely aligned with its accuracy, indicating balanced performance across all classes. In contrast, baseline models often show much lower F1 scores than accuracy, indicating bias toward familiar classes encountered during pretraining. Unlike these methods, our training-free ZARA integrates structured knowledge, class-aware retrieval, and agent-based reasoning to deliver accurate, interpretable predictions, enabling flexible and generalizable zero-shot HAR across diverse sensors and activity sets. Dataset Metrics Level"
        },
        {
            "title": "Opportunity",
            "content": "UCI-HAR Easy"
        },
        {
            "title": "Shoaib",
            "content": "PAMAP2 USC-HAD Medium"
        },
        {
            "title": "DSADS",
            "content": "Time(s) Hard Avg Pre-trained TS Embedder + Task-Specific Heads Moment-small Moment-large Mantis Acc F1 Acc F1 Acc F1 Classifier-Free Generalization ZARADTW ZARAMoment-S ZARAMoment-L ZARAMantis Acc F1 Acc F1 Acc Acc F1 66.0 64.8 63.5 62.7 90.0 89.9 90.5 90.5 88.5 88. 91.0 91.0 92.5 92.5 77.5 77.5 78.8 78.6 91.3 91.2 90.4 90. 87.9 87.7 87.9 87.8 90.0 90.0 86.2 85.9 91.0 90.8 93.3 92. 96.7 96.7 97.6 97.6 97.6 97.6 97.1 97.1 71.7 71.8 73.3 73. 84.6 85.1 71.7 71.6 73.3 73.4 75.8 76.1 76.7 76.9 54.2 52. 47.1 45.2 53.8 53.7 55.4 56.4 53.3 53.0 55.8 56.7 60.0 60. 67.5 66.8 72.1 72.2 86.7 86.0 86.3 86.1 86.3 86.2 88.3 88. 86.3 86.1 66.3 66.3 65.3 65.6 71.5 71.2 59.4 57.3 62.2 62. 65.3 64.2 65.6 64.1 72.6 72.3 74.2 73.7 90.5 90.2 82.6 82. 86.3 86.0 84.7 83.9 84.2 84.4 0.3826 0.0438 0.1003 0.1826 Table 2: ZARA with different retrieval embedder (true zero-shot) vs. corresponding foundation models that use frozen embedder plus an extra classifier trained on each database split. Note that Mantis is pre-trained on HAR datasets. Avg. retrieval time per query reported in the rightmost column. Best scores are shown in bold; second-best are underlined. Ablation Studies Retrieval Embedder Choice. As shown in Table 2, pretrained TS foundation models demonstrate strong HAR performance. However, they lack true zero-shot capability, requiring retraining of classifiers whenever the candidate set changes. In contrast, ZARA leverages their zero-shot feature extraction ability as retrieval embedders, enabling true Classifier-Free Generalization (CFG). We compare four retrieval strategies: Dynamic Time Warping (DTW) (Muller 2007), classical distance-based method that aligns TS sequences by minimizing temporal distortion, and 3 pretrained TS foundation models, Moment-small, Momentlarge (Goswami et al. 2024), and Mantis (Feofanov et al. 2025). Moment is pre-trained via masked TS prediction, while Mantis is pre-trained on classification tasks and has been pre-trained on human activity datasets such as UCIHAR. Despite architectural differences, ZARA maintains robust performance across all retrieval strategies, with average zero-shot accuracy of 79.1% (DTW), 79.4% (Momentsmall), 80.8% (Moment-large), and 81.6% (Mantis) across benchmarks, showing that our class-wise multi-sensor retrieval ensures consistent effectiveness regardless of the underlying retrieval embedder. To assess the retrieval models themselves, we follow their original protocols by training classifier on frozen embeddings using the database split. Notably, ZARA often outperforms these baselines despite using no classifier. ZARA with Moment-small exceeds its baseline on 6/8 datasets in accuFigure 5: Impact of Retrieval on Evidence Pruning and Decision and Insight Agents. The upper bound indicates the proportion of queries for which the pruned candidate set still contains the correct class under each setting. racy and 7/8 in F1; ZARA with Moment-large wins 7/8 in both metrics; and ZARA with Mantis wins 4/8 in F1. Overall, it ranks first on 4 datasets and second on 7. This highlights ZARAs strength in CFG, unlike Moment and Mantis, which require retraining for each label set. ZARAs agentdriven framework enables plug-and-play deployment across datasets, label spaces, and sensor configurations, while also supporting interpretable, reasoning-driven decision making. We also compare the retrieval latency of all four methods, measured as the average time (in seconds) to process single 6 Figure 6: Accuracy with and without the Evidence Pruning Agent, along with upper bounds for each setting. The dashed line indicates the average length of the pruned candidate set. query on an Apple M2 Max CPU with 64GB memory: DTW (0.3826), Moment-small (0.0438), Moment-large (0.1003), and Mantis (0.1826). Moment-small is the fastest, while DTW is significantly slower due to its pairwise sequence alignment. Mantis, though lightweight, is slower than Moment because it concatenates channel embeddings instead of averaging. Still, it retrieves more informative samples, offering speedquality trade-off. While absolute latency depends on query data, database and hardware, relative rankings reflect method-level efficiency. Removing Retrieval Reduces Performance. To assess the contribution of the Evidence Retrieval module, we ablate it by replacing top-k retrieval with global class-wise feature distributions computed over the entire database. These summaries are fed to the LLM without conditioning on queryspecific evidence. As shown in Figure 5, removing retrieval degrades ZARAs zero-shot reasoning across all benchmarks, with average accuracy falling from 81.6% to 71.8%, and the upper bound, the proportion of queries where the pruned set contains the ground-truth label, dropping from 91.4% to 86.7%. The magnitude of decline varies across datasets, likely reflecting differences between database-wide feature statistics and those of individual query instances. These results confirm that raw global summaries are insufficient for fine-grained inference: Retrieval surfaces more query-relevant evidence, enabling more informed pruning and prediction. Skipping Evidence Pruning Hurts. To quantify the impact of the Evidence Pruning Agent, we ablate it and reevaluate ZARA across all eight benchmarks. Without pruning, ZARAs average zero-shot accuracy drops from 81.6% to 68.2%. Figure 6 shows that our pruning agent typically narrows each query to 23 candidates per benchmark, with the easy level datasets yielding even smaller shortlists. Moreover, these pruned shortlists retain the correct class with an average upper-bound accuracy of 91.4%, confirming that pruning effectively preserves high-quality candidates. This compact candidate set allows the LLM to extract finer-grained feature-importance knowledge for deeper analysis. In contrast, omitting pruning forces the LLM to reason over much larger candidate pool, degrading focus and performance. However, even in this degraded setting, ZARAs 68.2% still outperforms every baseline. Figure 7: Impact of prior knowledge injection on initial and secondary Feature Selector agents. No Prior Knowledge Fails. To evaluate the contribution of prior knowledge, we disable the pair-wise featureimportance knowledge base, forcing the agent to rely solely on its intrinsic understanding of human activities and motion sensor data for feature selection. As shown in Figure 7, this results in substantial drop in ZARAs average zero-shot accuracy across all eight benchmarks, from 81.6% to 63.4%. The average upper-bound accuracy, defined as the fraction of instances where pruning retains the correct class, declines from 91.4% to 87.0%. This reduction clearly indicates that both the initial and secondary Feature Selector stages lose discriminative capability in the absence of domain-specific prior knowledge. Benchmarks with fewer classes (e.g., Opportunity, UCI-HAR, Shoaib) are less affected in the first narrowing stage, as it primarily removes clearly irrelevant classes. However, in the second stage, the Feature Selector tends to pick suboptimal features due to the absence of effective criteria for distinguishing between activities, resulting in overly coarse feature selection that cannot recover the lost accuracy. This gap highlights the importance of prior knowledge injection in guiding feature selection and enabling robust, effective zero-shot motion time-series analysis."
        },
        {
            "title": "Conclusions",
            "content": "We have presented ZARA, the first end-to-end framework that enables Classifier-Free Generalization for human activity recognition, achieving zero-shot classification from raw motion time-series without any finetuning or task-specific adaptation. ZARA integrates multisensor retrieval-augmented generation, automated pair-wise domain-knowledge injection, and hierarchical agent-based LLM reasoning to deliver flexible, interpretable predictions across diverse datasets and sensor configurations. Ablation studies highlight the critical roles of the Evidence Pruning agent, the prior knowledge base, and the retrieval module, while retrieval embedder comparisons reveal clear accuracylatency trade-offs for real-world deployment. Extensive evaluation on eight HAR benchmarks shows that ZARA transforms off-the-shelf LLMs from near-chance to stateof-the-art performance, outperforming all baselines and offering practical path to scalable, transparent, and adaptive HAR in the wild. 7 References Abedin, A.; Ehsanpour, M.; Shi, Q.; Rezatofighi, H.; and Ranasinghe, D. C. 2021. Attend and Discriminate: Beyond the State-of-the-Art for Human Activity Recognition Using Wearable Sensors. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 5(1). Altun, K.; Barshan, B.; and Tuncel, O. 2010. Comparative study on classifying human activities with miniature inertial and magnetic sensors. Pattern Recognition, 43(10): 36053620. Anguita, D.; Ghio, A.; Oneto, L.; Parra, X.; and Reyes-Ortiz, J. L. 2013. Public Domain Dataset for Human Activity Recognition using Smartphones. In The European Symposium on Artificial Neural Networks. Ansari, A. F.; Stella, L.; Turkmen, C.; Zhang, X.; Mercado, P.; Shen, H.; Shchur, O.; Rangapuram, S. S.; Arango, S. P.; Kapoor, S.; Zschiegner, J.; Maddix, D. C.; Wang, H.; Mahoney, M. W.; Torkkola, K.; Wilson, A. G.; BohlkeSchneider, M.; and Wang, Y. 2024. Chronos: Learning the Language of Time Series. Transactions on Machine Learning Research. Banos, O.; Garcıa, R.; Terriza, J. A. H.; Damas, M.; Pomares, H.; Rojas, I.; Saez, A.; and Villalonga, C. 2014. mHealthDroid: Novel Framework for Agile DevelopIn International ment of Mobile Health Applications. Workshop on Ambient Assisted Living and Home Care. Chen, B.; Wongso, W.; Li, Z.; Khaokaew, Y.; Xue, H.; and Salim, F. 2025. COMODO: Cross-Modal Video-toIMU Distillation for Efficient Egocentric Human Activity Recognition. arXiv:2503.07259. Chowdhury, R. R.; Kapila, R.; Panse, A.; Zhang, X.; Teng, D.; Kulkarni, R.; Hong, D.; Gupta, R. K.; and Shang, J. 2025. ZeroHAR: Sensor Context Augments Zero-Shot Wearable Action Recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 39(15): 16046 16054. Cormack, G. V.; Clarke, C. L. A.; and Buettcher, S. 2009. Reciprocal rank fusion outperforms condorcet and indiIn Proceedings of the vidual rank learning methods. 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 09, 758759. New York, NY, USA: Association for Computing Machinery. ISBN 9781605584836. DeepMind, G. 2025. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Douze, M.; Guzhva, A.; Deng, C.; Johnson, J.; Szilvasy, G.; Mazare, P.-E.; Lomeli, M.; Hosseini, L.; and Jegou, H. 2025. The Faiss library. arXiv:2401.08281. Erickson, N.; Mueller, J.; Shirkov, A.; Zhang, H.; Larroy, P.; Li, M.; and Smola, A. 2020. AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505. Fang, X.; Xu, W.; Tan, F. A.; Hu, Z.; Zhang, J.; Qi, Y.; Sengamedu, S. H.; and Faloutsos, C. 2024. Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - Survey. Transactions on Machine Learning Research. Feofanov, V.; Wen, S.; Alonso, M.; Ilbert, R.; Guo, H.; Tiomoko, M.; Pan, L.; Zhang, J.; and Redko, I. 2025. Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification. arXiv:2502.15637. Girdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V.; Joulin, A.; and Misra, I. 2023. ImageBind: One Embedding Space To Bind Them All. In CVPR. Goswami, M.; Szafer, K.; Choudhry, A.; Cai, Y.; Li, S.; and Dubrawski, A. 2024. MOMENT: Family of Open Time-series Foundation Models. In International Conference on Machine Learning. Haresamudram, H.; Anderson, D. V.; and Plotz, T. 2019. On the role of features in human activity recognition. In Proceedings of the 2019 ACM International Symposium on Wearable Computers, ISWC 19, 7888. New York, NY, USA: Association for Computing Machinery. ISBN 9781450368704. Ji, S.; Zheng, X.; and Wu, C. 2024. HARGPT: Are LLMs Zero-Shot Human Activity Recognizers? arXiv:2403.02727. Kwapisz, J. R.; Weiss, G. M.; and Moore, S. A. 2011. Activity recognition using cell phone accelerometers. SIGKDD Explor. Newsl., 12(2): 7482. Leng, Z.; Kwon, H.; and Ploetz, T. 2023. Generating Virtual On-Body Accelerometer Data from Virtual Textual In ProDescriptions for Human Activity Recognition. ceedings of the 2023 ACM International Symposium on Wearable Computers, ISWC 23. Li, Z.; Deldari, S.; Chen, L.; Xue, H.; and Salim, F. D. 2025. SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition. arXiv:2410.10624. Luo, Y.; Chen, Y.; Salekin, A.; and Rahman, T. 2024. Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals. arXiv:2412.09758. Meert, W.; Hendrickx, K.; Craenendonck, T. V.; Robberechts, P.; Blockeel, H.; and Davis, J. 2021. DTAIDistance (Version v2). Moon, S.; Madotto, A.; Lin, Z.; Saraf, A.; Bearman, A.; and IMU2CLIP: Language-grounded Damavandi, B. 2023. Motion Sensor Translation with Multimodal Contrastive Learning. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings of the Association for Computational Linguistics: EMNLP 2023, 1324613253. Singapore: Association for Computational Linguistics. Murahari, V. S.; and Plotz, T. 2018. On attention models for human activity recognition. In Proceedings of the 2018 ACM International Symposium on Wearable Computers, ISWC 18, 100103. New York, NY, USA: Association for Computing Machinery. ISBN 9781450359672. Mackey, L.; Belgrave, D.; Fan, A.; Paquet, U.; Tomczak, J.; and Zhang, C., eds., Advances in Neural Information Processing Systems, volume 37, 107469107493. Curran Associates, Inc. Zhang, Y.; Ayush, K.; Qiao, S.; Heydari, A. A.; Narayanswamy, G.; Xu, M. A.; Metwally, A. A.; Xu, S.; Garrison, J.; Xu, X.; Althoff, T.; Liu, Y.; Kohli, P.; Zhan, J.; Malhotra, M.; Patel, S.; Mascolo, C.; Liu, X.; McDuff, D.; and Yang, Y. 2025. SensorLM: Learning the Language of Wearable Sensors. arXiv:2506.09108. Muller, M. 2007. Dynamic time warping. Information Retrieval for Music and Motion, 2: 6984. OpenAI. 2024. GPT-4o mini: advancing cost-efficient https://openai.com/index/gpt-4o-miniintelligence. advancing-cost-efficient-intelligence/. Ordonez, F. J.; and Roggen, D. 2016. Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition. Sensors, 16(1). Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Meila, M.; and Zhang, T., eds., Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, 87488763. PMLR. Reiss, A.; and Stricker, D. 2012. Introducing New BenchIn 2012 16th marked Dataset for Activity Monitoring. International Symposium on Wearable Computers, 108 109. Roggen, D.; Calatroni, A.; Rossi, M.; Holleczek, T.; Forster, K.; Troster, G.; Lukowicz, P.; Bannach, D.; Pirkl, G.; Ferscha, A.; Doppler, J.; Holzmann, C.; Kurz, M.; Holl, G.; Chavarriaga, R.; Sagha, H.; Bayati, H.; Creatura, M.; and del R. Millan, J. 2010. Collecting complex activity datasets in highly rich networked sensor environments. 2010 Seventh International Conference on Networked Sensing Systems (INSS), 233240. Shoaib, M.; Bosch, S.; Incel, O. D.; Scholten, H.; and Havinga, P. J. M. 2014. Fusion of Smartphone Motion Sensors for Physical Activity Recognition. Sensors, 14(6): 1014610176. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Weiss, G. 2019. WISDM Smartphone and Smartwatch Activity and Biometrics Dataset . UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5HK59. Yoon, H.; Tolera, B. A.; Gong, T.; Lee, K.; and Lee, S.-J. 2024. By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 22192241. Miami, Florida, USA: Association for Computational Linguistics. Zhang, M.; and Sawchuk, A. A. 2012. USC-HAD: daily activity dataset for ubiquitous activity recognition In Proceedings of the 2012 using wearable sensors. ACM Conference on Ubiquitous Computing, UbiComp 12, 10361043. New York, NY, USA: Association for Computing Machinery. ISBN 9781450312240. Zhang, X.; Teng, D.; Chowdhury, R. R.; Li, S.; Hong, D.; Gupta, R. K.; and Shang, J. 2024. UniMTS: Unified In Globerson, A.; Pre-training for Motion Time Series. 9 Appendix This appendix provides additional implementation and evaluation details to support the main findings in the paper. We first describe the baseline models used for comparison, followed by summary of the datasets and preprocessing steps. We then present per-class performance results for all benchmarks to complement the aggregate metrics reported in the main text. Finally, we include the full prompts used to query the LLM for each reasoning stage in ZARA. Baselines We provide implementation details for all baselines used in our study, including how each was reproduced or adapted for zero-shot HAR evaluation. HARGPT (Ji, Zheng, and Wu 2024). This method directly prompts LLMs to classify motion time-series data. We follow their original setup by downsampling input signals to 10Hz and applying their prompt template for raw numerical input. To evaluate the multi-modal capabilities of the underlying LLM (GPT-4o-mini (OpenAI 2024)), we additionally provide plotted sensor signals as input. For visual inputs, we draw each 6-channel sensor as an individual subplot and concatenate them into single composite figure to avoid visual clutter in multi-sensor datasets. Gemini (DeepMind 2025). To assess the improvements brought by ZARA, we use Gemini-2.0-Flash, the same LLM backbone adopted in our framework, as standalone baseline. Similar to HARGPT, Gemini is evaluated with raw sequences and plotted sensor signals. Additionally, we include third modality for Gemini: Markdown-formatted structured tables, to test if ZARA-style structured input enhances recognition accuracy. The Gemini baselines allow us to directly evaluate the added value of ZARAs retrieval, knowledge, and reasoning modules beyond the capabilities of the base model alone. ImageBind (Girdhar et al. 2023). ImageBind learns unified embedding space across six modalities: image, text, audio, depth, thermal, and IMU. We use the publicly released imagebind huge checkpoint for zero-shot evaluation. Since ImageBind only supports single-sensor input and requires fixed-length windows (62000), we evaluate each sensor placement separately. To meet the input length requirement, we apply two strategies, repeat padding and linear interpolation to 2000 steps, and report the best sensor placement result for each dataset. IMU2CLIP (Moon et al. 2023). IMU2CLIP aligns inertial measurement unit (IMU) motion data with video and text by projecting them into the joint embedding space of CLIP. Like ImageBind, it only supports single-sensor input and requires fixed-length windows (61000). We evaluate each sensor placement separately and apply both repeat padding and interpolation to meet the input size constraint, reporting the best result per dataset. NormWear (Luo et al. 2024). NormWear is foundation model designed to extract generalized, informative representations from multivariate wearable signals. It has been pretrained on diverse corpus of physiological data, including PPG, ECG, EEG, GSR, and IMU, collected from various public datasets. For zero-shot human activity recognition, we follow the official documentation and adopt their suggested prompt, What is the activity being performed currently?, along with the corresponding activity options for inference. IMUGPT (Leng, Kwon, and Ploetz 2023). IMUGPT generates synthetic training data by first prompting GPT-4omini to produce diverse textual activity descriptions. These texts are converted into 3D motion sequences, and then into virtual IMU streams. For evaluation, we adopt DeepConvLSTM, the best-performing backbone from the original paper. To ensure fair zero-shot comparison, we exclude the supervised distribution calibration phase, which relies on labeled downstream data. UniMTS (Zhang et al. 2024). UniMTS proposes the first unified pretraining framework for motion time-series that generalizes across diverse device configurations (e.g., position, orientation) and activity types. It adopts contrastive learning approach to align motion signals with text descriptions enriched by LLMs, enabling the model to capture the semantic structure of human activities and improve crossactivity generalization. Retrieval Strategies All retrieval strategies in ZARA follow two-stage pipeline. First, each candidate is re-ranked within individual sensor placements based on similarity to the query. Then, results across all placements are aggregated via reciprocal rank fusion (RRF) to generate the final retrieval list. We evaluate four retrieval strategies: DTW and three pretrained foundation encoders. Dynamic Time Warping (DTW) (M uller 2007). We implement DTW using the multi-dimensional variant from the dtaidistance package (Meert et al. 2021). For each query segment and each database candidate, we first apply z-score normalization independently to each of the six sensor channels. Then, we compute the DTW distance between the query and each candidate segment individually. The distance is computed using the distance fast method with pruning enabled to accelerate comparisons. We negate the distances to obtain similarity scores and return the top-k candidates. Moment (Goswami et al. 2024). Moment is time-series foundation model (TSFM) based on the T5 architecture, pretrained on range of time-series tasks including classification, anomaly detection, and forecasting. We evaluate both the moment-small and moment-large variants, with embedding dimensions of 512 and 1024 respectively. For multichannel inputs, Moment averages the per-channel embeddings to produce single representation for retrieval. However, Moment does not support true zero-shot classification. Following their original pipeline, we freeze the Moment encoder and train an SVM classifier on the database split. Results are reported using greedy search over SVM hyperparameters to select the best-performing configuration. 10 # Classes Classes Dataset Opportunity UCI-HAR Shoaib PAMAP2 USC-HAD 12 7 12 12 MHealth 12 WISDM 18 DSADS 19 Stand, Walk, Sit, Lie Standing, Sitting, Laying, Walking, Walking downstairs, Walking upstairs Walking, Standing, Jogging, Sitting, Biking, Downstairs, Upstairs Lying, Sitting, Standing, Ironing, Vacuum cleaning, Ascending stairs, Descending stairs, Walking, Nordic walking, Cycling, Running, Rope jumping Sleeping, Sitting, Elevator down, Elevator up, Standing, Jumping, Walking downstairs, Walking right, Walking forward, Running forward, Walking upstairs, Walking left Climbing stairs, Standing still, Sitting and relaxing, Lying down, Walking, Waist bends forward, Frontal elevation of arms, Knees bending (crouching), Jogging, Running, Jump front & back, Cycling Walking, Jogging, Stairs, Sitting, Standing, Typing, Brushing Teeth, Eating Soup, Eating Chips, Eating Pasta, Eating Sandwich, Kicking Ball, Playing Catch Ball, Drinking, Dribbling Ball, Writing, Clapping, Folding Clothes Sitting, Standing, Lying on back, Lying on right side, Ascending stairs, Descending stairs, Standing in elevator, Moving around in elevator, Walking slowly, Rowing, Jumping, Walking on treadmill in flat positions, Walking on treadmill in inclined positions, Running on treadmill fast, Exercising on stepper, Exercising on cross trainer, Playing basketball, Cycling on an exercise bike in horizontal positions, Cycling on an exercise bike in vertical positions Table 3: Dataset classes and sensor placements. Sensor Placements Back, upper arms, lower arms Waist right pockets, left pockets, belt, Right upper arm, right wrist Wrist, chest, ankle Front right hip Chest, right wrist, left ankle Hand Torso, right arm, left arm, right leg, left leg Mantis (Feofanov et al. 2025). Mantis is foundation model for time-series classification, built on the Vision Transformer (ViT) architecture and pre-trained via contrastive learning. Mantis has also been pre-trained on human activity recognition datasets. For input processing, it first scales all time-series inputs to fixed length of 512, then extracts 256-dimensional embedding from each channel and concatenates them to form unified representation for classification or retrieval. As Mantis also does not support true zero-shot classification, we follow its original method: using the frozen Mantis encoder to generate embeddings and training random forest classifier on the database split."
        },
        {
            "title": "Datasets and Data Preprocessing",
            "content": "Due to the cost constraints of API-based inference and the need for detailed ablation studies, we evaluate each dataset using randomly sampled inference subset. For every dataset, we ensure balanced representation by sampling an equal number of non-overlapped instances per activity class and subject. This strategy maintains balance between cost-efficiency and diversity across datasets, activity types, and subjects. To ensure fair comparison, the datasets used are kept consistent across all baselines. All datasets consist of multiple activity classes, with their corresponding sensor placements summarized in Table 3. Opportunity (Roggen et al. 2010). The dataset contains recordings from 4 subjects at sampling rate of 30 Hz. We designate Subject 4 as the inference user and use data from the remaining subjects to build the retrieval database. Motion sensor data are segmented into non-overlapping 2second windows (60 timesteps each). For Inference, we randomly sample 50 windows per activity class from the inference split, resulting in balanced set of 200 samples. UCI-HAR (Anguita et al. 2013). The dataset contains recordings from 30 volunteers, sampled at 50 Hz. The dataset is pre-segmented using fixed-width sliding windows of 2.56 seconds with 50% overlap. Following the original split, we use data from test set (9 subjects) for inference and the remaining for the database. From the inference set, we randomly sample 40 windows per activity, ensuring userbalanced representation within each class, resulting in total of 240 samples. Shoaib (Shoaib et al. 2014). The dataset contains recordings from 10 subjects, sampled at 50 Hz. We use data from subjects 1 and 9 for inference and the remaining subjects for the database. The recordings are segmented into nonoverlapping windows of 2 seconds (100 timesteps). For inference, we randomly sample 30 windows per activity from the inference users (15 from each) yielding class-balanced test set of 210 samples. PAMAP2 (Reiss and Stricker 2012). This dataset contains recordings from 9 subjects at sampling rate of 100 Hz. We designate subjects 5 and 6 for inference, and use the rest for the database. Recordings are segmented into nonoverlapping 2-second windows (200 time steps). For inference, we randomly sample 20 windows per activity from the inference users (10 from each) except for rope jumping, which has limited data. For this activity, we include 18 samples from subject 5 and 2 from subject 6, resulting in total of 210 samples. USC-HAD (Zhang and Sawchuk 2012). This dataset includes motion recordings from 14 subjects at sampling rate of 100 Hz. We designate subjects 13 and 14 for inference, using the remaining subjects to build the database. Data are segmented into non-overlapping 2-second windows (200 time steps). For inference, we randomly sample 20 windows per activity (10 from each subject), resulting in 240 total samples. Mhealth (Ba nos et al. 2014). The MHealth dataset contains recordings from 10 subjects at sampling rate of 50 Hz. We use subjects 1 and 6 for inference and the remaining subjects to construct the database. Signals are segmented into non-overlapping 2-second windows (100 time steps). For inference, we randomly sample 20 windows per activity (10 from each subject), yielding total of 240 evaluation samples. WISDM (Weiss 2019). We use the smartwatch-on-hand subset of the WISDM dataset, recorded at 20 Hz. Accelerometer and gyroscope signals are aligned by timestamp, and we select 47 users whose data show no alignment anomalies. Among them, 8 users are held out for inference and the rest are used to construct the database. Following the datasets recommendation, we segment the data into nonoverlapping 10-second windows (200 time steps). For inference, we randomly sample 16 windows per activity (2 from each subject), resulting in total of 288 inference samples. DSADS (Altun, Barshan, and Tuncel 2010). The DSADS dataset contains recordings from 8 users at sampling rate of 25 Hz. We designate subjects 2 and 4 for inference and use the remaining users to build the database. We adopt the predefined 5-second windows (125 time steps) provided by the dataset. For inference, we randomly sample 10 windows per activity (5 from each subject), yielding total of 190 inference samples. Ablation Removing Retrieval Reduces Performance. Table 4 provides the exact values underlying Figure 5, comparing ZARAs performance with and without the Evidence Retrieval module. We report both the final zero-shot classificaDataset With Retrieval No Retrieval Opportunity UCI-HAR Shoaib PAMAP2 USC-HAD MHealth WISDM DSADS Average Acc 92.5 90.0 97.1 76.7 60.0 86.3 65.6 84.2 81.6 UB 96.0 99.6 99.5 84.2 80.8 99.6 78.8 92.6 91.4 Acc 84.0 86.3 91.4 57.9 47.9 76.3 54.9 75.3 71.8 UB 92.0 99.2 99.5 72.1 71.3 98.3 75.0 85.8 86.7 Table 4: Impact of Retrieval on Evidence Pruning and Decision and Insight Agents. We report zero-shot accuracy (Acc) and upper-bound accuracy (UB) with and without retrieval across all datasets. Dataset Opportunity UCI-HAR Shoaib PAMAP2 USC-HAD MHealth WISDM DSADS Average Pruning Upper Bound No Pruning Avg. Length 92.5 90.0 97.1 76.7 60.0 86.3 65.6 84.2 81.6 96.0 99.6 99.5 84.2 80.8 99.6 78.8 92.6 91.4 73.0 78.3 93.3 55.8 50.4 76.7 54.2 64.2 68. 2.04 2.20 2.42 2.58 2.86 2.59 2.56 2.65 2.49 Table 5: Impact of the Evidence Pruning Agent: Accuracy (%) and Upper Bound with and without pruning, along with the average pruned shortlist length per dataset. tion accuracy and the pruning-stage upper bound accuracy for each dataset. Skipping Evidence Pruning Hurts. Table 5 provides the dataset-level breakdown of ZARAs zero-shot accuracy with and without the Evidence Pruning Agent, as well as the corresponding upper-bound accuracy and average length of the pruned candidate shortlist. These results complement Figure 6 in the main paper, confirming that pruning significantly improves model performance while preserving high-quality candidates. No Prior Knowledge Fails. Table 7 provides the exact values plotted in Figure 7, comparing ZARAs performance with and without the prior knowledge base across all eight datasets. These results confirm that prior knowledge plays critical role in both narrowing and distinguishing among activity classes. Retrieval Latency by Dataset. Table 6 reports the average per-query latency (in seconds) of each retrieval method across all eight datasets. The measurements were taken on an Apple M2 Max CPU with 64GB memory. Latency 12 Dataset Opportunity UCI-HAR Shoaib PAMAP2 USC-HAD Mhealth WISDM DSADS # of Channels Window Size Database Size DTW Moment-s Moment-l Mantis 30 6 30 18 6 15 6 30 60 128 100 200 200 100 200 125 6968 7352 5040 7138 11889 2799 14287 6840 0.5814 0.1389 0.4935 0.5104 0.2584 0.1504 0.3152 0. 0.0683 0.0169 0.0686 0.0446 0.0180 0.0405 0.0190 0.0741 0.1508 0.0342 0.1700 0.1053 0.0389 0.0900 0.0369 0.1762 0.3072 0.0623 0.3122 0.1777 0.0699 0.1528 0.0679 0.3105 Table 6: Per-query retrieval latency (in seconds) and dataset characteristics. Prompt Template Below we provide the full prompts used by ZARA to query the LLM during inference. These cover all stages of reasoning, First Feature Selector (Figure 9), Evidence Pruning (Figure 10), Second Feature Selector (Figure 11), and Decision Insight (Figure 12), along with their corresponding inputs and output formats. While the exact wording may vary slightly across datasets or instances, the underlying structure is consistent throughout all experiments. Dataset With knowledge No knowledge Opportunity UCI-HAR Shoaib PAMAP2 USC-HAD Mhealth WISDM DSADS Average Acc 92.5 90.0 97.1 76.7 60.0 86.3 65.6 84.2 81. UB 96.0 99.6 99.5 84.2 80.8 99.6 78.8 92.6 91.4 Acc 82.0 68.8 77.6 56.7 35.4 80.8 53.8 52.1 63. UB 99.0 98.9 97.1 71.3 81.7 94.6 73.6 79.5 87.0 Table 7: Impact of Prior Knowledge Injection on Feature Selector Accuracy and Upper Bound. varies depending on window length, number of channels, and database size, but the relative ranking remains consistent: DTW is consistently the slowest, Moment-small is the fastest, and Mantis offers balanced trade-off between speed and retrieval quality. Per-Class Evaluation. In Table 1 of the main paper, we report only the overall accuracy and macro F1 score for each dataset. ZARA consistently achieves high per-class accuracy and F1, showcasing its ability to distinguish wide range of behaviors by leveraging structured knowledge and retrieved evidence. In contrast, baseline methods often show much lower F1 scores than accuracy, indicating tendency to favor familiar classes seen during pretraining. This underscores ZARAs stronger generalization to unseen activities in more balanced and interpretable manner. To further assess performance consistency across different activity types, we provide detailed per-class accuracy in Figure 8. Predefined Features To support feature-based reasoning and retrieval, we extract comprehensive set of handcrafted features from each sensor channel (6 axes per sensor + 2 magnitude channels). These features cover both timeand frequency-domain characteristics, designed to capture fine-grained temporal, statistical, and spectral patterns in motion signals. The full set of features used for each channel is summarized in Table 8. 13 Category Feature Description Time-domain Features Frequency-domain (FFT) Mean, Standard Deviation (STD), Variance, Maximum, Minimum, Median, Root Mean Square (RMS), Peak Amplitude, Zero-Crossing Rate, Slope, Mean/RMS/STD of FirstOrder Differences, Range, Sum, Signal Absolute Value, Mean Absolute Value, Interquartile Range, Skewness, Kurtosis, Signal Magnitude Area Band Power (Low, Mid, High), Band Power Ratio (Low, Mid, High), Dominant Frequency, Power of Dominant Frequency, Second Peak Frequency and Power, Spectral Centroid, Spectral Entropy, Spectral Skewness, Spectral Kurtosis, Weighted Average Frequency, Spectral Energy, Max Power Index Frequency-domain (STFT) STFT Max / Mean / STD in Low, Mid, High bands, STFT Entropy (Mean, Max, STD), STFT Centroid (Mean, Max, STD) Autocorrelation First Peak Lag, First Minimum Lag, First Zero-Crossing Lag Jerk-based Features Jerk RMS, Peak, Zero-Crossing Rate Cross-Channel Features Pearson Correlation Between Channels Table 8: Predefined features extracted per sensor channel (6 axes + 2 magnitudes per sensor). Figure 8: Per-Class Evaluation. 14 System Instruction Task Youre an expert in Human Activity Recognition, with focus on identifying the most effective features for distinguishing between human activities. Glossary of Abbreviations GLOSS TEXT Instructions Based on the user-provided Top Features per Activity Pair, select up to TOP unique features that best distinguish the specified Target Activities. When selecting features, prioritize those that: Appear consistently across multiple activity pairs, or Have relatively high importance scores within specific pairs. Output Format Return only the following, with no extra text or line breaks: Index Feature Name User Prompt Target Activities ACTIVITY LIST Top Features per Activity Pair (Ranked by Importance Score) (PAIR NUM activity pairs in total) PAIR WISE KNOWLEDGE Figure 9: Prompt template for the first Feature Selector agent. System Instruction Task Youre an expert in Human Activity Recognition. Your task is to narrow down the set of plausible activities for the QUERY segment, based on the given features and their statistical distributions in the user-supplied activities table. Instructions Each row in the activities table represents an activity class, with cells showing the mean std of each feature. The QUERY row presents the feature values of the segment to classify. You must select at least two activity classes, and ideally all activity classes that are reasonably or even marginally plausible for the QUERY. For each selected activity class, provide brief explanation of why it is plausible match. Output Format Return only the following, in this exact order, with no additional text or line breaks: Index Activity Reason User Prompt Activities Table ACTIVITIES FEATURES TABLE Figure 10: Prompt template for Evidence Pruning agent. 15 System Instruction Task Youre an expert in Human Activity Recognition, with focus on identifying the most effective features for distinguishing between human activities. Glossary of Abbreviations GLOSS TEXT Instructions Based on the user-provided Top Features per Activity Pair, select up to TOP unique features that best distinguish the specified Target Activities. When selecting features, prioritize those that: Appear consistently across multiple activity pairs, or Have relatively high importance scores within specific pairs. For each selected feature, give: Definition concise, clear explanation of the feature. Discriminative Power Summarize the following: * Which activity pairs this feature helps to distinguish. * The relative importance rate of this feature within each activity pair, indicating how effectively it differentiates between the two activities in that pair. Output Format Return only the following, with no extra text or line breaks: Index Feature Name Definition Discriminative Power User Prompt Target Activities ACTIVITY LIST Top Features per Activity Pair (Ranked by Importance Score) (PAIR NUM activity pairs in total) PAIR WISE KNOWLEDGE Figure 11: Prompt template for second Feature Selector agent. 16 System Instruction Task You are an expert in Human Activity Recognition. Your goal is to determine the most probable activity class for the QUERY segment by comparing its feature values against the statistical distributions in the user-provided activities table. Sensor Feature Explanation Guide Table This table describes each feature and indicates which activity classes it helps to distinguish between. FEATURES REFERENCE TABLE Instructions Each row in the activities table corresponds to an activity class, with each cell showing the mean standard deviation for feature. The QUERY row presents the feature values of the segment to classify. Select the single most likely activity class, and base your decision on specific feature(s) in the QUERY row. In your explanation: Explicitly compare the Querys feature values to each classs distribution, explaining why the predicted class is better match than each alternative. When unsure, refer to the Discriminative Power in the guide table to justify how strongly each feature helps distinguish the specific activities. Output Format Respond with exactly one line in this JSON format (no extra text or line breaks): json { \"reason\": \"<your detailed explanation>\", \"predicted_class\": \"<ClassName>\" } User Prompt Activities Table ACTIVITIES FEATURES TABLE Figure 12: Prompt template for Decision Insight agent."
        }
    ],
    "affiliations": [
        "University of New South Wales, Sydney"
    ]
}