{
    "paper_title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents",
    "authors": [
        "Tianshi Zheng",
        "Kelvin Kiu-Wai Tam",
        "Newt Hue-Nam K. Nguyen",
        "Baixuan Xu",
        "Zhaowei Wang",
        "Jiayang Cheng",
        "Hong Ting Tsang",
        "Weiqi Wang",
        "Jiaxin Bai",
        "Tianqing Fang",
        "Yangqiu Song",
        "Ginny Y. Wong",
        "Simon See"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 2 7 1 7 0 . 0 1 5 2 : r a"
        },
        {
            "title": "NewtonBench",
            "content": "NEWTONBENCH: BENCHMARKING GENERALIZABLE SCIENTIFIC LAW DISCOVERY IN LLM AGENTS Tianshi Zheng1, Kelvin Kiu-Wai Tam1, Newt Hue-Nam K. Nguyen1 Baixuan Xu1, Zhaowei Wang1, Jiayang Cheng1, Hong Ting Tsang1, Weiqi Wang1 Jiaxin Bai1, Tianqing Fang, Yangqiu Song1, Ginny Y. Wong2, Simon See2 1The Hong Kong University of Science and Technology, 2NVIDIA {tzhengad, kwtamai, khnnguyen}@connect.ust.hk, yqsong@cse.ust.hk {gwong, ssee}@nvidia.com Code and Data: https://github.com/HKUST-KnowComp/NewtonBench"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are emerging as powerful tools for scientific law discovery, foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from fundamental methodological trilemma, forcing trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NEWTONBENCH, benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shiftssystematic alterations of canonical lawsto generate vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive evaluation of 11 state-of-the-art LLMs reveals clear but fragile capability for discovery in frontier models: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover paradoxical effect of tool assistance: providing code interpreter can hinder more capable models by inducing premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge for the future of automated science. By providing scalable, robust, and scientifically authentic testbed, NEWTONBENCH offers crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery."
        },
        {
            "title": "INTRODUCTION",
            "content": "Scientific law discovery, the process of distilling complex natural phenomena into concise, predictive mathematical principles, represents cornerstone of human intellectual achievement, having consistently catalyzed paradigm shifts across the sciences (Langley et al., 1987; Popper, 2002). This endeavor demands sophisticated interplay of abilities central to the scientific method: formulating hypotheses, methodical experimentation, and synthesizing empirical findings into universal laws (Peirce, 1958; Schmidt & Lipson, 2009). The rapid advancement of Large Language Models (LLMs) has brought this grand challenge into sharp focus (OpenAI, 2024; Google, 2025). Exhibiting remarkable emergent capabilities in mathematical reasoning (Ahn et al., 2024), logical reasoning (Yu et al., 2024), and agentic planning (Huang et al., 2024b), these models appear to possess foundational skills for scientific inquiry. This Equal Contribution. Corresponding Author."
        },
        {
            "title": "NewtonBench",
            "content": "Table 1: Comparison of benchmarks for scientific law discovery. The Sci. Rel. column (scientific relevance) indicates if problems are derived from real-world scientific laws (versus being synthetic). The Mem.-Free column indicates whether the benchmark fully prevents memorization. Benchmark # Problems Discovery Scope Sci. Rel. Mem.-Free Scalable Data Acquisition Difficulty Traditional Real-World Laws AI Feynman (Udrescu & Tegmark, 2020) EmpiricalBench (Cranmer, 2023) Out-of-Distribution Laws LLM-SR (Shojaee et al., 2025a) EvoSLD (Lin et al., 2025) 120 9 4 5 Function Function Function Function Transformed Laws LSR-Transform (Shojaee et al., 2025b) Function Synthetic Laws LSR-Synth (Shojaee et al., 2025b) PhysSymbol (Liu et al., 2025) NEWTONBENCH (Ours) 128 5,000 324 Function Function Model System Partially Tunable Fixed Passive Observation Passive Observation Fixed Fixed Passive Observation Passive Observation Fixed Passive Observation Fixed Tunable Passive Observation Passive Observation Multi-dimensional Active Exploration has sparked widespread curiosity (Armitage, 2025; Fang et al., 2025), crystallized by provocative questions such as whether an LLM could rediscover Newtons laws from its own intelligence (He, 2023). However, answering such questions requires moving beyond anecdotal observations. To foster and guide progress toward automated science, it is critical to develop robust testbed to rigorously benchmark the true abilities and inherent limitations of current LLMs in this domain. Nevertheless, our closer inspection reveals that existing benchmarks for LLM-driven scientific law discovery exhibit substantive shortcomings that prevent robust evaluation (as illustrated in Table 1). Current approaches fall into three paradigms, each with critical limitation. First, the Out-of-Distribution (OOD) Laws paradigm uses complex, lesser-known principles to ensure relevance and memorization resistance, but their scarcity makes it impractical to scale. Second, the Transformed Laws paradigm provides scalability by rewriting known principles, yet remains vulnerable to memorization, as models may simply recall the original law rather than reason from the provided data (as evidenced in Appendix E.1). Finally, the Synthetic Laws paradigm achieves maximum scalability and is inherently memorization-free, but at the cost of scientific relevance, as it relies on algorithmically generated equations that lack correspondence to real-world phenomena. Collectively, these paradigms expose fundamental methodological trilemma: forced trade-off between scientific relevance, scalability, and resistance to memorization. While the trilemma highlights the trade-offs between existing paradigms, more fundamental limitation underlies them all: they frame scientific discovery as problem of static function discovery, where the goal is simply to fit mathematical formula to variables in tabular dataset. This static framing is stark departure from the authentic scientific process, which relies on the interactive exploration of complex model systems (Cartwright, 1983; de Silva et al., 2020)for instance, J.J. Thomsons inference of the e/m ratio from cathode ray manipulation. Therefore, to meaningfully benchmark LLMs, evaluations must move beyond static function fitting and embrace the more scientifically authentic challenge of model discovery in interactive environments (Fang et al., 2025). To address these limitations and facilitate more rigorous evaluation, we introduce NEWTONBENCH. Our design is built on two core principles, each targeting fundamental shortcoming in prior work. First, to resolve the methodological trilemma, we introduce the metaphysical shift (or counterfactual shift)a technique that systematically alters the mathematical structure of canonical physical laws, for instance, by modifying operators or exponents (see examples in Figure 1). This generates vast suite of laws that are conceptually grounded but physically novel. This approach simultaneously achieves the scalability of synthetic laws, maintains the scientific relevance of real-world principles, and ensures resistance to memorization by creating problems that cannot be solved by recall, thus forcing models to reason from first principles. Complementing this approach, our second principle tackles the limitation of static function discovery. To this end, we designed NEWTONBENCH as an interactive, system-oriented environment. Rather than fitting formula to pre-existing table, an agent must actively design experiments by specifying input parameters (e.g., setting the mass of an object or the initial velocity in simulation) and interpreting the resulting feedback from the virtual environment. Critically, the target law is embedded within complex model with confounding variables. This design compels the agent to"
        },
        {
            "title": "NewtonBench",
            "content": "engage in the interactive exploration of complex model system, elevating the core challenge from simple function fitting to the more scientifically representative task of model discovery. Furthermore, NEWTONBENCH is designed to precisely probe the limits of an LLMs scientific capabilities. It features two independent dimensions of difficulty control: the intrinsic complexity of the target law (tuned via the metaphysical shift) and the extrinsic complexity of the surrounding experimental system. This allows for fine-grained analysis of models breaking points. To isolate the challenge of discovery itself, we introduce an advanced setting where the LLM is provided with code execution interface for tasks like numerical regression or hypothesis testing. By removing computational limitations as primary bottleneck, this setup is designed to push models from being merely compute-bound to being truly discovery-bound, revealing the genuine frontiers of their scientific reasoning abilities. Experimental results from 11 state-of-the-art LLMs on NEWTONBENCH reveal clear but fragile capability for scientific law discovery. While frontier models like GPT-5 and Gemini-2.5-pro demonstrate proficiency in simple, isolated systems, their performance degrades precipitously as the intrinsic complexity of the law or the extrinsic complexity of the experimental system increases. This fragility is further underscored by an extreme sensitivity to observational noise, where even minimal perturbations cause sharp decline in symbolic accuracy. Furthermore, our analysis uncovers paradoxical effect of code assistance: providing code interpreter boosts weaker models by offloading computation, but paradoxically hinders stronger models. We trace this performance degradation to premature shift from exploration to exploitation, where capable agents over-rely on the tool for local optimization at the expense of discovering the globally correct law. Collectively, these results demonstrate that while LLMs are beginning to develop foundational scientific reasoning skills, achieving robust, generalizable discovery in complex, interactive environments remains the core challenge for the future of automated science."
        },
        {
            "title": "2 TASK FORMALIZATION",
            "content": "We formalize the discovery task in NEWTONBENCH around two core concepts: Equation and Model. An agent is required to discover target equation by experimenting within given model. Definition 1 (Equation). An equation is treated as mathematical expression that acts as function, mapping set of input variables to single numerical output. It is structurally represented by an Expression Tree , form of Abstract Syntax Tree (AST). The nodes of are categorized as follows: 1) Leaf Nodes NL: These represent the operands of the equation. They can be either input variables (e.g., mass, distance) or numeric literals (e.g., coefficients, known constants). 2) Internal Nodes NI : These represent mathematical operators that are applied to their child nodes. The operators can be unary (e.g., sin(x), x) or binary (e.g., + y, y). The complete set of operators used in NEWTONBENCH is detailed in Appendix A.3.2. The evaluation of an equation for given assignment of values to its input variables is performed via post-order traversal of its expression tree . This recursive evaluation process is formally described in Appendix A.3.1. Definition 2 (Model). model represents an experimental system and is defined as an ordered sequence of equations, = (f1, f2, . . . , fk). The model takes set of system-level input variables VM and produces set of final outputs YM. The computation within proceeds sequentially through its ordered equations. The inputs for any equation fi can be drawn from the models primary inputs VM or the outputs of any preceding equations {yj < i}. The models final output, YM, is designated subset of the collected outputs from all equations, i.e., YM {y1, . . . , yk}. Based on these definitions, in our benchmark, one equation ftarget is designated as the hidden physical law for the agent to discover. All other equations in the given model, forming the set of assisting equations Fassist = {f1, . . . , fk} {ftarget}. The physical laws in assisting equatons are provided as known information via prompt to foster discovery. The tasks complexity is determined by the structure of (illustrated in Figure 1), which we define across three parallel settings. The simplest setting is Vanilla Equation, where the model contains only the target law (M = (ftarget)). The challenge escalates in the Simple System and Complex"
        },
        {
            "title": "NewtonBench",
            "content": "Figure 1: An illustration of core designs in NEWTONBENCH: experimentation with model system, metaphysical shifts in physical laws from various domains, and agentic exploration settings. System settings, where ftarget is embedded within larger model (k > 1). In these more advanced model discovery tasks, the agent must leverage assisting equations to disentangle confounding variables and uncover the target law. Details on system configurations are provided in Appendix A.2. The solvability proof of our task settings is provided in Appendix E.2."
        },
        {
            "title": "3 NEWTONBENCH",
            "content": "In this section, we introduce the construction of NEWTONBENCH, detailing its three core components: physical law curation (3.1), the virtual environment (3.2), and the evaluation metrics (3.3), with full implementation details provided in Appendix A."
        },
        {
            "title": "3.1 PHYSICAL LAW CURATION",
            "content": "Our benchmarks physical law curation process involves two primary stages: first, the collection of canonical seed laws, and second, the application of Metaphysical Shifts to generate novel variations. For the first stage, the selection of our seed laws is guided by three criteria: they must be foundational to their respective fields, span diverse range of physics domains, and, critically, each must allow for dimensionally consistent mutations (often via adjusting physical constants). The second stage involves applying the principle of Metaphysical Shifts (Wang & Song, 2025), which serves primarily to prevent agents from solving tasks by memorizing known laws. This principle is grounded in philosophical concepts regarding the contingency of natural laws and the existence of possible worlds1 (Lewis, 1986; Lange, 2008). In our context, metaphysical shift is systematic alteration of canonical physical law, which effectively creates novel yet physically plausible universe for the agent to discover in each task. Operationally, these shifts are implemented as mutation operations (Koza, 1994) on the equations expression tree. These mutations can alter either the mathematical operators (e.g., changing an addition to multiplication) or the values of numeric constants (e.g., modifying an exponent to change quadratic relationship to cubic one). critical consequence of such alterations is the 1This approach is analogous to the use of counterfactual reasoning in physics (Elwenspoek, 2012)."
        },
        {
            "title": "NewtonBench",
            "content": "potential disruption of dimensional consistency. To preserve the physical coherence of the modified equation, this disruption is offset by systematically adjusting the dimensional units of the embedded physical constant. This necessity dictates that every target law in our benchmark includes at least one such constant to serve this compensatory role. Three levels of equation difficulty are categorized based on the cumulative extent of mutations applied to canonical law2. Easy laws are generated via 12 mutations from the original law. Medium laws are subsequently derived from Easy laws through 12 additional mutations, and Hard laws are similarly derived from Medium laws. We curated 108 shifted laws from 12 canonical laws under strict guidelines to ensure scientific plausibility, conducted by three domain-expert co-authors. The size of our shifted laws is comparable to existing synthetic benchmarks (Shojaee et al., 2025b). Appendix A.3.3 provides the complete set of laws, their mutated forms, and the curation guidelines."
        },
        {
            "title": "3.2 VIRTUAL ENVIRONMENT & TASKS",
            "content": "The LLM agent interacts with this virtual environment through two primary interfaces: an experimental tool to probe the physical model system and code interpreter to perform complex computations. Each of the 108 shifted laws is instantiated in three model systems (Vanilla Equation, Simple System, Complex System), yielding 324 total tasks. Experimental Tool core design principle of our benchmark is the shift from passive data observation to active agentic exploration. We formalize this by framing the experimental system, or model M, as an interactive black-box tool. Instead of analyzing static dataset, the agent must strategically probe to gather evidence about the hidden target equation ftarget. The agent interacts with by invoking <run experiment> tool. This interaction follows precise protocol: 1) The agent proposes specific assignment of values for the models input variables, VM. 2) The environment evaluates the full model with these inputs and returns the corresponding set of final model outputs, YM. By iteratively executing these experiments, the agent builds dataset of input-output pairs to deduce the structure of ftarget. This task is non-trivial, especially in the Simple and Complex System settings, as the agent must leverage the provided assisting equations (Fassist) to reason about the models internal state and isolate the behavior of ftarget. To facilitate this process, we provide instructional scaffolding in the initial prompt regarding tool syntax and objectives (detailed in Appendix C), while reserving the experimental strategy entirely for the agents reasoning capabilities. Code Assistance While LLMs excel at symbolic reasoning, their native computational capabilities can be bottleneck for precisely determining numerical constants or fitting complex functional forms, particularly for equations involving exponential or logarithmic operations. To ensure our benchmark evaluates scientific discovery rather than raw computational prowess, we introduce an optional Code Assistance setting. In this configuration, agents are provided with code interpreter tool, invoked via <python> tags. The agent can write and execute arbitrary Python code and receives the output from the execution environment. Crucially, the agent must autonomously decide how and when to leverage this toolfor instance, to perform numerical computation, regression, or verify hypothesis. By providing this computational offloading capability, we aim to shift the evaluation from being compute-bound to discovery-bound, thereby better isolating the agents scientific reasoning abilities."
        },
        {
            "title": "3.3 EVALUATION METRICS",
            "content": "We evaluate agent performance using two metrics: Symbolic Accuracy to assess the correctness of the discovered equations structure, and Root Mean Squared Logarithmic Error (RMSLE) to quantify its predictive fidelity to the data. 2Consequently, equation difficulty arises from two factors: equation complexity and the level of distributional shift. 5 1.80 2. 2.96 4."
        },
        {
            "title": "NewtonBench",
            "content": "Table 2: Experiment results aggregated over 12 domains by equation difficulty (easy, medium, hard) and system complexity, reported as mean and standard deviation from four runs. Within each agent configuration, the highest score is in bold and the second-highest is underlined. LLM Agents easy medium hard easy medium hard easy medium hard SA(%) RMSLE #Tokens (k) Vanilla Equation Simple System Complex System Average Vanilla Agent GPT-4.1-mini 20.8 (9.488) 2.8 (2.268) 0.7 (1.389) 10.4 (2.660) 1.4 (2.778) 0.0 (0.000) 5.6 (2.268) 0.0 (0.000) 0.0 (0.000) 4.6 (1.260) 4.3363 (0.358) GPT-4.1 o4-mini 20.1 (4.744) 1.4 (1.604) 0.0 (0.000) 10.4 (3.495) 3.5 (3.495) 1.4 (1.604) 7.6 (2.660) 2.8 (3.208) 0.7 (1.389) 5.3 (0.684) 3.6171 (0.170) 91.0 (1.389) 75.0 (0.000) 47.2 (11.785) 72.9 (1.389) 50.7 (7.979) 22.2 (3.928) 46.5 (1.389) 19.4 (6.001) 2.8 (2.268) 47.5 (2.063) 1.0908 (0.115) 12.28 GPT-5-mini 92.4 (2.660) 83.3 (7.172) 47.9 (10.486) 79.2 (2.778) 59.0 (3.495) 22.9 (4.744) 50.7 (5.727) 22.9 (1.389) 4.9 (2.660) 51.5 (0.154) 0.9598 (0.096) 9.88 GPT92.4 (1.389) 91.7 (0.000) 83.3 (6.001) 94.4 (3.928) 83.3 (3.928) 65.3 (3.586) 68.8 (4.167) 47.2 (3.208) 29.9 (2.660) 72.9 (0.812) 0.4525 (0.075) 19.07 DeepSeek-V 47.2 (6.804) 9.7 (4.811) 2.1 (2.660) 20.8 (6.612) 0.7 (1.389) 0.0 (0.000) 6.9 (3.586) 0.0 (0.000) 0.0 (0.000) 9.7 (1.643) 2.8380 (0.067) 1.59 DeepSeek-R 89.6 (3.495) 65.3 (5.319) 29.2 (5.319) 66.0 (9.178) 33.3 (6.001) 11.1 (2.268) 38.9 (8.178) 22.9 (9.983) 3.5 (4.167) 40.0 (2.024) 1.3535 (0.278) 16.46 QwQ-32b 80.6 (2.268) 47.2 (15.713) 15.3 (4.811) 56.9 (5.782) 24.3 (7.979) 0.7 (1.389) 21.5 (4.167) 11.1 (5.072) 0.0 (0.000) 28.6 (3.603) 2.4241 (0.282) 13.99 Qwen-3-235b 82.6 (4.744) 55.6 (6.001) 16.7 (4.536) 54.9 (9.454) 20.8 (6.612) 1.4 (1.604) 31.9 (7.349) 4.2 (3.586) 0.7 (1.389) 29.9 (1.892) 2.0118 (0.041) 12.86 Gemini-2.5-flash 90.3 (3.586) 77.8 (5.072) 35.4 (8.893) 84.0 (5.727) 47.9 (5.258) 20.1 (4.744) 47.9 (4.744) 26.4 (4.811) 4.9 (2.660) 48.3 (1.826) 0.8253 (0.072) 31.39 Gemini-2.5-pro 98.6 (1.604) 88.2 (1.389) 66.7 (5.072) 90.3 (4.811) 79.9 (5.258) 48.6 (9.488) 61.8 (2.660) 37.5 (3.586) 13.9 (2.268) 65.0 (2.173) 0.4697 (0.066) 20.77 Agent with Code Assistance GPT-4.1-mini 40.3 (4.811) 13.9 (3.928) 4.9 (2.660) 20.8 (6.612) 10.4 (3.495) 4.9 (2.660) 16.0 (7.979) 5.6 (2.268) 0.7 (1.389) 13.0 (0.636) 3.0000 (0.073) GPT-4. o4-mini 59.7 (1.604) 18.1 (3.586) 7.6 (1.389) 28.5 (8.295) 9.0 (2.660) 4.9 (2.660) 13.2 (4.744) 2.1 (2.660) 2.1 (1.389) 16.1 (0.729) 2.4598 (0.233) 88.9 (4.536) 69.4 (6.804) 34.7 (4.811) 70.1 (8.599) 45.8 (3.586) 13.9 (6.001) 45.8 (5.319) 15.3 (2.778) 5.6 (2.268) 43.3 (3.422) 1.3706 (0.099) 11.00 GPT-5-mini 91.7 (2.268) 72.9 (4.744) 39.6 (5.258) 70.1 (5.727) 44.4 (7.522) 13.9 (3.928) 47.9 (8.893) 18.1 (4.811) 3.5 (2.660) 44.7 (2.069) 1.0295 (0.058) 10.20 GPT-5 92.4 (4.167) 93.7 (2.660) 72.9 (5.258) 90.3 (1.604) 77.1 (1.389) 54.2 (6.612) 71.5 (3.495) 43.1 (3.586) 31.2 (5.258) 69.6 (2.055) 0.6133 (0.152) 18.80 DeepSeek-V3 46.5 (7.305) 17.4 (4.167) 4.2 (1.604) 18.7 (4.167) 4.2 (2.778) 0.7 (1.389) 11.1 (0.000) 2.1 (1.389) 2.8 (2.268) 12.0 (1.342) 2.7577 (0.050) 2.50 DeepSeek-R1 77.8 (6.804) 54.2 (5.319) 23.6 (5.782) 60.4 (3.495) 38.9 (6.001) 14.6 (6.159) 38.9 (5.072) 19.4 (5.556) 2.1 (1.389) 36.7 (2.053) 1.3545 (0.117) 16.86 QwQ-32b 82.6 (5.258) 59.7 (9.213) 23.6 (3.586) 64.6 (6.159) 30.6 (5.072) 2.8 (0.000) 32.6 (4.167) 11.1 (3.928) 0.7 (1.389) 34.3 (1.728) 1.7024 (0.208) 13.84 Qwen-3-235b 87.5 (4.811) 52.8 (2.268) 22.9 (6.564) 64.6 (4.167) 34.0 (3.495) 11.1 (2.268) 29.9 (4.167) 13.2 (4.167) 0.0 (0.000) 35.1 (1.716) 2.2896 (0.229) 12.49 Gemini-2.5-flash 85.4 (5.727) 58.3 (2.268) 14.6 (4.167) 70.8 (9.213) 41.0 (5.258) 7.6 (4.167) 40.3 (4.811) 18.1 (3.586) 2.1 (1.389) 37.6 (1.581) 1.1950 (0.145) 18.78 Gemini-2.5-pro 95.8 (1.604) 88.9 (2.268) 61.1 (5.072) 88.2 (2.660) 68.1 (5.782) 39.6 (6.944) 63.2 (6.944) 36.8 (2.660) 16.0 (4.167) 62.0 (0.388) 0.5693 (0.026) 22.32 Symbolic Accuracy Following standard practice in scientific law discovery, Symbolic Accuracy is binary metric that verifies if discovered equation ˆf is mathematically equivalent to the groundtruth law ftarget. The equivalence check intentionally disregards the values of physical constants, as they are difficult to determine precisely from empirical data. Accuracysym = I(equiv( ˆf , ftarget)) (1) Here, I() is the indicator function and equiv(, ) checks for symbolic equivalence. However, traditional rule-based equivalence checks can be brittle when handling the algebraic complexity of expressions. We therefore introduce an LLM-as-a-judge protocol for this verification step, which achieves 98.3% agreement with human experts, demonstrating its reliability (see Appendix A.4.1). Data Fidelity via RMSLE To measure the data fidelity of submitted law, we adopt the RMSLE metric. Unlike the metric of Normalized Mean Squared Error (NMSE) used in some prior studies (Shojaee et al., 2025b; Lin et al., 2025), RMSLE offers superior numerical stability and is better suited for physical quantities that may span multiple orders of magnitude due to its logarithmic scale. Given dataset of points with ground-truth values yi and corresponding predictions ˆyi from the submitted law, RMSLE is calculated as (implementation details in Appendix A.4.2): RMSLE = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (log(ˆyi + 1) log(yi + 1))2 (2)"
        },
        {
            "title": "4 EXPERIMENT AND ANALYSIS",
            "content": "In this section, we present our main experimental results (4.1) and conduct further analysis regarding noise robustness (4.2), cross-domain performance (4.3), inference scaling (4.4), and impact of code assistance(4.5). The complete experimental results are provided in Appendix B."
        },
        {
            "title": "4.1 CAN LLM AGENTS PERFORM GENERALIZABLE SCIENTIFIC LAW DISCOVERY?",
            "content": "The main objective of NEWTONBENCH is to authentically assess the generalizable scientific law discovery abilities of LLMs, framed by the intuitive question: (To what extent) Can LLMs Rediscover Newtons Laws? To answer this question, we evaluate 11 state-of-the-art LLMs (details in Appendix A.1), including open-source models such as DeepSeek-R1 and closed-source models such as GPT-5 and Gemini-2.5-pro. We present the general benchmark performance in Table 2, and the full results of each physics domains are reported in Appendix B.1. We analyze model performance from multiple perspectives and summarize our main findings as follows: NEWTONBENCH is reasoning-intensive, failing non-thinking LLMs. Our benchmark proves highly challenging for models with weaker reasoning abilities. All three non-thinking LLMs (GPT4.1-mini, GPT-4.1, and DeepSeek-V3) achieve overall symbolic accuracies below 10%. While these models show limited proficiency (20-50% accuracy) even in the simplest setting, their performance degrades precipitously in all other configurations, demonstrating that success in NEWTONBENCH is contingent on strong, generalizable reasoning. Performance of Reasoning Models Diverges with Increasing Complexity. Most reasoning models achieve overall accuracies between 30-70%, with GPT-5 and Gemini-2.5-pro distinguishing themselves at 72.9% and 65.0%, respectively. While these models all demonstrate proficiency in the easiest settings (80-100% accuracy), substantial performance gap emerges as the difficulty escalates. In the most challenging setting, GPT-5 and Gemini-2.5-pro maintain accuracies of 29.9% and 13.9%, whereas all other reasoning models fall below 5%. This stark performance gap demonstrates that the benchmarks difficulty controls are highly effective at stress-testing and stratifying advanced reasoning abilities. Code Assistance Exhibits Dichotomous Impact. The provision of code interpreter has surprisingly dichotomous effect on agent performance. For less capable models (SA < 40%), access to the code tool substantially improves symbolic accuracy. Conversely, for more capable models (SA 40%), the inclusion of the tool leads to slight degradation in performancea consequence of these models satisficing through over-exploitation, as we detail in 4.5. So, to what extent can LLMs rediscover Newtons Laws? Our findings indicate clear but fragile capability. While current frontier reasoning models can deduce scientific laws in simple, well-isolated systems, their performance degrades precipitously as the complexity of the system or target equation increases. This exposes fundamental limitations in their generalizable reasoning and highlights that robust generalization remains the core challenge. Figure 2: Impact of noise levels on performance. Figure 3: Result across physics domains."
        },
        {
            "title": "4.2 SENSITIVITY TO OBSERVATIONAL NOISE",
            "content": "To investigate robustness against noisy observations, we conducted an experiment on GPT-5-mini with four levels of Gaussian noise (0.0001, 0.001, 0.01, and 0.1). As illustrated in Figure 2, performance degrades significantly even with minimal noise, with the introduction of just 0.0001 noise level causing 13-15% reduction in accuracy compared to the ideal, noise-free setting. As the noise level was increased from 0.0001 to 0.1, symbolic accuracy declined proportionally, while data fidelity (RMSLE) remained relatively stable. Notably, code assistance did not affect noise robustness, with the baseline and code-assisted agents degrading similarly. This demonstrates that the symbolic accuracy of LLMs is extremely fragile to noise in observational data."
        },
        {
            "title": "4.3 CROSS-DOMAIN PERFORMANCE DISPARITIES",
            "content": "Performance across the physics domains, illustrated in Figure 3, reveals substantial difference in average accuracy, with scores ranging from 18% to 54%. Bose-Einstein Distribution, the most advanced and obscure domain in our benchmark, yields the lowest average accuracy at 18.1%. Furthermore, the performance disparities are exacerbated as system complexity increases. For instance, in the simple setting, Fouriers Law yields 58.1% accuracy, comparable to the easiest domain, Acoustic Velocity (60.4%). In the complex setting, however, accuracy for Fouriers Law plummets to 2.3%, while Acoustic Velocity remains at 45.0%. This demonstrates that the intrinsic nature of physical domain is primary factor governing the difficulty of model discovery. Full domain analysis is provided in Appendix E.3. 4."
        },
        {
            "title": "INFERENCE SCALING WITH TASK COMPLEXITY",
            "content": "Can LLMs effectively scale inference for Figure 4 commore challenging tasks? pares strong reasoning LLMs (Gemini-2.5pro/flash, GPT-5/5-mini) with non-reasoning LLMs (GPT-4.1/4.1-mini, DeepSeek-V3) on their token cost and number of rounds required to solve tasks of varying difficulty. For strong reasoning LLMs, token consumption (i.e., reasoning length) increases significantly as task difficulty rises. In contrast, the token cost for non-reasoning models remains consistently low. This demonstrates that reasoning models can substantially scale up their computational effort to solve more complex tasks, whereas non-reasoning models fail to do so, even when consuming more experiment rounds. This superior scaling likely drives the advantage of strong models on complex tasks. Full results are provided in Appendix B.2. Figure 4: Inference cost among difficulty levels."
        },
        {
            "title": "4.5 THE PARADOX OF CODE ASSISTANCE: THE EXPLORATION-EXPLOITATION TRADE-OFF",
            "content": "To understand the dichotomous impact of code assistance, we conducted an experiment on four representative LLMs with varying code-use budgets. As illustrated in Figure 5(a), the performance divergence is most pronounced when the code budget increases from zero to one: the performance of stronger models (Gemini-2.5-flash, GPT-5-mini) degrades, while that of weaker models improves. We hypothesize this divergence stems from fundamental shift in the models problem-solving strategies, specifically the balance between exploration and exploitation3. To investigate this hypothesis, we adapt common analysis approach for reasoning LLMs (Wang et al., 2025b;a) by identifying signature tokens associated with exploration (e.g., What if, Alternatively) and exploitation (e.g., Confirm, Refine). From these, we calculate the exploration rate (the percentage of exploration tokens among all such planning tokens; see Appendix E.4 for details). 3The trade-off between discovering novel solutions (exploration) and optimizing the current best-known solution (exploitation)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 5: Results under different code-use budgets. Figure 6: Functionality distribution for code usage (unlimited budget). Figure 5(b) shows the results for two reasoning LLMs where reasoning traces are accessible. We observe sharp drop in the exploration rate for Gemini-2.5-flash as the code budget increases from zero to one, while the rate remains stable for Qwen-3-235b. This suggests the performance degradation in stronger models reflects over-exploitation4. To understand the mechanism driving this strategic shift, we analyzed the functional distribution of code usage for strong and weak model (GPT-5-mini and GPT-4.1). Figure 6 reveals that GPT5-mini allocates significantly smaller proportion of its code use to basic calculation compared to GPT-4.1, instead favoring function-fitting. This supports the hypothesis that code serves distinct roles: computational tool for weaker models versus an equation refinement tool for stronger ones. In summary, for weaker LLMs, whose primary bottleneck is arithmetic computation, code execution provides crucial assistance, thereby improving their performance. Conversely, stronger LLMs already possess sufficient computational prowess and tend to leverage code for tasks like functionfitting. This can accelerate convergence to good enough solution, causing the model to prematurely settle in local optimum. Such over-exploitation stifles the broader exploration needed to discover globally optimal answer, paradoxically degrading the performance of these more capable models (further case studies in Appendix D). This finding highlights the importance of managing the exploration-exploitation trade-off in agentic systems, particularly in how tools are leveraged by models of varying capability for data-driven discovery."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Symbolic Regression Symbolic regression (SR) aims to discover mathematical formulas from data, with foundational approaches being Genetic Programming (GP) that evolve populations of candidate expressions (Koza, 1994; Augusto & Barbosa, 2000; Billard & Diday, 2002; Schmidt & Lipson, 2009). Meanwhile, more advanced approaches have been proposed, from physics-inspired, Pareto-driven strategies (Udrescu & Tegmark, 2020; Udrescu et al., 2020) to deep learning-based models (Petersen et al., 2021; Kamienny et al., 2022). The latest evolution in SR leverages the reasoning capabilities of LLMs to hypothesize and refine scientific equations (Shojaee et al., 2025a;b). LLM-Driven Scientific Discovery The agentic capabilities of LLMs are increasingly being applied to automate the scientific process (Zheng et al., 2025; Wei et al., 2025), from assisting with experimental procedures in chemistry and biomedicine (Gottweis et al., 2025; Yang et al., 2025; Luo et al., 2025; Chen et al., 2025) to automating complex research workflows in machine learning 4These observations are correlational and should be interpreted as suggestive rather than causal."
        },
        {
            "title": "NewtonBench",
            "content": "(Chan et al., 2025; Huang et al., 2024a; Jiang et al., 2025; Jansen et al., 2025). This trend culminates in the pursuit of fully autonomous AI Scientist capable of managing the entire research pipeline for open-ended discovery (Lu et al., 2024; Yamada et al., 2025). Virtual Environment for LLM Agents Virtual environments for LLM agents have evolved from general-purpose sandboxes for tasks like instruction following (Shridhar et al., 2021), web navigation (Yao et al., 2023), and compositional planning (Prasad et al., 2024), to specialized platforms for scientific reasoning, ranging from curriculum-based tasks (Wang et al., 2022) to open-ended discovery (Jansen et al., 2024)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present NEWTONBENCH, the first scientific law discovery benchmark designed to resolve the long-standing trade-off between scientific relevance, scalability, and resistance to memorization. Moreover, it elevates the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe complex systems to uncover hidden governing principles. Our benchmarking reveals that while frontier models possess clear capability for discovery, this ability is fragile, degrading precipitously with increasing system complexity and observational noise. We further uncover paradoxical effect where tool assistance hinders more capable models, inducing premature shift from exploration to exploitation that causes them to satisfice on suboptimal solutions. Looking forward, we hope NEWTONBENCH serves as crucial litmus test for the reasoning capabilities of frontier LLMs and agentic systems, faithfully measuring genuine scientific intelligence to guide the development of AI capable of authentic discovery."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "NEWTONBENCH is designed exclusively for benchmarking and evaluating the scientific discovery capabilities of LLM agents in fully contamination-free, virtual environment. The metaphysical shifts applied to physical laws ensure the tasks are novel and not directly suitable for training LLMs to discover real-world physics. Due to the interactive, model-system nature of the tasks, we exclude traditional symbolic regression methods that do not support model systems and LLM-based symbolic regression pipelines whose prior reliance and workflows are incompatible with our protocol; these are outside the scope of this evaluation. We caution that the benchmark is not intended as training dataset for developing stronger physics discovery models, but rather as controlled tool for fair and rigorous evaluation. All shifted laws were curated by expert co-authors (Physics Olympiad backgrounds) and cross-validated."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All LLM evaluations in this work were conducted via public APIs, specifically using the OpenRouter and OpenAI-API platforms, with the total experimental cost estimated at 10,000 USD. Detailed model configurations are provided in Appendix A. Experimental results are reported with measures of statistical significance to ensure robustness. We additionally provide all prompt templates and system implementations in both Appendix and Supplementary Materials to facilitate full reproducibility of our experiments. All code and data will be publicly released to foster future research."
        },
        {
            "title": "REFERENCES",
            "content": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges, 2024. URL https://arxiv.org/ abs/2402.00157. Hanae Armitage. Researchers create virtual scientists to solve complex biological problems. Stanford Medicine News Center, July 2025. URL https://med.stanford.edu/news/ all-news/2025/07/virtual-scientist.html. Douglas Adriano Augusto and Helio JC Barbosa. Symbolic regression via genetic programming. In Proceedings. Vol. 1. Sixth Brazilian symposium on neural networks, pp. 173178. IEEE, 2000. Lynne Billard and Edwin Diday. Symbolic regression analysis. In Classification, clustering, and data analysis: recent advances and applications, pp. 281288. Springer, 2002. Nancy Cartwright. How the Laws of Physics Lie. Oxford University Press, 06 1983. ISBN 9780198247043. doi: 10.1093/0198247044.001.0001. URL https://doi.org/10.1093/ 0198247044.001.0001. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Madry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2025. URL https://arxiv.org/abs/2410.07095. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel AduAmpratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery, 2025. URL https://arxiv.org/abs/2410.05080. Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression.jl, 2023. URL https://arxiv.org/abs/2305.01582. Brian M. de Silva, David M. Higdon, Steven L. Brunton, and J. Nathan Kutz. Discovery of physics from data: Universal laws and discrepancies. Frontiers in Artificial Intelligence, 3:25, 2020. doi: 10.3389/frai.2020.00025. URL https://www.frontiersin.org/articles/10. 3389/frai.2020.00025/full."
        },
        {
            "title": "NewtonBench",
            "content": "DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412. 19437. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Miko Elwenspoek. Counterfactual Thinking in Physics, pp. 6280. De Gruyter, Berlin, Boston, ISBN 9783110268669. doi: doi:10.1515/9783110268669.62. URL https://doi. 2012. org/10.1515/9783110268669.62. You-Le Fang, Dong-Shan Jian, Xiang Li, and Yan-Qing Ma. Ai-newton: concept-driven physical law discovery system without prior physical knowledge, 2025. URL https://arxiv.org/ abs/2504.01538. Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago Costa, Jose Penades, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. Towards an ai co-scientist, 2025. URL https://arxiv.org/abs/2502.18864. Kaiming He. Deep residual learning and how it shaped the ai landscape. Future Science Prize Laureate Lecture, The Chinese University of Hong Kong, 2023. Invited talk. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation, 2024a. URL https://arxiv.org/abs/2310. 03302. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, 2024b. URL https://arxiv.org/abs/2402.02716. B. Iglewicz and D.C. Hoaglin. How to Detect and Handle Outliers. ASQ Quality Press, ISBN 9780873892605. URL https://books.google.com.hk/books?id= 1993. FuuiEAAAQBAJ. Peter Jansen, Marc-Alexandre Cˆote, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents, 2024. URL https://arxiv.org/abs/2406.06769. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. Codescientist: End-toend semi-automated scientific discovery with code-based experimentation, 2025. URL https: //arxiv.org/abs/2503.22708. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code, 2025. URL https://arxiv. org/abs/2502.13138. Pierre-Alexandre Kamienny, Stephane dAscoli, Guillaume Lample, and Francois Charton. End-toend symbolic regression with transformers, 2022. URL https://arxiv.org/abs/2204. 10532. John R. Koza. Genetic programming as means for programming computers by natural selection. Statistics and Computing, 4:87112, June 1994. doi: 10.1007/BF00175355. URL https: //link.springer.com/article/10.1007/BF00175355. Marc Lange. Could the laws of nature change?*. Philosophy of Science, 75(1):6992, 2008. doi: 10.1086/587823."
        },
        {
            "title": "NewtonBench",
            "content": "Pat Langley, Herbert A. Simon, Gary L. Bradshaw, and Jan M. Zytkow. Scientific discovery: computational explorations of the creative process. MIT Press, Cambridge, MA, USA, 1987. ISBN 0262620529. David K. Lewis. On the Plurality of Worlds. Wiley-Blackwell, Malden, Mass., 1986. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Xiao Liang, Zhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: survey of reasoning large language models, 2025. URL https://arxiv.org/abs/2502.17419. Haowei Lin, Xiangyu Wang, Jianzhu Ma, and Yitao Liang. Evosld: Automated neural scaling law discovery with large language models, 2025. URL https://arxiv.org/abs/2507. 21184. Jiaqi Liu, Songning Lai, Pengze Li, Di Yu, Wenjie Zhou, Yiyang Zhou, Peng Xia, Zijun Wang, Xi Chen, Shixiang Tang, Lei Bai, Wanli Ouyang, Mingyu Ding, Huaxiu Yao, and Aoran Wang. Mimicking the physicists eye:a vlm-centric approach for physics formula discovery, 2025. URL https://arxiv.org/abs/2508.17380. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/ abs/2408.06292. Yi Luo, Linghang Shi, Yihao Li, Aobo Zhuang, Yeyun Gong, Ling Liu, and Chen Lin. From intention to implementation: automating biomedical research via llms. Science China Information Sciences, 68(7), June 2025. ISSN 1869-1919. doi: 10.1007/s11432-024-4485-0. URL http: //dx.doi.org/10.1007/s11432-024-4485-0. Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, and Yoshitaka Ushiku. Rethinking symbolic regression datasets and benchmarks for scientific discovery, 2024. URL https://arxiv. org/abs/2206.10540. OpenAI. Introducing openai o1 preview, 2024. URL https://openai.com/index/ introducing-openai-o1-preview/. OpenAI. 2025a. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 04 OpenAI. Gpt-5 is here. https://openai.com/gpt-5/, 08 2025b. OpenAI. Introducing openai o3 and o4-mini. introducing-o3-and-o4-mini/, 04 2025c. https://openai.com/index/ Charles Sanders Peirce. Collected Papers of Charles Sanders Peirce, volume 1-6. Harvard University Press, Cambridge, MA, 1958. Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Claudio P. Santiago, Soo K. Kim, and Joanne T. Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients, 2021. URL https://arxiv.org/abs/1912.04871. K.R. Popper. ISBN 9780415278447. Yq6xeupNStMC. The Logic of Scientific Discovery. ISSR Library. Routledge, 2002. URL https://books.google.com.tw/books?id= Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models, 2024. URL https://arxiv.org/abs/2311.05772. Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025a. URL https: //qwenlm.github.io/blog/qwq-32b/. Qwen. Qwen3 technical report, 2025b. URL https://arxiv.org/abs/2505.09388."
        },
        {
            "title": "NewtonBench",
            "content": "Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science, 324(5923):8185, 2009. doi: 10.1126/science.1165893. URL https://www.science. org/doi/abs/10.1126/science.1165893. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan Reddy. Llm-sr: Scientific equation discovery via programming with large language models, 2025a. URL https://arxiv.org/abs/2404.18400. Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa Doan, and Chandan Reddy. Llm-srbench: new benchmark for scientific equation discovery with large language models, 2025b. URL https://arxiv.org/abs/2504.10415. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning, 2021. URL https://arxiv.org/abs/2010.03768. Silviu-Marian Udrescu and Max Tegmark. Ai feynman: physics-inspired method for symbolic regression, 2020. URL https://arxiv.org/abs/1905.11481. Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity, 2020. URL https://arxiv.org/abs/2006.10782. Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hierarchical reasoning in llms through reinforcement learning, 2025a. URL https://arxiv.org/ abs/2509.03646. Ruoyao Wang, Peter Jansen, Marc-Alexandre Cˆote, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader?, 2022. URL https://arxiv.org/abs/2203. 07540. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025b. URL https://arxiv.org/abs/ 2506.01939. Weiqi Wang and Yangqiu Song. MARS: Benchmarking the metaphysical reasoning abilities of language models with multi-task evaluation dataset. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, July 2025. URL https://aclanthology.org/2025.acl-long. 79/. Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Xuming He, Qiang Zhang, Chenyu You, Shuangjia Zheng, Ning Ding, Wanli Ouyang, Nanqing Dong, Yu Cheng, Siqi Sun, Lei Bai, and Bowen Zhou. From ai for science to agentic science: survey on autonomous scientific discovery, 2025. URL https://arxiv.org/abs/2508.14111. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025. URL https://arxiv.org/abs/2504.08066. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses, 2025. URL https://arxiv.org/abs/2410.07076. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable realworld web interaction with grounded language agents, 2023. URL https://arxiv.org/ abs/2207.01206. Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural language reasoning, survey. ISSN 0360-0300. doi: 10.1145/3664194. URL ACM Comput. Surv., 56(12), October 2024. https://doi.org/10.1145/3664194."
        },
        {
            "title": "NewtonBench",
            "content": "Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, and Yangqiu Song. From automation to autonomy: survey on large language models in scientific discovery, 2025. URL https://arxiv.org/abs/2505.13259."
        },
        {
            "title": "APPENDIX",
            "content": "A IMPLEMENTATION DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.1 LLM DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 MODEL SYSTEMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.3 EQUATION DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.4 EVALUATION DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 FULL RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 RESULTS FOR INDIVIDUAL DOMAINS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.2 RESULTS OF NON-PERFORMANCE METRICS . . . . . . . . . . . . . . . . . . . . . . . . . 38 PROMPT TEMPLATES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 C.1 GENERAL PROMPTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 C.2 DOMAIN-SPECIFIC PROMPTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 CASE STUDIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 CASE STUDY FOR NON-REASONING LLMS . . . . . . . . . . . . . . . . . . . . . . . . . . 47 D.2 CASE STUDY FOR REASONING LLMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 MISCELLANEOUS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 E.1 MEMORIZATION IN LSR-TRANSFORM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 E.2 SOLVABILITY PROOF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 E.3 PHYSICS DOMAIN ANALYSIS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 EXPLORATION AND EXPLOITATION TOKENS . . . . . . . . . . . . . . . . . . . . . . . . . 59 E.5 USE OF LARGE LANGUAGE MODELS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 LLM DETAILS In our experiments, we evaluated 11 state-of-the-art LLMs, including both open-source and proprietary models. The models are summarized as follows: GPT-4.1-mini (OpenAI, 2025a) is lightweight proprietary non-reasoning LLM from OpenAI, designed for efficient inference. GPT-4.1 (OpenAI, 2025a) is the latest flagship proprietary non-reasoning LLM from OpenAI. o4-mini (OpenAI, 2025c) is proprietary reasoning LLM developed by OpenAI, optimized for lightweight inference and efficient deployment. GPT-5-mini (OpenAI, 2025b) is lightweight reasoning LLM from OpenAI, aiming for improved efficiency on downstream tasks. GPT-5 (OpenAI, 2025b) is the strongest reasoning LLM by OpenAI with advanced reasoning capabilities. DeepSeek-V3 (DeepSeek-AI, 2024) is an open-source non-reasoning LLM released by DeepSeek, built for robust general-purpose language understanding. DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source reasoning LLM from DeepSeek, trained with reinforcement learning to enhance reasoning and alignment. QwQ-32b (Qwen, 2025a) is an open-source reasoning LLM by Qwen team, leveraging reinforcement learning and agent-driven tool use for complex problem solving. Qwen-3-235b (Qwen, 2025b) is an open-source reasoning LLM by Qwen team, pre-trained on large corpora for high-performance language and reasoning tasks. Gemini-2.5-flash (Google, 2025) is proprietary reasoning LLM from Googles Gemini series, optimized for ultra-long context windows and rapid inference. Gemini-2.5-pro (Google, 2025) is the advanced proprietary reasoning LLM in the Gemini series, offering enhanced multimodal understanding and superior reasoning abilities. By reasoning and non-reasoning, we distinguish whether deliberate thinking processoften incentivized by reinforcement learning with verifiable rewards (RLVR)is triggered before the generation of the first answer token (Li et al., 2025). In all experiments, we set the LLM temperature to 0.4. Statistical significance is assessed based on four independent runs for each experiment. The main results table reports the standard deviation, while other plots display error bars representing the 95% confidence interval for the mean. In each test case, LLM agents are allowed up to 10 experimental rounds, with at most 20 input-parameter sets per round. This design prevents unbounded interaction and helps isolate strategy design."
        },
        {
            "title": "NewtonBench",
            "content": "A.2 MODEL SYSTEMS All model systems in NEWTONBENCH are human-curated and inspired by classical experiments from each domain. As an example, we illustrate Simple System in the Acoustics domain. Detailed descriptions of the model systems across all 12 domains are provided in the Supplementary Materials. SIMPLE SYSTEM (LAW OF SOUND SPEED IN IDEAL GAS) The model simulates the emission of sound pulse within gas chamber, directed toward wall positioned at known distance, and records the time required for the echo to return. The agents task is to deduce the target law, ftarget, by interacting with the simulation. System-Level Input Variables (VM) The simulation accepts the following inputs: γ: The adiabatic index of the gas in the chamber : The molar mass of the gas : The temperature of the gas d: The distance to the wall System-Level Final Output (YM) After the simulation runs, it returns the following output: t: The total time taken for the echo to return Ordered Sequence of Equations (M) The simulation environment internally computes the final measured time by executing fixed sequence of calculations. This sequence of operations, = (f1, f2), is hidden from the agent. However, the fundamental principle underlying the assisting equation (f2) are provided to the agent to guide its discovery. 1. f1 (target): 2. f2 (assist): vsound = ftarget(γ, T, ) = 2d vsound"
        },
        {
            "title": "NewtonBench",
            "content": "A.3 EQUATION DETAILS A.3.1 EXPRESSION TREES The output of any given expression tree (AST) for specific set of input variables is computed using the canonical recursive algorithm detailed in this section. The evaluation strategy follows postorder traversal: the value of any internal node (an operator) is computed only after the values of all its children have been determined. This ensures that operators are always applied to fully evaluated operands. The algorithm requires the trees root node and map containing the numerical values for all variables present in the expression. Algorithm 1 provides formal, high-level description of this process. return n.value return V[n.name] Algorithm 1 Recursive Evaluation of an Expression Tree Require: Node n, variable mapping : var (cid:55) Ensure: Numerical result of the expression rooted at 1: function EVALUATE(n, V) 2: if n.type = CONSTANT then 3: 4: else if n.type = VARIABLE then 5: 6: else if n.type = OPERATOR then 7: 8: 9: 10: 11: 12: 13: 14: end if 15: end function values {Initialize empty list} for each n.children do EVALUATE(c, V) values.APPEND(v) end for op n.operator return APPLY(op, values) As formalized in Algorithm 1, the evaluation logic operates through two fundamental cases: Base cases: Recursion terminates at leaf nodes. For CONSTANT nodes, the intrinsic numerical value is returned directly. For VARIABLE nodes, the corresponding value is retrieved from the variable mapping V. Recursive case: For OPERATOR nodes, the algorithm first recursively evaluates all child nodes in post-order fashion, collecting their results. The operator is then applied to these operands via the APPLY function, returning the computed result. The time complexity is O(n) where is the number of nodes in the AST, as each node is visited exactly once during the traversal."
        },
        {
            "title": "NewtonBench",
            "content": "A.3.2 OPERATOR SETS The complete operator space of NEWTONBENCH is presented in Table 3, which specifies the symbol and arity for each supported mathematical operator. Table 3: Mathematical operators used across all physics modules"
        },
        {
            "title": "Arctangent",
            "content": "+ xy exp(x) log(x) sin(x) cos(x) tan(x) arcsin(x) arccos(x) arctan(x)"
        },
        {
            "title": "Unary",
            "content": ""
        },
        {
            "title": "NewtonBench",
            "content": "A.3.3 PHYSICAL LAWS Table 4 summarizes the physical laws and their corresponding metaphysical shifts. These metaphysical shifts are achieved through mutation operations applied to the original equations, curated by human experts. The curation process follows four core guidelines: 1. Each mutation must be performed within the set, and no new variables should be introduced. 2. Mutations should increase, rather than decrease, the overall complexity of the equation. 3. The new mutation operation must not simply reverse the previous mutation. 4. The outcome of consecutive mutations should not be attainable through single mutation operation. Physical plausibility caveat. All mutated laws are dimensionally coherent by construction, but some may be physically implausible in our universe; we intend these counterfactual variants as scientifically grounded stress tests of reasoning and interactive discovery, not as claims about realworld physics. Physics Law Domain Original Equation Shifted Equations Table 4: Full physical laws and their metaphysical shifts. Newtons Law of Universal Gravitation Mechanics (Gravitation) = m1m2 Coulombs Law Electricity and Magnetism (Electrostatics) = q1 q2 r2 Amp`eres Force Law Electricity and Magnetism (Magneto-statics) = I1I Fouriers Law Thermodynamics (Thermal Conduction) = AT Snells Law Optics (Geometrical Optics) θ2 = sin1 (cid:16) n1 sin(θ1 ) n2 Law of Radioactive Decay Modern Physics (Nuclear Physics) (t) = N0eλt Law of Damped Harmonic Motion Wave and Accoustics (Oscillations) (cid:113) ω = (cid:0) 2m (cid:1)2 Maluss Law Optics (Physical Optics) = I0 cos2(θ) Law of Sound Speed in Ideal Gas Wave and Acoustics (Acoustics) = (cid:113) γRT Hookes Law Mechanics (Elasticity) = kx Bose-Einstein Distribution Modern Physics (Statistical Mechanics) = 1 (cid:16) Cω (cid:17) 1 Easy = m1m2 r1.5 = m1 r2 1m2 = m2 2 r2 = q1q2 r3 = (q1q2)3 r2 = q3 1 q2 r2 = I1 I2 r3 = (I1I2 )2 = I2 = AT d2 = AT d3 = A(T ) (cid:17) θ2 = cos1 (cid:16) n1 sin(θ1 ) θ2 = sin1 (cid:16) n2 sin(θ1 ) θ2 = tan1 (cid:16) n1 sin(θ1) n1 n2 n2 (cid:17) (cid:17) (cid:17) N(t) = N0eλ (t0.5) (t) = N0eλ (te) 0 eλ (t) = 1.5 (cid:113) k (cid:16) 2m (cid:16) 2m ω = ω = (cid:18) m (cid:17)2 2m (cid:17)2(cid:19)2 ω = = I0 (sin(θ) + cos(θ))2 = I = I0 (cid:16) sin(θ) cos(θ) (cid:16) cos(θ) sin(θ) (cid:17)2 (cid:17)2 = (cid:113) γR 2 = γR (cid:113) = M Medium = (m1m2 ) r1.5 = m1 r2.6 1m2 = m2 2 r2 r2 = q1q2(q1+q2) = (q1+q2)3 r2 1 q2 = q3 2 r2.5 = (I1 I2)1. r3 = K(I1I2)2r = I2 r3.8 = A+T d2 d3 = A+T = (A+T )2 θ2 = cos1 (cid:16) n1 cos(θ1) θ2 = cos1 (cid:16) n2 sin(θ1) θ2 = tan1 (cid:16) n1 tan(θ1 ) n1 n2 n2 (cid:17) (cid:17) (cid:17) (t) = N0e2λ+(t0.5 ) (t) = N0eλ+(t0.5 ) 0 eλ+(t0.5) (t) = 1.5 (cid:113) b 2m2 (cid:16) 2m (cid:16) 2m ω = k m2 ω = (cid:18) m1.3 = I0 (2 sin(θ) + cos(θ))2 (cid:17)2(cid:19)2 ω = (cid:17)2 = I0 = sin2(θ) cos3(θ) (cid:16) cos(θ) sin(θ) (cid:17)e (cid:113) γR 2 1.5 (cid:16) (cid:17) 1 3 = = γT = RT 1.5 = 2kx2 = 2k2x = 2k3x2 = 2(k + x2) = 2(k2 + x3) = 2(k3 + x2) = = = 1 (cid:16) Cω (cid:17) (cid:18) 1 Cω0.5 (cid:18) 1 ω 3 (cid:19) +1 (cid:19) 1 1 = (cid:18) 1 ω1.5 (cid:19) (cid:19) (cid:18) = = +1 1 e(C ω0.5T )1 1 Cω1.5 3 = m(T 2.5)ec = m2.5ce(T )2 = c2.5e(T m) Hard = (m1+m2)2 r1.5 = m1.3 1 r2.6 1 + m2 = (m2 2) r2 = q1 q2(q1+q2) re 2 (q1+q2)3 = q2 r2 1 q2 = q3 2 re = (I1+I2)1. r3 = K(I1 I2)2r = I0.9 2 r3.8 d2 = (A+T )A2T = (A+T )2.5 = kA+(T )2 de (cid:18) n2 de θ2 = cos θ2 = cos1 (cid:18) θ2 = tan1 (cid:18)(cid:16) n1 n2 (cid:19) (cid:19) 1 cos(θ1) n2 n2 sin(θ1) n2.5 1 (cid:17)2 (cid:19) tan(θ1) (t) = N0e(2λ+2)+(t0.5) (t) = N0eλ+3+(t0.5) (t) = log(N 1. 0 )eλ+(t0.5 ) (cid:17)1.5 ω = (cid:18) (cid:16) b 2m2 (cid:16) 2m m2 ω = (cid:17)2(cid:19)2 ω = m1.3 (cid:17)0.7 (cid:16) 2m = I0 (2 sin(θ) + 1.5 cos(θ))2 = I0 = I0 (cid:18) (cid:18) sin2(θ) cos3(θ) sin2(θ) cos(θ) (cid:19)e (cid:19)e = (cid:113) eγ 2 1.5 (cid:16) (cid:17) 1 3 = ln(γ) v = (RT 1.5)2.8 = 2 (cid:0)k + sin (cid:0)x2(cid:1)(cid:1) = 2 (cid:0)sin (cid:0)k2(cid:1) + x3(cid:1) = 2 (cid:0)sin (cid:0)k3(cid:1) + x2(cid:1) = = = (cid:18) (cid:19) +1 1 ω1.5 2 1 e(C ω0.5T 2.3)1 1 ω1.5 3 ln (cid:18) (cid:19) = log(m(T 2.5))ec = log(m2.5c)e(T 2) = log(c2.5)e(T +m) Law of Heat Transfer Thermodynamics (Calorimetry) = mc(T ) = mc(T )2.5 = m2.5c(T ) = mc2.5(T )"
        },
        {
            "title": "NewtonBench",
            "content": "A.4 EVALUATION DETAILS A.4.1 SYMBOLIC ACCURACY EVALUATION DETAILS We applied the LLM-as-a-Judge framework to verify the symbolic equivalence between the submitted law and the ground-truth law, utilizing prompt engineering and few-shot demonstrations (details provided below). We evaluated the effectiveness of LLM-as-a-Judge across four LLMs using human-labeled dataset of 120 pairs of equations (results shown in Table 5). Based on performance and open-source accessibility, we ultimately selected Nemotron-ultra-253b for our experiments. From our experience, GPT-4.1 also achieves (almost) identical precision and results when serving as the judge model. We will publicly release the judge evaluation data to guarantee reproducibility. Table 5: Agreement (%) between LLM-as-a-Judge and human evaluator."
        },
        {
            "title": "Agreement",
            "content": "Nemotron-nano-8b GPT-oss-20b GPT-4.1 Nemotron-ultra-253b 83.3 90.8 98.3 98.3 LLM-as-a-Judge Prompt You are mathematical judge. Your task is to determine if two equations are equivalent. **Instructions:** 1. Compare the two equations carefully 2. Consider algebraic manipulations, variable reordering, and variable renaming 3. Determine if they represent the same mathematical relationship 4. Provide your reasoning step by step first, and then provide only one answer under the format of **Answer: YES/NO** 5. Try converting both equations into the same algebraic form to make comparison easier. - e.g. rewrite ln(x ** 2) into 2ln(x) **Output format:** Reasoning: (Your reasoning steps) Answer: (YES/NO) **Reminder:** - Equations may be expressed in standard mathematical notation or as Python code. If the Python implementation implies the same mathematical relationship, the equations are considered equivalent. - Constants may differ in form or value. As long as they serve the same functional role (e.g., both scale the output proportionally), they are considered interchangeable. For example, constant expressed as sqrt(k) in one equation and as in another may be equivalent if both affect the output in the same way and can be interchangeable by selecting suitable value for the constant - Variable names may differ, but the index and structure of variables must match exactly for the equations to be considered equivalent. For example, index of 4 and 4.03 are considered different - YES/NO must be on the same line as \"Answer:\" **Examples:** <provided in the next page> **Your Task:** Compare these two equations and determine if they are equivalent: Parameter Descriptions: {param_description} Equation 1: {equation1} Equation 2: {equation2}"
        },
        {
            "title": "NewtonBench",
            "content": "LLM-as-a-Judge Prompt (Few-shot Examples) **Examples:** Equation 1: (HIDDEN_CONSTANT_C * x1 * x2) ** 2 / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): = 6.7e-05 return (C * (x1 * x2) ** 2) / x3 ** 2 Reasoning: Although the constant in equation 1 is HIDDEN_CONSTANT_C**2 and constant in equation 2 is C, both constant serve the same scaling role ...... Answer: YES Equation 1: (C * x1 * x2) / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): = 6.7e-05 return (C * x1) / (x3 ** 4 * x2) Reasoning: The second equation changes the exponent on x3 and alters the position of x2 ...... Answer: NO Equation 1: sqrt(C * x1 * (x2 ** 2)) / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): = 6.7e-05 return sqrt(C * x1) * x2 / x3 ** 2 Reasoning: Since sqrt(x2 ** 2) = x2, both expressions represent the same mathematical relationship ...... Answer: YES Equation 1: (G * x1 * x2) / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): = 6.7e-05 return (C * x1 * x2) / x3 ** 2. Reasoning: The exponent on x3 differs slightly ...... Answer: NO Equation 1: (C * x1 * x2) / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): = 6.7e-05 product = x1 * x2 return (G * product) / x3 ** 2 Reasoning: Variable naming differs but structure and operations are equivalent. serves the same role as ...... Answer: YES Equation 1: * ln(x ** 2) Equation 2: def discovered_law(x1, x2, x3): = 2.02 return * ln(x) Reasoning: * ln(x**2) is the same as 2C * ln(x) and the constant (2C) servers the same role as ...... Answer: YES Equation 1: (C * x1 * x2) / x3 ** 2 Equation 2: def discovered_law(x1, x2, x3): return (x1 * x2) / x3 ** 2 Reasoning: Equation 1 has constant variable while Equation 2 has numerical constant of 1 ...... Answer: YES"
        },
        {
            "title": "NewtonBench",
            "content": "A.4.2 DATA FIDELITY EVALUATION DETAILS For data fidelity evaluation, the submitted equations are assessed on separate set of 5,000 data samples. Following Matsubara et al. (2024), we use the sampling distributions in Table 6. To ensure RMSLE is well-defined, equations are curated to yield non-zero results, and inputs are sampled so that all ground-truth outputs are non-negative. We further improve stability by applying outlier filtering using the Modified Z-Score method (Iglewicz & Hoaglin, 1993). Table 6: Sampling Distributions for Each Domain Domain Equation Variables and Descriptions Gravitation = (m1, m2, r) Coulombs Law = (q1, q2, r) Amperes Force Law = (I1, I2, r) Fouriers Law = (k, A, T, d) Snells Law θ2 = (n1, n2, θ1) Radioactive Decay = (N0, λ, t) Harmonic Motion ω = (k, m, b) Malus Law = (I0, θ) Acoustic Velocity = (γ, T, ) Hookes Law = (k, x) Bose-Einstein Distribution = (ω, ) Heat Transfer = (m, c, ) m1: mass of the first object m2: mass of the second object r: distance between the two objects q1: charge of the first particle q2: charge of the second particle r: distance between the particles I1: current in the first wire I2: current in the second wire r: distance between the wires k: thermal conductivity A: cross-sectional area T: temperature difference d: distance across material n1: refractive index of medium 1 n2: refractive index of medium 2 θ1: angle of incidence N0: initial number of atoms λ: decay constant t: time k: spring constant m: mass b: damping coefficient I0: initial intensity of the light θ: angle between the polarization axis γ: adiabatic index of the gas T: temperature M: molar mass k: spring constant x: displacement from the equilibrium ω: angular frequency of the photons T: temperature m: mass c: specific heat capacity : temperature difference Sampling Distributions m1 : Ulog(100, 103) m2 : Ulog(100, 103) : Ulog(1, 101) q1 : Ulog(101, 101) q2 : Ulog(101, 101) : Ulog(101, 101) I1 : Ulog(103, 101) I2 : Ulog(103, 101) : Ulog(103, 101) : Ulog(101, 101) : Ulog(104, 102) : Ulog(101, 103) : Ulog(102, 100) n1 : (1.0, 1.5) n2 : (1.0, 1.5) θ1 : (0, π/2) N0 : Ulog(104, 102) λ: Ulog(103, 101) t: Ulog(102, 101) : Ulog(102, 104) : Ulog(101, 101) : Ulog(102, 100) I0 : Ulog(100, 2000) θ : (106, π/2) γ : (1.3, 1.7) : Ulog(101, 103) : Ulog(103, 101) : Ulog(103, 103) : Ulog(103, 100) ω : Ulog(108, 1010) : Ulog(101, 103) : Ulog(103, 103) : Ulog(102, 104) : Ulog(102, 102)"
        },
        {
            "title": "B FULL RESULTS",
            "content": "B.1 RESULTS FOR INDIVIDUAL DOMAINS In this section, we illustrate the detailed performances of all LLM Agents in each physics domains: Figure 7: Gravitation (Newtons Law of Universal Gravitation). Figure 8: Electrostatics (Coulombs Law). Figure 9: Magnetostatics (Amp`eres Force Law). Figure 10: Thermal Conduction (Fouriers Law). Figure 11: Geometrical Optics (Snells Law). Figure 12: Nuclear Physics (Law of Radioactive Decay). Figure 13: Oscillations (Law of Damped Harmonic Motion). Figure 14: Physical Optics (Maluss Law). Figure 15: Acoustics (Law of Sound Speed in Ideal Gas). Figure 16: Elasticity (Hookes Law). Figure 17: Statistical Mechanics (Bose-Einstein Distribution). Figure 18: Calorimetry (Law of Heat Transfer)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 7: Full LLM Performances in domain Gravitation (Newtons Law of Universal Gravitation)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 8: Full LLM Performances in domain Electrostatics (Coulombs Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 9: Full LLM Performances in domain Magnetostatics (Amp`eres Force Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 10: Full LLM Performances in domain Thermal Conduction (Fouriers Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 11: Full LLM Performances in domain Geometrical Optics (Snells Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 12: Full LLM Performances in domain Nuclear Physics (Law of Radioactive Decay)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 13: Full LLM Performances in domain Oscillations (Law of Damped Harmonic Motion)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 14: Full LLM Performances in domain Physical Optics (Maluss Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 15: Full LLM Performances in domain Acoustics (Law of Sound Speed in Ideal Gas)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 16: Full LLM Performances in domain Elasticity (Hookes Law)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 17: Full LLM Performances in domain Statistical Mechanics (Bose-Einstein Distribution)."
        },
        {
            "title": "NewtonBench",
            "content": "Figure 18: Full LLM Performances in domain Calorimetry (Law of Heat Transfer)."
        },
        {
            "title": "NewtonBench",
            "content": "B.2 RESULTS OF NON-PERFORMANCE METRICS In this section, we present various non-performance metrics, evaluated across all difficulty levels and aggregated over 12 domains for each LLM Agent: Table 7: Average number of rounds Table 8: Average number of experiments (a set of parameters processed) Table 9: Average total tokens Table 10: Average number of tokens per round"
        },
        {
            "title": "NewtonBench",
            "content": "Table 7: Average amount of rounds across all difficulty levels aggregated over all 12 domains. Within each agent settings, the highest numbers are marked as bold, with the second-highest underlined."
        },
        {
            "title": "LLM Agents",
            "content": "easy medium hard easy medium hard easy medium hard"
        },
        {
            "title": "Vanilla Agent",
            "content": "GPT-4.1-mini 6.88 6.97 GPT-4.1 o4-mini GPT-5-mini GPT-5 8.01 8.52 2.83 3.18 2. 2.51 2.40 2.38 DeepSeek-V3 4.62 4. DeepSeek-R1 3.03 3.35 QwQ-32b 3.03 3. Qwen-3-235b 2.92 3.22 Gemini-2.5-flash 4.49 4. Gemini-2.5-pro 3.03 3."
        },
        {
            "title": "Agent with Code Assistance",
            "content": "7.53 8.72 3.80 3.20 2.80 5. 4.08 4.64 3.49 5.81 3.11 6. 6.50 7.44 7.62 2.94 3.40 2. 2.41 2.56 2.65 4.18 4.35 3. 3.47 3.16 3.27 3.00 2.92 4. 5.10 2.95 2.88 6.74 7.45 3. 2.97 3.39 4.51 4.15 3.79 3. 5.76 3.15 6.40 7.09 2.99 2. 3.17 4.24 3.72 2.99 3.53 5. 3.32 6.65 7.26 3.19 2.76 3. 4.49 3.60 2.85 3.53 5.53 3. 6.87 7.57 4.10 3.13 4.02 4. 4.23 3.40 3.76 6.33 3.64 6. 7.74 3.36 2.69 2.96 4.53 3. 3.41 3.29 5.31 3.14 GPT-4.1-mini 8. 8.81 8.95 8.21 8.75 8.96 8. 8.78 8.86 8.67 7.85 10.03 10. 9.44 10.23 10.66 10.16 10.71 10. 10.08 GPT-4.1 o4-mini GPT-5-mini GPT-5 4. 5.20 4.12 5.18 3.60 4.15 DeepSeek-V 7.85 9.20 DeepSeek-R1 5.12 5.46 QwQ-32b 3.95 6.00 Qwen-3-235b 4.60 4.97 Gemini-2.5-flash 7.24 8.60 Gemini-2.5-pro 5.51 5.44 6. 5.99 5.38 8.41 5.92 5.94 5. 9.73 6.46 4.53 5.67 4.51 5. 4.27 5.02 7.76 8.29 5.28 5. 4.24 5.94 4.98 5.28 7.69 8. 5.45 5.77 6.43 6.55 6.45 7. 5.97 5.01 5.35 9.90 6.68 4. 5.31 5.20 8.10 5.69 4.72 5. 8.27 6.03 5.59 5.90 5.76 8. 5.78 4.88 5.49 8.83 6.26 6. 5.97 6.97 7.44 6.20 4.97 5. 9.39 7.08 5.51 5.48 5.20 8. 5.66 5.07 5.23 8.70 6."
        },
        {
            "title": "NewtonBench",
            "content": "Table 8: Average Experiments across all difficulty levels aggregated over all 12 domains. Within each agent settings, the highest numbers are marked as bold, with the second-highest underlined."
        },
        {
            "title": "LLM Agents",
            "content": "easy medium hard easy medium hard easy medium hard"
        },
        {
            "title": "Vanilla Agent",
            "content": "GPT-4.1-mini 44.78 43.60 47.65 30.98 32. 33.96 33.25 34.81 36.26 GPT-4.1 o4-mini 105.86 111.17 113.44 69.00 72.51 73. 63.21 63.23 66.40 17.23 19.35 26. 13.48 17.10 21.95 12.67 14.85 21. GPT-5-mini 19.80 22.74 31.81 14.47 18. 24.83 21.23 21.00 26.80 GPT-5 24. 24.07 30.21 22.98 23.42 35.81 33. 34.87 44.92 DeepSeek-V3 22.05 24.03 24. 14.74 15.88 16.98 14.94 15.87 15. DeepSeek-R1 22.37 24.49 28.32 17.50 18. 22.37 17.49 17.58 18.14 QwQ-32b 13. 13.29 13.12 Qwen-3-235b 9.12 9.83 10. 8.44 6.85 7.22 10.08 6.51 7. 6.93 7.59 7.22 7.34 8.16 8. Gemini-2.5-flash 21.99 25.49 30.79 20.23 23. 27.06 19.98 22.59 26.25 Gemini-2.5-pro 16. 16.48 17.85 13.40 13.28 15.08 15. 14.22 17."
        },
        {
            "title": "Agent with Code Assistance",
            "content": "GPT-4.1-mini 21.11 22.10 24.55 14.36 15. 16.40 15.51 16.13 18.78 GPT-4.1 o4-mini 30.22 38.51 44.89 27.82 28.11 30. 25.47 25.01 26.80 13.74 16.01 22. 10.88 13.83 17.35 10.15 13.39 15. GPT-5-mini 16.11 21.29 27.49 15.22 20. 25.22 17.61 19.74 24.35 GPT-5 20. 21.72 27.24 19.54 22.40 28.45 25. 26.12 33.03 DeepSeek-V3 15.06 18.24 18. 12.71 13.40 12.42 12.07 11.92 11. DeepSeek-R1 12.63 15.47 16.20 11.06 13. 13.30 10.98 11.26 10.35 QwQ-32b Qwen-3-235b 9.59 8.19 10.67 10.99 8.50 8. 6.69 6.43 6.33 6.93 6.80 6. 6.28 5.99 5.90 6.08 5.85 6. Gemini-2.5-flash 15.51 15.49 17.35 12.72 13. 14.67 13.10 13.84 15.69 Gemini-2.5-pro 12. 11.47 12.41 9.42 9.44 10.38 10. 9.02 11.68 37.50 82.03 18.25 22. 30.43 18.31 20.76 9.75 8.21 24. 15.54 18.29 30.81 14.75 20.84 24. 13.90 12.74 7.68 7.16 14.61 10."
        },
        {
            "title": "NewtonBench",
            "content": "Table 9: Average Total Tokens across all difficulty levels aggregated over all 12 domains. Within each agent settings, the highest numbers are marked as bold, with the second-highest underlined. Vanilla Equation Simple System Complex System Average LLM Agents easy medium hard easy medium hard easy medium hard Total Tokens 1803 Vanilla Agent GPT-4.1-mini 1284 1591 1521 1944 2612 1856 1711 2096 GPT-4. o4-mini 2605 2790 2810 2589 2898 2911 3324 3324 4624 19944 7254 12149 17196 9115 18918 12277 GPT-5-mini 3457 7574 5252 10009 14396 8021 10589 9853 GPT-5 7101 8372 19424 16862 31388 17054 22722 37455 DeepSeek-V3 1516 1582 1646 1543 1665 1525 1607 1541 DeepSeek-R1 13538 18337 14647 17972 19666 18260 19231 QwQ-32b 9450 13422 11947 13945 16716 12838 14326 Qwen-3-235b 8104 12615 16282 10729 14313 12603 13517 14616 Gemini-2.5-flash 19327 40681 18696 32575 42013 32682 49863 Gemini-2.5-pro 9238 13721 13961 18215 28272 22203 24319 Agent with Code Assistance GPT-4.1-mini 1919 2312 2465 2989 3371 3635 3579 3523 GPT-4. o4-mini 2989 4798 5836 4259 5181 4774 4914 5419 4423 18410 6214 11193 15168 8453 14227 GPT-5-mini 3702 8748 12927 11643 13660 10213 11399 12514 GPT4821 9168 20185 10678 17893 17431 23295 35488 DeepSeek-V3 2148 2536 2454 2692 2345 2684 2449 DeepSeek-R1 11281 14616 18596 17651 19815 18014 17719 18677 QwQ-32b 7724 13689 17667 10872 13985 13638 14177 15890 Qwen-3-235b 6881 15832 10516 12612 14301 12506 14114 Gemini-2.5-flash 5388 18256 24028 18092 21096 19407 27087 25411 Gemini-2.5-pro 9301 12932 25447 13820 19348 22930 29170 36998 1591 16458 12858 31388 20773 2963 4795 10177 18800 2499 16860 13840 18777"
        },
        {
            "title": "NewtonBench",
            "content": "Table 10: Average Tokens per Round across all difficulty levels aggregated over all 12 domains. Within each agent settings, the highest numbers are marked as bold, with the second-highest underlined. Vanilla Equation Simple System Complex System Average LLM Agents easy medium hard easy medium hard easy medium hard Tokens per Round Vanilla Agent GPT-4.1-mini GPT-4.1 o4-mini 198 335 272 216 329 309 367 415 414 421 319 463 270 366 488 1633 2771 4694 3318 4107 2918 3420 4321 GPT-5-mini 1518 2825 4490 2365 3890 2925 3780 4416 GPT-5 2599 6168 4005 5937 8657 5004 8561 DeepSeek-V3 379 366 356 426 400 412 394 399 DeepSeek-R 3351 4294 4837 4612 5771 4991 5523 5190 QwQ-32b 3711 4999 4756 5575 5510 5754 6151 Qwen-3-235b 3148 4270 5284 5081 5103 4253 4628 4795 Gemini-2.5-flash 1941 3889 6904 3982 5879 6090 6038 7563 Gemini-2.5-pro 3124 7757 4806 6409 9201 7026 9531 Agent with Code Assistance GPT-4.1-mini GPT-4.1 o4-mini GPT-5-mini GPT-5 233 360 983 870 454 272 517 355 454 493 378 490 424 474 472 398 508 1684 2495 1794 2191 1609 1877 2101 2046 1426 1901 2017 1795 2057 1208 1972 3264 2279 4363 3130 3803 4857 DeepSeek-V3 280 303 317 334 325 333 341 DeepSeek-R1 2281 2793 3092 3405 3568 3279 3211 QwQ-32b 2043 2640 3436 2754 3698 3280 3309 3684 Qwen-3-235b 2406 3039 2246 2516 2820 2814 2680 Gemini-2.5-flash 709 1656 1297 2193 2221 2361 3162 Gemini-2.5-pro 1672 2344 3797 2493 4671 3668 4703 5164 309 3271 3411 5627 394 4861 4550 5542 6751 341 469 1730 3127 316 3121 3096 2093"
        },
        {
            "title": "C PROMPT TEMPLATES",
            "content": "C.1 GENERAL PROMPTS"
        },
        {
            "title": "Vanilla Agent System Prompt",
            "content": "You are an AI research assistant tasked with discovering scientific laws in simulated universe. Your goal is to propose experiments, analyze the data they return, and ultimately deduce the underlying scientific law. Please note that the laws of physics in this universe may differ from those in our own. You can perform experiments to gather data but you must follow the protocol strictly. **Workflow:** 1. 2. 3. 4. Analyze the mission description provided. Design set of experiments to test your hypotheses. Use the <run_experiment> tag to submit your experimental inputs. The system will return the results (up to 20 data points per experiment) in an <experiment_output> tag. - If returned value is nan, it indicates that the calculation encountered an error, such as: - ValueError (e.g., using asin on value outside the valid range of [-1, 1]) - OverflowError (e.g., using exp on an extremely large input) - You may ignore any data points that return nan, as they do not contribute to valid hypothesis testing. - Consider adjusting your input parameters to avoid invalid ranges and improve data coverage. 5. 6. 7. 8. 9. You can run up to 10 rounds of experiments. Use them wisely so that before submitting your final law, ensure you have: - fully explored the experimental space - Verified your hypotheses against the data - made the most of the available rounds to strengthen your conclusions Only one action is allowed per round: either <run_experiment> or <final_law>. After submitting <run_experiment>, wait for <experiment_output> before proceeding. You should verify your hypotheses by checking if the output from the experiments matches the output from your hypotheses. When confident, submit your final discovered law using the <final_law> tag. This ends the mission."
        },
        {
            "title": "Agent with Code Assistance System Prompt",
            "content": "You are an AI research assistant tasked with discovering scientific laws in simulated universe. Your goal is to propose experiments, analyze the data they return, and ultimately deduce the underlying scientific law. Please note that the laws of physics in this universe may differ from those in our own. You can perform experiments to gather data but you must follow the protocol strictly. **Rules**: 1. **Math calculation**: - You are always encouraged to use the <python> tag to assist with any non-trivial mathematical reasoning. This includes, but is not limited to: - Performing exponentiation, logarithmic transformations, and other advanced math operations. - Comparing predicted outputs from your proposed law against actual experiment results. - Calculating metrics such as mean squared error to evaluate the accuracy of your hypotheses. - Performing sensitivity analysis and mathematical modeling to understand how variations in experimental conditions affect outcomes. 2. **Enhanced Tool Use**: - Avoid Redundant Calls: Do not call the same tool with identical parameters more than once. - Evaluate Before Repeating: Always review the tools output before deciding to call another tool. Only proceed if the result is incomplete, unclear, or unsatisfactory. - **Turn-Based Strategy**: Use your Python calls strategically within each turn for iterative analysis and refinement."
        },
        {
            "title": "NewtonBench",
            "content": "Agent with Code Assistance System Prompt (Experimentation Protocol) **Workflow:** 1. 2. 3. 4. Analyze the mission description provided. Design set of experiments to test your hypotheses. Use the <run_experiment> tag to submit your experimental inputs. The system will return the results (at most 20 sets of data per experiment) in an <experiment_output> tag. You can run up to 10 rounds. Use them wisely so that before submitting your final law, ensure you have: - fully explored the experimental space - Verified your hypotheses against the data - made the most of the available rounds to strengthen your conclusions Use the <python> tags to test your hypotheses, perform calculations, or explore data between experiments. The system will return the results from the <python> tags in <python_output> tag. 5. 6. 7. 8. **CRITICAL: Only one action per turn**: - <run_experiment> tag for running experiments - <python> tag for calculations/analysis (up to 1 call per turn) - <final_law> tag for submitting your final discovered law 9. **NO MIXING**: Never use multiple action types in the same turn (e.g., Python + Experiment) 10. **NO DUPLICATES**: Never use multiple tags of the same type in one turn 11. After submitting <run_experiment>, wait for <experiment_output> before proceeding. 12. After submitting <python>, wait for <python_output> before proceeding. 13. You should verify your hypotheses by checking if the output from the experiments matches the output from your hypotheses. 14. You should take advantage of <python> tags to do all the tasks you deem necessary within each turn. 15. Analyze the results from <python_output> tags to refine your understanding. 16. When confident, submit your final discovered law using the <final_law> tag. This ends the mission. **Important Notes:** - You are equipped with one tool: <python>. This tool is only for performing complex math calculations (e.g., exponentiation, logarithms, data analysis, hypothesis testing). **NEVER** use the run_python_code tool to run experiments or submit final laws. These actions must be done using the <run_experiment> and <final_law> tags respectively. **NEVER** include any comments inside the submitted final laws Always respond with the appropriate tag when submitting experiments or final laws. The environment will handle execution and feedback. - - -"
        },
        {
            "title": "Condition Without Noise",
            "content": "**How to Run Experiments:** To gather data, you must use the <run_experiment> tag. Provide JSON array specifying the parameters for one or arbitrarily many experimental sets. Note that all measurements returned by the system are **noise-free**. You can assume that the data are perfectly accurate and deterministic."
        },
        {
            "title": "Condition With Noise",
            "content": "**How to Run Experiments:** To gather data, you must use the <run_experiment> tag. Provide JSON array specifying the parameters for one or arbitrarily many experimental sets. All measurements returned by the system are subject to **random noise**, simulating the imperfections of real-world sensors."
        },
        {
            "title": "Submission Requirements",
            "content": "**Final Submission:** Once you are confident you have determined the underlying force law, submit your findings as single Python function enclosed in <final_law> tags. **Submission Requirements:** 1. 2. 3. 4. The function must be named discovered_law The function signature must be exactly: {function_signature} The function should return {return_description}. If you conclude that one of these parameters does not influence the final force, you should simply ignore that variable within your functions logic rather than changing the signature. If your law contains any constants, you must define the constant as local variable inside the function body. Do NOT include the constant as function argument. Import any necessary libraries inside the function body (e.g. math, numpy, etc.) if needed 5. 6. **Critical Boundaries:** - Do NOT include any explanation or commentary inside the <final_law> blocks and the function body. Only output the <final_law> block in your final answer. - {example} **Reminder:** 1. Always remember that the laws of physics in this universe may differ from those in our own, including factor dependency, constant scalars, and the form of the law. When doing the experiments, use broad range of input parameters, for example, values spanning from 10ˆ-3 to 10ˆ15 to ensure robustness across scales. 2. C.2 DOMAIN-SPECIFIC PROMPTS The prompt templates for domain-specific instructions are provided in Supplementary Materials."
        },
        {
            "title": "D CASE STUDIES",
            "content": "In this section, we provide four case studies, two from GPT-4.1 (Vanilla / Code Assisted, in Appendix D.1) and two from Gemini-2.5-flash (Vanilla / Code Assisted, in Appendix D.2). Reasoning steps are partially summarized into objective narration for conciseness. The case studies intuitively demonstrate the effectiveness of code assistance in offloading computation for GPT-4.1, while it undermines active exploration for Gemini-2.5-flash."
        },
        {
            "title": "NewtonBench",
            "content": "D.1 CASE STUDY FOR NON-REASONING LLMS D.1.1 VANILLA NON-REASONING LLMS Vanilla GPT-4.1s Failed Conclusion of Easy Vanilla Equation in domain Gravitation SYSTEM: // The model is tasked with discovering scientific law in simulated universe. // It can only run experiments and has no access to code assistance. ... ENVIRONMENT: // The task is to find the law of gravitation between two masses. The environment // can measure the force between mass1 and mass2 at given distance. ... LLM: // The model runs wide-ranging set of 20 experiments, varying mass1, mass2, and // distance across many orders of magnitude to gather diverse initial dataset. <run_experiment> [ {\"mass1\": 1e-15, \"mass2\": 1e-15, \"distance\": 1e-8}, {\"mass1\": 1e15, \"mass2\": 1e15, \"distance\": 1e8}, // ... and 18 other wide-ranging experiments ... ] </run_experiment> ... ENVIRONMENT: <experiment_output> [\"6.674e-23\", \"6.674e+13\", ...] </experiment_output> ... LLM: // The model attempts to find the relationship with distance by running additional // experiments in which distance is varied exponentially while mass1 and mass2 // are held constant. <run_experiment> [ {\"mass1\": 2, \"mass2\": 2, \"distance\": 4}, {\"mass1\": 2, \"mass2\": 2, \"distance\": 8}, {\"mass1\": 2, \"mass2\": 2, \"distance\": 16}, {\"mass1\": 2, \"mass2\": 2, \"distance\": 32}, // ... and 16 more experiments ... ] </run_experiment> ... ENVIRONMENT: <experiment_output> [\"3.337000000000000e-05\", \"1.179807664409755e-05\", ...] </experiment_output> ... // The model requests for more experiment to collect more data ... LLM: // Lacking the ability to do precise computation, the model cannot identify the true // exponent (1.5) for distance even though it have collected large set of data. // Instead, it defaults to known law from our universe (inverse-square), leading to an // incorrect conclusion. <final_law> def discovered_law(mass1, mass2, distance): return (6.674e-5 * mass1 * mass2) / (distance ** 2) </final_law>"
        },
        {
            "title": "NewtonBench",
            "content": "D.1.2 CODE-ASSISTED NON-REASONING LLMS Code-Assisted GPT-4.1s Successful Deduction of Easy Vanilla Equation in domain Gravitation SYSTEM: // The model is tasked with discovering scientific law in simulated universe. // Other than running experiments, it is given access to Python interpreter. ... ENVIRONMENT: // The task is to find the law of gravitation between two masses. The environment // can measure the force between mass1 and mass2 at given distance. ... LLM: // The model conducts experiments by varying mass1, mass2, and distance across // different orders of magnitude to obtain initial data <run_experiment> [ {\"mass1\": 1.0, \"mass2\": 1.0, \"distance\": 1.0}, {\"mass1\": 1e3, \"mass2\": 1e3, \"distance\": 1e3}, {\"mass1\": 1e6, \"mass2\": 1e6, \"distance\": 1e3}, // ... and 7 other experiments ... ] </run_experiment> ... ENVIRONMENT: <experiment_output> [6.674e-05, 0.00211, 2110.5, ...] </experiment_output> ... LLM: // The model offloads the cognitive load of calculation to the tool. It uses the tool to // tests the standard inverse-square law hypothesis against the data. <python> import numpy as np force = np.array([6.674e-05, 0.00211, ...]) # Test if is proportional to m1*m2/dˆ2 pred_force = 6.674e-5 * (mass1 * mass2) / (distance ** 2) ratios = force / pred_force print(\"Ratios (measured/predicted):\", ratios) </python> ... ENVIRONMENT: // The result of the code is returned, providing evidence that the hypothesis is wrong. <python_output> Ratios (measured/predicted): [ 1. </python_output> 31.622 ... ] 1.414 1.414 3.162 1. 1. 1. ... LLM: // The model employs the tool to conduct numerical search for the unknown exponent. // This mitigates the most computationally intensive task, which the model is unable to // perform independently. <python> import numpy as np # ... (loads new data) ... # Try to fit = * m1 * m2 / dˆn for different def best_fit_n(mass1, mass2, distance, force): ns = np.linspace(0.5, 4.0, 71) best_n = None # ... (iterates through 71 possible exponents to find the lowest error) ... return best_n n, _, _ = best_fit_n(mass1, mass2, distance, force) print(f\"Best fit exponent n: {n}\") </python>"
        },
        {
            "title": "NewtonBench",
            "content": "Code-Assisted GPT-4.1s Successful Deduction of Easy Vanilla Equation in domain Gravitation (Law Submission) ENVIRONMENT: // The precise best-fit exponent is returned, value the model cannot discover without // the computational assistance. <python_output> Best fit exponent n: 1.5 </python_output> ... LLM: // Utilizing the Python interpreter tool, the model successfully derives and submits // the correct final law. <final_law> def discovered_law(mass1, mass2, distance): = 6.674e-05 return (C * mass1 * mass2) / (distance ** 1.5) </final_law>"
        },
        {
            "title": "NewtonBench",
            "content": "D.2 CASE STUDY FOR REASONING LLMS D.2.1 VANILLA REASONING LLMS Vanilla Gemini-2.5-flashs Successful Deduction of Easy Simple System in domain Calorimetry SYSTEM: // The model is tasked with discovering scientific law in simulated universe. // It can only run experiments and has no access to code assistance. ... ENVIRONMENT: // The task is to find the law of heat transfer. In this environment, the LLM will not be // provided with the heat output (Q) directly, but instead it will receive the power // outputs (P_i), which is functionally related to Q. ... LLM: // The model forms its initial assumption that = m*c*delta_T. // It gathers data by running set of experiments. <run_experiment> [ {\"m\": 1, \"c\": 1, \"delta_T\": 1}, {\"m\": 2, \"c\": 1, \"delta_T\": 1}, // ... and 7 other experiments ... ] </run_experiment> ... ENVIRONMENT: <experiment_output> [{\"P_cond\": 27.5242251438, \"P_conv\": 36.173370687, \"P_rad\": 16.131345123},...] </experiment_output> ... LLM: // After an initial experiment, the model begins complex chain of reasoning. It // manually calculates intermediate values and tests the initial hypothesis. Now will analyze the data from Experiment 1. For each data point, will: 1. Calculate P_sum = P_cond + P_conv + P_rad. 2. Calculate = (m * c) / 100. 3. Calculate Q_inferred = P_sum * t. ... delta_T Q_inferred Ratio (Q_inferred / (m*c*delta_T)) --------------- 1 1 1 0.798 0.798 1 2 1 4.575 2.287 <- Key Anomaly Identified 1 100 1 81063.6 810.6 <- Key Anomaly Identified ... The ratio is clearly not constant. This means Q_total is NOT simply * * delta_T. ... LLM: // This is the critical moment of exploration. Instead of testing another simple integer // power law, the model uses its internal reasoning to deduce non-obvious, non-integer // power law directly from the data points. It looks like Q_total is proportional to and delta_T. For c, its not simple linear relationship. Lets try to assume Q_total = * * delta_T * f(c). ... Lets see if Q_total_calculated / (m * delta_T) is proportional to cˆx. For c=1, value is approx 1. For c=2, value is approx 5.7. For c=100, value is approx 101329.5. If cˆx = 5.7 for c=2, then = log2(5.7) = 2.509. If cˆx = 101329.5 for c=100, then = log100(101329.5) = 2.5028. This is very close to 2.5. Lets assume Q_total = * * delta_T * cˆ2.5."
        },
        {
            "title": "NewtonBench",
            "content": "Vanilla Gem25fs Successful Deduction of Easy Simple System in domain Calorimetry (Law Submission) LLM: Now lets calculate for each point using = Q_total_calculated / (m * delta_T * cˆ2.5). The values for are consistently around 1.0. The average is ... 1.0030306. This is very close to 1. So, the hypothesis is Q_total = * delta_T * cˆ2.5. ... // The model designs targeted experiment to validate its novel hypothesis, which // successfully fits the newly collected experimental data. ... LLM: // Rather than being limited to hypotheses with simple integer exponents, the model is // able to conduct sufficiently broad exploration. It correctly deduces the non-integer // exponent (2.5) from the data and submits the correct law <final_law> def discovered_law(m, c, delta_T): import math return * delta_T * (c**2.5) </final_law>"
        },
        {
            "title": "NewtonBench",
            "content": "D.2.2 CODE-ASSISTED REASONING LLMS Code-Assisted Gemini-2.5-flashs Failed Conclusion of Easy Simple System in domain Calorimetry SYSTEM: // The model is tasked with discovering scientific law in simulated universe. // Other than running experiments, it is given access to Python interpreter ... ENVIRONMENT: // The task is to find the law of heat transfer. In this environment, the LLM will not be // provided with the heat output (Q) directly, but instead it will receive the power // outputs (P_i), which is functionally related to Q. ... LLM: // The model assumes the heat transfer equation = * * delta_T and conducts // experiments to test this hypothesis. Initial Hypothesis: The most common heat transfer law is = * * delta_T. Experiment Design: will run experiments with varying m, c, and delta_T values to see how the total power output changes. <run_experiment> [ {\"m\": 1.0, \"c\": 1.0, \"delta_T\": 1.0}, {\"m\": 2.0, \"c\": 1.0, \"delta_T\": 1.0}, // ... and 3 other experiments ... ] </run_experiment> ... ENVIRONMENT: <experiment_output> [{\"P_cond\": 29.960884111, \"P_conv\": 18.062543578,\"P_rad\": 31.4602080983}, ...] </experiment_output> ... LLM: // The LLM use Python tool to analyzes the data by first estimating the true heat transfer // (Q_total) from the measured power, assuming 20% energy loss. It then tests the // hypothesis against this estimated value to see if any of them are consistent match. ... <python> import json # ... (loads data) ... # Assume Q_total = * * delta_T for now Q_total_hypothesis = * * delta_T ... # ... (calculates ratio of the hypothesis output and the implied energy) ... print(f\"m={m}, c={c}, delta_T={delta_T}: Q_total_calculated / (m*c*delta_T)={ratio}) ... ENVIRONMENT: // The result of the code is returned, providing evidence that the hypothesis is wrong. <python_output> m=1.0, c=2.0, delta_T=1.0: Q_total_calculated / (m*c*delta_T) = 2.76 m=10.0, c=100.0, delta_T=50.0: Q_total_calculated / (m*c*delta_T) = 1014.88 ... </python_output>"
        },
        {
            "title": "NewtonBench",
            "content": "Code-Assisted Gemini-2.5-flashs Failed Conclusion of Easy Simple System in domain Calorimetry (Law Submission) LLM: // This is where the model gets trapped. Instead of using the data to inspire new kind // of hypothesis (like the vanilla model did), it gets stuck in rigid analytical loop. // It repeatedly uses the Python tool to test other simple, integer-based variations // (e.g., cˆ2, delta_Tˆ2), each time confirming they are also wrong. The previous analysis showed that my initial hypothesis Q_total = * * delta_T does not hold, as the ratio is not constant. Now, lets try to find relationship between Q_total_calculated and m, c, delta_T Checking ratios of Q_total / (m * * delta_Tˆ2): <python> // ... (code to test = * * delta_Tˆ2) ... </python> // --> Output shows this is wrong. Checking ratios of Q_total / (m * cˆ2 * delta_T): <python> // ... (code to test = * cˆ2 * delta_T) ... </python> // --> Output shows this is wrong. ... LLM: // Access to the Python tool limits the models strategy to what it could easily program: // brute-force checks of simple formulas. By offloading the calculation, it also offloaded // the creative pattern-matching required for scientific discovery. After its simple // ideas were repeatedly proven wrong, it was unable to generate new one, ultimately // abandoning its data-driven analysis and defaulting to its first hypothesis, even though // it is incorrect. have used all my turns and the analysis has been challenging. Neither do simple variations like * cˆ2 * delta_T or * * delta_Tˆ2. ... // They start to have some confusions from this part None of these are constant. This indicates that the relationship is not simple product of powers of m, c, and delta_T with single constant. Lets go back to the most common heat transfer law: = * * delta_T. will submit the most straightforward and universally recognized form of this law. The mission is to discover ground_truth_law(m, c, delta_T). will assume the simplest form for the ground truth law. <final_law> def discovered_law(m, c, delta_T): return * * delta_T </final_law>"
        },
        {
            "title": "E MISCELLANEOUS",
            "content": "E.1 MEMORIZATION IN LSR-TRANSFORM With the enhanced reasoning abilities of frontier LLMs, the effectiveness of equation transformation in preventing LLM memorization may vary. In Table 11, we present the performance of eight LLMs on LSR-Transform (Shojaee et al., 2025b) without observing any data. We observe that the latest LLMs demonstrate significant improvements over previous, weaker models, indicating their ability to deduce the transformed law from the original solely based on memorized knowledge (with an example from o4-mini below). On the other hand, as NEWTONBENCH curates counterfactual physical laws via metaphysical shift, making recall of canonical laws insufficient. Therefore, all LLMs achieve zero scores in the DataBlind setting. Table 11: DataBlind performance on LSR-Transform (first 3 results are from original paper)"
        },
        {
            "title": "DataBlind Accuracy",
            "content": "GPT-3.5-turbo Llama-3.1-8b GPT-4o-mini Nemotron-ultra-253b Kimi-K2 Qwen-3-next o4-mini GPT-4.1 2.10 3.61 7.21 28.83 30.63 33.33 34.23 35.14 LSR-Transform Example"
        },
        {
            "title": "Question",
            "content": "Find an equation in the field of classical mechanics that describes the mass ( ) needed to store energy in an oscillating system, given physical input variables: mean stored energy ( E_n ), driving frequency ( omega ), natural frequency (omega_0), and amplitude ( ). DataBlind Response by o4-mini (Correct) In (lightly damped) driven harmonic oscillator of mass m, spring-constant (so that omega_0² = k/m), and driving frequency omega, the instantaneous total energy is E(t) = (1/2) omega ² x² sin²(omega t) + (1/2) x² cos²(omega t) = (1/2) x² [ omega² sin²(omega t) + omega_0² cos²(omega t) ]. Averaging over one period = 2pi/omega gives the mean stored energy <E> = E_n = (1/4) x² (omega ² + omega_0²). Solving for yields the mass required to store mean energy E_n at amplitude x: = 4 E_n / [x² (omega² + omega_0²)] That is, = 4 E_n / [x² (omega² + omega_0²)]."
        },
        {
            "title": "NewtonBench",
            "content": "E.2 SOLVABILITY PROOF In this section, we formally show that, under the explicit design assumptions (A0A4) enforced by NEWTONBENCH, each curated task is finitely solvable in the noiseless setting. We begin by establishing the problem setting through precise definitions and stating the core structural assumptions guaranteed by the benchmarks design. We then proceed with proof by reduction. First, we show that the complex system-discovery task can be reduced to standard black-box function identification problem. Second, we prove that this reduced problem is identifiable with finite number of queries, thus establishing finite solvability for tasks satisfying these assumptions. E.2.1 PROBLEM SETTING AND ASSUMPTIONS Definition 3 (Task Formalization). task is defined by: Model: An ordered list of scalar equations = (f1, . . . , fk) that maps system-level inputs VM to final outputs yM {y1, . . . , yk}. One equation ftarget is unknown; all others, fassist = {ftarget}, and the models computational graph are known. Interaction: An agent queries the model by choosing inputs from domain Rm and observing the output YM(x). Objective: The agent must output an equation ˆf that is symbolically equivalent to ftarget. Definition 4 (Solvability). task is solvable if there exists finite interactive procedure (a finite number of experiments) that returns an equation ˆf symbolically equivalent to ftarget. Definition 5 (Evaluation Map and Separating Set). Let = {fj(; θj)}N ily where fj = {u1, . . . , um} , define the evaluation map j=1 be candidate fam- : and θj Rqj parameterizes the j-th structure. For finite set Ej,S : Rqj Rm, Ej,S(θ) = (cid:0)fj(u1; θ), . . . , fj(um; θ)(cid:1). set is: structure-separating if for any = ℓ, there are no parameters θ, ϑ such that Ej,S(θ) = Eℓ,S(ϑ); parameter-injective for if Ej,S is injective; separating if it is structure-separating and parameter-injective for every j. Structural Assumptions of the Benchmark The solvability proof rests on the following assumptions enforced by the benchmarks design. A0 (Noiseless Determinism): For any input D, the output YM(x) is computed deterministically by the model . A1 (Known and Invertible Assisting Path): Every observed output YM depends on ftarget via known path of assisting functions. For at least one such path, the composite function Φx is pointwise injective with known inverse on its image. That is, for ux being the inputs to ftarget: y(x) = Φx (cid:0)ftarget(ux)(cid:1). A2 (Reachability of Target Inputs): The inputs to ftarget are controllable. There exists non-empty open set of achievable target-input values and computable mapping x(u) such that the model fed with x(u) realizes ftarget at input u. A3 (Finite, Grammar-Bounded, and Nondegenerate Candidate Family): The target law j=1 with the following ftarget belongs to curated family of symbolic forms = {fj(; θj)}N properties: (a) Grammar-bounded arises set (e.g., from {+, , , , pow, exp, log, sin, cos}), finite variable set, and placeholders for grammar over construction context-free structure finite operator (finite"
        },
        {
            "title": "Each",
            "content": "set): fj"
        },
        {
            "title": "NewtonBench",
            "content": "real-valued constants, with maximum abstract-syntax-tree depth N. Expressions are canonicalized to remove syntactic symmetries (e.g., commutativity/associativity, neutral elements) and are restricted to be real-analytic on . This yields finite set of symbolic structures {σ1, . . . , σN }; each structure σj induces parametric form fj(; θj) with qj real parameters. (b) Structural Uniqueness: For = ℓ, there are no parameters θ, ϑ such that fj(; θ) fℓ(; ϑ) on . (c) Parameter Uniqueness: For each j, the map θj (cid:55) fj(; θj) is injective (as functions on ). (d) Analyticity: Each fj(; θj) is real-analytic on . A4 (Availability of Finite Separating Set): There exists an integer and finite set = {u1, . . . , um} that is separating in the sense of the preceding definition. E.2.2 PROOF OF SOLVABILITY The proof proceeds by first reducing the system discovery task to standard function identification problem, and then proving the latter is finitely solvable. Step 1. Reduction to Direct Oracle for ftarget Lemma E.1 (Path Inversion and Target Isolation). Under assumptions A1A2, for any chosen input there exists computable experiment input x(u) such that, from the observed outputs YM(x(u)), the agent can compute direct observation of ftarget(u). Proof. To query ftarget at an input , the agent first computes the required system input x(u) via A2. The agent then runs the experiment with x(u) and observes the output set YM(x(u)). By A1, the structure of the model is known, so the agent can identify the specific output YM that is related to the targets output via the known function Φx(u). This observation is = Φx(u)(ftarget(u)). Since Φx(u) and its inverse are known and computable, the agent computes the direct output of the target law as: := Φ1 x(u)(y) = ftarget(u). Corollary E.2 (Equivalence to Function Identification). The interactive task in NEWTONBENCH reduces to querying noiseless black-box oracle (cid:55) ftarget(u) over the open set . Step 2. Finite-Sample Identifiability We now show that the oracle for ftarget can be uniquely identified from the finite family with finite number of queries. Theorem E.3 (Finite-Sample Identifiability of the Target Law). Under assumptions A0A4, there exists finite integer and set = {u1, . . . , um } such that experiments are sufficient to uniquely determine both the symbolic structure and the numerical constants of ftarget. Proof. By A4, fix any separating set = {u1, . . . , um } of size m. 1. Structure Identification: Query the oracle (via Lemma 1) at all points in to obtain the observation vector = (ftarget(u1), . . . , ftarget(um )). For each candidate structure {1, . . . , }, attempt to solve the system of equations Ej,S(θ) = for the parameters θ. Since is structure-separating by A4, there exists exactly one index = for which solution exists. This uniquely identifies the symbolic structure of ftarget as fj . 2. Parameter Identification: For the identified structure j, is parameter-injective by A4; therefore the system Ej,S(θ) = admits unique solution θ. This uniquely determines the numerical constants. Thus, finite number of experiments is sufficient for complete identification."
        },
        {
            "title": "NewtonBench",
            "content": "Corollary E.4 (Constructive Finite Solver). constructive procedure to solve any NEWTONBENCH task exists: 1. Choose any set = {u1, . . . , um } that is separating (A4), and let = S. 2. For each ui S, compute x(ui), run one experiment to observe the output set YMi, and from it, apply path inversion (Lemma 1) to get zi = ftarget(ui). 3. For each candidate structure {1, . . . , }, solve for θ in Ej,S(θ) = (z1, . . . , zm ). 4. The unique structure that admits solution is the correct one; by A4, its parameter solution θ is unique. Return fj (; θ) as the discovered law. E.2.3 CONCLUSION Under the design guarantees (A0A4) and in the noiseless configuration, each curated task in NEWTONBENCH is finitely solvable. The assisting equations allow the hidden law to be observationally isolated (Lemma 1), and the finite, grammar-bounded, and nondegenerate nature of the candidate laws implies that finite set of experiments uniquely identifies the target structure and its constants (Theorem 1). This guarantee is conditional on A0A4; if these assumptions are relaxed (e.g., noninvertible assisting paths or unreachable target inputs), finite solvability need not follow."
        },
        {
            "title": "NewtonBench",
            "content": "E.3 PHYSICS DOMAIN ANALYSIS Table 12 presents an analysis of LLM performance across various physics domains, alongside several qualitative metrics for each domain. We classified the Web Freq. as High or Low based on the number of results returned by the Google Search API for the laws name, using threshold of 300,000 results. The Abstract Level was assigned by human experts, who judged how directly the underlying physical concept corresponds to tangible, everyday phenomena. We observe potential inverse relationship between LLM performance and the Abstract Level; performance generally decreases as the physical concepts become less tangible. In contrast, any correlations with the other metrics are not as readily apparent from this analysis. Table 12: Physics domain performance analysis Physics Domain SA (%) Feat. Operation Edu. Level Web Freq. Abstract Level Law of Sound Speed in Ideal Gas Amp`eres Force Law Newtons Law of Universal Gravitation Snells Law Maluss Law Coulombs Law Hookes Law Law of Radioactive Decay Fouriers Law Law of Damped Harmonic Motion Law of Heat Transfer Bose-Einstein Distribution 53.87 49.58 48.57 43.35 38.47 37.79 35.44 31.91 29.29 27.44 26.26 18.10 Root Basic Arith. Basic Arith. Trigonometric Trigonometric Basic Arith. Basic Arith. Exponential Basic Arith. Root Basic Arith. Exponential College College High School High School College High School High School College College College High School College Low High High High Low High High High High Low Low Low Moderate Realistic Realistic Realistic Moderate Realistic Realistic Abstract Moderate Moderate Moderate Abstract"
        },
        {
            "title": "NewtonBench",
            "content": "E.4 EXPLORATION AND EXPLOITATION TOKENS Our token-based exploration proxy is stylistic heuristic and is used only as suggestive evidence, not as definitive behavioral measure. The full signature word set for exploration and exploitation is provided in Table 13. The exploration rate is then calculated using equation 3, where Nexplore is the total frequency of words from the exploration set and Nexploit is the total frequency for the exploitation set. Rateexplore = Nexplore Nexplore + Nexploit 100% (3) Table 13: Exploration vs Exploitation word sets Exploration alternatively, what if, suspect, reconsider, re-examine, re-evaluate, perhaps, hypothesis, adjust, assume, invalidates, incorrect, but, however Exploitation confirm, verify, calculate, analyze, clearly, fit, compare, estimate, suggest, further, investigate, approximate, test"
        },
        {
            "title": "NewtonBench",
            "content": "E.5 USE OF LARGE LANGUAGE MODELS In this manuscript, large language models were used exclusively for grammar checking. Additionally, image generation models were employed to enhance the visual presentation of figures."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "The Hong Kong University of Science and Technology"
    ]
}