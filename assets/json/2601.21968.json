{
    "paper_title": "OVD: On-policy Verbal Distillation",
    "authors": [
        "Jing Xiong",
        "Hui Shen",
        "Shansan Gong",
        "Yuxin Cheng",
        "Jianghan Shen",
        "Chaofan Tao",
        "Haochen Tan",
        "Haoli Bai",
        "Lifeng Shang",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 8 6 9 1 2 . 1 0 6 2 : r OVD: On-policy Verbal Distillation Jing Xiong 1 Hui Shen 1 Shansan Gong 1 Yuxin Cheng 1 Jianghan Shen 2 Chaofan Tao 3 Haochen Tan 3 Haoli Bai 3 Lifeng Shang 3 Ngai Wong"
        },
        {
            "title": "Abstract",
            "content": "Knowledge distillation offers promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student models exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (09) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io. 1. Introduction Large language models (LLMs) demonstrate remarkable capabilities across diverse tasks, yet their ability to perform complex multi-step reasoning remains fundamental challenge (Wei et al., 2022; Guo et al., 2025a). While recent advances in reinforcement learning (RL) enable models to develop long-horizon reasoning behaviors (Shao et al., 1The University of Hong Kong, Hong Kong, China 2Nanjing University, Nanjing, China 3Huawei Technologies, China. Correspondence to: Jing Xiong <junexiong@connect.hku.hk>. Preprint. January 30, 2026. 2024; Singh et al., 2023), the high computational costs and resource requirements of training large reasoning models create significant barrier to widespread deployment. Knowledge distillation offers promising solution by transferring reasoning capabilities from powerful teacher models to more efficient student models (Hinton et al., 2015; Hsieh et al., 2023). However, existing distillation approaches face limitations when applied to RL, particularly in terms of memory efficiency and the ability to leverage verbal feedback (Shinn et al., 2023). The traditional knowledge distillation methods for LLMs operate at the token level, requiring the teacher model to output probability distributions over the entire vocabulary at each decoding step (Kim & Rush, 2016; Gu et al., 2023). While this fine-grained supervision provides rich gradient signals, it introduces severe memory bottleneck for RL. For example, with batch size = 8, = 4 samples per problem, sequence length = 8192, and vocabulary size = 152K, storing the Qwen-7B models logits in FP32 consumes around 160 GB of memory per batch, far exceeding modern accelerators capacity. This memory cost scales linearly with trajectory length, making token-level distillation impractical for long-horizon reasoning chains in complex problems. Additionally, token-level distillation fail to capture the hierarchical structure of reasoning, and strictly matching the student and teacher distributions constrains the models exploration. More broadly, the potential of distillation methods that incorporate verbal feedback from environment agents remains underexplored. Recent studies show that training language models with environment agents can substantially enhance models search capabilities (Jin et al., 2025; Sun et al., 2025; Fang et al., 2025). However, the verification capabilities of environment agents, particularly in the context of on-policy distillation (Agarwal et al., 2024), have not been sufficiently studied. This limitation prevents distillation methods from fully leveraging the rich supervision provided by verifiable agents, especially in information retrieval domain. To address these challenges, we introduce On-policy Verbal Distillation (OVD), which reformulates distillation as trajectory matching rather than token-level probability matching, OVD: On-policy Verbal Distillation enabling principled integration of verbal feedback into onpolicy distillation. In OVD, teacher agents provide verbal scores on reasoning correctness to evaluate trajectories, allowing the student model to learn both reasoning patterns and interaction behaviors through RL. By replacing fullvocabulary logit supervision with verbal trajectory evaluation, OVD significantly reduces the memory overhead of distillation. Our main contributions are: We propose On-policy Verbal Distillation (OVD), an on-policy reinforcement learning distillation framework that supervises student trajectories using discrete verbal feedback from teacher models instead of tokenlevel probability matching. This design greatly reduces memory cost and promotes on-policy exploration. We conduct extensive experiments on Web Q&A and mathematical reasoning benchmarks, demonstrating that OVD substantially outperforms existing RL-based web search methods while exhibiting superior sample efficiency. By scaling trajectory sampling on single randomly selected instance, we achieve an 25.7% performance improvement on math benchmarks. We provide theoretical analysis of OVD, showing that its rejection sampling scheme yields unbiased gradient estimates, and demonstrate that an interactive environment agent is key to success on Web Q&A tasks. 2. Why Verbal Distillation? In this section, we explain why OVD adopts verbal scoring over token-level distillation for improved memory efficiency. Memory Bottleneck. Token-level distillation suffers from three fundamental limitations: (i) it treats all tokens equally, ignoring the hierarchical structure of reasoning; (ii) it requires access to the teachers token-level probability distribution, which may be unavailable for black-box models; and (iii) When distillation relies on large-scale rollouts from reasoning model, token-level supervision incurs prohibitive memory overhead, as it requires storing logits over the entire vocabulary for each token in trajectories of length L. In on-policy training with group relative optimization (GRPO) (Shao et al., 2024), maintaining such logits for all samples simultaneously leads to: = LV (d32 +d16) = 1.5B LV d32 (1) where is the batch size, is the number of samples per problem, d32 = 4 bytes (FP32) and d16 = 2 bytes (BF16). Frameworks maintain both FP32 and BF16 copies (Wijmans 2 Table 1. Memory consumption analysis for token-level distillation components (Qwen2.5-7B example with sequence length 8192). Component Shape Example Size Logits (FP32) Logits (BF16) KV Cache (1 layer) KV Cache (28 layers) [L, ] [L, ] [2, Hkv, L, d] [NL, 2, Hkv, L, d] 8192 152K 8192 152K 2 4 8192 128 28 2 4 8192 128 Dtype FP32 BF16 BF16 BF Memory 4 = 5.0 GB 2 = 2.5 GB 16 MB 469 MB ) ( m 30 25 20 15 10 5 Logits (FP32) Logits (BF16) KV Cache (28 layers) Logits Total (FP32+BF16) 0 4K 8K 24K 16K Sequence Length (L) 32K 104 Figure 1. Memory consumption scales linearly with sequence length for token-level distillation on Qwen2.5-7B. Logits storage grows 16 faster than KV cache per token, making it the primary memory bottleneck for long-context distillation. et al., 2024). With 32K128K and long-horizon trajectories, this creates prohibitive memory bottleneck of O(N ). In contrast, OVD replaces token-level logits with verbal scores (09) for trajectory quality assessment, reducing memory to O(N v) per batch where is the number of reasoning steps (e.g., sentences or logical derivations), is the verbal vocabulary size, and since verbal scores are provided at the step level rather than per token. This yields approximately /v reduction (e.g., 48000 for = 32, = 152K, and = 10), enabling longer trajectories, larger batch sizes, and more samples per problem without requiring teacher token-level distributions (see Figures 1 and 2). Table 1 provides breakdown using Qwen2.5-7B (Yang et al., 2024) as an example. Verbal Feedback. Beyond memory benefits, verbal scoring provides semantically meaningful feedback at multiple granularities. Instead of matching low-level token distributions, the teacher evaluates reasoning quality (correctness, relevance, and coherence) and delivers interpretable feedback that explains why steps are correct or incorrect, what mistakes were made, and how to improve. This approach naturally supports step-level, trajectory-level, or hybrid supervision without vocabulary alignment. Table 2 summarizes key differences: vs. Token-Level Distillation. Token-level methods require full-vocabulary logits for KL divergence, incurring significant memory overhead. OVD uses verbal scores (09) for trajectory quality assessment and rejection sampling, eliminating full-vocabulary logits while maintaining black-box compatibility. vs. SequenceLevel Distillation. Sequence-level methods only provide sparse feedback at trajectory end. OVD provides dense OVD: On-policy Verbal Distillation 250 200 150 100 ) ( m 0 Logits (FP32) Logits (BF16) KV Cache (28 layers) Logits Total (FP32+BF16) 8 12 16 20 24 28 32 Number of Rollouts per Problem (N ) Figure 2. Memory consumption scales linearly with the number of rollouts per problem at fixed sequence length = 8192 for Qwen2.5-7B. Both logits and KV cache grow proportionally to , with logits requiring 16 more memory than KV cache. With = 32 rollouts, logits alone require 240 GB, making token-level distillation impractical for RL. Table 2. Comparison of distillation methods for language models. Method Granularity Feedback On-Policy Mem. SeqKD Token KD On-Policy KD OVD (Ours) Sequence Token Token Step Samples Probabilities Probabilities Verbal Low High High Med. step-level supervision, enabling better credit assignment for multi-step reasoning. vs. Outcome-Based RL. Standard GRPO with only answer correctness rewards suffers from sparse feedback. 3. On-policy Verbal Distillation In this section, we present OVD, novel framework that transfers reasoning capabilities from teacher to student model through trajectory optimization with verbal feedback. 3.1. Problem Formulation Consider teacher model πE and student model πS. Given problem x, the student model generates multistep reasoning trajectories, while the teacher model provides verbal feedback scores. We denote trajectory as = (s1, s2, . . . , sK), where each sk represents reasoning step (e.g., sentence or logical derivation) and is the number of steps. The goal is to train πS to produce highquality reasoning trajectories that lead to correct answers, guided by the teachers feedback. 3.2. Teacher Model In OVD, tasks are categorized into web Q&A and math reasoning; the two types of tasks receive verbal feedback from the Environment Agent and the Reasoning Agent, respectively. 3 Environment Agent. We construct environment agents as teachers in OVD, encapsulating web Q&A tasks with verifiable outcomes. Each environment πE defines (i) state space, (ii) an action space of reasoning steps or tool calls, and (iii) verbal feedback scoring intermediate reasoning. While environment scaling has progressed (Fang et al., 2025; Guo et al., 2025b; Yao et al., 2022; Lu et al., 2025), it remains unclear how students can effectively leverage such supervision on-policy. Following Search-R1 (Jin et al., 2025), we incorporate search as intermediate actions within on-policy reasoning. To enable scalable simulation without real retrieval, we adopt ZeroSearch (Sun et al., 2025) and initialize the environment agent via supervised fine-tuning to mimic search engine outputs, from which trajectory discrimination emerge without explicit supervision. Reasoning Agent. We employ large reasoning model (e.g., QwQ-32B) as the reasoning agent to provide highquality supervision. Given trajectory, it evaluates intermediate steps and produces fine-grained verbal feedback on reasoning correctness, coherence, and progress, serving as learned critic complementary to token-level rewards. 3.3. On-policy Sampling i=1: y(j) xi) for = 1, . . . , . Each trajectory y(j) Policy Sampling. At each iteration, the student generates trajectories per problem in batch {xi}B πS( = (s(j) 1 , . . . , s(j) ) has variable-length reasoning steps. OnKj policy sampling ensures feedback on the students own distribution, mitigating distribution mismatch in off-policy methods trained on pre-generated teacher outputs. Verbal Rejection Sampling. We employ verbal scoring interface where the teacher provides quality assessments as discrete scores rather than full-vocabulary logits. At the step level, the teacher scores individual reasoning steps sk; at the trajectory level, it evaluates the complete chain = (s1, . . . , sK). The teacher outputs distribution over discrete verbal vocabulary of size = 10 (scores {0, 1, . . . , 9}), from which scores are sampled to preserve judgment uncertainty and enable stochastic exploration. This design allows flexible trade-offs between dense step-level supervision and efficient trajectory feedback. Building on these verbal scores, we apply verbal rejection sampling to select high-quality trajectories for training. Unlike standard rejection sampling (Liu et al., 2023; Xiong et al., 2025) that requires full-vocabulary logits (inaccessible for black-box models), our method operates over the compact score space. Trajectories receiving low scores are rejected; for each rejected trajectory, we resample from the teacher agent and continue generation until complete trajectory is obtained. The completed trajectories are then used OVD: On-policy Verbal Distillation Figure 3. The policy model performs trajectory sampling (left), while the teacher model applies rejection sampling and returns search results for Web Q&A (right). to construct training samples for GRPO, ensuring that the student is optimized on realized on-policy rollouts rather than truncated fragments. This mechanism creates mixture of student explorations and teacher-guided acceptance signals while maintaining strict on-policy learning. The formal definition of the acceptance probability and theoretical analysis of this mechanism are provided in Section 3.5. The trajectories used for policy optimization include complete environment interaction traces (e.g., search queries, retrieved documents, and tool calls). We optimize the policy model with PPO-style clipped gradients on these environment-augmented trajectories, enabling the student to acquire both reasoning patterns and tool-use behaviors, thereby transferring agentic capabilities from the environment agent to the policy model via RL. 3.4. Verbelized Distillation In this section, we introduce verbal distillation with GRPO within unified training procedure. The complete OVD training pipeline is illustrated in Figure 3. Reward Design. For generated trajectories, we compute the outcome-based reward R(y) as: R(y) = (cid:40) F1(y, y) δ(y, y) for Web Q&A for math reasoning (2) each step by its reward. The policy gradient objective is: = Ex,y (cid:34) (cid:88) k= Rk log πS(skx, s<k) (3) (cid:35) where log πS(yx) = (cid:80)K k=1 log πS(skx, s<k). This formulation enables flexible credit assignment at different granularities: for step-level scoring, each Rk corresponds to step feedbacks; for trajectory-level scoring, we set Rk = R(y) for all k, which is special case of the above objective. Policy Optimization. We optimize the policy using grouprelative advantages and clipped gradients. Following GRPO, we normalize rewards within each problems trajectory group to remove difficulty bias: A(y(j) ) µi)/(σi + ϵ), where µi and σi are the mean and standard deviation of rewards for problem xi. We then apply PPO clipping (Schulman et al., 2017) to stabilize updates: ) = (R(y(j) LRL = [min (ρA, clip(ρ, 1 ϵc, 1 + ϵc)A)] (4) where ρ = πS(yx)/πold (yx) is the importance ratio and ϵc = 0.2 bounds the policy update magnitude. The resulting training batch contains mixture of (i) accepted student trajectories that pass the quality threshold, and (ii) teacher trajectories for distilling expert knowledge. All trajectories contribute to LRL based on their rewards, enabling the model to learn from both exploration and expert demonstrations. where denotes the ground-truth reference answer, F1(y, y) = 2yy y+y measures word overlap, and δ(y, y) {0, 1} denotes exact matching under mathematical equivalence (numerical tolerance, algebraic normalization). This design provides soft partial credit for open-ended answers and strict binary feedback for correctness. 3.5. Theoretical Analysis We analyze OVD through the lens of interactive imitation learning and verbal rejection sampling, providing theoretical justification for our verbal distillation mechanism. Detailed proofs are provided in Appendix A.1. Policy Gradient Objective. We optimize the policy model by decomposing the trajectory probability and weighting Connection to Imitation Learning. OVD can be viewed as an interactive imitation learning framework (Ross et al., 4 OVD: On-policy Verbal Distillation 2011; Ho & Ermon, 2016; Zhang & Cho, 2016) for LLMs. Unlike classical imitation learning, which suffers from distribution shift due to training solely on expert demonstrations, OVD uses on-policy student trajectories and teacherprovided verbal scores to construct mixed distillation distribution over teacher and student outputs. Verbal Rejection Sampling. Unlike standard rejection sampling that relies on logits (Liu et al., 2023; Xiong et al., 2025), we propose verbal rejection sampling that operates solely on discrete scores S(y) {0, 1, . . . , 1} from the teacher, where is the verbal vocabulary size (we use = 10 in practice). Given trajectory sampled from the student policy πS and score threshold θ {0, 1, . . . , 1}, the acceptance decision is: a(y) = 1[S(y) θ] (5) where S(y) is the verbal score sampled from the teachers distribution over score tokens, and 1[] is the indicator function. Trajectories with S(y) < θ are rejected and replaced by teacher demonstrations πT . Connection to Off-Policy Rejection Sampling. Our verbal rejection sampling is related to rejection and importance resampling for off-policy learning (Chung et al., 2018; Schlegel et al., 2019), but differs in assumptions and acceptance criteria. Prior work corrects distribution mismatch between the behavior and target policies using explicit density ratios (e.g., πT (y)/πS(y)), either through importance reweighting or probabilistic acceptance, requiring access to normalized policy likelihoods. In contrast, OVD samples directly from the student policy πS and defines an implicit target distribution via verbal scores S(y), using these scores for acceptance instead of density-ratio correction. Thus, OVD performs on-policy sampling with teacher-induced off-policy objective. Distillation Distribution. Following verbal rejection sampling, the effective training distribution becomes mixture of accepted student trajectories and teacher demonstrations: ptrain(y) = αt π(t) (y)1[S(y) θ]+(1αt)πT (y) (6) yπ(t) where αt = [1[S(y) θ]] is the acceptance rate at iteration (i.e., the probability that student trajectory passes the threshold). This mixture distribution enables the student to learn from both its own successful explorations (accepted trajectories with S(y) θ) and expert demonstrations (teacher trajectories when student fails), mitigating distribution shift while maintaining on-policy learning. As training progresses and student quality improves, αt increases, gradually shifting from teacher-guided to on-policy learning. We show that the resulting gradient estimator is unbiased under the induced surrogate distribution (Theorem 3.1) and that mixture training reduces variance. Theorem 3.1 (Unbiased Gradient Estimation). The verbal on-policy rejection sampling procedure yields an unbiased gradient estimator: θJ(πS) = EyπS [1[S(y) θ] R(y) θ log πS(y)] + EyπT [(1 αt) R(y) θ log πS(y)] (7) where αt = EyπS [1[S(y) θ]] is the expected acceptance rate. Proposition 3.2 (Variance Reduction). The variance of the gradient estimator under verbal on-policy rejection sampling satisfies: V[θJRS] V[θJ0] EyπS (cid:2)1[S(y) < θ]R(y)θ log πS(y)2(cid:3) (8) where the reduction comes from replacing rejected trajectories (with S(y) < θ) by lower-variance teacher demonstrations. Proposition 3.3 (Convergence under Mixture Training). Under the verbal rejection sampling regime with mixture distribution ptrain(y), the student policys expected reward satisfies: Eyptrain[R(y)] αt(1δt)J(πT )+(1αt)J(πT ) (9) where δt is the performance gap of accepted student trajectories. As training progresses, αt 1 and δt 0, yielding J(π(t) ) J(πT ). Proposition 3.4 (Score Granularity and Approximation Quality). Let Q(y) denote the true quality of trajectory (e.g., estimated by the teachers internal value function), and let Sv(y) {0, 1, . . . , 1} be the v-level discretized score. The approximation error satisfies: Ey (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) Q(y) Sv(y) 1 (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) 1 2(v 1) (10) Thus, larger verbal vocabulary yields finer-grained quality assessment and more precise trajectory selection, with approximation error decreasing as O(1/v). The key insight is that verbal rejection sampling provides curriculum: initially, the student relies on teacher demonstrations (αt small), but as it improves, more student trajectories are accepted (αt increases), eventually achieving teacher-level performance. The scores S(y) serve as quality metric that adapts to the students current capability, automatically adjusting the mixture ratio αt throughout training. Proposition 3.4 shows that increasing the score granularity improves the precision of this quality assessment. Collectively, these results demonstrate that OVD effectively addresses key challenges in knowledge distillation for language models. By combining verbal teacher feedback with 5 OVD: On-policy Verbal Distillation on-policy student exploration, our method enables the student to gradually approach teacher-level performance while maintaining stable training through unbiased gradient estimates and variance reduction. The mixture training distribution naturally balances exploitation of teacher knowledge with exploration of student capabilities, avoiding the distribution shift that plagues pure imitation learning while preserving the benefits of expert guidance. The complete training algorithm is provided in Appendix A.5. 4. Experiments 4.1. Evaluation Tasks We evaluate OVD on two types of reasoning tasks: Web Q&A and mathematical reasoning. Web Q&A Tasks. We evaluate on eight question answering benchmarks spanning both single-hop and multi-hop reasoning: Single-hop QA: (1) NQ (Kwiatkowski et al., 2019): Natural Questions dataset with factoid questions from Google search queries. (2) TriviaQA (Joshi et al., (3) 2017): Trivia questions with evidence documents. PopQA (Mallen et al., 2023): Questions about popular entities to test factual knowledge. Multi-hop QA: (4) HotpotQA (Yang et al., 2018): Questions requiring reasoning over multiple supporting documents. (5) 2Wiki (Ho et al., 2020): Multi-hop questions constructed from Wikipedia. (6) Musique (Trivedi et al., 2022): Compositional multi-hop reasoning questions. (7) Bamboogle (Press et al., 2023): Challenging questions designed to be difficult for search engines. (8) GAIA (Mialon et al., 2023): Real-world questions requiring tool use and reasoning to solve practical tasks. We evaluate on its validation set and restrict tool access to search engine only. Mathematical Reasoning Tasks. To evaluate the scalability of OVD, we conduct experiments on diverse math reasoning benchmarks: (1) SVAMP (Patel et al., 2021): Simple variations on arithmetic word problems testing robustness. (2) ASDiv (Miao et al., 2020): Academic standard division of mathematical problems. (3) MAWPS (Koncel-Kedziorski et al., 2016): Math word problems from various sources. (4) TABMWP (Lu et al., 2022): Tabular math word problems requiring table understanding. (5) Minerva (Lewkowycz et al., 2022) is rigorous scientific evaluation benchmark inspired by prior work on mathematical and scientific reasoning in LLMs (Lewkowycz et al., 2022). (6) OlympiadBench (He et al., 2024): Mathematical olympiad competition problems. (7) The SAT-Math dataset was obtained from the College Board and provides standardized test performance data for U.S. high school students (College Board, 2016). (8) The MMLU dataset (Hendrycks et al., 2020) is benchmark designed to evaluate models knowledge and reasoning across 57 diverse academic and professional subjects. (9) Gaokao (Zhang et al., 2023): Chinese national college entrance exam (Gaokao) mathematics problems for evaluating LLM performance. (10) AIME2024 (AI-MO, n.d.) is curated benchmark designed to evaluate LLMs on challenging competition-level mathematical problem solving tasks. 4.2. Main Results Table 3 presents the main results on Web Q&A benchmarks, with exact match (EM) scores evaluated on eight test datasets. Methods marked with (OVD) use score sampling during testing instead of deterministic thresholdbased selection. Our method consistently outperforms the key web search-based baselines (Search-o1, Search-R1, and ZeroSearch) across both model backbones. Several key observations emerge from our experiments. (i) Absolute gains over strong search-based baselines. On Qwen-2.5-3B-Base, OVD exceeds Search-R1 by 10.8 points (43.6% vs. 32.8%), despite the latter having access to real-time search during inference. Compared to ZeroSearch, OVD achieves consistent improvement on average (43.6% vs. 34.5%). On LLaMA-3.2-3B-Base, OVD achieves substantial improvements over Search-R1 and ZeroSearch. (ii) Sampling over the score vocabulary improves performance. On Qwen-2.53B, enabling score sampling at test time improves performance from 39.04% to 41.09%, with particularly notable gains on the GAIA dataset. (iii) On multi-hop tasks, which require more complex reasoning, OVD shows consistent advantages: on Musique, which requires compositional reasoning, OVD achieves 22.2% versus 13.0% for ZeroSearch. On Bamboogle, benchmark designed to challenge search engines, OVD (22.4%) matches Search-o1, while Search-R1 and ZeroSearch lag far behind. These results demonstrate the effectiveness of combining verbal distillation with onpolicy learning for challenging Web Q&A tasks. (iv) OVD is particularly pronounced on the most challenging GAIA dataset, where OVD achieves significant improvements over strong baseline (13.33% vs. 7.81% on Qwen-2.5-3BBase, and 9.09% vs. 4.69% on LLaMA-3.2-3B-Base). 4.3. Ablation Study We conduct comprehensive ablation studies to investigate the impact of key design choices in our framework: the choice of environment agent (Prompt-based vs. SFT-based), training reject threshold, and inference reject threshold. Figure 4 presents the performance trends across seven Web Q&A benchmarks. Speculative Decoding. We compare two policy model distillation strategies: (i) prompt-based environment agent, as shown in Fig. 4a, with rejection threshold of 5 during training and thresholds of 0, 5, and 10 during testing, and (ii) an SFT-based environment agent, as shown in Fig. 4b, 6 OVD: On-policy Verbal Distillation Table 3. Main results using different LLMs as the backbone. Results are obtained with 7B models unless otherwise specified (14B). The best performance is set in bold. denotes sampling trajectories based on token probabilities over the score vocabulary during testing. Method Single-Hop QA Multi-Hop QA NQ TriviaQA PopQA HotpotQA 2Wiki Musique Bamboogle GAIA Avg. Direct Answer CoT RAG RA-Agent Search-o1 R1 Search-R1 ZeroSearch OVD (Training Rej=10, Testing Rej=0) OVD (Training Rej=10, Testing Rej=5) OVD (Training Rej=10, Testing Rej=10) OVD (Training Rej=5, Testing Rej=0) OVD (Training Rej=5, Testing Rej=5) OVD (Training Rej=5, Testing Rej=10) OVD (Training Rej=10, Testing Rej=10, 14B) OVD (Training Rej=10, Testing Rej=5, 14B) OVD (Training Rej=10, Testing Rej=0, 14B) OVD (Training Rej=5, Testing Rej=5) OVD (Training Rej=10, Testing Rej=5) Direct Answer CoT RAG RA-Agent Search-o1 R1 Search-R1 ZeroSearch OVD (Training Rej=10, Testing Rej=0) OVD (Training Rej=10, Testing Rej=5) OVD (Training Rej=10, Testing Rej=10) OVD (Training Rej=5, Testing Rej=0) OVD (Training Rej=5, Testing Rej=5) OVD (Training Rej=5, Testing Rej=10) OVD (Training Rej=5, Testing Rej=5) OVD (Training Rej=10, Testing Rej=5) 12.40 15.00 31.60 15.20 16.60 14.20 40.60 43.00 51.00 50.00 50.00 49.20 47.00 46.00 48.00 47.40 48.60 48.80 25.60 16.20 26.20 30.00 22.40 24.20 28.40 41.20 23.80 39.40 43.40 41.60 44.80 46.80 45.60 45.80 42.00 Qwen-2.5-3B-Base 5.60 3.60 15.20 6.60 8.20 20.80 44.20 41.40 69.00 69.80 65.20 63.20 66.40 64.80 65.40 66.40 64.80 66.60 43.20 LLaMA-3.2-3B-Base 7.40 2.80 26.40 11.40 8.80 30.00 44.00 18.00 48.20 54.80 33.00 54.60 54.80 57.20 54.80 54.40 16.00 16.20 24.20 12.60 14.80 19.60 29.20 33.80 39.40 39.00 42.00 39.20 38.00 38.80 39.00 37.00 38.40 39.40 22. 12.60 16.00 23.40 16.60 19.40 22.80 29.60 22.00 31.40 31.00 34.00 33.60 37.00 35.20 34.20 31.00 30.60 33.60 58.00 28.40 31.00 34.80 60.00 61.60 68.80 67.80 69.20 67.20 67.40 66.80 66.60 65.80 66.40 68.00 39.40 29.60 44.40 57.60 36.20 48.40 44.20 60.00 41.00 57.40 62.20 57.80 64.80 64.20 64.00 64.40 57.80 19.20 18.00 23.20 16.60 22.40 28.40 32.00 34.60 42.00 43.70 45.00 35.20 37.60 43.80 41.80 43.60 38.60 41.40 15.80 9.20 10.20 17.60 21.00 17.40 28.40 31.60 26.80 31.00 29.20 21.00 33.60 36.20 35.60 36.20 34.40 4.40 3.60 8.20 2.60 5.20 6.40 11.20 13.00 21.00 27.90 26.40 21.80 23.20 25.00 24.00 23.20 22.80 24.20 10. 2.00 5.80 9.60 5.60 6.00 7.00 13.60 4.80 14.00 18.40 34.40 18.80 22.20 18.00 22.20 17.20 16.80 12.80 15.20 13.60 22.40 5.56 12.50 13.89 23.20 32.00 42.40 15.20 24.80 40.00 33.60 24.80 16.80 28.80 12.80 8.00 21.60 11.20 26.40 32.00 11.11 19.44 11.10 15.20 25.60 21.00 22.40 27.20 32.80 28.00 28.00 2.40 2.40 5.40 3.00 2.40 1.80 7.81 7.27 13.33 12.12 11.51 9.09 7.88 10.30 10.30 8.50 10.90 11.50 12.72 1.20 1.20 1.20 0.60 1.20 3.00 4.69 3.03 7.27 7.88 7.27 9.09 8.48 7.88 7.27 8.48 13.43 13.15 22.63 12.33 15.38 16.45 29.69 31.07 40.97 42.79 43.96 37.51 39.04 41.94 41.09 39.59 38.41 41.09 22. 10.78 16.03 22.00 17.53 19.68 21.86 30.52 18.69 30.58 34.06 31.01 35.19 37.01 37.04 36.51 34.16 with training rejection thresholds of 5 and 10 and testing rejection thresholds of 5 and 10. Interestingly, we observe that moderate testing threshold of 5 consistently outperforms both the permissive threshold of 0 (accepting all trajectories) and the strict threshold of 10 (where the teacher model intervenes in nearly all cases), suggesting that an appropriate balance between student exploration and teacher guidance is crucial for optimal performance. This finding becomes important in speculative decoding scenarios, where smaller draft model generates candidate steps verified by larger target model. By properly tuning the threshold during inference, the teacher model can effectively filter out low-quality reasoning trajectories after verification, leading to consistent performance improvements. (iii) Fully replacing the policy model trajectories with those from the teacher during training (i.e., using rejection threshold of 10) does not necessarily yield better gains, as it may harm the diversity of the policy models exploration trajectories. Emergent Discriminative Capability. As shown in Fig. 4, SFT-based environment agents substantially outperform prompt-based approaches across all datasets. Specifically, the best SFT configuration (T10, QR test T10) achieves an average score of 0.486, while the best prompt-based configuration (T5, QR test T10) only reaches 0.241, representing 101.7% relative improvement. Notably, following ZeroSearch (Sun et al., 2025), the SFT-based environment agents were trained on simulator data to mimic search engine outputs, without explicit training as discriminators. Remarkably, this elicits emergent discriminative capabilities for distinguishing trajectory quality. 4.4. Scaling RL Training Table 4 presents the scaling behavior of RL training on mathematical reasoning benchmarks, comparing OVD with RLVR under different sample quantity settings (1 random example vs. 2 random examples per problem) across training steps (300, 500, 600, and 800 steps). (i) OVD demonstrates remarkable sample efficiency, achieving substantial perfor7 Method Baselines Zero-shot Table 4. Scaling RL training results across different training steps and methods under different random-example settings. OVD: On-policy Verbal Distillation Steps SVAMP ASDiv MAWPS TABMWP Minerva OlympiaBench SAT-Math MMLU Gaokao AIME24 Avg. 0 15.6 26.7 24. Scaling RLVR (1 random example) 58.8 One-example 68.2 One-example 64.5 One-example 75.3 One-example 300 500 600 800 60.8 66.4 63.9 74.2 60.4 69.3 66.1 79.1 8.8 49.1 57.7 61.4 71. 9.9 14.7 16.5 13.6 16.2 OVD (1 random example, Training Rejection Threshold = 7, Testing Rejection Threshold = 0) OVD OVD OVD OVD 47.9 61.0 68.9 73.2 58.2 65.0 73.7 79.7 55.1 63.1 73.8 75. 59.9 63.9 70.2 73.0 14.3 14.3 16.5 18.8 300 500 600 800 Scaling RLVR (2 random examples) Two-example Two-example Two-example Two-example 73.7 77.1 75.9 81.1 300 500 600 76.2 80.0 78.1 80.0 80.4 86.0 84.6 87.6 70.8 75.17 75.6 73.7 16.5 16.5 16.5 15.8 OVD (2 random examples, Training Rejection Threshold = 7, Testing Rejection Threshold = 0) OVD OVD OVD OVD 87.1 87.7 82.6 90. 72.3 74.9 74.0 77.6 78.6 79.8 76.3 83.1 81.8 81.1 77.7 83.3 15.1 17.3 17.3 18.0 300 500 600 800 21. 28.9 29.6 31.0 31.1 29.0 29.0 29.6 31.6 30.2 27.7 31.1 29.6 29.5 29.3 30.4 31.6 90.6 87.5 87.5 87.5 84. 87.5 87.5 81.2 90.6 81.2 81.2 84.4 81.2 81.2 75.0 87.5 84.4 50.0 30.8 16. 29.6 49.2 49.5 49.2 49.5 49.7 49.7 50.5 50.2 51.1 51.3 49.6 49.0 50.2 50.3 50.6 49.4 28.6 35.7 28.6 35. 42.9 28.6 35.7 42.9 35.7 42.9 35.7 35.7 50.0 35.7 28.6 35.7 20.0 16.7 13.3 10.0 6.7 13.3 10.0 16.7 6.7 13.3 13.3 13. 6.7 10.0 10.0 13.3 45.8 49.7 47.9 52.7 45.1 47.5 51.0 55.3 52.2 55.1 54.5 54.7 55.2 54.1 53.5 56.7 Prompt-based Environment Agent SFT-based Environment Agent c n o P 0.6 0.4 0.2 T5, QR test T5 T5, QR test T10 T5, QR test T"
        },
        {
            "title": "NQ\nTriviaQA",
            "content": "PopQA HotpotQA 2Wiki"
        },
        {
            "title": "Avg",
            "content": "e S a f 0.6 0.4 0.2 0 T10, QR test T10, QR test T5 T5, QR test T10 T5, QR test T"
        },
        {
            "title": "NQ\nTriviaQA",
            "content": "PopQA HotpotQA 2Wiki"
        },
        {
            "title": "Avg",
            "content": "(a) Prompt-based configurations. (b) SFT-based configurations. Figure 4. Ablation study comparing different simulator training strategies and threshold configurations across seven Q&A benchmarks. Datasets are ordered by increasing difficulty from left to right: NQ, TriviaQA, PopQA (single-hop), HotpotQA, 2Wiki, Musique, Bamboogle (multi-hop). (a) Prompt-based simulators show modest performance with average scores around 0.20-0.24. (b) SFT-based simulators substantially outperform prompt-based approaches, with the best configuration (T10, QR test T10) achieving an average score of 0.486, demonstrating the importance of proper simulator design and threshold alignment. mance gains with minimal training examples. With just 1 random training example per problem, OVD improves the baseline performance (23.9%) by over 30 percentage points, reaching 55.3% at 800 training steps. (ii) Compared to one-example RLVR at the same training budget, OVD consistently outperforms by 2.6 percentage points (55.3% vs. 52.7%), demonstrating that verbal feedback enables more effective learning from limited demonstrations. (iii) Remarkably, OVD exhibits exceptional training efficiency by rapidly converging to near-optimal performance with minimal training steps. With 2 training examples per problem, OVD achieves 55.2% average accuracy at just 300 training steps, already approaching the peak performance of 2-random-example training at 800 steps (55.3%). This finding demonstrates that OVD can effectively leverage additional training examples to accelerate convergence, reaching the sample efficiency ceiling in early training stages. 5. Conclusion We introduced On-policy Verbal Distillation (OVD), memory-efficient framework that transfers reasoning capa8 OVD: On-policy Verbal Distillation bilities from large teacher models to smaller student models via verbal feedback. By replacing token-level probability matching with trajectory-level optimization, OVD significantly reduces memory consumption and enables black-box distillation. Theoretical analysis shows that OVD provides unbiased gradient estimates with reduced variance and guarantees convergence under the mixture training distribution. 9 OVD: On-policy Verbal Distillation 6. Impact Statement This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Stanczyk, P., Ramos, S., Geist, M., and Bachem, O. Gkd: Generalized knowledge distillation for auto-regressive sequence models. CoRR, 2023. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, Ustun, A., and Hooker, S. Back J., Pietquin, O., to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. AI-MO. Aimo validation aime dataset. Dataset, n.d. Aksitov, R., Miryoosefi, S., Li, Z., Li, D., Babayan, S., Kopparapu, K., Fisher, Z., Guo, R., Prakash, S., Srinivasan, P., et al. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023. Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Chung, W., Ghiassian, S., Nath, S., and White, M. Rejection sampling for off-policy learning. In NeurIPS 2018 Workshop on Continual Learning, 2018. URL https://marcpickett.com/ cl2018/CL-2018_paper_71.pdf. College Board. Sat https://collegereadiness.collegeboard.org/sat/practice. practice tests, 2016. Cover, T. M. Elements of information theory. John Wiley & Sons, 1999. 10 Cui, G., Yuan, L., Wang, Z., Wang, H., Zhang, Y., Chen, J., Li, W., He, B., Fan, Y., Yu, T., et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Fang, R., Cai, S., Li, B., Wu, J., Li, G., Yin, W., Wang, X., Wang, X., Su, L., Zhang, Z., et al. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311, 2025. Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pp. 1042110430. PMLR, 2023. Gao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Gray, R. M. and Neuhoff, D. L. Quantization. IEEE transactions on information theory, 44(6):23252383, 2002. Gu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Guo, J., Yang, L., Chen, P., Xiao, Q., Wang, Y., Juan, X., Qiu, J., Shen, K., and Wang, M. Genenv: Difficultyaligned co-evolution between llm agents and environment simulators. arXiv preprint arXiv:2512.19682, 2025b. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hinton, G., Vinyals, O., and Dean, J. the knowledge in neural network. arXiv:1503.02531, 2015. Distilling arXiv preprint Ho, J. and Ermon, S. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016. OVD: On-policy Verbal Distillation Ho, N., Schmid, L., and Yun, S.-Y. Large language models are reasoning teachers. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), pp. 1485214882, 2023. Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Hong, J., Lee, N., and Thorne, J. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024. Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 80038017, 2023. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efficient integerarithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge arXiv preprint dataset for reading comprehension. arXiv:1705.03551, 2017. Jung, S., Yoon, S., Kim, D., and Lee, H. Todi: Token-wise distillation via fine-grained divergence control. arXiv preprint arXiv:2505.16297, 2025. Kim, Y. and Rush, A. M. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. 13171327, 2016. Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. Mawps: math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pp. 11521157, 2016. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Langley, P. Crafting papers on machine learning. In ICML, pp. 12071216, 2000. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. Lin, A., Wohlwend, J., Chen, H., and Lei, T. Autoregressive knowledge distillation through imitation learning. arXiv preprint arXiv:2009.07253, 2020. Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J., and Liu, J. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023. Lloyd, S. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129137, 1982. Lu, P., Qiu, L., Chang, K.-W., Wu, Y. N., Zhu, S.-C., Rajpurohit, T., Clark, P., and Kalyan, A. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022. Lu, Z., Yang, Y., Ren, H., Hou, H., Xiao, H., Wang, K., Shi, W., Zhou, A., Zhan, M., and Li, H. Webgen-bench: Evaluating llms on generating interactive and functional websites from scratch. arXiv preprint arXiv:2505.03733, 2025. Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A. Teaching small language models to reason. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: short papers), pp. 17731781, 2023. Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, 2023. 11 OVD: On-policy Verbal Distillation Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198 124235, 2024. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Miao, S.-Y., Liang, C.-C., and Su, K.-Y. diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th annual meeting of the Association for Computational Linguistics, pp. 975984, 2020. Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Garcia, X., Liu, P. J., Harrison, J., Lee, J., Xu, K., et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Niu, L., Sun, H., Zhao, F., Chen, S., Bai, Z., Zhang, J., Yuan, C., and Wang, X. Cotd-po: Chain-of-thought distillation with preference optimization. In Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 1997519986, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 56875711, 2023. Spigler, G. Proximal policy distillation. arXiv preprint arXiv:2407.15134, 2024. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Sun, H., Qiao, Z., Guo, J., Fan, X., Hou, Y., Jiang, Y., Xie, P., Zhang, Y., Huang, F., and Zhou, J. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. Team, Q. Qwq-32b: Embracing the power of reinforcement learning, 2025. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Schlegel, M., Chung, W., Graves, D., Qian, J., and White, M. Importance resampling for off-policy prediction. Advances in Neural Information Processing Systems, 32, 2019. Wen, Y., Li, Z., Du, W., and Mou, L. F-divergence minimization for sequence-level knowledge distillation. arXiv preprint arXiv:2307.15190, 2023. West, P., Bhagavatula, C., Hessel, J., Hwang, J., Jiang, L., Le Bras, R., Lu, X., Welleck, S., and Choi, Y. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: Human language technologies, pp. 46024625, 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Wijmans, E., Huval, B., Hertzberg, A., Koltun, V., and Krahenbuhl, P. Cut your losses in large-vocabulary language models. arXiv preprint arXiv:2411.09009, 2024. 12 OVD: On-policy Verbal Distillation Wu, L., Li, J., Wang, Y., Meng, Q., Qin, T., Chen, W., Zhang, M., Liu, T.-Y., et al. R-drop: Regularized dropout for neural networks. Advances in neural information processing systems, 34:1089010905, 2021. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Wu, S., Chen, H., Quan, X., Wang, Q., and Wang, R. Ad-kd: Attribution-driven knowledge distillation for language model compression. arXiv preprint arXiv:2305.10010, 2023. Xiong, W., Yao, J., Xu, Y., Pang, B., Wang, L., Sahoo, D., Li, J., Jiang, N., Zhang, T., Xiong, C., et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Ye, T., Dong, L., Chi, Z., Wu, X., Huang, S., and Wei, F. Black-box on-policy distillation of large language models. arXiv preprint arXiv:2511.10643, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhang, J. and Cho, K. Query-efficient imitation learning for end-to-end autonomous driving. arXiv preprint arXiv:1605.06450, 2016. Zhang, X., Li, C., Zong, Y., Ying, Z., He, L., and Qiu, X. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. 13 OVD: On-policy Verbal Distillation A. Appendix A.1. Theoretical Proofs This section provides detailed proofs for the theoretical results presented in Section 3.5. A.1.1. PROOF OF PROPOSITION 3.3 (CONVERGENCE UNDER MIXTURE TRAINING) Proof. We prove that under the verbal rejection sampling regime, the student policy converges to teacher-level performance. The proof proceeds in three steps: (1) decompose the expected reward under the mixture distribution, (2) bound the quality of accepted student trajectories, and (3) show convergence as training progresses. Mixture Distribution Decomposition. Recall that the training distribution at iteration is: ptrain(y) = αt π(t) (y) 1[S(y) θ] + (1 αt) πT (y) (11) [a(y)] is the expected acceptance rate, and a(y) = min(1, exp(S(y)/β)/M ) is the verbal acceptance where αt = probability. yπ(t) The expected reward under ptrain can be decomposed as: Eyptrain[R(y)] = αt yπ(t) [R(y) 1[S(y) θ]] + (1 αt) EyπT [R(y)] = αt yπ(t) [R(y) S(y) θ] + (1 αt) J(πT ) (12) (13) Quality Bound for Accepted Trajectories. Let δt 0 denote the performance gap between accepted student trajectories and teacher trajectories: J(πT ) δt := [R(y) S(y) θ] yπ(t) J(πT ) (14) By the definition of verbal rejection sampling, trajectories with S(y) θ are selected precisely because the teacher assigns them high scores, indicating quality approaching teacher-level performance. Therefore: yπ(t) [R(y) S(y) θ] = (1 δt) J(πT ) Substituting into the decomposition from Step 1: Eyptrain [R(y)] = αt (1 δt) J(πT ) + (1 αt) J(πT ) = [1 αtδt] J(πT ) (1 αtδt) J(πT ) (15) (16) (17) (18) Convergence Analysis. As training progresses, two key properties hold: Property 1: Increasing acceptance rate. As the student policy π(t) distribution, it generates higher-quality trajectories, leading to: improves through policy gradient updates on the mixture αt = yπ(t) [a(y)] as (19) Property 2: Decreasing quality gap. The performance gap δt between accepted student trajectories and teacher trajectories decreases monotonically because: δt = J(πT ) yπ(t) [R(y) S(y) θ] J(πT ) 0 as Combining these properties, we have: lim Eyptrain[R(y)] = lim [1 αtδt] J(πT ) = J(πT ) 14 (20) (21) Since the student policy is trained to maximize Eyptrain [R(y)], this implies: OVD: On-policy Verbal Distillation J(π(t) ) J(πT ) as (22) This completes the proof. Remark 1 (Curriculum Learning Interpretation). The convergence result reveals that verbal rejection sampling induces natural curriculum learning schedule. Initially, when αt 0, the student relies almost entirely on teacher demonstrations, ensuring stable learning from high-quality trajectories. As training progresses, the increasing acceptance rate αt 1 gradually shifts the distribution toward the students own policy, enabling autonomous exploration and refinement. This adaptive schedule is determined automatically by the verbal scores S(y), without manual tuning of curriculum parameters. Remark 2 (Convergence Rate). The bound Eyptrain [R(y)] [1 αtδt] J(πT ) shows that the performance gap is controlled by the product αtδt. Since both terms converge to limiting values (αt 1 and δt 0), the convergence is asymptotically guaranteed. In practice, convergence speed depends on the temperature parameter β in the acceptance probability, which controls how aggressively low-quality trajectories are rejected. A.1.2. PROOF OF THEOREM 3.1 (UNBIASED GRADIENT ESTIMATION) Proof. We prove that the verbal on-policy rejection sampling procedure yields an unbiased gradient estimator for the policy objective. The proof proceeds in three steps: (1) establish validity of the acceptance probability, (2) derive the gradient estimator under the rejection sampling distribution, and (3) verify unbiasedness. Validity of Acceptance Decision. The acceptance decision is based on simple threshold comparison: a(y) = 1[S(y) θ], where S(y) {0, 1, . . . , 9} is the verbal score and θ {0, 1, . . . , 9} is the threshold parameter. This indicator function yields: a(y) = (cid:40) 1 0 if S(y) θ (trajectory accepted) if S(y) < θ (trajectory rejected) The expected acceptance rate at iteration is: αt = yπ(t) [1[S(y) θ]] = Pr yπ(t) (S(y) θ) (23) (24) This is simply the probability that student trajectory receives score at or above the threshold. Gradient Estimator Derivation. Under the threshold-based verbal rejection sampling protocol, trajectory sampled from πS is accepted if S(y) θ and rejected otherwise, with rejected trajectories replaced by teacher demonstrations πT . Let αt = EyπS [1[S(y) θ]] denote the expected acceptance rate. The effective distribution over training trajectories is: pRS(y) = πS(y) 1[S(y) θ] + (1 αt) πT (y) The gradient estimator under rejection sampling is: θJ(πS) = EypRS[R(y) θ log πS(y)] = = (cid:90) (cid:90) pRS(y) R(y) θ log πS(y) dy [πS(y) 1[S(y) θ] + (1 αt) πT (y)] R(y) θ log πS(y) dy = EyπS [1[S(y) θ] R(y) θ log πS(y)] + (1 αt) EyπT [R(y) θ log πS(y)] Unbiasedness Verification. Define the target distribution as the mixture: p(y) := αt πaccept (y) + (1 αt) πT (y) 15 (25) (26) (27) (28) (29) (30) OVD: On-policy Verbal Distillation where πaccept (y) = πS(y) 1[S(y) θ]/αt is the distribution of accepted student trajectories (normalized). The policy objective under this target distribution is: J(πS) = Eyp [R(y)] By the policy gradient theorem: θJ(πS) = Eyp [R(y) θ log πS(y)] = αt yπaccept = αt EyπS [R(y) θ log πS(y)] + (1 αt) EyπT [R(y) θ log πS(y)] (cid:21) R(y) θ log πS(y) + (1 αt) EyπT [R(y) θ log πS(y)] (cid:20) 1[S(y) θ] αt = EyπS [1[S(y) θ] R(y) θ log πS(y)] + (1 αt) EyπT [R(y) θ log πS(y)] (31) (32) (33) (34) (35) This matches the gradient estimator derived in Step 2, confirming that the estimator is unbiased for the mixture distribution objective. Remark 3 (Comparison with Importance Sampling and Off-Policy Rejection Sampling). Unlike standard importance sampling methods (Schlegel et al., 2019) that require explicit density ratios πT (y)/πS(y) (which necessitate access to teacher logits), verbal rejection sampling uses the teachers quality assessment S(y) as binary decision criterion. This enables distillation from black-box models where πT (y) is inaccessible. Classical rejection sampling for off-policy learning (Chung et al., 2018) similarly uses acceptance/rejection decisions, but relies on importance weights derived from policy ratios πT (y)/πS(y) that require full policy distributions. Our method differs in two key aspects: (i) we operate on-policy, sampling trajectories directly from πS rather than behavior policy, eliminating distribution mismatch; (ii) we use verbal scores S(y) as quality proxy instead of explicit density ratios, enabling black-box distillation. The threshold-based acceptance 1[S(y) θ] provides simple yet effective mechanism for trajectory selection without requiring continuous probability distributions or policy access. Remark 4 (Distillation Mechanism). The two-term structure of the gradient estimator EyπS [1[S(y) θ] R(y)θ log πS(y)] + (1 αt)EyπT [R(y)θ log πS(y)] reflects the distillation mechanism. The first term enables learning from the students successful explorations (weighted by acceptance probability), while the second term provides safety net through teacher demonstrations when student samples are insufficient. This ensures unbiased gradient signals even when the student policy is far from optimal, gracefully interpolating between imitation learning (αt 0) and autonomous learning (αt 1). A.1.3. PROOF OF PROPOSITION 3.2 (VARIANCE REDUCTION) Proof. We prove that verbal rejection sampling reduces the variance of gradient estimates compared to vanilla on-policy learning. The proof compares the variance of two estimators and establishes lower bound on the variance reduction. Variance of Vanilla On-Policy Estimator. The vanilla on-policy gradient estimator is: ˆg0 = R(y) θ log πS(y), where πS Its variance is: V[ˆg0] = EyπS (cid:2)R(y) θ log πS(y)2(cid:3) EyπS [R(y) θ log πS(y)]2 Variance of Rejection Sampling Estimator. The threshold-based rejection sampling gradient estimator is: (cid:40) ˆgRS = R(y) θ log πS(y) R(y) θ log πS(y) if S(y) θ (with probability αt) if S(y) < θ (with probability 1 αt) (36) (37) (38) where πS, πT , and αt = Pr(S(y) θ). 16 Expanding the variance: OVD: On-policy Verbal Distillation V[ˆgRS] = (cid:2)ˆgRS2(cid:3) E[ˆgRS]2 = αt EyπS (cid:2)R(y)θ log πS(y)2 S(y) θ(cid:3) + (1 αt) EyπT (cid:2)R(y)θ log πS(y)2(cid:3) E[ˆgRS]2 (39) (40) Variance Comparison. The key insight is that rejected trajectories (with S(y) < θ) are replaced by teacher demonstrations, which typically have lower variance due to consistent high quality. Decomposing the vanilla estimator variance by acceptance status: V[ˆg0] = EyπS (cid:2)R(y)θ log πS(y)2(cid:3) θJ02 = αt EyπS (cid:2)R(y)θ log πS(y)2 S(y) θ(cid:3) + (1 αt) EyπS (cid:2)R(y)θ log πS(y)2 S(y) < θ(cid:3) θJ0 (41) (42) The variance reduction comes from replacing the second term (rejected trajectories) with teacher demonstrations. Assuming teacher variance EyπT [R(y)θ log πS(y)2] VT is bounded, and that rejected trajectories have higher variance than accepted ones, we obtain: V[ˆg0] V[ˆgRS] (1 αt) (cid:2)EyπS (cid:2)R(y)θ log πS(y)2 S(y) < θ(cid:3) VT (cid:3) (cid:2)1[S(y) < θ] R(y)θ log πS(y)2(cid:3) (1 αt) VT = EyπS (43) (44) Since rejected trajectories (low scores) typically have poor rewards and high variance, the first term dominates, yielding: V[ˆgRS] V[ˆg0] EyπS (cid:2)1[S(y) < θ]R(y)θ log πS(y)2(cid:3) + O(VT ) (45) This completes the proof. Remark 5 (Variance Reduction Mechanism). The bound V[ˆgRS] V[ˆg0] E[1[S(y) < θ]R(y)θ log πS(y)2] reveals that variance reduction comes entirely from replacing rejected trajectories (those scoring below threshold θ). The thresholdbased mechanism provides clean separation: trajectories passing the quality bar are used as-is, while those falling short are replaced with teacher demonstrations. This binary decision is self-regulating: it provides maximum stabilization during early training when many trajectories are rejected, then gracefully reduces intervention as the student improves and αt 1. Remark 6 (Practical Implications). In standard on-policy RL, low-reward trajectories introduce high-variance noise that can destabilize training. Verbal rejection sampling mitigates this by replacing such trajectories with consistently high-quality teacher demonstrations. The result is more stable gradient estimates and faster convergence, particularly in the early training phase when the student policy is weak and exploration is noisy. As 1 during later training, the method naturally transitions to standard on-policy learning, preserving exploration benefits while maintaining the variance reduction gains from earlier stages. A.1.4. PROOF OF PROPOSITION 3.4 (SCORE GRANULARITY AND APPROXIMATION QUALITY) Proof. We prove that the approximation error of discretizing continuous quality values into discrete scores decreases as O(1/v). Our analysis follows the classical theory of uniform scalar quantization from information theory (Gray & Neuhoff, 2002). Setup. Assume the true trajectory quality Q(y) is normalized to [0, 1]. We apply uniform quantization with levels, mapping Q(y) to an integer score: Sv(y) = (v 1) Q(y) {0, 1, . . . , 1} (46) The normalized discrete score is Sv(y) = Sv(y)/(v 1) {0, estimate. 1 v1 , 2 v1 , . . . , 1}, which reconstructs the quantized quality Pointwise Error Bound. For any trajectory y, the quantization error is: OVD: On-policy Verbal Distillation (cid:12) (cid:12) (cid:12) (cid:12) Q(y) Sv(y) 1 (cid:12) (cid:12) (cid:12) (cid:12) = = (cid:12) (cid:12) (cid:12) (cid:12) Q(y) (v 1) Q(y) (cid:12) (cid:12) (cid:12) (cid:12) (v 1) Q(y) (v 1) Q(y) 1 1 1 1 (47) (48) (49) The last inequality holds because the fractional part of any real number satisfies x < 1. Expected Error Bound. Since the pointwise error is uniformly bounded, the expected error over all trajectories satisfies: Ey (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) Q(y) Sv(y) (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) Ey (cid:21) (cid:20) 1 1 = 1 1 (50) For tighter bound, we invoke the uniform quantization error distribution theorem. The quantization step size is = 1 v1 . For uniform quantization of continuous variable [0, 1], the quantization error = Sv within each quantization interval is uniformly distributed over [0, ] (Gray & Neuhoff, 2002). The expected absolute error within each interval [ v1 , i+ v1 ] is: E[e [i/(v 1), (i + 1)/(v 1)]] = (cid:90) 0 1 dx = 2 = 1 2(v 1) Averaging over all intervals (assuming Q(y) has bounded density): Ey (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) Q(y) Sv(y) 1 (cid:12) (cid:21) (cid:12) (cid:12) (cid:12) = 1 2(v 1) = (cid:19) (cid:18) 1 (51) (52) This result is direct application of the mean absolute quantization error for uniform quantizers, fundamental result in rate-distortion theory. Implications for Rejection Sampling. finer-grained verbal vocabulary enables more precise trajectory selection. With larger v, the threshold θ can be set more precisely to separate highand low-quality trajectories, as the approximation error in quality assessment decreases as 1/v. This improved granularity leads to better alignment between the acceptance decision 1[Sv(y) θ] based on verbal scores and the ideal decision 1[Q(y) q] based on true quality, ultimately resulting in more effective rejection sampling. This completes the proof. Remark 7 (Practical Choice of v). While theory suggests larger improves precision, practical considerations limit the vocabulary size. First, very fine-grained scores (e.g., = 100) may exceed the teachers ability to consistently distinguish quality levels, as even sophisticated models struggle to calibrate distributions over excessively many score tokens. Second, with levels, each score requires sufficient samples for the teacher to calibrate its distribution, making sample efficiency concern for large v. Third, smaller (e.g., = 10) provides good balance between expressiveness and interpretability, as human evaluators and practitioners can more easily understand and validate the scoring behavior. In practice, we find = 10 (scores 0-9) provides sufficient granularity while remaining interpretable and well-calibrated. Remark 8 (Connection to Classical Results). The O(1/v) approximation bound connects our verbal rejection sampling to well-established quantization theory. In Lloyd-Max quantization (Lloyd, 1982), optimal (non-uniform) quantizers achieve mean squared error scaling as O(1/v2) for smooth distributions, whereas our uniform quantization achieves O(1/v) for mean absolute error. This result aligns with rate-distortion theory (Cover, 1999), where the fundamental trade-off between (number of bits needed: log2 v) and distortion (approximation error) is central to lossy compression. Similar discretization analysis appears in weight quantization for neural networks (Jacob et al., 2018), where precision-efficiency trade-offs are critical for deployment. Our result shows that verbal scores inherit favorable properties from classical quantization theory, providing theoretical grounding for the discrete score design. 18 OVD: On-policy Verbal Distillation Figure 5. Reward convergence on Qwen-2.5-7B across training steps under different rejection thresholds Figure 6. Reward convergence on LLaMA-3.2-3B across training steps under different rejection thresholds A.1.5. ADDITIONAL THEORETICAL REMARKS Connection to Importance Sampling. The acceptance probability a(y) can be interpreted as soft importance weight. In standard importance sampling, we would use w(y) = πT (y)/πS(y), but this requires access to πT (y) which is unavailable for black-box teachers. Our verbal scoring approach provides proxy for this ratio through the teachers quality assessment. Convergence Rate. Under standard assumptions (bounded rewards, Lipschitz policy gradients), the convergence rate of OVD is O(1/ ) where is the number of iterations. The constant factor improves with higher acceptance rates, as more on-policy samples are retained. A.2. RL Scaling Random Sample Example We provide representative example randomly sampled from the Omni-MATH benchmark (Gao et al., 2024) to illustrate our RL scaling experiments. Notably, our scaling results are obtained by repeatedly sampling and training on this single problem instance, demonstrating the effectiveness of our approach without requiring extensive datasets: RL Scaling Samples (Randomly Sampled from Training Dataset) Problem 1 : The lock opens only if specific three-digit number is entered. An attempt consists of randomly selecting three digits from given set of five. The code was guessed correctly only on the last of all attempts. How many attempts preceded the successful one? Lets think step by step and output the final answer within . Answer: Problem 2 : For the ellipse 25x2 100x + 4y2 + 8y + 16 = 0, find the distance between the foci. Lets think step by step and output the final answer within . Answer: 2 462 5 A.3. Reward Convergence Across Training Steps We analyze the reward convergence behavior of OVD across different training steps under various rejection threshold configurations. Figure 5 and Figure 6 illustrate the reward trajectories for Qwen-2.5-7B and LLaMA-3.2-3B models respectively, demonstrating that appropriate threshold settings lead to faster convergence and higher final rewards. The results show that rejection thresholds significantly impact learning dynamics, with moderate thresholds (5-7) typically achieving the best balance between exploration and exploitation. A.4. Verbal Scoring Prompt We present the detailed prompt template used for verbal scoring in OVD. The prompt instructs the teacher model to evaluate the quality of each reasoning step on discrete 1-10 scale, providing structured feedback without requiring access to 19 OVD: On-policy Verbal Distillation token-level probability distributions. This design enables step-level quality assessment while maintaining compatibility with black-box teacher models. The scoring rubric divides the scale into four meaningful ranges: incorrect/irrelevant (1-3), partially correct (4-6), correct and well-reasoned (7-9), and optimal (10), allowing the teacher to express nuanced judgments about reasoning quality. By outputting only single score token, this approach dramatically reduces memory overhead compared to full-vocabulary logit storage, as analyzed in Section 2. Verbal Scoring Prompt Problem: Previous Steps: s1, s2, . . . , sk1 Current Step: sk Rate the quality of the current reasoning step on scale of 1-10, where: 1-3: Incorrect or irrelevant 4-6: Partially correct but incomplete 7-9: Correct and well-reasoned 10: Optimal step toward the solution Output only single number from 1 to 10: A.5. Complete Algorithm We provide the complete OVD training algorithm with all implementation details. Algorithm 1 presents the unified framework applicable to both mathematical reasoning and Web Q&A tasks. Trajectory Generation for Different Tasks. The algorithm adapts to different task types through trajectory structure: (i) For mathematical reasoning tasks, trajectory = (s1, s2, . . . , sK) consists of sequential reasoning steps leading to final answer, where each step sk represents logical derivation or calculation. (ii) For Web Q&A tasks, trajectories include search interactions: = (q1, d1, s1, q2, d2, s2, . . . , qK, dK, sK), where qk denotes search query, dk represents retrieved documents, and sk is the reasoning step based on the retrieved information. The student model learns to generate both effective search queries and reasoning steps that leverage the retrieved context, while the teacher provides verbal scores evaluating the quality of each search-reasoning iteration. Algorithm 1 On-policy Verbal Distillation (OVD) Input: Teacher πT , Student πS, Dataset D, Samples per problem , Score threshold θ, Clipping threshold ϵc for each training iteration do i=1 from Sample batch {xi}B for each problem xi do for = 1 to do Generate τ (j) if S(τ (j) Replace τ (j) end if end for Compute advantages: A(τ (j) πS(xi); Query score S(τ (j) ) < θ or incorrect then ); Compute reward R(τ (j) ) with teacher trajectory πT (xi) and recompute reward end for Update πS using clipped policy gradient with importance ratios ρ(j) ) = (R(τ (j) ) µi)/(σi + ϵ) end for Output: Trained student model πS 20 OVD: On-policy Verbal Distillation A.6. Hyperparameters and Implementation Details A.6.1. IMPLEMENTATION DETAILS We provide detailed hyperparameter configurations and implementation details to ensure the reproducibility of our experiments. Computational Infrastructure. All experiments are conducted on high-performance computing cluster equipped with 8 AMD Instinct MI210 accelerators, each featuring 64 GB of HBM2e memory. The GPUs are based on the CDNA 2 microarchitecture (gfx90a). Distributed training is performed using 4 GPUs per training instance. GPU acceleration is supported via the ROCm platform. Environment Simulation Infrastructure. The simulated search environment is deployed on NVIDIA A100 GPUs to support low-latency inference during training. 7B-parameter model (sunhaonlp/Simulation LLM google 7B V2) 14B-parameter model is (sunhaonlp/Simulation LLM google 14B V2) is distributed across two NVIDIA A100 GPUs using tensor parallelism to enable scalable inference with moderate latency. single NVIDIA A100 GPU with"
        },
        {
            "title": "40 GB memory.",
            "content": "deployed on Baselines To evaluate the effectiveness of OVD, we compare our method with the following baselines. (i) Vanilla Prompting Methods: This category includes Direct Answer, Chain-of-Thought (CoT) (Wei et al., 2022), and standard Retrieval-Augmented Generation (RAG) (Lewis et al., 2020). Direct Answer generates responses without explicit reasoning, CoT encourages step-by-step reasoning, and RAG retrieves relevant documents before answering. (ii) Advanced Search Methods: We consider RA-Agent and Search-o1 (Li et al., 2025), which iteratively search for relevant information and perform multi-step reasoning with retrieved context. (iii) RL-Based Reasoning Methods: This category includes R1 (Guo et al., 2025a), Search-R1 (Jin et al., 2025), and ZeroSearch (Sun et al., 2025). In R1, the policy model is trained to perform in-depth reasoning based solely on its internal knowledge. Search-R1 enables the policy model to interact with real search engine during inference. ZeroSearch learns search-like reasoning without actual retrieval through simulated search environments. A.6.2. HYPERPARAMETERS We employ different hyperparameters for trajectory scoring depending on the task type. Mathematical Reasoning. For math tasks, we use QwQ-32B (Team, 2025) as the teacher model. The teacher evaluates reasoning trajectories by computing verbal scores on 0-9 scale based on the prompt template described in Appendix A.4. Temperature is set to 0.7 for score generation to allow calibrated probability distributions over the score vocabulary. The student models are initialized from pre-trained base models without instruction tuning, with Qwen2.5-Math-1.5B used for mathematical tasks. We train the Generalized Reward Preference Optimization (GRPO) framework with carefully selected hyperparameters to ensure stable and effective optimization under long-horizon rollouts. We adopt Qwen2.5-Math-1.5B as the base policy model for mathematical reasoning tasks. The training batch size is set to 128, with dynamic batch sizing enabled to accommodate variable-length responses. The maximum prompt length is 1,024 tokens, and the maximum response length is capped at 3,072 tokens, allowing sufficient capacity for long-form reasoning trajectories. We use constant learning rate of 1 106 for policy optimization. For each prompt, rollout is performed by sampling 8 candidate responses with temperature of 0.6, and rollouts are generated using vLLM-based inference engine with tensor parallel size of 2. To stabilize training and prevent policy drift, we apply KL-divergence regularization at both the actor and algorithm levels, using low-variance KL formulation with coefficient of 0.001, together with an additional KL control term using the same coefficient. Web Q&A. For web search tasks, we use an SFT-based environment agent trained following ZeroSearch (Sun et al., 2025). The environment agent is initialized from Qwen2.5-7B-Base or Qwen2.5-14B-Base and fine-tuned on simulated search data to mimic search engine outputs. During training, the agent provides step-level verbal scores evaluating both query quality and reasoning coherence. The rejection threshold during teacher trajectory generation is set to 7. For experiments marked with 14B in Table 3, we use the 14B-based environment agent. For Web Q&A tasks, the student models are initialized from OVD: On-policy Verbal Distillation the pre-trained base models Qwen2.5-3B-Base and LLaMA-3.2-3B-Base without instruction tuning. We train the Generalized Reward Preference Optimization (GRPO) framework with carefully selected hyperparameters to ensure stable and effective optimization. We adopt Llama-3.2-3B (meta-llama/Llama-3.2-3B) and Qwen-2.5-3B (Qwen/Qwen-2.5-3B) as the base policy models. Training is conducted for total of 203 optimization steps. The training batch size is set to 64, while the validation batch size is 512. The maximum prompt length is 4,096 tokens, and the maximum response length is capped at 500 tokens, allowing sufficient capacity for both mathematical reasoning and web-based question answering tasks. We use learning rate of 1 106 with linear warmup schedule covering 95% of the total training steps. Policy optimization is performed within the GRPO framework, using mini-batch size of 32 and micro-batch size of 16. To stabilize training and prevent policy drift, we apply KL-divergence regularization with coefficient of 0.001, using low-variance KL formulation. B. Related Work This section reviews reinforcement learning methods for improving language models, covering preference optimization, inducing reasoning abilities, and agent training. B.1. Post-training Methods for Language Models Proximal Policy Optimization. Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) has become standard approach for aligning LLMs with human preferences. Stiennon et al. (2020) first demonstrated the effectiveness of training reward models from human feedback for open-ended language generation tasks, which was later scaled to instruction-following settings by Ouyang et al. (2022). The RLHF paradigm typically involves training reward model on human preference data and then fine-tuning the LLM using policy gradient methods, among which Proximal Policy Optimization (PPO) (Schulman et al., 2017) has been most widely adopted due to its stability and sample efficiency. Ziegler et al. (2019) first applied PPO to fine-tune language models from human preferences, establishing the foundation for modern RLHF pipelines. More recently, Ahmadian et al. (2024) revisited vanilla policy gradient methods, showing that they can achieve competitive performance while reducing implementation complexity. Despite its effectiveness, PPO-based training requires maintaining multiple models and is computationally expensive, often requiring thousands of GPU hours. Direct Preference Optimization. Direct Preference Optimization (DPO) (Rafailov et al., 2023) reformulates the RLHF objective to directly optimize the policy without explicit reward modeling, significantly simplifying the training pipeline. Subsequent work has proposed various improvements: IPO (Azar et al., 2024) provides general theoretical framework addressing overfitting issues in DPO, ORPO (Hong et al., 2024) eliminates the need for reference model through odds ratio-based optimization, and SimPO (Meng et al., 2024) further simplifies the objective with reference-free reward formulation. Reasoning via Reinforcement Learning. Recent work demonstrates that reinforcement learning (RL) and environment interaction provide effective supervision for enhancing reasoning and agentic capabilities of LLMs. Existing methods can be broadly categorized by their supervision source: (1) Self-Training RL bootstraps reasoning from model-generated rationales, including iterative refinement in STaR (Zelikman et al., 2022), EM-style optimization in ReST (Aksitov et al., 2023), and large-scale self-training beyond human data for mathematical reasoning (Singh et al., 2023); (2) Policy Optimization directly optimizes reasoning behaviors with RL objectives, where GRPO substantially improves mathematical reasoning (Shao et al., 2024), pure RL induces emergent step-by-step reasoning transferable via distillation (Guo et al., 2025a), and recent variants enhance training stability and implicit process supervision (Yu et al., 2025; Cui et al., 2025); (3) Environment-Grounded Agents learn through interaction with external or simulated environments, enabling search-augmented reasoning (Jin et al., 2025), simulated retrieval behaviors (Sun et al., 2025), and improved generalization via environment scaling (Fang et al., 2025). Effectively leveraging environment-derived supervision remains an open problem. B.2. Knowledge Distillation for Large Language Models Knowledge distillation (Hinton et al., 2015) is core technique for compressing large models into smaller ones, and LLM distillation methods can be broadly categorized by training paradigm and supervision signal. Token-Level Distillation. Token-level methods provide dense supervision at each decoding step, offering richer gradient 22 OVD: On-policy Verbal Distillation signals than sequence-level approaches. These methods can be categorized by their divergence objectives: (1) Forward KL minimizes KL(pT pS), encouraging mode-covering behavior where the student covers all teacher modes (Wu et al., 2021); (2) Reverse KL minimizes KL(pSpT ), promoting mode-seeking behavior that prevents the student from assigning probability mass to unlikely regions (Gu et al., 2023); (3) Adaptive schemes dynamically adjust the distillation objective based on token importance, using fine-grained divergence control (Jung et al., 2025) or attribution-based token selection (Wu et al., 2023). Sequence-Level Distillation. Sequence-level methods supervise the student using trajectory generated by the teacher, providing global and structured learning signals. These approaches can be grouped by the form of sequence supervision: (1) Output Imitation directly trains the student on teacher-generated sequences (Kim & Rush, 2016), with extensions based on imitation learning (Lin et al., 2020), symbolic supervision (West et al., 2022), and unified f-divergence formulations (Wen et al., 2023); (2) Trajectory Traces leverage intermediate reasoning steps as supervision. Inspired by Chain-of-Thought prompting (Wei et al., 2022), teachers generate step-by-step rationales that are distilled into smaller students (Ho et al., 2023; Magister et al., 2023; Fu et al., 2023), enabling effective transfer of multi-step reasoning ability; (3) Policy Distillation integrates sequence-level reasoning supervision with policy optimization, such as Proximal Policy Distillation (Spigler, 2024) and CoTD-PO (Niu et al., 2025), allowing active exploration and online updates. Notably, distilling structured rationales can even enable smaller models to outperform larger ones under limited data (Hsieh et al., 2023). On-Policy vs Off-Policy Distillation. Off-policy distillation pre-generates training data from teacher models, but suffers from distribution mismatch. On-policy distillation (Agarwal et al., 2024) addresses this by having the student generate its own samples during training. GKD (Agarwal et al., 2023) further unifies these approaches by interpolating between on-policy and off-policy sampling strategies. More recently, Ye et al. (2025) proposed black-box on-policy distillation, which extends on-policy learning to settings where the teacher models internal probabilities are inaccessible. However, their approach still requires discriminative model to match the distribution. To address this limitation, our work incorporates verbal scoring from environments agent, eliminating the need for token-level probability distillation while providing dense step-level supervision for reasoning tasks."
        }
    ],
    "affiliations": [
        "Huawei Technologies, China",
        "Nanjing University, Nanjing, China",
        "The University of Hong Kong, Hong Kong, China"
    ]
}