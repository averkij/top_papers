{
    "paper_title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
    "authors": [
        "Kshitij Ambilduke",
        "Ben Peters",
        "Sonal Sannigrahi",
        "Anil Keshwani",
        "Tsz Kin Lam",
        "Bruno Martins",
        "Marcely Zanon Boito",
        "André F. T. Martins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community."
        },
        {
            "title": "Start",
            "content": "From TOWER to SPIRE: Adding the Speech Modality to Text-Only LLM Kshitij Ambilduke*1, Ben Peters2, Sonal Sannigrahi2,3, Anil Keshwani4, Tsz Kin Lam5, Bruno Martins3,6, Marcely Zanon Boito7, André F.T. Martins2,3,8,9 1Paris-Saclay University 2Instituto de Telecomunicações 3Instituto Superior Técnico, Universidade de Lisboa 4Sapienza University of Rome 5University of Edinburgh 6INESC-ID 7NAVER LABS Europe 8ELLIS Unit Lisbon 9Unbabel Correspondence: benzurdopeters@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multimodality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWERs original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable success across various text-based natural language processing tasks (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2024; Yang et al., 2024; Alves et al., 2024; Martins et al., 2024), motivating research into extending them to other modalities. This has led to the development of multimodal LLMs (MLLMs) capable of processing speech, audio, images and video (Team et al., 2023; Driess et al., 2023; Rubenstein et al., 2023; Liu et al., 2023; Tang et al., 2023; Défossez et al., 2024; Hu et al., 2024; Laurençon et al., 2024; Huang et al., 2024; Nguyen et al., 2025). For speech-LLM integration, an effortless approach is to link the output of an automatic speech recognition (ASR) system to text-only LLM (Huang et al., 2024). However, this *Equal contribution. Work begun during an internship at Instituto de Telecomunicações. Equal contribution. solution fails to fully leverage the LLMs superior language modeling capabilities for disambiguating transcripts. More popular are solutions that investigate equipping LLMs with speech processing capabilities through modality projection (Shu et al., 2023; Radhakrishnan et al., 2023; Wu et al., 2023a; Tang et al., 2023; Xue et al., 2024; Hu et al., 2024). Typically, speech foundation model generates speech representations that are mapped to the embedding space of the LLM. The speech model is then fine-tuned along with projector on speech-totext tasks to equip the LLM with speech processing capabilities. In this setting, key challenges include prompt overfitting and high training costs, as tuning these MLLMs requires the adaptation of the speech projector module on vast amounts of raw speech data (Tang et al., 2023; Hu et al., 2024). An alternative approach for MLLMs is the use of speech discretization, where continuous speech features are transformed prior to training into sequences of discrete speech units (DSUs), which can be processed similarly to text (Chou et al., 2023a; Zhang et al., 2023; Rubenstein et al., 2023; Chang et al., 2024; Défossez et al., 2024; Trinh et al., 2024; Maiti et al., 2024; Nguyen et al., 2025). This approach simplifies training by eliminating the need for additional parameters beyond extended embedding matrices. Finally, while both projector-based and discretization-based MLLMs have shown promising results on text-to-speech and/or speech-to-text tasks, their development has prioritized speech-centric tasks at the expense of textual performance. Currently, limited research has focused on integrating speech while preserving the LLMs original capabilities in textual tasks (Chou et al., 2023b; Huang et al., 2024). In this work we present SPIRE, speechaugmented LLM built on top of the open multilingual model TOWER (Alves et al., 2024). SPIRE can process English speech and perform ASR 5 2 0 2 3 1 ] . [ 1 0 2 6 0 1 . 3 0 5 2 : r Figure 1: Illustration of the model training method for SPIREBASE and SPIREFULL. and speech translation (ST) while maintaining TOWERs strong performance on machine translation (MT). SPIRE encodes speech via HuBERTbased (Hsu et al., 2021) k-means clusterization, as in previous work (Zhang et al., 2023; Rubenstein et al., 2023; Chang et al., 2024). We perform training in two stages: continued pre-training (CPT) and instruction tuning (IT). For the CPT stage, we use mixture of ASR data and small fraction of TOWERs text CPT data. For IT, we again leverage TOWERs task-specific MT data, as well as additional English ASR and ST data. SPIRE is trained using approximately 42.5K hours of speech. Figure 1 illustrates our training process. We make the following contributions: We present pipeline for integrating speech as an additional modality into an existing LLM, enabling it to transcribe and translate English speech while preserving its original MT capabilities; We compare speech integration at two stages, namely CPT and IT, demonstrating that both stages are essential for achieving optimal performance on speech tasks; We reach ASR and ST results that are close to those of strong speech-centric models trained on larger amounts of dataWhisperlarge-v3 (Radford et al., 2023) and SeamlessM4T (Barrault et al., 2023)while outperforming SeamlessM4T on MT; We provide reproducible pipeline to the community: all our models, datasets and scripts are made available."
        },
        {
            "title": "2 Related Work",
            "content": "Speech-to-Text Models An increasing number of studies have explored integrating speech into 1https://github.com/utter-project/SpireLM LLMs (Zhang et al., 2023; Rubenstein et al., 2023; Hassid et al., 2024). For discrete speech input, Hassid et al. (2024) demonstrate the benefits of initializing speech LLM from text-based LLM. SpeechGPT (Zhang et al., 2023) applies IT on speech-to-text cross-modal ASR, text-tospeech (TTS), and text-based question answering. AudioPALM (Rubenstein et al., 2023) is trained in multi-task fashion, similarly to SpeechGPT, but on multilingual input. Recently, VoxtLM (Maiti et al., 2024) was trained jointly on DSUs and text data for ASR, TTS, and open-ended speech/text generation. Our work is most similar to that of Spirit-LM (Nguyen et al., 2025), which adapts an LLM with an interleaved mixture of DSU and text data which requires an expensive DSU-to-transcript step to create. In contrast, we adopt more costeffective input representation that can be extended to any language, regardless of the availability of speech aligner. Our focus is on successfully incorporating speech input while preserving the original competence of the model, so that the resulting model can successfully perform both speech-to-text and text-only tasks. None of the aforementioned models are trained to preserve the original models performance in text tasks. Adapting LLMs Previous approaches involve training from scratch with taskand domainspecific data (Singhal et al., 2023; Lewkowycz et al., 2022), performing CPT with diverse training data mix designed to broadly extend the models knowledge (Wu et al., 2023b), or doing IT use-case-specific data (Chen et al., 2023). Recent work has explored combining the latter two approaches (Xu et al., 2024a; Alves et al., 2024; Wei et al., 2021; Roziere et al., 2023). In our approach to integrating DSUs into TOWER, we take inspiration from Alves et al. (2024) in adopting two-step CPT+IT process. Our work differs in that we focus on adding the speech modality, whereas Alves et al. (2024) focused on increasing the multilingual capabilities of an LLM. Continuous and Discrete Speech Representations Self-supervised speech representation models produce contextualized high-dimensional speech vectors directly from raw audio (Hsu et al., 2021; Baevski et al., 2020; Chen et al., 2022), largely outperforming statistical speech features on downstream tasks (Yang et al., 2021). These continuous representations can be used to derive DSUs that capture both linguistic content and prosody through clustering (Borsos et al., 2023; Kharitonov et al., 2022). DSUs provide better alignment with textual data, facilitating the transfer of successful training settings from the text domain (Cui et al., 2024). Building on Lakhotia et al. (2021), which demonstrated that HuBERT (Hsu et al., 2021) is powerful feature extractor, several studies have adopted this approach, incorporating k-means clustering step for discretization (Zhang et al., 2023; Rubenstein et al., 2023; Lam et al., 2024; Chang et al., 2024; Nguyen et al., 2025). Xu et al. (2024b) study the optimal settings to obtain DSUs in terms of cluster size and feature extraction layer. We use their findings to inform our initial choices. Chang et al., 2024) to produce DSUs by first extracting continuous speech representations for our speech data from the 22nd layer of an English HuBERT-large model, and then using k-means clustering (K = 5000) to produce centroids that are used to convert our continuous speech representation into discrete sequence of cluster IDs.3 We train our k-means model on collection of 235K audio files (approximately 720 hours), drawn from three speech corpora: CoVoST-2 (Wang et al., 2021b), VoxPopuli (Wang et al., 2021a), and Multilingual Librispeech (MLS; Pratap et al., 2020). The CoVoST subset consists of 62K audio files from 10,049 speakers, with maximum of 8 audio files per speaker. The VoxPopuli subset includes 65K audio files from 639 speakers, capped at 250 audio files per speaker. Finally, the MLS subset contains 107K audio files from 5,490 speakers."
        },
        {
            "title": "3.2 SPIREBASE",
            "content": "SPIREBASE is trained from TOWERBASE-7B using both text-only and aligned speech-to-text datasets. Following previous work, we incorporate fraction of TOWERs original training data to preserve its original performance (Scialom et al., 2022; de Masson DAutume et al., 2019)."
        },
        {
            "title": "3.2.1 Data",
            "content": "Our goal is to equip an LLM with speech capabilities while preserving its preexisting text capabilities. As our starting point, we select TOWER (Alves et al., 2024) as our LLM, which was developed from Llama-2 (Touvron et al., 2023) with twostep approach: CPT on mixture of monolingual and parallel data (TOWERBASE), followed by IT on translation-related tasks (TOWERINSTRUCT). We use similar approach to extend TOWER to speech. First, we perform CPT with combination of text-only and aligned speech-to-text datasets, followed by IT using both text-only general-purpose and task-specific data curated in TOWERBLOCKS,2 alongside task-specific speech-to-text datasets. We name our model SPIRE."
        },
        {
            "title": "3.1 Speech Discretization",
            "content": "To more easily transfer the training set-up of TOWER, we use DSUs as opposed to an auxiliary speech encoder. For all speech datasets that were used, we follow recent discretization methodology (Zhang et al., 2023; Rubenstein et al., 2023; 2https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.2 We use mixture of monolingual and parallel text in Chinese (zh), Dutch (nl), English (en), French (fr), German (de), Italian (it), Korean (ko), Portuguese (pt), Russian (ru), and Spanish (es), that was sourced from the TOWER training data, as well as English ASR data sourced from popular opensource ASR datasets, as reported in Table 2. Both speech and text data are downsampled to create 6B token data mixture (5B speech; 1B text), measured by the model tokenizer.4 Note that the 5B speech tokens include both DSUs (4.4B tokens) and their text transcriptions (0.6B tokens). Text Data The monolingual text data split corresponds to data from mC4 (Raffel et al., 2019), multilingual web-crawled corpus which we uniformly sample from across all languages. The parallel data split includes uniformly sampled instances to and from English (enxx) for the 10 3Optimizing the layer selection for feature extraction is complex research problem (Pasad et al., 2023; Mousavi et al., 2024). In this work we follow the insights from Gow-Smith et al. (2023) and Xu et al. (2024b). 4Preliminary experiments on the data mixture led to this particular choice. languages, sourced from various public sources. Further details can be found in Alves et al. (2024). Speech Data We collect 35K hours of speech data from SPGI Speech (ONeill et al., 2021), GigaSpeech (Chen et al., 2021), MLS, and VoxPopuli. The transcription normalization process is explained in Appendix A.1."
        },
        {
            "title": "3.2.2 CPT Setup",
            "content": "We train SPIREBASE using the MegatronLLM codebase (Cano et al., 2023) on 8 A100-80GB GPUs for 6 days. We use the same hyperparameters as TOWER, except for the effective batch size, which in our case is 2,304. To incorporate the DSUs in the CPT stage, we extend the models original vocabulary by 5000 types, e.g., <extra_id_x>. This allows us to have vocabulary that can encode both text in subword units and speech in DSUs. For the extended vocabulary, we initialize new embeddings from multivariate Gaussian distribution. The mean of this distribution is set to the average of the original embeddings, while the covariance is derived from the empirical covariance of the original embeddings, scaled by factor of 1 105 (Hewitt, 2021)."
        },
        {
            "title": "3.3 SPIREFULL",
            "content": "The SPIREFULL model is obtained by performing instruction tuning on SPIREBASE using taskspecific text-only and aligned speech-to-text data."
        },
        {
            "title": "3.3.1 Data",
            "content": "We use mixture of text and speech instructions for ASR, MT, and ST. The prompt formats used during training are shown in Table 1. Text Data We use TOWERBLOCKS (Alves et al., 2024), which includes high quality translation bitexts between English and the other languages supported by TOWER. It also includes instructions for the translation-related tasks of named entity recognition and automatic post-editing. ASR Data We use 0.8K hours of ASR data from CommonVoice version 17 (CV; Ardila et al., 2020). The down-sampling strategy is described in Appendix A.1. ST Data In our IT set, we use 842 hours of speech across three ST training sets: FLEURS (all nine language pairs; we filter out examples whose transcriptions overlap with the FLORES devtest set), Europarl-ST (Iranzo-Sánchez et al., 2020) (en ASR (CPT) Speech:<extra_id_i> <extra_id_j> English: {TRANSCRIPT} MT (CPT) Source_lang: Source-sentence Target_lang: {TRANSLATION} ASR (IT) Speech: <extra_id_i> <extra_id_j> English: {TRANSCRIPT} Direct ST (IT) Speech: <extra_id_i> <extra_id_j> TARGET_LANG: {TRANSLATION} Multi-turn ST (IT) Speech: <extra_id_i> <extra_id_j> English:{TRANSCRIPT} TARGET_LANG: {TRANSLATION} Table 1: Prompt formats used at different training stages. {de, es, fr, it, nl, pt}), and CoVoST-2 (enzh). (cid:1) Since this amounts to far less data for ST than what is available for ASR, and since en{ko, ru} have only examples from the tiny FLEURS set, we augment our speech collection with pseudo-labeled data. This has been shown to be effective for other ST systems (Barrault et al., 2023). For the CV, SPGI, and GigaSpeech datasets, we select 300K ASR samples to be pseudo-labeled each. These examples are translated to all nine target languages using TowerInstruct-13B.5 Although this produces very large ST corpus, not all predictions are of high quality, so we filter out examples for which the transcript-translation combination has COMETQE6 (Rei et al., 2022b) score under 85. Finally, for each language pair, we sample 60K examples to be used in direct ST prompts and another 60K samples to be used in multi-turn prompts. This process results in 180K direct ST prompts and 180K multi-turn prompts for each language pair.7 3.3."
        },
        {
            "title": "IT Training Setup",
            "content": "Similar to TOWER, we use the chatml template (OpenAI, 2023) to format our instructions in dialogue form. We train models using Axolotl8 on 4 H100-80GB GPUs for 2.7 days. We use learning rate of 7 106 and cosine scheduler with 100 5https://huggingface.co/Unbabel/TowerInstruct-13B-v0.1 6https://huggingface.co/Unbabel/wmt22-cometkiwi-da 7Due to our aggressive filtering, we were left with slightly fewer examples for en zh. (cid:1) 8https://github.com/axolotl-ai-cloud/axolotl Phase # DSUs # Hours Model Base Model CPT Speech Text IT Speech Pseudo Text Dataset SPGI Speech Gigaspeech MLS VoxPopuli CV Europarl-ST FLEURS CoVoST-2 Task ASR ASR ASR ASR ASR ST ST ST SPGI Speech Pseudo-ST Pseudo-ST GigaSpeech Pseudo-ST CV CPT CPT CPT CPT IT IT IT IT IT IT IT 645M 1.2B 2.4B 69M 105M 122M 11M 12M 350M 161M 212M 5.1K 9.9K 19.2K 0.5K 0.8K 1.0K 0.09K 0.09K 2.8K 1.3K 1.7K Table 2: Statistics for the speech data used for training. Numbers of hours are approximated from the number of deduplicated DSUs. warm-up steps. We train for 4 epochs with an effective batch size of 576 and weight decay of 0.01. We impose maximum sequence length of 4096 and use the AdamW optimizer (Loshchilov and Hutter, 2019). Other hyperparameters are derived from TOWERINSTRUCT (Alves et al., 2024)."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our models across three tasks: ASR, MT, and ST. First, we present our results for ASR (4.1), confirming the competitive performance of SPIRE in the speech domain. We then present MT results (4.2), demonstrating that the speech performance does not come at the expense of the original models MT performance. Finally, we present results for ST (4.3) to investigate model performance on task that requires both ASR and MT capabilities. Our Models Across experiments, we compare all models shown in Table 3. These models vary in which base model they were trained on, whether they underwent speech-centric CPT, and on what set of instructions IT was performed. Apart from SPIRE models, we report few ablation studies in the IT stage where: i) no CPT was performed (TOWERFULL); ii) no data from TOWERBLOCKS was seen during IT (SPIRENOBLOCKS), and iii) pseudo-labeled ST data and FLEURS were omitted (SPIRENOPSEUDO). TOWERFULL SPIREBASE SPIREFULL TowerBase-7B SpireBase SpireBase SPIRENOBLOCKS SPIRENOPSEUDO SpireBase SpireBase SPIRE Variants Table 3: Our models and their variants, along with their base models. The CPT and IT columns indicate which data was seen during training. Evaluation Setup Across models and tasks, we perform inference with greedy decoding with maximum of 256 generated tokens. For the TOWER and SPIRE models, we decode with vllm. However, since vllm does not support all of our baselines, we use alternative libraries (namely, transformers) where necessary. Unless specified otherwise, we use zero-shot prompts for all models and tasks."
        },
        {
            "title": "4.1 ASR Experiments",
            "content": "Datasets and Metrics We evaluate ASR performance across multiple test sets, in order to cover variety of recording styles: Librispeech (LS) test-clean and test-other (Panayotov et al., 2015), FLEURS (Conneau et al., 2023), and VoxPopuli.9 We report the Word Error Rate (WER) between the hypotheses and gold transcripts, after Whisper normalization (Radford et al., 2023). Baselines We report results for the following models: Whisper (Radford et al., 2023) is an encoderdecoder transformer model trained on over 5 million hours of labeled data, it performs multilingual ASR and to-English ST. We report results for the 74M parameter Whisper-base and the 1.5B parameter Whisper-large-v3 versions. SeamlessM4T (Barrault et al., 2023) is an encoder-decoder transformer trained on 406K hours of speech that performs ASR, ST and MT across 100 languages. We report results for the 2.3B parameter SeamlessM4T-large-v2 version of the model. Spirit-LM (Nguyen et al., 2025) is the most similar work to ours. It is decoder-only model, trained from Llama-2 on 307B tokens of text, 458K hours of unlabeled speech, and When reporting results, we highlight when possible. toplines 9For CPT models, LS is an in-domain evaluation because its training set is part of MLS. LibriSpeech Clean Other FLEURS VoxPopuli 4.3 6.0* 2.6 5.0 1. 7.6 11.0* 4.9 11.9 3.7 11.4 - 8.1 12.1 5.8 14.7 - 7.5 9.8 9.2 Baselines HuBERT-large+CTC Spirit-LM SeamlessM4T Whisper-base Whisper-large-v3 Our models 40.7 TOWERFULL 15.8 SPIRENOBLOCKS 14.3 SPIRENOPSEUDO 13.7 SPIREBASE 15.8 SPIREFULL *We were unable to reproduce Spirit-LMs ASR performance; therefore, we report their self-reported LS results using ten-shot prompts. 13.8 7.4 7.3 56.3 7.1 14.3 10.4 11.1 11.0 10.7 9.5 4.1 3.9 28.9 4.2 Table 4: WER on various ASR test sets. 111K hours of labeled speech. Unfortunately, despite the availability of their inference code, we were unable to reproduce its reported performance on speech tasks. HuBERT-large+CTC is CTC-based ASR model trained using the same speech representation model we use for DSU generation, and using the same ASR data from the IT stage (Section 3.3.1). This model allows us to compare our IT-only TOWERFULL model against model which has access to continuous speech representations.10 Results Our results are presented in Table 4. SPIREFULLs performance demonstrates that performing both the CPT and IT stages is an effective strategy to give speech capabilities to text LLM. Notably, SPIREFULL outperforms the HuBERTlarge+CTC baseline on three out of four datasets an impressive result given that the CTC model has helpful inductive bias and access to continuous features, both of which SPIREFULL lacks. The performance gap between SPIREFULL and TOWERFULL (5.3 points in LS test-clean) demonstrates that combining CPT and IT is more effective than using IT alone. We further observe that while TOWERFULL obtains better results than SPIREBASE, it performs worse than HuBERT-large+CTC, showing that the CPT stage is crucial in outperforming model that has access to continuous features. Additionally, the minimal difference between SPIRENOBLOCKS and SPIREFULL in the IT stage suggests that incorporating textual tasks 10The hyperparameters for this ASR model are described in Appendix B. enxx spB xxen C22 spB Baselines SeamlessM4T TOWERBASE-7B TOWERINSTRUCT-7B 87.22 87.38 88. 39.0 37.8 38.8 87.42 88.02 88.27 39.9 41.7 42.0 Our models TOWERFULL SPIRENOBLOCKS SPIRENOPSEUDO SPIREBASE SPIREFULL 88.57 82.98 88.40 87.41 88. 39.4 34.2 38.9 37.4 39.3 88.17 85.93 88.22 87.97 88.21 41.7 36.1 42.0 41.4 41.8 Table 5: COMET-22 (C22) and spBLEU (spB) on the FLORES devtest set between English and the other languages supported by TOWER And SPIRE. does not negatively impact ASR performance. For SPIREBASE, it is surprising that FLEURS and VoxPopuli results were somewhat strong zero-shot settings, given that non-instruction-tuned models often struggle to work in out-of-domain without incontext learning examples.11 Finally, although SPIREFULL cannot match the performance of SeamlessM4T or Whisper-largev3, both of which were trained on far more speech data, it does exceed the performance of Whisperbase on both sections of LS and FLEURS. It also outperforms Spirit-LM on LS, which is notable because both models are derived from Llama-2 and make use of HuBERT DSU tokens."
        },
        {
            "title": "4.2 MT Experiments",
            "content": "Having demonstrated that our CPT and IT approaches work well for ASR, we now turn to MT. The key question is whether SPIRE can maintain TOWERs strong performance on MT, despite its speech-centric CPT. We report performance for translation-related tasks in Appendix C. Datasets and Metrics We use two datasets for MT: FLORES-200 (Team et al., 2024), which covers all of our models languages, and the WMT23 test set (Kocmi et al., 2023), which covers en{de, ru, zh}. We report COMET-22 (Rei et al., 2022a) 11We also tried prompting SPIREBASE with few-shot examples, but the results were significantly worse. This may be due to the length of the DSU sequences, which likely led to in-context examples that were too long for the model to handle effectively. ende C22 spB enru spB enzh C22 spB deen C22 spB ruen spB zhen C22 spB Baselines SeamlessM4T TOWERBASE-7B TOWERINSTRUCT-7B 77.76 79.96 82. 27.8 36.1 38.8 83.22 83.08 84.66 34.2 34.2 34.9 80.14 83.49 85.09 29.7 33.3 35.3 78.69 83.56 84. 26.6 41.1 45.1 80.58 80.06 82.94 32.5 32.7 36.7 76.96 78.48 80.14 23.8 23.5 26.1 Our models TOWERFULL SPIRENOBLOCKS SPIRENOPSEUDO SPIREBASE SPIREFULL 82.63 67.97 82.18 79.88 82.50 39.2 24.4 38.5 34.7 39.5 84.55 73.86 84.31 83.04 84.60 34.5 26.6 34.7 33.7 34.9 85.39 77.80 85.31 83.85 85. 37.2 29.6 37.6 32.4 37.3 84.65 73.24 85.04 83.19 85.24 45.2 28.7 45.5 40.5 45.2 82.41 78.09 82.56 80.20 82.58 35.6 29.1 36.2 32.4 36.4 79.68 73.01 79.91 78.65 79. 25.9 17.6 26.2 23.1 26.3 Table 6: COMET-22 (C22) and spBLEU (spB) on the WMT23 test set. and spBLEU12 (Papineni et al., 2002) scores via the SacreBLEU toolkit (Post, 2018). 2 (Wang et al., 2021b) for en{de, zh}. We report the same metrics as for MT. Baselines We compare the SPIRE models against the text-to-text translation performance of SeamlessM4T. Additionally, we report the performance of TOWERBASE-7B and TOWERINSTRUCT-7B as toplines. Results Our results showcase that even after the speech-centric CPT and mixed speech and text IT stage, the SPIRE models retain TOWERs performance on both FLORES  (Table 5)  and WMT23  (Table 6)  . This indicates that neither CPT nor IT on speech data negatively impact the models ability to perform MT. This is true for both CPT-only models, where SPIREBASE achieves performance comparable to TOWERBASE on both datasets; and for IT models, where SPIREFULL and TOWERFULL both perform slightly better than TOWERINSTRUCT on enxx, which is possibly an artifact of the large number of multi-turn enxx ST instructions in their IT set. Notably, our strongest SPIRE model also outperforms SeamlessM4T by both metrics on all WMT23 language pairs, and for both enxx and xxen on FLORES."
        },
        {
            "title": "4.3 ST Experiments",
            "content": "As SPIRE has shown success at both ASR and MT, we now investigate its performance on ST. Datasets For ST, we evaluate our models on FLEURS (Conneau et al., 2023), covering ST between en and all TOWER languages, and CoVoST12nrefs:1case:mixedeff:notok:flores200 smooth:expversion:2.5.1 ST approaches We evaluate ST performance on both direct ST and self-cascaded ST,13 in which each model transcribes the audio before translating its own output to the target language (i.e., ASR followed by MT). To assess the impact of ASR error propagation, we also report MT results given gold transcriptions. Results Our ST results on FLEURS and CoVoST2 are presented in Table 7. Among our models, the ones that were trained on large quantities of pseudo-labeled ST data (TOWERFULL, SPIRENOBLOCKS, and SPIREFULL) achieve far higher scores on direct ST than the one that did not (SPIRENOPSEUDO). This indicates that merely performing CPT with ASR and MT data is not enough to achieve generalization to the task of direct ST, even if the model excels at both ASR and MT. Indeed, we also attempted direct ST with SPIREBASE and it failed to produce output in the target language, even when given few-shot prompts. We also observe that SPIRENOBLOCKS performs nearly as well at direct ST as SPIREFULL, even though its MT performance is much poorer (see Table 5 and 6), showing that, surprisingly, competence at MT is not very helpful for direct ST. SPIREFULL achieves the best selfcascaded performance by significant margin for 13We also tried inference with the multi-turn prompt format shown in Table 1, but results were similar to self-cascade. Reporting the self-cascade enables comparison to models that do not support the multi-turn format, i.e. SeamlessM4T. Direct C22 spB FLEURS Self-Cascade spB C22 Gold Direct C22 spB spB CoVoST-2 Self-Cascade spB C22 Gold C22 spB Baselines SeamlessM4T 84.63 33.7 75.45 22. 86.79 38.7 84.79 36.8 72.36 19. 86.55 39.0 Our Models TOWERFULL SPIRENOBLOCKS SPIRENOPSEUDO SPIREFULL 79.10 81.11 62.80 81.33 26.1 27.1 11.7 27. 83.42 79.46 83.79 85.21 31.9 28.9 32.2 33.7 88.27 82.44 88.10 88.36 39.2 34.0 38.7 39.2 71.52 74.02 59.88 74.25 20.1 23.2 6.8 23. 74.17 68.09 78.15 78.78 25.8 22.8 29.7 30.0 87.14 69.31 87.10 87.11 38.5 26.8 39.0 38.5 Table 7: ST results on FLEURS and CoVoST-2 for enxx reporting COMET-22 (C22) and spBLEU (spB) using direct ST (direct), self-cascaded ST (self-cascade), and MT from gold-transcriptions (gold). Scores are averaged over all language pairs. both datasets, outperforming both models that were trained with fewer speech samples (TOWERFULL and SPIRENOPSEUDO), and the model that was not tuned on MT (SPIRENOBLOCKS). This suggests that, unlike direct ST, both ASR and MT competencies are necessary for strong self-cascade performance. Although SPIREFULL does not reach the direct ST performance of SeamlessM4T, it manages to achieve competitive performance despite using far less ST data (Barrault et al., 2023). Our selfcascading experiments additionally demonstrate that SPIREFULL maintains greater robustness to its own outputs than SeamlessM4T."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work we presented SPIRE, simple and effective recipe for adapting text-based, translationspecialist LLM to the speech modality while preserving the original performance on text-based tasks. We investigated the impact of speech integration on two stages of LLM adaptation, CPT and IT, finding that both contribute to the final models performance on speech tasks. Our results demonstrate that we are able to successfully integrate new modality without compromising the original models capabilities. SPIRE achieves competitive performance on ASR, while its MT abilities remain on par with the original TOWER model. Finally, for the ST task, we find that the leveraging ASR and MT data does not directly transfer to ST performance. Nonetheless, the model achieves promising performance with both direct and self-cascaded ST. As future work, we intend to extend this recipe to multilingual settings by replacing our English HuBERT speech component by the multilingual mHuBERT-147 (Boito et al., 2024). We also plan to leverage the flexibility of DSU modeling to investigate the integration of speech generation tasks. To benefit the research community, we only use publicly available and licensed data to train our models, making our results reproducible."
        },
        {
            "title": "Limitations",
            "content": "The downstream tasks we evaluate on are restricted to MT and ASR/ST, which provide an idea of the model performance but do not give us the full picture. We plan to address this by utilizing the LM-harness evaluation (Gao et al., 2024) to evaluate on suite of text-based benchmarks such as MMLU (Multitask Language Understanding) (Hendrycks et al., 2021b,a), Arc (Commonsense Reasoning) (Clark et al., 2018), Belebele (Reading Comprehension) (Bandarkar et al., 2024), and HellaSwag (Sentence Completion) (Zellers et al., 2019). Lastly, our model handles speech and text on the input side but is currently limited to generating only text."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by UK Research and Innovation (UKRI) under the UK governments Horizon Europe funding guarantee (grant number 10039436: UTTER), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 for Responsible AI), by Fundação (Center and para Ciência Tecnologia (FCT) through the project with reference UIDB/50021/2020 (DOI:10.54499/UIDB/50021/2020), by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicações. This work was performed using HPC resources from GENCIIDRIS (Grant 2023-AD011014668R1). We thank Duarte Alves and Giuseppe Attanasio for their insightful comments."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Duarte Miguel Alves, José Pombal, Nuno Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. In First Conference on Language Modeling. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: massively-Âmultilingual speech corpus. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 42184222, Marseille, France. European Language Resources Association. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand. Association for Computational Linguistics. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023. Seamlessm4t-massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Marcely Zanon Boito, Vivek Iyer, Nikolaos Lagos, Laurent Besacier, and Ioan Calapodescu. 2024. mHuBERT-147: Compact Multilingual HuBERT Model. In Interspeech 2024. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023. Audiolm: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31:25232533. Alejandro Hernández Cano, Matteo Pagliardini, Andreas Köpf, Kyle Matoba, Amirkeivan Mohtashami, Xingyao Wang, Olivia Simin Fan, Axel Marmet, Igor Krawczuk, Zeming Chen, Deniz Bayazit, Francesco Salvi, Antoine Bosselut, and Martin Jaggi. 2023. epfllm megatron-llm. Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-Weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, et al. 2024. Exploring speech recognition, translation, and understanding with discrete speech units: comparative In ICASSP 2024-2024 IEEE International study. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1148111485. IEEE. Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Zhao You, and Zhiyong Yan. 2021. GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021, pages 36703674. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, and Xiong Xiao. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079. Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, and Michael Auli. 2023a. Toward joint language modeling for speech units and text. arXiv preprint arXiv:2310.08715. Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, and Michael Auli. 2023b. Toward joint language modeling for speech units and text. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 65826593, Singapore. Association for Computational Linguistics. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2023. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. IEEE. Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. 2024. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751. Cyprien de Masson DAutume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. Advances in Neural Information Processing Systems, 32. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speechtext foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378. Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, and Shervin Malmasi. 2023. MultiCoNER v2: large multilingual dataset for finegrained and noisy named entity recognition. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 20272051, Singapore. Association for Computational Linguistics. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Edward Gow-Smith, Alexandre Berard, Marcely Zanon Boito, and Ioan Calapodescu. 2023. NAVER LABS Europes multilingual speech translation systems for the IWSLT 2023 low-resource track. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 144158, Toronto, Canada (in-person and online). Association for Computational Linguistics. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. 2024. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). John Hewitt. 2021. for pretrained https:/nlp.stanford.edu/ Initializing new word language modjohnhew//vocabembeddings els. expansion.html. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460. Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, et al. 2024. Wavllm: Towards robust and adaptive speech large language model. arXiv preprint arXiv:2404.00656. Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2024. Audiogpt: Understanding and generating speech, muIn Proceedings of sic, sound, and talking head. the AAAI Conference on Artificial Intelligence, volume 38, pages 2380223804. Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerda, Javier Jorge, Nahuel Roselló, Adria Giménez, Albert Sanchis, Jorge Civera, and Alfons Juan. 2020. Europarl-st: multilingual corpus for speech translation of parliamentary debates. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82298233. IEEE. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. 2022. Text-free prosody-aware generative spoken language modeling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86668681, Dublin, Ireland. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 142, Singapore. Association for Computational Linguistics. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux. 2021. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:13361354. Tsz Kin Lam, Alexandra Birch, and Barry Haddow. 2024. Compact speech translation models via discrete speech units pretraining. arXiv preprint arXiv:2402.19333. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when buildarXiv preprint. ing vision-language models? ArXiv:2405.02246. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning (LLaVA). arXiv preprint. ArXiv:2304.08485 [cs]. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. 2024. Voxtlm: Unified decoder-only models for consolidating speech recognition, synthesis and speech, text continuation tasks. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1332613330. IEEE. Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno Guerreiro, Ricardo Rei, Duarte Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, et al. 2024. Eurollm: Multilingual language models for europe. arXiv preprint arXiv:2409.16235. Pooneh Mousavi, Jarod Duret, Salah Zaiem, Luca Della Libera, Artem Ploujnikov, Cem Subakan, and Mirco Ravanelli. 2024. How should we extract discrete audio tokens from self-supervised models? arXiv preprint arXiv:2406.10735. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, et al. 2025. Spiritlm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052. Patrick ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael Shulman, et al. 2021. Spgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition. arXiv preprint arXiv:2104.02014. OpenAI. 2023. URL https://github.com/openai/ openai-python/blob/release-v0.28.1/chatml. md. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Ankita Pasad, Bowen Shi, and Karen Livescu. 2023. Comparative layer-wise analysis of self-supervised speech models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Brussels, Belgium. Association for Computational Linguistics. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. MLS: Large-Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020, pages 27572761. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Srijith Radhakrishnan, Chao-Han Yang, Sumeer Khan, Rohit Kumar, Narsis Kiani, David Gomez-Cabrero, and Jesper Tegnér. 2023. Whispering LLaMA: cross-modal generative error correction framework for speech recognition. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1000710016, Singapore. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925. Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 2022. Fine-tuned language models are continual learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 61076122, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi. 2023. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2023. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: NLLB Team, Marta Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, et al. 2022. No language left behind: Scaling human-centered machine translation (2022). URL https://arxiv. org/abs/2207.04672. NLLB Team et al. 2024. Scaling neural machine translation to 200 languages. Nature, 630(8018):841. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Viet Anh Trinh, Rosy Southwell, Yiwen Guan, Xinlu He, Zhiyong Wang, and Jacob Whitehill. 2024. Discrete multimodal transformers with pretrained large language model for mixed-supervision speech processing. arXiv preprint arXiv:2406.06582. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021a. VoxPopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online. Association for Computational Linguistics. Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino. 2021b. Covost 2 and massively multilingual speech translation. Interspeech 2021. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Linguistics: EMNLP 2023, pages 1575715773, Singapore. Association for Computational Linguistics. Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Wolf. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023a. On decoder-only architecture for speech-to-text and large language model integration. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023b. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a. paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations. Yaoxun Xu, Shi-Xiong Zhang, Jianwei Yu, Zhiyong Wu, and Dong Yu. 2024b. Comparing discrete and continuous space llms for speech recognition. In Proc. Interspeech 2024. Hongfei Xue, Wei Ren, Xuelong Geng, Kun Wei, Longhao Li, Qijie Shao, Linju Yang, Kai Diao, and Lei Xie. 2024. Ideal-llm: Integrating dual encoders and language-adapted llm for multilingual speech-to-text. arXiv preprint arXiv:2409.11214. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, ShangWen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung yi Lee. 2021. SUPERB: Speech Processing Universal PERformance Benchmark. In Proc. Interspeech 2021, pages 11941198. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational NER measures entity recognition on MultiCoNER 2023 (Fetahu et al., 2023) test. We report COMET22 for APE and sequence F1 score for NER. We evaluate APE for en{de, ru, zh} and NER for {de, en, es, fr, it, pt, zh}. Table 8 reports our results. We achieve comparable scores to TOWERINSTRUCT across both tasks and all language directions considered. Notably, we outperform TOWERFULL in all settings, which may be suggestive of the benefit of including text data in the CPT stage. APE NER enxx xxen Multilingual TOWERINSTRUCT-7B TOWERFULL SPIREFULL 83.08 82.65 83.13 80.29 79.90 80. 71.56 65.07 67.10 Table 8: Results on APE and NER reporting COMET22 () and sequence F1 score () respectively."
        },
        {
            "title": "A Data",
            "content": "A.1 Speech Data Preprocessing Normalization In order to make transcripts consistent across the different datasets, the following normalization is applied: GigaSpeech (CPT): we lower-case the text and replace punctuation tags: <COMMA>, QUESTIONMARK>, <EXCLAMATIONPOINT> with their appropriate punctuation. <PERIOD>, MLS (CPT): we apply tail-end normalization step here which uniformly samples each speaker to have at maximum 13 transcriptions. This allows us to have better distribution of speakers. CV (IT): we subsampled from CommonVoice to ensure minimum duration of 3 seconds per sample. To enhance transcript diversity, we limit each transcript to 4 unique speakers. Deduplication As in previous work (Zhang et al., 2023; Rubenstein et al., 2023; Chang et al., 2024), we merge consecutive repeated DSU tokens into single token to reduce sequence length. CTC-based ASR model We train CTC-based ASR model using the HuggingFace Transformers library (Wolf, 2019), leveraging the ASR data from the IT stage (CommonVoice, Table 2) as training data. Our ASR model is made of the HuBERT-Large14 speech representation model, followed by three hidden layers and vocabulary projection layer. We train for 50 epochs with dropout of 0.3 and learning rate of 1e-4 with warm-up ratio of 0.15. The best checkpoint is selected using CER scores. This was obtained at step 220K (at epoch 12.8). Translation-related Tasks TOWER was evaluated on translation-related tasks in addition to MT. We follow their evaluation setup and use the Tower-eval suite15 (Alves et al., 2024) to evaluate SPIRE models on APE and NER. For detailed description of the tasks, we refer the readers to Alves et al. (2024). Briefly, APE measures final translation quality on WMT23 after postediting with NLLB-3.3B (Team et al., 2022), and 14https://huggingface.co/facebook/hubert-large-ll60k 15https://github.com/deep-spin/tower-eval"
        }
    ],
    "affiliations": [
        "ELLIS Unit Lisbon",
        "INESC-ID",
        "Instituto Superior Técnico, Universidade de Lisboa",
        "Instituto de Telecomunicações",
        "NAVER LABS Europe",
        "Paris-Saclay University",
        "Sapienza University of Rome",
        "Unbabel",
        "University of Edinburgh"
    ]
}