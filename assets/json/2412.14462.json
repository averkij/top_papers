{
    "paper_title": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion",
    "authors": [
        "Jixuan He",
        "Wanhua Li",
        "Ye Liu",
        "Junsik Kim",
        "Donglai Wei",
        "Hanspeter Pfister"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to a more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explicitly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordance-aware-any."
        },
        {
            "title": "Start",
            "content": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion Jixuan He1,2,*, Wanhua Li1,* Ye Liu1,3 Junsik Kim1 Donglai Wei4 Hanspeter Pfister1 1Harvard University 2 Cornell Tech 3The Hong Kong Polytechnic University 4 Boston College 4 2 0 2 9 1 ] . [ 1 2 6 4 4 1 . 2 1 4 2 : r Figure 1. Given foreground-background object-scene pair, our model can perform affordance-aware object insertion conditioning on different position prompts, including points, bounding boxes, masks, and even null prompts."
        },
        {
            "title": "Abstract",
            "content": "As common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explic- *Equal Contribution. Work done while at Harvard University. itly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordanceaware-any. 1. Introduction In the image composition scenario, common sense guides our perception of the authenticity of synthesized images: person cannot levitate, water bottle needs surface for support, and boat should be floating in the water rather than on the ground. Deviation from such common sense often leads to semantic inconsistencies in synthesized images. From composition perspective, the backgrounds semantic richness plays pivotal role in defining the placement and characteristics of foreground objects. To better describe the influence of background semantics on the foreground, we borrow the concept of affordance into object-scene composition tasks. Previously, Kulal et al. [24] explored the concept of human affordance for image synthesis. Their work, focused on inserting humans into masked scenes, extends beyond mere color or view adjustments. To generalize the setting to arbitrary object-scene synthesis, we term the new task Affordance-Aware Object Insertion. This task challenges models to identify suitable locations and make necessary adjustments to foreground objects, ensuring the generated images adhere to physical laws. This work aims to build foundation model for affordance-aware object insertion, which can put any object into any scene as shown in Figure 1. There are three primary challenges involved. First, the model must accurately recognize the appropriate affordance relationship between background and the foreground object to be inserted. Adjustments to the inserted foreground object are crucial for achieving the intended semantic consistency. The second challenge lies in the models ability to generalize across diverse range of foreground objects. Previous generative image editing methods, such as textual inversion [11], DreamBooth [40], and DreamEdit [26], are subject-specific thus hard to generalize. Our goal is to train model that can generalize to any object. Third, our model should support variety of input prompts for users to specify insertion locations, ranging from sparse formats like points and bounding boxes to dense masks. Moreover, in the absence of explicit prompts, the model can autonomously determine appropriate insertion locations by analyzing the semantic content of both the background and foreground. We address these challenges through three components: task, data, and model. Extending the concept of affordance beyond Kulal et al.s initial scope [24], the task of affordance-aware object insertion aims to place an arbitrary object into any scene, accommodating different positional prompts, even in the absence of explicit positional cues. It has significant implications for applications such as automated dataset synthesis. To support this task, large-scale dataset is necessary. Existing image composition datasets, such as DreamEditBench [26], are limited in terms of the diversity of foreground object categories and the number of training samples. To overcome these limitations, we curate new dataset called SAM-FB which is derived from SA-1B [23] for affordance learning. SAM-FB contains variety of foreground object categories and over 3 million samples. With SAM-FB, we further propose Mask-Aware Dual Diffusion (MADD) model to utilize the large-scale data, which is diffusion-based framework that facilitates the seamless integration of diverse objects into any scene. During the denoising procedure, object position is progressively refined while the target RGB image is synthesized simultaneously, ensuring accurate alignment between objects and positions to achieve affordance-aware insertion. Furthermore, we present unified representation for sparse and dense prompts, enabling our model to interpret and respond to various types of position inputs effectively. The contributions of this work are as follows: We formulate the task of affordance-aware object insertion, which extends general object-scene composition with affordance guidance, aiming to achieve realistic object insertion with diverse prompts. We introduce SAM-FB, large-scale dataset with diverse object categories for affordance-aware object insertion, which consists of over 3 million samples. To facilitate the learning of the affordance concept, we propose the mask-aware dual diffusion model, which adopts dual-stream architecture to simultaneously denoise the appearance and the insertion mask. 2. Related Work Affordance. J.J. Gibson [3] first introduces the concept of affordance, then series of papers [4, 10, 14, 28] dug into this concept and brought it into the image synthesis. Initially grounded in psychology, the concept emphasizes that an objects appearance should correspond with its utilitarian aspects as perceived by humans. Further exploration within the field of image synthesis involved adjusting the orientation and gestures of generated human figures to align with their background. Therefore, prior work primarily focuses on the interaction between humans and objects [12, 49, 52] or the human-scene relationship [7, 9, 46]. Kulal et al. [24] made progress by introducing model trained on person movement video data for placing people within scenes and adjusting its pose according to the surroundings. However, their models scope was limited to human figures. Despite these advancements, the concept of affordance in image composition, which encompasses the positioning, viewing angle, and color harmony of objects within scenes, has remained relatively unexplored. Our work offers generalized and versatile solution for object-scene composition. Image Editing. Image editing aims to modify existing images using generative models. Generally, image editing can be divided into semantic editing (e.g. adding or removing objects, changing the background), style editing (e.g. altering color or texture), and structural editing (e.g. changing object size or viewpoint). By utilizing generative models like GANs [13] or Diffusion [8], users can edit image content by providing instructions and multi-modal prompts. For instance, InstructPix2Pix [5] and MoEController [25] allow semantic and style editing through text-based instructions provided by users, while Imagen Editor [45] enables more precise control of edit locations by accepting userprovided masks for the areas to be edited. Additionally, other image editing models, such as TF-ICON [33] and Im-"
        },
        {
            "title": "Dataset",
            "content": "Sample No. Category No. DreamEditBench MureCom SAM-FB (Ours) 440 640 3,160,403 22 32 3,439 Table 1. Dataset comparison. Our dataset contains significantly more training samples and object categories. ageBrush [48], accept reference images from users, constraining the appearance of objects during editing. Image Composition. Image composition is sub-task within image editing, primarily focused on controllably inserting foreground objects into background based on reference images. Recently, the utilization of generative models empowers high-quality and controllable image composition. Diffusion models have been successfully applied in various domains, including image composition [31, 42], video generation [18], and data augmentation [44]. Stable Diffusion (SD) [39] introduced methods for blending feature and semantic information from images and text, enabling object generation based on prompt instructions. Inpainting [2] is common approach to achieve image composition. SD-based inpainting models [27, 34, 50] allow users to repaint specific region with mask, but its hard to insert desired object into that region. Blended Diffusion [1] enables finer control by leveraging text descriptions of the foreground, but its crucial for user to describe the foreground precisely. PBE [47], ObjectStitch [42], and GLI-GEN [29] utilize reference images to incorporate rich visual information, achieving better perspective and color harmony. However, these models require users to provide precise positional cues, such as bounding boxes or masks, to indicate the insertion location. Our proposed method offers viable solution for handling vague positional information, such as point prompts or even blank prompts. 3. Dataset We constructed the SAM-FB benchmark dataset for the affordance-aware object insertion task. Design Decisions. We designed the dataset to have: 1) Sufficient training samples. 2) wide variety of foreground objects. 3) Well-aligned input-output pairs such as (f , b, p, x) tuples, where , b, p, and represent the foreground image, background image, position prompt, and ground truth image, respectively. Unfortunately, there is no existing dataset that meets the aforementioned requirements. As shown in Table 1, existing datasets such as DreamEditBench [26] and MureCom [32] contain only very limited number of samples and object categories. Our dataset is built upon the SA-1B [23] dataset to ensure diverse category of foreground objects. SA-1B Figure 2. Pipeline of constructing the SAM-FB dataset. The background is inpainted and high-quality foreground objects are preserved through data quality control stage. contains 11 million high-quality images collected from all kinds of reality scenes and 1 billion well-annotated classagnostic masks. We masked out the foreground object and inpainted the mask to obtain the background. The ground truth is the original image. To automate the process and improve the dataset quality, scalable automatic annotation pipeline, and data quality control are proposed. In the end, we obtain over 3 million well-annotated training samples organized in (f , b, p, x) format, where consists of diverse position prompts generated from foreground masks. Scalable Automatic Annotation Pipeline. As shown in Figure 2, the pipeline generates foreground-background pairs from images in the SA-1B dataset. For each input image, we first run SAM to obtain object masks as candidates. We then perform the Non-Maximum Suppression (NMS) with high threshold (0.6) to deduplicate these masks. With these masks, we crop the objects to form the foreground , and the remaining content is inpainted with LAMA [43] to create the background b. To ensure the inpainting quality of the background, we expand the boundaries of the object mask to avoid residual foreground content in the background following GroundingSAM [30, 38]. The original image serves as the ground truth x. The binary object mask is designated as and we can derive multiple different positions prompts such as point, bounding boxes using it, so that compose the input-output pairs (f , b, p, x), This pipeline is scalable and no manual annotations are required in this process. With this pipeline, its feasible and easy for users to further scale up the SAM-FB dataset or customize their datasets with new source images. Data Quality Control. We only keep high-quality foregrounds, since we may obtain masks that include unintended elements due to challenges in controlling the granularity and semantics of the segmentation while using SAM. They typically suffer from three issues: 1) Incompleteness. Some objects are obscured by others, resulting in only small part of the original objects being visible. 2) Back3 Figure 3. The framework of MADD. Foreground objects are encoded using DINOv2 encoder, serving as the guidance signal through the cross-attention mechanism. The position prompt encoder unifies different types of position prompts, which are then concatenated with the latent mask mt. The background is encoded using VAE encoder and then concatenated with the latent image zt. We use dual branch structure to denoise RGB image and object mask simultaneously. ground Masks. It is also possible to get some background masks such as piece of sky or coast, which cannot be treated as foreground object. 3) Undersized Objects. Some tiny objects feature blurry details, thereby reducing data quality. To achieve quality control, we employ combination of rule-based and learning-based methods to filter the foreground. We first filter masks by setting rules on their basic attributes. Then, we further employ deep neural network to filter out low-quality objects. We annotate 2,000 foreground images with binary labels (good and bad) and fine-tune pre-trained ResNet-50 [15] on those manually annotated images. The model is then able to assess the quality of the foreground. After the strict quality control, only 0.25% of the masks are left. Samples of low-quality objects and details of the filters are in the supplementary. To justify the diversity of our proposed dataset, we use RAM [51] to recognize the categories of foregrounds we keep. We have identified 3439 different categories, which is vast improvement over previous datasets. The word cloud of categories is shown in our supplementary. 4. Method 4.1. Task Formulation We formally define the affordance-aware object insertion problem as follows: given foreground object , background scene b, and position prompt p, the model is required to predict synthesized image ˆx that places the object into the scene following the correct affordances and aligns with the intentions of the position prompt: ˆx = G(f , b, p). (1) The model must support flexible prompts p, which include points, bounding boxes, masks, and even null prompts. 4.2. Mask-Aware Dual Diffusion Inspired by the recent success of diffusion models [39], we adopt SD as the backbone of our model to achieve highquality image synthesis. SD with condition guidance represents Markov process where Gaussian noise ϵt (0, 1) is added to the ground truth z0 to produce zt = αtz0 +σtϵt. Here, αt and σt are scalars sampled by different samplers such as DDIM [41] and DDPM [17]. The objective of the SD is to reconstruct the reverse process pθ(z0c): pθ(z0:T c) = p(zT ) (cid:89) t=1 pθ(zt1zt, c), (2) where is the condition to guide the denoising process. In our task, z0 = and consists of foreground image , background image b, and position prompt p. For affordance-aware object insertion, one key goal is to determine an appropriate location and size for the object to be inserted. The final inserted region, e.g., the insertion mask contains the accurate location, size, and shape information, yet hasnt been explicitly modeled in Eq. 2. To facilitate the learning of the affordance concept, we explicitly model the insertion mask in the diffusion process. Specifically, we propose Mask-Aware Dual Diffusion model, which adopts dual-stream architecture to simultaneously denoise the appearance and the insertion mask. Instead of just predicting the composed image, MADD also predicts the precise mask ˆm of the desired inserted object explicitly at each time step. This means that 4 the reverse process of mt and zt also depends on each other, so we have pθ(zt1zt, mt, , b, p) for the RGB stream and pθ(mt1mt, zt, , b, p) for the mask stream, where mt = αtm0 + σtϵm (0, 1), and m0 = s. Note that although is included in our SAM-FB dataset, it can be directly derived from the difference between the ground truth and the background b. , ϵm For the diffusion process of MADD, we have pθ1(z0:T c, m0:T ) = p(zT ) (cid:89) t=1 pθ1(zt1zt, mt, c) pθ2(m0:T c, z0:T ) = p(mT ) (cid:89) pθ2(mt1zt, mt, c), t=1 (3) where θ1 and θ2 represents parameters for different tasks: θi = θshared θi expertise. Every time step, the denoiser ϵθ takes the foreground, background, position prompt, and time step as input to predict the noise map ϵt for the RGB image and ϵm = ϵθ(αtz0 + σtϵt, αtm0 + σtϵm ϵt, ϵm for the mask as follows: , c). (4) Figure 3 shows the framework of the MADD. Inspired by Liu et al. [31], we utilize single UNet with two expert input-output branches to denoise them simultaneously. The two tasks share the entire UNet except for the conv_in, first Down Block, last Up Block, and the conv_out. These independent blocks serve as the expertise inputoutput branch. Skip connection is also performed between the corresponding expertise input and output branches. This dual diffusion architecture reuses most of the parameters, encouraging the model to incorporate knowledge from two relevant tasks. Modeling pθ1 and pθ2 is equivalent to estimating the noise terms ϵt and ϵm at each time step. This can be achieved by minimizing the loss function for RGB images and insertion masks: Lz(θ1) = (cid:2)wtϵt ϵθ(αtz0 + σtϵt; t, mt, c)2(cid:3) Lm(θ2) = (cid:2)wtϵm ϵθ(αtm0 + σtϵm ; t, zt, c)2(cid:3) (5) (6) wt is function weighing the contributions of denoising tasks to the training objective, commonly set to 1. During the training procedure, the time step is sampled from uniform distribution U[0, 1]. The final loss function for MADD is obtained by combining the above losses: L(θ) = Lz(θ1) + Lm(θ2). (7) 4.3. Affordance Condition Foreground Encoder. In MADD, foreground images serve as guidance condition and are injected into the mainstream using cross-attention. The foreground embedding can be any tokenized features obtained by ViT-like visual encoder. In SD, the CLIP [37] encoder is used to provide text semantics for cross-attention. Following Efficient3DiM [20], we use DINOv2 [35] as the encoder. While CLIP visual features are designed to align with textual features, typically encapsulating high-level semantic features, DINOv2 features preserve detailed object information, which is crucial for our task. Background Encoder. Following practices in SD-based image editing methods [5, 42], we use frozen pre-trained VAE Encoder to encode the background image to 4channel latent map. As the background image is pixelaligned with the output, we concatenate the background latent map with the zt and send them to the UNet. The pretrained VAE decoder is then used to transform the latent output back to RGB space. Position Prompt Encoder. To accommodate various position prompts, we have designed unified representation that processes different types of prompts. Specifically, we convert both sparse and dense position prompts into unified dense representation. Each type of prompt is represented as 1-channel position map. Note that these position prompts serve as rough guides for the placement of foreground objects, therefore we consider building probability map for the 1-channel position map. For point prompts, we transform them into Gaussian heatmap where the point is positioned at the center of the Gaussian. When dealing with bounding box prompts, we fill the area inside the box with ones and the outside with zeros. For mask prompts, we set the values inside the mask to 1 and all other values to 0. For the null prompt, we use an all-one image, implying that all positions are possible. Subsequently, we resize the position map to match the spatial dimensions of the latent map mt, and concatenate it with mt along the channel dimension before feeding it into the diffusion U-Net. 4.4. Implementation Details To leverage prior knowledge of real-world images, we initialize our model with pre-trained Stable Diffusion Inpainting model for the neural layers with the same configuration. Fine-tuning diffusion model is resourceconsuming, but we can make it more efficient using the feature that the model is image-size unaware. Therefore, we first train the model on 128 128 resolution to accelerate training, with batch size of 1024 on 2 A-100 GPUs for 35K steps. We then fine-tune it on 256256 resolution with batch size of 256 for 15K steps. For each foreground object, all different position prompts can be generated with the mask s. During training, we randomly selected one of the position prompts as input with equal probability. We use Classifier-Free Guidance [16] to improve the insertion quality. During training, we drop all the conditions with probability of 0.1 and only drop with probability of Figure 4. Qualitative results of MADD on the SAM-FB test set. Each row corresponds to one type of prompt, i.e., point, bounding box, mask, and null, respectively. Our MADD simultaneously predicts the RGB image and the object mask. 0.1. We also perform strong data augmentation to both foreground images and position prompts to prevent the model from learning mere copy-and-paste process. Please refer to the supplementary for details. 5. Experiments Method FID CLIP Score MSE mask bbox mask bbox mask bbox [39] [47] [29] [24] 15.41 15.47 0.7079 0.8058 33.68 24.59 14.21 883 860 0.7664 2373 1615 830 0.7944 845 857 14.49 14.42 0.8014 0.8637 5.1. Results on the SAM-FB Test Set Ours 13.53 13.60 0.8727 0.8658 760 775 Evaluation Metrics. Following the previous work [6, 19, 22, 33], we employ the FID score to measure the quality of images obtained by generative models and the CLIP score to evaluate the semantic similarity between the edited region and the reference foreground. Quantitative Comparison. We compare our method with Stable Diffusion [39], PBE [47], GLI-GEN [29], and Human Affordance [24] on the SAM-FB test set to show the effectiveness of our method. The results in Table 2 show that our method attains the best FID score and the highest CLIP score, illustrating the superiority of our model. We further present our results with different prompts in Table 3, we see that the mask prompt achieves the best results as it provides more accurate position information. Qualitative Results. We left 3786 images as test split. Figure 4 presents the visualization results on the SAM-FB test set. In each group, the leftmost image depicts the background marked with position prompt. Our MADD predicts the RGB image and mask of the inserted object, which are shown in the last two images of each group. These Table 2. Method comparisons on the SAM-FB test set. Diffusion, Human Affordance. GLIGEN, PBE, Stable Prompt Mask Bbox Point Null Avg. FID MSE CLIP Score 0.8727 0.8658 0.8567 0.8034 0.8415 13.96 13.69 792 13.66 772 13.60 775 13.53 760 Table 3. Comparison of position prompts on the SAM-FB test set. results demonstrate that MADD not only inserts objects with high quality but also accurately predicts object masks. Figure 1 presents additional results from the SAM-FB test set, illustrating that our model is capable of performing affordance-aware object insertion. Ablation Study. Table 4 presents the results of our ablation study. We replace the image encoder in the Human Affordance model with DINOv2 as the baseline. Next, we add 6 (a) Location Adjustment (b) View Adjustment (c) Size Adjustment (d) Automatic Localization Figure 5. We test ambiguous prompts (points and blank) on the in-the-wild images. When providing the prompt of point, 5a, 5b, and 5c show that our model can adjust properties of foreground objects to achieve the affordance insertion.5d illustrates that the model could find the suitable position to insert the object. Method FID () CLIP100 () Baseline + Classifier-Free + Dual diffusion + Expertise branch 25.89 21.93 21.75 21.55 89.12 91.13 91.57 91. Table 4. Ablation study on the SAM-FB test set with 128 128 resolution using mask prompts. Classifier-Free Guidance to enhance the affordance condition signal. Then we diffuse RGB image and object mask simultaneously, but sharing the entire UNet. Finally, we use two expertise branches for mask and RGB streams. The results show that all of them improved performance. 5.2. Results on In-the-wild Images Ambiguous Prompts. The training process with position augmentations ensures that the model can understand the affordance relationship between the scene and objects. By using MADD, the model explicitly refines the objects position, making it capable of handling ambiguous position information. Moreover, the model can adjust the objects position, size, and view to ensure coherence with the background scene. In Figure 5a, the model adjusts the persons position around the provided point, placing them on the ground and in front of the bike instead of in the air. Figure 5b demonstrates that the model can change an objects view. The cars orientation is adjusted to align with the lane. Figure 5c shows that the model adjusts the size of the coffee beans to match the background scene, even though only point prompt and reference image of the statue are proFigure 6. MADD can give different feasible solutions for ambiguous prompts such as point and blank. vided. Furthermore, the model is capable of placing objects even in the absence of position prompts, as shown in Figure 5d. In this case, the model automatically places the cake onto the plate without any additional clues. When no prompts are provided, the model can still determine feasible insertion. Additionally, when ambiguous prompts are given, MADD can generate diverse and feasible insertions, as shown in Fig. 6. For prompt of point, the model finds reasonable insertions around the point, and for blank prompt, the model searches for suitable insertions globally. Human Evaluation. To test the generalization ability of our MADD model, we performed affordance insertion on in-the-wild images and compared the results with Stable Diffusion XL [36], GLI-GEN [29], PBE [47], and ObjectStitch [42]. Instead of merely relying on metrics like FID and CLIP Score, we also conducted user study to achieve more comprehensive comparison. 7 (a) Rank distribution for different methods. Our method has the highest proportion of rank 1 and the least proportion of rank 5. (b) Rank-1 distribution for each criterion. Each pie chart represents the proportion of times each model achieved Rank-1 for specific evaluation criterion. Our method dominates every metric. Figure 7. Human evaluation results on in-the-wild Images. We compared 10 groups of images according to different criteria. Our MADD model outperforms SDXL [39], GLI-GEN [29], ObjectStitch [42] and PBE [47] on overall ranking and each criteria. Figure 8. MADD can work on images of higher resolution, generating sharper edges, clearer reflections, and improved texture details. 2) Foreground Clarity and Detail, 3) Foreground Appearance Consistency with Reference, 4) Lighting and Shade on Foreground, 5) Color Consistency. Figure 7a shows the distribution of ranks for different models, where rank 1 and rank 5 represent the best and worst quality, respectively. Our model achieves 50% of Rank-1 placements and 1.60% of Rank-5 placements, outperforming other methods. Figure 7b provides details for each criterion. Our model consistently achieved the highest proportion of Rank-1 placements across all evaluation criteria, as indicated by the largest segments in each pie chart, especially for maintaining the consistency of foreground appearance with the reference image. This dominance in Rank-1 distribution across multiple criteria highlights the models superior performance compared to others in each assessed aspect. Figure 9 shows additional in-the-wild examples with null Figure 9. More in-the-wild examples with null prompts. The model can generate an affordance-feasible solution to insert the foreground objects according to the background scene. We asked 10 users to rank 10 groups of composited images generated by different models according to the following criteria: 1) Foreground and Background Integration, prompts. The results demonstrate the strong generalization ability of our method. The supplementary material contains more samples to further illustrate that our model can provide suitable and diverse solutions in in-the-wild scenarios. 5.3. Results on Higher-Resolution Images Even though we trained our model at resolution of 256x256, it is also capable of handling higher-resolution inputs, such as 512x512. To demonstrate this, we fine-tuned our model on SAM-FB at resolution of 512x512 for only 200 steps and compared it with the original checkpoint on the test split. Figure 8 shows the results of the original checkpoint and the fine-tuned version at higher resolution using the same input. We highlighted the details of the inserted object. It is demonstrated that the model generates sharper edges, clearer reflections, and improved texture details after fine-tuning at higher resolution. 6. Conclusion In this paper, we extend the notion of affordance to more generalized setting, rather than being limited to human-centered tasks. We proposed the affordance-aware object insertion task which involves placing any object into any scene using various types of position prompts. To support this challenging task, we have constructed SAM-FB dataset which consists of 3,160,403 high-quality foreground-background pairs across over 3,000 object categories. We present dual-stream diffusion model, MADD, which is designed to exploit affordance relationships between foregrounds and backgrounds by simultaneously denoising the foreground appearance and the object mask. Leveraging its understanding of affordance, MADD supports variety of position prompts, varying from points to masks, and can automatically insert objects even in the absence of explicit prompts. Moreover, MADD exhibits strong generalization capabilities with foreground objects. Our method outperforms previous diffusion-based image composition models on both the SAM-FB test set and inthe-wild images, seamlessly integrating various objects into any background while keeping semantic consistency."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1820818218, 2022. 3 [2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417424, 2000. 3 [3] Marc Bornstein. The ecological approach to visual perception, 1980. 2 [4] Tim Brooks and Alexei Efros. Hallucinating poseIn European Conference on Computer compatible scenes. Vision, pages 510528. Springer, 2022. 2 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2, [6] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 6 [7] Vincent Delaitre, David Fouhey, Ivan Laptev, Josef Sivic, Abhinav Gupta, and Alexei Efros. Scene semantics from long-term observation of people. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pages 284298. Springer, 2012. 2 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [9] David Fouhey, Vincent Delaitre, Abhinav Gupta, Alexei Efros, Ivan Laptev, and Josef Sivic. People watching: Human actions as cue for single view geometry. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 732745. Springer, 2012. 2 [10] David Fouhey, Xiaolong Wang, and Abhinav Gupta. In defense of the direct perception of affordances. arXiv preprint arXiv:1505.01085, 2015. 2 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [12] Georgia Gkioxari, Ross Girshick, Piotr Dollar, and Kaiming He. Detecting and recognizing human-object interactions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 83598367, 2018. 2 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [14] Abhinav Gupta, Scott Satkin, Alexei Efros, and Martial Hebert. From 3d scene geometry to human workspace. In CVPR 2011, pages 19611968. IEEE, 2011. 2 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 2 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 4 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video dif9 fusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [19] Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. Zero-shot generation of coherent storybook from plain text story using diffusion models. arXiv preprint arXiv:2302.03900, 2023. 6 [20] Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, and Liangliang Cao. Efficient3dim: Learning generalizable single-image novel-view synthesizer in one day. arXiv preprint arXiv:2310.03015, 2023. 5 [21] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:1210412114, 2020. 1 [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 6 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2, 3 [24] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei Efros, and Krishna Kumar Singh. Putting people in their place: Affordance-aware human insertion into scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1708917099, 2023. 2, 6, [25] Sijia Li, Chen Chen, Moeconand Haonan Lu. Instruction-based arbitrary image manipulaarXiv preprint troller: tion with mixture-of-expert controllers. arXiv:2309.04372, 2023. 2 [26] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. Dreamedit: Subject-driven image editing. arXiv preprint arXiv:2306.12624, 2023. 2, 3 [27] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10758 10768, 2022. [28] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, MingHsuan Yang, and Jan Kautz. Putting humans in scene: Learning affordance in 3d indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1236812376, 2019. 2 [29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 3, 6, 7, 8, 4 [30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3 [31] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generarXiv preprint ation with latent structural diffusion. arXiv:2310.08579, 2023. 3, 5 [32] Lingxiao Lu, Bo Zhang, and Li Niu. Dreamcom: Finetuning text-guided inpainting model for image composition. arXiv preprint arXiv:2309.15508, 2023. 3 [33] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22942305, 2023. 2, 6 [34] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146111471, 2022. [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 7 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [38] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 4, 6, 8 [40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. [41] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 4 and Stefano Ermon. arXiv preprint [42] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. ObjectIn Prostitch: Object compositing with diffusion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. 3, 5, 7, 8 10 [43] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with arXiv preprint arXiv:2109.07161, fourier convolutions. 2021. [44] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023. 3 [45] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi PontTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18359 18369, 2023. 2 [46] Xiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge watching: Scaling affordance learning from sitcoms. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 25962605, 2017. 2 [47] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 3, 6, 7, 8, 4 [48] Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Imagebrush: Learning Hu, Lili Qiu, Hideki Koike, et al. visual in-context instructions for exemplar-based image manipulation. Advances in Neural Information Processing Systems, 36, 2024. 3 [49] Bangpeng Yao and Li Fei-Fei. Modeling mutual context of object and human pose in human-object interaction activities. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1724. IEEE, 2010. [50] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Generative image inpainting with contextual attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 55055514, 2018. 3 [51] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong arXiv preprint arXiv:2306.03514, image tagging model. 2023. 4 [52] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in knowledge base representation. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13, pages 408424. Springer, 2014. 2 11 Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More Implementation Details A.1. SAM-FB Dataset Figure 11a shows the object masks before and after the data quality control. We see that with our designed data quality control, the foreground object masks have better quality. Figure 11b illustrates the word cloud of our SAMFB dataset, we observe that our SAM-FB dataset consists of diverse object categories. Table 5 demonstrates the specific threshold for each filter operation of our data quality control stage. With these filter operations, only 0.25% of the masks are left, which ensures the quality of the obtained dataset. A.2. Traning Details We constructed our dual-stream UNet based on the Stable Diffusion Inpainting v1.5 model, incorporating several modifications. Specifically, we replicated both the first down-sampling block (including conv_in and the first Down Block) and the last up-sampling block (including the last Up Block and conv_out) to accommodate dualstream inputs and outputs. To bypass the need to start from scratch, we initialized our model using the pre-trained Stable Diffusion Inpainting v1.5 checkpoint for the unchanged blocks. To optimize our computing resources, we employed gradual scaling-up approach to train our model. Initially, the model was trained at resolution of 128 128, with batch size of 1024 and learning rate of 1.25 104. This phase included 5,000 warm-up steps, followed by constant schedule. We then fine-tuned the model at higher resolution of 256 256, reducing the batch size to 256 and adjusting the learning rate to 5 105. For the diffusion process, the time step is 1000 with linear noise scheduler. We use 2048 samples of SAM-FB for testing and the rest for training. To ensure fair comparison with other methods, we re-implemented and re-trained the closest method, Human Affordance, since the authors did not release either the model weights or the full dataset they used. We adopted the diffusion model architecture provided by the authors and trained it using our SAM-FB dataset. Additionally, we replaced the mask input in the original model with our position map to ensure compatibility with the SAM-FB dataset. A.3. Data Augmentation To prevent the model from learning copy-and-paste process, we introduce different augmentations for the background images, foreground images, and position prompts. Background Augmentation Given that the images in the SA-1B dataset are not square, we rescale and crop them focusing on the objects. Our first step is to resize each source Filter condition None (Initial) Relative Size Aspect Ratio Components Num. Color Std. ResNet50 Score"
        },
        {
            "title": "Threshold Reserved Percentage",
            "content": "[0.1, 0.75] 3 4 45 0.7 100% 7.10% 6.88% 6.71% 1.69% 0.25% Table 5. Reserved percentage for foreground quality control filters. We combine different rule-based and learning-based conditions. Through this process, foreground objects with high quality are reserved. Figure 10. FID-CLIP score curve on 128 128 resolution with different guidance scale [1.0, 3.0, 4.0, 5.0, 6.0, 7.0]. image so that its shorter edge measures 256 pixels. Following this, we randomly center 256 256 bounding box around each valid object mask, randomly chosen to provide varied backgrounds for the same object. This technique introduces slight background differences for identical objects. Additionally, we allow the bounding boxes to partially crop the objects. This strategy is specifically designed to enable the model to learn from scenarios where objects are partially obscured by the edges of the image. Foreground Augmentation To prevent the model from simply copying and pasting foreground objects and to increase the diversity of foreground objects, data augmentation is necessary. We used geometric and color augmentations based on Kulals work [24] and StyleGAN-ADA [21]. Geometric augmentations included isotropic scaling, rotation, anisotropic scaling, and cutout, each with probability of 0.4, 0.4, 0.2, and 0.2 respectively. Color augmentations included brightness, contrast, saturation, image-space filtering, and additive noise, each with probability of 0.2. Position Prompt Augmentation Our method requires the model to perceptively adjust the position of the inserted ob- (a) Examples for foreground quality control (b) Word cloud of foreground categories Figure 11. 11a shows the candidate foreground samples in the pipeline. The upper row shows four low-quality samples. The lower row shows the samples after data quality control. 11b shows the word cloud of foreground categories in the SAM-FB dataset. Method Baseline +Classifier-Free +Dual Diffusion +Expertise branch FID CLIP Score100 mask bbox point null 25.89 21.93 21.75 21. 26.21 22.03 21.81 21.66 26.37 22.31 21.90 21.76 27.35 22.74 22.39 22.24 Avg. 26.46 22.25 21.96 21.80 mask bbox point null 89.12 91.13 91.57 91.68 89.50 90.95 91.05 90.96 79.92 85.49 88.25 89. 79.31 85.26 88.34 88.30 Avg. 84.46 88.21 89.80 90.14 Table 6. Experimental results on SAM-FB test set. The difference between the four kinds of prompts indicates that the performance will be better with more precise position prompt. ject in response to an ambiguous position prompt; therefore, it is also necessary to augment the position prompts. For points, we perform random jittering to deviate them from their original positions. For bounding boxes, we randomly enlarge each box. We adopt mask enlarging and feathering for mask prompts. A.4. Classifier Free Guidance An essential mechanism we employ to support the Null position prompt is Classifier-Free Guidance (CFG). This approach not only enhances the performance of welltrained Stable Diffusion model but also simplifies handling Null prompts by converting them into Classifier-Free cases. It does this by controlling additional guidance signals each time we sample zt1 from zt and ˆϵt. Ho & Salimans[16] figured out that it is feasible to use the same model to generate an inherent Classifier-Free Guidance signal p(zt) as long as we drop certain conditions during the training procedure. Since we have three different conditions , b, and p, we drop all the conditions with 0.1 probability and only drop with 0.1 probability. During the inference steps, we can use the CFG to guide the sampling schedule as follows: ˆϵt = (1 + s)ϵθ(zt; b, , p, t) sϵθ(zt; , t) (8) where is the classifier-free guidance scale. We test the FID and CLIP scores for different prompts with different guidance scales s. Figure 10 shows the FIDCLIP curves using different guidance scales ranging from [1.0, 3.0, 4.0, 5.0, 6.0, 7.0]. We set the guidance scale to 4.0 as it usually gives good results across all different prompts. B. Evaluation B.1. Evaluation metrics Our method aims to establish reasonable relationship between foreground objects and background scenes while maintaining the appearance of the object similar to reference image and generating high-quality synthetic image. To assess these two capabilities, we employ two metrics. Firstly, we use the FID score, which is widely used to measure the harmony of images obtained by generative models, to evaluate the quality of synthetic images. We use pretrained Inception model to extract features and calculate the FID score between generated images and ground truth images on the SAM-FB test set. Secondly, we evaluate the appearance of the inserted object and the reference foreground image using the CLIP score. We use CLIP image encoder to extract features. The CLIP score is calculated as the cosine similarity between the two features. B.2. Human Evaluation When comparing the results generated by our methods and baseline methods, we asked user to evaluate the quality of the generated image following these five criteria: Foreground and Background Integration: How natu2 Figure 12. Example of objects with details. Our model could keep the appearance better even with some details compared with SD [39], GLI-GEN [29] and PBE [47]. The first row demonstrates the ability to keep some image texture, and the second row illustrates the ability to keep text texture. rally does the inserted foreground blend with the background? Does it look out of place or integrate seamlessly? Foreground Clarity and Detail: Assess the clarity, detail, and resolution of the inserted foreground. Foreground Appearance Consistency with Reference: Check if the inserted foregrounds appearance (shape, texture) matches the foreground in the reference image. Lighting and shade on Foreground: Evaluate whether highlights and shades on the inserted foreground are realistic and consistent with the background lighting. Color Consistency: Assess the overall color harmony. Do the inserted foreground and background tones, hues, and saturation levels align? We conducted our user survey by asking 10 users to rank outputs from different models in 10 affordance insertion settings, therefore gaining 100 data points. C. More Results C.1. Details Maintaining Using DINOv2 features, the model preserves the appearance more effectively than other image editing models, especially for objects with detailed textures. Figure 12 compares our model with other image editing models. Our model demonstrates the ability to retain the details of an objects appearance, even the texture of the object. C.2. SAM-FB Test Split Figure 13 shows visualization result with different baselines on the SAM-FB test split. Our model generated more authentic results compared to other baseline models. Figure 13. Samples on SAM-FB test split. Our model inserted the bag with an authentic appearance. C.3. Detailed Ablations We show the detailed ablation results on different types of position prompts and the result is presented in Table 6. With all the designs, the model achieves the lowest FID score and the highest CLIP score on average. C.4. In-the-wild Generalization We compare our methods with other baseline models on in-the-wild images, and Figure 14 shows the visualization results for comparison. In the first five rows, we performed the affordance insertion task providing bounding box on both common objects like apples and uncommon objects like cruta. Our methods generated the images with the Figure 14. Example of in-the-wild insertion results with details. Our model could keep the appearance better and adjust the foregrounds properties better compared with SD [39], GLI-GEN [29] and PBE [47] on common objects. In the last row, the model generated reasonable insertion when provided with ambiguous prompts. highest quality, authentic to the reference foreground with proper lighting. In the last row, we show more affordance insertion results when providing ambiguous prompts. The model will automatically find the proper affordance relationship and adjust the location and view of the foreground object. Its notable that in the first case when inserting the cruise, it is actually the view from the back of the reference image. C.5. Video Demo Please refer to the video demo on the project website https://kakituken.github.io/affordance-any.github.io/. C.6. Failure Cases Figure 15 shows some failure cases when using the model to perform affordance insertion. Generally, when prompted null position, it requires the model to search for 4 Figure 15. Some failure cases when using our model to perform affordance insertion. possible position to insert the object. However, if there are already similar objects in the scene e.g., traffic light in the first row, it is easy to mislead the model and end up inserting nothing. When the background is too complex and the foreground object is too small, such as vegetable in supermarket shown in the second row, it is also difficult for the model to insert the object correctly."
        }
    ],
    "affiliations": [
        "Boston College",
        "Cornell Tech",
        "Harvard University",
        "The Hong Kong Polytechnic University"
    ]
}