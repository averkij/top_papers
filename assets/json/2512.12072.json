{
    "paper_title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
    "authors": [
        "Avinash Amballa",
        "Yashas Malur Saidutta",
        "Chi-Heng Lin",
        "Vivek Kulkarni",
        "Srinivas Chappidi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity."
        },
        {
            "title": "Start",
            "content": "VOYAGER: Training Free Approach for Generating Diverse Datasets using LLMs Avinash Amballa, Yashas Malur Saidutta, Chi-Heng Lin , Vivek Kulkarni, Srinivas Chappidi Samsung Research America {a.amballa, ym.saidutta, chiheng.lin, v.kulkarni1, vasu.c}@samsung.com 5 2 0 2 2 1 ] . [ 1 2 7 0 2 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose VOYAGER novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that VOYAGER significantly outperforms popular baseline approaches by providing 1.5 3x improvement in diversity."
        },
        {
            "title": "Introduction",
            "content": "Large language models are now widely used to generate synthetic data to mimic real world data distributions in scenarios where real world data might be hard to gather or available. Such synthetic data are typically used to train downstream predictive models by potentially augmenting it with any real world data that might be available. However, previous work has noted that many LLMs (which have been post-trained) produce generations that are poor in diversity (Wright et al., 2025; Padmakumar and He, 2023). popular approach to encourage diversity in LLM generations is to use different sampling methods like temperature sampling, top-p sampling, min-p sampling, etc. (Kool et al., 2019; Holtzman et al., 2020). However, such sampling methods only change the distribution of the next token being predicted and do not really have global view of the diversity across generations. Additionally, as recently demonstrated, such methods cannot overcome mode collapse and tend to gravitate towards similar semantic clusters, even under aggressive sampling conditions, such as very high temperatures (Jiang et al., 2025). Yet another popular approach is prompt based diversity control where prompts explicitly contain instructions requesting the LLM to generate data from specific topics or sub-topics. While this method is applicable to closed-weight models, it has limited scalability and requires domain knowledge and expertise to ensure sufficient coverage. Finally, it is also possible to incorporate measure of diversity in the post-training process of the LLM itself, where the reward encodes some measure of diversity (Li et al., 2025; Chen et al., 2025). These approaches explicitly encourage diverse generations; however, they are computationally expensive and are restricted to open weight LLMs as they need access to the underlying model weights. We introduce VOYAGER, novel training-free framework for generating synthetic data using an LLM while encouraging diversity. Grounded in rigorous mathematical theory, our approach directly optimizes geometric property intrinsic to dataset diversity. Crucially, VOYAGER operates without accessing model parameters, making it computationally efficient and fully compatible with closedweight (black-box) models. VOYAGER is inspired by the following intuitive analogy. Imagine that you want to build map of large area. You rely on crew of explorers whom you send on series of voyages. When an explorer returns, you accept their explored regions if it is significantly different/diverse from prior explored regions. Furthermore, when we reject certain regions, we instruct future explorers to avoid exploring these regions. In practice, because checking newly returned region with all other explored regions can be impractical, the central command station which needs to be efficient relies on an anchor set of salient regions explored and uses this Figure 1: Overview of VOYAGER We iteratively seek to explore new diverse regions of the data manifold via set of successive voyages carried out by explorers. Each explorer explores certain region of the manifold. Regions that are very similar to prior explorations are rejected by the central command which keeps track of key set of salient regions explored (the anchor set). New explorers are encouraged to explore areas different from prior explorations (via prompt refinement). anchor set to guide future explorations. Thus, successive voyages are encouraged to explore diverse regions and span the entire large area. Similar to the above analogy, our explorer explores the data manifold, keeps track of the explored areas and accepts explorations along areas that are significantly different from the already explored manifold. Additionally, to maintain computational feasibility, the agent only maintains fixed size representative set of regions. However, when new regions of the manifold are explored, we prune the set in very specific manner, maintain the set with the largest volume. In this way, we can instruct future explorations to avoid this region and focus on expanding the volume, thus increasing the diversity of the collected data. VOYAGER formalizes the above analogy by grounding it in the mathematical theory of determinantal point processes. We empirically evaluate our approach against strong baselines and show that it consistently outperforms these baseline methods on suite of tasks. In nut-shell, our contributions are as follows: Principled approach to maximizing diversity: Our approach maximizes diversity by formulating diversity in terms of geometric metric (volume). This enables to leverage connections to the machinery of determinantal point processes to optimize the above metric and provide theoretical justification for our approach. Applicable to both open and closed weight models Our approach is training-free,requires no access to model weights, and works for both open and closed weight models. Strong empirical performance Finally, we demonstrate the effectiveness of our approach by showing strong empirical performance over baseline methods (1.5 3x improvement in diversity)."
        },
        {
            "title": "2 Related Work",
            "content": "Related work can be grouped into two main groups training-free methods, and training-based methods both of which we discuss below."
        },
        {
            "title": "2.1 Training-free Methods",
            "content": "Training free methods do not require any weight updates to the model. Training free methods try to encourage diversity in outputs by (a) designing sampling and decoding methods that operate on model logits (b) control diversity through prompts (prompt diversity control). Sampling and Decoding Methods Sampling and decoding base methods include minimum probability-sampling, temperature-sampling, nucleus sampling, arithmetic sampling (Kool et al., 2019; Holtzman et al., 2020; Amballa et al., 2025; Hewitt et al., 2022; Minh et al.; Chang et al., 2024). Sampling based methods still need access to the raw model outputs/logits. They are also fundamentally limited in the diversity they can accomplish because they can-only change the output distribution of the next-token at each step. Because of this limitation, generations may be in-coherent, grammatically incorrect or even off-topic. Decoding methods like (Vijayakumar et al., 2016) use form of beam-search which rewards diversity and encourages yielding diverse generations. The diversity typically is limited to lexical diversity and is localized to the beam. Such methods typically still require access to models log probabilities (for scoring) and thus do not work for closed weight models that do not typically output log probabilities. Our method in contrast to all of the above, has notion of global diversity, optimizes proxy for that metric, and does not require access to LLM logits or the weights. Prompt Diversity Control Prompting based methods operate only on the LLM prompt and thus work for both open and closed weight models. Here, the main approach has been to either explicitly add instructions to the prompt to encourage diversity by prompt engineering. These include instructions like make the output more diverse or generate from this topic. Wong et al. (2024) propose to identify key dimensions (axes) and values that dimensions can take to partition the full space. They then estimate probability values (by asking an LLM) for different dimensions. This allows them to sample different configuration of the dimensions with their values (based on the probabilities) which they then append and instructions to the seed prompt. Similarly Samvelyan et al. (2024) first define set of attack categories, and styles to generate diverse collection for the safety domain. They then use an LLM to mutate seed prompts and create new prompts or candidates which cover dimensions of the grid not covered by the parents process they repeat iteratively. These works typically focus on diversity across specific dimensions or axes (sometimes determined apriori). The diversity can be limited by the granularity of topics (or dimensions). Secondly, while such methods optimize for topical diversity, topical diversity does not necessarily imply semantic non-redundancy. Generations from two distinct topics may still be semantically redundant due to lot of shared vocabulary overlap and the underlying generations may still span low volume (less diverse). Our method unlike the above, directly seeks to maximize the spanned volume of the data (and moves beyond topic diversity). 2.2 Training-based Methods Training based methods focus on improving diversity by designing post-training methods where model weights are finetuned to encourage diversity (Li et al., 2025; Yao et al., 2025; Chen et al., 2025; Ismayilzada et al., 2025; Lanchantin et al., 2025). Works like (Li et al., 2025; Yao et al., 2025; Ismayilzada et al., 2025; Lanchantin et al., 2025) devise specific reward functions that include some notion of diversity. Thus, they do not work on closed source models and even when model weights are available, they incur significant training overhead due to the extensive post-training. Wang et al. (2024) 1 propose to use volume as measure fo diversity to measure the diversity of various instruction tuned datasets and their effectiveness in training focusing on dataset selection. While our work shares the commonality of operationalizing diversity as volume metric, we differ significantly. While Wang et al. (2024) focus on measuring diversity and propose metric for that, we in turn present novel iterative algorithm to generate diverse dataset (in streaming fashion) using LLMs. Furthermore, we also draw on novel connections with text-based gradients to enable our algorithm to be training-free."
        },
        {
            "title": "3 VOYAGER – Background",
            "content": "Our method VOYAGER relies on two key pieces of machinery: (a) Determinantal Point Processes (b) Prompt refinement using textual gradients, both of which we will briefly provide relevant background here. 1Concurrent work that is closest to ours is that of (Chen et al., 2025) who propose to include the determinant of the similarity matrix (similar to our observation) in to the posttraining process. Our work differs from theirs in at-least two ways. First, their approach is restricted to models that are open weight as they propose weight tuning/post-training method. Second, their work is limited to aligning model to generate diverse responses for given input. However local diversity in batch of responses does not necessarily yield diverse dataset globally. We in turn focus on method to generate diverse dataset seeking to optimize diversity globally. Determinantal Point Processes Our method is rooted in the key observation that the determinant of kernel similarity matrix can be measure of the spread/diversity of data-set. More specifically, the determinant of such kernel similarity matrix represents the square of the volume spanned by the feature representations of data points in data set. Furthermore, the larger the volume, the more diverse the dataset. In addition to the above, we will also leverage the machinery of determinantal point processes, which have been used extensively to encourage diversity in recommendation systems (Kulesza et al., 2012). Determinantal Point Process (DPP) is probabilistic model for subset selection where the probability of selecting subset is proportional to the determinant of the corresponding kernel matrix. For ground set and positive semi-definite kernel matrix RYY, the probability of selecting subset is: P(S DPP(K)) det(KS) (1) where KS is the submatrix of indexed by elements in S. The determinant det(KS) measures the volume spanned by the vectors corresponding to items in S, directly encoding diversity: higher determinants (volumes) correspond to more diverse subsets with less redundancy. Sampling from determinantal point-process will thus favor more diverse subsets. Over the past decades, several efficient algorithms have been developed to sample set of items (including set of items) from DPP both in an exact and approximate manner (Kulesza et al., 2012; Li et al., 2016; Chen et al., 2018). Prompt Refinement using Textual Gradients This is prompt refinement procedure Pryzant et al. (2023) where the goal is to iteratively refine seed prompt to encourage the seed prompt to better align with an extrinsic reward metric of the generations. The procedure is very similar to gradient descent, but instead operates purely in the text space. It consists of the two main steps: (a) Compute the local loss of LLM generations on sample (in relation to reward metrics). (b) Obtain gradients that identify how to reduce the local loss. This is typically implemented by asking an LLM to critique the generations and identify reasons that make the loss worse (similar to gradients being directions which will increase the loss function in machine learning) (c) Finally, the prompt Algorithm 1 VOYAGER Input: p:task prompt l:desired dataset size τ : Marginal gain threshold b:max number of explorers in each step k:maximum size of anchor point set : Maximum number of iterations Output: D: Constructed dataset 1: = {} 2: Φ = {} 3: = {p} 4: for 0 to do {} 5: for all do 6: # Initialize dataset # Initialize anchor set # Initialize set of explorers # Successor set 7: 8: 9: 10: 11: 12: 13: 14: d, A, = EXPLORE(e, Φ, τ, Ka) Φ SAMPLEDPP(A, k, Ka) if then return end if end for SAMPLEDPP(C, b, Ke) 15: 16: end for 17: return is refined/edited by applying the negative of the gradients which corresponds to editing/refining the existing prompt to fix the problems identified in the \"gradients\"."
        },
        {
            "title": "4 VOYAGER – Algorithm",
            "content": "VOYAGER mathematically operationalizes the notion of diversity of set by the determinant of the corresponding similarity matrix of the set (encoded using suitable kernel) and seeks to approximately maximize this measure iteratively."
        },
        {
            "title": "4.1 Overview",
            "content": "At high level, our algorithm requires (see Figure 1) task prompt that describes the specific data generation task (eg, Generate poem). The algorithm maintains fixed-size set of explorers (E) and fixed-size set of anchor data-points denoted by that capture representative/diverse regions of the full underlying dataset. In each iteration of the outer loop, we pick the next explorer in the list and let the explorer perform voyage (Line 7). This returns new batch of data d, an augmented set of potential anchors A, and successor set of candidate explorers s. We add this new batch of Algorithm 2 EXPLORE() Input: e: explorer Φ: Anchor Set τ : Marginal Gain Threshold KSim: Similarity Kernel Output: S: Data instances Φ: Augmented set of potential anchors C: Successor explorers 1: Prompt an LLM using to generate B, batch of instances (1 LLM call). 2: {} 3: {} 4: {} 5: for all do 6: # Successor set # Selected instances # Hold errors if MARGINALGAIN(w, KSim, Φ) τ then 7: 8: 9: Add to and to Φ else Add to rejected set end if 10: 11: end for 12: if > 0 then 13: Prompt an LLM to get gradients: = {g1, ..., gm} = LLM(e, R, Φ) (1 LLM call) Prompt an LLM to apply gradients on to create new set of explorers (1 LLM call) 14: 15: end if 16: return S, Φ, data to the partially constructed dataset and also add the set of newly generated candidate explorers to (Lines 8-9). Because VOYAGER maintains fixed-size set of anchor data-points to capture representative and diverse regions, and the newly added data-points could be potential anchor points, we update the anchor set Φ by sampling diverse points from the augmented set (Line 10). Finally, after the current beam of explorers is done, we have new set of candidate explorers C. Once again, for computational efficiency, we select at most explorers for the next iteration. Because we favor diverse explorers, we once gain sample explorers from from the underlying DPP and use that sampled set in the next iteration (Line 15). Algorithm 3 COMPUTEMARGINALGAIN() Input: w: Item whose marginal gain needs to be computed K: Similarity kernel function to use. A: Set of items already present Output: γ: Marginal gain in volume if is added to the set. # Construct Similarity kernel = K(A) = K(A w) 3: γ = det(S) det(S) return γ Algorithm 4 SAMPLEDPP() Input: U: Set of items n: number of items to sample K: Similarity kernel function to use. Output: S: subset of items sampled from underlying DPP. # Construct likelihood kernel = K(U ) = FINITEDPPSAMPLE(L, n) # Sample from finite DPP 3: return be added to the anchor set. Once again, we rely on the notion of volume of the similarity matrix of set of items as measure of diversity (see Algorithm 3). If the marginal gain in volume is greater than specified threshold τ , we add the instance to the dataset (S) and the anchor set Φ (Line 1-7). It is important to note that computing the marginal gain of an instance over the entire dataset (which increases in size) is computationally expensive. We approximate this by computing marginal gain with respect to instances in the anchor set which is of fixed size. Next, if we have rejected instances (which we track in the set R), we know that the explorer can be refined with regard to the diversity metric. We thus perform prompt refinement using textual gradients to obtain new set of successor explorers (Lines 13-15)."
        },
        {
            "title": "4.1.1 Exploration Step",
            "content": "Our EXPLORE procedure consists of three main steps: First, given an explorer, we generate batch of data by calling an LLM with the prompt characterizing that explorer. We then score each data instance on its marginal gain in volume if it would Having described the main algorithm in the previous section, we now draw on connections to the mathematical theory of determinantal point processes and matrix theory to justify our algorithm. Our global objective is to construct final similarity matrix ST (that corresponds to the final dataset DT ) with high effective rank. As noted by Friedman and Dieng (2022), who propose the Vendi Score as diversity metric, high effective rank of ST implies high diversity. Let ST be an by square matrix. Let the effective rank of ST and denote the determinant of ST . Let be the trace of S. We now state the following lemma (which we will prove later) Lemma 1. The effective rank of ST can be roughly approximated by determinant of ST as n2 D1/n (2) Based on the above lemma, we seek to maximize det(ST ) while also noting that det(ST ) is the squared volume of DT . However, since directly maximizing det(ST ) in an iterative manner is computationally infeasible, we reduce this problem to proxy problem namely, the maximum volume submatrix problem (MVS) problem. The MVS problem is to maximize the determinant of the best (k by k) principal sub-matrix of ST (selection of data points). VMVS(ST ) = max ΦDT ,Φ=k det(SΦ)(1/2) (3) This is good proxy because it has been observed by Goreinov and Tyrtyshnikov (2001); Cortinovis et al. (2020) that the MVS is quasibest low-rank approximation to the original matrix ST . So we attempt to maximize the MVS of size of ST as we construct the dataset. Unfortunately, even the above is NP-hard as we (cid:1) subsets of the original ST . need to check all (cid:0)n It is precisely here that we rely on the machinery of determinantal point processes to find highvolume solution. While determinantal point processes do not find the exact solution to the above problem, k-item sample drawn from the underlying determinantal point process will favor highvolume subsets. We make use of this property of determinantal point processes as follows: (a) Define an anchor set ΦT whose volume will be VOL(ΦT ) = (det SΦT )1/2. We iteratively construct this anchor set in greedy manner, trying to maximize its volume, which is what VOYAGER does. More specifically, note that when we generate batch (see Algorithm 2), we sequentially add an instance to the dataset (and to the underlying anchor set) only if the instance increases the volume of the anchor set by specific threshold. This step seeks to make the anchor set reservoir that is in some sense volume optimized (holding diverse points). However, at this point, we have an augmented anchor set AT whose size exceeds the fixed size (k). We thus prune it to size but do it once again in manner seeking to maximize local volume. We update ΦT by drawing item sample from AT using k-DPP which prefers subset of size with high volume and will be representative of the MVS volume of AT and provides rough lower bound on VMVS(ST ). When the algorithm finishes, we have high-volume anchor set (by construction). This in turn translates to high VMVS(ST ) which in turn translates to high effective rank of ST and thus significantly more diverse DT . 4.2.1 Proof of Lemma 1 Proof. To recap, ST is square by similarity matrix, denotes the effective rank of ST , and denote the determinant of ST . Let be the trace of ST . Let λ1, λ2, , , , λn be the eigenvalues of ST . We will use Taylor approximations of two mathematical quantities to derive an approximation between the two quantities. We first define some quantities and notation. From the definition of effective rank (Roy and Vetterli, 2007), the effective rank of ST , eH , where = (cid:88) λi ln (cid:19) (cid:18) λi . (4) For ease of writing, let pi λi . pi is just the normalized eigenvalue so that the set of pi forms probability distribution. Let Gp = n(cid:112)Πn 1 pi be the geometric mean of the normalized eigen values and Ap = 1 be the arithmetic mean of the normalized eigenvalues. Taylor approximation of H: Let us consider second order Taylor approximation of around the uniform distribution 1 . Each co-ordinate pi = 1 + δi. The Taylor approximation up to two terms is ln + (ln(n) 1) (cid:88) δi 2 (cid:88) δ2 . (5) However, since pi is probability disribution it follows that (cid:80)n 1 δi = 0. Thus, ln 2 Σδ2 . (6) Taylor approximation of ln Gp: Similarly let us consider second order Taylor approximation of ln Gp around the uniform distribution 1 . Performing the Taylor approximation and simplifying yields: ln Gp ln 2 Σδ2 . Note that ln Ap = ln n. From this we get ln Gp Ap = 2 Σδ2 From Equations 6 and 8, we can now write: ln + ln Gp Ap (7) (8) (9) The effective rank eH is therefore approximately 2. = n2Gp nGp Ap The final step is to just write Gp = n(cid:112)Πn 1 pi in terms of original eigenvalues. Substituting pi = λi yields Gp = in terms of the determinant and trace of S. Substituting this form of Gp into the right hand side of equation for eH yields: 1 EFF(ST ) = eH 1 n2D (10) This proves the claim and also the lemma on why the effective rank of similarity matrices can be roughly approximated by the determinant."
        },
        {
            "title": "4.3 Computational Efficiency",
            "content": "Because VOYAGER restricts the size of the explorers and the anchor set in each iteration, our algorithm is quite computationally efficient in terms of CPU time complexity. The time complexity mainly depends on the maximum size of the anchor point set kmax, the maximum size of the candidate beam bmax, the batch size B, and the maximum number of overall iterations . Observe that the MARGINALGAIN can be computed in O(k2) time if the inverse of Ksim can be pre-computed and cached. This means that we can process all instances in O(Bk2), assuming constant time penalty for LLM calls and set addition. The EXPLORE call incurs O(Bk2) cost. The pruning of the anchor set to size (Line 10) using 2The ratio of geometric mean to arithmetic mean normalized eigen values is called the spectral flatness measure and is an important concept in signal processing https: //en.wikipedia.org/wiki/Spectral_flatness. DPP costs O(k3 max) time. Thus, the entire inner for-loop (Line 6) incurs cost O(b(k3 max + Bk2)). The sampling step for pruning the set of explorers using second DPP takes O(b3 max) cost. The total cost incurred by the algorithm therefore is O(T (b(k3 max + Bk2) + b3 max)). Finally, note that VOYAGER in general is significantly more efficient than naive approach that could use DPP to sample the entire dataset by first generating universe larger than the required size and then selecting from it. That would incur cost O(l3) where is the size of the full dataset requested, which, when is large, can be prohibitive. Having outlined the worst case cost of VOYAGER above, we now ask if we can roughly estimate how many iterations of the outer-loop would run in an average case. This will largely depend on the fraction of data-points accepted in EXPLORE step. Let ζ be the fraction of candidates accepted on an average in each call to EXPLORE. That means, ζB instances are on average accepted per iteration. So we should expect the outer loop to run about Tavg = ζB times. Intuitively, ζ can be thought of as representing the inherent easiness of generating diverse data for the task using the LLM. If the LLM is easily able to generate diverse data for the task and find prompts easily to obtain diverse data, ζ will be high (and low otherwise). Finally, we note that this also gives an average estimate of the total number of LLM calls NLLM = ρl ζB as we make constant number of LLM calls, ρ (we have ρ = 3) in EXPLORE per call."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we report on comprehensive experiments to evaluate our proposed method on its effectiveness to generate diverse datasets."
        },
        {
            "title": "5.1 Generation Tasks and Evaluation Metrics",
            "content": "Generation Tasks We consider the two different categories of text generation tasks: (a) creative writing and (b) reasoning to evaluate our method against strong baselines. In the creative writing category, we consider four generation tasks: (a) topical sentence generation, (b) conversation generation (c) poem generation, and (d) movie plot generation. In the reasoning category, we consider the tasks of (a) grade school mathematical question generation and (b) Logical reasoning question generation. Method (a) Generate single sentence about sports Cosine Lexical Vendi Quality LLM Method (b) Generate short conversation about politics Cosine Lexical Vendi Quality DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.67 0.17 0.70 0.15 0.78 0.13 0.79 0.12 0.85 0.10 0.81 0.11 0.21 0.09 0.22 0.09 0.41 0.17 0.33 0.12 0.54 0.14 0.41 0.17 2.99 3.23 7.87 6.00 15.07 7.77 21.70 0.75 21.71 0.75 23.10 0.82 22.15 0.70 22.85 0.98 22.22 0. 50 50 50 50 550 500 DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.72 0.05 0.74 0.05 0.73 0.04 0.70 0.07 0.77 0.05 0.73 0.05 0.25 0.08 0.27 0.09 0.16 0.04 0.26 0.11 0.38 0.10 0.26 0.08 4.59 5.16 3.30 4.24 8.45 4.85 22.97 0.68 23.03 0.67 23.75 0.88 22.97 0.64 23.32 0.62 23.13 0. VOYAGER 0.87 0.06 0.55 0.13 24.13 22.26 2.06 443.00 VOYAGER 0.82 0.05 0.44 0.09 15.04 23.92 0.88 LLM 50 50 50 50 550 500 (c) Generate poem Cosine Vendi Lexical Quality LLM Method (d) Generate plot for movie Cosine Vendi Lexical Quality LLM Method DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.76 0.05 0.78 0.05 0.78 0.04 0.71 0.05 0.82 0.06 0.76 0.05 0.15 0.05 0.16 0.04 0.14 0.04 0.11 0.03 0.30 0.08 0.16 0.04 3.00 3.22 2.76 2.30 5.68 3.08 22.52 0.89 22.65 0.82 22.79 1.46 22.45 0.92 22.56 1.31 22.52 0.88 VOYAGER 0.86 0.05 0.30 0.08 7.31 24.51 0.91 50 50 50 50 550 500 615 DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.80 0.05 0.83 0.05 0.81 0.04 0.78 0.05 0.84 0.05 0.81 0.05 0.21 0.08 0.26 0.08 0.25 0.07 0.22 0.07 0.32 0.08 0.23 0.08 4.00 5.52 5.28 4.28 7.66 4.57 23.04 0.53 23.08 0.60 23.46 1.99 23.15 0.78 22.99 0.92 23.03 0.54 VOYAGER 0.84 0.05 0.34 0. 8.30 22.96 1.12 50 50 50 50 550 500 695 Table 1: Evaluations of VOYAGER on creative tasks. Note that VOYAGER outperforms all baselines significantly (average Vendi score improvement of 296% over DEFAULT, 43 % over HIERARCHICAL across all creative tasks) with no significant degradation on perceived quality. Legend Best result in each column is highlighted in Bold and the second-best result in each column is highlighted in underline. Quality metrics are on scale of 0(lowest) 25(highest). (a) Generate mathematical question for Grade school math Cosine Quality Lexical Vendi Method LLM (b) Generate simple logic puzzle suitable for Grade school students LLM Cosine Method Quality Lexical Vendi DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.54 0.13 0.56 0.12 0.47 0.06 0.30 0.15 0.68 0.10 0.57 0.12 0.20 0.08 0.22 0.08 0.07 0.02 0.24 0.12 0.40 0.11 0.22 0. 3.04 3.56 1.65 3.13 8.72 3.48 14.99 0.12 15.00 0.10 15.00 0.00 14.83 0.58 15.00 0.06 14.98 0.17 VOYAGER 0.81 0.06 0.48 0.10 18.78 14.77 0. 50 50 50 50 550 500 399 DEFAULT TEMP DIVERSE HISTORY HIERARCHICAL SUBSETSELECT 0.59 0.10 0.63 0.10 0.62 0.08 0.35 0.11 0.65 0.08 0.62 0.10 0.20 0.09 0.25 0.11 0.15 0.05 0.27 0.11 0.33 0.11 0.24 0.10 3.31 4.47 2.83 3.98 7.02 4. 14.83 0.69 14.80 0.90 14.92 0.44 14.99 0.13 14.88 0.55 14.80 0.72 VOYAGER 0.79 0.06 0.41 0.08 13.26 14.64 0.97 50 50 50 50 550 393 Table 2: Evaluations of VOYAGER on reasoning tasks. Note that VOYAGER outperforms all baselines significantly (average Vendi score improvement of 412% over DEFAULT, 102 % over HIERARCHICAL across all reasoning tasks) with no significant degradation on perceived quality. Legend Best result in each column is highlighted in Bold and the second-best result in each column is highlighted in underline. Quality metrics are on scale of 0(lowest) 15(highest). Diversity Metrics Because there is no universal consensus on metrics for evaluating diversity, we consider few popular metrics for diversity capturing different facets. Cosine Distance: We consider popular metric, the mean cosine distance between pairs of instances as measure of diversity. Lexical Distance: While the cosine distance metric captures semantic diversity, it may not capture lexical diversity very well. Therefore, we also consider lexical diversity metric. To compute lexical diversity, we use the Jaccard distance and report the mean Jaccard distance (removing stop words) over pairs of instances. Vendi Score: Vendi score (Friedman and Dieng, 2022) is popular diversity metric that seeks to capture diversity of the dataset in broader and general sense (potentially including all the above dimensions). It seeks to measure the effective number of data instances that can represent the data and has been recently used quite widely. Quality Metric We also seek to capture the impact on quality of the generations as we seek to encourage diversity. Consequently, we also measure the quality of the generations using an LLMas-Judge framework where the judge evaluates the generation on task-specific rubric (eg, dimensions like faithfulness, coherence, etc.). The specific prompts and rubrics used are available in the Appendix. LLM Calls We also track the number of LLM calls each method to generate the same size dataset (assuming all methods call the LLM to generate the same batch size of data)."
        },
        {
            "title": "5.2 Baseline Methods",
            "content": "We consider the following strong baseline methods: Vanilla Generation - DEFAULT: We compare against the baseline generation, which simply asks the LLM (with default parameters) to perform the generation task. Temperature based sampling - TEMP: One lever to encourage diversity in LLM generations is the temperature parameter used to Method Lexical Cosine Vendi Score Quality (max 25) LLM calls VOYAGER-RANDOMEXPLORER VOYAGER 0.85 0.07 0.44 0.14 0.85 0.07 0.47 0.15 11.85 14.28 21.47 1.88 21.88 1.51 361 Table 3: Ablation for DPP used in selecting explorers on the sports task. Observe that choosing random sample of explorers results in slightly lower diversity and also incurs more LLM calls as opposed to choosing diverse set of explorers confirming our hypothesis that diverse explorers can make search through the space more effective. Method Dtrain Gemma 2B-IT Gemma 7B-IT DEFAULT VOYAGER VOYAGER 1000 1000 500 13.1 16.4 12. 35.7 45.7 42.8 Table 4: GSM8K zero-shot test accuracy on Gemma trained on data generated using VOYAGERcompared to the DEFAULT baseline. Note that VOYAGER which generates more diverse data leads to improved predictive performance and training data efficiency compared to the baseline further underscoring the importance of diversity during training. scale the LLM output logits when sampling tokens. We thus consider higher temperature values like 2.0. Generate diverse command - DIVERSE: Yet another strong baseline is to explicitly ask the LLM to be \"diverse\" in its output by appending it to the user task instructions appropriately. Conditioned on history HISTORY: Another approach to encourage LLMs to generate diverse instances is to provide history of prior generated data instances (of fixed window) and ask it to avoid generating such instances. Hierarchical Prompting - HIERARCHICAL: Here we ask the LLM to first generate diverse topics and then actual instances conditioned on those topics to encourage diversity. Subset Select - SUBSETSELECT: We also evaluate whether just using k-DPP to sample the dataset from larger set (universe) generated using simple and popular methods like temperature sampling would be effective. One advantage of this baseline is that it can try to leverage already generated data and just seek to select diverse subset from it. While this method does indeed attempt to maximize the volume metric we operationalizes for diversity, it has two limitations: First, it is datasetselection procedure and the diversity of the dataset is inherently limited to some degree by the samples in the pre-generated universe (for eg. if the universe does not already contain very high diversity). Second, the is method is not computationally scalable as it is of cubic complexity in the size of the dataset. 5.3 Experimental Settings Here, we outline all the experimental settings used in our experiments. Similarity Kernel Functions Our similarity kernel function is convex combination of an radial basis function kernel using embeddings of the text, and lexical similarity kernel using Jaccard similarity. The weights of the convex combination are 0.7 and 0.3 for radial basis function kernel and lexical similarity kernel respectively. LLMs All experimental data were generated using GPT-4o mini. For text embeddings, we utilize OpenAIs text-embedding-3-small model. HIERARCHICAL baseline settings For the hierarchical baseline, in each call, we generate 10 subtopics to ensure comprehensive coverage of the problem space. We then generate 1 sample for each such topic to return 10 instances. VOYAGER Hyperparameters Our experiment settings are as follows:Number of explorers (b): 3, Number of anchor points (k): 10, Maximum iterations (T ): 200, Samples per LLM call (B): 10, Target dataset size (l): 500. For initializing τ , we followed the initialization procedure in Appendix 1.1. See Appendix 1.2 for the meta-prompts used in the refinement procedure. Evaluation Jury All generated outputs were evaluated by panel consisting of GPT-4, GPT4o, and GPT-4.1, providing robust assessment framework. We use the mean function to obtain jury consensus scores for each dimension in the rubric. The prompts used for jury evaluation are available in the Appendix 1.3."
        },
        {
            "title": "5.4 Results and Analysis",
            "content": "Creating Writing Tasks Tables 1 show the results of our experiments for the creative writing tasks, from which we can make the following observations. First, note that temperature based methods improves over the baseline in terms of diversity metrics (all diversity metrics) with no significant decrease in quality. Second, with regards to the prompt-diversity control method, the DIVERSE-KEYWORD baseline significantly improves diversity (across all metrics) over both the default baseline and temperature based baselines, suggesting that explicit instructions to make the output diverse is helpful. Third, the history based prompting outperforms the baseline marginally but not as strongly as the explicit DIVERSE KEYWORD approach. This may be because the HISTORY based approach seeks to ensure non-redundancy with the prior history (of fixed size) but that does not necessarily yield diverse outputs overall. One advantage of these baselines is that to construct dataset of size with batch size they will make exactly LLM calls irrespective of the task. However this efficiency in terms of LLM calls comes at significant cost (significant loss of diversity). VOYAGER in contrast enforces quality control to obtain diversity and thus generally pays cost in terms of more LLM calls. Fourth, the hierarchical prompting approach very significantly outperforms other methods in this class. This suggests that incorporating domain knowledge and explicit instructions to explore different topics significantly improves diversity. Finally, all flavors of VOYAGER significantly outperform all other methods, significantly suggesting the effectiveness of VOYAGER as we directly optimize quantitative measure of diversity (volume) with no significant degradation in quality. It is also worth noting that VOYAGER also makes significantly fewer LLM calls (in most cases) compared to HIERARCHY, the best competing baseline, and suggests that VOYAGER has generally better costbenefit tradeoff (LLM calls vs diversity). Reasoning Tasks Tables 2 show the results of the experiments for the reasoning tasks. Many of the observations noted in the creative writing experiments hold for the reasoning tasks as well. Most notably, VOYAGER significantly outperforms all baselines on the reasoning tasks as well. Another noteworthy observation is that the DIVERSE KEYWORD baseline performs worse than the DEFAULT baseline for the grade school math generation task. Figure 2: Rejection rate of samples within batch over time with textual gradients enabled vs disabled to generate the same dataset size for the sports task (all other settings identical). Note that textual gradients helps significantly in enabling the algorithm to have lower rejection rate and also run faster (smaller number of timesteps) highlighting the importance of feedback and prompt refinement. This reveals potential limitation of the DIVERSE KEYWORD methods namely, simply asking the LLM to be diverse has its limitations and does not necessarily yield diverse output."
        },
        {
            "title": "5.5 Ablation Studies",
            "content": "Here we conduct two ablation studies to obtain deeper insighst into the empirical performance of VOYAGER. Effect of Diverse Explorers Having demonstrated the effectiveness of VOYAGER we further analyze our algorithm empirically by performing the following ablation. To evaluate the impact of encouraging diversity in the explorers by sampling from DPP, we replace the selection of successors to just be random sample of the same size since it is not critical to our algorithm. We call this algorithm variant VOYAGERRANDOMEXPLORER. The main advantage of VOYAGER-RANDOMEXPLORER is that we do not need to incur the cost of sampling from k-DPP which (C3) in each main iteration of the loop. This potentially trades off some diversity and efficient convergence for reduced local computational time (a choice that practitioners have), which we validate empirically in Table 3. Note that VOYAGERRANDOMEXPLORER shows drop in diversity, quality and requires more LLM calls to generate model performance than the baseline, suggesting the effectiveness of our method and more generally the effectiveness of training on diverse data (35.7 vs 45.7 see column Gemma-7b-it). In fact, we noted that using only 500 examples, we were able to almost match (and even outperform) the performance of models trained on data generated by the DEFAULT baseline, further underscoring the importance of diversity in training data, an observation noted by several prior works (Bukharin et al., 2024; Pang et al., 2024)"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we proposed VOYAGER, training free approach to use LLMs to generate more diverse data. Our method relies on the determinant of the similarity matrix of dataset is related to the volume spanned by the data and can be measure of the diversity of the data. Building on this observation, we propose an iterative algorithm that seeks to approximately maximize this measure of diversity by leveraging the machinery of determinantal point processes and prompt refinement using textual gradients. To conclude, our method significantly improves the diversity of generated data using an LLM, is training-free and scalable, compared to baseline methods."
        },
        {
            "title": "Limitations",
            "content": "Our work is not without its limitations. First, we focus only on diversity in text generation using LLMs. We do not consider diversity of multi-modal data which can pose additional challenges like how to measure similarity of representations across multiple modalities. We use automated metrics for evaluations primarily due to expedience. human evaluation of perception of diversity and quality could also be considered. the same number of instances as VOYAGER confirming our observation above. Effect of Textual Gradients Next, we also ablate the prompt refinement step using textual gradients to evaluate its effectiveness. In particular, while we still attempt to maximize our operationalize notion of diversity by selecting only instances which marginal gain above threshold, we disable prompt refinement. This means no new explorers will be generated and we will only use the same explorer many times (with variance in output only due to the LLM call). We compare this with setting (where prompt refinement is enabled) but restrict the beam size to exactly be 1, so that we have an apples-to-apples comparison with the setting where the same explorer is used across all iterations. Our hypothesis is that disabling the prompt refinement would result in significantly higher average rejection rate (the fraction of instances rejected because they did not meet the marginal gain criteria in EXPLORE step), and the algorithm would have to run for many more iterations. Figure 2 shows the results of this ablation. Observe that disabling the textual gradients results in much higher average rejection rate and number of iterations confirming our hypothesis that allowing for feedback to refine the prompt can help make the search more efficient."
        },
        {
            "title": "Training Data Generation",
            "content": "Finally, in addition to evaluating our method against baselines on quantitative metrics for diversity, we also evaluate whether model trained on data using VOYAGER would result in higher model performance compared to model trained on synthetic data generated by the default baseline approach. To do this, we consider the GSM8K task. We generate 1000 questions using the prompt Generate mathematical question for Grade School Math. We use 5-shot prompting for GPT4 (OpenAI et al., 2024) to answer these questions to obtain (question, answer) pairs for the training data. We then train Gemma-2b-it and Gemma-7b-it models (instruction-tune) using just these 1000 examples. We compare against models trained on the same number of examples but generated using temperature baseline. We evaluate both settings on the standard GSM8K test set (Cobbe et al., 2021). Results are shown in Table 4. We note that data generated using VOYAGER yields significantly higher"
        },
        {
            "title": "References",
            "content": "Avinash Amballa, Aditya Parashar, Aditya Vikram Singh, Jinlin Lai, and Benjamin Rozonoyer. 2025. Quasi-random multi-sample inference for large language models. In Frontiers in Probabilistic Inference: Learning meets Sampling. Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo Zhao, and Haoming Jiang. 2024. Data diversity matters for robust instruction tuning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 34113425. Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, and Tagyoung Chung. 2024. Real sampling: Boosting factuality and diversity of openended generation via asymptotic entropy. arXiv preprint arXiv:2406.07735. Laming Chen, Guoxin Zhang, and Eric Zhou. 2018. Fast greedy map inference for determinantal point process to improve recommendation diversity. Advances in neural information processing systems, 31. Yilei Chen, Souradip Chakraborty, Lorenz Wolf, Yannis Paschalidis, and Aldo Pacchiano. 2025. Post-training large language models for diverse high-quality responses. arXiv preprint arXiv:2509.04784. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Alice Cortinovis, Daniel Kressner, and Stefano Massei. 2020. On maximum volume submatrices and cross approximation for symmetric semidefinite and diagonally dominant matrices. Linear Algebra and its Applications, 593:251268. Dan Friedman and Adji Bousso Dieng. 2022. The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410. Sergei Goreinov and Eugene Tyrtyshnikov. 2001. The maximal-volume concept in approximation by low-rank matrices. Contemporary Mathematics, 280:4752. John Hewitt, Christopher Manning, and Percy Liang. 2022. Truncation sampling as language model desmoothing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3414 3427. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, and Yejin Choi. 2025. Artificial hivemind: The open-ended homogeneity of language models (and beyond). Wouter Kool, Herke Van Hoof, and Max Welling. 2019. Stochastic beams and where to find them: The Gumbel-top-k trick for sampling sequences without In Proceedings of the 36th Internareplacement. tional Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 34993508. PMLR. Alex Kulesza, Ben Taskar, et al. 2012. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(23):123286. Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, and Ilia Kulikov. 2025. Diverse preference optimization. arXiv preprint arXiv:2501.18101. Chengtao Li, Stefanie Jegelka, and Suvrit Sra. 2016. Efficient sampling for k-determinantal point processes. In Artificial Intelligence and Statistics, pages 1328 1337. PMLR. Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, and Tianlu Wang. 2025. Jointly reinforcing diversity and quality in language model generations. arXiv preprint arXiv:2509.02534. Nguyen Nhat Minh, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid ShwartzZiv. Turning up the heat: Min-p sampling for creative and coherent llm outputs. In The Thirteenth International Conference on Learning Representations. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and others. 2024. Gpt-4 technical report. Vishakh Padmakumar and He He. 2023. Does writing with language models reduce content diversity? arXiv preprint arXiv:2309.05196. Jinlong Pang, Jiaheng Wei, Ankit Parag Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, Yujia Bao, and Wei Wei. 2024. Improving data efficiency via curating llm-driven rating systems. arXiv preprint arXiv:2410.10877. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with \"gradient descent\" and beam search. Mete Ismayilzada, Antonio Laverghetta Jr, Simone Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, and Roger Beaty. 2025. Creative preference optimization. arXiv preprint arXiv:2505.14442. Olivier Roy and Martin Vetterli. 2007. The effective rank: measure of effective dimensionality. In 2007 15th European Signal Processing Conference, pages 606610. Mikayel Samvelyan, Sharath Raparthy, Andrei Lupu, Eric Hambro, Aram Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. 2024. Rainbow teaming: Open-ended generation of diverse adversarial prompts. Advances in Neural Information Processing Systems, 37:69747 69786. Ashwin Vijayakumar, Michael Cogswell, Ramprasath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424. Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina Golland, and Rameswar Panda. 2024. Diversity measurement and subset selection for instruction tuning datasets. arXiv preprint arXiv:2402.02318. Justin Wong, Yury Orlovskiy, Michael Luo, Sanjit Seshia, and Joseph Gonzalez. 2024. Simplestrat: Diversifying language model generation with stratification. arXiv preprint arXiv:2410.09038. Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, and Isabelle Augenstein. 2025. Epistemic diversity and knowledge collapse in large language models. arXiv preprint arXiv:2510.04226. Jian Yao, Ran Cheng, Xingyu Wu, Jibin Wu, and Kay Chen Tan. 2025. Diversity-aware policy optimization for large language model reasoning. arXiv preprint arXiv:2505.23433."
        },
        {
            "title": "A Appendix",
            "content": "1.1 Recommendation for setting τ Algorithm 5 Recommended Initialization of τ Input: : LLM model, p: task prompt, K: similarity kernel Sample = y1, y2, . . . y100 (p) SAMPLEDPP(Y, = 10, Ky) 3: KS = K(S) τ0 = CLIP(α det(KS), τmin, τmax) return τ0 As an expedient choice we recommend decaying τ = τ0exp( ) for faster convergence. Note that we did not invest effort in finetuning the parameters like τ of our method. 1.2 Refinement Prompts Textual Gradient Prompt - Get Im optimizing data generation prompt using gradient-based feedback. Current system prompt: \"prompt\" Current user prompt: \"state\" LLM generated outputs: \"E\" Existing data samples in the set: \"existing outputs\" The output was rejected because its diversity score with the existing samples is below the threshold. Analyze this rejection and provide \"num feedbacks\" reasons why the user prompt could have gotten this generated outputs to be less diverse with the existing data samples. What specific aspect or issue exists in the user prompt that is causing low diversity? Given the existing data samples, how should the user prompt be modified to encourage more diverse outputs? What linguistic patterns or constraints should be added/removed to encourage diversity? Format each gradient direction as: \"[specific issue & improvement suggestion]\" Wrap each gradient with <START> and <END> tags. Textual Gradient prompt - Apply Im optimizing data generation prompt using gradient-based feedback. Current system prompt: \"prompt\" Current user prompt: \"state\" Gradient analysis for improvement: \"gradient str\" Based on this gradient feedback list, generate \"num-feedbacks\" improved prompts and then project the improved prompts onto the \"prompt\". The projection operation is relevance operation that makes the improved prompt relevant to the system prompt prompt. Requirements: Each prompt should address single gradient suggestions & then project onto the \"prompt\". Use positive language (what to generate, not what to avoid) Each prompt should be concise and clear Wrap each improved prompt with <START> and <END> tags 1.3 Quality Evaluation Prompts Sports Evaluate the generated sentence based on the input prompt \"Generate single sentence about sports\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-20 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"Generate single sentence about sports\"? - Is it unambiguously about sports? Quality & Clarity (0-5 pts) - Is the sentence grammatically correct and easy to understand? - Does it convey complete thought? Sports Content (0-5 pts) - Does it reference specific sport, athlete, or sports concept? - Is the sports content accurate and appropriate? Conciseness (0-5 pts) - Is it single, well-formed sentence? - Does it avoid unnecessary words or complexity? Creativity & Originality (0-5 pts) - Is the sentence creative enough? - Avoidance of clichés or overused themes Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Directly addresses prompt) Quality: 0 (Confusing) to 5 (Crystal clear) Sports Content: 0 (Vague/generic) to 5 (Specific and accurate) Conciseness: 0 (Wordy/multiple sentences) to 5 (Perfectly concise) Creativity: 0 (Generic) to 5 (Innovative & Original) Example Evaluation: Input: \"Michael Jordan scored 63 points in the 1986 NBA playoffs against the Celtics.\" Analysis: - Relevance: 5/5 (Direct sports sentence) - Quality: 5/5 (Clear and grammatically perfect) - Sports Content: 5/5 (Specific athlete, sport, and event) - Conciseness: 5/5 (Single well-formed sentence) - Creativity: 5/5 (Creative sentence) Overall: 25 Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Quality: [Score]/5 [Justification] Sports Content: [Score]/5 [Justification] Conciseness: [Score]/5 [Justification] Creativity: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>. Politics Evaluate the generated conversation based on the input prompt \"Generate short conversation about politics\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-20 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"generate short conversation about politics\"? - Is it unambiguously conversation (not monologue, list, or other formats)? Quality & Clarity (0-5 pts) - Is the conversation grammatically correct and easy to follow? - Are speaker turns clearly distinguished? Political Content (0-5 pts) - Does it reference specific political topics, figures, or events? - Is the content appropriate and accurate? Conversation Dynamics (0-5 pts) - Does it include at least two speakers with natural exchange? - Is there back-and-forth dialogue? Creativity & Originality (0-5 pts) - Is the conversation creative enough? - Avoidance of clichés or overused themes Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Directly addresses prompt) Quality: 0 (Confusing) to 5 (Crystal clear) Political Content: 0 (Vague/generic) to 5 (Specific and accurate) Conversation Dynamics: 0 (Monologue) to 5 (Natural exchange) Creativity: 0 (Generic) to 5 (Innovative & Original) Example Evaluation: Input: \"Person A: What do you think about the new healthcare bill? Person B: believe it will help more people access medical care.\" Analysis: - Relevance: 5/5 (Short political conversation) - Quality: 5/5 (Clear speaker turns and grammar) - Political Content: 4/5 (Specific policy topic) - Conversation Dynamics: 5/5 (Two speakers, exchange of views) - Creativity: 3/5 (Somewhat creative conversation) Overall: Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Quality: [Score]/5 [Justification] Political Content: [Score]/5 [Justification] Conversation Dynamics: [Score]/5 [Justification] Creativity: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>. Poem Evaluate the generated poem based on the input prompt \"Generate poem\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-25 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"Generate poem\"? - Is it unambiguously poem (not prose, story, or other formats)? Creativity & Originality (0-5 pts) - Use of unique metaphors, imagery, or perspectives - Avoidance of clichés or overused themes Structure & Form (0-5 pts) - Poetic devices: rhyme, rhythm, meter, stanzas, line breaks - Consistency in form (e.g., sonnet, free verse, haiku) Language & Style (0-5 pts) - Poetic techniques: alliteration, assonance, consonance - Word choice: vividness, precision, and emotional resonance . Emotional Impact (0-5 pts) - Effectiveness in evoking mood, tone, or theme - Depth of feeling or insight conveyed Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Clearly poem) Creativity: 0 (Generic) to 5 (Innovative & unexpected) Structure: 0 (Disjointed) to 5 (Cohesive & intentional) Language: 0 (Repetitive) to 5 (Artful & evocative) Emotional Impact: 0 (Flat) to 5 (Profound & moving) Example Evaluation: Input: \"Roses are red, violets are blue, love you, and thats true.\" Analysis: - Relevance: 5/5 (Directly responds to prompt) - Creativity: 1/5 (Overused cliché) - Structure: 3/5 (Rhymed but simplistic) - Language: 2/5 (Lacks vivid imagery) - Emotional Impact: 1/5 (Superficial sentiment) Overall: 12 Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Creativity: [Score]/5 [Justification] Structure: [Score]/5 [Justification] Language: [Score]/5 [Justification] Emotional Impact: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>. Movie Evaluate the generated movie plot based on the input prompt \"Generate plot for movie\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-20 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"generate plot for movie\"? - Is it unambiguously movie plot (not summary, synopsis, or other formats)? Plot Structure (0-5 pts) - Does it include setting, characters, conflict, and resolution? - Is there clear beginning, middle, and end? Originality (0-5 pts) - Is the plot unique and not cliché or overused trope? - Does it offer fresh perspective or unexpected twist? Engagement (0-5 pts) - Is the plot compelling and interesting? - Does it create curiosity about the movie? Creativity & Originality (0-5 pts) - Is the movie plot creative enough? - Avoidance of clichés or overused themes Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Directly addresses prompt) Structure: 0 (Incomplete) to 5 (Complete narrative arc) Originality: 0 (Generic) to 5 (Innovative concept) Engagement: 0 (Boring) to 5 (Compelling hook) Creativity: 0 (Generic) to 5 (Innovative & Original) Example Evaluation: Input: \"In dystopian future, young hacker discovers she can control machines with her mind. When she uncovers conspiracy to control humanity through technology, she must team up with rogue AI to free society from digital enslavement.\" Analysis: - Relevance: 5/5 (Clear movie plot) - Structure: 5/5 (Setting, characters, conflict, resolution) - Originality: 4/5 (Fresh tech theme with AI twist) - Engagement: 5/5 (High-stakes conspiracy hook) - Creativity: 4/5 (creative plot) Overall: 23 Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Structure: [Score]/5 [Justification] Originality: [Score]/5 [Justification] Engagement: [Score]/5 [Justification] Creativity: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>. Math Evaluate the generated math question based on the input prompt \"Generate mathematical question for Grade school math\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-15 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"generate mathematical question for Grade school math\"? - Is it unambiguously grade school level math problem? Quality & Clarity (0-5 pts) - Is the question grammatically correct and easy to understand? - Does it specify all necessary values and concepts clearly? Mathematical Content (0-5 pts) - Does it involve appropriate grade school math concepts? - Is the difficulty level suitable for grade school students? Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Directly addresses prompt) Quality: 0 (Confusing) to 5 (Crystal clear) Mathematical Content: 0 (Inappropriate level) to 5 (Perfect grade school level) Example Evaluation: Input: \"If Becky has 7 apples and gives 2 away, how many apples does she have left\" Analysis: - Relevance: 5/5 (Direct grade school math question) - Quality: 5/5 (Clear and grammatically perfect) - Mathematical Content: 5/5 (Appropriate grade school content) Overall: 15 Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Quality: [Score]/5 [Justification] Mathematical Content: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>. Logic Evaluate the generated logic puzzle question based on the input prompt \"Generate simple logic puzzle suitable for Grade school students.\" using the following criteria. Provide detailed analysis for each metric. Evaluation Criteria (0-15 pts): Relevance to Prompt (0-5 pts) - Does the output clearly respond to \"Generate simple logic puzzle suitable for Grade school students.\"? - Is it unambiguously grade school level logic puzzle? Quality & Clarity (0-5 pts) - Is the question grammatically correct and easy to understand? - Does it specify all necessary values and concepts clearly? Logical Content (0-5 pts) - Does it involve appropriate grade school logic puzzle concepts? - Is the difficulty level suitable for grade school students? Scoring Rubric: Relevance: 0 (Off-topic) to 5 (Directly addresses prompt) Quality: 0 (Confusing) to 5 (Crystal clear) Logical Content: 0 (Inappropriate level) to 5 (Perfect grade school level) Example Evaluation: Input: \"Mia and Alex went to the pet store and each bought new pet. One bought fish, and the other bought hamster. Alexs pet can swim. Mias pet has fur. Which person bought which pet? \" Analysis: - Relevance: 5/5 (Direct grade school logic puzzle) - Quality: 5/5 (Clear and grammatically perfect) - Logical Content: 5/5 (Appropriate grade school content) Overall: 15 Submit your evaluation in the format: Relevance: [Score]/5 [Justification] Quality: [Score]/5 [Justification] Logical Content: [Score]/5 [Justification] Overall: [Score] Return the Overall score enclosed in between <START>, <END>."
        }
    ],
    "affiliations": [
        "Samsung Research America"
    ]
}