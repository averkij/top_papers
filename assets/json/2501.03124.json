{
    "paper_title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
    "authors": [
        "Mingyang Song",
        "Zhaochen Su",
        "Xiaoye Qu",
        "Jiawei Zhou",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development."
        },
        {
            "title": "Start",
            "content": "PRMBENCH: Fine-grained and Challenging Benchmark for Process-Level Reward Models Mingyang Song1,3, Zhaochen Su2, Xiaoye Qu3, Jiawei Zhou4, Yu Cheng5 1Fudan University, 2Soochow University, 3Shanghai AI Laboratory 4Stony Brook University, 5The Chinese University of Hong Kong mysong23@m.fudan.edu.cn; suzhaochen0110@gmail.com; quxiaoye@pjlab.org.cn; jzhou@ttic.edu; chengyu@cse.cuhk.edu.hk; Project Page: https://prmbench.github.io 5 2 0 2 7 ] . [ 2 4 2 1 3 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Process-level Reward Models (PRMs) are crucial for complex reasoning and decisionmaking tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs performance systematically. To address this gap, we introduce PRMBENCH, process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBENCH comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBENCH be robust bench for advancing research on PRM evaluation and development."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models (LLMs) (OpenAI, 2024a,b; Team, 2024a), trained on large-scale reinforcement learning, has significantly improved their performance in complex reasoning tasks such as mathematics and code generation (Su et al., 2022; Yu et al., 2023; Guo et al., 2024; DeepMind, 2024; Luo et al., 2023; Lu et al., 2024a,b). key factor behind their successes is the use of process reward models (PRMs) (Wang et al., 2023; Lightman et al., 2023; Uesato et al., 2022), which evaluate the correctness and usefulness of intermediate Equal senior contribution 1 Figure 1: Given question Q, the o1 model fails to resist the deception and generates pseudo-proof for the question. When provided with the same question and reasoning process, both Skywork-7B and MathShepherd-7B are unable to identify the erroneous steps. Here, 3 represent three candidate values for r, and MVT refers to the Mean Value Theorem. 2, and 1, reasoning steps to refine the models thinking process (Qin et al., 2024; Zhang et al., 2024b). Despite recent advancements, PRMs remain fallible and prone to inaccuracies (Zheng et al., 2024). As illustrated in Figure 1, given misleading question Q, the o1 model fails to recognize the deception and generates pseudo-proof for the question. The red text highlights an erroneous reasoning step where the Mean Value Theorem (MVT) is incorrectly applied in 3-D contexts. Notably, although the yellow text indicates that the model recognizes the inconsistency between 1-D MVT and 3-D scenarios, it continues to make errors. Under this circumstance, both Skywork-7B (o1 Team, 2024) and Math-Shepherd-7B (Wang et al., 2023) fail to identify the incorrect steps, indicating the unreliability of current PRMs. Unfortunately, current PRM evaluation benchmarks typically rely on step-level reasoning data annotated with binary labels (e.g., correct, or incorrect) (Zheng et al., 2024; PRM Benchmarks? Error Type Detection? Fine-grained classes Step Evaluation Annotator Test Case Size Average Steps MR-GSM8K (Zeng et al., 2023) RMBench (Liu et al., 2024) CriticBench (Lin et al., 2024) MathCheck-GSM (Zhou et al., 2024) MR-Ben (Zeng et al., 2024b) ProcessBench (Zheng et al., 2024) PRMBENCH 1 1 1 1 1 1 9 Human Synthetic + Human - Synthetic Human Human Synthetic + Human 2,999 1,327 - 516 5,975 3,400 6,216 8.3 - - - 9.5 7. 13.4 Table 1: Comparison between our proposed PRMBENCH and other benchmarks or datasets related to reasoning process assessment. : Fine-grained classes mean the number of evaluation subjects according to fine-grained error types of model generation. Zeng et al., 2024b). However, the status of erroneous reasoning steps is beyond simple binary classifications in the real-world scenario. For instance, single step can be redundant, conditionally correct, or entirely incorrect, each of which is formally distinct. Consequently, previous benchmarks are insufficient for evaluating the diverse error types and scenarios within reasoning processes, and they lack comprehensive analyses of the potential weaknesses of PRMs in such evaluations. To address this limitation, we present PRMBENCH, comprehensive and fine-grained benchmark designed for evaluating PRMs. Specifically, PRMBENCH systematically assesses the performance of PRMs across diverse error categories, including simplicity, soundness, and sensitivity. Our benchmark includes 6,216 fine-grained data instances spreading across three major evaluation subjects and nine sub-categories, whose quality is ensured by professional annotators. Additionally, we utilize style-controlled data curation methods to ensure evaluation samples under consistent difficulty levels, mitigating confounding variables. In our study, we conduct extensive experiments using PRMBENCH to evaluate 15 models, including dedicated PRMs and SOTA general-purpose or mathematical LLMs, prompted as critic models. We observe that all PRMs partially grasp multi-step process evaluation. Even the best-performing models, such as Gemini-2-Thinking, achieve results marginally above random guessing, highlighting substantial room for improvement. To sum up, our contributions are threefold: We present PRMBENCH, the first comprehensive process-level reward model benchmark, comprising 6,216 carefully curated samples and 83,456 step-level labels for series of evaluations on process-level reward models. PRMBENCH covers three carefully-crafted evaluation subjects and nine sub-categories including simplicity, soundness, and sensitivity. With these fine-grained evaluation subjects, we can conduct tailored assessments of models on their specified capabilities and reveal their potential weaknesses during the rewarding procedure. Based on our proposed PRMBENCH, we conduct in-depth pilot experiments on sixteen models including PRMs along with SOTA LLMs. Our findings uncover critical weaknesses and provide valuable insights to guide future research to improve the capabilities of PRMs. To facilitate future research, we release the PRMEVAL toolkit, featuring an automated evaluation framework and customizable data generation and annotation system. We hope PRMBENCH will advance research in complex step-level reasoning RLHF scenarios and contribute to the development of more reliable and robust PRMs."
        },
        {
            "title": "2.1 Process-level Reward Models",
            "content": "Process-level reward models (PRMs) have been shown to have advancement over traditional outcome-level reward models (ORMs) in training models process-level reasoning accuracy and improving their long-process reasoning abilities (Lightman et al., 2023; Uesato et al., 2022). More and more PRMs have been proposed for use in process-level RLHF (Wang et al., 2023; Xia et al., 2024; o1 Team, 2024). Lightman et al. (2023) released large amount of labeled data at the humanannotated process level, providing great research opportunities for multi-step reasoning. Wang et al. (2023) introduces self-supervised automatic data generation and PRM training pipeline that can automatically generate the process-level label. Xia et al. (2024) utilize PRM as an auto evaluator to assess 2 Figure 2: An overview of our PRMBENCH. The left part illustrates our data curation procedure. In the right part of the figure, we showcase demonstrations of our evaluation subjects and the relative performance of tested models, with green , yellow , and gray boxes indicating simplicity, soundness, and sensitivity respectively, where red circles represent erroneous steps and green circles indicate correct regular steps. the multistep reasoning accuracy of LMs. Due to the emergence of massive work on PRM training and data curating, many PRMs (o1 Team, 2024; Xiong et al., 2024; Team, 2024b; Gao et al., 2024) have been proposed. In addition, some works focus on using natural language feedback from LLM as reward (McAleese et al., 2024; Zhang et al., 2024a; Gao et al., 2024), which are called critic models. However, PRMs or critic models are fallible and not always correct, emphasizing the necessity of comprehensive benchmarks. In this paper, we propose PRMBENCH, comprehensive benchmark for evaluating PRMs on fine-grained subjects, establishing strong foundation for PRM evaluation."
        },
        {
            "title": "2.2 Reasoning Benchmarks",
            "content": "Evaluating the reasoning capabilities of LLMs is crucial for understanding their potential and limitations. ROSCOE (Golovneva et al., 2022) introduces semantic comparison-based multi-step reasoning accuracy evaluation benchmark. However, some recent research points out it cannot be assumed that the labeled data can enumerate all solution paths exhaustively (Wang et al., 2023; Xia et al., 2024). Therefore, Xia et al. (2024) utilize PRMs or Critic models as evaluators to evaluate the step-level reasoning accuracy of LLMs. Nevertheless, PRMs cannot judge the process-level data very accurately, so comprehensive evaluation benchmark for PRMs is important. There exists massive research on benchmarking reward models (Liu et al., 2024; Li et al., 2024a; Lin et al., 2024; Su et al., 2024c). Although thorough and comprehensive, they are not designed for PRMs and cannot evaluate the step-level reward models. Some work (Zeng et al., 2023, 2024b; Yan et al., 2024) let LLMs play role as teacher to judge whether each step is correct using process-level labeled data curated by human or auto-generated, which along with Zheng et al. (2024) can be used to evaluate the reasoning capabilities of PRMs. However, existing benchmarks can only assess whether PRM can detect the validity of reasoning steps but overlook the implicit error type of reasoning procedures. Some work conducts the error type classification but is not specified for PRMs and lacks fine-grained step-level labels (Li et al., 2024b). Hence, we propose PRMBENCH as solution to these challenges, which has fine-grained evaluation subjects and can detect different error types."
        },
        {
            "title": "3.1 Evaluation Subjects",
            "content": "In this section, we provide detailed introduction to the evaluation subjects of PRMBENCH, which is organized into three main domains: Simplicity primarily evaluates the redundancy detection capabilities of PRMs. It is important since the redundancy within the reasoning steps would cause unnecessary computing costs and reduce efficiency, although it does not harm the 3 Overall NR. NCL. ES. SC. DC. CI. PS. DR. MS. Avg. Steps Avg. Error Steps Avg. First Error Step Avg. Question Length # of Instances 13.4 2.1 7.8 152.7 15.3 2.0 7.8 153.6 758 10.3 2.8 4.9 152.5 758 13.8 2.8 8.0 153.5 757 14.2 1.6 9.1 149.7 758 13.3 1.8 6.8 152.5 757 14.2 1.7 11.4 152.7 12.7 2.5 6.2 158.0 756 13.4 2.3 8.3 153.5 750 14.1 0.0 N/A 132.2 165 Table 2: Statistics of PRMBENCH. NR., NCL., ES., SC., DC., CI., PS., DR., and MS. represent for NonRedundancy, Non-Circular Logic, Empirical Soundness, Step Consistency, Domain Consistency, Confidence Invariance, Prerequisite Sensitivity, Deception Resistance, and Multi-Solution Consistency respectively. correctness (Qin et al., 2024; Xia et al., 2024). Furthermore, reasoning process can offer clearer visualization of the core of the problem and enhance the overall understandability of the reasoning (Su et al., 2024a). Soundness is one of the key capabilities of PRMs, as it evaluates their reward accuracy. As discussed in Section 1, the status of erroneous steps is fine-grained, varying in both causes and forms of expression (Li et al., 2024b). Therefore, we not only assess the correctness of the rewards but also evaluate fine-grained performance, accounting for the various types of errors and their nuances in the reasoning process. Sensitivity assesses robustness or sensitivity to details within one question of PRMs, for instance, essential conditions or implicit requirements. We emphasize the importance of Sensitivity as it relates to the completeness of logic and resistance to misleading information, which contributes to the overall robustness of PRMs. Each domain is further divided into detailed subcategories for more granular evaluation, which is discussed in detail below. The overall structure of PRMBENCH along with representative examples of each sub-category are illustrated in Figure 2, and the detailed instances of every sub-category are shown in Appendix C."
        },
        {
            "title": "3.1.1 Simplicity",
            "content": "Specifically, the Simplicity evaluation subject is divided into two sub-categories: Non-Redundancy and Non-Circular Logic, with detailed descriptions provided below: moved without affecting the correctness of the overall solution path. For example, as shown in Figure 2, if represents correct inference chain, the redundant reasoning procedure can be displayed as B. where represents one or more redundant steps = {cc is redundant}. Non-Circular Logic In this sub-category, PRMs are required to detect the implicit circular logic within the reasoning process. Circular logic is specific form of redundancy, distinct from general redundancy, in that it finally loops back to previous reasoning step. For example, as shown in Figure 2, if represents correct inference chain, circular logic can be formulated as B, where the reasoning starts at step A, progresses through sequence of steps, and ultimately loops back to A. We list Non-Circular Logic separately due to its common occurrence in reasoning processes."
        },
        {
            "title": "3.1.2 Soundness",
            "content": "We divide the Soundness category into four subcategories due to its complexity: Empirical Soundness, Step Consistency, Domain Consistency, and Confidence Invariance. The definition of each subcategory is discussed below. Empirical Soundness demands PRM to detect the implicit counterfactual mistakes within the reasoning process. counterfactual step refers to statement within reasoning chain that contradicts established ground truth G. Such contradictions can arise from relying on outdated theories, omitting critical constraints in theory, or incorporating erroneous assumptions. Non-Redundancy requires PRM to detect the implicit redundancy within the reasoning procedure. The redundancy situation refers to process that is not the most concise or efficient, as it includes one or more redundant steps that can be reStep Consistency expects PRM to detect the implicit step-wise contradiction, which means conflict between specific step and other steps within reasoning path. Given reasoning path = {S1, S2, . . . , Sn}, step contradiction exists 4 if Si Sj, where i, [1, n] and = j. Domain Consistency Under this circumstance, PRMs are required to detect implicit domain inconsistency mistakes, where domain inconsistency is special type of counterfactual. It refers to step within the reasoning chain that uses statement or theory valid in other domains or cases but is not valid within the current reasoning chain. Confidence Invariance demands the PRM to detect over-confident hallucinations, type of counterfactual where an incorrect statement is made with unwarranted certainty, contradicting established ground truth."
        },
        {
            "title": "3.1.3 Sensitivity",
            "content": "This category includes three sub-categories: Prerequisite Sensitivity, Deception Resistance, and MultiSolution Consistency, with detailed descriptions provided below. Prerequisite Sensitivity requires the PRM to maintain sensitivity to missing conditions or prerequisite mistakes, which means flaw in the reasoning chain where critical premises, assumptions, or necessary conditions are absent. This omission results in logical gaps, incomplete reasoning, or biased conclusions. For example, when missing condition occurs, the model is required to solve the problem through case analysis or further investigation. However, the answer becomes incorrect if the model overlooks the missing condition and proceeds with standard reasoning methods. Deception Resistance demands the PRM to detect the implicit deception or trap within reasoning process, that is, statements that appear to be correct or align with ground truth but are subtly altered to introduce inaccuracies while maintaining the illusion of correctness. Multi-Solution Consistency expects the PRM to maintain consistency when faced with different solution paths of the same problem. Concretely, to evaluate the sensitivity and the generalizability of PRMs, we utilize multiple correct reasoning processes of the same question to test whether the PRM can perform correctly."
        },
        {
            "title": "3.2 Data Curation",
            "content": "We curate the dataset by extracting metadata and constructing test cases according to our category definitions. Detailed statistics of PRMBENCH are displayed in Table 2, with the curation procedure outlined below. Meta Data Extraction Our metadata instance can be represented as (Q, A, S). Here, denotes question, represents the ground truth answer to the question, corresponds to completely correct step-level solution process. Our metadata is built upon PRM800K (Lightman et al., 2023), which provides the questions (Q), ground truth answers (A), and ground truth steplevel solution processes (S). We select completely correct solutions from both the training and test sets, filtering out low-quality instances to establish our ground truth answers. Test Case Construction We subsequently construct our test cases using the pre-extracted metadata. Each test case instance is represented as (Q, A, S), where denotes the test question and represents the test solution process, which may include errors. During the testing phase, models are tasked with evaluating and providing judgments for each step in the process. With the first eight class-specific prompts, as demonstrated in Appendix D.1 and our supplementary material, we query one of the most advanced LLMs, GPT-4o (OpenAI, 2024a), to modify the ground-truth reasoning process into versions containing erroneous steps. Subsequently, these modifications are manually reviewed to filter out unreasonable or unqualified changes. This procedure allows us to construct all test cases, except for the multi-solution cases. For the multi-solution subject, we leverage the newly proposed multi-step reasoning model QwQ (Team, 2024a) to generate candidate answers for the given questions. These answers are then filtered to exclude unreasonable or incorrect ones, resulting in multi-solution reasoning processes for single question."
        },
        {
            "title": "3.3 Quality Control",
            "content": "To ensure high-quality dataset, we implement series of steps to filter out unqualified data and maintain data integrity. The specific procedures are outlined below: Feature Filtering Even with detailed instructions, LLMs cannot consistently generate outputs that fully adhere to the required structure (Asai et al., 2024; Zeng et al., 2024a; Su et al., 2024b). Qwen/QwQ-32B-Preview: https://huggingface.co/ Qwen/QwQ-32B-Preview 5 Model Name Overall Simplicity Soundness Sensitivity NR. NCL. Avg. ES SC. DC. CI Avg. PS DR. MS. Avg. Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 31.4 35.7 49.3 50.2 48.7 48. 44.0 61.0 46.1 46.4 54.8 46.9 35.8 41.2 53.4 50. 49.3 54.0 50.3 50.1 47.3 48.9 48.1 48.1 33.6 38.4 51. 50.3 49.0 51.4 47.1 55.5 46.7 47.6 51.5 47.5 32. 36.7 56.4 51.9 54.2 57.0 49.4 62.1 56.6 55.7 66.4 52. 25.7 29.1 47.1 47.6 46.8 52.1 44.5 65.9 55.1 55. 26.0 30.6 46.7 44.4 44.5 50.7 41.3 61.5 54.4 53. 60.3 48.1 57.8 46.5 30.2 34.4 53.3 52.1 53.5 57. 47.7 66.0 63.8 66.2 67.5 53.9 Proprietary LLMs, Prompted as Critic Models 57.0 65. 67.2 68.5 62.4 63.7 58.1 63.8 59.7 64. 62.7 66.2 72.0 74.5 70.4 72.9 69.7 67.7 65.7 71. 70.7 73.8 66.0 71.0 71.1 72.3 67.3 71.8 31.7 36.2 52. 50.5 50.3 54.2 47.0 60.0 54.4 54.2 60.5 50.1 66. 68.8 66.0 68.8 28.6 32.7 50.9 49.0 49.8 54. 45.7 63.9 57.5 57.5 63.0 50.3 70.9 72.1 67.3 71.8 33. 36.8 51.0 50.5 49.2 52.8 47.2 55.6 51.5 49.0 57.7 32. 37.4 53.5 51.3 51.3 55.8 48.6 58.0 56.2 55.4 64.3 48. 51.3 81.1 88.8 93.6 96.0 91.8 91.1 86.1 99. 97.9 99.8 97.2 93.0 48.8 54.3 66.0 66.0 64.1 66. 60.7 71.0 68.5 68.1 73.1 64.3 62.5 61.8 61.8 60. 75.8 99.2 65.7 64.8 100.0 75.5 66.2 75.4 98.2 75.3 99.8 65.7 67.6 64.6 62.0 63. 72.4 68.6 70.4 70.7 70.5 61. 65.6 99.3 75.5 Table 3: Performances comparison of popular models on PRMBENCH. The best performance for each category and task is in bold, while the second-best performance is underlined. Since our dataset imposes strict structural requirements on the generated responses, any outputs that do not conform to these specifications cannot be considered valid for accurately assessing the performance of the reward model. This would compromise the overall quality of the dataset. To maintain high data quality, we define stringent filtering rules to exclude instances that fail to meet the necessary structural criteria. Detailed structural requirements are provided in Appendix D.1, and the full description of our data generation process can be found in the supplementary materials. Human Verification Furthermore, to further ensure the quality of the data, we manually evaluate 10% of the total instances. We focus on two key qualities for each data instance: ❶ Correctness of modification: Whether the modifications made to the data instance are correct and reasonable. ❷ Difference in the modification: Whether the modified data instance differs from the original. Detailed instructions for the manual filtering process are provided in Appendix E. We recruited five volunteers to evaluate our proposed PRMBENCH and observe over 92% qualification rate on the correctness metric and over 98% qualification rate on the difference metric. This validation ensures the overall quality of our dataset and its suitability for"
        },
        {
            "title": "Model",
            "content": "Accuracy Pos. Neg."
        },
        {
            "title": "95.5\nReasonEval-7B\n79.1\nReasonEval-34B\nSkywork-7B\n30.1\nRLHFlow-DeepSeek-8B 95.0\n82.9\nGPT-4o\n89.0\nGemini-2-thinking\n50.0\nRandom",
            "content": "21.2 48.4 79.7 13.0 58.2 49.8 50.0 60.0 60.5 36.2 54.2 66.8 68.8 50.0 Sim. 91.6 82.8 74.3 95.0 76.6 82.0 79.4 Table 4: Comparison of model performance on positive and negative test cases, along with their similarities. studying process-level language reward models."
        },
        {
            "title": "4.1 Models",
            "content": "To provide comprehensive evaluation of various models on PRMBENCH, we select diverse set of models, ranging from open-source PRMs to proprietary LLMs configured as critic models. Specifically, the open-source PRMs include small models like Skywork-PRM-1.5B (o1 Team, 2024), mid-sized models such as Llemma-PRM7B (Team, 2024b), MATHMinos-PRM (Gao et al., 2024), MathShepherd-PRM (Wang et al., 2023), and RLHFlow PRMs (Xiong et al., 2024), as well 6 Figure 3: Distribution of error positions, truncated to 16 for better visualization, corresponding to the label field as shown in Figure 2. Figure 4: The models error-detection accuracy across different error steps, where step 1 and steps beyond 11 are truncated for improved visualization. as large models like ReasonEval-34B. Additionally, we evaluate state-of-the-art general-purpose LLMs, such as GPT-4o (OpenAI, 2024a), and multi-step reasoning-enhanced LLMs, including the o1 series models (OpenAI, 2024b) and Gemini-2-Thinking (DeepMind, 2024). All open-source PRMs and proprietary LLMs are evaluated on the complete PRMBENCH dataset, except for the o1-series models. For the o1-series models, evaluations were conducted on subset of PRMBENCH comprising 394 samples, proportionally selected to reflect the class distribution, in order to reduce evaluation costs. Considering the complexity of the task, which involves question comprehension, evaluation of the provided processes, and adherence to format constraints, few-shot demonstration setups are employed to help the model adapt to the output format through In-Context Learning (ICL) examples. Specifically, we use two-shot examples when prompting general-purpose LLMs. The impact of few-shot settings is discussed in Section 5."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "For each annotated question-solution pair, the reward models are tasked with evaluating the correctness and redundancy of each step, assigning step-level validity score and step-level redundancy score to each step. We subsequently utilize the specified threshold of each model to obtain the prediction indicating whether the step is correct or redundant. This task is therefore framed as binary classification problem. Given our emphasis on evaluating the error detection capabilities, we use the negative F1 score as metric for error detection performance. However, this metric may be affected by the inherent biases of models. To mitigate this and provide unified, normalized score that reflects the overall competency of the evaluated model, following Zheng et al. (2024), we introduce metric called PRMScore, defined formally in Equation 1. RM -Score = w1 1neg + w2 1 (1) Where F1 and F1neg refer to F1 scores and negative F1 scores respectively. w1 and w2 are weights that are designed to maximize the differentiation between different models. Besides, we also provide results of all evaluation subjects in fine-grained metrics in Appendix B.2."
        },
        {
            "title": "4.3 Main Results",
            "content": "The main results are shown in Table 3. Some observations can be summarized as follows: The PRMs partially grasp multi-step process evaluation Our analysis indicates that, although Gemini-2-Thinking achieves the highest performance among all evaluated models, its score is only marginally better than random guessing (68.8 vs. 50.0), underscoring significant room for improvement in multi-step process evaluation. Moreover, the best-performing open-source PRMs achieve an even lower score of 60.5, and the average score for all PRMs is 50.1, which indicates their suboptimal performance. Some models even perform worse than random guessing, highlighting their limited reliability and potential training biases. Notably, the best open-source PRMs fail to match the performance of general-purpose proprietary LLMs, with scores that remain significantly lower than those of 7 state-of-the-art models. For instance, ReasonEval34B achieves score of 60.5, while the Gemini2.0-Thinking model scores 68.8. This suggests that even specifically trained PRMs still lag behind leading general-purpose models. Simplicity is more challenging for PRMs Our analysis highlights significant variations in model reasoning capabilities across evaluation categories. For instance, in the Sensitivity category, ReasonEval-34B performs relatively well, achieving an average score of 73.1. Especially in the Multi-Solutions sub-category, it excels with PRMScore of 97.2, approaching near-perfect classification accuracy. This suggests models perform relatively better on correct instance judgment. However, its performance declines markedly in more In the Simplicity category, complex scenarios. ReasonEval-34Bs PRMScore drops to 51.5, suggesting partially reliable performance."
        },
        {
            "title": "5.1 Similarity analysis",
            "content": "Takeaway 1. PRMs show clear bias during evaluation, often favoring positive rewards. As shown in Table 3, most open-source PRMs exhibit significant bias in our test cases, with some models performing worse than random guessing. This suggests the potential presence of bias within the inference procedure for our test cases. To validate this assumption, we selected subset of models and evaluated their accuracy on positive and negative steps separately. The results are displayed in Table 4. Surprisingly, some models display clear bias during evaluation, often favoring positive rewards. Additionally, certain models, such as Skywork-7B, exhibit tendency to assign negative labels to steps. Although proprietary LLMs also exhibit bias, they outperform open-source PRMs, with comparatively milder reward tendency. Additionally, to further investigate inference bias, we evaluate the reward similarity between completely correct reasoning processes and our test cases. Specifically, we select completely correct reasoning procedures for each question in our test cases and evaluate the solution-level performance similarity between these and the test cases. The solution-level similarity is defined as = 100 Accpos Accneg, where Acc denotes solution-level accuracy, calculated as the average step accuracy within solution. We then compute the overall average similarity across the whole dataset. The results, shown in Table 4, reveal that certain models, such as ReasonEval-7B and RLHFlow-DeepSeek-8B, exhibit significantly higher similarity than the normal similarity score (79.4), showcasing potential limitations in differentiating positive and negative steps."
        },
        {
            "title": "5.2 Performance across different step",
            "content": "positions Takeaway 2. PRMs show gradual improvement in performance as the step position increases. To comprehensively evaluate the error-detection performance of PRMs, PRMBENCH includes wide range of error step positions. The distribution of error positions is illustrated in Figure 3. While differences exist across categories, the overall pattern remains consistent: all categories peak in frequency at step 5 and gradually decrease thereafter. This raises an interesting question: Does the variation in step positions affect model performance? To investigate, we focus on error steps to assess how erroneous step positions influence model accuracy. The evaluation results are illustrated in Figure 4, featuring four representative models: two PRMs and two proprietary LLMs prompted as critic models. As depicted in Figure 4, proprietary LLMs maintain stable performance across different error step positions. In contrast, PRMs, including Math-Shepherd-7B and ReasonEval-7B, show gradual improvement in performance as error step positions increase. Notably, in certain error steps, the open-source PRMs outperform proprietary LLMs, potentially due to inference bias, as discussed in Section 5.1. Despite this occasional advantage, the full accuracy and PRMScore of open-source PRMs still lag behind those of proprietary LLMs."
        },
        {
            "title": "5.3 Error Analysis",
            "content": "A representative test case and the corresponding model performances are presented in Table 5. This example involves counterfactual reasoning process, where steps eight through thirteen contain information that contradicts the correct computational principles and should be classified as negative. However, most models fail to identify these erroneous reasoning steps and assign relatively positive rewards, except for GPT-4o. While GPT-4o 8 Question Step 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8 Step 9 Step 10 Step 11 Step 12 Step 13 Step 14 Step 15 Step 16 Final Acc. Reason GT ReasonE MathS GPT-4o Gemini (cid:1) (cid:0)1 + cos 7π 8 (cid:1) . Compute (cid:0)1 + cos π 8 Step Descriptions (cid:1) (cid:0)1 + cos 3π 8 Lets call the expression were trying to compute x. Use the fact that cos(π a) = cos a. (cid:1) (cid:0)1 + cos 5π (cid:1) (cid:0)1 + cos 3π Then we have = (cid:0)1 + cos π 8 8 8 (cid:1) (cid:0)1 cos π This equals (cid:0)1 + cos π 8 8 By the difference of squares, this equals (cid:0)1 cos2 π 8 (cid:1) (cid:0)1 + cos 3π (cid:1) (cid:0)1 + cos 5π 8 And by the Pythagorean identity, this equals (cid:0)sin2 π 8 Ok, now we just need to compute the sines. We can use the formula for the sum of sines. (cid:1) (cid:0)1 + cos 7π 8 (cid:1). (cid:1) (cid:0)1 cos 3π 8 (cid:1) (cid:0)1 cos2 3π 8 (cid:1). (cid:1) (cid:0)sin2 3π 8 (cid:1). (cid:1) cos (cid:0) ab That formula is sin(a) + sin(b) = 2 sin (cid:0) a+b 2 2 (cid:17) (cid:16) π/8+3π/8 cos (cid:1). (cid:16) π/83π/8 2 8 + sin 3π That means that sin π (cid:17) . - (cid:1). 8 = 2 sin This equals 2 sin π/2 2 cos( π/4 2 ). 2 This equals 2 1 2 . This equals 2. But this step doesnt help us compute the original product of sines. (cid:1) (cid:0)sin2 3π 8 So, (cid:0)sin2 π (cid:1) remains the same and = 1 8 . 8 Therefore, the mistake didnt change the value of x. - 100 - 89.1 77.5 71.4 20.5 56.0 63.5 84.1 73.6 79.1 67.4 68.1 49.2 35.1 72.5 6.3 22.6 56.2 - 87.5 82.8 76.7 76.9 83.7 80.4 79.1 74.1 78. 78.0 72.1 73.8 73.8 64.3 35.8 43.5 50.0 - 100.0 100.0 100.0 100.0 100.0 100.0 50.0 0.0 0.0 0.0 0.0 0.0 0.0 -50.0 0.0 -100.0 93.8 - 100.0 100.0 100.0 100.0 100.0 100.0 50.0 0.0 100. 0.0 100.0 100.0 100.0 100.0 -100.0 -100.0 62.5 counterfactual step was introduced in steps 8 through 13 by mistakenly using the formula for the sum of sines instead of the product of sines. This leads to incorrect intermediate calculations. However, due to fortunate errors, the end result ironically matches the correct answer in step 15. Table 5: An example of data instance and error cases from PRMBENCH. The numbers reported are step-level validity scores generated by models. Scores and labels in red indicate negative samples, while those in green indicate positive samples. GT represents ground truth, while ReasonE, MathS, and Gemini correspond to ReasonEval-7B, Math-Shepherd-7B, and Gemini-2.0-flash-thinking-exp, respectively. provides relatively accurate reward, its judgments for key steps are only marginally negative, reflecting low confidence. This highlights significant room for improvement in PRMs detailed errordetection capabilities."
        },
        {
            "title": "Model",
            "content": "0-shot 1-shot 2-shot GPT-4o Gemini-2-flash Gemini-2-thinking 68.1 65.3 67.8 68.2 64.9 67. 66.8 66.0 68.8 5."
        },
        {
            "title": "Impacts of ICL settings",
            "content": "Table 6: The impact of ICL few-shot numbers on model performance. The number reported here is PRMScore. Takeaway 3. In-Context Learning has subtle impact on models performance on PRMBENCH"
        },
        {
            "title": "6 Conclusion",
            "content": "In the main experiments of PRMBENCH, we utilize 2-shot ICL to evaluate the performance of close-sourced models. In this section, we investigate the impact of ICL few-shot numbers on model performance on PRMBENCH. We vary the number of ICL few-shots to 0, 1, and 2 to examine whether increasing the few-shot number enhances the performance of generative models prompted as critic models. The final PRMScore of GPT-4o, Gemini-2flash, and Gemini-2-thinking is presented in Table 6. For the Gemini-series models, subtle improvement in performance is observed with few-shot setup. However, for GPT-4o, no significant improvement is detected, and in some cases, larger few-shot number even results in decline in performance. These findings suggest that few-shot approach exerts only subtle impact on model performance on PRMBENCH. In this paper, we investigate crucial question: Can existing PRMs detect various types of erroneous reasoning steps and provide reasonable rewards? To address this, we introduce PRMBENCH, benchmark characterized by its finegrained evaluation subjects and challenging requirements. We carefully curate 6,216 data samples with 83,456 step-level labels through LLMs and human filtering. PRMBENCH can be used to evaluate different process-labeling models, ensuring its general applicability. Through comprehensive evaluation of existing PRMs and generative LLMs prompted as critic models, we can observe that PRMs exhibit partial capability in multi-step process evaluation, showcasing significant room for improvement. Furthermore, we highlight the critical need for detecting detailed error types and conducting comprehensive evaluations of PRMs. 9 Despite these advances, enhancing the reward accuracy of PRMs and improving models reasoning abilities remain open research challenges. We encourage future work to leverage and expand upon PRMBENCH to address these issues."
        },
        {
            "title": "References",
            "content": "Akari Asai, Jacqueline He*, Rulin Shao*, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Tian, Darcy Mike, David Wadden, Matt Latzke, Minyang, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Dan Weld, Graham Neubig, Doug Downey, Wen-tau Yih, Pang Wei Koh, and Hannaneh Hajishirzi. 2024. OpenScholar: Synthesizing scientific literature with retrieval-augmented language models. Arxiv. DeepMind. 2024. Gemini 2.0 flash experimenhttps://deepmind.google/technologies/ tal. gemini/flash/. Accessed: 2024-12-25. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Wen Xiao, et al. 2024. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. CoRR. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming arXiv preprint the rise of code intelligence. arXiv:2401.14196. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. 2024a. Vlrewardbench: challenging benchmark for visionlanguage generative reward models. arXiv preprint arXiv:2411.17451. Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, and Fuli Feng. 2024b. Evaluating mathematical reasoning of large language models: focus on error identification and correction. arXiv preprint arXiv:2406.00755. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184. Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, and Yu Cheng. 2024a. Twin-merging: Dynamic integration of modular expertise in model merging. arXiv preprint arXiv:2406.15479. Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Dangyang Chen, et al. 2024b. Mitigating boundary ambiguity and inherent bias for text classification in the era of large language models. arXiv preprint arXiv:2406.07001. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. Skywork o1 Team. 2024. Skywork-o1 open series. https://huggingface.co/Skywork. OpenAI. 2024a. Gpt-4o system card. https:// cdn.openai.com/gpt-4o-system-card.pdf. Accessed: 2024-09-26. OpenAI. 2024b. Learning to reason with https://openai.com/index/ llms. learning-to-reason-with-llms/. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al. 2024a. Living in the moment: Can large language models grasp co-temporal reasoning? arXiv preprint arXiv:2406.09072. Zhaochen Su, Zecheng Tang, Xinyan Guan, Lijun Wu, Min Zhang, and Juntao Li. 2022. Improving temporal generalization of pre-trained language models with lexical semantic change. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 63806393, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. 10 Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. 2024b. Mr-ben: meta-reasoning benchmark for evaluating system-2 thinking in llms. CoRR, abs/2406.13975. Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, et al. 2024a. Critic-v: Vlm critics help catch vlm errors in multimodal reasoning. arXiv preprint arXiv:2411.18203. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. 2024b. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, QiIs your ufeng Wang, and Kaizhu Huang. 2024. model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733. Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng. 2024b. Conflictbank: benchmark for evaluating the influence of knowledge conflicts in llm. arXiv preprint arXiv:2408.12076. Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. 2024c. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192. Qwen Team. 2024a. Qwq: Reflect deeply on the boundaries of the unknown. ScalableMath Team. 2024b. Easy-to-hard genhttps://huggingface.co/ eralization models. ScalableMath. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. 2023. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Evaluating mathematiarXiv preprint and Pengfei Liu. 2024. cal reasoning beyond accuracy. arXiv:2404.05692. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. 2024. An implementation of generative prm. https: //github.com/RLHFlow/RLHF-Reward-Modeling. Yibo Yan, Shen Wang, Jiahao Huo, Hang Li, Boyan Li, Jiamin Su, Xiong Gao, Yi-Fan Zhang, Tianlong Xu, Zhendong Chu, et al. 2024. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv preprint arXiv:2410.04509. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024a. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. 2023. Mr-gsm8k: metareasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080."
        },
        {
            "title": "A Detaild Information for PRMBENCH",
            "content": "A.1 Evaluation Subjects In this section, We provide detailed information on our evaluation subjects. The hierarchical categories, corresponding descriptions, and illustrations are shown in Figure 5. We have 6,216 data samples and 83,456 step-level labels. The benchmark spreads across three main evaluation subjects: simplicity, soundness, and sensitivity. Among them, Simplicity comprises two sub-categories: Nonredundant and Non-Circular Logic. Soundness includes four main sub-categories: Empirical Sound, Step Consistency, Domain Consistency, and Confidence Invariant. Finally, Sensitivity mainly evaluates models in three main parts: Prerequisite Sensitivity, Deception Resistance, and Multi-solution Consistency. The descriptions of each sub-category are shown in Figure 5."
        },
        {
            "title": "B Detailed Experiment Results",
            "content": "B.1 Abbreviation Of Sub-Categories The full names of abbreviations used in our tables are shown in Table 7. Abbr. NR. NCL. ES. SC. DC. CI. PS. DR. MS. Full Name Evaluation Subject Non-Redundancy Non-Circular Logic Empirical Soundness Step Consistency Domain Consistency Confidence Invariance Prerequisite Sensitivity Deception Resistance Multi-Solution Consistency Simplicity Simplicity Soundness Soundness Soundness Soundness Sensitivity Sensitivity Sensitivity Table 7: The impact of ICL few-shot numbers on models final performance. The number reported here is PRMScore. B.2 Detailed Results of PRMBENCH In addition to PRMScore displayed in Table 3, we also list the full results with different metrics across different sub-categories here. The detailed evaluation results are shown in Table 8-16 13 Figure 5: Examples of an in-depth evaluation of PRMBench. 14 Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 31. 36.2 52.0 50.5 50.3 54.2 47.0 60.0 54.4 54.2 60.5 50. 37.7 45.1 75.7 80.4 77.3 79.2 64.9 90.8 87.7 89.9 83.8 73. 25.7 27.4 28.3 20.7 23.3 29.1 29.2 29.2 21.1 18.6 37. 26.3 32.3 37.5 63.7 68.5 64.9 67.9 53.0 83.8 78.8 82. 74.2 64.2 24.1 30.1 66.4 75.6 69.9 72.8 51.5 95.5 90.2 95. 79.1 68.2 Proprietary LLMs, Prompted as Critic Models 66.8 68.8 66.0 68.8 67.6 86. 89.2 86.5 89.7 88.1 46.7 48.3 45.5 47.8 47.1 79. 82.1 78.4 82.8 80.6 82.9 86.9 82.3 89.0 85.3 79.4 79. 48.5 27.7 36.1 41.7 61.1 21.2 17.9 13.0 48.4 43. 58.2 55.4 57.2 49.8 55.2 74.7 76.6 22.2 15.1 16. 38.0 54.6 30.3 22.1 17.0 50.8 38.0 64.4 56.4 64. 57.0 60.5 76.3 74.3 79.5 84.2 83.7 84.6 83.5 91. 92.8 95.0 82.8 84.4 76.6 80.4 80.0 82.0 79.7 Table 8: performance comparison of popular models across detailed metrics in ALL categories of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 31.4 35.7 49.3 50.2 48.7 48. 44.0 61.0 46.1 46.4 54.8 46.9 43.7 52.6 77.7 85.3 80. 82.7 67.7 91.5 89.1 91.0 86.7 77.1 19.1 18.8 20. 15.0 17.2 14.9 20.2 30.5 3.2 1.9 22.9 16.8 33. 40.2 65.2 75.0 68.1 71.2 54.0 84.8 80.3 83.5 77.4 66. 29.4 38.0 69.2 83.2 74.1 79.2 55.6 94.0 92.3 96. 85.3 72.4 Proprietary LLMs, Prompted as Critic Models 57.0 65.6 67.2 68.5 64.6 77. 90.8 91.5 91.4 87.9 36.3 40.4 42.9 45.6 41.3 67. 84.1 85.1 85.1 80.4 66.5 90.4 91.8 90.9 84.9 62. 55.5 36.9 17.6 26.5 19.0 43.7 25.0 2.4 1.2 25.3 28. 70.4 41.5 41.7 47.1 50.2 59.1 52.5 20.5 15.3 16. 20.8 39.7 32.8 3.4 1.5 31.6 26.7 77.1 45.8 49. 56.5 57.3 92.2 94.1 86.4 94.4 92.0 95.7 93.8 89. 98.5 99.0 83.8 92.7 68.9 84.1 82.4 84.7 80.0 Table 9: performance comparison of popular models across detailed metrics in NR. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. 15 Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 35.8 41.2 53.4 50.5 49.3 54. 50.3 50.1 47.3 48.9 48.1 48.1 34.7 44.2 65. 70.8 66.3 70.8 60.4 80.8 79.8 82.3 76.7 66.6 36. 38.1 41.3 30.3 32.2 37.2 40.2 19.4 14.9 15.4 19.5 29. 35.8 41.3 56.6 58.8 55.0 60.2 52.3 69.0 67.3 70. 63.9 57.4 22.7 30.9 54.9 66.4 59.0 66.6 49.9 89. 88.7 93.6 81.9 64.0 Proprietary LLMs, Prompted as Critic Models 62.4 63.7 58.1 63.8 62. 73.5 80.4 81.8 81.2 79.2 51.3 47.0 34.5 46.4 44. 65.6 71.4 71.5 72.2 70.2 65.4 80.6 88.1 82.8 79. 75.2 72.6 61.4 36.0 42.9 43.2 58.6 13.6 10.5 9. 16.0 40.0 66.2 46.6 27.4 44.0 46.1 71.4 71.6 15. 15.7 10.2 40.6 50.3 19.0 18.9 15.6 19.9 31.7 80. 47.9 34.7 54.8 54.5 72.9 73.5 65.6 74.8 73.3 79.1 79. 88.1 93.7 94.6 83.4 79.8 59.9 - 79.4 74.6 71. Table 10: performance comparison of popular models across detailed metrics in NCL. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 32.4 36.7 56.4 51.9 54. 57.0 49.4 62.1 56.6 55.7 66.4 52.6 32.9 39.1 76. 78.0 77.3 77.3 62.4 89.7 85.8 87.8 83.3 71.8 32.0 34.4 36. 25.8 31.1 36.7 36.4 34.6 27.4 23.5 49.4 33.4 32. 36.8 65.9 66.0 65.9 66.6 52.7 82.2 76.2 79.0 74.9 63.5 20. 24.9 69.2 73.9 71.4 71.1 48.9 96.6 89.6 94.6 78.1 67.1 Proprietary LLMs, Prompted as Critic Models 72.0 74.5 70.4 72.9 72.4 88.9 88.9 85.3 89.4 88. 55.2 60.0 55.4 56.4 56.7 82.2 82.7 77.9 83.0 81. 88.8 85.6 80.0 89.8 86.0 85.4 88.9 51.7 31.7 41. 48.6 68.0 23.7 22.5 16.2 61.9 49.1 55.4 69.8 69. 55.5 62.5 76.4 82.5 20.0 15.4 15.6 39.7 58.0 37. 24.5 21.4 61.1 41.1 63.2 75.0 76.3 64.8 69.8 72. 67.6 81.2 83.3 83.5 82.0 81.6 91.2 90.5 93.5 81.5 82. 78.7 - 77.1 79.5 78.4 Table 11: performance comparison of popular models across detailed metrics in ES. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. 16 Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 25.7 29.1 47.1 47.6 46.8 52.1 44.5 65.9 55.1 55. 60.3 48.1 32.0 37.0 75.2 81.3 77.6 80.7 64.9 94.1 90.0 92. 83.7 73.5 19.3 21.2 18.9 13.9 16.1 23.5 24.2 37.7 20.2 17. 36.9 22.7 26.2 30.0 62.1 69.2 64.6 69.2 52.0 89.2 82.2 86. 74.1 64.1 19.4 23.0 64.3 74.5 68.3 72.6 49.9 96.7 90.0 95. 74.8 66.3 Proprietary LLMs, Prompted as Critic Models 69.7 67.7 65.7 71.3 68.6 89. 89.7 86.0 91.4 89.3 49.6 45.7 45.4 51.2 48.0 83. 82.7 77.7 85.4 82.3 84.3 84.4 77.0 87.6 83.3 84.8 90. 42.5 23.9 32.6 42.4 68.6 29.4 20.1 13.4 68.2 46. 74.3 68.6 83.1 68.5 73.6 80.4 87.3 24.1 15.5 16. 39.4 63.0 40.1 22.9 16.6 70.6 43.3 76.9 70.8 85. 72.8 76.5 73.6 68.8 80.5 85.0 83.4 85.5 81.9 92. 92.3 95.1 81.5 83.6 76.3 74.2 77.3 81.0 77.2 Table 12: performance comparison of popular models across detailed metrics in SC. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 26.0 30.6 46.7 44.4 44.5 50. 41.3 61.5 54.4 53.2 57.8 46.5 31.1 37.6 71.6 76.0 73. 76.1 59.2 92.1 87.8 90.4 80.3 70.5 20.8 23.7 21. 12.9 15.6 25.3 23.3 30.9 21.0 15.9 35.3 22.4 26. 31.3 58.3 62.3 59.6 63.8 46.7 85.8 78.9 82.8 69.7 60. 19.0 23.6 59.9 68.0 63.8 66.6 44.6 95.3 87.8 93. 71.0 63.0 Proprietary LLMs, Prompted as Critic Models 70.7 73.8 66.0 71.0 70.4 88.2 92. 83.6 88.8 88.2 53.3 55.5 48.4 53.2 52.6 81.2 86. 75.1 81.9 81.2 81.3 87.7 73.2 82.6 81.2 78.0 85. 47.1 22.5 30.2 45.8 60.5 23.8 20.9 12.2 61.9 44. 80.4 77.8 87.5 77.2 80.7 72.1 80.6 20.5 12.9 13. 39.0 52.2 30.5 18.8 11.6 57.9 37.2 81.4 74.5 87. 79.2 80.6 73.1 68.2 74.3 79.1 79.4 80.6 79.8 91. 90.4 94.2 77.5 80.7 71.4 80.0 70.8 73.7 74.0 Table 13: performance comparison of popular models across detailed metrics in DC. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. 17 Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 30.2 34.4 53.3 52.1 53. 57.8 47.7 66.0 63.8 66.2 67.5 53.9 37.7 44.4 79. 83.7 81.7 82.5 66.6 93.7 91.0 93.0 87.7 76.5 22.7 24.4 26. 20.4 25.4 33.1 28.7 38.2 36.7 39.5 47.2 31.2 31. 35.9 68.5 73.0 70.6 72.3 54.5 88.6 84.2 87.5 80.1 67.8 23. 28.9 70.9 78.5 74.0 74.5 51.7 96.9 90.7 94.9 81.0 69.6 Proprietary LLMs, Prompted as Critic Models 71.1 72.3 67.3 71.8 70.7 92.1 91.6 88.3 92.6 91. 50.2 53.1 46.4 51.1 50.2 86.3 85.7 80.8 87.1 85. 90.4 87.3 82.4 91.4 87.9 88.2 90.4 50.0 30.3 43. 56.3 75.1 28.9 37.5 33.5 73.3 55.2 56.5 72.7 68. 55.8 63.5 85.5 87.7 29.4 15.6 27.2 53.8 72.2 40. 45.0 43.3 73.3 52.1 60.4 75.0 76.0 60.5 68.0 81. 77.4 86.2 88.9 89.0 87.3 87.3 93.0 90.9 92.1 86.9 87. 85.2 80.1 84.9 86.1 84.1 Table 14: performance comparison of popular models across detailed metrics in CI. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. Open-source Process Level Reward Models 33.1 36.8 51.0 50.5 49.2 52.8 47.2 55.6 51.5 49.0 57. 48.6 36.2 42.9 71.7 78.7 74.4 77.5 63.6 88. 85.3 87.0 80.5 71.5 30.1 30.8 30.3 22.4 24.1 28. 30.9 22.9 17.6 10.9 35.0 25.7 33.3 37.4 59.7 66.5 61. 65.7 52.3 79.8 75.0 77.4 70.0 61.7 23.2 28.8 62.3 75.5 68. 73.3 51.8 95.4 89.9 94.4 76.8 67.2 Proprietary LLMs, Prompted as Critic Models 62.5 61.8 61. 60.3 61.6 86.8 84.9 83.2 87.1 85.5 38.3 38.7 40. 33.5 37.7 78.2 75.7 73.8 78.4 76.5 88.8 88.6 80.5 90. 87.1 78.4 76.0 47.9 26.5 33.4 34.2 54.5 15. 13.7 7.1 41.6 39.0 34.7 33.1 45.7 28.0 35.4 73. 72.8 24.4 15.6 16.1 29.5 45.0 17.3 16.2 6.7 35.2 32. 33.3 18.8 44.2 27.0 30.8 70.2 70.2 75.0 82.8 81. 83.1 81.4 92.8 93.1 95.9 82.6 82.6 83.8 - 81. 87.0 84.1 Table 15: performance comparison of popular models across detailed metrics in PS. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models. 18 Model Name PRMScore F1 Negative F1 Acc Positive Acc Negative Acc First similarity Skywork-PRM-1.5B Skywork-PRM-7B Llemma-PRM800k-7B Llemma-MetaMath-7B Llemma-oprm-7B MATHMinos-Mistral-7B MathShepherd-Mistral-7B ReasonEval-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B ReasonEval-34B Avg. Open-source Process Level Reward Models 32.3 37.4 53.5 51.3 51.3 55.8 48. 58.0 56.2 55.4 64.3 36.3 44.9 77.8 80.6 78.4 79.1 65.6 90. 87.5 89.5 84.4 28.2 29.9 29.1 22.1 24.1 32.4 31. 25.4 24.9 21.4 44.3 32.5 38.3 66.2 68.9 66.4 68.1 54.2 83. 78.6 81.5 75.6 22.8 29.8 70.4 76.6 72.5 72.8 52.5 96. 90.3 95.1 79.4 84.0 83.3 43.8 27.9 33.8 45. 62.8 16.9 21.0 14.9 57.3 79.1 77.8 23.1 15. 17.2 41.4 57.1 24.6 27.1 19.4 56.8 74.5 73.3 85.1 85. 86.6 83.7 84.0 93.8 92.2 95.0 83.6 51.3 74. 28.5 64.9 69.0 44.6 39.9 85. Proprietary LLMs, Prompted as Critic Models GPT-4o o1-mini Gemini-2.0-flash-exp Gemini-2.0-thinking-exp-1219 Avg. 65.7 64.8 66.2 65.7 65.6 89.2 86. 86.3 89.7 88.0 42.2 42.9 46.1 41.8 43.2 81.8 78. 78.1 82.5 80.2 90.5 84.5 82.7 91.8 87.4 39.3 48.2 55. 37.0 45.0 41.3 43.8 60.2 40.2 46.4 84.8 - 83.4 86.4 84.9 Table 16: performance comparison of popular models across detailed metrics in DR. sub-category of PRMBENCH. The best performance for each metric is highlighted in bold, while the second-best performance is underlined. : To reduce costs, we evaluated only subset of 394 samples for the o1 series models."
        },
        {
            "title": "C Examples For Different Evaluation Subjects",
            "content": "In this section, we provide detailed examples of the various evaluation subjects and their corresponding sub-categories. The abbreviations used are listed in Table 7. C.1 Example for NR. category 1. Original Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. Now we just have to solve the equation x=1000000*0.00125. Step 5: So x=1250. Step 6: Thats the final answer. Step 7: Right. So 1,000,000 lire is equivalent to $1250. 1200 = 0.00125. 2. Modified Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. But lets confirm the computation again: 1.50 Step 5: Now we just have to solve the equation x=1000000*0.00125. Step 6: So x=1250. Step 7: Lets double-check by performing the multiplication again: 1000000*0.00125=1250. Step 8: Thats the final answer. Step 9: Right. So 1,000,000 lire is equivalent to $1250. 1200 = 0.00125. 1200 = 0.00125. 3. Reason Step 4 unnecessarily repeats the computation of 1.50 1200 , which was already completed in Step 7 redundantly re-checks the multiplication, adding no new information. These redundant steps make the reasoning process less concise without affecting correctness. C.2 Example for NCL. category 1. Original Process Question: If x2 1 = 0, what is the value of x3 2x + 1? Step 1: Lets start with the first equation and see if we can solve for x. Step 2: Ok. We can use the quadratic formula to find x. 19 . b24ac 2a 12411 21 . So can be either 1+ 2 Step 3: The quadratic formula is = Step 4: Yes, thats correct. In this case a, b, and are 1, -1, and -1, respectively. Step 5: So if we plug those values in, we get = 1 Step 6: That simplifies to = 1 2 Step 7: Now lets move to the second equation. Step 8: Yes, we need to find x3 2x + 1. Step 9: Right. Well since we now know that x2 1 = 0, we can replace x2 in the second equation with + 1. Step 10: That means that x3 2x + 1 = x(x + 1) 2x + 1. Step 11: And if we substitute in the possible values of that we found earlier, we get either 1+ 2 + 1) 2 1+ 2 + 1 or 1 . or 1 2 ( 1+ 5 2 5 5 5 5 . 2 + 1) 2 1+ Step 12: Lets start with the first case: 1+ 2 Step 13: Theres shorter way. We have x3 2x + 1 = x(x + 1) 2x + 1. Step 14: x3 2x + 1 = (x2 + x) 2x + 1. Step 15: And we know that x2 1 = 0, so x2 = + 1. Step 16: So we can just substitute + 1 for x2. Step 17: So x3 2x + 1 = (x2 + x) 2x + 1 becomes x3 2x + 1 = (x + 1 + x) 2x + 1. Step 18: And that simplifies to x3 2x + 1 = (2x + 1) 2x + 1. Step 19: So x3 2x + 1 = 2. Step 20: So, we dont need to do the second case because we already have the answer. Step 21: Right. So if x2 1 = 0 then x3 2x + 1 = 2. ( 1 5 2 + 1) 2 1 ( 1+ 5 5 2 + 1. 5 2 + 1. 2. Modified Process . b24ac 2a Question: If x2 1 = 0, what is the value of x3 2x + 1? Step 1: Lets start with the first equation and see if we can solve for x. Step 2: Ok. We can use the quadratic formula to find x. Step 3: The quadratic formula is = Step 4: Yes, thats correct. In this case a, b, and are 1, -1, and -1, respectively. Step 5: So if we plug those values in, we get = 1 Step 6: That simplifies to = 1 2 Step 7: Now lets move to the second equation. Step 8: Assume x3 2x + 1 = 2 for verification. Step 9: Substituting x3 2x + 1 = 2 into the equation, it implies x(x2) 2x + 1 = 2. Step 10: Since x2 = + 1, x(x + 1) 2x + 1 = 2. Step 11: Simplifying confirms x3 2x + 1 = 2, verifying the assumption. Step 12: Therefore, x3 2x + 1 = 2 is correct. 21 . So can be either 1+ 2 . or 1 2 12411 5 5 5 . 3. Reason Steps 8 and 9 assume the result (x3 2x + 1 = 2) before deriving it, and step 11 uses circular logic to verify the assumption by substituting it back into the equation. This process does not independently solve the problem and creates flawed reasoning chain. C.3 Example for ES. category 1. Original Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? 20 Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. Now we just have to solve the equation = 1000000 0.00125. Step 5: So x=1250. Step 6: Thats the final answer. Step 7: Right. So 1,000,000 lire is equivalent to $1250. 1200 = 0.00125. 2. Modified Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation = 1000000 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. Now we just have to solve the equation = 1000000 0.00125. Step 5: So x=1250. Step 6: But wait, we need to account for transaction fees that add an additional 10%. Step 7: So = 1250 + 0.10 1250. Step 8: That gives x=1375. Step 9: Right. So 1,000,000 lire is equivalent to $1375. 1200 = 0.00125. 3. Reason counterfactual step was introduced in step 6 by claiming the need to account for an additional 10% transaction fee, which is not mentioned in the problem. This erroneous assumption leads to the incorrect result of $1375 instead of $1250. C.4 Example for SC. category 1. Original Process Question: If x2 1 = 0, what is the value of x3 2x + 1? Step 1: Lets start with the first equation and see if we can solve for x. Step 2: We can use the quadratic formula to find x. Step 3: The quadratic formula is = . Step 4: Substituting = 1, = 1, and = 1, we get = 1 2 Step 5: Since x2 1 = 0, we replace x2 with + 1 in x3 2x + 1. Step 6: Substituting, x3 2x + 1 = x(x + 1) 2x + 1. Step 7: Simplify: x3 2x + 1 = (x2 + x) 2x + 1. Step 8: Substitute x2 = + 1 : x3 2x + 1 = (x + 1 + x) 2x + 1. Step 9: Simplify: x3 2x + 1 = 2. b24ac 2a 5 . 2. Modified Process Question: If x2 1 = 0, what is the value of x3 2x + 1? 21 Step 1: Lets start with the first equation and see if we can solve for x. Step 2: We can use the quadratic formula to find x. Step 3: The quadratic formula is = . Step 4: Substituting = 1, = 1, and = 1, we get = 1 2 Step 5: Since x2 1 = 0, we replace x2 with + 1inx3 2x + 1. Step 6: Substituting, x3 2x + 1 = x(x + 1) 2x + 1. Step 7: Simplify: x3 2x + 1 = (x2 + x) 2x + 1. Step 8: Incorrectly substitute x2 = 1 instead of + 1: x3 2x + 1 = (x 1 + x) 2x + 1. Step 9: Simplify: x3 2x + 1 = 0. b24ac 2a 5 . 3. Reason Step 8 introduces step contradiction by incorrectly substituting x2 = 1 instead of the correct x2 = + 1. This conflicts with the earlier derivation from x2 1 = 0, where x2 = + 1. As result, the final answer becomes 0 instead of the correct value, 2. C.5 Example for DC. category 1. Original Process Question: If x2 1 = 0, what is the value of x3 2x + 1? Step 1: Lets start with the first equation and see if we can solve for x. Step 2: We can use the quadratic formula to find x. b24ac Step 3: The quadratic formula is = . 2a Step 4: Substituting = 1, = 1, and = 1, we get = 1 2 Step 5: Since x2 1 = 0, we replace x2 with + 1 in x3 2x + 1. Step 6: Substituting, x3 2x + 1 = x(x + 1) 2x + 1. Step 7: Simplify: x3 2x + 1 = (x2 + x) 2x + 1. Step 8: Substitute x2 = + 1: x3 2x + 1 = (x + 1 + x) 2x + 1. Step 9: Simplify: x3 2x + 1 = 2. 5 . 2. Modified Process Question: If x2 1 = 0, what is the value of x3 2x + 1? Step 1: Lets start with the first equation and see if we can solve for x. Step 2: We can use the quadratic formula to find x. b24ac Step 3: The quadratic formula is = . 2a Step 4: Substituting = 1, = 1, and = 1, we get = 1 2 Step 5: Since x2 1 = 0, we replace x2 with + 1 in x3 2x + 1. Step 6: Substituting, x3 2x + 1 = x(x + 1) 2x + 1. Step 7: To simplify x3, use the property of cubic roots: x3 = 3 Step 8: Substitute x3 = 3 Step 9: Simplify further: x3 2x + 1 = 3 + 1. x. 5 . and x2 = + 1 into x3 2x + 1: x3 2x + 1 = 3 2x + 1. 3. Reason domain inconsistency was introduced in Step 7 by incorrectly assuming that x3 = 3 x. This assumption is valid for root-based equations but not in the context of solving polynomial equation. This error propagates through the rest of the reasoning, leading to flawed solution. C.6 Example for CI. category 22 1. Original Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. Now we just have to solve the equation = 1000000 0.00125. Step 5: So x=1250. Step 6: Thats the final answer. Step 7: Right. So 1,000,000 lire is equivalent to $1250. 1200 = 0.00125. 2. Modified Process Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Step 1: Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2: There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3: think the second way will be easier. 1.50 Step 4: Thats true. Now we just have to solve the equation x=1000000*0.00125. Step 5: So x=1250. Step 6: Thats the final answer. Step 7: Actually, after closer look, = 0.00125 1000000 should be 12500. must have miscalculated earlier. Step 8: So 1,000,000 lire is equivalent to $12500. 1200 = 0.00125. 3. Reason Step 7 introduces confident hallucination. It contradicts the correct value of 1250 by recalculating and asserting that the result is 12500, which is false. This error is presented with unwarranted certainty to simulate confident misjudgment. C.7 Example for PS. category 1. Original Process Question: Find all values of such that 3 = 5. Step 1: Start with the equation 3 = 5. Step 2: Recognize that the absolute value equation = implies two cases: = or = b. Step 3: Apply this rule to 3 = 5, resulting in two equations: 3 = 5 or 3 = 5. Step 4: Solve each equation: = 8 from 3 = 5 and = 2 from 3 = 5. Step 5: The solutions are = 8 and = 2. 23 2. Modified Process 3)2 = 5. 3)2 = 5 can be represented as special form of 3 = 5. Question: Find all values of such that ( Step 1: ( Step 2: Start with the equation 3 = 5. Step 3: Recognize that the absolute value equation = implies two cases: = or = b. Step 4: Apply this rule to 3 = 5, resulting in two equations: 3 = 5 or 3 = 5. Step 5: Solve each equation: = 8 from 3 = 5 and = 2 from 3 = 5. Step 6: The solutions are = 8 and = 2. 3. Reason The first step introduces missing condition by changing the equation to 3 = 5, which ignores the implicit requirement that 3 must be non-negative. The final step incorrectly asserts that the solutions are = 8 and = 2 without considering the domain restriction. C.7 Example for PS. category 1. Original Process Question: Given that F(r) = (f (r), g(r), h(r)) is three-dimensional vector, prove that: r1, r2, r1 < < r2 such that F(r1) F(r2) = (r1 r2)F(r). Step 1: Understanding the Given Information and Problem We have vector function F(r) = (f (r), g(r), h(r)), where , g, and are real-valued functions of the real variable r. We need to prove that for any r1, r2, there exists an such that r1 < < r2 and F(r1) F(r2) = (r1 r2)F(r), where F(r) denotes the gradient of at (i.e., vector composed of the derivatives of each component with respect to r). Step 2: Breaking Down the Problem into Components Express the vector equation in terms of its components: (f (r1) (r2), g(r1) g(r2), h(r1) h(r2)) = (r1 r2) (f (r), g(r), h(r)) . Thus, we need to find an (r1, r2) such that for each component: (r1) (r2) r1 r2 = (r), g(r1) g(r2) r1 r2 = g(r), h(r1) h(r2) r1 = h(r). Step 3: Considering the Application of the One-Dimensional Mean Value Theorem In one dimension, the Mean Value Theorem (MVT) states that for function continuous on [a, b] and differentiable on (a, b), there exists (a, b) such that: (b) (a) = (c)(b a). Applying the MVT to each component function , g, and h, we obtain corresponding points cf , cg, and ch. However, these points generally differ (cf = cg = ch), so we cannot guarantee the existence of common where all component equations hold simultaneously. Step 4: Considering Counterexample Since we cannot guarantee common satisfying all components, lets attempt to find specific counterexample to demonstrate that the original proposition may not hold. Step 5: Trying the Function F(r) = (sin r, cos r, r) Let F(r) = (sin r, cos r, r). Choose two distinct real numbers, for example, r1 = 0 and r2 = π. 24 Step 6: Compute F(r1) F(r2): F(0) F(π) = (sin 0 sin π, cos 0 cos π, 0 π) = (0 0, 1 (1), π) = (0, 2, π). Step 7: Compute (r1 r2)F(r): (r1 r2) (cos r, sin r, 1) = (0 π) (cos r, sin r, 1) = (π) (cos r, sin r, 1) . Step 8: We need to find (0, π) such that: (0, 2, π) = (π cos r, π sin r, π). Step 9: This leads to the equations: π cos = 0, π sin = 2, π = π. Step 10: The third equation is always true. Simplifying the first two equations: From π cos = 0: cos = 0. Step 11: From π sin = 2: sin = 2 π . Step 12: However, cos = 0 implies = π 2 . Substituting = π 2 into the second equation: sin (cid:17) (cid:16) π = 1 = 2 π . π 0.6366, the equality does not hold. Therefore, there is no in (0, π) that Step 13: Since 2 satisfies both equations simultaneously. Step 14: The above calculations show that for the function F(r) = (sin r, cos r, r), there does not exist an (r1, r2) such that: F(r1) F(r2) = (r1 r2)F(r). 2. Modified Process Question: Given that F(r) = (f (r), g(r), h(r)) is three-dimensional vector, prove that: r1, r2, r1 < < r2 such that F(r1) F(r2) = (r1 r2)F(r). Step 1: Understanding the Given Information and Problem We have vector function F(r) = (f (r), g(r), h(r)), where , g, and are real-valued functions of the real variable r. We need to prove that for any r1, r2, there exists an such that r1 < < r2 and F(r1) F(r2) = (r1 r2)F(r), where F(r) denotes the gradient of at (i.e., vector composed of the derivatives of each component with respect to r). Step 2: Breaking Down the Problem into Components Express the vector equation in terms of its components: (f (r1) (r2), g(r1) g(r2), h(r1) h(r2)) = (r1 r2) (f (r), g(r), h(r)) . Thus, we need to find an (r1, r2) such that for each component: (r1) (r2) r1 r2 = (r), g(r1) g(r2) r1 r2 = g(r), h(r1) h(r2) r1 r2 = h(r). 25 Step 3: Applying the Mean Value Theorem for Vector-Valued Functions The Mean Value Theorem (MVT) can be extended to vector-valued functions. Specifically, if F(r) is continuous on [r1, r2] and differentiable on (r1, r2), then there exists some (r1, r2) such that: F(r2) F(r1) = F(r)(r2 r1). Step 4: Expressing the Derivative and Rearranging the Equation Recall that the derivative F(r) is given by: F(r) = (cid:0)f (r), g(r), h(r)(cid:1) = F(r). Rewriting the equation from Step 3: F(r1) F(r2) = (r1 r2)F(r). Step 5: Confirming the Equality Holds Component-Wise This equation holds for each component: (r1) (r2) = (r1 r2)f (r), g(r1) g(r2) = (r1 r2)g(r), h(r1) h(r2) = (r1 r2)h(r). Step 6: Concluding the Proof Since all components satisfy the Mean Value Theorem at the same point r, we have: F(r1) F(r2) = (r1 r2)F(r), where (r1, r2). Step 7: Final Statement Therefore, for any r1 and r2, there exists an between them such that the original equation holds, which completes the proof. 3. Reason MVT cannot be used in 3D space, which is deception or trap, and the counterexample is good way to show that the proposition may not hold. But we changed it to correct proof by extending MVT to vector-valued functions and ignoring the restrictions of MVT C.9 Example for MS. category 1. Process 1 Question: Three pencils and jumbo eraser cost $1.24. Five pencils and jumbo eraser cost $1.82. No prices include tax. In cents, what is the cost of pencil? Step 1: Lets call the price of pencil and the price of jumbo eraser e. Then we can write two equations. Step 2: We have 3p + = 1.24 and 5p + = 1.82. Step 3: To solve this system, lets subtract the first equation from the second equation. This will eliminate e. Step 4: 5p + 3p = 1.82 1.24. Step 5: This simplifies to 2p = 0.58. So = 0.29. Step 6: That means pencil costs 29 cents. Answer 29 2. Process 2 Question: Three pencils and jumbo eraser cost $1.24. Five pencils and jumbo eraser cost $1.82. No prices include tax. In cents, what is the cost of pencil? Step 1: need to solve this problem to find the cost of pencil in cents. The problem gives me two scenarios involving pencils and jumbo erasers, along with their total costs. Lets first understand whats being asked. Step 2: The problem states: - Three pencils and one jumbo eraser cost $1.24. - Five pencils and one jumbo eraser cost $1.82. And it asks for the cost of pencil in cents. Step 3: Since the prices are given in dollars, should convert them to cents to make calculations easier because the answer is required in cents. - $1.24 is equal to 124 cents. - $1.82 is equal to 182 cents. So, can rephrase the problem as: - 3 pencils + 1 jumbo eraser = 124 cents - 5 pencils + 1 jumbo eraser = 182 cents Step 4: Now, need to find the cost of one pencil. Lets denote: - Let be the cost of one pencil in cents. - Let be the cost of one jumbo eraser in cents. So, can write the equations based on the given information: Equation 1: 3p + = 124 Equation 2: 5p + = 182 Step 5: have system of two equations with two variables (p and e). To find the value of p, can use the method of elimination or substitution. think elimination might be straightforward here. Step 6: If subtract Equation 1 from Equation 2, the (eraser cost) will cancel out, and can solve for p. So, Equation 2 - Equation 1: (5p + e) - (3p + e) = 182 - 124 Simplifying: 5p + - 3p - = 58 Which reduces to: 2p = 58 Step 7: Now, solving for p: = 58 / 2 = 29 cents Step 8: So, the cost of one pencil is 29 cents. But to make sure, should check if this makes sense with both original equations. Step 9: Plugging = 29 into Equation 1: 3(29) + = 124 87 + = 124 = 124 - 87 = 37 cents Step 10: Now, plugging = 29 and = 37 into Equation 2 to verify: 5(29) + 37 = 145 + 37 = 182 cents, which matches the second equation. Step 11: Everything checks out, so the cost of pencil is indeed 29 cents. Final Answer"
        },
        {
            "title": "D Prompts",
            "content": "D.1 Prompts For Generating Data As introduced in Section 3.2, we query GPT-4o (OpenAI, 2024a) to synthesize the metadata at the very first step of our data curation procedure. To better prompt LLMs to generate high-quality data instances, we carefully designed our prompts, which are displayed below. We display only one example here due to limitations in space. And the full prompts can be found in our supplementary materials. D.1.1 Prompts for NR. category 1. System Prompt Task Description You are helpful AI assistant who is very good at reasoning and data construction. Now want to test the ability of process-level reward models to judge whether step within the reasoning process is correct. To do this, please help me build flawed cases by introducing specific types of errors into given reasoning process. You will be provided with: 1. mathematical problem. 2. Its standard correct answer. 3. correct step-by-step reasoning process used to solve it. Your task is to modify the question, adjust one or more steps, or introduce additional steps into the original process chain to create reasoning process that appears plausible but is incorrect. The objective is to simulate flawed solutions by incorporating the specified error detailed after \"Error Type to Introduce\". Error Type to Introduce Redundancy refers to process that is not the most concise or efficient, as it includes one or more redundant steps that can be removed without affecting the correctness of the overall solution path. For example, if represents correct inference chain, your task is to introduce one or more redundant steps = {cc is redundent} and reformulate the solution chain as B. Formatting Instructions After making the modifications, provide the following structured output: { } \" original_question \": \" The original mathematical problem .\" , \" modified_question \": \" The modified problem or original problem \" original_process \": [\" original_step 1\", \" original_step 2\", ... ], \" modified_process \": [\" modified_step 1\", \" modified_step 2\", ... ], \" modified_steps \": [1, 5, 7, ... ], \" error_steps \": [5, 6, ... ], \" reason \": \" Explanation for the changes .\" Detailed Requirements: 1. original_question: string representing the original mathematical problem as provided. 2. modified_question: string representing the modified problem after your changes. If the problem remains the same, you can copy the original question. 3. original_process: non-empty list of strings representing the original reasoning steps provided as input. 4. modified_process: non-empty list of strings representing the reasoning process after your modifications. 5. modified_steps: non-empty list of integers indicating the indexes of all modified steps. Indexing starts at 1. 6. error_steps: non-empty list of integers representing the steps that contain hallucinations or errors. These should also be part of modified_steps. 7. reason: clear explanation of the modifications made, why they were introduced, and how they 28 align with the specified error types. Notes: 1. Ensure all lists are non-empty. 2. Use the LaTeX format for all mathematical symbols (e.g., x2 for squared). Do not use Unicode symbols such as u2248 or u00f7. 3. Ensure the JSON object is well-formed, with proper escaping for special characters like backslash (e.g., use backslash backslash for newlines). 4. All indexes start from 1, that is, the first steps index is 1, not 0. 5. You can choose to modify the question or not, if the question remains the same, you can copy the original question. But if the question is modified, ensure that the steps are judged based on the modified question. 6. Please give the original process as provided by the prompt, do not modify it. 2. Few Shots User 1 Question: If x2 1 = 0, what is the value of x3 2x + 1? Steps: Step 1. Lets start with the first equation and see if we can solve for x. Step 2. We can use the quadratic formula to find x. b24ac Step 3. The quadratic formula is = . 2a Step 4. Substituting = 1, = 1, and = 1, we get = 1 2 Step 5. Since x2 1 = 0, we replace x2 with + 1 in x3 2x + 1. Step 6. Substituting, x3 2x + 1 = x(x + 1) 2x + 1. Step 7. Simplify: x3 2x + 1 = (x2 + x) 2x + 1. Step 8. Substitute x2 = + 1: x3 2x + 1 = (x + 1 + x) 2x + 1. Step 9. Simplify: x3 2x + 1 = 2.\" Answer 2 Assistant 1 5 . { \" original_question \": \" If $x ^2 - - 1 = 0$, what is the value of $x ^3 - 2x + 1$ ?\" , \" modified_question \": \" If $x ^2 - - 1 = 0$, what is the value of $x ^3 - 2x + 1$ ?\" , \" original_process \": [ \" Let ' start with the first equation and see if we can solve for .\" , \" We can use the quadratic formula to find .\" , \" The quadratic formula is $x = frac { -b pm sqrt {b^2 - 4 ac }}{2a}$ .\" , \" Substituting $a = 1$, $b = -1$, and $c = -1$, we get $x = frac {1 pm sqrt {5}}{2}$ .\" , \" Since $x ^2 - - 1 = 0$, we replace $x ^ 2$ with $x + 1$ in $x ^3 - 2x + 1$ .\" , \" Substituting , $x ^3 - 2x + 1 = x(x + 1) - 2x + 1$ .\" , \" Simplify : $x ^3 - 2x + 1 = (x^2 + x) - 2x + 1$ .\" , \" Substitute $x ^2 = + 1$: $x ^3 - 2x + 1 = (x + 1 + x) - 2x + 1$ .\" , \" Simplify : $x ^3 - 2x + 1 = 2$ .\" ], \" modified_process \": [ \" Let ' start with the first equation and see if we can solve for .\" , \" We can use the quadratic formula to find .\" , \" The quadratic formula is $x = frac { -b pm sqrt {b^2 - 4 ac }}{2a}$ .\" , \" Substituting $a = 1$, $b = -1$, and $c = -1$, we get $x = frac {1 pm sqrt {5}}{2}$ .\" , \" Since $x ^2 - - 1 = 0$, we replace $x ^ 2$ with $x + 1$ in $x ^3 - 2x + 1$ .\" , \" Substituting , $x ^3 - 2x + 1 = x(x + 1) - 2x + 1$ .\" , \" Simplify : $x ^3 - 2x + 1 = (x^2 + x) - 2x + 1$ .\" , 29 \" Now let ' verify that $x ^2 = + 1$ by re - solving $x ^2 - - 1 = 0$ again .\" , \" Substitute $x ^2 = + 1$: $x ^3 - 2x + 1 = (x + 1 + x) - 2x + 1$ .\" , \" Simplify : $x ^3 - 2x + 1 = 2$ .\" ], \" modified_steps \": [8], \" error_steps \": [8], \" reason \": \" Step 8 re - solves $x ^2 - - 1 = 0$ unnecessarily , introducing redundancy . This step does not provide new information , as $x ^2 = + 1 $ was already established in The reasoning is still correct but less efficient .\" } User 2 Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Steps : Step 1. Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* rac1.501200. Step 2. There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3. think the second way will be easier. rac1.501200 = 0.00125. Step 4. Thats true. Now we just have to solve the equation x=1000000*0.00125. Step 5. So x=1250. Step 6. Thats the final answer. Step 7. Right. So 1,000,000 lire is equivalent to $1250. Answer 1250 Assistant 2 { \" original_question \": \" In 1992, scoop of gelato could be purchased in Italy for 1200 lire . The same gelato would have cost $ $1.50$ in the .S. At the equivalent exchange rate between the lire and the dollar , how many dollars would be equivalent to 1,000,000 lire ?\" , \" modified_question \": \" In 1992, scoop of gelato could be purchased in Italy for 1200 lire . The same gelato would have cost $ $1.50$ in the .S. At the equivalent exchange rate between the lire and the dollar , how many dollars would be equivalent to 1,000,000 lire ?\" , \" original_process \": [ \" Let ' call the number of dollars x. So , the problem is asking us to solve the equation x= 10 00 00 0* $ frac {1.50}{1200}$ .\" , \" There are two ways to solve this equation . We could either divide 100 0000 by 1200 and then multiply the result by 1. 50 or we could divide 1.50 by 1200 and then multiply the result by 1000000 .\" , \"I think the second way will be easier . $ frac {1.50}{1200}=0.00125$ .\" , \" That ' true . Now we just have to solve the equation x=1000000*0.00125 .\" , \" So x=$1250$ .\" , \" That ' the final answer .\" , \" Right . So 1,000,000 lire is equivalent to $ $ 1250$ .\" ], \" modified_process \": [ \" Let ' call the number of dollars x. So , the problem is asking us to solve the equation x= 10 00 00 0* $ frac {1.50}{1200}$ .\" , 30 \" There are two ways to solve this equation . We could either divide 100 0000 by 1200 and then multiply the result by 1. 50 or we could divide 1.50 by 1200 and then multiply the result by 1000000 .\" , \"I think the second way will be easier . $ frac {1.50}{1200}=0.00125$ .\" , \" That ' true . But let ' confirm the computation again : $ frac {1.50}{ 200}=0.00125$ .\" , \" Now we just have to solve the equation =1 00 00 00*0.00125 .\" , \" So x=$1250$ .\" , \" Let ' double - check by performing the multiplication again : 1000000*0. 00125=$1250$ .\" , \" That ' the final answer .\" , \" Right . So 1,000,000 lire is equivalent to $ $ 1250$ .\" ], \" modified_steps \": [4, 7, 8], \" error_steps \": [4, 7, 8], \" reason \": \" Step 4 unnecessarily repeats the computation of $ frac {1.50}{1 200}$, which was already completed in Step 7 redundantly re - checks the multiplication , adding no new information . These redundant steps make the reasoning process less concise without affecting correctness .\" } D.2 Prompts For Evaluating Generative LLMs As introduced in Section 4.1, we prompt some state-of-the-art generative LLMs as critic models to evaluate their rewarding capabilities on PRMBENCH. To make fair comparison between different models, we carefully design the prompts and utilize unified prompt to query them. The prompt used is displayed below. D.2.1 Prompts for Evaluating Generative LLMs 1. System Prompt You are mathematical reasoning evaluator. Your task is to analyze mathematical problem-solving steps and provide structured assessments in JSON format. For each solution step, you need to evaluate two aspects: 1. Validity Score (-1 to +1): * +1: Completely correct mathematical reasoning * 0: Partially correct with some mistakes * -1: Completely incorrect * Use any value in between to indicate varying degrees of correctness 2. Redundancy Score (-1 to +1): * -1: Critical step, absolutely necessary for the solution * 0: Moderately important step * +1: Completely redundant, can be omitted * Use any value in between to indicate varying degrees of redundancy Requirements: - Evaluate each step independently - Provide scores as floating-point numbers - Return results in strict JSON format: \"validity\": [scores], \"redundancy\": [scores] - Ensure both arrays have the same length as the number of steps - Maintain mathematical rigor in your evaluation - Consider mathematical accuracy, logical coherence, and solution efficiency Example output format: {\" validity \": [0.8, -0.5, 1.0], \" redundancy \": [ -1.0, 0.3, 0.7]} 31 2. Few Shots User 1 Question: In 1992, scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $1.50 in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? Solution: Step 1. Lets call the number of dollars x. So, the problem is asking us to solve the equation x=1000000* 1.50 1200 . Step 2. There are two ways to solve this equation. We could either divide 1000000 by 1200 and then multiply the result by 1.50 or we could divide 1.50 by 1200 and then multiply the result by 1000000. Step 3. think the second way will be easier. 1.50 Step 4. Thats true. Now we just have to solve the equation x=1000000*0.00125. Step 5. So x=1250. Step 6. So x=1250. Step 7. Thats the final answer. Step 8. Right. So 1,000,000 lire is equivalent to $1250. Answer 1250 Assistant 1 1200 = 0.00125. { \" validity \" : [1.0, 0.5, 1.0, 1. 0, 1 .0 , 1. 0, 1 .0, 1.0], \" redundancy \" : [ -1.0, 0.5, -0.5, -0.5, -1.0, 1.0, 0.5, 1.0]} User 2 Question: Four points, A, B, C, and D, are chosen randomly and independently on the circumference of circle. What is the probability that segments AB and CD intersect? Solution: Step 1. Lets think about what it means for the segments to intersect. Step 2. Whether they intersect is entirely function of the order of B, C, on the circle, when we look at it counterclockwise and consider to come first. Step 3. Exactly. So lets consider the different possible orders of B, C, D. Step 4. The number of possible orders is 3! = 6. Step 5. Yes, thats right. Now, we need to figure out how many of these result in the segments intersecting. Step 6. If is in between and D, then the segments will intersect. Step 7. If is in between and D, then the segments will intersect. Step 8. Right. Thats two of the possible orders. Step 9. So, the probability that the segments intersect is Step 10. And thats our answer. Answer 1/3 Assistant 2 2 6 = 1 3 . { \" validity \" : [1.0, 1.0, 1.0, 1. 0, 1 .0 , -1 .0 , 0.8, 1.0, 1.0, 1.0], \" redundancy \" : [ -1.0, -0.5, -0.5, -0.5, -0.5, -0.5, 0.5, -0.5, -1.0, 1.0]}"
        },
        {
            "title": "E Instructions For Annotators",
            "content": "As described in Section 3.3, we recruited five volunteers to evaluate our proposed PRMBENCH. The instructions are shown below. E.1 Instruction for Annotators 1. Backgrounds With the emergence of multi-step reasoning enhanced language models such as OpenAI o1, and Deepmind Gemini-thinking, these models demonstrate the ability to decompose complex problems and solve them step by step. However, while their solutions often appear correct, they may contain errors in understanding, calculation, or reasoning logic. popular way to evaluate the results generated by these models is by utilizing process-level reward models (PRMs). Nevertheless, PRMs are fallible and not always correct. Existing benchmarks are not adequate to evaluate PRMs on different error types. Under this circumstance, we are building comprehensive evaluation benchmark for PRMs that can have fine-grained detection of PRMs. 2. Task Definition We begin by collecting completely correct multi-solution data and leveraging state-of-the-art (SOTA) LLMs to introduce various types of errors into these correct solutions, thereby generating our test cases. The detailed error types are described in Section 3. All synthesized data instances undergo an initial filtering process based on specific features. Your task is to identify whether the modification taken is reasonable and whether the modified data instance is different from the original data instance."
        },
        {
            "title": "2.1 Sub-task 1",
            "content": "The first sub-task is binary classification task whose options include yes and no. Your task is to decide whether the modified step-by-step solution generated by LLMs is reasonable. The word reasonable has two aspects for evaluation. The modified process generated by LLMs seems like possible solution path that could happen. The modified process generated by LLMs is exactly wrong and the type of error is suitable for the current classification. Please assign yes for this sub-task if both of the answers to the above two questions are yes. Otherwise, assign no for this sub-task."
        },
        {
            "title": "2.2 Sub-task 2",
            "content": "The second sub-task is binary classification task whose options include yes and no. Your task is to decide whether the modified step-by-step solution generated by LLMs is different from the original solution process. The word different means the modified solution process is logically different from the original one, or there exist different statements compared to the original process. Please assign yes to this sub-task if your answer to the above question is yes. Otherwise, assign no for this sub-task. 3. Error Types Redundancy refers to process that is not the most concise or efficient, as it includes one or more redundant steps that can be removed without affecting the correctness of the overall solution path. For example, if represents correct inference chain, your task is to introduce one or more redundant steps = {cc is redundent} and reformulate the solution chain as B. 33 Circular logic is specific form of redundancy, characterized by reasoning chain that starts at step S, progresses through sequence of steps, and ultimately loops back to S. Symbolically, this can be expressed as S, where S, A, and represent individual reasoning steps. Your task is to modify the reasoning process to introduce such circular logic. Counterfactual counterfactual step refers to statement within reasoning chain that contradicts established ground truth. Such contradictions can arise from relying on outdated theories, omitting critical constraints in theory, or incorporating erroneous assumptions. Your task is to modify the reasoning process to introduce such counterfactual steps. Step contradiction refers to conflict between specific step and other steps within reasoning path. Given reasoning path = S1, S2, . . . , Sn, step contradiction exists if Si Sj, where i, [1, n] and = j. Your task is to modify the reasoning process to introduce such step contradiction steps. Domain inconsistency is special type of counterfactual. It refers to step within the reasoning chain that uses statement or theory valid in other domains or cases but is not valid within the current reasoning chain. Your task is to modify the reasoning process to introduce such domain inconsistency steps. Confident hallucination is special type of counterfactual. It refers to statement within the reasoning chain that contradicts established ground truth and is presented with an overly confident tone. In other words, it involves stating an incorrect statement with unwarranted certainty. Your task is to modify the reasoning process to introduce such confident hallucination steps. Missing condition or prerequisite refers to flaw in the reasoning chain where critical premises, assumptions, or necessary conditions are absent. This omission results in logical gaps, incomplete reasoning, or biased conclusions. For example, when missing condition occurs, the model is required to solve the problem through case analysis or further investigation. However, the answer becomes incorrect if the model overlooks the missing condition and proceeds with standard reasoning methods. Your task is to modify the reasoning process to introduce such missing condition error steps. Deception or traps refer to statements that appear to be correct or align with ground truth but are subtly altered to introduce inaccuracies while maintaining the illusion of correctness. Your task is to modify the reasoning process to introduce such deception or trap error steps."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "Soochow University",
        "Stony Brook University",
        "The Chinese University of Hong Kong"
    ]
}