{
    "paper_title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models",
    "authors": [
        "Jamie Hayes",
        "Ilia Shumailov",
        "Christopher A. Choquette-Choo",
        "Matthew Jagielski",
        "George Kaissis",
        "Katherine Lee",
        "Milad Nasr",
        "Sahra Ghalebikesabi",
        "Niloofar Mireshghallah",
        "Meenatchi Sundaram Mutu Selva Annamalai",
        "Igor Shilov",
        "Matthieu Meeus",
        "Yves-Alexandre de Montjoye",
        "Franziska Boenisch",
        "Adam Dziedzic",
        "A. Feder Cooper"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 3 7 7 8 1 . 5 0 5 2 : r Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Jamie Hayes*1, Ilia Shumailov1, Christopher A. Choquette-Choo1, Matthew Jagielski1, George Kaissis1, Katherine Lee1, Milad Nasr1, Sahra Ghalebikesabi1, Niloofar Mireshghallah2, Meenatchi Sundaram Muthu Selva Annamalai3, Igor Shilov4, Matthieu Meeus4, Yves-Alexandre de Montjoye4, Franziska Boenisch5, Adam Dziedzic5 and A. Feder Cooper*6 1Google DeepMind, 2University of Washington, 3University College London, 4Imperial College London, 5CISPA Helmholtz Center for Information Security, 6Cornell University, *Equal contribution State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittleachieving close-to-arbitrary successand insights from strong attacks in simplified settings do not translate to todays LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRAone of the strongest MIAsto GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC < 0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested. 1. Introduction In membership inference attack (MIA), an adversary aims to determine whether specific data record was part of models training set (Shokri et al., 2017; Yeom et al., 2018). MIAs pose significant privacy risk to ML models, but state-of-the-art attacks are often too computationally expensive to run at the scale of pre-trained large language models (LLMs). This is because strong MIAs require training multiple reference models to calibrate membership predictionsand pre-training even one LLM is often prohibitively expensive in research settings. As result, current work makes one of two compromises: running weaker attacks that avoid training reference models (e.g., attacks that fine-tune an LLM), or running strong attacks that train small reference models on small datasets. However, both exhibit notable limitations (Section 2). Weaker attacks are more practical, but they have been shown to be brittleoften performing no better than random guessing (Duan et al., 2024; Fu et al., 2024; Mireshghallah et al., 2022b). Stronger attacks, when run in simplified settings, fail to capture the complex dynamics of large-scale, pre-trained language models; as result, their insights do not reliably generalize to modern LLMs (Meeus et al., 2024a). Results from both of these approaches leave key questions unanswered about the effectiveness of MIAs on LLMs. In particular, are the fidelity issues of weaker attacks due to omitting reference models, or do they point to deeper, more fundamental challenge with applying membership inference to large language models? Current research has not offered an answer because, to date, there are no baselines of how stronger MIAs perform on large-scale, pre-trained LLMs. Corresponding author(s): jamhay@google.com, afedercooper@gmail.com; A. Feder Coopers contributions originated with 20232024 student researcher position at Google Research. 2025 Google DeepMind. All rights reserved Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models In this paper, we bridge this gap by running stronger attacks at scale significantly larger than previously explored. We pre-train over 4,000 GPT-2like reference models, ranging from 10 million to 1 billion parameters (Liu et al., 2024), on subsets of the C4 dataset (Raffel et al., 2020) that are three orders of magnitude larger than those used in prior MIA studiesup to 100 million examples, compared to fewer than 100,000 in previous work (Meeus et al., 2024c). We use these models to conduct detailed investigation of the Likelihood Ratio Attack (LiRA) (Carlini et al., 2022a), one of the strongest MIAs in the literature. This substantial effort proves worthwhile, as we uncover three key insights that advance the state of the art in understanding the potency and reliability of membership inference attacks on large language models: Strong membership inference attacks can succeed on pre-trained LLMs. We are the first to execute strong attacks at this scale, and find that LiRAin contrast to weaker fine-tuning attacks can easily beat random baselines (Section 3.1). Our results on Chinchilla-optimal models (trained for 1 epoch) exhibit non-monotonic relationship between model size and MIA vulnerability: larger models are not necessarily more at risk (Section 3.2). The overall success of strong MIAs is limited on pre-trained LLMs. Even though we demonstrate that LiRA can succeed at LLM scale, we are only able to achieve impressive results (i.e., AUC 0.7) when diverging from typical training conditionsspecifically, by varying training-dataset sizes and training for multiple epochs (Section 4.1). The relationship between MIA success and related privacy metrics is not straightforward. We find that examples seen later in training tend to be more at risk (Section 5.1); however, this trend is complicated by sample length, which also affects vulnerability. We also study if there is any relationship between training-data extraction and MIA, and observe no correlation with MIA success. This suggests that the two privacy attacks capture different signals related to memorization (Section 5.2). Altogether, our contributions serve not only as an extensive benchmark of strong MIAs on pre-trained LLMs, but also provide some initial answers to urgent open questions about the conditions under which MIAs exhibit threat to privacy for language models. Our work also quantifies the performance gap between weaker (more feasible) and stronger attacks, establishing an upper bound for what weaker attacks could realistically achieve in this setting. Our hope is that this guides future research on MIA, informing the development of stronger and more practical attacks, as well as more effective defenses. 2. Background and related work Membership inference is key approach for assessing empirical privacy and information-leakage risks in ML models. The most effective attacks calibrate their predictions based on how models behave on specific data points (Shokri et al., 2017; Yeom et al., 2018). Using the target models architecture and training setup, the attacker trains multiple reference models on different subsets of the training data. The attacker queries each reference model with given data point and computes membership inference score from the models output (e.g., loss or logit). By comparing these scores across reference models, the attacker learns how the score distributions differ between members (in the training data) and non-members (unseen data, out of the training data). The attacker can use this signal to infer membership of examples in the target models training set (Carlini et al., 2022a; Sablayrolles et al., 2019; Watson et al., 2022; Ye et al., 2022; Zarifzadeh et al., 2024). The number of reference models necessary for successful attacks varies across methodsfrom tens or hundreds for the Likelihood Ratio Attack (LiRA) (Carlini et al., 2022a) and Attack-R (Ye et al., 2022), to as few as 1 or 2 for the Robust Membership Inference Attack (RMIA) (Zarifzadeh et al., 2024). (See Appendix A.1.) While these attacks have been successfully applied to smaller settings, they are 2 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models often considered impractical for contemporary language models due to the prohibitive computational cost of training even single reference LLM. As result, prior work attempts to approximate stronger, reference-model-based attacks in various ways. Small-scale, strong, reference-based attacks. Song and Shmatikov (2019) were the first to train (10) reference models, in order to evaluate privacy in smaller language models (RNNs). However, insights from such settings do not translate to todays LLMs (Meeus et al., 2024c), as the training dynamics differ significantly. Other work has applied MIAs using only single reference model for small, pre-trained masked language models (Mireshghallah et al., 2022a), but this approach reduces attack precision, as effective calibration of membership predictions becomes more difficult with fewer reference models. Larger-scale, weak, reference-free attacks. To avoid the cost of training reference models, weaker attacks consider range of signals to infer membership, typically leveraging black-box access to the model. For example, Yeom et al. (2018) use model loss computed on the target example, Carlini et al. (2021) use normalized model loss and zlib entropy of the target example, and Mattern et al. (2023) compare the model loss to the loss achieved for neighboring samples. More recent work experiments with token probabilities Shi et al. (2024); Zhang et al. (2025b) and changes in loss based on prompting with different context (Wang et al., 2025; Xie et al., 2024). Beyond black-box query-access, other work attempts to derive membership signal from changing the model. For instance, prior work has perturbed inputs or model parameters and observed resulting changes in model loss on the target, or used (parameter-efficient) fine-tuning on domain-specific datasets to detect privacy or confidentiality risks (Chang et al., 2024; Fu et al., 2024; Kandpal et al., 2024; Lukas et al., 2023; Meeus et al., 2025; Mireshghallah et al., 2022b; Panda et al., 2025; Rossi et al., 2024). However, fine-tuning-based attacks introduce new data to the problem setup, which may complicate the validity of using MIAs to detect benchmark contamination (Deng et al., 2024; Maini and Bansal, 2025; Maini et al., 2024; Oren et al., 2023) and to draw reliable conclusions about other sensitive data issues (Cooper and Grimmelmann, 2024; Cooper et al., 2023, 2024, 2025; Duarte et al., 2024; Lee et al., 2023; Meeus et al., 2024b; Shi et al., 2024; Wei et al., 2024; Zhang et al., 2025a). Further, recent approach that evaluates attacks on LLMs using post-hoc collected datasets also exhibits serious limitations. While prior work has reported high success rates on variety of models and datasets (AUC 0.8) (Meeus et al., 2024a; Shi et al., 2024; Wang et al., 2025; Xie et al., 2024; Zhang et al., 2025b), such evaluations rely on the models training-date cutoff as proxy for distinguishing between member and non-member data points (Maini et al., 2024). These newer data introduce distribution shift, which undermines the validity of the reported results (Das et al., 2024; Duan et al., 2024; Maini et al., 2024; Meeus et al., 2024c). And further, as others have noted, when current MIAs are evaluated in controlled privacy game like this, they often barely outperform random guessing (Duan et al., 2024; Meeus et al., 2024c). 3. Examining strong MIAs in realistic settings for pre-trained LLMs Altogether, the limitations of prior work raise the key question that motivates our work: are the fidelity issues of weaker attacks due to omitting reference models, or do they point to deeper, more fundamental challenge with applying membership inference to large language models? This is big question, so we break it down into smaller ones that we can test with specific experiments that reveal different information about the effectiveness of strong MIAs on pre-trained LLMs. The initial step of our evaluation involved deciding which strong MIA method to use across our experiments. We evaluated two of the strongest attacks in the literatureLiRA (Carlini et al., 2021) and RMIA (Zarifzadeh et al., 2024)and, for the experiments that follow, we opted to use LiRA because we 3 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models observed it to be more effective in our pre-trained LLM setting. We defer details about LiRA and comparisons with RMIA to Appendix A, and focus on our results using LiRA in the remainder of the paper. In this section, we investigate the relationship between the number of reference models and attack success (Section 3.1). Based on our results, we decide to use 128 reference models throughout all following experiments in this work. Then, we test the effectiveness of strong attacks under realistic settingssettings that reflect how LLMs are actually trained. To do so, we run LiRA using target and reference models of various sizes, which we train according to Chinchilla-scaling laws (Hoffmann et al., 2022) (Section 3.2). Together, these experiments inform our first key result: strong membership inference attacks can succeed on pre-trained LLMs. In the following sections, we expand upon these results to other training and attack conditions; we will refine our first key result by investigating the limits of strong MIA success rates (Section 4), and by digging beneath these average rates to reveal how attacks impact individual-example members. General setup. For all experiments, we pre-train GPT-2 architectures of varying sizesfrom 10M to 1Bon subsets of the C4 dataset (Raffel et al., 2020) using the open-source NanoDO library (Liu et al., 2024). The training datasets we use are 3 orders of magnitude larger than those in prior MIA studies: up to 50M examples, compared to fewer than 100K examples in previous work (Meeus et al., 2024c). We explore datasets of this size because, while it is well established that MIA success depends on both model capacity and training-dataset size, the nature of this relationship remains unexplored at the scale of pre-trained LLMs. For each attack, we start with fixed dataset of size 2ð‘ examples drawn from C4, from which we randomly subsample reference-model training sets of size ð‘. For instance, if ð‘ is 10M examples, we select them by randomly subsampling from fixed dataset of 10M2=20M examples. (This means our MIA analysis runs over an overall dataset size of 50M2=100M in our largest experimental setting.) We use different random seed for each subsample, which yields the different member (in) and non-member (out) distributions for each example that we use to run LiRA. Specific experimental configurations vary, so we introduce additional setup as needed. (See Appendix for more details.) 3.1. Warm-up: How many reference models should we use? To determine the number of reference models to use for all of our experiments, we train 140Mparameter models on 7M examples. 7M examples equates to approximately 2.8B training tokens i.e., what is optimal for this model size, according to Chinchilla scaling laws (Hoffmann et al., 2022) with an over-training multiplier of 20. As shown in Figure 1, we test range of reference models. The plot shows multiple Receiver Operating Characteristic (ROC) curves, indicating the True Positive Rate (TPR) for the given False Positive Rate (FPR) on loglog scale. The Area under the Curve (AUC) is provided for each ROC curve. The dashed red line represents the baseline for which membership predictions would be effectively arbitrary (i.e., TPR and FPR are equal; AUC is 0.5). We choose to report AUC as our primary metric, as it is more challenging to visualize the TPR over wide range of FPR in streamlined way. (For comparison, see Figure 2b, which provides such an alternate visualization for only limited Figure 1 LiRA with different numbers of reference models. We attack 140M-parameter model trained on 7M examples. As reference models increase, LiRAs performance improves (measured with ROC AUC). However, there are diminishing returns: AUC is effectively unchanged from 128 to 256 reference models. 4 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models range of FPR, at the cost of not surfacing overall AUC.) We also investigate the performance of different observation signals (Appendix A.2), and choose to use samples loss. Note that, while LiRA clearly beats the random baseline, it is not remarkably successful in this setting: regardless of the number of reference models, it never achieves an AUC of 0.7. Further, even though LiRAs attack success increases with more reference models, there are diminishing returns. From 1 to 8 reference models, the AUC has relative increase of 13.3%; for the next 8 increase (from 8 to 64), the AUC only increases 7.6%; and, doubling from 128 to 256 only yields 0.2% improvement. 3.2. Training and attacking compute-optimal model In practice, models are typically trained based on observed scaling laws: for given model size, the scaling law suggests the optimal number of tokens to use for training. In this section, to assess MIA in realistic conditions for pre-trained LLMs, we attack models of various sizes that we have trained for 1 epoch, setting the number of samples to be optimal according to Chinchilla scaling (Hoffmann et al., 2022). Specifically, we set the number of training set tokens to be 20 larger than the number of model parameters. We only train for 1 epoch, common choice in large training runs (Bai et al., 2023; Touvron et al., 2023). Additional details about the specific training and experimental recipes are in Appendix and Appendix E, such as the number of samples used in training across different model sizes. In Figure 2, we show the results of attacking models of sizes 10M, 85M, 302M, 489M, 604M and 1018M. These model sizes come from the default configurations available in NanoDO (Liu et al., 2024). For improved readability, we exclude the results for the 140M model in our plots in this section, as we investigated this architecture above. Note that the attack on the 140M model with 128 reference models has an AUC of 0.683, which puts its performance below both the 85M and 302M models. Interestingly, we observe that there is non-monotonic relationship between model size and MIA vulnerability under these training conditions. In Figure 2a, the 85M-parameter model shows the highest AUC (0.699), followed by the 302M model (AUC 0.689), and then the 140M model (see Figure 1, AUC 0.683); the 489M model exhibits the lowest AUC (0.547). This is also supported in Figure 2b, which provides different view of the same attack. Each line compares the TPR for the different-sized models at different fixed settings of FPR. From 10M to 302M, there is consistent pattern of the TPR increasing with model size (regardless of the setting of FPR); but then, when increasing to 489M, there is significant drop in TPR. Before running this experiment, our expectation was that each line would look approximately horizontal, as the training-dataset size is being scaled proportionally (and optimally, according to Hoffmann et al. (2022)) to model size. There are many reasons why this may not have occurred. First, the most pronounced differences in TPR are at extremely small values. Even subtle differences in training runs may flip few samples from correct to incorrect member predictions, which, in the low TPR regime, can have large effect on MIA success. Second, Chinchilla scaling (Hoffmann et al., 2022) is not the only such law. Sardana et al. (2023), Hu et al. (2024), and Grattafiori et al. (2024) all introduce other ways to optimally select the number of training tokens for given model. In future work, we will investigate if these other token-size-selection methods stabilize TPR as model size grows. As we discuss below (Section 4.2), repeating this experiment by training these same architectures on fixed dataset size exhibits vastly different results. We additionally test different training configurations; in Appendix we alter the learning rate schedule, and observe that there is modest effect on attack performance. (See Appendix B, where, as sanity check, we also confirm that the larger models converge to lower loss values, reflecting their increased capacity to fit the training data.) 5 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) ROC for 6 Chinchilla-optimal models (1 epoch). (b) TPR at fixed FPR for different model sizes. Figure 2 MIA vulnerability across compute-optimally trained models of different sizes. (a) ROC curves using 128 reference models demonstrate varying MIA susceptibility for models with 10M (AUC 0.592), 85M (AUC 0.699), 302M (AUC 0.689), 489M (AUC 0.547), 604M (AUC 0.654) and 1018M (AUC 0.553) parameters when trained under Chinchilla-optimal conditions for 1 epoch. The 85M and 302M models shows the highest vulnerability, indicating that increasing model size does not uniformly decrease MIA risk in this setting. (b) How TPR (for each given FPR) varies by model size for different Chinchilla-optimal models. 4. Investigating the limits of strong attacks Even in the most successful (i.e., high AUC) case, overall performance on inferring membership is not particularly impressive when running LiRA with large number of reference models on computeoptimal models trained for single epoch. Similar to our experiments with LiRA and varied numbers of reference models (Figure 1), the maximum AUC we observe remains under 0.7 for all model sizes (Figure 2). This raises natural follow-on question: if we free ourselves from the constraints of these typical training settings, will it then be possible to improve MIA success? Can we identify an upper bound on how strong MIAs could possibly perform on pre-trained LLMs? To address this question, in this section we run attacks on models trained on different-sized (i.e., not always Chinchilla-optimal) datasets (Section 4.2) for more than 1 epoch (Section 4.1). Our experiments show that diverging from typical settings can indeed improve attack success. However, while these experiments are useful sanity check, they do not directly suggest conclusions about the effectiveness of strong MIAs in general. Instead, they suggest that there appears to be an upper bound on how well strong MIAs can perform on LLMs under practical conditions. In other words, these experiments inform our second main observation: the success of strong MIAs is limited in typical LLM training settings. 4.1. Effects of scaling the compute budget (i.e., training for more epochs) In Figure 3a, we compare MIA ROC for the 44M model architecture under different training configurations. We keep the total number of tokens surfaced to the model during training Chinchilla-optimal, but we alter when these tokens are surfaced. As baseline, we train for 1 epoch on the entire dataset; when we attack this model with LiRA, it yields an AUC of 0.620. (See Figure 3a, 1 of 1.) We then take half of the training dataset and train the same architecture over 2 epochs. In both settings the total number of training tokens is Chinchilla-optimal, however, in the latter experiment, the model has processed each training example twice rather than once. When we attack the model trained for 2 epochs, we observe significant increase in MIA vulnerability: the AUC is 0.744higher both than this model 6 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 44M model, split dataset in half and train for 2 epochs, or train on the entire dataset for 1 epoch. (b) 140M model, training for 10 epochs. Figure 3 Varying epochs while keeping the overall dataset size Chinchilla-optimal. In (a), we compare training 44M model on the whole Chinchilla-optimal dataset in 1 epoch (AUC 0.620, after 1 of 1 epoch) to training for 2 epochs on only half of the dataset (AUC 0.744, after 2 of 2 epochs). In (b), we train 140M model on the whole Chinchilla-optimal dataset for 10 epochs. With more epochs, AUC increases. See main text for additional observations. when it has only completed 1 epoch of training (0.628, 1 of 2) and than the model trained for 1 epoch on the entire dataset (0.620, 1 of 1). This underscores that increasing training epochs, even on smaller dataset to maintain Chinchilla optimality for overall training tokens, amplifies vulnerability to MIA, compared to training for fewer epochs on larger dataset. However, we also observe that there is no significant uplift in TPR at small FPR between epochs 1 and 2 for the 2-epoch experiment. We also observe that the MIA at the second epoch is less successful than the one after 1 epoch for small FPR. To investigate this further, we additionally perform experiments with the 140M architecture for various numbers of epochs. In Figure 3b, we show how the ROC curves and resulting AUC change over the course of training for 10 epochs. As expected, the AUC increases with more epochs, starting from 0.573 and reaching 0.797 at the end of the tenth epoch.1 Interestingly, like Figure 3a, there again seems to be an FPR inflection point where TPR for later epochs is smaller than earlier epochs. In Appendix C, we also train the 140M model architecture on fewer than the 7 million Chinchillaoptimal examples, and (similar to Figure 3a) we observe that there is more dramatic increase in MIA vulnerability. We show that attacking 140M model trained on 219 524,000 examples exhibits both greater absolute MIA success and faster relative increase in success in the first few training epochs. 4.2. Effects of scaling the training dataset size We next run two sets of experiments to investigate the role of training-dataset size on MIAbeyond training on the Chinchilla-optimal number of tokens. We train 140M models on datasets ranging from 50K to 10M examples (again for single epoch) and measure these models susceptibility to LiRA. In Figure 4a, we show the ROC curves for the different models, which suggest that TPR@FPR is not necessarily positively correlated with decreasing the training dataset size. In other words, as we train models on smaller datasets, it is not always the case that TPR for given FPR increases. Rather, AUC is highest for moderately sized datasets (around 1M examples, in this case with AUC of 0.753), and decreases for both very small and very large datasets (under 0.7, for both). Indeed, the capacity 1At the end of epoch 1, the AUC of 0.573 differs from the AUC of 0.678 we found in the experiments in Figure 2a, where the model is only trained for 1 epoch. We believe this is because of the substantially different learning rates between the two experimental setups. 7 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 140M model, trained on various dataset sizes for single epoch. (b) Various model sizes on fixed-size dataset (223 samples) for single epoch. Figure 4 Varying sizes of training data and models. In (a), we train and attack 140M models on different-size datasets, ranging from 50K to 10M examples, and show MIA success does not monotonically increase with increasing dataset size. In (b), we train different architectures on fixed dataset size, and plot how TPR varies at fixed FPR. Here, there is monotonic increase in MIA success if we fix the training set size and increase the model size. of the model also has an effect on susceptibility to successful strong MIA. In Figure 4b, we train different model sizes with fixed training set size of 223 8.3M examplesa number that is significantly larger than Chinchilla-optimal for several of our models (e.g., 10M, 44M). We plot the average and standard deviation of TPR rates, where we repeat this experiment 16 times using different random seeds, which has the effect of dictating the batch order. That is, for each model size, we train 16 sets of 128 reference models, and we also vary the target model over each experimental run. We include the associated AUC-ROC curves for each model size in Appendix C, which are consistent with Figure 4b in demonstrating MIA prediction variability. We observe monotonic increase in TPR at different fixed FPRs as the model size increases. Notice, this is quite different from results in Figure 2b, where we scale the training set size with model size. As model capacity grows, vulnerability to MIA also grows if we keep the training set size constant. Further, we also note that there is significantly more variance in TPR for larger model sizes and at smaller fixed FPR. 5. Analyzing sample vulnerability to membership inference The instability in membership predictions that we observe above suggests natural follow-on question: when does strong MIA succeed? More particularly, which samples are actually vulnerable to MIA, and (how) does this vulnerability vary during training? In this section, we approach these questions by digging deeper into our strong attacks on 140M model (trained with Chinchilla-optimal training dataset size for single epoch). We show how our large-scale experiments yield novel insights about the behavior of individual membership predictions (Section 5.1). Samples seen later in training tend to be more vulnerable; however, this trend is complicated by sample length, which also affects vulnerability. While sample length has previously been linked to extraction risk (Carlini et al., 2023; Nasr et al., 2023), we observe no correlation between MIA and extraction, which suggests that the two are capturing different signals related to memorization (Section 5.2). Together, this analysis informs our third key takeaway: the relationship between MIA vulnerability and related privacy metrics is not straightforward. Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) Evolution of per-sample MIA success for member samples over training steps. (b) Token-length distributions for samples that are least and most vulnerable to MIA, and samples for which MIA has difficulty predicting membership. Figure 5 Sample vulnerability to MIA. We show different aspects of per-sample vulnerability for 140M model. (a) We plot the evolution of per-sample vulnerability throughout training. We show the probability of individual training samples being correctly identified as members at different stages of the training processthe per-example true positive probabilities, ð‘ƒ(predicted as membermember). (b) We plot distributions over sample lengths, according to MIA vulnerability for the largest 1,000 samples with smallest ð‘ƒ(predicted as membermember) (least vulnerable), ð‘ƒ(predicted as membermember) (most vulnerable), and ð‘ƒ(predicted as membermember) closest to 0.5 (difficult samples for MIA; this is equivalent to LiRA score of 1). 5.1. Identifying patterns in per-sample MIA vulnerability We first investigate how sample MIA vulnerability evolves over the course of training. In Figure 5a, we provide scatter plot that illustrates the per-sample true-positive probabilities by training step. That is, we plot how the probability of training sample being correctly predicted as member (ð‘ƒ(predicted as membermember)) changes as model training progresses. There is considerable variance in the underlying sample true-positive probabilities. At any particular training step, the true-positive probabilities over batch of samples can vary by more than 15%, having significant effect on overall attack success. We explore this high degree of instability further in Appendix I, where we plot the mean and standard deviation of per-example true positive probabilities. The mean ð‘ƒ(predicted as membermember) for many samples is close to 0.5; their predictions are close to arbitrary, meaning that they are challenging for MIA (see Figure 5b). The associated standard deviations are also quite large (on average, 0.143). As result, the sample membership predictions can easily flipsince they can change to be above or below 0.5depending on the random seed (dictating batch order) and the specific target model. Nevertheless, the density of the points shifts upward toward the end of training (around step 60,000). Unsurprisingly, samples in batches that are processed in later epochs tend to be more vulnerable, as indicated by their higher probability of being correctly identified as members. This result highlights that the recency of exposure influences samples vulnerability to membership inference. Put differently, samples introduced earlier in training are more likely to be forgotten (Carlini et al., 2022b): they are less vulnerable to MIA. While this appears to be the dominant trend in this setting, the details are bit more complicated. In Figure 5b, we investigate if there are other patterns in sample vulnerability. We plot the distribution over training samples according to their length in tokens, and partition this distribution according to their vulnerability. We consider samples that are members, but which LiRA predicts confidently and incorrectly to be non-members, to be the least vulnerable. (LiRA being confident and wrong reduces 9 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models the probability of sample being correctly identified as member to below 0.5.) In contrast, samples that LiRA correctly and confidently predicts to be members are the most vulnerable. (This brings the probability to above 0.5.) We also highlight samples where LiRA struggles to determine if member is member or non-member. (These samples have score of 1; as noted above, the probability that these samples are predicted to be members is 0.5, indicating their predictions are effectively arbitrary.) In summary, Figure 5b suggests that it is not just the case that samples seen later in training are more vulnerable (Figure 5a); it is also often the case that vulnerable sequences tend to be longer. (See also Figure 19 in the Appendix, which illustrates similar results for samples that have higher proportion of <unk> tokens and higher average TFIDF scores.) This result is consistent with those in Carlini et al. (2023), which show that sequences that are vulnerable to extraction tend to be greater in length. 5.2. Comparing MIA vulnerability and extraction Results such as those in Figure 5b, which show alignment between MIA vulnerability and training-data extraction attacks (Carlini et al., 2021), are consistent with prior work on memorization in machine learning. In general, it is assumed that successful membership inference attack and successful extraction of training data imply that some degree of memorization has occurred for the attacked ML model. For MIA, this is assumed because the success of such attacks hinges on the models tendency to behave differently for data it has seen during training (members) compared to unseen data (non-members); this differential behaviour is frequently ascribed to the model having memorized certain aspects of the training data. Figure 6 We plot the 1,000 samples predicted most strongly as member of training by LiRA for the Chinchilla-optimal 140M model trained for 1 epoch (which contains approx. 7M training samples). We plot samples that are members (713) of training in blue and non-members in orange (287). We plot the negative log-probability of (the first 100 tokens of) each of these samples from which we can derive extraction rates according to (ð‘›, ð‘)-discoverable extraction metric introduced by Hayes et al. (2025). As final experiment, we therefore investigate whether, in this setting, the samples that are vulnerable to our strong MIAs are also vulnerable to extraction attacks. In Figure 6, we compute extraction metrics for the 1,000 samples identified as most vulnerable to MIA in the 140M model, using the first 50 tokens of sample as prefix and measuring the log-probability of the next 50 tokens (a variant of discoverable extraction, introduced in Carlini et al. (2021). Specifically, we use samples negative log-probability as proxy for computing modified version of discoverable extractionthe (ð’, ð’‘)-discoverable extraction metric introduced by Hayes et al. (2025). Traditional discoverable extraction evaluates attack success as binary outcome (success or failure); in contrast, (ð‘›, ð‘)-discoverable extraction quantifies the number of attempts ð‘› an adversary needs to extract particular training sample at least once with probability ð‘ (ranging from 0 to 1), given specific prompt, model, and decoding scheme. Generally, smaller negative logprobability implies that sample is easier to extract. Hayes et al. (2025) show that traditional discoverable extraction underestimates what is actually extractable (given more than one attempt); we therefore choose this metric for extraction, as we expect it to provide more reliable signal for memorization. In our experiments, with respect to MIA score after 1 epoch of training, LiRA is able to identify 10 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models training members with better-than-random AUC. Out of 1,000 samples with the highest LiRA score, 713 of these are indeed training members. Despite obtaining useful MIA signal, we are unable to extract any of the correctly identified member samples with meaningful probability! In Figure 6, the smallest negative log-probability of member samplei.e., the member sample most vulnerable to extractionis approximately 5. To understand this in terms of (ð‘›, ð‘)-discoverable extraction, an adversary would need to attempt extraction over ð‘› = 230, 000 times to extract the sample with confidence ð‘ > 90%! Altogether, while much prior work draws direct connection between MIA vulnerability and extraction risk (e.g., Carlini et al., 2021), our results suggest more nuanced story. Our results suggest that the success of strong MIA on given member sample (i.e., an MIA true positive) does not necessarily imply that the LLM is more likely to generate that sample than would be expected under the data distribution (Hayes et al., 2025). 6. Conclusion and future work In this paper, we present the first large-scale study of strong membership inference attacks on large language models. To enable strong attacks that calibrate their membership predictions using reference models, we train thousands of GPT-2-like models (ranging from 10M1B parameters) on enormous training datasets sampled from C4datasets that are up to three orders of magnitude larger than those used in prior work. Through dozens of experiments, we aim to answer an urgent open question in ML privacy research: are the fidelity issues of weaker attacks due to omitting reference models, or do they point to deeper, more fundamental challenge with applying membership inference to large language models? We uncover three novel groups of findings: while (1) strong MIAs can succeed on pre-trained LLMs (Section 3), (2) their success is limited (i.e., AUC < 0.7) for LLMs trained using practical settings (Section 4), and (3) the relationship between MIA vulnerability and related privacy metricsnamely, extractionis not straightforward (Section 5). Further, as the first work to perform large-scale strong MIAs on pre-trained LLMs, we are also the first to clarify the extent of actual privacy risk MIAs pose in this setting. By evaluating the effectiveness of strong attacks, we are able to establish an upper bound on the accuracy that weaker, more feasible attacks can achieve. More generally, we also identify the conditions under which MIAs are effective on pre-trained LLMs. Together, our findings can guide others in more fruitful research directions to develop novel attacks and, hopefully, more effective defenses. They also suggest that, in the future (and with additional compute cost), it may be possible and worthwhile to derive scaling laws for MIAs."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pages 26332650, 2021. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pages 18971914. IEEE, 2022a. Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. Advances in Neural Information Processing Systems, 35:1326313276, 2022b. Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian TramÃ¨r, and Chiyuan Zhang. Quantifying Memorization Across Neural Language Models. In International Conference on Learning Representations, 2023. Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, and Reza Shokri. Context-aware membership inference attacks against pre-trained large language models. arXiv preprint arXiv:2409.13745, 2024. A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright, Memorization, and Generative AI. arXiv preprint arXiv:2404.12590, 2024. A. Feder Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher CallisonBurch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, Madiha Zahrah Choksi, Jack M. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, Andres Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, Mark Lemley, Cass Matthews, Christine McLeavey, Corynne McSherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, and Elana Zeide. Report of the 1st Workshop on Generative AI and Law. arXiv preprint arXiv:2311.06477, 2023. A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Matthew Jagielski, Katja Filippova, Ken Ziyu Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Niloofar Mireshghallah, Ilia Shumailov, Eleni Triantafillou, Peter Kairouz, Nicole Mitchell, Percy Liang, Daniel E. Ho, Yejin Choi, Sanmi Koyejo, Fernando Delgado, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Solon Barocas, Amy Cyphert, Mark Lemley, danah boyd, Jennifer Wortman Vaughan, Miles Brundage, David Bau, Seth Neel, Abigail Z. Jacobs, Andreas Terzis, Hanna Wallach, Nicolas Papernot, and Katherine Lee. Machine Unlearning Doesnt Do What You Think: Lessons for Generative AI Policy, Research, and Practice. arXiv preprint arXiv:2412.06966, 2024. A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, and Percy Liang. Extracting memorized pieces of (copyrighted) books from open-weight language models. arXiv preprint arXiv:2505.12546, 2025. Debeshee Das, Jie Zhang, and Florian TramÃ¨r. Blind baselines beat membership inference attacks for foundation models. arXiv preprint arXiv:2406.16201, 2024. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 86988711, 2024. Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? In First Conference on Language Modeling, 2024. AndrÃ© Duarte, Xuandong Zhao, Arlindo Oliveira, and Lei Li. De-cop: detecting copyrighted content in language models training data. In Proceedings of the 41st International Conference on Machine Learning, pages 1194011956, 2024. Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. Membership inference attacks against fine-tuned large language models via self-prompt calibration. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 12 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, and A. Feder Cooper. Measuring memorization in language models via probabilistic extraction. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 92669291, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.469/. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training ComputeOptimal Large Language Models, 2022. URL https://arxiv.org/abs/2203.15556. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher Choquette-Choo, and Zheng Xu. User inference attacks on large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1823818265, 2024. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021. Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin Bout AI Generation: Copyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133, 2023. Peter J. Liu, Roman Novak, Jaehoon Lee, Mitchell Wortsman, Lechao Xiao, Katie Everett, Alexander A. Alemi, Mark Kurzeja, Pierre Marcenac, Izzeddin Gur, Simon Kornblith, Kelvin Xu, Gamaleldin Elsayed, Ian Fischer, Jeffrey Pennington, Ben Adlam, and Jascha-Sohl Dickstein. NanoDO: minimal Transformer decoder-only language model implementation in JAX, 2024. URL http: //github.com/google-deepmind/nanodo. Version 0.1.0. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-BÃ©guelin. Analyzing leakage of personally identifiable information in language models. In 2023 IEEE Symposium on Security and Privacy (SP), pages 346363. IEEE, 2023. Pratyush Maini and Hritik Bansal. Peeking behind closed doors: Risks of llm evaluation by private data curators. In The Fourth Blogpost Track at ICLR 2025, 2025. Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. Llm dataset inference: Did you train on my dataset? Advances in Neural Information Processing Systems, 37:124069124092, 2024. 13 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1133011343, 2023. Matthieu Meeus, Shubham Jain, Marek Rei, and Yves-Alexandre de Montjoye. Did the neurons read your book? document-level membership inference for large language models. In Proceedings of the 33rd USENIX Conference on Security Symposium, pages 23692385, 2024a. Matthieu Meeus, Igor Shilov, Manuel Faysse, and Yves-Alexandre de Montjoye. Copyright traps for large language models. In Forty-first International Conference on Machine Learning, 2024b. Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, and Yves-Alexandre de Montjoye. Sok: Membership inference attacks on llms are rushing nowhere (and how to fix it). arXiv preprint arXiv:2406.17975, 2024c. Matthieu Meeus, Lukas Wutschitz, Santiago Zanella-BÃ©guelin, Shruti Tople, and Reza Shokri. The canarys echo: Auditing privacy risks of llm-generated synthetic text. arXiv preprint arXiv:2502.14921, 2025. Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference attacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 83328347, 2022a. Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans, and Taylor BergKirkpatrick. An empirical analysis of memorization in fine-tuned autoregressive language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 18161826, 2022b. Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian TramÃ¨r, and Katherine Lee. Scalable Extraction of Training Data from (Production) Language Models. arXiv preprint arXiv:2311.17035, 2023. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set contamination in black-box language models. In The Twelfth International Conference on Learning Representations, 2023. Ashwinee Panda, Xinyu Tang, Christopher A. Choquette-Choo, Milad Nasr, and Prateek Mittal. Privacy auditing of large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=60Vd7QOXlM. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Lorenzo Rossi, BartÅ‚omiej Marek, Vincent Hanke, Xun Wang, Michael Backes, Adam Dziedzic, and Franziska Boenisch. Auditing empirical privacy protection of private llm adaptations. In Neurips Safe Generative AI Workshop 2024, 2024. Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and HervÃ© JÃ©gou. White-box vs black-box: Bayes optimal strategies for membership inference. In International Conference on Machine Learning, pages 55585567. PMLR, 2019. 14 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 318. IEEE, 2017. Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 196206, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Cheng Wang, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, and Kai-Wei Chang. Con-recall: Detecting pre-training data in llms via contrastive decoding. In Proceedings of the 31st International Conference on Computational Linguistics, pages 10131026, 2025. Lauren Watson, Chuan Guo, Graham Cormode, and Alexandre Sablayrolles. On the importance of difficulty calibration in membership inference attacks. In International Conference on Learning Representations, 2022. Johnny Wei, Ryan Wang, and Robin Jia. Proving membership in llm pretraining data via data watermarks. In Findings of the Association for Computational Linguistics ACL 2024, pages 13306 13320, 2024. Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Gong, and Bhuwan Dhingra. Recall: Membership inference via relative conditional log-likelihoods. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 86718689, 2024. Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. Enhanced membership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 30933106, 2022. Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF), pages 268282. IEEE, 2018. Sajjad Zarifzadeh, Philippe Liu, and Reza Shokri. Low-cost high-power membership inference attacks. In Forty-first International Conference on Machine Learning, 2024. Jie Zhang, Debeshee Das, Gautam Kamath, and Florian TramÃ¨r. Membership Inference Attacks Cannot Prove that Model Was Trained On Your Data, 2025a. URL https://arxiv.org/abs/2409. 19798. Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Frank Yang, and Hai Li. Min-k%++: Improved baseline for pre-training data detection from large language In The Thirteenth International Conference on Learning Representations, 2025b. URL models. https://openreview.net/forum?id=ZGkfoufDaU. Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models A. Comparing membership inference attacks and signals At the beginning of this project, we considered two candidates for strong membership inference attacks to use in our experiments: the Likelihood Ratio Attack (LiRA) (Carlini et al., 2022a) and the Robust Membership Inference Attack (RMIA) (Zarifzadeh et al., 2024). Both attacks involve training reference models (Section 2) that enable the computation of likelihood ratios (which result in stronger attacks), though they differ in important ways. LiRA (Carlini et al., 2022a) estimates membership by comparing the loss of an example ð’™ in target model to empirical loss distributions from reference models trained with and without ð’™. In contrast, RMIA (Zarifzadeh et al., 2024) performs and aggregates statistical pairwise likelihood ratio tests between ð’™ and population samples ð’›, using both reference models and ð’› to estimate how the inclusion of ð’™ versus ð’› affects the probability of generating the observed model ðœƒ. By leveraging signal from both models and population samples, Zarifzadeh et al. (2024) observe that RMIA can outperform LiRA using fewer reference models. However, no prior work has compared these methods in the pre-trained LLM setting and with large numbers of reference models, leaving open the question of which attack fares better under these conditions. In this appendix, we investigate this question for the first time, and our results clearly indicate that LiRA outperforms RMIA for large number of reference models in the online setting. However, RMIA can outperform LiRA if the population dataset is large enough and the attack is performed for certain small numbers of reference models. This pattern is not completely straightforward: LiRA seems to perform better with 1 or 2 reference models, while RMIA performs better with 416, and then LiRA once again outperforms RMIA for > 16 reference models. Overall, though, attacks with larger numbers of reference models perform better, which means that for our setting LiRA is the best choice for our experiments. Our aim is to test the strongest attacks possible, as this is useful for an upper bound on attack performance. For those with smaller compute budgets that wish to still run strong attacks on 16 reference models, RMIA may be better choice. We train 140M-parameter models on 7M examples, which equates to approximately 2.8B training tokens (i.e., what is optimal for this model size, according to Chinchilla scaling laws (Hoffmann et al., 2022) with an over-training multiplier of 20). First, we provide more details on the strong attacks we study (Appendix A.1). Second, we show how different choices of inference signal impact attack performance (Appendix A.2), which provide more detail about the choices we make in our overall experimental setup (which we introduce in Section 3). Finally, we show our full results that compare the performance of LiRA and RMIA using different numbers of reference models (Appendix A.3). A.1. More background on membership inference attacks Formalization. Given target model with parameters ðœƒ and an input ð’™, an MIA method ðœ‡ aims to determine membership score Î›ðœ‡ (ð’™) capturing meaningful information whether ð’™ was used to train target model ðœƒ. Using ground truth information about the membership of target records ð’™, the performance for method ðœ‡ is computed using the membership score and threshold-agnostic metric such as ROC AUC or TPR at low FPR. Let ð‘“ðœƒ(ð’™) denote scalar statistic computed from the target model on ð’™, e.g., loss or confidence score. In its simplest form, ð‘“ðœƒ(ð’™) can be used directly as the membership score, based on the assumption that training examples yield lower loss than non-members, or Î›Loss(ð’™) = ð‘“ðœƒ(ð’™) (Yeom et al., 2018). No reference models (Section 2) are used in this baseline approach."
        },
        {
            "title": "Different methods have been proposed that use reference models to improve upon this membership",
            "content": "16 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models signal. For such strong attacks, we denote reference models ðœ™ Î¦ as set of models trained on data from similar distribution as the data used to train the target model ðœƒ. Typically, set of reference models contains an equal amount of models trained on data including target example ð’™ (Î¦IN) and excluding ð’™ (Î¦OUT). This is the setup we adopt in this work, which corresponds to the online attack in Carlini et al. (2022a); Zarifzadeh et al. (2024). In contrast, the offline attack assumes only access to Î¦OUT. LiRA (Carlini et al., 2022a) leverages the target record loss computed on the IN reference models Î¦IN and the OUT models Î¦OUT in order to compute membership score. Specifically, for each model ðœ™ Î¦IN Î¦OUT, one computes ð‘“ðœ™(ð’™). Let ð‘IN and ð‘OUT be the empirical distributions of these values from Î¦IN and Î¦OUT, respectively. LiRA defines the membership signal as the likelihood ratio Î›LiRA(ð’™) = ð‘IN( ð‘“ðœƒ(ð’™)) ð‘OUT( ð‘“ðœƒ(ð’™)) . In practice, ð‘IN and ð‘OUT are modeled as univariate Gaussians fit to the empirical values of ð‘“ðœ™(ð’™) from the respective datasets. RMIA (Zarifzadeh et al., 2024) also compares the target models output on ð’™ to that of set of reference models Î¦; however, it uses different likelihood ratio test: ð›¼(ð’™) = ð‘“ðœƒ(ð’™) ð”¼ðœ™Î¦ [ ð‘“ðœ™(ð’™)] . The expected value in the denominator is approximated empirically by computing over the reference models that one actually trains. To improve robustness, RMIA further contextualizes this score relative to reference population â„¤. For each ð’› â„¤: ð›¼(ð’›) = ð‘“ðœƒ(ð‘§) ð”¼ðœ™Î¦ [ ð‘“ðœ™(ð‘§)] , ð¿(ð‘¥, ð‘§) = ð›¼(ð‘¥) ð›¼(ð‘§) . The final membership signal is defined as the fraction of population points ð’› for which this ratio exceeds threshold ð›¾: Î›RMIA(ð’™) = 1 â„¤ ð’›â„¤ (cid:20) ð›¼(ð’™) ð›¼(ð’›) (cid:21) ð›¾ . A.2. Different signal observations In our initial experiments in Section 3 for comparing which strong attack to useLiRA (Carlini et al., 2022a) or RMIA (Zarifzadeh et al., 2024)we also investigated the efficacy of different membership inference signals. We compare using the model loss and model logits (averaged over the entire sequence), for example, in Figure 7, looking at the ROC curve for LiRA and 140M sized model trained on 7ð‘€ examples. The plot shows the True Positive Rate (TPR) against the False Positive Rate (FPR) on loglog scale, with one curve each for logit and loss signals. The logit curve has an AUC of 0.576, while the loss curve has higher AUC of 0.678. This indicates that using the loss as signal results in more effective attack compared to using logits in this specific experimental setup. In general, we opt to use loss as our membership inference signal metric, as we observe it to be more effective. A.3. MIA attack performance for different number of reference models Figure 8 compares LiRA and RMIA, showing ROC curves and AUC for different numbers of reference models. Figure 9 provides an alternate view of the same results, plotting AUC for both attacks as function of reference models. LiRAs performance generally dominates RMIAs. LiRA continues to improve 17 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Figure 7 Influence of signal type on MIA Performance. We plot ROC curves that compare the efficacy of using model logits (AUC 0.576) versus model loss (AUC 0.678) as signals for membership inference with LiRA. The results indicate that, in this setting, the loss provides stronger signal for distinguishing members from non-members. as we increase the number of reference models, while RMIAs effectiveness plateaus. For example, with 416 reference models, RMIA surpasses the performance of LiRA (it essentially matches LiRA using 16 reference models). With 4 reference models, LiRA has an AUC of 0.594 (which under-performs RMIAs corresponding AUC of 0.643), but LiRAs AUC increases to 0.678 with 64 reference models (which outperforms RMIAs AUC of 0.658). Also note that RMIA exhibits distinct diagonal pattern at low FPR.2 While both attacks clearly beat the random baseline, neither is remarkably successful in this setting: regardless of the number of reference models, neither achieves an AUC of 0.7. Understanding RMIA. We now further investigate RMIA, decoupling its different components. First, we consider the simplest form of RMIA (simple), eliminating its dependence on â„¤ population and using ð›¼(ð’™) directly as membership signal. We also instantiate LiRA and RMIA with reference population of size â„¤ = 10, 000 and ð›¾ = 1. Figure 10 shows the ROC curves for all three MIAs attacking one target model with 10M parameters trained for 1 epoch on training set size of 219. We use 128 reference models and consider 2219 = 220 target records ð’™ with balanced membership to analyse MIA. We find all three attacks to reach similar ROC AUC values. We also gauge MIA performance by evaluating the TPR at low FPR. To understand the values RMIA reaches for TPR at low FPR, an important subtlety arises from the entropy of the score distribution. Attacks that produce very coarse membership scores inherently limit achievable TPR at very low FPR. 2While RMIA aims to be strong attack that works well in low-compute settings, we find that large population â„¤ is necessary to obtain meaningful TPR at very low FPR thresholds. That is, for minimally acceptable FPRmin, RMIA requires population size â„¤ that is . In practice, this is quite expensive, as RMIAs membership score is computed via pairwise comparisons with these â„¤ reference points (i.e., there are (â„¤) pairwise likelihood ratio tests for target record ð’™, see Appendix A.1). In these initial experiments we only used â„¤ =10,000 examples. We measure performance of RMIA on larger population sizes in Appendix A.3. 1 FPRmin 18 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) LiRA (b) RMIA Figure 8 Comparing LiRA and RMIA. We train 140M-parameter reference models on dataset size of 7M. ROC curves illustrate the effectiveness of (a) LiRA (Carlini et al., 2022a) and (b) RMIA (Zarifzadeh et al., 2024) for different numbers of reference models. As we increase the number of reference models, LiRAs performance (measured with AUC) surpasses RMIAs. Figure 9 Comparing performance of LiRA and RMIA with an increasing number of reference models (c.f. Figure 8). We plot MIA ROC AUC achieved by both attack methodologies for an increasing number of reference models. As the number of reference models increases, LiRAs performance continues to improve, while RMIAs gains saturate, making LiRA the overall stronger attack. For example, as RMIA compares ð›¼(ð’™) to ð›¼(ð’›) for all ð’› â„¤ to compute its membership score Î›RMIA(ð’™), there are maximally â„¤ unique values Î›RMIA(ð’™) can take for all ð’™. This limits the scores entropy and the possibility of achieving meaningful TPR at very low FPR. This explains the diagonal pattern for RMIA in Figure 10, where â„¤ = 10, 000. By contrast, both LiRA and RMIA (simple) provide membership score not limited in entropy, leading to more meaningful values for TPR at lower FPR. We next test further increasing the size of the population â„¤ when computing RMIA. For the same setup, Figure 11 shows how MIA performance varies with the size of â„¤. We observe very similar values for RMIA (simple) and RMIA AUC for all sizes of â„¤ that we test. When examining TPR at low FPR, we find that increasing â„¤ improves the MIA performance at low FPR. Indeed, the increased entropy in Î›RMIA(ð’™) now allows the attack to reach meaningful values of TPR for FPR as low as 106. Notably, for all values of â„¤ we consider, LiRA still outperforms RMIA at low FPR, while the â„¤ likelihood comparisons in RMIA for every target record ð’™ also incur additional computational cost. Finally, we evaluate RMIA under varying thresholds ð›¾. As ð›¾ increases, it becomes less likely that Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Figure 10 Comparing performance of LiRA, RMIA (simple) and RMIA on 10M parameter model trained for 1 epoch with training set size of 219. Figure 11 Performance of RMIA for increasing size of the population â„¤ on 10M parameter model trained for 1 epoch with training set size of 219. ð›¼(ð’™) significantly exceeds ð›¼(ð’›) for many ð‘§ â„¤. Figure 12 shows, again for the same setup, how RMIA performs for varying values of ð›¾, considering both â„¤ = 10, 000 (12a) and â„¤ = 300, 000 (12b). While the MIA AUC remains relatively stable as ð›¾ increases, the TPR at low FPR varies. For â„¤ = 10, 000, the TPR at FPR = 104 decreases for increasing value of ð›¾, reaching 0 for ð›¾ 1.1. This is due to the reduced granularity of RMIAs membership score: for larger ð›¾, fewer ð‘§ satisfy ð›¼(ð‘¥)/ð›¼(ð‘§) ð›¾, constraining the entropy of the RMIA score, making it harder to reach meaningful values of TPR at low FPR. larger reference population (â„¤ = 300,000) mitigates this issue, allowing meaningful TPR even at low FPR for similar ð›¾ values. Taken together, considering multiple sizes of the reference population â„¤ and values of ð›¾, we find LiRA to outperform RMIA when sufficient amount of reference models is available, especially in the low-FPR regime. We therefore adopt LiRA as the primary attack throughout this work. Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) â„¤ = 10, 000 (b) â„¤ = 300, 000 Figure 12 Performance of RMIA for increasing value of ð›¾ on 10M parameter model trained for 1 epoch with training set size of 219. A.4. MIA performance in the offline setting As stated in Section A.1, the literature distinguishes between an online and offline setting for referencemodel based MIAs (Carlini et al., 2022a; Zarifzadeh et al., 2024). In the online setting, the attacker has access to reference models trained on data including (Î¦IN) and excluding (Î¦OUT) the target example ð’™. In the offline setting, the attacker only has access to models not trained on ð’™, thus to Î¦OUT. Throughout this work, we consider the strongest attacker and thus report all results in the online setting. For completion, we here also instantiate MIAs in the offline setting in the same experimental setup as considered above. We adopt the offline versions for both LiRA and RMIA as originally proposed (Carlini et al., 2022a; Zarifzadeh et al., 2024). For LiRA, without Î¦IN we are not able to approximate the probability ð‘IN( ð‘“ðœƒ(ð’™)), and thus just consider the one-sided hypothesis test as membership signal instead of the likelihood ratio: Î›LiRA,offline(ð’™) = 1 ð‘OUT( ð‘“ðœƒ(ð’™)). 21 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models For RMIA, we now compute the denominator in ð›¼(ð’™) by taking the expectation over the reference models that are available to the attacker, or: ð›¼offline(ð’™) = ð‘“ðœƒ(ð’™) ð”¼ðœ™Î¦OUT [ ð‘“ðœ™(ð’™)] . Note that Zarifzadeh et al. (2024) proposes to further adjust the denominator by using variable ð‘Ž (their Appendix B.2.2) to better approximate the ð”¼ðœ™Î¦ [ ð‘“ðœ™(ð’™)] while only using Î¦OUT in the offline setting. We here set ð‘Ž = 1 and just compute the empirical mean across all reference models in Î¦OUT to approximate the expectation in the denominator. We then compute ð›¼offline(ð’›) and use as membership inference signal: Î›RMIA,offline(ð’™) = 1 â„¤ 1 ð’›â„¤ (cid:20) ð›¼offline(ð’™) ð›¼offline(ð’›) (cid:21) . ð›¾ Figure 13 compares the MIA performance between the online and offline setting, for LiRA, RMIA (simple) which does not use the reference population â„¤ and RMIA with ð›¾ = 1 and â„¤ = 300, 000. We again consider the 10M parameter model trained for 1 epoch with training set size of 219 using 128 reference models for the online setting and 64 in the offline setting (on average per target example). We find that, in this configuration and with this number of reference models, offline RMIA outperforms offline LiRA, in terms of both ROC AUC and TPR at low FPR. This suggests that RMIAs offline signal more accurately captures membership information compared to the one-sided hypothesis test used in offline LiRA. In the online setting, in contrast, LiRA and RMIA achieve similar ROC AUC values, with LiRA performing better than RMIA in the low-FPR regime. 22 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) Offline (b) Online Figure 13 MIA performance in the offline and online setting, on 10M parameter model trained for 1 epoch with training set size of 219, considering 128 reference models in the online setting and only the corresponding models Î¦OUT in the offline setting (on average 64 per sample). 23 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models B. More experiments on Chinchilla-optimal models We provide additional details on our experiments involving LiRA attacks on Chinchilla-optimal (Hoffmann et al., 2022) models of different sizes in Section 3.2: 10M, 44M, 85M, 140M, 489M, and 1018M parameters. We provide concrete training hyperparameters in Appendix E. (a) Validation loss over time (1 epoch) (b) LiRA ROC for different learning rate schedules Figure 14 Investigating training dynamics hyperparameters. As sanity check, we plot the loss (a) throughout the single training epoch that we run for our experiments involving Chinchilla-optimal trained models of various sizes. We also test (b) the effect of different learning rate schedules on LiRAs attack success for 140M models using 128 reference models. In Figure 14a, we show the decrease in validation loss over single epoch for these models. The ð‘¥-axis represents the fraction of the training epoch completed (from 0.0 to 1.0), and the ð‘¦-axis shows the corresponding loss. As expected, all models exhibit characteristic decrease in loss as training progresses. Larger models (namely, 489M and 1018M) demonstrate faster convergence to lower loss values, reflecting their increased capacity to fit the training data. They also maintain lower loss throughout the epoch compared to smaller models (10M140M). In the Chinchilla-optimal setting, we also investiInvestigating the role of learning rate schedule. gate the role of hyperparameters on MIA performance. In Figure 14b, we present ROC curves that compare the MIA vulnerability (with LiRA) of 140M-parameter models (trained on approximately 7M records, with 128 reference models), where we vary the learning rate schedule: Linear (AUC 0.676), Cosine (no global norm clipping, AUC 0.660), Cosine (no weight decay, AUC 0.673), and standard Cosine (AUC 0.675). As with all of our ROC plots, the TPR is plotted against the FPR on loglog scale. The AUC values for each curve are relatively close. This indicates that, while there are some minor differences in attack performance, the choice of learning rate schedule among those tested does not lead to drastically different MIA outcomes. 24 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models C. Additional experiments on LiRA limitations Figure 15 The role of duplicates on MIA vulnerability. We observe no significant differences (particularly as FPR increases) between models trained on C4 and de-duplicated C4. Investigating the role of duplicate training examples. Given the relationship between MIA and memorization, and that prior work observes an important relationship between memorization and training-data duplication (Lee et al., 2021), we test the relationship between MIA vulnerability and the presence of duplicates. In Figure 15, we test the Chinchilla-optimally trained 140M model on C4 and de-duplicated version of C4. We de-duplicate C4 according to methodology described in Lee et al. (2021), where we remove sequences that share common prefix of at least some threshold length. This reduced the C4 dataset size from 364,613,570 to 350,475,345 examples. We observe that the presence of duplicates has negligible impact on AUC: it is 0.683 for C4, and 0.680 for de-duplicated C4. In other words, at least in terms of average attack success, the presence of duplicates does not seem to have significant impact. However, further work is needed to assess attack success changes with more stringent de-duplication, since our de-duplication procedure only remove 10M examples from the dataset. In Figure 16, we reduce the training set size from 7M Varying training epochs and dataset size. (Figure 16a) 219 500ð¾ (Figure 16b) on the 140M model and train for 10 (Figure 16a) and 20 epochs (Figure 16b). Figure 16 consists of two subplots, (a) and (b), both showing ROC curves that illustrate how MIA vulnerability changes with an increasing number of training epochs. The goal of these experiments are to investigate if MIA becomes better with more training epochs, and if so, how attack performance improves over epochs as function of training dataset size. Subplot (a): This plot shows MIA performance for model (indicated as 140M trained on approximately 7 million examples across 10 epochs. The AUC increases with more epochs, starting from 0.573 at 1 epoch and reaching 0.797 at 10 epochs. Subplot (b): This plot shows more dramatic increase in MIA vulnerability for 140M model trained on 219 (approximately 524,000) points over 1 to 20 epochs. The AUC starts at 0.604 for 1 epoch, rapidly increases to 0.864 by 2 epochs, 0.944 by 3 epochs, and approaches perfect MIA (AUC close to 1.000) after 13 epochs. 25 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 140M model with training set size of 7ð‘€ examples for 10 epochs. (b) 140M model with training set size of 219 500ð¾ examples for 20 epochs. Figure 16 ROC curves demonstrate that MIA success significantly increases as models are trained for more epochs. (a) 140M model shows AUC rising from 0.573 (1 epoch) to 0.797 (10 epochs). (b) Another 140M model trained on smaller dataset shows rapid escalation in AUC, from 0.604 (1 epoch) to near-perfect inference (AUC = 1) by 13-20 epochs, highlighting that overfitting from prolonged training severely heightens privacy risks. D. Discussion and other experimental results Does memorisation imply strong membership inference attacks? While memorisation is key factor that can make model susceptible to membership inference attacks, it does not automatically guarantee that strong MIAs will always be successful. Memorisation refers to model learning specific details about its training data, rather than just general patterns. When model heavily memorises training samples, it often exhibits distinct behaviours for these samples, which MIA attackers, in principle, can exploit. Indeed, studies have shown that the risk of membership inference is often highest for those samples that are highly memorised. However, the practical success and strength of MIA can also depend on other factors, such as the model architecture, the type of data, the specifics of the attack method, and whether the memorisation leads to clearly distinguishable outputs or behaviours for member versus non-member data. Some models might memorise data in ways that are not easily exploitable by current MIA techniques, or the signals of memorisation might be subtle for well-generalising models, making strong attacks more challenging despite the presence of memorisation. How do we calibrate the FPR in practice? We acknowledge that calibrating the False Positive Rate (FPR) in real-world scenarios is challenging and unresolved issue. The key difficulty lies in getting the necessary cooperation or data from different parties (e.g., model developers, data owners) to establish reliable baseline for what constitutes false positive in practical setting (Zhang et al., 2025a). Full results for Figure 2b and Figure 4b In Figure 17 and Figure 18 we give individual ROC curves for experimental results summarized in Figure 2b and Figure 4b, respectively. For each subplot, each line indicates different target model that we use to perform the attack on. As discussed previously, some larger models appear to have more variance in their ROC curves over different experimental runs. In Figure 18i, we see that although AUC is similar over different target models, there is catastrophic failure against one model at small FPRs. 26 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 10M (b) 85M (c) 302M (d) 489M (e) 604M (f) 1018M Figure 17 Accompanying AUC-ROC curves for Figure 2b over different model sizes. For each subplot, each line indicates different target model that we use to perform the attack on. 27 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 10M (b) 44M (c) 85M (d) 140M (e) 302M (f) 425M (g) 489M (h) 509M (i) 604M (j) 1018M Figure 18 Accompanying AUC-ROC curves for Figure 4b over different model sizes. For each subplot, each line indicates different target model that we use to perform the attack on. 28 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models E. Experiment details In Table 1, we give experimental hyperparameters and details. Unless otherwise stated, we used the AdamW optimizer (Loshchilov and Hutter, 2017) with cosine scheduler. The initial learning rate is set to 107 and increases linearly over 750 warm up steps to peak learning rate of 3 104, after which it decreases according to the cosine schedule to final value of 3 105. We use 128 reference models, and single target model to measure MIA vulnerability over 2 the training set size (the training set is subsampled from dataset twice its size). That is, for each reference and target model, the training set is subsampled from the same, larger dataset. This means each example in this larger dataset falls into the training set of 64 reference models. The batch size is fixed to 128 and sequence length to 1024, if an example has fewer tokens we pad to 1024. The weight decay is set to 0.1, and global clipping norm is set to 1.0. Note that we can approximately convert the training set size to total number of training tokens by multiplying the training set size by 400, as this the approximate average number of tokens within C4 sample. This means, e.g., in Figure 2 the 1018M model was trained on 20.4B tokens. 29 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Table 1 Experimental details Experiment Training set size Model size Other information (which diverges from default experimental settings) Figure 8a Figure 8b 7M 7M 500K 2.2M 4.25M 7M 15.1M 24.4M 30.2M 50.9M 2.2M 1.1M Figure 2 Figure 3a Figure 3b 7M Figure 4a 50K 100K 500K 1M 5M 10M Figure 4b 223 Figure 3 Figure 6 Figure 9 Figure Figure 11 Figure 12 223 7M 7M 500K 500K 500K Figure 7 7M Figure 14b 50K Figure 15 Figure 16a Figure 16b Figure 17 Figure 18 Figure 7M 7M 219 - - - Figure 20 223 Figure 22 - 140M 140M 10M 44M 85M 140M 302M 489M 604M 1018M 44M 44M 140M 140M 140M 140M 140M 140M 140M 10M 44M 85M 140M 302M 425M 489M 509M 604M 1018M 140M 140M 140M 10M 10M 10M 140M 140M 140M 140M 140M - - - - - Max 256 reference models Max 64 reference models, 10K population 10 epochs 80 warm up steps 256 reference models 10K population size 10K-300K population size 10K-300K population size Cosine, Cosine with 0 weight decay, Cosine with no clipping, Linear. We use 50 warm up steps. 10 epochs 20 epochs Identical to Figure 2 where we use 16 different target models Identical to Figure 4b where we use 16 different target models Identical to Figure 5b 10M-302M model sizes Identical to Figure 3 30 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models F. More per-example MIA results Figure 5b indicates that it is often the case that vulnerable sequences tend to be longer. Beyond sequence length, we observe that samples more vulnerable to MIA tend to have higher mean TF-IDF scores (Figure 19a), suggesting that texts with distinctive, uncommon terms leave stronger signals for membership inference. We compute these TF-IDF scores without normalization, collecting document frequency statistics over random subsample of the original dataset, then taking the mean across all tokens in each sample. Similarly, examples containing unknown tokens (<unk>) appear more vulnerable to MIA (Figure 19b). (a) Mean TF-IDF scores across vulnerability categories (b) Unknown token (<unk>) counts across vulnerability categories Figure 19 Text property distributions by MIA vulnerability. The most vulnerable examples tend to have higher TF-IDF scores compared to least vulnerable examples (a), and more likely to contain at least one unknown token (b). 31 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models G. Evolution of losses over different model sizes In Figure 20, we plot the evolution of losses over different model sizes for three examples in the C4 dataset. Each of these models are trained for 1 epoch on 223 8.3M samples. This is sanity check that the losses decreases (on the same sample) as the model size increases. It is also interesting to note that the distance between member and non-member distributions doesnt significantly shift as the model size grows. Example ID: 16777211, Example ID: 16777212, Example ID: 16777213, Member Member Non-member 10M 44M 85M 140M 302M Figure 20 For three different samples (referenced by their ID in the C4 dataset, and if they were member or non-member of training), we plot the loss of the reference distributions and the loss of the sample of the target model (as vertical red line). We plot this over different model sizes (ð‘¦-axis). 32 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models H. Comparing MIA over different number of reference models for all Chinchillaoptimally trained model sizes In Figure 21 we replicate Figure 17, where we vary the number of reference models used in LiRA. Each column represents LiRA which uses different number of total reference models to perform the attack. Unsurprisingly, as more reference models are used the attack becomes better. This mirrors our findings in Figure 8a. The key point of these figures is to show the general pattern of where the ROC curve is relative to the reference line ð‘¦ = ð‘¥, as well as the fact that there is variance (in the insets) across runs. These are (as result) not to be taken as detailed results that should be closely examined. (This is why they are not very large.) Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models (a) 10M (b) 85M (c) 302M (d) 489M (e) 604M (f) 1018M Figure 21 Accompanying AUC-ROC curves for Figure 2b over different model sizes. For each subplot, each line indicates different target model that we use to perform the attack on. Each column represent LiRA which uses different number of total reference models to perform the attack. Unsurprisingly, as more reference models are used the attack becomes better. This mirrors our findings in Figure 8a. Each subplot also records the average AUC of the attack. 34 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models I. Showing variance in sample predictions As noted in Section 5.1, we observe significant degrees of instability in some membership predictions: there is considerable variance in the underlying sample true-positive probabilities. At any particular training step, the true-positive probabilities over batch of samples can vary by more than 15%. In this appendix, we provide some additional figures that dig into this instability. We plot the mean and standard deviation of the per-sample true-positive probabilities, ð‘ƒ(predicted as membermember) for 224 = 16, 777, 216 samples. We compute variance across 64 target models; this experiment trained 128 models on different random splits of the 224 samples. We loop over each model, selecting it as the target model and the remainder as reference models used for LiRA. Since each sample had probability of 0.5 for inclusion in the training set, for each sample, we have on average 64 target models where the sample was in training. (a) Per-sample true positive probability for 224 samples (mean and standard deviation computed over 64 target models). Ordered from smallest to largest. (b) Histogram of average persample true positive probabilities from Figure 22a. (c) Histogram of standard deviation of per-sample true positive probabilities from Figure 22a. Figure 22 Different views of instability in per-example true positive probabilities. We compute the mean and standard deviation for each samples true positive probability (i.e., ð‘ƒ(predicted as membermember)) for 224 samples across 64 target models. (a) shows the mean and variance of these true positive probabilities, where we sort the results by the mean of each examples true positive probability. (b) and (c) together show different view of the same data; the former shows histogram of the mean true positive probabilities for these examples, and the latter shows the histogram of the standard deviation. In Figure 22, we provide three plots that give different views of the same data. Figure 22a plots the true positive probability for each example. We sort examples by the mean value of their true-positive probability (i.e., the mean of ð‘ƒ(predicted as membermember) over 64 target models), and we also show the variance over the 64 target models. Together, Figures 22b and 22c provide an alternate view of Figure 22a. Figure 22b plots the histogram of the mean ð‘ƒ(predicted as membermember) for the 224 samples, each across the 64 target models. The average across these mean true positive probabilities for each example is 0.543. However, note that this is skewed distribution; there are many examples that have their mean ð‘ƒ(predicted as membermember) > 0.6. Figure 22c shows related histogram: the standard deviations for the mean per-example true positive probabilities shown in Figures 22b. On average, the standard deviation for an examples true positive probability is close to 15%, as we note in the figure; it is 0.143. Note that there is large amount of mass on either side of this average. Importantly, there are many examples for which the standard deviation of the true positive probability computed over 64 target models exceeds 0.2. Overall, variance is significant. The individual example true positive probabilities for each target are, 35 Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models when considered together, highly unstable. This variance can help explain why attack ROC AUC is perhaps lower than one might have hoped; there is considerable variance in the underlying example predictions. Altogether, this provides additional nuance concerning the extent of (alternatively, the limits of) attack robustness."
        }
    ],
    "affiliations": [
        "CISPA Helmholtz Center for Information Security",
        "Cornell University",
        "Google DeepMind",
        "Imperial College London",
        "University College London",
        "University of Washington"
    ]
}