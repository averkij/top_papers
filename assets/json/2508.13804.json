{
    "paper_title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding",
    "authors": [
        "Maciej Skorski",
        "Alina Landowska"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How do large language models understand moral dimensions compared to humans? This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluate top language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on 100K+ texts spanning social media, news, and forums. Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\\% of human annotators, achieving much better-than-average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities."
        },
        {
            "title": "Start",
            "content": "Beyond Human Judgment: Bayesian Evaluation of LLMs Moral Values Understanding"
        },
        {
            "title": "Alina Landowska\nSWPS University",
            "content": "5 2 0 2 9 1 ] . [ 1 4 0 8 3 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "How do large language models understand moral dimensions compared to humans? This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluated the best language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from 700 annotators in 100K+ texts spanning social networks, news and forums. Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25% of human annotators, achieving much better than average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities. Keywords: moral reasoning, large language models, uncertainty quantification, natural language processing, soft labels modelling"
        },
        {
            "title": "1.1 Background",
            "content": "Moral Foundations Theory (MFT) provides comprehensive framework for understanding human moral reasoning across cultures, identifying core dimensions typically expressed as virtue/vice pairs: Care vs. Harm, Fairness vs. Cheating, Loyalty vs. Betrayal, Authority vs. Subversion, and Sanctity vs. Degradation (Graham et al., 2013; Haidt, 2012). These foundations shape individual and collective decision-making, from political preferences to social behavior (Feinberg and Willer, 2013; Graham et al., 2009; Roy and Goldwasser, 2021; Nguyen et al., 2022), making their computational detection 1 crucial for understanding discourse dynamics and developing ethically-aligned AI systems."
        },
        {
            "title": "Social Media Post",
            "content": "\"My heart breaks seeing children separated from families at the border\" \"Everyone deserves equal access to healthcare regardless of income\" \"Respect your elders and follow traditional values that built this nation\" \"Stand with our troops - they sacrifice everything for our freedom\" \"Marriage is sacred and should be protected from secular corruption\""
        },
        {
            "title": "Sanctity",
            "content": "Table 1: Posts and Associated Moral Foundation The computational linguistics community has successfully fine-tuned pre-trained language models to predict moral values (Nguyen et al., 2024; Zangari et al., 2025a; Preniqi et al., 2024), achieving good alignment with human judgment when domain similarity and sufficient training data are available. However, systematic evaluation of large language models (LLMs) remains limited despite their rapid advances and potential as compelling alternative that should suffer less from poor generalization and distribution shift. This paper addresses these limitations through rigorous large-scale evaluation of state-of-the-art language models across established moral reasoning corpora, employing Bayesian methods to resolve disagreeing annotations."
        },
        {
            "title": "1.2 Contribution",
            "content": "Bayesian uncertainty modelling of moral annotations We introduce Bayesian modelling of annotator disagreements for moral foundation evaluation, moving beyond deterministic ground-truth assumptions. This captures both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model sensitivity across domains and foundations). Large-scale evaluation. We analyse marketleading large language models with 1M+ queries across 100K+ texts and 250K+ annotations from diverse sources, providing the most comprehensive moral reasoning evaluation to date. Statistical analysis of Type I/II errors. We demonstrate that AI performs comparably to top annotators in balanced accuracy, considerably improving false negatives at the price of slightly increased false positive rates - contrary to fears that AI underpredicts moral values. Novel GPU-optimized implementation of Bayesian labels. We developed TensorFlow framework using sparse operations for scalable Bayesian inference, valuable resource for the computational linguistics community."
        },
        {
            "title": "1.3 Related Work",
            "content": "Dictionary methods for predicting moral values were initially developed (Hopp et al., 2021) and remain widely used in applied studies, but demonstrate poor precision compared to pre-trained language models (Nguyen et al., 2024). Pretrained language models show strong alignment with human moral judgments given sufficient training data (Nguyen et al., 2024; Zangari et al., 2025a; Preniqi et al., 2024), though they suffer from distribution shift and poor cross-domain generalization. This makes large language models compelling alternative. Large language models were recently compared to human performance (Bulla et al., 2025), concluding LLMs superiority. However, their strict majority voting for ground truth excluded nuanced moral content, retaining only less ambiguous cases and departing from the inclusive consensus practices (marked when flagged by at least one annotator) (Nguyen et al., 2024; Zangari et al., 2025a; Preniqi et al., 2024). Additionally, leave-one-out estimation of annotator ranking lacked statistical robustness. Another recent work (Skorski and Landowska, 2025) found LLMs showing imbalance - lower than anticipated recall - under inclusive annotator consensus. Our Bayesian method resolves these problems by modeling annotator disagreement, striking the balance between inclusive and overly exclusive consensus rules, while our large-scale evaluation across multiple datasets and modern models ensures robustness of findings. More on moral foundation theory. For comprehensive coverage of (computational) moral foundation theory challenges, see (Zangari et al., 2025b)."
        },
        {
            "title": "2.1 Datasets",
            "content": "Our robust evaluation utilizes three established and diverse corpora totaling 250K+ annotations of moral values from hundreds of annotators with diverse expertise (from experts to crowd-workers) across 100K+ texts spanning social media, news, and forum discussions. Moral Foundations Twitter Corpus (MFTC) (Hoover et al., 2020): 128,454 annotations from 23 trained annotators across 33,686 tweets from seven discourse domains. Extended Moral Foundations Dictionary (eMFD) (Hopp et al., 2021): 73,001 crowd-sourced annotations from 654 contributors on 54,867 text segments extracted from approximately 8,000 news documents by major outlets including The Washington Post, CNN, Fox News, and Breitbart, selected via GDELT. Moral Foundations Reddit Corpus (MFRC) (Trager et al., 2022): 61,226 annotations from 6 trained coders across 17,885 Reddit comments from 12 subreddits covering politics and everyday moral discourse."
        },
        {
            "title": "2.2 Bayesian Annotation Modelling",
            "content": "As in the related work, we frame the problem as binary prediction tasks for individual moral foundations (abbreviated to care, fairness, authority, loyalty, sanctity). Optionally, we consider the aggregated \"any moral content\" category derived from positive labels across foundations. We note that this aggregated category is inherently unreliable when only positive labels are provided (as in eMFD), since true negatives cannot be distinguished from unlabeled moral content, creating systematic classification ambiguity. Given substantial inter-annotator disagreement shown by PABAK scores in Table 2, we obtain probabilistic ground truth labels using Bayesian aggregation methods (Paun and Simpson, 2021). We model annotator disagreements using Dawid-Skene variant with weak Dirichlet priors to estimate ground truth labels and annotator reliability. For texts, annotators, and categories: Model Specification. True category prevalences follow π Dir(α) where α = (1, 1, ..., 1) pro-"
        },
        {
            "title": "Care\nFairness\nLoyalty\nAuthority\nSanctity\nAny",
            "content": "0.71 0.63 0.62 0.52 0.58 0.34 0.67 0.64 0.82 0.78 0.83 0.38 0.33 0.36 0.38 0.41 0.44 1.00 Table 2: Prevalence-Adjusted Bias-Adjusted Kappa (PABAK) scores measuring inter-annotator agreement across datasets and moral foundations vides uniform priors. Each annotator has confusion matrix Θj with rows θjk Dir(βk), where βk = (2, 0.5, ..., 0.5) encodes weak belief that annotators identify category correctly more often than incorrectly. classification results. The classification prompt used was:"
        },
        {
            "title": "Moral Foundations Theory Classification",
            "content": "You are an expert in moral psychology, classifying text according to Haidts theory. For each moral foundation, mark true if moral values from that foundation are expressed in the text, false if not expressed. Answer only with valid JSON in this format: { } \"care/harm\": [true / false], \"fairness/cheating\": [true / false], \"loyalty/betrayal\": [true / false], \"authority/subversion\": [true / false], \"sanctity/degradation\": [true / false] Experiments were tracked using W&B2 and reInference. For text with true category zi and peated to ensure stability and reproducibility. annotations yi = (yi1, ..., yiJ ):"
        },
        {
            "title": "Appendix A provides details and rationale about",
            "content": "Pr{zi = k} πk (cid:89) j=1 θjk,yij This posterior accounts for varying annotator reliability while estimating both confusion matrices and ground truth labels. Implementation. We developed GPUoptimized implementation in TensorFlow1 using custom graphs and sparse tensor operations for computational efficiency. The framework supports MAP estimation and Monte Carlo sampling with optional fixed effects modeling (Appendix B). Testing on A100 GPUs achieved 1,000 optimization steps per second for 100k annotations, enabling rapid convergence across large-scale datasets."
        },
        {
            "title": "2.3 Large Language Models",
            "content": "Three recent strong large language models were analysed for moral foundations classification: (1) Claude Sonnet 4 (Anthropic, 2025) with 200,000 token context window; (2) DeepSeek-V3 (DeepSeek, 2024) with 671 billion total parameters, 37 billion active parameters, and 128,000 token context window; and (3) Llama 4 Maverick (Meta, 2025) with 402 billion total and 17 billion active parameters, and 256,000 token context window. All models were queried using the temperature setting of 0.30 to balance deterministic responses with modest variability. Each text was analysed individually (no batching of inputs) to ensure focused the prompt selection."
        },
        {
            "title": "2.4 Metrics",
            "content": "We apply standard classification metrics to Bayesian-estimated confusion matrices: BalT +F + anced Accuracy 1 N +F ), Preci2 ( P +F , False Positive Rate +F , Recall sion P +T , and False Negative Rate N +T ."
        },
        {
            "title": "2.5 Supplementary Materials",
            "content": "Complete code, detailed ditional https://github.com/maciejskorski/ moral-foundations-llm-eval data preprocessing scripts, adat: experimental evaluations and available results, are"
        },
        {
            "title": "3 Results",
            "content": "Bayesian models were fitted for each language model and dataset, the model considered one additional annotator each time, and confusion matrices obtained through MAP were used to calculate classification metrics and percentile ranks."
        },
        {
            "title": "3.1 Accuracy Analysis",
            "content": "Balanced Accuracy results appear in Table 3 and Figures 1 to 3, The key insights are: AI Superiority over Humans. AI models consistently outperformed humans across datasets, typically ranking in the top 25% of annotators. Human 1https://www.tensorflow.org/ 2https://wandb.ai/ 3 Model Metric Any Authority Care Fairness Loyalty Sanctity Moral Foundations Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Human Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Acc% Pct Acc% Pct Acc% Pct Avg% Acc% Pct Acc% Pct Acc% Pct Human Avg Acc% Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Human Acc% Pct Acc% Pct Acc% Pct Avg% MFTC Dataset 75 50 77 68 42 72 80 83 82 88 79 79 eMFD Dataset 85 95 93 100 95 100 63 74 81 89 83 92 64 78 75 78 71 76 71 88 97 88 98 89 98 62 MFRC Dataset 74 57 68 14 62 14 75 83 100 83 83 100 69 90 100 90 100 89 100 82 71 81 71 83 71 75 83 91 82 84 88 64 80 86 73 43 72 43 81 79 80 79 80 75 72 72 78 85 83 91 62 86 86 84 86 83 86 90 100 85 92 87 96 67 80 84 75 82 86 63 91 100 82 100 87 100 Table 3: Model performance on moral foundation classification across datasets. Acc% shows balanced accuracy and Pct shows the corresponding percentile. performance averaged 67-76%, while AI achieved 62-95% depending on dataset. Similar Dataset Difficulty. Average AI performance was similar across datasets: MFRC (83.7%), eMFD (81.9%), and MFTC (81.5%), with negligible difference between the highest and lowest performing datasets. Model Strengths. While all models outperformed human annotators, there is no strong winner among them. Claude appears to be slight overall leader, scoring high consistently and achieving first or second-best results most of the time, with particular excellence in nuanced Care and Sanctity foundations. DeepSeek and Llama 4 perform similarly most of the time, but show visible gap of 8-9% behind Claude on MFRC in Sanctity and Fairness. Consistent Accuracy accross Foundations. All moral foundations achieved strong performance across datasets, with overall average accuracy exceeding 80%: Care (85.1%), Sanctity (84.3%), Loyalty (81.6%), Authority (80.9%), and Fairness (80.0%). Particularly high results for \"Sanctity\" are notable given its known difficulty to classify due to cultural contexts. Figure 1: DeepSeek-V3 vs human accuracy (MFTC) Figure 2: Claude Sonnet 4 vs human accuracy (MFRC)"
        },
        {
            "title": "3.2 False Positive vs. False Negative Analysis",
            "content": "Figure 4 and Table 4 compare false positive and false negative rates across models and foundations. The key findings are: AI-Human Error Trade-offs. Across all foundations and datasets, AI models achieve 2 4 lower false negative rates compared to humans (19.4% vs 52.7% on average), at the expense of 4 Moral Dimensions Authority Care Fairness Loyalty Sanctity Model FNR FPR FNR FPR FNR FPR FNR FPR FNR FPR Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Human Baseline Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Human Baseline Claude Sonnet 4 DeepSeek-V3 Llama 4 Maverick Human Baseline 16.5 18.7 14.6 56.5 14.7 24.6 23.9 53.1 33.6 18.2 12.6 56.5 17.2 14.4 20.0 5. 25.3 11.1 18.0 13.4 18.4 19.0 20.7 16.0 MFRC Dataset 5.3 7.3 10.8 42.4 15.2 13.7 11.4 5.3 12.3 36.4 28.1 40. MFTC Dataset 7.2 15.2 19.9 50.3 35.8 28.5 28.2 6.9 9.3 28.6 22.8 43.5 eMFD Dataset 9.1 7.5 7.9 59. 15.2 16.0 14.1 15.2 19.3 25.0 18.2 54.9 27.2 18.0 28.8 9.4 25.8 8.7 11.5 6.2 15.0 11.5 13.4 16.2 8.7 9.6 9.7 52. 20.1 22.9 13.3 46.9 44.9 16.8 19.9 59.4 19.5 21.4 24.8 5.4 18.7 16.7 27.7 8.6 10.6 13.0 14.8 16.4 7.9 31.3 15.2 55. 7.8 26.7 16.1 59.1 36.1 47.8 32.2 61.2 9.5 4.5 10.5 3.7 12.2 3.8 9.2 7.3 4.1 2.4 3.8 13.5 Table 4: False positive and negative rates (%) by model across moral foundations and datasets. Human baseline represents average performance across all annotators. Figure 3: Llama 4 Maverick vs human accuracy (eMFD) slightly higher average false positive rates (16.1% vs 10.1%), overall remaining more balanced (as visually revealed in Figure 4). This reveals opposing strategies: AI provides balanced detection while humans classify more conservatively, systematically under-detect genuine moral signals. Foundation-Specific Patterns. On average, the largest improvement in FNR appears in Care (40.8 point FNR versus humans), followed by Authority (35.6 points), Loyalty (34.5 points) and Sanctity (34.0 points). Fairness remains more challenging with an average improvement of 24.2 points. Human Conservatism Impact. On MFRC and MFTC, annotated by pool of experts, AI appears more balanced (average FNR-FPR AI gap of 1.4pp and 1.6pp, respectively) than in eMFD (gap of 8.7pp) crowd-annotated. This can be explained by the Bayesian method that estimates AI performance by seeking consensus with much higher number of conservative annotators. Figure 4: Error trade-offs in moral foundation detection. AI models (shapes) vs human baselines (circles) across datasets with colors denoting moral foundations. Diagonal lines indicate error balance (FPR = FNR)."
        },
        {
            "title": "4 Case Study",
            "content": "As noted in the annotator training guidelines (Hoover et al., 2020), \"Ambiguities present considerable challenges for human annotators who must strike an acceptable balance between exploiting often weak signals of moral sentiment while also avoiding unfounded speculation about authorial intent\". Our results suggest AI models strike this balance more effectively, demonstrated by consistently higher recall rates. To verify this finding, we presented sample of posts where AI predicted substantially more moral content than human annotators to an independent domain expert, who confirmed that AI was correct in the vast majority of cases. Below we present three examples where 5 no human annotator detected elements of moral languagerepresenting false negatives in human annotation that AI correctly identified. Example: Claude Sonnet 4 Classification User: You are an expert in moral psychology, classifying text according to Haidts theory... Text: Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprisings of 2010-2012. Assistant: { \"care/harm\": true, \"fairness/cheating\": true, \"loyalty/betrayal\": false, \"authority/subversion\": true, \"sanctity/degradation\": false } Reasoning: Care/harm: Concern for harm under dictator, suggests intervention to protect people Fairness: Implies Algerians deserve better; criticizes Frances abandonment Authority: Direct challenge to political authority through regime change Example: Claude Sonnet 4 Classification User: You are an expert in moral psychology, classifying text according to Haidts theory... Text: Victim blaming are we? Even if the therapist is her friend, she shouldve known not to tell her anything. Its immoral and extremely disrespectful, and being friends with his wife is no excuse. Assistant: { \"care/harm\": true, \"fairness/cheating\": true, \"loyalty/betrayal\": true, \"authority/subversion\": true, \"sanctity/degradation\": true } Reasoning: Care/harm: Concern for victims wellbeing and harm from breach of confidentiality Fairness: Views therapists actions as unfair betrayal of professional ethics Loyalty: Frames situation as betrayal of professional duties and patient trust Authority: Challenges therapists abuse of professional authority Sanctity: Treats therapeutic confidentiality as sacred/inviolable Example: Claude Sonnet 4 Classification User: You are an expert in moral psychology, classifying text according to Haidts theory... Text: If you care about this cat so much and it has been WEEK since you have seen her, WHY HAVE YOU NOT CALLED THE POLICE AND REPORTED THE THEFT??? Once you do all that, you need to cut contact with your \"boyfriend\". wouldve had my cat back day of or wouldve filed the theft immediately. This is pushing ESH because of how long youve waited for YOUR CAT. Assistant: { \"care/harm\": true, \"fairness/cheating\": true, \"loyalty/betrayal\": false, \"authority/subversion\": true, \"sanctity/degradation\": false } Reasoning: Care/harm: Strong concern for cats welfare and emotional distress from separation Fairness: Views theft as clear injustice requiring legal consequences Authority: Appeals to police authority to enforce justice and restore order"
        },
        {
            "title": "Conclusion",
            "content": "This work presents the most comprehensive evaluation to date of moral foundation detection capabilities in state-of-the-art large language models, analyzing 1M+ queries across 250K+ annotations from three established corpora. By employing Bayesian annotation modeling, we moved beyond deterministic ground-truth assumptions to explicitly account for annotator disagreement in moral foundation classification. Our key findings demonstrate that AI models consistently outperform human annotators in moral foundation detection, typically ranking in the top 25% across all datasets. Critically, AI models achieve 2 4 lower false negative rates, while accepting slightly higher false positive rates, revealing fundamentally different error strategies. While humans frequently miss genuine moral foundations due to biases, AI models provide more balanced detection of moral content. These findings have important implications for deploying LLMs in moral foundation analysis. The superior recall capabilities make AI models valuable for detecting moral foundations that humans might overlook, though slightly higher false positive rates require careful calibration for specific applications. Our uncertainty-aware evaluation framework provides foundation for more nuanced assessment of AI moral foundation detection that accounts for inherent subjectivity rather than treating classification as deterministic. Future work should focus on improving model calibration for moral foundation detection and extending evaluation to other moral frameworks."
        },
        {
            "title": "Limitations",
            "content": "Fixed Effects Modeling Limitations. Our Bayesian framework supports demographic fixed effects (e.g., annotator background, expertise level) to account for systematic differences, but current datasets provide limited demographic metadata. This somewhat constrains our ability to model known sources of variation in moral foundation detection across annotator groups. Content Moderation Limitations. Content moderation systems present in AI models may introduce systematic bias in similar studies by refuse to process morally relevant content. In our additional experiments, Azure OpenAI rejected approximately 10% of Twitter data citing abusive or sexual language, while one Anthropic model questioned 6 the factual correctness of referring to an \"Algerian dictator\" in our Macron example. Data Availability Constraints. The three corpora span different periods (2016-2022) and textual domains, reflecting constraints of available annotated datasets rather than ideal experimental design. These temporal and domain variations may somewhat limit generalizability of findings. Evaluation Framework Boundaries. Our framework focuses on English-language binary classification within Moral Foundations Theory. This excludes cross-cultural variation in moral foundation expression and ignores gradations of moral intensityboth essential sources of realworld variability. Extending uncertainty-aware evaluation to multilingual, fine-grained moral foundation detection remains critical future work."
        },
        {
            "title": "References",
            "content": "Luana Bulla, Stefano De Giorgis, Misael Mongiovì, and Aldo Gangemi. 2025. Large Language Models meet moral values: comprehensive assessment of moral abilities. Computers in Human Behavior Reports, 17:100609. Matthew Feinberg and Robb Willer. 2013. The moral roots of environmental attitudes. Psychological Science, 24(1):5662. Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean Wojcik, and Peter Ditto. 2013. Moral foundations theory: The pragmatic validity of moral pluralism. Advances in Experimental Social Psychology, 47:55130. Jesse Graham, Jonathan Haidt, and Brian Nosek. 2009. Liberals and conservatives rely on different sets of moral foundations. Journal of personality and social psychology, 96(5):10291046. Jonathan Haidt. 2012. The Righteous Mind: Why Good People Are Divided by Politics and Religion. Vintage. Joe Hoover, Gwenyth Portillo-Wightman, Leigh Yeh, Shreya Havaldar, Aida Mostafazadeh Davani, Ying Lin, Brendan Kennedy, Mohammad Atari, Zahra Kamel, Madelyn Mendlen, Gabriela Moreno, Christina Park, Tingyee E. Chang, Jenna Chin, Christian Leong, Jun Yen Leung, Arineh Mirinjian, and Morteza Dehghani. 2020. Moral Foundations Twitter Corpus: Collection of 35k Tweets Annotated for Moral Sentiment. Social Psychological and Personality Science, 11(8):10571071. Frederic R. Hopp, Jacob T. Fisher, Devin Cornell, Richard Huskey, and René Weber. 2021. The extended Moral Foundations Dictionary (eMFD): Development and applications of crowd-sourced approach to extracting moral intuitions from text. Behavior Research Methods, 53(1):232246. Tuan Dung Nguyen, Ziyu Chen, Nicholas George Carroll, Alasdair Tran, Colin Klein, and Lexing Xie. 2024. Measuring Moral Dimensions in Social Media with Mformer. Proceedings of the International AAAI Conference on Web and Social Media, 18:1134 1147. Tuan Dung Nguyen, Georgina Lyall, Alasdair Tran, Minkyoung Shin, Nicholas Carroll, Colin Klein, and Lexing Xie. 2022. Mapping topics in 100,000 real-life moral dilemmas. In Proceedings of the International AAAI Conference on Web and Social Media, volume 16, pages 699710. Silviu Paun and Edwin Simpson. 2021. Aggregating and Learning from Multiple Annotators. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts, pages 69, online. Association for Computational Linguistics. Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, Charalampos Saitis, and Kyriaki Kalimeri. 2024. MoralBERT: Fine-Tuned Language Model for Capturing Moral Values in Social Discussions. In Proceedings of the 2024 International Conference on Information Technology for Social Good, pages 433442, Bremen Germany. ACM. Shamik Roy and Dan Goldwasser. 2021. Analysis of Nuanced Stances and Sentiment Towards Entities of US Politicians through the Lens of Moral Foundation Theory. In Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, pages 113, Online. Association for Computational Linguistics. Maciej Skorski and Alina Landowska. 2025. The Moral Gap of Large Language Models. Preni Golazizian, Jackson Trager, Alireza S. Ziabari, Aida Mostafazadeh Davani, Farzan KarimiMalekabadi, Ali Omrani, Zhihe Li, Brendan Kennedy, Nils Karl Reimer, Melissa Reyes, Kelsey Cheng, Mellow Wei, Christina Merrifield, Arta Khosravi, Evans Alvarez, and Morteza Dehghani. 2022. The Moral Foundations Reddit Corpus. arXiv preprint. Lorenzo Zangari, Candida M. Greco, Davide Picca, and Andrea Tagarelli. 2025a. ME2-BERT: Are events and emotions what you need for moral foundation prediction? In Proceedings of the 31st International Conference on Computational Linguistics, pages 95169532, Abu Dhabi, UAE. Association for Computational Linguistics. Lorenzo Zangari, Candida Maria Greco, Davide Picca, and Andrea Tagarelli. 2025b. survey on moral foundation theory and pre-trained language models: Current advances and challenges. AI & SOCIETY."
        },
        {
            "title": "A AI Prompt",
            "content": "For estimating moral competence under typical usage conditions, we deliberately avoided prompt 7 where _ \" \" \" h n t by o o on m l _ = . nn . _ t ( _ i ) _ t = . nn . _ t ( t _ i , o o r l x d s ] s = 1) # [ = . math . exp ( _ ) t = . math . exp ( _ t ) _ t = . n s ( _ t , [ 0 , 2 , 1 ] ) _ t = . h ( _ t , s , e a ] # [ e a ] * e ( *K, K) ) # [ o o g _ = . nn . e n _ k _ r ( _ t , b r = sum ) _ g =None , # [ o _ , m r c s ] : ] _ += _ [ None , _ = . u _ s x ( _ , _ = . u _ ( _ ) s =1 ) # [ m ] _ += s _ o . _ b ( ) _ += . u _ ( f o _ o . _ b ( t ) ) u o _ t z = . i e . Adam ( 1 2 , _ r = 2000 ) @tf . c n ( ) t n _ p ( _ i , h . d t e ( ) t _ i , e : n _ ) : s = _ ( _ i , d t = e . d t ( s , e _ i , o _ ) [ _ i , t _ i ] ) i e . l _ d t ( ( d t , [ _ i , t _ i ] ) ) u o h _ i , o _ , _ r = . @tf . c n ( ) f a ( _ i , s t ( 1 ) ) : n ( \" c \" ) i . range ( _ r ) : s = i _ p ( _ i , t _ i , o _ ) # c e c h = . nn . t ( t _ i , p n = . u _ ( . s (K) * 1 . 0 / * . a . m i x = 1) g _ t ( t ) , s =1 ) . numpy ( ) engineering techniques that might bias models toward higher recall or precision. We tested an alternative prompt variant that explicitly requested step-by-step reasoning, without noticing significant differences"
        },
        {
            "title": "Moral Foundations Theory Classification",
            "content": "You are an expert in moral psychology, classifying text according to Haidts theory. For each moral foundation, mark true if moral values from that foundation are expressed in the text, false if not expressed. Answer only with valid JSON in this format: { } \"care/harm\": [true / false], \"fairness/cheating\": [true / false], \"loyalty/betrayal\": [true / false], \"authority/subversion\": [true / false], \"sanctity/degradation\": [true / false], \"reasoning\": [summary of reasoning], Provide step-by-step reasoning."
        },
        {
            "title": "B TensorFlow Implementation",
            "content": "Our Bayesian aggregation uses sparse tensor operations for efficient likelihood computation across annotator-item pairs. The implementation leverages TensorFlows sparse lookup operations to handle high-dimensional confusion matrices, enabling scalable inference on GPU hardware. Full implementation details are available in our code repository. Listing 1: TensorFlow implementation l _ ( _ i , t _ i , o _ ) : \" \" \" l n h g e o o t o r Dawid n m e model , i i o l p a c and o o n i a c . r t : : _ i f . i e , p [K]"
        },
        {
            "title": "L o g i t s",
            "content": "f l p a c s b o ( o h _ i o a r i i ) . i e , :"
        },
        {
            "title": "L o g i t s",
            "content": "f n a s m o l t ) e [ , , ] = ( o o ) s : p [ , , K] f o t e e ( o a s a r n _ t . r e r , p [N , *K] r e r o g o i b v o where : i : u : e d s e d o ( m , o o ) r f e m d l u t s : . s : l Log e o = P ( o i P ( ) + P ( t ) , t ) + g"
        },
        {
            "title": "Combines d a t a l i k e l i h o o d w i t h D i r i c h l e t p r i o r s on p i",
            "content": "and t t a a r a n : P ( o i ) = _ g sum_k _ o _ e _ , _ + P ( ) + _ g ( t _ )"
        }
    ],
    "affiliations": []
}