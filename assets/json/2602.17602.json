{
    "paper_title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
    "authors": [
        "Hojung Jung",
        "Rodrigo Hormazabal",
        "Jaehyeong Jo",
        "Youngrok Park",
        "Kyunggeun Roh",
        "Se-Young Yun",
        "Sehui Han",
        "Dae-Woong Jeong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension."
        },
        {
            "title": "Start",
            "content": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models Hojung Jung1,, Rodrigo Hormazabal2, Jaehyeong Jo1, Youngrok Park1, Kyunggeun Roh3, Se-Young Yun1, Sehui Han2, Dae-Woong Jeong2 1KAIST AI 2LG AI Research 3Seoul National University ghwjd7281@kaist.ac.kr 6 2 0 2 9 1 ] . [ 1 2 0 6 7 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Molecular generation with diffusion models has emerged as promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension. 1. Introduction Molecular generation with AI has the potential to significantly speed up materials design (Sanchez-Lengeling and Aspuru-Guzik, 2018) and drug discovery (Zhang et al., 2025). While this promise has led to many different modeling strategies, generating valid and novel molecules is challenging due to the vast combinatorial search space (Dobson, 2004). Here, the primary challenge is not generating novel structures, but ensuring the structures remain chemically valid and relevant. Even minor atom-level error can produce structure that is chemically impossible or Work done during an internship at LG AI Research. 1 Figure 1. MolHIT achieves SOTA result on MOSES dataset. (Top) Near-perfect validity, outperforming existing graph diffusion models. (Bottom) Pareto-optimal in quality-novelty trade-off. synthetically inaccessible. Consequently, it is necessary to develop generative models that efficiently explore this immense chemical space while generating valid and synthesizable molecules. One common approach is to treat molecules as 1D sequences, most commonly through the SMILES representation (Weininger, 1988). By representing molecular graphs into strings, these models can leverage powerful natural language processing techniques to learn patterns of characters. While this simpler learning objective results in generating valid molecules, they suffer from memorization, often reproducing patterns or common subsequences in the training set. This limited exploration capability creates performance plateau as shown in Fig. 1, where high validity is achieved at the expense of reduced number of new structures. Title Suppressed Due to Excessive Size Figure 2. Overview of MolHIT. (a) Markov chain of Hierarchical Discrete Diffusion Model (HDDM). Clean states (S0) are transited to the mid-level states (S1) and finally to the masked state (S2). (b) Generation process of MolHIT. From the masked prior, atoms are denoised into mid-level states and then to atomic tokens in coarse-to-fine manner. (c) Phase diagram of HDDM showing the transition probability of the forward process. (d) Decoupled atom encoding scheme, separately encoding the aromatic and charged atom types. To overcome the exploration limits of sequence-based approaches, graph generative models (Jo et al., 2022; Liu et al., 2023) treat molecules as interconnected systems of atoms and bonds. Unlike 1D models that often overfit to specific textual patterns, graph-based architectures are designed to internalize the underlying topological principles of chemical structures, allowing them to generalize beyond the training set and discover novel structures. In particular, discrete diffusion models (Austin et al., 2021) have been widely studied for molecular graph generation as they naturally align with the categorical nature of atoms and bonds (Vignac et al., 2022; Xu et al., 2024; Qin et al., 2024; Seo et al., 2025). While these models excel at structural exploration, they are prone to generating invalid or chemically unstable samples compared to well-optimized 1D models. This creates performance gap that raises fundamental research question: Can we leverage the inductive biases of graph modeling to match the validity of sequence models while maintaining their superior capacity for structural novelty? We identify two critical limitations in existing molecular graph generation with discrete diffusion. (1) First, current uniform or absorbing transition treats each atom category as an independent category, even though there is well known chemical relationship that some atoms are easier to be replaced with another. Neglecting well-established domain priors often makes the learning unnecessarily hard, especially in molecular settings where high-quality molecule data is scarce. (2) Second, existing graph models rely on naive atom encodings, ignoring the fact that single atom can have different characteristics when it has formal charge or consists of ring (aromaticity). We reveal that this makes molecular graph generation tasks ill-posed and unnecessarily challenging, which we demonstrate in the reconstruction experiments in Fig. 3 where previous atom encoding fails. In light of these observations, we introduce MolHIT, hierarchical discrete diffusion framework designed to bridge the gap between structural innovation and chemical validity. Our framework is built upon the Hierarchical Discrete Diffusion Model (HDDM), where additional categories are added to represent natural chemical groups into the diffusion process. This coarse-to-fine approach allows the model to establish high-level chemical identities before refining them into specific atom types, thereby capturing the meaningful dependencies of molecular structure that uniform or absorbing kernels often overlook. Furthermore, we introduce Decoupled Atom Encoding (DAE) to resolve the information loss found in naive representations by explicitly split atoms based on their specific chemical roles, such as formal charge and aromaticity. By providing chemical role into each token, DAE not only resolves the reconstruction problem in original atom encoding, but also reduces the burden of differentiating atom roles solely with the (O(n2)) bond features. Combined together, MolHIT reaches new Pareto frontier in generating novel structures with high quality, surpassing both existing 1D and 2D models  (Fig. 1)  . We extensively evaluate MolHIT with experiments on large molecular benchmarks, including unconditional generation tasks on MOSES (Polykovskiy et al., 2020) and GuacaMol (Brown et al., 2019) benchmarks and conditional generation tasks, including scaffold extension and multi2 Title Suppressed Due to Excessive Size Figure 3. Existing atom encoding for molecular graph is ill-posed. (Left) Reconstruction success rate on the Moses dataset with previous encoding and our decoupled atom encoding. (Right) Proportion of generated molecules containing pyrrolic nitrogen [nH]. property guided generation tasks. Across all benchmarks and tasks, MolHIT shows significant improvements over previous graph diffusion models, resulting in new state-ofthe-art that surpass 1D models. Our contributions can be summarized as follows: We introduce MolHIT, molecular graph diffusion model built upon novel Hierarchical Discrete Diffusion Model (HDDM) framework with mathematically guaranteed ELBO. We identify critical limitation in the prior graph generative models atom encoding and propose simple solution: Decoupled Atom Encoding (DAE). By representing atoms based on their specific chemical roles, we find DAE enhances both the models generative expressiveness and chemical reliability. We achieve the SOTA performance on the MOSES benchmarks in multiple metrics, significantly outperforming both existing graph diffusion models and 1D sequence-level baselines. We test our algorithm on practical downstream tasks including multi-property guided generation and scaffold extension, achieving the highest performance compared to the previous graph diffusion approach. 2. Preliminaries 2.1. Discrete Diffusion Models Given discrete state space with categories, discrete diffusion models define noising and denoising process within discrete space. Specifically, for S, the noising process is described by Markov chain as follows: q(xtxt1) = Cat(xt; xt1Qt). (1) Here, marginal probability of xt in timestep t, given clean data x0 can be calculated with q(xtx0) = Cat(xt; x0 Qt), Qt = QtQt1 Q1. (2) As shown by Austin et al. (2021), one can design multiple types of diffusion process, where two types of processes are widely used because of the closed-form calculation of the forward process and natural noising process. Uniform transition Uniform transition assumes uniform prior pT (xT = c) = 1 for all {1, . . . , K}. Then one could define forward noising process by interpolating clean data x0 with the prior in the following way: q(xtx0) = Cat(xt; (1 αt)"
        },
        {
            "title": "1\nK",
            "content": "11T + αtx0), (3) where αt is monotonic decreasing function with α0] = 1, αT = 0, which we call diffusion scheduler. Marginal transition To facilitate the diffusion learning, marginal transition assumes data prior π to be an optimal probability distribution that approximates the empirical data distribution from the training set. This has been primarily adopted for graph diffusion models DiGress (Vignac et al., 2022; Siraudin et al., 2024), where further details are in Appendix B.2. Absorbing transition Unlike uniform and marginal transition where diffusion process operates on the given categories, one can introduce an additional masked (absorbing) state with prior em being one-hot vector of the masked state. Then, one can naturally define diffusion process as an absorbing process in Markov chain, which results in the following forward form: q(xtx0) = Cat(xt; αtx0 + (1 αt)em). (4) xtQ ts x0 Qt1 x0 QtxT ), one Given q(xt1xt, x0) = Cat(xt1; could estimate posterior pθ(xt1xt, x0) by learning to estimate the clean data ˆx0 given the noisy data xt. This enables training the diffusion models with simple cross-entropy loss, where the loss function becomes directly linked to the negative evidence lower bound (NELBO) (Austin et al., 2021; Sahoo et al., 2024). 2.2. Molecular Graph Generation with Discrete Diffusion Given molecular graph = (X, E), denote RndX , RnndE for the atom matrix and adjacency matrix (bond matrix) where is the number of atoms and dX , dE are feature dimensions of atoms and edges. The forward process of discrete diffusion operates independently on the atom and bond matrices: Gt = (Xt, Et) : Xt = X0 QX,t, Et = E0 QE,t. (5) where, we define QX,t = QX,t QX,1, QE,t = QE,t QE,1 are forward transition matrix usually calculated in closed form for efficiency. Title Suppressed Due to Excessive Size Given noisy graph Gt, neural network is trained to estimate clean graph G0 = (X0, E0) through predicting clean atoms and bonds independently, which in practice results in the following cross-entropy (CE) loss: Lθ = Et, Gtq(G0) (cid:34) (cid:88) i=1 log pX θ (X0,i Gt, t) log pE θ (E0,ij Gt, t) (cid:35) , (cid:88) + λ 1i<jn (6) where λ > 0 is weighting factor that balances the relative contribution of node and edge loss. 3. MolHIT Framework 3.1. Hierarchical Discrete Diffusion Models We introduce Hierarchical Discrete Diffusion Models (HDDM), which generalize the discrete diffusion framework into multi-stage setting. Unlike standard discrete diffusion (Austin et al., 2021), where the forward transitions operate either within the clean vocabulary space or toward an absorbing (masked) state, HDDM introduces additional mid-level states that bridge the corruption process. To design discrete diffusion in this augmented space, we first show that there exists simple forward process that admits tractable closed-form transition kernel. Specifically, for clean state space S0 with categories, suppose we add additional + 1 categories such that we have an augmented state space with cardinality = K+G+1. As illustrated in Figure 2-(a), we partition into three disjoint subsets: S0, mid-level states S1 with categories, and the masked state S2 = {m}. Now, we define the transition kernel via row-stochastic matrix Φ [0, 1]KG, where Φij represents the probability of mapping any element S0 to mid-level element S1. This operator induces transition matrix Q(1) [0, 1]DD on the full space , structured as block matrix relative to the partition (S0, S1, S2): Q(1) = 0KK 0GK 01K 01G Φ IG 0K1 0G1 1 Lemma 3.1. Define diffusion schedules αt, βt to be monotonically decreasing functions satisfying the boundary conditions α0 = β0 = 1 and α1 = β1 = 0, such that αt βt for all t. We define the forward diffusion process of the hierarchical Markov chain via the transition kernel Qts from timestep to as: Qts = αtsI+(βts αts)Q(1) +(1βts)Q(2), (7) where αts := αt/αs, βts := βt/βs. Then, the transition kernels satisfy the ChapmanKolmogorov equation, such that QtsQsr = Qtr for any < < t. Consequently, the cumulative forward transition from the initial state to timestep is given by: Qt = αtI + (βt αt)Q(1) + (1 βt)Q(2), (8) where denotes the identity matrix in . Note that the above forward transition operators can be naturally extended to arbitrary hierarchies in state space. We provide proof of the above lemma with generalized forward process in Appendix C.1. Now for training guarantee, one can derive negative ELBO (NELBO), which we prove in Theorem C.2 in Appendix. In practice, one can define Φ as deterministic projection that clusters clean atom categories into meaningful groups. We show in this special case, NELBO of HDDM can be further simplified as in the following. Theorem 3.2. If the forward transition kernels Qt in Eq. 8 is induced from the deterministic projection Φ, the continuous time NELBO of HDDM is given as: NELBO(θ) = αt(β EQ,t α t) βt αt log xθ, [zt S1] + logQ(1)xθ, Q(1)x) [zt = m] (9) β t(βt αt) βt(1 βt) αtβ βt(1 βt) + log xθ, [zt = m] + C, Here, IG is the identity matrix, indicating that states in S1 are absorbing the clean states in S0 under Q(1). Similarly, we define the masking operation via the transition matrix Q(2), which maps all states in S0 S1 to the unique absorbing state in S2: Q(2) = (cid:20)0(K+G)(K+G) 1(K+G)1 (cid:21) 01(K+G) 1 These transition matrices form the basis of the HDDM forward process as in the following lemma: for some constant and the denoiser xθ(zt, t). We provide proof in Appendix C.2. For sanity check, one can observe that Eq. 9 reduces to the NELBO of the original masked diffusion models when βt = αt (i.e, no S1). With Theorem 3.2, we can design simple cross-entropy loss for HDDM training in principled way. We empirically find that regularization loss in Eq. 9 does not improve the performance, so we take the original loss in Eq. 6. 4 Title Suppressed Due to Excessive Size 3.2. Decoupled Atom Encoding Existing graph diffusion frameworks (Vignac et al., 2022; Xu et al., 2024; Qin et al., 2024) typically rely on coarse atom encoding scheme, where node identities are determined solely by their atomic numbers. While this simplifies the encoding, we identify that this one-to-many mapping between atomic tokens and their physical states (e.g., protonation or aromaticity) causes the generative task to be ill-posed. As illustrated in Fig. 3 (Left), this leads to systematic reconstruction failure in molecules requiring finegrained atomic descriptors, such as specific nitrogen motifs found in drug-like scaffolds. Consequently, models using these coarse encodings suffer from representational bias, struggling to generate essential motifs that are statistically prevalent in the training distribution (Fig. 3, Right). To resolve these representational gaps and ensure the model can generalize across diverse chemical spaces, we introduce Decoupled Atom Encoding (DAE). DAE expands atomic state space by explicitly encoding aromaticity and formal charge as primary node attributes. This results in nearperfect reconstruction ratio both on the MOSES and GuacaMol dataset. Furthermore, by providing the model with necessary structural priors, MolHIT successfully recovers the distribution of complex motifs such as pyrrolic nitrogen ([nH]), which baselines using coarse encoding struggle to capture (Fig. 3, Right). Further details are in Appendix D.1. 3.3. Forward and Reverse Process of MolHIT Forward process of MolHIT Since the atom and bond are perturbed independently throughout the forward process, we decouple their transition dynamics in graph diffusion. This flexibility is particularly advantageous for molecular graph modeling. We empirically observe that uniform transition kernel is essential for edge generation, whereas HDDM yields superior performance for atom types compared to uniform approach. Therefore, we employ an HDDM process for atoms and uniform transition for edges, resulting in the following forward process dynamics: QX,t = αX,tI + (βX,t αX,t)Q(1) QE,t = αE,tI + (1 αE,t)1dE 1T , dE X,t + (1 βX,t)Q(2) X,t, (10) Our preliminary experiments show robustness on the HDDM scheduler αX,t, βX,t, and therefore we simply opt for linear schedule for αX,t = αE,t = 1 and βX,t = 1 t2 for the experiments. Grouping strategy Given the mid-level states S1, HDDM allows for the design of arbitrary transition kernels from S0 to S1. We implement deterministic grouping kernel that clusters atom elements based on their intrinsic chemical Algorithm 1 PN-sampler with temperature sampling 1: Input: Sample size S, Timesteps , Temperature τ ,"
        },
        {
            "title": "Nucleus threshold p",
            "content": "for = down to with step do 2: for = 1 to do Sample Ptrain(N ) 3: 4: GT pT (GT ) {G = (X, E)} 5: 6: 7: 8: 9: 10: 11: 12: 13: end for end for Store Gfinal ˆp0(X), ˆp0(E) fθ(Gt, t) ˆp 0(X) TopP(Softmax(ˆp0(X)/τ ), p) ˆX0 Categorical(ˆp 0(X)) ˆE0 Categorical(ˆp0(E)) Gtt q(Gtt ˆG0) where ˆG0 = ( ˆX0, ˆE0) properties and aromaticity. For instance, in the MOSES dataset, we partition 12 atom types into four semantic groups: {C}, {N, O, S}, {F, Cl, Br}, and {c, o, n, nH, s}. This hierarchical structure simplifies the initial stages of diffusion by focusing on broad chemical categories before refining specific identities. We extend this strategy to other datasets, such as GuacaMol, by adapting the groupings to their respective atom vocabularies. Full details of these partitions are provided in Appendix D.2 Project and Noise (PN-sampler) Due to the standard ELBO guarantee as we prove in Theorem 3.2, one can sample from the original posterior update as in prior works (Austin et al., 2021). While standard posterior updates follow the transition q(GttGt, G0) as justified by the ELBO guarantee in Theorem 3.2, we empirically find that this approach often restricts the structural exploration necessary for complex molecular generation. To address this, we design Project-and-Noise (PN) sampler. PN sampler projects models denoising prediction pθ(G0Gt) onto the clean manifold (one-hot vector) via categorical sampling to obtain discrete candidate ˆG0. This candidate is then directly re-noised to the preceding timestep = using the cumulative transition kernel Qt, effectively bypassing posterior constraints of Gt to encourage greater diversity in the generated graph. The overall algorithm is illustrated in Alg. 1. Temperature sampling While temperature and top-p sampling have become standard techniques for managing the quality-diversity trade-off in generative domains (Holtzman et al., 2019; Ficler and Goldberg, 2017; Hashimoto et al., 2019), their application to molecular graph generation remains largely unexplored. We evaluate the impact of these sampling strategies and demonstrate that our PNsampler effectively controls this trade-off. We empirically 5 Title Suppressed Due to Excessive Size Table 1. Comprehensive MOSES benchmark results. Scaffold Novelty (Scaf-Novel) measures the ratio of novel scaffold molecules to the number of generated molecules, while Scaffold Retrieval (Scaf-Ret.) quantifies test scaffold retrievals. All of the results are the averaged value over 3 runs of 25,000 samples. Bold denotes the best in each category, and underline indicates SOTA performance within the 2D Graph models. Empty values are due to the absence of publicly available checkpoints or samples. Category Model Quality Scaf-Novel Scaf-Ret. Valid Unique Novel Filters FCD SNN Scaf - Training set 1D Sequence VAE[25] 2D Graph CharRNN [42] SAFE-GPT [33] GenMol [27] DiGress [47] DisCo [51] Cometh [45] DeFoG [38] MolHIT 95.4 92.8 92.6 92.8 62.1 82.5 - 82.1 88. 94.2 0.22 0.29 0.12 0.05 0.26 - 0.36 0.26 0.39 0.031 0.035 0.015 0.012 0.031 - 0.023 0.031 0.033 100.0 100.0 97.7 97.5 99.8 99. 87.1 88.3 87.2 92.8 99.1 99.7 99.9 98.9 64.0 100.0 100.0 100.0 99.9 99.8 69.5 84.2 43.7 68.9 94.2 97.7 96.4 92.1 91.4 100.0 99.7 99.4 97.7 98.1 97.5 95.6 97.3 98. 98.0 0.48 0.57 0.52 0.72 16.4 1.25 1.44 1.44 1.95 1.03 0. 0.58 0.56 0.57 0.64 0.53 0.50 0.51 0.55 0.55 - 5.9 11.0 6.3 1.6 12.8 15.1 16.8 14. 14.4 find that temperature sampling can be naturally adopted for PN-sampler, where doing temperature sampling only for the atom prediction (line 7 in Alg. 1) results in the best performance. Table 2. Full GuacaMol benchmark results using unfiltered dataset. Metric abbreviations: Val. (Validity), V.U. (Unique), V.U.N. (Novel). DiGress (org.) is original DiGress trained with filtered dataset and DiGress (full) values are from the re-implementation of DiGress on unfiltered, full GuacaMol dataset. 3.4. Conditional Modeling To enable conditional modeling, we train conditional model by modifying the original graph transformer architecture in DiGress (Vignac et al., 2022) by adding adaptive layer normalization (adaLN) for node attention only. For sampling, we adopt classifier-free guidance (CFG) (Ho and Salimans, 2022). We provide the details in Appendix D.8. 4. Experiments We evaluate MolHIT on two large-scale molecular datasets: MOSES (Polykovskiy et al., 2020) and Guacamol (Brown et al., 2019). The MOSES dataset consists of 1.9M molecules containing 7 heavy atom types, which we augment into 12 tokens using DAE (Sec. 3.2). Similarly, the GuacaMol (Brown et al., 2019) dataset, which originally contains 12 heavy atom types, is decoupled into 56 tokens via DAE. For the model architecture, we utilize the original graph transformer from DiGress (Vignac et al., 2022), maintaining the same model size. All reported results represent the average of three independent runs, and standard deviations are provided in Appendix D.3. 4.1. Unconditional Generation on MOSES Evaluation Following previous graph diffusion works (Vignac et al., 2022; Qin et al., 2024), we measure with official benchmarks for Moses (Polykovskiy et al., 2020) which includes 7 metrics: Validity (%), Uniqueness (%), Novelty (%), Filters (%), FCD, SNN, Scaf. We also measure Quality (Lee et al., 2025), which is defined by the proportion of molecules that are valid, unique, synthetic accessibility (SA (Bickerton et al., 2012) 4), and drug-like (QED (Ertl and Schuffenhauer, 2009) 0.6). Formal definitions of the Model Val. V.U. V.U.N. KL Div. FCD Training set 100.0 100.0 DiGress [47] (org.) DiGress [47] (full) DiGress + DAE MolHIT (Ours) 85.2 74.7 65.2 87.1 85.2 74.6 65.2 87.1 85.1 74.0 64.9 86.0 99.9 92.9 92.4 87.0 96.7 92. 68.0 61.1 49.2 54.9 metrics are provided in Appendix D.5. Scaffold novelty metrics While the standard MOSES benchmark provides foundation for evaluating molecular generative models, simple metrics like novelty may not reflect the capability for generating new molecules. For instance, high novelty score itself can come from merely generating novel-looking noise outside the manifold of drug-like molecules, while high uniqueness may not reflect true structural diversity if the model is trapped in narrow chemical subspace. To address this, we introduce two metrics given ntotal generated molecules: (1) Scaffold Novelty = Sgen Strain/ntotal, which quantifies the efficiency of structural extrapolation; and (2) Scaffold Retrieval = Sgen Stest/ntotal, which measures distributional fidelity. Further details are in Appendix D.6. Baselines For graph generative models, we compare with DiGress (Vignac et al., 2022), DisCo (Xu et al., 2024), Cometh (Siraudin et al., 2024), DeFoG (Qin et al., 2024), which are previous SOTA in atom-level graph diffusion. We also compare with 1D baselines; VAE (Kingma and Welling, 2013), Char-RNN (Segler et al., 2018), SAFE-GPT (Noutahi et al., 2024), GenMol (Lee et al., 2025). Result As shown in Table 1, MolHIT significantly outperforms previous graph-based baselines across nearly all 6 Title Suppressed Due to Excessive Size Table 3. Multi-property guided generation on MOSES with four different conditions. We report mean absolute error (MAE), Pearson correlation (Pearson r), and validity. Avg. denotes the macro-average across four properties. Bold denotes best performances. All results are the averaged value over 3 runs of 10,000 samples. Method Marginal Marginal + DAE MolHIT (Ours) QED 0.117 0.107 0.061 SA 0.115 0.094 0. MAE logP 0.067 0.061 0.049 Pearson Validity (%) MW 0.272 0.227 0.081 Avg. 0.143 0.122 0. QED 0.489 0.565 0.804 SA 0.570 0.559 0. logP 0.802 0.836 0.950 MW 0.396 0.437 0. Avg. 0.564 0.599 0.807 75.03 87.85 96.31 Table 4. Scaffold extension results on the MOSES dataset. Results are averaged over 3 runs of 10,000 targets. Table 5. Incremental performance gains on the MOSES dataset by integrating DAE, the PN Sampler, and HDDM into the DiGress. Model Validity (%) Diversity Hit@1 Hit@5 Method Quality FCD Validity (%) DiGress Marginal + DAE MolHIT (Ours) 50.8 64.8 83.9 44.8 58.0 57.4 2.07 1.67 3.92 6.41 6.37 9. DiGress [47] + DAE + PN Sampler + HDDM (MolHIT) 82.5 87.6 92.9 94.2 1.25 0.89 1.65 1.03 87.1 96.2 99.4 99.1 key metrics, including Quality, Validity, FCD, and Scaffold Novelty. While 1D sequence-based models (SAFE-GPT, GenMol) excel in Validity, they exhibit clear tendency toward memorization, evidenced by their lower Scaf-Novelty and novelty scores. On the other hand, MolHIT achieves new state-of-the-art both for Quality (94.2%) and Scaffold Novelty (0.39) while achieving near perfect validity score (99.1). The above results validate that MolHIT effectively navigates the valid drug-like manifold without sacrificing its ability to explore novel chemical space. 4.2. Unconditional Generation on GuacaMol Setup Compared to MOSES where molecules contain charged atoms are filtered, GuacaMol benchmark (Brown et al., 2019) contains broader chemical space, including compounds with formal charges that are not eliminated by neutralization. Previous atom encoding (Vignac et al., 2022) fails to reconstruct these properties  (Fig. 5)  , and they train model only with manually filtered dataset which are failed to be reconstructed. This helps improve the validity measure, but making models learn from the imperfect, biased distribution. In contrast, we utilize the full GuacaMol dataset for training to evaluate the robustness of our model. We run 3 run of generating 10,000 samples for each experiment. 4.3. Multi-property guided generation Generating molecules with targeted chemical properties is important for practical applications in materials science and drug discovery. For this, we evaluate the capacity of MolHIT under the multi-conditional generation scenario. Setup We train conditional graph transformer (Sec. 3.4) on the MOSES dataset, labeled with four key chemical properties: Quantitative Estimate of Drug-likeness (QED), Synthetic Accessibility (SA), Molecular Weight (MW), and the lipophilicity (logP). We utilize RDKit (Landrum, 2006), an open-source cheminformatics toolkit, for all property labeling and condition evaluation. Evaluation For inference, we generate 10,000 samples conditioned on target properties of the molecules that are randomly sampled from the test split. We measure Mean Absolute Error (MAE) and the Pearson correlation coefficient (r) for conditioning and validity for the structural fidelity of the samples. We compare MolHIT against two baselines: (1) Marginal transition (effectively DiGress without geometric prior) and (2) Marginal transition with DAE (incorporating decoupled atom encoding into the marginal transition baseline). Results Table 2 shows that MolHIT achieves the highest performance among all metrics except FCD. For FCD, the strong performance of original DiGress without DAE indicates that using DAE does not always lead to the generative task being easier since it can be hard to model with differentiate extended atom vocabulary. However, as in Appendix D.1, we find that using DAE substantially increase the amount of molecules having charged or special atoms, which is not rare in the GuacaMol. Note that the original DiGress is trained for 1,000 epochs, while our results are from training only with 40 epochs, so further training will improve the metrics. Results Table 3 shows that MolHIT significantly outperforms all baselines across every metric. For conditioning precision, MolHIT achieves macro-averaged MAE of 0.058, 52.4% reduction compared to the Marginal+DAE baseline. MolHIT also exhibits high reliability, reaching Pearson of 0.807 on average, including near-perfect 0.950 for log and 0.804 for QED. The results also show this improved conditioning does not come with the cost of lower validity, where MolHIT achieves validity higher than 95%, outperforming baselines with large gap. We provide more experimental details of the multi-property guided generation in Appendix D.8. 7 Title Suppressed Due to Excessive Size 1.0 down to 0.8 consistently improves the quality and validity of generated structures, while further reducing the threshold leads to sharp decline in both chemical metrics and structural diversity. Notably, when sampling with top-p, MolHIT achieves high validity of 99.4% and quality score of 95.1%, demonstrating the effectiveness of the nucleus sampling in MolHIT. 5. Related Works Discrete diffusion models Along with the success of continuous diffusion models (Ho et al., 2020; Song et al., 2020), discrete diffusion models formulate noise process within discrete state space. Hoogeboom et al. (2021) investigate uniform transition of the discrete diffusion models, while D3PM (Austin et al., 2021) explore different types of transition mechanism which include absorbing transition. Recently and independently developed alongside our work, Zhou et al. (2025) propose hierarchical discrete diffusion approach to language modeling. While similar in spirit, our HDDM is derived from semigroup-consistent family of closed-form transition kernels Q(1), Q(2) parameterized by explicit diffusion scheduler αt, βt while Zhou et al. (2025) is developed in the CTMC framework (Campbell et al., 2022). Moreover, HDDM supports an arbitrary row-stochastic projection Φ, which generalizes the deterministic hierarchical mapping used in Zhou et al. (2025). Diffusion models for molecular generation Various diffusion models and its techniques have been applied for molecular graph generation. GDSS (Jo et al., 2022) formulate continuous diffusion modeling through the system of SDE with score matching objective. DiGress (Vignac et al., 2022) utilize primary form of discrete diffusion models with uniform-style transition with data dependent prior. Siraudin et al. (2024); Xu et al. (2024); Qin et al. (2024) apply CTMC framework as in Campbell et al. (2022) to further boost the performance. Another axis for molecular generation is to model 1D sequence. SAFE-GPT (Noutahi et al., 2024) trains an Autoregressive model with their unique representation of molecule while GenMol (Lee et al., 2025) adopts masked diffusion framework in wide range of drug discovery tasks. We defer further related works in Appendix B.1. 6. Conclusion In this work, we present MolHIT, novel molecular diffusion model with hierarchical discrete diffusion framework. Our algorithm results in state-of-the-art performance in large molecular datasets. It unlocks new capacity for end-to-end atom-level molecular generation, directly generating atoms with formal charges or explicit nH for the first time, moving us towards more realistic molecule generation. Figure 4. Effect of top-p sampling in MolHIT. 4.4. Scaffold Extension Setup We evaluate pretrained unconditional models generative capability when conditioned on given substructure. For this, we use Bemis-Murcko scaffold (Bemis and Murcko, 1996) of the molecules in the test split and treat fix this part during the diffusion sampling. For each experiment, we utilize 10,000 unique target scaffolds, generating multiple candidates per target to assess the models distributional coverage. Specifically, we measure the Hit@1 and Hit@5 ratios, which are the probability that the ground-truth extension is recovered within the top samples along with standard metrics of validity and diversity. Further experimental setup is provided in Appendix D.9. Result Table 4 shows that MolHIT significantly outperforming DiGress in all metrics. Interestingly, applying DAE to DiGress improves validity and diversity while reducing in Hit@1, which may due to the extended expressivity of the model. However, DAE results in higher diversity, which results in matched Hit@5 ratio for original DiGress. 4.5. Ablation Studies Component analysis To show the contribution of each component on MolHITs performance, we conduct an ablation study by testing on the Moses dataset. The result in Table 5 shows that our atom encoding method (DAE), sampler (PN sampler), and diffusion algorithm (HDDM) all contributes to get to the highest value of Quality, FCD, and Validity among graph diffusion models. Effect of temperature sampling In Fig. 4, we analyze MolHIT trained on the MOSES dataset across range of topp values. Our results demonstrate that as top-p decreases, clear trade-off emerges between sample quality and scaffold novelty. Specifically, lowering the top-p value from 8 Title Suppressed Due to Excessive Size"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of molecule generation. We hope our work accelerates the discovery of useful drugs and materials, improving human lives. However, one might maliciously use our model to generate harmful substances to humans and environments."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 2, 3, 4, 5, 8 Guy Bemis and Mark Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal chemistry, 39(15):28872893, 1996. 8, 21 Richard Bickerton, Gaia Paolini, Jeremy Besnard, Sorel Muresan, and Andrew Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):9098, 2012. 6 Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, et al. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745, 2025. 13 Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):10961108, 2019. 2, 6, 7, 18 Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. 8, Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 12 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. 12 Christopher Dobson. Chemical space and biology. Nature, 432(7019), 2004. 1 Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019. Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):8, 2009. 6, 22 Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. arXiv preprint arXiv:1707.02633, 2017. 5 Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Wilson. Protein design with guided discrete diffusion. Advances in neural information processing systems, 36:1248912517, 2023. 12 Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. arXiv preprint arXiv:1904.02792, 2019. 5 Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6, 22 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 8, Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. 5 Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. 8 Emiel Hoogeboom, Vıctor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 88678887. PMLR, 2022. 12 John Irwin and Brian Shoichet. Zinca free database of commercially available compounds for virtual screening. Journal of chemical information and modeling, 45(1): 177182, 2005. 18 Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 23232332. PMLR, 2018. 12 Title Suppressed Due to Excessive Size Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International conference on machine learning, pages 1036210383. PMLR, 2022. 2, 8 Hojung Jung, Youngrok Park, Laura Schmid, Jaehyeong Jo, Dongkyu Lee, Bongsang Kim, Se-Young Yun, and Jinwoo Shin. Conditional synthesis of 3d molecules with time correction sampler. Advances in Neural Information Processing Systems, 37:7591475941, 2024. 12 Seo Hyun Kim, Sunwoo Hong, Hojung Jung, Youngrok Park, and Se-Young Yun. Klass: Kl-guided fast inference in masked diffusion models. arXiv preprint arXiv:2511.05664, 2025. 12 Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 6, 19 Greg Landrum. RDKit: Open-source cheminformatics. ht tps://www.rdkit.org, 2006. 7 Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, and Arash Vahdat. Genmol: drug discovery generalist with discrete diffusion. arXiv preprint arXiv:2501.06158, 2025. 6, 8, 19, 20 Chengyi Liu, Wenqi Fan, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, and Qing Li. Generative diffusion models on graphs: Methods and applications. arXiv preprint arXiv:2302.02591, 2023. 2 Gang Liu, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph diffusion transformers for multi-conditional molecular generation. Advances in Neural Information Processing Systems, 37:80658092, 2024. 13, 22 Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 12 David Mendez, Anna Gaulton, Patrıcia Bento, Jon Chambers, Marleen De Veij, Eloy Felix, Marıa Paula Magarinos, Juan Mosquera, Prudence Mutowo, Michał Nowotka, et al. Chembl: towards direct deposition of bioassay data. Nucleic acids research, 47(D1):D930 D940, 2019. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. 13 Emmanuel Noutahi, Cristian Gabellini, Michael Craig, Jonathan SC Lim, and Prudencio Tossou. Gotta be safe: new framework for molecular design. Digital Discovery, 3(4):796804, 2024. 6, 8, 19 Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. 13, 15 Youngrok Park, Hojung Jung, Sangmin Bae, and SeYoung Yun. Temporal alignment guidance: Onmanifold sampling in diffusion models. arXiv preprint arXiv:2510.11057, 2025. 12 Daniil Polykovskiy, Alexander Zhebrak, Benjamin SanchezLengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): benchmarking platform for molecular generation models. Frontiers in pharmacology, 11:565644, 2020. 2, 6, 20, Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. Frechet chemnet distance: metric for generative models for molecules in drug discovery. Journal of chemical information and modeling, 58(9):17361741, 2018. 21 Yiming Qin, Manuel Madeira, Dorina Thanou, and Pascal Frossard. Defog: Discrete flow matching for graph generation. arXiv preprint arXiv:2410.04263, 2024. 2, 5, 6, 8, 19, 20, 21 Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 3, 12, 15 Benjamin Sanchez-Lengeling and Alan Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400): 360365, 2018. 1 Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo de Almeida, Alexander Rush, Thomas Pierrot, and Simple guidance mechaVolodymyr Kuleshov. arXiv preprint nisms for discrete diffusion models. arXiv:2412.10193, 2024. 13 Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120131, 2018. 6, 10 Title Suppressed Due to Excessive Size Cai Zhou, Chenyu Wang, Dinghuai Zhang, Shangyuan Tong, Yifei Wang, Stephen Bates, and Tommi Jaakkola. Next semantic scale prediction via hierarchical diffusion language models. arXiv preprint arXiv:2510.08632, 2025. 8 Hyunjin Seo, Taewon Kim, Sihyun Yu, and SungSoo Ahn. Learning flexible forward trajectories for masked molecular diffusion. arXiv preprint arXiv:2505.16790, 2025. 2 Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. 13, 15 Antoine Siraudin, Fragkiskos Malliaros, and Christopher Morris. Cometh: continuous-time discrete-state graph diffusion model. arXiv preprint arXiv:2406.06449, 2024. 3, 6, 8, 19, 20, Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 8 Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734, 2022. 2, 3, 5, 6, 7, 8, 19, 20, 21, 22 David Weininger. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):3136, 1988. 1 Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. 12 Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 3859238610. PMLR, 2023. 12 Zhe Xu, Ruizhong Qiu, Yuzhong Chen, Huiyuan Chen, Xiran Fan, Menghai Pan, Zhichen Zeng, Mahashweta Das, and Hanghang Tong. Discrete-state continuoustime diffusion for graph generation. Advances in Neural Information Processing Systems, 37:7970479740, 2024. 2, 5, 6, 8, Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. 13 Kang Zhang, Xin Yang, Yifei Wang, Yunfang Yu, Niu Huang, Gen Li, Xiaokun Li, Joseph Wu, and Shengyong Yang. Artificial intelligence in drug development. Nature medicine, 31(1):4559, 2025. 1 11 Title Suppressed Due to Excessive Size"
        },
        {
            "title": "Table of contents",
            "content": "A Limitation and future directions Further background B.1 Further related works . . . . . . . . B.2 Details of masked diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Mathematical derivations C.1 Generalized HDDM forward process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment details D.1 Decoupled Atom Encoding (DAE) . D.2 Grouping in HDDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Full experimental results with standard deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Implementation of baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Unconditional generation with MOSES and GuacaMol . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 Structure novelty metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7 Unconditional generation with GuacaMol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.8 Multi-property guided generation . D.9 Scaffold extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 12 13 13 13 14 17 18 19 20 20 21 21 22 A. Limitation and future directions Limitations While our models improve the traditional diffusion based molecular generation, we have not tested with the model size increase or architectural improvement, in which we believe have further room for better performance. Moreover, we have not fully trained the model until performance saturation on GuacaMol dataset and we believe performance improvement with further training. Future directions There are many interesting future directions. One is to apply Hierarchical Discrete Diffusion Models to the language domains (Sahoo et al., 2024), or image models (Chang et al., 2022) and combining with different sampling schemes for diffusion models (Jung et al., 2024; Park et al., 2025; Wu et al., 2025; Kim et al., 2025). Another direction is to further improve MolHITs framework with more advanced tokenization incorporating motifs or functional groups (Jin et al., 2018) and apply into the 3D molecular generation (Hoogeboom et al., 2022; Xu et al., 2023) and proteins (Gruver et al., 2023). B. Further background B.1. Further related works Further backgrounds on discrete diffusion models Recently, Masked Language Models are actively studied due to its simple form and potential of bi-directional modeling (Devlin et al., 2019). Campbell et al. (2022) formulate the discrete diffusion using Continuous Time Markov Chain (CTMC) framework and propose correction sampler leveraging tau-leaping. SEDD (Lou et al., 2023) introduce score entropy loss in analogous to the score matching loss in the continuous diffusion models and show scalability in language modeling. Recently, masked diffusion models is further simplified (Sahoo et al., 12 Title Suppressed Due to Excessive Size 2024; Ou et al., 2024; Shi et al., 2024) and reaching to the level that is comparable to the standard AR modeling in large scale (Nie et al., 2024; Bie et al., 2025) and even in multi-modal setting (Yang et al., 2025). Conditional generation with discrete diffusion For conditional generation, Liu et al. (2024) propose graph diffusion transformer for multi-conditional generation on polymer dataset and shows the effectiveness of the classifier-free guidance. Schiff et al. (2024) propose simple mechanism for conditional sampling which is in analogous with CFG in continuous diffusion. B.2. Details of masked diffusion models Marginal transition Let = (X1, . . . , XN ) be discrete random vector with Xk {1, . . . , K}, and let denote the empirical data distribution on {1, . . . , K}N . Define the family of product of distributions: (cid:40) = : q(x) = (cid:89) k=1 qk(xk), qk K1 , (cid:41) where K1 is the (K 1)-simplex. Then, one can define the marginal terminal distribution π as the product of the data marginals: π(x) = (cid:89) k=1 πk(xk), πk(a) = PXP [Xk = a] , {1, . . . , K}. Equivalently, π is the (unique) KL projection of onto C: π = arg min qC KL(P q) . Then, marginal transition defines forward process of discrete diffusion as follows: q(xtx0) = Cat(xt; αtx0 + (1 αt)π). C. Mathematical derivations C.1. Generalized HDDM forward process (11) (12) (13) (14) We now derive generalized forward process that incorporates arbitrary multi-level hierarchies. Let be the total discrete state space with dimension = (cid:80)n k=0 Kk. We partition into + 1 disjoint subsets S0, S1, . . . , Sn, where S0 represents the clean atomic states (with S0 = K0) and Sk represents the k-th level of intermediate hierarchical states (with Sk = Kk). We further define the cumulative subspace up to level as Ti := (cid:83)i k=0 Sk. For each hierarchical stage {1, . . . , n}, we define the transition kernel as row-stochastic matrix Φi [0, 1]Ti1Ki. This kernel encodes the probabilistic mapping from the cumulative lower-level states in Ti1 to the specific higher-level states in Si. To characterize the evolution in full space , we induce global transition matrix Q(i) [0, 1]DD on which embeds the local kernel Φi into the full space as follows: Q(i)(xnext x) = Φi(xnext x) 1 0 if Ti1 and xnext Si, if Ti1 and xnext = x, otherwise. (15) In matrix notation, Q(i) forms block structure where the transitions from Ti1 are governed by Φi, while the remaining diagonal blocks form an identity matrix. Under this formulation, each Q(i) represents the probabilistic projection onto the i-th hierarchical level, enabling us to design the diffusion forward process with the following lemma: 13 Title Suppressed Due to Excessive Size Proposition C.1. Suppose monotonically decreasing functions α(i) satisfying 0 α(1) 1 and boundary conditions α(i) from timestep to timestep (s t) as: α(n) := αi(t) (i = 1, 2, ..., n) defined in 0 1 are 1 = 0 for all i. We define the transition matrix 0 = 1, α(i) Qts := α(1) ts + (α(2) ts α(1) ts )Q(0) + + (1 α(n) ts )Q(n), (16) ts = α(i) where α(i) α(i) 2019) as follows: for every i. Then, transition kernel defined by Eq. 16 satisfies ChapmanKolmogorov consistency (Durrett, QtsQsr = Qtr 0 1. Moreover, one could represent cumulative forward transition from initial timestep 0 to in the following form: α(1) )Q(1) + (1 α(n) Qt = α(1) + (α(2) )Q(n). (17) (18) Proof. First, we can observe Q(i) is projection operator; i.e, Q(i)2 = Q(i) for all by definition. In fact, this can be generalized as Q(i)Q(j) = Qmax(i,j) for any 1 i, by the definition of the Q(i) and φi. Now, suppose Eq. 16 holds for some N. Then, one can observe: ts )Q(0) + + (1 α(j+1) ts )Q(0) + + (α(j+1) (cid:17) ts α(j) Q(j+1) + 1 α(j+1) )Q(j+1)(cid:17) (cid:16) ts )Q(j)(cid:17) (cid:16) (cid:16) ts sr srI + (α(2) α(1) srI + (α(2) α(1) (cid:17) (cid:16) 1 α(j+1) ts αsr + (1 α(j+1) sr (cid:17) ) Q(j+1) sr α(1) sr α(1) sr)Q(0) + + (1 α(j+1) sr)Q(0) + + (α(j+1) sr α(j) sr )Q(j+1)(cid:17) sr)Q(j)(cid:17) (Q(j+1))2 (α(j+1) ts α(j) ts ) α(j+1) ts α(j+1) sr Q(j) α(1) sr α(j+1) sr (cid:17) (cid:16) 1 α(j+1) ts 1 α(j+1) sr (cid:17) Q(j+1) + (cid:16) + + (α(j+1) sr α(j) sr) α(j+1) sr Q(j) αsr + (1 α(j+1) sr (cid:17) ) Q(j+1) (Q(j+1))2 (cid:16) = = ts α(1) ts α(1) QtsQsr (cid:16) ts + (α(2) α(1) ts + (α(2) α(1) (cid:17) (cid:16) ts + + (1 α(j+1) α(1) ) (cid:17) )(1 α(j+1) 1 α(j+1) ) ts + + sr ts (cid:16) (cid:16) = α(j+1) ts + + α(1) ts α(j+1) ts (cid:16) (cid:17) (cid:16) ts + + (1 α(j+1) α(1) ) (cid:17) 1 α(j+1) )(1 α(j+1) ts (cid:16) sr ts ) (cid:16) tr α(1) tr + (α(2) α(1) (cid:17) (cid:16) 1 α(j+1) ts (cid:16) + + = + tr )Q(0) + + (1 α(j) tr )Q(j)(cid:17) (cid:16) (cid:16) + ts + + (1 α(j+1) α(1) )(1 α(j+1) ts (cid:17) ) sr 1 α(j+1) ts (cid:17) ) Q(j+1) + (Q(j+1))2 (cid:17) (cid:16) ) 1 α(j+1) sr (cid:17) Q(j+1) αsr + (1 α(j+1) sr = α(1) tr + (α(2) tr α(1) tr ) + + (1 α(j+1) tr )Q(j+1), (19) where the second to the last equation comes from the inductive assumption on j. Since = 1 case is trivial, the result follows by mathematical induction on j. Proof of Lemma 3.1 Lemma 3.1 is now just special case of above generalized formula in Proposition C.1. C.2. Proof of Theorem 3.2 Let be the one-hot representation of the masked state which we take as prior, and let S0 denote an element in clean state. For forward transition kernel Qts defined induced by the row stochastic matrix Φ [0, 1]KG and the masking operation as in Lemma 3.1, the conditional transition is defined as q(ztzs) = Cat(zt; zsQts). By applying the closed-form transition from Lemma 3.1 and Bayes rule, the posterior distribution q(zszt, x) can be derived as follows: (cid:32) q(zszt, x) = Cat zs; (cid:2)αtsI + (βts αts)Q(1) + (1 βts)Q(2)(cid:3) zt (cid:2)αsx + (βs αs)Q(1)x + (1 βs)m(cid:3) (cid:33) zT [αtI + (βt αt)Q(1) + (1 βt)Q(2)]T (20) 14 We can divide into the following 3 cases depending on which state zt belongs to. Title Suppressed Due to Excessive Size Case 1. zt q(zszt, x) = Cat(zs; x). (21) Case 2. zt S1 When the observed state at time belongs to the mid-level space S1, the posterior depends on the state of zt under the stochastic transition. Using the block structure of the transition kernels, we can obtain: q(zszt, x) = Cat zs; (cid:18) (αsβts αt)x + (βt βtsαs)zt βt αt (cid:19) , (22) Case 3. zt S2 = {m} When the observed state is the mask token S2, the posterior distribution q(zszt, x) becomes weighted combination of the clean state, its stochastic projection, and the mask prior. Using the normalization constant 1 βt, the posterior is given by: (cid:32) q(zszt, x) = Cat zs; αs(1 βts)x + (1 βts)(βs αs)Q(1)x + (1 βs)m 1 βt (cid:33) . (23) Parameterization Inspired by the masked diffusion literature (Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024), we derive simplified loss form through the parameterizing neural network θ to estimate only the probability in clean final state in S0. This leads to the posterior pθ(zszt) in following closed forms depending on the current state zt. Case 1. zt S0 Case 2. zt S1 pθ(zszt) = Cat(zs; zt). (cid:32) pθ(zszt) = Cat zs; (αsβts αt)Q(1) zt xθ(zt, t) + (βt βtsαs)zt Q(1)xθ (βt αt)zT Q(1)xθ (cid:33) . Case 3. zt S2 = {m} pθ(zszt) (cid:32) = Cat zs; (cid:32) = Cat zs; [αtsm + (βts αts)Q(1) + (1 βts)Q(2) m] [αsxθ + (βs αs)Q(1)xθ + (1 βs)m] αs(1 βts)xθ + (1 βts)(βs αs)Q(1)xθ + (1 βs)m 1 βt (cid:33) . 1 βt (24) (25) (cid:33) . (26) ELBO analysis Now, we start with analyzing the ELBO of the Hierarchical Discrete diffusion models in discrete timesteps. LT = t{ 1 , 2 ,...,1} Eq(ztx) [T DKL (q(zszt, x) pθ(zszt))] Case 1. zt DKL (q(zszt, x) pθ(zszt)) = 0. 15 (27) (28) Title Suppressed Due to Excessive Size Case 2. zt DKL (q(zszt, x) pθ(zszt)) (cid:88) = zs{x,φ(x)} q(zszt, x) log q(zszt, x) pθ(zszt) = q(zs = xzt, x) log = (αsβts αt) βt αt log q(zs = xzt, x) pθ(zs = xzt) Q(1)xθ x, Q(1) zt xθ, + 0. + q(zs = φ(x)zt, x) log q(zs = Q(1)xzt, x) pθ(zs = Q(1)xzt) (29) Case 3. zt S2 = {m} DKL (q(zszt, x) pθ(zszt)) (cid:88) = zs{x,φ(x),m} q(zszt, x) log q(zszt, x) pθ(zszt) = q(zs = xzt = m, x) log + q(zs = Q(1)xzt = m, x) log q(zs = Q(1)xzt = m, x) pθ(zszt) (30) + q(zs = mzt = m, x) log q(zs = xzt = m, x) pθ(zszt) q(zs = mzt = m, x) pθ(zszt) = (αs αsβts) 1 βt log 1 xθ, + (1 βts)(βs αs) 1 βt DKL (cid:16) (cid:17) Q(1)xQ(1)xθ(zt, t) + 0. Combined together, each term in Eq. 27 can be expressed as follows: DKL (q(zszt, x) pθ(zszt)) = (αsβts αt) βt αt (cid:16) (cid:17) logzt, Q(1)xθ log xθ, zt, Q(1)x (31) (cid:18) (1 βts)(βs αs) 1 βt logQ(1)x, Q(1)xθ(zt, t) + (αs αsβts) 1 βt (cid:19) log xθ, zt, + Cs,t, for some constant Cs,t, which leads into following continuous time NELBO of HDDM: NELBO(θ) (cid:88) = lim i= DKL (cid:0)q(zt(i1)zt(i), x) pθ(zt(i1)zt(i))(cid:1) (cid:88) (αt(i)βt(i)t(i1) αt(i)) βt(i) αt(i) = lim (cid:18) (1 βt(i)t(i1))(βt(i) αt(i1)) 1 βt(i) i= (cid:16) (cid:17) logzt(i), Q(1)xθ log xθ, zt(i), Q(1)x logQ(1)x, Q(1)xθ(zt(i), t(i)) + (αt(i1) αt(i1)βt(i)t(i1)) 1 βt(i) (cid:19) log xθ, zt(i), + Ct(i1),t(i) (cid:20) αt(β (cid:90) 1 α t) βt αt = t(1) (cid:18) β (cid:16) (cid:17) logzt, Q(1)xθ log xθ, zt, Q(1)x t(βt αt) βt(1 βt) logQ(1)x, Q(1)xθ(zt, t) + αtβ βt(1 βt) (cid:19) (cid:21) log xθ, zt, + Ct dt. (32) As result, we obtain the general NELBO of HDDM in the following Theorem:"
        },
        {
            "title": "General NELBO in HDDM",
            "content": "Title Suppressed Due to Excessive Size Theorem C.2. Suppose forward process of two-level HDDM is defined with stochastic operator Q(1), Q(2), which are induced from the Φ and masking operator, respectively as in Lemma 3.1. Then, for some constant C1, the negative evidence lower bound (NELBO) can be expressed as follows: NELBO(θ) = EQ,t (cid:18) β t(βt αt) βt(1 βt) αt(β α t) βt αt (cid:16) (cid:17) logzt, Q(1)xθ log xθ, zt, Q(1)x logQ(1)x, Q(1)xθ(zt, t) + αtβ βt(1 βt) (cid:19) log xθ, zt, + C1. (33) Note that above theorem holds for arbitrary stochastic row matrix Φ, which means we can design any stochastic mapping from S0 to S1. When Q(1) is deterministic (i.e, its rows are composed of one-hot vectors), we can parameterize model to estimate the categories that are in the same mid-level state. This parameterization leads to further simplified form of Theorem C.2 as follows: NELBO in HDDM with deterministic grouping Corollary C.3. NELBO(θ) = EQ,t αt(β α t) βt αt log xθ(zt, t), [zt S1] β βt(1 βt) (cid:16) for some constant C2. (βt αt) logQ(1)x, Q(1)xθ(zt, t) + αt log xθ(zt, t), (cid:17) [zt = m] + C2, (34) D. Experiment details D.1. Decoupled Atom Encoding (DAE) While standard graph-based diffusion models typically adopt coarse node encoding based solely on atomic numbers (Z), decoupled atom encoding (DAE) expands the original token vocabulary by explicitly decoupling three standards: aromaticity, hydrogen saturation, and formal charge magnitude. Decoupled Atom Encoding (DAE) expands the token vocabulary by explicitly decoupling three critical chemical descriptors: aromaticity, hydrogen saturation, and formal charge magnitude. Unlike previous methods that rely on implicit hydrogen estimation (e.g., via RDKits valence rules), DAE treats these attributes as primary node features to be explicitly encoded and decoded. This approach resolves the one-to-many mapping problem between atomic tokens and their physical states, enabling the near-perfect reconstruction of drug-like scaffolds from the MOSES and GuacaMol datasets. Furthermore, this extended vocabulary facilitates the reliable generation of complex heteroaromatics and zwitterionic species which are extremely rare for baselines using coarse tokenization. Specifically, we emphasize that tokenizing [nH] as distinct state is fundamentally different from modeling explicit hydrogen atoms as separate nodes. While the latter can significantly increases graph complexity and computational overhead, DAE preserves graph sparsity while maintaining chemical precision. Table 6. Comparison of Atom Vocabularies on the MOSES Dataset. DAE resolves structural ambiguities by decoupling elements into specific aromatic and hydrogen-locked states. Method Elemental Basis Unique Tokens (Vocabulary) Size Standard Encoding {C, N, S, O, F, Cl, Br} C, N, S, O, F, Cl, Br DAE (Ours) {C, N, S, O, F, Cl, Br} Aliphatic: C, N, S, O, F, Cl, Br 7 12 Aromatic: c, n, nH, s, Title Suppressed Due to Excessive Size Figure 5. The ratios of generated molecules having formal charge. MolHIT can reach to the training level proportion, while models with previous coarse encoding (left two) barely generate the charged atoms. DAE in MOSES The MOSES dataset consists primarily of stable, neutral drug-like molecules which is clean lead filtered from the ZINC dataset (Irwin and Shoichet, 2005). In this context, the reconstruction bottleneck is primarily structural. Previous coarse-grained encodings fail to resolve the placement of pyrrolic hydrogens ([nH]), critical motif in heteroaromatic rings like indole or imidazole. By explicitly decoupling aromaticity and hydrogen counts, DAE enables model can explicitly distinguish these motifs, resulting in improved generation quality. Table 7. Vocabulary expansion for the Guacamol dataset. DAE scales from 12 elemental types to 56 semantic tokens including aromatic and charged atoms. Category Standard Encoding (Size: 12) DAE Tokens (Size: 56) Neutral Aliphatic {C, N, O, F, B, Br, Cl, I, P, S, Se, Si} C, N, O, F, B, Br, Cl, I, P, S, Se, Si Aromatic States (None / Implicit) c, c+, c-, n, nH, n+, nH+, n-, s, s+, o, o+, se, se+, Charged & Hypervalent (None / Implicit) C+, C-, N+, NH+, NH2+, NH3+, N-, NH-, O+, O-, F+, F-, B-, Br+2, Br-, Cl+, Cl+2, Cl+3, Cl-, I+, I+2, I+3, P+, P-, S+, S-, Se+, Se-, SiDAE in GuacaMol GuacaMol (Brown et al., 2019) is constructed from standardized subset of ChEMBL (Mendez et al., 2019), restricted to common medicinal-chemistry elements. In this unconstrained space, previous models suffer from fundamental reconstruction failure; for instance, standard coarse-grained encoding achieves only 1.88% success rate on the [nH] group, with negligible 0.09% identity preservation rate. While previous models implicitly rely on the relaxation technique which can improve the success rates (e.g., increasing charged group success from 80.43% to 96.54%), this it only preserves 80.07% of total molecules, indicating failure to maintain the original chemical identity. As illustrated in Figure 5, MolHIT addresses this through Decoupled Atom Encoding (DAE), which expands the vocabulary to 56 tokens by encode-decode the atoms with extended vocabulary space, resulting in 100 % success rate and over 99.98% in identity preservation rate. Moreover, as illustrated in Fig. 6, the effect of DAE also happens in generative performance, where it enables generating molecules with formal charge which consist of about 6% in GuacaMol dataset. D.2. Grouping in HDDM Grouping Details for MOSES and GuacaMol We employ dataset-specific grouping strategies to align the intermediate state space S1 with the underlying chemical distribution of each corpus. Table 8 summarizes these partitions. 18 Title Suppressed Due to Excessive Size Figure 6. Reconstruction Fidelity and Identity Preservation. We measure the proportion of generated molecules that have at least one atom with formal charge. Table 8. Deterministic grouping kernels for node state space partitioning in MOSES and GuacaMol. Dataset Group ID Atom Elements (S0) MOSES GuacaMol Group 1 Group 2 Group 3 Group 4 Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 {C} {N, O, S} {F, Cl, Br} {c, o, n, [nH], s} {F, Cl, Br, I, F, Cl, Br} {C, N, O, P, S, Se} {c, n, [nH], o, s, se, p} {N+, n+, [nH]+, P+, [NH]+, [NH2]+, [NH3]+, Br+2, Cl+2, Cl+3, I+2, I+3} {O, N, [NH], O+, S+, B, C+, C, c+, c, n, s+, o+, se+, F+, Cl+, I+, P, S, Se+, Se, Si} {B, Si} Table 9. Unconditional generation on MOSES dataset with full statistics. We bring the reported value from Cometh and DeFoG from their work. Category Model Quality Scaf-Novel Scaf-Ret. Valid Unique Novel Filters FCD SNN Scaf - Training set 95.4 100.0 100.0 100.0 0.48 0.59 - 1D Sequence VAE[25] 92.8 0.2 CharRNN [42] 92.6 2.5 SAFE-GPT [33] 92.8 0.0 62.1 0.0 GenMol [27] 2D Graph DiGress [47] DisCo [51] Cometh [45] DeFoG [38] 82.5 0.7 - 82.1 0.1 88.5 0.0 0.22 0.01 0.29 0.04 0.12 0.00 0.05 0.00 0.26 0.00 - 0.36 0.00 0.26 0.00 0.031 0.003 97.7 0.1 0.035 0.003 97.5 2.6 0.015 0.000 99.8 0.0 0.012 0.001 99.7 0. 99.7 0.0 99.9 0.0 98.9 0.0 64.0 0.5 69.5 0.6 99.7 0.0 0.58 0.01 5.9 1.0 0.56 0.01 11.0 0.8 84.2 5.1 99.4 0.3 6.3 0.7 43.7 0.3 97.7 0.0 0.57 0.01 68.9 0.4 98.1 0.1 16.36 0.07 0.64 0.01 1.6 0.1 0.57 0.00 0.52 0.03 0.72 0.02 0.031 0.000 87.1 0.9 100.0 0.0 94.2 0.2 97.5 0.0 - 88. 100.0 97.7 95.6 0.023 0.000 87.2 0.0 100.0 0.0 96.4 0.1 97.3 0.0 92.1 0.0 98.9 0.0 0.031 0.000 92.8 0.0 99.9 0.0 1.25 0.03 1.44 1.44 0.02 1.95 0. 0.53 0.00 12.8 1.4 0.50 15.1 0.51 0.00 16.8 0.7 0.55 0.00 14.4 0.0 MolHIT 94.2 0. 0.39 0.00 0.033 0.001 99.1 0.0 99.8 0.0 91.4 0.2 98.0 0.00 1.03 0.02 0.55 0.00 14.4 1. D.3. Full experimental results with standard deviations For statistical significance, we run 3 experiments for every experiment. We put the result including standard deviation of unconditional MOSES generation in Table 9, GuacaMol experiment in Table 10, multi-property guided generation result in Table 11, and scaffold extension result in Table 12. 19 Table 10. Full statistics of GuacaMol benchmark results (unfiltered). Val.: Validity, V.U.: Unique, V.U.N.: Novel. All results are averaged over 3 runs. Title Suppressed Due to Excessive Size Model Training set DiGress (org.) DiGress (full) DiGress+DAE Val. 100.0 V.U. V.U.N. KL FCD 100.0 99.9 92. 85.2 85.2 85.1 74.7 0.4 74.6 0.5 74.0 0.4 92.4 0.5 61.1 0.2 65.2 0.4 65.2 0.4 64.9 0.4 87.0 0.4 49.2 0.6 92.9 68.0 MolHIT (Ours) 87.1 0.5 87.1 0.3 86.0 0.5 96.7 0.1 54.9 0. Table 11. Full statistics of multi-property guided generation on MOSES with 4 different conditions. We report mean absolute error (MAE; ), Pearson correlation (r; ), and validity. Avg. is the macro-average across properties. Bold denotes best values. Method QED SA MAE LogP MW Avg. QED SA LogP MW Avg. Pearson Validity (%) Marginal 0.117 (0.002) Marginal + DAE 0.107 (0.001) 0.061 (0.001) MolHIT 0.115 (0.003) 0.094 (0.001) 0.040 (0.001) 0.067 (0.001) 0.061 (0.000) 0.049 (0.001) 0.272 (0.009) 0.227 (0.004) 0.081 (0.005) 0.143 (0.004) 0.122 (0.001) 0.058 (0.002) 0.489 (0.003) 0.565 (0.005) 0.804 (0.009) 0.570 (0.012) 0.559 (0.009) 0.790 (0.011) 0.802 (0.003) 0.836 (0.005) 0.950 (0.004) 0.396 (0.001) 0.437 (0.015) 0.685 (0.024) 0.564 (0.005) 0.599 (0.002) 0.807 (0.011) 75.03 (0.74) 87.85 (0.46) 96.31 (0.23) Table 12. Full statistics of scaffold extension results on MOSES. Results are averaged over 3 runs of 10,000 targets. Hit@k denotes the recovery of ground-truth within samples. Model Valid (%) Diversity Hit@1 Hit@5 DiGress 50.8 0.5 Marginal + DAE 64.8 0. 44.8 1.8 2.07 0.09 6.41 0.21 58.0 0.1 1.67 0.10 6.37 0.24 MolHIT (Ours) 83.9 0.4 57.4 0.6 3.92 0.23 9.79 0.09 D.4. Implementation of baselines For all baselines, we use released checkpoints when available. Otherwise, we train the models using their official codebase, following the training hyperparameters reported in the paper or provided in the codebase. Note that the original GenMol (Lee et al., 2025) model was trained on much larger molecule dataset, so we train the model on the MOSES dataset for fair comparison. D.5. Unconditional generation with MOSES and GuacaMol Training details For our model backbone, we adopt the graph transformer proposed by Vignac et al. (2022), which simultaneously predicts node and edge features. To ensure fair comparison across all experimental settings, we maintain consistent architecture of 12 transformer blocks without altering any internal dimensional configurations. The total trainable parameter count is approximately 16.2M. The introduction of additional token indices (DAE and HDDM) adds negligible overhead where representing variance of less than 0.01% in total parameters. For training stability, we employ gradient clipping with threshold of 2.0 and an Exponential Moving Average (EMA) rate of 0.999. We early stop with 100 epoch training with MOSES and 50 epochs with GuacaMol, compared to the original 300 epoch training of other graph diffusion baselines (Vignac et al., 2022; Siraudin et al., 2024; Qin et al., 2024). We also remove calculating geometric prior originally used in Vignac et al. (2022), where they use extra graph features as conditional information. In our experiments, this has negligible effects on the performance. Evaluation of MOSES The following metrics are utilized to evaluate the generative performance on the MOSES dataset, following the standardized protocols established by Polykovskiy et al. (2020). Validity (): The fraction of generated molecules that pass RDKits sanitization checks and basic chemical valency rules. High validity is primary indicator that the DAE system successfully constrains the sampling process to the chemically feasible manifold. Uniqueness (): The proportion of valid molecules that are not duplicates. This measures the models ability to avoid mode collapse and explore diverse structural space. Novelty (): The fraction of valid, unique molecules that were not present in the training set. This differentiates between model that has memorized the data and one that has learned the underlying generative distribution. 20 Title Suppressed Due to Excessive Size Filters (): The percentage of generated molecules that pass common medicinal chemistry filters (e.g., MCULE, BRENK, and PAINS). This evaluates the drug-likeness and synthetic viability of the generated samples. Frechet ChemNet Distance (FCD (Preuer et al., 2018), ): measure of the distance between the multivariate distributions of generated and test molecules in the feature space of ChemNet. FCD captures both chemical and biological similarity, serving as the most rigorous metric for distributional fidelity. Similarity to Nearest Neighbor (SNN, ): The average Tanimoto similarity between generated molecule and its closest neighbor in the test set. High SNN indicates that the model has captured the specific structural motifs and chemical space of the benchmark. Scaffold Similarity (Scaf, ): The cosine similarity between the frequencies of BemisMurcko scaffolds (Bemis and Murcko, 1996) in the generated and test sets. This assesses whether the models learned distribution of backbone structure matches the architectural diversity of real-world leads. Baselines D.6. Structure novelty metric Scaffold Novelty: We evaluate the models ability to innovate beyond the training distribution using BemisMurcko scaffolds (Bemis and Murcko, 1996). The absolute number of unique generated scaffolds absent from the training set: Scaf-Novel = Sgen Strain ntotal . (35) This metric quantifies the models capacity for structural extrapolation, measuring its efficiency in exploring the beyond the molecular structure of the given dataset. Scaffold Retrieval: This assesses the models ability to rediscover known, high-quality frameworks from the held-out test set. This is defined as the absolute number of unique test-set scaffolds successfully generated: from the training set: Scaf-Ret = Sgen Stest ntotal . (36) Scaffold retrieval serves as rigorous test of distributional accuracy. high retrieval density demonstrates that the model has not merely learned to generate novel-looking noise, but has accurately captured the underlying manifold of valid, drug-like molecules defined by the test distribution. D.7. Unconditional generation with GuacaMol GuacaMol experiment For experiment on GuacaMol, we test our algorithm on the unfiletered, full dataset. Previous graph diffusion model baselines (Vignac et al., 2022; Siraudin et al., 2024; Qin et al., 2024) train the model on the filtered dataset, where they filter out the molecules that are failed to be reconstructed back. This can bias the training data distribution. In contrast, we use full, unfiltered dataset for the experiment and since there is no graph diffusion baseline, we compare with the original DiGress trained on full GuacaMol dataset with coarse atom encoding, Discrete Diffusion using marginal transition with DAE, and compare them with MolHIT. D.8. Multi-property guided generation Data construction For conditional generation, we augment the MOSES dataset (Polykovskiy et al., 2020) with four continuous molecular descriptors: Quantitative Estimate of Drug-likeness (QED), Synthetic Accessibility (SA) score, Octanol-Water Partition Coefficient (logP), and Molecular Weight (MW). Quantitative Estimate of Drug-likeness (QED, ): widely used composite score that summarizes multiple molecular properties (e.g., lipophilicity, polarity, and molecular size) into single measure of drug-likeness; higher values indicate more drug-like compounds. 21 Title Suppressed Due to Excessive Size Synthetic Accessibility (SA, ): An empirical estimate of synthetic difficulty that combines fragment-based contributions with complexity penalty; lower values indicate molecules that are easier to synthesize. OctanolWater Partition Coefficient (logP): measure of lipophilicity that is informative of solubility and membrane permeability; excessively high logP is typically associated with poor solubility and unfavorable ADMET profiles. Molecular Weight (MW): The molecular mass in Daltons. Consistency with the training distribution (e.g., MOSES) helps ensure generated molecules remain within drug-like regime. All properties are calculated using the RDKit library and the sascorer module (Ertl and Schuffenhauer, 2009). To ensure stable convergence of the conditioning vector within our AdaLayerNorm layers, we perform min-max normalization on these values using the global statistics of the training split, which are in Table 13. Table 13. Min and max values for molecular property conditioning in MOSES training / test split. Split Statistic QED SA Score logP MW (Da) Training Test Min Max Min Max 0.1912 0. 0.2265 0.9484 1.2694 7.4831 1.3339 6.6916 -5.3940 5.5533 -4.2894 5.7255 250.017 349. 250.042 349.990 Conditional graph transformer While we maintain the core node-edge attention mechanism of the original graph transformer (Vignac et al., 2022), we introduce several key modifications to enable conditional modeling. First, we remove the persistent global feature vector ywhich in the original framework is updated at every layerand replace it with centralized conditioning vector C. This vector is composed of sinusoidal timestep embedding (Ho et al., 2020) and an optional MLP-encoded external property condition c. Second, to integrate into the denoising process, we replace standard Layer Normalization with Adaptive Layer Normalization (AdaLayerNorm) for node features. Specifically, for node embedding x, the normalization is defined as: AdaLN(x, C) = (1 + γ(C)) LayerNorm(x) + β(C) where γ and β are affine transformations of the conditioning vector. This allows the global context (time and properties) to directly modulate the scale and shift of node representations. Finally, we implement Classifier-Free Guidance (CFG) support by incorporating dropout mechanism on the property embedding during training, while ensuring the temporal signal remains persistent to maintain denoising stability. Our conditional graph transformer naturally inherits permutation equivariance, which is different from the Liu et al. (2024). Evaluation details For sampling, we employ Classifier-Free Guidance (CFG) (Ho and Salimans, 2022) with guidance scale of = 1.0. We observe that in our discrete graph-diffusion framework, increasing the guidance weight beyond unity did not consistently yield better property alignment. We leave the better design the sampler or models to be effective in higher guidance strength as promising avenue for future research. D.9. Scaffold extension Task Formulation Given ground-truth molecule from the test split, we use RDKit to extract its scaffold G. The task is to generate completed molecule such that M. To isolate the generative capability from size prediction errors, we bound the generation size (number of atoms) to match G. Sampling Protocol At each reverse timestep t, the region corresponding to is forced to be the same (i.e, q(xtxscaffold) = xscaffold, ensuring the scaffold region is strictly fixed during the generation. The extension region is initialized from the limit distribution (i.e, prior) pprior and evolved via the standard reverse process. We generate = 1, 5 independent samples per scaffold to capture the models exploration capability. 22 Title Suppressed Due to Excessive Size Metric Definitions Metrics are computed per scaffold and then averaged across the test set. Let M1, . . . , MK be the generated graphs for single scaffold. Validity: The fraction of Mi that are chemically valid according to RDKit sanitization. Diversity: Calculated on the unique valid set U. We define diversity as 1 1 2 Tanimoto similarity using Morgan fingerprints (r = 2, 2048 bits). (cid:80) u,vU Sim(u, v), where Sim is the Exact Match (Hit@K): binary indicator, set to 1 if the ground truth (canonical SMILES) is present in the generated set {M1, . . . , MK}, and 0 otherwise."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "LG AI Research",
        "Seoul National University"
    ]
}