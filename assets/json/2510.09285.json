{
    "paper_title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
    "authors": [
        "Siyuan Huang",
        "Xiaoye Qu",
        "Yafu Li",
        "Yun Luo",
        "Zefeng He",
        "Daizong Liu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs."
        },
        {
            "title": "Start",
            "content": "SPOTLIGHT ON TOKEN PERCEPTION FOR MULTIMODAL REINFORCEMENT LEARNING Siyuan Huang12 Xiaoye Qu1 Yafu Li3 Yu Cheng3 1Shanghai AI Laboratory 3The Chinese University of Hong Kong 2Shanghai Jiao Tong University 4Nanjing University 5Peking University Yun Luo Zefeng He4 Daizong Liu5 5 2 0 2 0 1 ] . [ 1 5 8 2 9 0 . 0 1 5 2 : r Project Page: https://github.com/huaixuheqing/VPPO-RL"
        },
        {
            "title": "ABSTRACT",
            "content": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in rollout trajectory is sparsely distributed, where only small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose VisuallyPerceptive Policy Optimization (VPPO), novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through dual mechanism: it reweights trajectorys advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading opensource RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish new token-level perceptual perspective for analyzing multimodal RLVR but also present novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning from verifiable rewards (RLVR), particularly with online algorithms like Group Relative Policy Optimization (GRPO), has dramatically advanced the reasoning capabilities of Large Language Models (LLMs) in text-centric domains (Shao et al., 2024; Guo et al., 2025; OpenAI, 2024; Team et al., 2025; Yang et al., 2025a; Anthropic, 2025; Zhang et al., 2025; Qu et al., 2025; Yan et al., 2025). Recently, many works have attempted to translate this success to Large Vision-Language Models (LVLMs). These efforts primarily focus on three directions: data-centric enhancements (Li et al., 2025; Liang et al., 2025; Liu et al., 2025a; Yao et al., 2025; Chen et al., 2025a; Meng et al., 2025; Huang et al., 2025; Yang et al., 2025b), reward-centric engineering (Shen et al., 2025b; Xia et al., 2025; Wang et al., 2025b; Xiao et al., 2025; Yu et al., 2025a; Wan et al., 2025; He et al., 2025), and other algorithmic adjustments (Wang et al., 2025a; Zhao et al., 2025). However, prevailing RLVR frameworks for LVLMs largely neglect the critical role of visual perception in the optimization process. Effective reasoning is contingent upon accurate perception, which provides the essential grounding for logical deduction (Xiao et al., 2025). The geometry problem in Figure 1 exemplifies this dependency. Given question: In circle O, AC is parallel to OB, and BOC = 50. What is the measure of OAB? To correctly answer this question, critical insight should be derived from the visual diagram, namely segments OA and OB are radii of the Work was done during Siyuan Huangs internship at Shanghai AI Laboratory. Corresponding authors."
        },
        {
            "title": "Preprint",
            "content": "circle O, rendering AOB isosceles. Therefore, without explicitly integrating perceptual ability into the core learning objectives, models cannot develop genuine multimodal reasoning capabilities (Yu et al., 2025a; Xiao et al., 2025). In this paper, we analyze the perceptual mechanisms of multimodal RLVR through an innovative lens of token perception, investigating the impact of tokens with varying visual dependency on reasoning. With granular analysis, we first point out that in the Chain-of-Thought (CoT) (Wei et al., 2022) processes of multimodal reasoning, the token perception distribution in rollout trajectory exhibits distinct pattern, where the majority of tokens are generated with low visual dependency, while critical minority of tokens emerge with high dependency. After aggregating the token perception at the trajectory level, we further observe that different reasoning trajectories also exhibit significant divergence in their overall perceptual quality, as only part of trajectories are genuinely perception-driven paths. Although those paths without significant visual perception may still fortuitously arrive at the correct answer, the resulting models will exhibit weak multimodal perception capabilities. These observations pinpoint foundational flaw inherited from text-based RLVR, i.e, existing implementations directly train over all tokens with limited understanding of which tokens actually facilitate multimodal perception and reasoning. The indiscriminate broadcasting of single, coarse reward to every trajectory and token hinders further performance gains by failing to prioritize critical perception-related trajectories and tokens. Building upon the above discovery of token perception, we introduce Visually-Perceptive Policy Optimization (VPPO), novel policy gradient algorithm to explicitly integrate the token perception into the policy update of multimodal RL, as illustrated in Figure 1. Specifically, our VPPO first quantifies the visual dependency of each token. Based on this visual dependency, we devise two strategies. First, to align the learning objective with perception-grounded trajectories, VPPO reweights each trajectorys advantage using its average dependency. In this way, the learning signal is steered toward robust, perception-grounded reasoning paths over spurious shortcuts. Second, to focus the learning signal on what truly matters, VPPO constructs sparse gradient mask to concentrate policy updates exclusively on critical visually-grounded reasoning tokens. This directly counters signal dilution, yielding lower-variance gradient that leads to faster convergence and stronger final policy. Notably, our VPPO can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Figure 1: Our VPPO framework explicitly relies on token visual dependency to shape trajectory advantages and filter token gradients. To validate the effectiveness of our proposed VPPO, we conduct extensive experiments across suite of eight challenging multimodal reasoning benchmarks, covering mathematical, geometric, logical, and multi-discipline reasoning. Based on Qwen2.5-VL series models, our 7B variant achieves remarkable 19.2% average accuracy improvement over baseline, also surpassing previous open-source leading methods. This robust performance seamlessly scales to the 32B model, which also brings 7.6% average accuracy improvement. Crucially, these performance gains are achieved alongside superior training stability and faster convergence, underscoring its efficiency and robustness. To sum up, our main contributions are threefold: In this paper, we make the first attempt to analyze the perceptual mechanisms of multimodal RLVR through an innovative lens of token perception. We discover that only critical minority of tokens emerge with high visual dependency, while only part of the trajectories are genuinely perception-driven paths. We introduce VPPO, novel policy gradient algorithm that explicitly focuses on token perception, leveraging visual dependency to align trajectory-level objectives and focus tokenlevel gradient updates. In this way, the model spotlights perception while reasoning. Our extensive experiments on eight perception and reasoning benchmarks demonstrate the superior performance of our VPPO. We further show its robust scalability across both 7B"
        },
        {
            "title": "Preprint",
            "content": "and 32B model scales. Meanwhile, we perform in-depth ablation studies to validate the critical designs in our VPPO."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal Reasoning. While Large Language Models (LLMs) have achieved powerful reasoning in text-only domains (Guo et al., 2025), their visual counterparts, Large Vision-Language Models (LVLMs) (Bai et al., 2025b; Hurst et al., 2024; Team et al., 2024; Bai et al., 2025a), still exhibit significant performance gap when tasked with this complex integration (Wang et al., 2024b; Dong et al., 2025; Su et al., 2025a;b; Shen et al., 2025a; Qu et al., 2024). Bridging this gap requires frameworks that can adapt the reasoning successes from text-only models to the unique demands of the multimodal space, where foundational algorithms like PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) are being actively explored. Dominant Strategies in Multimodal RL. Most strategies focus on enhancing components external to the core learning algorithm. These approaches are largely either data-centric, focusing on the curation of visually-grounded datasets (Bai et al., 2025c; Li et al., 2025; Liang et al., 2025), distillation of Chain-of-Thought data (Chen et al., 2025b; Huang et al., 2025; Meng et al., 2025), and design of training curricula (Chen et al., 2025c; Wei et al., 2025); or reward-centric, seeking to engineer more informative, perception-aware signals (Wang et al., 2025e; Ma et al., 2025; Fan et al., 2025; Liu et al., 2025b; Yang et al., 2025b; Xia et al., 2025; Chen et al., 2025d; Wan et al., 2025). Other tactics include modifying rollouts or integrating external vision tools (Liu et al., 2025a; Wang et al., 2025a; Zheng et al., 2025b). While modality-agnostic algorithmic advances like Dynamic Sampling Policy Optimization (DAPO) (Yu et al., 2025b) introduce effective techniques like dynamic sampling and clip-higher, they still broadcast uniform learning signal to all tokens. Our VPPO counters this core limitation by intervening internally, using visual dependency to reweight trajectory advantages and focus gradient updates on pivotal moments of visually-grounded reasoning. Pivotal Tokens in Reasoning. Prior works in RL for large language models identify the pivotal tokens via high-entropy forking points (Wang et al., 2025c), low-confidence error points targeted for exploration (Vassoyan et al., 2025), or contrastive estimation between models trained on correct vs. incorrect data (Lin et al., 2024). However, for the multimodal domain, pivotal token is not merely logical fork but critical moment of visually-grounded reasoning. In this paper, we introduce VPPO, the first multimodal RL algorithm designed to formally identify the perceptually pivotal tokens via dependency and then leverage them for targeted optimization."
        },
        {
            "title": "3 METHOD",
            "content": "In this paper, as shown in Figure 2, we introduce Visually-Perceptive Policy Optimization (VPPO) that explicitly focuses on token perception by hierarchically shaping trajectory-level advantages and filtering token-level gradients. This targeted signal modulation fosters more stable, efficient, and interpretable learning."
        },
        {
            "title": "3.1 PRELIMINARY: GROUP RELATIVE POLICY OPTIMIZATION (GRPO)",
            "content": "Given multimodal prompt (I, q) consisting of visual input and textual query q, the old policy πθold generates group of responses, {oi}G i=1. In the RLVR framework, binary reward Ri {0, 1} is assigned to each complete response based solely on whether its final extracted answer matches the ground truth. While GRPO mitigates reward sparsity through group-based advantage estimation, it remains fundamentally reliant on this coarse, outcome-based signal. The advantage ˆAi for response oi is its normalized reward: ˆAi = Ri mean({Rk}G std({Rk}G k=1) k=1) 3 (1)"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of our VPPO framework. Given the original and masked image inputs, we first obtain the corresponding output distributions. Then, we compute token-level visual dependency score for each trajectory. Subsequently, these token-level scores are used to generate two hierarchical control signals: at the macro-level, they are averaged into trajectory-level dependency to shape the advantage, while at the micro-level, the top-k% tokens are identified to create sparse binary token gradient mask. In this way, the uniform advantage is transformed into fine-grained, targeted learning signal for the final policy update. The policy πθ is then updated to maximize clipped surrogate objective, where this uniform advantage ˆAi is broadcast to every timestep t: LGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi, clip(ri,t(θ), 1 ε, 1 + ε) ˆAi (cid:17) (2) where ri,t(θ) = πθ(oi,tI,q,oi,<t) πθold (oi,tI,q,oi,<t) is the probability ratio. While scalable, this outcome-based verification introduces two-tiered limitation as follows: 1. Trajectory-Level Ambiguity: It treats all correct solutions equally, failing to distinguish reasoning path that is strongly grounded in visual evidence from one that arrives at the same answer through linguistic priors or hallucination. 2. Token-Level Uniformity: The single, coarse reward is then applied indiscriminately to every token in the sequence, failing to selectively reward the specific, pivotal moments of visually-grounded reasoning that led to the correct outcome."
        },
        {
            "title": "3.2 VISUALLY-PERCEPTIVE POLICY OPTIMIZATION (VPPO)",
            "content": "To study the perception in multimodal reasoning, we first develop metric to quantify visual dependency at each token and analyze the token perception in Section 3.2.1. Subsequently, based on the token perception, we further aggregate them into the trajectory-level dependency and uncover key insights into their non-uniform nature in Section 3.2.2. Based on these findings, we introduce VPPO in Section 3.2.3 for perception-centric multimodal reasoning."
        },
        {
            "title": "3.2.1 QUANTIFYING TOKEN VISUAL DEPENDENCY",
            "content": "We define tokens visual dependency as the information gain provided by the visual context. This is quantified by computing the Kullback-Leibler (KL) divergence between the policys predictive distribution conditioned on the true image versus perturbed version, formally measuring the distributional shift attributable to visual input. The choice of KL divergence is validated in Appendix F, where it outperforms other metrics like Jensen-Shannon Divergence and simple probability shifts. Definition 3.1 (Token-level visual dependency). Let be the visual input and be noninformative, perturbed version. At given state st = (q, o<t), the visual dependency at step"
        },
        {
            "title": "Preprint",
            "content": "t is the KL divergence between the policys output distributions conditioned on and : S(st, I) := DKL (πθ(st, I) πθ(st, )) . (3) high value indicates that the image provides critical information for the token prediction at step t, marking it as key moment of visually-grounded reasoning. With the above metric measuring the visual dependency for each token, we analyze the empirical distribution of token perception. To achieve this, we perform inference with the Qwen2.5-VL7B model on the vision-dominant subset of the MathVerse (Zhang et al., 2024) benchmark. We then compute the token visual dependency for every token across all generated trajectories and demonstrate their frequency distribution in Figure 3. The y-axis is on logarithmic scale to better visualize the distributions long tail, and Kernel Density Estimation (KDE) curve is overlaid for easier visualization of the trend. This analysis leads to our first key insight: Insight 1: Token Visual Dependency is Sparsely Distributed. Within the trajectory, visual reasoning is driven by sparse set of pivotal tokens. Figure 3 shows the sparse distribution of token-level visual dependency. Plotted on logarithmic y-axis, the frequency drops exponentially as dependency increases. This highly skewed distribution confirms that only small fraction of tokens are critical for visually-grounded reasoning. Further analysis confirms their semantic importance, as these high-dependency tokens predominantly consist of numbers, geometric concepts, and logical operators essential for the reasoning process. Broadcasting uniform learning signal to all tokens thus dilutes the reward by rewarding many irrelevant, nonperceptual steps."
        },
        {
            "title": "3.2.2 ANALYSIS OF REASONING TRAJECTORIES",
            "content": "Figure 3: The skewed distribution of token-level visual dependency. After analyzing the token-level dependency, we aggregate this metric to the trajectory level by defining the trajectory dependency S(τ ) as the mean of the token-level dependency scores over full trajectory τ . This score represents the trajectorys overall reliance on visual evidence. To explore its distribution, we use the same experimental setup as before, plotting the frequency of these trajectory dependency scores in Figure 4. This reveals our second key insight: Insight 2: Trajectories Exhibit Heterogeneous Visual Grounding. Not all correct reasoning paths are created equal. As shown in Figure 4, the distribution of trajectorylevel visual dependency is heterogeneous. While loosely Gaussian, the distribution is right-skewed with long tail, revealing that distinct subset of high-dependency trajectories pulls the mean (0.09) to the right of the distributions peak. Standard RL frameworks, by assigning uniform reward, fail to distinguish and preferentially learn from these more robust, visually-grounded solutions."
        },
        {
            "title": "3.2.3 VPPO POLICY GRADIENT ALGORITHM",
            "content": "Figure 4: Distribution of trajectory dependency on perception. Based on these insights, we introduce VPPO, novel gradient algorithm that reshapes the learning signal at two levels of granularity to explicitly focus on token perception. Micro-level: Token-level Gradient Filtering (TGF). Inspired by Insight 1, we focus on the learning signal exclusively on pivotal tokens. For each trajectory τi, we identify the set of indices Ki corresponding to the top-k% of tokens with the highest visual dependency scores. This set defines binary gradient mask mi,t: mi,t = I(t Ki) = (cid:26)1 0 if token is pivotal visual-reasoning token otherwise (4)"
        },
        {
            "title": "Preprint",
            "content": "This mask ensures that policy gradients are computed only for the pivotal tokens that bridge vision and language, effectively filtering out noise from generic tokens and combating signal dilution. Macro-level: Trajectory-level Advantage Shaping (TAS). Inspired by Insight 2, we prioritize learning from superior, high-dependency trajectories. We compute shaping factor α(τi) for each trajectory τi in batch by normalizing its trajectory dependency: α(τi) = βmin + (βmax βmin) S(τi) minτj S(τj) maxτj S(τj) minτj S(τj) (5) where [βmin, βmax] is scaling range. This factor rescales the original GRPO advantage, creating Shaped Advantage: ˆA(τi) = α(τi) ˆAGRPO(τi). This adaptively amplifies updates for trajectories with high visual engagement and dampens those that are less visually grounded. VPPO Objective. the shaped advantage ˆA LVPPO(θ) = Integrating these two modulations yields the final VPPO objective. It channels exclusively to the most dependent tokens via the mask mi,t:"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 mi,t min (cid:16) ri,t(θ) ˆA i, clip(ri,t(θ), 1 ε, 1 + ε) ˆA (cid:17) (6) where ˆA = α(τi) ˆAGRPO,i. The synergy between the shaping factor α(τi) and the mask mi,t provides structured, interpretable, and efficient solution to the uniform learning signal problem. detailed, step-by-step implementation of the entire training procedure is provided in Appendix B."
        },
        {
            "title": "3.3 THEORETICAL ANALYSIS",
            "content": "We provide theoretical analysis of how VPPO constructs lower-variance policy gradient estimator. Let vt = θ log πθ(otst, I) be the per-step policy gradient. The standard GRPO estimator for trajectory τ serves as our baseline: gGRPO(τ ) = ˆAGRPO(τ ) 1 (cid:88) t=0 vt (7) The VPPO estimator refines this by incorporating shaping factor α(τ ) and restricting the sum to the set of top-k% visually dependent tokens Kτ : gVPPO(τ ) = α(τ ) ˆAGRPO(τ ) (cid:88) vt (8) tKτ Theorem 3.1 (Variance Reduction). The variance of the VPPO estimator is approximately related to the GRPO estimator by the following expression: Var(gVPPO) E[α(τ )2] Var(gGRPO) (9) The full derivation, along with the underlying assumptions, is provided in Appendix C. This result reveals significant variance reduction. By design, the sparsity ratio is fraction in (0, 1), while the shaping factor α(τ ) is scaled to narrow band around 1, ensuring their product E[α(τ )2] is substantially less than 1. Therefore, our VPPO reduces variance by filtering out low-dependency gradients and regularizing update magnitudes for less visually-grounded trajectories, leading to more stable and efficient learning signal."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Models, Data, and Baselines. To have fair comparison with previous works, following Wang et al. (2025a), we apply VPPO to the Qwen2.5-VL-7B and Qwen2.5-VL-32B base models and train on the ViRL39K, diverse collection of multimodal reasoning problems. We benchmark our models against comprehensive suite of state-of-the-art, open-source reasoning LVLMs across both model scales. Our 7B comparison includes DAPO (Qwen2.5-VL-7B) (Yu et al., 2025b), MMEureka-7B (Meng et al., 2025), ThinkLite-7B (Wang et al., 2025d), VL-Rethinker-7B (Wang et al., 2025a), R1-ShareVL-7B (Yao et al., 2025), NoisyRollout-7B (Liu et al., 2025a), and PAPO-D7B (Wang et al., 2025e), while the 32B class includes MM-Eureka-32B (Meng et al., 2025) and NoisyRollout-32B (Liu et al., 2025a)."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Main Results (avg@8 acc %). All benchmarks use exact match on verifiable instances for objective results, avoiding any LLM-as-a-judge. Notably, our results are achieved via direct RL without any supervised fine-tuning. Our reproduction uses official author-provided prompts. NoisyRollout is trained using the training set of Geo3k. Model Mathematical & Geometric Logical Multidiscipline Avg. MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro MM-Eureka-7B ThinkLite-7B VL-Rethinker-7B NoisyRollout-7B R1-ShareVL-7B PAPO-D-7B Qwen2.5-VL-7B + GRPO + DAPO + VPPO MM-Eureka-32B NoisyRollout-32B Qwen2.5-VL-32B + VPPO 67.1 64.2 68.8 67.8 68.0 68. 39.0 66.5 68.3 71.6 71.8 73.0 68.5 75.1 Open-Source Models (Trained via Pure RL) 65.4 64.6 65.7 65.5 65.1 66.8 55.7 65.8 66.6 68. 72.0 72.2 68.7 73.1 67.5 62.6 68.3 50.0 70.9 80.6 42.5 72.3 82.1 82.8 40.3 37.6 40.7 51.8 41.2 44.1 37.1 40.2 41.5 46. 31.1 32.0 31.9 22.1 30.1 30.6 18.4 30.7 30.5 33.3 Scaling to Larger Models 73.4 60.2 68.8 86.3 51.0 56.6 47.0 53.4 43.2 27.9 39.3 44.6 65.5 66.5 68.9 71.0 69.9 68.3 46.4 68.1 68.0 71.5 75.0 75. 71.0 77.7 46.3 39.4 46.3 47.3 45.6 46.7 42.4 45.6 46.8 47.9 56.8 56.2 52.8 59.2 30.3 28.0 37.0 34.5 35.1 36. 25.1 35.2 35.9 37.9 43.1 43.1 39.6 47.1 51.7 49.4 53.5 51.3 53.2 55.3 38.3 53.1 55.0 57.5 60.8 58. 57.0 64.6 Training Details. Following Wang et al. (2025e), our models are trained for 2 epochs with learning rate of 1e-6 and rollout batch size of 384. We set the maximum response length to 2048 for 7B models following previous works such as R1-ShareVL, NoisyRollout, and PAPO-D, and 4096 for 32B models. To ensure training stability and enable fair comparison, small entropy penalty (coefficient 0.06) is applied to both VPPO and the baseline. More details are described in Appendix D. For VPPO, we set the gradient filtering ratio to = 0.4 and the advantage shaping range to βmin = 0.9, with βmax adjusted dynamically per batch. More hyperparameter details are available in Appendix A. Evaluation Benchmarks. We conduct comprehensive evaluation on eight diverse multimodal reasoning benchmarks. Following Wang et al. (2025e), we use an exact-match scoring methodology, eliminating reliance on LLM-as-a-judge systems. The benchmarks span mathematical, geometric, logical, and multi-discipline reasoning, including DynaMath (Zou et al., 2024), Geo3k (Lu et al., 2021), MathVerse (Zhang et al., 2024), MathVision (Wang et al., 2024a), MMK12 (Meng et al., 2025), We-Math (Qiao et al., 2024), LogicVista (Xiao et al., 2024), and MMMU-Pro (Yue et al., 2024) (see Appendix for full breakdown). We report average accuracy@8 at an inference temperature of 1.0, using single, fixed evaluation pipeline for all models to ensure fair comparison."
        },
        {
            "title": "5.1 MAIN RESULTS",
            "content": "As shown in Table 1, VPPO consistently outperforms the entire field of strong, open-source competitors across both 7B and 32B parameter classes. In the 7B class, our model achieves an average accuracy of 57.5%, significantly outperforming the next-best model PAPO. This superior perfor-"
        },
        {
            "title": "Preprint",
            "content": "mance scales directly to the 32B class, where VPPO again leads the field with an average accuracy of 64.6%. These results across different model scales demonstrate the effectiveness of our VPPO. These state-of-the-art results are underpinned by superior training dynamics, as illustrated in the training curves against the baselines (Figure 5), which demonstrates that VPPO exhibits significantly faster initial convergence, achieving higher performance more efficiently. This demonstrates that our targeted, hierarchical learning signal not only leads to better final model but also acts as potent implicit regularizer, ensuring more efficient and robust path to high performance."
        },
        {
            "title": "5.2 ABLATION STUDIES",
            "content": "Figure 5: Training dynamics for VPPO and baselines. Table 2: Ablation of Trajectory-level Advantage Shaping (TAS) and Token-level Gradient Filtering (TGF). Their combination yields the best results, confirming the efficacy of our hierarchical design. Model Configuration MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. Baseline (DAPO) + TAS only + TGF only VPPO (TAS + TGF) 68.3 70.4 71. 71.6 66.6 67.5 68.6 68.1 82.1 83.3 80.9 41.5 43.5 45.3 82. 46.5 30.5 31.3 34.7 33.3 68.0 69.3 70.3 71.5 46.8 47.4 48. 47.9 35.9 37.3 37.3 37.9 55.0 56.3 57.1 57.5 Ablation Study on VPPO Components. We first analyze the effectiveness of our two primary mechanisms: Trajectory-level Advantage Shaping (TAS) and Token-level Gradient Filtering (TGF). As shown in Table 2, both components individually outperform the baseline. TGF provides the largest single contribution, highlighting the importance of directing the learning signal to pivotal tokens. However, the combination of both mechanisms in the full VPPO model achieves optimal performance, confirming the synergistic value of our hierarchical design. Sensitivity to Gradient Filtering Ratio k. We investigate how performance varies with the token filtering ratio in TGF. As shown in Figure 6, performance peaks around = 0.4. This highlights crucial trade-off: that is too low provides insufficient learning signal, while that is too high reintroduces noise from non-pivotal tokens, validating our sparse update strategy. Sensitivity to Advantage Shaping Range. We analyze the sensitivity of our model to the TAS scaling range [βmin, βmax]. The results in Table 3 reveal that conservative lower bound with dynamic upper bound (βmin = 0.9, βmax = Dyn.) is the optimal configuration. This setting adaptively reweights the advantage based on the dependency distribution within each batch, preventing overly aggressive updates while still rewarding visually-grounded reasoning. Figure 6: Ablation on the gradient filtering ratio (k). The line shows the average score, while bars show per-benchmark deviation from their mean. Validation of the dependency Calculation Method. To further validate the robustness of our core visual dependency metric, we conducted two additional, detailed ablation studies presented in the appendix. The first study (Appendix E) evaluates our choice of image perturbation strategy against several alternatives. The second (Appendix F) compares our KL-divergence metric against other computationally-feasible calculation heuristics. Superiority over Entropy-based Token Section. As depicted in Table 4, we compare different methods for selecting pivotal tokens in multimodal reasoning. For text-only LLMs, high-entropy"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Ablation study on the scaling range [βmin, βmax] for Trajectory-level Advantage Shaping (TAS), including both fixed and dynamic (Dyn.) configurations. TAS Configuration MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. Baseline (DAPO) βmin = 0.8, βmax = 1.2 βmin = 0.8, βmax = Dyn. βmin = 0.9, βmax = 1.1 68. 68.7 69.8 69.1 βmin = 0.9, βmax = Dyn. 70.4 66.6 67.5 67.6 67.6 67. 82.1 41.5 82.9 82.6 82.6 43.4 43.1 43.2 83.3 43. 30.5 31.9 31.5 31.5 31.3 68.0 69.4 70.3 69.2 69. 46.8 46.5 47.1 46.6 47.4 35.9 55.0 36.7 37.3 37. 55.9 56.2 55.9 37.3 56.3 Table 4: Performance comparison of Token-level Gradient Filtering (TGF) under three guidance signals: visual dependency (our method), predictive entropy, and random selection. Guidance Mechanism MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. Baseline (DAPO) + Random (k = 0.4) + Entropy (k = 0.2) + Entropy (k = 0.4) + Entropy (k = 0.6) + Entropy (k = 0.8) Our TGF (k = 0.4) 68.3 69.3 70.1 69.3 69.9 69.6 71.2 66. 66.2 67.2 67.6 67.4 66.9 68.6 82.1 41.5 76.8 77.9 80.0 81.0 81.1 42.0 45.0 42.8 43.4 41. 80.9 45.3 30.5 31.0 32.6 31.7 31.4 31.2 34.7 68. 69.3 70.6 69.4 69.1 69.0 70.3 46.8 47.5 48.0 47.4 47.1 46.6 48.2 35. 36.2 36.4 37.0 36.9 36.2 37.3 55.0 54.8 56.0 55.7 55.8 55.3 57.1 forking tokens is an effective optimization strategy (Wang et al., 2025c). However, this strategy can not achieve significant improvement in multimodal tasks across different settings. The reason is pivotal moment of visually-grounded reasoning tokens may correspond to low-entropy prediction. In contrast, our token dependency accurately identifies these visually-grounded tokens and achieves better performance for multimodal reasoning."
        },
        {
            "title": "5.3 QUALITATIVE ANALYSIS",
            "content": "Figure 7: The top 40% most visually-dependent tokens are highlighted in purple, forming the core reasoning chain targeted by our gradient filtering mechanism. To further understand the token perception, we provide qualitative analysis in Figure 7. As shown in this figure, high dependency is assigned to foundational concepts like circumference and the angle value 25. The dependency then correctly propagates to intermediate conceptual entities (triangle OAB, arc) and, crucially, to the logical syntax that structures the proof (Since, Therefore). This demonstrates sophisticated understanding that captures not only what concepts are important but how they are linked to form coherent proof."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we identify the uniform learning signal as core bottleneck in multimodal reasoning and introduce Visually-Perceptive Policy Optimization (VPPO) as principled solution. By implementing novel, two-tiered strategy, VPPO first prioritizes visually-grounded trajectories through reward shaping and then focuses policy updates exclusively on sparse set of pivotal perception tokens. This hierarchical signal modulation not only establishes new state-of-the-art across diverse suite of challenging benchmarks but also fosters greater training stability and efficiency. Our work demonstrates that for complex multimodal tasks, the structure of the learning signal is as important as the reward itself. We believe that this principle of targeted, modality-aware signal modulation offers promising and robust path forward for advancing the reasoning capabilities of Large VisionLanguage Models."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude sonnet 4, 2025. URL https://www.anthropic.com/claude/sonnet. Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025c. Liang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, and Baobao Chang. G1: Bootstrapping perception and reasoning abilities of vision-language model via reinforcement learning. arXiv preprint arXiv:2505.13426, 2025a. Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. Vinci. r1-v: Reinforcing super generalization ability in vision-language models with less than $3, 2025b. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025c. Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. GrpoarXiv preprint care: Consistency-aware reinforcement learning for multimodal reasoning. arXiv:2506.16141, 2025d. Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 90629072, 2025. Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, and Yu Cheng. Framethinker: Learning to think with long videos via multi-turn frame spotlighting. arXiv preprint arXiv:2509.24304, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025."
        },
        {
            "title": "Preprint",
            "content": "Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, and Xing Xu. Truth in the few: High-value data selection for efficient multi-modal reasoning. arXiv preprint arXiv:2506.04755, 2025. Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, and Jiacheng Zhu. Modomodo: Multi-domain data mixtures for multimodal llm reinforcement learning. arXiv preprint arXiv:2505.24871, 2025. Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances llms reasoning capability. arXiv preprint arXiv:2411.19943, 2024. Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025a. Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025b. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, and Junjie Yan. One rl to see them all: Visual triple unified reinforcement learning. arXiv preprint arXiv:2505.18129, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. Xiaoye Qu, Qiyuan Chen, Wei Wei, Jiashuo Sun, Daizong Liu, and Jianfeng Dong. Alleviating hallucination in large vision-language models with active retrieval augmentation. ACM Transactions on Multimedia Computing, Communications and Applications, 2024. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Chuming Shen, Wei Wei, Xiaoye Qu, and Yu Cheng. Satori-r1: Incentivizing multimodal reasoning with spatial grounding and verifiable rewards. arXiv preprint arXiv:2505.19094, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025b. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025a. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025b. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Jean Vassoyan, Nathanaël Beau, and Roman Plaud. Ignore the kl penalty! boosting exploration on critical tokens to enhance rl fine-tuning. arXiv preprint arXiv:2502.06533, 2025. Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025b. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025c. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024b. Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025d. Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025e. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022."
        },
        {
            "title": "Preprint",
            "content": "Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, et al. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. arXiv preprint arXiv:2507.05255, 2025. Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025. Tong Xiao, Xin Xu, Zhenya Huang, Hongyu Gao, Quan Liu, Qi Liu, and Enhong Chen. Advancing multimodal reasoning capabilities of multimodal large language models via visual perception reward. arXiv preprint arXiv:2506.07218, 2025. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b. Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, et al. R1-sharevl: Incentivizing reasoning capability of multimodal large language models via share-grpo. arXiv preprint arXiv:2505.16673, 2025. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/ hiyouga/EasyR1, 2025a. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025b. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        },
        {
            "title": "C Proofs for Theoretical Analysis",
            "content": "C.1 Formal Setup and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of Theorem 3.1 (Variance Reduction) . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "K Prompt Template",
            "content": "L Qualitative Case Studies: VPPO vs. Baseline"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "14 15 15 16 17 20 21 22 23 23 25 25 Overall Setup. Our implementation is built upon the EasyR1 framework (Zheng et al., 2025a; Sheng et al., 2024). All experiments were conducted using PyTorch 2.6.0 with CUDA 12.4. The base models for our experiments are the open-source Qwen2.5-VL-7B and Qwen2.5-VL-32B. Training Details. We train all models for two epochs on the ViRL39K dataset (Wang et al., 2025a). The vision tower is unfrozen during training. For the online RL process, we generate 8 responses per question. Our reward signal is simple binary accuracy score (1 for correct, 0 for incorrect). Our training objective follows the DAPO recipe, incorporating dynamic sampling, clip-higher, and token-level policy gradient loss, without KL divergence penalty. All key hyperparameters for the optimizer, RL process, and evaluation are detailed in Table 5. VPPO Configuration. Our proposed VPPO method introduces two key mechanisms, Trajectorylevel Advantage Shaping (TAS) and Token-level Gradient Filtering (TGF), whose specific hyperparameters are detailed in Table 5. The underlying visual dependency metric that guides these mechanisms was also carefully selected. As detailed in our ablation studies, the final VPPO configuration uses the following validated components: Dependency Calculation: Visual dependency is calculated using KL Divergence, which we found to be empirically superior to other heuristics (see Appendix F). This is imple-"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Key hyperparameters for training and evaluation. Hyperparameter Value General Training Optimizer Learning Rate LR Schedule Epochs Freeze Vision Tower RL Process Global Batch Size Rollout Batch Size Rollouts per Prompt Rollout Top-p Max Response Length Reward Signal DAPO Recipe Sampling Method Clip Ratio Low Clip Ratio High Loss Averaging Mode KL Penalty VPPO Specific TAS βmin TAS βmax TGF Ratio (k) AdamW 1e-6 Constant (no warmup or decay) 2 False 128 384 8 0.99 2048 (7B), 4096 (32B) Binary Accuracy (1/0) Dynamic Sampling 0.2 0.28 Token-level None 0.9 Dynamical (batch-normalized) 0.4 Evaluation Generation Temperature Top-p Max New Tokens 1.0 1.0 2048 (7B), 4096 (32B) mented with the efficient low_var_kl estimation function provided by the EasyR1 framework. Masking Strategy: We use Random Patch Blackening as the image perturbation method, which was validated as the most effective strategy in Appendix E. The image is divided into non-overlapping patches of size 14x14, and each patch is independently set to black with probability of 0.5. Computational Resources. The 7B model was trained on 8 NVIDIA H800 80GB GPUs, and the 32B model on 32, respectively."
        },
        {
            "title": "B TRAINING PROCEDURE",
            "content": "For clarity and reproducibility, we provide detailed, step-by-step description of our VisuallyPerceptive Policy Optimization (VPPO) training procedure in Algorithm 1. This pseudocode elaborates on the high-level methodology presented in Section 3.2 of the main text. It details the four core phases of each training step: (1) data generation via rollouts, (2) the calculation of token-level visual dependency, (3) our hierarchical signal modulation, and finally, (4) the policy update using the modulated learning signal."
        },
        {
            "title": "C PROOFS FOR THEORETICAL ANALYSIS",
            "content": "This section provides the detailed derivations for the theorems presented in Section 3.3."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 The Visually-Perceptive Policy Optimization (VPPO) Algorithm 1: Input: Current policy πθ, old policy πθold , batch of prompts = {(Ij, qj)}B 2: Hyperparameters: Group size G, saliency filtering ratio k, shaping range [βmin, βmax] 3: procedure VPPO_TRAINING_STEP(πθ, πθold , D) 4: Initialize lists for trajectories [], original distributions [] j=1 Phase 1: Data Generation (Rollouts) for each prompt (I, q) in do for = 1 to do Generate trajectory τi = (o1, ..., oT ) using πθold (I, q) Store original distributions Pi = {πθold(st, I)}T Append τi to and Pi to t=1 end for end for Initialize list for saliency scores [] for each trajectory τi and its distributions Pi in (T , P) do Phase 2: Saliency Calculation Let (I, q) be the prompt for τi Create masked image MaskingStrategy(I) Compute masked distributions Initialize token saliency scores Si [] for = 1 to do = {πθold (st, )}T t=1 Si,t DKL(Pi,t Append Si,t to Si i,t) end for Append Si to end for Phase 3: Hierarchical Signal Modulation Compute rewards {Ri}T Initialize lists for shaped advantages ˆA [] and masks [] for each trajectory τi and its saliency scores Si in (T , S) do i=1 and standard advantages { ˆAi}T i=1 (cid:80)T t=1 Si,t Si 1 αi Normalize( Si, within batch, [βmin, βmax]) Append αi ˆAi to ˆA Macro-level Advantage Shaping Micro-level Gradient Filtering Ki Indices of top values in Si Append (I(t Ki))T t=1 to end for 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: Compute loss LVPPO(θ) using , ˆA, and per Eq. (6) Update policy parameters: θ OptimizerStep(θLVPPO(θ)) 35: 36: 37: end procedure Phase 4: Policy Update C.1 FORMAL SETUP AND ASSUMPTIONS Let vt = θ log πθ(otst, I) denote the score function, or the per-step policy gradient, at timestep t. The proofs rely on the following standard assumptions. Assumption 1 (Uncorrelated Gradients). The per-step gradients within trajectory are approximately uncorrelated. Formally, for = j, E[vT vj] 0. This is common assumption in policy gradient analysis, as gradients at different timesteps are often driven by different and nearly independent states. Assumption 2 (Advantage Independence). The trajectory-level advantage, ˆAGRPO(τ ), is treated as random variable that is independent of the per-step gradients, vt. This is justified as the advantage is scalar value computed over the entire trajectorys outcome, while the gradients are high-dimensional vectors dependent on specific states."
        },
        {
            "title": "Preprint",
            "content": "Assumption 3 (dependency-Advantage Independence). For the purpose of this analysis, we assume the trajectory shaping factor α(τ ) and the advantage ˆAGRPO(τ ) are uncorrelated. This simplification allows us to isolate the distinct variance reduction effects of trajectory-level advantage shaping and token-level gradient filtering. Assumption 4 (Second-Moment Dominance). In high-dimensional optimization, the variance of the gradient estimator, Var(g) = E[g2] E[g]2, is dominated by the second moment, E[g2]. This is because for well-behaved optimization, the expected gradient E[g]2 is typically much smaller than the expectation of the squared norm. Therefore, we analyze the variance by comparing the second moments: Var(g) E[g2]. C.2 PROOF OF THEOREM 3.1 (VARIANCE REDUCTION) Theorem C.1. Under Assumptions 1-4, the variance of the VPPO gradient estimator is reduced by factor of approximately E[α(τ )2] compared to the GRPO estimator. Proof. We will derive and compare the second moments of the GRPO and VPPO gradient estimators. 1. Second Moment of the GRPO Estimator. First, we analyze the GRPO estimator, gGRPO(τ ) = ˆAGRPO(τ ) (cid:80)T t=0 vt. E[gGRPO2] = (cid:13) (cid:13) ˆAGRPO(τ ) (cid:13) (cid:13) (cid:13) 1 (cid:88) t=0 vt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 ˆAGRPO(τ ) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) t=0 vt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Assumption 2 = E[ ˆAGRPO(τ )2] 2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vt 1 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) t=0 (cid:33)T 1 (cid:88) vj (cid:32)T 1 (cid:88) vt t=0 j= 1 (cid:88) t=0 vt2 + vT vj (cid:88) t=j E[vT vj] E[vt2] + (cid:88) t=j 1 (cid:88) t=0 E[vt2] = E[ ˆAGRPO(τ )2] = E[ ˆAGRPO(τ )2] = E[ ˆAGRPO(τ )2] 1 (cid:88) t=0 Assumption 1 E[ ˆAGRPO(τ )2]"
        },
        {
            "title": "Preprint",
            "content": "2. Second Moment of the VPPO Estimator. Next, we perform the same derivation for the VPPO estimator, gVPPO(τ ) = α(τ ) ˆAGRPO(τ ) (cid:80) vt. tKτ E[gVPPO2] = = (cid:13) (cid:13) α(τ ) ˆAGRPO(τ ) (cid:13) (cid:13) (cid:13) α(τ )2 ˆAGRPO(τ )2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vt (cid:88) vt tKτ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) tKτ Assumption 2 = E[α(τ )2 ˆAGRPO(τ )2] (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) vt tKτ 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Assumption 3 = E[α(τ )2]E[ ˆAGRPO(τ )2] (cid:88) vt2 + (cid:88) tKτ t,jKτ ,t=j Assumption 1 E[α(τ )2]E[ ˆAGRPO(τ )2] E[vt2] (cid:88) tKτ vT vj 3. Comparison and Conclusion. Assuming the expected norm of the per-step gradients is roughly constant across timesteps, E[vt2] C, the summations for the GRPO and VPPO estimators simplify. The GRPO sum runs over all timesteps, while the VPPO sum runs only over the set of pivotal tokens, Kτ , where Kτ = . This yields: E[gGRPO2] E[ ˆAGRPO(τ )2] E[gVPPO2] (k ) E[α(τ )2]E[ ˆAGRPO(τ )2] By taking the ratio and applying Assumption 4, we arrive at the relationship shown in the main text: Var(gVPPO) E[gVPPO2] E[α(τ )2] E[gGRPO2] E[α(τ )2] Var(gGRPO) (10) This demonstrates direct reduction in variance proportional to the sparsity ratio and the expected squared shaping factor, which leads to more stable training."
        },
        {
            "title": "D THE ROLE OF THE ENTROPY PENALTY IN STABILIZING TRAINING",
            "content": "In our main experimental setup, small entropy penalty is added to the loss function. This section provides detailed analysis of why this regularization is critical component for achieving stable training with online RL in the context of LVLMs. Figure 8: Catastrophic policy collapse in the DAPO baseline when trained without regularization. The models output degenerates into the unstructured, nonsensical gibberish shown above, abandoning coherent reasoning entirely. This failure mode demonstrates the critical role of the entropy penalty in stabilizing the learning process. The Phenomenon of Policy Collapse. During our initial experiments, we observed that the DAPO baseline, when trained without any regularization, quickly fell into catastrophic failure mode. After brief period of exploration, its policy would collapse, causing the model to generate incoherent"
        },
        {
            "title": "Preprint",
            "content": "gibberish (Figure 8), sequences of tokens that were not only nonsensical but often appeared as random, unformatted strings with no resemblance to valid language. This is severe form of the well-known RL phenomenon known as policy collapse, where the model forgoes meaningful reasoning entirely in favor of an exploit, however nonsensical, that it has correlated with positive reward. Sparse, Coarse-Grained Rewards. This collapse is direct consequence of the sparse and coarse-grained nature of the reward signal in the RLVR framework. The model receives single binary reward for an entire, often lengthy, trajectory. This incentivizes the optimizer to find any shortcut or exploit that correlates with positive reward, regardless of whether it constitutes genuine reasoning. If random, nonsensical sequence happens to produce the correct final answer by chance, the uniform learning signal of DAPO strongly reinforces every token in that flawed sequence. Without counteracting force, the optimizer can rapidly converge on this suboptimal, degenerate policy because its deceptively easy way to secure reward. (a) Training Accuracy Dynamics (b) Policy Entropy Dynamics Figure 9: Effect of the entropy penalty coefficient (λ) on training dynamics. (a) Training accuracy versus training steps. The unregularized baseline (λ = 0.00) suffers from sharp performance collapse, while our chosen coefficient of λ = 0.06 achieves the highest and most stable accuracy. (b) Policy entropy versus training steps. The accuracy collapse in (a) is shown to be direct result of uncontrolled entropy divergence when no penalty is applied. The penalty successfully regularizes the policy, preventing this failure mode. The Entropy Penalty as Regularizer. The entropy penalty serves as an essential stabilizing force. We empirically observed that policy collapse in our setup is consistently accompanied by sharp and uncontrolled increase in policy entropy. This pathological state occurs when the sparse reward fails to guide the optimizer, which can then push the policy into chaotic regime that manifests as incoherent gibberish. To counteract this and determine the optimal setting, we performed an ablation study on the entropy penalty coefficient. Figure 9 visualizes the direct impact of this penalty on the training dynamics, showing how the policy entropy diverges and training accuracy collapses without regularization. The final performance for each setting is presented in Table 6. The combined results demonstrate that the penalty is critical for preventing this failure mode. We found that coefficient of 0.06 strikes the best empirical balance, achieving the highest and most stable training accuracy by keeping exploration within the bounds of coherent language generation. Implications for VPPO. To ensure fair and controlled comparison, we apply the same entropy penalty (with coefficient of 0.06) to both the DAPO baseline and our VPPO method. This addition is primarily to stabilize the baseline, allowing for direct and meaningful performance comparison. Within the standard two-epoch training regime, this penalty successfully prevents the baselines immediate policy collapse. By focusing updates on sparse, meaningful set of pivotal tokens, VPPO is inherently more robust to the noisy, uniform rewards that destabilize the baseline, underscoring the profound stability benefits of our hierarchical signal modulation."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Ablation study on the entropy penalty coefficient for the DAPO baseline. We compare the performance of the baseline under different entropy penalty settings. While training without penalty (0.0) is possible, it results in extremely low performance due to policy instability. coefficient of 0.06 is shown to be crucial for achieving stable and effective training. Entropy Penalty MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. 0.0 (No Penalty) 0.02 0.04 0.06 (Default) 0.08 60.2 66.2 68.3 68.3 69. 61.3 64.6 64.7 66.6 66 79. 80.2 80.9 82.1 81.2 33.8 39. 42.2 41.5 42.9 26.0 28.0 29. 30.5 31.1 59.8 65.9 67.9 68. 67.8 38.4 42.8 46.0 46.8 32.8 34.1 35.1 35.9 35.4 49. 52.7 54.3 55.0 55."
        },
        {
            "title": "CALCULATION",
            "content": "In our main paper, the calculation of visual dependency, S(st, I) := DKL (πθ(st, I) πθ(st, )), relies on perturbed, non-informative image . The choice of this perturbation method is key hyperparameter that can influence which tokens are identified as dependent. To validate our choice, we conduct an ablation study comparing our default strategy against several common alternatives. (a) Original (b) Patch Blacken (c) Gauss Noise (d) Gauss Blur (e) Complete Mask Figure 10: Visual examples of the masking strategies for dependency calculation. Panel (a) shows the original, unperturbed image. Panels (b)-(e) illustrate the effect of the different image perturbation methods evaluated in our ablation study, corresponding to the methods tested in Table 7. These different perturbation methods are visualized in Figure 10. The specific strategies evaluated are as follows: Random Patch Blackening (Our Default): This is the strategy used for all main results. Following the ViT architecture of our base model, the image is divided into patches of size 14x14. Each patch is then independently dropped (set to black) with probability of 0.5. Additive Gaussian Noise: Gaussian noise with standard deviation of 189 is added to each pixel value in the image. This value was calibrated such that pixel has approximately 50% chance of being saturated to its maximum or minimum value, effectively losing its original information. Gaussian Blur: Gaussian blur with radius of 6.0 is applied to the entire image, degrading fine-grained details. Complete Masking: The entire image is replaced with solid, neutral grey canvas (RGB value 128, 128, 128), removing all visual information. For each strategy, we trained our model using the same hyperparameters and evaluated its performance. The results are presented in Table 7. Analysis of Results. The results in Table 7 confirm that our default strategy, Random Patch Blackening, achieves the best overall performance with an average accuracy of 57.5%. It demonstrates consistent, albeit modest, advantage over Additive Gaussian Noise (56.5%), Gaussian Blur (56.4%), and Complete Masking (56.4%)."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Ablation study on the masking strategy for visual dependency calculation. We compare the impact of different image perturbation methods on final model performance. The results validate our choice of Random Patch Blackening as the most effective strategy. Masking Strategy MathVerse DynaMath MMKK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. Random Patch Blackening Additive Gaussian Noise Gaussian Blur Complete Masking 71.6 70.2 69.1 71.0 68. 67.7 68.2 68.1 82.8 82.3 82. 82.1 46.5 43.9 45.4 43.3 33. 32.9 32.5 32.8 71.5 69.8 70. 69.0 47.9 47.0 46.9 47.0 37. 38.0 37.0 37.9 57.5 56.5 56. 56.4 We hypothesize that this strategys effectiveness stems from its patch-based nature, which aligns with the models underlying ViT architecture. By removing entire, discrete patches of the image, this method forces the model to perform more robust, localized reasoning from incomplete visual evidence. This is more challenging and informative task than reasoning from globally degraded gist of the image, as might be the case with noise or blur. Interestingly, Complete Masking also performs competitively, suggesting that significant portion of the dependency signal is captured by the stark contrast between the presence and complete absence of visual information. However, the consistent edge of Random Patch Blackening indicates that forcing the model to reason with partial visual context provides more effective and nuanced signal for identifying pivotal tokens. These findings validate our choice of using Random Patch Blackening as the default perturbation method for all experiments in the main paper."
        },
        {
            "title": "F ABLATION STUDY ON METHODS FOR DEPENDENCY CALCULATION",
            "content": "Our proposed method relies on quantifying visual dependency by measuring the KL divergence between the policys full output distributions, πθ(st, I) and πθ(st, ). While principled, this is not the only way to measure the influence of visual input. To validate our choice, we conduct an ablation study comparing our default method against other computationally-feasible, alternative token-scoring heuristics. The methods evaluated are as follows: KL Divergence (Our Default): This is the strategy used for all main results. It measures the total change across the entire vocabulary distribution. Our implementation uses memory-efficient estimation of the true KL value. SKL(st, I) = DKL (πθ(st, I) πθ(st, )) Jensen-Shannon Divergence (JSD): This method is symmetrized and smoothed version of KL divergence. It is implemented using the same memory-efficient estimation technique, testing whether symmetric distance metric is more effective than the asymmetric information gain measured by KL. SJSD(st, I) = DJS (π(st, I) π(st, )) Top-1 Probability Drop: This simple heuristic measures only the change in probability for the token ot that was actually sampled, testing how much the image boosts the confidence of the final choice. STop-1(st, I) = πθ(otst, I) πθ(otst, ) For each strategy, we trained our model using the same hyperparameters and evaluated its performance. The results are presented in Table 8. The most significant finding is the substantial underperformance of the Top-1 Probability Drop heuristic, which lags behind our default method by 7.2% in average accuracy. This demonstrates that simple heuristic focused only on the single sampled token is an insufficient proxy for visual reliance. It captures only fraction of the total change and is blind to significant shifts happening"
        },
        {
            "title": "Preprint",
            "content": "Table 8: Ablation study on the method for dependency calculation. We compare the impact of different computationally-feasible token-scoring heuristics on final model performance. The results validate our choice of using KL Divergence as the most effective method for quantifying visual dependency. Guidance Metric MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. KL Divergence (Default) JS Divergence Top-1 Probability Drop 71.6 71.8 61.5 68.1 67.6 64. 82.8 82.7 74.9 46.5 45.1 31. 33.3 32.6 30.1 71.5 70.8 62. 47.9 47.8 44.7 37.9 36.9 33. 57.5 56.9 50.3 elsewhere in the output distribution, such as when the visual input dramatically alters the ranking of the next most likely candidates. In contrast, Jensen-Shannon Divergence (JSD) performs very competitively, achieving result only 0.6% below our default. This is expected, as both KL and JS Divergence are principled, fulldistribution metrics that measure the overall change between the two output distributions. However, the slight but consistent advantage of KL Divergence is theoretically significant. KL Divergence is an asymmetric measure of information gain, while JSD is symmetric distance metric. The core motivation of our work is to specifically measure the information gain provided by the visual input to guide the policy. Therefore, KL Divergence is the more theoretically aligned choice. The empirical results, validating that this principled selection also yields the best performance, confirm its superiority for this task."
        },
        {
            "title": "G ABLATION STUDY ON ROLLOUT GROUP SIZE",
            "content": "The number of rollouts per prompt, or the group size (G), is critical hyperparameter in online RL algorithms like VPPO. It directly influences the trade-off between the quality of the advantage estimation and the computational cost of data generation. larger group size provides more stable and accurate estimate of the expected reward, but at the cost of increased computation. To validate our choice and explore this trade-off, we conduct an ablation study on the rollout group size. Our main experiments use default setting of = 8. We evaluate this against smaller group size of = 5 (chosen to ensure fair comparison with the experimental setup of PAPO-D (Wang et al., 2025e)) and larger group sizes of = 12 and = 16 to assess potential performance gains from more extensive sampling. The results, presented in Table 9, show the impact of this hyperparameter on final model performance. Table 9: Ablation study on the number of rollouts per prompt (G). We compare model performance across different group sizes. The results validate our choice of = 8 as providing strong balance between advantage estimation quality and computational efficiency. The = 5 setting is included for direct comparison with the methodology of PAPO-D. Rollout Group Size (G) MathVerse DynaMath MMK12 Geo3k MathVision We-Math LogicVista MMMU-Pro Avg. = 5 = 8 (Default) = 12 = 16 70.7 71.6 71.3 72. 68.2 68.1 68.1 68.4 80.7 82. 83.5 84.2 44.8 46.5 46.9 46. 32.9 33.3 32.9 33.2 69.5 71. 70.2 71.1 48.4 47.9 48.3 48. 36.8 37.9 37.8 37.0 56.5 57. 57.4 57.7 The results in Table 9 reveal clear trend of diminishing returns as the group size increases. Increasing the group size from = 5 to our default of = 8 yields substantial performance gain of 1.0% on average, demonstrating the value of more stable advantage estimate. However, further increases in group size offer minimal additional benefit. Increasing the rollouts by 50% to = 12 results in 0.1% decrease in average performance, while doubling the rollouts to"
        },
        {
            "title": "Preprint",
            "content": "G = 16 provides only marginal 0.2% improvement over our default setting. Given that the computational cost of the rollout phase scales linearly with the group size, doubling the work for such small gain is not an efficient trade-off. This analysis confirms that our default setting of = 8 strikes an optimal balance between the quality of the advantage estimation and computational efficiency, capturing the vast majority of the potential performance gains without incurring unnecessary computational expense."
        },
        {
            "title": "H LIMITATIONS",
            "content": "While our results demonstrate the effectiveness of VPPO, it is important to acknowledge its current limitations and outline avenues for future research. Computational Overhead. Our method introduces modest and fully manageable computational overhead. To compute the KL divergence, VPPO requires second forward pass through the model using perturbed (masked) visual input during the rollout phase. Empirically, we found this resulted in only minor increase in total training time (approximately 10% increase, from 15.5 to 17 hours on our 7B setup). Given the significant gains in final performance and training stability, we believe this minor additional cost represents highly favorable and practical trade-off. However, exploring even more efficient, single-pass approximations of visual dependency remains an interesting direction for future research. Scope of Generalization. Our experiments have demonstrated the effectiveness of VPPO on models up to the 32B parameter scale. While the strong results on both 7B and 32B models suggest positive scaling trend, the efficacy of our method on extremely large-scale models (e.g., 72B+ parameters) has not yet been verified. Such models may exhibit different emergent properties, and further research is needed to confirm if our hierarchical modulation remains optimal at that scale. Furthermore, the benefits of VPPO were demonstrated on reasoning-intensive benchmarks (e.g., math, geometry, logic). Its applicability to more subjective or creative tasks, such as detailed image captioning or visual storytelling, where the notion of single visually-grounded reasoning chain is less clear, remains an open question. Methodological Assumptions and Hyperparameters. The dependency calculation at the core of VPPO is contingent on the choice of image perturbation method. Our ablation study (Appendix E) validates our choice of Random Patch Blackening, but it is plausible that the optimal masking strategy is taskor domain-dependent. Similarly, while our ablations (Subsection 5.2) identified optimal values for the key hyperparameters, i.e. the filtering ratio and the shaping range [βmin, βmax], these values were determined on our specific training dataset and may require re-tuning when applying VPPO to new datasets or model scales to achieve maximum performance."
        },
        {
            "title": "I ANALYSIS OF THE TRAINING DATASET",
            "content": "This section provides further details on the ViRL39K dataset (Wang et al., 2025a), which serves as the foundation for our reinforcement learning experiments. The choice of this dataset was deliberate, as its core properties align perfectly with the requirements for training robust multimodal reasoning model. Topical Diversity and Reasoning Depth. primary strength of ViRL39K is its broad topical diversity. The dataset is not confined to single domain but instead contains approximately 39,000 queries spanning wide range of challenging subjects, including mathematics, physics, chemistry, biology, and chart interpretation. This diversity is crucial for training general-purpose reasoning model, as it prevents overfitting to narrow task distribution and encourages the development of more fundamental, transferable reasoning skills. Suitability for Reinforcement Learning. The most critical feature of ViRL39K for our study is its verifiability. Every instance in the dataset is programmatically generated and comes with definitive, unambiguous ground-truth answer. This property is indispensable for any RLVR framework, as it allows for the implementation of clean, reliable, and automated reward function. By enabling"
        },
        {
            "title": "Preprint",
            "content": "a simple binary accuracy signal, it removes any need for subjective, model-based judges and ensures that the learning process is guided by objective correctness. For comprehensive overview of the datasets construction process and statistical breakdown, we refer the reader to the original publication."
        },
        {
            "title": "J ANALYSIS OF EVALUATION BENCHMARKS",
            "content": "This section provides brief analysis of the eight benchmarks used in our main evaluation. We deliberately selected this suite to cover wide spectrum of challenges, from domain-specific mathematical skills to general logical cognition, ensuring holistic assessment of our models capabilities. Mathematical and Geometric Reasoning. This category forms the core of our evaluation, testing deep, domain-specific skills. DynaMath (Zou et al., 2024) is unique benchmark designed to test the robustness of visual mathematical reasoning. Instead of using static set of questions, it employs programbased generation to create numerous variants of seed problems, systematically altering numerical values and function graphs to challenge models ability to generalize rather than memorize. Geo3k (Lu et al., 2021) is large-scale benchmark focused on high-school level geometry. Its key feature is the dense annotation of problems in formal language, making it particularly well-suited for evaluating interpretable, symbolic reasoning approaches. MathVerse (Zhang et al., 2024) is specifically designed to answer the question: Do MLLMs truly see the diagrams? It tackles the problem of textual redundancy by providing six distinct versions of each problem, systematically shifting information from the text to the diagram. This allows for fine-grained analysis of models reliance on visual versus textual cues. MATH-Vision (Wang et al., 2024a) elevates the difficulty by sourcing its problems from real math competitions (e.g., AMC, Math Kangaroo). Spanning 16 mathematical disciplines and 5 difficulty levels, it provides challenging testbed for evaluating advanced, competition-level multimodal reasoning. MMK12 (Meng et al., 2025) is benchmark focused on K-12 level multimodal mathematical problems. It provides strong test of foundational math reasoning skills that are essential for more advanced applications. We-Math (Qiao et al., 2024) introduces novel, human-centric evaluation paradigm. It assesses reasoning by decomposing composite problems into sub-problems based on hierarchy of 67 knowledge concepts. This allows for fine-grained diagnosis of models specific strengths and weaknesses, distinguishing insufficient knowledge from failures in generalization. Logical Reasoning. To assess more general cognitive abilities, we include dedicated logical reasoning benchmark. LogicVista (Xiao et al., 2024) is designed to fill critical gap by evaluating general logical cognition beyond the mathematical domain. It covers five core reasoning skills (inductive, deductive, numerical, spatial, and mechanical) across variety of visual formats, testing the fundamental reasoning capabilities that underlie many complex tasks. Multi-discipline Reasoning. Finally, to test performance on challenging, college-level problems that require true multimodal integration, we use robust version of well-known benchmark. MMMU-Pro (Yue et al., 2024) is hardened version of the popular MMMU benchmark. It was specifically created to be unsolvable by text-only models by filtering out questions with textual shortcuts, augmenting the number of choices to reduce guessing, and introducing vision-only format. It serves as strong test of models ability to seamlessly integrate visual and textual information in high-stakes, academic context."
        },
        {
            "title": "K PROMPT TEMPLATE",
            "content": "For all training and evaluation experiments, we used the single, standardized prompt template shown below. Its structured format is designed to elicit consistent Chain-of-Thought (CoT) response, which is crucial for the automated parsing of final answers."
        },
        {
            "title": "Reasoning Template",
            "content": "SYSTEM: You are helpful assistant. USER: {question} You first think through the reasoning process as an internal monologue, enclosed within <think> </think> tags. Then, provide your final answer enclosed within boxed{}. QUALITATIVE CASE STUDIES: VPPO VS. BASELINE To provide more intuitive understanding of how VPPO improves reasoning performance, this section presents qualitative analysis of three representative examples. For each case, we contrast the failure mode of the baseline with the correct reasoning process of our VPPO-7B model on the exact same problem. Notably, for each of these examples, our VPPO-7B model produced the correct answer on all eight of its generation passes, demonstrating the stability and robustness of its learned reasoning process. These case studies are specifically chosen to highlight the practical impact of our hierarchical signal modulation. They illustrate how the baselines uniform learning signal can lead to critical errors in visual perception or logical deduction, while VPPOs targeted approach successfully and consistently guides the model to the correct solution by focusing on pivotal, visually-grounded steps."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 1 by ThinkLite-VL-7B Question: As shown in the figure, square DEFG model should be cut on piece of triangle ABC paper. Among them, and are on BC, and are on AB and AC respectively, AH perpendicular BC and it intersects DE at M, if BC = 12.0, AH = 8.0, then the edge length of the square DEFG is () Choices: (A) 24 Ground truth: (A) 24 <think> To solve for the edge length of the square DEFG, we need to leverage the properties of similar triangles and the given information. 5 cm (B) 4cm (C) 24 7 cm (D) 5cm 5 cm Identify the similarity of triangles: Since DEFG is square inscribed in the triangle 1. ABC, ADE and ABC are similar by AA similarity (Angle-Angle similarity) criterion. This is because ADE = ABC and DAE = BAC. 2. Use the similarity ratio: The ratio of the corresponding sides of similar triangles is equal. Therefore, we have: DE BC = AM AH 3. Express the given and unknown values: Here, DE is the side length of the square we need to find, BC = 12 cm, and AH = 8 cm. We need to find the length of AM (the height from to DE). 4. Since and are on BC, and and are on AB and AC respectively, we can use the fact that the height from to DE is also the height from to BC divided by the ratio of the side lengths of the triangles. The height from to DE (AM) is the height from to BC (AH) minus the height from to BC. 5. Lets denote the side length of the square as x. Then, the height from to DE (AM) is 8 8x 3 (Incorrect expression for AM; it should be AM = AH = 8 x). Using the similarity ratio, we have: 12 = 8 2x 12 = 8 2x 3 8 6. Solve the equation: 12 12 12 = 1 = 1 12 = = 1 + 2x 12 6 = 24 7 (Incorrect calculation; from the previous step 6 = 1, the result should be = 6.) Therefore, the edge length of the square DEFG is 24 </think> The correct answer is . 7 cm."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 1 by VPPO-7B Question: As shown in the figure, square DEFG model should be cut on piece of triangle ABC paper. Among them, and are on BC, and are on AB and AC respectively, AH perpendicular BC and it intersects DE at M, if BC = 12.0, AH = 8.0, then the edge length of the square DEFG is () Choices: (A) 24 Ground truth: (A) 24 <think> To determine the edge length of the square DEFG, we need to use the properties of similar triangles and the given dimensions of the triangle and the square. Here is the step-by-step reasoning: 5 cm (B) 4cm (C) 24 7 cm (D) 5cm 5 cm 1. Identify the similar triangles: Since DEFG is square, DE is parallel to BC, and therefore ADE ABC by the AA similarity criterion (Angle-Angle similarity). This gives us the ratio of the corresponding sides of the similar triangles. 2. Set up the proportion: Since ADE ABC, we have: DE BC = AM AH Let the side length of the square DEFG be x. Then, DE = and AM = AH = 8 x. Since DE BC = AM AH , we can write: 12 = 8 8 3. Solve the equation: Cross-multiplying gives: Expanding and simplifying: 8x = 12(8 x) 8x = 96 12x 8x + 12x = 96 20x = 96 96 20 = 24 5 = Therefore, the edge length of the square DEFG is 24 </think> The correct answer is . 5 cm."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 2 by NoisyRollout-7B Ground truth: (C) 27 <think> To solve for the measure of angle CDB, we need to use the properties of circles and the relationships between the angles. Identify the type of angle given: The angle formed by the two radii at the center of the cir1. cle is 126. 2. Use the fact that the angle subtended by an arc at the center is twice the angle subtended by the same arc at any point on the circumference. 3. The angle at the center is 126, so the angle subtended by the same arc at point (which is an inscribed angle) would be half of 126, which is 63. (Misapplication of the inscribed angle theorem; is an endpoint of the arc, not vertex viewing the arc.) 4. Since AB is diameter, angle ACB is right angle (90). 5. Triangle BCD is right triangle with angle ACB as 90. (Incorrect statement; ACB is in ABC, and it is not an angle of BCD). The angle CDB is the angle we need to find. The angle CDB is an inscribed angle that intercepts the same arc as the 63 angle at the center. Therefore, angle CDB is half of the supplementary angle of 126, which is 54. (Multiple logical errors: refers to non-existent 63 central angle, incorrectly relates arcs, and the conclusion contradicts the premise.) So, angle CDB = 54. </think> 54 ."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 2 by VPPO-7B Ground truth: (C) 27 <think> To determine the measure of angle CDB, we need to use some properties of circles and the relationships between the angles in the circle. Here is the step-by-step reasoning: 1. Identify the given information and the relevant theorems: - We are given that the measure of the central angle AOC is 126. According to the central angle theorem, the measure of the central angle is twice the measure of the inscribed angle that subtends the same arc. - The measure of the inscribed angle ABC that subtends the same arc as the central angle AOC is half of the central angle. Therefore, ABC = 1 2 126 = 63. 2. Use the fact that the sum of the interior angles in quadrilateral is 360: - Since AB is the diameter of the circle, ACB = 90 (angle in semi-circle). Therefore, in ABC, we have: BAC = 180 ABC ACB = 180 63 90 = 27. 3. Relate the angles in the circle: - Since BAC and CDB are angles that subtend the same arc BC, they are equal. Therefore, CDB = BAC = 27. So, the measure of angle CDB is . </think> The correct answer is ."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 3 by VL-Rethinker-7B Question: In the triangle with hypotenuse length 20, find y. Choices: (A) 4 (C) 8 (B) 6 Ground truth: (D) 10 <think> To find the value of in the given right triangle, we can use the trigonometric relationship for the sine function (Incorrect function; cosine should be used as is the adjacent side). The sine of an angle in right triangle is defined as the length of the opposite side divided by the length of the hypotenuse. (D) 10 Given: - The hypotenuse = 20 - The angle θ = 60 - The opposite side to the angle θ is (Incorrect identification; is the side adjacent to the 60 angle, not opposite.) The sine of 60 is given by: 20 (Incorrect formula based on the wrong side identification.) We know that sin(60) = 3 2 . Therefore: sin(60) = 3 2 = 20 To solve for y, we can multiply both sides of the equation by 20: 3 2 = 20 = 10 (This is the value for x, the opposite side, not y.) Next, we simplify the expression. Using the approximate value of 3 1.732: 10 1.732 = 17.32 However, we can now check the answer choices. The value 10 3 17.32 matches closest the provided options. (Illogical conclusion; 17.32 is not close to the integer options, and the chosen answer (8) does not follow from this value.) </think> ."
        },
        {
            "title": "Preprint",
            "content": "Generated Sample 3 by VPPO-7B (D) 10 Question: In the triangle with hypotenuse length 20, find y. Choices: (A) 4 (C) 8 (B) 6 Ground truth: (D) 10 <think> To find the length of the side in the right triangle where the hypotenuse is 20 and the angle is 60 degrees, we can use the properties of 30-60-90 triangle. In 30-60-90 triangle, the sides are in the ratio 1 : 3 : 2, where the hypotenuse is the longest side and is twice the length of the side opposite the 30-degree angle. Here, the hypotenuse is 20, which is twice the length of the side opposite the 30-degree angle. Therefore, the side opposite the 30-degree angle is half of the hypotenuse. Lets go through the steps: 1. Identify the given information: the hypotenuse is 20 and the angle is 60 degrees. 2. In right triangle with 60-degree angle, the side opposite the 30-degree angle is half the hypotenuse. Here, the side is the side opposite the 30-degree angle in the 30-60-90 triangle formed by the given angle. The relationship in 30-60-90 triangle is: = 1 2 hypotenuse = 1 2 = 10 Therefore, the length of is 10. </think> The correct answer is ."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Peking University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}