{
    "paper_title": "ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning",
    "authors": [
        "Zhaorun Chen",
        "Mintong Kang",
        "Bo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents."
        },
        {
            "title": "Start",
            "content": "SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Zhaorun Chen 1 Mintong Kang 2 Bo Li"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 6 2 ] . [ 1 8 3 7 2 2 . 3 0 5 2 : r Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose SHIELDAGENT, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, SHIELDAGENT first constructs safety policy model by extracting verifiable rules from policy documents and structuring them into set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, SHIELDAGENT retrieves relevant rule circuits and generates shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce SHIELDAGENT-BENCH, dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that SHIELDAGENT achieves SOTA on SHIELDAGENT-BENCH and three existing benchmarks, outperforming prior methods by 11.3% on average with high recall of 90.1%. Additionally, SHIELDAGENT reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents. 1University of Chicago, Chicago IL, USA 2University of Illinois at Urbana-Champaign, Champaign IL, USA. Correspondence to: Zhaorun Chen, Bo Li <{zhaorun, bol}@uchicago.edu>. Our project is available and continuously maintained here: https: //shieldagent-aiguard.github.io/ 1 LLM-based autonomous agents are rapidly gathering momentum across various applications, integrating their ability to call external tools and make autonomous decisions in real-world tasks such as web browsing (Zhou et al., 2023), GUI navigation (Lin et al., 2024), and embodied control (Mao et al., 2023). Among these, LLM-based web agents, such as OpenAIs Operator (OpenAI, 2025b), deep research agent (OpenAI, 2025a), and Anthropics computer assistant agent (Anthropic, 2024), have become particularly prominent, driving automation in areas like online shopping, stock trading, and information retrieval. Despite their growing capabilities, users remain reluctant to trust current web agents with high-stakes data and assets, as they are still highly vulnerable to malicious instructions and adversarial attacks (Chen et al., 2024c; Wu et al., 2025), which can lead to severe consequences such as privacy breaches and financial losses (Levy et al., 2024). Existing guardrails primarily focus on LLMs as models, while failing to safeguard them as agentic systems due to two key challenges: (1) LLM-based agents operate through sequential interactions with dynamic environments, making it difficult to capture unsafe behaviors that emerge over time (Xiang et al., 2024); (2) Safety policies governing these agents are often complex and encoded in lengthy regulation documents (e.g. EU AI Act (Act, 2024)) or corporate policy handbooks (GitLab, 2025), making it difficult to systematically extract, verify, and enforce rules across different platforms (Zeng et al., 2024). As result, safeguarding the safety of LLM-based web agents remains an open challenge. To address these challenges, we introduce SHIELDAGENT, the first LLM-based guardrail agent designed to shield the action trajectories of other LLM-based autonomous agents, ensuring explicit safety compliance through probabilistic logic reasoning and verification. Unlike existing approaches that rely on simple text-based filtering (Xiang et al., 2024), SHIELDAGENT accounts for the uniqueness of agent actions and explicitly verifies them against relevant policies in an efficient manner. At its core, SHIELDAGENT automatically constructs robust safety policy model by extracting verifiable rules from policy documents, iteratively refining them, and grouping them based on different action types to form set of structured, action-based probabilistic rule SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning circuits (Kang & Li, 2024). During inference, SHIELDAGENT only verifies the relevant rule circuits corresponding to the invoked action, ensuring both precision and efficiency. Specifically, SHIELDAGENT references from hybrid memory module of both long-term shielding workflows and shortterm interaction history, generates shielding plan with specialized operations from rich tool library, and runs formal verification code. Once rule is verified, SHIELDAGENT performs probabilistic inference within the circuits and provides binary safety label, identifies any violated rules, and generates detailed explanations to justify its decision. While evaluating these guardrails is critical for ensuring agent safety, existing benchmarks remain small in scale, cover limited risk categories, and lack explicit risk definitions (see Table 1). Therefore, we introduce SHIELDAGENTBENCH, the first comprehensive agent guardrail benchmark comprising 2K safety-related pairs of agent instructions and trajectories across six web environments and seven risk categories. Specifically, each unsafe agent trajectory is generated under two types of attacks (Chen et al., 2024c; Xu et al., 2024) based on different perturbation sources (i.e., agentbased and environment-based), capturing risks present both within the agent system and the external environments. We conduct extensive experiments demonstrating that SHIELDAGENT achieves SOTA performance on both SHIELDAGENT-BENCH and three existing benchmarks (i.e., ST-WebAgentBench (Levy et al., 2024), VWA-Adv (Wu et al., 2025), and AgentHarm (Andriushchenko et al.)). Specifically, SHIELDAGENT outperforms the previous best guardrail method by 11.3% on SHIELDAGENT-BENCH, and 7.4% on average across existing benchmarks. Grounded on robust safety policy reasoning, it achieves the lowest false positive rate at 4.8% and high recall rate of violated rules at 90.1%. Additionally, SHIELDAGENT reduces the number of closed-source API queries by 64.7% and inference time by 58.2%, demonstrating its ability to effectively shield LLM agents actions while significantly improving efficiency and reducing computational overhead. 2. Related Works 2.1. Safety of LLM Agents While LLM agents are becoming increasingly capable, numerous studies have demonstrated their susceptibility to manipulated instructions and vulnerability to adversarial attacks, which often result in unsafe or malicious actions (Levy et al., 2024; Andriushchenko et al.; Zhang et al., 2024b). Existing attack strategies against LLM agents can be broadly classified into the following two categories. (1) Agent-based attacks, where adversaries manipulate internal components of the agent, such as instructions (Guo et al.; Zhang et al., 2024c), memory modules or knowledge bases (Chen et al., 2024c; Jiang et al., 2024), and tool libraries (Fu et al., 2024; Zhang et al., 2024a). These attacks are highly effective and can force the agent to execute arbitrary malicious requests. However, they typically require some access to the agents internal systems or training data. (2) Environment-based attacks, which exploit vulnerabilities in the environment that the agents interact with to manipulate their behavior (Liao et al., 2024), such as injecting malicious HTML elements (Xu et al., 2024) or deceptive web pop-ups (Zhang et al., 2024d). Since the environment is less controlled than the agent itself, these attacks are easier to execute in real world but may have lower success rate. Both attack types pose significant risks, leading to severe consequences such as life-threatening failures (Chen et al., 2024c), privacy breaches (Liao et al., 2024), and financial losses (Andriushchenko et al.). Therefore in this work, we account for both agent-based and environmentbased adversarial perturbations in the design of SHIELDAGENT. Besides, we leverage SOTA attacks (Chen et al., 2024c; Xu et al., 2024) from both categories to construct our SHIELDAGENT-BENCH dataset which involves diverse risky web agent trajectories across various environments. 2.2. LLM Guardrails While LLM agents are highly vulnerable to adversarial attacks, existing guardrail mechanisms are designed for LLMs as models rather than agents, leaving critical gap in safeguarding their sequential decision-making processes (Andriushchenko et al.). Current guardrails primarily focus on filtering harmful inputs and outputs, such as LlamaGuard (Inan et al., 2023) for text-based LLMs, LlavaGuard (Helff et al., 2024) for image-based multimodal LLMs, and SafeWatch (Chen et al., 2024a) for video generative models. However, these methods focus solely on content moderation, failing to address the complexities of action sequences, where vulnerabilities often emerge over time (Debenedetti et al., 2024). While GuardAgent (Xiang et al., 2024) preliminarily explores the challenge of guardrailing LLM agents with another LLM agent, it focus solely on textual space and still relies on the models internal knowledge rather than explicitly enforcing compliance with external safety policies and regulations (Zeng et al., 2024), limiting its effectiveness in real-world applications. To our knowledge, SHIELDAGENT is the first multimodal LLM-based agent to safeguard action sequences of other LLM agents via probabilistic policy reasoning to ensure explicit and efficient policy compliance. 3. SHIELDAGENT As illustrated in Fig. 1, SHIELDAGENT consists of two main stages: (1) constructing an automated action-based safety policy model (ASPM) that encodes safety constraints from SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Figure 1: Overview of SHIELDAGENT. (Top) From AI regulations (e.g. EU AI Act) and platform-specific safety policies, SHIELDAGENT first extracts verifiable rules and iteratively refines them to ensure each rule is accurate, concrete, and atomic. It then clusters these rules and assembles them into an action-based safety policy model, associating actions with their corresponding constraints (with weights learned from real or simulated data). (Bottom) During inference, SHIELDAGENT retrieves relevant rule circuits w.r.t. the invoked action and performs action verification. By referencing existing workflows from hybrid memory module, it first generates step-by-step shielding plan with operations supported by comprehensive tool library to assign truth values for all predicates, then produces executable code to perform formal verification for actions. Finally, it runs probabilistic inference in the rule circuits to provide safety label and explanation and reports violated rules. government regulations or platform-wide policies, and (2) leveraging the ASPM to verify and enforce these safety policies on the shielded agents actions via robust probabilistic safety policy reasoning. Notably, while SHIELDAGENT can be generalized to guardrail arbitrary agents and environments, we use web agents as an example for illustration. 3.1. Overview Let πagent be the action policy of an agent we aim to shield, where at each timestep i, the agent receives an observation oi from the environment and then produces an action ai πagent(oi) to progressively interacts with the environment. Then SHIELDAGENT As is guardrail agent aiming to safeguard the action of πagent, leveraging ASPM which encodes safety constraints in logical knowledge graph GASPM with rules, as well as variety of tools and hybrid memory module. Our guardrail task can be formulated as: (ls, Vs, Ts) = As(ai (oi, H<i, GASPM)) (1) where As takes as input the past interaction history H<i = {(oj, aj)j [1, 1]}, the observation oi, and the invoked action ai at step i, and consequently produces: (1) binary flag ls indicating whether action ai is safe; (2) list of flags indicating rule violation Vs = {lj rj [1, n]}, if applicable; (3) textual explanation Ts justifying the shielding decision. 3.2. Action-based Safety Policy Model To achieve tractable verification, we first construct an actionbased safety policy model (ASPM) that structurally encodes all safety constraints in logical knowledge graph GASPM. 3.2.1. OVEWVIEW OF ASPM Specifically, all constraints are represented as linear temporal logic (LTL) rules (Zhu et al., 2017) where each rule includes corresponding atomic predicates as decision variables1. Please refer to 3.2.2 for details. Thus let P, denote the predicate and rule space respectively, we have: GASPM = (cid:0)P, R, πθ (cid:1) s.t. = {Pa, Ps}, = {Ra, Rp} (2) where πθ denotes the probabilistic logic model (parameterized by θ) which organizes the rules (see 3.2.4). Specifically, GASPM partitions into state predicates ps Ps to represent system states or environmental conditions, and action predicates pa Pa to represent target actions. Consequently, is divided into action rules Ra which encodes safety specifications for target actions, and physical rules Rp which capture internal constraints on system variables. Specifically, while Rp does not directly constrain actions in Pa, these knowledge rules are critical for the logical reason1Each predicate can be assigned boolean value per time step to describe the agent system variables or environment state. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning ing in ASPM, enhancing the robustness of our shield (Kang & Li, 2024). Therefore, by structuring the solution space this way, we achieve clear and manageable verification of target actions. Refer to Appendix A.2 for more details. Specifically, we construct ASPM from policy documents via the following steps: (1) Extract structured safety rules from government regulations (Act, 2024), corporate policies (GitLab, 2025), and user-provided constraints; (2) Refine these rules iteratively for better clarity, verifiability, and efficiency; (3) Cluster the optimized rules by different agent actions and obtain set of action-based rule circuits (Kisa et al., 2014) where each circuit associates an agent action with relevant rules for verification; (4) Train the ASPM by learning rule weights from either real-world interactions or simulated data, ensuring adaptive and robust policy verification. 3.2.2. AUTOMATIC POLICY AND RULE EXTRACTION Since policy definitions are typically encoded in lengthy documents with structures varying widely across platforms (Act, 2024; GitLab, 2025), directly verifying them is challenging. To address this, SHIELDAGENT first extracts individual actionable policies from these documents and further translates them into manageable logical rules for tractable verification. Policy Extraction. Given policy documents, we first query GPT-4o (prompt detailed in Appendix H) to extract individual policy into structured format that contains the following elements: term definition, application scope, policy description, and reference (detailed in Appendix C.2.1). These elements ensure that each policy can be interpreted independently and backtracked for verification during shielding. LTL Rule Extraction. Since natural language constraints are hard to verify, we further extract logical rules from these formatted policies via GPT-4o (prompt detailed in Appendix H). Specifically, each rule is formulated as = [Pr, Tr, ϕr, tr] that involves: (1) set of predicates Pr from finite predicate set = {Pa, Ps}; (2) natural language description of the constraint Tr; (3) formal representation of the rule in LTL; (4) the rule type tr (i.e. action or physical). Please refer to Appendix C.3 for more details. 3.2.3. ASPM STRUCTURE OPTIMIZATION While the procedure in 3.2.2 extracts structured LTL rules from policy documents, they may not fully capture the original constraints or be sufficiently concrete for verification. Therefore, we propose bi-stage optimization algorithm to iteratively refine the rules in ASPM by: (1) improving their alignment with the original natural language policies, (2) enhancing verifiability by decomposing complex or vague rules into more atomic and concrete forms, and (3) increasing verification efficiency by merging redundant predicates and rules. As detailed in Algorithm 2 in Appendix C.4, the optimization process alternates between two stages, i.e., Verifiability Refinement (VR) and Redundancy Pruning (RP). Verifiability Refinement (VR). In this stage, we refine rules to be: (1) accurate, i.e., adjusting incorrect LTL representations by referencing their original definitions; (2) verifiable, i.e., refining predicates to be observable and can be assigned boolean value to be deterministically used for logical inference; and (3) atomic, i.e., decomposing compound rules into individual rules such that their LTL representations cannot be further simplified. Specifically, we prompt GPT-4o (prompt detailed in Appendix H) by either traversing each rule or prioritizing vague rules under an optimization budget. For example, based on the observation that concrete, useful rules usually have more specialized predicates that distinguish from each other, we devise an offline proxy to estip , , Pr mate the vagueness of rules via Vr = max{V 1 }, where quantifies the vagueness for each of its predicates pi by averaging its top-k embedding similarity with all other predicates of the same type Pi (i.e., either action or state): Vi = 1 (cid:88) i=m Sα(m)s.t. Sα = desc(cid:0){eiej Pi}(cid:1) (3) where ei denotes the normalized vector representation of predicate pi obtained by SOTA embedding model (e.g. OpenAIs text-embedding-3-large model (OpenAI, 2024)). Please refer to Appendix C.4 for more details. Redundancy Pruning (RP). Since the previous VR stage operates at the rule level without taking account of the global dynamics, it may introduce repetitive or contradictory rules into ASPM. To address this, RP evaluates ASPM from global perspective by clustering rules with semantically similar predicates. Then within each cluster, we prompt GPT-4o (see Appendix H) to merge redundant predicates and rules, enhancing both efficiency and clarity in ASPM. Iterative Optimization. By alternating between VR and RP, we progressively refine ASPM, improving rule verifiability, concreteness, and verification efficiency. This process iterates until convergence, i.e., no further rule optimizations are possible, or the budget is reached. Finally, human experts may review the optimized rules and make corrections when necessary, and the resulting ASPM thus effectively encodes all safety specifications from the given policy documents. 3.2.4. ASPM INFERENCE & TRAINING Given that rules in ASPM can be highly interdependent, we equip ASPM with logical reasoning capabilities by organizing it into set of action-based rule circuits πθ := {Cpa θa pa Pa}, where Cpa represents the rule circuit responsible θa for verifying action pa, where its rules are assigned soft weight θr to indicate their relevant importance for guardrail decision-making. Refer to Appendix C.5 for more details. Action-based ASPM Clustering. Observing that certain agent actions exhibit low logical correlation to each other (e.g. delete data and buy product), we further construct 4 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning an action-based probabilistic circuit πθ (Kisa et al., 2014) from ASPM to boost its verification efficiency while retaining precision. Concretely, we first apply spectral clustering (Von Luxburg, 2007) to the state predicates Ps, grouping rules that exhibit strong logical dependencies or high semantic relevance. Then, we associate each action predicate pa with its relevant constraints by unifying rule clusters that involve pa into single probabilistic circuit Cpa (weights θa θa are trained in 3.2.4). During verification, the agent only needs to check the corresponding circuit w.r.t. the invoked action, thereby substantially reducing inference complexity while preserving logical dependencies among rules. ASPM Inference. At each step i, SHIELDAGENT first extracts action predicates pa from the agent output and retrieves corresponding action rule circuits from GASPM to verify the invoked action ai. Then, SHIELDAGENT generates shielding plan to assign boolean values vi to each in Cpa state predicates pi by leveraging diverse set of θa verification operations and tools (detailed in 3.3). In each action circuit Cpa , the joint distribution over all posθa sible assignments of predicates (i.e., world) is modeled via Markov Logic Network (Richardson & Domingos, 2006). Let µp denote the assignment of predicate p, the probability of the proposed world µ with action pa invoked is given by: Pθ(µpa = 1{µps = vs}) = 1 (cid:88) exp θrI[µ r] (4) rRpa where I[µ r] = 1 indicates that the world µ follows the logical rule and is constant partition for normalization. However, since the absolute value of world probability is usually unstable (Gurel et al., 2021), directly thresholding it as the guardrail decision may cause high false positive rate. Thus inspired by the control barrier certificate (Ames et al., 2019), we propose the following relative safety condition: ls(ai) = 1 iff Pθ(µpa=1) Pθ(µpa=0) ϵ (5) where Pθ(µpa = 1) is the probability in Eq. (4), rewritten for brevity, and Pθ(µpa=0) = Pθ(µpa = 0{µps = vs}) reverses the value of the invoked action while keeping others unchanged. Specifically, condition Eq. (5) guarantees the safety of the action sequence from dynamic perspective, allowing executing action ai only when the safety likelihood increases or remains within tolerable region bounded by ϵ from the current state (i.e. no action taken). Users are allowed to adjust ϵ to adapt to different levels of safety requirements (e.g. higher ϵ for more critical safety needs). ASPM Weight Learning. Since some rules in ASPM may be inaccurate or vary in importance when constraining different actions, treating them all as absolute constraints (i.e., rule weights are simply infinity) can lead to high false positive rate. To improve ASPMs robustness, we Algorithm 1 SHIELDAGENT Inference Procedure Require: Interaction history H<i = {(oj, aj) [1, 1]} from the target agent; Current observation oi; Agent output ai; Safety policy model GASPM = (cid:0)P, R, πθ (cid:1); Safety threshold ϵ. 1: pa EXTRACT(ai) = (cid:0)Ppa , Rpa , θa 2: Cpa θa 3: Vs = {pi : vi s} 4: for each rule = [Pr, Tr, ϕr, tr] Rpa do 5: Wr RETRIEVEWORKFLOW(r, pa) 6: 7: (cid:1) RETRIEVE(pa, GASPM) Initialize predicate-value map while ps Pr s.t. Vs[ps] is not assigned do Extract action predicates As PLAN(Wr, r, Pr) Generate an action plan with shielding operations (e.g., SEARCH, CHECK) for each step ti in action plan As do oi EXECUTE(ti s, H<i, oi) Get step result Vs[ps] PARSE(cid:0)oi Attempt to assign truth value to any unassigned predicates (cid:1), ps Pr 8: 9: 10: Run formal verification (cid:0)µpa= (cid:1) Pθ (cid:0)µpa=0 (cid:1) Calculate safety condition via Eq. (4) and Eq. (5) end for end while lr VERIFY(r, Vs) 11: 12: 13: 14: end for 15: ϵs Pθ 16: if ϵs ϵ then ls 1 17: 18: else 19: 20: end if 21: return (cid:0)ls, Vs, Ts textual explanation ls 0 Action pa is safe Action pa is unsafe (cid:1) Return safety label, violated rules, optimize rule weights for each circuit θa over dataset = {ζ (i), y(i))}N i=1 via the following guardrail hinge loss: Lg(θ) = (ζ,Y)D max(0, y(i)(Pθ(µ(i) pa=1)Pθ(µ(i) pa=0))) (6) where labels y(i) = 1 if action a(i) is safe or y(i) = 1 if unsafe. Specifically, y(i) can be derived from either real-world safety-labeled data or simulated pseudo-learning (Kang & Li, 2024). The learned weights act as soft constraints, capturing the relative importance of each rule in guardrail decisionmaking. We illustrate the training process in Algorithm 3. 3.3. SHIELDAGENT Framework In this section, we detail the verification workflow of SHIELDAGENT for each action rule circuit. Specifically, SHIELDAGENT integrates specialized shielding operations designed for diverse guardrail needs, supported by rich tool library. To further enhance efficiency, it employs hybrid memory module that caches short-term interaction history and stores long-term successful shielding workflows. Shielding Pipeline. As illustrated in the lower part of Fig. 1, 5 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Table 1: Comparison of SHIELDAGENT-BENCH with existing agent safety benchmarks. SHIELDAGENT-BENCH extends prior work by offering more samples, operation risk categories, and types of adversarial perturbations (both agent-based and environment-based). In addition, SHIELDAGENT-BENCH provides verified annotations of both risky inputs and output trajectories, explicitly defining each case of safety violations, and annotating relevant policies for verifying each trajectory. Benchmark #Sample #Operation Risk #Attack Type #Environment Risky Trajectory Risk Explanation #Rule ST-Web (Levy et al., 2024) AgentHarm (Andriushchenko et al.) VWA-Adv (Wu et al., 2025) SHIELDAGENT-BENCH 234 440 3110 3 1 1 7 0 0 1 2 3 0 6 36 0 0 at each step i, SHIELDAGENT first extracts action predicates from the agent output and retrieves corresponding rule circuits for verification. Then it formats all the predicates and rules in query and retrieves similar shielding workflows from the long-term memory. Using them as few-shot examples, it then produces step-by-step shielding plan supported by diverse set of operations and tools to assign truth values for the predicates. Once all predicates are assigned, it then generates model-checking code to formally verify each rule. For each violated rule, it provides an in-depth explanation and potential countermeasures. Finally, it performs probabilistic inference (as detailed in 3.2.4) to deliver the final guardrail decision (see details in Appendix D). Shielding Operations. SHIELDAGENT includes four inbuilt operations for rule verification: (1) Search: Retrieves relevant information from past history Hi and enumerates queried items as output; (2) Binary-Check: Assigns binary label to the input query; (3) Detect: Calls moderation APIs to analyze target content and produce guardrail labels for different risk categories; (4) Formal Verify: Run model-checking algorithms to formally verify target rules. Tool Library. To support these operations, SHIELDAGENT is equipped with powerful tools, including moderation APIs for various modalities (e.g., image, video, audio) and formal verification tools (e.g., Stormpy). To enhance guardrail accuracy, we fine-tuned two specialized guardrail models based on InternVL2-2B (Chen et al., 2024b) for enumerationbased search and binary-check operations. Memory Modules. To optimize efficiency, SHIELDAGENT employs hybrid memory module comprising: (1) History as short-term memory: To copilot with the shielded agent πagent in real time, SHIELDAGENT incrementally stores agent-environment interactions as KV-cache, minimizing redundant computations. Once the current action sequence is verified, the cache is discarded to maintain clean and manageable memory; (2) Successful workflows as longterm memory: Since verifying similar actions often follows recurring patterns, SHIELDAGENT also stores successful verification workflows for diverse action circuits as permanent memory, enabling efficient retrieval and reuse of these effective strategies. This module is also continually updated to incorporate new successful shielding experiences. Figure 2: Pipeline for curating SHIELDAGENT-BENCH. We adopt the AWM web agent (Wang et al., 2024) and collect safe trajectories by executing instructions with full policy compliance. For risky trajectories, we attack the agent with two SOTA agent-based and environment-based algorithms and produce unsafe trajectories across seven risk categories. Built on the MCP framework (Anthropic, 2024), SHIELDAGENT collectively integrates these modules to handle diverse shielding scenarios while allowing users to customize new tools to extend the guardrail capabilities. 4. SHIELDAGENT-BENCH Dataset Existing guardrail benchmarks primarily evaluate the content generated by LLMs rather than their actions as decision-making agents. To bridge this gap, we introduce SHIELDAGENT-BENCH, the first comprehensive benchmark for evaluating guardrails for LLM-based autonomous agents, encompassing safe and risky trajectories across six diverse web environments. As shown in Fig. 2, we curate 960 safetyrelated web instructions and collect 3110 unsafe trajectories by attacking agents to violate targeted safety policies via two practical perturbations. Furthermore, we categorize the resulting failure patterns into seven common risk categories. Safety-related Instructions. We selectively reuse the instruction templates from WebArena (Zhou et al., 2023) and ST-WebAgentBench (Levy et al., 2024) across six environments (i.e., Shopping, CMS, Reddit, GitLab, Maps, SuiteCRM), and curate instructions that yield potential safety risks by augmenting the templates with safety-critical information (e.g. API token). Finally, we obtain 960 high-quality safety-related instructions. Specifically, each sample in our 6 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Table 2: Agent guardrail performance comparison of SHIELDAGENT with various baselines on SHIELDAGENT-BENCH. For each perturbation source (i.e., agent-based and environment-based), we report the individual accuracy for each risk category, along with average accuracy (ACC@G) and false positive rate (FPR@G) for the final guardrail label. Additionally, we report the average rule recall rate (ARR@R). Inference cost is measured by the average number of queries (NoQ) to GPT-4o and inference time (seconds per sample). The best performance is in bold. Perturbation Source Guardrail Risk Category Overall Cost Access Content Hallu. Instr. Operation Error Long-term ACC@G FPR@G ARR@R NoQ Time Agent-based Environmentbased Direct Rule Traverse GuardAgent SHIELDAGENT Direct Rule Traverse GuardAgent SHIELDAGENT 68.2 83.4 77.0 92.0 75.0 85.0 89.3 95. 78.6 85.9 77.6 89.2 81.6 86.2 88.2 92.7 76.3 78.0 74.0 85.0 80.3 87.7 85.5 93.3 73.3 74.9 76.7 83.2 88.1 86.3 86.7 95.2 69.2 87.9 85.3 93.0 73.5 88.0 83.1 91. 74.3 70.5 84.7 88.7 70.3 69.3 77.7 89.3 68.8 87.0 76.9 91.3 82.0 83.0 80.9 92.0 73.3 82.0 81.4 90.4 75.8 81.6 84.8 91. 7.6 18.1 14.3 5.6 6.6 15.0 10.7 4.0 31.5 69.0 55.9 87.5 31.5 75.0 70.0 92.7 1 27.1 13.6 9.5 1 31.5 14.8 11. 6.3 75.3 62.3 31.1 6.7 80.1 58.7 33.8 u, ζ dataset consists of (Is, ζs, ζ u), where Is is the instruction, ζs is the safe trajectory, and ζ are unsafe trajectories induced by two types of attacks, respectively. Each ζ includes the complete interactions between the agent and the environment at each step, including: (1) all conversations, (2) visual screenshots, (3) HTML accessibility trees. u, ζ Policy-Targeted Agent Attacks. We consider two types of adversarial perturbations against agents, each instanced by practical attack algorithm: (1) Agent-based: we adopt AgentPoison (Chen et al., 2024c), which injects adversarial demonstrations in the agents memory or knowledge base to manipulate its decision-making; (2) Environment-based: we adopt AdvWeb (Xu et al., 2024), which stealthily manipulates the environment elements to mislead the agent. Specifically, we adapt both algorithms to attack SOTA web agent, AWM (Wang et al., 2024) to violate at least one extracted safety policy per instruction, ensuring policycentered safety violation for tractable guardrail evaluation. Comprehensive Risk Categories. We carefully investigate the extracted policies, risky trajectories induced by our attack, and concurrent studies on agents risky behaviors (Levy et al., 2024), and categorize the unsafe trajectories into seven risk categories: (1) access restriction, (2) content restriction, (3) hallucination, (4) instruction adherence, (5) operational restriction, (6) typical error patterns, and (7) long-term risks. Please refer to Appendix for more details. Quality Control. For each trajectory, human annotators manually review its guardrail label and all violated policies, ensuring reliable testbed for evaluating agent guardrails. 5. Experiment 5.1. Setup Datasets. We evaluate SHIELDAGENT against guardrail baselines on our SHIELDAGENT-BENCH dataset and three existing benchmarks: (1) ST-WebAgentBench (Levy et al., 2024), which includes 234 safety-related web agent tasks with simple safety constraints; (2) VWA-Adv (Wu et al., 2025), consisting of 200 realistic adversarial tasks in the VisualWebArena (Koh et al., 2024); and (3) AgentHarm (Andriushchenko et al.), comprising 110 malicious tasks designed for general agents. Notably, to properly evaluate agent guardrails, each sample must include an instruction, agent trajectory, enforced policy, and ground-truth label as protocolsall of which are available in SHIELDAGENTBENCH. However, existing benchmarks only provide task instructions (see Table 1). To address this, we augment them by collecting corresponding policies and both safe and unsafe trajectories using various algorithms. See Appendix for details on the curation pipeline and dataset statistics. Baselines. We consider three representative baselines: (1) Direct prompt: We provide GPT-4o with the complete policy and directly prompt it to produce an overall safety label and any violated rules. (2) Rule traverse: We traverse each rule and prompt GPT-4o to identify potential violation. We flag the trajectory as unsafe once rule is flagged as violated. (3) GuardAgent (Xiang et al., 2024): We follow their pipeline and set the guard request to identify any policy violations in the agent trajectory. To ensure fair comparison, we provide all methods with the same safety policy as input and collect the following outputs for evaluation: (i) binary flag (safe or unsafe); (ii) list of violated rules, if any. Metrics. We evaluate these guardrails using three holistic metrics: (1) Guardrail Accuracy: We report the accuracy (ACC) and false positive rate (FPR) based on the overall safety label, capturing the end-to-end guardrail performance. (2) Rule Recall Rate: For each rule, we compute their average recall rates (ARR) from the list of reported violations, reflecting how well the guardrail grounds its decisions based on the underlying policy. (3) Inference Cost: We report the average number of API queries to closed-source LLMs (e.g., GPT-4o) and the inference time (in seconds) per sample for different guardrail methods, capturing both monetary and SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Table 3: Comparison of guardrails across three existing benchmarks. Averaged accuracy (ACC) and false positive rate (FPR) are reported. The best performance is in bold. Guardrail ST-Web VWA-Adv AgentHarm ACC FPR ACC FPR ACC FPR 74.1 Direct GuardAgent 84.0 SHIELDAGENT 91.1 4.2 6.6 4.4 90.3 89.9 94. 4.2 4.4 3.4 76.9 78.4 86.9 4.4 4.1 3.9 Figure 3: Performance comparison of SHIELDAGENT with rule traverse and GuardAgent baselines on STWebAgentBench. We report the individual guardrail accuracy for each risk category. computational overhead for real-time applications. 5.2. Results SHIELDAGENT-BENCH. As shown in Table 2, SHIELDAGENT achieves SOTA performance, outperforming the best baseline (rule traverse) by an average of 10.2% in terms of accuracy. It also attains the lowest false positive rate at 4.8% and high rule recall rate of 90.1%, attributed to the robust logical reasoning of ASPM. In terms of efficiency, SHIELDAGENT reduces API queries by 64.7% and inference time by 58.2% due to its streamlined verification pipeline. (1) Policy Grounding: The high ARR demonstrates SHIELDAGENTs strong ability to ground decisions in self-extracted constraints, highlighting the effectiveness of our ASPM pipeline in both rule extraction and rigorous verification. (2) Guardrail Robustness: Guardrails generally perform better on environment-based perturbations, as these are externally observable by the guardrail, unlike agent-based which rely on internal agent configurations. Nonetheless, SHIELDAGENT performs consistently well across both types due to its proactive evidence-grounded verification, making it robust and agnostic to attack modality. (3) Guardrail by Category: SHIELDAGENT leads across most risk categories, particularly in access restriction and instruction adherence, with slightly lower performance on hallucination-related risks that often require external knowledge beyond the policy. Existing Datasets. As shown in Table 3 and Fig. 3, SHIELDAGENT outperforms the baselines across all three benchmarks by an average of 7.4% in ACC. Specifically: (1) On ST-WebAgentBench, SHIELDAGENT shows notable gains in User Consent and Boundary and Scope Limitation, high8 Table 4: Comparison of online guardrail performance of different guardrail methods across six web environments. We report the policy compliance rate (%) conditioned on task success for the tasks from each web environment, along with the average time cost. The best performance is in bold. Shopping CMS Reddit GitLab Maps SuiteCRM AWM Agent + Direct + Rule Traverse + GuardAgent + SHIELDAGENT 46.8 50.2 58.7 57.9 65. 53.2 56.1 62.9 61.5 68.4 45.9 48.3 55.4 54.8 60.2 22.8 26.5 32.0 36.1 50.7 67.9 70.2 75.1 74.3 80.5 36.0 38.5 41.0 40.6 55.9 lighting its strength in grounding and enforcing target policies; (2) On VWA-Adv, SHIELDAGENT achieves the highest ACC and lowest FPR, demonstrating robust guardrail decisions grounded in logical reasoning. (3) On AgentHarmthat spans broader range of agent tasks, SHIELDAGENT achieves SOTA performance, showing its generalizability to guardrail across diverse agent types and scenarios. Online Guardrail. We further evaluate SHIELDAGENTs performance in providing online guardrails for web agents. Specifically, we use the AWM agent as the task agent and integrate each guardrail method as post-verification module that copilots with the agent. These guardrails verify the agents actions step-by-step and provide interactive feedback to help it adjust behavior for better policy compliance. Notably, this evaluation setting comprehensively captures key dimensions such as guardrail accuracy, fine-grained policy grounding, and explanation clarity, which are all critical components for effectively guiding the task agents behavior toward better safety compliance. As shown in Table 4, SHIELDAGENT also outperforms all baselines in this online setting, achieving the highest policy compliance rate. These results highlight SHIELDAGENTs effectiveness as System 2 (Li et al., 2025) to seamlessly integrate with task agents to enhance their safety across diverse environments. 6. Conclusion In this work, we propose SHIELDAGENT, the first LLMbased guardrail agent that explicitly enforces safety policy compliance for autonomous agents through logical reasoning. Specifically, SHIELDAGENT leverages novel actionbased safety policy model (ASPM) and streamlined verification framework to achieve rigorous and efficient guardrail. To evaluate its effectiveness, we present SHIELDAGENTBENCH, the first benchmark for agent guardrails, covering seven risk categories across diverse web environments. Empirical results show that SHIELDAGENT outperforms existing methods in guardrail accuracy while significantly reducing resource overhead. As LLM agents are increasingly deployed in high-stakes, real-world scenarios, SHIELDAGENT marks critical step toward ensuring their behavior aligns with explicit regulations and policiespaving the way for more capable and trustworthy AI systems. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning"
        },
        {
            "title": "Acknowledgment",
            "content": "This work is partially supported by the National Science Foundation under grant No. 1910100, No. 2046726, NSF AI Institute ACTION No. IIS-2229876, DARPA TIAMAT No. 80321, the National Aeronautics and Space Administration (NASA) under grant No. 80NSSC20M0229, ARL Grant W911NF-23-2-0137, Alfred P. Sloan Fellowship, the research grant from eBay, AI Safety Fund, Virtue AI, and Schmidt Science."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Act, E. A. I. The eu artificial intelligence act, 2024. Ames, A. D., Coogan, S., Egerstedt, M., Notomista, G., Sreenath, K., and Tabuada, P. Control barrier functions: Theory and applications. In 2019 18th European control conference (ECC), pp. 34203431. IEEE, 2019. Andriushchenko, M., Souly, A., Dziemian, M., Duenas, D., Lin, M., Wang, J., Hendrycks, D., Zou, A., Kolter, J. Z., Fredrikson, M., et al. Agentharm: Benchmarking robustness of llm agents on harmful tasks. In The Thirteenth International Conference on Learning Representations. Anthropic. Introducing the model context protocol, 11 2024. URL https://www.anthropic.com/ news/model-context-protocol. Chen, Z., Pinto, F., Pan, M., and Li, B. Safewatch: An efficient safety-policy following video guardrail model with transparent explanations. arXiv preprint arXiv:2412.06878, 2024a. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024b. Chen, Z., Xiang, Z., Xiao, C., Song, D., and Li, B. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. Debenedetti, E., Zhang, J., Balunovic, M., Beurer-Kellner, L., Fischer, M., and Tram`er, F. Agentdojo: dynamic environment to evaluate attacks and defenses for llm agents. arXiv preprint arXiv:2406.13352, 2024. Fu, X., Li, S., Wang, Z., Liu, Y., Gupta, R. K., BergKirkpatrick, T., and Fernandes, E. Imprompter: Tricking llm agents into improper tool use. arXiv preprint arXiv:2410.14923, 2024. GitLab. The gitlab handbook, 02 2025. URL https: //handbook.gitlab.com/. Guo, C., Liu, X., Xie, C., Zhou, A., Zeng, Y., Lin, Z., Song, D., and Li, B. Redcode: Risky code execution and generation benchmark for code agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Gurel, N. M., Qi, X., Rimanic, L., Zhang, C., and Li, B. Knowledge enhanced machine learning pipeline against diverse adversarial attacks. In International Conference on Machine Learning, pp. 39763987. PMLR, 2021. Helff, L., Friedrich, F., Brack, M., Kersting, K., and Schramowski, P. Llavaguard: Vlm-based safeguards for vision dataset curation and safety assessment. arXiv preprint arXiv:2406.05113, 2024. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. Jiang, C., Pan, X., Hong, G., Bao, C., and Yang, M. Ragthief: Scalable extraction of private data from retrievalaugmented generation applications with agent-based attacks. arXiv preprint arXiv:2411.14110, 2024. Kang, M. and Li, B. r2-guard: Robust reasoning enabled llm guardrail via knowledge-enhanced logical reasoning. arXiv preprint arXiv:2407.05557, 2024. Kisa, D., Van den Broeck, G., Choi, A., and Darwiche, A. Probabilistic sentential decision diagrams. In Fourteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2014. Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881905, 2024. Levy, I., Wiesel, B., Marreed, S., Oved, A., Yaeli, A., and Shlomov, S. St-webagentbench: benchmark for evaluating safety and trustworthiness in web agents. arXiv preprint arXiv:2410.06703, 2024. Li, Z.-Z., Zhang, D., Zhang, M.-L., Zhang, J., Liu, Z., Yao, Y., Xu, H., Zheng, J., Wang, P.-J., Chen, X., et al. From 9 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. Liao, Z., Mo, L., Xu, C., Kang, M., Zhang, J., Xiao, C., Tian, Y., Li, B., and Sun, H. Eia: Environmental injection attack on generalist web agents for privacy leakage. arXiv preprint arXiv:2409.11295, 2024. Lin, K. Q., Li, L., Gao, D., Yang, Z., Bai, Z., Lei, W., Wang, L., and Shou, M. Z. Showui: One vision-languageaction model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents, 2024. Mao, J., Ye, J., Qian, Y., Pavone, M., and Wang, Y. language agent for autonomous driving. arXiv preprint arXiv:2311.10813, 2023. OpenAI. New embedding models and api updates, 01 2024. URL https://openai.com/index/ new-embedding-models-and-api-updates/. Introducing deep research, OpenAI. URL introducing-deep-research/. 02 2025a. https://openai.com/index/ OpenAI. URL introducing-operator/. Introducing 2025b. https://openai.com/index/ operator, 01 Zhang, B., Tan, Y., Shen, Y., Salem, A., Backes, M., Zannettou, S., and Zhang, Y. Breaking agents: Compromising autonomous llm agents through malfunction amplification. arXiv preprint arXiv:2407.20859, 2024a. Zhang, H., Huang, J., Mei, K., Yao, Y., Wang, Z., Zhan, C., Wang, H., and Zhang, Y. Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llmbased agents. arXiv preprint arXiv:2410.02644, 2024b. Zhang, Y., Chen, K., Jiang, X., Sun, Y., Wang, R., and Wang, L. Towards action hijacking of large language model-based agent. arXiv preprint arXiv:2412.10807, 2024c. Zhang, Y., Yu, T., and Yang, D. Attacking visionlanguage computer agents via pop-ups. arXiv preprint arXiv:2411.02391, 2024d. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Zhu, S., Tabajara, L. M., Li, J., Pu, G., and Vardi, M. Y. Symbolic ltlf synthesis. arXiv preprint arXiv:1705.08426, 2017. Richardson, M. and Domingos, P. Markov logic networks. Machine learning, 62:107136, 2006. Von Luxburg, U. tutorial on spectral clustering. Statistics and computing, 17:395416, 2007. Wang, Z. Z., Mao, J., Fried, D., and Neubig, G. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. Wu, C. H., Shah, R. R., Koh, J. Y., Salakhutdinov, R., Fried, D., and Raghunathan, A. Dissecting adversarial robustness of multimodal lm agents. In The Thirteenth International Conference on Learning Representations, 2025. Xiang, Z., Zheng, L., Li, Y., Hong, J., Li, Q., Xie, H., Zhang, J., Xiong, Z., Xie, C., Yang, C., et al. Guardagent: Safeguard llm agents by guard agent via knowledgeenabled reasoning. arXiv preprint arXiv:2406.09187, 2024. Xu, C., Kang, M., Zhang, J., Liao, Z., Mo, L., Yuan, M., Sun, H., and Li, B. Advweb: Controllable black-box attacks on vlm-powered web agents. arXiv preprint arXiv:2410.17401, 2024. Zeng, Y., Yang, Y., Zhou, A., Tan, J. Z., Tu, Y., Mai, Y., Klyman, K., Pan, M., Jia, R., Song, D., et al. Air-bench 2024: safety benchmark based on risk categories from regulations and policies. arXiv preprint arXiv:2407.17436, 2024. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning A. Detailed Introduction to SHIELDAGENT A.1. Notations Let denote the environment, and let πagent be the action policy of an agent we aim to shield. At each step i, the agent receives an observation oi and maps it to partial state si = (oi) via state-space mapping function . Specifically for web agents, extracts accessibility trees (AX-trees) from the webpages HTML and visual screenshots, condensing key information from lengthy observations (Zhou et al., 2023). Then, the agent generates an action ai by sampling from policy ai πagent(si) and progressively interacts with the environment . A.2. Solution Space Given the uniqueness of verifying agent trajectories, we further categorize the predicates into two types: (1) action predicate pa: indicates the action to be executed (e.g. delete data); and (2) state predicate ps: describes the environment states involved for specifying the condition that certain actions should be executed (e.g. is private). detailed explanation can be found in Appendix C.3. Consequently, we characterize the solution space of LLM-based agents with the following two types of rules. Action rule: an action rule ϕa specifies whether an action pa should be executed or not under certain permissive or preventive conditions pc. Note ϕa must involve at least one pa. For example, the deletion action cannot be executed without user consent (i.e., is user authorized delete data). Physical rule: physical rule ϕp specifies the natural constraints of the system, where conditions can logically depend on the others. For example, if dataset contains private information then it should be classified as red data under GitLabs policy (i.e., is private is red data). Since predicates can sometimes be inaccurately assigned, ϕp can serve as knowledge in ASPM to enhance the robustness of our shield (Kang & Li, 2024). With these rules, SHIELDAGENT can effectively reason in the solution space to shield the agent action with high accuracy and robustness. B. Additional Results B.1. ST-WebAgentBench Table 5: Comparison of guardrail performance across three risk categories in ST-WebAgentBench (Levy et al., 2024). Specifically, we report the averaged accuracy (ACC) and false positive rate (FPR) for each evaluation category, along with overall averages. The best performance is in bold. Guardrail User Consent Boundary Strict Execution Overall ACC FPR ACC FPR ACC FPR ACC FPR 78.0 Direct 84.3 Rule Traverse 80.1 GuardAgent SHIELDAGENT 91.4 5.0 10.7 4.5 4.2 72.3 85.0 88.9 93.5 3.4 11.5 8.7 4.0 71.9 80.5 83.0 88.3 4.3 7.0 6.5 5. 74.1 83.3 84.0 91.1 4.2 9.7 6.6 4.4 B.2. VWA-Adv Specifically, VWA-Adv (Wu et al., 2025) attacks web agents by perturbing either the text instruction by adding suffix or the image input by adding bounded noise. Specifically, VWA-Adv constructs 200 diverse risky instructions based on the three environments from VisualWebArena (Koh et al., 2024). The environments are detailed as follows: Classifieds. Classifieds is similar environment inspired by real-world platforms like Craigslist and Facebook Marketplace, comprising roughly 66K listings and uses OSClassan open-source content management systemallowing realistic tasks such as posting, searching, commenting, and reviewing. Shopping. This environment builds on the e-commerce site from WebArena (Zhou et al., 2023), where successful navigation requires both textual and visual comprehension of product images, reflecting typical online shopping tasks. 11 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Reddit. Adopting the social forum environment from WebArena, this environment hosts 31K+ posts (including images and memes) across different subreddits. The content variety offers broad coverage of social media interactions and challenges in forum-based tasks. Table 6: Guardrail performance comparison on VWA-Adv (Wu et al., 2025) across three environments in VisualWebArena (Koh et al., 2024), i.e., Classifieds, Reddit, Shopping, under two perturbation sources, i.e., text-based and imagebased. We report accuracy (ACC) and false positive rate (FPR) for each environment. The best performance is in bold. Perturbation Source Guardrail Classifieds Reddit Shopping Overall ACC FPR ACC FPR ACC FPR ACC FPR Text-based Image-based Direct GuardAgent SHIELDAGENT Direct GuardAgent SHIELDAGENT 87.8 90.5 93.2 93.7 92.4 91.0 4.6 6.8 3.4 3.5 3.9 3.4 91.1 87.3 93. 91.2 87.2 96.6 3.9 2.6 4.9 4.3 2.7 2.7 90.1 91.8 95.1 87.9 90.0 94.9 5.0 5.8 3. 3.6 4.1 3.0 89.7 89.9 93.9 90.9 89.9 94.2 4.5 5.1 3.8 3.8 3.6 3.0 B.3. AgentHarm Table 7: Guardrail performance comparison on AgentHarm (Andriushchenko et al.) across 11 harm categories. The reported numbers have been slightly perturbed (by 0.1 in an alternating pattern) and the overall is recalculated as the arithmetic mean of the 11 categories. The best performance is in bold. Fraud Cybercrime Self-harm Harassment Sexual Copyright Drugs Disinfo. Hate Violence Terrorism Overall Direct GuardAgent SHIELDAGENT ACC 75.7 FPR 5. ACC 82.6 FPR 4.7 ACC 89.1 FPR 4.6 82.4 3.6 66.1 4.0 92.9 4.9 76.5 3. 75.1 4.5 82.5 3.9 80.6 3.8 75.9 3.4 92.4 2.5 82.2 3. 82.1 6.3 94.0 4.0 72.0 3.9 69.6 4.3 89.0 2.1 82.0 7. 76.6 3.8 80.4 5.5 76.9 4.1 80.1 3.2 81.9 4.2 71.0 3. 77.7 3.7 81.7 3.8 75.8 4.4 92.4 3.3 83.9 4.7 71.1 5. 83.9 4.2 88.3 3.2 76.9 4.4 78.4 4.1 86.9 3.9 C. Action-based Probabilistic Safety Policy Model C.1. Automated Policy Extraction We detail the prompt for automated policy extraction in Appendix and LTL rule extraction in Appendix H. C.2. Safety Policy Model Construction C.2.1. AUTOMATIC POLICY AND RULE EXTRACTION Specifically, we detail the prompt used for extracting structured policies in Appendix H). Specifically, each policy contains the following four elements: 1. Term definition: clearly defines all the terms used for specifying the policy, such that each policy block can be interpreted independently without any ambiguity. 2. Application scope: specifies the conditions (e.g. time period, user group, region) under which the policy applies. 3. Policy description: specifies the exact regulatory constraint or guideline (e.g. allowable and non-allowable actions). 4. Reference: lists original document source where the policy is extracted from, such that maintainers can easily trace them back for verifiability. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning C.3. Linear Temporal Logic (LTL) Rules Temporal logic represents propositional and first-order logical reasoning with respect to time. Linear temporal logic over finite traces (LTLf ) (Zhu et al., 2017) is form of temporal logic that deals with finite sequences, i.e., finite-length trajectories. Syntax. The syntax of an LTLf formula φ over set of propositional variables is defined as: φ ::= φ φ1 φ2 φ φ φ1 φ2. (7) Specifically, LTLf formulas include all standard propositional connectives: AND (), OR (), XOR (), NOT (), IMPLY (), and so on. They also use the following temporal operators (interpreted over finite traces): Always ((cid:50)φ1): φ1 is true at every step in the finite trajectory. Sometimes ((cid:51)φ1): φ1 is true at least once in the finite trajectory. Next ( φ1): φ1 is true in the next step. Until (φ1 φ2): φ1 must hold true at each step until (and including) the step when φ2 first becomes true. In finite trace, φ2 must become true at some future step. Specifically, φ1 and φ2 are themselves LTLf formulas. An LTLf formula is composed of variables in and logic operations specified above. Trajectory. finite sequence of truth assignments to variables in is called trajectory. Let Φ denote set of LTLf specifications (i.e., {ϕ ϕ Φ}), we have ζ = Φ to denote that trajectory ζ satisfies the LTLf specification Φ. C.4. ASPM Structure Optimization We detail the prompt for the verifiability refinement of ASPM in Appendix and redundancy merging in Appendix H. We detail the overall procedure of the iterative ASPM structure optimization in Algorithm 2. Figure 4: The number of rules during each iteration step for GitLab policy. Specifically, the orange bar denotes the number of rules after each verifiability refinement step, and the blue bar denotes the number of rules after each redundancy pruning step. Figure 5: The number of predicates during each iteration step for GitLab policy. Specifically, the orange bar denotes the number of predicates after each verifiability refinement step, and the blue bar denotes the number of predicates after each redundancy pruning step. C.5. Training ASPM 13 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Algorithm 2 ASPM Structure Optimization Require: Predicate set = {Pa, Ps}; Rule set = {Ra, Rp}; Embedding model E; Clustering algorithm C; Refinement budget Nb; Max iterations Mit; Surrogate LLM; Graph = (P, E) with initial edge weights E. Calculate via Eq. (3) Compute vagueness score for each rule Count how many refinements have been done Tracks if any update occurred in this iteration Pop the most vague rule Refine rule to be verifiable; update its predicates if needed (cid:1) (cid:12) R(cid:9) changed false while = Nb do ( , r) HeapPop(U) if LLM verifiable(r) = false then rnew LLM refine(cid:0)r, Pr Update R: replace with rnew Update P: if rnew introduces or revises predicates Recompute Vp for any changed predicate in rnew Recompute Vrnew = max{Vp Prnew } Push (Vrnew , rnew) into + 1 changed true 1: Initialize vagueness score for each predicate Vp, 2: Vr = max{Vp1, . . . , VpPr }, Pr 3: Initialize max-heap (cid:8)(Vr, r) (cid:12) 4: 0 5: for = 1 to Mit do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end if 18: end while 19: 20: C(G) 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: end if 33: 34: end for 35: return ASPM GASPM with optimized structure and randomized weights Update G: add pmerged, remove predicates in Update to replace references of predicates in with pmerged Recompute Vpmerged and any affected Vr Push updated rules into by their new Vr changed true pmerged LLM merge(cid:0)C, R(cid:1) if pmerged = then end if end for if changed = false then for each cluster do break Cluster predicates in to prune redundancy Merge similar predicates/rules in if beneficial No more refinements or merges D. SHIELDAGENT Framework E. SHIELDAGENT-BENCH E.1. Risk Categories We categorize the unsafe trajectories from SHIELDAGENT-BENCH into the following seven risk categories. (1) Access restriction: Ensuring the agent only interacts with explicitly authorized areas within an application (e.g., enforcing user-specific access control); (2) Content restriction: Verifying that content handling follows predefined policies (e.g., preventing exposure of private or harmful data); (3) Hallucination: the cases where the agent generates or retrieves factually incorrect or misleading outputs in information-seeking tasks; (4) Instruction adherence: Assessing the agents ability to strictly follow user-provided instructions and constraints without deviation; (5) Operational restriction: Enforcing explicit policy-based operational constraints, such as requiring user permission before executing sensitive actions; (6) Typical error pattern: Identifying common failure patterns like infinite loops or redundant executions; (7) Long-term SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Figure 6: The vagueness score of the rule set during each iteration step for optimizing the GitLab policy. Specifically, we leverage GPT-4o as judge and prompt it to evaluate the vagueness of each rule within the rule set. lower vagueness score signifies that the rules are more concrete and therefore more easily verified. risks: Evaluating actions with delayed consequences, such as repeated failed login attempts leading to account lockout. F. Detailed Experiment Results F.1. Dataset Distribution We detail the distribution of samples in our proposed SHIELDAGENT-BENCH dataset in Fig. 8. Table 8: Distribution of samples in our proposed SHIELDAGENT-BENCH dataset. For each environment, we report the number of safe and unsafe trajectories. Each instruction is paired with one safe trajectory (i.e., compliant with all policies) and one unsafe trajectory (i.e., violating at least one policy), such that these paired trajectories are always equal in quantity. Environment Unsafe Safe Total Shopping CMS Reddit GitLab Maps SuiteCRM 265 260 230 450 160 190 265 530 260 520 230 460 450 900 160 320 190 380 G. Case Study 15 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning If two co-occurring predicates appear in different clusters, merge them Initialize adjacency matrix Build adjacency matrix Cluster the state predicates into groups Form predicate clusters Cp ℓ ℓ s, pj s, pj p, pj Cm s), emb(pj s)(cid:1) θ; else 0. {rs ps ℓ p} {ps labels[ps] = ℓ} s] = labels[pj s.t. pi Cm s) co-occur in any rule OR cosSim(cid:0)emb(pi s) that co-occur do s] then Cℓ Algorithm 3 ASPM TRAINING PIPELINE Require: Rule set R; state predicates Ps and action predicates Pa; similarity threshold θ; number of clusters k. 1: {0, 1}PsPs 0 2: Aij 1 if (pi 3: labels SPECTRALCLUSTERING(A, k) 4: for ℓ = 1 to do 5: 6: end for 7: for each pair (pi if labels[pi 8: Cℓ Cℓ 9: end if 10: 11: end for 12: for ℓ = 1 to do 13: 14: end for 15: GASPM 16: for each pa Pa do 17: 18: 19: 20: 21: 22: 23: 24: 25: end for 26: for each action circuit Cpa θa do 27: 28: 29: 30: 31: 32: for each rule cluster ℓ for each rule ℓ then GASPM[pa] = GASPM[pa] ℓ break for each rule Cpa θa Initialize rule weight θr randomly end for for epoch = 1 to max epochs do Cr do do end if end for end for if pr do Group rules which share state predicates in the same cluster Initialize ASPM as an empty dictionary with actions as keys Associate action circuits with any relevant rule clusters pa=0) Run probabilistic inference to obtain corresponding safety probabilities Calculate loss w.r.t. the groundtruth labels via Eq. (6) pa=1) and Pθ(µ(i) for = 1 to do Compute Pθ(µ(i) via Eq. (4) Compute loss L(θ) Update θ using gradient descent end for 33: 34: 35: 36: 37: end for 38: return Action-based safety policy model GASPM with trained weights end for 16 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Figure 7: An example of the access restriction policy and the corresponding extracted rules from GitLab policy handbook. Figure 8: An example of the content restriction policy and the corresponding extracted rules from GitLab policy handbook. SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Figure 9: An example of the hallucination policy and the corresponding extracted rules from GitLab policy handbook. 18 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning H. Prompt Template Prompt Template for Policy Extraction SYSTEM: You are helpful policy extraction model to identify actionable policies from organizational safety guidelines. Your task is to exhaust all the potential policies from the provided organization handbook which sets restrictions or guidelines for user or entity behaviors in this organization. You will extract specific elements from the given guidelines to produce structured and actionable outputs. USER: As policy extraction model to clean up policies from {organization (e.g. GitLab)}, your tasks are: 1. Read and analyze the provided safety policies carefully, section by section. 2. Exhaust all actionable policies that are concrete and explicitly constrain behaviors. 3. For each policy, extract the following four elements: 1. Definition: Any term definitions, boundaries, or interpretative descriptions for the policy to ensure it can be interpreted without any ambiguity. These definitions should be organized in list. 2. Scope: Conditions under which this policy is enforceable (e.g. time period, user group). 3. Policy Description: The exact description of the policy detailing the restriction or guideline. 4. Reference: All the referenced sources in the original policy article from which the policy elements were extracted. These sources should be organized piece by piece in list. Extraction Guidelines: Do not summarize, modify, or simplify any part of the original policy. Copy the exact descriptions. Ensure each extracted policy is self-contained and can be fully interpreted by looking at its Definition, Scope, and Policy Description. If the Definition or Scope is unclear, leave the value as None. Avoid grouping multiple policies into one block. Extract policies as individual pieces of statements. Provide the output in the following JSON format: ```json [ { \"definition\": [\"Exact term definition or interpretive description.\"], \"scope\": \"Conditions under which the policy is enforceable.\", \"policy_description\": \"Exact description of the policy.\", \"reference\": [\"Original source where the elements were extracted.\"] }, ... ] ``` Output Requirement: - Each policy must focus on explicitly restricting or guiding behaviors. - Ensure policies are actionable and clear. - Do not combine unrelated statements into one policy block. 19 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Prompt Template for Linear Temporal Rule Extraction SYSTEM: You are an advanced policy translation model designed to convert organizational policies into structured Linear Temporal Logic (LTL) rules. Your task is to extract verifiable rules from the provided safety guidelines and express them in machine-interpretable format while maintaining full compliance with logical correctness. USER: As policy-to-LTL conversion model, your tasks are: 1. Carefully analyze the policys definition, scope, and policy description. 2. Break down the policy into structured rules that precisely capture its constraints and requirements. 3. Translate each rule into LTL using atomic predicates derived from the policy. Translation Guidelines: Use atomic predicates that are directly verifiable from the agents observations and action history. Prefer positive predicates over negative ones (e.g., use store data instead of is data stored). If rule involves multiple predicates, decompose it into smaller, verifiable atomic rules whenever possible. Emphasize action-based predicates, ensuring that constrained actions are positioned appropriately within logical expressions (e.g., only authorized users can access personal data should be expressed as: (is authorized has legitimate need) access personal data (8) ). Predicate Formatting: Each predicate must include: Predicate Name: Use snake case format. Description: brief, clear explanation of what the predicate represents. Keywords: list of descriptive keywords providing relevant context (e.g., actions, entities, attributes). LTL Symbol Definitions: Always: ALWAYS Eventually: EVENTUALLY Next: NEXT Until: UNTIL Not: NOT And: AND Or: OR Implies: IMPLIES Output Format: ```json [ { \"predicates\": [ [\"predicate_name\", \"Description of the predicate.\", [\"kw1\", \"kw2\", ...]] ], \"logic\": \"LTL rule using predicate names.\" }, ... ] ``` Output Requirements: Ensure each rule is explicitly defined and unambiguous. Keep general when applicable predicates click create project). (e.g., use create project instead of Avoid combining unrelated rules into single LTL statement. 20 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Prompt Template for Verifiability Refinement (VR) SYSTEM: You are helpful predicate refinement model tasked with ensuring predicates in the corresponding rules are clean, verifiable, concrete, and accurate enough to represent the safety policies. Your task is to verify each predicate and refine or remove it if necessary. USER: As predicate refinement model, your tasks are: 1. Check if the provided predicate satisfies the following criteria: Verifiable: It should be directly verifiable from the agents observation or action history. Concrete: It should be specific and unambiguous. Accurate: It must represent the intended fact or condition precisely. Atomic: It should describe only one fact or action. If it combines multiple facts, break it into smaller predicates. Necessary: The predicate must refer to meaningful information. If it is redundant or assumed by default, remove it. Unambiguous: If the same predicate name is used in different rules but has different meanings, rename it for clarity. 2. If refinement is needed, refine the predicate accordingly with one of the following: Rewrite the predicate if it is unclear or inaccurate. Break it down into smaller atomic predicates if it combines multiple facts or conditions. Rename the predicate to reflect its context if it is ambiguous. Remove the predicate if it is redundant or unnecessary for the rule. Output Requirements: Provide step-by-step reasoning under the section Reasoning. Include the label on whether the predicate is good, needs refinement, or redundant. If refinement is needed, provide structured JSON including: Updated predicate with definitions and keywords. Each of the updated rules which are associated with the updated predicate. Definitions of the predicate in each rules context. Output Format: Reasoning: 1. Step-by-step reasoning for why the predicate is good, needs refinement, or is redundant. 2. If yes, then reason about how to refine or remove the redundant predicate. Decision: Yes/No If yes, then provide the following: Output JSON: { \"rules\": [ \"predicates\": [ [\"predicate_name\", \"Predicate definition.\", [\"keywords\"]] ], \"logic\": \"logic_expression_involving_predicates\" { } ] } {Few-shot Examples} 21 SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning Prompt Template for Redundancy Pruning (RP) SYSTEM: You are helpful predicate merging model tasked with analyzing collection of similar predicates and their associated rules to identify whether there are at least predicates that can be merged or pruned. Your goal is to simplify and unify rule representation while ensuring the meaning and completeness of the rules remain intact after modifying the predicates. USER: As predicate merging model, your tasks are: 1. Identify predicates in the cluster that can be merged based on the following conditions: Redundant Predicates: If two or more predicates describe the same action or condition but use different names or phrasing, merge them into one. Identical Rule Semantics: If two rules describe the same behavior or restriction but are phrased differently, unify the predicates and merge their logics to represent them with fewer rules. 2. Ensure the merged predicates satisfy the following: Consistency: The merged predicate must be meaningful and represent the combined intent of the original predicates. Completeness: The new rules must perfectly preserve the logic and intent of all original rules. Output Requirements: Provide step-by-step reasoning under the section Reasoning. Include decision label on whether the predicates should be merged. If merging is needed, provide structured JSON including: Updated predicates with definitions and keywords. Updated rules with the new merged predicates. Output Format: Reasoning: 1. Step-by-step reasoning for why the predicates should or should not be merged. 2. If merging is needed, explain how the predicates and rules were updated to ensure completeness and consistency. Decision: Yes/No If yes, then provide the following: Output JSON: { \"rules\": [ \"predicates\": [ [\"predicate_name\", \"Predicate definition.\", [\"keywords\"]] ], \"logic\": \"logic_expression_involving_predicates\" { } ] } Few-shot Examples"
        }
    ],
    "affiliations": [
        "University of Chicago, Chicago IL, USA",
        "University of Illinois at Urbana-Champaign, Champaign IL, USA"
    ]
}