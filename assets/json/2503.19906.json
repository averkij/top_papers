{
    "paper_title": "AvatarArtist: Open-Domain 4D Avatarization",
    "authors": [
        "Hongyu Liu",
        "Xuan Wang",
        "Ziyu Wan",
        "Yue Ma",
        "Jingye Chen",
        "Yanbo Fan",
        "Yujun Shen",
        "Yibing Song",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 6 0 9 9 1 . 3 0 5 2 : r AvatarArtist: Open-Domain 4D Avatarization Hongyu Liu1,2, Xuan Wang2, Ziyu Wan3 Yue Ma1 Jingye Chen Yanbo Fan2 Yujun Shen"
        },
        {
            "title": "Yibing Song",
            "content": "Qifeng Chen1, 1HKUST 2Ant Group 3City University of Hong Kong https://kumapowerliu.github.io/AvatarArtist Figure 1. Gallery of the proposed AvatarArtist. Each row features several triplets, where the first column of each triplet is the source image. The subsequent two images in each triplet are results that follow the pose and expression of the driving image, as demonstrated in the bottom right corner of the first three columns. Specifically, our method is applicable to an open domain, encompassing diverse range of categories including 3D cartoons, video game characters, sculptures, skulls, etc."
        },
        {
            "title": "Abstract",
            "content": "This work focuses on open-domain 4D avatarization, with the purpose of creating 4D avatar from portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation, and propose practical takes advantage of both generative adversarial networks (GANs) and diffusion training paradigm that models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of multi-domain image-triplane dataset, which drives the development of general 4D 1 avatar creator. Extensive experiments suggest that our model, termed AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies. 1. Introduction Avatarization (dynamic) from one single portrait image has become fundamental ability of AI content generation. It enables the transfer of motion and expression from source video to digital avatar while preserving both motion accuracy and subject identity. This technology has broad applications in virtual reality, social media, gaming, and online education, facilitating efficient character production and enhancing interactive experiences in computer vision and computer graphics. Studies on avatarization can mainly be categorized as 2D and 4D aspects. The 2D-based methods [21, 60 62, 80, 81, 86] typically employ self-supervised learning scheme, with monocular video stream data accompanied by facial landmarks or implicit motion representations [61]. More recently, the emergence of powerful generative models, such as diffusion models, which can handle various types of images, has further advanced the field. Some 2D methods [45, 71, 75] incorporate prior knowledge from diffusion models (e.g., Stable Diffusion [58]), enabling them to effectively handle multi-style avatarization (e.g., cartoon, realistic). Despite achieving impressive results, these 2D methods fail to accurately represent 3D structures. Geometric distortion and content inconsistency often arise when the head pose undergoes significant rotation. Moreover, the iterative computation of diffusion models incurs substantial costs for generating each frame of animated videos, significantly increasing the overall computational burden. On the other hand, 4D-based methods [10, 15, 36, 46, 88] leverage neural rendering pipelines [30, 48] and 3DMM [35] for efficient avatarization where 3D geometric consistency is maintained across multiple viewpoints. During model inference, these models animate the image feature first then camera pose to perform neural rendering of target view generations. Despite the demonstrated success, these methods suffer from lack of 4D data from diverse domains. The human portrait animation is restricted to limited domain and is difficult to generalize as that of 2D-based methods. Having examined both 2D and 4D-based avatarization methods, we intuitively assume that if sufficient and well- * This work is done partially when Hongyu is an intern at Ant Group. Joint corresponding authors. suited 4D datasets covering diverse domains were available, it would be possible to develop 4D avatarization approach for open-domain inputs using diffusion models. Recently, Rodin [69, 82], diffusion-based single-imageguided static avatar generation method, has demonstrated impressive performance in the synthetic digital domain. To train this model, dataset of image-3D representation pairs was constructed using multi-view digital human data. Inspired by this, we believe that an appropriate 4D dataset for our method should consist of image-4D representation pairs spanning multiple domains. In this work, we propose AvatarArtist, diffusion-based 4D avatarization model. It is challenging to obtain multiview, multi-expression 4D captures to create image-4D representation pairs with fitting process similar to Rodin. Therefore, we resort to synthetic data generation. Fortunately, we found that 4D GANs, such as Next3D [64, 88], can greatly assist in this process. Specifically, Next3D proposed parametric triplane 4D representation, which divides the traditional triplane [8] into dynamic and static components. The dynamic part is aligned with the 3DMM mesh [35, 68] in UV space, allowing expression changes through mesh rasterization and rendering. With Next3D, we can generate an unlimited amount of image-parametric triplane data simply by sampling, but only for single realistic domain due to the mode collapse issue of GAN. Hence we propose to finetune Next3D to efficiently obtain multiple GANs of diverse domains. While training Next3D only requires 2D images and their corresponding 3DMM meshes, achieving effective multi-domain fine-tuning demands diverse and comprehensive data coverage across various domains. To overcome this limitation, we leverage 2D diffusion models [58] to enrich the diversity of the training data. Specifically, we utilize the SDEdit [47] pipeline and landmark-guided ControlNet to transfer portrait images (e.g., FFHQ) from the realistic domain to other domains. This process ensures coherent pose and expression between the output and input 2D portraits, allowing us to reuse the 3DMM mesh of the 2D portrait from the realistic domain in non-realistic domains. Consequently, we can train 4D GANs for different domains and generate imageparametric triplane datasets across multiple domains. The entire data generation process combines the advantages of both diffusion models and GANs: diffusion models provide multi-domain data for the GAN, while the GAN transforms 2D images into 4D representations in an unsupervised manner. Using this dataset, we could adopt the latent Diffusion Transformer (DiT) [53] to model its distribution. The process begins with training VAE to compress triplanes into latent representations, followed by employing DiT to generate latent guided by single portrait image. Although the diffusion model is able to generate triplanes effectively, 2 Figure 2. The overall training pipeline of our method. We first generate 2D images from different domains using 2D diffusion model. These images are then used to train 4D GANs for each domain. Subsequently, the trained 4D GANs generate image-4D representation pairs across domains, which are used to train DIT and the rendering model. there are still two issues for rendering high-quality frames from these planes. First, not like Rodin which uses simple MLP decoder for the digital domain, rendering triplanes from multiple diverse domains into high-quality images is much more challenging. Second, Parametric triplanes primarily focus on motion modeling but are less effective in preserving identity. Next3D employs CNN to enhance identity preservation, but we found the performance of CNN degrades significantly in the open domain. To address these, we introduce motion-aware cross-domain renderer based on ViT [73] that incorporates features from the source image, improving cross-domain rendering ability and preserving the identity information. Additionally, we use an implicit motion representation, similar to Portrait4D [15], to avoid artifacts caused by mesh inaccuracies. Compared to baseline methods, our approach delivers superior quantitative and qualitative performance, offering high visual fidelity, accurate identity representation, and precise motion rendering. 2. Related Work We address one-shot, open-domain image-driven talking face generation, which synthesizes talking head video from single reference portrait and sequence of driving expression images. This section provides concise overview of previous talking head generation methods, broadly categorized into 2D talking face generation and 3D-aware talking portrait synthesis, along with brief discussion on stylized 3D avatar generation. 2.1. 2D Talking Face Generation Great progress has been made in image-driven 2D talking head generation [5, 17, 20, 39, 44, 6062, 67, 76, 78, 80, 81]. Numerous approaches harness the capabilities of Generative Adversarial Networks to synthesize highfidelity talking head videos, most of which fall into the warping-then-rendering scheme. The identity features are first encoded from the reference image and then warped according to the driving signals, finally being rendered into sequence of talking portraits. More specifically, various types of motion representation, such as landmarks [61, 80], depth [26], and latent code [5], are exploited to deduce the warping field, ensuring that the synthesized portraits exhibit expressions and motions that faithfully correspond to the driving signals. With the advent of diffusion model-based image generation, several methods employ large pre-trained diffusion models to assist in the task of one-shot talking face generation. By leveraging the powerful prior of pre-trained diffusion models, recent methods [45, 71, 75] have demonstrated that they possess strong generalization capabilities when handling various styles of reference portraits. However, due to lack of understanding of three-dimensional structures, these 2Dbased methods often exhibit obvious geometric distortions when handling larger head movements. Additionally, they lack the ability to control the viewpoint of the generated images with precision. 2.2. 3D-aware Talking Portrait Synthesis To achieve high geometric fidelity in synthesizing portraits with varying head poses, researchers have introduced intermediate 3D representations that capture facial geometry and pose, ensuring structural accuracy across viewpoints. major breakthrough in novel view synthesis is Neural Radiance Fields (NeRF)[8, 11, 27, 36, 37, 46, 48, 65, 77, 79, 90], which enables precise 3D reconstructions with explicit camera control. NeRF has been widely adopted in 3D-aware one-shot talking head generation, enhancing 3D coherence and pose control for more natural outputs. More recently, GAGAvatar[10] leveraged 3D Gaussian Splatting (3DGS) to accelerate generation while maintaining high quality. However, most methods [70] rely on in-the-wild video data, making 3D learning from monocular videos highly ill-posed due to depth ambiguity, lighting variations, and facial occlusions. Some approaches incorporate 3D supervision from monocular 3D face reconstruction [12, 14, 18], multi-view lab-captured videos[27, 90], or synthetic multiview data[15, 16]. While these improve results, they are constrained by limited high-quality 3D data and training challenges. As result, there remains no open-domain, oneshot 4D portrait generation method capable of generalizing 3 Figure 3. The pipeline of dataset generation.We use text prompts to transform images from the realistic domain to the target domain while ensuring pose and expression consistency with SDEdit [47] and landmark-guided ControlNet [84]. This enables direct reuse of the original mesh, avoiding errors in non-realistic domain extraction. After domain transfer, we train 4D GANs to generate image-parametric triplane pairs, which serve as data for the next stage. The parametric triplane comprises dynamic and static components, with the dynamic region aligned to the mesh. across diverse conditions. 3.1. Data Curation from 4D GANs 2.3. Stylized Avatar Generation To generate avatars across different domains, some methods [2, 4, 31, 32, 34, 40, 54, 63, 66] use CLIP as constraint or leverage diffusion models to generate reference images, which are then utilized to create stylized avatars based on text prompts. Additionally, StyleAvatar3D [83] and Rodin [69, 82] collect domain-specific datasets to train generative models for stylized avatar synthesis. While these methods significantly advance stylized avatar generation, they do not focus on single-image-guided, animatable avatar creation. Meanwhile, other approaches [3, 19, 52, 89] employ CLIP as constraint and text as guidance to fine-tune GAN models, enabling the generation of stylized portrait images that align with textual descriptions. Although these methods demonstrate strong manipulation capabilities for stylized portraits, they cannot directly generate avatars. 3. Method We aim to develop system that generates 4D avatar from an open-domain image Is, driven by the motion of target individual It. The key to training deep generative model for open-domain avatarization is large-scale, high-quality dataset. In Sec.3.1, we introduce how GANs and image generation techniques help construct diverse and consistent training data. With this dataset, we use latent Diffusion Transformer (DiT) to model the 4D distribution (Sec.3.2). To ensure accurate motion transfer while preserving the source identity, we further employ motion-aware crossdomain renderer (Sec.3.3). The overall training pipeline is shown in Figure2. Next, we detail each component. Benefiting from adversarial training, the recent GAN methods have demonstrated great potential in generating highquality 4D avatars in an unsupervised manner using nonmultiview images and 3DMM meshes only. Therefore, we would like to fully leverage this capability of GANs to curate 4D data. Nonetheless, the instability of GAN training easily caused mode-collapse, failing to cover the distribution of different modes. In this section, we will discuss how to properly use GAN to generate open-domain image-4D representation pairs data. Base GAN Model. We select Next3D [64, 88] as our base GAN model for generating the 4D dataset, given its training efficiency and the proposed robust 4D representation (parametric triplane). Specifically, as shown in Figure 3, given randomly sampled latent code z, the mapping network translates into an intermediate latent vector, which will modulate conv layers of StyleGAN to generate parametric triplane R256256432. The parametric triplane consists of two parts: static component representing non-facial regions and dynamic component aligned with the 3DMM mesh in UV space. During inference, given specific mesh, the dynamic part is deformed through rasterization and combined with the static part to form triplane with expressions. Then neural rendering and super-resolution module are applied to generate the final image. We follow Next3D and use FaceVerse [68] to extract the corresponding parametric mesh. Multi-Domain Tuning. To train Next3D models across various domains, the first priority is to obtain diversified images from different domains and extract the corresponding 3DMM meshes. However, it is very challenging to accurately obtain 3DMM meshes for non-realistic portraits. To address this issue, we use 4 fed into the 4D GAN with random to generate images in different identities along with corresponding triplanes. For each domain, we generated 20K samples, resulting in total of 20K 28 = 560K image-triplane pairs. 2) The multiview, multi-expression image-parametric triplane dataset. These data assist in learning our motion-aware cross-style renderer. We generate both static and dynamic components following the portrait4D [15]. The dynamic data, which are responsible for head reenactment, consists of synthetic identities with multiple expressions per subject and varying camera poses for each expression. The static data, on the other hand, are employed to enhance the generalizability of 3D reconstruction and contain only single expression per identity, also with varying camera poses. Expressions (meshes) in the dynamic dataset are sampled from the VFHQ dataset [74], while those in the static dataset are sampled from the FFHQ dataset. All camera poses are sampled from FFHQ. 3.2. 4D Generation latent space by default. Recently, the latent diffusion model has shown great potential in modeling complex data distributions like images [55], videos [56], and triplanes [7, 59, 69]. Following this trend, this section will depict how we leverage latent diffusion for 4D generation. As shown in Figure 4, we will first introduce triplane VAE to compress the triplane representations into latent space, followed by training an image-conditioned DiT [53]. All the data used for training was sourced from our curated datasets. Triplane VAE. For training efficiency, the DiT [53] is trained in compact To achieve this, we propose triplane variational autoencoder (VAE) [33] to obtain the latent code of the triplane representations. Specifically, our VAE compresses the triplane R256256432 to latent zt R646448. To optimize the VAE model, we compute the L1 loss between the reconstructed planes and input triplanes. Meanwhile, we also get depth and rendered images to calculate L1 and LPIPS losses, respectively. We did not apply adversarial loss since we found it introduced training instability. For more details, please refer to our supplementary material. Image Guided Diffusion Transformer. We follow the Direct3D [72] and PixArt-α [9]to build our image-guided DiT. For the noised latent zt we flatten it to sequence and send it to the DiT as input. We separately extract the semantic and detailed information from conditional images and inject them into each DiT block. For semantic information, we use CLIP [57] to extract the images semantic embeddings, which are then integrated with the model via cross-attention. To capture fine-grained details, In we employ the DINO [50] to extract image tokens. each DiT block, we concatenate the image tokens with the flattened zt and feed them into self-attention layer to Figure 4. The pipeline of DiT. We first train VAE to compress the parametric triplane into latent space, and then train DiT to denoise the noisy latent. We incorporate features from DINO [6] and CLIP [57] into the DiT to guide the generation process. the pre-trained 2D diffusion model to generate the target domain from realistic images given specific prompts, so that the 3D meshes from realistic domains can be re-used. Specifically, given portrait from the realistic domain, we add noise over this image with specific strength [47] and then denoise it by StableDiffusion to generate highquality and diversified images of the target domain under the guidance of prompts. However, corrupting the images with noise also raises challenges in maintaining the original expression. Hence, we also incorporate the facial landmark through ControlNet [84], which provides the conditional signals of expression. As result, the newly sampled image from the target domain could closely align with the pose and expression of the source realistic image, enabling the direct transfer of 3DMM mesh labels from the realistic domain to the target domain. Through the pipeline mentioned above, we totally collected image data from 28 different domains (anime, lego, etc.) transferred from FFHQ [28]. To ensure the efficiency of the data pipeline and avoid model collapse, for each domain, we generate 6,000 images only and use this data to finetune independent GAN from the Next3D model trained with FFHQ [28]. We follow the DATID-3D [31] to use the ADA loss and density regularization loss to guarantee the diverse content generation ability of GAN during tuning. Data Synthesis. We utilize the trained multiple 4D GANs to build two datasets. 1) The image-parametric triplane paired dataset. We randomly sampled poses and meshes of portrait images from the FFHQ dataset, which are then 5 model the intrinsic relationship between the image tokens and zt. During training, we leverage the objective of IDDPM [49] and predict the noise and variance at each time step t. We also randomly drop the conditional image with probability of 10% to enable classifier-free guidance [25] during inference. 3.3. Motion-Aware Cross-Domain Renderer In the rendering process of Next3D, CNN refines the rasterized parametric triplane to protect the identity information and eliminate identity leakage caused by rasterization. However, we found that this rendering approach fails to achieve acceptable quality in our setting (see Figure 8). Since our parametric triplane is generated from different domains, simple rendering network cannot effectively resolve identity leakage across various domains. Addiinaccurate mesh extraction sometimes leads to tionally, mismatched expressions in the generated results. To address these issues, we propose motion-aware cross-domain renderer. As shown in Figure 5, we first employ an encoder EI to extract features from the source images, which are subsequently fed into Vision Transformer (ViT) model [73]. In the ViT model, we inject the parametric triplane generated by DiT into the selfattention mechanism, which aims to neutralize facial expressions and canonicalize poses inspired by [15], thereby eliminating expression-specific information from the source image. Then, we change the expression with motion embedding [67] by injecting it with cross attention. This embedding is an implicit representation without spatial information, thus preventing identity leakage. The output of the ViT is decoded to match the resolution of the rasterized parametric triplane, after which it is fused with the triplane. Finally, we apply volumetric rendering followed by superresolution techniques to generate the final output Io. The Io, rendered from novel camera viewpoint, preserves the identity features from the source image Is meanwhile following the facial expression of the target image It. For training this model, we adopt the loss terms following the [15, 65, 87]. For more details, please refer to our supplementary material. 4. Experiments In this section, we first illustrate our implementation details. Then, we compare our method with existing methods qualitatively and quantitatively. We compare our approach with both 2D and 4D reenactment methods. Specifically, we include comparisons with 2D techniques such as LivePortrait [21] and XPortrait [75], as well as 4D methods like InvertAvatar [88] and Portrait4Dv2 [16]. Finally, an ablation study validates the effectiveness of our contributions. More results are provided in the supplementary files. Figure 5. The pipeline of motion-aware cross-domain renderer. We use an encoder to extract the feature from the source image. This feature is sent to ViT to predict results under the guidance of generated parametric triplane and motion embedding. Finally, decoder processes the output of the ViT and fuses it with the results of rasterization to produce the final output. 4.1. Implementation Details During the training of the Next3D, we extract facial poses and corresponding 3DMM meshes from the FFHQ dataset using FaceVerse [68]. All domains are fine-tuned based on GAN pre-trained on the FFHQ dataset, with each domain iterating over total of 300K images. For VAE training, we adopt the same training framework as the VAE used in Stable Diffusion. We utilize the AdamW optimizer [41] with learning rate of 1e-4. The VAE model is trained on an NVIDIA A100 (80G) GPU for 100K steps with batch size of 32. Our diffusion model follows the network configuration of DiT-XL/2 [9, 53, 72], consisting of 28 layers of DiT blocks. The diffusion model is trained with 1000 denoising steps using linear variance scheduler. We employ the AdamW optimizer with learning rate of 1e4 and train the model for 800K steps. During inference, we use 19 steps of the DPMSolver [42], with guidance scale set to 4.5. For the motion-aware cross-domain renderer, we train on total of 12 million images across all domains. For more details, please refer to the supplementary materials. 4.2. Qualitative Results As shown in Figure 6, we present visual comparison of the results of self-reenactment and cross-reenactment tasks. The first column contains the input images, with the bottomright corner showing the target image and the larger images representing the source images. The first row displays the self-reenactment results. We observe that InvertA6 Figure 6. Qualitative comparison with SOTA methods. The leftmost column in the figure shows the input images, with the bottom-right corner representing the target image. The first row displays the results of self-reenactment, while the following three rows show the results of cross-reenactment. It can be observed that our method achieves superior performance in terms of expression and pose consistency, as well as identity preservation. Table 1. Quantitative evaluation of state-of-the-art methods and our approach on the VFHQ dataset [74]. For self-reenactment, both the source and target images are from the VFHQ dataset. For cross-ID reenactment, the source images are generated from different domains, while the target motions are from VFHQ. indicates lower is better while indicates higher is better. Red highlights the best result, and Blue highlights the second-best result."
        },
        {
            "title": "Self reenactment",
            "content": "LPIPS FID ID AED APD Cross reenactment FID CLIP AKD APD LivePortrait [21] XPortrait [75] InvertAvatar [88] Portrait4Dv2 [16]"
        },
        {
            "title": "Ours",
            "content": "0.27 0.31 0.42 0.29 0.26 46.49 60.29 84.71 66.60 52. 0.65 0.63 0.32 0.58 0.69 0.025 0.036 0.049 0.034 0. 4.28 18.07 15.58 5.08 11.72 100.3 78.6 194.7 140.5 89. 0.91 0.89 0.64 0.75 0.84 4.92 10.67 20.78 7.13 2. 139.35 237.4 134.9 63.3 52.3 vatar exhibits noticeable artifacts, while XPortrait shows misalignment in pose compared to the target. Although Portrait4D and LivePortrait achieve relatively good results, there are inconsistencies in the expression, particularly in the eye and mouth regions, when compared to the target. In contrast, our method produces more consistent results, achieving better alignment with the target in both pose and expression. For the cross-reenactment, our source images are from non-realistic domains, while the target expressions are 7 Table 2. Ablaiton study on the FFHQ [28] dataset. The source images are generated from different domains, while the target images are from FFHQ. The Next3D rendering means replacing our render model with simple CNN. Method FID Cross reenactment CLIP AKD APD Next3D render 130.72 0.73 Ours 68.69 0.86 5. 2.56 42.93 40.89 extracted from real-human domains. We observe that both InvertAvatar and Portrait4D struggle to handle portraits that significantly differ from real-human domains effectively. InvertAvatar tends to exhibit severe geometric distortions and fails to adequately animate the source image. Portrait4D, on the other hand, suffers from identity leakage and generates content that lacks precision. While 2D-based methods preserve the identity of the input image, they fail to ensure that the pose aligns with the target image. In contrast, our method demonstrates exceptional performance when handling non-realistic domains, achieving good accuracy in both expression and pose consistency, as well as identity preservation. 4.3. Quantitative Results The quantitative results are summarized in Table 1. We evaluate our method on 100 VFHQ video clips[74] through self-reenactment and cross-reenactment tests. For selfreenactment, the source image is either the first frame or random intermediate frame from the video, while for cross-reenactment, we use 50 source images from different domains with target images from VFHQ. To assess image quality, we compute LPIPS[85] and FID[24]. Identity consistency is measured using the ID metric[13] for self-reenactment and CLIPScore[57] for cross-reenactment, as the ID metric is unreliable for non-human domains. Expression accuracy is evaluated with Average Expression Distance (AED)[38] for self-reenactment and Average Keypoint Distance (AKD)[43] for cross-reenactment, as 3DMM struggles with non-realistic humans. Additionally, Average Pose Distance (APD) is used to assess pose consistency, with pose information extracted using[23]. As shown in Table 1, our method performs slightly worse than 2D approaches in self-reenactment, but remains comparable while surpassing 4D methods in overall effectiveness. In cross-reenactment, although 2D methods better preserve identity, our approach achieves higher accuracy in capturing pose and expression, demonstrating the advantages of 4Dbased techniques. 4.4. Ablation Study We analyze the impact of different data generation pipelines and the performance of each module in our model. Figure 7. Visualization of ablation study on data generation methods. It is only when combining SDEdit and ControlNet that we can ensure the generated images retain both the same expression and pose as the original images. The corresponding prompts are shown above images. Figure 8. Visualization of ablation study on motion-aware crossdomain renderer. The Next3D rendering approach involves using CNN as replacement for our render model. Effectiveness of Different Data Generation Methods.As shown in Figure 7, the leftmost column presents the input images, all from the realistic domain. Without SDEdit, ControlNet provides some control over expressions, but the generated results still deviate significantly from the originals (w/o SDEdit). When using only SDEdit without ControlNet, the results preserve the pose, but the expressions still show noticeable discrepancies (w/o ControlNet). By combining ControlNet and SDEdit, we achieve images that maintain both the expression and pose of the original, while shifting entirely to different domain (Full). This enables the reuse of 3DMM data from the realistic domain to train 4DGANs in various domains. Effectiveness of Models. We design the motion-aware cross-domain renderer to better capture the appearance information from the original image, thereby enhancing fidelity. Additionally, since 3DMM is not perfectly ac8 curate, we incorporate motion embedding to assist in the animation process. As shown in Figure 8, we replaced our renderer with CNN similar to the one used in Next3D. The results exhibited significant identity leakage (i.e., facial artifacts resembling the target subjects mesh), and the generated expressions did not accurately match the target (e.g., eye regions in the second row). In contrast, our method better preserves the source identity, and the implicit motion embedding effectively corrects motion inaccuracies from the mesh. The corresponding quantitative metrics in Table 2show that our approach outperforms all compared methods across all evaluated metrics. 5. Conclusion We propose AvatarArtist, 4D avatar generation model for open-domain inputs. We use parameterized triplane as 4D representation and employ 4D GANs to build an open-domain image-triplane dataset. Specifically, 2D diffusion model generates images from various domains, which train domain-specific 4DGANs to produce data for our model. Our model consists of DiT and motion-aware cross-domain renderer. DiT converts the input image into parameterized triplane, while the renderer refinement module synthesizes and optimizes results. Experiments show that AvatarArtist effectively handles open-domain inputs, successfully transferring target motion while preserving appearance and geometric consistency. 6. Acknowledgment This project was supported by the National Key R&D Program of China under grant number 2022ZD0161501."
        },
        {
            "title": "References",
            "content": "[1] Civitai. https://civitai.com/. 3, 4 [2] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov. 3davatargan: Bridging domains for personalized editable In Proceedings of the IEEE/CVF Conference on avatars. Computer Vision and Pattern Recognition (CVPR), pages 45524562, 2023. 4 [3] Aibek Alanov, Vadim Titov, and Dmitry Vetrov. Hyperdomainnet: Universal domain adaptation for generative Advances in Neural Information adversarial networks. Processing Systems, 35:2941429426, 2022. 4 [4] Qingyan Bai, Zifan Shi, Yinghao Xu, Hao Ouyang, Qiuyu Wang, Ceyuan Yang, Xuan Wang, Gordon Wetzstein, Yujun Shen, and Qifeng Chen. Real-time 3d-aware portrait editing from single image. In European Conference on Computer Vision, pages 344362. Springer, 2024. 4 [5] Egor Burkov, Igor Pasechnik, Artur Grigorev, and Victor Lempitsky. Neural head reenactment with latent pose In Proceedings of the IEEE/CVF conference descriptors. on computer vision and pattern recognition, pages 13786 13795, 2020. 3 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 5 [7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In arXiv, 2021. [8] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. 2, 3 [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 5, 6, 3 [10] Xuangeng Chu and Tatsuya Harada. Generalizable and In The Thirty-eighth Ananimatable gaussian head avatar. nual Conference on Neural Information Processing Systems, 2024. 2, 3 [11] Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, and Tatsuya Harada. GPAvatar: Generalizable In The Twelfth Inand precise head avatar from image(s). ternational Conference on Learning Representations, 2024. 3 [12] Radek Danˇeˇcek, Michael Black, and Timo Bolkart. Emoca: In Emotion driven monocular face capture and animation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031120322, 2022. 3 [13] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 8 [14] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 00, 2019. [15] Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, and Baoyuan Wang. Portrait4d: Learning one-shot 4d head In Proceedings of avatar synthesis using synthetic data. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71197130, 2024. 2, 3, 5, 6, 4 [16] Yu Deng, Duomin Wang, and Baoyuan Wang. Portrait4d-v2: Pseudo multi-view data creates better 4d head synthesizer. arXiv preprint arXiv:2403.13570, 2024. 3, 6, 7 [17] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, and Egor Zakharov. Megaportraits: One-shot megapixel neural head avatars. In 9 Proceedings of the 30th ACM International Conference on Multimedia, pages 26632671, 2022. 3 [18] Yao Feng, Haiwen Feng, Michael Black, and Timo Bolkart. Learning an animatable detailed 3d face model from in-thewild images. ACM Transactions on Graphics (ToG), 40(4): 113, 2021. [19] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. 4 [20] Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, Xuan Wang, Baoyuan Wu, and Yujiu Yang. Toontalker: In Proceedings of the Cross-domain face reenactment. IEEE/CVF International Conference on Computer Vision, pages 76907700, 2023. 3 [21] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 2, 6, 7 [22] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2(3):4, 2022. 3 [23] Thorsten Hempel, Ahmed A. Abdelrahman, and Ayoub AlHamadi. Toward robust and unconstrained full range of rotation head pose estimation. IEEE Transactions on Image Processing, 33:23772387, 2024. 8 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [26] Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu. Depth-aware generative adversarial network for talking head In Proceedings of the IEEE/CVF convideo generation. ference on computer vision and pattern recognition, pages 33973406, 2022. 3 [27] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: real-time nerf-based parametric head In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 20374 20384, 2022. 3 [28] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 5, 8 [29] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021. 3 [30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 2 [31] Gwanghyun Kim and Se Young Chun. Datid-3d: Diversitypreserved domain adaptation using text-to-image diffusion for 3d generative model, 2022. 4, 5, 3 [32] Gwanghyun Kim, Ji Ha Jang, and Se Young Chun. Podia3d: Domain adaptation of 3d generative model across large domain gap using pose-preserved text-to-image diffusion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2260322612, 2023. 4 [33] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 5 [34] Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, and Xuansong Xie. Diffusiongan3d: Boosting text-guided 3d generation and domain adaptation by combining 3d gans In Proceedings of the IEEE/CVF and diffusion priors. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1048710497, 2024. 4 [35] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6):194:1194:17, 2017. 2 [36] Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao, Zhigang Wang, Mulin Chen, Bang Zhang, Zhongjian Wang, Liefeng Bo, and Xuelong Li. One-shot high-fidelity talkinghead synthesis with deformable neural radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1796917978, 2023. 2, 3 [37] Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, and Jan Kautz. Generalizable one-shot 3d neural head avatar. Advances in Neural Information Processing Systems, 36, 2024. 3 [38] C.Z. Lin, D.B. Lindell, E.R. Chan, and G. Wetzstein. 3d gan inversion for controllable portrait image animation. In ECCV Workshop on Learning to Generate 3D Shapes and Scenes, 2022. 8 [39] Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, et al. Human motionformer: Transferring human motions with vision transformers. arXiv preprint arXiv:2302.11306, 2023. [40] Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, and Qifeng Chen. Headartist: Textconditioned 3d head generation with self score distillation. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 4 [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2019. 6 [42] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 6 [43] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. 8 [44] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 3 [45] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024. 2, 3 [46] Zhiyuan Ma, Xiangyu Zhu, Guo-Jun Qi, Zhen Lei, and Lei Zhang. Otavatar: One-shot talking face avatar with controllable tri-plane rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116910, 2023. 2, 3 [47] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2, 4, 5 [48] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3 [49] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 6 [50] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [51] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58655874, 2021. 2 [52] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation In Proceedings of the IEEE/CVF of stylegan imagery. international conference on computer vision, pages 2085 2094, 2021. 4 [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2, 5, 6 [54] Juan Perez, Thu Nguyen-Phuoc, Chen Cao, Artsiom Sanakoyeu, Tomas Simon, Pablo Arbelaez, Bernard Ghanem, Ali Thabet, and Albert Pumarola. Styleavatar: In Proceedings of the Stylizing animatable head avatars. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 86788687, 2024. 4 [55] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 5 [56] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 5 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5, [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [59] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation In Proceedings of the IEEE/CVF using triplane diffusion. Conference on Computer Vision and Pattern Recognition (CVPR), pages 2087520886, 2023. 5 [60] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23772386, 2019. 2, 3 [61] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. 2, 3 [62] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for In Proceedings of the IEEE/CVF articulated animation. Conference on Computer Vision and Pattern Recognition, pages 1365313662, 2021. 2, 3 [63] Luchuan Song, Lele Chen, Celong Liu, Pinxin Liu, and Chenliang Xu. Texttoon: Real-time text toonify head avatar from single video. arXiv preprint arXiv:2410.07160, 2024. 4 [64] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. Next3d: Generative neural texture rasterization for 3d-aware head avatars. In CVPR, 2023. 2, 4, [65] Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Ravi Ramamoorthi, and Koki Nagano. Real-time radiance fields for single-image portrait view synthesis. 2023. 3, 6 [66] Ziyu Wan, Despoina Paschalidou, Ian Huang, Hongyu Liu, Bokui Shen, Xiaoyu Xiang, Jing Liao, and Leonidas Guibas. Cad: photorealistic 3d generation via adversarial distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1019410207, 2024. 4 [67] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1797917989, 2023. 3, 6 11 [80] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9459 9468, 2019. 2, 3 [81] Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, and Fang Wen. Metaportrait: Identity-preserving talking head generIn Proceedings of ation with fast personalized adaptation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2209622105, 2023. 2, [82] Bowen Zhang, Yiji Cheng, Chunyu Wang, Ting Zhang, Jiaolong Yang, Yansong Tang, Feng Zhao, Dong Chen, and Baining Guo. Rodinhd: High-fidelity 3d avatar generation with diffusion models. arXiv preprint arXiv:2407.06938, 2024. 2, 4 [83] Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, and Chunhua Shen. Styleavatar3d: Leveraging image-text diffusion models for high-fidelity 3d avatar generation, 2023. 4 [84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 4, 5 [85] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 8 [86] Jian Zhao and Hui Zhang. Thin-plate spline motion model In Proceedings of the IEEE/CVF for image animation. Conference on Computer Vision and Pattern Recognition, pages 36573666, 2022. 2 [87] Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Incremental gan inversion and Yebin Liu. Invertavatar: In ACM SIGGRAPH 2024 for generalized head avatars. Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. [88] Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Incremental gan inversion In ACM SIGGRAPH 2024 and Yebin Liu. Invertavatar: for generalized head avatars. Conference Papers, pages 110, 2024. 2, 4, 6, 7 [89] Yiming Zhu, Hongyu Liu, Yibing Song, Ziyang Yuan, Xintong Han, Chun Yuan, Qifeng Chen, and Jue Wang. One model to edit them all: Free-form text-driven image Advances in manipulation with semantic modulations. Neural Information Processing Systems, 35:2514625159, 2022. 4 [90] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf: Morphable facial neural radiance field. In European conference on computer vision, pages 268285. Springer, 2022. 3 [68] Lizhen Wang, Zhiyua Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. Faceverse: fine-grained and detailcontrollable 3d face morphable model from hybrid dataset. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2022), 2022. 2, 4, 6 [69] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45634573, 2023. 2, 4, [70] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. Oneshot free-view neural talking-head synthesis for video conIn Proceedings of the IEEE/CVF conference ferencing. on computer vision and pattern recognition, pages 10039 10049, 2021. 3 [71] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animations. arXiv:2403.17694, 2024. 2, 3 [72] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 5, 6, 3 [73] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In Neural Information Processing Systems (NeurIPS), 2021. 3, 6 [74] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchmark In The IEEE Conference for video face super-resolution. on Computer Vision and Pattern Recognition Workshops (CVPRW), 2022. 5, 7, 8 [75] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 3, 6, 7 [76] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. VASA-1: Lifelike audio-driven talking faces generated in real time. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 3 [77] Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, et al. Real3d-portrait: One-shot realistic 3d talking portrait synthesis. arXiv preprint arXiv:2401.08503, 2024. [78] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In European conference on computer vision, pages 85101. Springer, 2022. 3 [79] Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, et al. Nofa: Nerf-based one-shot facial avatar reconstruction. In ACM SIGGRAPH 2023 Conference Proceedings, pages 112, 2023. 3 1 AvatarArtist: Open-Domain 4D Avatarization"
        },
        {
            "title": "Appendix",
            "content": "In the supplementary materials, we first discuss the limitations of our proposed method (Sec. A). Following that, we explore another 4D representation, providing detailed analysis of the parametric triplane (Sec. B).We then explored different model architectures to validate the superiority of our DIT + render approach. We provide additional implementation details, including the domains used during training, the specific training procedures for each model, and other relevant training configurations (Sec. D). We provide additional comparisons and visual results to further demonstrate the effectiveness of our method (Sec. E). Last but not least, we present more results in the supplementary video. A. Limitations While our method can handle inputs from various domains and generate high-fidelity avatars, it does not adequately separate the head region from the background, nor does it decouple neck rotation from the camera pose, which limits the realism of the final results. The 4D representation we employ uses mesh as the primary driving signal. Although we incorporate motion embeddings as supplementary motion signal, the process of obtaining the mesh is both time-consuming and imprecise, which adversely affects the overall efficiency and accuracy of the avatar generation. B. Exploration of the 4D Representation In Portrait4D [15], 4D GAN (GenHead) based on deformation field representation [51] achieved impressive generative results. Specifically, the GenHead consists of part-wise triplane generator Gca for synthesizing the canonical triplane and part-wise deformation field for morphing the canonical head. It generates the 3D deformation field based on FLAME [35] expression coefficients and synthesizes the canonical triplane using the shape parameter from FLAME. During inference, the canonical triplane can be driven by applying the deformation field to compute the offset for each point in the triplane with the corresponding Flame parameters. This canonical tri-plane and deformation field can also form type of 4D representation. However, it is not suitable for our task. First, the deformation field changes according to different facial expressions, making it an unstable representation. In contrast, our representation only varies based on the subjects identity, ensuring consistency across different expressions for the same individual. Additionally, Figure S1. Visualization of generation results of different 4D GANs, including Next3D [64] and GenHead [15], on the unrealistic domain. We use the domain of Lego here. GenHead tends to produce artifacts, whereas Next3D achieves much better results, generating more plausible content. Figure S2. Visualization of different model results. Model and Model are two different end-to-end method which not use the Dit. For more details, please refer the Sec. C. we found that GenHead does not perform well in opendomain generation. We suspect that this representation requires highly precise canonical space modeling, which is particularly challenging for non-realistic domains. In contrast, NeTX3Ds representation focuses more on motion modeling while delegating identity preservation to separate CNN. Compared to GenHead, this representation is more implicit and better suited for generating characters across different domains. (See Figure S1). 2 C. Effectiveness of model design D.3. VAE To demonstrate the clear effectiveness of using DiT model for triplane generation, we conduct experiments comparing it with two feedforward approaches, as illustrated in Fig. S2. Model A, similar to Portrait4D, uses only 4D RGB data, preserving identity well but struggling with motion transfer due to the absence of unified 4D representation and limitations of cross-attention for crossdomain motion retargeting. Model B, which operates without cross-attention, uses an encoder-decoder to convert input images into parametric triplanes and ViT decoder to refine animated features. While effective at transferring expressions, the encoder-decoder based feedforward model fails to reconstruct accurate triplanes, leading to identity loss and making it more challenging for the ViT decoder to bridge the identity gap. In contrast, similar to VASA1 [76], our diffusion + renderer pipeline leverages the target parametric triplanes fitting ability of powerful generative model. This enables our method to simultaneously maintain both motion and identity, achieving the highest quality results. D. More implementation details D.1. Training Domains As mentioned in our main paper, we used 28 domain images during training, including the original realistic domain. We categorize our domains into two types. The first type uses the official Stable Diffusion 2.1 model [58] as the generative model. For this type, the text prompts used are shown in Table S1, and we generate images in 20 different domain styles, with 6,000 images per domain. The second type, as shown in Table S2, utilizes third-party models in Civitai [1] as the generative models, where each model corresponds to specific style. For these models, the same text prompt is used across all models, and we set the prompt as masterpieces, portrait, high-quality. D.2. 4D GAN The 4D GANs (Next3D) for different domains were finetuned from the original FFHQ GAN. Similar to DATID3D [31], the training was stopped once the GAN had seen 200,000 images. We set the batch size to 32 and used 8 A100 GPUs to fine-tune the model for 2 hours. learning rate of 0.002 was used for both the generator and discriminator. For the discriminators input, we applied image blurring, progressively reducing the blur degree as described in [8, 29], and we did not employ style mixing during training. We used the ADA loss combined with R1 regularization, with the regularization coefficient set to λ = 5. Additionally, the strength of the density regularization was set to λden = 0.25. We follow the LVDM [22] and use lightweight 3D autoencoder as our VAE. This VAE consists of an encoder and decoder D. Both the encoder and decoder comprise multiple layers of 3D convolutions. During training, we render the parametric triplane to obtain both depth maps and rendered images, and compute the L1 and LPIPS losses separately. We also add KL divergence loss to ensure that the latent feature distribution is similar to the Gaussian prior p(h) = (0, 1). The weight of L1 loss in triplane and depth is 1, the weight of LPIPS loss in the image is 1, and the weight of KL loss is 1 105. We randomly sample camera poses during rendering, with the sampling ranges set to pitch in [0.25, 0.65] radians, yaw in [0.78, 0.78] radians, and roll in [0.25, 0.25] radians. The visual results of our VAE are shown in Figure S3. D.4. DiT The VAE compresses the triplane into zt R646448. The DiT reshapes zt to 64 256 8, adds positional embeddings, and then flattens it before feeding it into the Transformer for training. Following the approach in Direct3D [72], at each DiT block, we concatenate DINO tokens with the flattened zt and pass them through selfattention mechanism to capture the intrinsic relationships between the DINO tokens and zt. Afterward, we discard the image tokens, retaining only the noisy tokens for input to the next module. Moreover, to reduce the number of parameters and computational cost, we adopt adaLN-single, as introduced in PixArt [9]. This method predicts set of global shift and scale parameters = [γ1, β1, α1, γ2, β2, α2] using time embeddings. trainable embedding is then added to in each block for further adjustment. During training, the batch size is set to 1536, and the training is conducted over 48 Tesla A100 GPUs (batch size 32 for each GPU), each with 80GB of memory, for total of 5 days. D.5. Motion-Aware Cross-Domain Renderer During the Next3D rendering process in Figure. S4, CNN is used to refine the dynamic components after rasterization, eliminating artifacts introduced in the rasterization stage (e.g., teeth completion, identity leakage). When training Next3D for different domains, we fine-tune this CNN, as well as the MLPs used in both super-resolution and neural rendering. Therefore, unified renderer is required to handle parametric triplanes from various domains and mitigate issues caused by rasterization. As mentioned in our main paper, we find simple CNN can not handle the cross-domain parametric triplanes, and we propose the motion-aware cross-domain renderer. To train the motion-aware cross-domain renderer, we use the trained 4DGAN to generate the 4D images (i.e., multiTable S1. List of full-text prompts corresponding to each domain. The images for these domains were generated using SD-V1.5 as the base model, in combination with corresponding prompts."
        },
        {
            "title": "Concise Name of Domain",
            "content": "Pixar Lego Greek statue Elf Zombie Tekken Devil Steampunk Mario Orc Masque Skeleton Peking Opera Yoda Hobbit Stained glass Graffiti Pixel-art Retro Ink Full text prompt 3D render of face in Pixar style 3D render of head of lego man 3D model FHD photo of white Greek statue FHD photo of face of beautiful elf with silver hair in live action movie FHD photo of face of zombie 3D render of Tekken game character FHD photo of face of devil in fantasy movie Steampunk style portrait, mechanical, brass and copper tones 3D render of face of Super Mario FHD photo of face of an orc in fantasy movie FHD photo of face of person in masquerad FHD photo of face of skeleton in fantasy movie FHD photo of face of character in Peking opera with heavy make-up FHD photo of face of Yoda in Star Wars FHD photo of face of Hobbit in Lord of the Rings Stained glass style, portrait, beautiful, translucent Graffiti style portrait, street art, vibrant, urban, detailed, tag pixel art style portrait, low res, blocky, pixel art style Retro game art style portrait, vibrant colors portrait in ink style, black and white image Table S2. List of models used for each domain. The images for these domains were generated using specific models as base models. All models were sourced from Civitai [1], an AI-Generated Content (AIGC) social platform."
        },
        {
            "title": "Concise Name of Domain",
            "content": "3D-Animation Toon AAM Counterfeit Pencil Lyriel XXM Model Name 3D Animation Diffusion-V1.0 ToonYou-Beta6 AAM Anime Mix Counterfeit-V3.0 Pencil Sketch Lyriel-V1.6 XXMix9realistic view, multi-expression images of the same individual), and we are able to simultaneously obtain the corresponding depth, parametric triplane, and rendering features. The data is separated into static and dynamic parts similar to Portrait4D [15], as mentioned in our main paper. The overall training objective of our renderer is defined as follows: = Lre + Lf + Ltri + Ldepth + Lopa + Ladv, (S1) where Lre represents combination of the LPIPS and L1 distances between the generated image Io and its corresponding ground truth. Ltri measures the L1 difference between the generated triplane features and their ground truth. Lf computes the L1 difference between the generated rendering features and their respective ground truth. Ldepth evaluates the L1 difference between the generated depth map and its ground truth counterpart. Lopa is the L1 difference between the predicted opacity and the ground truth. Finally, Ladv represents the adversarial loss between Io and the ground truth image, utilizing the discriminator from the Next3D model. The loss balancing weights for each term in Eq. (S1) are set to 1, 1, 0.1, 1, 1, and 0.01 for Lre, Lf, Ltri, Ldepth, Lopa, and Ladv, respectively. For the first 1000K images, Ladv is not applied, and the parameters in both the neural renderer and super-resolution components are kept fixed. After 1000K images, Ladv is introduced, and the trainable parameters of the neural renderer and super-resolution modules are unfrozen. We employ volume rendering with 48 coarse samples and 48 fine samples per ray. The initial 4 Figure S3. Visualization of reconstruction results of our VAE. The domain is Yoda, 3D-Animation, Zommbie, and Counterfeit, respectively. The ground truth images are generated with the Next3D. volume rendering resolution is set to 642 for the first 1000K images, gradually increasing to 1282 as training progresses. The model is trained on total of 8 million images. We utilize the Adam optimizer with (β1, β2) = (0.9, 0.999) and learning rate of 1 104 across all networks. The batch size is set to 96, with an even split between dynamic and static data. The training is conducted over 24 Tesla A100 GPUs, each with 80GB of memory, for total of 4 days. E. Additional Comparisons and Visual Results E.1. User Study For more comprehensive evaluation, we conducted user study with 10 participants, who were asked to assess image sharpness, temporal consistency, expression consistency, and identity consistency. They did so by selecting the best method while reviewing 12 cross-ID reenactment results generated by different approaches. Model Trained Domains / Untrained Domains Sharpness Temporal Expression Identity LivePortrait Xportrait Invertavatar Portrait4D 3.625 / 2.5 2.375 / 1 2.625 / 2.5 1.875 / 3.5 3.625 / 2.5 1.625 / 1 2.25 / 2.5 2.5 / 3 3.5 / 1.5 2 / 1 2.375 / 2 2.125 / 2.5 3.5 / 1.5 1.875 / 1.5 2.75 / 2.5 2.375 / 2 Ours 4.625 / 5 4.25 / 5 4.375 / 4.5 4.125 / 5 Table S3. User Study. For each evaluation criterion, participants were presented with five videos, each corresponding to the results produced by different method. They were instructed to rate the videos on scale from 1 to 5, where 5 indicates the highest quality and 1 the lowest. Multiple methods could receive the same score. As shown in Table S3, our method exhibits significant advantages over the others. 5 Figure S4. Visualization of rendering process of Next3D. After rasterization, CNN is employed to remove artifacts introduced during the rasterization process, which is critical for final performance, as mentioned in the Next3D [64]. E.2. Visual Comparisons In Figure S6, we present additional visual comparisons, demonstrating that our method achieves superior performance. Moreover, we present our geometric results in Figure S5. For more visual results, please refer to our video results. 6 Figure S5. The geometry results of our method. 7 Figure S6. Qualitative comparison with state-of-the-art methods. The leftmost column of the figure presents the input images, with the bottom-right corner indicating the target image. The first row illustrates the results of self-reenactment, while the subsequent rows showcase the results of cross-reenactment. Our method demonstrates superior performance in terms of expression and pose consistency, as well as identity preservation. For more visual results, please refer to our video results."
        }
    ],
    "affiliations": [
        "Ant Group",
        "City University of Hong Kong",
        "HKUST"
    ]
}