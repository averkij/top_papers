{
    "paper_title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
    "authors": [
        "Haokun Lin",
        "Haobo Xu",
        "Yichen Wu",
        "Ziyu Guo",
        "Renrui Zhang",
        "Zhichao Lu",
        "Ying Wei",
        "Qingfu Zhang",
        "Zhenan Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 9 8 4 1 . 8 0 5 2 : r Quantization Meets dLLMs: Systematic Study of Post-training Quantization for Diffusion LLMs Haokun Lin 1,3, Haobo Xu 2, Yichen Wu3,4, Ziyu Guo5, Renrui Zhang5 , Zhichao Lu3, Ying Wei6, Qingfu Zhang3, Zhenan Sun1 Equal Contribution 1 NLPR & MAIS, Institute of Automation, CAS 3 City University of Hong Kong 5 The Chinese University of Hong Kong 2 Tsinghua University 6 Zhejiang University 4 Harvard University"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion large language models (dLLMs) have introduced promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable success in wide range of text generation tasks, with auto-regressive architecturessuch as GPT [Brown et al., 2020a,b, Achiam et al., 2023], LLaMA [Touvron et al., 2023a,b, Dubey et al., 2024], and the Qwen [Bai et al., 2023, Qwen et al., 2025, Yang et al., 2025a] seriesdominating recent advances in both research and application. Recently, diffusion-based large language models (dLLMs) have emerged as promising alternative for natural language generation [Nie et al., 2025, Zhu et al., 2025, Ye et al., 2025a, Gong et al., 2024, Song et al., 2025]. By leveraging bidirectional context encoding and iterative denoising, dLLMs offer finer-grained control over the generation process compared to traditional auto-regressive approaches. Despite their potential, the efficient deployment of dLLMs remains challenging, as the increased number of model parameters often leads to significantly higher memory usage and computational cost [Li et al., 2025, Yu et al., 2025]. Current efforts toward optimizing dLLM inference have primarily focused on designing specialized key-value (KV) cache mechanisms [Wu et al., 2025, Ma et al., 2025, Liu et al., 2025b, Wang Technical Report Figure 1: Visualizations of activation outliers in LLaDA-8B-Base (1) and LLaDA-8B-Instruct (2). Outliers are observed at the inputs of various linear layers and can be classified as Normal Outliers (a(1)c(1)/a(2)c(2)), with relatively large magnitudes across tokens, and Massive Outliers (d(1), d(2)), with extremely large values on few tokens. Notably, these massive outliers are identified at the second linear layer of the feed-forward network (FFN) module. et al., 2025]. However, quantization [Li et al., 2024, Liu et al., 2025a, Wei et al., 2025], wellestablished yet orthogonal technique for compressing and accelerating neural networks, has been largely underexplored in the context of dLLMs. In the domain of auto-regressive LLMs, post-training quantization(PTQ) [Chee et al., 2024, Ashkboos et al., 2023, Tseng et al., 2024, Zhao et al., 2023] has been widely adopted to reduce the memory footprint of weights and activations, and to enable faster inference through kernel-level optimization. Yet, how well existing PTQ techniques generalize to diffusion LLMs remains an open and intriguing question. In this paper, we present comprehensive study on the quantization of diffusion-based large language models (dLLMs). First, we identify that dLLMs exhibit clear activation outliersi.e., unusually large activation valueswhich are known to be key challenge for low-bit quantization [Dettmers et al., 2022, Xiao et al., 2023, Sun et al., 2024]. Specifically, as shown in Figure 1 and 2, we observe such outliers across multiple layers and input activations in LLaDA-Base, LLaDA-Instruct [Nie et al., 2025], and Dream [Ye et al., 2025a] models, suggesting that this is common phenomenon across different dLLMs. Second, we implement state-of-the-art weight-only [Lin et al., 2023, Frantar et al., 2022] and weight-activation quantization [Xiao et al., 2023, Ashkboos et al., 2024, Lin et al., 2024b] methods on representative diffusion models and conduct detailed analysis from the following perspectives: Bit-width effects: We find that 4-bit is the most effective configuration for weight-only quantization, while 8-bit is recommended for weight-activation quantization as near-lossless setting. Quantization methods: Through extensive evaluation, we observe that GPTQ consistently outperforms AWQ across most tasks. For weight-activation quantization, rotation-based methods such as DuQuant and QuaRot demonstrate clear advantages over SmoothQuant. Task type sensitivity: While most PTQ methods perform competitively on general QA benchmarks, we observe notable degradation on more complex tasks such as math reasoning and code generation. Model type robustness: Our results show that the instruction-tuned LLaDA model exhibits greater robustness to quantization compared to the base counterpart. To the best of our knowledge, this is the first systematic evaluation of post-training quantization on diffusion LLMs. We hope our findings provide valuable guidance for the community and inspire further research toward efficient and deployable dLLMs. 2 Figure 2: Visualizations of activation outliers in Dream-7B-Base. We observe relatively large normal outliers in the input to the FFN up-projection layer (c), while the massive outliers (d) exhibit smaller peak values compared to those in LLaDA models (Figure 1)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion Language Model Diffusion models have achieved remarkable success in image, video, and audio generation by learning to reverse forward noise process [Jiang et al., 2025, Zhao et al., 2024a]. However, applying diffusion to language generation presents unique challenges due to the discrete nature of textual data. To address this, DiffusionBERT [He et al., 2022] leverages BERT [Devlin et al., 2019] architecture to model the reverse dynamics of discrete diffusion process with an absorbing state, as proposed in D3PM [Austin et al., 2021a]. More recently, Masked Diffusion Models (MDMs) [Lou et al., 2023, Ou et al., 2024, Shi et al., 2024] have drawn increasing attention by adopting forward process that progressively replaces input tokens with designated [MASK] token. This year, efforts have been made to scale up MDMs to the billion-parameter regime. Representative examples include LLaDA-8B [Nie et al., 2025], which utilizes bidirectional Transformer as the mask denoiser and achieves performance comparable to LLaMA [Dubey et al., 2024], and Dream [Ye et al., 2025a], which is initialized from pre-trained autoregressive model and delivers competitive generation capabilities. These advancements indicate that diffusion-based approaches offer viable alternative paradigm for language modeling. Despite these encouraging results, the deployment of diffusion large language models (dLLMs) [Gong et al., 2024, Yang et al., 2025c] remains constrained by the computational demands of Transformerbased architectures, which involve hundreds of millions of parameters. To address this, we explore the potential of extending established post-training quantization techniques from conventional LLMs to the dLLM models, aiming to reduce memory footprint and accelerate inference while preserving generation quality. Notably, some recent works [Wu et al., 2025, Liu et al., 2025b, Ma et al., 2025] propose caching strategies to accelerate the inference of dLLMs. Our work is orthogonal to these efforts and can be seamlessly integrated by quantizing dLLM caches. 2.2 Network Quantization Compared to pruning and distillation [Lin et al., 2024a, Zhang et al., 2024, Xing et al., 2025, Ye et al., 2025c,b], quantization has been extensively studied as an effective technique to compress neural networks by using low-bit representations for high-precision tensors [Zhao et al., 2024b, Xu et al., 2024, Yang et al., 2025b, Huang and Wu, 2025]. Existing methods are typically categorized into two groups: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ [Wu et al., 2024, Yang et al., 2024] applies quantization after model training, while QAT [Tao et al., 2022, Chen et al., 2024, 2025] incorporates quantization effects during training. Due to the high computational cost of training large language models (LLMs), PTQ has become increasingly popular for its efficiency and ability to preserve model performance without retraining [Liu et al., 2024, Dong et al., 2024]. In this work, we follow this paradigm and focus on applying PTQ to dLLMs. Weight-only quantization compresses the model by quantizing weight matrices, effectively reducing model size and memory access during inference. For example, GPTQ [Frantar et al., 2022] extends the Optimal Brain Quantization [Frantar and Alistarh, 2022] algorithm to LLMs, AWQ [Lin et al., 3 2023] introduces reparameterization strategy to alleviate the difficulty of weight quantization, and SqueezeLLM [Kim et al., 2023] employs non-uniform quantization to improve compression quality. Weight-activation quantization quantizes both the model weights and input activations, enabling further inference acceleration by leveraging integer matrix multiplication kernels. SmoothQuant [Xiao et al., 2023] proposes to shift the quantization difficulty from activations to weights via scaling. OmniQuant [Shao et al., 2023] jointly optimizes clipping thresholds and scaling factors for improved quantization fidelity. More recently, rotation-based methods [Lin et al., 2024c, Hu et al., 2025] have demonstrated superior performance: QuaRot [Ashkboos et al., 2024] introduces Hadamard-based rotation to smooth the weight-activation landscape, while DuQuant [Lin et al., 2024b] leverages outlier-aware rotation matrices and channel permutation to better align the activation distribution with quantization-friendly structures. In this work, we provide comprehensive evaluation of state-of-the-art LLM-oriented PTQ methods applied to diffusion-based language models. All methods are re-implemented on dLLMs, and we present in-depth analyses and insights into their quantization performance."
        },
        {
            "title": "3 Preliminary and Observation",
            "content": "3.1 Masked Diffusion Model Masked diffusion model is variant of diffusion-based generative models that incorporates binary mask into the denoising process. Instead of reconstructing the entire input, the model focuses on predicting the corrupted or missing regions while preserving the observed parts. Specifically, given an input and mask m, the forward process adds Gaussian noise to the unmasked regions, producing noised sample xt at step t. The reverse process is then parameterized by neural network ϵθ, which estimates the noise conditioned on both the timestep and the mask. The training objective is, LMDM = Ex,m,ϵ,t (cid:2) ϵ ϵθ(xt, m, t)2 (cid:3) , where ϵ denotes the Gaussian noise, and ϵθ learns to predict and remove it under the masking constraint. 3.2 Quantization Quantization coverts the floating-point tensor into low-bit integer Xq. Specifically, the b-bit uniform quantization can be represented as: Xq = clamp (cid:18)(cid:22) (cid:25) +z, 0, 2b 1 (cid:19) , where = max(X) min(X) 2b 1 , = (cid:22) min(X) (cid:25) . (1) The notation means the nearest rounding operation, is the quantization step size and denotes the zero point. 3.3 Outliers in dLLMs Outliers, prominent characteristic of large language models (LLMs), are primarily determined by relatively large activation values [Dettmers et al., 2022]. These outliers are typically categorized into two types: normal outliers and massive outliers [Lin et al., 2024b]. Normal outliers [Xiao et al., 2023] refer to activations across all tokens with relatively large magnitudes, and they are the more prevalent type. Massive outliers [Sun et al., 2024, Liu et al., 2024], on the other hand, exhibit significantly larger values at limited set of tokens. These outliers present substantial challenges for LLM quantization. Whether dLLMs contain these outliers remains an important yet under-explored question. In this work, we provide detailed preliminary exploration and identify the presence of outliers in dLLMs. We first identify the presence of activation outliers in diffusion-based language models. Specifically, we randomly sample batch of calibration data from the WikiText-2 dataset [Merity et al., 2016] and use it as input for single forward pass to visualize the activation distributions across different 4 layers. As shown in Figure 1, we observe clear outliers in the input activations of both LLaDA-8Bbase and LLaDA-8B-instruct. These outliers can be categorized into two types: Normal Outliers and Massive Outliers, consistent with the taxonomy observed in standard LLMs. Interestingly, the Massive Outliers tend to occur in the second linear layer of the feed-forward network (FFN) modules, mirroring patterns reported in previous studies on conventional LLMs [An et al., 2025]. However, compared to LLMs, the Normal Outliers in LLaDA exhibit slightly lower magnitudes, indicating less extreme but still significant deviation. Another key difference is that massive outliers in dLLMs appear across more tokens, rather than being restricted to only few tokens as in LLMs. This broader distribution increases the difficulty of weight-activation quantization, as it reduces the effectiveness of global clipping or scaling strategies. This observation is corroborated by the near-zero performance of SmoothQuant under W4A4 settings (see Table 4), suggesting that existing outlier-handling strategies may be insufficient for dLLMs in low-bit quantization regimes. Furthermore, we also detect similar outlier patterns in the Dream-7B model, as visualized in Figure 2. This indicates that the existence of outliers is not specific to particular model architecture, but rather general phenomenon across diffusion-based language models. These findings highlight the need for careful handling of outliers during the quantization process, especially when targeting both weights and activations under aggressive bit-width constraints."
        },
        {
            "title": "4 Quantizing Diffusion LLM",
            "content": "In this section, we conduct experiments to address the overarching question: How does quantization affect diffusion-based language models? To systematically explore this, we further investigate the following sub-questions: RQ1: What are the preferred bit-widths for weight-only and weight-activation quantization? RQ2: What are the most effective quantization methods for dLLMs? RQ3: How do different task categories influence the performance of quantized dLLMs? RQ4: How does quantization affect different types of dLLMs? 4.1 Experimental Setup Evaluated dLLMs and Quantization Baselines. We conduct comprehensive evaluations on three recent diffusion-based language models, LLaDA-8B-Base, LLaDA-8B-Instruct [Nie et al., 2025] and Dream 7B-Base [Ye et al., 2025a]. For weight-only quantization, we adopt state-of-the-art baselines GPTQ [Frantar et al., 2022] and AWQ [Lin et al., 2023], which are widely used in LLM quantization. We utilize group-wise per-channel quantization and set the group size to 128. For weight-activation quantization, we evaluate SmoothQuant [Xiao et al., 2023] as well as recent rotation-based approaches, including QuaRot [Ashkboos et al., 2024] and DuQuant [Lin et al., 2024b]. Following standard practice, we apply per-channel quantization to weights and per-token quantization to activations. We select calibration data (128 samples) from WiKiText2 [Merity et al., 2016] for baselines, except Pile [Gao et al., 2020] for AWQ. More details are illustrated in Appendix A. Evaluation Benchmarks. We evaluate the performance of quantized dLLMs across three task categories, following the setup of LLaDA [Nie et al., 2025]: 1). General knowledge tasks, including MMLU [Hendrycks et al., 2020], ARC-E, ARC-C [Clark et al., 2018], Hellaswag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], and PIQA [Bisk et al., 2020]; 2). Mathematical reasoning tasks, such as GSM8K [Cobbe et al., 2021] and Math [Hendrycks et al., 2021]; and 3).Code generation tasks, including HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021b]. These benchmarks collectively provide comprehensive assessment of quantized dLLMs from multiple perspectives. Evaluation Metrics. We report accuracy on widely used QA and math benchmarks, and adopt Pass@1 as the evaluation metric for code generation tasks. Performance degradation relative to fullprecision models is used as the primary metric for assessing different quantized dLLMs. Following [Liu et al., 2025a], we categorize the performance degradation compared to full-precision models into three levels: negligible(<1%), moderate (14%), and significant (>4%). 5 Table 1: Model performance on general tasks under weight-only quantization. Model Setting Method WinoGrande PIQA ARC-C ARC-E Hellaswag MMLU 5-shot Avg Drop FP Model - LLaDA-8B W4A16 g128 W3A16 g128 GPTQ AWQ GPTQ AWQ FP Model - LLaDA-8B -Instruct W4A16 g128 W3A16 g128 GPTQ AWQ GPTQ AWQ 69. 69.7 67.3 67.2 66.4 70.2 69.2 68.8 67.4 66.3 74. 73.9 70.3 73.3 69.2 71.3 74.2 71.0 73.7 69.8 46. 47.9 44.5 45.7 42.8 54.3 54.8 53.3 50.7 50.5 71. 72.5 73.4 71.1 71.8 75.9 77.5 76.1 76.4 74.7 70. 70.4 68.4 68.8 66.4 68.6 68.3 68.1 66.7 66.4 65. 64.7 65.6 63.5 64.0 64.0 63.4 63.4 62.1 62.3 65. 65.3 63.2 63.7 61.8 65.7 66.0 64.9 64.1 63.1 - 0.3% 3.5% 2.7% 5.6% - 0.3% 0.1% 2.4% 4.0% Table 2: Model performance on general tasks under weight-activation quantization. Model Setting Method WinoGrande PIQA ARC-C ARC-E MMLU Avg Drop FP Model - LLaDA-8B W8A8 W4A4 SmoothQuant QuaRot DuQuant SmoothQuant QuaRot DuQuant FP Model - LLaDA-8B -Instruct W8A8 W4A4 SmoothQuant QuaRot DuQuant SmoothQuant QuaRot DuQuant 69.9 67.7 68.6 67.9 49.4 63.4 64.9 70.2 69.6 69.1 68. 52.3 65.2 66.4 74.6 70.8 71.1 70.4 58.8 68.1 69.3 71.3 72.1 71.3 71. 65.1 69.8 72.2 46.4 45.5 45.2 45.9 29.2 43.7 42.8 54.3 53.5 54.1 54. 34.0 51.3 52.7 71.1 70.5 70.8 71.4 40.9 69.2 70.0 75.9 75.9 76.2 76. 54.0 75.1 74.8 65.7 65.0 66.1 66.0 27.1 61.8 64.0 64.0 64.0 64.1 63. 32.8 61.1 61.2 65.5 63.9 64.4 64.3 41.1 59.2 62.2 67.1 67.0 67.0 67. 47.7 64.5 65.4 - 2.5% 1.8% 1.9% 37.3% 6.6% 5.1% - 0.2% 0.3% 0.2% 29.0% 3.9% 2.5% 4.2 Ideal Quantization Bit Precision (RQ1) 4-bit is the Recommended Choice for Weight-Only Quantization. We observe that both GPTQ and AWQ perform well on general commonsense QA and math tasks under 4-bit quantization (Table 1 and Table 3). In most cases, the performance degradation remains within the negligible to moderate range (i.e., <4%). For example, 4-bit GPTQ-quantized LLaDA-8B-instruct slightly improves the average accuracy on six QA tasks from 65.7% to 66.0%, and shows only minor drop of 0.6% on the MATH and GSM8K benchmarks. In contrast, reducing the quantization bit-width to 3-bit leads to significant performance drop, particularly on math and code generation tasks, as shown in Table 3. Therefore, we recommend 4-bit quantization as the standard configuration for weight-only quantization of diffusion-based LLMs. The development of more robust 3-bit quantization methods remains an open research direction. Weight-Activation Quantization: 8-bit is Tolerable, While 4-bit Remains Challenging. As shown in Table 2 and Table 4, quantizing LLaDA models to W8A8 results in only minor performance degradation, largely independent of the specific quantization method. This suggests that even simple techniques such as SmoothQuant are effective in mitigating activation outliers in LLaDA models, leading to nearly lossless quantized variants. However, reducing precision to W4A4 introduces sharp performance drop across most benchmarks. In the majority of cases, performance degradation exceeds the significant threshold (>4%). For instance, SmoothQuant experiences drop of over 20% across all evaluated tasks, indicating that the simple rebalancing between weights and activations is insufficient under low-precision settings for dLLMs. The degradation is especially pronounced in base models, with accuracy drops exceeding 10% on code generation tasks and math reasoning-heavy benchmarks. These results highlight the difficulty of achieving effective 4-bit weight-activation quantization in dLLMs, and point to the need for more advanced techniques. Improving performance under this challenging setting remains an open research problem for the community. Table 3: Model performance on mathematics and code tasks under weight-only quantization. Model Setting Method GSM8K (4-shot) Math (0-shot) Gen Len 256 Gen Len Avg Drop HumanEval (0-shot) MBPP (3-shot ) Gen Len 512 Gen Len 512 Avg Drop FP Model LLaDA-8B W4A16 g128 LLaDA-8B -Instruct W3A16 FP Model W4A16 g128 W3A16 g128 - GPTQ AWQ GPTQ AWQ - GPTQ AWQ GPTQ AWQ 69.7 68.5 67.4 63.3 64. 78.5 78.8 78.9 76.4 76.3 21.3 21.3 20.6 13.4 17. 33.5 32.4 33.6 30.0 30.1 45.5 44.9 44.0 38.4 40. 56.0 55.6 56.2 53.2 53.2 - 1.4% 3.2% 15.7% 10.7% - 0.6% 0.5% 5.0% 5.0% 32.9 28.7 29.9 26.2 28. 37.8 36.6 37.1 34.2 34.1 39.4 39.4 37.2 35.4 34. 37.4 33.8 35.6 30.0 31.8 36.2 34.0 33.5 30.8 31. 37.6 35.2 36.4 32.1 33.0 - 5.9% 7.3% 14.8% 13.9% 6.4% 3.2% 14.7% 12.4% Table 4: Model performance on mathematics and code tasks under weight-activation quantization. Model Setting Method GSM8K (4-shot) Math (0-shot) Gen Len 256 Gen Len 256 Avg Drop HumanEval (0-shot) MBPP (3-shot ) Gen Len Gen Len 512 Avg Drop FP Model - LLaDA-8B W8A8 W4A4 SmoothQuant QuaRot DuQuant SmoothQuant QuaRot DuQuant FP Model - LLaDA-8B -Instruct W8A8 W4A4 SmoothQuant QuaRot DuQuant SmoothQuant QuaRot DuQuant 69. 69.4 69.9 70.7 0.3 62.9 64.4 78.5 78.2 78.9 78.1 2.7 75.1 77.3 21. 20.2 20.7 20.7 2.0 15.2 14.8 33.5 33.3 33.1 33.3 2.4 29.9 30.7 45. 44.8 69.9 45.7 1.2 62.9 39.6 56.0 55.7 78.9 55.7 2.6 75.1 54.0 - 1.6% 0.4% 0.4% 97.4% 14.1% 13.0% - 0.4% 0.1% 0.5% 95.4% 6.2% 3.5% 32. 27.4 31.7 33.5 0.0 23.8 25.6 37.8 37.2 35.4 37.2 0.0 32.3 34.8 39. 40.2 40.6 38.8 0.0 34.6 33.6 37.4 37.1 36.6 37.4 0.6 32.8 29.2 36. 33.8 36.2 36.2 0.0 29.2 29.6 37.6 38.6 36.0 37.3 0.3 32.6 32.0 - 6.5% 0.0% 0.0% 100.0% 19.3% 18.1% - 1.3% 4.3% 0.8% 99.2% 13.4% 14.9% 4.3 Optimal Quantization Methods (RQ2) GPTQ Outperforms AWQ on Most Tasks As shown in Table 1, GPTQ outperforms AWQ on average accuracy under both 3-bit and 4-bit quantization for LLaDA-8B and LLaDA-8B-instruct. This demonstrates the reliability and competitiveness of GPTQ, particularly on QA tasks. This trend also holds for math reasoning tasks, except for the 3-bit quantization setting on LLaDA-8B, where both GPTQ and AWQ suffer critical performance degradation (>10%). We hypothesize that the suboptimal performance of AWQ may stem from the fact that activation outliers in the LLaDA model series are less prominent than in traditional LLMs. Since AWQ identifies the top 1% of salient weights using activation-driven statistics, its effectiveness can be reduced when the outlier structure is weak in LLaDA models, thereby diminishing its advantage. For code generation tasks, the situation becomes more complex. Both GPTQ and AWQ fail to maintain acceptable performance on the HumanEval and MBPP benchmarks under low-bit quantization. more detailed analysis of these results is provided in Section 4.4. Notably, AWQ performs relatively better than GPTQ in the 3-bit configuration for code tasks, suggesting some resilience under extreme compression. Considering all evaluations across task types and bit-widths, we recommend GPTQ as the safer and more generally effective choice for weight-only quantization of diffusion-based language models. Rotation-Based Methods Achieve Leading Performance Under Weight-Activation Quantization. For both LLaDA-8B and LLaDA-8B-instruct, rotation-based methodsQuaRot and DuQuantconsistently outperform SmoothQuant across all evaluation tasks and quantization settings. The advantage becomes especially pronounced under 4-bit weight-activation quantization, where SmoothQuant suffers near-complete performance collapse on code and math tasks. In contrast, rotation-based approaches retain non-trivial portion of model capability, highlighting their robustness in low-precision settings. These results suggest that rotation transformations are more effective in mitigating activation outliers in dLLMs, which aligns with findings from prior studies in the LLM community [Lin et al., 2024b, Ashkboos et al., 2024]. When comparing QuaRot and DuQuant in detail, our experiments show that DuQuant consistently outperforms QuaRot across most scenarios. For instance, on commonsense QA tasks, DuQuant achieves lower performance drops than QuaRot for both LLaDA-8B (5.1% vs. 6.6%) and LLaDA-8B-instruct (2.5% vs. 3.9%). This observation remains consistent across math and code generation tasks. Consequently, we recommend DuQuant as the most effective method for weight-activation quantization in diffusion-based language models. 7 Table 5: Evaluation of weight-only quantized Dream-7B on general tasks. Model Setting Method WinoGrande PIQA ARC-C ARC-E Avg Drop FP Model - Dream-7B W4A16 W3A16 g128 GPTQ AWQ GPTQ AWQ 68.4 68.2 65.2 63.3 62. 74.4 73.9 69.6 69.6 67.7 59.0 58.1 55.8 49.9 50. 83.1 82.1 82.0 73.4 74.5 71.2 70.6 68.2 64.1 63. - 0.8% 4.3% 10.1% 10.3% 4.4 Influence of Task Categories on Quantization (RQ3) Quantization is More Challenging for Math and Code Tasks. Compared to general-purpose benchmarksprimarily QA tasks as shown in Table 1 and Table 2quantized models experience significantly larger performance drops on math and code tasks, illustrated in Table 3 and Table 4. For math reasoning tasks, both AWQ and GPTQ exhibit substantial degradation under 3-bit quantization (see Table 3), despite maintaining competitive performance on general QA benchmarks. similar trend is observed for rotation-based methods under W4A4 configurations. This degradation may be attributed to the multi-step reasoning nature of math problems, which amplifies the cumulative effect of quantization errors. In such tasks, precise intermediate representations are critical; even small perturbations introduced by low-bit quantization can propagate and compound, ultimately leading to incorrect final answers. In code generation tasks, the challenges are even more pronounced. Under 4-bit quantization, GPTQ and AWQ show performance drops exceeding 5%, while QuaRot and DuQuant degrade by over 10% under W4A4 for both LLaDA-8B and LLaDA-8B-instruct models. We also observe that the standard deviation on the HumanEval benchmark is relatively high, approximately 3%, indicating that more robust and stable benchmarks may be needed to accurately assess code generation capabilities under quantization. Code generation tasks often require the model to maintain long-range context and generate syntactically correct, semantically meaningful sequences. These demands are highly sensitive to the precision of both weights and activations. Quantization-induced distortion in attention patterns or token representations can disrupt code syntax or logic, causing severe performance degradation. These observations highlight that math and code tasks impose stricter precision requirements than simpler retrieval-based or classification-style QA tasks. Maintaining accurate intermediate states, multi-hop logic, and long-context dependency are especially vulnerable under aggressive quantization. Consequently, task-specific quantization strategies or adaptive precision control mechanisms may be necessary to improve the robustness of dLLMs on math and code benchmarks. This represents critical direction for future research in efficient diffusion-based LLM deployment. 4.5 Impact of Model Types (RQ4) Instruct-Tuned Models are More Robust than Base Models. We observe an interesting phenomenon: LLaDA-8B-instruct consistently exhibits smaller performance degradation than its base counterpart (LLaDA-8B) under nearly all quantization settings. For instance, under general tasks, both DuQuant and QuaRot result in only minor accuracy drops for the instruct model, whereas the drop exceeds 5% for the base model. This trend remains consistent across more challenging math and code tasks. For example, 3-bit quantized GPTQ and AWQ lead to performance degradation of approximately 5% for the instruct variant, while the base model suffers drops as high as 10%. Our Observations Hold Consistently across Different dLLMs. To assess the generality of our findings, we further evaluate various quantization methods on different diffusion-based language model: Dream-7B. As shown in Table 5, both GPTQ and AWQ perform competitively under 4bit quantization, while performance drops become more pronounced in the 3-bit setting. This observation reinforces our recommendation that 4-bit quantization offers near-lossless trade-off between efficiency and performance. Moreover, GPTQ consistently outperforms AWQ across nearly all benchmarks, suggesting that GPTQ is more reliable choice across different types of dLLMs. Notably, the 3-bit quantized models exhibit risk-level degradation even on general tasks, indicating that aggressive quantization may be more challenging for the Dream model series compared to LLaDA. 8 Due to resource constraints, we did not evaluate weight-activation quantization for Dream-7B. We leave this for future work as part of our ongoing exploration."
        },
        {
            "title": "5 Limitation and Future Work",
            "content": "In this work, our primary focus is on evaluating downstream task performance of quantized dLLMs. Quantization offers an effective way to reduce memory consumption and accelerate inference. However, fully integrating low-bit inference for diffusion LLMs remains challenging. Specifically, adapting existing LLM-optimized kernels to the architectural characteristics of diffusion LLMs involves substantial engineering effort, which we leave for future work. We plan to continue this line of research along the following directions: 1). Expanded Evaluation: We will provide more comprehensive evaluation across broader set of dLLMs, tasks, and model sizes. 2). Stepwise Analysis: We aim to explore how the number of generation steps in diffusion decoding interacts with quantization levels, and whether step-aware quantization strategies can be beneficial. 3). Remasking Strategies: We intend to evaluate different remasking strategies under quantized settings, and provide practical guidance on selecting suitable quantization configurations. We hope our work initiates further discussion and exploration in the community. To facilitate future research, we will release our code and implementation details to support the development and deployment of quantized diffusion LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "This work provides the first in-depth investigation into the challenges and opportunities of applying post-training quantization (PTQ) to diffusion-based language models (dLLMs). Through extensive empirical evaluation, we uncover several key findings: (1) activation outliers are prevalent across dLLMs and are critical barriers to low-bit quantization; (2) certain PTQ methods, GPTQ and DuQuant, demonstrate notable advantages under constrained settings; and (3) quantization behavior varies across tasks and model types, with instruct-tuned models showing greater resilience. These findings offer practical guidance for designing more effective and robust quantization strategies. Looking forward, we believe that our study lays the groundwork for future research in compression of dLLMs, enabling their deployment in real-world, resource-constrained environments."
        },
        {
            "title": "A Additional Implementation Details",
            "content": "Weight-only Quantization Methods. For GPTQ, we use 128 calibration samples from the WikiText-2 dataset with sequence length of 2048. We adopt asymmetric quantization and set the group size to 128. The method is implemented using the AutoGPTQ repository1. For AWQ, we use 128 samples from the PileVal dataset with sequence length of 512. We implement AWQ with the llm-awq repository2 and apply the same settings as GPTQ, using asymmetric quantization and group size of 128. Weight-activation Quantization Methods. For SmoothQuant, we set the hyperparameter α = 0.5 in the scaling equation to compute the diagonal matrix: sj = max(Xj)α/ max(Wj)1α. We use 128 calibration samples from the WikiText-2 dataset with sequence length of 2048. Asymmetric per-tensor quantization is applied to weights, and per-channel quantization is applied to activations. For QuaRot, we follow the original configuration by preserving 16-bit precision for query states, and applying symmetric activation quantization. We also use WikiText-2 (128 samples, sequence length 2048) as the calibration dataset. For DuQuant, we use the same calibration setup and α value as SmoothQuant. Additionally, we apply activation and weight clipping ratios of 0.9 and 0.8, respectively. The rotation step is set to 256, and the block size is 128. General Tasks. We employ the lm-evaluation-harness3 repository to benchmark models across all tasks. For general tasks other than MMLU, we adopt 0-shot setting with 128 Monte Carlo samples. For MMLU, we use 5-shot setting with single Monte Carlo sample. To evaluate LLaDA models, we configure the diffusion steps, block size, and generation length to 1024, set the classifier-free guidance (CFG) scale and temperature to 0.0, and apply the low confidence remasking strategy. For Dream, we set the maximum number of new tokens to 128, the CFG scale to 1.0, the temperature to 0.0, and the top-p threshold (probability of retaining generated tokens) to 0.95. Mathematics and Code Tasks. The benchmarking details of LLaDA on mathematics and code tasks are provided in Tab. A1. All other configurations remain the same as in the general tasks. Table A1: Configuration for mathematics and code tasks. Dataset # fewshots generation length diffusion steps block size GSM8K Math HumanEval MBPP 4 0 0 256 256 512 512 256 256 512 512 32 64"
        },
        {
            "title": "B More Visualizations",
            "content": "Figure B1: More visualizations of activation outliers in LLaDA-8B-Base. 1https://github.com/AutoGPTQ/AutoGPTQ. 2https://github.com/mit-han-lab/llm-awq. 3https://github.com/EleutherAI/lm-evaluation-harness 10 Figure B2: More visualizations of activation outliers in LLaDA-8B-Instruct."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Systematic outliers in large language models. arXiv preprint arXiv:2502.06415, 2025. Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020a. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020b. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios 11 Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062, 2024. Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, et al. Scaling law for quantization-aware training. arXiv preprint arXiv:2505.14302, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. In Conference on Neural Information Processing Systems, 2022. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. Peijie Dong, Lujun Li, Yuedong Zhong, Dayou Du, Ruibo Fan, Yuhan Chen, Zhenheng Tang, Qiang Wang, Wei Xue, Yike Guo, et al. Stbllm: Breaking the 1-bit barrier with structured binary llms. arXiv preprint arXiv:2408.01803, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elias Frantar and Dan Alistarh. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020. 12 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Xing Hu, Yuan Cheng, Dawei Yang, Zukang Xu, Zhihang Yuan, Jiangyong Yu, Chen Xu, Zhe Jiang, and Sifan Zhou. Ostquant: Refining large language model quantization with orthogonal and scaling transformations for better distribution fitting. arXiv preprint arXiv:2501.13987, 2025. Hong Huang and Dapeng Wu. Quaff: Quantized parameter-efficient fine-tuning under outlier spatial stability hypothesis. arXiv preprint arXiv:2505.14742, 2025. Yue Jiang, Haokun Lin, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Jing Dong, et al. Image-level memorization detection via inversion-based inference perturbation. In The Thirteenth International Conference on Learning Representations, 2025. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025. Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, and Zhenan Sun. Mopeclip: Structured pruning for efficient vision-language models with module-wise pruning error metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2737027380, 2024a. Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. Advances in Neural Information Processing Systems, 37:8776687800, 2024b. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activationaware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024c. Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact. arXiv preprint arXiv:2403.01241, 2024. Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, and Lu Hou. Quantization hurts reasoning? an empirical study on quantized reasoning models. arXiv preprint arXiv:2504.04823, 2025a. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025b. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. In The Twelfth International Conference on Learning Representations, 2023. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. Compression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025. Quan Wei, Chung-Yiu Yau, Hoi-To Wai, Yang Katie Zhao, Dongyeop Kang, Youngsuk Park, and Mingyi Hong. Roste: An efficient quantization-aware supervised fine-tuning approach for large language models. arXiv preprint arXiv:2502.09003, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024. 14 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, and Jiajun Zhang. Efficientllm: Scalable pruning-aware pretraining for architectureagnostic edge language models. arXiv preprint arXiv:2502.06663, 2025. Haobo Xu, Yuchen Yan, Dingsu Wang, Zhe Xu, Zhichen Zeng, Tarek Abdelzaher, Jiawei Han, and Hanghang Tong. Slog: An inductive spectral graph neural network beyond polynomial filter. In Forty-first International Conference on Machine Learning, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Lianwei Yang, Haisong Gong, Haokun Lin, Yichen Wu, Zhenan Sun, and Qingyi Gu. Dopq-vit: Towards distribution-friendly and outlier-aware post-training quantization for vision transformers. arXiv preprint arXiv:2408.03291, 2024. Lianwei Yang, Haokun Lin, Tianchen Zhao, Yichen Wu, Hongyu Zhu, Ruiqi Xie, Zhenan Sun, Yu Wang, and Qingyi Gu. Lrq-dit: Log-rotation post-training quantization of diffusion transformers for text-to-image generation. arXiv preprint arXiv:2508.03485, 2025b. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025c. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025a. URL https://hkunlp.github.io/blog/2025/dream. Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2497224982, 2025b. Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, and Yansong Tang. Voco-llama: Towards vision compression with large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2983629846, 2025c. Runpeng Yu, Qi Li, and Xinchao Wang. Discrete diffusion in large language and multimodal models: survey, 2025. URL https://arxiv.org/abs/2506.13759. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plugand-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=Tr0lPx9woF. Tianchen Zhao, Tongcheng Fang, Haofeng Huang, Enshu Liu, Rui Wan, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024a. Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. arXiv preprint arXiv:2405.17873, 2024b. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Harvard University",
        "NLPR & MAIS, Institute of Automation, CAS",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "Zhejiang University"
    ]
}