{
    "paper_title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration",
    "authors": [
        "Ramil Khafizov",
        "Artem Komarichev",
        "Ruslan Rakhimov",
        "Peter Wonka",
        "Evgeny Burnaev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene reconstruction that enhances the CUT3R model by integrating prior information. Unlike existing feed-forward methods that rely solely on input images, our method leverages auxiliary data, such as depth, camera calibrations, or camera positions, commonly available in real-world scenarios. We propose a lightweight modification to CUT3R, incorporating a dedicated encoder for each modality to extract features, which are fused with RGB image tokens via zero convolution. This flexible design enables seamless integration of any combination of prior information during inference. Evaluated across multiple benchmarks, including 3D reconstruction and other multi-view tasks, our approach demonstrates significant performance improvements, showing its ability to effectively utilize available priors while maintaining compatibility with varying input modalities."
        },
        {
            "title": "Start",
            "content": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration Ramil Khafizov1, Artem Komarichev1, Ruslan Rakhimov2, Peter Wonka3, Evgeny Burnaev1,4 1Skoltech, 2T-Tech, 3KAUST, 4AIRI 5 2 0 2 5 1 ] . [ 1 9 7 3 1 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce G-CUT3R, novel feed-forward approach for guided 3D scene reconstruction that enhances the CUT3R model by integrating prior information. Unlike existing feedforward methods that rely solely on input images, our method leverages auxiliary data, such as depth, camera calibrations, or camera positions, commonly available in real-world scenarios. We propose lightweight modification to CUT3R, incorporating dedicated encoder for each modality to extract features, which are fused with RGB image tokens via zero convolution. This flexible design enables seamless integration of any combination of prior information during inference. Evaluated across multiple benchmarks, including 3D reconstruction and other multi-view tasks, our approach demonstrates significant performance improvements, showing its ability to effectively utilize available priors while maintaining compatibility with varying input modalities. Introduction The pursuit of robust 3D scene reconstruction, as well as the development of versatile models capable of unifying diverse 3D perception tasks, including depth estimation, feature matching, dense reconstruction, and camera localization, is complex and long-standing challenge in computer vision and computer graphics. Traditional approaches, such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS) (Yao et al. 2018), rely on per-scene optimization, which is computationally expensive, slow to converge, and dependent on precisely calibrated datasets, limiting their practicality in real-world scenarios. This has led to the development of feed-forward methods (Fan, Su, and Guibas 2017; Wu et al. 2016; Wang et al. 2024b) as promising alternative. These models leverage large-scale training data and learned priors to achieve orders-of-magnitude faster inference and improved generalization, making them ideal for time-sensitive and scalable applications like real-time robotic perception and interactive 3D asset creation. Recent advances in feed-forward 3D reconstruction have placed these methods as compelling alternatives to traditional SfM techniques such as COLMAP (Schonberger and Frahm 2016), offering enhanced efficiency and robustness in generating 3D scene representations. In particular, DUSt3R (Wang et al. 2024b) has pioneered this paradigm by leveraging pairs of RGB images to simultaneously predict point clouds and camera poses, achieving impressive results with minimal input. Building upon this foundation, subsequent works (Leroy, Cabon, and Revaud 2024; Wang et al. 2025b,a) have significantly extended the capabilities of feed-forward approaches. For example, MASt3R (Leroy, Cabon, and Revaud 2024) improves the robustness of 3D reconstruction by grounding predictions in geometric and semantic constraints, improving accuracy in complex scenes. CUT3R (Wang et al. 2025b) introduces recurrent processing mechanism that sequentially refines reconstructions of image sequences, allowing better handling of temporal and spatial coherence in dynamic environments. Meanwhile, VGGT (Wang et al. 2025a) advances DUSt3Rs framework by adopting fully multi-view approach, concurrently utilizing all available images to produce more comprehensive and consistent 3D models. Despite the significant advancements achieved by DUSt3R (Wang et al. 2024b) and its derivatives, feedforward 3D reconstruction methods typically rely exclusively on RGB images, neglecting additional data sources such as calibrated camera intrinsics, poses, and depth maps from RGB-D or LiDAR sensors, which are commonly available in real-world applications. Effectively incorporating these diverse modalities to enhance the quality of 3D reconstructions remains critical challenge in computer vision. Inspired by Pow3R (Jang et al. 2025), we propose G-CUT3R, lightweight and modality-agnostic extension to the CUT3R framework (Wang et al. 2025b) that seamlessly integrates geometric priors through streamlined encoding process and carefully designed fusion techniques in the decoder stage. Using ray-based encoding for camera parameters and depthmaps, alongside zero-initialized convolutional layers for stable feature integration, G-CUT3R outperforms Pow3R and other state-of-the-art methods. Our approach bridges the gap between traditional SfM techniques and modern feed-forward pipelines, enabling more reliable 3D reconstructions for complex real-time applications. Our primary contributions are as follows: G-CUT3R, novel feed-forward method for guided 3D scene reconstruction that utilizes prior information, e.g., camera intrinsics, poses, and depths alongside with RGB images. Comprehensive experiments demonstrating significant performance improvement, achieving state-of-the-art results across multiple benchmark datasets and tasks. G-CUT3R (Depth Guidance) CUT3R (No Guidance) Spann3R Figure 1: Comparison of methods. Visual results across three different approaches: our G-CUT3R with depth guidance, original CUT3R without any guidance, and Spann3R. Our method produces cleaner and more complete 3D reconstruction. Related work Structure from Motion. Joint estimation of 3D scene geometry and camera poses from set of 2D images is fundamental problem in computer vision (Hartley and Zisserman 2003; Crandall et al. 2012; Jiang, Cui, and Tan 2013). Traditional SfM pipelines, such as COLMAP (Schonberger and Frahm 2016), rely on sequence of well-established steps: detecting and matching keypoints across images, estimating initial camera poses and 3D points, and refining these estimates through bundle adjustment. Keypoint matching typically uses hand-crafted features such as SIFT (Lowe 1999) or, more recently, learned features such as DIFT (Tang et al. 2023). Recent advances have integrated machine learning to enhance various components of the SfM pipeline. Methods like D2-Net (Dusmanu et al. 2019), LIFT (Yi et al. 2016), and others (Chen et al. 2021) employ trained models to improve keypoint detection, descriptor matching, and correspondence estimation, achieving robust performance under challenging conditions. In particular, VGGSfM (Wang et al. 2024a) introduced fully differentiable SfM framework, enabling end-to-end optimization of the reconstruction process. Although machine learning approaches have significantly improved accuracy and robustness, the core principles of SfM remain rooted in geometric optimization and correspondence-based reconstruction. Deep Learning Approaches. Recent advancements in deep learning have introduced novel alternatives to traditional SfM methods. DUSt3R (Wang et al. 2024b) represents significant deviation from conventional SfM pipelines by predicting point clouds from image pairs without relying on geometric constraints or inductive biases. Unlike traditional SfM, which depends on keypoint matching and geometric optimization, DUSt3R generates predictions in shared coordinate frame, enabling robust reconstruction across diverse scenes. This approach addresses several challenges inherent in classical methods, such as sensitivity to initialization and sparse correspondences. Building on this paradigm, several works have proposed variations with distinct architectural innovations. MASt3R (Leroy, Cabon, and Revaud 2024) improves the estimation of the pixel-wise correspondence between image pairs, strengthening the efficacy of unconstrained feed-forward models for SfM tasks. CUT3R (Wang et al. 2025b) introduces recurrent formulation of DUSt3R, achieving computational efficiency at the expense of marginal accuracy degradation. More recently, VGGT (Wang et al. 2025a) proposes multi-view architecture that processes multiple images simultaneously, moving beyond pairwise processing to improve reconstruction consistency and robustness. Guidance through Prior Information. While feed-forward deep learning methods, such as DUSt3R, achieve superior results in unconstrained 3D geometry prediction, integrating prior information remains significant challenge. In many applications, incomplete or noisy geometric priors (such as those derived from LiDAR or similar sensors) are available and can enhance reconstruction accuracy and consistency. Unlike traditional SfM pipelines, which naturally incorporate priors through geometric constraints, fully feed-forward approaches struggle to leverage such information effectively. To address the challenge of integrating prior information, Pow3R (Jang et al. 2025) extends the DUSt3R framework by incorporating optional depth and camera pose priors as additional inputs, providing guidance to improve reconstruction quality while maintaining the flexibility of feed-forward models. We extend the CUT3R model with prior-guided regularization. CUT3Rs continuous formulation naturally accommodates informative priors, and its known consistency issues create clear opportunity for improvement. Our lightweight regularizer boosts both efficiency and accuracy without incurring the memory costs of larger alternatives such as VGGT (Wang et al. 2025a). Although priorguided ideas have also been explored in related work (e.g., Pow3R (Jang et al. 2025)), our method is built specifically for CUT3R: we introduce distinct parameterization of the priors and different injection strategy during training. Detailed ablation studies isolate and quantify the impact of every module."
        },
        {
            "title": "Method",
            "content": "Overview. We introduce G-CUT3R, novel method that takes as input set of {Ii}N i=1 RGB images Ii R3HW with the corresponding auxiliary information Φ {K, P, D} as guidance to reconstruct the 3D scene. We denote R33 as camera intrinsics, = [R t] R44 as camera pose, and RHW as depth map with corresponding mask {0, 1}HW for sparse depth. The views are passed sequentially to the network and produce 3D pointmaps and camera poses simultaneously, retrieving and updating the state that encodes the understanding of the 3D scene. Recap of CUT3R We follow the recently proposed CUT3R method (Wang et al. 2025b) that enables efficient processing of thousands of images in recurrent, memory-constrained manner. CUT3R is presented as framework that takes set of images (i.e., either ordered or unordered) as input and outputs corresponding point maps. The process begins with each image being passed through Vision Transformer (ViT) encoder, denoted as EI , to extract features , defined as = EI (I). This step leverages the strengths of ViT, which is known for capturing global dependencies in images through self-attention. To maintain sequential context, especially for ordered images, CUT3R introduces state tokens sj. These tokens interact with the image features via cross-attention in the decoder stage, allowing for mutual updates. The interaction is formalized as: [z ], sj = Decoders([zj, ], sj1). Here, zj represents the learnable pose token and [z ] denotes the updated features enriched with state information, while sj is the updated state token. This recursive mechanism ensures that the features of each image are informed by the context of previous images, enhancing the models ability to understand complex scenes and temporal dynamics. j, j, I"
        },
        {
            "title": "Incorporating Prior Information",
            "content": "The baseline feed-forward 3D reconstruction pipeline in CUT3R lacks the capability to leverage additional prior information, including camera poses or depth maps, to enhance scene reconstruction accuracy. To address this limitation, we propose lightweight and modality-agnostic extension to the CUT3R framework, by modifying only the decoder stage to seamlessly integrate additional input modalities, including noisy depth maps and both intrinsic and extrinsic camera parameters, as illustrated in Fig. 2. This approach ensures compatibility with diverse data sources while preserving the integrity of the pre-trained model, making it suitable for advanced tasks such as 3D reconstruction and scene understanding. The flexible design of this extension also holds potential for application to other feed-forward reconstruction pipelines. Modality Encoding. We encode camera intrinsics and poses as ray images, representing each pixel (m, n) in an image of resolution as normalized 3D direction. This yields R3HW and R3HW , computed as follows: = [K 1[m, n, 1]T ; 1] [K 1[m, n, 1]T ; 1] , = (1) Here, represents the normalized ray directions derived from camera intrinsics transformed by the pose , and encodes the translational component of the pose. The homogeneous coordinate [m, n, 1]T is transformed by the inverse intrinsic matrix 1, projected via , and normalized to ensure the resulting ray directions have unit length. In cases where only camera intrinsics are available, encoding is performed in the local camera coordinate system: = 1[m, n, 1]T 1[m, n, 1]T (2) This produces R3HW , representing ray directions in the cameras local frame. Depth maps RHW are normalized in the range of [0, 1] and paired with the corresponding binary masks RHW to form composite representation, concatenated channel-wise: = [D; ] (3) The resulting R2HW encapsulates depth values and their validity masks, enabling robust handling of sparse or incomplete depth data prevalent in real-world sensor outputs. To prepare modalities for fusion, each is processed through dedicated convolutional layer to extract initial feature maps, aligning their representations within shared feature space: = ConvD(X D), = ConvK(X K), = ConvP (X ), where ConvD, ConvK, and ConvP are modality-specific convolutional layers tailored to the input dimensions and characteristics of D, K, and , respectively. These layers produce feature maps D, K, RCHW , where denotes the number of output channels. (4) Modality Fusion. We perform fusion five times within the CUT3R decoder, with the first fusion occurring before the initial decoder layer and subsequent fusions following each of the first four decoder layers. The features D, K, and are processed by dedicated ViT encoders, each comprising four layers, to extract intermediate representations tailored to the geometric and semantic properties of each modality. These encoders are not shared between modalities to preserve the unique characteristics of each modality. To integrate the features from additional modalities D, K, and with the RGB image features , we compute guidance feature by summing the modality-specific features: = + + (5) Figure 2: Overview of the G-CUT3R architecture. Our method processes set of RGB images from videos or collections of images, together with variable set of auxiliary inputs, including depth maps (Depth), camera intrinsics (K), and camera poses (Pose). These inputs are sequentially fed into the network, which employs modality-specific convolutional layers and ViT encoders to extract and fuse features. These features are fused with RGB image features via zero-initialized convolutional layers within the decoder stage, enabling the model to generate accurate 3D pointmaps and camera poses while updating state token to maintain scene context across sequential inputs. Table 1: Datasets for fine-tuning."
        },
        {
            "title": "Scene Type",
            "content": "Dynamic? ScanNet ScanNet++ ARKitScenes Waymo MegaDepth DL3DV Co3Dv2 WildRGBD Indoor Indoor Indoor Outdoor Outdoor Mixed Object-centric Object-centric"
        },
        {
            "title": "Static\nStatic\nStatic\nDynamic\nStatic\nStatic\nStatic\nStatic",
            "content": "The guidance feature is combined with the RGB image features using ZeroConv layer (Zhang, Rao, and Agrawala 2023), 1 1 convolution layer initialized with zero weights: (6) fused = + ZeroConv(G) The zero-initialized weights ensure that, at the very beginning of the training process, the additional modalities do not disrupt the pre-trained behavior of the CUT3R decoder. During fine-tuning, the model gradually learns to incorporate the guidance features. This allows the model to improve performance without destabilizing existing weights. This approach ensures stable and effective integration of multimodal inputs, enhancing the robustness and adaptability of the model in complex vision tasks. Training Objective Our model predicts pointmaps ˆX R3HW , the corresponding confidences ˆC RHW , and camera poses ˆP . Following the CUT3R framework (Wang et al. 2025b), the training objective comprises two primary components: pointmap prediction loss Lpoint and camera pose prediction loss Lpose. The pose loss separately evaluates orientation error (via quaternion difference) and translation error, ensuring precise alignment of the predicted and the ground truth poses. The loss functions are defined as follows: Lpoint = (cid:88) (cid:16) ˆC ˆX α log ˆC (cid:17) , Lpose = (cid:88) (cid:0)ˆq + ˆt t(cid:1) , (7) where and ˆX represent the ground truth and predicted pointmaps, respectively, ˆC denotes the predicted confidences, and α is hyperparameter controlling the weight of the confidence regularization term. For pose loss, ˆq and are the predicted and the ground truth quaternions, respectively, and ˆt and are the predicted and ground truth translations. This composite loss ensures accurate point map reconstruction and robust pose estimation, which are critical for tasks such as 3D scene understanding and camera localization. Training Strategy Our training strategy builds upon the CUT3R framework, utilizing short sequences of four images to ensure both computational efficiency and scalability. Unlike approaches that train separate models for each input modality (e.g., depth priors or camera parameters), we adopt unified training paradigm. single model is trained to handle arbitrary combinations of modalities, enhancing its versatility in realworld scenarios. During training, the model is exposed to random subsets of available modalities, simulating diverse input conditions. While modality-specific models may exhibit faster initial convergence, our unified model achieves comparable performance with sufficient training iterations, offering superior flexibility and practical deployment benefits. Datasets. The model is trained on diverse set of indoor and outdoor datasets, as summarized in Table 1. These include the Waymo Open Dataset (Sun et al. 2020), Co3Dv2 (Reizenstein et al. 2021), ScanNet (Dai et al. 2017), ARKitScenes (Baruch et al. 2021), DL3DV (Ling et al. 2024), WildRGBD (Xia et al. 2024), MegaDepth (Li and Snavely 2018), and ScanNet++ (Yeshwanth et al. 2023). Dataset preparation follows the same protocol as CUT3R to ensure consistency. To balance representation across datasets, we sample equal subsets of 10,000 examples from each, creating comprehensive and diverse training corpus that supports robust generalization across indoor and outdoor scenes. employ Implementation details. We ViT-Large model (Dosovitskiy et al. 2021) as the image encoder and ViT-Base for all decoders. Each modality encoder consists of four transformer blocks with 12 attention heads and an embedding dimension of 768, striking balance between feature richness and computational efficiency. Architectural parameters, such as 16 16 patch size and embedding dimension, are aligned with those of the CUT3R RGB encoder to ensure compatibility. Linear layers serve as output heads to predict pointmaps, confidences, and camera poses. The model parameters are initialized using pre-trained CUT3R weights for images of size 224 224. We train the model using the Adam-W optimizer with learning rate of 105, which incorporates linear warmup and cosine weight decay schedule. The model is trained on four NVIDIA A100 GPUs for seven days, ensuring robust convergence and scalability."
        },
        {
            "title": "Experiments",
            "content": "We evaluate G-CUT3R across three tasks: scene-level 3D reconstruction, video depth estimation, and relative pose estimation, supported by thorough ablation studies and efficiency analyses. For all evaluations, we assess the impact of guidance using all possible combinations of auxiliary inputs, including camera intrinsics K, camera pose t, and depthmaps D."
        },
        {
            "title": "Benchmark Suites",
            "content": "Indoor static. The 7-scenes (Shotton et al. 2013) and NRGBD (Azinovic et al. 2022) follow the low-overlap protocol of Wang et al. (2025b), utilizing 35 views per scene. Indoor dynamic. ScanNet (Dai et al. 2017) and ScanNet++ (Yeshwanth et al. 2023) include handheld RGB-D sequences and scenes with staged dynamic objects. Outdoor dynamic. Waymo (Sun et al. 2020) provides automotive scenarios with moving agents and LiDAR depth data. Training, validation, and test splits align with the official splits provided by the dataset. Baselines All baseline models are feed-forward at test time and process images at resolution of 224224: CUT3R (Wang et al. 2025b) our primary backbone model. Spann3R (Wang and Agapito 2024) transformer-based SfM model without prior information. Pow3R our re-implementation of Jang et al. (2025), adapted to the CUT3R framework. We evaluate our method against Spann3R (Wang and Agapito 2024) and CUT3R (Wang et al. 2025b), using their official checkpoints at resolution of 224 224. Training. All CUT3R-family models (CUT3R, Pow3R, GCUT3R) are fine-tuned for 10 epochs on an identical 80,000sample subset, as detailed in Tab. 1. No synthetic augmentations are applied beyond random cropping and flipping. 3D Reconstruction We evaluate scene-level 3D reconstruction on the 7-scenes and NRGBD datasets using standard metrics: Accuracy (Acc), Completeness (Comp), and Normal Consistency (NC), as reported in Tab. 2. Following prior works (Leroy, Cabon, and Revaud 2024; Wang et al. 2025b), these metrics assess the quality of 3D scene reconstruction. Consistent with CUT3R (Wang et al. 2025b), we assess performance under low-overlap conditions, where each scene comprises only 35 images, simulating challenging real-world scenarios with sparse viewpoints. discrepancy exists between the original CUT3R checkpoint and our unguided G-CUT3R variant due to differences in training data. Specifically, G-CUT3R is initialized from CUT3R checkpoints but fine-tuned on smaller subset of the original training datasets, constrained by data availability. Consequently, direct comparison with the original CUT3R would be biased, as its superior performance may result from exposure to larger training corpus. To isolate the effect of guidance, we train G-CUT3R variant without guidance on the same subset, ensuring fair baseline for comparison. As shown in Tab. 2, incorporating guidance consistently improves performance across both datasets. Camera poses contribute the most to enhancements in Accuracy and Completeness, while depth fusion significantly improves Normal Consistency. The fusion of multiple modalities outperforms single-modality configurations, with optimal results achieved when all modalities are combined. Video Depth Estimation We evaluate depth quality and consistency in video depth estimation using Absolute Relative Error (Abs. Rel) and the percentage of inlier points (δ < 1.25), following established methods (Wang et al. 2025b; Zhang et al. 2024). For CUT3R and G-CUT3R, metrics are computed without scale alignment, while for Spann3R, scale alignment is applied. Results are reported in Tab. 3. Depth fusion yields the most significant improvement on the ScanNet dataset (Dai et al. 2017), while on the Bonn Figure 3: Qualitative results on video sequences from the 7-scenes and ScanNet datasets. We compare our G-CUT3R method with CUT3R, demonstrating superior visual quality and reconstruction accuracy. Table 2: 3D reconstruction comparison on 7-scenes and NRGBD datasets. NC R, Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Acc Acc NC 7-scenes Comp NRGBD Comp - - - - + - + - + + - - - - - + - + + + 0.298 0.298 0.326 0.347 0.236 0.313 0.151 0.317 0.238 0.144 0.226 0. 0.262 0.277 0.184 0.246 0.101 0.244 0.168 0.085 0.205 0.254 0.207 0.220 0.167 0.195 0.115 0.199 0.143 0.091 0.112 0.110 0.171 0.191 0.104 0.166 0.055 0.168 0.100 0.050 0.650 0. 0.663 0.662 0.667 0.690 0.675 0.693 0.694 0.695 0.730 0.728 0.742 0.738 0.751 0.778 0.761 0.781 0.785 0.787 0.416 0.422 0.246 0.191 0.178 0.249 0.172 0.228 0.157 0.167 0.323 0. 0.145 0.116 0.084 0.089 0.074 0.099 0.054 0.052 0.417 0.252 0.195 0.173 0.146 0.148 0.149 0.142 0.114 0.130 0.285 0.163 0.097 0.081 0.071 0.055 0.062 0.053 0.037 0.037 0.684 0. 0.708 0.714 0.741 0.746 0.745 0.731 0.769 0.767 0.789 0.835 0.829 0.840 0.878 0.889 0.886 0.857 0.913 0.913 Method Spann3R CUT3R - - G-CUT3R - G-CUT3R + G-CUT3R - G-CUT3R - G-CUT3R + G-CUT3R + G-CUT3R - G-CUT3R + Table 3: Video depth evaluation on Bonn and ScanNet."
        },
        {
            "title": "Camera Pose Estimation",
            "content": "Method R, Abs. rel δ <1.25 Abs. rel δ <1.25 Bonn ScanNet Spann3R CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R - - - + - - + + - + - - - - + - + - + + - - - - - + - + + + 0.144 0.109 0.126 0.125 0.105 0.108 0.104 0.105 0.067 0.066 81.3 88.8 89.9 85.8 89.1 94.5 88.4 94.4 96.9 97. 0.051 0.039 0.04 0.04 0.04 0.023 0.04 0.023 0.03 0.03 96.7 98.6 98.5 98.5 98.6 99.9 98.7 99.8 99.9 99.9 dataset (Palazzolo et al. 2019), improvements in Abs. Rel from depth fusion are comparable to those from pose integration. Combining depth and pose further enhances performance on Bonn, while on ScanNet, where the initial error is already low, only depth fusion provides substantial improvements. Following the evaluation protocol of Zhao et al. (2022); Wang et al. (2025b), we align each predicted trajectory to the ground truth using seven-degree-of-freedom similarity transform and report the following metrics: Absolute Translation Error (ATE) global drift of the entire trajectory; Relative Translation Error (RTE) average translational drift between consecutive frames; Relative Rotation Error (RRE) average rotational drift between consecutive frames. Experiments are conducted on the Sintel (Butler et al. 2012), TUM dynamics (Sturm et al. 2012), and ScanNet (Dai et al. 2017) datasets. Results, including mean and standard deviation over three seeds, are presented in Supplementary Material. Incorporating pose guidance signifiImpact of priors. cantly reduces ATE by 61% on Sintel (from 0.077 to 0.030), 23% on TUM RGB-D (from 0.013 to 0.010), and 29% on Table 4: 3D reconstruction comparison of our method, both with and without zero convolutions, alongside an adaptation incorporating prior information in CUT3R inspired by Pow3R. Evaluations are performed on Waymo and ScanNet++ datasets, using the L2 metric to assess reconstruction quality for four consecutive views. Method Pow3R Pow3R Pow3R Pow3R Pow3R Pow3R Pow3R Pow3R Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/o ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) Ours (w/ ZeroConv) R, L2/ L2/2 L2/3 L2/4 L2/"
        },
        {
            "title": "Waymo",
            "content": "ScanNet++ L2/3 L2/2 - + - - + + - + - + - - + + - + - + - - + + - + - - + - + - + + - - + - + - + + - - + - + - + + - - - + - + + + - - - + - + + + - - - + - + + + 1.194 1.196 1.235 1.190 1.232 1.197 1.233 1. 1.796 1.796 1.798 1.723 1.797 1.722 1.734 1.730 1.235 1.236 1.235 1.042 1.235 1.042 1.042 1.042 1.216 1.192 1.350 1.201 1.301 1.189 1.344 1.291 1.723 1.736 1.721 1.667 1.723 1.672 1.667 1.665 1.259 1.259 1.215 1.095 1.215 1.097 1.054 1.055 1.312 1.262 1.411 1.244 1.385 1.225 1.442 1. 1.756 1.761 1.803 1.776 1.802 1.800 1.776 1.773 1.300 1.297 1.248 1.145 1.246 1.142 1.091 1.089 1.458 1.342 1.611 1.326 1.553 1.350 1.554 1.543 1.766 1.772 2.006 1.814 2.002 1.816 1.965 1.959 1.327 1.322 1.305 1.181 1.301 1.176 1.159 1.155 0.050 0.051 0.051 0.049 0.050 0.050 0.050 0. 0.055 0.056 0.056 0.053 0.056 0.053 0.053 0.053 0.049 0.049 0.049 0.042 0.049 0.042 0.042 0.042 0.071 0.079 0.074 0.073 0.072 0.082 0.089 0.074 0.088 0.086 0.072 0.087 0.073 0.087 0.072 0.072 0.074 0.074 0.066 0.060 0.067 0.068 0.060 0.061 0.077 0.085 0.076 0.080 0.085 0.089 0.093 0. 0.092 0.092 0.083 0.091 0.081 0.091 0.082 0.079 0.075 0.075 0.067 0.061 0.067 0.069 0.061 0.061 L2/4 0.087 0.092 0.086 0.085 0.091 0.092 0.097 0.084 0.100 0.100 0.081 0.099 0.080 0.097 0.079 0.078 0.086 0.086 0.070 0.063 0.070 0.080 0.063 0. ScanNet (from 0.007 to 0.005) compared to the no-guidance variant. Depth or intrinsic priors alone provide marginal improvements, but when combined with pose guidance, they further decrease RRE by 812% across all datasets, enhancing local pose accuracy. Ablation Study The original Pow3R implementation (Jang et al. 2025) is designed specifically for the DUSt3R architecture, which excludes direct comparison with our method. To address this, we adapt the Pow3R approach to incorporate guidance within the CUT3R framework, allowing fair evaluation against our proposed method. Similarly to our approach, we introduce conditional encoder block to integrate intrinsics and depth guidance, and conditional decoder block for pose guidance. During training, the guidance data are randomly selected, reflecting our method strategy. To ensure fair comparison, both our methodwith and without zero convolutionsand the Pow3R adaptation (Jang et al. 2025) are trained on the same dataset subset, comprising Waymo (Sun et al. 2020) and ScanNet++ (Yeshwanth et al. 2023), for an equal number of epochs. We evaluate the L2 distance between ground truth and reconstructed pointmaps. Unlike standard Accuracy and Completeness metrics, which measure distances to the nearest points, the L2 metric computes distances between points corresponding to identical pixel coordinates, offering complementary perspective on reconstruction quality. The results are presented in Tab. 4. Our ablation study demonstrates two key findings. First, the use of zero convolution layers significantly enhances reconstruction performance across metrics. Second, our approach to integrating prior information outperforms the Pow3R adaptation, attributed to its modality-agnostic fusion strategy and effective leveraging of diverse input modalities. Conclusion In this work, we present G-CUT3R, lightweight and modality-agnostic extension to the CUT3R framework that enhances 3D scene reconstruction by integrating geometric priors such as camera settings, poses, and depth data. Our method employs straightforward encoding and carefully designed fusion during decoding. When tested on various datasets, G-CUT3R shows clear improvements over the existing state-of-the-art methods, producing more accurate and detailed 3D reconstructions. Our experiments further confirm that our fusion approach and design choices lead to better performance compared to other methods, making GCUT3R versatile and robust solution for 3D vision tasks. References Azinovic, D.; Martin-Brualla, R.; Goldman, D. B.; Nießner, M.; and Thies, J. 2022. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 62906301. Baruch, G.; Chen, Z.; Dehghan, A.; Dimry, T.; Feigin, Y.; Fu, P.; Gebauer, T.; Joffe, B.; Kurz, D.; Schwartz, A.; and Shulman, E. 2021. ARKitScenes - Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile In Thirty-fifth Conference on Neural InforRGB-D Data. mation Processing Systems Datasets and Benchmarks Track (Round 1). Butler, D. J.; Wulff, J.; Stanley, G. B.; and Black, M. J. 2012. naturalistic open source movie for optical flow evaluation. In European conference on computer vision, 611625. Springer. Chen, H.; Luo, Z.; Zhang, J.; Zhou, L.; Bai, X.; Hu, Z.; Tai, C.-L.; and Quan, L. 2021. Learning to match features In Proceedings of with seeded graph matching network. the IEEE/CVF international conference on computer vision, 63016310. Crandall, D. J.; Owens, A.; Snavely, N.; and Huttenlocher, D. P. 2012. SfM with MRFs: Discrete-continuous optimizaIEEE transaction for large-scale structure from motion. tions on pattern analysis and machine intelligence, 35(12): 28412853. Dai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser, T.; and Nießner, M. 2017. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929. Dusmanu, M.; Rocco, I.; Pajdla, T.; Pollefeys, M.; Sivic, J.; Torii, A.; and Sattler, T. 2019. D2-net: trainable cnn for joint description and detection of local features. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, 80928101. Fan, H.; Su, H.; and Guibas, L. J. 2017. point set generation network for 3d object reconstruction from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, 605613. Hartley, R.; and Zisserman, A. 2003. Multiple view geometry in computer vision. Cambridge university press. Jang, W.; Weinzaepfel, P.; Leroy, V.; Agapito, L.; and Revaud, J. 2025. Pow3r: Empowering unconstrained 3d reconstruction with camera and scene priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, 10711081. Jiang, N.; Cui, Z.; and Tan, P. 2013. global linear method In Proceedings of the IEEE for camera pose registration. international conference on computer vision, 481488. Leroy, V.; Cabon, Y.; and Revaud, J. 2024. Grounding image In European Conference on matching in 3d with mast3r. Computer Vision, 7191. Springer. Li, Z.; and Snavely, N. 2018. MegaDepth: Learning SingleView Depth Prediction from Internet Photos. In Computer Vision and Pattern Recognition (CVPR). Ling, L.; Sheng, Y.; Tu, Z.; Zhao, W.; Xin, C.; Wan, K.; Yu, L.; Guo, Q.; Yu, Z.; Lu, Y.; et al. 2024. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2216022169. Lowe, D. G. 1999. Object recognition from local scaleinvariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, 1150 1157. Ieee. Palazzolo, E.; Behley, J.; Lottes, P.; Giguere, P.; and Stachniss, C. 2019. ReFusion: 3D reconstruction in dynamic environments for RGB-D cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 78557862. IEEE. Reizenstein, J.; Shapovalov, R.; Henzler, P.; Sbordone, L.; Labatut, P.; and Novotny, D. 2021. Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction. In International Conference on Computer Vision. Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-frommotion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, 41044113. Shotton, J.; Glocker, B.; Zach, C.; Izadi, S.; Criminisi, A.; and Fitzgibbon, A. 2013. Scene coordinate regression forests for camera relocalization in RGB-D images. In Proceedings of the IEEE conference on computer vision and pattern recognition, 29302937. Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; and Cremers, D. 2012. benchmark for the evaluation of RGB-D SLAM systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, 573580. IEEE. Sun, P.; Kretzschmar, H.; Dotiwalla, X.; Chouard, A.; Patnaik, V.; Tsui, P.; Guo, J.; Zhou, Y.; Chai, Y.; Caine, B.; et al. 2020. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2446 2454. Tang, L.; Jia, M.; Wang, Q.; Phoo, C. P.; and Hariharan, B. 2023. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36: 13631389. Wang, H.; and Agapito, L. 2024. 3D Reconstruction with Spatial Memory. arXiv preprint arXiv:2408.16061. Wang, J.; Chen, M.; Karaev, N.; Vedaldi, A.; Rupprecht, C.; and Novotny, D. 2025a. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, 52945306. Wang, J.; Karaev, N.; Rupprecht, C.; and Novotny, D. 2024a. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2168621697. Wang, Q.; Zhang, Y.; Holynski, A.; Efros, A. A.; and Kanazawa, A. 2025b. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1051010522. Wang, S.; Leroy, V.; Cabon, Y.; Chidlovskii, B.; and Revaud, J. 2024b. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2069720709. Wu, J.; Zhang, C.; Xue, T.; Freeman, B.; and Tenenbaum, J. 2016. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29. Xia, H.; Fu, Y.; Liu, S.; and Wang, X. 2024. RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos. arXiv:2401.12592. Yao, Y.; Luo, Z.; Li, S.; Fang, T.; and Quan, L. 2018. MVSNet: Depth Inference for Unstructured Multi-view Stereo. European Conference on Computer Vision (ECCV). Yeshwanth, C.; Liu, Y.-C.; Nießner, M.; and Dai, A. 2023. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1222. Yi, K. M.; Trulls, E.; Lepetit, V.; and Fua, P. 2016. Lift: In European conferLearned invariant feature transform. ence on computer vision, 467483. Springer. Zhang, J.; Herrmann, C.; Hur, J.; Jampani, V.; Darrell, T.; Cole, F.; Sun, D.; and Yang, M.-H. 2024. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825. Zhang, L.; Rao, A.; and Agrawala, M. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, 38363847. Zhao, W.; Liu, S.; Guo, H.; Wang, W.; and Liu, Y.-J. 2022. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In European Conference on Computer Vision, 523542. Springer. G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material provides additional details to support the main findings of our G-CUT3R model. We include comprehensive information on our training dataset preprocessing and camera pose estimation evaluation, along with the source code provided for G-CUT3R. Training Dataset Preprocessing Our training dataset is derived from subset of the CUT3R dataset (Wang et al. 2025b), comprising both ordered and unordered image sequences. For ordered sequences, we sample frames at random intervals ranging from 1 to k, where varies by dataset. For unordered images, we select samples based on their visual overlap. For the DL3DV dataset (Ling et al. 2024), we utilize the depth maps provided by the CUT3R authors. The preprocessing pipeline follows the methodology outlined in CUT3R (Wang et al. 2025b), and we refer readers to the original paper for further details. Camera Pose Estimation We evaluate relative pose estimation of our G-CUT3R method against Spann3R (Wang and Agapito 2024) and CUT3R (Wang et al. 2025b) on the Sintel (Butler et al. 2012), TUM dynamics (Sturm et al. 2012), and ScanNet (Dai et al. 2017) datasets, employing Absolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot) as metrics, with trajectories aligned to ground truth following the protocol of Wang et al. (2025b). The results are presented in Tab. 5. As anticipated, pose fusion provides the most substantial improvement in performance, while the integration of camera intrinsics or depth priors yields only marginal benefits. Table 5: Evalution on camera pose estimation on Sintel, TUM dynamics and ScanNet datasets."
        },
        {
            "title": "Method",
            "content": "K R, ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot"
        },
        {
            "title": "ScanNet",
            "content": "Spann3R CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R G-CUT3R - - - + - - + + - + - - - - + - + - + + - - - - - + - + + + 0.329 0.090 0.077 0.080 0.030 0.097 0.035 0.099 0.035 0.039 0.110 0.172 0.177 0.180 0.038 0.171 0.042 0.172 0.050 0. 4.471 0.746 0.919 0.925 1.454 0.867 1.400 0.866 1.146 1.100 0.056 0.011 0.013 0.012 0.010 0.013 0.010 0.013 0.011 0.011 0.021 0.013 0.017 0.017 0.011 0.015 0.011 0.016 0.010 0. 0.591 0.597 0.634 0.618 0.660 0.635 0.641 0.619 0.652 0.630 0.096 0.006 0.007 0.007 0.005 0.007 0.005 0.007 0.005 0.005 0.023 0.007 0.008 0.008 0.006 0.008 0.006 0.008 0.006 0. 0.661 1.299 1.351 1.336 1.328 1.340 1.408 1.324 1.408 1."
        }
    ],
    "affiliations": [
        "AIRI",
        "KAUST",
        "Skoltech",
        "T-Tech"
    ]
}