{
    "paper_title": "VeGaS: Video Gaussian Splatting",
    "authors": [
        "Weronika Smolak-Dyżewska",
        "Dawid Malarz",
        "Kornel Howil",
        "Jan Kaczmarczyk",
        "Marcin Mazur",
        "Przemysław Spurek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 4 2 0 1 1 . 1 1 4 2 : r VeGaS: Video Gaussian Splatting Weronika Smolak-Dyzewska, Dawid Malarz, Kornel Howil, Jan Kaczmarczyk, Marcin Mazur, Przemysław Spurek Jagiellonian University Faculty of Mathematics and Computer Science weronika.smolak@doctoral.uj.edu.pl Figure 1. Graphical summary of our Video Gaussian Splatting (VeGaS) model. The initial step involves the use of diagonal 3D Gaussians and frames with equal distances. Then, dynamic frame fitting and Gaussian folding are employed to approximate nonlinear structures within video stream. Each frame is modeled by 2D Gaussians obtained by conditioning of 3D Folded-Gaussians at frame occurrence time ti. This representation allows for the creation of high-quality renderings of video data and facilitates wide range of modifications."
        },
        {
            "title": "Abstract",
            "content": "Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to limited set of basic transformations. To address this issue, we introduce *These authors contributed equally to this work the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS. 1. Introduction Implicit Neural Representations (INRs) [27] employs neural networks to describe discrete data as smooth, continuous function. They have emerged as promising method for continuously encoding variety of signals, including images [14], videos [5], audio [30], and 3D shapes [24]. INRs are frequently utilized in the context of 2D imagery by training networks that map pixel coordinates to RGB color values, thus encoding structure of images in neural network weights. This approach offers several benefits, including applications in compression [5], hyper-resolution [14], or as an integral part of generative models [11, 28]. On the other hand, the 3D Gaussian Splatting (3DGS) framework [13], initially proposed for the modeling of 3D scenes, has recently been adapted with 2D images. In particular, the GaussianImage method [40] has demonstrated promising results in image reconstruction by efficiently encoding images in the 2D space, with strong focus on model efficiency and reduced training time. Furthermore, the MiraGe representation [33] has demonstrated the feasibility of generating realistic modifications of 2D images. Similarly to 2D images, INR produces continuous representation of videos [5]. In such case, neural networks transform the pixel coordinates and time frames into RGB color. Such models provide good reconstruction quality and compression ratios. Unfortunately, INR ultimately failed with the editing of videos. To solve such problem, we can use the Gaussian Splatting solution. Video Gaussian Representation (VGR) [29] uses Gaussians in the canonical position and deformation function, which transfers such Gaussian to each time frame. This model is capable of handling variety of video processing tasks, such as video editing. Nevertheless, these changes are restricted to linear transformations and translations. This paper introduces the Video Gaussian Splatting (VeGaS) model, which presents evidence that the 3DGS approach can be adapted for use with 2D video data. In particular, video frames are treated as parallel planes within 3D space, and 3D Gaussian Splatting is employed to model the transitions observed between the content of subsequent frames. By conditioning 3D Gaussian components at specified time points, 2D Gaussians are tailored to the selected frames. It is crucial to emphasize that our solution surpasses the classical Gaussian Splatting model by enhancing its capacity to integrate intricate distributions, thus facilitating more exact modeling of swift alterations in video sequences. In particular, we introduce Folded-Gaussians, family of functions that model nonlinear structures and produce classical 2D Gaussian distributions after conditioning. It should be noted that the employment of the 3DGS framework for video modeling allows for the utilization of both extensive Gaussians to represent the background, which remains largely static over time, and brief Gaussians to represent elements present in only few frames. Furthermore, VeGaS employs MiraGe-based representation to model individual frames, which allows one to modify both the entire video and selected frames, resulting in high-quality renderings, as shown in Figure 2."
        },
        {
            "title": "The following represents a comprehensive account of",
            "content": "our significant contributions: we introduce Folded-Gaussians, novel family of functions that model nonlinear structures and can be readily incorporated into the 3D Gaussian Splatting framework, we propose the VeGaS model, which allows for the processing of 2D video data using the Folded-Gaussians, we conduct experiments that demonstrate the superiority of VeGaS for reconstruction tasks and show its efficiency in producing realistic modifications of video data. i O c n r n a p u n r i n Figure 2. Video edition. Note that VeGaS enables modification of selected objects on global scale, including operations such as multiplication and scaling. The model was trained on the DAVIS dataset [25]. 2. Related Works The decomposition of videos into layered representations enables the utilization of sophisticated video editing techniques. In [12], the authors decompose an image into textured layers and learn corresponding deformation field, which allows for efficient video editing. The method outlined in [38] involves the partitioning of videos into discrete motion groups, with each group being driven by an MLPbased representation. INVE [9] employs bidirectional warping field to facilitate extensive video tracking and editing over extended periods of time. In [2], the authors propose an improvement to the rendering of lighting and color details. This is achieved by incorporating additional layers and residual color maps, which serve to enhance the representation of illumination effects in the video. CoDeF [23] employs multi-resolution hash grid and shallow MLP to model frame-by-frame deformations relative to canonical image. This approach enables editing in the canonical space, with changes effectively propagated across the entire video. comparable representation is utilized in GenDeF [35] for the generation of controllable videos. The generative potential of latent diffusion models has been harnessed in various research endeavors within the context of video editing [26]. In [41], the authors integrate control signals into the network during video reconstruction, thereby guiding the editing process. related technique involves frame interpolation to generate edited videos from specifically edited keyframes [22], while another method employs token merging approach to incorporate control signals during [18]. Moreover, some works investigate inversion techniques for video editing [17, 42]. 3D Gaussian Splatting (3DGS) [13] models 3D static scenes using set of Gaussian components. Recently, numerous generalizations have been proposed for the representation of dynamic scenes. In [21], the authors employ multiview dynamic dataset coupled with an incremental, frame-based strategy. Nevertheless, this method does not account for inter-frame correlation and requires considerable amount of storage space for extended sequences. The approach presented in [15, 37] employs an MLP to represent temporal alterations in Gaussians. In contrast, in [36] the authors utilize an MLP in conjunction with decomposed neural voxel encoding technique to enhance training and storage effectiveness. In [20], dynamic scenes are divided into dynamic and static segments, which are optimized independently and then combined to facilitate decoupling. Other research has sought to enhance the reconstruction of dynamic scenes by incorporating external priors. For instance, diffusion priors have been shown to serve as effective regularization terms in the optimization process [39]. In [6], the authors propose 4DRotorGS which employs fourdimensional Gaussian, with the fourth dimension dedicated to time. Furthermore, 3DGS was utilized to modify scene geometry based on the underlying meshes. In [7], the positioning of 3D Gaussians on an explicit mesh enables the utilization of mesh rendering to facilitate adaptive refinement. This method is contingent upon the use of an extracted mesh as proxy; therefore, it is inoperable in the event of failure in the mesh extraction process. On the other hand, in [8] explicit meshes are derived from 3DGS representations through the regularization of Gaussians over surfaces. This process, which involves significant optimization and refinement phase, is particularly resource-intensive. Another example is given in [10], where sparse control points are used for 3D scene dynamics. However, this approach encounters difficulties with extensive edit movements and requires precise static node selection. In turn, GaMeS [32] integrates 3DGS with mesh extraction, though this approach is only effective for static scenes. In contrast, D-MiSo [31] is mesh-based approach, specifically designed for dynamic scenes, which employs simple 3DGS technique pipeline to allow real-time editing of dynamic scenes. In [29], the authors introduce the Video Gaussian Representation (VGR), which employs 3D Gaussian Splatting to model video data. This approach is closely related to ours and therefore represents the most reasonable baseline for the VeGaS model. VGR uses Gaussians in the canonical position, transferring them further to each frame occurrence time, and deformation function. This model is capable of handling variety of video processing tasks, such as video editing. Nevertheless, the possible changes are restricted to linear transformations and translations, which represents limitation relative to our proposed solution. 3. Folded-Gaussians In this section, we introduce the concept of FoldedGaussian distribution, which can be seen as generalization of classical Gaussian distribution in order to capture nonlinear structures. It should be noted that FoldedGaussians constitute novel family of distributions that we further employ to represent video data (see the next section). Accordingly, in order to emphasize this relationship, terminology based on the concept of space-time variable is employed, given that each video can be regarded as sequence of successive frames occurring at discrete time points. For the readers convenience, we begin with simple two-dimensional toy example, before extending our presentation to the multidimensional case. Toy Example in R2 Our toy example starts with twodimensional Gaussian distribution (m, Σ) of space-time random variable = (s, t) R, which is given by mean vector = (ms, mt) and covariance matrix Σ = (cid:20)σ2 0 (cid:21) . 0 σ2 (1) In this case, the density function is defined by the following formula: (m, Σ)(x) = (ms, σ2 )(s) (mt, σ )(t), where (m, σ2)(x) = 1 2πσ (cid:18) exp m2 2σ2 (cid:19) . (2) (3) Using such distribution, we can model ellipses, which can be considered as simple linear structure spanned along the coordinate axes. Therefore, we propose generalization of classical 2D Gaussian that allows us to deal with nonlinear patterns. Specifically, we are looking for twodimensional distribution for which conditioning on the time (x) = 0 (x) = (x) = x2 (x) = Figure 3. Folded-Gaussian distribution is capable of capturing both linear and nonlinear patterns. It is crucial to highlight that the conditional distributions (marked in red) are classical Gaussians. variable would produce one-dimensional Gaussians aligned along an arbitrary curve (not necessarily linear), as shown in Figure 3. possible solution is to ensure that conditional distribution of st is Gaussian distribution (PDF) is then factorized into two independent normal densities, i.e., (m, Σ)(x) = (ms, Σs)(s) (mt, σ2 )(t). (8) (ms + (mt t), a(t)σ2 ), (4) where : and : [0, 1] are functions designed to capture desired time-dependent shift and rescaling of the space variable, respectively. In practice, we use polynomial of given order as and the likelihood of the time variable (scaled to the unit interval) as a, i.e., a(t) = (mt, σ2 (mt, σ )(t) )(mt) . (5) Finally, to recover joint distribution of space-time variable, we can apply the standard chain rule for random variables, which leads to density function given by the following formula: Note that such distribution permits the modeling of only simple linear structures that are spanned along coordinate axes (see the leftmost picture in Figure 3 for an appropriate example in the two-dimensional case). To address the aforementioned limitation, we propose incorporating non-trivial conditioning, which allows for more flexible representation. In particular, we apply the following time-dependent transformation on the space variable: (cid:112)a(t)(s ms) + ms + (mt t), (9) where : (0, 1] and : Rd1 are suitably chosen functions. This yields the new space variable with the conditional normal density (ms + (mt t), a(t)σ2 )(st) (mt, σ2 )(t). (6) (mst, Σst,a, )(st) = It is important to note that while marginal distributions of and conditional distributions of st are both Gaussian, the resulting joint distribution is not Gaussian anymore. This makes it an interesting object not only for applications, but also for further theoretical studies. To ensure the applicability of our contribution to wider range of contexts, we extend the discussion to an arbitrary dimension in the following paragraph. Folded-Gaussians in Rd We begin with multivariate Gaussian distribution (m, Σ), which is defined for space-time random variable = (s, t) Rd1 by mean vector = (ms, mt) and covariance matrix Σ = (cid:20)Σs (cid:21) , 0 σ2 (7) where Σs denotes the diagonal covariance matrix of the space component of x. The probability density function (ms + (mt t), a(t)Σs)(st), (10) which in turn gives rise to novel Folded-Gaussian distribution with the PDF defined as1: FN (m,Σ, a, )(x) = (mst, Σst, a, )(st) (mt, σ2 )(t), (11) where st represents the new time-conditioned space random variable and = (st, t) (we do not change the notation as we do not believe it would cause confusion). notable benefit of Folded-Gaussians is their ability to effectively capture range of relationships present in the data. This is due to the inherent flexibility in selecting the functions and a. In the context of the VeGaS model, the polynomial function (with trained coefficients) is employed in conjunction with the likelihood-based function (as in Equation 5). Consequently, our approach allows 1Note that in this case we are using the standard chain rule for random variables. us to encompass both linear and non-linear patterns, as illustrated in Figure 3, which refers to simplified case of two-dimensional distributions. Furthermore, the incorporation of likelihood-based time-dependent rescaling leads to the disappearance of the tails of Folded-Gaussians, thus facilitating the capture of elements present in only portion of video stream (an optimal scenario would entail these elements initially approaching the camera and subsequently receding from view). The following paragraph offers further theoretical insight into the Folded-Gaussian distribution, including the formal arguments that underpin Equations (10) and (11). Theoretical Study It should first be noted that the transformation given in Equation (9) is of an affine form As + b. Consequently, the distribution of the random variable st is also Gaussian with parameters Ams +b and AΣsAT . Given that in our case we have (cid:16) = (cid:112)a(t)Id, = ms + (mt t), (12) 1 (cid:112)a(t) (cid:17) we can conclude that st (ms + (mt t), a(t)Σs), (13) which justifies the assertion made in Equation (10). Moreover, straightforward assessment of the correctness of the definition of the Folded-Gaussian distribution, as given by Equation (11), can be conducted through the following calculation: (cid:90) FN (m, Σ, a, )(x)dx = (cid:90) (cid:18)(cid:90) (mst, Σst, a, )(st)dst (cid:19) (mt, σ2 )(t)dt (14) (cid:90) = (mt, σ )(t)dt = 1. Similarly, based on Equation (10), the PDF of the conditional distribution of the random variable st can be computed as follows: FN (m, Σ, a, )(st, t) (cid:82) FN (m, Σ, a, )(st, t)ds = (mt, σ2 )(t) (mst, Σst, a, )(st) )(t) (cid:82) (mst, Σst, a, )(st)dst (mt, σ2 = (mst, Σst, a, )(st), which corroborates our previous assertion. 4. Video Gaussian Splatting This section introduces our Video Gaussian Splatting (VeGaS) model. We start with brief overview of the 3D Gaussian Splatting (3DGS) [13] method and then proceed to the MiraGe [33] approach for 2D images, while at the same time tailoring the presentation for straightforward integration into our proposed solution. The section concludes with detailed description of the VeGaS model. 3D Gaussian Splatting The 3D Gaussian Splatting (3DGS) [13] method employs family of three-dimensional Gaussian distributions G3DGS = {(N (m, Σ), ρ, c)}, (16) characterized by set of attributes, including location (mean) m, covariance matrix Σ, opacity ρ, and color c. In practice, the covariance matrix Σ is factorized as Σ = RSSRT , where is the rotation matrix and is diagonal matrix containing the scaling parameters. Therefore, it is also possible to use the notation (m, R, S) instead of (m, Σ). The efficiency of the 3DGS technique is primarily attributable to its rendering process, which involves the projection of 3D Gaussians onto two-dimensional space. Throughout the training process, all parameters are optimized according to the mean square error (MSE) cost function. Since such procedure often results in local minima, 3DGS can employ supplementary training methods that include component creation, removal, and repositioning based on the proposed heuristic, which is both fast and effective strategy. In addition, the GS training process is executed within the CUDA kernel, allowing for rapid training and real-time rendering. Gaussian Spatting for 2D images The MiraGe [33] approach employs the 3DGS technique to accommodate 2D images. This is accomplished by employing flat Gaussians positioned on the plane spanned by the canonical vectors e1 = (1, 0, 0) and e2 = (0, 1, 0), which gives rise to specific type of parametrization. In essence, this method deals with family of 3D Gaussian components of the form GMiraGe = {(N (m, R, S), ρ, c)}, (17) where = (m1, m2, 0), = diag(s1, s2, ε), and (15) = [r1, r2, e3] = cos θ sin θ cos θ sin θ 0 0 0 0 . 1 (18) (ε is small positive constant used to ensure compatibility with the three-dimensional framework.) Subsequently, utilizing the parametrization proposed by the GaMeS [32] model, such flat Gaussians can be represented by three points (triangle face) = [m, v1, v2] = (m, R, S), (19) with the vertices defined as v1 = + s1r1, and v2 = + s2r2. On the other hand, given face representation = [m, v1, v2], the Gaussian component (m, R, S) = (T 1(V )) (20) can be reconstructed through the mean m, the rotation matrix = [r1, r2, e3], and the scaling matrix = diag(s1, s2, ε), where the parameters are defined by the following formulas: r1 = v1 v1 , r2 = orth(v2 m; r1, e2), (21) s1 = v1 m, s2 = v2 m, r2. (22) In this context, orth() represents one iteration of the GramSchmidt process [1]. We would like to highlight that the formulas presented above have been adjusted to align with our framework and may therefore differ slightly from those provided in [32, 33]. The use of the GaMeS parameterization allows for the modification of the position, scale, and rotation of Gaussians by altering the underlying triangle face. Furthermore, the MiraGe extension facilitates the manipulation of 2D images within 3D space, thereby creating the illusion of threedimensional effects. Figure 4. Video edition. Note that VeGaS permits selection of single frame and modification of some of its elements. The model was trained on the DAVIS dataset [25]. Video Gaussian Splatting Consider video comprising sequence of frames [It1, . . . , Itn], indexed by their occurrence times scaled to the unit interval [0, 1]. In this context, the MiraGe model may be employed for each consecutive frame, as it can be treated as separate 2D image. Consequently, this would result in joined family of 3D Gaussian distributions Gt1 MiraGe . . . Gtn MiraGe, (23) where each Gti MiraGe is given by Equation (17), which could be considered an adequate representation of the entire data. However, such an approach completely ignores the relationships that naturally exist within video stream. To overcome the mentioned limitation, we propose to construct Gaussians related to successive frames by conditioning of corresponding three-dimensional FoldedGaussian distributions at frames occurrence times. The resulting Video Gaussian Splatting (VeGaS) model is thus formally defined as collection on 3D Folded-Gaussians GVeGaS = {(FN (m, Σ, a, ), ρ, c)}, (24) where for each component, three-dimensional extensions (see the preceding paragraph) around two-dimensional conditional distributions (mst1 , Σst1, a, ), . . . , (mstn , Σstn , a, ) (25) are built using Equation (10), with respect to the common opacity ρ and color c. It is important to highlight our method does not use the fixed frames occurrence times t1, t2, . . . , tn, but learns them through an optimization procedure that guarantees superior reconstruction quality. Specifically, we use the dynamic frame fitting function ft : Z+ [0, 1], which maps the frame number to its scaled occurrence time tk as follows tk = ft(k) = (cid:88) i= σ(w)i = (cid:88) i=1 ewi (cid:80)n1 j=1 ewj , (26) where w1, w2, ..., wn1 are trainable parameters. (For interpolated frames between Ik and Ik+1, we use evenly spaced times between tk and tk+1). Additionally, it should be noted that in the VeGaS model we are dealing with two types of spatial distributions. First, we have Folded-Gaussians, which represent the dynamics in the video stream. Second, we generate flat conditional distributions, which are then extended to 3D Gaussians by adding small orthogonal component (controlled by the value of ε). In this case, two opposing cameras reconstruct single 2D image one produces the original image, while the other produces its mirrored version. 5. Experiments This section presents an extensive experimental study of the VeGaS model in variety of settings, which compares its efficiency with respect to diverse range of state-of-the-art solutions. Datasets The efficacy of our method was evaluated on two datasets: the Bunny dataset [16] and the DAVIS dataset [25]. The Bunny dataset comprises 132 frames with resolution of 7201280. In accordance with the specifications outlined in [4], the video is cropped to resolution of 6401280. The DAVIS dataset is high-quality and high-resolution collection of videos utilized for the purpose of video object segmentation. It encompasses multitude of videos, each comprising total of less than 100 frames. This dataset is accessible in two distinct versions: fullresolution version and more compact 480p version. Table 1. Frame reconstruction. Performance of the VeGaS model on the evaluation setting proposed in [29], using various videos from the DAVIS dataset [25], in terms of the PSNR metric. Note that, in each situation, VeGaS obtains the best metric scores. Model Bear Cows Elephant Breakdance-Flare Train Camel Kite-surf Average Omnimotion [34] CoDeF [23] VGR [29] VeGaS-Full (our) VeGaS-480p (our) 22.96 29.17 30.17 31.79 33.23 23.93 28.82 28.24 27.64 30.27 26.59 30.50 29.82 30.93 33. 24.45 25.99 27.18 29.37 33.19 22.85 26.53 28.09 31.20 32.86 23.98 26.10 27.74 30.76 32.23 23.72 27.17 27.82 35.84 38.23 24.07 27.75 28.44 31.08 33.31 Figure 5. Frame interpolation. Qualitative results obtained by VeGaS and VGR [29] on selected video object from the DAVIS dataset [25]. Frames at times and + 1 are reconstructions of two consecutive original frames, while frames at times + 1/4, + 2/4, and + 3/4 are interpolated. Note that VeGaS produces outcomes that are slightly more favorable. Implementation Details The initialization process entails the uniform sampling of Gaussian means m1 and m2, which are positioned as points within two-dimensional bounding box. The activation function utilized for mt is the sigmoid function, which is initialized in manner that ensures the resulting values from the activation process are uniformly distributed between 0 and 1. Similarly, the exponential function is employed as the activation function for σt, with the initial values distributed uniformly between 0.01 and 1. The coefficients of the polynomial function are sampled uniformly between 1 and 1. Furthermore, the rotation matrix is parameterized as single number (rotation angle) and the activation function employed is sigmoid multiplied by 2π. The angle is initialized to be uniformly distributed between 0 and 2π. The model is trained for 30,000 steps at batch size of 3, utilizing polynomial function of degree 7 and 500,000 initial Gaussians, unless otherwise specified. The learning rates, densification, pruning, and opacity reset settings are all consistent with the 3DGS [13] framework. In accordance with the MiraGe approach, two cameras are utilized: the initial camera generates the original image, while the second camera produces its mirrored version. struction tasks. The first experimental setup was adapted from that proposed in [29], where the authors introduce the VGR model and evaluate its performance compared to two state-of-the-art baselines, namely Omnimotion [34] and CoDeF [23]. Table 1 presents the values of rendering quality metrics for various videos from the DAVIS dataset. As there is no information in [29] on the resolution used for the evaluation, we report our results on both cases. It should be noted that, in each situation, VeGaS obtains the best metric scores. Furthermore, our model is capable of reconstructing videos with high quality and fidelity, as illustrated in Figure 6, which provides visualizations based on selected object from the DAVIS dataset [25]. In the other experiments, the scene setting proposed by the authors of [43] was used to evaluate the DNeRV model against various NeRF-based baselines, namely [3], E-NeRV [19], HNeRV [4], and DNeRV [43]. In accordance with this protocol, the full-resolution version of each scene is centercropped to resolution of 9601920. The results (i.e., the values of the rendering quality metrics) are presented in Table 2, which shows the values obtained for various video objects from the DAVIS dataset. It should be noted that VeGaS outperforms all the considered NeRF-based models. Frame Reconstruction We conducted series of experiments to assess the efficacy of our method in frame reconFrame Interpolation In subsequent experiments, we utilized the continuous representation of the video data proTable 2. Frame reconstruction. Performance of the VeGaS model on setting proposed in [43], using various videos from the DAVIS dataset [25], in terms of the PSNR and SSIM metrics. Note that VeGaS outperforms all baseline models. NeRV [3] E-NeRV [19] HNeRV [4] DNeRV [43] VeGaS (our) PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Blackswan Bmx-bumps Bmx-trees Breakdance Camel Car-round Car-shadow Car-turn Cows Dance-twril Dog Average 28.48 29.42 26.24 26.45 24.81 24.68 26.41 27.45 22.55 25.79 28.17 26.40 0.812 0.864 0.789 0.915 0.781 0.857 0.871 0.813 0.702 0.797 0.795 0.818 29.38 28.90 27.26 28.33 25.85 26.01 30.41 29.02 23.74 27.07 30.40 27. 0.867 0.851 0.876 0.941 0.844 0.912 0.922 0.888 0.819 0.864 0.882 0.879 30.35 29.98 28.76 30.45 26.71 27.75 31.32 29.65 24.11 28.19 30.96 28.93 0.891 0.872 0.861 0.961 0.844 0.912 0.936 0.879 0.792 0.845 0.898 0. 30.92 30.59 29.63 30.88 27.38 29.35 31.95 30.25 24.88 29.13 31.32 29.66 0.913 0.890 0.882 0.968 0.887 0.937 0.944 0.892 0.827 0.870 0.905 0.901 34.92 33.01 31.78 32.27 31.12 32.75 36.41 31.44 27.97 30.45 34.52 32. 0.932 0.915 0.896 0.950 0.886 0.941 0.956 0.852 0.834 0.850 0.914 0.902 Table 3. Ablation study. Effect of batch size and degree of the polynomial function on the performance of VeGaS in terms of the PSNR metric and the final number of Gaussians (in parentheses). The model was trained on the Bunny dataset [16]. Batch size 1 Polynomial degree 5 7 9 Mean training time 1 3 5 36.73 (1.26M) 38.15 (0.57M) 37.84 (0.33M) 37.31 (1.77M) 38.31 (0.58M) 37.94 (0.32M) 37.30 (1.73M) 38.39 (0.59M) 37.95 (0.32M) 37.36 (1.72M) 38.53 (0.62M) 37.92 (0.31M) 37.42 (1.83M) 38.24 (0.59M) 37.94 (0.31M) 31m20s 56m30s 1h15m29s tional frames, the Folded-Gaussians were sliced at uniform intervals between each pair of consecutive frames. Figure 5 compares the results obtained by VeGaS and VGR [29] on selected video object from the DAVIS dataset. The qualitative study reveals that interpolations using our method yield superior results. However, it should be noted that the source code for VGR has not been publicly released by the authors of [29], preventing direct comparison of the respective rendering metrics scores. Video Edition To illustrate the adaptability of the VeGaS model in editing video data, series of experiments were carried out on entire scenes and specific objects from the DAVIS dataset. The results, presented in Figures 2 and 4, confirm that our method allows for both global modification (e.g., multiplication or scaling) of selected objects and for the choice of single frame to modify some of its elements. Ablation Study In our ablation study, we examined the impact of different hyperparameters of the VeGaS model trained on the Bunny dataset [16]. Table 3 presents the final rendering quality metric scores and the number of Gaussians obtained for various batch sizes and degrees of the polynomial function . In turn, Table 4 presents the final metric values, numbers of Gaussians, and training times with reference to the initial numbers of Gaussians. As can be observed, VeGaS attains superior results when applied with batch size of 3 and polynomial degree of 7, with Figure 6. Frame reconstruction. Qualitative results obtained by VeGaS and VGR [29] on selected video object from the DAVIS dataset [25]. The bottom row illustrates the discrepancy between the ground truth and the reconstructed image. Note that VeGaS is capable of reconstructing videos with high quality and fidelity. Table 4. Ablation study. Effect of the initial number of Gaussians on the performance of VeGaS in terms of the PSNR metric, final number of Gaussians, and training time. The model was trained on the Bunny dataset [16] with batch size 3 and polynomial degree 7. Initial Gaussians PSNR Final Gaussians Training time 0.10M 0.20M 0.30M 0.40M 0.50M 0.60M 38.53 38.85 38.99 38.96 39.02 38.86 0.62M 0.62M 0.62M 0.64M 0.65M 0.66M 57m00s 57m29s 58m03s 59m36s 58m58s 1h00m53s vided by our model to examine the potential for frame interpolation at desired upsampling rate. To generate addistarting number of 0.50M Gaussian components. 6. Conclusions In this paper, we propose the VeGaS model, which has been designed for the processing of video. To construct VeGaS, we have introduced novel family of Folded-Gaussian distributions, which allow for the capture of nonlinear patterns in the video stream. The results of the conducted experiments demonstrate that our method enables superior reconstructions and realistic modifications within video frames."
        },
        {
            "title": "References",
            "content": "[1] Ake Björck. Numerics of Gram-Schmidt orthogonalization. Linear Algebra and Its Applications, 197:297316, 1994. 6 [2] Cheng-Hung Chan, Cheng-Yang Yuan, Cheng Sun, and Hwann-Tzong Chen. Hashing neural video decomposition with multiplicative residuals in space-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77437753, 2023. 2 [3] Hao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim, and Abhinav Shrivastava. Nerv: Neural representations for videos. Advances in Neural Information Processing Systems, 34:2155721568, 2021. 7, 8 [4] Hao Chen, Matthew Gwilliam, Ser-Nam Lim, and Abhinav Shrivastava. Hnerv: hybrid neural representation In Proceedings of the IEEE/CVF Conference for videos. on Computer Vision and Pattern Recognition, pages 10270 10279, 2023. 6, 7, 8 [5] Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong Wang. Videoinr: Learning video implicit neural representation for continuous space-time super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20472057, 2022. 2 [6] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [7] Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, and Yu-Kun Lai. Mesh-based gaussian splatting for real-time large-scale deformation, 2024. [8] Antoine Guédon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023. 3 [9] Jiahui Huang, Leonid Sigal, Kwang Moo Yi, Oliver Wang, and Joon-Young Lee. Inve: Interactive neural video editing. arXiv preprint arXiv:2307.07663, 2023. 2 [10] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled arXiv gaussian splatting for editable dynamic scenes. preprint arXiv:2312.14937, 2023. 3 [11] Adam Kania, Artur Kasymov, Maciej Zieba, and Przemysław Spurek. Hypernerfgan: Hypernetwork approach to 3d nerf gan. arXiv preprint arXiv:2301.11631. 2 [12] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):112, 2021. [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 5, 7 [14] Sylwester Klocek, Łukasz Maziarka, Maciej Wołczyk, Jacek Tabor, Jakub Nowak, and Marek Smieja. Hypernetwork functional image representation. In International Conference on Artificial Neural Networks, pages 496510. Springer, 2019. 2 [15] Agelos Kratimenos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting. arXiV, 2023. 3 [16] Janus B. Kristensen. Big buck bunny. 2010. 6, 8 [17] Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, and Dong Xu. video is worth 256 bases: Spatial-temporal expectation-maximization inversion for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75287537, 2024. 3 [18] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74867495, 2024. 3 [19] Zizhang Li, Mengmeng Wang, Huaijin Pi, Kechun Xu, Jianbiao Mei, and Yong Liu. E-nerv: Expedite neural video representation with disentangled spatial-temporal context. In European Conference on Computer Vision, pages 267284. Springer, 2022. 7, [20] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis, 2023. 3 [21] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. 3 [22] Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu, Wenliang Zhao, Lior Shapira, and Xiaohui Xie. Maskint: Video editing via interpolative non-autoregressive masked transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74037412, 2024. 3 [23] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for In Proceedings of temporally consistent video processing. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80898099, 2024. 2, 7 [24] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. 2 [25] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and Alexander SorkineHornung. benchmark dataset and evaluation methodology In 2016 IEEE Conference for video object segmentation. on Computer Vision and Pattern Recognition, CVPR 2016, 2016. 2, 6, 7, video decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26572666, 2022. 2 [39] Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, and Baoquan Chen. Bags: Building animatable gaussian splatting from monocular video with diffusion priors, 2024. 3 [40] Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, and Jun Zhang. Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting. In European Conference on Computer Vision, 2024. 2 [41] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 3 [42] Youyuan Zhang, Xuan Ju, and James Clark. Fastvideoedit: Leveraging consistency models for efficient text-to-video editing. arXiv preprint arXiv:2403.06269, 2024. 3 [43] Qi Zhao, Salman Asif, and Zhan Ma. Dnerv: Modeling inherent dynamics via difference neural representation In Proceedings of the IEEE/CVF Conference for videos. on Computer Vision and Pattern Recognition, pages 2031 2040, 2023. 7, [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [27] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, pages 74627473. Curran Associates, Inc., 2020. 1 [28] Ivan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. Adversarial generation of continuous images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1075310764, 2021. 2 [29] Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, YanPei Cao, and Xiaojuan Qi. Splatter video: Video gaussian representation for versatile processing. arXiv preprint arXiv:2406.13870, 2024. 2, 3, 7, 8 [30] Filip Szatkowski, Karol Piczak, Przemysław Spurek, Jacek Tabor, and Tomasz Trzcinski. Hypernetworks build implicit In Joint European Conneural representations of sounds. ference on Machine Learning and Knowledge Discovery in Databases, pages 661676. Springer, 2023. 2 [31] Joanna Waczynska, Piotr Borycki, Joanna Kaleta, Sławomir Tadeja, and Przemysław Spurek. D-miso: Editing dynamic 3d scenes using multi-gaussians soup. arXiv preprint arXiv:2405.14276, 2024. 3 [32] Joanna Waczynska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, and Przemysław Spurek. Games: Mesh-based adapting and modification of gaussian splatting. arXiv preprint arXiv:2402.01459, 2024. 3, 5, [33] Joanna Waczynska, Tomasz Szczepanik, Piotr Borycki, Sławomir Tadeja, Thomas Bohné, and Przemysław Spurek. Mirage: Editable 2d images using gaussian splatting. arXiv preprint arXiv:2410.01521, 2024. 2, 5, 6 [34] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1979519806, 2023. 7 [35] Wen Wang, Kecheng Zheng, Qiuyu Wang, Hao Chen, Zifan Shi, Ceyuan Yang, Yujun Shen, and Chunhua Shen. Gendef: Learning generative deformation field for video generation. arXiv preprint arXiv:2312.04561, 2023. 3 [36] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 3 [37] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. 3 [38] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised"
        }
    ],
    "affiliations": [
        "Jagiellonian University Faculty of Mathematics and Computer Science"
    ]
}