{
    "paper_title": "Performance Prediction for Large Systems via Text-to-Text Regression",
    "authors": [
        "Yash Akhauri",
        "Bryan Lewandowski",
        "Cheng-Hsi Lin",
        "Adrian N. Reyes",
        "Grant C. Forbes",
        "Arissa Wongpanich",
        "Bangding Yang",
        "Mohamed S. Abdelfattah",
        "Sagi Perel",
        "Xingyou Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes."
        },
        {
            "title": "Start",
            "content": "Performance Prediction for Large Systems via Text-to-Text Regression Yash Akhauri1,2, Bryan Lewandowski1, Cheng-Hsi Lin1, Adrian N. Reyes1, Grant C. Forbes3, Arissa Wongpanich1, Bangding Yang1, Mohamed S. Abdelfattah2, Sagi Perel1 and Xingyou Song1 1Google, 2Cornell University, 3North Carolina State University Work performed as student researcher at Google Research. Code: https://github.com/google-deepmind/regress-lm In many industries, predicting metric outcomes of large systems is fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as general, scalable alternative. For predicting resource efficiency on Borg, Googles massive compute cluster scheduling system, 60M parameter encoder-decoder, trained from random initialization, achieves up to near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the models inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes. 1. Introduction Performance prediction has been an important problem for various industrial system use cases, ranging from latency prediction (Madhyastha et al., 2006), execution times (Venkataraman et al., 2016), scheduling conflicts (Manousis et al., 2020), and transaction response timing (Stewart et al., 2007). While traditional methods have significantly relied on expert domain knowledge to model specific numeric metrics, more recent data-centric techniques have predominantly used machine-learning based regression methods by training models which predict metric 𝑦 given set of features 𝑥. 5 2 0 2 6 2 ] . [ 1 8 1 7 1 2 . 6 0 5 2 : r Figure 1 Overview: Using (𝑥, 𝑦) metric data collected from variety of system logs, we train encoder-decoder via standard next-token cross-entropy for performance prediction. However, standard machine learning regression techniques such as random forests and multi-layer perceptrons require the features 𝑥 to be represented as flat fixed-length tensors, which can be difficult for certain prediction problems with highly non-tabular representations. Even if such feature engineering concerns are resolved, compressing features only as numbers without much context can 2025 Google LLC. All rights reserved Performance Prediction for Large Systems via Text-to-Text Regression be highly lossy, and end up treating the system as blackbox (Fu et al., 2021), which can drastically decrease predictive performance. Graybox techniques (Didona et al., 2015) have attempted to mitigate these concerns by combining both domain-expertise and machine learning, by e.g. learning the coefficients to an already-derived symbolic expression (e.g. latency of website as linear function of users). These approaches often require large amounts of prior knowledge on the relationship between 𝑥 and 𝑦, and are very restrictive in their applicability. However, the advent of language models has since enabled text-to-text regression, which can avoid many of the issues regarding tensor-based feature engineering. Recently, Song et al. (2024a) showed that models even of relatively small size (<1B parameters), which we term as Regression Language Models (RLMs), are capable of enormous transfer learning if trained over large amounts of (𝑥, 𝑦) regression data, being able to improve prediction accuracy simply by absorbing information from multiple different regression tasks. Surprisingly, training with next-token prediction over numeric representations via cross-entropy loss is also quite sample efficient (Song and Bahri, 2025), capable of matching or even outperforming standard regression methods when given the same data. In this paper, we demonstrate RLMs can be applied to even simulating efficiency outcomes by training over large amounts of data from large-scale industrial systems such as Googles Borg compute cluster, which can drastically reduce production costs. In summary, our contributions are as follows: RLMs are capable of predicting numeric outcomes, such as efficiency metrics of industrial systems (e.g. Googles entire compute cluster), with high precision over complex feature representations and multi-modal outcome distributions. By fine-tuning over very small amounts of new (𝑥, 𝑦) training data, such models are highly capable of few-shot adaptation, wherein pretrained model is able to remain highly accurate on unseen compute clusters or scenarios. Comprehensive ablations provide insights into the performance improvement effects of changing sequence length, model size, feature observability, architecture, learning rate, and early stopping, in addition to the models natural uncertainty quantification abilities. 2. System Performance Prediction 2.1. Background We provide brief overview of Borg (Verma et al., 2015), the compute management system at Google. The system comprises of centralized manager for scheduling jobs, and list of machines located in different cells with compute resources. Each user may send in task request, containing binary executable with additional details such as resources needed, number of replicas, and so on. The role of the manager is to place each task on an appropriate machine (i.e., one where the requisite resources are available). To effectively allocate computing tasks to resources, the scheduler must know the resource consumption (i.e. utilization) of task on that machine and ultimately how much useful work can be done (i.e. productivity), in order to execute specialized bin-packing algorithm for assigning jobs, to maximize total productivity across the entire compute cluster. For given process, this productivity metric is formally defined as Millions of Instructions Per Second per Google Computing Unit (Schneider and Mattia, 2024) of usage, or MIPS per GCU. Intuitively, this metric represents the amount of work that gets done per unit of time by using per unit of computing resource. Empirically, after job assignment has been made, MIPS per GCU can be computed periodically 2 Performance Prediction for Large Systems via Text-to-Text Regression by profiling the total instructions executed and total CPU cycles consumed in 10-second time window. This metric can be affected by numerous factors, ranging from current memory and CPU usage, hardware type, and workload mixes across numerous machines. Furthermore, even variations in time can even have an effect, as workloads within physical cell depend on the usage patterns by users within the same geographic location. Additionally, hyperparameters affecting the behavior of the underlying bin-packing algorithm may also strongly affect the outcome. Fortunately, Google has developed digital twin of Borg, sophisticated backtesting framework which uses checkpoint files of real-world clusters to replicate the state of those clusters, perform the scheduling algorithm, and determine the aggregate cluster-wide MIPS per GCU across all machines. However, due to the inherently sequential nature, generating even single outcome can require between 1 to 18 hours of computation regardless of resources used. Fortunately, such outcomes are logged as valuable offline datasets for training cheap regression models, albeit from limited data. If regressor is able to accurately predict this metric with negligible inference time, this can lead to large savings not just from avoiding computation, but also from faster optimization of the MIPS per GCU overall. For instance, Google Vizier (Golovin et al., 2017) is regularly used to tune the Borg schedulers hyperparameters, but unfortunately its Gaussian Process regressor (Song et al., 2024b) can at most only observe tabular data formats, drastically limiting its predictive and overall optimization performance. 2.2. Prediction Task The goal is to predict the MIPS per GCU efficiency metric, i.e. single observed floating point number, after specialized bin-packing algorithm has been performed, given the initial state of the compute cluster, time window used for collecting metric data, and hyperparameters of the scheduling algorithm. More specifically, the following available information may all be used as input features: Cluster name (i.e. cell). Physical location of the cell. The window of time used to collect the state of the cell and the exact time the performance profiles were collected. Hyperparameters affecting the behavior of the Borg schedulers bin-packing algorithm. Concurrent entities or teams using the largest amount of compute within this cell. Distribution and network hierarchy of hardware platforms installed in the cell. list of job-on-machine performance profiling results. All of these features are presumed to be important based on minimal domain knowledge, but it is not apriori clear how exactly they may affect this metric. For example, the efficiency metric is heavily dependent on the cell, which is the specific cluster of machines and hardware, but the actual cell name itself is not true feature. Features such as timing windows and hyperparameter names can be shared or correlated across different tasks, but may affect efficiency metrics differently. Furthermore, the prediction task can be dynamic, as new features values may appear over time, due to platform upgrades and hardware changes. The features may also contain varying types, e.g. categorical hyperparameters, different subsets of hardware types, and cyclical timestamps. Additional complexity arises due to the deep nested nature of many of the features e.g. job-on-machine profilings containing information about jobs and which machines they utilize, which also contain information about hardware profiles, and so forth. Lastly, the metric may also possess various forms of noise, some dependent on the features observed by the regressor, as we discuss below. Performance Prediction for Large Systems via Text-to-Text Regression 2.3. Observed Features vs. Uncertainty The most broad abstraction to define our prediction task is to assume there is an underlying distribution 𝑝( 𝑦𝑥) where 𝑥 is the full state of the world. Thus the randomness induced by 𝑝( 𝑦𝑥) can be considered irreducible noise, also known as aleatoric uncertainty (Hüllermeier and Waegeman, 2019), which limits the optimal performance of pointwise regressor. For Borg, this can come from the inherent randomness of the bin-packing algorithm and stochastic load/demand (e.g. for user-facing services). More formally, based on the well-known Bias-Variance decomposition, the expected squared error of pointwise regressor 𝑓𝜃 per input 𝑥 is lower bounded by the variance of the 𝑦: 𝔼𝑦𝑝( 𝑦 𝑥 ) (cid:2)( 𝑦 𝑓𝜃(𝑥))2(cid:3) Var( 𝑦𝑥) (1) However, if regressor is only able to observe partial subset or limited representation 𝜙(𝑥) of the full state, it will be unable to distinguish separate 𝑥, 𝑥 if 𝜙(𝑥) = 𝜙(𝑥). This lack of distinguishability induces epistemic uncertainty, and instead leads to an even higher right hand variance term Var( 𝑦𝜙(𝑥)), limiting the regressors optimal performance even more (theory in Appendix A.1). For practical purposes, given an offline test dataset = {(𝑥𝑖, 𝑦𝑖)}𝑖0, we can estimate TotalVariance𝜙(D) to provide lower bounds on the mean squared error (MSE) of regression methods evaluated over all test data, as: TotalVariance𝜙(D) = 1 𝐾 𝐾 𝑘= Var( 𝑦𝑥 X𝑘) (2) where X1, . . . , X𝐾 are equivalence classes partitioning D, i.e. 𝑥, 𝑥 X𝑘 iff 𝜙(𝑥) = 𝜙(𝑥), and variance is calculated empirically over the set of all 𝑦-values obtained from inputs within X𝑘. Note that if 𝜙 is null (no observed features), this expression would simply be the variance of the entire 𝑦-population. Similar bounds exist for other regression-based metrics, e.g. rank correlations, since determining the relative rankings of 𝑦-values from the same 𝑥-equivalence class would be impossible, and density estimator 𝑝𝜃s log-likelihood would be bounded by the conditional entropy of 𝑝( 𝑦𝜙(𝑥)). In any case, it thus would be ideal if the regressor is able to maximize the amount of features it observes in order to minimize epistemic uncertainty. 2.4. Why Text-based Regression? Representing all of the features above into one single fixed-length tensor in ℝ𝑑 will be notoriously difficult if using traditional tabular regressor such as multi-layer perceptron (MLP) or random forest. Many of the features such as the list of jobs and platforms can contain arbitrary cardinalities per 𝑥, and would therefore need to be grouped using hand-selected methods or heuristics. Even if useful set of hand-selected metrics can be found, tabular featurization requires apriori defining finite number of classes per categorical parameter and maximum/minimum bounds for numbers for normalization when new class of machines or workloads emerges, the entire process must be started from scratch and all the training data produced from the prior methods rigid featurization are made incomplete and invalidated. flexible text-based input representation simply resolves these issues by allowing variable sequence length inputs and does not require explicit enumeration of categorical features nor normalization of continuous ones. By observing all available features, especially nested ones difficult to represent as tabular formats, the model minimizes epistemic uncertainty and opens the doors to achieving the best performance possible. Furthermore, such text-based model would not need to restart from scratch when encountering new sources of data, but rather can be simply fine-tuned over new tasks to allow fast few-shot adaptation by transferring knowledge learned from previously trained checkpoints. 4 Performance Prediction for Large Systems via Text-to-Text Regression cell: cell_a 2024/06/02 17:00:00 PDT, Day:Fri Week:21 search_space: {JOB/data_pipeline/PRODUCTION_WORKLOAD: [machineE, machineA, none_selected]} assignments: {\"JOB/data_pipeline/PRODUCTION_WORKLOAD\": \"machineA\"} distributions: - platform: {machineA} num_machines: 1.239e+03 low_level_zones: 5.200e+01 mid_level_zones: 5.200e+01 high_level_zones: 4.300e+01 resources: 5.481e+05 job_profiles: - job: {user: data_pipeline, group_name: data_pipeline_workers} platform_profiles: machineD: {mean_mips_per_resource_usage: 8.165e+02} machineA: {mean_mips_per_resource_usage: 9.590e+02} machineF: {mean_mips_per_resource_usage: 8.321e+02} machineC: {mean_mips_per_resource_usage: 7.098e+02} limits: job_requested_resource_limit: 1.217e+04 job_requested_num_vms: 1087 Figure 2 Example anonymized string representations of some features used to construct 𝑥. More detailed representation can be found in Appendix B.1. Feature Type Cell Name Physical Location Time Window Scheduler Hyperparameters Machine Distribution Job-on-machine Performance Average Character Count 3 8 86 1,082 461 268,157 Table 1 Average character counts for each YAML feature. Figure 3 Distribution of string lengths. In Figure 2, we provide an example of string, represented in standard format (e.g. YAML), that can be sent to the model. Table 1 gives the average character counts for each of the features across all data, while Figure 3 gives the distribution of total string lengths. 3. Method Below, we outline the technical details of our RLM, primarily based on OmniPred (Song et al., 2024a). For reference survey on standard language model training and fine-tuning, we refer the reader to Zhang et al. (2023). 3.1. Preliminaries: Text-to-Text Regression In the standard language model setting, given batch of (prompt, response) pairs, one performs model updates by minimizing the next-token cross-entropy loss over response tokens. In the textto-text regression case, the prompt corresponds to string representation of 𝑥, while the response corresponds also to textual or structured token representation of the floating value 𝑦. This training can be performed over any collection of data points, due to the universality of string representations. At inference time, the model can be interpreted as density estimator 𝑝𝜃( 𝑦𝑥), from which pointwise prediction (cid:98) 𝑦 can be made by aggregating i.i.d. samples 𝑦 (1) , . . . , 𝑦 (𝑠) 𝑝𝜃( 𝑦𝑥). 3.2. Design Choices Some of the following design choices for our RLM can be considered contrary to popular belief. We justify their choices below, some of which require nuanced discussion based on findings from prior work and this papers ablations: Decoding-based Output: We train with cross-entropy loss over tokens rather than error-based loss (e.g. MSE) over an additional value head. The cross-entropy loss magnitude is agnostic to the strength of the prediction gap, leading to more stable training over multiple different 𝑦-values, and avoids over-focusing on tasks which naturally have larger errors due to wider 𝑦-value spreads. 5 Performance Prediction for Large Systems via Text-to-Text Regression Furthermore, it has been broadly observed across both optimization (Tan et al., 2025) and reward modeling communities (Mahan et al., 2024; Zhang et al., 2025) that presumably due to their overly compressive nature, embedding-based or logit-based methods can perform worse than simply decoding the numeric prediction end-to-end. Use of Encoder: Many current LLM designs only use the decoder-only architecture, as it simplifies designs by concatenating (prompt, response) pairs together without the need for specifying separate sequence lengths. However, as we show in our ablations in Section 5.2, separate encoder layers are necessary for processing complex 𝑥, and decoder layers alone are suboptimal. No Language Pretraining: Despite the current trend of pretraining models on human-generated text in e.g. English, it is not necessary nor guaranteed beneficial to use pretrained LLM checkpoint from which to train regression model. In fact, Song et al. (2024a) found that tabular regression is possible tabula rasa with randomly-initialized language model. This is presumably because regression only requires learning the correlations between different structured tokens and does not necessarily benefit from the semantic meaning behind words. y-Tokenization: The cross-entropy loss does not have an inherent notion of numeric distance, as tokenization effectively discretizes the real number line. Since the model needs to learn an embedding for each token, it is important to minimize the vocabulary size used for representing floats, e.g. using <0>, <1>,..., <999> is far less effective than using few digit tokens <0>, <1>, ..., <9>. We use the P10 tokenization found in (Charton, 2022), in which 𝑦 is represented using special sign, mantissa, and exponent tokens, e.g. <+><7><2><5><E-1> represents 725 101 = 72.5. This tokenization is also normalization-free, which allows easy multi-task training without needing to precompute minimum or maximum 𝑦-value bounds for every separate task. Context-Free: Our model is context-free regressor which only observes single 𝑥 and returns single 𝑦, instead of first preprocessing multiple (𝑥1, 𝑦1), (𝑥2, 𝑦2), . . . in-context (Vacareanu et al., 2024). This maximizes use of the sequence length for observing the string representation of single 𝑥, which may already be thousands of tokens long. This further allows unlimited data to be absorbed within the model weights, rather than having finite limits at inference time due to the context buffer. This distinction is analogous to using random forests and MLPs instead of Gaussian Processes for traditional regression, and weight-based updates instead of hidden memory states for meta-learning. 3.3. Fine-tuning Similar to standard LLM practices, the RLM also allows fine-tuning against additional data even after pretraining. This serves two different purposes: (1) adaptation to new unseen regression task using pretrained knowledge, and (2) refocusing over previously seen task (e.g. if the pretraining dataset was too large). To do so, we simply restore both the weights of pretrained checkpoint and the optimizer state and resume standard training with possibly lower learning rate, over the new data. The number of new examples can be arbitrarily low (e.g. 1-512), and effectively acts as replacement to in-context learning via gradient update-based few-shot learning. Due to the relatively small size of our model, this procedure also only requires few minutes on single GPU. This can be seen as form of meta-learning (Finn et al., 2017) where pretraining leads to checkpoint which can quickly be gradient-adapted to new tasks. Note that since the language model is able to observe task-identifier feature, e.g. the cell name, it can perform simultaneous regression over multiple tasks simply by pretraining over all task data without any specialized techniques, and then gain the ability to regress over new tasks after fine-tuning. 6 Performance Prediction for Large Systems via Text-to-Text Regression 3.4. Regression Scaling Paradigm The RLM maximizes scaling on multiple axes. However, we find that the two most important scaling factors for regression are diverse training data and feature observability. Since the task of regression is fundamentally based on learning the continuity of functions, e.g. that 𝑦1 should be close to 𝑦2 if 𝑥1 and 𝑥2 are also close, better feature observability allows the model to learn better continuous representations of inputs 𝑥, while more training data provides better coverage over the input space. In contrast, other axes such as model size are not necessarily very important; the task of regression is inherently discriminative and does not require large models for text generation. Furthermore, sequence length requirements can especially be reduced if user with domain knowledge can efficiently compress string representations by e.g. removing commas or whitespaces, and also placing the presumably most important features at the beginning of the string representation. For this specific paper, our default model (exact details in Appendix C) only requires 2 layer encoder-decoder (60M parameters) with 2K maximum sequence length for sufficient results. 4. Experiments Overall, while our results are demonstrated on performance prediction for Googles Borg compute cluster, they are meant to serve an impression of what results may appear when text-to-text regression is applied to any large system. The general conclusions are: Maximizing feature observability enables significantly improved regression performance, surpassing previous baselines. Large-scale pretraining on extensive datasets proves crucial, particularly for effective transfer learning and robust adaptation to new tasks. RLMs prove to be universal in their capabilities, allowing both pointwise prediction and density estimation, while also scaling efficiently with data, model size, and sequence length. 4.1. Data and Evaluation Protocol Specifically for our application, we define task 𝑇 as collection of 28K-56K {(𝑥𝑖, 𝑦𝑖)}𝑖0 state-outcome pairs specifically from cell during month (June or November). Pairs are randomly shuffled into train/validation/test splits in standard 8/1/1 ratio. The model may be pretrained on combination of training splits from multiple tasks = {𝑇1, 𝑇2, . . .}, evaluated at test time either in-distribution for task 𝑇 if 𝑇 , and out-of-distribution if 𝑇 . Since each task is parameterized by cell and month, an out-of-distribution evaluation can be performed across either/both unseen time and cell axes, depending on the figure (exact details in Appendix C). To maximize benchmarking quality, our paper uses total pool of 40 cells with the largest 𝑦-value spreads. To evaluate regression performance, we use variety of common measurements, appropriate to the analysis. MSE is used alongside lower bound TotalVariance𝜙 to assess precision. To avoid bias from different 𝑦-scalings between tasks, scale-invariant Spearman rank-correlation (𝜌) can also be used, especially when only rankings are sufficient, common in optimization-based applications. In ablation studies, we also use validation cross-entropy losses as simple yet accurate proxies of evaluation performance. 4.2. Results We begin with case study on the RLMs ability to regress from strings given enough training data, on the task 𝑇 with the highest spread of 𝑦-values. In Figure 4 (left), we pretrain the RLM simultaneously 7 Performance Prediction for Large Systems via Text-to-Text Regression Figure 4 Left: Diagonal fit () is better. RLMs pointwise prediction against ground truth target. Right: Higher is better (). Spearman-rank correlation of the test evaluations, after removing top outliers by MSE error. over 8 tasks (1M data points, or 2B tokens) and evaluate in-distribution on 𝑇 to achieve 0.86 rank correlation. Furthermore, when fine-tuning the pretrained checkpoint on only 512 few-shot examples from completely new task 𝑇 , we see that the RLM can also achieve an equally strong result. In contrast, randomly initialized checkpoint only given 512 examples is unable to achieve the same level of performance in either in-distribution and out-of-distribution cases, demonstrating the importance of large-scale pretraining and transfer learning. Figure 5 Better density capture of target points is better. Kernel Density Estimate (KDE) plot of samples from 𝑝𝜃( 𝑦𝑥) along with actual target points, over varying timestamps. Note that samples are generated from 𝑥s with distinct timestamps, while some target points may share timestamps. Note that large contribution to the outlier residual errors likely comes from aleatoric uncertainty, i.e. 𝑦 sampled from probability distribution 𝑝( 𝑦𝑥) rather than deterministic pointwise outcome. Figure 5 also visualizes the regressors density estimation abilities when varying along single feature in 𝑥, i.e. the time when the computation was performed. Since the RLMs representation 𝜙(𝑥) is very rich and unique for every state 𝑥, the training data itself does not possess any multi-modality 8 Performance Prediction for Large Systems via Text-to-Text Regression on 𝑦-values. However, it is remarkable that after training, the output 𝑝𝜃( 𝑦𝑥) still expresses multiple modes, due to the RLMs inductive bias and learned continuous representations of 𝑥. In Figure 6, we further compare the RLMs performance against theoretically optimal results achievable by baselines using limited representations 𝜙(𝑥), i.e. observing only tabular hyperparameter features or nothing at all (null), using the TotalVariance𝜙 lower bounds in Section 2.3. The RLM produces much lower residuals overall, with 100x lower MSE than possible with tabular features, demonstrating the importance of maximizing feature observability. Figure 6 Left-skewness () is better. Note both axes are log-scaled. Left: Distribution of residuals as per-sample squared error, along with mean squared error as vertical line. Right: Analogous results, but for an out-of-distribution task. In Figure 7, we demonstrate the value of large-scale pretraining, especially for transfer-learning over new tasks. Our starter checkpoints are pretrained over set of 𝑁 tasks for various 𝑁 over distinct cells. While the in-distribution case shows negligible gains when varying 𝑁, the out-of-distribution case shows that higher 𝑁 leads to substantially better results on unseen cells. Figure 7 Higher is better (). Evaluation task rank-correlation, when varying the number of finetuning examples starting from pretrained checkpoint trained over varying numbers of tasks. Runs repeated over 10 seeds each and averaged. 9 Performance Prediction for Large Systems via Text-to-Text Regression Regardless if the cause of error is from inherent randomness (aleatoric) or limited data coverage (epistemic), any regression method may fundamentally be inaccurate given certain inputs. For these cases, it should at least possess higher uncertainty (e.g. density variance), crucial to downstream applications such as Bayesian Optimization. In Figure 8, we confirm there is high correlation between the variance of 𝑝𝜃( 𝑦𝑥) and the residual error from sample (𝑥, 𝑦). Furthermore, in Figure 9, we see that the RLMs density at single example appropriately expresses bimodality when needed, corroborating Figure 5. Figure 8 Correlation between prediction uncertainty (density variance) and residual squared error. Figure 9 RLM density 𝑝𝜃( 𝑦𝑥) for an example input 𝑥, along with true 𝑦-values. While the above results show the RLMs useful properties, we finally present the overall comprehensive set of benchmarking results starting from single pretrained checkpoint, over various tasks denoted by 𝐶𝑚𝑜𝑛𝑡ℎ (see Appendix B.2 for naming conventions). Multi-task results demonstrate the RLMs 𝑐𝑒𝑙𝑙 ability to perform inference simultaneously (when in-distribution) and via transfer learning (when out-of-distribution). Fundamentally, it is able to do so by identifying the task using the (cell, time) features and then observing other intra-task features to precisely predict the metric. We see that the RLM can achieve very precise pointwise regression (Figure 10) on variety of tasks with different properties and scales, when the task presumably possesses low noise. For these cases, the Spearman rank is also very high, with the majority of cases reaching above 0.93. Figure 10 Diagonal fit () is better. Scatter plot of predictions and ground truth targets over multiple tasks. Performance Prediction for Large Systems via Text-to-Text Regression Furthermore, in tasks likely to possess higher aleatoric noise which make pointwise predictions inappropriate, the RLM can perform density estimations instead (Figure 11), capturing many different modes and density shapes. Figure 11 Better density capture of target points is better. KDE plot over multiple tasks. EV = 1 MSERLM/MSEnull and 𝑅2 To quantify the actual gains of the RLM and to demonstrate the consistency of our technique, in Figure 12 we perform multiple runs and aggregate both the explained variance for pointwise regression and McFaddens Pseudo-𝑅2 (McFadden, 1974) for density estimation, respectively denoted as 𝑅2 NLL = 1 NLLRLM/NLLnull. Explained further in Appendix A.2, these measurements capture the overall modeling gain of 𝑝( 𝑦𝑥) from observing 𝑥 against the null case 𝑝( 𝑦). Interestingly, the RLM achieves near perfect prediction on task 𝐶 𝑁𝑂𝑉 4 with 𝑅2 1, and EV even if the RLM achieves lower 𝑅2 EV on noisy tasks, it can still achieve higher 𝑅2 NLL instead, as shown on tasks 𝐶 𝑁𝑂𝑉 , 𝐶 𝐽𝑈 𝑁 21 24 , 𝐶 𝑁𝑂𝑉 22 . Figure 12 Higher is better (). Left: Explained variance per task. Right: McFaddens Pseudo-𝑅2 per task. Tasks are sorted by decreasing resultant rank correlations. 5. Experiments: Ablations 5.1. Cross-Entropy Loss We investigate the relationship between cross-entropy loss as proxy for more traditional regression metrics such as MSE and Spearman rank correlation. During training run, we evaluate all saved checkpoints, and based on the validation loss curve, label them as either underfitted or overfitted. In Figure 13, we see direct correlation with MSE, while interestingly in Figure 14, we find that overfitted models can still maintain higher rank correlation. 11 Performance Prediction for Large Systems via Text-to-Text Regression Figure 13 Lower is better (). MSE across checkpoint-steps. Figure 14 Higher is better (). Spearman 𝜌 across checkpoint-steps. 5.2. Architecture In Figure 15, we show the importance of processing the input 𝑥 with encoders. When accounting for equal model sizes (via layer count), we find that encoder-decoder architectures perform substantially better than decoder-only architectures even when using bidirectional attention on inputs. Note the stark contrast to LLM designs such as Gemma (Mesnard et al., 2024) and Llama (Touvron et al., 2023) which are decoder-only. We hypothesize that while decoder-only models are strong at producing outputs and chains of thought given relatively simple prompts, the information pathways routing through the decoder are insufficient to deal with complicated prompts 𝑥, although further investigation is needed. Figure 15 Lower is better (). Validation losses when varying architectures. Note: 0E4D is decoder-only model. Figure 16 Lower is better (). Lowest observed validation losses when varying model sizes. In Figure 16, we also see that larger models with more parameter counts does improve regression performance, but surprisingly this quickly plateaus within the O(100M) range, which is orders of magnitudes lower than state-of-the-art general LLM models within the O(1B) range. This supports the broad utility of our text-to-text method, which requires relatively low amounts of compute, e.g. at most 1 GPU. 5.3. Feature Importance In Figure 17, we see that increasing the input length allows the model to observe more features from 𝑥 for predicting 𝑦, and thus achieving lower validation loss. When approximately 𝐿 3000, the additional tokens mostly come from the last remaining, longest, and least important features, specifically the job-on-machine performance mentioned in Table 1, which explains the diminishing 12 Performance Prediction for Large Systems via Text-to-Text Regression returns with higher sequence lengths. Figure 17 Lower is better (). Lowest observed validation losses when training over varying maximum sequence lengths. Figure 18 Lower is better (). Validation losses when showing certain features (C = Cell, = Window). = using rest of the features. In Figure 18, we also see that the models behavior based on observing certain features aligns with our expectations from domain knowledge. For example, the performance of cluster may heavily depend on temporal cycles there are fewer e.g. YouTube jobs at night due to lower user counts, or certain jobs may not run over the weekend. Experimentally, the model validates this intuition, as its performance substantially improves when its 𝑥 representation contains the period in which we performed bin-packing, i.e. time window feature, shown as e.g. 24-06-12T06:00:00Z to 2024-06-12T06:05:00Z. 5.4. Few-Shot Adaptation Relevant for future practitioners, we provide useful ablations on the optimal settings for fine-tuning, and how they may affect results. In Figure 19, we see that finding the optimal learning rate matters significantly for reducing prediction errors when fine-tuning on multiple examples In Figure 20, we further see that earlier checkpoints from pretraining also can lead to better results when fine-tuning on out-of-distribution tasks. This is because an overly pretrained model may metaoverfit to the pretraining tasks themselves, making it harder to adapt to unseen tasks, especially those more different from pretraining. Figure 19 Lower is better (). MSE after OOD fine-tuning from fixed checkpoint, while varying the learning rate. Early checkpoint (Step 10K) is used for adaptation. Figure 20 Lower is better (). MSE after OOD fine-tuning from specific checkpoint-step during pretraining. fixed learning rate of 5 105 was used. 13 Performance Prediction for Large Systems via Text-to-Text Regression 6. Conclusion We have validated the text-to-text regression approach for performance prediction in complex, industrial environments, by specifically demonstrating its effectiveness on Googles extensive compute cluster system. Our relatively cheap and simple encoder-decoder RLM, without relying on general language pretraining, can directly train over rich, non-tabular inputs like system logs and configuration files, return highly accurate floating point predictions, and quickly adapt to new tasks with minimal extra data. Ultimately, this work showcases RLMs as powerful, general, and scalable tools for predicting metric outcomes from raw text. They alleviate the burdens of manual feature engineering and open new avenues for creating universal simulators for complex systems. Furthermore, by accurately modeling numeric feedback from varied inputs, RLMs can serve as foundational aspect for developing sophisticated reward models to quickly give real-world feedback and operational experience (Silver and Sutton, 2025), catalyzing future research on reinforcement learning for language models."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Uri Alon, Jonathan Lai, Mangpo Phothilimthana, Amir Yazdanbakhsh, David Smalling, Dara Bahri, Michal Lukasik, Rong-Xi Tan, Ke Xue, Shao-Hua Sun, Kuang-Huei Lee, Xinyun Chen, Chansoo Lee, Daiyi Peng, Jiyoun Ha, Aviral Kumar, Zi Wang, Gaurav Dhiman, and Yutian Chen for useful discussions and Yili Zheng, David Lo, Martin Dixon, Daniel Golovin, Denny Zhou, Claire Cui, Ed Chi, and Benoit Schillings for continuing support. 14 Performance Prediction for Large Systems via Text-to-Text Regression"
        },
        {
            "title": "References",
            "content": "F. Charton. Linear algebra with transformers. Trans. Mach. Learn. Res., 2022, 2022. D. Didona, F. Quaglia, P. Romano, and E. Torre. Enhancing performance prediction robustness by combining analytical modeling and machine learning. In L. K. John, C. U. Smith, K. Sachs, and C. M. Lladó, editors, Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering, Austin, TX, USA, January 31 - February 4, 2015, pages 145156. ACM, 2015. doi: 10.1145/2668930.2688047. C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1126 1135. PMLR, 2017. S. Fu, S. Gupta, R. Mittal, and S. Ratnasamy. On the use of ML for blackbox system performance prediction. In J. Mickens and R. Teixeira, editors, 18th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2021, April 12-14, 2021, pages 763784. USENIX Association, 2021. D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley. Google vizier: service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017, pages 14871495. ACM, 2017. doi: 10.1145/3097983.3098043. E. Hüllermeier and W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: tutorial introduction. CoRR, abs/1910.09457, 2019. T. Kudo and J. Richardson. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 6671, 2018. H. V. Madhyastha, T. E. Anderson, A. Krishnamurthy, N. Spring, and A. Venkataramani. structural approach to latency prediction. In J. M. Almeida, V. A. F. Almeida, and P. Barford, editors, Proceedings of the 6th ACM SIGCOMM Internet Measurement Conference, IMC 2006, Rio de Janeriro, Brazil, October 25-27, 2006, pages 99104. ACM, 2006. doi: 10.1145/1177080.1177092. D. Mahan, D. V. Phung, R. Rafailov, C. Blagden, N. Lile, L. Castricato, J.-P. Fränken, C. Finn, and A. Albalak. Generative reward models, 2024. A. Manousis, R. A. Sharma, V. Sekar, and J. Sherry. Contention-aware performance prediction for virtualized network functions. In H. Schulzrinne and V. Misra, editors, SIGCOMM 20: Proceedings of the 2020 Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, Virtual Event, USA, August 10-14, 2020, pages 270282. ACM, 2020. doi: 10.1145/3387514.3405868. D. McFadden. Conditional logit analysis of qualitative choice behavior. In P. Zarembka, editor, Fontiers in Econometrics, pages 105142. Academic press, New York, 1974. T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, L. Hussenot, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. Héliou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A. ChoquetteChoo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, 15 Performance Prediction for Large Systems via Text-to-Text Regression J. Labanowski, J. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, and et al. Gemma: Open models based on gemini research and technology. CoRR, abs/2403.08295, 2024. doi: 10.48550/ARXIV.2403.08295. I. Schneider and T. Mattia. Carbon accounting in the cloud: methodology for allocating emissions across data center users, 2024. N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 45964604. PMLR, 1015 Jul 2018. D. Silver and R. S. Sutton. Welcome to the era of experience, 2025. X. Song and D. Bahri. Decoding-based regression, 2025. X. Song, O. Li, C. Lee, B. Yang, D. Peng, S. Perel, and Y. Chen. Omnipred: Language models as universal regressors. CoRR, abs/2402.14547, 2024a. X. Song, Q. Zhang, C. Lee, E. Fertig, T. Huang, L. Belenki, G. Kochanski, S. Ariafar, S. Vasudevan, S. Perel, and D. Golovin. The vizier gaussian process bandit algorithm. CoRR, abs/2408.11527, 2024b. doi: 10.48550/ARXIV.2408.11527. C. Stewart, T. Kelly, and A. Zhang. Exploiting nonstationarity for performance prediction. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007, EuroSys 07, page 3144, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595936363. doi: 10.1145/1272996.1273002. R.-X. Tan, M. Chen, K. Xue, Y. Wang, Y. Wang, F. Sheng, and C. Qian. Towards universal offline black-box optimization via learning string embedding space. In International Conference in Machine Learning, 2025. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. R. Vacareanu, V. Negru, V. Suciu, and M. Surdeanu. From words to numbers: Your large language model is secretly capable regressor when given in-context examples. CoRR, abs/2404.07544, 2024. doi: 10.48550/ARXIV.2404.07544. S. Venkataraman, Z. Yang, M. J. Franklin, B. Recht, and I. Stoica. Ernest: Efficient performance prediction for large-scale advanced analytics. In K. J. Argyraki and R. Isaacs, editors, 13th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2016, Santa Clara, CA, USA, March 16-18, 2016, pages 363378. USENIX Association, 2016. A. Verma, L. Pedrosa, M. R. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes. Large-scale cluster management at Google with Borg. In Proceedings of the European Conference on Computer Systems (EuroSys), Bordeaux, France, 2015. L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025, 2025. S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, et al. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2023. 16 Performance Prediction for Large Systems via Text-to-Text Regression A. Additional Theory A.1. Epistemic Uncertainty Following up from Section 2.3 on partial observability, when only 𝜙(𝑥) is observed rather than the full state 𝑥, we can quantify the gap in terms of the well known Law of Total Variance: Var( 𝑦𝜙(𝑥)) = 𝔼𝑥𝑝(𝑥 𝜙(𝑥 ) ) [Var( 𝑦𝑥)] + Var𝑥𝑝(𝑥 𝜙(𝑥 ) ) (𝔼[ 𝑦𝑥]) (3) Here: 𝔼𝑥𝑝(𝑥 𝜙(𝑥 ) ) [Var( 𝑦𝑥)] is the average aleatoric uncertainty. It represents the expected value of the irreducible noise, averaged over all possible full states 𝑥 that are consistent with the observed partial representation 𝜙(𝑥). Var𝑥𝑝(𝑥 𝜙(𝑥 ) ) (𝔼[ 𝑦𝑥]) is the crucial term representing the contribution of epistemic uncertainty It is the variance of the true conditional mean 𝔼[ 𝑦𝑥] (the optimal to the total variance. prediction if were known) due to the unobservability of given only 𝜙(𝑥). Since Var𝑥𝑝(𝑥 𝜙(𝑥 ) ) (𝔼[ 𝑦𝑥]) 0, it follows that Var( 𝑦𝜙(𝑥)) 𝔼𝑥𝑝(𝑥 𝜙(𝑥 ) ) [Var( 𝑦𝑥)] (4) This inequality formally shows that observing only partial representation 𝜙(𝑥) increases (or at best, keeps the same) the variance that lower-bounds the regressors achievable error, compared to the average aleatoric variance. The optimal performance of regressor observing 𝜙(𝑥) is now limited by Var( 𝑦𝜙(𝑥)). Thus, the limited observability captured by 𝜙(𝑥) induces epistemic uncertainty which directly contributes to higher overall variance, further limiting the regressors optimal performance. A.2. Explained Negative Log-Likelihood It is well known that the traditional explained variance 𝑅2 EV can be computed in terms of MSE ratios, i.e. 1 MSERLM/MSEnull, where the null case simply corresponds to the variance of the total 𝑦-population. McFadden (1974) also provides an analogous expression for density estimation cases, where negative log-likelihood (NLL) is instead used to calculate performance: 𝑅2 NLL = 1 NLLRLM/NLLnull (5) Note that simply using absolute NLLRLM as the final measurement is ambiguous, since there may not be an established reference for what is good or bad. Instead, the reference is chosen as NLLnull corresponds to density estimator which models 𝑦 without using 𝑥. Here, we choose the RLM itself, due to its decoders universal density approximation abilities (Song and Bahri, 2025) over more constrained distributions such as Gaussians. Note that NLLnull is also an empirical approximation of the true entropy, which is impossible to calculate precisely as it requires online access to the true distribution. 17 Performance Prediction for Large Systems via Text-to-Text Regression B. Additional Data Information B.1. Example String Representation In Figure 21, we show what the model exactly observes. Note the natural use of the newline character as natural separator between different features. ncell: cell_ann2024-06-12T06:00:00Znn2024-06-12T06:05:00ZnnLarge users: [datapipeline-prod, researcher_human_1, monitoring-jobs, model-server-public-prod-jobs, ecommerce, ads_fetch, researcher_human_2, real-time-diagnostics , storage_1, storage_2, storage_3, webserver, storage_aggregation, storage_frontend, researcher_human_5, routing, storage_4, researcher_human_4, storage_5, analytics-storage_6]nsearch_space: {machineA: [machineA , none_selected]} {machineD,machineC: [machineD, machineC, none_selected]} {machineA,machineE: [machineA, machineE, none_selected]} {machineF: [machineF, none_selected]} {machineD,machineA: [machineD, machineA , none_selected]} {machineD: [machineD, none_selected]} {machineE: [machineE, none_selected]} { machineD,machineE: [machineD, machineE, none_selected]} {JOB/storage_4/PRODUCTION_WORKLOAD: [machineE, none_selected]} {JOB/processing_1/me.processing_1.processing_1/PRODUCTION_WORKLOAD: [machineE, none_selected]}n nassignments: {\"JOB/storage_4/PRODUCTION_WORKLOAD\": \"none_selected\", \"JOB/processing_1/me.processing_1.processing_1/ PRODUCTION_WORKLOAD\": \"machineE\", \"machineD\": \"machineD\", \"machineD,machineA\": \"machineA\", \"machineD,machineE\": \"machineD\", \"machineD, machineC\": \"none_selected\", \"machineA\": \"machineA\", \"machineA,machineE\": \"machineE\", \"machineF\": \"machineF\", \"machineE\": \" none_selected\"}ncell: cell_andistributions:nplatform: {machineA}n num_machines: 1.239e+03n low_level_zones: 5.200e+01n mid_level_zones: 5.200e+01n high_level_zones: 4.300e+01n resources: 5.481e+05nplatform: {machineE}n num_machines: 1.070e+02n low_level_zones: 3.900e+01n mid_level_zones: 3.900e+01n high_level_zones: 3.500e+01n resources: 2.430e+04nplatform: {machineD}n num_machines: 5.640e+02n low_level_zones: 3.000e+01n mid_level_zones: 3.000e+01n high_level_zones: 1.900e+01n resources: 2.769e +05nplatform: {machineC}n num_machines: 6.100e+01n low_level_zones: 6.000e+00n mid_level_zones: 6.000e+00n high_level_zones: 3.000e+00n resources: 2.306e+04nplatform: {machineF}n num_machines: 1.880e+02n low_level_zones: 1.200e+01n mid_level_zones: 1.200e+01n high_level_zones: 6.000e+00n resources: 4.527e+04nnjob_profiles:njob: {user: leaf, group_name: bun}n platform_profiles:n machineD: {mean_productivity_per_resource_usage: 1.048e+00, mean_mips_per_resource_usage: 8.588e+02}n machineF: {mean_productivity_per_resource_usage: 1.060e+00, mean_mips_per_resource_usage: 8.285e+02}n machineE: { mean_productivity_per_resource_usage: 1.100e+00, mean_mips_per_resource_usage: 8.292e+02}n machineC: { mean_productivity_per_resource_usage: 9.721e-01, mean_mips_per_resource_usage: 7.911e+02}n observed_resource_limit: 7.861e+02n observed_num_vms: 9.000e+00n limits: {machine_limit: 1.000e+00, switch_limit: 1.000e+00, low_level_limit: 1.000e+00, has_port_limit: true, job_requested_resource_limit: 8.832e+02, job_requested_num_vms: 10, group_requested_resource_limit: 8.832e+02}n productivity_per_resource_1: 1.048e+00n productivity_per_resource_2: 1.045e+00njob: {user: cent, group_name: rock}n platform_profiles:n machineD: {mean_productivity_per_resource_usage: 3.803e+01, mean_mips_per_resource_usage: 5.716e+02}n machineA: {mean_productivity_per_resource_usage: 7.685e+00, mean_mips_per_resource_usage: 4.902e+02}n machineE: { mean_productivity_per_resource_usage: 5.893e+01, mean_mips_per_resource_usage: 5.610e+02}n machineC: { mean_productivity_per_resource_usage: 3.274e+01, mean_mips_per_resource_usage: 7.173e+02}n observed_resource_limit: 6.400e+02n observed_num_vms: 2.000e+01n limits: {job_requested_resource_limit: 6.400e+02, job_requested_num_vms: 20, group_requested_resource_limit: 6.400e+02}n productivity_per_resource_1: 1.941e+01n productivity_per_resource_2: 3.435e+01njob: { user: pond, group_name: green}n platform_profiles:n machineD: {mean_productivity_per_resource_usage: 1.413e+06, mean_mips_per_resource_usage: 7.420e+02}n observed_resource_limit: 1.100e+02n observed_num_vms: 1.900e+01n limits: { job_requested_resource_limit: 1.362e+04, job_requested_num_vms: 1401, group_requested_resource_limit: 1.362e+04}n productivity_per_resource_1: 1.413e+06n productivity_per_resource_2: 1.413e+06n Figure 21 Example anonymized string representation of 𝑥, truncated to fit. B.2. Data Organization and Statistics Due to corporate privacy concerns, we are required to anonymize all cell names, but make sure the naming scheme is consistent across all figures. Each task is labeled as 𝐶𝑚𝑜𝑛𝑡ℎ , parameterized by cell (integer index) and month (name). The index of cell is sorted in reverse order according to its spread of 𝑦-values, i.e. variance of 𝑝( 𝑦) unconditioned on 𝑥, and we show the distribution of spreads in Figure 22. Thus 𝐶 𝐽𝑈 𝑁 2 within November are the highest-spread cells, with June cells having dataset sizes 54K-56K and November having 28K. 2 within June and 𝐶 𝑁𝑂𝑉 and 𝐶 𝑁𝑂𝑉 and 𝐶 𝐽𝑈 𝑁 𝑐𝑒𝑙𝑙 1 1 Figure 22 Spread of cells included in our study, separated by month. 18 Performance Prediction for Large Systems via Text-to-Text Regression C. Experimental Settings C.1. Default Settings We used the standard T5X EncoderDecoder which can found in the open-source codebase https: //github.com/google-research/t5x. Below are default settings (unless otherwise overridden). For training/pretraining: Optimizer: Adafactor (Shazeer and Stern, 2018) with base learning rate 0.1 and square root decay with 1000 warm-up steps, 0.5 decay factor, 2000 steps per decay, and 10000 steps per cycle. Batch size 128 with 8 microbatches. Vocabulary and Tokenizer: SentencePiece tokenizer (Kudo and Richardson, 2018) with T5Xs default vocabulary of 32000 subword tokens, in addition to the custom P10 tokens (Charton, 2022) for representing 𝑦-objectives, with 4-digit mantissas. Early stopping: We train for maximum of 100K steps, but early stop based on validation loss if overfitting is detected. Architecture: 2 encoder layers, 2 decoder layers, 16 heads, 64 head dimension, 512 embedding dimension, 2048 MLP dimension, leading to 58M parameters. Default sequence length 2048, with truncation occurring afterwards. For fine-tuning: Optimizer: Restore checkpoint optimizer state, but change the learning rate to 5 105. Batch size: 128. Note that if the finetuning data size is lower, we simply repeat up to this size. Early stopping: Perform up to 200 epochs of finetuning, but early stop based on validation loss if overfitting is detected. For inference: Sampling: 128 parallel decoding examples per single 𝑥, while removing accidental \"extreme\" outliers outside of the known range [500, 3000] of possible achievable MIPs per GCU values. Pointwise Aggregation: Take the numeric mean of samples (to minimize MSE) or numeric median (to minimize Spearman rank). C.2. Main Training Runs All of the figures in this paper stem from few main training setups, focused on evaluating on both in-distribution and out-of-distribution cases, or ablations. Specifically, these main settings: Limit Testing: We train larger model (267M params) with 4096 sequence length and batch size 256, over nearly all tasks except the highest spread cells across the two months {𝐶 𝐽𝑈 𝑁 , 𝐶 𝑁𝑂𝑉 }. 1 We then also evaluate on the highest spread cells possible across the two months, i.e. 𝐶 𝐽𝑈 𝑁 (in-distribution) and 𝐶 𝑁𝑂𝑉 2 . Adaptation Testing: We train five separate checkpoints over {1, 4, 8, 16, 32} different tasks by all checkpoints, and chosen randomly, but making sure there is uniquely seen task 𝐶 𝐽𝑈 𝑁 also evaluate on out-of-distribution task 𝐶 𝑁𝑂𝑉 10 unseen by all checkpoints. 1 Ablations: We simply pretrain on 7 high spread tasks from June. More specifically, the pretraining tasks for Limit Testing are the combination of 𝐶 𝐽𝑈 𝑁 𝐶 𝐽𝑈 𝑁 31 , 𝐶 𝑁𝑂𝑉 19 to 𝐶 𝑁𝑂𝑉 27 . For Adaptation Testing, the pretraining tasks were: 17 , and 𝐶 𝑁𝑂𝑉 10 , 𝐶 𝑁𝑂𝑉 to 𝐶 𝑁𝑂𝑉 2 2 to 𝐶 𝐽𝑈 𝑁 10 , 𝐶 𝐽𝑈 𝑁 22 to Performance Prediction for Large Systems via Text-to-Text Regression 2 2 2 1 Cell: 𝐶 𝐽𝑈 𝑁 4 Cells: 𝐶 𝐽𝑈 𝑁 8 Cells: 𝐶 𝐽𝑈 𝑁 16 Cells: 𝐶 𝐽𝑈 𝑁 33 , 𝐶 𝑁𝑂𝑉 𝐶 𝑁𝑂𝑉 32 Cells: 𝐶 𝐽𝑈 𝑁 𝐶 𝐽𝑈 𝑁 28 , 𝐶 𝐽𝑈 𝑁 32 , 𝐶 𝑁𝑂𝑉 𝐶 𝑁𝑂𝑉 1 2 29 , 𝐶 𝐽𝑈 𝑁 33 , 𝐶 𝑁𝑂𝑉 34 , 𝐶 𝐽𝑈 𝑁 9 , 𝐶 𝐽𝑈 𝑁 3 , 𝐶 𝐽𝑈 𝑁 6 , 𝐶 𝑁𝑂𝑉 , 𝐶 𝐽𝑈 𝑁 12 , 𝐶 𝐽𝑈 𝑁 25 , 𝐶 𝐽𝑈 𝑁 10 , 𝐶 𝐽𝑈 𝑁 25 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 9 30 31 , 𝐶 𝑁𝑂𝑉 27 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝑁𝑂𝑉 28 , 𝐶 𝐽𝑈 𝑁 28 , 𝐶 𝑁𝑂𝑉 29 , 𝐶 𝐽𝑈 𝑁 2 31 , 𝐶 𝑁𝑂𝑉 2 , 𝐶 𝑁𝑂𝑉 5 , 𝐶 𝑁𝑂𝑉 8 , 𝐶 𝑁𝑂𝑉 12 , 𝐶 𝑁𝑂𝑉 30 , 𝐶 𝑁𝑂𝑉 32 , , 𝐶 𝐽𝑈 𝑁 2 30 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 3 31 , 𝐶 𝑁𝑂𝑉 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 4 7 6 , 𝐶 𝑁𝑂𝑉 , 𝐶 𝑁𝑂𝑉 5 2 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 10 , 𝐶 𝐽𝑈 𝑁 8 9 13 , 𝐶 𝑁𝑂𝑉 12 , 𝐶 𝑁𝑂𝑉 11 , 𝐶 𝑁𝑂𝑉 , 𝐶 𝑁𝑂𝑉 23 , 𝐶 𝐽𝑈 𝑁 14 , 𝐶 𝑁𝑂𝑉 24 , 𝐶 𝐽𝑈 𝑁 16 , 𝐶 𝑁𝑂𝑉 25 , 𝐶 𝐽𝑈 𝑁 18 , 𝐶 𝑁𝑂𝑉 26 , 𝐶 𝐽𝑈 𝑁 27 , 30 , 𝐶 𝑁𝑂𝑉 31 , 1 For Ablations, the pretraining tasks were 𝐶 𝐽𝑈 𝑁 1 to 𝐶 𝐽𝑈 𝑁 8 . C.3. Individual Figures Distribution Of Input String Length, Figure 3: For this, we use the evaluation results from Limit Testing and simply make the histogram of the inputs used across all cells during test-time. Outlier Sensitivity In Rank Correlation, Figure 4: We used the 8-cell pretrained model from Adaptation Testing, and also use the same cells for evaluation. Note that all cells are technically out-of-distribution for randomly initialized checkpoint. We use the same fine-tuning procedure for all model tests. We repeat the fine-tuning procedure 10 times and report the mean for the Spearman rank correlation. We pick single seed for our Diagonal Fit scatter-plot. Density Capture by RLM, Figure 5: From Adaptation Testing, we use the 8-cell out-of-distribution result. We take the predictions as well as samples (128 samples per input), and make density plot of the metric (MIPS per GCU). MSE Histogram for In and Out-Of-Distribution Adaptation, Figure 6: We use the Limit Testing setting, with pretrained model checkpoint at 15K steps, which achieved the lowest validation loss within 5K step intervals. Recall from Section 2.3 that to obtain the theoretically optimal tabular and null residuals, we grouped all 𝑦s into the equivalence classes and took the mean 𝑦-value of each equivalence class as the theoretically optimal prediction. Pretraining Diversity and Few-Shot Adaptation, Figure 7: From Adaptation Testing, we plot all of the results. For fine-tuning experiments, we use pretraining checkpoint early stopped at 10K steps, motivated by our checkpoint-learning rate analysis in Figure 20, and observing validation loss. Prediction Uncertainty and Multi-modality, Figure 8 and Figure 9: We simply fine-tune the 8-cell pretrained base model from Adaptation Testing on in-distribution task 𝐶 𝐽𝑈 𝑁 for 512 samples, and pick seed. For Figure 8, we report the scatter plot of prediction standard deviation across its 128 generated samples per input on the axis as prediction uncertainty, and squared-error on the axis. For Figure 9, we identify set of time-stamps where the ground truth target demonstrates bi-modal distribution, and pick one of these time-stamps. At this exact time-stamp, we have one input and 128 generated regressor predictions. We plot the ground-truth values observed at that time-stamp, as well as the 128 generated values (Sampled Predictions). We then plot the histogram and density plot on the sampled predictions. Scatter and Density Plots, Figure 10 and Figure 11: We use the Limit Testing setting, with pretrained model checkpoint at 15K steps. We fine-tune the model for each task individually, over 512 examples, with learning rate of 1 104 for single seed. 20 Performance Prediction for Large Systems via Text-to-Text Regression Explained Variance and Negative Log-Likelihood, Figure 12: We use the Limit Testing setting, with pretrained model checkpoint 15K steps. To reliably estimate NLLnull, we fine-tune the randomly initialized variant of the Limit Testing model on empty strings with 1024 examples. NLLRLM is also fine-tuned on 1024 examples. Learning rate is 1 104, and we report results over 5 seeds. Validation Loss and MSE, Spearman 𝜌, Figure 13 and Figure 14: We train new base model, with 4 layers, 4096 sequence length, on high spread cells (𝐶 𝐽𝑈 𝑁 , 𝐶 𝐽𝑈 𝑁 10 ) and save checkpoints at 1K step intervals. We then evaluate every checkpoint on in-distribution cell 𝐶 𝐽𝑈 𝑁 2 over 100 seeds, without any fine-tuning. We report the mean MSE and Spearman-𝜌 over 100 seeds. , 𝐶 𝐽𝑈 𝑁 9 , 𝐶 𝐽𝑈 𝑁 3 , 𝐶 𝐽𝑈 𝑁 8 , 𝐶 𝐽𝑈 𝑁 5 , 𝐶 𝐽𝑈 𝑁 2 Encoder-Decoder Ablation, Figure 15: We train four models on the Ablation setting with the same default model dimensions, but with different encoder and decoder layers. (0E4D, 1E3D, 3E1D, 2E2D) had roughly the same number of parameters (62.3M, 60.2M, 58M, 56M) respectively, and minimum validation loss was observed at step (18K, 17K, 17K, 23K) respectively. Parameter Size Ablation, Figure 16: We train four models, of sizes (45.5M, 58.1M, 83.2M, 234.3M) (2 layers, 4 layers, 8 layers and 32 layers respectively), with default model dimensions on the Ablation setting, but with changed sequence length of 1024. We then measure the minimum validation loss, achieved at respective steps (27K, 16K, 13K, 11K). Sequence-Length Ablation, Figure 17: We train six models at sequence lengths of (4096, 2048, 1024, 768, 512, 256), with default model size and dimensions on the Ablation setting. We measure the minimum validation loss, achieved at respective steps (26K, 23K, 16K, 18K, 13K, 12K). Feature Ablation, Figure 18: We train three models all of default dimensions but with changed sequence length of 1024, and measure the minimum validation loss, achieved at step (22K, 20K, 23K) for (CR, R, WR) respectively on the Ablation setting. Checkpoint and Learning Rate Ablation, Figure 19 and Figure 20: We use the default model size but with sequence length of 4096. This model is trained on 𝐶 𝐽𝑈 𝑁 10 and then fine-tuned and evaluated on the highest-spread out-of-distribution task 𝐶 𝑁𝑂𝑉 For Figure 19, we pick the checkpoint at 10K steps and fine-tune the base model on 𝐶 𝑁𝑂𝑉 at learning rates of (5 103, 1 103, 5 104, 1 104, 5 105, 1 105, 5 106, 1 106) for (0, 4, 8, 16, 32, 64, 128, 256) examples each. We report the mean of MSE over 10 fine-tuning seeds. to 𝐶 𝐽𝑈 𝑁 1 2 1 . For the checkpoint ablation in Figure 20, we set fine-tune learning rate of 5 105 and take checkpoints at every-step, starting from 1K up to 100K. We fine-tune every checkpoint of the base model on 𝐶 𝑁𝑂𝑉 1 with validation-loss based early stopping, for (0, 4, 8, 16, 32, 64, 128, 256) fine-tuning examples. Due to the scale of the study, we only use one seed."
        }
    ],
    "affiliations": [
        "Cornell University",
        "Google",
        "North Carolina State University"
    ]
}