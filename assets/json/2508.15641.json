{
    "paper_title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding",
    "authors": [
        "Pengcheng Fang",
        "Yuxia Chen",
        "Rui Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 When and What: Diffusion-Grounded VideoLLM with Entity-Aware Segmentation for Long Video Understanding Pengcheng Fang, Yuxia Chen, Rui Guo 5 2 0 2 1 2 ] . [ 1 1 4 6 5 1 . 8 0 5 2 : r Fig. 1: Teaser of our videolanguage model. Given video and natural-language instruction, we extract key nouns (e.g., dog, frisbee) and introduce special timestamp tokens (e.g., <24>, <96>--<120>) to enable precise temporal grounding. The nouns are linked to segmentation-guided object embeddings (Obj-Seg row), while video frames provide visual evidence along the timeline. The model handles three query types: (1) event localization (When does the dog first touch the frisbee? returns time), (2) interval description (Describe the scene between <96> and <120>), and (3) object-centric tracking (Track the red frisbee; does the dog pick it up?). Blue bubbles show concise, time-aligned answers produced by the LLM conditioned on video frames, object masks, and the mixed token sequence. AbstractUnderstanding videos requires more than answering open-ended questionsit demands the ability to pinpoint when events occur and how entities interact across time. While recent Video-LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame-level features are weak in capturing continuity, and languagevision alignment often drifts from the entities of interest. In this paper, we present GroundedVideoDiT, Video-LLM designed to overcome these limitations by introducing three key innovations. First, Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object-grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine-grained temporal reasoning. Together, these designs equip Grounded-VideoDiT with robust grounding capabilities, as validated by state-of-the-art results on Charades-STA, NExTGQA, and multiple VideoQA benchmarks. I. INTRODUCTION With the exponential growth of video data, video understanding has become central focus in multimodal research. In tasks such as question answering and caption generation, there is growing demand for fine-grained modeling of when, where, and who is doing what within video. Unlike static long videos span extended durations and contain images, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 dynamic, time-evolving content. Effective understanding requires not only recognizing spatial semantics but also capturing temporal dependencies, tracking object trajectories, and reasoning about the timing and location of specific events. This involves addressing challenges such as redundant frames, frequent scene changes, multi-entity interactions, and longrange temporal dependencies. Designing unified videolanguage framework with strong temporal modeling, precise object perception, and robust tracking remains key research problem. Recent Video-LLMs have attempted to bridge the gap between visual perception and temporal reasoning by adapting static vision-language frameworks with video-specific enhancements. These methods typically rely on frame sampling and image encoders to extract appearance features, followed by token compression or projection modules that interface with frozen LLMs. In addition, some models enhance temporal awareness through positional encodings, discrete timestamp tokens, or external prompts, while others attach segmentation masks or object anchors post-generation for entity visualization. Although these strategies improve basic temporal sensitivity and visual grounding, they often fail to explicitly model temporal evolution. First, temporal modeling is usually delegated to positional cues, lacking learnable latent variables that evolve over time. Second, segmentation and tracking are typically applied after language modeling, preventing early attention alignment and weakening reasoning consistency in multi-object scenarios. Third, token-level reasoning over time remains limited, as temporal structures are neither disentangled nor explicitly incorporated into the language modeling pipeline. To address the aforementioned limitations in long-video understanding, we propose Diffusion-Grounded VideoLLM, unified and lightweight framework that integrates structured temporal and entity-aware information prior to language modeling. This framework redefines the role of diffusion models in video understandingnot as generative models, but as efficient temporal feature extractors that capture dynamic changes across frames via conditional denoising. In parallel, we incorporate semantic segmentationbased object representations and apply cross-frame tracking to explicitly model the evolution of entities over time. To support joint reasoning, we design mixed-token input layout that encodes temporal, spatial, and linguistic information into unified sequence, providing the language model with structured and time-sensitive context. The overall architecture maintains high scalability and inference efficiency, significantly enhancing the models ability to localize key events, track entities, and reason about temporal relationships in long videos. Our contributions are summarized as follows: (1) We introduce diffusion-based video encoder to capture inter-frame dynamics and temporal structures, generating differentiable temporal latent tokens that serve as informative inputs to the LLM. (2) Semantic segmentation is incorporated before language modeling, enabling object-level representation and crossframe consistency to support robust multi-entity tracking and grounding. (3) We design unified mixed-token input structure that encodes visual, textual, temporal, and object-level information into single sequence, enabling joint spatiotemporal reasoning in an end-to-end manner. (4) Our framework achieves state-of-the-art performance on long-video understanding benchmarks, including NExTQA and ActivityNet-Captions. II. RELATED WORK A. Video Large Language Models With the success of large language models (LLMs) on visual tasks, researchers have begun exploring their extension to video understanding [1][3]. Flamingo [4] first demonstrated that simply inserting visual tokens into the text stream, while keeping the LLM frozen, enables zeroor few-shot video and image question answering. However, Flamingo performs global averaging over fixed number of frames, lacking explicit temporal modeling and entity-level alignment. Subsequent works such as Video-LLaMA [5] and VideoChatGPT [6] project CLIP-ViT features or chunked frame representations into LLaMA, combined with instruction tuning, to enable video dialogue. This direct use of image features can lead to representations that lack inherent temporal structure, making temporal reasoning still dependent on position encodings in the language model. More recent approaches have begun to explicitly inject temporal information: TimeChat [7] adds timestamp-aware frame encoder and sliding Q-Former, significantly improving long-video localization; Grounded-VideoLLM [8] adopts spatial-temporal dualstream structure and introduces discrete time tokens to avoid the tokenization issues of continuous time values. Additionally, VideoGPT+ [9] and LongVLM [10] enhance inference efficiency through long-sequence compression or streaming visual encoders. Despite rapid progress, most existing VideoLLMs still rely on visual block + text block concatenation structure and lack an independent temporal latent space. In contrast to these approaches, our proposed Diffusion-Grounded VideoLLM leverages diffusion process to learn temporal latent representations from the frame sequence, and integrates Grounded-SAM2 and WAN to perform cross-frame segmentation and tracking of objects in video. Furthermore, we introduce mixed-token input strategy that interleaves visual, temporal, and entity-level tokens within unified sequence, enabling native alignment of whenwherewho at the token level. Together, these components significantly enhance the models fine-grained temporal and spatial understanding in long video scenarios. B. Diffusion Models for Video Understanding Diffusion Models (DMs) have demonstrated significant advantages in high-fidelity video generation tasks due to their unique denoising modeling capability. Works such as VideoDM [Ho et al., 2022] and Imagen-Video [Singer et al., 2023] have validated the superiority of the stepwise denoising mechanism in modeling continuous inter-frame changes, promoting its exploration in the field of video understanding. DiffSumm [11] achieves unsupervised video summarization JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 by adding noise to feature layers and performing denoising to obtain frame-level importance distributions; KDA introduces residual diffusion module in temporal sentence localization to improve the accuracy of segment-level semantic alignment. However, due to the high computational cost of generative diffusion models, the aforementioned works are mostly applied in generation or matching stages, making it difficult to explicitly collaborate with language models to accomplish reasoning tasks. Additionally, the temporal priors acquired through the diffusion process are hard to align with the structured input form of tokens, limiting their application in multi-turn question answering and complex reasoning tasks. To address these issues, this paper proposes Diffusion-Grounded VideoLLM, which redefines the role of diffusion models in video understanding: we use them as video feature extractors with temporal latent variables, replacing traditional image-video encoders. Instead, we directly apply temporal noise to framelevel sequences and generate temporal latent tokens through conditional denoising process as part of the input to the LLM. This design aims to fully leverage the advantages of diffusion models in modeling temporal evolution, enabling them to provide time-sensitive contextual representations for language models in more efficient and structurally explicit manner, thereby enhancing the understanding and reasoning capabilities for long video tasks. C. Grounded Segmentation and Tracking In recent years, researchers have explored incorporating segmentation-level visual features into video large language models to alleviate the ambiguity in entity semantics caused by the simple concatenation of frame-level global features and text. SA2VA [12] utilizes mask tokens extracted by SAM as auxiliary visual representations of LLM-generated answers, achieving performance gains in video question answering tasks. VideoGLaMM [13] uses sparse masks from SAM as object anchors. It then applies optical flow to propagate these masks across frames, enabling instance-level grounding in video dialogue. While these methods demonstrate the value of segmentation features in enhancing semantic clarity, they place the segmentation module after LLM inference, serving primarily as post hoc refinement. As result, segmentation does not directly participate in multimodal joint modeling and cannot guide attention during language inference to form consistent and stable entity-level semantics. To overcome this limitation, this paper takes segmentation and tracking module in front of language modeling stage for the first time, constructing unified entity tracking pipeline conditioned on the referent mentioned in the input query. Given language instruction, we first generate object masks on key frames and propagate them across time using dense matching flow, thereby obtaining temporally aligned and semantically consistent object track embeddings. These embeddings are then encoded as explicit tokens and jointly fed into the LLM alongside video, text, and temporal encodings, forming unified mixed-token sequence. This design enables the alignment of the object-time-semantics to occur before language inference, leading to more accurate and coherent spatiotemporal reasoning. III. METHOD Our task is temporal grounding and its variants, where the objective is to identify the temporal segment in video that corresponds to given natural-language prompt. The proposed framework consists of four main components: object-centric prompt alignment and tracking, diffusion-based video encoding, feature regularization via KL divergence, and explicit temporal position encoding. The overall architecture is illustrated in Fig. 2. A. Object-Centric Prompt Alignment and Tracking Given video = {xt}T t=1 and natural-language prompt q, our goal is to obtain temporally consistent binary masks that explicitly bind the entities mentioned in to visual evidence in V. These masks are fed directly into our video inpainting diffusion encoder (Sec. III-B) as hard spatial constraints. Unlike previous temporal grounding pipelines, which rely on large language model (LLM) to implicitly align free-form text and video features, our method explicitly grounds and tracks the target entities before any video encoding, ensuring controllability, interpretability, and robustness. a) Step 1: Noun-centric prompt parsing.: Long and compositional prompts may contain verbs, modifiers, and clauses that introduce ambiguity for open-vocabulary grounding models. Since our targets are visual entities, we extract only the noun phrases from using syntactic parser E: = {ni}M i=1 = E(q), (1) where ni denotes the i-th target noun (e.g., person, red umbrella). b) Step 2: Frame-wise open-vocabulary grounding.: For each frame xt, Grounding-SAM2 produces set of region proposals Ot = {ot,k}Kt k=1, each associated with an openvocabulary matching score to query string. For each ni , we select the highest-scoring proposal: ˆki,t = arg max Score(ni, ot,k), ). (2) detection for ni is considered valid if ˆsi,t τi, where τi is confidence threshold. ˆsi,t = Score(ni, ot,ˆki,t c) Step 3: AND-gated co-occurrence for high-precision starts.: To ensure that all referenced entities are present before tracking begins, we define an AND gate: gt = (cid:89) i= [ˆsi,t τi], (3) which is 1 if and only if all nouns are detected in frame t. We further require persistence over consecutive frames: Γ(K) = K1 (cid:89) u= gt+u. (4) The tracking start time is the earliest frame ts where Γ(K) = 1. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 2: Model overview. user instruction with query and an accompanying video are provided as inputs. Key nouns extracted from the query are passed to frozen Grounded-SAM2 module to obtain object-level segmentation masks and embeddings (left). In parallel, frozen video diffusion encoder processes the video together with the masks and produces multi-scale features; we retain the intermediate representation at 10% of the diffusion trajectory as context tokens (right). The query is further encoded by text encoder. We then assemble text tokens, segmentation tokens, and video tokensincluding entity and timestamp tagsusing our Mixed Tokens Strategy, and feed the merged sequence into large language model (bottom). During training, Grounded-SAM2, the video diffusion backbone, and the LLM are kept frozen, while the LLM is adapted only via LoRA. d) Step 4: Raw mask tracking.: At ts, we seed one binary mask mi,ts for each noun ni from the corresponding proposal ots,ˆki,ts . We then propagate these masks forward to all subsequent frames using Grounding-SAM2s tracking module: Ti = {mi,t}T t=ts , mi,t {0, 1}HW . (5) For use in the diffusion inpainting model, we compute the per-frame union mask: Mt = (cid:95) i=1 mi,t, (6) where denotes pixel-wise OR. This preserves the original binary structure without any weighting, normalization, or smoothing, ensuring compatibility with inpainting conditioning. e) Step 5: Optional temporal span extraction.: If the downstream grounding variant requires an explicit temporal interval, we select the longest contiguous span [ˆts, ˆte] satisfying gt = 1 for all [ˆts, ˆte] and ˆte ˆts + 1 L, where is minimum span length. f) Advantages.: Our object-centric grounding module offers strong controllability through the AND-gated persistence rule, which enforces compositional presence constraints and prevents premature or partial triggers. It is inherently interpretable, as every decision is backed by binary mask evidence that can be visually inspected, and fully compatible with the diffusion inpainting encoder since the raw masks {Mt} are preserved without lossy post-processing. B. Object-Conditioned Diffusion Video Encoder Transformer-based multimodal encoders excel at coarse cross-modal alignment but often under-represent fine temporal transitions because they operate on static token sequences. Diffusion models, in contrast, expose an explicit denoising trajectory: intermediate states retain progressively refined spatial patterns and short-horizon motion cues, which we repurpose as video features for temporal grounding. a) Notation and setup.: Let = {xt}T t=1 be the input frames and = {Mt}T t=1 the binary object masks (Sec. III-A). short textual instruction yhl foregrounds the target (e.g., highlight the masked region). We pack the conditioning into = (M, yhl). Let τ [0, 1] denote the normalized diffusion time (smaller τ indicates earlier denoising). We JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 denote by Eθ masked video diffusion encoder (instantiated in practice with an inpainting-capable latent diffusion backbone) and by gϕ lightweight projection head; Pool() aggregates spacetime positions. b) Early-step feature extraction.: Following the standard forward noising process in latent space, perturbed state xτ is drawn as q(xτ x0) = N(cid:0)ατ x0, σ2 where x0 is the clean video latent and (ατ , στ ) are schedule coefficients. We then query the denoiser at an early diffusion step τ0 to obtain spatiotemporal features conditioned on c: τ I(cid:1), (7) hτ0 = Eθ (cid:0)X, c, τ0 (cid:1). (8) Choosing small τ0 preserves high-frequency appearance and short-range motion before the trajectory is dominated by the generative prior. c) Compact video representation.: Because hτ0 is highdimensional and generation-oriented, we obtain compact embedding via spatiotemporal pooling followed by learned projection: where = 0, . . . , df 2 1. (Learned MLP): etime(t) = MLPtime (cid:0)[τt, τ 2 ](cid:1) . (12) We adopt the sinusoidal encoding (Eq. 11) by default and evaluate the learned variant (Eq. 12) in ablations. c) Multimodal fusion.: Each frame token is fused with its corresponding temporal and textual embeddings via concatenation, followed by layer normalization and projection to match the input size of the language model: ut = LN ([zt; etext; etime(t)]) Rdv+dt+df , ut = Wproj ut + bproj RdLLM . = {ut}T (13) where Wproj RdLLM(dv+dt+df ). The resulting sequence t=1 is input to frozen pretrained language model. d) Efficient adaptation via LoRA.: To adapt the backbone LLMθ with minimal overhead, we employ Low-Rank Adaptation (LoRA). For any frozen weight matrix W0 Rdoutdin (e.g., in Q/K/V/O projections or feedforward layers), LoRA introduces trainable low-rank update: = gϕ (cid:0)Pool(hτ0)(cid:1), Rd. (9) = W0 + α AB, Rdoutr, Rrdin , (14) The embedding is consumed by the downstream temporal reasoning module. Mask conditioning in enforces objectcentric focus, while early-step diffusion states encode finegrained temporal dynamics, yielding an efficient, object-aware representation for temporal grounding. d) Instantiation.: In our implementation, Eθ is realized with unified video creation/editing backbone that supports spatial masks and text conditioning (e.g., VACE-compatible inpainting model [14], [15]); the formulation above remains model-agnostic, and we only learn gϕ together with the subsequent grounding head. C. Explicit Temporal Position Encoding and Multimodal Fusion Given framewise visual embeddings = {zt}T t=1, where zt Rdv is obtained from Sec. III-B, our goal is to incorporate temporal position and textual context before feeding into the temporal reasoning module. a) Text prompt encoding.: grounding query is encoded using frozen text encoder Enctext, with sentence-level semantics extracted via CLS pooling: etext = PoolCLS(Enctext(q)) Rdt. (10) b) Temporal position encoding.: We encode absolute time using normalized timestamps τt = t1 1 [0, 1], and map each τt into continuous embedding etime(t) Rdf . We consider two variants: (Sinusoidal): where only and are optimized during training. e) Output heads.: The language model produces contextualized hidden states = {ht}T t=1. Temporal grounding is formulated as start/end localization using two token-level classifiers: (cid:0)w ps(t) = Softmaxt (cid:1) , (15) with final prediction computed by maximizing joint probability: pe(t) = Softmaxt (cid:0)w ht ht (cid:1) , (ˆs, ˆe) = arg max 1seT ps(s) pe(e). (16) This formulation enables fine-grained temporal reasoning through tight integration of visual, textual, and positional cues. Meanwhile, LoRA (Eq. 14) ensures efficient adaptation with minimal trainable parameters. D. Feature Regularization via KL Divergence While the diffusion-based video encoder introduced in Sec. III-B effectively captures temporal dependencies, its features may lack fine-grained discriminative signals crucial for precise temporal grounding. To address this, we introduce feature regularization strategy that aligns diffusion features with stronger auxiliary representation using KL divergence loss. a) Auxiliary reference features.: We leverage pretrained video feature extractor Eaux (e.g., vision-language or action recognition model) as stable reference. Given video frame xt and the corresponding object mask Mt (from Sec. III-A), we extract two representations: [etime(t)]2k = sin [etime(t)]2k+1 = cos (cid:19) (cid:19) , , (cid:18) τt ωk (cid:18) τt ωk ωk = 2k df , (t) diff = Ediff (xt, Mt), (t) aux = Eaux(xt), (17) (11) where Ediff is our diffusion-based encoder. Notably, Eaux operates on the full frame without mask conditioning, offering complementary, object-agnostic semantics. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 TABLE I: Comparison of Temporal Video Grounding performance on Charades-STA and DiDeMo. We report R@IoU thresholds (0.3, 0.5, 0.7) and mIoU. The best results are in bold, the second best are underlined. The last row (ours) is highlighted in gray. Model LLM Scale Charades-STA DiDeMo R@0.3 R@0.5 R@0.7 mIoU R@0. R@0.5 R@0.7 mIoU Video-LLaMA SeViLA Video-ChatGPT Valley VideoChat2 VideoChat Momenter VTimeLLM GroundingGPT TimeChat VTG-LLM HawkEye Grounded-VideoLLM LLaVA-ST Ours 7B 3B 7B 7B 7B 7B 7B 7B 7B 7B 13B 7B 4B 7B 7B 25.2 27.0 27.2 38.0 38.0 38.2 42.6 51.0 - 47.7 52.0 50.6 54.2 63.1 58. 10.6 10.5 6.2 14.8 14.3 15.0 26.6 27.5 29.6 22.9 33.8 31.4 36.4 44.8 41.2 3.4 5.8 1.9 4.0 3.8 4.1 11.6 11.4 11.9 12.5 15.7 14.5 19.7 23.4 21.0 16.8 18.3 19.7 24.6 24.6 24.8 28.5 31.2 - 31.6 34.7 33.7 36.8 42.4 39.5 20.1 23.5 19.8 33.2 34.0 34.5 38.2 45.0 - 42.8 46.5 44.8 48.6 56.2 53.0 8.2 9.8 6.5 13.4 14.1 14.5 21.8 28.8 26.5 24.4 31.2 29.7 33.4 39.8 37.0 2.5 3.6 1.2 4.5 4.7 4.9 9.4 12.0 10.7 11.3 13.8 13.4 15.9 20.1 18. 14.3 15.9 13.7 21.8 22.0 22.4 26.5 27.9 - 28.2 30.1 29.5 32.0 37.6 35.2 TABLE II: Results on Open-Ended VideoQA. We report Accuracy (Acc.) and Score across four datasets, with NExT-QA added in the leftmost column. The best results are in bold. NExT-QA results (except ours) are placeholder values pending actual evaluation. Model NExT-QA MSVD-QA Acc. Score Acc. Score MSRVTT-QA Acc. Score ANet-QA Acc. Score Video-LLMs w/o temporal grounding capability. Video-LLaMA Video-ChatGPT Vista-LLaMA MovieChat LongVLM VideoChat2 Chat-UniVi P-LLaVA-7B ST-LLM VideoGPT+ 28.5 34.2 48.1 45.6 46.0 42.5 41.8 50.2 52.1 1.9 2.5 3.1 3.0 3.0 2.8 2.8 3.4 3.5 Video-LLMs w/ temporal grounding capability. Momcnet VTimeLLM LITA Grounded-VideoLLM Ours 46.3 53.2 56.9 3.0 3.4 3.6 51.6 64.9 75.2 70.0 70.0 70.5 70.0 74.6 74.6 68.9 76.3 78.0 2.5 3.3 3.8 3.8 3.8 3.6 3.8 4.0 3.9 3.6 4.1 4.3 29.6 35.0 59.8 59.4 59.8 54.6 54.6 62.0 63.2 55.6 60.3 62.1 1.8 2.8 3.3 3.3 3.3 3.1 3.1 3.4 3.4 3.0 3.6 3.7 12.4 35.2 38.5 34.2 34.6 35.1 34.8 50.0 50.9 40.8 56.8 58.4 1.1 2.7 3.3 3.4 3.3 3.3 3.3 3.3 3.4 3.2 3.5 3.6 b) KL-based alignment.: To encourage consistency, both features are converted into probability distributions via softmax: diff = softmax(F (t) p(t) diff ), aux = softmax(F (t) p(t) aux). (18) The per-frame alignment loss is defined as: LKL(t) = KL (cid:16) pdiff (t), , p(t) aux (cid:17) , and the overall KL loss is averaged over all frames: LKL ="
        },
        {
            "title": "1\nT",
            "content": "(cid:88) = 1T L(t) KL. (19) (20) c) Impact.: This regularization guides the diffusion encoder to capture semantically richer cues while maintaining temporal fidelity. Empirically, it improves discriminability for object-centric reasoning, stabilizes training, and accelerates convergence by aligning with more discriminative auxiliary signal. IV. EXPERIMENTS Focusing on fine-grained long-video understanding, we evaluate Diffusion-Grounded VideoLLM on three representative tasks: Temporal Video Grounding, Grounded VideoQA, and Open-Ended VideoQA. Specifically, for Temporal Sentence Grounding we use Charades-STA and report mIoU and Recall@1 at IoU thresholds 0.3, 0.5, 0.7, following [1], [7]. For Grounded VideoQA, we use NExT-GQA reporting Acc@GQA and evidence-consistency metrics IoP@0.3, 0.5 and mIoP [16]. For Open-Ended VideoQA we use NExTQA and ActivityNet-QA, reporting overall Accuracy with category-wise breakdowns (temporal, causal, counting, entityrelation), following prior practice. Unless otherwise noted, we adopt official splits and evaluation scripts and report mean std over 3 seeds under zero-shot (ZS) and fine-tuning (FT) implementation and metric details are settings. Additional deferred to the Appendix. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 Fig. 3: Qualitative comparisons on three tasks. We show results on Temporal Grounding, Grounded VideoQA, and Open-ended VideoQA. Given Video with Segmentation and timestamp tokens, our model (OURS) produces more precise localizations (e.g., 32.0s--58.0s) and more specific, object-aware answers (e.g., small red airplane) than prior methods (LLaVA-ST, VTimeLLM, TimeChat, VideoLLaMA). Checkmarks indicate correct predictions; crosses indicate errors. A. Implementation Details We build Diffusion-Grounded VideoLLM on top of Phi3.5-Vision-Instruct-3.8B as the language backbone [17]. To encode temporal dynamics, we introduce the Diffusion Temporal Latent (DTL) module, which follows the WAN design by injecting Gaussian noise into frame-wise features and performing conditional denoising to derive differentiable [15]. For entity-level grounding, we temporal latent tokens integrate Grounded-SAM2 with DINO-based tracking [18], where each query-guided segmentation yields temporally consistent object track embedding. These embeddings are fused as explicit object tokens into the multimodal sequence, ensuring cross-frame consistency and reducing ambiguity in multi-entity reasoning. The model is trained with AdamW using cosine learning rate schedule. We set the base learning rate to 1e-4 with 5% warm-up ratio, and train for 3 epochs with global batch size of 128. Each video is uniformly sampled into = 96 frames, divided into = 12 segments for temporal noise injection. For efficient fine-tuning, we apply LoRA to the LLM with rank = 64 and α = 128. All experiments are conducted on 8 H800 GPUs. B. Main Results 1) Temporal Video Grounding: Temporal Video Grounding requires the model to identify the precise time interval corresponding to given query sentence, which demands both finegrained temporal reasoning and robust handling of ambiguous events. As shown in Table I, our Diffusion-Grounded VideoLLM achieves 39.5 mIoU on Charades-STA and 35.2 mIoU on DiDeMo, consistently improving over prior Video-LLMs. Notably, the gains are especially pronounced at higher IoU thresholds (e.g., 58.7 R@0.3 on Charades-STA), highlighting the models strength in localizing moments with finer granularity. These improvements largely stem from the diffusion-based temporal latent module, which captures structured temporal dynamics, together with segmentation-guided entity grounding that reduces ambiguity across multiple events. 2) Grounded VideoQA: Grounded VideoQA poses dual challenge: the model must not only produce correct answers but also ground them with temporally aligned evidence, making it stringent test of temporal reasoning and visual grounding ability. TABLE III: Results on NExT-GQA. Acc@GQA is the percentage of questions that are both correctly answered and visually grounded with IoP 0.5. The best results are in bold. Model Acc@GQA mIoP IoP@0.5 mIoU IoU@0.5 VIOLETv2 Temp[CLIP] NG+ SeViLA HawkEye LangRepo FrozenBiLM NG+ VideoStreaming LLoVi Grounded-VideoLLM Ours 12.8 16.0 16.6 17.1 17.5 17.8 24.3 26.7 28.4 23.6 25.7 29.5 31.3 24.2 32.2 37.3 34.5 36. 23.3 25.5 22.9 28.7 23.7 31.0 36.9 34.4 35.9 3.1 12.1 21.7 25.7 18.5 9.6 19.3 20.0 21.1 23.2 1.3 8.9 13.8 19.5 12.2 6.1 13.3 15.3 18.0 19.9 Table III shows results on NExT-GQA. Our DiffusionGrounded VideoLLM achieves the best overall Acc@GQA score of 28.4, surpassing Grounded-VideoLLM (26.7) and prior approaches such as FrozenBiLM. It also delivers strong grounding performance, with the highest IoP@0.3 (45.1) and improved IoU metrics. Compared to models specialized for grounding, such as SeViLA or LLoVi, our model achieves better balance between answer accuracy and grounding quality. These results validate that combining diffusion-based temporal latents with segmentation-guided object embeddings enhances both answer correctness and evidence alignment, pushing the frontier on grounded video reasoning. 3) Open-Ended VideoQA: Open-Ended VideoQA involves reasoning over unconstrained natural language questions grounded in complex video contexts, where the main difficulty lies in combining factual correctness with fine-grained temporal and causal understanding across long sequences. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 8 As shown in Table II, our model achieves the best overall performance across four benchmarks. On NExT-QA, it reaches 56.9 Acc. and Score of 3.6, marking clear improvement over previous Video-LLMs. On short-video datasets such as MSVD-QA and MSRVTT-QA, the model attains 78.0 and 62.1 Acc., respectively, with consistently higher Scores, reflecting strong generalization across diverse domains. On the more it further delivers 58.4 Acc., challenging ActivityNet-QA, outperforming the best existing grounded baseline by +1.6. These gains confirm that diffusion-based temporal latents combined with segmentation-guided entity tokens substantially enhance open-ended reasoning, while maintaining robustness across both shortand long-form VideoQA scenarios. C. Ablation Study 1) Cumulative Component Analysis: We follow the implementation of Grounded-VideoLLM, which employs InternVideo2 as the video encoder and Phi-3.5-Vision-Instruct-3.8B as the backbone LLM, serving as our base configuration. Building on this setup, we progressively integrate our proposed components. TABLE IV: Cumulative ablation analysis on CharadesSTA. We progressively add proposed components, and indicates inclusion of each module. DTL Obj Time R@0.3 R@0.5 R@0.7 mIoU 54.2 55.6 56.0 55.0 55.2 58. 36.4 37.8 38.3 37.2 37.5 41.2 19.7 21.0 21.2 20.9 21.0 21.0 36.8 38.2 38.8 37.5 37.8 39.5 As shown in Table IV, introducing the Diffusion Temporal Latent (DTL) significantly improves fine-grained temporal localization, as reflected by higher R@0.7 and mIoU on Charades-STA. This confirms that denoised temporal latents are effective for capturing precise event boundaries. Adding segmentation-guided object embeddings further boosts evidence alignment, with notable gains in mIoP on NExT-GQA, highlighting their strength in handling multi-entity reasoning. Finally, incorporating the mixed token structure with discrete time tokens yields the largest additional improvements, enhancing both answer accuracy and grounding quality across temporal alignment. Removing tasks by providing explicit either object or time tokens from the full model leads to consistent degradation, verifying that they play complementary and indispensable roles. 2) Token Budget Analysis: We study how the allocation of object and time tokens affects both accuracy and efficiency. As shown in Table V, introducing moderate number of object and time tokens leads to consistent improvements in evidence alignment and temporal reasoning. In particular, setting 4 object tokens and 8 time tokens provides the best balance, yielding higher Acc@GQA and mIoP while keeping throughput and latency within practical bounds. However, further increasing the number of tokens results in diminishing returns and even slight performance drops, as excessive tokens dilute visual context and slow down inference. TABLE V: Budget trade-offs of object and time tokens on Charades-STA. moderate allocation (4 Obj / 8 Time) achieves the best balance between performance and efficiency. Obj / Time R@0.3 R@0.5 R@0.7 mIoU Throughput Latency 2 / 4 4 / 8 8 / 16 16 / 32 52.3 53.8 53.2 52. 35.5 36.9 36.2 35.8 20.5 21.0 20.9 20.7 33.6 34.5 34.0 33.7 1.00 0.92 0.78 0.61 1.0 1.1 1.3 1.6 3) Token Budget Analysis: We further ablate the diffusion hyperparameters, including the number of denoising steps S, scheduling strategy, and guidance scale (GS). As shown in Table VI, moderate step size of S=4 combined with cosine scheduling and GS= 1.0 yields the most stable trade-off. Larger slightly improves accuracy but significantly increases inference cost, while higher GS values destabilize alignment. We therefore adopt S=4, cosine schedule, and GS= 1.0 as the default configuration in all experiments. TABLE VI: Ablation of diffusion hyperparameters on Charades-STA. S=4, cosine scheduling, and GS= 1.0 offer the best balance of performance and efficiency. 2 4 6 4 Schedule linear cosine cosine cosine GS 1.0 1.0 1.0 1.5 R@0.5 mIoU Throughput 34.1 36.9 37.0 36.2 32.8 34.5 34.7 33. 1.00 0.82 0.65 0.82 D. Qualitative Results In Figure 3, we present qualitative comparisons on NExTGQA against Timechat and Grounded-VideoLLM. The first case involves multiple entities with subtle temporal overlap. SeViLA predicts the correct answer but grounds to an incomplete segment, while Grounded-VideoLLM aligns better temporally but still confuses the entity reference. In contrast, our model accurately identifies both the queried object and its precise time span. The second case features frequent scene changes and requires long-range reasoning. Both baselines either provide fragmented grounding or over-extend the evidence window, whereas our approach pinpoints the relevant frames and maintains consistency across entities. These qualitative examples highlight the advantage of diffusion-based temporal latents and segmentation-guided object embeddings in resolving ambiguities that remain challenging for prior models. Additional visualization cases are provided in the Appendix. V. CONCLUSION In this work, we introduced Grounded-VideoDiT, VideoLLM specifically designed for fine-grained temporal grounding and entity-level alignment. Unlike prior approaches that rely on coarse video encoding and implicit timestamp representation, our framework advances video understanding through three key innovations: (i) Diffusion Temporal Latent (DTL) encoder that sharpens boundary sensitivity and temporal consistency, (ii) object-grounded representations that JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 9 [12] H. Yuan, X. Li, T. Zhang, Z. Huang, S. Xu, S. Ji, Y. Tong, L. Qi, J. Feng, and M.-H. Yang, Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos, arXiv preprint arXiv:2501.04001, 2025. II-C [13] S. Munasinghe, H. Gani, W. Zhu, J. Cao, E. Xing, F. S. Khan, and S. Khan, Videoglamm: large multimodal model for pixel-level visual grounding in videos, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 19 03619 046. II-C [14] Z. Jiang, Z. Han, C. Mao, J. Zhang, Y. Pan, and Y. Liu, Vace: All-inone video creation and editing, arXiv preprint arXiv:2503.07598, 2025. III-B0d [15] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang et al., Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. III-B0d, IV-A [16] J. Xiao, A. Yao, Y. Li, and T.-S. Chua, Can trust your answer? visually grounded video question answering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 20413 214. IV [17] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi4 technical report, arXiv preprint arXiv:2412.08905, 2024. IV-A [18] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan et al., Grounded sam: Assembling open-world models for diverse visual tasks, arXiv preprint arXiv:2401.14159, 2024. IV-A explicitly bind language queries to localized evidence, and (iii) mixed token scheme with discrete temporal tokens for explicit timestamp modeling. Together, these contributions enable precise localization, robust alignment, and richer temporal reasoning. Extensive experiments across Charades-STA, NExT-GQA, and multiple VideoQA benchmarks validate the effectiveness of our design. Grounded-VideoDiT consistently achieves state-of-the-art or highly competitive performance, particularly at stricter thresholds where temporal precision and entity grounding are most challenging. Qualitative analyses further demonstrate the robustness of our model in scenarios with frequent scene changes, short-duration events, and multi-entity reasoning, underscoring its generality and reliability. Looking ahead, our framework offers foundation for broader advances in video-language research. Future directions include extending grounded reasoning to long-form videos with richer narratives, integrating audio and multimodal cues for holistic event understanding, and developing interactive video assistants that can ground, reason, and act in real-world scenarios. We believe Grounded-VideoDiT provides not only strong step toward fine-grained temporal grounding, but also versatile blueprint for the next generation of Video-LLMs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. Huang, X. Wang, H. Chen, Z. Song, and W. Zhu, Vtimellm: Empower llm to grasp video moments, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 27114 280. II-A, IV [2] Z. Wang, S. Yu, E. Stengel-Eskin, J. Yoon, F. Cheng, G. Bertasius, and M. Bansal, Videotree: Adaptive tree-based video representation for llm reasoning on long videos, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 32723283. II-A [3] Y. Guo, J. Liu, M. Li, D. Cheng, X. Tang, D. Sui, Q. Liu, X. Chen, and K. Zhao, Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 3, 2025, pp. 3302 3310. II-A [4] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, Advances in neural information processing systems, vol. 35, pp. 23 71623 736, 2022. II-A [5] H. Zhang, X. Li, and L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, arXiv preprint arXiv:2306.02858, 2023. II-A [6] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, Video-chatgpt: Towards detailed video understanding via large vision and language models, arXiv preprint arXiv:2306.05424, 2023. II-A [7] S. Ren, L. Yao, S. Li, X. Sun, and L. Hou, Timechat: time-sensitive multimodal large language model for long video understanding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 31314 323. II-A, IV [8] H. Wang, Z. Xu, Y. Cheng, S. Diao, Y. Zhou, Y. Cao, Q. Wang, W. Ge, and L. Huang, Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models, arXiv preprint arXiv:2410.03290, 2024. II-A [9] M. Maaz, H. Rasheed, S. Khan, and F. Khan, Videogpt+: Integrating image and video encoders for enhanced video understanding, arXiv preprint arXiv:2406.09418, 2024. II-A [10] Y. Weng, M. Han, H. He, X. Chang, and B. Zhuang, Longvlm: Efficient long video understanding via large language models, in European Conference on Computer Vision. Springer, 2024, pp. 453470. II-A [11] Z. Shang, Y. Zhu, H. Li, S. Yang, and X. Wu, Video summarization using denoising diffusion probabilistic model, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 7, 2025, pp. 67766784. II-B"
        }
    ],
    "affiliations": [
        "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
        "Tencent AI Lab, Shenzhen, China",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}