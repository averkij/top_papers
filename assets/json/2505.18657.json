{
    "paper_title": "MLLMs are Deeply Affected by Modality Bias",
    "authors": [
        "Xu Zheng",
        "Chenfei Liao",
        "Yuqian Fu",
        "Kaiyu Lei",
        "Yuanhuiyi Lyu",
        "Lutao Jiang",
        "Bin Ren",
        "Jialei Chen",
        "Jiawen Wang",
        "Chengxin Li",
        "Linfeng Zhang",
        "Danda Pani Paudel",
        "Xuanjing Huang",
        "Yu-Gang Jiang",
        "Nicu Sebe",
        "Dacheng Tao",
        "Luc Van Gool",
        "Xuming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 5 6 8 1 . 5 0 5 2 : r a"
        },
        {
            "title": "MLLMs are Deeply Affected by Modality Bias",
            "content": "Xu Zheng1,3, Chenfei Liao1, Yuqian Fu3 Kaiyu Lei4 Yuanhuiyi Lyu1 Jiawen Wang8 Chengxin Li9,10 Lutao Jiang1 Bin Ren5,6,3 Linfeng Zhang11 Danda Pani Paudel3 Xuanjing Huang12 Yu-Gang Jiang12 Nicu Sebe6 Dacheng Tao13 Luc Van Gool3 Xuming Hu1,2, Jialei Chen7 1HKUST(GZ) 2CSE, HKUST 3INSAIT, Sofia University St. Kliment Ohridski 4Xian Jiaotong University 5University of Pisa, IT 6University of Trento, IT 7Nagoya University 8China University of Mining & Technology, Beijing 9Tongji University 10SPIC Energy Science and Technology Research Institute 11Shanghai Jiao Tong University 12Fudan University 13College of Computing & Data Science, Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: ① Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. ② Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. ③ Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systemsadvancing progress toward Artificial General Intelligence."
        },
        {
            "title": "1.1 Background",
            "content": "Multimodal Large Language Models (MLLMs) [1; 2; 3; 4; 5; 6] have revolutionized the ability to handle diverse modalities, including text, image, audio, video, and other emerging modalities (tactile [7; 8; 9], event [10; 11; 12], panoramic image [13; 14; 15], etc). This expansion into the multimodal domain typically involves pretraining with multimodal data pairs or fine-tuning on specialized multimodal instruction datasets [16; 17; 18; 19; 20]. MLLMs excel at understanding complex These authors have equal contributions. Corresponding author. Preprint. Under review. Figure 1: The big picture of our position on modality bias in MLLMs. (a) We define modality bias in multimodal models, with focus on MLLMs. (b) We outline research roadmap on modality bias, highlighting three key directions. (c) We summarize five contributing factors to modality bias: three primary and two secondary. multimodal patterns and translating them into coherent language representation space [21; 22; 23]. Despite significant advancements, challenges remain, where one major issue is \"Modality Bias\". As in [24], MLLMs often generate content that is disproportionately influenced by the underlying language model used during pretraining, rather than the input images or other modalities. In cases where images are noisy or even absent, MLLMs still confidently generate answers, highlighting clear bias towards learned language patterns over multimodal integration [25; 26]. An ideal MLLM should be modality-balanced, effectively integrating useful information from all modalities to provide reliable, accurate, and comprehensive answers [27]. Achieving this balance is crucial for overcoming modality bias and ensuring that the model can leverage the full potential of each modality in multimodal tasks [28; 29]."
        },
        {
            "title": "1.2 Modality Bias Phenomenon",
            "content": "Multimodal learning improves neural networks cross-modal comprehension by fusing heterogeneous data modalities (e.g., images, text, audio, and video), thereby enabling better world modeling [30; 31; 32; 33; 34; 35]. It is widely assumed that leveraging multiple input modalities will lead to improved model performance [36]. However, research has demonstrated that these modalities are not always utilized to their full potential [37; 38; 39; 40]. Despite achieving superior performance over unimodal models, multimodal models still fail to fully exploit the capabilities of each modality [41]. 2 As noted by [42], multimodal models, particularly those employing contrastive learning techniques such as CLIP [43; 44; 45; 46; 47], learn representations from multimodal data but may inadvertently inherit biases due to the imbalanced and biased nature of the training data. As discussed in [48], this issue severely hinders the effectiveness of multimodal learning. In detail, such condition occurs when certain modalities dominate the training process while others remain underrepresented, constraining the models capacity to capture the comprehensive information embedded in multimodal data distributions. Such dominance can lead to models over-reliance on the dominant modality, which impedes their ability to generalize effectively to unseen data or situations where the dominant modality is absent. Moreover, in multimodal systems, modality bias can manifest in several other ways. For instance, if model is primarily trained on image-text pairs, but audio or video data is only sparsely represented, the model may learn representations that are disproportionately influenced by the image-text modalities, while neglecting the rich information available from the audio or video inputs. This type of imbalance leads to model that may perform well under normal circumstances but struggles to generalize when the dominant modality is absent. From model learning perspective, [49] identifies the differing convergence rates of modalities as core cause of modality bias. The varying levels of difficulty in fitting category labels across different modalities contribute to this disparity. Some modalities may align more easily with the target labels than others, leading to an unequal contribution to the final learned representations. This uneven convergence further exacerbates the problem, reinforcing the bias towards certain modalities. Recent studies in multimodal scene understanding have further highlighted this issue. For example, research by [38], [33], and [50] shows that multimodal segmenters often over-rely on certain modalities, resulting in significant performance degradation when these dominant modalities are missing or unavailable. These findings underscore the need to address modality imbalance in order to ensure that all modalities contribute effectively to the learning process."
        },
        {
            "title": "1.3 Our Position",
            "content": "In the context of Multimodal Large Language Models (MLLMs), the presence of modality bias is also evident. For instance, empirical results in [24] reveal that MLLMs exhibit modality bias in the generated content. Specifically, the output of MLLMs is often primarily influenced by the language models prior knowledge, rather than by the input images. That is, MLLMs frequently produce confident responses even when relevant images are absent or when incongruent visual input is provided. This phenomenon is also validated by our experiments in Sec. 4. Moreover, work [51] further confirms that the modality bias in MLLMs stems from the complex interactions between multiple modalities, which complicates the multimodal debiasing process. Thus, we propose the position that MLLMs are deeply affected by modality bias. Firstly, we offer the definition of modality bias in Sec. 2. Secondly, we review the research roadmap about modality bias in MLLMs in Sec. 3.1. Thirdly, we analysis the key factors of modality bias in MLLMs in Sec. 3.2, accompanied with case study in Sec. 4. Finally, we conclude further the targeted solutions of modality bias in MLLMs, including current works and future directions in Sec. 5 and Fig. 4. The big picture of our position is shown in Fig. 1."
        },
        {
            "title": "2 Proposed Definition of Modality Bias",
            "content": "Modality bias arises when certain modalities dominate the learning process, while others are underutilized or contribute less effectively [52]. This imbalance often results in model that is biased towards the dominant modality, thus failing to fully leverage the potential of the underrepresented modalities [53; 54; 55; 56]. As result, the models performance can degrade significantly when the dominant modality is missing, unavailable, or unreliable. To mathematically describe this imbalance, let us define the contribution of each modality Mi as C(Mi), where {1, 2, . . . , n} represents the different modalities (e.g., image, text, audio). The total contribution of all modalities is given by the sum of these individual contributions: Ctotal = C(Mi). (cid:88) i= 3 (1) Figure 2: Further definition of modality bias and three potential results. If certain modalities dominate, the distribution of contributions becomes imbalanced, such that C(Mi) {C(Mx), C(My), , C(Mz)} for some = {x, y, , z}, as shown in Fig. 2. This imbalance can lead to several issues: 1 Over-reliance on dominant modalities: The model may become overly sensitive to the dominant modality Mi, resulting in biased predictions that fail to incorporate the full diversity of information from the multimodal data. 2 Underutilization of certain modalities: Modalities that are underrepresented in the training data, such as audio or video, contribute less to the learned representations, leading to models that lack robustness when these modalities are needed. 3 Decreased performance in missing modality scenarios: When dominant modality is missing during inference (for example, if an image is unavailable), the models performance can drastically drop, as it has not sufficiently learned how to balance the different modalities during training. To capture the extent of modality bias, we can define relative measure of imbalance, known as the modality imbalance ratio modality, as the ratio of the contribution of the dominant modality to the underutilized modality3: modality = C(Mdominant) C(Munderutilized) . (2) This ratio quantifies the disparity between the contributions of the modalities and can serve as diagnostic tool to identify and address modality bias. high value of modality indicates strong bias towards the dominant modality, which can hinder the models ability to generalize effectively. In conclusion, modality bias is fundamental issue in multimodal learning that arises from the unequal contributions of different modalities. It leads to suboptimal learning outcomes and impairs the models ability to generalize, especially when certain modalities are missing or unavailable. Addressing modality bias involves ensuring that all modalities are effectively utilized and contribute in balanced manner, thereby improving the robustness and performance of multimodal systems."
        },
        {
            "title": "3.1 Research Road-map",
            "content": "The exploration process of modality bias in MLLMs can be divided into three directions: (a) How to prove the bias? (b) How to solve the bias through datasets? (c) How to solve the bias through methods? These three directions are defined by their different focuses, including bias/debias, datasets/methods, collaborating to highlight and solve modality bias in MLLMs. 1 How to prove the bias? 3The definition is for better illustration of modality bias, not for calculating. 4 With the modality bias in MLLMs emerging gradually as research focus, several datasets and benchmarks have been proposed to measure the modality bias in MLLMs [25; 26; 57; 51; 58]. Park et al. [25] directly proposed metric named Modality Importance Score (MIS) to measure each modalitys contribution in the video question answering task. Based on comprehensive benchmark, the modality imbalance in current multimodal datasets is proven. Lee et al. [57] and Leng et al. [51] mainly laid emphasis on the modality prior, which is key reason for modality bias in MLLMs. Specifically, Lee et al. [57] introduced counterfactual images in VLind-Bench to measure the language priors of LVLMs, proving that LVLMs have great over-reliance on language priors. Leng et al. [51] proposed more comprehensive benchmark, namely Curse of Multi-Modalities (CMM), including three modalities: language, visual, and audio. The results of CMM further explain the contributors to hallucinations, where the over-reliance on unimodal priors plays an important role. Liu et al. [58] explored the bias from the perspective of vision-knowledge conflicts, proving the over-reliance of MLLMs on text queries. Moreover, Tong et al. [26] proposed the Multimodal Visual Patterns (MMVP) benchmark, further exploring the contrastive language-image pre-training (CLIP)s weaknesses, which lead to MLLMs failures in understanding visual information. 2 How to solve the bias through datasets? With the modality bias proven to be common phenomenon in datasets, which is the foundation of training and validation for MLLMs, researchers set their sights on how to solve the bias in datasets [27; 28; 29]. Chen et al. [28] proposed MORE, VQA dataset that requires multi-hop reasoning and overcoming unimodal biases, providing counterexample data to drive the LVLMs to overcome modality bias. Meanwhile, several works focus on decreasing the modality bias in multimodal datasets. Chen et al. [27] proposed MMStar, meticulously designed multimodal benchmark, of which each sample shows visual dependency, avoiding the modality bias in datasets. Yue et al. [29] built robust benchmark MMMU-Pro based on MMMU [59]. Through steps such as making questions embedded in images, MMMU-Pro is equipped with the ability to force MLLMs to both \"see\" and \"read\". 3 How to solve the bias through methods? Besides datasets, applying specific methods to reduce the modality bias in MLLMs is another tendency [24; 26; 60; 61; 58; 62; 63; 64; 65]. Typically, Pi et al. [61] and Zhang et al. [64] introduced preference learning methods, such as Bootstrapped Preference Optimization (BPO) and Noise-Aware Preference Optimization (NaPO), solving the modality bias problem based on building negative response datasets. Meanwhile, Zhang et al. [24], Liu et al. [58], and Tong et al. [26] proposed frameworks and methods to \"force\" MLLMs to pay more attention to images and boost MLLMs visual understanding abilities. Moreover, Li et al. [65] focused on the Multimodal Reward Models (MM-RMs) for MLLMs, proposing shortcut-aware MM-RM learning algorithm, decreasing MLLMs reliance on unimodal spurious correlations. Most above works consider unimodal dependency, especially on textual modality, as the key reason for modality bias. Thus, the boosting of the visual modality gradually turns into major research topic."
        },
        {
            "title": "3.2 Key Factors of Modality Bias in MLLMs",
            "content": "Based on Sec 3.1, the key factors of modality bias in MLLMs can be concluded as follows: dataset imbalances, varying modal capabilities, training objectives, and the interactions between modalities. These factors contribute to the unequal utilization of modalities during training, leading to biases towards certain modalities and suboptimal performance. We summarize three key factors as follows: 1 Dataset Imbalances: The training dataset composition significantly influences modality utilization. Datasets often have imbalanced modality distributions, where some modalities, such as text or images, are more abundant or have different information density [27; 28; 29]. This imbalance leads to models learning representations biased towards the more frequent modalities, while under-utilizing the less represented ones, even when multiple modalities are available. In addition, textual data is often more semantically dense or informative than visual data in certain tasks, due to its structured and explicit nature. As result, models tend to prioritize textual inputs during learning, treating accompanying modalities such as images merely as auxiliary cues, further amplifying the reliance on dominant modalities. 2 Asymmetric Modal Backbone Capabilities: Different modalities vary in complexity and in the architectural designs used to process them. Language models often benefit from mature and highly 5 optimized transformer-based architectures [66; 67; 68], which are not only effective but also backed by extensive research and industrial-scale pretraining. In contrast, processing visual or acoustic data typically requires more diverse and specialized backbones [69; 70; 71; 72; 73; 74] and may not benefit from equally massive pretraining corpora. Moreover, the rapid advancement of language models, fueled by large-scale datasets and sustained community focus, has further widened the performance gap across modalities. As result, multimodal models with strong language backbones tend to over-rely on text inputs, under-utilizing other modalities, particularly those that demand more complex or less mature processing pipelines. 3 Training Objectives: The choice of training objectives fundamentally shapes how multimodal models utilize different modalities and often exacerbates modality bias. Pretraining strategies in many state-of-the-art multimodal modelssuch as CLIP-style contrastive learning, imagetext matching (ITM), masked language modeling (MLM), or caption generationtend to prioritize textimage alignment due to the abundance of paired data and the relative ease of textual supervision. These objectives implicitly encourage the model to rely heavily on language as the semantic anchor, such as LanguageBind [75] and UniBind [76]. Consequently, modalities like audio, video, point clouds, or thermal datawhich are harder to align, less semantically rich in isolation, or lack large-scale supervisionare under-optimized during pretraining. Furthermore, most objectives do not explicitly encourage consistent cross-modal alignment or robust fusion across diverse modalities, resulting in imbalanced feature representations and limited generalization to underrepresented input types. Additionally, two other factors contribute to modality bias: 4 Differences in Convergence Rates: Each modality converges at different rates during training. Some modalities, like images and text, are more easily aligned with target labels due to their structure and high information density, while others, such as audio or video, require more complex processing. This disparity results in certain modalities being more influential in the models final learned representation, amplifying modality bias. 5 Modal Interactions and Integrations: The interaction between modalities also affects modality bias. If the relationships between modalities are not explicitly learned, the model may favor the more easily processed modality, like text, over others. The complexity of integrating multimodal information can exacerbate bias, as the model may struggle to effectively combine all modalities, resulting in predictions that under-utilize available data. In summary, modality bias is driven by factors such as dataset imbalances, differences in modal capabilities, training objectives, and the interactions between modalities. Addressing these factors is essential to mitigating modality bias and improving multimodal model performance. Strategies to balance modality contributions during training, optimize multimodal integration, and address dataset imbalances are critical for building fairer and more robust multimodal systems."
        },
        {
            "title": "4 Case Study",
            "content": "Figure 3: Case study for exploring modality bias in MLLMs. Dataset: MMMU-Pro, MLLM: Qwen2.5VL. Based on this case study, the three main factors proposed in Sec. 3.2 are further illustrated and proved. \"white\" means the image pixels are all set to 255. \"black\" means the image pixels are all set to 0. 6 Table 1: Directly applying missing modality evaluation with MVLMs on MMMU-pro dataset. Basic prompt (B-P) for Image only is: \"Based on the provided images, please answer the question.\" Model Inference Standard Standard 10 Image-Text Text /w white Image Image-Text Text w/ white Image w/ B-P Qwen2.5-VL-7B-I Qwen2.5-VL-32B-I Direct CoT Direct CoT 48.32 - 52.14 - 57.80 - 59.88 - 34.91 28.73 37.57 -13.41 -19.59 - 35.20 28.38 39.94 -16.94 -23.76 - 40.17 27.23 43.94 -17.63 -30.57 - 40.12 26.36 48.55 -19.76 -33.52 - 21.73 -15.84 21.39 -18.55 28.32 -15.62 26.82 -21.73 14.91 -22.66 14.97 -24.97 15.38 -28.56 14.34 -34.21 Model Inference Standard 4 Standard 10 Image-Text Text /w black Image Image-Text Text w/ black Image w/ B-P Qwen2.5-VL-7B-I Qwen2.5-VL-32B-I Direct CoT Direct CoT 48. - 52.08 - 57.80 - 59. - 34.86 29.02 37.63 -13.46 -19.30 - 21.45 28.38 39.94 -30.63 -23.70 - 40.17 26.99 43.87 -17.63 -30.81 - 40.35 26.65 48.44 -19.48 -33.18 - 21.62 -16.01 21.45 -18.49 28. -15.66 26.82 -21.62 14.51 -23.12 14. -25.08 15.03 -28.84 14.86 -33.58 Model Inference Standard 4 Standard 10 Image-Text Text w/ noise Image Image-Text Text w/ noise Image w/ B-P Qwen2.5-VL-7B-I Qwen2.5-VL-32B-I Direct CoT Direct CoT 48.32 - 51.73 - 57. - 60.12 - 34.97 26.99 37. -13.35 -21.33 - 35.66 27.40 40. -16.07 -24.33 - 40.23 26.13 43. -17.52 -31.62 - 40.12 28.03 48. -20.00 -32.09 - 21.56 -16.01 21. -18.73 28.27 -15.43 27.17 -20.86 14. -22.71 13.35 -26.65 14.28 -29.42 13. -34.04 The results presented in Tab. 1 and Tab. 2 reveal several key insights regarding the performance of the multimodal large model when tested with different input combinations across the MMMU-pro dataset. The process of case study is shown in Fig. 4 These insights can be linked to the three key factors identified in our analysis of modality bias in Multimodal Large Language Models (MLLMs): 1 dataset imbalances, 2 asymmetric modal backbone capabilities, and 3 training objectives. Lower Consistency with Image-only Inputs 27.17% (Complete & Image-only, Direct) and 28.21% (Complete & Image-only, CoT): The relatively low consistency between the complete input and image-only input suggests that the image modality alone is not sufficient for the model to make consistent predictions. When the model only has access to visual data, its predictions tend to be less reliable, underscoring the inadequacies of the model in processing visual data in isolation. This result supports the factor of 1 dataset imbalances, where the richness and complexity of image data, compared to more compact textual data, pose challenges for the model. Although images provide important visual cues, the model struggles to effectively utilize the image modality alone, indicating that the image modality is underutilized in the absence of complementary text data. Consistency between Complete and Text-only Inputs 56.53% (Complete & Text-only, Direct) and 43.64% (Complete & Text-only, CoT): The finding that over half of the samples show consistency between the complete (both image and text) and text-only inputs across both inference techniques (Direct and CoT) is significant. It suggests that textual information alone is strong foundation for the models predictions, and in many cases, the image modality does not substantially alter the models output. This highlights the dominance of the language modality, which is particularly advantageous due to its well-established processing capabilities. This result is consistent with the factor of 2 asymmetric modal backbone capabilities, where models with stronger language backbones, such as this one, tend to perform better on language tasks, often overshadowing the visual modality and limiting the models ability to effectively integrate multimodal information. 7 Table 2: Prediction consistency analysis of Qwen2.5-VL-7B-Instruct model on MMMU-pro. Complete means both images and text are used as input; Direct and CoT refer to the inference techniques. Choices Standard 4 (Direct) Standard 4 (CoT) Standard 10 (Direct) Standard 10 (CoT) Num. Percentage Num. Percentage Num. Percentage Num. Percentage Complete & Text-only Consistent Complete & Image-only Text-only & Image-only All Complete & Text-only Inconsistent Complete & Image-only Text-only & Image-only All Complete & Text-only Consistent Complete & Image-only Text-only & Image-only All Complete & Text-only Consistent Complete & Image-only Text-only & Image-only All 978 470 267 752 1260 1267 353 254 185 142 522 216 125 All Samples 56.53% 27.17% 26.76% 15.43% 43.47% 72.83% 73.24% 20.40% 755 449 220 975 1242 1281 Correct Samples 26.32% 14.67% 10.68% 8.20% 264 174 125 Wrong Samples 30.15% 12.49% 16.06% 7.22% 338 224 275 43.64% 28.21% 25.95% 12.72% 56.36% 71.70% 74.05% 27.63% 24.08% 15.26% 10.05% 7.22% 19.51% 12.94% 15.91% 5.49% 821 234 112 909 1468 1496 284 126 61 49 537 173 63 47.46% 15.14% 13.53% 6.47% 52.54% 84.86% 86.47% 36.82% 16.42% 7.28% 3.53% 2.83% 31.04% 7.86% 9.99% 3.64% 568 260 232 87 1162 1498 844 231 135 56 337 125 176 45 32.83% 15.03% 13.41% 5.03% 67.17% 84.97% 86.59% 48.79% 13.36% 7.80% 3.24% 2.43% 19.46% 7.22% 10.16% 2.60% Inconsistency between Text-only and Image-only Inputs 26.76% (Text-only & Image-only, Direct) and 25.95% (Text-only & Image-only, CoT): The low consistency between text-only and image-only inputs highlights the challenge the model faces when dealing with these two distinct modalities separately. This discrepancy suggests that both text and image provide complementary yet crucial information for accurate predictions. Textual data offers rich semantic context, nuances, and detail that images alone cannot convey, while images provide visual cues and spatial relationships that text cannot fully express. The low consistency between these two modalities, especially in the CoT setting, where reasoning and integration are critical, points to the challenge of combining these modalities effectively. This underscores the factor of 3 training objectives, where existing training strategies often fail to adequately balance multimodal learning, leading to modality-specific shortcuts. In the case of this model, the failure to effectively combine text and image information results in inconsistent predictions, especially when reasoning across modalities is required. Our findings underscore the importance of balanced training strategies and model architectures to address modality bias and improve multimodal integration. This also highlights the need for future research aimed at developing MLLMs that can more effectively process and combine diverse sources of information, mitigating the impact of modality bias."
        },
        {
            "title": "5.1 Current Works",
            "content": "1 Enhance visual modalitys contribution in datasets: With the in-depth exploration in modality bias, especially in the vision-language modality combination, visual information tends to be proven to be ignored, resulting in MLLMs over-reliance on the textual modality [24]. Thus, researchers naturally attempt to enhance visual modalitys contribution in datasets to balance the information from different modalities. Typical cases include MMStar [27] and MMMU-Pro [29], where MMStar carefully selects visually dependent samples and MMMU-Pro not only filters out visually independent samples but also embeds questions into images. Such works provide an optimization direction for current multimodal datasets. While few works contain systematic index to evaluate the modality bias in datasets [25], the others tend to prove the datasets necessity based on MLLMs disappointing results. More modality bias evaluation methods need to be explored to construct better debias multimodal datasets. Importantly, the feedback from MLLMs on these datasets should also be considered, as their performance can inform how datasets are optimized, offering valuable insights for future improvements. 8 2 Turn the focus of MLLMs from textual information into visual information: Considering the ignorance of visual modality in the inference of MLLMs, it is an intuitive approach to force MLLMs to lay more emphasis on visual modality. Works such as [62; 24] apply strategies, mostly training-free, to guide MLLMs towards visual modality. While Zhao et al.[63] proposed novel framework to help MLLMs compress the influence of textual bias, enhancing visual modality across the model. However, due to the excessive focus on the visual modality in such works, theres difficulty in applying them to broader modality bias situations. The real-world application requires the exploration of bias in richer modality combination. Figure 4: Targeted solutions of modality bias in MLLMs, including current works and future directions. 3 Apply preference optimization strategies: Besides the adjustments of multimodal datasets content and MLLMs focus, another popular method is to use preference optimization strategy to correct modality bias internally [61; 64]. Pi et al. [61] built preference dataset containing samples reflecting modality bias generated from the pretraining process. Zhang et al. [64] forced MLLMs to generate answers according to specific modality through adding noise, thus creating the preference dataset. Considering solving modality bias as preference optimization goal is creative and reasonable idea that brings new insights to researchers. However, existing preference optimization methods only generate modality bias samples in limited ways, while the real causes of modality bias are complex and multi-stage, which await further exploration."
        },
        {
            "title": "5.2 Future Directions",
            "content": "1 Measure modality bias in MLLMs: The exploration of an objective and systematic metric to measure modality bias is crucial for the development of related research. For example, for dataset construction, metric is needed as flag that offers researchers clear direction to make progress. Fields like semantic segmentation [77; 78] and image restoration [79] have seen huge development with the existence and optimization of evaluation metrics, where modality bias in MLLMs still remains almost blank. Therefore, more research works are being called for regarding measuring modality bias in MLLMs. 2 Explore modality bias in more modality combinations: Despite several works attempting to address the modality bias problem, the research focus is mainly set on the modality bias in LVLMs, which is part of the MLLMs. Although the textual information and visual information show great importance for world understanding [80], modalities like audio and tactile also matter [81; 82; 83]. As to the robotics field [84; 85; 86], tactile information is indispensable for robots to understand environments and handle downstream tasks such as dexterous manipulation [87]. Due to the modality limitation of current debiasing methods, it is hard for them to be applied in broader situations, hindering their applications in the real world. Thus, more generalized debiasing strategies are required in real-world applications to handle conditions that are more complex and have more modalities besides images and texts. 3 Apply XAI for modality bias in MLLMs: Last but not least, finding the causes of modality bias in MLLMs and visualizing them will have great positive influence on future works. Even though current works attempt to dig out the reasons for modality bias in MLLMs, they propose opinions from the phenomenon level. The internal mechanism of modality bias still lacks exploration, which is theoretical evidence and guidance to support future works. Thus, explainable AI [88; 89] is required here, such as visualizing the interaction process between modalities, to deeply analyze the theoretical causes and working mechanism of modality bias in MLLMs, which can be more solid inspiration for future works."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper aims to highlight the phenomenon of modality bias in MLLMs and call for research work targeted at better integrating multiple modalities. Our position is that MLLMs are deeply affected by modality bias, which is proved and explored by both the theoretical analysis and case study. 9 Moreover, we offer an in-depth discussion targeted at modality bias in MLLMs, including the key factors, potential results, and targeted solutions, hoping to bring new insights to the development of more robust and generalizable multimodal systems."
        },
        {
            "title": "References",
            "content": "[1] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [2] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [3] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, et al., Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479, 2025. [4] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [5] P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [6] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [7] R. S. Dahiya, G. Metta, M. Valle, and G. Sandini, Tactile sensingfrom humans to humanoids, IEEE transactions on robotics, vol. 26, no. 1, pp. 120, 2009. [8] L. Zou, C. Ge, Z. J. Wang, E. Cretu, and X. Li, Novel tactile sensor technology and smart tactile sensing systems: review, Sensors, vol. 17, no. 11, p. 2653, 2017. [9] C. Chi, X. Sun, N. Xue, T. Li, and C. Liu, Recent progress in technologies for tactile sensors, Sensors, vol. 18, no. 4, p. 948, 2018. [10] G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, et al., Event-based vision: survey, IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 1, pp. 154180, 2020. [11] X. Zheng, Y. Liu, Y. Lu, T. Hua, T. Pan, W. Zhang, D. Tao, and L. Wang, Deep learning for event-based vision: comprehensive survey and benchmarks, arXiv preprint arXiv:2302.08890, 2023. [12] H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, High speed and high dynamic range video with an event camera, IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 6, pp. 19641980, 2019. [13] X. Zheng, P. Y. Zhou, A. V. Vasilakos, and L. Wang, 360sfuda++: Towards source-free uda for panoramic segmentation by learning reliable category prototypes, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [14] X. Zheng, P. Zhou, A. V. Vasilakos, and L. Wang, Semantics distortion and style matter: Towards source-free uda for panoramic segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2788527895, 2024. [15] D. Zhong, X. Zheng, C. Liao, Y. Lyu, J. Chen, S. Wu, L. Zhang, and X. Hu, Omnisam: Omnidirectional segment anything model for uda in panoramic semantic segmentation, arXiv preprint arXiv:2503.07098, 2025. [16] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al., Mmbench: Is your multi-modal model an all-around player?, in European conference on computer vision, pp. 216233, Springer, 2024. 10 [17] X. Fang, K. Mao, H. Duan, X. Zhao, Y. Li, D. Lin, and K. Chen, Mmbench-video: longform multi-shot benchmark for holistic video understanding, Advances in Neural Information Processing Systems, vol. 37, pp. 8909889124, 2024. [18] M. Mathew, D. Karatzas, and C. Jawahar, Docvqa: dataset for vqa on document images, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 2200 2209, 2021. [19] L. Li, G. Chen, H. Shi, J. Xiao, and L. Chen, survey on multimodal benchmarks: In the era of large ai models, arXiv preprint arXiv:2409.18142, 2024. [20] J. Li, W. Lu, H. Fei, M. Luo, M. Dai, M. Xia, Y. Jin, Z. Gan, D. Qi, C. Fu, et al., survey on benchmarks of multimodal large language models, arXiv preprint arXiv:2408.08632, 2024. [21] J. Wu, Z. Zhang, Y. Xia, X. Li, Z. Xia, A. Chang, T. Yu, S. Kim, R. A. Rossi, R. Zhang, et al., Visual prompting in multimodal large language models: survey, arXiv preprint arXiv:2409.15310, 2024. [22] C. Jiang, Z. Wang, M. Dong, and J. Gui, Survey of adversarial robustness in multimodal large language models, arXiv preprint arXiv:2503.13962, 2025. [23] Y. Huo and H. Tang, When continue learning meets multimodal large language model: survey, arXiv preprint arXiv:2503.01887, 2025. [24] Y.-F. Zhang, W. Yu, Q. Wen, X. Wang, Z. Zhang, L. Wang, R. Jin, and T. Tan, Debiasing multimodal large language models, arXiv preprint arXiv:2403.05262, 2024. [25] J. Park, K. J. Jang, B. Alasaly, S. Mopidevi, A. Zolensky, E. Eaton, I. Lee, and K. Johnson, Assessing modality bias in video question answering benchmarks with multimodal large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, pp. 1982119829, 2025. [26] S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie, Eyes wide shut? exploring the visual shortcomings of multimodal llms, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. [27] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al., Are we on the right way for evaluating large vision-language models?, arXiv preprint arXiv:2403.20330, 2024. [28] M. Chen, Y. Cao, Y. Zhang, and C. Lu, Quantifying and mitigating unimodal biases in multimodal large language models: causal perspective, arXiv preprint arXiv:2403.18346, 2024. [29] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, et al., Mmmu-pro: more robust multi-discipline multimodal understanding benchmark, arXiv preprint arXiv:2409.02813, 2024. [30] P. Xu, X. Zhu, and D. A. Clifton, Multimodal learning with transformers: survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 1211312132, 2023. [31] K. Bayoudh, R. Knani, F. Hamdaoui, and A. Mtibaa, survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets, The Visual Computer, vol. 38, no. 8, pp. 29392970, 2022. [32] S. Jabeen, X. Li, M. S. Amin, O. Bourahla, S. Li, and A. Jabbar, review on methods and applications in multimodal deep learning, ACM Transactions on Multimedia Computing, Communications and Applications, vol. 19, no. 2s, pp. 141, 2023. [33] X. Zheng, Y. Lyu, and L. Wang, Learning modality-agnostic representation for semantic segmentation from any modalities, in European Conference on Computer Vision, pp. 146165, Springer, 2024. 11 [34] C. Liao, X. Zheng, Y. Lyu, H. Xue, Y. Cao, J. Wang, K. Yang, and X. Hu, Memorysam: Memorize modalities and semantics with segment anything model 2 for multi-modal semantic segmentation, arXiv preprint arXiv:2503.06700, 2025. [35] T. Brödermann, C. Sakaridis, Y. Fu, and L. Van Gool, Cafuser: Condition-aware multimodal fusion for robust semantic perception of driving scenes, IEEE Robotics and Automation Letters, 2025. [36] M. A. Manzoor, S. Albarri, Z. Xian, Z. Meng, P. Nakov, and S. Liang, Multimodality representation learning: survey on evolution, pretraining and its applications, ACM Transactions on Multimedia Computing, Communications and Applications, vol. 20, no. 3, pp. 134, 2023. [37] Y. Wei, R. Feng, Z. Wang, and D. Hu, Enhancing multimodal cooperation via sample-level modality valuation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2733827347, 2024. [38] X. Zheng, Y. Lyu, J. Zhou, and L. Wang, Centering the value of every modality: Towards efficient and resilient modality-agnostic semantic segmentation, in European Conference on Computer Vision, pp. 192212, Springer, 2024. [39] X. Zheng, Y. Lyu, L. Jiang, D. P. Paudel, L. Van Gool, and X. Hu, Reducing unimodal bias in multi-modal semantic segmentation with multi-scale functional entropy regularization, arXiv preprint arXiv:2505.06635, 2025. [40] X. Zheng, H. Xue, J. Chen, Y. Yan, L. Jiang, Y. Lyu, K. Yang, L. Zhang, and X. Hu, Learning robust anymodal segmentor with unimodal and cross-modal distillation, arXiv preprint arXiv:2411.17141, 2024. [41] X. Peng, Y. Wei, A. Deng, D. Wang, and D. Hu, Balanced multimodal learning via on-the-fly gradient modulation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 82388247, 2022. [42] I. Alabdulmohsin, X. Wang, A. P. Steiner, P. Goyal, A. DAmour, and X. Zhai, Clip the bias: How useful is balancing data in multimodal learning?, in The Twelfth International Conference on Learning Representations. [43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from natural language supervision, in International conference on machine learning, pp. 87488763, PmLR, 2021. [44] M. Tschannen, B. Mustafa, and N. Houlsby, Clippo: Image-and-language understanding from pixels only, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1100611017, 2023. [45] C. Yang, Z. An, L. Huang, J. Bi, X. Yu, H. Yang, B. Diao, and Y. Xu, Clip-kd: An empirical study of clip model distillation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1595215962, 2024. [46] K. Wu, H. Peng, Z. Zhou, B. Xiao, M. Liu, L. Yuan, H. Xuan, M. Valenzuela, X. S. Chen, X. Wang, et al., Tinyclip: Clip distillation via affinity mimicking and weight inheritance, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2197021980, 2023. [47] X. Li, Z. Wang, and C. Xie, An inverse scaling law for clip training, Advances in Neural Information Processing Systems, vol. 36, pp. 4906849087, 2023. [48] S. Xu, M. Cui, C. Huang, H. Wang, and D. Hu, Balancebenchmark: survey for multimodal imbalance learning, arXiv preprint arXiv:2502.10816, 2025. [49] Y. Yang, F. Wan, Q.-Y. Jiang, and Y. Xu, Facilitating multimodal classification via dynamically learning modality gap, Advances in Neural Information Processing Systems, vol. 37, pp. 62108 62122, 2024. 12 [50] C. Liao, K. Lei, X. Zheng, J. Moon, Z. Wang, Y. Wang, D. P. Paudel, L. Van Gool, and X. Hu, Benchmarking multi-modal semantic segmentation under sensor failures: Missing and noisy modality robustness, arXiv preprint arXiv:2503.18445, 2025. [51] S. Leng, Y. Xing, Z. Cheng, Y. Zhou, H. Zhang, X. Li, D. Zhao, S. Lu, C. Miao, and L. Bing, The curse of multi-modalities: Evaluating hallucinations of large multimodal models across language, visual, and audio, arXiv preprint arXiv:2410.12787, 2024. [52] Y. Guo, L. Nie, H. Cheng, Z. Cheng, M. Kankanhalli, and A. Del Bimbo, On modality bias recognition and reduction, ACM Transactions on Multimedia Computing, Communications and Applications, vol. 19, no. 3, pp. 122, 2023. [53] A. Vosoughi, S. Deng, S. Zhang, Y. Tian, C. Xu, and J. Luo, Cross modality bias in visual question answering: causal view with possible worlds vqa, IEEE Transactions on Multimedia, 2024. [54] Y. Niu, K. Tang, H. Zhang, Z. Lu, X.-S. Hua, and J.-R. Wen, Counterfactual vqa: cause-effect look at language bias, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1270012710, 2021. [55] I. Gat, I. Schwartz, A. Schwing, and T. Hazan, Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies, Advances in Neural Information Processing Systems, vol. 33, pp. 31973208, 2020. [56] S. Ramakrishnan, A. Agrawal, and S. Lee, Overcoming language priors in visual question answering with adversarial regularization, Advances in neural information processing systems, vol. 31, 2018. [57] K.-i. Lee, M. Kim, S. Yoon, M. Kim, D. Lee, H. Koh, and K. Jung, Vlind-bench: Measuring language priors in large vision-language models, arXiv preprint arXiv:2406.08702, 2024. [58] X. Liu, W. Wang, Y. Yuan, J.-t. Huang, Q. Liu, P. He, and Z. Tu, Insight over sight? exploring the vision-knowledge conflicts in multimodal llms, arXiv preprint arXiv:2410.08145, 2024. [59] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. [60] H. Zhao, Z. Cai, S. Si, X. Ma, K. An, L. Chen, Z. Liu, S. Wang, W. Han, and B. Chang, Mmicl: Empowering vision-language model with multi-modal in-context learning, in ICLR, 2024. [61] R. Pi, T. Han, W. Xiong, J. Zhang, R. Liu, R. Pan, and T. Zhang, Strengthening multimodal large language model with bootstrapped preference optimization, in European Conference on Computer Vision, pp. 382398, Springer, 2024. [62] S. Liu, K. Zheng, and W. Chen, Paying more attention to image: training-free method for alleviating hallucination in lvlms, in European Conference on Computer Vision, pp. 125140, Springer, 2024. [63] H. Zhao, S. Si, L. Chen, Y. Zhang, M. Sun, M. Zhang, and B. Chang, Looking beyond text: Reducing language bias in large vision-language models via multimodal dual-attention and soft-image guidance, arXiv preprint arXiv:2411.14279, 2024. [64] Z. Zhang, H. Tang, J. Sheng, Z. Zhang, Y. Ren, Z. Li, D. Yin, D. Ma, and T. Liu, Debiasing multimodal large language models via noise-aware preference optimization, arXiv preprint arXiv:2503.17928, 2025. [65] Z. Li, X. Wen, J. Lou, Y. Ji, Y. Lu, X. Han, D. Zhang, and L. Sun, The devil is in the details: Tackling unimodal spurious correlations for generalizable multimodal reward models, arXiv preprint arXiv:2503.03122, 2025. [66] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. 13 [67] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models with longtermism, arXiv preprint arXiv:2401.02954, 2024. [68] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, comprehensive overview of large language models, arXiv preprint arXiv:2307.06435, 2023. [69] B. Ren, Y. Liu, Y. Song, W. Bi, R. Cucchiara, N. Sebe, and W. Wang, Masked jigsaw puzzle: versatile position embedding for vision transformers, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2038220391, 2023. [70] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, et al., survey on vision transformer, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87110, 2022. [71] B. Ren, G. Mei, D. P. Paudel, W. Wang, Y. Li, M. Liu, R. Cucchiara, L. Van Gool, and N. Sebe, Bringing masked autoencoders explicit contrastive properties for point cloud self-supervised learning, in Proceedings of the Asian Conference on Computer Vision, pp. 20342052, 2024. [72] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. [73] B. Ren, Y. Li, J. Liang, R. Ranjan, M. Liu, R. Cucchiara, L. V. Gool, M.-H. Yang, and N. Sebe, Sharing key semantics in transformer makes efficient image restoration, Advances in Neural Information Processing Systems, vol. 37, pp. 74277463, 2024. [74] S. Huang, B. Gong, Y. Feng, X. Chen, Y. Fu, Y. Liu, and D. Wang, Learning disentangled identifiers for action-customized text-to-image generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77977806, 2024. [75] B. Zhu, B. Lin, M. Ning, Y. Yan, J. Cui, H. Wang, Y. Pang, W. Jiang, J. Zhang, Z. Li, C. Zhang, Z. Li, W. Liu, and L. Yuan, Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, in ICLR, OpenReview.net, 2024. [76] Y. Lyu, X. Zheng, J. Zhou, and L. Wang, Unibind: Llm-augmented unified and balanced representation space to bind them all, in CVPR, pp. 2674226752, IEEE, 2024. [77] M. A. Rahman and Y. Wang, Optimizing intersection-over-union in deep neural networks for image segmentation, in International symposium on visual computing, pp. 234244, Springer, 2016. [78] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, Generalized intersection over union: metric and loss for bounding box regression, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 658666, 2019. [79] B. Hu, L. Li, J. Wu, and J. Qian, Subjective and objective quality assessment for image restoration: critical survey, Signal Processing: Image Communication, vol. 85, p. 115839, 2020. [80] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [81] J. Liu, S. Chen, X. He, L. Guo, X. Zhu, W. Wang, and J. Tang, Valor: Vision-audio-language omni-perception pretraining model and dataset, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [82] V. Dave, F. Lygerakis, and E. Rueckert, Multimodal visual-tactile representation learning through self-supervised contrastive pre-training, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 80138020, IEEE, 2024. 14 [83] Y. Lyu, X. Zheng, D. Kim, and L. Wang, Omnibind: Teach to build unequal-scale modality interaction for omni-bind of all, arXiv preprint arXiv:2405.16108, 2024. [84] N. Zhang, J. Ren, Y. Dong, X. Yang, R. Bian, J. Li, G. Gu, and X. Zhu, Soft robotic hand with tactile palm-finger coordination, Nature Communications, vol. 16, no. 1, p. 2395, 2025. [85] R. J. Kirschner, K. Karacan, A. Melone, and S. Haddadin, Categorizing robots by performance fitness into the tree of robots, Nature Machine Intelligence, pp. 112, 2025. [86] A. Agarwal, A. Wilson, T. Man, E. Adelson, I. Gkioulekas, and W. Yuan, Vision-based tactile sensor design using physically based rendering, Communications Engineering, vol. 4, no. 1, p. 21, 2025. [87] K. F. Gbagbe, M. A. Cabrera, A. Alabbas, O. Alyunes, A. Lykov, and D. Tsetserukou, Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations, in 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 28642869, IEEE, 2024. [88] A. Bennetot, I. Donadello, A. El Qadi El Haouari, M. Dragoni, T. Frossard, B. Wagner, A. Sarranti, S. Tulli, M. Trocan, R. Chatila, et al., practical tutorial on explainable ai techniques, ACM Computing Surveys, vol. 57, no. 2, pp. 144, 2024. [89] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian, Z. Wen, T. Shah, G. Morgan, et al., Explainable ai (xai): Core ideas, techniques, and solutions, ACM Computing Surveys, vol. 55, no. 9, pp. 133, 2023."
        }
    ],
    "affiliations": [
        "CSE, HKUST",
        "China University of Mining & Technology, Beijing",
        "College of Computing & Data Science, Nanyang Technological University",
        "Fudan University",
        "HKUST(GZ)",
        "INSAIT, Sofia University St. Kliment Ohridski",
        "Nagoya University",
        "SPIC Energy Science and Technology Research Institute",
        "Shanghai Jiao Tong University",
        "Tongji University",
        "University of Pisa, IT",
        "University of Trento, IT",
        "Xian Jiaotong University"
    ]
}