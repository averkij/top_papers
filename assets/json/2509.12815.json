{
    "paper_title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation",
    "authors": [
        "Biwen Lei",
        "Yang Li",
        "Xinhai Liu",
        "Shuhui Yang",
        "Lixin Xu",
        "Jingwei Huang",
        "Ruining Tang",
        "Haohan Weng",
        "Jian Liu",
        "Jing Xu",
        "Zhen Zhou",
        "Yiling Zhu",
        "Jiankai Xing",
        "Jiachen Xu",
        "Changfeng Ma",
        "Xinhao Yan",
        "Yunhan Yang",
        "Chunshi Wang",
        "Duoteng Xu",
        "Xueqi Ma",
        "Yuguang Chen",
        "Jing Li",
        "Mingxin Yang",
        "Sheng Zhang",
        "Yifei Feng",
        "Xin Huang",
        "Di Luo",
        "Zebin He",
        "Puhua Jiang",
        "Changrong Hu",
        "Zihan Qin",
        "Shiwei Miao",
        "Haolin Liu",
        "Yunfei Zhao",
        "Zeqiang Lai",
        "Qingxiang Lin",
        "Zibo Zhao",
        "Kunhong Li",
        "Xianghui Yang",
        "Huiwen Shi",
        "Xin Yang",
        "Yuxuan Wang",
        "Zebin Yao",
        "Yihang Lian",
        "Sicong Liu",
        "Xintong Han",
        "Wangchen Qin",
        "Caisheng Ouyang",
        "Jianyin Liu",
        "Tianwen Yuan",
        "Shuai Jiang",
        "Hong Duan",
        "Yanqi Niu",
        "Wencong Lin",
        "Yifu Sun",
        "Shirui Huang",
        "Lin Niu",
        "Gu Gong",
        "Guojian Xiao",
        "Bojian Zheng",
        "Xiang Yuan",
        "Qi Chen",
        "Jie Xiao",
        "Dongyang Zheng",
        "Xiaofeng Yang",
        "Kai Liu",
        "Jianchen Zhu",
        "Lifu Wang",
        "Qinglin Lu",
        "Jie Liu",
        "Liang Dong",
        "Fan Jiang",
        "Ruibin Chen",
        "Lei Wang",
        "Chao Zhang",
        "Jiaxin Lin",
        "Hao Zhang",
        "Zheng Ye",
        "Peng He",
        "Runzhou Wu",
        "Yinhe Wu",
        "Jiayao Du",
        "Jupeng Chen",
        "Xinyue Mao",
        "Dongyuan Guo",
        "Yixuan Tang",
        "Yulin Tsai",
        "Yonghao Tan",
        "Jiaao Yu",
        "Junlin Yu",
        "Keren Zhang",
        "Yifan Li",
        "Peng Chen",
        "Tian Liu",
        "Di Wang",
        "Yuhong Liu",
        "Linus",
        "Jie Jiang",
        "Zhuo Chen",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media."
        },
        {
            "title": "Start",
            "content": "Tencent Hunyuan Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation Tencent Hunyuan3D https://3d.hunyuan.tencent.com 5 2 0 2 6 ] . [ 1 5 1 8 2 1 . 9 0 5 2 : r Figure 1: High quality 3D assets generated by Hunyuan3D Studio."
        },
        {
            "title": "Abstract",
            "content": "The creation of high-quality 3D assets, cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into cohesive and user-friendly system. This unified framework allows for the rapid transformation of single concept image or textual description into fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents significant leap forward for AI-assisted workflows in game development and interactive media."
        },
        {
            "title": "Introduction",
            "content": "The demand for high-fidelity 3D content has surged, driven by the expanding frontiers of 3D games, virtual production, and the metaverse. However, traditional 3D asset creation remains complex, time-consuming, and often costly endeavor, typically requiring expertise across multiple software suites for modeling, UV mapping, texturing, and rigging. This process can form bottleneck in game production, limiting creative iteration and accessibility. Recent advancements in generative AI (Zhang et al., 2024a; Xiang et al., 2025; Weng et al., 2024; Lei et al., 2024), particularly diffusion models, have driven rapid progress in specific areas of 3D 1 content creation, such as geometry generation where systems like Hunyuan3D series (Yang et al., 2024; Zhao et al., 2025; Lai et al., 2025b; Hunyuan3D et al., 2025b; Lai et al., 2025a) demonstrate scalable, high-resolution asset synthesis from single images or text prompts. However, despite these breakthroughs in shape formation, the field continues to struggle with integrating these advances into assets that simultaneously meet the dual demands of high visual fidelity and technical readiness for real-time rendering in game engines. Many existing solutions address only isolated parts of the pipeline (e.g., generating geometry without game-optimized topology or producing textures that lack material accuracy), leaving artists with the challenging task of integrating and refining these outputs into usable, performant asset. To bridge this critical gap, we introduce Hunyuan3D Studio, comprehensive AI platform that reimagines the entire 3D asset creation workflow from the ground up. Our system is built upon the foundation of large-scale generative models but extends far beyond them into fully integrated production environment. This report details the architecture of this end-to-end pipeline, which is designed to transform high-level creative concept into game-engine-ready asset with minimal manual intervention. We demonstrate that this integrated approach not only significantly accelerates content creation but also democratizes 3D artistry by lowering the technical barriers to producing production-quality assets. This paper is organized as follows: Section 2 provides comprehensive overview of the Hunyuan3D Studio pipeline and its core modules. Sections 3 through 9 delve into the technical specifics of each module. Finally, Section 10 discusses conclusions, limitations, and future work."
        },
        {
            "title": "2 Hunyuan3D Studio Pipeline",
            "content": "The Hunyuan3D Studio pipeline is architected as sequential yet modular workflow, where each stage processes the asset and enriches it with data crucial for the next. This design ensures that the entire processfrom an initial idea to deployed game assetis seamless, automated, and maintains the highest possible fidelity. The pipeline, as shown in Figure. 2, comprises seven core technological modules, each addressing fundamental stage in the asset creation process: Figure 2: The pipeline of Hunyuan3D Studio. Controllable Image Generation (Concept Design): The pipeline initiates with multimodal input processing, supporting text-to-image and image-to-multi-view synthesis. dedicated A-Pose standardization module ensures character models maintain consistent skeletal orientation, while neural style transfer adapts visual aesthetics to match target game art styles. High-Fidelity Geometry Generation: This module generates detailed 3D geometry (highpoly mesh) from single or multi-view images, leveraging advanced diffusion-based architectures to ensure geometric alignment with input references and preserve intricate surface details. Part-level 3D Generation: Using connectivity analysis and semantic partitioning algorithms, complex models are automatically decomposed into logical, functional components (e.g., rifles magazine, barrel, and stock), enabling independent editing and animation. Polygon Generation (PolyGen): This module abandons traditional graphics-based retopology methods and instead employs an autoregressive model for face-by-face generation to construct low-polygon assets. Taking point cloud of the geometric surface as input, PolyGen intelligently retopologizes high-fidelity meshes, producing game-ready assets with low vertex counts and well-structured, deformation-aware edge flow. Semantic UV Unwrapping: This module implements context-aware UV segmentation that groups surfaces by material type and texel density requirements, minimizing seams and ensuring efficient texture space utilization. Texture Synthesis and Editing: Integrating generative models, the system produces physically-accurate PBR texture sets from text or image prompts, supported by nondestructive editing layer for refinement via natural language commands. Animation Module: The final automation stage infers joint placement and bone hierarchies, calculating vertex weights to create ready-to-animate assets that are configured for standard game engines. These modules are orchestrated through unified asset graph, where outputs from each stage propagate metadata to downstream processes. This enables parametric control, where high-level artistic adjustments cascade through the entire pipeline, and reversibility, allowing for incremental updates without full recomputation. The final output is configured and exported with all necessary specifications for the target game engine, such as Unity or Unreal Engine. This modular yet integrated approach ensures Hunyuan3D Studio addresses the full spectrum of game asset creationfrom conceptualization to engine integrationwhile maintaining artistic control and technical rigor. The following sections will provide detailed technical dissection of each modules architecture and functionality."
        },
        {
            "title": "3 Controllable Image Generation",
            "content": "Our controllable image generation pipeline leverages state-of-the-art open-source models, comprising modules for image stylization and pose standardization, as described below. 3.1 Image Stylization Figure 3: Visualization results of our image stylization module with pre-defined styles. Our image stylization module enables users to generate 3D design drawings in diverse, pre-defined popular game art styles through configurable option prior to 3D model generation, as shown in Figure. 3. It employs multi-style image-to-image generation model based on Qwen-ImageEdit Wu et al. (2025) and further adapted with Low-Rank Adaptation (LoRA) Hu et al. (2022). The model processes user-provided subject image with textual style instruction formatted as Change the style to {style type} 3D model. White Background. to produce stylized output that maintains content consistency with the input image while faithfully adhering to the specified 3 artistic style. The training data is constructed in triplet format {input reference image, style type, stylized 3D design drawing}, which establishes precise correspondence between photorealistic subject images and their stylized counterparts. For text-to-image stylization, where no reference image is provided, the system first generates reference image from the text prompt using an in-house general text-to-image model, and then processes it through the same image-to-image stylization pipeline to achieve the final stylized output. 3.2 Pose Standardization Standardizing the pose (e.g., to an A-pose) from an arbitrary character reference image requires simultaneously achieving precise pose control and maintaining strict character consistency. In addition, it involves the removal of background elements and props from reference images. To achieve this, we leverage character image with arbitrary poses/viewpoints as the reference and inject it as conditioning input into FLUX.1-dev DiT to guide the generation process, as shown in Figure. 4. Figure 4: Overall workflow of our pose standardization module. Figure 5: Visualization results of our pose standardization module. Dataset Construction. We first construct image pairs based on rendered character data in the following format: [character image with arbitrary poses/viewpoints, standard A-pose front views of the same character]. Subsequently, rendered data that includes props (such as handheld weapons and pedestals) is processed through state-of-the-art editing models, including Flux-Kontext Batifol et al. (2025). This step isolates the character by removing all props and background elements, ensuring the subjects consistency is maintained. Finally, the resulting image pairs are manually curated and incorporated into the dataset to equip the model with the capability for prop and background removal. Model Training. We implement the progressive learning strategy, starting with an initial resolution of 512 512 pixels and increasing to 768 768. Consequently, the model learns to extract more intricate features, which substantially improves the fidelity of generated 4 outputs, particularly in reproducing detailed facial characteristics and complex clothing textures. In addition, reference images of the same character under different scenarios are randomly selected and injected as conditional inputs, thereby achieving generalized pose control and consistent generation performance. Moreover, we have assembled supplementary high-quality datasets specifically targeting challenging categories, including half-body portraits, non-human humanoids, and anthropomorphic characters. These datasets are used for post-training techniques like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to bolster the models generalizability and robustness."
        },
        {
            "title": "4 High-Fidelity Geometry Generation",
            "content": "4.1 Preliminary Our geometry-generation pipeline is built upon the state-of-the-art Hunyuan3D 2.1 (Hunyuan3D et al., 2025b) and Hunyuan3D 2.5 (Lai et al., 2025a) frameworks. The Hunyuan3D 2.1 framework comprises two modules: Hunyuan3D-ShapeVAE variational encoderdecoder transformer that first compresses and then reconstructs 3-D geometry. The encoder receives point cloud endowed with 3-D positions and surface normals {xi = (Pi, Ni) Pi, Ni R3}, and embeds it into compact shape latents via Vector-Set Transformer with importance sampling (Zhang et al., 2023). The decoder employs to query 3-D neural field Fg RDHWd on uniform grid Qg RDHW3, and subsequently maps Fg to signed-distance values Sg RDHW. Hunyuan3D-DiT flow-based diffusion model that operates in the latent space of ShapeVAE. The network stacks 21 Transformer layers, each enhanced with Mixture-ofExperts (MoE) sub-layer to significantly enlarge capacity and expressive power. Hunyuan3DDiT is trained to map gaussian noises to shape latents with the flow matching objective (Lipman et al., 2022; Esser et al., 2024). 4.2 Conditional Generation (a) 3D shape generation pipeline (b) Image-to-multiview pipeline Figure 6: Overview of conditional generation. Hunyuan3D-DiT is primarily conditioned on single input image, which is first resized to 518 518, removed background and then encoded by frozen DINOv2 backbone (Oquab et al., 2023) to obtain the image latent cI RBLC, which is subsequently fused into the generated shape latent via cross attention. To provide additional geometric and prior guidance, Hunyuan3D-Studio supplies two additional control signals: an explicit 3-D bounding box and multi-view images. The overall pipeline is illustrated in Figure 6(a). 4.2.1 Bounding Box Condition To better align the generated 3D assets with user intent, we introduce bounding box as the simplest 3-D control signal to steer the DiT model. Given bounding box, we first encode its height, width and length into single shape latent cB RB1C with two-layer MLP. Then, cB is concatenated with cI along the sequence dimension to form the final conditioning vector. 5 During training we deliberately misalign the object proportions between the image and the point cloudvia mild deformation of either modalityto force the network to rely on the bounding-box signal. 4.2.2 Generated Multi-view Image Condition To harness the powerful capabilities of image-generation models, we use multi-view images produced by an image diffusion model as the condition for character generation. Image to multiview image generation. As, shown in Figure 6(b), to synthesize high-fidelity multiview images from single input, we introduce lightweight module built upon pretrained text-to-image foundation model (Li et al., 2024). This is achieved by training Low-Rank Adaptation (LoRA) layer (Hu et al., 2022). The training process begins with the curation of dataset comprising object-centric images from arbitrary camera poses, each paired with its corresponding ground-truth multiview images. During training, both the input single-view and target multiview images are encoded into their latent representations using the models pretrained Variational Autoencoder (VAE). The LoRA layer is conditioned on two sources of information: (1) the noise-free latent of the single-view image, which is concatenated with the noised multiview latent, providing structural guidance; and (2) semantic condition vector extracted from the input image using pretrained SigLIP vision encoder (Zhai et al., 2023). The LoRA parameters are then optimized using standard flow-matching loss. Multi-view image injection. Similar to the single image condition, we first encode all the images into image latents {ci Ii = org, front, left, back, right}. Each non-original view is marked by sinusoidal positional embedding with fixed index. After positional encoding, the latents from generated views are concatenated with the original-image latent to form the final condition. 4.3 Visualization Bbox condition. As illustrated in Figure 7, the bbox control signal not only succeeds in producing high-quality geometry when image-only geometric generation fails, but also generates 3D assets with appropriate proportions and well-structured forms according to the given bbox. Figure 7: 3D geometry generated with bounding box control. Generated multi-view image condition. Leveraging state-of-the-art multi-view generation model as guidance, our approach produces high-fidelity 3D character assets, as exemplified in Figure 8. Figure 8: 3D geometry generated with generated multi-view image control. Figure 9: Our part level shape generation results."
        },
        {
            "title": "5 Part-level 3D Generation",
            "content": "Generating 3D shapes at part level is pivotal for downstream applications such as mesh retopology, UV mapping, and 3D printing. However, existing part-based generation methods often lack sufficient controllability, produce inadequate geometric quality in generated parts, and suffer from limited semantic coherence. This section establishes new paradigm for creating production-ready, editable, and structurally sound 3D assets. Fig. 9 shows our part-level shape generation results. As shown in Figure. 10, given an input image, we first obtain the holistic shape using Huyuan3D 2.5 Lai et al. (2025a). The holistic mesh is then fed to part detection module P3-SAM Ma et al. (2025) to obtain the semantic features and part bounding boxes. Finally, X-Part Yan et al. (2025) decompose the holistic shape into parts. P3-SAM and X-Part will be introduced in section 5.1 and 5.2 Figure 10: Pipeline of our image to 3D part generation. Given an input image, we first obtain the holistic shape using Huyuan3D 2.5 Lai et al. (2025a). The holistic mesh is then fed to part detection module P3-SAM Ma et al. (2025) to obtain the semantic features and part bounding boxes. 7 Figure 11: Training pipeline of P3-SAM. 5. P3-SAM: Native 3D Part Segmentation Ma et al. (2025) 3D part segmentation is fundamental step in our part generation pipeline. In this section, we propose native 3D Point-Promptable Part segmentation model termed P3-SAM, designed to fully automate the segmentation of any complex 3D objects into components with precise mask and strong robustness. As pioneering promptable image segmentation work, SAM provides feasible implementation approach. However, our method focuses on achieving precise part segmentation automatically, and we simplify the architecture of SAM. Without adopting the complex segmentation decoder and multiple types of prompts from SAM, our model is designed to handle only one positive point prompt. Specifically, as shown in Fig. 11, P3-SAM contains feature extractor, three segmentation heads, and an IoU prediction head. We employ PointTransformerV3 as our feature extractor and integrate its features from different levels as extracted point-wise features. The input point prompt and feature are fused and passed to the segmentation heads to predict three multi-scale masks and an IoU (Intersection of Union) predictor is utilized to evaluate the quality of the masks. To automatically segment an object, as shown in Fig. 12, we apply our segmentation model using point prompts sampled by FPS (Farthest Point Sampling) and utilize NMS (Non-Maximum Suppression) to merge redundant masks. The point-level masks are then projected onto mesh faces to obtain the part segmentation results. Another key aspect of this method is to eliminate the influence of 2D SAM, and rely exclusively on raw 3D part supervision for training native 3D segmentation model. While existing 3D part segmentation datasets are either too small or lack part annotation, this work addresses the data scarcity by developing an automated part annotation pipeline for artist-created meshes and used it to generate dataset comprising 3.7 million meshes with high-quality part-level masks. Our model demonstrates excellent scalability with this dataset and achieves robust, precise, and globally coherent part segmentation. For more details of P3-SAM, please refer to the paper Ma et al. (2025). Comparison with SOTA. We evaluate each method on three datasets: PartObj-Tiny, PartObjTiny-WT, and PartNetE. PartObj-Tiny is subset of Objarvse, containing 200 data samples across 8 categories, with manually annotated part segmentation information. PartObj-Tiny-WT is the watertight version of PartObj-Tiny. To evaluate the performance of various networks on watertight data, we converted the meshes from PartObj-Tiny to watertight versions and successfully obtained 189 watertight meshes. PartNetE, derived from PartNet-Mobility, contains 1,906 shapes covering 45 object categories in the form of point clouds. We also evaluate various networks on it to verify their generalization performance on point cloud. Table 1 and 2 confirming the superior performance of P3-SAM under diverse conditions. 8 Figure 12: Pipeline of automatic segmentation using P3-SAM. Task Method Human Animals Daily Build. Trans. Plants Food Elec. AVG. Fully Seg. w/o Connect. Seg. w/ Connect. Interact. Find3D SAMPart3D SAMesh PartField Ours PartField Ours Point-SAM Ours 23.99 55.03 66.03 54.52 60. 80.85 80.77 8.63 49.01 23.99 57.98 60.89 58.07 59.43 83.43 86.46 9.38 53.45 22.67 49.17 56.53 56.46 62. 77.83 80.97 17.47 52.36 16.03 40.36 41.03 42.47 50.82 69.66 67.77 11.19 38.50 14.11 47.38 46.89 49.09 57. 73.85 68.44 7.63 51.52 21.77 62.14 65.12 59.16 70.53 80.21 90.30 13.95 62.57 25.71 64.59 60.56 55.4 54. 85.27 92.90 23.02 50.80 19.83 51.15 57.81 56.29 61.96 82.30 81.52 12.73 51.86 21.28 53.47 56.86 53.93 59. 79.18 81.14 13.00 51.23 Table 1: The comparison of our method with previous methods on PartObjectarverse-Tiny. The first two blocks represent class-agnostic part segmentation without and with connectivity, respectively, and the last block represents interactive segmentation. Task Fully Segmentation w/o Connectivity Interactive Seg. Method Find3D SAMPart3D SAMesh PartField Ours Point-SAM Ours PartObj-Tiny-WT PartNetE wait 21. wait 56.17 wait 26.66 wait 59.1 55.35 65.39 13.11 15.06 49.11 63. Table 2: The comparison of our method with previous methods on the watertight version of PartObjectarverse-Tiny. 5.2 -Part: high-fidelity and structure-coherent shape decomposition Yan et al. (2025) This section shows how to decompose the shapes into parts. Decomposing complete 3D shape into meaningful semantic parts would greatly facilitate various downstream tasks. For instance, breaking down complex geometry into simpler parts can significantly ease the process of mesh re-topology and uv-unwrapping. However, generating shapes at the part level presents two major challenges: 1) The decomposed geometry must maintain meaningful part-level semantics, and 2) The generation process must recover geometrically plausible structures for internal regions. Mainstream part-generation methods adopt the latent vecset diffusion framework Zhao et al. (2025), where each part is represented as an independent set of latent codes for diffusion. The generation process can be executed independently for individual parts (e.g., HoloPart Yang et al. (2025a)) or simultaneously for all parts (e.g., PartCrafter Lin et al. (2025), PartPacker Tang et al. (2025)) to enhance part synchronization. Furthermore, multi-view or 3D segmentation are frequently employed for better part decomposition Yang et al. (2025a;b). However, these approaches are highly sensitive to inaccuracies in the segmentation results. Alternative works Lin et al. (2025); Tang et al. (2025) do not explicitly rely on segmentation, but they still fail to offer controllable part-based generation and often produce decomposed parts with ambiguous boundary. Motivated by these observations, we introduce -Part, controllable and editable diffusion framework, which enables semantically meaningful and structurally coherent part generation. Our objective is to generate high-fidelity and structure-coherent part geometries from given object point cloud, while ensuring flexible controllability over the decomposition process. Figure. 9 Figure 13: Pipeline of our shape decomposition. shows the pipeline of our shape decomposition method. First, to achieve the controllability, we propose part-level cues extraction module that uses bounding boxes as prompts to indicate part locations and scales, instead of directly using segmentation results as input. Compared with fine-grained and point-level segmentation cues, bounding boxes provide coarser form of guidance, which mitigates overfitting to the input. Besides, the bounding box provides additional volume scale information for the partially visible part, benefiting the generation and controllability. Second, despite inaccuracies in the segmentation results, we notice that the high-dimension pointwise semantic feature is free from the information compression caused by the cluster algorithm or prediction head used in Liu et al. (2025c), resulting in more accurate semantic representations. Therefore, we carefully introduce the semantic features into our framework with delicately designed feature perturbation, which benefits the meaningful part decomposition. Third, we integrate -Part into bounding box based part editing pipeline. It supports local editing, such as merging small number of parts within an object and adjusting their scales, to facilitate interactive part generation. To prove the effectiveness of -Part, we conducted extensive experiments on various benchmarks. Our results show that -Part achieves state-of-the-art performance in part-level decomposition and generation. For more details of -Part, please refer to the paper Yan et al. (2025). Comparison with SOTA. We evaluate our method on 200 samples from the ObjaversePart-Tiny dataset, each comprising rendered images and corresponding ground-truth part geometries. To assess geometric quality, we employ Chamfer Distance (CD) and F-Score. The F-Score is computed at two different thresholds [0.1, 0.5] to capture both coarse-level and fine-level geometric alignment. Prior to metric computation, each object is normalized to the range [1, 1]. To ensure pose-agnostic evaluation, we rotate each object by [0, 90, 180, 270] degrees and report the best score among these orientations as the final metric. As shown in Table 3 and Figure 14 our method outperforms all baselines. Method SAMPart3D PartField HoloPart OmniPart Ours CD 0. 0.17 0.26 0.23 0.11 Fscore-0.1 0.68 0.59 0.63 0.80 Fscore-0.5 0. 0.57 0.43 0.46 0.71 Table 3: Shape Decomposition Results 10 Figure 14: Shape Decomposition Results."
        },
        {
            "title": "6 Polygon Generation with Auto-regressive Models",
            "content": "In this section, our goal is to generate clean topology for shapes produced by geometry generative models or given by users. Despite the delicate shapes produced in previous sections, they typically consist of huge amount of messy triangles and are hard to be directly applied in downstream applications (e.g., UV segmentation and rigging). Therefore, we leverage an auto-regressive model to directly predict vertices and faces from the point cloud of generated shapes. Our model is 11 trained in two stages, including pre-training and post-training as shown in Figure 15. Figure 15: Mesh-RFT Framework Overview. The pipeline comprises two stages: 1) Mesh Generation Pre-training using an Hourglass AutoRegressive Transformer and Shape Encoder; and 2) Reinforcement Post-training which employs Mask DPO with reference and policy networks for subsequent refinement. 6.1 Mesh Pretrianing with Efficient Tokenization and Architecture Mesh Tokenization. To model meshes with the next-token-prediction paradigm, the first step is to tokenize them into 1D sequences. We leverage the Blocked and Patchified Tokenization (BPT) (Weng et al., 2025) as the basic tokenization for meshes. Specifically, BPT combines two core mechanisms: 1) Block-wise Indexing partitions 3D coordinates into discrete spatial blocks, converting Cartesian coordinates into block-offset indexes to exploit spatial locality. 2) Patch Aggregation further compresses face-level data by selecting high-degree vertices as patch centers and aggregating connected faces into unified patches. Each patch is encoded as center vertex followed by peripheral vertices, reducing vertex repetition and enhancing spatial coherence. With BPT, both training and inference efficiency are significantly improved. Network Architecture. Our polygon generative model consists of point cloud encoder and an auto-regressive mesh decoder. The point cloud encoder is highly motivated by Michelangelo (Zhao et al., 2023) and Hunyuan3D series (Zhao et al., 2025; Hunyuan3D et al., 2025b), which applies the Perceiver (Jaegle et al., 2021) architecture to encode point cloud into condition tokens cp. Then, we leverage the Hourglass Transformer (Hao et al., 2024) as the mesh decoder backbone, conditioned on point cloud tokens by cross attention layers. Training & Inference Scheme. The mesh token distribution p(mi) is modeled with Hourglass Transformer with parameter θ, maximizing the log probability. The cross-attention is leveraged for various conditions cp. L(θ) = i=1 p(mim1:i1, cp; θ), (1) To further leverage the high-poly mesh data and improve training efficiency, we adopt the truncated training strategy (Hao et al., 2024). Specifically, for each training iteration, we randomly selected slice of mesh sequence with fixed number of faces (e.g., 4k faces). And in the inference stages, we apply the rolling cache strategy to reduce the gap between the training and inference stages. 6.2 Mesh Post-Training with Topology-Aware Masked DPO Preference Dataset Construction. We establish pipeline for constructing the preference dataset used in the second-stage fine-tuning, which consists of candidate generation, multi-metric evaluation, and preference ranking. For each input point cloud P, we generate eight candidate meshes {M1 } using the pre-trained model Gpre . Each candidate is evaluated using three metrics: Boundary Edge Ratio (BER) and Topology Score (TS) for topological quality, and Hausdorff Distance (HD) for geometric consistency. , . . . , M8 , θ 12 Figure 16: The effectiveness of the post-training stage. The post-training stage enhances the mesh completeness (Row #1) and connectivity (Row #2) and reduces the broken faces (Row #3). preference relation Mi Mj ) is defined if and only if: ) < BER(Mj ) > TS(Mj ) < HD(Mj ) from all pairwise comparisons to form the dataset. BER(Mi TS(Mi HD(Mi , ) ) (2) We compile preference triplets (P, M+ Masked Direct Preference Optimization. To address localized geometric imperfections and inconsistent face density, we leverage Masked Direct Preference Optimization (M-DPO) (Liu et al., 2025b), which extends DPO with quality-aware localization masks. We define binary masking function ϕ(M) {0, 1}M that identifies high-quality regions (value 1) versus low-quality regions (value 0) based on per-face quality assessment. Each region corresponds to subsequence in the block patch tokenization (BPT). subsequence is classified as high-quality region only if all faces within it have quad ratio above predefined threshold and the average topology score exceeds another threshold. Let Gref = Gpre is: θ be the frozen reference model and Gψ be the trainable policy. The M-DPO objective (cid:2)log σ (cid:0)βL+(P, M+ ) βL(P, )(cid:1)(cid:3) (3) LM-DPO(πψ; πref) = ,M where the positive and negative terms are: (P,M+ )D ) = log L+(P, M+ (cid:12) (cid:12)πψ(M+ (cid:12) (cid:12)πref(M+ (cid:12) (cid:12)πψ(M (cid:12) (cid:12)πref(M Here, denotes element-wise multiplication and 1 is the ℓ1 norm. M-DPO enables targeted refinement of low-quality regions while preserving satisfactory areas. )(cid:12) ) ϕ(M+ (cid:12)1 )(cid:12) ) ϕ(M+ (cid:12)1 ) (1 ϕ(M ) (1 ϕ(M ))(cid:12) (cid:12)1 ))(cid:12) (cid:12)1 L(P, ) = log (4) Figure 17: Generalization results on dense, out-of-distribution meshes. Our model demonstrates superior geometric fidelity and surface continuity, maintaining high-quality reconstruction even under complex and unseen input conditions. Figure 18: Part-aware polygon generation. With shapes segmented into several parts as input, our model can generate the corresponding meshes conditioned on partial point clouds separately without further fine-tuning. 14 6.3 Experiments Pre-training vs. post-training. In Figure 16, we show the improvement after the post-training. In our experiments, we found that the post-training stage is crucial for improving the completeness and topology quality of the generated meshes. Comparison with existing methods. As shown in Figure 17, we compare our model with existing polygon generation methods. Our model can generate much more complex meshes with significantly improved topology quality and stability. Part-aware polygon generation. With shapes segmented into several parts as input, our model can generate the corresponding meshes conditioned on partial point clouds separately without further fine-tuning as shown in Figure 18. This would be much easier for the model to generate the topology for complicated meshes."
        },
        {
            "title": "7 Semantic UV",
            "content": "The results of traditional UV unwrapping methods often lack semantic significance, which notably affects the quality of downstream texturing and the efficiency of resource utilization. Consequently, these traditional methods cannot be directly applied in professional pipelines, such as those used in game development and film production. To handle this challenge, we introduce SeamGPT, novel framework that generates artist-style cutting seams through an auto-regressive approach. Our method formulates surface cutting as sequence prediction problem, where cutting seams are represented as an ordered series of 3D line segments. Given an input mesh M, our goal is to generate seam edges = {si}i[Ns]. The overview of SeamGPT is shown in Fig. 19. We first introduce our seam representation strategy in Sec. 7.1, which encodes cutting seams as sequential tokens. In Sec. 7.2, we detail our auto-regressive generation process, which mimics the sequential decision-making of professional artists. Figure 19: SeamGPT architecture: Point cloud encoder extracts shape context; Causal transformer decoder generates axis-ordered seam coordinates. Color indicates the prediction order is of the seam segments (red to blue). 7.1 Mesh Seam Representation seam sequence of Ns segments {si}i[Ns] is defined as: = {s1, s2, . . . sNs }, where each segment si is 3D line segment represented by two vertices: si = (pi t), i.e. head and tail. Each vertex is defined by its 3D coordinates: = (x, y, z). Thus, seam sequence can be decomposed at multiple levels: h, pi = {s1, s2, . . . sNs } h, p2 h, p1 h, x1 h, = {p1 = {x1 , p2 h, z1 , . . . , pNh , z1 , y1 , pNh } , . . . , xNs , yNs Segment level Point level Coord. level (5) , zNs } 15 Seam ordering. For an auto-regressive model to function properly, consistent order of sequences is required. Following existing practice for mesh generation Siddiqui et al. (2023); Weng et al. (2024); Hao et al. (2024) and wireframe generation Ma et al. (2024), we first sort vertices yzx order, where represents the vertical axis, and then sort two vertices within an edge lexicographically, placing the lowest yzx-ordered vertex first. Finally, seam edges are sorted in ascending yzx-order based on the sorted values of their vertices. The resulting order can be seen through the color coding of the generated meshes presented in Figure 19, i.e. from red to blue. Quantization of coordinates. Autoregressive models typically sample from multinomial distribution over discrete set of possible values. To adhere to this convention, we quantize vertex coordinates into fixed number of discrete bins. The quantization resolutiondetermined by the number of binsdirectly affects the precision of the predicted seam. Higher quantization levels yield more detailed and accurate representations but also increase the complexity of the generation process. To balance precision and tractability, we employ 1024-level quantization, enabling effective representation of complex seams. 7.2 Autoregressive Seam Prediction In autoregressive seam prediction, seam sequence is generated by sequentially predicting each coordinate ci based on its conditional probability given all previously generated coordinates P(cic<i). The probability of the entire seam is then given by the joint probability of all its coordinates: 6Ns P(S) = (6) P(cic<i). i=1 Global Shape Conditioning. Point clouds are flexible and universal 3D representation that can be efficiently derived from other 3D formats, including meshes. We use point cloud encoder to extract representative features for characterizing the input 3D shapes. In the context of surface cutting, seams are encouraged to align with the vertices and edges of the original mesh, such that cutting the mesh along seams does not create excessive extra faces. To guide the decoder in producing vertex and edge-aligned seam placement, instead of sampling point clouds uniformly, we sample structural points only on vertices and along edges. Specifically, we sample total of 61,440 points, evenly split between: 30,720 points on vertices and 30,720 points on edges. If the input mesh has fewer than 30,720 vertices, we use repeated over-sampling. Points along an edge are sampled uniformly by interpolating between its start and end points with samples, where is determined based on the edges length. Finally, the input points are fed into jointly trained point cloud encoder from Team (2025), which processes the point cloud through series of crossand self-attention layers and compresses the point cloud to latent shape embedding of length 3072 and dimension 1024. Another option to create shape embeddings is to use mesh encoders, such as Zhou et al. (2020). However, the computational cost of mesh encoder does not scale well when the input has large number of vertices. We show in the ablation study that point cloud conditioning produces much better results than mesh conditioning. Seam Count control. Given an input shape, multiple possible suitable cutting solutions exist. Depending on the application requirements, one can make many cuts to decompose mesh, or just few. To regulate the cutting granularity, we concatenate length embedding to the shape embedding. We find that modulating the length embedding directly controls cutting granularity. HourGlass Decoder Architecture. Following Hao et al. (2024), we build an hourglass-like autoregressive decoder architecture to sequence at multiple levels of abstraction. The architecture employs multiple Transformer stacks at each level, with transitions between levels managed by causality-preserving shortening and upsampling layers that bridge these hierarchical stages. There are three hierarchical levels: coordinates, vertices, and edges. The input coordinate sequence is shortened by factor of 3 at the vertex level. It is then further shortened by factor of 2 at the edge level. Both shortening and upsampling layers are implemented to preserve causality. The expanded sequence is combined with higher-resolution sequences from earlier levels via residual connections, similar to U-Nets. Training Strategy. We employ two loss functions to for model training: cross-entropy loss for token prediction and KL-divergence loss to regularize the shape embedding space, ensuring it remains compact and continuous. Training begins with 2,000-step warm-up phase and is parallelized across 64 Nvidia H20 GPUs (98GB Mem.) with total batch size of 128. The model converges after one week of training. During training, we first scale all samples to fit within cubic 16 Figure 20: Qualitative UV flatten results on FAM benchmark ( Nefertiti, Cow, and Fandisk). bounding box in the range of 1 to 1. We then apply data augmentation techniques, including random scaling within [0.95, 1.05], random vertex jitter, and random rotation. Bimba Lucy Ogre Armadi. Bunny Nefert. Dragon Homer Happy Fandi. Spot Arm Cow Avg. Xatalas Young (2024) 15.44 0.01 0.66 0. Nuvo Srinivasan et al. (2024) 19.12 57.89 26.22 114.21 FAM Zhang et al. (2024b) 12.10 35.14 11.55 Edge-CLS Ours 8. 22.85 23.54 10.68 0.01 2.01 59. 11.18 2.47 61.84 16.84 7.33 3. 50.47 0.03 20.92 11.21 4.67 0. 0.22 61.03 904.89 15.39 0.56 7. 21.92 14.19 20.16 10.28 99.84 8. 12.77 29.98 1.94 18.37 267.43 19. 12.93 37.34 12.70 52.95 23.00 27. 61.68 12.21 12.47 8.15 9.37 5. 5.95 20.98 87.20 14.88 8.49 9. 2.24 86.95 19.37 13.04 Table 4: Quantitative results on Flatten-Anything benchmark using the face distortion metric. Xatalas Young (2024) Nuvo Srinivasan et al. (2024) FAM Zhang et al. (2024b) Ours Bowl 0.91 3. 3.80 0.49 Ball 0.26 1.33 0. 0.31 Sheep Driver Chicken Apple Giraffe Bottle Avg. 1.19 10.43 6.45 1. 4.61 33.07 15.33 4.25 2.36 9. 18.98 1.86 3.11 15.39 6.64 4. 2.85 21.04 11.77 2.59 0.57 6. 4.36 0.67 1.98 12.63 8.52 1. Table 5: Quantitative results on Toys4K Benchmark using the face distortion metric."
        },
        {
            "title": "7.3 Experiment",
            "content": "Benchmarks and Evaluation Metric. We conduct experiments on diverse collection of 3D surface models from Flatten Anything (FAM) Zhang et al. (2024b), which primarily includes low-poly meshes, CAD models, and 3D scanned meshes. We also evaluate on Toys4K Stojanov et al. (2021), dataset of non-manifold artist-created meshes. Our evaluation leverages the Mesh distortion metrics, which is computed as the average conformal energy over all triangular faces of the mesh. Baselines and Implementations. We compare SeamGPT against several state-of-the-art methods for mesh UV-unwrapping. XAtlas Young (2024) employs bottom-up approach with bounded distortion charts. Nuvo Srinivasan et al. (2024) leverages neural fields with explicit parameterization constraints. FAM Zhang et al. (2024b) implements interpretable sub-networks in bi-directional cycle mapping framework. We also built another baseline called Edge-CLS, which takes mesh as input and uses graph convolution and Transformer layers to compute per-edge features. These features are then fed into an MLP classifier to predict whether each edge is seam edge or not (i.e., this is an edge classification baseline). We train Edge-CLS on the same training set and use the same UV-unwrapping process as SeamGPT. 17 SeamGPT-based UV-unwrapping. Once SeamGPT generates cutting seams, we implement streamlined unwrapping process to create practical UV maps. We first map each predicted seam point to its nearest vertex on the input mesh, then connect these vertices through shortest geodesic paths along the mesh edges. We then cut the mesh by duplicating vertices along these paths, creating independent boundaries for flattening. Finally, we apply Blenders Minimum Stretch algorithm to the segmented mesh, optimizing UV coordinates to evenly distribute stretching while preserving the semantic structure defined by our seams. This process yields low-distortion UV mappings that respect functional and aesthetic boundaries, improving upon conventional automated methods. Comparison results. Tables 4 and 5, along with Figure 20, present qualitative and quantitative results. SeamGPT achieves the best performance across all metrics. In contrast: XAtlas generates over-fragmented cuts, FAM fails to produce subtle cuts consistently, Edge-CLS performs well only on sharp edge features but struggles with generating seams on smooth, featureless regions. Our method consistently produces semantic and reasonable cuts regardless of surface characteristics. User study. To further assess our methods practical utility, we conducted user study with 20 professional 3D artists evaluating Boundary quality and Editability. Boundary quality measures how unfragmented UV map is, while editability reflects how well the mapping supports appearance editing. Participants rated UV unwrappings from all methods on 5-point scale. As shown in Table 6, SeamGPT significantly outperforms existing methods in both metrics. Xatalas Young (2024) Nuvo Srinivasan et al. (2024) FAM Zhang et al. (2024b) Edge-CLS Ours Fandisk Cow Nefertiti Avg. Boundary 4.42 1.32 1.74 3.89 4. Editability 4.42 1.32 1.53 3.84 4.32 Boundary 2.68 1.16 1.84 2.79 4.16 Editability 2.37 1.21 1.53 2.32 4.16 Boundary 2.79 1.42 2.05 2.58 3.47 Editability 2.47 1.42 1.84 2.16 3.58 Boundary 3.30 1.30 1.88 3.09 4. Editability 3.09 1.32 1.63 2.77 4.02 Table 6: User Study about Boundary quality and Editability. 7.4 Ablation Study Point cloud sampling strategy. As shown in Figure. 21, when conditioned on point clouds uniformly sampled across the mesh surface, the generated seams remain logically valid from surface-cutting perspective but may not precisely align with the input meshs vertices and edges. In contrast, sampling point clouds along edges and vertices produces seams that naturally conform to the mesh topologies. This could prevents creating excessive extra mesh faces. We also found that sampling along edges and vertices significantly improves model convergence, as the transformer gains explicit positional awareness of potential cutting coordinates. Figure 21: Ablation of point sampling strategy. Mesh encoder vs Point cloud encoder. An alternative approach for generating shape embeddings employs mesh encoders, as demonstrated by Zhou et al. Zhou et al. (2020). We implemented an encoder combining graph convolutions (operating on both vertices and edges) with full self-attention transformer. This encoder produces vertex-wise tokens that are subsequently fed to the decoder via cross-attention mechanisms. As shown in Figure 22, pointcloud encoder yields superior results compared to mesh encoders. Furthermore, the computational cost of our mesh encoder scales poorly with increasing vertex counts. Mesh encoderbased methods often fail to accurately capture the precise positions of original vertices, resulting in significant misalignment between the Figure 22: Ablation study of encoder and decoder. 18 generated seam edges and the original mesh. Does Pointer networks works? In the case that the cutting seam forms subset of the edges in the mesh, we can also adopt the Pointer Network Vinyals et al. (2015) architecture, which auto-regressively produce the pointers to the mesh edges. We follow the implementation of Polygen Nash et al. (2020) to build pointer network with mesh encoder that produces edgewise embedding and casual transformer to create pointers to the edges that lie on the seams auto-regressively. Pointer network struggles to generate consistent seams, often resulting in discontinuous cuts as demonstrated in Figure 22. Seam length control and diversity. We define as the ratio of seam segment count to the number of mesh vertices. Empirically, valid cutting seams typically has value within the range [0.1,0.35]. Above this range result in over-cutting, while values below it lead to insufficient cuts. As shown in Figure 23, controlling allows us to adjust the granularity of the cuts. Additionally, due to the non-deterministic nature of autoregressive transformers, we can generate diverse valid cutting seams from the same length control. Figure 23: Seam length control and diversity. We can control the cutting granularity by adjusting seam length. Diverse valid cutting seams can be generated."
        },
        {
            "title": "8 Texture Generation and Editing",
            "content": "Texture generation and editing technologies have critical importance in 3D asset creation. Physically Based Rendering (PBR) workflows rely on accurate texture maps to emulate real-world material behaviors under varying lighting conditions. High quality texture bridge the gap between geometric abstraction and perceptual realism, enhencing immersion and aesthetic coherence. we introduced high-fidelity texture synthesis methodology in Zhao et al. (2025); Hunyuan3D et al. (2025a); Lai et al. (2025a) that lift 2D diffusion model into geometry-conditioned multi-view generative model i, subsequently baking its outputs into high-resolution texture maps via view projection. This framework systematically addresses two critical challenges in multi-view based texture generation: Cross-view consistency and geometric alignment in Feng et al. (2025). Expansion of RGB textures into photorealistic PBR material textures in He et al. (2025). In this report, we extend our texture generation framework into comprehensive system supporting multimodal texture editing. First, we augment our existing multi-view PBR material generation model to accommodate text and image-guided multimodal editing. Second, we propose materialbased 3D segmentation method that generates part-wise material segmentation maps from input geometry-only meshes, enabling localized texture editing. Finally, we introduce 4K material ball generation model that synthesizes high-resolution tileable texture ballsincluding Base Color, Metallic, Roughness, and Normal mapsfrom textual prompts, facilitating professional artistic workflows. Figure 24: visualization results of multimodal texture editing: Perform textand image-guided editing on textured meshes. 8.1 Multimodal Texture Editing We introduce text-guided texture editing model trained on meticulously curated dataset of 80k high-quality 3D assets with PBR materials. These assets were rendered into multi-view HDR images, and Vision-Language Model (VLM) was employed to generate descriptive captions for textures and editing instructions. Leveraging the Flux Kontext framework, we constructed extensive image editing pairs across multiple viewpoints. Our texture foundation model then inferred consistent multi-view textures from these pairs, synthesizing large-scale corpus of text-texture pairs for fine-tuning the editing model. During training, we followed flux kontext to unify textual prompts and reference image features into joint latent sequence. Starting from base texture generation model, the system was optimized end-to-end using 30,000 text-texture 20 pairs, resulting in unified model capable of texture synthesis and editing under both textual and visual guidance. We introduce streamlined Mixture of Experts (MoE) architecture for the image-guided texture editing model to handle diverse image inputs. To determine whether an input image aligns with the target geometry, we compute the CLIP similarity between geometrically rendered views and the input image. If the guidance image exhibits high geometric correspondence with the target mesh, we inject image features via VAE encoder; otherwise, we use CLIP image embeddings for feature infusionanalogous to IP-Adapters methodology. This adaptive conditioning mechanism ensures robust texture editing under arbitrary image conditions. The fascinating multimodal editing results are demonstrated in Fig. 24, indicating that we can perform material editing with diverse styles on objects in games, such as props and characters, both globally and locally. 8.2 4K Matertial Map Generation We innovatively adapt the 3D VAE framework 25originally designed for encoding continuous video framesto compress multi-domain material data (renders, base color, bump, roughness, metallic, etc.) into unified latent representations, enabling scalable 4K-resolution texture synthesis. Specifically, we fine-tune the 3D VAE using textured 3D assets to achieve domain-invariant feature extraction, resulting in PBR-VAE module. Subsequently, we fine-tune 3D Diffusion Transformer (DiT) with material ball datasets to establish the core architecture of our material ball generation model. Figure 25: Visualization of material generation framework."
        },
        {
            "title": "9 Animation Module",
            "content": "9.1 Method overview In this section, we present the animation module, which is composed of two main branches: the humanoid character animation module and the general character animation module. Each character input is first processed by detection module. If the input is identified as humanoid character, it is directed to the humanoid animation branch; otherwise, it is routed to the general branch. The humanoid branch consists of template-based auto-rigging module and motion retargeting module. To balance the accuracy of skeleton generation with ease of use, we adopt 22 body joints as the template skeleton. We follow Guo et al. (2025b) to construct our rigging and skinning model. However, unlike Guo et al. (2025b), which does not incorporate rig-related information during skinning prediction, our model integrates both skeletal and vertex features to achieve more 21 Figure 26: Comparison with UniRig Zhang et al. (2025). Left: results of rigging for general character using our generated mesh. Right: results of skinning applied to the general character input. accurate results. In addition, our system includes pose standardization module that converts user-provided models in arbitrary poses into canonical T-pose. Feeding T-pose models into the motion retargeting module yields more reliable and precise outcomes. In contrast, the general branch integrates an autoregressive skeleton generation module with geometry topology-aware skinning module. Since general characters vary in both skeletal topology and the number of joints, most existing approaches to skeleton generation are based on autoregressive techniques, such as Song et al. (2025); Zhang et al. (2025); Guo et al. (2025a); Liu et al. (2025a). These architectures have already demonstrated stability and accuracy in skeleton generation tasks, and our module is built upon these autoregressive methods. With respect to the skinning module, prior algorithms typically consider only mesh vertices and skeletal joints as input features, while paying little attention to the topological relationships among them. In contrast, our skinning module explicitly incorporates these topological relationships, leading to more robust and stable results. 9.2 Implement details We used internally purchased and manually annotated datasets, which consist of approximately 80,000 high-quality general-character samples and 10,000 humanoid samples. All modules employ Zhao et al. (2023) as the mesh encoder, while the autoregressive module is implemented using OPT-350M Zhang et al. (2022) as the transformer backbone. For the humanoid auto-rigging and skinning module, we apply motion-based data augmentation during training. The model is trained with batch size of 6 on 8 H20 GPUs for 3 days. For the general-purpose rigging module, we train with batch size of 16 on 24 H20 GPUs for 2 days. For the general-purpose skinning module, we adopt batch size of 16 and train on 8 GPUs for 2 days. 9.3 Qualitative results As shown in Figure 26, our method produces more detailed results with fewer errors on general characters. Moreover, for the general skinning task, since our algorithm incorporates both skeletal and mesh topology information, it achieves higher overall accuracy compared to existing approaches."
        },
        {
            "title": "10 Conclusion",
            "content": "Hunyuan3D Studio represents paradigm shift in 3D content creation by integrating generative AI into seamless, end-to-end pipeline that transforms single-image or text inputs into game-ready assets with optimized geometry, PBR textures, and engine-compatible topology. Our systems core innovation lies in its modular yet unified architecture, which combines advanced neural models for geometry generation, component-aware segmentation, automated retopology (PolyGen), semantic UV unwrapping, and texture synthesisall orchestrated to bridge the gap between creative intent and technical execution. Experimental validation confirms that assets generated by Hunyuan3D 22 Studio meet the stringent requirements of modern game engines while significantly reducing production time and technical barriers. By democratizing access to high-quality 3D content, the platform empowers both artists and developers to iterate rapidly and focus on creativity rather than manual workflows."
        },
        {
            "title": "11 Contributors",
            "content": "Project Sponsors: Jie Jiang, Linus, Yuhong Liu, Di Wang, Tian Liu, Peng Chen Project Leaders: Chunchao Guo, Zhuo Chen Core Contributors: PolyGen: Biwen Lei, Jing Xu, Yiling Zhu, Haohan Weng, Jian Liu, Zhen Zhou, Jiankai Xing Part: Yang Li, Jiachen Xu, Changfeng Ma, Xinhao Yan, Yunhan Yang, Chunshi Wang UV: Xinhai Liu, Duoteng Xu, Xueqi Ma, Yuguang Chen, Jing Li Texture: Shuhui yang, Mingxin Yang, Sheng Zhang, Yifei feng, Xin Huang, Di Luo, Zebin He Animation: Lixin Xu, Puhua Jiang, Changrong Hu, Zihan Qin, Shiwei Miao Geometry: Jingwei Huang, Haolin Liu, Yunfei Zhao, Zeqiang Lai, Qingxiang Lin, Zibo Zhao, Kunhong Li, Huiwen Shi Image: Ruining Tang, Xianghui Yang, Xin Yang, Yuxuan Wang, Zebin Yao Contributors: Engineering: Yihang Lian, Sicong Liu, Xintong Han, Wangchen Qin, Caisheng Ouyang, Jianyin Liu, Tianwen Yuan, Shuai Jiang, Hong Duan, Yanqi Niu, Wencong Lin, Yifu Sun, Shirui Huang, Lin Niu, Gu Gong, Guojian Xiao, Bojian Zheng, Xiang Yuan, Qi Chen, Jie Xiao, Dongyang Zheng, Xiaofeng Yang, Kai Liu, Jianchen Zhu Data: Lifu Wang, Qinglin Lu, Jie Liu, Liang Dong, Fan Jiang, Ruibin Chen, Lei Wang, Chao Zhang, Jiaxin Lin, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Yinhe Wu, Jiayao Du, Jupeng Chen Art Designer: Xinyue Mao, Dongyuan Guo, Yixuan Tang, Yulin Tsai, Yonghao Tan, Jiaao Yu, Junlin Yu, Keren Zhang, Yifan Li"
        },
        {
            "title": "References",
            "content": "Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv 2506, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas uller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. Yifei Feng, Mingxin Yang, Shuhui Yang, Sheng Zhang, Jiaao Yu, Zibo Zhao, Yuhong Liu, Jie Jiang, and Chunchao Guo. Romantex: Decoupling 3d-aware rotary positional embedded multiattention network for texture synthesis, 2025. URL https://arxiv.org/abs/2503.19011. Jingfeng Guo, Jian Liu, Jinnan Chen, Shiwei Mao, Changrong Hu, Puhua Jiang, Junlin Yu, Jing Xu, Qi Liu, Lixin Xu, et al. Auto-connect: Connectivity-preserving rigformer with direct preference optimization. arXiv preprint arXiv:2506.11430, 2025a. Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, and Ran Zhang. Make-itanimatable: An efficient framework for authoring animation-ready 3d characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025b. Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, and Wenhan Luo. Materialmvp: Illumination-invariant material generation via multi-view pbr diffusion, 2025. URL https://arxiv.org/abs/2503.102 89. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material, 2025a. URL https://arxiv.org/abs/2506.15442. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025b. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 46514664. PMLR, 2021. Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025a. Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qingxiang Lin, Jingwei Huang, Yuhong Liu, et al. Unleashing vecset diffusion model for fast shape generation. arXiv preprint arXiv:2503.16302, 2025b. Biwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, and Xuansong Xie. Diffusiongan3d: Boosting text-guided 3d generation and domain adaptation by combining 3d gans and diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1048710497, 2024. 24 Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. URL https://arxiv.org/abs/2506.05573. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi. Riganything: Template-free autoregressive rigging for diverse 3d assets. ACM Transactions on Graphics (TOG), 44(4):112, 2025a. Jian Liu, Jing Xu, Song Guo, Jing Li, Jingfeng Guo, Jiaao Yu, Haohan Weng, Biwen Lei, Xianghui Yang, Zhuo Chen, et al. Mesh-rft: Enhancing mesh generation via fine-grained reinforcement fine-tuning. arXiv preprint arXiv:2505.16761, 2025b. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond, 2025c. Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. Xueqi Ma, Yilin Liu, Wenjun Zhou, Ruowei Wang, and Hui Huang. Generating 3d house wireframes with semantics. In European Conference on Computer Vision, pp. 223240. Springer, 2024. Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning, pp. 72207229. PMLR, 2020. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoderonly transformers. arXiv preprint arXiv:2311.15475, 2023. Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, and Guosheng Lin. Magicarticulate: Make your 3d models articulation-ready. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 1599816007, June 2025. Pratul Srinivasan, Stephan Garbin, Dor Verbin, Jonathan Barron, and Ben Mildenhall. Nuvo: Neural uv mapping for unruly 3d representations. In European Conference on Computer Vision, pp. 1834. Springer, 2024. Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. CVPR, 2021. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. Tencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. 25 Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015. Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C. L. Philip Chen. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1109311103, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, and Chunchao Guo. X-part: high-fidelity and structure-coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, YanPei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025a. Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025b. Jonathan Young. xatlas - mesh parameterization / uv unwrapping library, 2024. URL https: //github.com/jpcy/xatlas. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1197511986, 2023. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. One model to rig them all: Diverse skeleton rigging with unirig. arXiv preprint arXiv:2504.12451, 2025. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024a. Qijian Zhang, Junhui Hou, Wenping Wang, and Ying He. Flatten anything: Unsupervised neural surface parameterization. arXiv preprint arXiv:2405.14633, 2024b. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. 26 Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, and Yaser Sheikh. Fully convolutional mesh autoencoder using efficient spatially varying kernels. Advances in neural information processing systems, 33:92519262, 2020."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan Hunyuan3D Studio"
    ]
}