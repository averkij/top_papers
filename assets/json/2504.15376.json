{
    "paper_title": "Towards Understanding Camera Motions in Any Video",
    "authors": [
        "Zhiqiu Lin",
        "Siyuan Cen",
        "Daniel Jiang",
        "Jay Karhade",
        "Hewei Wang",
        "Chancharik Mitra",
        "Tiffany Ling",
        "Yuhan Huang",
        "Sifan Liu",
        "Mingyu Chen",
        "Rushikesh Zawar",
        "Xue Bai",
        "Yilun Du",
        "Chuang Gan",
        "Deva Ramanan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like \"follow\" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video."
        },
        {
            "title": "Start",
            "content": "Zhiqiu Lin1 Chancharik Mitra1 Siyuan Cen2 Daniel Jiang1 Jay Karhade1 Hewei Wang1 Tiffany Ling1 Yuhan Huang1 Sifan Liu3 Mingyu Chen4 Rushikesh Zawar5 Xue Bai5 Yilun Du6 Chuang Gan7 Deva Ramanan1 4Emerson 2UMass Amherst 7MIT-IBM 6Harvard 5Adobe 1CMU 3USC 5 2 0 2 1 2 ] . [ 1 6 7 3 5 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce CameraBench, large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of 3,000 diverse internet videos, annotated by experts through rigorous multi-stage quality control process. One of our contributions is taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like follow (or tracking) require understanding scene content like moving subjects. We conduct large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then finetune generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video. Project page: https://linzhiqiu. github.io/papers/camerabench 1. Introduction We must perceive in order to move, but we must also move in order to perceive. J. J. Gibson, The Ecological Approach to Visual Perception [14] Humans perceive the visual world through movement. Motion parallax [47], for instance, enables precise depth perception essential for navigating the physFigure 1. Examples of camera movements. We show videos with their camera trajectories: tracking shot of toddler (row 1), Hitchcocks dolly zoom effect (row 2), Spielbergs dramatic pan and tilt in Jurassic Park (row 3), Nolans roll shot in Inception (row 4), pedestal-up shot from The Legend of Zelda (row 5), and selfie by an amateur photographer, arcing to showcase the scenery while centering themselves (row 6). Please watch the videos at our website. ical world [13]. Similarly, camera motion is crucial for modern vision techniques that process videos of dynamic scenes. For example, Structure-from-Motion (SfM) [48, 56, 69] and Simultaneous Localization and Mapping (SLAM) [8, 12, 52] methods must first estimate camera motion (pose trajectory) to reconstruct the scenes in 4D. Likewise, without understanding camera motion, video-language models (VLMs) [54, 64, 66] would not fully perceive, reason about, or generate video dynamics. Human perception of camera motion. Understanding camera motion comes naturally to humans because we intuitively grasp the invisible subject the camera operator who shapes the videos viewpoint, framing, 1 Figure 2. Taxonomy of camera motion primitives. Our taxonomy, developed in collaboration with cinematographers and vision researchers, is the first to comprehensively capture camera motion across object-, ground-, and camera-centric reference frames, using precise cinematography terms [9] to eliminate ambiguity. It covers camera steadiness, translation, rotation, intrinsic changes, and common object-centric movements, all detailed in this paper. We refine the taxonomy iteratively over three months by actively annotating diverse videos and incorporating feedback from vision researchers and cinematographers to ensure both accuracy and completeness. and narrative. For example, in video tracking childs first steps, one can feel parents joy and excitement through their handheld, shaky movement. Professional cinematographers and filmmakers even use camera motion as tool [9, 51] to enhance visual storytelling and amplify the emotional impact of their shots. Hitchcocks iconic dolly zoom moves the camera forward while zooming out, maintaining the subjects framing while altering the background to create the impression of vertigo. In Jurassic Park (1993), Spielberg uses slow upward tilt and rightward pan to evoke sense of awe as the protagonists (and the audience) first see the dinosaurs. In Inception (2010), Nolan uses camera roll to mirror shifting gravity, blurring the line of reality. Similarly, game developers use camera movement to enhance player immersion. In Legend of Zelda: Breath of the Wild (2017), smooth pedestal-up shot transitions from the characters viewpoint to breathtaking aerial view, hinting at the journey ahead. Even amateur photographers use camera motion as tool; for example, selfie videos allow one to play the role of both the cinematographer and the subject. See Figure 1 for videos. contrast, classic computer vision methods learn camera motion from what is visible in the frame, relying on techniques like SfM and SLAM to estimate camera poses from video sequences. While these geometry-based approaches perform well on simple, static scenes, it is unclear how well they generalize to dynamic, real-world videos due to the difficulty of separating camera motion from scene dynamics [33, 58]. Moreover, these approaches do not capture the high-level semantics of camera motion [51], such as the intent behind shot (e.g., tracking subject or revealing scene) or the context in which the motion occurs (e.g., handheld, gimbalstabilized, or vehicle-mounted). On the other hand, recent multimodal vision systems like GPT-4o and Gemini [41, 45, 54] show strong human-like perceptual capabilities through large-scale training, yet their ability to understand camera motion remains largely untested. Inspired by these end-to-end approaches, we propose data-driven framework for benchmarking and developing models that can perceive camera motion as humans do. While this seems straightforward, it presents conceptual and technical challenges that are discussed next. Computational approaches to camera motion. In Challenges. While humans can easily perceive cam2 era motion, mapping this perception to data annotations is inherently challenging. First, camera motion can be ambiguous without specified reference frame of motion. For example, people might describe birds-eye-view camera moving forward along its own axis as moving downward because it descends toward the ground. In fact, humans tend to describe camera motion based on the scene or object context, such as saying The camera is following the subject in tracking shot, while the camera actually leads the subject by moving backward (row 1 of Figure 1). In addition, many common camera movement terms are misused. For example, amateurs often confuse terms like zoom-out and dolly-out, even though the former refers to adjusting the lens (an intrinsic change), while the latter involves physically moving the camera backward (an extrinsic change). Beyond these conceptual challenges, annotating real-world videos (e.g., from the internet) is also technically difficult. While prior work often treats camera motion as multi-class or multi-label classification task [1, 22, 44], internet videos often exhibit more complex motion dynamics. For example, drone shot might smoothly move forward before abruptly reversing direction mid-flight, making it difficult to classify as either dolly-in or dolly-out. As such, we collaborate with team of dozen computer vision researchers and professional cinematographers (including some authors of this paper) highly skilled in camera movements to address these challenges by constructing precise taxonomy of camera motion primitives and developing robust and scalable annotation framework, which we explain next. Building taxonomy of motion primitives. Drawing from professional filmmaking guides [9, 11, 51] and months of discussions between vision researchers and cinematographers, we construct unified taxonomy (Figure 2) that includes: (1) three reference frames of camera motion (e.g., object-, ground-, and camera-centric), and (2) precise terminology covering camera translation (e.g., moving upward), rotation (e.g., rolling clockwise), intrinsic change (e.g., zooming in), circular motion (e.g., arcing), steadiness (e.g., static, smooth, or shaky), tracking shots (e.g., side-tracking), and more. We iteratively refine the taxonomy by sourcing diverse internet videos, annotating them, and incorporating feedback from cinematographers in the film industry. Designing an annotation framework. To comprehensively annotate camera motion, straightforward approach is to frame each aspect as classification task [1, 22], e.g., Does the camera pan right or left? with options like pan-right, pan-left, or no-pan. However, real-world videos often contain conflicting or ambiguous motions, making direct classification challenging. To make it more flexible, some recent work collects natural language descriptions [32, 57] Figure 3. Data curation. We design our taxonomy through collaboration with vision researchers and cinematographers in an iterative process sourcing diverse internet videos, annotating them, and refining the taxonomy to address missing labels and improve consensus. Our annotation framework is built to handle both simple, clear motions and complex, ambiguous ones. Similar to previous efforts in crowdsourced annotation [43], we find large gap in accuracy between novices and experts (in our case, cinematographers). Interestingly, we find that proper training program and instruction can nearly bridge the gap. See Section 3 for details. instead. However, we find this caption-only approach error-prone. For instance, when camera translation dominates video, annotators often overlook rotation in their descriptions. To address these challenges, we adopt label-then-caption approach. First, annotators determine whether the camera motion is clear and consistent. If so, they classify each aspect directly. If motion is ambiguous or conflicting, they only answer the questions they are confident about, leaving others as am not sure. These unanswered questions are excluded from the final dataset. Next, we ask annotators to provide natural language description to capture conflicting movements (e.g., The camera first pans left, then right) or uncertain cases (e.g., The background is too dark to perceive any camera movement). To better understand the role of camera motion in visual storytelling, we encourage annotators to describe why the camera moves in particular way whether it follows subject, reveals scene, or enhances immersion (e.g., first-person camera moves forward as the person walks, with slight panning as they look around). We show that this framework enables humans to precisely annotate any video regardless of how complex the camera motion is. Collecting diverse videos. Existing datasets that involve camera motion often have limited coverage of videos, featuring only static scenes [71], narrow domains (e.g., only movies [1, 22]), or unedited footage [16]. To capture the diversity of real-world camera motions, we source internet videos of diverse genres (e.g., nature, human activity, films, advertisements, video games), types (e.g., 2D, 3D, synthetic, real), point of view, capturing devices, and post-production effects (e.g., overlays). Unlike existing datasets [1] that use automatic shot segmentation tools [50], we manually segment videos to single, continuous shots for precise annotation. 3 (a) Human study (b) Human performance during training programs Figure 4. Human study and training program. We hire 100 participants from diverse backgrounds, including non-expert with limited knowledge about camera movements and experts from the filmmaking industry with hands-on cinematography experience. Figure (a) shows the average accuracy of both groups in selecting motion primitives on 30 videos, where experts clearly outperform non-experts. In addition, around 80% of participants who review our multimodal guidelines (including textual definitions, video examples, and edge cases) significantly outperform the remaining 20% who only see textual definitions. Figure (b) shows that extended practice with detailed error feedback boosts accuracy for all participants. We hire only those who complete all five rounds (with 30 videos each) to annotate our dataset. We exclude complex (ambiguous/conflicting) motions from accuracy calculations. Human study for scalable annotation. While developing the taxonomy (over three months), our team of cinematographers and vision researchers secured core set of 800 labeled videos. Each video is labeled by at least 5-8 experts, with disagreements resolved through discussion until consensus is reached. We then use these verified annotations to conduct human study with over 100 participants from diverse backgrounds half with cinematography experience (professional cinematographers and film school students) and half without (graphic/UI/UX designers, freelancers, and college students from fields like literature and computer science). Figure 4 shows that participants with cinematography experience (experts) outperform non-experts by more than 15% in accuracy. To ensure quality annotations at scale, we implement rigorous screening and training program. First, we develop detailed guidelines with textual definitions, video examples, and complex edge cases. Next, incoming annotators attend lectures given by the authors and complete five more rounds of exams, each with 30 videos. After each exam, we generate PDF with feedback on errors to help annotators correct 4 any misunderstandings. Our results show that our training program improves the accuracy of both expert and non-expert annotators by 10-15%. We only hire those who successfully complete all the training (averaging 20 hours) and continuously monitor their performance through random audits. For any disagreements, we hold feedback sessions and revise annotations to reach consensus. As of this writing, we have 150K binary labels across 3,000 fully annotated videos, and we plan to continue expanding this dataset. We will open-source all data and details about our training program. CameraBench. We introduce CameraBench to benchmark and develop models for human-like understanding of camera motion, using our initial set of videos (each reviewed by at least one author during the quality control phase). Our comprehensive annotations, which include both labels and captions, allow us to evaluate models on wide range of tasks, including binary classification of motion primitives, video-text retrieval, video captioning, and video question-answering (VQA). We evaluate diverse set of 20 methods, including discriminative [29, 30, 34, 45, 60] and generative VLMs [4, 28, 35, 41, 54, 68], and SfM/SLAM [33, 56, 58] methods. Although not all models can perform every task (e.g., SfM/SLAM cannot perform VQA tasks or reason about object-centric frames), we ensure fair comparisons by carefully designing the benchmarking protocol. For SfM/SLAM, we set up heuristics to rank scores and ensure axis alignment with conventions. For discriminative VLMs, we evaluate both CLIPScore [19] and ITMScore [29] for classification and retrieval tasks. For generative VLMs, we evaluate their captioning, VQA, and discriminative scoring using VQAScore [36]. For VLMs, we carefully design the prompts (e.g., sentence descriptions and question-answering formats). We will release the code for all experiments. Findings. We find that classic SfM/SLAM methods [48] often fail to handle dynamic or low-parallax scenes (e.g, when the camera is stationary or only rotating), thus struggling with even classifying basic motion primitives (e.g., Is the camera moving up or not?). We also observe that recent learning-based SfM/SLAM methods like MegaSAM [33, 58] handle dynamic scenes much better and outperform the classic COLMAP [48] by 1-2x. However, they may still confuse camera motion with object or scene motion in complex scenarios. We argue that our benchmark serves as reality check for future SfM/SLAM methods before they are applied to dense trajectory estimation tasks and helps identify areas for improvement in these methods. On the other hand, we find that generative VLMs show promise in understanding camera motion, particularly in tasks requiring semantic reasoning. This motivates us to use our dataset to post-train VLMs for better camera motion understandFigure 5. Example annotations. Our videos are fully annotated with binary labels for 50 motion primitives from our taxonomy, along with natural language description capturing key aspects of camera movement. See the supplement for videos. ing. With our small-scale yet high-quality fine-tuning data, we show that VLMs can achieve 1-2x improvements across both discriminative and generative tasks. Contributions. We take the first step toward achieving human-like perception and understanding of camera motion by: (1) introducing comprehensive taxonomy of camera motion primitives, developed in collaboration with computer vision researchers and cinematographers, (2) designing robust annotation framework and structured training program to improve annotation quality, (3) curating dataset and benchmark featuring real-world videos of dynamic scenes across diverse genres, and (4) analyzing strengths and limitations of existing models to guide future research. We will open-source all data, models, code, taxonomy, training guidelines, and analyses to advance research in video understanding. 2. Related Work Camera motion in vision datasets. Existing datasets typically represent camera motion in three ways: (1) Camera trajectory. Per-frame camera poses provide geometric description of motion, but obtaining groundtruth trajectories for real-world dynamic scenes is nearly impossible. For example, datasets [7, 21, 24, 37, 71] like RealEstate10K rely on multi-view geometry methods [48] to estimate pseudo ground-truth trajectories, and they are mostly limited to static scenes. To achieve more accurate trajectories, some datasets use simulators with camera control to generate synthetic videos [23, 49]. However, camera trajectories only offer camera-centric view of motion, ignoring object and scene context. (2) Motion labels. Datasets with discrete labels often suffer from limited categories. MovieNet [22, 46] defines only four types of movements and focus solely on movies. AVE [1] expands the taxonomy but confuses rotation as translation (e.g., grouping pan and truck) and intrinsic as extrinsic change (e.g., grouping dolly and zoom). We also find that AVE contains contradictory annotations, such as videos labeled as both static and pan. Recent datasets [32, 53] add object-centric motion labels like arc shots but force videos into single label, failing to capture co-occurring motions. (3) Motion descriptions. Recent video-language models [20, 32, 57] leverage human-collected motion descriptions, but their datasets, taxonomies, or annotation guidelines are either not open-source or undocumented. Camera motion in generative models. Our study is partly inspired by the growing interest in incorporating camera movement into video generative models. For instance, text-to-video generation models [62, 65] often learn camera control using synthetic camera movements, or are trained and evaluated on largely static scenes with SfM-estimated camera trajectories [2, 3, 6, 18, 25, 31, 39, 49, 61, 63, 64, 70, 72]. Yet, it remains unclear whether SfM can reconstruct accurate trajectories for real-world or synthetic videos. More recently, closed-source models like MovieGen [44] train in-house classifiers to augment captions with camera motion labels, and Goku [5] trains captioner [66] to generate camera motion descriptions. 3. Taxonomy Design and Data Curation We collaborate with cinematographers who use precise terminology for camera movements [9] to iteratively develop our taxonomy and annotation framework. Taxonomy design. Our cinematographers come from diverse backgrounds, including film school students and professionals with over 10 years of experience from the US and China. To develop the taxonomy, we take hands-on approach by annotating camera movements in real-world videos sourced from streaming platforms like YouTube, spanning diverse genres (e.g., nature, film, advertisements, news, video games, abstract art, selfies, sports, tutorials, drone footage, studio productions, performance shows, screen recordings, vlogs, anime, motion graphics), types (e.g., 2D, 2.5D, 3D, synthetic, real), point of view (e.g., first-person, third-person), capturing devices (e.g., smartphones, dashcams, GoPros, steadicams, fisheyes), and post-production effects (e.g., overlays, framings, mixed reality elements). See the supplement for detailed statistics. We adhere to educational licenses to ensure these videos can be used for benchmarking. Since internet videos often contain multiple shots, we manually segment them into single, continuous shots with well-defined camera movements. These shots capture varying levels of motion complexity, including single-motion sequences, compound motions (e.g., dolly-in and zoom-out), ambiguous motions (e.g., motions that are too subtle), and multiple motion sequences (e.g., tilt-up followed by tilt-down). Each video is labeled by at least 5 team members, and over months of discussion sessions, we refine label definitions, add missing labels, and reach perfect consensus on an initial set of 800 videos. Due to space constraints, we present an overview of our taxonomy of motion primitives in Figure 2, show example annotations in Figure 5, and refer readers to Appendix for detailed definitions: Motion type. The camera motion is nonexistent (no), clear and consistent (simple), subtle (minor), or ambiguous/conflicting (complex). Steadiness. The camera remains still (static) or exhibits different levels of shakiness (no shaking, minimal shaking, unsteady, very unsteady). Translation. The camera physically moves forward or backward (dolly), up or down (pedestal), or to the right or left (truck). Rotation. The camera rotates along its own axis to the right or left (pan), up or down (tilt), or clockwise or counterclockwise (roll). Intrinsic change. The camera adjusts its focal length to zoom in or out (zoom). Object-centric movements. The camera orbits around still subject (or the frame center) in circular path (arc), or tracks moving subject from behind (tail-tracking), the front (lead-tracking), the side (side-tracking), from an aerial view (aerial-tracking), or using other motions (tilt-/pan-/arc-tracking). We also consider whether the camera moves or zooms to make the subject appear larger or smaller within the frame. Others. We include the speed of camera movement (slow/regular/fast), cinematic effects (dolly-zoom/motion-blur), and scene movement (static/mostly-static/dynamic). Caption. We provide description for complex motion that does not fit predefined primitives. We direction also the Comments. specify above the moprimitives for tion (in/out/up/down/right/left/CW/CCW). Lastly, humans tend to interpret the direction of camera translation relative to the ground due to natural bias in the first video of toward gravity. For example, Figure 5 (row 1, column 1), the camera moves forward (dolly-in) while pointing directly at the ground in birds-eye-view. Yet, most humans describe it as moving downward (pedestal-down). Our supplement details how our labeling policy resolves this ambiguity using two separate questionnaires to label camera translation in ground-centric and camera-centric frames. Human study. We set up our labeling interface using LabelBox under an educational license and recruit participants via crowdsourcing platforms, university and film school boards, and professional studios. Over two months, we gather 100 participants, with half having hands-on cinematography experience. Initially, 20 participants annotate 30 videos based on our taxonomy definitions. Figure 4 shows that cinematography experience significantly improves annotation performance. Improving human performance. We find that nonexperts often struggle with confusable motions, such as rotation vs. translation or extrinsic vs. intrinsic changes due to limited understanding of parallax effects [47]. To address this, we develop detailed guidelines with positive/negative video examples and corner cases for each primitive. Our results show that these guidelines benefit both non-experts and experts, with even cine6 Figure 6. VQA examples of CameraBench. We evaluate 9 challenging camera motion understanding skills (with 81 sub-tasks detailed in the supplement). Each VQA question is paired with positive video (answer: Yes) and negative video (answer: No), making it vision-centric benchmark that cannot be addressed using language priors [15, 27, 35]. matographers finding the examples helpful. Lastly, Figure 4-(b) shows that extended practice (five more rounds with error feedback) further improves performance by 10-15% as participants better align with our guidelines over time. We hire only participants who successfully complete all training to scale up our dataset and keep monitoring their performance through random audits. 4. CameraBench for Motion Understanding We repurpose our motion primitive labels and captions for both discriminative (classification, retrieval) and generative (VQA, captioning) tasks. Baselines. We evaluate diverse set of 20 models, including 6 SfM/SLAM methods: COLMAP [48] and learning-based variants such as MegaSAM [33], CUT3R [58], and others [10, 56, 59]. We also report 3 discriminative VLMs [30, 73] like InternVideo2 [60] and 11 generative VLMs including Qwen2.5-VL [4], GPT-4o [41], and LLaVA-Video [68], among others [28, 54, 60, 66, 67]. Appendix details these models. Classification of motion primitives. We evaluate models on binary classification of basic motion primitives. Since SfM/SLAM reconstructs trajectories in the camera-centric frame, we restrict this task to primitives Figure 7. Failures of SfM/SLAM. The top row shows lead-tracking shot where the camera moves backward (relative to the ground) as the subject walks forward. However, since the subjects framing remains unchanged and the background lacks distinct textures, MegaSAM [33] fails to detect camera translation and COLMAP [48] crashes. The second row shows roll shot in low-parallax scene: both methods do not converge and output trajectories with nonexistent motion. defined in the camera-centric frame. For SfM/SLAM, we compute the seven degrees of translation, rotation, and 7 Table 1. Binary classification on motion primitives defined in the camera-centric frame. We report Average Precision per primitive. We bold the best and underline second best results. We find that (1) learning-based SfM/SLAM methods like MegaSAM [33] achieve superior performance across most primitives, significantly outperforming COLMAP [48]. Nonetheless, all methods remain far from solving this task with 50% AP. (2) Although generative VLMs (evaluated using VQAScore [36]) are weaker than SfM/SLAM, they generally outperform discriminative VLMs. Motivated by this, we apply supervised fine-tuning (SFT) to Qwen2.5-VL [4] on separately annotated training set of 1400 videos. We show that simple SFT on small-scale (yet high-quality) data significantly boosts performance by 1-2x, making it match the SOTA MegaSAM in overall AP."
        },
        {
            "title": "Random Chance",
            "content": "SfM/SLAM"
        },
        {
            "title": "VGGSFM",
            "content": "DUSt3R MASt3R CUT3R"
        },
        {
            "title": "CLIPScore",
            "content": "UMT-B16-CLIP UMT-L16-CLIP LanguageBind-CLIP LanguageBindV1.5-CLIP InternVideo2-S2-CLIP"
        },
        {
            "title": "ITMScore",
            "content": "UMT-B16-ITM UMT-L16-ITM InternVideo2-S2-ITM"
        },
        {
            "title": "VQAScore",
            "content": "LLaVA-OneVision-7B LLaVA-Video-7B InternVideo2-Chat-8B Tarsier-Recap-7B InternLMXComposer2.5-7B 49.0 InternVL2.5-8B InternVL2.5-26B mPLUG-Owl3-7B GPT-4o InternVL3-8B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B 67.9 63.6 47.6 66. 61.2 72.0 63.0 66.8 67.2 Translation (Dolly/Pedestal/Truck)"
        },
        {
            "title": "Zooming",
            "content": "Rotation (Pan/Tilt/Roll)"
        },
        {
            "title": "Static Avg",
            "content": "In"
        },
        {
            "title": "Out",
            "content": "29.3 9.7 Up 6."
        },
        {
            "title": "Left",
            "content": "In"
        },
        {
            "title": "Left",
            "content": "Up"
        },
        {
            "title": "CW CCW",
            "content": "8.6 15.8 11.5 11.1 10.2 15. 15.4 12.7 7.7 8.9 10.2 9. 12.2 36.2 56.6 58.9 47.5 68. 73.8 27.0 27.2 32.7 33.6 41. 31.7 40.6 52.4 46.8 54.7 69. 59.7 13.1 28.9 24.0 21.1 50. 43.9 11.9 28.7 30.7 23.5 24. 24.2 10.4 9.0 9.8 12.3 13. 14.5 9.4 11.5 10.6 12.6 13. 15.2 18.5 15.1 10.6 12.9 11. 12.9 29.2 15.5 18.2 14.1 19. 19.1 7.8 11.0 5.8 11.4 8. 10.5 12.6 16.5 19.3 25.7 11. 28.1 21.1 13.9 21.1 18.8 19. 20.1 11.1 12.8 19.7 38.2 18. 40.2 34.2 29.1 20.0 10.8 11. 10.3 9.7 14.3 17.6 14.7 16. 19.3 17.6 23.7 10.4 25.9 23. 16.9 38.2 29.0 32.5 22.3 31. 26.5 34.1 48.9 38.3 38.7 37. 45.3 19.4 18.5 14.2 15.0 15. 16.6 21.9 15.8 23.7 27.1 17. 28.8 14.6 23.4 27.2 17.3 38. 30.5 33.8 28.5 32.1 33.3 30. 35.3 26.9 38.1 27.6 44.2 11. 11.5 11.7 11.8 12.0 12.8 23. 19.7 20.2 23.6 23.4 21.5 10. 23.2 19.4 18.5 21.9 27.3 29. 27.7 30.4 26.1 13.9 21.7 18. 42.2 15.9 11.1 11.8 17.5 14. 14.2 15.0 12.3 12.4 21.1 10. 16.2 12.2 14.4 11.8 18.6 21. 12.9 41.7 29.5 26.4 23.2 27. 27.5 14.2 17.3 24.6 46.6 21. 10.2 9.9 8.9 9.4 10.1 9. 9.2 9.8 16.7 14.4 16.9 10. 15.0 16.5 32.1 31.6 10.6 39. 28.1 33.4 27.2 32.6 41.2 43. 60.9 59.4 66.6 59.1 79.5 11. 16.0 20.1 19.9 20.6 15.1 21. 29.4 33.5 33.6 22.6 22.8 14. 37.4 42.5 31.4 44.7 41.6 47. 36.5 43.2 50.6 46.4 58.7 63. 58.0 65.0 82.2 13.5 17.4 16. 16.7 18.8 16.9 33.2 29.1 33. 36.8 22.7 27.3 13.9 30.9 38. 26.6 42.1 49.3 53.5 44.6 50. 46.8 28.3 46.6 32.9 63.2 65. 73.8 13.1 21.9 14.1 16.1 14. 16.2 31.0 24.5 16.9 26.9 17. 24.6 14.7 37.6 44.9 26.1 43. 42.0 47.8 38.4 53.2 53.4 19. 43.3 27.3 40.3 47.5 65.3 8. 8.3 8.5 9.2 9.1 10.0 11. 18.4 31.4 37.2 22.8 21.6 17. 36.9 43.6 37.0 35.5 36.5 40. 25.7 44.0 31.0 42.1 61.4 61. 50.4 60.7 71.5 18.8 7.3 13. 17.6 8.3 14.2 13.5 17.2 19. 16.1 19.6 15.2 11.7 11.5 14. 10.4 24.0 21.3 27.6 26.0 26. 33.3 48.7 55.5 57.9 53.5 66. 75.8 15.6 10.0 9.5 10.2 10. 12.1 12.3 13.4 20.8 21.7 16. 18.7 18.1 25.3 18.2 12.2 28. 22.3 25.0 25.5 29.0 30.9 7. 16.7 13.1 15.7 15.1 22.0 10. 13.0 10.9 10.4 11.4 8.9 9. 14.0 18.8 22.1 20.2 30.7 21. 23.4 25.1 17.8 32.0 20.1 22. 20.2 28.8 29.1 27.3 41.3 37. 43.1 42.7 50.1 14.0 14.0 13. 14.7 14.2 14.2 18.4 20.6 22. 25.6 22.0 21.0 16.5 29.5 29. 20.8 36.4 31.5 36.8 29.5 35. 35.3 Qwen2.5-VL-7B (SFT) 83.2 48.6 27.2 48. 62.6 54.3 51.3 70.7 77.6 86. 70.4 58.0 38.5 46.3 65.2 59. focal change from estimated camera extrinsics and intrinsics between the first and last frame. For discriminative VLMs, we use textual definitions of each primitive (The camera pans to the left.) to compute matching scores. For generative VLMs, we apply VQAScore [36], using the probability of answering Yes to binary question (Does the camera pan to the left?). Appendix details our textual prompts per primitive. Results. Table 1 shows that (1) learning-based SfM/SLAM methods like MegaSAM significantly outperform COLMAP and set the state-of-the-art. Nonetheless, no methods fully solve this task, as the best overall AP remains 50%. Figure 7 shows failure case where SfM/SLAM struggles with dynamic subjects while the background has minimal texture. (2) While weaker than SfM/SLAM, generative VLMs like GPT-4o show promising results, significantly outperforming discriminative VLMs. This motivates us to fine-tune Qwen2.5-VL using supervised fine-tuning (SFT) on separate set of 1400 videos (with no overlap with the testset). Despite the small dataset size, our SFT model achieves 2x performance, matching that of MegaSAM. We note that certain motions like roll remain particularly challenging for VLMs, likely due to their long-tailed nature in internet 8 Figure 8. Camera motion descriptions. Our SFT model generates more accurate motion descriptions than state-of-theart VLMs like GPT-4o and Gemini-2.5-Pro. See Figure 9 and Figure 10 for more examples. and suggest directions for future improvement. Lastly, we show that our high-quality dataset can be used to finetune VLMs for improved camera motion understanding. videos. We hypothesize that collecting more videos could further improve model performance. Beyond camera-centric motion primitives. We collect 10K VQA samples across 9 top-level skills and 81 sub-tasks. Crucially, these tasks go beyond cameracentric frame reasoning to evaluate more aspects such as object-centric motion, scene dynamics, steadiness, and more. Some tasks also require logical (e.g., verifying if only one motion type exists or if motion is absent) and linguistic reasoning (e.g., checking if motion description is accurate). We follow community best practices [15, 27], pairing each question with two videos with opposite answers so that models cannot answer blindly without seeing the video (see Figure 6). VQA results. Table 2 shows that all open-source VQA models perform at or below chance on CameraBench. Nonetheless, our SFT model fine-tuned on our small training set achieves state-of-the-art results across all skills, especially the most challenging ones (e.g., Tracking Shot and Only Motion) that require objectcentric and logical reasoning. Captioning/retrieval tasks. We summarize key findings: (1) Captioning  (Table 3)  . We prompt VLMs with Describe the camera movements in this video. Our SFT model produces more accurate cations than state-of-theart VLMs (see Figure 8, Figure 9, and Figure 10). Table 3 shows caption generation evaluated using automated metrics, e.g., SPICE, ROUGE-L, BLEU-2, METEOR, LLMas-a-Judge, . (2) Video-text retrieval  (Table 4)  . We use random video pairs in CameraBench to evaluate retrieval performance and show that the discriminative VQAScore [36] of generative VLMs, especially our SFT model, outperforms all baselines. 5. Conclusion Limitations and future work. We apply supervised fine-tuning to VLMs, but future work may explore other post-training techniques [17, 34] using our released data. While we cover common motion primitives, identifying long-tailed patterns [40, 42] in internet videos could be interesting. Optimizing our predefined prompts [38] may improve VLM performance. We also plan to expand our data curation to broader video understanding tasks. Lastly, given the complementary nature of SfM/SLAM and VLM performance, integrating them could be promising direction for improving video understanding. Conclusions. We take the first step toward human-like camera motion understanding by introducing taxonomy of motion primitives and robust annotation framework, developed in collaboration with cinematographers. We implement training program to transform laypeople into proficient annotators of camera movements. We curate diverse benchmark to analyze existing models 9 Table 2. VQA evaluation. We report both accuracy (Acc) and question accuracy (Q-Acc) [27] that scores point only if both videos are answered correctly for given question. We bold the best and underline second-best results. While most VLMs perform at or below chance, our SFT model achieves the best overall performance."
        },
        {
            "title": "Model",
            "content": "Motion &"
        },
        {
            "title": "Overall",
            "content": "Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc Acc Q-Acc"
        },
        {
            "title": "Random Chance",
            "content": "50.0 25.0 50.0 25.0 50.0 25. 50.0 25.0 50.0 25.0 50.0 25. 50.0 25.0 50.0 25.0 50.0 25. 50.0 25.0 mPLUG-Owl3-7B LLaVA-Video-7B LLaVA-OneVision-7B InternVideo2-Chat-8B Tarsier-Recap-7B 51.8 53.5 54.3 52.4 51. InternLMXComposer2.5-7B 52.8 InternVL2.5-8B InternVL2.5-26B InternVL3-8B InternVL3-26B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o Gemini-2-Flash Gemini-2.5-Pro 54. 56.2 54.4 56.2 55.2 65.5 67. 55.8 53.6 58.2 15.5 12.8 19. 13.7 12.3 12.8 14.9 17.3 14. 17.3 17.4 39.9 42.1 27.0 25. 28.7 64.9 66.1 63.8 64.4 62. 57.8 59.8 63.5 59.8 63.5 60. 59.8 60.5 52.6 46.8 51.3 35. 36.2 31.0 31.6 29.2 19.5 23. 26.4 23.0 26.4 24.1 25.3 26. 10.3 2.9 11.6 61.5 57.2 69. 51.7 50.5 56.6 57.5 60.8 57. 60.8 67.8 69.3 70.0 61.2 56. 60.1 31.6 22.4 54.0 5.2 4. 17.2 31.6 35.2 31.6 35.2 37. 46.0 48.2 32.2 29.3 34.5 48. 52.1 53.1 50.2 49.8 49.6 51. 53.8 51.3 53.8 51.9 55.1 56. 58.1 44.5 48.9 13.1 17.8 24. 2.9 2.5 1.7 12.8 15.6 12. 15.6 17.0 26.0 28.3 32.8 17. 21.4 49.2 49.9 55.4 49.7 49. 53.3 49.7 51.2 49.7 51.2 52. 51.7 53.2 53.3 41.1 45.7 12. 5.4 20.7 13.8 12.5 14.8 0. 14.5 0.0 14.5 10.7 22.6 24. 20.4 8.8 13.2 54.1 54.9 60. 52.2 51.5 53.2 58.1 60.3 58. 60.3 57.2 66.0 67.3 64.1 46. 52.3 24.3 13.9 28.2 5.5 5. 9.9 22.5 25.8 22.5 25.8 21. 41.0 42.6 36.2 20.5 25.8 53. 59.9 60.7 48.5 47.8 49.1 55. 58.4 55.2 58.4 56.2 57.3 59. 51.7 46.5 49.7 17.1 29.2 31. 2.3 2.0 11.6 14.1 18.9 14. 18.9 21.5 30.2 32.4 20.2 24. 26.9 45.9 51.3 43.3 50.9 50. 51.2 50.0 52.5 50.0 52.5 47. 48.4 50.3 42.1 39.2 42.8 8. 2.9 6.1 4.3 3.8 2.4 0. 2.4 0.0 2.4 4.7 26.3 28. 8.5 15.1 15.3 63.4 68.0 52. 50.6 49.8 48.4 50.0 53.6 50. 53.6 62.5 71.1 72.8 61.9 63. 64.5 39.7 41.8 6.3 1.3 1. 7.8 0.0 3.8 0.0 3.8 30. 47.6 50.1 32.7 37.4 39.1 55. 58.8 57.1 51.3 50.6 51.7 54. 57.2 54.5 57.2 57.6 63.8 65. 59.0 51.8 54.7 25.4 24.1 24. 5.3 4.8 9.3 16.7 19.8 16. 19.8 22.3 38.2 40.7 29.8 24. 28.2 Qwen2.5-VL-7B (SFT) 80.3 65.0 86.8 75. 87.4 76.4 69.5 43.7 59.2 35. 76.1 55.3 85.5 72.7 79.0 59. 83.5 68.5 78.5 60.5 Table 3. Automated evaluation of caption generation. We assess video captioning models on their ability to describe motion using standard reference-based metrics such as SPICE and ROUGE-L, as well as LLM-as-a-judge evaluation. For the latter, we prompt the model with the question: Reference caption: {reference} Candidate caption: {candidate} Does the candidate caption match the reference caption? Answer Yes or No. We then report the average confidence score P(Yes) [36]. Notably, our SFT model consistently outperforms other models across all automated metrics."
        },
        {
            "title": "Caption Generation",
            "content": "SPICE ROUGE-L BLEU-2 METEOR LLM-Judge"
        },
        {
            "title": "Model",
            "content": "mPLUG-Owl3-7B LLaVA-Video-7B LLaVA-OneVision-7B InternVideo2-Chat-8B Tarsier-Recap-7B InternLMXComposer2.5-7B InternVL2.5-8B InternVL2.5-26B InternVL3-8B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o Gemini-2-Flash Gemini-2.5-Pro 0.22 0. 0.22 0.22 0.23 0.21 0.20 0. 0.20 0.18 0.18 0.24 0.25 0. 0.24 0.20 Qwen2.5-VL-7B (Ours SFT) 0.50 0.20 0. 0.21 0.21 0.22 0.19 0.10 0. 0.15 0.16 0.12 0.17 0.19 0. 0.21 0.15 0.47 0.08 0.12 0. 0.11 0.11 0.08 0.04 0.09 0. 0.06 0.05 0.08 0.10 0.06 0. 0.06 0.33 0.19 0.19 0.20 0. 0.20 0.19 0.21 0.23 0.17 0. 0.28 0.29 0.30 0.25 0.22 0. 0.46 0.08 0.09 0.09 0.13 0. 0.10 0.08 0.11 0.08 0.07 0. 0.18 0.19 0.10 0.07 0.14 0. 10 Table 4. Evaluation on video-text retrieval. We compare CLIPScore, ITMScore, and VQAScore models on skill-based and caption-based video-text retrieval tasks, measured by Text, Video, and Group scores as defined in [27, 55]. Skill-based task refers to evaluating on all 8 skills except for Complex Description. Caption-based task refers to evaluating on the Complex Description skill. We show that repurposing generative VLMs (especially our SFT model) for discriminative scoring using VQAScore sets the state-of-the-art."
        },
        {
            "title": "Model",
            "content": "Skill-based Task Caption-based Task"
        },
        {
            "title": "Random Chance",
            "content": "25.0 25.0 16.6 25.0 25."
        },
        {
            "title": "CLIPScore",
            "content": "UMT-B16 UMT-L"
        },
        {
            "title": "LanguageBind",
            "content": "LanguageBindV1.5 InternVideo2-S"
        },
        {
            "title": "ITMScore",
            "content": "UMT-B16 UMT-L16 InternVideo2-S"
        },
        {
            "title": "VQAScore",
            "content": "mPLUG-Owl3-7B LLaVA-OneVision-7B LLaVA-Video-7B InternVideo2-Chat-8B Tarsier-Recap-2 21. 26.8 23.7 24.0 24.1 9.3 17. 14.7 19.9 18.2 28.3 26.2 24. 17.8 21.4 35.1 InternLMXComposer-2.5-7B 14.3 InternVL-2.5-8B InternVL-2.5-26B InternVL-3-8B InternVL-3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o 22.0 22.1 31.9 35.7 35.0 41. 43.8 38.3 5.8 4.1 4.4 9. 8.3 2.3 9.5 9.1 10.7 8. 39.7 38.4 39.7 40.9 18.0 23. 33.0 43.9 45.1 46.0 44.6 40. 42.7 44.5 42.4 3.5 2.8 2. 6.2 5.4 0.7 4.3 3.9 5. 4.1 20.5 19.6 18.8 13.3 8. 15.4 9.8 17.5 18.7 25.0 26. 24.2 29.5 32.1 25.8 44.0 46. 39.5 53.6 55.9 25.0 42.7 30. 45.2 52.3 54.2 57.6 56.4 53. 41.2 43.4 40.4 55.8 57.4 60. 63.4 65.5 65.6 67.8 39.9 26. 19.0 17.3 39.6 38.7 18.9 37. 33.0 37.0 41.7 53.0 52.8 53. 50.7 26.3 30.4 54.2 51.4 54. 57.3 60.5 63.0 67.7 69.2 40. Qwen2.5-VL-7B (SFT) 60.3 79.9 57.7 89.2 87. 16.6 19.8 13.0 11.1 33.2 33. 8.6 25.3 18.7 26.2 31.0 39. 42.7 40.9 37.2 16.1 22.6 29. 38.7 39.1 45.8 48.2 51.8 53. 56.4 31.6 82.0 Figure 9. Comparing motion descriptions for different VLMs (example 1 of 2). 11 Figure 10. Comparing motion descriptions for different VLMs (example 2 of 2)."
        },
        {
            "title": "References",
            "content": "[1] Dawit Mureja Argaw, Fabian Caba Heilbron, Joon-Young Lee, Markus Woodson, and In So Kweon. The anatomy of video editing: dataset and benchmark suite for aiassisted video editing. In European Conference on Computer Vision, pages 201218. Springer, 2022. 3, 5 [2] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. 6 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. 6 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 7, 8, 21 [5] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based arXiv preprint video generative foundation models. arXiv:2502.04896, 2025. [6] Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, and Chun-Hao Paul Huang. Boosting camera motion control for video diffusion transformers. arXiv preprint arXiv:2410.10802, 2024. 6 awareness. In European Conference on Computer Vision, pages 464480. Springer, 2024. 5 [8] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. IEEE transactions on pattern analysis and machine intelligence, 29(6):10521067, 2007. 1 [9] Kyle Deguzman. Types of camera movements in film explained: Definitive guide, 2020. 2, 3, 6 [10] Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. Mast3r-sfm: fully-integrated solution for unconstrained structure-from-motion. arXiv preprint arXiv:2409.19152, 2024. 7, [11] Dimitris Eleftheriotis. Cinematic journeys: Film and movement. Edinburgh University Press, 2010. 3 [12] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsdslam: Large-scale direct monocular slam. In European conference on computer vision, pages 834849. Springer, 2014. 1 [13] Steven Ferris. Motion parallax and absolute distance. Journal of experimental psychology, 95(2):258, 1972. 1 [14] James Gibson. The ecological approach to visual perception. 2003. 1 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904 6913, 2017. 7, [16] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 3 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9 [18] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 6 [19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 4, 20 [20] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models. arXiv preprint arXiv:2501.02955, 2025. 5 [7] Robin Courant, Nicolas Dufour, Xi Wang, Marc Christie, and Vicky Kalogeiton. Et the exceptional trajectories: Text-to-camera-trajectory generation with character [21] Yunzhong Hou, Liang Zheng, and Philip Torr. Learning camera movement control from real-world drone videos. arXiv preprint arXiv:2412.09620, 2024. 13 [22] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 709727. Springer, 2020. 3, 5 [23] Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, and Baoquan Chen. Cinematographic camera diffusion model. In Computer Graphics Forum, page e15055. Wiley Online Library, 2024. 5 [24] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. arXiv preprint arXiv:2412.09621, 2024. 5 [25] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. arXiv preprint arXiv:2411.10836, 2024. 6 [26] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. Evaluating and improving compositional text-to-visual generation. In The First Workshop on the Evaluation of Generative Foundation Models at CVPR, 2024. 20 [27] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 7, 9, [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4, 7 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4, 20 [30] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1994819960, 2023. 4, 7 [31] Teng Li, Guangcong Zheng, Rui Jiang, Tao Wu, Yehao Lu, Yining Lin, Xi Li, et al. Realcam-i2v: Real-world image-to-video generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. 6 [32] Xiaozhe Li, Kai Wu, Siyi Yang, YiZhan Qu, Guohua Zhang, Zhiyu Chen, Jiayao Li, Jiangchuan Mu, Xiaobin Hu, Wen Fang, et al. Can video generation replace cinematographers? research on the cinematic language of generated video. arXiv preprint arXiv:2412.12223, 2024. 3, 5 [33] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. arXiv preprint arXiv:2412.04463, 2024. 2, 4, 7, 8, 22 [34] Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, and Deva Ramanan. Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models, 2023. 4, [35] Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Revisiting the role of language priors in vision-language models. arXiv preprint arXiv:2306.01879, 2024. 4, 7 [36] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291, 2024. 4, 8, 9, 10, 20 [37] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for In Proceedings of the deep learning-based 3d vision. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 5 [38] Shihong Liu, Zhiqiu Lin, Samuel Yu, Ryan Lee, Tiffany Ling, Deepak Pathak, and Deva Ramanan. Language models as black-box optimizers for vision-language models. arXiv preprint arXiv:2309.05950, 2024. 9 [39] Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. Chatcam: Empowering camera control through conversational ai. Advances in Neural Information Processing Systems, 37:5448354506, 2025. 6 [40] Ravi Teja Mullapudi, Fait Poms, William Mark, Deva Ramanan, and Kayvon Fatahalian. Learning rare category classifiers on tight labeling budget. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 84238432, 2021. [41] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 4, 7 [42] Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails of vision-language models. arXiv preprint arXiv:2401.12425, 2024. 9 [43] Pietro Perona. Vision of visipedia. Proceedings of the IEEE, 98(8):15261534, 2010. 3 [44] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv e-prints, pages arXiv2410, 2024. 3, 6 [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 4, 20 [46] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and Dahua Lin. unified framework for shot type classification based on subject centric lens. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 1734. Springer, 2020. 5 14 [47] Brian Rogers and Maureen Graham. Motion parallax as an independent cue for depth perception. Perception, 8 (2):125134, 1979. 1, [48] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. 1, 4, 5, 7, 8, 21 [49] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-form motion control: synthetic video generation dataset with controllable camera and object motions. arXiv preprint arXiv:2501.01425, 2025. 5, 6 [50] Tomas Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. 3 [51] Raymond Spottiswoode. grammar of the film: An analysis of film technique. Univ of California Press, 1969. 2, 3 [52] Takafumi Taketomi, Hideaki Uchiyama, and Sei Ikeda. Visual slam algorithms: survey from 2010 to 2016. IPSJ transactions on computer vision and applications, 9 (1):16, 2017. 1 [53] Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms analyze compositions in compiled videos? arXiv preprint arXiv:2411.10979, 2024. 5 [54] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 2, 4, 7, [55] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 10 [56] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2168621697, 2024. 1, 4, 7, 21 [57] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 3, 5 [58] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. 2, 4, 7, 21 [59] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 7, 21 [60] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. 4, [61] Yuelei Wang, Jian Zhang, Pengtao Jiang, Hao Zhang, Jinwei Chen, and Bo Li. Cpa: Camera-pose-awareness diffusion transformer for video generation. arXiv preprint arXiv:2412.01429, 2024. 6 [62] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-tovideo generation. Advances in Neural Information Processing Systems, 37:3432234348, 2025. 6 [63] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. 6 [64] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. arXiv preprint arXiv:2502.04299, 2025. 1, 6 [65] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 6 [66] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025. 1, 6, [67] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2.5omnilive: comprehensive multimodal system for longarXiv term streaming video and audio interactions. preprint arXiv:2412.09596, 2024. 7 [68] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 4, 7 [69] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 2037. Springer, 2022. 1 [70] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. 6 15 [71] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 3, [72] Zhenghong Zhou, Jie An, and Jiebo Luo. Latent-reframe: Enabling camera control for video diffusion model without training. arXiv preprint arXiv:2412.06029, 2024. 6 [73] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2023."
        },
        {
            "title": "Outline",
            "content": "Below is the outline of the supplement: Section shows more statistics and examples of CameraBench. Section details the annotation framework. Section details our guidelines, training program, and quality control pipeline. Section details the experimental setup and provides additional results. Section details our label taxonomy. Section details the 9 top-level skills and 81 sub-tasks in CameraBench. A. CameraBench Details Dataset statistics. CameraBench consists of 3,381 video clips with an average duration of 5.7 seconds and frame rate of 29.4 FPS. The training split includes 1,402 videos. Using the same set of skills and tasks (detailed in Appendix F), we generate 98,112 video-QA pairs and 1,402 video-caption pairs for training. Word clouds. Figure 11 shows the word cloud of our collected camera motion descriptions. Figure 12 visualizes the word cloud for metadata such as shot compositions, genres, points of views, and capturing devices. Figure 11. Word cloud of our camera motion captions. Video genres. Our videos are manually sourced and tagged by genre. Figure 13 shows the genre distribution in our dataset. The other genre includes more tags such as animal/pet, abstract art, sports, drone footage, lectures, screen recordings, mixed media, tutorials, and selfies. These tags will be released with the dataset. More examples. Figure 14 presents more annotation examples from our dataset. B. Annotation Framework Framework. We design our annotation framework to Figure 12. Word cloud of video metadata, including genres, types, shot compositions, point of views, capturing devices, and post-production effects. Figure 13. Genre distribution. ensure precision and efficiency by preventing contradictory labels and eliminating redundant work. We detail how we annotate the 50 motion primitives and descriptions below. Given video, we first ask: Is there camera motion? First, check if the video has any camera motion (including small movements like handshakes). If yes, select the motion steadiness; otherwise, select static and then stops. Is the motion clear and consistent? If there is camera motion, choose simple for clear and consistent motion, complex for ambiguous or conflicting motion, or minor for small, barely noticable motion. Next, if the camera motion is simple, all motion primitives must be labeled comprehensively; otherwise, they are treated as negative samples (e.g., simple-motion video not labeled as pan-right or pan-left is automatically assigned to no-pan). For complex-motion or minor-motion videos, annotators only select clearly identifiable, unambiguous primitives (e.g., consistent and non-conflicting motion). For example, if camera first performs dolly-in and then dolly-out, the video is labeled as complex, with none of dolly-in, dolly-out, or no-dolly assigned. In these scenarios, annotators provide description explaining the complex motion patterns. If the motion is too intricate to fully describe, they should focus on what is clear and noticeable or simply state the reason for the camera movement (e.g., handheld shot tracking subject or first-person camera following persons perspective as they look around). Note that for camFigure 14. More annotation examples from our dataset. describe, they should focus on what is clear and noticeable or simply state the reason for the camera movement (e.g., handheld shot tracking subject or first-person camera following persons perspective as they look around). Note that for camera translation, we ask annotators to label and describe movement relative to the ground, as this aligns with most peoples intuition. We then use separate questionnaire to re-label videos with camera-centric translation primitives, such as dolly and pedestal. Annotation interface. Figure 15 shows the annotation interface we use, and Figure 16 lists some of the questions in our annotation framework. Labelers can revise their labels at any time, as it is common to adjust previous labels based on later questions. era translation, we ask annotators to label and describe movement relative to the ground, as this aligns with most peoples intuition. We then use separate questionnaire to re-label videos with camera-centric translation primitives, such as dolly and pedestal. Figure 15 shows the annotation interface that we use, and Figure 16 shows all the questions within this annotation framework. The interface is designed to maintain consistency, allowing labelers to review key attributes of the video while minimizing annotation time. If necessary, labelers can revise their responses before submission. Next, if the camera motion is simple, all motion primitives must be labeled comprehensively; otherwise, they are treated as negative samples (e.g., simple-motion video not labeled as pan-right or pan-left is automatically assigned to no-pan). For complex-motion or minor-motion videos, annotators only select clearly identifiable, unambiguous primitives (e.g., consistent and non-conflicting motion). For example, if camera first performs dolly-in and then dolly-out, the video is labeled as complex, with none of dolly-in, dolly-out, or no-dolly assigned. In these scenarios, annotators provide description explaining the complex motion patterns. If the motion is too intricate to fully 18 Figure 15. Annotation interface based on LabelBox. Figure 16. Example questions in our annotation framework. C. Training Program and Quality Control Tutorials. To help participants familiarize themselves with camera movements and align with our labeling policy, we provide tutorial with clear guidelines, textual definitions, video examples, and complex edge cases. Figure 17 shows few random pages from our guidelines. Caption guidelines. Labeling complex-motion videos can be challenging when movements are conflicting, sequential, occur at different speeds, or lack sufficient background information. To improve clarity in such complex scenarios, we ask annotators to provide descriptions that include (1) the purpose of the movement (if clear), such as following subject, revealing scene, or enhancing immersion; (2) the major camera motions, such as panning, arcing, or zooming, and whether the movement is steady or shaky. We ask annotators to provide details when the motions are sequential and easy to 19 perceive. If the motion is highly intricate or fragmented, we ask them to write high-level summary instead. (2) conciseness: Caption quality. For motion descriptions, we ask annotators to focus on the following three criteria: (1) clearness: Does the description clearly convey the intended information? Is the description expressed in as few words as possible without losing clarity? (3) grammar and fluency: Does the text sound natural and free of errors? Annotators are encouraged to use LLMs like ChatGPT to polish their initial description (e.g., for grammar refinement). The suggested prompt is: Please help me polish my text to make it clear, concise, and grammatically correct. Maintain the intended meaning and tone while improving readability. Avoid using overly complex or fancy words unless necessary. If the text includes specific details, ensure they remain intact. Additionally, make sure the polished version flows naturally and is easy to understand. Training program. Before annotating the main dataset, participants undergo five rounds of training, each with 30 videos. After each round, they receive detailed PDF report (Figure 18) showing their accuracy and comparison with the ground truth, helping them review and refine their responses. If participants still have doubts, the authors of this paper offer direct guidance. After five rounds, their performance typically improves by 1520%. Figure 17. Example guidelines from our tutorial. 20 Figure 18. Examples of our PDF feedback to participants. Wrong answers are colored in red. Quality control pipeline. We hire only annotators who successfully complete all training. Each annotator is then assigned specific role to ensure annotation accuracy and consistency: 1. Labeler: Each video is independently labeled by two labelers. 2. Reviewer: Reviewers check for consensus and resolve label disagreements. Beyond these roles, the authors of this paper conducted an additional review of 3,000 videos, correcting inaccurate labels and refining motion descriptions to ensure clarity and accuracy. D. Experimental Setup and Results Video-text retrieval results. Table 5 shows Text Score, Video Score, and Group Score on all video-text retrieval tasks. VLM details. For discriminative VLMs, we adapt their official codebases to compute CLIPScore [19, 45] and ITMScore [29] for video-text matching scores. For generative VLMs, we also adapt their official codebases but implement the logic to calculate VQAScore [26, 36] for discriminative scoring. While GPT-4o provides logprob API for computing VQAScore, Gemini-2/2.5 disables its logprob API during this work. We note that almost all VLMs utilize uniform frame sampling; however, the number of frames used varies across models. To ensure optimal performance on our dataset, we use the recommended number of frames for each model. We set the number of frames sampled to 4 for GPT-4o. Notably, some models deviate from simple uniform sampling. Gemini-2/2.5 [54] processes video file inputs directly, with its frame sampling procedure hidden from Table 5. Evaluation of video-text retrieval models. We compare all VLMs on text, video, and group score across all skills."
        },
        {
            "title": "Random Chance",
            "content": "CLIPScore UMT-B16 UMT-L16 LanguageBind LanguageBindV1.5 InternVideo2-S2 ITMScore UMT-B16 UMT-L16 InternVideo2-S2 18.2 11.5 20.3 14. VQAScore mPLUG-Owl3-7B LLaVA-Video-7B LLaVA-OneVision-7B InternVideo2-Chat-8B InternVideo2-Chat-26B 17.6 InternLMXComposer2.5-7B 32.1 InternVL2.5-8B InternVL2.5-26B InternVL3-8B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o Qwen2.5-VL-7B (SFT) 14.5 15.5 17.2 46.5 27. 65.2 21.6 29.1 43.6 Motion & Steadiness"
        },
        {
            "title": "Avg\nOverall",
            "content": "T V T V T V T V T 25.0 25.0 16. 25.0 25.0 16.6 25.0 25.0 16. 25.0 25.0 16.6 25.0 25.0 16. 25.0 25.0 16.6 25.0 25.0 16. 25.0 25.0 16.6 25.0 25.0 16. 25.0 15.9 17.2 18.9 1.4 0. 13.5 18.2 3.0 2.0 6.8 5. 3.0 4.7 8.8 9.8 39.9 39. 46.6 16.9 22.6 43.6 52.0 53. 54.7 58.3 55.7 63.9 67.1 43. 84.8 2.4 1.4 3.7 2.4 0. 0.0 4.1 6.1 15.9 10.1 18. 5.1 9.8 25.7 12.8 15.2 16. 19.4 24.7 42.9 45.3 22.6 65. 21.8 12.6 18.4 20.7 32.2 3. 2.3 8.1 10.3 3.5 2.3 26. 8.1 8.1 6.9 10.3 54.0 51. 47.1 33.3 23.0 9.2 51.7 55. 52.9 55.7 41.4 37.9 39.2 2. 75.9 79.3 74.7 77.0 71.3 48. 69.0 70.1 77.0 74.7 76.2 72. 70.1 70.8 73.6 94.2 0.0 2. 5.8 5.8 3.5 1.2 6.9 2. 52.9 50.6 47.1 33.3 20.7 8. 50.6 55.2 49.4 52.1 40.2 37. 38.5 2.3 75.9 36.8 40.2 33. 32.2 2.3 16.1 29.9 37.9 48. 31.0 50.6 37.9 44.8 31.0 36. 50.6 59.8 63.1 59.8 65.5 67. 52.9 77.0 2.3 3.5 13.8 10. 3.5 5.8 10.3 6.9 41.4 51. 46.0 28.7 29.9 44.8 43.7 62. 43.7 45.4 50.6 50.6 51.3 46. 90.8 2.3 3.5 10.3 9.2 1. 2.3 3.5 4.6 28.7 25.3 39. 10.3 24.1 28.7 29.9 47.1 37. 39.8 33.3 36.8 38.6 28.7 77. 23.3 21.9 22.6 21.7 9.6 11. 12.3 7.4 23.9 15.7 17.7 20. 42.5 22.8 14.0 15.5 20.6 24. 20.8 34.0 37.3 40.7 56.8 0. 1.3 5.8 3.8 0.0 3.6 2. 2.9 17.0 14.5 14.1 9.8 17. 17.5 17.9 18.3 16.1 19.5 18. 23.7 26.4 34.7 66.7 0.2 0. 4.0 2.5 0.0 1.6 0.7 0. 9.2 6.0 6.0 5.2 13.9 10. 4.7 8.6 8.7 10.3 9.8 15. 16.8 26.0 50.6 27.1 18.6 26. 22.0 8.5 22.0 13.0 29.9 13. 15.8 8.5 18.6 20.3 11.9 17. 4.4 11.9 15.8 16.4 26.6 30. 33.3 41.8 1.7 2.8 5.1 7. 4.5 5.7 5.1 7.9 20.9 15. 18.1 18.1 10.2 19.2 29.6 26. 29.9 33.7 27.7 17.0 20.8 29. 39.0 0.6 0.6 2.8 5.7 2. 1.7 1.1 4.5 6.8 5.1 3. 9.6 5.1 6.2 14.8 4.4 6. 9.6 10.7 10.7 12.4 17.5 24. 31.1 27.8 25.3 25.7 13.4 18. 24.8 21.3 31.8 8.9 23.9 28. 47.3 7.5 19.4 29.6 38.5 42. 43.9 47.5 49.3 43.2 56.9 6. 7.5 11.1 10.0 2.1 13.8 16. 11.7 48.6 54.9 49.5 18.7 29. 37.4 62.4 57.3 59.4 61.8 60. 59.3 60.7 54.7 85.3 4.8 4. 7.6 6.6 0.8 5.8 8.0 4. 27.4 8.4 20.9 9.9 22.3 6. 18.5 27.7 35.8 39.2 40.7 43. 45.2 36.2 56.4 24.2 27.3 28. 30.6 1.2 23.3 20.9 19.1 22. 39.4 39.4 11.5 27.6 5.5 29. 42.2 46.4 49.7 46.4 46.1 47. 45.5 72.1 5.8 4.6 17.0 12. 3.6 12.7 13.6 10.0 44.2 53. 52.7 16.4 19.1 32.7 53.3 62. 52.4 54.9 13.0 23.9 27.8 26. 89.7 3.6 3.0 10.3 8.5 0. 8.2 7.0 7.3 18.2 33.6 32. 3.6 11.5 15.8 13.0 18.0 17. 8.6 4.3 7.9 9.4 7.2 18. 10.8 2.9 7.2 1.4 2.2 6. 6.5 2.2 4.3 3.6 2.9 18. 12.2 13.0 5.8 3.6 2.7 10. 19.4 18.1 34.2 31.8 35.4 7. 15.5 17.6 16.4 71.2 2.4 0. 16.6 18.3 13.0 15.1 16.2 24. 59.0 9.8 14.6 24.5 27.1 10. 5.8 8.3 16.6 89.9 0.7 0. 1.4 3.6 0.7 2.2 0.7 1. 3.6 6.5 5.8 1.4 0.7 4. 2.4 0.0 8.6 10.4 2.9 2. 3.8 10.1 59.0 26.8 23.7 24. 24.2 9.3 14.7 19.1 18.2 26. 17.8 24.3 21.4 35.1 14.3 22. 22.1 31.9 35.7 35.0 41.4 43. 38.3 60.3 4.1 4.4 9.7 8. 2.3 9.1 10.7 8.7 38.4 40. 39.7 18.0 23.1 33.0 43.9 45. 46.0 48.6 40.8 42.7 44.5 42. 79.9 2.8 2.6 6.2 5.4 0. 3.9 5.0 4.1 19.6 13.3 18. 8.0 15.4 9.8 17.5 18.7 25. 27.8 24.2 29.5 32.1 25.8 57. the user. We also note that Qwen2.5-VL [4] uses framesper-second (FPS) sampling. Unlike uniform sampling, FPS sampling ensures consistent number of frames per second of video. SFT details. We use separate training set of 1,400 videos (with no overlap with the test set) to fine-tune Qwen2.5-VL-7B [4] using the official supervised finetuning code. Our main results are based on full finetuning, which we found to outperform LoRA fine-tuning after 10 epochs. For full fine-tuning, we adopt DeepSpeed ZeRO-3 while freezing the vision tower and multimodal projector. We use learning rate of 1e-4 with cosine scheduling and warmup ratio of 0.1. For comparison, we also run LoRA fine-tuning (rank 64) with slightly higher learning rate of 2e-4. Hyperparameter details for the best run are shown in Table 6. key ablation studies the number of frames sampled per second (FPS). We compare models fine-tuned with different FPS rates on the binary classification tasks, and observe consistent performance boost with higher FPS outperforming lower FPS across the board. Results are shown in Table 7. SfM/SLAM details. We benchmark six classic and learning-based SfM and SLAM methods. For COLMAP [48], we use the default parameters for feature extraction, matching, and mapping but replace exhaustive matching with sequential matching using window size of 10 to balance accuracy and speed. Due to COLMAPs sensitivity to initialization, we also evaluTable 6. SFT hyperparameters for Qwen-2.5-VL. Hyperparameter finetuning type per device train batch size gradient accumulation steps learning rate num train epochs lr scheduler type warmup ratio freeze vision tower freeze multi modal projector video fps video max pixels image max pixels deepspeed template bf16 flash attn Value full 4 2 1.0e-4 10.0 cosine 0.1 true true 8.0 16384 262144 ds z3 config.json qwen2 vl true fa ate VGGSfM [56], which incorporates learning-based front-end for feature extraction and matching, along with learnable camera and point initializer for improved convergence. We observe that VGGSfM converges quickly and therefore use exhaustive matching for this method while keeping its default hyperparameters. Additionally, we evaluate DUST3R [59], MAST3R [10], and CUT3R [58], which propose unified paradigm for solv21 Table 7. FPS/SFT ablations. We report Average Precision (AP) for binary classification of camera-centric motion primitives. Our results show that higher FPS generally improves performance. Additionally, full fine-tuning of generative VLMs outperforms LoRA-based fine-tuning. Model/FPS MegaSAM 2 FPS 4 FPS 8 FPS 30 FPS Qwen-2.5-LoRA-SFT 2 FPS 4 FPS 8 FPS Qwen-2.5-Full-SFT 2 FPS 4 FPS 8 FPS 8 FPS Translation (Dolly/Pedestal/Truck)"
        },
        {
            "title": "Zooming",
            "content": "Rotation (Pan/Tilt/Roll) In"
        },
        {
            "title": "Out",
            "content": "Up"
        },
        {
            "title": "Left",
            "content": "In"
        },
        {
            "title": "Left",
            "content": "Up"
        },
        {
            "title": "Down CW CCW",
            "content": "65.9 72.7 75.0 73.8 79.3 82.1 84.8 78.5 80.8 83.2 43.3 42.6 43.4 43.9 38.2 40.9 43.6 43.9 46.2 48. 19.4 23.0 27.6 24.2 12.9 15.5 17.3 22.7 25.1 27.2 21.3 31.8 42.8 29.1 27.1 30.2 32.5 42.6 45.9 48. 36.6 44.6 46.2 45.3 59.3 62.5 64.9 56.8 59.4 62.6 35.8 39.9 39.9 44.2 37.6 40.3 42.7 49.5 51.8 54. 11.1 11.1 11.1 11.1 47.5 50.6 53.1 45.7 48.2 51.3 10.2 10.2 10.2 10.2 63.4 66.7 69.1 64.2 67.5 70. 62.9 72.6 77.9 79.5 74.2 77.1 79.6 72.4 75.2 77.6 75.8 78.8 82.4 82.2 83.5 86.8 89.9 81.3 84.1 86. 68.2 79.0 75.6 73.8 69.7 72.4 75.0 65.9 68.2 70.4 59.5 60.9 57.6 65.3 58.5 61.2 63.9 53.4 55.7 58. 73.1 72.5 67.3 71.5 33.2 35.8 38.0 34.1 36.3 38.5 85.9 70.4 76.8 75.8 38.1 40.7 43.0 41.8 43.9 46."
        },
        {
            "title": "Static Avg",
            "content": "19.6 24.4 19.7 22.0 64.2 67.3 69.5 61.7 63.5 65.2 45.9 49.0 50.2 50.1 52.4 55.3 57.8 54.3 56.8 59. ing 3D tasks using pointmap prediction. To benchmark MAST3R efficiently, we replace its default exhaustive pair optimization strategy with more efficient sparse optimization method to prevent out-of-memory (OOM) errors. For all these methods, we resize the longer side of images to 512 and utilize their 512-size checkpoints, aligning with the official evaluation procedures. Finally, we evaluate MegaSAM [33], recently released method designed for 4D reconstruction in dynamic videos. We use its default parameters but skip the final causalSAM step, as it optimizes only the depth rather than the points. To convert the camera poses obtained from SfM and SLAM methods into motion primitive scores, we use straightforward approach based on the normalized relative pose between the first and last frames of the trajectory. The motion scores are derived as follows: translation scores are directly taken from the relative translation values along the three axes, while rotation scores are computed from the relative rotation along the roll, pitch, and yaw axes. We convert all axes to align with OpenCVs axis convention to ensure consistency. Lastly, the zoom score is determined by calculating the ratio of the focal lengths between the first and last frames. For CUT3R and MegaSAM, we use video sampling strategy of max(30FPS, 200 frames) to ensure continuous motion. In contrast, for COLMAP, DUST3R, and Mast3R, we sample at 1 FPS to enable efficient inference and avoid OOM errors. We further ablate MegaSAMs performance at 2, 4, and 8 FPS and observe only minimal differences compared to the default sampling strategy in Table 7. E. Full Taxonomy We provide the full taxonomy below: Motion type. The camera motion is nonexistent (no), clear and consistent (simple), subtle (minor), or ambiguous/conflicting (complex). Refer to Table 8 for details. Steadiness. Steadiness affects visual clarity and motion perception in video analysis. While professional cinematography favors stability, intentional shake adds stylistic effects, like in handheld footage. We select if the camera remains still (static) or exhibits different levels of shakiness (no shaking, minimal shaking, unsteady, very unsteady). Refer to Table 9 for details. Translation. The camera physically moves forward or backward (dolly), up or down (pedestal), or to the right or left (truck). Refer to Table 10 for definitions. Note that for camera translation, the choice of reference frame is crucial for consistent annotation. We define two reference frames: (1) The camera-centric reference frame defines motion relative to the cameras own coordinate system, where translations like forward and backward follow the cameras initial orientation. While widely used in existing datasets, it can sometimes be unintuitive for human perception. (2) In contrast, the ground-centric reference frame defines motion relative to the world coordinate system, typically the ground plane. To ensure we label direction consistently in the ground-centric reference frame, we define forward motion (dolly-in) in birds-eye view (looking directly downward at the ground) as moving north or toward the top of the frame, and backward motion (dolly-out) as moving south or toward the bottom. Similarly, in worms-eye view (looking directly upward at the sky), forward motion is defined as moving south (toward the F. Skills and Tasks in CameraBench Skills, tasks, and their textual definitions. We detail all 9 top-level skills and their 81 sub-tasks in Table 15. Additionally, we report the textual definitions used to construct the prompts for VLMs. bottom of the frame), and backward motion as moving north (toward the top). This approach aligns camera motion with human perception of directional movement. See Figure 19 for examples. Rotation. The camera rotates along its own axis to the right or left (pan), up or down (tilt), or clockwise or counterclockwise (roll). Refer to Table 11 for details. Note: Pure camera rotation (without translation) does not produce parallax effect. Take pan-left as an example: the entire scene appears to rotate leftward, but the relative positions of objects remain unchanged. In contrast, for truck-left, closer objects move faster due to camera translation. Intrinsic change. The camera adjusts its focal length to zoom in or out (zoom). Refer to Table 12 for details. Pure camera zooming (without translation) does not create parallax effect; it magnifies the scene while preserving object positions, making the scene appear to scale around the optical center. In contrast, camera translation introduces parallax, causing closer objects to change size within the frame more quickly. from behind (tail-tracking), Object-centric movements. (or The camera orbits around still subject the frame center) in circular path (arc), or tracks moving subject the front the side (side-tracking), (lead-tracking), from an aerial view (aerial-tracking), or using other motions (tilt-/pan-/arc-tracking). We also consider whether the camera moves or zooms to make the subject appear larger or smaller within the frame. Refer to Table 13 for details. Others. We include the speed of camera movement (slow/regular/fast), motion effects (dolly-zoom/motion-blur), and scene movement (static/mostly-static/dynamic). Refer to Table 14 for details. Figure 19. We define moving forward (dolly-in) for birdseye view camera in ground-centric reference frame as movement toward the north (the top of the frame) to maintain label consistency. Table 8. Motion type definitions and guidelines."
        },
        {
            "title": "Definition",
            "content": "no-motion minor-motion simple-motion"
        },
        {
            "title": "Motion Type",
            "content": "complex-motion The camera remains stationary with no intentional movement. Note: Unintentional shaking belongs to no motion. The camera moves slightly and intentionally, such as gentle pan or zoom. The motion is noticeable but remains subtle and not significant. The camera moves significantly in straightforward manner, such as steady pan, tilt, arc, or simple tracking shot. Note: Select this even if the video combines two or more motions, as long as they occur simultaneously at roughly the same speed. The camera exhibits complex movements that are difficult to classify. This includes: (1) Conflicting Motion: Opposing movements occur, such as panning left then right, often seen in drone maneuvers, video game shots, or fast-paced action scenes. (2) Sequential Motion: Two or more movements happen one after another rather than simultaneously (e.g., moving forward, then shifting position after stopping). (3) Simultaneous Motions at Different Speeds: Multiple simultaneous movements occur at significantly different speeds. (4) Unclear Motion / Missing Background Information: If the motion is difficult to analyze due to motion blur or lack of background cues. Table 9. Steadiness definitions and guidelines."
        },
        {
            "title": "Definition",
            "content": "static no-shaking minimal-shaking"
        },
        {
            "title": "Steadiness",
            "content": "unsteady very unsteady The camera remains completely stationary with no movement or vibration. The camera moves smoothly with no detectable shake, typically using high-end stabilizers. Select only if (1) the camera is moving and (2) no unintended motion is present. The camera exhibits slight shaking, whether stationary or moving, maintaining mostly stable shot. Select even if stationary with slight shake. Note: Select even if stationary with slight shake. The camera shows moderate shaking, whether stationary or in motion, introducing noticeable but controlled instability. Note: Select even if stationary with noticeable shake. The camera shakes consistently, typical of unstabilized handheld or action footage. Note: Select only if shaking is consistent throughout the video. Table 10. Camera translation definitions and guidelines."
        },
        {
            "title": "Truck",
            "content": "dolly-in/ dolly-out no-dolly pedestal-up/ pedestal-down no-pedestal truck-left/ truck-right no-truck The camera moves forward or backward relative to the ground plane and the initial frame. The camera does not move forward/backward during the shot. Select this when the camera moves upward or downward clearly and consistently relative to the ground or the orientation of the initial frame. Select this label when the camera does not move leftward/rightward during the shot. The camera physically moves to the left or right, changing its position relative to the initial frame. The camera does not move to the left or right during the shot. 24 Table 11. Camera rotation definitions and guidelines."
        },
        {
            "title": "Roll",
            "content": "pan-left/ pan-right no-pan tilt-up/ tilt-down no-tilt roll-CW/ roll-CCW no-roll The camera rotates its angle by pivoting left or right with respect to the initial frame. The camera does not pan left or right. The camera rotates its angle up or down vertically with respect to the initial frame. The camera does not tilt up or down. The camera performs clear and consistent clockwise (CW) or counterclockwise (CCW) roll by rotating around its own optical center. The camera does not roll clockwise/counterclockwise. Table 12. Camera intrinsic change definitions and guidelines."
        },
        {
            "title": "Zoom",
            "content": "zoom-in/ zoom-out no-zoom The camera adjusts its focal length to zoom in or out, changing the frame size. Note: This differs from physical camera movement. The camera does not adjust its focal length during the video. 25 Table 13. Object-centric movement definitions and guidelines. Object-centric Motion Options"
        },
        {
            "title": "Arc",
            "content": "arc-CW/ arc-CCW no-arc Arc-Tracking arc-tracking no-arc-tracking Lead-Tracking lead-tracking no-lead-tracking Tail-Tracking tail-tracking The camera moves in circular or semi-circular motion around the subject (or the frame center) in clockwise or counterclockwise direction. The camera does not move in circular or semi-circular motion during the video. The camera moves in circular or semi-circular path around the moving subject, often referred to as an orbit or circular tracking shot. The camera does not track or does not move in circular or semi-circular path around the moving subject. The camera moves ahead of the moving subject, capturing their face or front as they follow the cameras path. This is also referred to as leading shot. The camera does not track or does not move ahead of the moving subject. The camera follows directly behind the moving subject, keeping their back in view as they move forward. This is also known as follow shot or chase shot. no-tail-tracking The camera does not track or does not move behind the moving subject. Side-Tracking side-tracking no-side-tracking Aerial-Tracking aerial-tracking no-aerial-tracking Pan-Tracking pan-tracking no-pan-tracking Tilt-Tracking tilt-tracking"
        },
        {
            "title": "Subject Size Change",
            "content": "no-tilt-tracking subject-larger subject-smaller The camera moves parallel to the moving subject, following them from the side as they move through the scene. This is often referred to as trucking shot in film terminology. The camera does not track or does not move parallel to the moving subject. The camera tracks the moving subject from high vantage point, often using drone or crane to follow their movement. The camera either does not track the moving subject or is not positioned at high vantage point. The camera remains in fixed position but pivots horizontally to follow the subject as they move. The camera does not track the subject or does not pivot horizontally to follow their movement. The camera tilts up or down to follow the vertical movement of the subject. The camera does not track the subject or does not pivot vertically to follow their movement. The camera moves or zooms in towards the tracked subject, making them appear larger in the frame. The camera moves or zooms away from the tracked subject, making them appear smaller in the frame. no-subject-change The camera neither moves towards nor away from the subject. 26 Table 14. Others definitions and guidelines."
        },
        {
            "title": "Options",
            "content": "slow regular fast frame-freezing dolly-zoom motion-blur static mostly-static dynamic"
        },
        {
            "title": "Definition",
            "content": "The camera moves at noticeably slow pace. The camera moves at regular pace. If the speed does not stand out as particularly slow or fast, it is considered regular. The camera moves quickly, such as in crash zoom or whip pan. visual effect where scene motion is paused or frozen mid-action, creating still frame within moving sequence. camera effect where the background appears to compress or stretch while the subject stays the same size, often used to create sense of unease. visual effect where moving objects blur due to slow shutter speed or camera movement, often used to emphasize speed and fluid motion in action scenes. The entire scene, including all subjects and background, remains completely motionless throughout the video. The scene is largely still, with only minor elements or small parts exhibiting movement. significant portion of the frame is occupied by dynamic movement of subjects or scene elements (excluding camera motion) that visibly alters the scene. 27 Table 15. All tasks for each top-level skill in CameraBench. We list all 81 tasks of 9 skills in CameraBench."
        },
        {
            "title": "Tasks",
            "content": "Motion & Steadiness"
        },
        {
            "title": "Complex Description",
            "content": "Evaluates how steady the camera is and whether it moves in controlled manner, including shake detection and fixed vs. moving camera states. Determines whether scene is static or dynamic, and detects frame freeze effects. Evaluates the speed of camera movements, distinguishing between slow-moving and fast-moving shots, and detects motion blur. Classifies the direction of camera motion, including forward/backward, upward/downward, leftward/rightward, panning, tilting, rolling, and complex movement types like crane and arc shots. Distinguishes between commonly confused motion types, such as zooming versus physical movement, translation versus rotation, and differentating the reference frame in which the motion happens. Determines whether the camera exhibits motion, including intrinsic changes (zoom) and physical movement (translation, rotation, or arc motion). Identifies whether the camera is tracking subject, specifies different types of tracking shots. Identifies cases where the camera performs single motion type without any other movement. Determines whether given motion description correctly describes the camera movement in video. Clear Moving Camera, Fixed Camera Shake, Stable vs. Shaky Camera, Fixed vs. Moving Camera. (4 Tasks in Table 16) Static vs. Dynamic Scene, Frame Freeze Effect. (2 Tasks in Table 17) Slow vs. Fast Movement, Motion Blur Effect. (2 Tasks in ??) Dolly In vs. Out (Ground), Pedestal Up vs. Down (Ground), Truck Left vs. Right, Pan Left vs. Right, Tilt Up vs. Down, Roll CW vs. CCW, Side Tracking Left vs. Right, Lead vs. Tail Tracking, Arc CCW vs. CW, Crane Up vs. Down, Dolly Zoom In vs. Out, Zoom In vs. Out. (12 Tasks in Table 19) Zoom In vs. Dolly In, Zoom Out vs. Dolly Out, Only Zoom In vs. Only Dolly In, Only Zoom Out vs. Only Dolly Out, Pan Right vs. Truck Right, Pan Left vs. Truck Left, Only Pan Right vs. Only Truck Right, Only Pan Left vs. Only Truck Left, Tilt Up vs. Pedestal Up, Tilt Down vs. Pedestal Down, Only Tilt Up vs. Only Pedestal Up, Only Tilt Down vs. Only Pedestal Down, Dolly In Camera vs. Ground, Dolly Out Camera vs Ground, Pedestal Up Camera vs. Ground, Pedestal Down Camera vs. Ground. (16 Tasks in Table 20) Zoom In, Zoom Out, Dolly In, Dolly Out, Pedestal Up, Pedestal Down, Truck Right, Truck Left, Pan Right, Pan Left, Tilt Up, Tilt Down, Roll CW, Roll CCW, Arc CW, Arc CCW, Crane Up, Crane Down. (18 Tasks in Table 21) General Tracking, Aerial Tracking, Arc Tracking, Front-Side Tracking, Rear-Side Tracking, Lead Tracking, Tail Tracking, Tilt Tracking, Pan Tracking, Side Tracking, Tracking Subject Larger, Tracking Subject Smaller. (12 Tasks in Table 22) Only Zoom In, Only Zoom Out, Only Dolly In, Only Dolly Out, Only Pedestal Up, Only Pedestal Down, Only Truck Right, Only Truck Left, Only Pan Right, Only Pan Left, Only Tilt Up, Only Tilt Down, Only Roll CW, Only Roll CCW. (14 Tasks in Table 23) Complex Description. (1 Task) 28 Table 16. Motion & Steadiness Tasks"
        },
        {
            "title": "Clear Moving Camera",
            "content": "Positive: Does the camera have noticeable motion beyond minor shake or wobble? Positive: video where the camera has noticeable motion beyond minor shake or wobble. Negative: Is the camera free from noticeable motion beyond minor shake or wobble? Negative: video where the camera is free from noticeable motion beyond minor shake or wobble."
        },
        {
            "title": "Fixed Camera Shake",
            "content": "Positive: Is the camera completely still without any motion or shaking? Positive: video where the camera remains completely still with no motion or shaking. Negative: Is the camera stationary with minor vibrations or shaking? Negative: video where the camera is mostly stationary but has minor vibrations or shaking. Stable vs. Shaky Camera Positive: Is the camera movement exceptionally smooth and highly stable? Positive: video where the camera movement is exceptionally smooth and highly stable. Negative: Does the camera show noticeable vibrations, shaking, or wobbling? Negative: video where the camera shows noticeable vibrations, shaking, or wobbling. Fixed vs. Moving Camera Positive: Is the camera completely still without any visible movement? Positive: The camera is completely still without any visible movement. Negative: Is the camera not completely still and shows visible movement? Negative: The camera is not completely still and shows visible movement. Table 17. Scene Dynamics Tasks"
        },
        {
            "title": "Descriptions",
            "content": "Static vs. Dynamic Scene Positive: Is the scene in the video completely static? Negative: Is the scene in the video dynamic? Positive: video where the scene is completely static. Negative: video where the scene is dynamic and features movement."
        },
        {
            "title": "Frame Freeze Effect",
            "content": "Positive: Does the video contain frame freeze effect at any point? Positive: video that contains frame freeze effect at some point. Negative: Is the video free from any frame freeze effect? Negative: video that is free from any frame freeze effect. Table 18. Camera Motion Speed Tasks"
        },
        {
            "title": "Descriptions",
            "content": "Slow vs. Fast Movement Positive: Does the camera have noticeable motion but at slow motion speed? Positive: video where the camera has noticeable motion at slow speed. Negative: Does the camera have noticeable motion but at fast motion speed? Negative: video where the camera has noticeable motion at fast speed."
        },
        {
            "title": "Motion Blur Effect",
            "content": "Positive: Does the video contain noticeable motion blur? Positive: The video exhibits motion blur effect. Negative: Is the video free from any noticeable motion blur? Negative: The video is free from any noticeable motion blur. 29 Table 19. Camera Motion Direction Tasks"
        },
        {
            "title": "Descriptions",
            "content": "Dolly In vs. Out (Ground) Positive: Is the camera moving forward in the scene? Positive: shot where the camera is moving forward within the scene. Negative: Is the camera moving backward in the scene? Negative: shot where the camera is moving backward within the scene. Pedestal Up vs. Down (Ground) Positive: Does the camera move upward relative to the ground? Positive: The camera is moving upward relative to the ground. Negative: Does the camera move downward relative to the ground? Negative: The camera is moving downward relative to the ground. Truck Left vs. Right Pan Left vs. Right Tilt Up vs. Down Roll CW vs. CCW Positive: Does the camera move leftward in the scene? Negative: Does the camera move rightward in the scene? Positive: The camera moves leftward. Negative: The camera moves rightward. Positive: Does the camera pan to the left? Positive: The camera pans to the left. Negative: Does the camera pan to the right? Negative: The camera pans to the right. Positive: Does the camera tilt upward? Positive: The camera tilts upward. Negative: Does the camera tilt downward? Negative: The camera tilts downward. Positive: Does the camera roll clockwise? Positive: The camera rolls clockwise. Side Tracking Left vs. Right Positive: Is it side-tracking shot where the camera moves left to follow the subject? Positive: side-tracking shot where the camera moves left to follow the subject. Negative: Does the camera roll counterclockwise? Negative: The camera rolls counterclockwise. Negative: Is it side-tracking shot where the camera moves right to follow the subject? Negative: side-tracking shot where the camera moves right to follow the subject. Lead vs. Tail Tracking Positive: Is it tracking shot with the camera moving ahead of the subject? Positive: tracking shot where the camera moves ahead of the subject. Negative: Is it tracking shot with the camera following behind the subject? Negative: tracking shot where the camera follows behind the subject. Arc CCW vs. CW Positive: Does the camera move in counterclockwise arc? Positive: The camera arcs counterclockwise. Crane Up vs. Down Negative: Does the camera move in clockwise arc? Negative: The camera arcs clockwise. Positive: Is the camera craning upward in an arc? Positive: The camera cranes upward in an arc. Negative: Does the camera move downward in crane shot? Negative: The camera cranes downward in an arc. Dolly Zoom In vs. Out Positive: Does the shot feature dolly zoom effect with the camera moving backward and zooming in? Positive: The camera performs dolly zoom effect with backward movement and zoom-in. Negative: Does the shot feature dolly zoom effect with the camera moving forward and zooming out? Negative: The camera performs dolly zoom effect with forward movement and zoom-out. Zoom In vs. Out Positive: Does the camera zoom in? Positive: The camera zooms in. Negative: Does the camera zoom out? Negative: The camera zooms out. Table 20. Confusable Motion Tasks"
        },
        {
            "title": "Descriptions",
            "content": "Zoom In vs. Dolly In Zoom Out vs. Dolly Out Positive: Does the camera zoom in without physically moving forward? Positive: video where the camera zooms in without physically moving forward. Negative: Does the camera physically move forward without zooming in? Negative: video where the camera physically moves forward without zooming in. Positive: Does the camera zoom out without physically moving backward? Positive: video where the camera zooms out without physically moving backward. Negative: Does the camera physically move backward without zooming out? Negative: video where the camera physically moves backward without zooming out. Only Zoom In vs. Only Dolly In Positive: Does the camera only zoom in without any other camera movement? Positive: video where the camera only zooms in with no other movement. Negative: Does the camera only move forward without any other camera movement? Negative: video where the camera only moves forward with no other movement. Only Zoom Out vs. Only Dolly Out Positive: Does the camera only zoom out without any other camera movement? Positive: video where the camera only zooms out with no other movement. Pan Right vs. Truck Right Positive: Does the camera pan right without moving laterally to the right? Positive: The camera pans right without moving laterally to the right. Negative: Does the camera only move backward without any other camera movement? Negative: video where the camera only moves backward with no other movement. Pan Left vs. Truck Left Negative: Does the camera move laterally to the right without panning right? Negative: The camera moves laterally to the right without panning right. Positive: Does the camera pan left without moving laterally to the left? Positive: The camera pans left without moving laterally to the left. Negative: Does the camera move laterally to the left without panning left? Negative: The camera moves laterally to the left without panning left. Only Pan Right vs. Only Truck Right Positive: Does the camera only pan right with no other movement? Positive: video where the camera only pans right with no other movement. Only Pan Left vs. Only Truck Left Positive: Does the camera only pan left with no other movement? Positive: video where the camera only pans left with no other movement. Negative: Does the camera only move laterally to the right with no other movement? Negative: video where the camera only moves laterally to the right with no other movement. Tilt Up vs. Pedestal Up Negative: Does the camera only move laterally to the left with no other movement? Negative: video where the camera only moves laterally to the left with no other movement. Positive: Does the camera tilt up without moving physically upward? Positive: The camera tilts up without physically moving upward. Negative: Does the camera move physically upward without tilting up? Negative: The camera moves physically upward without tilting up. Tilt Down vs. Pedestal Down Positive: Does the camera tilt down without moving physically downward? Positive: The camera tilts down without physically moving downward. Negative: Does the camera move physically downward without tilting down? Negative: The camera moves physically downward without tilting down. Only Tilt Up vs. Only Pedestal Up Positive: Does the camera only tilt up with no other movement? Positive: video where the camera only tilts up with no other movement. Only Tilt Down vs. Only Pedestal Down Positive: Does the camera only tilt down with no other movement? Positive: video where the camera only tilts down with no other movement. Negative: Does the camera only move physically upward with no other movement? Negative: video where the camera only moves physically upward with no other movement. Dolly In Camera vs. Ground"
        },
        {
            "title": "Dolly Out Camera vs Ground",
            "content": "Pedestal Up Camera vs. Ground Pedestal Down Camera vs. Ground Negative: Does the camera only move physically downward with no other movement? Negative: video where the camera only moves physically downward with no other movement. Positive: Does the camera move forward only relative to its initial viewing direction but not relative to the ground? Positive: The camera moves forward only relative to its initial viewing direction but not relative to the ground. Negative: Does the camera move forward relative to both the ground and its initial viewing direction? Negative: The camera moves forward relative to both the ground and its initial viewing direction. Positive: Does the camera move backward only relative to its initial viewing direction but not relative to the ground? Positive: The camera moves backward only relative to its initial viewing direction but not relative to the ground. Negative: Does the camera move backward relative to both the ground and its initial viewing direction? Negative: The camera moves backward relative to both the ground and its initial viewing direction. Positive: Does the camera move upward only relative to its initial viewing direction but not relative to the ground? Positive: The camera moves upward only relative to its initial viewing direction but not relative to the ground. Negative: Does the camera move upward relative to both the ground and its initial viewing direction? Negative: The camera moves upward relative to both the ground and its initial viewing direction. Positive: Does the camera move downward only relative to its initial viewing direction but not relative to the ground? Positive: The camera moves downward only relative to its initial viewing direction but not relative to the ground. Negative: Does the camera move downward relative to both the ground and its initial viewing direction? Negative: The camera moves downward relative to both the ground and its initial viewing direction. 31 Table 21. Has Motion Tasks"
        },
        {
            "title": "Dolly In",
            "content": "Positive: Does the camera zoom in? Positive: The camera zooms in. Negative: Is the camera free from any zoom in effects? Negative: The camera is free from any zoom in effects. Positive: Does the camera zoom out? Positive: The camera zooms out. Negative: Is the camera free from any zoom out effects? Negative: The camera is free from any zoom out effects. Positive: Is the camera moving forward in the scene? Positive: The camera is moving forward within the scene. Negative: Is the camera free from any forward motion? Negative: The camera is free from any forward motion."
        },
        {
            "title": "Dolly Out",
            "content": "Positive: Is the camera moving backward in the scene? Positive: The camera is moving backward within the scene."
        },
        {
            "title": "Truck Right",
            "content": "Negative: Is the camera free from any backward motion? Negative: The camera is free from any backward motion. Positive: Does the camera move laterally to the left? Positive: The camera moves laterally to the left. Negative: Is the camera free from any leftward lateral movement? Negative: The camera is free from any leftward lateral movement. Positive: Does the camera move laterally to the right? Positive: The camera moves laterally to the right. Negative: Is the camera free from any rightward lateral movement? Negative: The camera is free from any rightward lateral movement."
        },
        {
            "title": "Pedestal Up",
            "content": "Positive: Does the camera move upward relative to the ground? Positive: The camera moves upward relative to the ground. Negative: Is the camera free from any upward pedestal motion? Negative: The camera is free from any upward pedestal motion."
        },
        {
            "title": "Pedestal Down",
            "content": "Positive: Does the camera move downward relative to the ground? Positive: The camera moves downward relative to the ground."
        },
        {
            "title": "Arc CCW",
            "content": "Negative: Is the camera free from any downward pedestal motion? Negative: The camera is free from any downward pedestal motion. Positive: Does the camera pan to the left? Positive: The camera pans to the left. Negative: Is the camera free from any leftward panning motion? Negative: The camera is free from any leftward panning motion. Positive: Does the camera pan to the right? Positive: The camera pans to the right. Negative: Is the camera free from any rightward panning motion? Negative: The camera is free from any rightward panning motion. Positive: Does the camera tilt upward? Positive: The camera tilts upward. Negative: Is the camera free from any upward tilting motion? Negative: The camera is free from any upward tilting motion. Positive: Does the camera tilt downward? Positive: The camera tilts downward. Negative: Is the camera free from any downward tilting motion? Negative: The camera is free from any downward tilting motion. Positive: Does the camera roll clockwise? Positive: The camera rolls clockwise. Negative: Is the camera free from any clockwise rolling motion? Negative: The camera is free from any clockwise rolling motion. Positive: Does the camera roll counterclockwise? Positive: The camera rolls counterclockwise. Negative: Is the camera free from any counterclockwise rolling motion? Negative: The camera is free from any counterclockwise rolling motion. Positive: Does the camera move in clockwise arc? Positive: The camera moves in clockwise arc. Negative: Is the camera free from any clockwise arc movement? Negative: The camera is free from any clockwise arc movement. Positive: Does the camera move in counterclockwise arc? Negative: Is the camera free from any counterclockwise arc movement? Positive: The camera moves in counterclockwise arc. Negative: The camera is free from any counterclockwise arc movement. 32 Table 22. Tracking Shot Tasks"
        },
        {
            "title": "General Tracking",
            "content": "Positive: Does the camera track the subject as they move? Positive: The camera tracks the subject as they move. Negative: Is the video not tracking shot? Negative: The video is not tracking shot."
        },
        {
            "title": "Aerial Tracking",
            "content": "Positive: Does the camera track the subject from an aerial perspective? Positive: The camera tracks the subject from an aerial perspective. Negative: Is the video not tracking shot from an aerial perspective? Negative: The camera is not tracking the subject from an aerial perspective."
        },
        {
            "title": "Arc Tracking",
            "content": "Positive: Does the camera follow the subject while moving in an arc? Positive: tracking shot where the camera follows the subject while moving in an arc. Negative: Is the video not tracking shot with arc movement? Negative: The camera is not tracking the subject with arc movement. Front-Side Tracking Positive: Is it tracking shot with the camera leading the subject from front-side angle? Positive: tracking shot where the camera leads the subject from front-side angle. Negative: Is the camera not leading the subject from front-side angle in tracking shot? Negative: The camera is not leading the subject from front-side angle in tracking shot. Rear-Side Tracking Positive: Is it tracking shot with the camera following behind the subject at rear-side angle? Positive: tracking shot where the camera follows behind the subject at rear-side angle. Negative: Is the camera not following behind the subject at rear-side angle? Negative: The camera is not following behind the subject at rear-side angle."
        },
        {
            "title": "Lead Tracking",
            "content": "Positive: Is it tracking shot with the camera moving ahead of the subject as they move? Positive: tracking shot where the camera moves ahead of the subject as they move. Negative: Is the camera not moving ahead of the subject in tracking shot? Negative: The camera is not moving ahead of the subject in tracking shot."
        },
        {
            "title": "Tail Tracking",
            "content": "Positive: Is it tracking shot with the camera following behind the subject as they move? Positive: tracking shot where the camera moves behind the subjects as they move. Negative: Is the camera not following behind the subject in tracking shot? Negative: The camera is not following behind the subject in tracking shot."
        },
        {
            "title": "Tilt Tracking",
            "content": "Positive: Does the camera tilt to track the subjects as they move? Positive: tracking shot where the camera tilts to follow the subjects. Negative: Is the camera not tilting to track the subjects? Negative: The camera is not tilting to track the subjects."
        },
        {
            "title": "Pan Tracking",
            "content": "Positive: Does the camera pan to track the subjects as they move? Positive: tracking shot where the camera pans to follow the subjects as they move. Negative: Is the camera not panning to track the subjects? Negative: The camera is not panning to track the subjects."
        },
        {
            "title": "Side Tracking",
            "content": "Positive: Is it tracking shot with the camera moving from the side to follow the subject as they move? Positive: tracking shot where the camera moves from the side to follow the subject. Negative: Is the camera not moving from the side to track the subject? Negative: The camera is not moving from the side to track the subject."
        },
        {
            "title": "Tracking Subject Larger",
            "content": "Positive: Does the subject appear larger during the tracking shot? Positive: The subject looks larger during the tracking shot. Negative: Does the subject being tracked not appear larger in size? Negative: The subject being tracked does not appear larger in size."
        },
        {
            "title": "Tracking Subject Smaller",
            "content": "Positive: Does the subject appear smaller during the tracking shot? Positive: The subject looks smaller during the tracking shot. Negative: Does the subject being tracked not appear smaller in size? Negative: The subject being tracked does not appear smaller in size. 33 Table 23. Only Motion Tasks"
        },
        {
            "title": "Only Zoom In",
            "content": "Positive: Does the camera only zoom in with no other movement? Positive: The camera only zooms in without any other movement. Negative: Does the camera not just zoom in? Negative: The camera does not just zoom in."
        },
        {
            "title": "Only Zoom Out",
            "content": "Positive: Does the camera only zoom out with no other movement? Positive: The camera only zooms out without any other movement. Negative: Does the camera not just zoom out? Negative: The camera does not just zoom out."
        },
        {
            "title": "Only Dolly In",
            "content": "Positive: Does the camera only move forward (not zooming in) with respect to the ground? Positive: The camera only moves forward (not zooming in) relative to the ground. Negative: Does the camera not just move forward with respect to the ground? Negative: The camera does not just move forward relative to the ground."
        },
        {
            "title": "Only Dolly Out",
            "content": "Positive: Does the camera only move backward (not zooming out) with respect to the ground? Positive: The camera only moves backward (not zooming out) relative to the ground. Negative: Does the camera not just move backward with respect to the ground? Negative: The camera does not just move backward relative to the ground."
        },
        {
            "title": "Only Pedestal Up",
            "content": "Positive: Does the camera only move upward (not tilting up) with respect to the ground? Positive: The camera only moves upward (not tilting up) relative to the ground. Negative: Does the camera not just move physically upward? Negative: The camera does not just move physically upward."
        },
        {
            "title": "Only Pedestal Down",
            "content": "Positive: Does the camera only move downward (not tilting down) with respect to the ground? Positive: The camera only moves downward (not tilting down) relative to the ground. Negative: Does the camera not just move physically downward? Negative: The camera does not just move physically downward."
        },
        {
            "title": "Only Truck Right",
            "content": "Positive: Does the camera only move rightward without any other camera movements? Positive: The camera only moves rightward without any other camera movements. Negative: Does the camera not just move laterally to the right? Negative: The camera does not just move laterally to the right."
        },
        {
            "title": "Only Truck Left",
            "content": "Positive: Does the camera only move leftward without any other camera movements? Positive: The camera only moves leftward without any other camera movements. Negative: Does the camera not just move laterally to the left? Negative: The camera does not just move laterally to the left."
        },
        {
            "title": "Only Pan Right",
            "content": "Positive: Does the camera only pan rightward without any other camera movements? Positive: The camera only pans rightward without any other camera movements. Negative: Does the camera not just pan right? Negative: The camera does not just pan right."
        },
        {
            "title": "Only Pan Left",
            "content": "Positive: Does the camera only pan leftward without any other camera movements? Positive: The camera only pans leftward without any other camera movements. Negative: Does the camera not just pan left? Negative: The camera does not just pan left."
        },
        {
            "title": "Only Tilt Up",
            "content": "Positive: Does the camera only tilt upward without any other camera movements? Positive: The camera only tilts upward without any other camera movements. Negative: Does the camera not just tilt up? Negative: The camera does not just tilt up."
        },
        {
            "title": "Only Tilt Down",
            "content": "Positive: Does the camera only tilt downward without any other camera movements? Positive: The camera only tilts downward without any other camera movements. Negative: Does the camera not just tilt down? Negative: The camera does not just tilt down."
        },
        {
            "title": "Only Roll CW",
            "content": "Positive: Does the camera only roll clockwise without any other camera movements? Positive: The camera only rolls clockwise without any other camera movements. Negative: Does the camera not just roll clockwise? Negative: The camera does not just roll clockwise."
        },
        {
            "title": "Only Roll CCW",
            "content": "Positive: Does the camera only roll counterclockwise without any other camera movements? Positive: The camera only rolls counterclockwise without any other camera movements. Negative: Does the camera not just roll counterclockwise? Negative: The camera does not just roll counterclockwise."
        }
    ],
    "affiliations": [
        "Adobe",
        "CMU",
        "Emerson",
        "Harvard",
        "MIT-IBM",
        "UMass Amherst",
        "USC"
    ]
}