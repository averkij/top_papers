{
    "paper_title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
    "authors": [
        "Shuai Tan",
        "Biao Gong",
        "Yutong Feng",
        "Kecheng Zheng",
        "Dandan Zheng",
        "Shuwei Shi",
        "Yujun Shen",
        "Jingdong Chen",
        "Ming Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 8 0 3 0 . 2 1 4 2 : r Mimir: Improving Video Diffusion Models for Precise Text Understanding Shuai Tan1*, Biao Gong1*, Yutong Feng2, Kecheng Zheng1, Dandan Zheng1, Shuwei Shi1, Yujun Shen1, Jingdong Chen1, Ming Yang1 1Ant Group 2Tsinghua University Figure 1. Samples generated by Mimir. Our model demonstrates powerful spatiotemporal imagination for input text prompts, e.g., (row-3) physically accurate petals, (row-4) the desert with illumination harmonization, which closely match human cognition. Abstract Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to *Equal contribution. Work done during internship at Ant Group. Project lead and corresponding author. prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: https://lucaria-academy.github.io/Mimir/ 1 1. Introduction Language is the most natural and efficient way for human to convey perspectives and creative ideas after thousands of years of evolution [14, 17]. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension [6, 21, 40]. Many diffusion based studies have explored powerful text encoders such as CLIP [33] and T5 [34], which still yield limited text understanding, particularly in video generation. In fact, human-provided concise prompts cannot capture the vast spatiotemporal visual details in videos, such as the speed of moving car or background changes along its route. This limitation has motivated researchers to explore semantic enhancement using large language models (LLMs), given their remarkable capabilities in text-related tasks [1, 45]. The recent success of LLMs showcases the power of decoder-only transformers, which offers three clear benefits for T2V generation. Firstly, it ensures precise text understanding which stems from terabytes of training data and the scalability of LLMs. Secondly, the capability for next token prediction allows the model to generate imaginative content that extends beyond the original input text, demonstrating creativity and contextual extrapolation. Finally, instruction tuning facilitates flexibility in prioritizing user interests, allowing the model to adapt its responses according to specific user directives. Therefore, we aim to achieve the integration of heterogeneous (i.e., encoder and decoderonly) LLMs to improve video diffusion models especially for precise text understanding. Achieving such integration is challenging due to the inherent volatility of decoder-only language models, i.e., these models prioritize predicting future tokens over representing the current text [31, 45, 51], thereby leading to the feature distribution gap and hindering the direct use of LLMs in established T2V models. promising approach involves fine-tuning decoder-only model to function as an encoder [45]. Recent T2I [14, 31, 51] have also explored various methods to enhance text prompt encoding. However, we contend that these strategies constrain the full potential of decoder-only LLMs, particularly regarding their reasoning capabilities through next token prediction. In this paper, we introduce Mimir which is an endto-end training framework featuring carefully tailored Token Fuser to harmonize the outputs from text encoders and decoder-only language models. Such design allows Mimir to fully leverage learned video priors while capitalizing on the text-related capabilities of LLMs. Specifically, the token fuser consists of two components. (1) It achieves non-destructive fusion by using Zero-Conv layers to merge the encoder tokens with all query and answer tokens generated by the decoder-only model. This integration fully takes advantage of the LLMs capacity for reasoning. (2) Figure 2. The core idea of Mimir. Text Encoder is well suited for fine-tuning pre-trained T2V models (), however it struggles with limited text comprehension ((cid:37)). In contrast, Decoder-only LLM excels at precise text understanding (), but cannot be directly used in established video generation models since the feature distribution gap and the feature volatility ((cid:37)) . Therefore, we propose the token fuser in Mimir to harmonize multiple tokens, achieving precise text understanding () in T2V generation (). The proposed semantic stabilizer, which employs learnable parameters to stabilize fluctuating text features (e.g., features from different answers like old car, dilapidated machine, and speeding car, which are detailed in Sec. 2.3 and Sec. 3.3). In summary, as shown in Fig. 2, Mimir integrates the decoder-only language model and ViT-style text encoder, trained within the diffusion framework to achieve precise text understanding () in T2V generation (). Extensive quantitative and qualitative  (Fig. 1)  results demonstrate the effectiveness of our approach in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. 2. Methodology In this section, we first present the preliminaries of diffusion models in Section 2.1 and describe the different types of tokens in Section 2.2. Then, we introduce the details of the token fuser in Section 2.3, which consists of two components: Non-Destructive Fusion and Semantic Stabilizer. 2.1. Preliminary To lower the high training and inference costs of running diffusion models directly in pixel space, most diffusionbased models now follow the approach introduced by Rombach et al. [36] known as latent diffusion models. This method typically consists of three key components: (a) Perceptual Video Compression and Decompression: To efficiently handle video data, pre-trained visual encoder [11] is used to map the input video into latent representation z. corresponding visual decoder is then employed to reconstruct the latent representation back into the pixel Figure 3. The framework of Mimir. Given text prompt, we employ text encoder and decoder-only large language model to obtain eθ and eβ. Additionally, we add an instruction prompt which, after processing by the decoder-only model, yields the corresponding instruction token ei. See token details in Sec. 2.2. To prevent any convergence issue in training caused by the feature distribution gap of eθ and eβ, the proposed token fuser first applies normalization layer and learnable scale to eβ. It then uses Zero-Conv to preserve the original semantic space in the early of training. These modified tokens are then summed to produce Rn4096. Meanwhile, we initialize four learnable tokens el, which are added to ei to stabilize divergent semantic features. Finally, the token fuser concatenates and es to generate videos. space, yielding the reconstructed video ˆx = D(E(x)). (b) Semantic Encoding: The text encoder is utilized to encode given prompt into the text feature, which serves as the controlling signal for the content of the generated video. (c) Diffusion Models in Latent Space: To model the actual video distribution, diffusion models [19, 39] are used to denoise normally distributed noise, aiming to reconstruct realistic visual content. Recent works on video generation commonly apply Diffusion in Transformer to carry out the denoising process. This process simulates the reverse of Markov chain with length of . To reverse the process in latent space, noise ϵ is added to to obtain noise-corrupted latent zt as described in [36]. Subsequently, Vision Transformer ϵθ is used to predict the noise from zt, the text embedding eθ, and the timestamp {1, ..., }. The optimization objective for this process can be formulated as: = EE(x),ϵN (0,1),eθ,t (cid:104) ϵ ϵθ (zt, eθ, t)2 (cid:105) , (1) where eθ refers to the text embedding. After the reversed denoising stage, the predicted clean latent is fed into the VAE decoder to reconstruct the predicted video. 2.2. Patches and Tokens Video Tokens. Following [36, 53], the core of video token construction lies in compressing the original RGB-T video into latent space and segmenting each frame of the video. Specifically, we represent video as R(N +1)HW 3, (2) where (N + 1) represents the number of frames, and represent the height and width of each frame, respectively. Then we employ 3D causal VAE [18] to compress it to the video latents R(n+1)hwC = E(x). Following the common setting in 3D VAEs for LVDMs [10, 59, 60], the temporal rate N/n and spatial rate H/h = W/w are set as 4 and 8, respectively. Subsequently, we patchify the video latents to generate visual token sequence zvision (n+1) with the length p. Text Tokens. We provide two types of text tokens. (1) Using the text encoder τθ, such as T5 [34], to capture stable word-level text tokens from the input prompt . The process of converting text to tokens is referred to eθ = τθ(T ). (2) Using the decoder-only LLM τβ, such as Phi3.5 [1], leveraging its detailed understanding and reasoning capabilities [51], to capture text tokens with fluctuations but richer semantics. To fully preserve the extensive semantic capabilities, we retain all query and answer tokens as the final decoder-only tokens eβ = τβ(T ). Besides, we also feed decoder-only LLM with four instruction prompts to generate instruction tokens ei. In the following sections, we will explain how the combination of these video and text tokens trains the transformer based T2V diffusion model. 2.3. Token Fuser Non-Destructive Fusion. As show in Fig. 3, our method consists of two language branches (i.e. encoder branch and decoder-only branch) and vision transformer ϵθ. Both branches of language encode the input prompt into tokens, and their sum is passed to ϵθ. To mitigate the incompatibility between these embeddings, we implement two effective schemes: (1) Normalization and Scaling [51]: We insert normalization layer followed by small learnable scale factor and bias directly after the decoder-only LLM. This step ensures that the two types of text tokens are brought to similar scale, allowing them to be aligned (2) Zero Convolution Layer: we in the fusion process. introduce zero-conv layer Zβ after the decoder-only token eβ. This ensures that the embedding eβ starts as zero at the beginning of training. Since our goal is to provide semantics 3 Table 1. Quantitative results on VBench [25]. The best and second results for each column are bold and underlined, respectively. Method ModelscopeT2V [30] OpenSora [60] OpenSoraPlan [28] CogVideoX-2B [53] CogVideoX-5B [53] Mimir Imaging Object Multiple Background Aesthetic Objects Class Quality Quality Consistency 1.52% 31.17% 55.85% 37.14% 92.00% 64.81% 63.38% 90.79% 58.57% 97.20% 26.98% 57.79% 67.39% 59.40% 97.50% 65.70% 84.86% 60.52% 60.27% 94.71% 60.62% 95.60% 65.70% 87.82% 61.35% 62.92% 63.91% 92.87% 85.29% 97.68% Color Spatial Consistency Relationship 63.20% 84.67% 83.38% 86.21% 84.17% 86.50% 8.26% 76.63% 38.69% 70.49% 64.86% 78.67% Temporal Style 14.52% 25.51% 21.86% 25.10% 25.86% 26.22% Figure 4. Comparison between CogVideoX-5B with Mimir in T2V, where Mimir generates the vivid stunning moment of rocket launch. from textual inputs to the Vision Transformer model ϵθ from both the encoder and decoder-only branches, we need to balance their contributions throughout training. Thus, we also insert zero-conv layer Zθ after the encoder τθ in residual manner, ensuring that the embedding eθ starts equal to the original tokens at the beginning. Compared to other commonly used adapters such as LoRA, zero-conv is lightweight and can smoothly achieve domain adaptation for textual or visual features. Afterwards, we sum the embeddings as = eθ + α eβ and feed into the Vision Transformer ϵθ, where α indicates the weight for decoderonly token: eθ = τθ(T ) + Zθ(τθ(T )), eβ = Zβ(τβ(T )) (3) Semantic Stabilizer. The Semantic Stabilizer serves two primary functions: (1) To ensure the denoising model (i.e., the vision transformer) accurately captures the essential semantic elements in the prompt, such as object, color, motion, and spatial relationships. (2) To stabilize the fluctuating textual features that emerge during next-token predictions, i.e., different descriptions of car such as old car and dilapidated machine, which we analyze in detail in Sec. 3.3. Specifically, we begin by generating instruction tokens ei based on four pre-defined, attributespecific instructions (e.g., Describe the detailed objects in the video). Next, we initialize four learnable tokens el with the same shape, designed as bridges to align with the visual space, resulting in our final semantic token es = ei + el. We then concatenate es along the sequence dimension to the previously defined token e. Finally, we concatenate the text-based token embeddings with the video embeddings and feed them together into the diffusion process. To train the model, we minimize the diffusion loss, reducing the discrepancy between the predicted noise and the groundtruth noise during optimization. The overall loss is LSeeD = EE(x),ϵN (0,1),T ,t (cid:104) ϵ ϵθ (zt, es, t)2 2 where refers to concatenation operation. (cid:105) , (4) 4 Table 2. User study results. The best and second results for each column are bold and underlined, respectively. Method Instruction Following Physics Simulation Visual Quality ModelScopeT2V OpenSora OpenSoraPlan CogVideoX-2b CogVideoX-5b 2.45% 3.50% 1.60% 52.15% 47.95% 49.20% 27.75% 54.75% 41.50% 63.50% 52.85% 54.80% 72.15% 57.30% 63.25% Mimir 82.00% 83.65% 89.65% Figure 5. Mimir demonstrates spatial comprehension and imagination, e.g., quantities, spatial relationships, colors, etc. Figure 6. Mimir demonstrates temporal comprehension and imagination, e.g., direction, order of motion and appearance / disappearance. 3. Experiments In this section, we comprehensively evaluate our method and provide detailed analysis of the reasons behind the effectiveness of our improvements, as well as the advantages of our approach in video generation performance. 3.1. Text-to-Video Generation Experimental Setup. In the setting of LLMs, we select Phi-3.5 [1] mini-instruct version as our decoder-only LLM to achieve balance between computational efficiency and performance. In the setting of diffusion models, we implement v-prediction [38] and zero SNR [29], following the noise schedule established in LDM [36]. We collect 500,000 high-quality video clips to train the Mimir model. We compare our approach against publicly accessible top-performing text-to-video models, including ModelscopeT2V [30], OpenSora [60], OpenSoraPlan [28], CogvideoX-2B [53], and CogvideoX-5B [53]. To evaluate the text-to-video generation, we employ several metrics from VBench [25]: Background Consistency to assess temporal quality, Aesthetic Quality and Imaging Quality for frame-wise evaluation, as well as Object Class, Multiple Objects, Color Consistency, Spatial Relationship, and Temporal Style for semantic understanding. Quantitative Evaluation. Tab. 1 presents the evaluation results. Mimir outperforms existing approaches across all metrics. Notably, it shows significant improvements in the Multiple Objects and Spatial Relationship metrics. These 5 Table 3. Ablation study results. The best and second results for each column are bold and underlined, respectively. Method Baseline B+Decoder-only B+Decoder-only+Norm B+Decoder-only+Norm+SS B+Decoder-only+ZeroConv B+Decoder-only+ZeroConv+SS Mimir Object Multiple Imaging Background Aesthetic Objects Class Quality Quality Consistency 65.70% 87.82% 61.35% 60.62% 95.60% 0.00% 60.10% 4.97% 36.38% 94.66% 65.24% 62.52% 85.50% 61.68% 97.12% 68.83% 87.18% 62.49% 58.11% 96.48% 84.98% 62.99% 92.03% 61.21% 97.20% 62.14% 97.33% 84.47% 63.02% 91.21% 62.92% 63.91% 92.87% 85.29% 97.68% Color Spatial Consistency Relationship 84.17% 37.50% 84.85% 85.21% 86.21% 86.43% 86.50% 64.86% 2.36% 59.28% 67.86% 69.17% 70.16% 78.67% Temporal Style 25.86% 3.66% 25.26% 24.33% 25.03% 23.68% 26.22% Figure 7. Visualization by t-SNE: (a) Given 50 prompts, we obtain the corresponding tokens using Encoder branch, Decoder-only branch and their sum, i.e., Mimir. (b) We feed one prompt into Decoder-only branch for 50 times to generate 50 query tokens, answer tokens and final tokens. Differences in feature distribution: (c) The original distribution of T5 encoder and Phi-3.5 Decoder. (d) The distribution of T5 encoder and Phi-3.5 Decoder after normalization across different value ranges. results demonstrate that, with the assistance of LLMs, the video generation model achieves marked performance enhancement compared to models that rely solely on the T5 encoder for semantic modeling. Qualitative Evaluation. Fig. 4 shows the comparison results between Mimir and the state-of-the-art method. With the support of the decoder-only Phi-3.5, Mimir is able to understand the input text prompt precisely, such as color, multiple objects, and quantities. Additionally, Mimir produces videos with high quality, showcasing its superior generative performance. User Study. To evaluate the quality of Mimir and the SOTAs from human perspective, we conducted blind user study with 10 participants. We randomly select 20 prompts and feed them into each compared method and Mimir, resulting in total of 120 video clips. Each participant is shown two videos generated by different methods for the same prompt and asked to choose which one performed better in terms of Instruction Following, Physics Simulation, and Visual Quality. This process is repeated 6 2 times. The results in Tab. 2 show Mimir superior performance across all aspects. 3.2. Ablation Studies Key Component To assess the effectiveness of the components in Mimir, we conduct an ablation study in progressive manner. the experiments are Specifically, (1) Baseline: Only T5 is used as arranged as follows: the text encoder, with all other LLM components removed. (2) B+Decoder-only: The encoder token and Decoder-only token are directly combined. (3) B+Decoder-only+Norm: The Decoder-only token undergoes normalization before being combined with the encoder token. (4) B+Decoderonly+Norm+SS: The Semantic Stabilizer is added on top of (3). (5) B+Decoder-only+ZeroConv: The Encoder and Decoder-only tokens are fused using the Zero Conv method. (6) B+Decoder-only+ZeroConv+SS: The SS is added on (7) Mimir: The complete model with all top of (5). components. The experimental results, as shown in Tab. 3, reveal that the direct combination of encoder and decoderonly tokens in (2) leads to model collapse due to the differences between the two token types. Normalization in (3) alleviates this issue, but semantic errors persist. With the addition of the Semantic Stabilizer in (4), preliminary In (5), the Zero understanding of semantics is achieved. Conv method smoothly combines the two tokens. Mimir, with the aid of all modules, achieves the best performance. Spatial Comprehension and Imagination. Based on the design of token fuser, our method accurately comprehends complex prompts, such as quantities, spatial relationships, and colors. For each aspect, we provide our method with 2 interesting prompts, as illustrated in Fig. 5. Temporal Comprehension and Imagination. Another crucial aspect of the text-to-video task is temporal comprehension. This means that the generated video should not only meet requirements such as quantities, spatial relationships, and colors, as shown in Fig. 5, but also maintain the coherence and order between frames according to the prompts instructions. Therefore, we further provide our method with several temporally related prompts. As shown Figure 8. We present more cases generated by Mimir. in Fig. 6, Mimir accurately understands the temporal relationships in the instructions, such as sequences from left to right or right to left. Moreover, when multiple actions are involved, our method comprehends the order of actions and generates the corresponding video. 3.3. Visualization and Analysis Due to the different optimization functions of the encoder there is significant gap and the decoder-only model, between their latent spaces. This gap can increase the difficulty of training, and may even lead to training collapse. To address this issue, we propose two solutions in Sec. 2: (1) adding normalization layer and learnable scale, and (2) incorporating zero convolution layer. For the first solution, we randomly sample several prompts and encode them using encoder and decoderonly model, resulting in the corresponding encoder tokens and decoder-only tokens. We then input the decoder-only tokens into the normalization layer to obtain the normalized decoder-only tokens. Subsequently, we count the number of tokens within each numerical range. As visualized in Fig. 7 (c), the original encoder tokens values are concentrated 7 between -0.5 and 0.5, while decoder-only model has much wider range that exceeds the -1 to 1 limits. After normalization in Fig. 7 (d), the magnitudes of the decoderonly tokens align with those of the encoder tokens, thereby reducing the training difficulty through this adjustment. For the second solution, we use t-SNE [46] to reveal the distribution gap between encoder and decoder-only tokens, which is shown in Fig. 7 (a). The zero convolution prevents the direct summation of features with different distributions, allowing for gradual integration of both types of tokens in the visual transformer during training. Moreover, we explore the feature fluctuations of the same prompt in both text encoder and decoder-only language model. Specifically, we randomly sample one prompt and encode it with decoder-only for 50 times, resulting in corresponding query tokens and answer tokens. The results are shown in Fig. 7(b). We observe that the query tokens produce identical embeddings when encoding the same prompt multiple times, represented as single point. In contrast, the results of answer tokens demonstrate the generative ability and the inherent volatility of decoder-only models, as encoding the same prompt yields broader range of features. This phenomenon arises from the powerful reasoning abilities of decoder-only language models. Specifically, when prompting LLMs to describe car, even with identical input text, the responses may vary (e.g., old car, dilapidated machine), leading to the feature fluctuations. However, completely eliminating these fluctuations would undermine the inherent strengths of decoder-only LLMs. Therefore, we use stable stabilizer to actively limit fluctuations and perform adaptive distribution adjustments for stable training. 4. Related Work Text-to-Video Generation. Video diffusion models [20] trains image and video jointly using 3D U-Net architecture and text conditions to handle the additional temporal dimension. Additionally, PYoCo [16] explores finetuning pretrained image diffusion model with video data as practical solution for the video synthesis task. It utilizes noise prior and pre-trained eDiff-I [3] model for generating videos. Subsequently, SVD [6] pretrains UNet-based image generation model [36] and then add temporal layer for video generation. Recently, inspired by the success of transformer-based model in text-to-image task, some works [21, 53, 60] utilize diffusion transformer architecture to tackle challenges in long-duration and highresolution video generation. However, current methods simply use CLIP or T5 as text encoder, which limit the text understanding. Therefore, we aim to integrate superior decoder-only LLMs (such as Phi3) into video diffusion model for optimizing the generated results by leveraging their precise understanding and reasoning capabilities. Large Language Model for Diffusion Framework. Language models play crucial role in image / video generation, and even some works [27, 5557] use only LLMs to generate images or videos, which reveals the powerful capability of LLMs. However, most of current diffusion models have not fully utilized the advantages of LLMs. common practice is viewing the language model as text encoder for extracting semantics. In the beginning, CLIP [33] first demonstrates the text-image alignment, and is therefore very popular in image-aligned semantic modeling among various text-to-image generation models [32, 35, 36, 41]. With the advent of the T5 series which are pretrained on text-only corpora, Imagen [37] observes that T5 is effective at encoding text for image synthesis. Many works [5, 79, 12] adopt the T5 series as the text encoding model. Recently, considering the superior text comprehension capabilities of decoder-only LLMs [2, 4245, 52, 54], some works [15, 23, 31, 50, 58] try to introduce LLMs into the designed framework. On one hand, LLM2Vec [4] discovers the potential for decoder-only methods to outperform encoder-only methods in both wordlevel and sequence-level tasks in an unsupervised manner. One the other hand, LiDiT [31] and SANA [51] provide LLM with complex instructions to encode the prompt for the semantic embedding and train DiT from scratch for image generation. Although ParaDiffusion [50] and LaViBridge [58] introduce an adapter to bridge Phi3 and pretrained generative vision models (i.e., PixArt [7]), we found that the simple adapter does not perform well in video generation due to the complexity of temporal modeling. Therefore, to the best of our knowledge, our Mimir is the first work to integrate Phi3 into the video diffusion framework. The core of Mimir is the proposed token fuser which stabilizes fluctuating text features and achieves nondestructive integration of heterogeneous (i.e., encoder and decoder-only) LLMs. Such design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. 5. Conclusion In this paper, we propose text-to-video diffusion model, Mimir, which leverages large language model embeddings within the video diffusion transformer to achieve precise text understanding for video spatiotemporal semantics. The core innovation of our approach lies in the token fuser, which fuse semantic features from encoder and decoderonly language models with different distributions. Ablation studies and visualizations validate the effectiveness of Mimir. Extensive quantitative and qualitative comparisons, along with detailed user study, demonstrate the superior performance of our method. Please refer to Supplementary Materials to view the limitations, ethical considerations and other details."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 2, 3, 5 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 8, 2 [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 8 [4] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Llm2vec: Large language models are secretly Reddy. powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. 8 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 8 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 8 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-toimage synthesis. ArXiv, abs/2310.00426, 2023. [8] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [9] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-{delta}: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. 8 [10] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for imarXiv preprint proving latent video diffusion model. arXiv:2409.01199, 2024. 3 [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified In flow transformers for high-resolution image synthesis. Forty-first International Conference on Machine Learning, 2024. 8 [13] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36773686, 2020. [14] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion In Proceedings of the for accurate instruction following. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47444753, 2024. 2 [15] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 8 [16] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2293022941, 2023. 8 [17] Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, and Yu Liu. Check locate rectify: trainingfree layout calibration system for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66246634, 2024. 2 [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 8 [21] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, 8 [22] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 2 [23] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 8 Ella: [24] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image genera9 tion. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 2 [25] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4, 5, 2 [26] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 2 [27] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [28] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 4, 5 [29] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. 5 [30] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models In Proceedings of the for high-quality video generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4, 5 [31] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models. arXiv preprint arXiv:2406.11831, 2024. 2, 8 [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 2, 8 [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 2, 3 [35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 8 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In CVPR, pages synthesis with latent diffusion models. 1068410695, 2022. 2, 3, 5, 8 [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 8 [38] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [40] stability.ai. Stable Diffusion 2.0 Release, 2022. 2 [41] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14214 14223, 2023. 8 [42] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 8 [43] InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. 2, 8 [46] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (11), 2008. 8 [47] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan 10 [59] Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. arXiv preprint arXiv:2405.20279, 2024. 3 [60] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, march 2024. URL https://github. com/hpcaitech/Open-Sora, 1(3):4, 2024. 3, 4, 5, Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. 2 [48] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2 [49] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In European Conference on Computer Vision, pages 207224. Springer, 2025. 2 [50] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Paragraph-to-image generation with information-enriched diffusion model. arXiv preprint arXiv:2311.14284, 2023. 8 [51] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 2, 3, 8 [52] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 4, 5, 8 [54] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 8 [55] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 8 [56] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [57] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36, 2024. 8 [58] Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, and Kwan-Yee Wong. Bridging different language models and generative vision models for text-to-image generation. arXiv preprint arXiv:2403.07860, 2024. 11 Mimir: Improving Video Diffusion Models for Precise Text Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "In the main paper, we provide method diagram and textual description of Mimir. Here, we present the detailed pseudocode of the Token Fuser in Mimir in Algorithm 1 for direct reference. In the following sections, We introduce the data processing in Sec. A, the evaluation metric in Sec. B, the user study in Sec. C, and additional experimental results in Sec. D. We also introduce limitations and social impact of our work in Sec. and Sec. respectively. A. Data Processing We construct collection of relatively high-quality video clips with text descriptions using combination of video filtering and recaptioning models. As shown in Fig. 9, the collected data undergoes multiple filtration steps: Basic Filtration, Quality Filtration, Aesthetic Filtration, Watermark Filtration, which removes data that does not meet fundamental requirements. After these video-based filtration steps, captions are generated for the videos. The videos and their captions are then evaluated for consistency to ensure the caption accurately describes the video content. Following this process, approximately 500,000 single-shot clips remain, with each clip averaging about 10 seconds. These high-quality video clips are ultimately used for training Mimir. Next, we provide detailed explanation of each stage of this pipeline. Basic Filtration. At this stage, we focus on computing video metadata and filtering out invalid videos. 1. Metadata Extraction: Most of important video properties such as length, width, frame rate, frame count, and duration are obtained and saving using FFmpeg. 2. Filtering Rules: Videos with fewer than 65 frames, duration of less than 1s, or an aspect ratio (width / height) outside the range [1, 2] are excluded. Videos with motion score of 0, determined using optical flow, are excluded. Quality Filtration. At this stage, we calculate basic quality indicators for the videos and remove those that do not meet the standards. 1. Quality Metrics: We use OpenCV to calculate the black area percentage, brightness, and black frame rate. 2. Filtering Rules: Black area > 0.8, excluding. Brightness < 0.2, excluding. Black frame rate > 0.4, excluding. Aesthetic Filtration. At this stage, we filter videos based on aesthetic-related operators. Algorithm 1 Token Fuser # Inputs # Text prompt provided by the user text_prompt = \"Input text prompt\" # Instructional input for fine-tuning instruction_prompt = \"Instruction description\" # 1. Encoding and Tokenization # Obtain token embeddings from text encoder e_theta = TextEncoder(text_prompt) # Obtain token embeddings from decoder-only model e_beta = DecoderModel(text_prompt) # Obtain instruction token from decoder-only model e_i = DecoderModel(instruction_prompt) # 2. Token Fusion to Address Feature Distribution Gap # Normalize e_beta and apply learnable scale # Apply normalization e_beta = Normalize(e_beta) # Scale normalized features e_beta = LearnableScale(e_beta) # Apply Zero-Conv to e_beta and e_theta to maintain original semantic space # Maintain semantic space for e_beta e_beta = ZeroConv(e_beta) # Maintain semantic space for e_theta e_theta = e_theta + ZeroConv(e_theta) # Sum modified tokens to form combined tokens = e_theta + e_beta # Shape: [n, 4096] # 3. Stabilizing Divergent Semantic Features # Initialize learnable tokens and add to instruction tokens # Four learnable tokens, shape: [4, 4096] e_l = InitializeLearnableTokens(count=4, dim=4096) # Stabilize instruction features e_s = e_i + e_l # 4. Final Token Fusion and Video Generation # Concatenate e_combined and stabilized tokens e_final = Concatenate(e_combined, e_stabilized) # Shape: [n+4, 4096] # Generate videos using the final fused tokens generated_video = VideoGenerator(e_final) # Output return generated_video Figure 9. The pipeline for preparing data. 1 1. Aesthetic Metrics: We use the aesthetic predictor 1 to calculate aesthetic score and OCR coverage. 2. Filtering Rules: Aesthetic score < 4.0, excluding. OCR coverage > 0.1, excluding. Watermark Filtration. At this stage, videos containing watermarks are excluded. Each video is analyzed using QWen2-VL-7B [2] to detect the presence of watermarks. Videos flagged as containing watermarks are excluded. Re-Caption. At this stage, we use CogVim2 [22, 47] to generate captions, which produces semantic and detailed descriptions of visual contents in videos. Caption Filtration. Due to hallucinations in large language models, not all output captions are immediately usable. To address this, we employ human designed rule-based methods and text quality metrics to clean the captions. 1. Text Quality Metrics: N-gram 2 repetition rates Semantic alignment between the video and the generated caption using CLIP Score. 2. Filtering Rules: 2-gram repetition > 0.056, excluding. 5-gram repetition > 0.047, excluding. 10-gram repetition > 0.045, excluding. Semantic consistency (CLIP score) < 0.25, excluding. This pipeline ensures the collection of high-quality video clips with accurate captions, which are suitable for training. B. Evaluation Metric We employ several evaluation metrics in VBench [25] to quantitatively assess our results, including Background Consistency, Aesthetic Quality, Imaging Quality, Object Class, Multiple Objects, Color Consistency, Spatial Relationship, and Temporal Style. The detailed metrics are introduced as follows: Background Consistency. This metric evaluates the temporal consistency of background scenes by calculating the similarity of CLIP [33] features across frames. Aesthetic Quality. This assesses the artistic and aesthetic value perceived by humans for each video frame using the LAION aesthetic predictor. It reflects qualities such as layout, color richness and harmony, photo-realism, naturalness, and overall artistic quality across frames. Imaging Quality. This measures distortions (e.g., overexposure, noise, blur) present in generated frames. It is evaluated using the MUSIQ [26] image quality predictor trained on the SPAQ [13] dataset. Object Class. This metric is computed using GRiT [49] to measure the success rate of generating the specific object classes described in the text prompt. 1https://github.com/christophschuhmann/improved-aesthetic-predictor 2https://github.com/EurekaLabsAI/ngram Multiple Objects. This evaluates the success rate of generating all the objects specified in the text prompt within each video frame. Beyond generating single object, it assesses the models ability to compose multiple objects from different classes in the same frame, which is an essential aspect of video generation. Color Consistency. This measures whether the synthesized object colors align with the text prompt. It uses GRiT [49] for color captioning and compares the results against the expected color. Spatial Relationship. This metric evaluates whether the spatial relationships in the generated video follow those specified by the text prompt. It focuses on four primary types of spatial relationships and performs rule-based evaluation similar to [24]. Temporal Style. This assesses the consistency of temporal style by using ViCLIP [48] to calculate the similarity between video features and temporal features. C. User Study To obtain genuine feedback reflective of practical applications, the 10 participants in our user study experiment come from diverse academic backgrounds. Since many of them do not major in computer vision, we provide detailed explanations for each question to assist their judgments. Instruction Following: Determine which video aligns more closely with the prompt, evaluate whether the main content is adequately presented in the video, and assess the accuracy and completeness of the prompt. Physics Simulation: Determine which video aligns more closely with real-world physical laws, including object motion, transformations, and other dynamics. Visual Quality: Determine which video has more harmonious overall visual composition and showcases finer details more exquisitely. D. Additional Experimental Results D.1. Short / Long Prompt To investigate the performance differences of Mimir when inputting short and coarse prompts versus long and fine prompts, we randomly sampled 4 prompts from the VBench dataset. Additionally, VBench provides enhanced versions of these 4 prompts through large language model. We input both versions into Mimir and generated corresponding videos. As shown in Fig. 10, leveraging the reasoning ability of the decoder-only LLM, even with short and coarse prompts, Mimir can generate results as detailed as those produced with long and fine prompts. This demonstrates that Mimirs token fuser effectively expands the semantic, leading to precise text understanding capabilities. 2 Figure 10. The comparison between results with short & course prompts and long & fine prompts. 3 Figure 11. More examples in terms of color rendering. D.2. More Interesting Prompts D.2.1. Spatial Semantic Understanding Color Rendering. As shown in Fig. 11, our method demonstrates the ability to accurately understand the color specifications in the prompt for different objects and generates videos containing objects with the correct colors. It highlights the effectiveness of our token fuser in ensuring semantic alignment between the input prompt and the generated video. By accurately capturing and representing color details, Mimir delivers coherent results, even in cases where multiple objects with distinct colors are specified. 4 Figure 12. More examples in terms of absolute & relative position. Absolute & Relative Position. As shown in Fig. 12, our method effectively understands the spatial relationships (i.e., the absolute & relative position) specified in the prompt, such as top, below, left, and right and generates videos where objects are positioned correctly according to these relationships. By accurately representing spatial arrangements, Mimir ensures that the generated videos meet the semantic requirements of complex prompts involving positional relationships between objects. Counting. As shown in Fig. 13, Mimir demonstrates strong ability to understand counting. For example, if the prompt specifies certain number of objects, Mimir accurately interprets this information and generates videos containing the correct quantity. By successfully handling quantity-specific prompts, Mimir proves its reliability in scenarios where precise numeric understanding is critical for video generation tasks. D.2.2. Temporal Semantic Understanding Sequential Actions. This involves capturing the sequence of actions performed by an object, such as cat looking up, then down, or following more complex pattern like up, down, and up again. It requires precise temporal understanding to maintain the correct order of actions. As shown in Fig. 14, Mimir precisely interprets and reproduces these action sequences. Illumination Harmonization. It means light changes in the environment, such as dawn transitioning to sunrise and then to sunset. As shown in Fig. 15, Mimir precisely generates these gradual scene changes, ensuring the illumination harmonization and the alignment with prompts. Object Transformation. It means transforming an object into another, such as car transforming into superhero. This is highly challenging task due to the complexity of capturing smooth transitions. As shown in Fig. 16, Mimir precisely understands the prompt and generates well. 5 Figure 13. More examples in terms of counting. Figure 14. More examples in terms of action sequence over time. 6 Figure 15. More examples in terms of light changes, showcasing the illumination harmonization over time. E. Limitations While our current work has made significant strides, it also possesses certain limitations. Firstly, the generated videos are typically limited to short durations (a few seconds to tens of seconds). This is primarily due to the significant computational resources and storage requirements needed for generating longer videos. Additionally, extending the video length may exacerbate temporal inconsistencies, such as discontinuities in actions or backgrounds across frames, 7 Figure 16. More examples in terms of object transformation over time. plex, high-quality synthetic videos. To address this, we are committed to promoting responsible use of T2V technology and actively contributing to the research community. We aim to share our generated results to support the development of more robust detection algorithms, fostering safer digital environment capable of mitigating the risks associated with increasingly sophisticated generative models. which can detract from the overall quality and realism. Secondly, the effectiveness of our T2V model is heavily dependent on the quality and diversity of the training data. In domains where the training dataset lacks coveragesuch as specific professional scenariosthe models performance can be suboptimal. This limitation highlights the importance of expanding and diversifying training datasets to improve the models generalizability across broader range of applications. F. Social Impact Our proposed T2V (Text-to-Video) model demonstrates strong potential for generating high-quality, contextually accurate video content directly from textual descriptions. This technology offers significant benefits across various domains, enabling more accessible, creative, and automated video generation workflows. However, like any generative technology, our T2V model also raises concerns about potential misuse. Malicious actors could exploit it to produce deceptive or harmful video content, such as fake news or misleading advertisements, amplifying the spread of misinformation on social media platforms. This misuse could lead to detrimental societal consequences, including the erosion of trust in digital media. Despite ongoing advancements in generative content detection technologies, challenges remain, especially in scenarios involving com-"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Tsinghua University"
    ]
}