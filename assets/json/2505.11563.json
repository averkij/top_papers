{
    "paper_title": "Object-Centric Representations Improve Policy Generalization in Robot Manipulation",
    "authors": [
        "Alexandre Chapin",
        "Bruno Machado",
        "Emmanuel Dellandrea",
        "Liming Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments."
        },
        {
            "title": "Start",
            "content": "Object-Centric Representations Improve Policy Generalization in Robot Manipulation Alexandre Chapin Ecole Centrale de Lyon, LIRIS 69130, Ecully, France alexandre.chapin@ec-lyon.fr Bruno Machado Ecole Centrale de Lyon, LIRIS 69130, Ecully, France alexandre.chapin@ec-lyon.fr Emmanuel Dellandrea Ecole Centrale de Lyon, LIRIS 69130, Ecully, France emmanuel.dellandrea@ec-lyon.fr Liming Chen Ecole Centrale de Lyon, LIRIS 69130, Ecully, France liming.chen@ec-lyon.fr Abstract: Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as structured alternative that segments visual input into finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark range of visual encodersobject-centric, global and dense methodsacross suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments. Keywords: Object-Centric Representation, Robot Manipulation, Visuomotor Policy Learning, Visual Representation Learning, Imitation Learning Figure 1: Overall architecture. We use set of pre-trained visual models with different structures of latent space - global, dense and object-centric - (a) as input to policy model for robotic manipulation learning (b). We showcase the benefits of Object-Centric Representations (VIDEOSAUR, DINOSAUR) over different visual models on the final performance of policies and the generalization capabilities in simulation and real-world scenarios (c). DINOSAUR* and VIDEOSAUR* are our versions of the OCR models with the attention module trained using mixture of robot data."
        },
        {
            "title": "Introduction",
            "content": "Visuomotor policy learning enables robots to map raw visual sensory inputs directly to control actions, allowing them to perceive and interact with their environment. The core objective is to learn policies that are both generalizable and sample-efficient. State-of-the art approaches have so far adopted common architectural scheme to learn visuomotor policies through imitation learning, dividing the learning process into distinct components: sensor encoding module, an observation trunk that integrates different modalities, and an action head that predicts the final action [1, 2, 3, 4]. key factor in the success of this process is the quality of the visual representation, which must capture task-relevant features while remaining robust to variations in the environment. As such, recent work has increasingly focused on the role of visual representations in enabling generalizable robot policies. Advances include pre-training on large-scale human egocentric video datasets using time-contrastive and language-aligned objectives [5, 6, 7], distilling knowledge from diverse vision foundation models into compact, robot-friendly encoders [8], and leveraging powerful self-supervised schemes such as masked autoencoding [9]. Despite these efforts, most prior approaches rely on shared architectural lineagetypically ResNet or Vision Transformersand encode visual scenes as global feature vector (holistic) or dense patch-based representation from the encoders final layers. Such representations often entangle task-relevant and irrelevant information, making them brittle to real-world shifts like lighting changes, novel textures, or cluttered environments [10, 11, 12]. They are clearly in contrast to human perception, which understands and acts in complex environments by forming symbolic representations of the world around them [13]. Rather than processing raw sensory data in isolation, we abstract meaningful entities - mental symbols - and reason about them as structured, reusable concepts. This compositional understanding enables us to rapidly generalize from prior experiences to new, unseen situations. At the heart of this capability is our ability to disentangle, bind, and manipulate information flexibly - trait that remains elusive in todays artificial systems [14]. There is growing body of evidence that highlights the need to rethink visual scene representations in robotics, emphasizing structure, abstraction, and task-relevance to bridge the gap between perception and control [15, 16]. promising approach toward such capability is object-centric representations (OCRs) which have emerged recently in computer vision [17, 18]. These methods restructure images by segmenting them into set of entities - objects - thereby embedding an inductive bias toward symbolic, compositional reasoning. OCRs introduce structure into the visual pipeline, allowing models to parse scenes into meaningful parts and reason over them explicitly. Notably, this approach mirrors how humans perceive and interact with the world - not as flat array of pixels, but as collection of distinct, interactive entities that can be tracked, manipulated, and abstracted into concepts. In this sense, object-centric perception may offer pathway toward bridging the gap between low-level visual input and high-level symbolic reasoning for robotic manipulation. Although the potential of OCRs in robotics has been increasingly recognized, much of the existing work has focused on their utility for image decomposition or scene reconstruction. few recent efforts have explored their role in control, particularly in reinforcement learning tasks [19, 20, 21]. However, these studies are often constrained to highly simplified, toy settings [22, 23, 24], leaving open some critical questions: Q1: Can OCR models better enable robotic manipulation policy learning over other visual representations ? Q2: Can OCRs enhance policy generalization capabilities in the presence of distractors, new textures, and lighting conditions? To answer these questions, we make the following contributions: We develop and open-source unified framework to evaluate different types of visual representations for robotic manipulation. We benchmark 7 distinct visual encoders, including global, dense and object-centric representations, across two simulated environments and novel, easily replicable real-world set of tasks. Unlike prior works, we show that slot-based object representations enable better robot manipulation policy learning and improve generalization in robot control, especially under realistic distractors and domain shifts, without requiring task-specific tuning."
        },
        {
            "title": "2 Related works",
            "content": "Pretrained vision based models for robot learning In recent years, the computer vision community has developed large-scale pretraining strategies for visual representation learning, yielding models such as MoCo [25], DINO [26], DINOv2 [27], and CLIP [28]. These models, originally trained for classification and image-text alignment, have been shown to be surprisingly effective for downstream visuomotor policy learning in robotics [11], even without access to domain-specific robot data. To better align visual representations with manipulation tasks, follow-up work explored pretraining on more relevant datasets and learning objectives in terms of manipulation. R3M [5] learns from large-scale human egocentric video (Ego4D [29]) using combination of time-contrastive learning, video-language alignment, and sparsity regularization, enabling efficient imitation learning in both simulation and the real world. VC-1 [6] combines masked autoencoding with mixed-domain training (ImageNet and robot video), while Theia [8] distills multiple vision foundation models (e.g., CLIP, SAM, DINOv2) into compact transformer backbone optimized for robotic policy learning. Recent work has questioned whether robot or manipulation-specific datasets always lead to better pretraining. In comprehensive study, Dasari et al. [30] found that curated general-purpose datasets such as ImageNet, Kinetics, and 100 Days of Hands often outperform Ego4D and RoboNet when used for visuo-motor pretrainingeven under identical architectures and objectives. The study showed that image distribution quality and diversity mattered more than dataset size or domain match, and that popular assumptions about dataset alignment (e.g., egocentric manipualtion data = better for robotics) may not always hold in practice. Similarly, [12] showed that vision transformers with strong emergent object segmentation properties generalize better under distribution shifts, such as changes in lighting, textures, and distractors. These findings motivate rethinking of visual representations for robot learningmoving away from flat global or dense feature maps toward more structured alternatives. In particular, objectcentric representations, which segment visual inputs into discrete entities, promise to capture taskrelevant structure more robustly and compositionally. Our work builds on this direction, introducing object-centric biases into visual pretraining and evaluating their impact on generalization and robustness in visuomotor policy learning. Object-centric learning Object-Centric Representations (OCR) aims to decompose images into structured representation composed of multiple vectors, commonly referred to as slots, each corresponding to an extracted entity. There has been recently growing interest in these methods due to their potential benefits in various domains, including autonomous driving [31], robotics [20, 32], and explainability [33]. Early OCR research primarily focused on structured representation learning through the lens of generative modeling [34]. Subsequent works adopted encoder-decoder architectures to obtain structured and disentangled latent spaces [35, 34, 22]. The most significant method in this field is Slot-Attention [17], recognized for its simplicity and efficiency. Further advancements have incorporated more powerful decoding mechanisms, such as diffusion models [36, 37], transformer decoders [38] or alse pre-trained backbones such as [26] to improve OCR performance in real-world scenarios [18]. More recently, recurrent neural networks and transformer architectures have been integrated into OCR methods [39, 23, 40, 41] to make them usable for videos. While OCR naturally segments input images into meaningful components, potentially aiding generalization in imitation learning [10, 12], most studies have focused on image reconstruction or semantic segmentation rather than control tasks. Only few exceptions have explored OCR in simple control environments [21, 20, 19]. Our goal in this paper is to evaluate whether OCR methods, with their inherent segmentation capabilities, can facilitate visuomotor policy learning and enhance the generalization of robotic manipulation models."
        },
        {
            "title": "3 Method",
            "content": "The overview of our framework (Figure 1) consists of sensor encoding module (Vision encoder), an observation trunk that integrates different modalities (e.g., text for instructions) and an action head that predicts the final action. We start by introducing an object-centric representation for videos and its variants for visual encoding, followed by policy training. Object-centric representation for videos Object-centric methods have gained traction in computer vision for their ability to generalize across scenes [42] by modeling object-level dynamics and interactions. Consider an input image O, the goal is to produce set of object representations = {s1, . . . , sK}. The image is first encoded using vision backbone into set of dense features = {f1, . . . , fN } (with >> K), and object-specific representation -or slotsare extracted using Slot Attention [17]. Slot Attention is differentiable module that performs iterative attention with competition, allowing distinct slots to specialize in representing different parts of the input scene. Conceptually, Slot-Attention simply use cross-attention module [43], with renormalization over queries by Eq. (1). Slots are then obtained by weighted sum of the values using the attention weights through Eq. (2): (cid:19) = softmax , RN D, RKD (1) S(i+1) = AV (2) (cid:18) QKT The query in Eq.(1) is the projected set of Slots S(i) at iteration i, key and values are projected dense features . This process is repeated iteratively in order to obtain the final slots. The original Slot-Attention model used simple Convolutionnal neural network as visual backbone followed by simple Deconvolutionnal decoder in order to reconstruct the input image from the slots as learning signal. This method is limited in real-world scenarios with complex images. DINOSAUR [18] introduces the use of frozen DINO [26] as visual backbone. The model also reconstruct the extracted features instead of the input image. VIDEOSAUR [41] develop DINOSAUR to videos with two main changes: adding transformer predictor model in order to init the slots of timestep from the slots predicted from timestep 1 and adding temporal consistency loss using feature similarity of the updated DINOv2 [27] along two consecutive timesteps Policy training Our primary objective is to assess the effectiveness of object-centric representations when used as inputs for robot policy learning, more specifically through the imitation learning framework. Given set of expert demonstrations = {τ1, . . . , τn} with τi = [(o0, a0), . . . , (oT , aT )], the policy π needs to learn mapping between the input observation ot (e.g.,visual inputs) to the next action at. To ensure fair comparison, we adopt policy architectures that can process global, dense and slot-based inputs within unified framework. Moreover, in line with prior work [5, 12, 10], we keep the pre-trained visual models frozen during the policy training process. For simulated environments, we use the BAKU architecture [1], which consists of an encoder, an observation trunk, and policy head. The observation trunk is transformer-based model that encodes sequence of past observations. Each observation includes visual features, language embeddings, and proprioceptive states, interleaved with learnable action tokens. The policy head, deterministic Multi-Layer Perceptron, uses the final action token to produce the next action prediction. 4 For real-world experiments, we build on Action Chunking Transformers (ACT) [44] as implemented in the LeRobot library1. The original model was using dense input representations. We then modified the input modality of ACT to also incorporate either single global visual feature vector or set of object-centric slot vectors. To support multi-task learning, we concatenate task-specific language embedding derived from the ModernBERT model [45] with the visual and proprioceptive inputs. This joint representation is then processed by transformer encoder-decoder to generate action chunks. Detailed architectural configurations are provided in Appendix E."
        },
        {
            "title": "4 Benchmarks and Experimental setup",
            "content": "Environments and tasks To comprehensively evaluate visual representations in robot manipulation, we selected three complementary environmentstwo in simulation and one in the real worldchosen for their diversity in task complexity, embodiment, and visual structure, as shown in Figure 2. In simulation, we use MetaWorld [46], well-established benchmark comprising tabletop manipulation tasks performed with Sawyer robotic arm. MetaWorld offers controlled setting with standardized tasks and supports structured generalization testing, making it suitable baseline to evaluate representation performance in simple, single-object scenarios. To challenge the scalability of object-centric representations, we also include LIBERO-90 [47], recent benchmark featuring multi-object, visually complex scenes across diverse domains such as kitchens, offices, and living rooms. LIBERO is particularly well-suited for our study, as it emphasizes multi-object reasoning and generalization to novel combinations of objectsconditions under which object-centric models are expected to outperform traditional alternatives. For real-world evaluation, we introduce suite of five tabletop manipulation tasks implemented using the LeRobot library and SO-100 low-cost robotic arm. These tasks were designed to reflect common household activities, such as picking, placing, folding, and drawer manipulation, and involve varied object properties including deformable, articulated, and rigid elements. The setup enables us to test the practical viability of object-centric models under realistic noise and embodiment constraints. All environments and tasks are described more in detail in Annex D. Figure 2: Overview environments. We evaluate the different visual models in two simulation (a: MetaWorld, b: LIBERO) and one real-world environments (c). Pre-trained visual models Based on recent state-of-the-art evaluations [12, 6, 10], we selected 7 pre-trained visual models for comparison that we depict in Table 1. These models span range of backbone architectures - including ResNet variants [48] and Vision Transformer-based models [49] - as well as diverse training objectives such as supervised learning, self-supervised learning, contrastive learning and distillation. All selected models have demonstrated state-of-the-art performance within their respective categories in benchmark evaluations. We also aimed to capture 1https://github.com/huggingface/lerobot 5 broad spectrum of feature representations to support comprehensive comparison such as the classical global and dense representations but also object-centric representations. Further detail on each model is provided in Appendix A. Robotic pre-training Although object-centric video models show promise in structured scene decomposition, they are typically trained on internet-scale in-the-wild video datasets, which depict data diversity but are often misaligned with the distribution of robotic manipulation environments. To bridge this gap and further increase training data diversity in line with the findings in [30], we introduce dedicated pretraining stage on robotic video datasets. We compile and preprocess collection of real-world robotic datasets spanning wide variety of manipulation skills, environments, and embodiments. In total, our training set includes over 188,000 trajectories, drawn from three major sources: BridgeData V2 [50], diverse set of demonstrations across household tasks using the WidowX-250 arm; Fractal [2], large-scale dataset collected with fleet of Everyday Robots across hundreds of kitchen manipulation tasks; and DROID [51], which contains unconstrained robot interactions across multiple labs and setups. The combined dataset offers rich visual and physical diversity, including varied viewpoints, object types, and lighting conditions. To adapt the object-centric model to the robotics domain, we train its slot attention module using self-supervised reconstruction loss applied to DINO feature maps extracted from temporal video slices. This approach enables the model to learn structured representations grounded in robot manipulation dynamics. In addition, we systematically evaluate how dataset composition impacts downstream performance by comparing models trained on single-source datasets to those trained on balanced mixture of all three sources (Robot Mixt.). Further details on each dataset and the effects of these training regimes on generalization and policy learning are presented in Annex C. Table 1: Models overview. Comparison of the data and sizes of the different pre-trained visual models . ViT: Vision Transformer, OC: Object-Centric layers, G: Global, D: Dense. DINOSAUR* and VIDEOSAUR* are our versions of the models with the attention module trained on robotic data. Model ResNet-50 [48] R3M [5] DINOv2 [27] VC-1 [6] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [41] VIDEOSAUR* [41] Backbone ResNet-50 ResNet-50 ViT ViT ViT ViT + OC ViT + OC ViT + OC ViT + OC Pre-training Dataset ImageNet [52] Ego4D [29] LVD-142M [27] Ego4D [29] and ImageNet [52] ImageNet [52] COCO [53] Robot Mixt. Youtube-VOS [54] Robot Mixt. # of params. Features 25.6M 25.6M 86M 86M 140M 88M 88M 90M 90M G & Slot Slot Slot Slot"
        },
        {
            "title": "5 Results",
            "content": "We evaluate the role of visual representations in learning and generalizing robotic manipulation policies, focusing specifically on object-centric representations (OCRs) and how they compare to dense and global alternatives. Our results aim to answer the following questions introduced in Section 1: Q1: Can OCRs better support robot policy learning than other visual representations? Q2: Can OCRs enhance policy generalization under viusal distribution shifts such as distractors, lighting changes, or new textures? We assess models in both simulation and real-world settings. In simulation, we report mean success rates over three random seeds with 50 rollouts per task; in the real world using LeRobot, we conduct 10 rollouts per task. All policies are trained using frozen visual encoders. 6 Figure 3: Overall success rate. Mean success rate over all tasks for each visual model on the three environments, e.g., MetaWorld (left), LIBERO (middle) and Real using LeRobot (right). DINOSAUR* and VIDEOSAUR* have been pretrained over robot data mixture. 5.1 Q1: Do OCRs improve manipulation policy learning? Figure 3 summarizes the average success rate across MetaWorld, LIBERO, and our real-world benchmark. Object-centric modelsespecially VIDEOSAUR*consistently achieve the highest overall performance, outperforming both dense (e.g., DINOv2, Theia) and global (e.g., ResNet-50, VC-1) baselines. In MetaWorld, all models except VC-1 perform above 60%. The low VC-1 performance may result from its MAE-based pretraining, which is sensitive to domain mismatch when not fine-tuned. Object-centric models perform comparably to top baselines here, despite the simplicity of the environment. In LIBERO, which features complex scenes with multiple objects, OCRs clearly outperform all other representations. VIDEOSAUR* improves over the best dense model (Theia) by +9%, showing its ability to handle multi-object interaction. In the real-world setup using LeRobot, OCRs again outperform other models. VIDEOSAUR* reaches 70% success rate compared to 50% for the best dense baseline. Interestingly, the simplest modelResNet-50 pretrained on ImageNetalso performs competitively, possibly due to its compact size and the diversity of visual pretraining data in line with the findings in [30]. Comparison of all the four OCR variants enables further insights. Firstly, incorporating robotic data into pre-training proves immensely beneficial when comparing the performance of OCR models (DINOSAUR* vs DINOSAUR, VIDEOSAUR* vs VIDEOSAUR). In particular, VIDEOSAUR* increases VIDEOSAURs mean performance by 13, 11 and 10 points in MetaWorld, LIBERO and LeRobot environments, respectively. Secondly, putting side by side DINOSAUR* with VIDEOSAUR* which differs only by an additional modeling of the temporal dynamics while using the same pretraining data , it can be seen from Figure 3 that taking into account temporal dynamics in VIDEOSAUR* constitutes another major factor of performance gain, improving DINOSAUR* by +9 and +26 points in the LIBERO and LeRobot environments, respectively. One cannot draw similar conclusion when comparing VIDEOSAUR and DINOSAUR because their pretraining data are different. It could explain in particular the performance decrease of VIDEOSAUR over DINOSAUR in Metaworld. Specifically, robotic data provides structured scenes with fixed cameras and consistent motion patterns, such as robotic arm moving within the frame. This contrasts with the in-the-wild dynamics used for training the original VIDEOSAUR, which often includes moving cameras and less predictable scene changes. Furthermore, the visual domain also differs substantially. Overall, our results for Q1 confirm that object-centric models are not only effective but scalable across domains, offering performance benefits in both structured simulation tasks and noisy realworld environments. 5.2 Q2: Do OCRs enhance generalization under visual distribution shifts? We now evaluate generalization to out-of-distribution conditions, including novel distractors, unseen textures, and lighting changes. Table 2 and Table 3 report success rates for these scenarios. 7 In MetaWorld, as can be seen in Table 2, dense models like DINOv2 and Theia outperform ResNet on average, confirming previous findings (e.g.,[12]). However, OCRsespecially VIDEOSAUR*consistently outperform all baselines under texture and lighting shifts. Notably, Theia performs best in the distractor scenario, likely due to CLIP-based text-image alignment helping the policy ignore irrelevant patches. Yet, Theia suffers sharp drop in the other scenarios, where OCRs prove to be much more robust. Table 2: Generalization MetaWorld. Comparison of final results for different levels of generalization on MetaWorld. ID: performance using In-domain test data Models ResNet-50 (pre-trained) [48] R3M [5] DINOv2 [27] VC1 [6] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [41] VIDEOSAUR* [41] Distractors 0.04 0.02 0.0 0.0 0.11 0.02 0.06 0.02 0.65 0.10 0.21 0.04 0.46 0.14 0.29 0.13 0.49 0. Textures 0.001 0.002 0.0 0.0 0.03 0.01 0.0 0.0 0.28 0.06 0.48 0.06 0.36 0.05 0.36 0.10 0.35 0.11 Lighting 0.22 0.02 0.0 0.0 0.39 0.04 0.23 0.03 0.48 0.10 0.71 0.06 0.65 0.14 0.57 0.10 0.65 0.13 Overall 0.10 0.0 0.18 0.10 0.47 0.46 0.49 0.41 0.50 ID 0.76 0.68 0.71 0.50 0.75 0.73 0.76 0.63 0.76 In real-world evaluations as can be seen in Table 3, VIDEOSAUR* shows strong robustness, with the lowest drops in the success rate under texture and lighting changes. Theia again performs best in the distractor scenario, but fails under other shifts. ResNet-50, while providing surprisingly good resilience to texture and lighting variation, lacks consistency across all shifts. Table 3: Generalization real-world. Comparison of final results for different levels of generalization on real-world. ID: performance using In-Domain test data. Models ResNet-50 (pre-trained) [48] DINOv2 [27] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [41] VIDEOSAUR* [41] Distractors Textures Lighting Overall 0.08 0 0.32 0.18 0.28 0.26 0.24 0.22 0 0.28 0.32 0.34 0.44 0.5 0.32 0.02 0.22 0.30 0.36 0.50 0.58 0.21 0.01 0.28 0.27 0.33 0.40 0. ID 0.5 0.22 0.32 0.40 0.44 0.60 0.72 In summary, results for Q2 demonstrate that object-centric representations generalize better across diverse distribution shifts, particularly those that perturb low-level appearance. This robustness likely stems from OCRs ability to filter task-irrelevant background and focus on object-level structure."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigate the potential of OCRs to enhance the generalization and robustness of robotic manipulation policies. Unlike traditional visual encoders, OCR-based models introduce inductive biases that better reflect the structured nature of physical interaction, yielding consistently superior performance across diverse set of simulated and real-world manipulation tasks. Our results highlight the importance of rethinking visual representations to advance both the efficiency and generalization of robotic agents: moving away from flat, pixel-level features toward more structured, object-based encodings. By leveraging object-centric biases, we can bridge the gap between low-level visual input and high-level symbolic reasoning, enabling robots to better understand and interact with their surroundings. 8 We believe OCRs present promising foundation for bridging the sim-to-real gap and achieving scalable, generalizable robotic control. Future research should explore how OCRs can be further integrated with diverse architectures, multimodal inputs, and self-supervised learning frameworks to maximize their scalability and downstream utility."
        },
        {
            "title": "7 Limitations",
            "content": "While we tried to conduct extensive and thorough experiments, several limitations remains and should be addressed in future works. First, the object-centric methods employed in this study do not inherently bind to specific objects and lack semantic grounding. As illustrated in Figure 7, some slots are allocated to background regions without capturing meaningful semantic content, and in some failure cases slots also capture distractors existing slots 8. This limitation suggests that incorporating semantic information, e.g., affordance, could enhance the interpretability and the practical utility of the object-centric representations. Furthermore, it may provide pathway to improve the generalization capabilities of OCRs in the presence of distractors, as suggested by the results obtained with Theia Second, our work does not account for the alignment with robot dynamics. We posit that integrating this aspect could significantly improve the practical applicability of our methods, particularly in light of recent advances highlighted in [10]. Future research should explore this avenue to better align object-centric models with robotic systems. Finally, the scope of our exploration has been constrained by the scale of the models and the datasets used for pre-training. Expanding this work to include larger models and more diverse datasets could yield more robust and generalizable results, in the light of what was done in [42]. This would also allow for more thorough evaluation of the models capabilities and limitations. Acknowledgments"
        },
        {
            "title": "References",
            "content": "[1] S. Haldar, Z. Peng, and L. Pinto. Baku: An efficient transformer for multi-task policy learning, 2024. URL https://arxiv.org/abs/2406.07539. [2] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. URL https://arxiv.org/abs/2212.06817. [3] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo, Y. L. Tan, L. Y. Chen, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine. Octo: An open-source generalist robot policy, 2024. URL https: //arxiv.org/abs/2405.12213. [4] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn. Openvla: An open-source vision-language-action model, 2024. URL https://arxiv.org/abs/2406.09246. [5] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: universal visual representation for robot manipulation, 2022. URL https://arxiv.org/abs/2203.12601. [6] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik, D. Batra, Y. Lin, O. Maksymets, A. Rajeswaran, and F. Meier. Where are we in the search for an artificial visual cortex for embodied intelligence?, 2024. URL https://arxiv.org/abs/2303.18240. 10 [7] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training, 2023. URL https: //arxiv.org/abs/2210.00030. [8] J. Shang, K. Schmeckpeper, B. B. May, M. V. Minniti, T. Kelestemur, D. Watkins, and L. Herlant. Theia: Distilling diverse vision foundation models for robot learning, 2024. URL https://arxiv.org/abs/2407.20179. [9] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot learning with masked visual pre-training, 2022. URL https://arxiv.org/abs/2210.03109. [10] G. Jiang, Y. Sun, T. Huang, H. Li, Y. Liang, and H. Xu. Robots pre-train robots: Manipulationcentric robotic representation from large-scale robot dataset. arXiv preprint arXiv:2410.22325, 2024. [11] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of pre-trained vision models for control, 2022. URL https://arxiv.org/abs/2203.03580. [12] K. Burns, Z. Witzel, J. I. Hamid, T. Yu, C. Finn, and K. Hausman. What makes pre-trained visual representations successful for robust manipulation?, 2023. URL https://arxiv.org/ abs/2312.12444. [13] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think like people, 2016. URL https://arxiv.org/abs/1604.00289. [14] K. Greff, S. van Steenkiste, and J. Schmidhuber. On the binding problem in artificial neural networks, 2020. URL https://arxiv.org/abs/2012.05208. [15] O. Kroemer, S. Niekum, and G. Konidaris. review of robot learning for manipulation: Challenges, representations, and algorithms, 2020. URL https://arxiv.org/abs/1907. 03146. [16] Y. Bengio, A. Courville, and P. Vincent. Representation learning: review and new perspectives, 2014. URL https://arxiv.org/abs/1206.5538. [17] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, and T. Kipf. Object-centric learning with slot attention, 2020. URL https: //arxiv.org/abs/2006.15055. [18] M. Seitzer, M. Horn, A. Zadaianchuk, D. Zietlow, T. Xiao, C.-J. Simon-Gabriel, T. He, Z. Zhang, B. Scholkopf, T. Brox, and F. Locatello. Bridging the gap to real-world objectcentric learning, 2023. URL https://arxiv.org/abs/2209.14860. [19] J. Yoon, Y.-F. Wu, H. Bae, and S. Ahn. An investigation into pre-training object-centric representations for reinforcement learning, 2023. URL https://arxiv.org/abs/2302.04419. [20] N. Heravi, A. Wahid, C. Lynch, P. Florence, T. Armstrong, J. Tompson, P. Sermanet, J. Bohg, and D. Dwibedi. Visuomotor control in multi-object scenes using object-aware representations, 2023. URL https://arxiv.org/abs/2205.06333. [21] D. Haramati, T. Daniel, and A. Tamar. Entity-centric reinforcement learning for object manipulation from pixels, 2024. URL https://arxiv.org/abs/2404.01220. [22] N. Watters, L. Matthey, M. Bosnjak, C. P. Burgess, and A. Lerchner. Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration, 2019. URL https://arxiv.org/abs/1905.09275. [23] T. Kipf, G. F. Elsayed, A. Mahendran, A. Stone, S. Sabour, G. Heigold, R. Jonschkowski, A. Dosovitskiy, and K. Greff. Conditional object-centric learning from video, 2022. URL https://arxiv.org/abs/2111.12594. 11 [24] C. Zhang, A. Gupta, and A. Zisserman. Is an object-centric video representation beneficial for transfer?, 2022. URL https://arxiv.org/abs/2207.10075. [25] X. Chen, H. Fan, R. Girshick, and K. He."
        },
        {
            "title": "Improved baselines with momentum contrastive",
            "content": "learning, 2020. URL https://arxiv.org/abs/2003.04297. [26] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers, 2021. URL https://arxiv.org/abs/ 2104.14294. [27] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. [28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. [29] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, M. Martin, T. Nagarajan, I. Radosavovic, S. K. Ramakrishnan, F. Ryan, J. Sharma, M. Wray, M. Xu, E. Z. Xu, C. Zhao, S. Bansal, D. Batra, V. Cartillier, S. Crane, T. Do, M. Doulaty, A. Erapalli, C. Feichtenhofer, A. Fragomeni, Q. Fu, A. Gebreselasie, C. Gonzalez, J. Hillis, X. Huang, Y. Huang, W. Jia, W. Khoo, J. Kolar, S. Kottur, A. Kumar, F. Landini, C. Li, Y. Li, Z. Li, K. Mangalam, R. Modhugu, J. Munro, T. Murrell, T. Nishiyasu, W. Price, P. R. Puentes, M. Ramazanova, L. Sari, K. Somasundaram, A. Southerland, Y. Sugano, R. Tao, M. Vo, Y. Wang, X. Wu, T. Yagi, Z. Zhao, Y. Zhu, P. Arbelaez, D. Crandall, D. Damen, G. M. Farinella, C. Fuegen, B. Ghanem, V. K. Ithapu, C. V. Jawahar, H. Joo, K. Kitani, H. Li, R. Newcombe, A. Oliva, H. S. Park, J. M. Rehg, Y. Sato, J. Shi, M. Z. Shou, A. Torralba, L. Torresani, M. Yan, and J. Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. URL https://arxiv.org/abs/2110.07058. [30] S. Dasari, M. K. Srirama, U. Jain, and A. Gupta. An unbiased look at datasets for visuo-motor pre-training, 2023. URL https://arxiv.org/abs/2310.09289. [31] S. Hamdan and F. Guney. Carformer: Self-driving with learned object-centric representations, 2024. URL https://arxiv.org/abs/2407.15843. [32] M. Mosbach, J. N. Ewertz, A. Villar-Corrales, and S. Behnke. Sold: Slot object-centric latent dynamics models for relational manipulation learning from pixels, 2025. URL https:// arxiv.org/abs/2410.08822. [33] B. Wang, L. Li, J. Zhang, Y. Nakashima, and H. Nagahara. Explainable image recognition via enhanced slot-attention based classifier, 2024. URL https://arxiv.org/abs/2407. 05616. [34] R. Kabra, D. Zoran, G. Erdogan, L. Matthey, A. Creswell, M. Botvinick, A. Lerchner, and C. P. Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition, 2021. URL https://arxiv.org/abs/2106.03849. [35] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner. Monet: Unsupervised scene decomposition and representation, 2019. URL https://arxiv. org/abs/1901.11390. [36] J. Jiang, F. Deng, G. Singh, and S. Ahn. Object-centric slot diffusion, 2023. URL https: //arxiv.org/abs/2303.10834. 12 [37] Z. Wu, J. Hu, W. Lu, I. Gilitschenski, and A. Garg. Slotdiffusion: Object-centric generative modeling with diffusion models, 2023. URL https://arxiv.org/abs/2305.11281. [38] G. Singh, F. Deng, and S. Ahn. arxiv.org/abs/2110.11405. Illiterate dall-e learns to compose, 2022. URL https:// [39] G. F. Elsayed, A. Mahendran, S. van Steenkiste, K. Greff, M. C. Mozer, and T. Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos, 2022. URL https:// arxiv.org/abs/2206.07764. [40] G. Singh, Y.-F. Wu, and S. Ahn. Simple unsupervised object-centric learning for complex and naturalistic videos, 2022. URL https://arxiv.org/abs/2205.14065. [41] A. Zadaianchuk, M. Seitzer, and G. Martius. Object-centric learning for real-world videos by predicting temporal feature similarities, 2023. URL https://arxiv.org/abs/2306.04829. [42] A. Didolkar, A. Zadaianchuk, A. Goyal, M. Mozer, Y. Bengio, G. Martius, and M. Seitzer. Zero-shot object-centric representation learning, 2024. URL https://arxiv.org/abs/ 2408.09162. [43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762. [44] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware, 2023. URL https://arxiv.org/abs/2304.13705. [45] B. Warner, A. Chaffin, B. Clavie, O. Weller, O. Hallstrom, S. Taghadouini, A. Gallagher, R. Biswas, F. Ladhak, T. Aarsen, N. Cooper, G. Adams, J. Howard, and I. Poli. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. URL https://arxiv.org/abs/2412.13663. [46] T. Yu, D. Quillen, Z. He, R. Julian, A. Narayan, H. Shively, A. Bellathur, K. Hausman, C. Finn, and S. Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning, 2021. URL https://arxiv.org/abs/1910.10897. [47] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. [48] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015. URL https://arxiv.org/abs/1512.03385. [49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https: //arxiv.org/abs/2010.11929. [50] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine. Bridgedata v2: dataset for robot learning at scale, 2024. URL https://arxiv.org/abs/2308.12952. [51] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma, P. T. Miller, J. Wu, S. Belkhale, S. Dass, H. Ha, A. Jain, A. Lee, Y. Lee, M. Memmel, S. Park, I. Radosavovic, K. Wang, A. Zhan, K. Black, C. Chi, K. B. Hatch, S. Lin, J. Lu, J. Mercat, A. Rehman, P. R. Sanketi, A. Sharma, C. Simpson, Q. Vuong, H. R. Walke, B. Wulfe, T. Xiao, J. H. Yang, A. Yavary, T. Z. Zhao, C. Agia, R. Baijal, M. G. Castro, D. Chen, Q. Chen, T. Chung, J. Drake, E. P. Foster, J. Gao, D. A. Herrera, M. Heo, K. Hsu, J. Hu, D. Jackson, C. Le, Y. Li, K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. ONeill, R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, 13 P. Yin, Y. Zhang, O. Bastani, G. Berseth, J. Bohg, K. Goldberg, A. Gupta, A. Gupta, D. Jayaraman, J. J. Lim, J. Malik, R. Martın-Martın, S. Ramamoorthy, D. Sadigh, S. Song, J. Wu, M. C. Yip, Y. Zhu, T. Kollar, S. Levine, and C. Finn. Droid: large-scale in-the-wild robot manipulation dataset, 2024. URL https://arxiv.org/abs/2403.12945. [52] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, Imagenet large scale visual recogniA. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. tion challenge, 2015. URL https://arxiv.org/abs/1409.0575. [53] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar. Microsoft coco: Common objects in context, 2015. URL https: //arxiv.org/abs/1405.0312. [54] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang. Youtube-vos: large-scale video object segmentation benchmark, 2018. URL https://arxiv.org/abs/1809.03327. [55] R. Cadene, S. Alibert, A. Soare, Q. Gallouedec, A. Zouitine, and T. Wolf. Lerobot: Statehttps://github.com/ of-the-art machine learning for real-world robotics in pytorch. huggingface/lerobot, 2024. [56] E. Todorov, T. Erez, and Y. Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026 5033. IEEE, 2012. doi:10.1109/IROS.2012.6386109. [57] A. Xie, L. Lee, T. Xiao, and C. Finn. Decomposing the generalization gap in imitation learning for visual robotic manipulation, 2023. URL https://arxiv.org/abs/2307.03659. [58] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. In Conference on Robot Learning (CoRL), 2021."
        },
        {
            "title": "A Implementation details",
            "content": "Backbone Following DINOSAUR [18] and VIDEOSAUR [41] works, we use DINOv2 [27] model as input backbone to our object-centric. We use the ViT-B14 version and use images of resolution 224224 as input. Object-centric layers For DINOSAUR model, Slot-Attention layer [17] is added on top of the backbone with classical MLP decoder, detailed in Table 4, which reconstruct the input patches from DINO model. For VIDEOSAUR, we follow the original paper and use transformer predictor which initialize slots from timestep with those of timestep 1 in order to transmit dynamic information. It also uses Slot-Attention layer on top of DINOv2 but encode here an history of frames. each frame is decoded independently with the SlotMixerDecoder introduced in the original paper [41]. An overview of models parameters is shown in Table 4. Policy models In simulation, we adapt the BAKU [1] model. We specifically use the model combining Transformer Observation trunk with MLP deterministic action head, as it was their best performing combination. We modified the observation trunk in order to accept different input shapes instead of the original single feature. Our version can either have one single vector per view (global), several vectors per view (object-centric) or dense set of features (dense). These vectors are interleaved one view after the other, followed by proprioception and combined with other timesteps for an history len of H. The task embedding is prepended at the beginning of the sequence. For real-world experiments, we modified the ACT [44] implementation of the LeRobot [55] library. First, as the model is made to only handle single-task, we compute task embedding with ModernBERT [45] and prepend the embedding vector at the input of the transformer decoder before the 14 Table 4: Object-centric and policy model details. DINOSAUR [18] Backbone: DINOv2 [27] - ViT-B14 Object-Centric: Slot-Attention [17] - 3 iteration, Slot size=128, Num slots=10 Decoder: MLP Decoder - 3 layers, Hidden dim=768 VIDEOSAUR [41] Backbone: DINOv2 [27] - ViT-B14 Object-Centric: Slot-Attention [17] - 3 iteration, Slot size=128, Num slots=10 Prediction layer: Transformer(Num layers=1, Hidden dim=512, Num heads=4) Decoder: Slot Mixer Decoder [41] - Allocator=Transformer(Num layers=2, Hidden dim=512, Num heads=4), Predictor=Transformer(Num layers=1, Hidden dim=512, Num heads=4) BAKU [1] Observation trunk: Transformer(Num layers=8, Num heads=4) (minGPT) Action Head: MLP(Hidden dim=256) (deterministic) Action chunking True (10 chunks) History len=4 ACT [55] Task embedding: ModernBERT (Base version) [45] VAE Encoder: VAE(Latent dim=32, Num layers=4) Transformer Encoder: Transformer Encoder(Num layers=4, Num Heads=8, Hidden dim=512) Transformer Decoder: Transformer Decoder(Num layers=1, Num Heads=8, Hidden dim=512) Action chunking True (100 chunks) Temporal Ensembling False observation features. Then, the original model was made to take dense feature representation from any single view as input. We made some changes in order to either accept single global vector or several slots. This required mainly to change the 2D positional embedding of the features to 1D positional embedding but also to modify the projection layer."
        },
        {
            "title": "B Training",
            "content": "The pretraining of our DINOSAUR and VIDEOSAUR models on robotic data is performed on single NVIDIA H100 GPU. All configurations are exposed on Table 5. Table 5: DINOSAUR and VIDEOSAUR pre-training configurations. Hyperparameters # GPUs Batch size Learning rate LR schedule Weight decay Optimizer Betas Training steps Warmup steps Warmup LR Schedule Gradient cliping Total GPU hours 1 128 1e-4 Linear 0.0 AdamW (0.9, 0.999) 100000 2500 100000 80 15 Pre-training data ablation Each of the visual models selected is pre-trained on different datasets, ranging from ImageNet [52], to egocentric manipulation data [29]. The object-centric models were originally trained on in-thewild images and videos datasets. In our work, we decided to evaluate whether training them on different kind of data might help to improve their downstream performances for robotic manipulation. Thus, we retrained from scratch these models on the large-scale robotic datasets BridgeV2 [50], Fractal [2] and DROID [51] but also on Mixture combining all of these data, in order to better align their representations with the task dynamics we aim to solve. Table 6: Pre-training data evaluation. Comparison of the pre-training data on the final global performance for VIDEOSAUR [41] model. Pre-training data Youtube-VOS [54] BridgeV2 (B) [50] Fractal (F) [2] DROID (D) [51] (Ours) Mixture (D+B+F) Metaworld LIBERO LeRobot 0.77 0.84 0.60 0.78 0.86 0.58 0.54 0.42 0.28 0.7 0.63 0.77 0.71 0.79 0."
        },
        {
            "title": "D Environments details",
            "content": "This section details the two simulation environments - MetaWorld [46] and LIBERO [47] - and the real-world setup - using the LeRobot library [55] - we use to evaluate our different visual models. MetaWorld [46] is benchmark built on top of the MuJoCo engine [56], featuring collection of tabletop manipulation tasks using Sawyer arm. Following [10], we selected subset of 10 tasks and collected 50 demonstrations per task using the official provided policies. MetaWorld is widely used robotic benchmark in the community, and permits to quickly evaluate and compare our models in different scenarios with existing baselines, even with different generalization levels thanks to the setup proposed in [57]. The set of 10 selected tasks are the following: Assembly (A R4): the goal is to grasp nut and place it around stick. Bin Picking (A R4): the goal is to pick green cube from blue bin and to place it into red bin. Button Press (A R4): the goald it to press the button. Disassemble (A R4): the goal is remove nut from around stick. Drawer Open (A R4): the goal is to open the drawer. Hammer (A R4): the goal is to drive screw into the wall using hammer. Pick Place Wall (A R4): the goal is to pick cube and place it at certain position situated behind wall. Shelf Place (A R4): the goal is to pick deformable blue puck and to place it into wooden shelf. Stick Pull (A R4): the goal is to pull pitcher using stick. Stick Push (A R4): the goal is to push pitcher using stick. LIBERO [47] is benchmark built on top of Robomimic [58], comprising multiple task suites for diverse purposes. We selected the LIBERO-90 suite, which includes 90 tasks, each with 50 demonstrations. The suite spans three distinct environments: kitchen, office, and living room; offering broad range of objects and task variations. This benchmark is particularly suitable to evaluate our object-centric models as it implies multi-object scenes as shown in Figure 2 in more visually diverse environments than MetaWorld and permits to better illustrate learning in complex scenarios. 16 LeRobot [55] is an open-source library providing models and tools to build low-cost robotic arm with teleoperation leader (300$) - SO-100enabling easy real-world experiment replication. We selected set of five tasks and collected 30 demonstrations per task using the setup shown in Figure 4. For our real world environments, we selected set of 5 tasks designed to evaluate range of essential robotic skills: Open Drawer (A R7): Open the top drawer of the teabox Close Drawer (A R7): Close the top drawer of the teabox Pick Coffee (A R7): Pick the coffe cap and place it into the top drawer of the teabox Fold bag (A R7): Fold the bag from bottom-right corner Banana bowl (A R7): Put the banana into the red bowl visualization of all the tasks is performed on Figure 5. Each task necessitates distinct manipulation skills, including picking, placing, opening, closing and folding. These skills involve interacting with various type of objects: rigid, deformable and articulated objects. These tasks are representative of common household activities, and successful execution indicates robots potential to assist in daily living tasks. This comprehensive evaluation ensures that the robotic model can handle the complexity and variability of real-world environments. Figure 4: Real world setup. Our setup is based on the LeRobot library. We use SO-100 arm on tabletop environment with two realsense cameras, one with top overview of the scene and second with side view. Figure 5: Overview tasks real-world. From left to right: Banana bowl, Open drawer, Pick coffee, Close drawer, Fold bag."
        },
        {
            "title": "E Baselines details",
            "content": "This section details the 7 state-of-the-art visual models we compare in our experiments. It covers different architecture types (Convolutionnal Neural Networks and Vision Transformers) but also different data (natural images, manipulation data, in-the-wild videos). (1) ResNet-50 [48]: classical baseline model pre-trained on ImageNet, widely used for its robust performance in various computer vision tasks. It serves as standard for comparison due to its simplicity and effectiveness. (2) R3M [5]: This model, based on ResNet-50 architecture, leverages egocentric manipulation data 17 Figure 6: Overview of different generalization levels. From left to right: new distractors, new textures of the table, new lighting conditions. Top row: Metaworld. Bottom row: Real-World and time-contrastive learning, making it particularly effective for robotic tasks that require understanding temporal sequences and object interactions. (3) DINOv2 [27]: Utilizes Vision Transformer architecture with self-supervised learning objectives, achieving state-of-the-art performance across numerous downstream tasks. Its training on large-scale curated dataset enhances its robustness and generalization capabilities. In our experiments, we tried the model with Global and Dense feature representation as input to the policy. As the Global representation was always outperforming the other alternative we only show in the experimental sections the result of the Global representation model. (4) VC-1 [6]: Combines the strengths of Vision Transformers with Masked-Auto Encoder objective, pre-trained on both manipulation data and ImageNet. This model is well-suited for tasks requiring fine-grained visual understanding and manipulation. (5) Theia [8]: Distilled from multiple state-of-the-art models, Theia integrates various modalities such as depth, segmentation, and language. Its diverse training objectives make it highly versatile and effective in complex robotic benchmarks. In our experiments, we took the best performing Theia model : CDiV, the model distilled from CLIP [28], DINOv2 [27] and pre-trained ViT on ImageNet. (6) DINOSAUR [18]: Built on top of DINOv2, this model focuses on object-centric representations using feature reconstruction. It adapts Slot-Attention to real-world scenarios, enhancing its ability to handle object-specific tasks. (7) VIDEOSAUR [41]: An extension of DINOSAUR to video data, incorporating temporal alignment loss. This model excels in unsupervised image segmentation in real-world settings, leveraging temporal information for improved performance."
        },
        {
            "title": "F Per task evaluation",
            "content": "To better understand the success and failure of the different models, we also analyze task-level success rates, as shown in Table 7 for MetaWorld and Table 8 for real-world experiments. As can be seen from Table 7, every models primarly fails on three tasks: assembly, bin picking and shelf place, each requiring precise object manipulation. The object-centric model performs on par with the best model, Theia [8], in most scenarios but underperform in the shelf place task. We make the assumption that this performance gap is due to the fact that the cube to grasp in this environment is of small size, which can be hard for our current object-centric model to segment. This is left to be solved with future works. However, in the assembly task, that implies bigger object, the object-centric model are outperforming the other models. In the real-world scenario, the object-centric model outperforms all other models in all tasks as shown in Table 8. 18 Table 7: Per task Metaworld. Comparison of the results per task on the MetaWorld environment. A: Assembly; BP: Bin Picking; BuP: Button Press; D: Dissamble; DO: Drawer Open; H: Hammer, P&P: Pick and Place; SP: Shelf place; StP:Stick Pull; StPus: Stick Push Models ResNet-50 (pre-trained) [48] R3M [5] DINOv2 [27] VC1 [6] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [41] VIDEOSAUR* [41] ResNet-50 (pre-trained) [48] R3M [5] DINOv2 [27] VC1 [6] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [41] VIDEOSAUR* [41] 0.73 0.38 0.57 0.11 0.36 0.90 0.80 0.20 0.80 0.99 1.0 0.97 0.85 1.0 1.0 0.80 0.60 1.0 BP 0.37 0.31 0.41 0.15 0.52 0.53 0.30 0.50 0.47 P&P 0.71 0.39 0.74 0.49 0.75 0.40 0.60 0.20 0.60 BuP 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 SP 0.39 0.29 0.09 0.07 0.55 0.20 0.20 0.40 0.27 0.62 0.35 0.54 0.47 0.72 0.60 1.0 0.67 0.60 StP 0.61 0.70 0.63 0.49 0.57 0.80 0.40 0.80 0.90 DO 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 StPus 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1. Table 8: Per task real-world. Comparison of the results per task on the real-world environment. OD: Open Drawer; CD: Close Drawer; PCD: Put Coffee into Drawer; BB: Banana to Bowl, F: Fold Models ResNet-50 (pre-trained) [48] R3M [5] DINOv2 [27] VC1 [6] Theia [8] DINOSAUR [18] DINOSAUR* [18] VIDEOSAUR [18] VIDEOSAUR* [41] OD 0/10 5/10 9/10 9/10 10/10 5/10 10/10 10/10 10/10 CD 10/10 1/10 0/10 1/10 5/10 4/10 6/10 9/10 10/10 PCD BB 7/10 4/10 0/10 1/10 0/10 0/10 0/10 0/10 0/10 0/10 6/10 1/10 4/10 0/10 6/10 1/10 7/10 4/10 4/10 0/10 2/10 1/10 1/10 4/10 2/10 4/10 5/ Overall 25/50 7/50 11/50 11/50 16/50 20/50 22/50 30/50 36/"
        },
        {
            "title": "G Visualization of slots",
            "content": "We visualize several slot decomposition of our VIDEOSAUR* model on different tasks on Figure 7. Note that the model has never seen the provided data before as it has been pre-trained on frozen to learn subsequent policy. Figure 7: Slots visualization. Visualization of set of slots (12 slots) extracted from VIDEOSAUR* model on the easy distractor setup in Metaworld"
        },
        {
            "title": "H Failure cases visualization",
            "content": "In order to better understand why the object-centric model fails in the distractor scenarios, we decided to check in detail the slots as shown on Figure 8. Figure 8: Failure slots visualization. Visualization of set of slots (8 slots) extracted from VIDEOSAUR* model on the hard distractor setup in Metaworld. The slot that should handle the hammer to perform the task also capture the brown box, leading to noise in the subsequent slot representation."
        }
    ],
    "affiliations": [
        "Ecole Centrale de Lyon, LIRIS 69130, Ecully, France"
    ]
}