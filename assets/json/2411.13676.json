{
    "paper_title": "Hymba: A Hybrid-head Architecture for Small Language Models",
    "authors": [
        "Xin Dong",
        "Yonggan Fu",
        "Shizhe Diao",
        "Wonmin Byeon",
        "Zijia Chen",
        "Ameya Sunil Mahabaleshwarkar",
        "Shih-Yang Liu",
        "Matthijs Van Keirsbilck",
        "Min-Hung Chen",
        "Yoshi Suhara",
        "Yingyan Lin",
        "Jan Kautz",
        "Pavlo Molchanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the \"forced-to-attend\" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 2 ] . [ 1 6 7 6 3 1 . 1 1 4 2 : r 2024-11-22 Hymba: Hybrid-head Architecture for Small Language Models Xin Dong*, Yonggan Fu*2, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu3, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin2, Jan Kautz, Pavlo Molchanov Abstract: We propose Hymba, family of small language models featuring hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the forced-to-attend burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in compact cache size. During development, we conducted controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67 cache size reduction, and 3.49 throughput. Models on Hugging Face: Hymba-1.5B-Base Hymba-1.5B-Instruct 1. Introduction Transformers, with their attention-based architecture, have become the dominant choice for language models (LMs) due to their strong performance, parallelization capabilities, and long-term recall through key-value (KV) caches [1]. However, their quadratic computational cost and high memory demands pose efficiency challenges. In contrast, state space models (SSMs) like Mamba [2] and Mamba-2 [3] offer constant complexity and efficient hardware optimization but struggle with memory recall tasks, affecting their performance on general benchmarks [4, 5]. While existing hybrid models that stack attention and SSM layers have demonstrated potential [6, 7], they can introduce bottlenecks when one layer type is not wellsuited for specific tasks, requiring compensation from subsequent layers. We propose Hymba, novel LM architecture that integrates attention heads and SSM heads within the same layer, offering parallel and complementary processing of the same inputs. This hybrid-head approach allows each layer to simultaneously harness both the high-resolution recall of attention and the efficient context summarization of SSMs, increasing the models flexibility and expressiveness in handling various types of information flows and memory access patterns. To further enhance the achievable performance of Hymba, we introduce learnable meta tokens that are prepended to the input sequences and interact with all Figure 1 (a) Visualize the hybrid-head module in Hymba; (b) Interpret from the memory aspect. subsequent tokens even in sliding window attention. These meta tokens appear to act as compressed representation of world knowledge and alleviate the issue of softmax attention not being able to attend to nothing [8, 9, 10], improving performance across both general and recall-intensive tasks. Sharing KV cache between attention heads is common practice. Inspired by findings in [11] that consec- * Equal contribution; additional affiliation: 2 with Georgia Institute of Technology, USA; 3 with HKUST 2024 NVIDIA. All rights reserved. Hymba: Hybrid-head Architecture for Small Language Models Figure 2 Performance comparison of Hymba-1.5B against sub-2B models in terms of average task accuracy, cache size (MB) relative to sequence length, and throughput (tok/sec). Specifically, the tasks include 5-shot MMLU, ARC-C, ARC-E, PIQA, Hellaswag, Winogrande, and SQuAD-C, and the throughput is measured on an NVIDIA A100 with sequence length of 8k and batch size of 128 using PyTorch. For models encountering out-of-memory (OOM) issues during throughput measurement, we halve the batch size until the OOM is resolved. This approach is used to measure the maximal achievable throughput without OOM. utive layers have high correlation in the KV cache, we propose sharing the KV cache between layers as well. Additionally, for most layers, we choose sliding window attention to further minimize cache costs. Comprehensive evaluations and ablation studies demonstrate that Hymba not only establishes new state-of-the-art (SOTA) benchmark performance across wide range of representative tasks but also achieves greater efficiency compared to transformers and previous hybrid models. We provide the benchmark with other representative small LMs in Fig. 2, with more comprehensive benchmarks in Fig. 9. For instance, in commonsense reasoning tasks, Hymba1.5B can outperform Llama-3.2-3B with 1.32% higher average accuracy, while requiring 11.67 smaller cache size and being 3.49 faster. To optimize Hymba for on-device tasks, we employ supervised finetuning and direct preference optimization [12]. Our instruction-tuned model, Hymba-1.5B-Instruct, achieves best-in-class performance on GSM8K, GPQA, and the Berkeley function-calling leaderboard, surpassing Llama-3.21B. Additionally, parameter-efficient finetuning shows Hymbas strong potential in this setting. For instance, DoRA [13]-finetuned version of Hymba1.5B outperforms Llama3.1-8B-Instruct by 2.4% on RoleBench [14]. 2. Hymba: The Proposed Hybrid-"
        },
        {
            "title": "Head Architecture",
            "content": "SSMs such as Mamba [2] were introduced to address the quadratic complexity and large inference-time KV cache issues of transformers. However, due to their low-resolution memory, SSMs struggle with memory recall and performance [4, 15, 5]. To overcome these limitations, we propose roadmap for developing efficient and high-performing small LMs in Tab. 1 and outlined as follows: Fused hybrid modules. Fusing attention and SSM Configuration Commonsense Reasoning (%) Recall (%) Throughput (token/sec) Cache Size (MB) Design Reason Ablations on 300M model size and 100B training tokens Transformer (Llama) State Space Models (Mamba) A. + Attention heads (sequential) B. + Multi-head structure (parallel) C. + Local / global attention D. + KV cache sharing E. + Meta tokens 44. 42.98 44.07 45.19 44.56 45.16 45.59 Scaling to 1.5B model size and 1.5T training tokens F. + Size / data G. + Extended context length (2K8K) 60.56 60.64 39.98 19.23 45.16 49.90 48.79 48.04 51. 64.15 68.79 721.1 4720.8 776.3 876.7 2399.7 2756.5 2695.8 664.1 664.1 414.7 1.9 156.3 148.2 41.2 39.4 40. Accurate recall while inefficient Efficient while inaccurate recall Enhance recall capabilities Better balance of two modules Boost compute/cache efficiency Cache efficiency Learned memory initialization 78.6 78.6 Further boost task performance Improve multi-shot and recall tasks Table 1 Design roadmap of our Hymba model. We evaluate the models (1) commonsense reasoning accuracy, averaged over 8 tasks, and (2) recall accuracy, averaged over 2 tasks, which corresponds to retrieving relevant information from past input. The throughput is on NVIDIA A100, sequence length 8k, batch size 128. The cache size is measured with 8k sequence length, assuming the FP16 format. Hymba: Hybrid-head Architecture for Small Language Models heads in parallel within hybrid-head module outperforms sequential stacking (Tab. 1 (A)-(B)). Both heads process the same information simultaneously, leading to improved reasoning and recall accuracy. We argue that sequential fusion lacks synergy, as both blocks operate on each set of inputs independently. Efficiency and KV cache optimization. While attention heads improve task performance, they increase KV cache requirements and reduce throughput. To mitigate this, we optimize the hybrid-head module by combining local and global attention and employing cross-layer KV cache sharing, as shown in Tab. 1 (C) and (D). This improves throughput by 3 and reduces cache by almost 4. Meta Tokens set of 128 pretrained embeddings prepended to inputs, functioning as learned cache initialization to enhance focus on relevant information. These tokens serve dual purpose: (i) they mitigate attention drain by acting as backstop tokens, redistributing attention effectively, and (ii) they encapsulate compressed world knowledge, see Tab. 1 (E) and Sec. 2.3. Scaling Ablation studies were performed on 300M parameter model using 100B training tokens; the final models were trained with 1.5T tokens and scaled up to models with 350M and 1.5B parameters (see Tab. 1 (F)). 2.1. Fused Hybrid-Head Module SSM models are efficient but suffer from limited recall capabilities and task performance [4, 15, 5, 16] as seen in Tab. 1. Given the high recall resolution of attention, in this step we aim to (1) combine the processing efficiency and context summarization capabilities of SSMs with the high recall resolution of attention, and (2) develop fused building block to achieve this goal, so it can serve as fundamental component for constructing future foundation models. Previous hybrid models [7, 17, 6] often combine attention and SSMs in sequential manner. This strategy may lead to information bottlenecks when layer type that is poorly suited for specific task cannot effectively process the information. Motivated by the multi-head attention structure in the vanilla Transformer [1], where different heads undertake different roles and focus on different contexts [18, 19], we propose an alternative approach: fusing attention and SSMs in parallel into hybrid-head module, as shown in Fig. 1 (a). The advantage of this design is that different attention and SSM heads can store, retrieve, and process the same piece of information in distinct ways, thereby inheriting the strengths of both operators. Design formulation. We show that the hybridhead module can be represented by unified and symmetric formulation. As shown in Fig. 1 (a), given the input sequence 𝑋, which is the original input sequence 𝑋 prepended with meta tokens introduced in Sec. 2.3, the input projection 𝑊in_proj = [𝑊 𝑄, 𝑊 𝐾, 𝑊 𝑉 , 𝑊 𝑆𝑆𝑀 , 𝑊 𝐺] projects 𝑋 to the query, key, and value of the attention heads using 𝑊 𝑄, 𝑊 𝐾, and 𝑊 𝑉 , respectively, as well as the input features and gates of the SSM heads using 𝑊 𝑆𝑆𝑀 and 𝑊 𝐺, respectively. Following [1], the output of attention heads 𝑌attn can be formulated as: 𝑌attn = softmax(𝑄𝐾 𝑇 ) 𝑊 𝑉 𝑋 = 𝑀attn 𝑋 (1) where 𝑀attn = softmax(𝑄𝐾 𝑇 ) 𝑊 𝑉 and 𝑄 = 𝑊 𝑄 𝑋, 𝐾 = 𝑊 𝐾 𝑋. Similar to the attention heads, the SSM heads in our model, for which we adopt Mamba [2], can also be represented using data-controlled linear operator 𝑀ssm, following [20, 16]. Specifically, the SSM head output 𝑌ssm can be formulated as: 𝛼𝑖,𝑗 = 𝐶𝑖 𝑖 𝑘=𝑗+1 exp(𝐴Δ𝑘) 𝐵𝑗Δ𝑗, (2) 𝑌ssm = 𝐺 𝛼(𝐴, 𝐵, 𝐶, Δ) 𝑊 𝑆𝑆𝑀 𝑋 = 𝑀ssm 𝑋, where 𝑀ssm = 𝐺 𝛼(𝐴, 𝐵, 𝐶, Δ) 𝑊 𝑆𝑆𝑀 , 𝐺 = 𝑊 𝐺 𝑋 is an output gate, and 𝐴, 𝐵, 𝐶, Δ are the SSM parameters following the definition in [2]. More specifically, 𝐴 is learnable matrix, 𝐵 = 𝑊𝐵𝑋𝑠𝑠𝑚, 𝐶 = 𝑊𝐶𝑋𝑠𝑠𝑚, and Δ = Softplus(𝑊Δ𝑋𝑠𝑠𝑚) with 𝑋𝑠𝑠𝑚 = 𝑊 𝑆𝑆𝑀 𝑋. We observed that the output magnitudes of the SSM heads, 𝑌ssm, are consistently larger than those of the attention heads, 𝑌attn, as visualized in Fig. 12 in Append. B. To ensure effective fusion, we normalize and re-scale them using learnable vectors to improve training stability, and then average the outputs, followed by final output projection. The overall formulation of our fused module can be represented symmetrically: 𝑌 = 𝑊out_proj (𝛽1norm(𝑀attn 𝑋) + 𝛽2norm(𝑀ssm 𝑋)) (3) where 𝛽1 and 𝛽2 are learnable vectors that re-scale each channel of the outputs from the attention and SSM heads, respectively. We further explore the optimal ratio of SSMs and attention in hybrid heads, along with their fusion strategy, in Append. B. Interpretation from the memory aspect. The components in the hybrid-head module can be interpreted as analogous to human brain functions. Specifically, as shown in Fig. 1 (b), the attention heads Hymba: Hybrid-head Architecture for Small Language Models provide high recall resolution and thus act like snapshot memories in the human brain, storing detailed recollections of moment or event. In contrast, the SSM heads summarize the context through constant cache and thus function as fading memories, which gradually forget the details of past events while retaining their core or gist. As shown in Tab. 10 in Append. B, in our Hymba, the summarized global context from fading memories enables allocating more snapshot memories for memorizing local information while maintaining recall capabilities. This is achieved by replacing most global attention with local attention, thus improving memory efficiency. Figure 3 Visualize the accuracy difference, measured using 1000 samples from Hellaswag [21], after removing the Attention or SSM heads in each layer. Head importance analysis. We analyze the relative importance of attention and SSM heads in each layer by setting 𝛽1 or 𝛽2 in Eq. 3 to 0 and recording the final accuracy. We present the results on Hellaswag [21] in Fig. 3 and on more tasks in Fig. 13 in Append. C. We find that (1) the relative importance of attention/SSM heads in the same layer is input-adaptive and varies across tasks, suggesting that they can serve different roles when handling various inputs; (2) The SSM head in the first layer is critical for language modeling, and removing it causes substantial accuracy drop to random guess levels; (3) Generally, removing one attention/SSM head results in an average accuracy drop of 0.24%/1.1% on Hellaswag, respectively. 2.2. KV Cache Optimization Our hybrid-head module improves recall and reasoning capabilities but can compromise memory and throughput efficiency due to the KV cache required by the attention heads. To address this, we aim to reduce the KV cache while maintaining comparable task performance. Figure 4 (a) The overall architecture of our Hymba model; (b) The building block of Hymba. context, we can more aggressively replace global full attention with local attention, achieving better balance between efficiency and performance. Exploring the ratio of local attention and global attention. As shown in Tab. 10 in Append. B, we initially replace global attention in all layers with SWA, which results in significant degradation in recall capabilities, with accuracy dropping by over 20% on recall-intensive tasks. In response, we progressively reinstate global attention in some layers. Interestingly, as shown in Tab. 1 (C), we find that using global attention in just three layers (i.e., the first, middle, and last layers) is sufficient to recover recall-intensive accuracy while maintaining comparable commonsense reasoning accuracy. In turn, this strategy achieves 2.7 throughput and 3.8 cache reduction. Cross-layer KV sharing. Recent works [23] observe that KV cache shares high similarity between adjacent layers, suggesting that using separate KV caches for each layer leads to both cache and parameter redundancy. In light of this, we employ cross-layer KV sharing [11], where keys and values are shared between consecutive layers (e.g., every two layers share the same KV cache). This strategy reduces both KV memory usage and model parameters, allowing the saved parameters to be reallocated to other model components. As shown in Tab. 1 (D), crosslayer KV sharing improves throughput by 1.15 while maintaining comparable recall accuracy and boosting commonsense accuracy by +0.60%. After the above optimization, Hymbas overall architecture is visualized in Fig. 4. 2.3. Meta Tokens Combine global and local attention. Local attention, also known as Sliding Window Attention (SWA) [22], offers more efficient alternative to global full attention, though it risks losing global context. However, with the presence of SSM heads in our hybrid-head module, which already summarize global We observed that the initial tokens, though not semantically important, often receive significant attention scores from subsequent tokens, similar to observations in prior work [10, 27]. As shown in Fig.7, more than 50% of the attention is focused on the BOS token for Llama3.2-3B. To address this, we aim to 4 Hymba: Hybrid-head Architecture for Small Language Models Figure 6 Schematics of the attention map of Hymba as combination of meta tokens, sliding window attention, and Mamba contributions. Figure 5 Averaged attention scores received by the meta tokens in the last layer of Hymba-1.5B model. Prompts of Article, Math and Code are from SQuAD [24], GSM8K [25], and GitHub-Code [26] datasets, respectively. guide the attention to focus more on tokens that meaningfully contribute to task performance. Specifically, we introduce set of learnable meta tokens 𝑅 = [𝑟1, 𝑟2, . . . , 𝑟𝑚] to serve as the initial tokens. Given the input sequence 𝑋 = [𝑥1, 𝑥2, . . . , 𝑥𝑛], these meta tokens are prepended to the input sequence, forming the modified input sequence: 𝑋 = [𝑅, 𝑋] = [𝑟1, 𝑟2, . . . , 𝑟𝑚, 𝑥1, 𝑥2, . . . , 𝑥𝑛] (4) where 𝑋 represents the new input sequence for our model. At inference time, since the meta tokens are fixed and appear at the beginning of any input sequences, their computation can be performed offline. Thus, the role of meta tokens at inference can also be viewed as learned cache initialization to modulate the subsequent tokens, allowing subsequent tokens to focus more on those that contribute meaningfully to task performance. Interpretation from the memory aspect. Similar to the analogy in Sec. 2.1, the meta tokens participate in the attention and SSM calculations of all subsequent tokens, analogous to metamemory in the human brain, which helps recognize where to locate needed information in other memories. To see this, we visualize the averaged attention scores received by the meta tokens in Fig. 5 for Hymba-1.5B model. We observe that when the prompts are from different domains (e.g., article, math, and codes), different meta Figure 7 Sum of attention score from different categories (i.e., Meta, BOS, Self, Cross) in Llama3.2-3B, Jamba and Hymba-1.5B. Parallel SSM and Attention fusion in the latter disentangles attention. tokens are activated. This suggests that different meta tokens encapsulate different world knowledge, which can be leveraged to guide the attention mechanism to focus on relevant information. We further analyze others roles of meta tokens and their connections with related works in Append. D. The role of Meta Tokens. We hypothesise, that they perform the following functions. Prevent token overwriting. As shown in [30], attention tends to overwrite and over-attend to some tokens, acting as garbage collector. Adding learnable tokens allowed for much more representative feature maps. Later, the same phenomenon was discovered in LLMs and named attention sinks [10, 27]. Therefore, the model should be provided with tokens that are independent of the input. Exit tokens to deal with forced-to-attend. Prepending tokens to the input affects the shape of the soft5 Hymba: Hybrid-head Architecture for Small Language Models Table 2 Benchmark Hymba with SOTA small LMs. All models have fewer than 2B parameters, except for Llama-3.2-3B, which is marked as gray. All results are obtained through - va at - n [28]. SQuAD-C (SQuAD-Completion) indicates variant of the SQuAD question answering task proposed by [29]. The throughput is measured with 8k sequence length and 128 batch size on an NVIDIA A100 GPU. The best results are highlighted in bold, and the second-best results are highlighted in underline, where Llama-3.2-3B is not included in the ranking due to its 3B model size. Model #Params. Train tokens Token/s Cache MMLU ARC-E ARC-C PIQA Wino. Hella. SQuAD-C Avg. (MB) 5-shot 0-shot 0-shot 0-shot 0-shot 1-shot 0-shot OpenELM-1 Rene-v0.1 Phi-1.5 SmolLM Cosmo h2o-danube2 Llama-3.2-1B Qwen2.5 AMD-OLMo SmolLM2 Llama-3.2-3B Hymba 1.1B 1.3B 1.3B 1.7B 1.8B 1.8B 1.2B 1.5B 1.2B 1.7B 3.0B 1.5B 1.5T 1.5T 0.15T 1T 0.2T 2T 9T 18T 1.3T 11T 9T 1.5T 246 800 241 238 244 271 535 469 387 238 191 664 346 113 1573 1573 1573 492 262 229 1049 1573 27.06 32.94 42.56 27.06 26.10 40.05 32.12 60.92 26.93 50.29 56.03 62.37 67.05 76.18 76.47 62.42 70.66 65.53 75.51 65.91 77.78 74.54 19.54 31.06 44.71 43.43 32.94 33.19 31.39 41.21 31.57 44.71 42.32 74.76 61.80 48.37 76.49 62.75 51.16 76.56 72.85 48.00 75.79 60.93 49.58 71.76 55.80 42.90 76.01 66.93 53.70 74.43 60.69 47.72 75.79 63.38 50.20 74.92 61.64 47.30 77.09 66.38 53.55 76.66 69.85 55.29 45.38 48.36 30.09 45.81 38.51 49.03 40.18 49.53 33.71 50.50 43.46 48.47 52.83 55.85 54.15 47.20 55.65 50.29 59.51 48.85 60.04 59. 79 51.19 76.94 45.90 77.31 66.61 53.55 55.93 61. max function by modifying the denominator. Quiet Attention [31] modifies the softmax denominator by adding one, allowing the attention to output zeros. Adding one is equivalent to prepending an all-zero token to the keys and values. Our meta tokens take this idea further by being learnable, allowing to learn an optimal softmax shape. Initialization for KV cache and SSM state. Learning initial tokens can be seen as form of learned prompt tuning [32, 33] or learned initialization. For inference, meta tokens are fixed, and the keys and values can be precomputed offline and stored. Task-specific meta tokens can be used, though in this work we use one set for all tasks. Meta tokens boost recall capabilities and commonsense reasoning accuracy. To analyze the impact of meta tokens on the attention mechanism, we visualize the entropy of the attention map for both the attention and SSM heads [20, 16] before and after introducing meta tokens. Specifically, the attention map entropy reflects the distribution of attention scores across tokens, where lower entropy indicates stronger retrieval effects [7], as the attention scores are concentrated around smaller subset of tokens, and vice versa. We provide the visualization in Fig. 15 in Append. D, where we observe that, after introducing meta tokens, both the attention and SSM heads exhibit an overall reduction in entropy. Combined with the improved reasoning and recall capabilities shown in Tab. 1 (E), this suggests that meta tokens may help both the attention and SSM heads focus more on subset of important tokens that contribute most to task performance. 2.4. Hymba Attention Map Hymbas attention pattern  (Fig. 6)  can be viewed as combination of individual components from sliding window attention, meta tokens, and SSM. We further categorize elements in the attention map into four types: (1) Meta: attention scores from all real tokens to meta tokens. This category reflects the models preference for attending to meta tokens. In attention map, they are usually located in the first few columns (e.g., 128 for Hymba) if model has meta tokens. (2) BOS: attention scores from Figure 8 Training pipeline adapted for Hymba family. For detailed loss curve of Hymba-Base-1.5B see Fig 14. 6 Hymba: Hybrid-head Architecture for Small Language Models all real tokens to the beginning-of-sequence token. In the attention map, they are usually located in the first column right after the meta tokens. (3) Self: attention scores from all real tokens to themselves. In the attention map, they are usually located in the diagonal line. (4) Cross: attention scores from all real tokens to other real tokens. In the attention map, they are usually located in the off-diagonal area. In Fig. 7, we visualize the real attention maps from Llama-3.2-3B and Hymba-1.5B on texts from Oliver Twist Chapter 29 [34] and sum up the attention scores from different categories. The summed scores are normalized by the context length. For SSM heads, we follow Ben-Kish et al. [16] and Zimerman et al. [35] to calculate their attention maps and normalize the attention maps to ensure each row sums to 1. We observe that the attention pattern of Hymba is significantly different from the vanilla Transformers. In vanilla Transformers, attention scores are more concentrated on BOS, which is consistent with the findings in [10]. In addition, vanilla Transformers also have higher proportion of Self attention scores. In Hymba, meta tokens, attention heads and SSM heads work complimentary to each other, leading to more balanced distribution of attention scores across different types of tokens. Specifically, meta tokens offload the attention scores from BOS, allowing the model to focus more on the real tokens. SSM heads summarize the global context, which focus more on current tokens (i.e., Self attention scores). Attention heads, on the other hand, pay less attention to Self and BOS tokens, and more attention to other tokens (i.e., Cross attention scores). This suggests that the hybrid-head design of Hymba can effectively balance the attention distribution across different types of tokens, potentially leading to better performance. 2.5. Hymba Model Family Building on the design insights explored above, we scale up the model sizes and training tokens to deliver the Hymba model family, which includes 125M model, 350M model, and 1.5B model. We train Hymba-125M/350M/1.5B models using mix of DCLM-Baseline-1.0 [36], SmoLM-Corpus [37], and proprietary high-quality dataset, with 1T, 250B, and 50B tokens, respectively. We combine the Warmup-Stable-Decay (WSD) learning rate scheduler [38], with maximum and minimum learning rates of 3e-3 and 1e-5, and the data annealing technique [39, 40] to ensure stable pretraining. We use sequence length of 2k and batch size of 2M tokens throughout the training process until the last 100B tokens, where we increase the sequence length to 8k and change the ROPE base following [41]. The overall training pipeline is illustrated in Fig. 8. More pretraining details are provided in Append. E. 3. Model Evaluations 3.1. Experiment Settings Baselines. Our baselines include popular (small) LMs with quadratic attention (e.g., Llama 3.2 [42], SmolLM [43], SmolLM2 [44], AMD-OLMo [45], StableLM [46], Olmo [47], Cosmo [48], Phi-1.5 [49], H2ODanube [50], OpenELM [51], and MiniCPM [38]), as well as hybrid models (e.g., Rene [52]). Benchmark settings. We adopt two benchmarking settings: (1) In Sec. 3.2, we directly benchmark our delivered Hymba against SOTA public small LMs, and (2) in Sec. 3.3, we train different architectures from scratch with the same dataset, number of layers, model size, and training recipes. Benchmark tasks. In addition to evaluating commonsense reasoning and recall-intensive tasks on our base models, we also evaluate our instruction-tuned models on downstream tasks such as math, function calling, and role-playing in Sec. 3.4. 3.2. Benchmark with SOTA Small LMs We present the benchmark results of our Hymba models with parameter sizes of 125M, 350M, and 1.5B, compared to SOTA small language models within the same size range. As highlighted in Tab. 2, with only 1.5T pretraining tokens, our Hymba-1.5B model achieves the best performance among all sub-2B LMs and demonstrates better throughput and cache efficiency compared to all transformer-based LMs, with this speedup becoming even more pronounced as the sequence length increases. For instance, compared to the strongest sub-2B baseline, SmolLM2-1.7B, trained on 11T tokens, our Hymba-1.5B, trained on only 1.5T tokens, achieves 1.02% average accuracy improvement, 19.91 cache size reduction, and 2.79 throughput. When comparing with small LMs trained on no more than 2T tokens, our model achieves 5.21%/5.41% average accuracy improvement over the most competitive baselines, Phi-1.5 and h2o-danube2-1.8B, respectively. Additionally, our model even outperforms Llama-3.2-3B, with 1.32% higher average accuracy, an 11.67 cache size reduction, and 3.49 throughput. We visualize the trade-offs between commonsense reasoning accuracy and cache size/throughput in Fig. 9. In addition, our delivered tiny LMs, Hymba125M/350M, consistently outperform all LMs of com7 Hymba: Hybrid-head Architecture for Small Language Models Figure 9 Visualize the trade-off between (a) commonsense reasoning accuracy (avr. ARC-C, ARC-E, PIQA, Hellaswag, OBQA, and Winogrande using [28]) and cache size, with throughput represented by the point size of different models, and (b) commonsense reasoning accuracy and throughput, with cache size represented by the point size. The throughput is measured with 8k sequence length and 128 batch size on an NVIDIA A100 GPU. The cache size is measured with 8k sequence length, assuming the FP16 format. parable model size, as summarized in Tab. 6 and Tab. 7 in Append. A.1. We have also provided Hymba-1.5B model trained exclusively on public data in Append. A.2. 3.3. Benchmark Different Architectures Under The Same Setting General and recall-intensive tasks performance comparison. We do comprehensive comparison between Hymba and other model architectures, including standard Transformer (Llama3 [53]), pure Mamba [2, 3], Mamba with FFN and hybrid architecture with sequential layer stacking (Samba [7]) on several downstream tasks. All models have the same number of layers and total parameters to facilitate equal comparison. Models are trained on the same data with the same hyperparameters and under the same codebase. To ensure our conclusions are generally valid, we run comparison experiments at different scales (1B and 300M) and different training datasets (SmolLM-corpus [37] and FineWeb [54]) in Tab. 3 and Tab. 9, respectively. We evaluate the models on language modeling, real-world recall-intensive, commonsense reasoning, and question-answering tasks. As shown in Tab. 3, our Hymba model consistently outperforms other 1B architectures across most tasks, e.g., achieving an average score 1.45% higher than the second-best model at the 300M scale and 1.74% higher at the 1B scale. The ablation study for the 300M scale is in Append. A. In addition, considering that Mamba models suffer from limited recall capabilities due to their constantsize cache and recurrent nature [16, 5, 15], we test the models on two real-world recall-intensive tasks, SWDE [5, 55] and SQuAD [5, 56], where the former is to to extract semi-structured relations from given raw HTML websites and the latter is to extract answers from given context passages. Echoing the previous findings, Mamba2 and Mamba2 with FFN architectures under-perform the Transformer model (i.e. Llama3) on these tasks (see Tab. 3). Hymba model augments the Mamba heads with attention heads, which allows the model to have large effective receptive field to establish long-range dependencies and high-resolution memory to store and retrieve key information in all layers. As result, Hymba outperforms the Transformer and Samba architectures (where the latter stacks Mamba and attention layers sequentially). Needle-in-the-Haystack performance comparison. We further do an apple-to-apple comparison between Hymba, Mamba2, and Llama3 on the synthetic retrieval task, needle-in-the-haystack. random and informative sentence (i.e., needle) is inserted into long document (i.e., haystack) and the model is required to retrieve the needle from the haystack to answer the questions. All models are of size 1B and trained with the same setting: (i.) pretrain is done with 1k sequence length; (ii.) finetune with 4k sequence length; (iii.) test with up to 16k sequence length. If model has ROPE, then we adjust the ROPE as in [57] during finetuning. As shown in Fig. 10, the Hymba model significantly outperforms the Mamba2 and Llama3 models. While the Mamba2 model has good extrapolation capabilities when the needle is inserted in the end of the haystack, it struggles to retrieve the needle when the needle is in the beginning 8 Hymba: Hybrid-head Architecture for Small Language Models Task Type Language Recall Intensive Commonsense Reasoning and Questionanswering Arch. Style (1B) Wiki. ppl. LMB. ppl. SWDE SQuAD-C Avg. Lambda PIQA ARC-C ARC-E Hella. Wino. TruthfulQA SIQA Avg. Mamba2 Mamba2 w/ FFN Llama3 Samba Hymba 19.17 12. 50.24 36.43 43.34 47.51 73.94 38.91 70.96 57.73 58.48 30.75 41.86 52.52 20.42 14.43 26.43 31.40 28.92 44.54 73.07 37.03 71.00 55.83 55.56 29.86 42.22 51.14 19.28 13. 75.95 18.70 47.33 47.95 73.45 39.68 73.74 57.64 56.20 31.64 42.22 52.82 19.91 12.65 30.00 42.33 36.17 49.08 73.23 39.59 73.36 58.49 57.54 28.84 42.48 52.83 18.62 10. 54.29 44.71 49.50 52.84 74.97 41.72 74.12 60.05 57.85 31.76 43.24 54.57 Table 3 Apple-to-apple comparison of our Hymba, pure Mamba2 [3], Mamba2 with FFN, Llama3 [39] style, and Samba- [7] style (Mamba-FFN-Attn-FFN) architectures. All models have 1B parameters and are trained from scratch for 100B tokens from SmolLM-Corpus [37] with exactly the same training recipe. All results are obtained through - va at - n [28] using zero-shot setting by us on HuggingFace models. The best and second best results are highlighted in bold and underline, respectively. or middle of the haystack. In contrast, Llama3 model has limited extrapolation capabilities [58, 57, 59] and struggles to the lost in the middle [60] scenario. Figure 10 Needle-in-the-haystack performance comparison across different architecture under apple-toapple setting. The white vertical line represents the finetuning sequence length (4k). 3.4. Instruction-tuned Model Implementation details of post-training. We post-trained Hymba-1.5B base model with two-stage strategy: the first full-finetuning (FFT) stage and another direct preference optimization (DPO) [12] training. The learning rates are 5e-5, and 3e6 for FFT and DPO, respectively. To accelerate training, we follow the training recipe [61, 62, 63] to pack the samples and use block size of 8192. We compare Hymba-1.5B-Instruct with competitive lightweight instruction-tuned models, i.e., Llama-3.2-1B-Instruct [42], OpenELM-11B-Instruct [51], Qwen2.5-1.5B-Instruct [64], and SmolLM-1.7B-Instruct [43]. We test the instructiontuned models on MMLU (5-shot), IFEval, GSM8K (5shot), GPQA (0-shot), and Berkeley Function-Calling Leaderboard v2 (BFCLv2) [65]. More details about the experimental settings, baseline models, and evaluation tasks are shown in Append. E. Evaluation results. The evaluation results are shown in Tab. 4. In general, Hymba-1.5B-Instruct achieves the highest performance on an average of all tasks, outperforming the previous SoTA model, Qwen2.5-Instruct, by around 2%. It demonstrates great ability in math, reasoning, and function calling, with the best-in-class performance. Evaluation on role-play tasks. In addition to full finetuning, we conduct experiments to evaluate whether Hymba is compatible with DoRA [13], parameter-efficient finetuning method that updates pretrained models using minimal set of parameters. This approach is especially well-suited for ondevice finetuning scenarios where computational resources are constrained. Additionally, DoRA significantly reduces storage requirements for saving multiple downstream models, as it only requires storing the finetuned DoRA parameters, which constitute less than 10% of the original models total parameters. Specifically, we further finetune the posttrained Hymba on RoleBench [14] using DoRA to enhance its role-playing capabilities. The training set of RoleBench is used for training, and the model is evaluated on two sub-tasks: instruction generalization (Inst. Gene.) and role generalization (Role. Gene.). As shown in the Tab. 5, our Hymba-DoRA significantly outperforms larger models. For instance, DoRA finetuned Hymba achieves scores of 40.0% / 9 Hymba: Hybrid-head Architecture for Small Language Models Table 4 The comparison between lightweight instruction-tuned models. The best and second-best results are highlighted in bold and underlined, respectively. * OpenELM and SmolLM cannot understand function calling, leading to 0 accuracy in most categories. Model #Params MMLU SmolLM OpenELM Llama-3.2 Qwen2.5 SmolLM2 Hymba-1.5B 1.7B 1.1B 1.2B 1.5B 1.7B 1.5B 27.80 25.65 44.41 59.73 49.11 52.79 IFEval GSM8K GPQA BFCLv2 Avg. 20.00 25.67 27.39 21.62 38.14 24.11 47.30 30.13 40.78 29.24 25.16 6.25 58.92 46.78 55.06 -* -* 20.27 43.85 22. 1.36 56.03 42.99 56.03 47.68 57.14 58.76 31.03 46.40 49. Model #Params Instruction Role Generalization Generalization Llama-7B Aplaca-7B Vicuna-13B Llama2-7B-chat RoleLlama-7B 7B 7B 13B 7B 7B Hymba-DoRA 1.5B 19.2 25.6 25.0 18.8 35.5 40.0 19.3 24.5 24.3 20.5 33. 37.9 Table 5 The comparison between DoRA-finetuned Hymba and baselines on RoleBench. All baseline results are from [14]. 37.9% on instruction generalization/role generalization, outperforming RoleLlama-7B [14] by 4.5%, and 4.4% respectively. This indicates the strong generalization of our model and the effectiveness of using parameter-efficient finetuning techniques to further enhance its performance. 4. Related Works Large language models. Prior to the rise of LLMs, transformer-based models [1, 66, 67, 68] proved highly effective at capturing relationships between tokens in complex sequences through the use of the attention mechanism [1]. These models also demonstrated considerable scalability [69, 70, 71] in terms of both model size and the volume of pretraining data. This scalability paved the way for the development of LLMs, such as GLM [72], OPT [73], Mistral [74], the Llama series [75, 53], Gemma [76], and GPT-4 [77], which showcase remarkable zero-shot and few-shot in-context learning abilities. Efficient language model architectures. Despite the promise of transformer-based LMs, the quadratic computational complexity and the linearly increasing KV cache size of attention modules with longer sequences limit their processing efficiency. To address this, efficient LMs featuring sub-quadratic complexity in sequence length and strong scaling properties have emerged [78, 79, 2, 3, 80, 81]. As pointed out by [2], popular efficient LM architectures such as RWKV [78] and RetNet [79] can be viewed as variants of SSMs [82, 83]. These models utilize linear dynamical system approach with constant-size memory to recurrently encode past information, achieving linear scaling with sequence length. Mamba[2], one of the most widely used SSMs, improves upon previous SSMs by selectively propagating or forgetting information along the sequence length in an inputdependent manner. This approach outperforms transformers on several downstream tasks while offering faster inference. Follow-up works such as Mamba2 [3] and GLA [80] introduce more hardware-friendly gating mechanisms to enhance training throughput over Mamba. However, despite their promise, SSMs have been identified as having limited recall capabilities [4] and underperforming on in-context learning tasks [84]. Hybrid language models. To combine the processing efficiency of SSMs with the recall capabilities of transformers, an emerging trend is the creation of hybrid models that incorporate both types of operators. Specifically, [84] proposes hybrid model called MambaFormer, which interleaves Mamba and attention modules to improve in-context learning capabilities. Similarly, [4] finds that introducing small number of attention layers into Mamba model can significantly enhance both commonsense reasoning and long-context capabilities. Jamba [6] and Zamba [17] develop sequentially stacked Mamba-Attention hybrid models. Jamba further integrates Mixture-of-Experts into the MLP layers, while Zamba employs shared attention module. Both models demonstrate improvements in inference speed and task accuracy compared to previous transformer-based models of similar size. Samba [7] introduces structure that sequentially stacks Mamba, SWA, and MLP layers by repeating the Mamba-MLP-SWA-MLP structure, achieving constant throughput as sequence lengths increase. Other recent work has also explored hybrid models that mix either linear RNNs or convolutions with attention [85, 86, 87, 88]. This work proposes new hybrid model featuring fused multi-head building block that stacks hybrid operators in parallel. Our model outperforms previous architectures, as demonstrated by 10 Hymba: Hybrid-head Architecture for Small Language Models extensive benchmarking in Sec. 3. 5. Conclusion In this work, we present Hymba, new family of small LMs featuring hybrid-head architecture that combines the high-resolution recall capabilities of attention heads with the efficient context summarization of SSM heads. To further optimize the performance of Hymba, we introduce learnable meta tokens, which act as learned cache for both attention and SSM heads, enhancing the models focus on salient information. Through the roadmap of Hymba, comprehensive evaluations, and ablation studies, we demonstrate that Hymba sets new SOTA performance across wide range of tasks, achieving superior results in both accuracy and efficiency. Additionally, our work provides valuable insights into the advantages of hybrid-head architectures, offering promising direction for future research in efficient LMs. 6. Acknowledgments This work would not have been possible without additional contributions from many people at NVIDIA, including Hanah Zhang, Maksim Khadkevich, Mohammad Shoeybi, Mostofa Patwary, Nikolaus Binder, Chenhan Yu, Meredith Price, and Oluwatobi Olabiyi."
        },
        {
            "title": "References",
            "content": "[1] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [2] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [3] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [4] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. [5] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. [6] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [7] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. [8] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliers by helping attention heads do nothing. Advances in Neural Information Processing Systems, 36:7506775096, 2023. [9] Evan Miller. Attention is off by one. [10] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [11] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. [12] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [13] Shih-yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weightdecomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. [14] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023. [15] Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. [16] Assaf Ben-Kish, Itamar Zimerman, Shady AbuHussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba, 2024. [17] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024. Hymba: Hybrid-head Architecture for Small Language Models [18] Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan. Interpreting key mechanisms of factual recall in transformer-based language models. arXiv preprint arXiv:2403.19521, 2024. [19] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Talking heads: Understanding inter-layer communication in transformer language models. arXiv preprint arXiv:2406.09519, 2024. Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. [29] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recallthroughput tradeoff, 2024. [20] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. [30] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. [21] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [22] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [23] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366, 2024. [24] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. [25] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [26] CodeParrot. Codeparrot/github-code datasets at hugging face. [27] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, 2024. [28] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, [31] Evan Miller. Attention if off by one, 2023. [32] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [33] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021. [34] Charles Dickens. The Adventures of Oliver Twist. Ticknor and Fields, 1868. [35] Itamar Zimerman, Ameen Ali, and Lior Wolf. unified implicit attention formulation for gatedlinear recurrent sequence models. arXiv preprint arXiv:2405.16504, 2024. [36] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. [37] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 2024. [38] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. 12 Hymba: Hybrid-head Architecture for Small Language Models [39] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [40] Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. Jetmoe: Reaching llama2 performance with 0.1 dollars. arXiv preprint arXiv:2404.07413, 2024. [41] bloc97. Dynamically scaled rope further increases performance of long context llama with zero finetuning, July 2023. [42] Meta AI. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. 2024. [43] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Smollm - blazingly fast and remarkably powerful, 2024. [44] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance, 2024. [45] Jiang Liu, Jialian Wu, Prakamya Mishra, Zicheng Liu, Sudhanshu Ranjan, Pratik Prabhanjan Brahma, Yusheng Su, Gowtham Ramesh, Peng Sun, Zhe Li, Dong Li, Lu Tian, and Emad Barsoum. Amd-olmo: series of 1b language models trained from scratch by amd on amd instinct mi250 gpus., October 2024. [46] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. [47] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. Preprint, 2024. [48] Huggingface. HuggingFaceTB/cosmo-1b. 2024. [49] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. [50] Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, and Sri Satish Ambati. H2o-danube-1.8 technical report. arXiv preprint arXiv:2401.16818, 2024. [51] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024. [52] Cartesia AI. The On-Device Intelligence Update. 2024. [53] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM to date. 2024. [54] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. [55] Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. OpenCeres: When open information extraction meets the semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 30473056, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [56] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad, 2018. [57] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin Scaling An, Xipeng Qiu, and Dahua Lin. laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023. [58] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. [59] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. Inf bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1526215277, 2024. [60] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Hymba: Hybrid-head Architecture for Small Language Models [61] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. [62] Shizhe Diao, Rui Pan, Hanze Dong, Kashun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. Lmflow: An extensible toolkit for finetuning and inference of large foundation models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations), pages 116127, 2024. [63] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. [64] Qwen Team. Qwen2.5: party of foundation models, September 2024. [65] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/ 8_berkeley_function_calling_leaderboard.html, 2024. [66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [67] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [68] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. [69] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters. arXiv preprint arXiv:2307.14995, 2023. [70] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [71] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [72] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [73] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [74] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [75] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. [76] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [77] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [78] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. 14 Hymba: Hybrid-head Architecture for Small Language Models [79] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [80] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [81] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. [82] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [83] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021. [84] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024. [85] Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [86] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin. Blockstate transformers. Advances in Neural Information Processing Systems, 36, 2024. [87] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers for speech recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [88] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. [89] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [90] Mikhail Burtsev, Yuri Kuratov, Anton Peganov, and Grigory Sapunov. Memory transformer. arXiv preprint arXiv:2006.11527, 2020. [91] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. Hymba: Hybrid-head Architecture for Small Language Models A. Extensive Benchmark for More B. Ablation Studies of Our Hymba"
        },
        {
            "title": "Architecture",
            "content": "A.1. Comparison with SOTA Tiny LMs at 350M and 125M Scales We perform further ablation studies and analyses of the design factors in our Hymba. Besides our 1.5B model, we also evaluate the 350M and 125M Hymba models on diverse set of benchmarks in Tab. 6 and Tab. 7, respectively. Consistent with the results of our 1.5B model, Hymba350M/125M models outperform the SOTA tiny LMs across most of tasks and achieve the best average score. This indicates that our Hymba scales effectively across different model sizes. A.2. Evaluating Hymba-1.5B Trained on Public Data Only We have also trained our Hymba-1.5B model exclusively on public data and evaluated its performance. Specifically, following the training settings in Sec. 2.5, we train Hymba-1.5B on DCLM-Baseline-1.0 [36] for 1T tokens in the first phase and on SmoLMCorpus [37] for 500B tokens in the second phase, keeping all other settings the same. The results are summarized in Tab. 8, where only the most competitive baselines from Tab. 2 are included. We observe that (1) Hymba-1.5B trained exclusively on public data only still surpasses all baseline small LMs in terms of average accuracy; and (2) Hymba-1.5B trained on public data primarily suffers from performance drops on 5-shot MMLU compared to the version trained on all data, including our proprietary dataset. This suggests that the public data used may lack sufficient factual knowledge, which is supplemented by our proprietary one. A.3. Apple-to-Apple Comparison with Other Architectures at 300M Scale In addition to the apple-to-apple architecture comparison under the same settings with 1B model size in Sec. 3.3 of our main paper, we further validate the superiority of our architecture at the 300M size. Specifically, we train different 300M model architectures on 100B tokens from FineWeb [54]. We set peak learning rates to 5e-4 and use warmup and cosine decay scheduler. The training sequence length is set to 1K. For models with sliding window attention, we set the sliding window size as 256. As shown in Tab. 9, Hymba achieves the best performance in almost all tasks (with second-best result in one task), yielding an average accuracy boost of +1.45% compared to the strongest baseline. Parallel vs. Sequential fusion We compare the hybrid-head module with sequential counterpart, which interleaves local attention and Mamba layers as adopted by [7], by calculating the models effective receptive field (ERF) and their overall cache size. All the compared models have the same parameter size and are training from scratch using exactly the same training recipe. ERF is an empirical measure of the averaged distance among tokens that allows effective information propagation [16, 89] defined as the following, 𝐸𝑅𝐹 𝑛𝑁 ℎ𝐻 𝑠𝑆 2𝑀 ℎ (𝑆, 𝑠) (𝑆 𝑠) (𝑁 𝑛 + 1) 𝐻𝑁 (𝑁 + 1) , (5) where 𝑆 is index of the last token in the sequence, 𝑁 is index of the last layer in the model, and 𝑀 ℎ(𝑆, 𝑠) is the normalized attention score between token 𝑠 and the last token in head ℎ. As shown in Fig. 11, we observe that (1) in line with common intuitions, Llama3 exhibits notably larger ERF compared to Mamba due to its higher recall resolution, albeit at the cost of larger cache size; (2) our multi-head structure demonstrates the best ERF across the four designs, with an order of magnitude larger ERF while maintaining cache size comparable to the sequential structure. This suggests that the parallel structure can better leverage the limited cache size to capture longer and more complex relationships among tokens compared to the sequential one. The differences in ERF are also reflected in task accuracy: According to Tab. 1, the multi-head design (Tab. 1 (B)) improves commonsense reasoning and recall accuracy by +1.08% and 4.74%, respectively, over the sequential design (Tab. 1 (A)). Based on this Figure 11 Visualize the ERF and cache size trade-off. 16 Hymba: Hybrid-head Architecture for Small Language Models Table 6 Benchmark Hymba with SOTA tiny LMs, all of which have fewer than 200M parameters. All results are obtained through g f ac / h va l, following Ben Allal et al. [43]. Model Mamba-130m-hf Cerebras-GPT GPT-neo LaMini-GPT Opt GPT2 Pythia MobileLM SmolLM Hymba #Params. MMLU (cloze) ARC (c+e) PIQA Hella. OBQA Wino. Avg. 130M 111M 125M 124M 125M 137M 160M 125M 135M 125M 27.41 25.56 27.25 26.47 25.67 26.29 26.68 - 30.23 33.01 27.75 31.30 33.26 31.25 31.09 31.92 35.51 43.99 31.12 44.95 63.33 58.16 62.35 62.89 61.97 62.51 61.64 65.30 69. 68.50 33.86 26.32 29.68 30.05 31.04 29.76 29.55 38.90 42.30 45.54 30.40 25.40 29.20 27.80 29.00 29.40 27.80 39.50 33.60 35.52 51.54 50.28 51.54 50.75 53.20 49.72 49.49 53.10 52. 52.25 42.43 37.58 40.81 40.95 41.29 40.50 40.08 46.46 48.44 49.35 Table 7 Benchmark Hymba with SOTA tiny LMs, all of which have fewer than 400M parameters. All results are obtained through g f ac / h va l, following Ben Allal et al. [43]. Model Bloom Cerebras-GPT-256M Cerebras-GPT-590M Opt Pythia GPT2-medium MobileLM SmolLM Hymba #Params. MMLU (cloze) ARC (c+e) PIQA Hella. OBQA Wino. Avg. 560M 256M 590M 350M 410M 380M 350M 360M 350M 27.49 25.91 26.93 26.57 28.94 27.77 - 34.17 32.86 29.69 32.40 31.94 35.05 34.30 43.65 51.10 65.13 61.37 62.84 64.36 66.92 66.38 68.60 72.00 35.98 28.44 31.99 36.09 39.21 37.06 49.60 53.80 34.54 52. 72.91 55.08 28.80 28.00 28.40 27.80 28.40 31.20 40.00 37.20 38.40 51.70 51.62 50.12 52.57 52.80 49.49 57.60 53.70 42.89 39.82 41.15 42.55 44.48 43.69 51.89 53. 57.85 55.34 benchmarking and analysis, we adopt the hybrid-head module as our basic building block. The ratio of SSMs and attention in hybrid heads. To determine the proper number of attention heads, we start with Mamba model and gradually replace Mambas hidden dimensions with attention heads, maintaining the same overall model size. As shown in Tab. 10 (1) (4), we observe that model performance improves as the ratio of attention parameters increases and gradually saturates when the parameter ratio of attention to Mamba reaches 1:2.12. We stop introducing more attention heads, considering that adding more would bring increased memory overhead. There are two interesting observations: (1) Although the attention-only model outperforms the Mamba-only model, the hybrid model with both attention and Mamba heads achieves the best performance; (2) with further KV cache optimization, the In our ratio of attention heads decreases further. final model, attention heads occupy no more than 1/5 of the Mamba heads, yet significantly boost both recall and commonsense reasoning compared to the vanilla Mamba. This suggests that the hybrid model leverages the strengths and diversity of both attention and SSM heads, achieving better trade-off between efficiency and performance. The hybrid-head fusion strategy. We have explored two straightforward methods to fuse the outputs of attention and SSM heads: concatenation and mean. For concatenation, we combine the outputs of all heads and use linear layer to project the concatenated output to the final output dimension. However, the parameter size of the linear layer increases with both the number of heads and the head dimensions. Additionally, based on the empirical comparison between Tab. 10 (9) and (11), the performance of concatenation fusion is not better than the simple mean fusion. Therefore, we adopt the mean fusion strategy in our final design. Impact of KV cache optimization. After applying series of KV cache optimization techniques, 17 Hymba: Hybrid-head Architecture for Small Language Models Table 8 Benchmark Hymba-1.5B trained with all data and public data only against SOTA small LMs. All models have fewer than 2B parameters, except for Llama-3.2-3B, which is marked in gray. The settings follow Tab. 2 in our main paper and we only include the most competitive baselines here. Hymba (Public Data) refers to our model trained exclusively on public datasets, without using our proprietary high-quality dataset. Model #Params. Phi-1.5 h2o-danube2 Qwen2.5 SmolLM2 Llama-3.2-3B Hymba Hymba (Public Data) 1.3B 1.8B 1.5B 1.7B 3.0B 1.5B 1.5B Train tokens 0.15T 2T 18T 11T 9T 1.5T 1.5T Token/s 241 271 469 238 191 664 664 Cache MMLU ARC-E ARC-C PIQA Wino. Hella. SQuAD-C Avg. (MB) 5-shot 0-shot 1-shot 1573 492 229 1573 918 42.56 40.05 60.92 50.29 56.03 76.18 70.66 75.51 77.78 74.54 0-shot 0-shot 0-shot 0-shot 76.56 72.85 48.00 76.01 66.93 53.70 75.79 63.38 50.20 77.09 66.38 53.55 76.66 69.85 55. 44.71 33.19 41.21 44.71 42.32 30.09 49.03 49.53 50.50 43.46 79 79 51.19 44.31 76.94 78.58 45.90 77.31 66.61 53.55 47.01 77.53 64.56 53. 55.93 59.82 55.85 55.65 59.51 60.04 59.74 61.06 60.81 Table 9 Apple-to-apple comparison of our Hymba, pure Mamba [2], Mamba with FFN, Llama3 [39] style, and Samba- [7] style (Mamba-FFN-Attn-FFN) architectures. All models have 300M parameters and are trained for 100B tokens from FineWeb dataset [54] with exactly the same training recipes. All results are obtained through - va at - n [28]. The best and second best results are highlighted in bold and underline, respectively. Task Type Language Recall Intensive Commonsense Reasoning and Questionanswering Arch. Style (300M) Wiki. ppl. LMB. ppl. SQuAD-C SWDE Avg. Lambda PIQA ARC-C ARC-E Hella. Wino. TruthfulQA SIQA Avg. Mamba Mamba w/ FFN Llama3 Samba Hymba 30.78 19. 21.31 17.14 19.23 38.95 69.64 24.91 50.67 44.95 51.70 23.86 39.20 42.98 33.41 23.64 17.56 13.10 15.33 36.37 69.26 25.00 50.34 44.08 51.78 26.23 39.53 42.82 30.04 20. 22.10 57.86 39.98 40.15 70.29 24.83 50.24 45.69 52.64 28.97 39.66 44.08 31.41 19.75 39.88 22.14 31.01 40.59 69.86 25.76 49.79 46.45 52.49 27.27 39.92 44.02 28.53 15. 45.24 58.33 51.79 44.67 70.73 26.28 53.20 48.23 53.35 27.87 39.92 45.53 moving from Tab. 10 (5) to Tab. 10 (9), we observe that our Hymba maintains comparable recall and commonsense reasoning accuracy while being 2.74 faster. In contrast, applying the same KV cache optimization to pure Transformer, as seen in the comparison between Tab. 10 (6) and (10), results in recall accuracy drop of 10% or more and degraded commonsense reasoning accuracy. This supports our analysis in Sec. 2.2, showing that the presence of SSM heads in our hybrid-head module has already summarized the global context, allowing us to more aggressively replace global full attention with local attention in our hybrid model. Figure 12 Left: visualization of output magnitudes of attention and SSM heads. SSM heads consistently have higher output magnitude than attention heads due to their structure. Right: visualization of attention and SSM heads gate magnitudes. Through model learning, the relative magnitudes of attention and SSM gates vary across different layers. 18 Hymba: Hybrid-head Architecture for Small Language Models Table 10 Ablation study of the design choices of Hymba. The design finally adopted by Hymba is highlighted in bold. Specifically, the task lists are the same as those in Tab. 3. The throughput is measured with 8k sequence length and 128 batch size on an NVIDIA A100 GPU. The cache size is measured with 8k sequence length, assuming the FP16 format. Design Factor Configuration Param. Ratio Attn:Mamba Avg. (General) Avg. (Recall) Throughput (Token/s) Cache (MB) Attn/Mamba Ratio 1) Mamba Heads Only 2) Mamba + 4 Attn Heads 3) Mamba + 8 Attn Heads 4) Mamba + 16 Attn Heads 5) 4) + GQA 6) Attn Heads Only (Llama) Sliding Window Fusion Meta Tokens 5) + All SWAs 7) 8) 5) + SWAs + Full Attn 9) 8) + Cross-layer KV sharing 10) 6) + Same KV compression 11) 9) Replace Mean by Concat 12) 13) 1) + Meta Tokens 9) + Meta Tokens 0:1 1:8.48 1:4.24 1:2.12 1:3.64 1:0 1:3.64 1:3.64 1:5.23 1:0 1: 5.82 0:1 1:5. 42.98 44.20 44.95 45.08 45.19 44.08 44.42 44.56 45.16 43.60 44.56 44.01 45.53 19.23 44.65 52.53 56.46 49.90 39.98 29.78 48.79 48.04 28. 48.94 19.34 51.79 4720.8 3278.1 1816.5 656.6 876.7 721.1 4485.09 2399.7 2756.5 3710.0 1413.9 4712.8 2695. 1.87 99.09 197.39 394.00 148.24 414.72 5.51 41.19 39.42 28.98 39.42 1.87 40.01 tention or SSM heads in each layer by setting 𝛽1 or 𝛽2 in Eq. 3 to 0 and record the final accuracy. We consider four datasets, which are presented in Fig. 3 and Fig. 13, and the task performance is measured using 1000 samples from each task, evaluated with lm-evaluation-harness [28] in zero-shot setting. Observations. As shown in Fig. 13, we observe that (1) the relative importance of attention/SSM heads in the same layer, indicated by the change in task performance before and after being removed, may vary across different tasks. In other words, the relative importance of attention/SSM heads in the same layer is input-adaptive, indicating that different types of heads learn to serve different roles and undertake different responsibilities when handling various inputs; (2) The SSM head in the first layer is critical for language modeling and removing it causes substantial increase in PPL or substantial drop in accuracy (to random guess levels). Generally, removing one attention/SSM head results in 0.46%/1.2% reduction in accuracy averaged across all layers and tasks, respectively. D. Meta Tokens: More Analysis and"
        },
        {
            "title": "Visualization",
            "content": "Relationship with prior works. Learnable tokens have also been leveraged in previous transformerbased models. Previous prompt tuning works [32, 33] prepend learnable prompts while keeping the model weights frozen during the task-specific tuning stage, aiming to adapt pretrained LM to downstream tasks in parameter-efficient manner. [90] introduces both learnable tokens and corresponding memory update modules to augment the memory mechanism in transformers. [30] appends set of learnable tokens 19 Figure 13 Visualize the task performance difference across three tasks after removing the Attention or SSM heads in each layer. The task performance is measured using 1000 samples from each task. Note that removing critical modules in specific layers causes significant gap compared to others, making their bars fall outside the box. For such layers, we annotate the task performance with text. C. Head Importance Analysis Setup. To understand how hybrid heads contribute to the final task performance, we zero out the atHymba: Hybrid-head Architecture for Small Language Models Figure 14 Training curves of Hymba-1.5B. E. Pretraining and Post-training Implementation Details Figure 15 Visualize the layer-wise attention map entropy of (a) attention heads, and (b) SSM heads with and without meta tokens. called registers to the image patches of vision transformers [89] to store global information and improve visual recognition. Our method combines ideas from all of these works in more flexible manner. It optimizes the meta tokens jointly with model weights during the pretraining stage, is compatible with sliding window attention heads and other attention types or SSMs, and converts the meta tokens into KV-cache initialization during inference, without modifying the architecture. Meta tokens reduce attention map entropy. We visualize the entropy of the attention map for both the attention and SSM heads [20, 16] before and after introducing meta tokens. As introduced in Sec. 2.3 of our main paper, the attention map entropy reflects the distribution of attention scores across tokens, where lower entropy indicates stronger retrieval effects [7], as the attention scores are concentrated around smaller subset of tokens. As shown in Fig. 15, we observe that after introducing meta tokens, both the attention and SSM heads exhibit an overall reduction in entropy. Specifically, entropy is significantly reduced in all attention heads and in 10 out of 12 layers of the SSM heads. This suggests that meta tokens can reduce attention map entropy, potentially helping both the attention and SSM heads focus more on subset of important tokens that contribute most to task performance, as indicated by the boosted performance in Tab. 10. We settings. Pretraining train Hymba125M/350M/1.5B models on 1.3T tokens, using mix of DCLM-Baseline-1.0 [36], SmolLM-Corpus [37], and an internal high-quality dataset for 1T, 250B, and 50B tokens, respectively. We adopt the WSD learning rate scheduler [38] with three phases: (1) warmup steps set to 1% of the total steps, (2) stable phase maintaining the peak learning rate of 3e-3, and (3) decay phase reducing the learning rate to 1e-5 over 20% of the total steps, while gradually annealing to smaller, higher-quality datasets like SmolLM-Corpus and the internal dataset. We use sequence length of 2K and batch size of 2M tokens throughout the training process, which is conducted on 128 NVIDIA A100 GPUs. Details of Hymba-125M/350M/1.5B models are shown in Tab. 11. We also show the training curves of Hymba-1.5B in Fig. 14. Implementation details of post-training. We post-trained our 1.5B base model with two-stage strategy: the first full-finetuning (FFT) stage and another direct preference optimization (DPO) [12] training. The learning rates are 5e-5, and 3e-6 for FFT and DPO, respectively. Both FFT and DPO training are carried out for one epoch with cosine scheduler. The global batch size is set to 1024. To accelerate training, we follow the training recipe [61, 62, 63] to pack the samples and use block size of 2048. We implement the finetuning and DPO training with the LMFlow toolkit [62]. In addition to full-finetuning, we also leverage Dora [13] to do parameter-efficient finetuning. Baselines and downstream tasks. We compare Hymba-1.5B-Instruct with competitive lightweight instruction-tuned models, i.e., Llama3.2-1B-Instruct [42], OpenELM-1-1B-Instruct [51], 20 Hymba: Hybrid-head Architecture for Small Language Models Table 11 Architecture details of Hymba models of different size. Attribute 125M 350M 1.5B Blocks Hidden Size SSM State Attn. Heads Query Groups Num. Full Attn Window Size MLP Hidden Tie Embedding Parameters 32 32 24 1600 768 512 16 16 16 25 12 8 5 4 4 3 3 3 1024 1024 1024 5504 2432 1664 True True True 125M 350M 1.52B [64], Qwen2.5-1.5B-Instruct and SmolLM-1.7BInstruct [43]. We test the instruction-tuned models on MMLU (5-shot), IFEval, GSM8K (5-shot), GPQA (0shot), and Berkeley Function-Calling Leaderboard v2 (BFCLv2) [65]. For BFCLv2, we use the official code from Gorilla project [65] and evaluate the BFCLv2live category, including live_simple, live_multiple, live_parallel, live_parallel_multiple, live_relevance. We exclude live_irrelevance, since we found some baseline models without function calling abilities, could achieve high in the live_irrelevance category (where the model is not required to call function) and very low in other tasks, but still got high overall accuracy although these models are not helpful at all. For the remaining tasks, we directly use the lm-evaluation-harness [91]."
        }
    ],
    "affiliations": ["NVIDIA"]
}