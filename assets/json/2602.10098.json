{
    "paper_title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
    "authors": [
        "Jingwen Sun",
        "Wenyao Zhang",
        "Zekun Qi",
        "Shaojie Ren",
        "Zezhi Liu",
        "Hanxin Zhu",
        "Guangzhong Sun",
        "Xin Jin",
        "Zhibo Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \\emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 8 9 0 0 1 . 2 0 6 2 : r VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model Jingwen Sun1,2,, Wenyao Zhang3,5, Zekun Qi4, Shaojie Ren2,6, Zezhi Liu2,7, Hanxin Zhu1, Guangzhong Sun1, Xin Jin2,5,, Zhibo Chen1,2, 1University of Science and Technology of China 2Zhongguancun Academy, Beijing, China 3Shanghai Jiao Tong University 4Tsinghua University 5Eastern Institute of Technology, Ningbo 6University of Chinese Academy of Sciences 7Nankai University Equal Contribution, Corresponding author Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: target encoder produces latent representations from future frames, while the student pathway sees only the current observation future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields simple two-stage recipeJEPA pretraining followed by action-head fine-tuningwithout the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods. Date: February 11, 2026 Code: https://github.com/ginwind/VLA-JEPA/ Project Page: https://ginwind.github.io/VLA-JEPA/"
        },
        {
            "title": "1 Introduction",
            "content": "Learning visuomotor policies from internet-scale video has become an increasingly attractive route for robot learning [32, 31]. Compared to robot interaction data, which are costly and narrow in coverage [21, 44], unlabeled videos are abundant and diverse, offering rich demonstrations of temporally extended change. This has motivated growing body of latent-action pretraining for VisionLanguageAction (VLA) [10, 39, 9, 34]: rather than learning actions only from scarce control trajectories, they first learn representations and transition structure from video, then adapt them to downstream control [75, 13, 20]. However, we argue that todays latent action from video objectives often do not learn what we actually need for control. For an embodied agent, the most useful notion of action is not compact descriptor of pixel differences; it is variable that captures how the underlying state will evolve under interaction, i.e., action-relevant state transition semantics. When pretraining is misaligned with this goal, the downstream policy inherits representations that are temporally predictive yet weakly tied to controllable structureleading to brittle behavior, poor transfer, and inefficient fine-tuning. Why latent-action pretraining often drifts away from action semantics? We identify four failure modes that repeatedly appear in latent-action pretraining pipelines built on unlabeled video: 1. Pixel-level objectives bias representations toward appearance, not action. common strategy is to use the future as supervisioneither by predicting future pixels directly or by compressing frame-toframe changes into latent variable interpreted as an action [75, 13, 82]. Even with compression mechanisms such as VQ-VAE [67], the supervision signal is frequently dominated by what changes visually: texture, illumination, background clutter, and viewpoint. These factors are high-variance but low-control; they are easy to predict yet only weakly connected to the controllable degrees of freedom that policy must master. action-relevant transition structure, while preventing future information from leaking into the predictor. This aligns naturally with JEPA (Joint-Embedding Predictive Architectures) [4, 2, 1, 23], which replaces pixel reconstruction with latent-space alignment. By predicting representations rather than pixels, JEPA is inherently robust to low-level noise and encourages semantic abstraction [30]. Motivated by this perspective, we introduce VLAJEPA, JEPA-style pretraining framework tailored to VLA policies. As shown in Figure 1, the key design is leakage-free state prediction: during pretraining, target video encoder produces latent targets from future context (e.g., short clip), while the latent action pathway receives only the current observation through VLM backbone. predictor maps the history latent states and the latent action representations to future latent states, which is trained as latent world model using JEPA alignment loss. Crucially, future frames are never provided as inputs to the VLM backbone; instead, they are used solely to construct the training targets, eliminating the shortcut that causes latent-action collapse. This yields two benefits: (1) semantic robustness to camera motion and background changes, since supervision operates in latent space rather than pixel space; (2) streamlined two-stage pipelineJEPA pretraining followed by action-head fine-tuningwithout introducing auxiliary modules or redefining the learned representations. This work makes three contributions: Analysis of latent-action pretraining pitfalls. We analyze why many future-supervision objectives (including compressed frame-difference latent actions) remain pixel-tethered, becoming biased toward appearance, vulnerable to nuisance motion in real-world videos, and prone to information leakage when future context enters the learner. VLA-JEPA: leakage-free, state-level JEPA pretraining for VLA. We propose JEPA-style latent predictive alignment scheme that learns action-relevant transition semantics by predicting and aligning future latent stateswithout pixel reconstruction, information leakage and only one-stage pretraining pipeline. Improved and robustness with simpler workflow. Across embodied control benchmarks (LIBERO, LIBERO-Plus, SimplerEnv) and real-world settings, VLA-JEPA yields consistent gains in robustness, and generalization, while simplifying training relative to prior multi-stage latent-action pipelines. Figure 1 VLA-JEPA model architecture 2. Real-world videos amplify noisy motion. This mismatch becomes especially pronounced on human videos and in-the-wild footage [32, 33], where camera motion and non-causal background changes can be stronger than interaction-induced state changes. Frame-difference-based latent-action objectives are then incentivized to encode these dominant signals, turning the latent action into deltaframe encoder of nuisance motion rather than representation of meaningful transition dynamics. 3. Information leakage makes latent action collapse into shortcut. Several latent-action pipelines model transitions by feeding both the current observation and future observation into the same module, or by allowing future context to influence the learned action variable during training [75, 13]. This design creates an easy shortcut: the latent action can simply encode the future itself instead of capturing how state transitions should be explained [78]. The resulting action becomes semantically emptyuseful for matching training losses, but not meaningful factor for control. 4. Multi-stage training pipelines are complex and fragile. To stabilize training and mitigate the above issues, many approaches rely on three-stage (or more) procedures: representation pretraining, latent-action learning/alignment, then policy learning [75, 19, 27]. These pipelines increase engineering complexity, introduce stage-wise inconsistencies, and make it harder to train and evaluate methods cleanly. Together, these issues point to single underlying problem: many latent-action objectives remain implicitly anchored to pixel variation, and thus learn dynamics that are predictive but not necessarily actioncentric [78]. For embodied control, we want valuebearing latent state that discards nuisance appearance while preserving factors governing state evolution under interaction [33, 58, 70, 36]. This motivates key principle: predict future latent states that reflect"
        },
        {
            "title": "2 Related Works",
            "content": "age and pixel shortcuts while enabling single-stage end-to-end pretraining."
        },
        {
            "title": "2.1 Vision-Language-Action Models",
            "content": "With the rapid advancement of Large Language Models (LLMs) [48, 37, 6, 24, 73, 60] and large-scale robot datasets [55, 25, 38, 22], Vision-LanguageAction (VLA) models have become dominant paradigm in robot learning [39, 9, 72]. The RT series [10, 88, 5] pioneered fine-tuning multimodal LLMs on robot demonstrations, and subsequent works further improved manipulation and navigation performance [39, 9, 62, 43, 81, 80, 59, 71]. However, most VLA methods rely heavily on large-scale action-labeled robot data, which is costly and difficult to scale [68]. To reduce dependence on explicit action supervision, recent works introduce multimodal Chain-of-Thought signals, such as hierarchical planning [5, 45, 61, 87, 35], subgoal or rollout prediction [65, 83, 84, 69, 46, 17, 76, 50], object-centric conditioning [22, 34, 63, 86, 79, 52, 15] and latent future embeddings or actions [12, 53, 82, 51, 85]. Nevertheless, these approaches suffer from relying on action-labeled data. In contrast, VLA-JEPA learns action-centric representations via latent predictive alignment, avoiding explicit future reconstruction and reducing the need for large-scale action supervision."
        },
        {
            "title": "2.2 Latent action learning for robotics",
            "content": "To leverage large-scale videos without action labels, ILPO [26], LAPO [64] and Genie [11] propose using latent action for video games. For robot learning, LAPA [75], IGOR [18], UniVLA [13], MotoGPT [20], Adaworld [29], CoMo [74] and StaMo [49] extract discrete or continuous motion tokens from frame transitions, and pretrain VLA to predict these latent actions before mapping them to real robot controls. To align the latent action to the real action space, villa-x [19], XR-1 [27], CLAP [77] and VITA [54] propose that extract latent action from both robot and human video and use unified codebook. However, since latent actions are often learned directly from adjacent frames, models may exploit pixel-level shortcuts and encode future-frame leakage. Although LAOF [14] and Motus [7] propose to use optical flow to constrain the space of latent action, it still entangles controllable dynamics with camera motion and background changes. As result, learned latent spaces may align more with visual deltas than with actionable control signals, requiring multi-stage training pipelines and additional alignment mechanisms [78]. On the contrary, our proposed VLAJEPA learns action-relevant representations without relying on delta frame info extraction, avoiding leak-"
        },
        {
            "title": "3 Methodology",
            "content": "To address the limitations of existing approaches, we introduce VLA-JEPA, unified framework that enables joint pre-training on both action-free and actionlabeled data. For action-free human videos, VLAJEPA extracts latent actions from vision-language prior representations by optimizing world-modelbased state transition objective. Building upon this foundation, we further integrate flow-matchingbased action generator for robot demonstrations to support precise end-effector trajectory generation. During fine-tuning, VLA-JEPA enables end-to-end fusion of the two objectives, allowing the learned state-transition dynamics to be effectively leveraged for downstream robotic control."
        },
        {
            "title": "3.1 Model Backbone",
            "content": "As shown in Figure 2, we adopt Qwen3-VL [3] as the core large vision-language model (VLM) in our framework. This VLM is built upon the Qwen3 [73] and uses SigLIP-2 [66] as its vision encoder. It is well established that the world knowledge acquired by VLMs during large-scale pretraining, including image understanding and key object detection, can be transferred to robot control tasks. To distill continuous latent action representations from the VLM, which encode state transition information for world modeling, we introduce set of learnable tokens that denoted as latenti and action, where denotes the time step. For instance, latent0 represents the state transition between s0 and s1. To model state transitions in continuous videos, we encode pixel-level human videos into time-stepaware feature sequences using an encoder architecture trained with V-JEPA2 [2], and employ time-causal attention mechanism to capture the correlations between the feature sequences and the latent actions."
        },
        {
            "title": "3.2 Learning from Human Videos",
            "content": "To enable VLM to learn from human demonstration videos, we design training framework that explicitly injects environment dynamics into the latent action tokens. Specifically, we consider human video dataset = {(O0, O1, ..., Ov, ℓ)}, where ℓ denotes the associated language instruction, and each Ov represents video captured from viewpoint v. Formally, Ov = (Iv,t0 , Iv,t1, ..., Iv,tn), where Iv,ti denotes the video frame at time step ti from view v, and is the Figure 2 VLA-JEPA supports cross-domain training on both human videos and robot data, where human videos are trained using an alignment loss under the latent world modeling objective, while robot data are trained with joint objective consisting of an alignment loss and robot action prediction loss. total number of frames within video. World State Encoder. Unlike traditional single-view video representation approaches, we encode diverse observations from the same episode into unified world state representation through world state encoder. Specifically, we adopt self-supervised VJEPA2 encoder as the single-view video state representation and integrate representations from multiple viewpoints via concatenation operator. The encoding process for each view and the subsequent aggregation across views are formulated as follows: sti = vF (Iv,ti) (1) where () is the single-view video encoder (e.g., V-JEPA2), and denotes the vector concatenation operator. The resulting sti is the unified world-state representation at timestamp ti. Latent Action Pretraining through World Modeling. To encourage the learnable latent action tokens to capture state transition dynamics, we introduce world state prediction objective based on an auto-regressive transformer-based world model. Formally, the VLM takes as input the multi-view observations at the initial time step t0, together with the language instruction ℓ. Conditioned on these inputs, the VLM maps set of special learnable tokens latenti into latent representations that summarize the underlying world dynamics: zti = pV LM θ (cid:0)latenti (cid:12) (cid:12) {Ij,t0 }v j=0, ℓ(cid:1) , (2) denotes the latent representation associated where zti with the i-th latent action token at time step ti. Subsequently, the unified representation zti is used to condition the world model, providing additional context for state prediction. Formally, given the sequence and the corresponding of encoded world states st0:i conditioning variables zt0:i , the world model predicts the next chunk of states as: ˆst1:i+1 = pW θ (st0:i, zt0:i), (3) ˆst1:i+1 is the predicted world state chunk over where [t1, ti+1]. In practice, each special latent token latenti is replicated times in the input sequence to enable variable-length latent action encoding, where is tunable hyperparameter. The world model employs time-causal attention mechanism. Within each time step, all latent action tokens and world state tokens attend to each other via bidirectional full attention. Across different time steps, attention is strictly causal: tokens at time step are allowed to attend only to tokens from time steps up to and including t, while attention to future time steps is masked out. From the perspective of joint-embedding predictive architecture [4, 2], our training objective can be interpreted as maximizing the evidence lower bound (ELBO) of the predictive log-likelihood in semantic space. Specifically, given the frozen V-JEPA2 encoder () that produces target world states sti , and the world model pW that predicts ˆsti , the objective can be written as: conditioned on zti θ log p(st1:T zt0:T 1) (cid:88) Estk () (cid:2) log pθ(ˆstk stk )(cid:3) k=1 DKL (cid:2)F ()pW θ (cid:3), (4) where () serves as the frozen target encoder (with stop-gradient), and pW is the online predictor. In practice, since () produces deterministic embeddings, the KL term vanishes, and the ELBO reduces to reconstruction loss in the latent space. θ Finally, we optimize the combined WM and VLM using teacher-forcing objective. This enables the unified representation zti informed by the world knowledge encoded in the VLM, in order to effectively characterize world state transitions. The world modeling loss is defined as: LWM = (cid:88) k=1 Estk ()(ˆstk stk ), (5) where stk time tk, ˆstk the video prediction horizon. denotes the ground-truth world state at is the corresponding prediction, and is"
        },
        {
            "title": "3.3 Action Prediction with Joint Optimiza-",
            "content": "tion Objectives Action Token Conditioning. To leverage the latent action representations learned from video data for guiding action prediction, we design joint optimization objectives. Specifically, for multi-view RGB videos in the robot dataset, we adopt the same training objective as in Equation 5 to fine-tune the latent action representations within the robot data domain. For practical action prediction, we intend the latent action to serve as conditioning signal for embodied action generation, analogous to the roles of the initial image observation Iv,t0 and the language instruction. Therefore, we append set of learnable embodied action tokens action after the latent action tokens. Leveraging the causal attention mechanism of the VLM, the model captures the dependencies among action, the latent action tokens, the initial visual observation, and the language instruction. Formally, given the visual tokens {Ii,t0 }v at the initial time step t0, the language instruction l, and sequence of latent action tokens latenti, we obtain global action-conditioning representation i=0 za = pV LM θ (action {Ii,t0}v i=0, ℓ, latenti) , (6) where za serves as an additional conditioning signal for the flow-matching-based action head. Conditional Flow-Matching Action Head. We adopt conditional flow matching to model distribution over continuous action trajectories. Specifically, let a0:H denote the ground-truth action sequence over horizon H, and ˆa0:H denote the predicted action sequence generated by the learned flow. Following standard flow-matching formulations, we define time-dependent interpolation at = (1 t) ϵ + a0:H , U(0, 1), (7) where ϵ (0, I) is Gaussian noise. The action head parameterizes vector field vθ(at, za), conditioned on za, which is trained to match the ground-truth conditional flow. The flow-matching objective is given by LFM = Ea0:H , ϵ, (cid:104)(cid:13) (cid:13)vθ(at, za) (a0:H ϵ)(cid:13) 2 (cid:13) 2 (cid:105) , (8) where vθ() denotes the predicted velocity field, (a0:H ϵ) is the target velocity induced by the linear interpolation, and 2 is the ℓ2 norm. At inference time, the learned vector field is integrated from noise to data space to obtain the predicted action trajectory ˆa0:H , conditioned on the action tokens za. In summary, the overall training objective for actionlabeled robot data is as follows: = LFM + βLWM, (9) where β is tunable hyperparameter."
        },
        {
            "title": "4 Experiments",
            "content": "To evaluate the generalization and robustness of VLA-JEPA, we perform comprehensive simulation experiments utilizing two distinct simulation environments and three public benchmarks, Simpler-Env & LIBERO, along with real-world experiments covering both in-distribution and out-of-distribution settings, as shown in Figure 3. We compare VLA-JEPA against the latest VLA baselines that support pretraining with human videos as well as VLA baselines pretrained solely on robot-collected data. Figure 3 Experiments setup on LIBERO, LIBERO-Plus, SimplerEnv and real-world Franka robot. We evaluate VLA-JEPA on 3 simulation benchmarks and 1 real-world environment. Table 1 Comparison on the LIBERO benchmark. We report the task success rate for each suite and the average across all tasks. Each task is evaluated over 50 episodes (500 episodes per suite), and success rates are computed over these trials. Bold denotes the best performance, and italics denotes the second best. Method LAPA [75] UniVLA [13] OpenVLA-OFT [40] π0 [9] π0-Fast [57] CoT-VLA [83] WorldVLA [16] villa-X [19] GR00T N1 [8] π0.5 [34] VLA-JEPA w/o human videos LIBERO Spatial Object Goal 73.8 96.5 97.6 96.8 96.4 87.5 87.6 97.5 94.4 98. 96.2 94.8 74.6 96.8 98.4 98.8 96.8 91.6 96.2 97.0 97.6 98.2 99.6 99.6 58.8 95.6 97.9 95.8 88.6 87.6 83.4 91.5 93.0 98.0 97.2 95.8 55.4 92.0 94.5 85.2 60.2 69.0 60.0 74.5 90.6 92.4 95.8 94.0 Avg 65.7 95.2 97.1 94.2 85.5 81.1 81.8 90.1 93.9 96.9 97.2 96."
        },
        {
            "title": "4.1 Implementation Details",
            "content": "During the pretraining phase, we train all parameters in the model except for the world state encoder. Specifically, we utilize Equation 5 as the training objective and pretrain on the large-scale human action dataset Something-Something-v2 [31], which contains 220K human videos. The model supports simultaneous pretraining of latent action using both robot action data and human videos without action labels. We also employ Equation 9 as the training objective and pretrain on the large-scale action-labeled robot dataset Droid [38], which consists of 76K high-quality demonstration trajectories. During fine-tuning, for both LIBERO and LIBEROPlus, we use the LIBERO dataset, which includes approximately 2K expert demonstrations collected in simulation environment, without incorporating the augmented dataset from LIBERO-Plus. For SimplerEnv, we utilize the Fractal dataset and the BridgeV2 dataset for post-training, corresponding to the two robot embodiment types in SimplerEnv. For the real-world experiments, we employ dataset of 100 demonstrations collected across three distinct tasks for post-training. All experiments are conducted on 8 NVIDIA A100 GPUs. Detailed implementation and protocols are provided in Appendix A."
        },
        {
            "title": "4.2 Simulation Setup and Baseline\nBenchmarks. We conduct generalization experiments\non the LIBERO [47] and SimplerEnv [42] benchmarks.\nThe LIBERO benchmark employs the Franka Emika\nPanda arm and comprises four task suites designed to\nfacilitate research on lifelong learning in robotic ma-\nnipulation. SimplerEnv features WidowX and Google\nRobot setups, offering diverse manipulation scenar-\nios with varying lighting, colors, textures, and robot\ncamera poses, thereby bridging the visual appear-\nance gap between real and simulated environments.\nFurthermore, we conduct robustness experiments on\nLIBERO-Plus [28], a large-scale benchmark designed\nto systematically stress-test VLAs through perturbed\ntasks across seven dimensions.",
            "content": "The three simulation environments correspond to (i) in-distribution scenarios in which policies, trained using simulated expert data, are validated on indistribution tasks in simulation (LIBERO), (ii) outof-distribution scenarios involving real-to-sim gap, in which policies are trained using real-world data and evaluated in simulation (SimplerEnv), and (iii) out-of-distribution scenarios in which policies, trained using simulated expert data, are validated on out-ofdistribution tasks in simulation (LIBERO-Plus). Baselines. We primarily compare VLA-JEPA with previous latent-action VLAs, future-prediction VLAs, as well as state-of-the-art open-source VLAs including Moto [20], LAPA [75], UniVLA [13], villa-X [19], CoTVLA [83], WorldVLA [16], RoboVLMs [41], GR00T N1 [8], OpenVLA-OFT [40], π0 [9], π0-Fast [57], and π0.5 [34]. other top-performing models on LIBERO, such as OpenVLA-OFT and π0.5, rely on extensive robot datasets for pre-training. In contrast, achieves better performance using less training data. Furthermore, previous latent-action-based VLAs and VLAs trained on human videos, such as UniVLA, villa-X, LAPA, and CoT-VLA, consistently underperform VLA-JEPA, which corroborates the limitations of such approaches discussed in Section 1. SimplerEnv. Table 2 reports the results on SimplerEnv. VLA-JEPA achieves the best performance on 2 out of 4 tasks for both the Google Robot and the WidowX Robot. In particular, it attains the highest average success rate on the Google Robot and the second highest average success rate on the WidowX Robot. Since SimplerEnv does not provide expert demonstrations, we observe that the quality of robot data plays crucial role in performance on this benchmark. For example, LAPA extracts only successful rollouts in SimplerEnv and uses them as expert demonstrations for training, and achieves the second-highest success rate on the WidowX Robot with merely 100 rollouts. This is mainly because training on successful rollouts effectively mitigates the real-to-sim gap. Villa-X, on the other hand, is trained on large-scale collection of robot data and human videos, achieving the highest average success rate on the WidowX Robot. In comparison, while VLA-JEPA and other methodssuch as UniVLA, RoboVLMs, and Motoall utilize less than 1% of the training data used by villa-X, VLA-JEPA attains the most competitive experimental results. This demonstrates the superiority of our pretraining method. LIBERO-Plus. As shown in Table 3, VLA-JEPA achieves the best performance on 5 out of 7 perturbations in LIBERO-Plus. It demonstrates substantial advantage over UniVLA, which is trained on human videos, and OpenVLA-OFT, π0, and other methods trained on large amounts of robot data. This indicates that latent actions, learned through our pretraining approach, possess level of world knowledge representation comparable to that encoded by text data. In particular, we find that VLA-JEPA demonstrates significant advantage over all baselines under perturbations in Language, Light, Background, and Layout, which verifies that our latent action can effectively handle task-agnostic disturbances, thereby achieving more robust and generalized policy."
        },
        {
            "title": "4.4 Real-world Experiments",
            "content": "For real-world experiments, we designed table-top manipulation tasks using Franka Research 3 arm equipped with Robotiq 2F-85 gripper. We collected Table 2 Comparison on the SimplerEnv benchmark. We report the average success rate for each individual task under the visual matching setting in both SimplerEnv-WidowX Robot and SimplerEnv-Google Robot. denotes that the method is trained on in-distribution expert demonstrations collected in simulation environment. All other methods are trained on subsets of the OXE dataset. Method Google Robot WidowX Robot Pick Move Drawer Place Avg Spoon Carrot Block Eggplant Avg LAPA [75] villa-x [19] UniVLA [13] RoboVLMs [41] GR00T N1 [8] MoTo [20] OpenVLA-OFT [40] π0 [9] π0-Fast [57] VLA-JEPA w/o human videos - 81.7 - 77.3 0.7 74.0 - 72.7 75.3 88.3 85. - 55.4 - 61.7 1.9 60.4 - 65.3 67.5 64.1 66.7 - 38.4 - 43.5 2.9 43.1 - 38.3 42.9 59.3 75.5 - 4.2 - 24.1 0.0 - - - - 49.1 86. - 44.9 - 51.7 1.4 - - - - 65.2 78.4 70.8 48.3 - 45.8 1.4 - 34.2 29.1 29.1 75.0 75.0 45.8 24.2 - 20.8 0.0 - 30.0 0 21.9 70.8 54. 54.2 19.2 - 4.2 0.0 - 30.0 16.6 10.8 12.5 20.8 58.3 71.7 - 79.2 13.9 - 72.5 62.5 66.7 70.8 79.2 57.3 40.8 42.7 37.5 3.8 - 41.8 40.1 48.3 57.3 57. Table 3 Comparison on the LIBERO-Plus benchmark. We report the average success rate across each perturbation dimension, where each perturbation includes the four task suites from the original LIBERO benchmark. Method LIBERO-Plus Camera Robot Language Light Background Noise Layout Avg UniVLA [13] OpenVLA-OFT [40] π0 [9] π0-Fast [57] WorldVLA [16] VLA-JEPA w/o human videos 1.8 56.4 13.8 65.1 0.1 63.3 40. 46.2 31.9 6.0 21.6 27.9 67.1 55.7 69.6 79.5 58.8 61.0 41.6 85.4 72.9 69.0 88.7 85.0 73.2 43.7 95.6 88. 81.0 93.3 81.4 73.2 17.1 93.6 70.5 21.2 75.8 79.0 74.4 10.9 66.3 38.2 31.9 74.2 68.9 68.8 38.0 85.1 74. 42.9 69.6 53.6 61.6 25.0 79.5 62.9 100 human demonstration trajectories for training, which include 3 picking and placing tasks. In alignment with the simulation experiments and extending beyond in-distribution task evaluation, we introduce two OOD experimental protocols to rigorously assess the models generalization capability and robustness for physical deployment. The first OOD protocol involves executing tasks not present in the training data, validating the models ability to acquire and transfer fundamental skills. The second protocol requires executing tasks seen during training but with randomized object layouts, thereby emulating the cluttered scenarios typical of real-world table-top manipulation tasks. For fair comparison, we finetune π0 and π0.5 on collected demonstration datasets. The detailed task settings can be found in Appendix B. As shown in Figure 4, VLA-JEPA achieves stateof-the-art performance under both the ID and the object layout OOD settings, demonstrating clear advantage over both π0 and π0.5. In the task OOD Figure 4 Real World Experimental Results setting, it attains the second-best result. During real-world deployment, we observed that the generalization capability of VLA-JEPA is less robust than that of π0.5, yet it produces execution trajectories that are significantly more stable and reliable. Specifically, π0.5 more accurately follows instructions to contact the target object compared to VLA-JEPA. However, its position control frequently violates the safety boundaries of the robot arm, leading to execution failures. In contrast, VLA-JEPA, due to its lack of fine-grained reasoning over textual instructions, is prone to grasping objects that do not align with the command. Nevertheless, it rarely breaches the robot arms safety constraints. In addition, VLA-JEPA acquires the skill of repeated grasping, i.e., reopening the gripper to attempt another grasp after failure, which is not observed in π0 or π0.5. We attribute this to the abundance of repeated-grasping knowledge present in human videos, whereas this skill does not require additional physical dynamics knowledge. In contrast, robot data rarely contain demonstrations specifically designed for repeated grasping. corresponding demonstration can be found in the Appendix B."
        },
        {
            "title": "4.5 Further Analysis and Ablation Study",
            "content": "In this section, we investigate the following questions under the multi-view setting to thoroughly evaluate the ability of our model: Q1: What impact does human video have? Tables 1, 2, 3 present comparison of VLA-JEPA pretrained with human videos versus without such pretraining, demonstrating that ablating the human video component from the pre-training data does not lead to significant performance drop across LIBERO and SimplerEnv benchmarks. Interestingly, even pretraining without human videos can lead to higher success rates on SimplerEnv. This indicates that, for ID scenario and real-to-sim gap OOD scenario, high-quality expert demonstrations are more critical for ensuring model performance compared to human videos. On the LIBERO-Plus benchmark, human demonstration videos provide substantial performance gains. Our analysis suggests that this is because human videos lack effective information about action trajectories, preventing VLA models from directly learning the physical dynamics of robotic actions. Instead, the primary benefit lies in enhancing the robustness and stability of the models pre-existing skills, such as repeated grasping. This process closely resembles human skill acquisition from observation: when person learns skill by watching video, the first attempt is unlikely to succeed, and proficiency is achieved only after repeated trials that establish the correspondence between the observed video knowledge and the underlying physical dynamics, ultimately leading to stable and reliable policy. Figure 5 illustrates how the success rates across different perturbation dimensions on the LIBERO-Plus benchmark vary as the proportion of human videos in Figure 5 Effect of the proportion of human video data in pre-training on success rates across different perturbation dimensions on the LIBERO-Plus benchmark. the pre-training data increases. These results further corroborate our hypothesis that human video data primarily enhances the robustness and stability of the VLA model by strengthening its existing skill repertoire, rather than introducing new action execution capabilities. Moreover, as the scale of human video data increases, the robustness of the resulting policy consistently improves. Q2: What is the impact of unified pretraining? According to Section 4.3, the experimental results demonstrate that our unified pretraining method consistently outperforms the previous two-stage pretraining paradigm. In addition, to more clearly analyze the information captured by latent action tokens, we visualize the attention weights from latent action tokens to image tokens in the internal attention maps of three VLAsLAPA, UniVLA, and VLA-JEPAunder simulated, human-video, and real-world robot image inputs. For fair comparison, all methods are evaluated using pretrained-only checkpoints, without any fine-tuning on either simulated or real data. As shown in Figure 6, the latent actions of LAPA focus on excessively dense visual information, which results in the inclusion of too many operation-irrelevant details, such as unrelated objects on the desktop. We attribute this to the leakage of information during the pretraining stage, leading to the degradation of latent actions into compressed representations of the target images, point further analyzed in Section 1. In contrast, UniVLA alleviates this issue through task-relevant textual guidance, yet its overemphasis on semantics causes attention to operation-irrelevant background elements in the images, such as the stationary pen in human videos or the texture of the tablecloth in real-world wrist-view recordings. By comparison, we focus more precisely on the operation, for instance, the robotic arm, the hand, and the Figure 6 Visualization of the attention weight matrix of latent action tokens attending to image tokens. objects to be manipulated. This demonstrates that the unified pretraining approach simplifies the training pipeline while improving training effectiveness by reducing the impact of task-irrelevant information. Q3: How does the number of future video horizon affect performance? We aim for latent actions to capture the motion dynamics between consecutive frames, such that the number of latent action tokens is always equal to the number of frames minus one. We vary the number of future video horizon {4, 8, 16} to investigate the impact of dynamic information on the performance. For fair comparison, all models are directly fine-tuned on the LIBERO dataset, with all other hyperparameters kept the same in the comparison. Table 4 Comparison of VLA-JEPA on the LIBERO benchmark with different numbers of future video horizon. Spatial Object Goal 10 4 8 95.0 94.8 92.8 99.2 99.8 98.8 95.8 95.8 98.0 89.0 94.0 92.2 Avg 94.8 96.1 95. As shown in Table 4, the model achieves its best when the video horizon is close to the predefined action horizon, which indicates that latent actions are effective for embodied action generation. When is too small, the encoded information is insufficient, leading to inferior performance, particularly on longhorizon tasks. In contrast, an excessively large introduces redundant information. It yields the best results on the goal-oriented task suite with relatively simple objectives, but performs worst on the spatial task suite that requires fine-grained manipulation."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper proposes VLA-JEPA, unified pretraining framework that learns latent actions from both human and robot videos via latent world modeling. Extensive experiments and analyses reveal that existing latent action pretraining methods suffer from information leakage and representation degeneration, which hinder the learning of meaningful temporal dynamics. In contrast, VLA-JEPA effectively mitigates these issues and enables the model to capture genuine inter-frame dynamics. As result, VLA-JEPA achieves competitive performance in both simulation benchmarks and real-world robotic experiments. We further believe that the human-video pretraining paradigm introduced by VLA-JEPA is highly scalable, and can be naturally extended by incorporating robot data and text-based reasoning data, thereby further improving the generalization and robustness of VLA models."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is supported by the Zhongguancun Academy, (Grant No.s C20250302)"
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture, 2023. [2] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [3] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. CoRR, abs/2511.21631, 2025. [4] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. 2023. [5] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rth: Action hierarchies using language. arXiv preprint, 2024. [6] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint, 2024. [7] Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, et al. Motus: unified latent action world model. arXiv preprint arXiv:2512.13030, 2025. [8] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint, 2025. [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi0: vision-language-action flow model for general robot control. arXiv preprint, 2024. [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: robotics transformer for real-world control at scale. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. [11] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [12] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint, 2025. [13] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. arXiv preprint, 2025. [14] Xizhou Bu, Jiexi Lyu, Fulei Sun, Ruichen Yang, Zhiqiang Ma, and Wei Li. Laof: Robust latent action learning with optical flow constraints. arXiv preprint arXiv:2511.16407, 2025. [15] Junhao Cai, Zetao Cai, Jiafei Cao, Yilun Chen, Zeyu He, Lei Jiang, Hang Li, Hengjie Li, Yang Li, Yufei Liu, et al. Internvla-a1: Unifying understanding, generation and action for robotic manipulation. arXiv preprint arXiv:2601.02456, 2026. [16] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint, 2025. [17] Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao, Feilong Tang, Donglin Wang, and Haoang Li. Unified diffusion vla: Vision-languageaction model via joint discrete denoising diffusion process. arXiv preprint arXiv:2511.01718, 2025. [18] Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, and Jiang Bian. Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. arXiv preprint arXiv:2411.00785, 2024. [19] Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, et al. villa-x: Enhancing latent action modeling in vision-languageaction models. arXiv preprint, 2025. [20] Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, and Xihui Liu. Moto: Latent motion token as the bridging language for robot manipulation. arXiv preprint, 2024. [21] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: Inthe-wild robot teaching without in-the-wild robots. In Proceedings of Robotics: Science and Systems (RSS), 2024. [22] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, et al. Graspvla: grasping foundation model pre-trained on billionscale synthetic action data. arXiv preprint, 2025. [23] Matthieu Destrade, Oumayma Bounou, Quentin Le Lidec, Jean Ponce, and Yann LeCun. Value-guided action planning with jepa world models, 2025. [24] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024. [25] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint, 2021. [26] Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent policies from observation. In International conference on machine learning, pages 17551763. PMLR, 2019. [27] Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, et al. Xr-1: Towards versatile vision-language-action models via learning unified vision-motion representations. arXiv preprint arXiv:2511.02776, 2025. [28] Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, and Xipeng Qiu. Libero-plus: In-depth robustness analysis of vision-language-action models. arXiv preprint arXiv:2510.13626, 2025. [29] Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, and Chuang Gan. Adaworld: Learning adaptable world models with latent actions. In Forty-second International Conference on Machine Learning. [30] Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, and Michael Rabbat. Learning latent action world models in the wild. arXiv preprint arXiv:2601.05230, 2026. [31] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In ICCV, 2017. [32] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. [33] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint, 2024. [34] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi0.5: vision-language-action model with open-world generalization. arXiv preprint, 2025. [35] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, and Shanghang Zhang. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 17241734. Computer Vision Foundation / IEEE, 2025. [36] Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, and Shanghang Zhang. Video2act: dualsystem video diffusion policy with robotic spatiomotional modeling, 2025. [37] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint, 2024. [38] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint, 2024. [39] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-languageaction model. arXiv preprint, 2024. [40] Moo Jin Kim, Chelsea Finn, and Percy Liang. Finetuning vision-language-action models: Optimizing speed and success. arXiv preprint, 2025. [41] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-languageaction models. arXiv preprint, 2024. [42] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, CoRL, 2024. [43] Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, and Ping Luo. Discrete diffusion vla: Bringing discrete diffusion to action decoding in vision-language-action policies. arXiv preprint, 2025. [44] Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint, 2024. [45] Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. Onetwovla: unified vision-language-action model with adaptive reasoning. arXiv preprint, 2025. [46] Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, and Donglin Wang. Hif-vla: Hindsight, insight and foresight through motion representation for vision-language-action models. arXiv preprint arXiv:2512.09928, 2025. [47] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: benchmarking knowledge transfer for lifelong robot learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, NeurIPS, 2023. [48] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. [49] Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, and Chunhua Shen. Stamo: Unsupervised learning of generalizable robot motion from compact state representation. arXiv preprint arXiv:2510.05057, 2025. [50] Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, et al. Mla: multisensory language-action model for multimodal understanding and forecasting in robotic manipulation. arXiv preprint arXiv:2509.26642, 2025. [51] Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, Chenyang Gu, Jiale Yu, Xiangju Mi, Renrui Zhang, Zhengping Che, et al. Last _{0}: Latent spatio-temporal chain-of-thought for robotic vision-language-action model. arXiv preprint arXiv:2601.05248, 2026. [52] Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. F1: vision-language-action model bridging understanding and generation to actions. arXiv preprint arXiv:2509.06951, 2025. [53] Jiangran Lyu, Ziming Li, Xuesong Shi, Chaoyi Xu, Yizhou Wang, and He Wang. Dywa: Dynamicsadaptive world action model for generalizable nonprehensile manipulation. arXiv preprint, 2025. [54] Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, and Sanglu Lu. Unifying perception and action: hybrid-modality pipeline with implicit visual chain-of-thought for robotic action generation. arXiv preprint arXiv:2511.19859, 2025. [55] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint, 2023. [56] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [57] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint, 2025. [58] Video Generation Pretraining. Disentangled robot learning via separate for-ward and inverse dynamics pretraining. . [59] Video Generation Pretraining. Disentangled robot learning via separate for-ward and inverse dynamics pretraining. . [60] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In ECCV, 2024. [61] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, et al. Sofar: Languagegrounded orientation bridges spatial reasoning and object manipulation. arXiv preprint, 2025. [62] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint, 2025. [63] Kanchana Ranasinghe, Xiang Li, Cristina Mata, Jongwoo Park, and Michael Ryoo. Pixel motion as universal representation for robot control. arXiv preprint, 2025. [64] Dominik Schmidt and Minqi Jiang. Learning to act without actions. In The Twelfth International Conference on Learning Representations. [65] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. ICLR, 2024. [66] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [67] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017. [68] Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, et al. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025. [69] Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-languageaction model. arXiv preprint, 2025. [70] Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. NeurIPS, 2024. [71] Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, et al. pragmatic vla foundation model. arXiv preprint arXiv:2601.18692, 2026. [72] Kechun Xu, Zhenjie Zhu, Anzhe Chen, Shuqi Zhao, Qing Huang, Yifei Yang, Haojian Lu, Rong Xiong, Masayoshi Tomizuka, and Yue Wang. Seeing to act, prompting to specify: bayesian factorization of vision language action policy. arXiv preprint arXiv:2512.11218, 2025. [73] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [74] Jiange Yang, Yansong Shi, Haoyi Zhu, Mingyu Liu, Kaijing Ma, Yating Wang, Gangshan Wu, Tong He, and Limin Wang. Como: Learning continuous latent motion from internet videos for scalable robot learning. arXiv preprint, 2025. [75] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint, 2024. [76] Bozhou Zhang, Nan Song, Jingyu Li, Xiatian Zhu, Jiankang Deng, and Li Zhang. Future-aware endto-end driving: Bidirectional modeling of trajectory planning and scene evolution. arXiv preprint arXiv:2510.11092, 2025. [77] Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, Cai Zhou, Jiwen Lu, and Yansong Tang. Clap: Contrastive latent action pretraining for learning vision-language-action models from human videos, 2026. [78] Chuheng Zhang, Tim Pearce, Pushi Zhang, Kaixin Wang, Xiaoyu Chen, Wei Shen, Li Zhao, and Jiang Bian. What do latent action models actually learn? arXiv preprint arXiv:2506.15691, 2025. [79] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint, 2025. [80] Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang, and He Wang. Uni-navid: videobased vision-language-action model for unifying embodied navigation tasks. arXiv preprint, 2024. [81] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based vlm plans the next step for vision-and-language navigation. Robotics: Science and Systems, 2024. [82] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, et al. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. arXiv preprint, 2025. [83] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-languageaction models. arXiv preprint, 2025. [84] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint, 2024. [85] Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, and Guanghui Ren. Acot-vla: Action chain-of-thought for vision-language-action models. arXiv preprint arXiv:2601.11404, 2026. [86] Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, et al. Flowvla: Visual chain of thought-based motion reasoning for vision-language-action models. arXiv preprint arXiv:2508.18269, 2025. [87] Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint, 2025. [88] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 VLA-JEPA Architecture VLM Backbone. We use Qwen3-VL-2B as the VLM backbone, which is implemented as dense Transformer. Its vision encoder consists of Vision Transformer and 3D convolutional modules. To capture physical dynamics, we adopt V-JEPA2 encoder checkpoint together with randomly initialized predictor as the latent world model, whose action inputs are the latent action tokens produced by the VLM. To enable the VLM to output time-aware latent actions and embodied actions, we designed latent action tokens latenti and an embodied action token action, which are incorporated as additional tokens into the vocabulary of Qwen3-VL. When generating the latent action tokens, we repeat the same latent action token latenti times to strengthen the models attention to the latent action tokens, where = 24/T , denotes the future video horizon, and 24 is the empirically optimal value. Latent World Model. To let the latent action tokens to capture the physical dynamics between frames, we adopt an auto-regressive Transformer as the architecture of the world model with time-causal attention mechanism. Specifically, within single time step, the latent action tokens and the image latent tokens attend to each other bidirectionally, whereas across different time steps, attention between tokens is constrained to be causal. Table 5 Configuration of the Latent World Model. Parameter Transformer Layers Attention heads Image token dimension Number of image tokens per time step Action token dimension Number of action tokens per time step Number of view Future video horizon Value 12 8 2048 256 2048 3 2 8 Action Head. To generate future actions conditioned on latent action embeddings, we adopt flow matchingbased Transformer architecture, DiT-B [56], as our action head. DiT-B is conditioned on an embodied action token sequence formed by repeating the action token 32 times, where 32 is the empirically optimal value. Table 6 Configuration of the Action head. Parameter Transformer Layers Attention heads Token dimension State dimension Action dimension Future action horizon Positional encoding Denoising timesteps Value 16 12 1024 8 7 7 Learnable 4 A.2 Training Details We preprocess all training datasets following unified pipeline. Observation images used as inputs to the VLM are resized to 224224. Video clips used by the world-state encoder are resized to 256256 to be consistent with the hyperparameter settings described in the previous subsection. For models pretrained with joint-position control (e.g. π0), we use joint-space delta positions as actions and apply minmax normalization to map them into [0, 1]. For models pretrained with end-effector control (e.g., VLA-JEPA), we use both end-effector delta positions and delta axisangle representations as actions, which are likewise minmax normalized to [0, 1] All gripper commands are binarized to {0, 1}. To handle multi-view observations, when Figure 7 Comparison of three models (π0, π0.5, VLA-JEPA) under the object-layout OOD setting. fewer than two camera views are available, we duplicate the world-state representation and concatenate the two copies. When more than two views are available, we select two of the available views and use them as the world-state representation. During training, we use batch size of 32 and train on 8 GPUs in parallel, resulting in global batch size of 256. We adopt cosine learning-rate schedule with linear warmup, with peak learning rate of 1e-5 for the VLM and the latent world model, and 1e-4 for action head. For the pretraining datasets, ssv2 and droid, we jointly train the model for 50K steps. For the simulation datasets, we continue training for 30K steps starting from the last pretrained checkpoint. For the real-world datasets, we further fine-tune the model for 20K steps, also initialized from the last pretrained checkpoint. Real-world Experiments Details Our real-world setup consists of Franka Research 3 robotic arm, Robotiq 2F-85 gripper, and three Intel RealSense D435 cameras, including two third-person views and one wrist-mounted view. The collected expert demonstrations involve picking and placing grapes, apples, mangoes, and oranges from table into plate or bowl. In the following experiments, each task is executed for 10 independent trials, and we report the average success rate over all trials. For task-level OOD evaluation in real-world experiments, we consider the following tasks: (i) picking up bananas from the table and placing them into bowl, (ii) picking up peaches from the table and placing them onto plate, and (iii) picking up grapes from the table and placing them onto the top level of shelf. We observe that, for the banana-picking task, both π0.5 and VLA-JEPA achieve approximately 50% success rate. For the peach-picking task, due to the irregular shape of peaches, the robot frequently violates the predefined safety boundaries during execution. For the final task, since shelves are not present in the training data, none of the models can successfully place the end-effector at the top level. However, compared with π0 and π0.5, which directly collide with the shelf, VLA-JEPA exhibits qualitatively different behavior by approaching the target from the rear side of the shelf and lifting the end-effector to higher position. Although the task is still not successfully completed, this behavior indicates that VLA-JEPA possesses improved task generalization ability compared with the baselines. For object-layoutlevel OOD evaluation in real-world experiments, we randomly select three tasks from the training set and randomly shuffle the object layouts to assess the robustness and generalization capability of the models. As shown in Figure 7, we observe that both π0 and π0.5 fail to reattempt grasping after an unsuccessful grasp, since the training data do not contain demonstrations of repeated grasping behaviors, i.e., reopening the gripper and regrasping after failure. Consequently, when grasp fails, these two policies do not actively release the gripper, as they have not learned the action of opening the gripper to reattempt grasp. In contrast, VLA-JEPA, which is pretrained on human videos and thus exposed to repeated grasping behaviors, is able to immediately open the gripper and attempt to grasp again after failure. We argue that repeated grasping does not require learning additional physical dynamics; instead, it mainly requires the model to learn when to perform regrasp action. Once this temporal decision is learned, the policy can internally map it to its own physical dynamics and execute the corresponding motion. This observation highlights one of the key advantages of pretraining on human videos."
        }
    ],
    "affiliations": [
        "Eastern Institute of Technology, Ningbo",
        "Nankai University",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology of China",
        "Zhongguancun Academy, Beijing, China"
    ]
}