{
    "paper_title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization",
    "authors": [
        "Meiling Tao",
        "Chenghao Zhu",
        "Dongyi Ding",
        "Tiannan Wang",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization."
        },
        {
            "title": "Start",
            "content": "PersonaFeedback: Large-scale Human-annotated Benchmark For Personalization Meiling Tao1,, Chenghao Zhu2,, Dongyi Ding3,, Tiannan Wang4, Yuchen Eleanor Jiang4, Wangchunshu Zhou4, 1University of Electronic Science and Technology of China, 2The Chinese University of Hong Kong, Shenzhen, 3South China Agricultural University, 4OPPO Equal contribution, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, new benchmark that directly evaluates LLMs ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the models ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization. Date: June 17, 2025 Correspondence: zhouwangchunshu@oppo.com Dataset: https://huggingface.co/datasets/PersonalAILab/PersonaFeedback 5 2 0 2 5 1 ] . [ 1 5 1 9 2 1 . 6 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have made tremendous progress in solving wide range of tasks, from common knowledge understanding to logical reasoning and creative writing [2, 15, 22, 37, 39, 47]. These advances have predominantly focused on enhancing the general intelligence of LLMs, often aligning models with universal values and preferences to improve their performance in diverse contexts. However, while these 1 Figure 1 Roadmap of PersonaFeedback: The Profiler infers user features based on the memory, the Generator generates personalized questions by combining these features with sample scene settings, and the Personalized Agent generates different answers. Finally, PersonaFeedback is constructed through manual annotation and consistency screening. advancements are foundational, they do not inherently guarantee or directly optimize for crucial aspect of the human-AI interaction: personalization. Personalization refers to the models ability to tailor its responses to individual users, considering their unique characteristics, preferences, and needs. Despite its potential to enhance user satisfaction and improve the human-AI experience, it remains nuanced challenge that extends beyond general competency. This focus on general capabilities is mirrored in the landscape of LLM evaluation. Current benchmarks have largely focused on evaluating models in terms of common tasks such as helpfulness, safety, and alignment with general preferences [18, 41, 42]. However, there has been little exploration of how effectively LLMs can generate responses tailored to the pluralistic nature of user personas [16, 32, 40]. Most existing benchmark [16, 32] rely on deriving implicit user personas from conversation history, assuming that the history of conversations alone is sufficient to understand users needs. While this approach is valuable, it is important to note that the ability to infer users persona from chat history and the ability to generate personalized responses are distinct and can be treated as independent tasks. In this paper, we introduce PersonaFeedback, new benchmark that directly evaluates LLMs ability to provide personalized responses given pre-defined user personas and queries. By explicitly providing the user persona, we separate the task of personalization from persona inference, allowing us to assess how well models 2 adapt their responses to specific user profiles without relying on implicit persona extraction from chat history. One of the challenges in evaluating personalization is quantifying the degree to which response is genuinely personalized. Previous framework [40] has attempted to automate this evaluation by adopting the LLM-as-ajudge method to assign scores to responses. Such method struggles with providing sufficient differentiation and lacks interpretability. To overcome this limitation, PersonaFeedback introduces binary-choice evaluation task, where models are presented with two potential responses and asked to choose the one that is more personalized. This approach allows us to effectively measure the degree of personalization by leveraging human-annotated test cases, enhancing evaluation automation, and better quantifying subtle differences in model performance. We illustrate data collecting process in Figure 1. PersonaFeedback consists of 8298 human-annotated test cases categorized into easy, medium, and hard tiers, based on the contextual complexity of the personas and the difficulty of distinguishing between two personalized responses. By presenting state-of-the-art (SoTA) LLMs with these test cases, we assess their proficiency in identifying personalized responses in various contexts and across different user profiles. Our empirical results reveal that although these SoTA models demonstrate impressive performance on complex task-solving benchmarks [6, 24] and RewardBench [18], they still fall short on the hard tier of PersonaFeedback. This sharp decline in performance highlights the challenges faced by even the most advanced models when dealing with more nuanced personalized scenarios. By observing the empirical results of our thorough evaluation, we derive several key insights: 1. Enhancing reasoning Does NOT yield to better personalization: Long-reasoning LLMs that excel at solving complex tasks do not show significant advantages over their base models when it comes to personalization. 2. Larger is better: As model size increases, the performance of open-source models improves steadily, with large-scale models showing clear advantage in handling personalized tasks. 3. Reward models struggle with personalized questions: Although SoTA reward models outperform base models on general questions, they lag behind on user-specific ones. 4. RAG falls short in personalization: As shown in Figure 3, despite being provided with relevant content, RAG setups do not outperform No Persona setups in personalization tasks. 5. Persona learning should be made explicit: As illustrated in Figure 3, explicitly providing user persona significantly boosts performance, suggesting that relying solely on implicit persona inference is insufficient."
        },
        {
            "title": "2 Related Work",
            "content": "Personalizing language models (LLMs) has gained attention in areas like recommendation systems, search, and research assistants, aiming to provide tailored responses based on user preferences [3, 7, 17, 21, 33, 35, 43, 49]. Recent efforts have expanded into domains like travel planning [44], writing assistants [26], book recommendations [50], shopping advice [48], and programming assistants [12]. common approach is fine-tuning personalized LLMs. For instance, Zhou et al. [52] combine persona prediction with response generation, while Tan et al. [34] apply LoRA [14] for user-specific fine-tuning. However, fine-tuning can be inefficient as it requires frequent re-training. In contrast, RAG-based personalized LLMs leverage user-specific data. For example, Salemi et al. [32] and Salemi et al. [31] introduce pseudo-RAG and retriever optimization, while Li et al. [19] and Mysore et al. [26] integrate user-authored documents for prompt augmentation. Yet, input length limitations persist, prompting some studies to summarize user histories for effective personalization [5, 30, 50]."
        },
        {
            "title": "2.1 Benchmarking Personalized LLMs",
            "content": "Existing LLM benchmarks provide standardized frameworks to assess various capabilities such as coding, task solving, and instruction following [20, 24, 51]. For reward models (RMs), which play vital role in 3 reinforcement learning from human feedback (RLHF) [28], benchmarks are also being developed to evaluate their effectiveness in guiding LLM alignment with human preferences [18, 42]. However, these benchmarks mainly focus on general model capabilities and alignment, rather than personalization. Recently, few lines of work have investigated in benchmarking personalization of LLMs []. LaMP [32], utilizes public datasets with user identifiers [13, 27] to mock users historical interactions and test cases. Zollo et al. [53] propose to leverage top-ranked reward models and prompt them to simulate different users to evaluate if personalized response is preferred by certain amount of users. AI Persona [40] addresses the lifelong learning process of persona and proposes using LLM-as-a-judge to score the personalization and helpfulness of response, given learned users persona and query. Jiang et al. [16] proposed PersonaMem benchmark, comprising 7 types of in-situ user queries, that focus on benchmarking LLMs adaptability to ever-changing user persona. Our PersonaFeedback explicitly includes user personas alongside queries, enabling direct evaluation of the models ability to tailor responses to specific personas."
        },
        {
            "title": "3 Methodology",
            "content": "To better evaluate the performance of reward models in personalized interaction scenarios, we propose PersonaFeedback. The benchmark considers both the degree of personalization and the helpfulness of responses, avoiding excessive personalization that disregards content quality. PersonaFeedback includes tasks with different difficulty levels (Easy, Medium, Hard) to comprehensively assess the models personalization capabilities. The final data samples consist of triplets (P, x, y), where denotes the persona profile, represents the input prompt for the model, and is the response generated by the model."
        },
        {
            "title": "3.1 Persona Construction",
            "content": "To build realistic and diverse user personas, we first collected 20 real user profiles as initial seeds. These seed profiles contained complete basic elements, including demographic, MBTI, and social background characteristics such as occupations and interpersonal relationships. Based on these real user data, we used seed hints and random combinations of basic elements to further expand the dimensions of user preferences, including daily lifestyle habits (diet, entertainment, shopping), travel patterns (business and leisure), social behaviors, use of productivity tools and content interests. Human annotators review and filter out personas that are overly idealized, inconsistent, or unrealistic. Retained personas have wide range of occupations, covering major categories such as STEM fields, business, education, healthcare, arts, service industry, and students. The details are referenced in Appendix A. Ultimately, we constructed 1,700 personas, of which 200 high-quality personas, manually selected, are specifically used to build the benchmark, while the remaining 1,500 are used for subsequent model training."
        },
        {
            "title": "3.2 Question Generation\nSpecific Questions To construct more authentic and diverse user queries while avoiding the issue of generating\noverly stereotypical and unrealistic questions that often arise from using complete static personas, we\nadopted the persona learning method from the AI PERSONA [40] framework to dynamically infer user\nfeatures. Specifically, we define a persona profile as a structured learnable dictionary with fields that include\nDemographic, Personality, and Preferences.",
            "content": "We have collected open-source data that covers multiple domains, such as social media, reviews, and forums. For each persona, we prompt LLMs to select content that the user is interested in as users memory data. Then, we randomly sample set of memory data for each persona, allowing the profiler to infer user features that these contents might reflect. The inferred features are then used by the generator to propose questions. The generator works as follows: Qi = (Pi, S), where Qi represents the i-th generated question, Pi represents the user features inferred from the users memory data, Mi represents set of memory data randomly sampled from the persona, and denotes list of sub-scenarios randomly sampled following real user interaction contexts. 4 Figure 2 An example of an annotated test case from the Specific Easy set of PersonaFeedback. Specifically, we annotate 150 questions for 10 real personas along with their corresponding memory data to serve as In-Context Learning (ICL) examples. Then randomly select one question each time to guide the LLM in generating authentic questions. We adopt an embedding model to calculate the similarity score of each question and use similarity score threshold as filter to ensure the diversity of our question data. Then we prompt LLM to rephrase the query when characteristic information is directly leaked. Finally, human annotators manually filter out unnatural or unreasonable questions. The details are referenced in Appendix B. After filtering, we obtain over 4,000 user-specific questions. General Questions We sample 30,000 questions from the ShareGPT-Chinese dataset1 and applied rigorous filtering, retaining only subjective open-ended questions with distinct personal characteristics, while removing objective and factual content answers. After applying length constraints and additional manual screening, we obtain final set of 1,600 high-quality questions. For each persona, we match queries that closely align with the individuals occupation, background, or personality. Subsequently, we validate these queries to ensure that they are consistent with the persona. This approach finally results in 3841 human-annotated questions."
        },
        {
            "title": "3.3 Answer Generation\nAnswer Generation We introduce a Personalization Agent and design three distinct answer generation strategies\nfor each question.",
            "content": "A1: Answers generated using core persona fields (Demographic, Personality) as well as preference traits inferred by the profiler that are directly related to the question. A2: Answers generated with 80% of the complete persona profile randomly masked. A3: Answers generated based solely on the question itself, without providing any additional persona information."
        },
        {
            "title": "3.4 Data Selection\nAnswer Selection We hire 9 human evaluators to select the response that best matches the profile of a given\npersona and is the most helpful. The evaluator selects the response that is more consistent with the profile of\nthe persona and more helpful based on the evaluation criteria (see Appendix C). The answer selected by the\nmajority of evaluators serves as the ground truth. Figure 2 presents an example of a selected test case by\nhuman labeler.",
            "content": "Difficulty Levels Based on the ground truth selected by human evaluators, we construct evaluation tasks at three difficulty levels. To generate answers with varying degrees of personalization and helpfulness, we rewrite them using four different models to generate multiple answers with similar quality but slightly different levels 1Original dataset please refer to https://huggingface.co/datasets/FreedomIntelligence/sharegpt-chinese 5 of personalization and helpfulness, forming an answer pool. We then randomly selected 2 answers from the answer pool to form answer pairs, and 9 human evaluators selected the best answer. We use Fleisss Kappa coefficient to calculate the consistency of the evaluators, excluding answer pairs with low consistency (κ 0.4). The remaining answer pairs are classified into Medium level and Hard level. Easy: Compares the ground truth selected by humans with answers that do not match persona characteristics or are more general. This primarily tests whether the model can recognize clear personalization differences. Medium: Pairs of answers with higher consistency between the evaluators (κ > 0.6) are classified as medium difficulty, indicating that the differences between these answers are relatively clear and most evaluators judgments tend to align. Hard: Pairs of answers with moderate consistency between the evaluators (0.4 < κ 0.6) are classified as hard difficulty, indicating that the personalization differences between these answers are small and the task is more difficult. Finally, we construct the benchmark data as shown in Table 1. For specific examples and concrete prompts been used please refer to Appendix and Appendix G. Table 1 The statics of PersonaFeedback Specific General Easy Medium Hard 1682 1667 Easy Medium Hard 1010 1321 1510 Total"
        },
        {
            "title": "3.5 Reward Model Training",
            "content": "We construct dataset consisting of 10,000 data points for reward model training. We use the responses generated by GPT-4o-mini given the persona profile as the chosen response and the responses generated without the persona as the rejected one to form training pairs. Then, we select Qwen2.5-0.5B-Instruct and Gemma-2B-it as our base models and adopt Bradley-Terry (BT) [4] as the training objective. The mathematical formulation of the Bradley-Terry (BT) loss is as follows: P(a1 a2x, a1, a2) = exp(r(x, a1)) exp(r(x, a1)) + exp(r(x, a2)) = σ(r(x, a1) r(x, a2)), where represents the query, a1 represents the chosen response, and a2 represents the rejected response. Results and details in Appendix show that such intuitive and simple preference data can effectively improve model performance on personalized benchmarks."
        },
        {
            "title": "4.1 Evaluation Settings",
            "content": "We evaluate wide range of models, including proprietary models and open-sourced models2 in PersonaFeedback. This comprehensive evaluation aims to assess the performance of the reward models in personalized scenarios, as shown in Table 2. We also explore models performance on three different settings in PersonaFeedback: (1) Persona Profile: The model has access to the configuration information of the persona. (2) RAG: The model do not maintain or explicitly learn the users persona, but it can retrieve and use relevant user memory data to select more appropriate responses. (3) No Persona: The model does not have access to any personalized information. This setting serves as baseline. 2See Appendix for complete list of models, including their references and Huggingface links. 6 Metric We use Accuracy as the evaluation metric, measuring the models ability to select the best personalized response and recognize high-quality answers under three levels of task difficulty. For classifier-based reward models, we define correct classification when the model assigns higher score to the chosen response than to the rejected response in the given persona and the input prompt x. For generative reward models, we use prompts to guide the model in selecting among the response options provided, thereby evaluating its ability to choose personalized content. Table 2 PersonaFeedback results for different model groups, including reasoning models, chat models, open-source models and reward models. Within each setting, models are sorted by Total Avg. in descending order. The highest score in each column is highlighted in bold. Model Easy Medium Hard Avg. Easy Medium Hard Avg. Total Avg. Specific General Reasoning o3-mini o4-mini Gemini-2.5-pro-exp-03-25 Deepseek-R1 o1-preview-2024-09-12 Gemini-2.0-flash-thinking-exp o1-mini Chat GPT-4.1 GPT-4.5-preview Deepseek-V3 GPT-4o-2024-11-20 Claude-3-5-sonnet-20241022 Claude-3-7-sonnet-20250219 Gemini-2.0-flash GPT-4o-mini Claude-3-haiku-20240307 GPT-4-turbo Open-Source QwQ-32B R1-Distill-Qwen-32B Qwen2.5-32B-Instruct R1-Distill-Qwen-14B Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct Llama-3-8B-Instruct Reward Model INF-ORM-Llama3.1-70B RM-Mistral-7B LDL-Reward-Gemma-2-27B-v0.1 Llama-3-OffsetBias-RM-8B Skywork-Reward-Llama-3.1-8B QRM-Llama3.1-8B-v2 ArmoRM-Llama3-8B-v0.1 91.8 88.5 88.6 90.8 95.5 89.8 88. 90.6 89.0 89.0 88.8 89.2 90.7 89.3 87.3 88.6 86.7 88.8 87.9 86.7 87.2 84.0 81.4 71.0 85.2 83.7 78.3 83.5 74.7 76.8 54.2 77.5 76.3 76.0 72.1 71.6 76.3 76.4 76.5 76.2 75.0 76.2 73.6 72.8 73.7 73.4 73.0 67. 74.7 72.7 74.4 73.7 72.0 63.4 59.8 75.9 73.1 74.0 69.8 72.2 71.2 62.6 68.6 70.0 67.0 71.3 69.6 68.0 67.5 69.1 67.2 68.0 68.6 70.2 64.8 64.6 63.3 65.7 67. 66.6 66.2 68.4 62.9 66.3 62.0 53.2 70.2 67.8 69.4 67.2 67.6 65.2 55.7 77.7 77.0 75.7 76.4 76.8 76.5 76.0 77.2 76.0 75.8 76.5 76.2 74.2 74.1 73.0 74.1 71.9 75.1 74. 75.2 73.0 72.8 67.3 60.1 76.1 73.7 73.3 72.2 70.3 70.3 57.9 89.1 88.1 90.7 88.8 88.3 89.7 88.2 89.0 91.0 90.4 89.2 89.4 89.0 88.4 87.2 86.1 87. 89.3 89.3 87.5 87.0 86.4 83.9 50.9 88.1 88.4 85.4 88.8 82.5 79.1 82.9 83.7 85.1 86.4 86.9 84.6 82.4 86.0 85.7 85.6 84.6 84.5 84.1 83.1 84.3 84.4 83.9 82. 84.7 86.1 84.3 81.4 83.3 79.3 51.0 85.2 82.4 82.9 79.4 81.3 76.5 75.3 70.5 71.2 71.5 68.9 70.9 69.8 68.9 71.1 69.3 68.3 66.5 67.5 70.1 69.0 68.8 63.8 67. 68.1 65.9 64.3 66.9 65.1 60.7 50.6 69.7 70.5 67.6 65.3 68.1 62.5 54.7 82.4 82.6 84.2 82.9 82.5 82.0 82.4 83. 83.4 82.6 81.6 81.8 82.0 81.9 81.4 79.5 80.7 82.1 82.0 80.3 79.8 79.7 76.2 50.9 82.3 81.6 79.9 79.4 78.3 73.8 72.9 79.9 79.6 79.6 79.4 79.4 79.0 79.0 80.0 79.4 78.9 78.9 78.8 77.8 77.7 76.9 76.6 76.0 78.3 77.7 77.6 76.1 76.0 71.4 55. 79.0 77.4 76.4 75.5 74.4 71.9 64.8 * All settings (easy, medium, hard) in the table above are binary choices. Therefore, the random baseline is 50."
        },
        {
            "title": "4.2 Main Results",
            "content": "Takeaway 1: Enhancing Reasoning Does NOT Yield To Better Personalization In personalization QA, reasoning models fail to demonstrate competitive advantages over non-reasoning counterparts despite their enhanced reasoning capabilities. As shown in Table 2, the average scores of reasoning models such as o3-mini and o4-mini on specific and general tasks are similar to those of chat models such as GPT-4.1 and GPT-4.5-preview. This indicates that enhancing reasoning ability alone does not necessarily translate into better personalization ability. Furthermore, in the comparison of open-source models, we observe that the performance of R1-Distill-Qwen-32B is lower than that of Qwen2.5-32B-Instruct. It is worth noting that even the top proprietary models have relatively low average accuracy on Hard difficulty tasks, suggesting that even the most advanced models still have room for improvement in understanding the subtlety of personalized responses. Takeaway 2: Larger is Better As the number of parameters increases, the performance of open-source models improves steadily, with large-scale models showing clear advantage in handling personalized tasks. As shown in Table 2, for the Qwen series, as the parameter scale increases from 7B to 32B, the models performance improves steadily, with an 11-point increase in the Specific Medium task. The R1-Distill-Qwen series shows similar trend. The average accuracy of QwQ-32B and Qwen2.5-32B-Instruct is comparable to that of some proprietary models such as GPT-4-turbo and 4o-mini. Takeaway 3: Reward Models Struggle with Personalized Questions Although SoTA reward models outperform base models on general questions, they lag behind on user-specific ones. As shown in Table 2, models like INF-ORM-Llama3.1-70B [25](ranks first in RewardBench [18]) achieve higher average scores on general questions than most generative models, likely due to their alignment with general question-answer pairs. However, for specific questionsespecially in the easy and medium settingsreward models show limited generalization compared to generative models. This suggests that while current opensource general reward models are effective for general and potentially personalized queries, there is still room for improvement in handling more personalized questions. Takeaway 4: RAG Falls Short in Personalization Takeaway 5: Persona Learning Should Be Made Explicit By comparing the RAG setting with the No Persona setting, we observed that LLMs do not benefit from the contextual information brought by the relevant content. As shown in Figure 3, the Persona Profile-based setting consistently outperforms RAG and No Persona across all tasks. Surprisingly, RAG performs similarly to No Persona in most cases, which seems counterintuitive, as RAG should provide more relevant information than model with no personalization. The main reasons may include: (1) In the RAG setting, the model needs to infer user preferences from the retrieved fragmented memories, which is an implicit reasoning task posing higher requirements of models ability. On the other hand, the No Persona setting allows the model to focus on the quality of the answer to the question itself, without interference from potentially noisy memory data. (2) The retrieved information may contain noise or conflicting details. For example, when user from Northeast China asks What should eat to recover faster after skiing?, the memory retrieved under the RAG setting only includes knowledge related to fat-loss diet, but lacks crucial information about the users origin in Northeast China. As result, the model cannot provide personalized advice tailored to the cold environment and regional dietary habits. This illustrates that personalized 8 Figure 3 Comparison of Reasoning and Chat models under Persona, RAG, and No Persona settings. information should be explicitly learned and utilized, as relying solely on the retrieval mechanism cannot ensure the effective use of key information."
        },
        {
            "title": "4.3 Correlation with Five Aspects of HelpSteer2",
            "content": "We used QRM-Llama3.1-8B-v2 [10], top-performing reward model on RewardBench, to estimate the five aspects proposed by HelpSteer2: [42] (helpfulness, correctness, coherence, complexity, and verbosity). For each aspect, we computed the accuracy by checking whether the chosen response scored higher than the rejected response (e.g., if chosen.helpfulness > rejected.helpfulness, it is counted as correct in helpfulness). The results are reported in Figure 4 (Left) as accuracy for each aspect over the whole set, as well as for the easy, medium, and hard subsets. Figure 4 Left: Accuracy on five HelpSteer2 aspects for the Specific (top) and General (bottom) sets. Right: Correlation heatmap between the selection label (Chosen) and the five HelpSteer2 aspects. 9 Observation on Accuracy: The accuracy results in Figure 4 (Left) provide several insights into the composition of our dataset. For the easy subset, where the comparison is often between general response and personalized one, the model achieves notably high accuracy on the verbosity aspect. However, as the difficulty increases in the medium and hard subsets, the accuracy drops across all five aspects. This trend indicates that our dataset is designed with clear progression in difficulty, and as the questions become more difficult, it is not enough to rely on any single HelpSteer2 aspect. Instead, higher level of personalization and helpfulness is required to distinguish answers in these harder cases. In addition, the accuracy values and trends are consistent across both the Specific and General sets, demonstrating the robustness of our data construction method. Analysis of Correlation: Afterwards, we compiled list of all responses with their scores on the five HelpSteer2 dimensions, along with chosen flag (1 if the response was selected in the pairwise comparison, else 0). We then calculated the correlation coefficients (using Pearson [11] correlation) between each dimension and the chosen label. As shown in Figure 4, the personalization metric shows little correlation from the five aspects addressed in Helpsteer2. This suggests that our personalization metric captures information not covered by existing dimensions, highlighting its distinctiveness and importance."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce PersonaFeedback, novel benchmark that decouples persona inference from personalization, enabling focused evaluation of how well LLMs can adapt their responses to explicit user personas. We curate dataset of 8,298 human-annotated test cases, categorized into easy, medium, and hard tiers, providing comprehensive evaluation suite for LLM personalization. We conduct extensive evaluation across wide range of models, revealing critical insights into the limitations of current systems, such as the insufficient effectiveness of long-reasoning models, the advantage of larger models, and the challenges faced by reward models and retrieval-augmented frameworks in personalization tasks. Our benchmark, along with detailed annotation protocols and evaluation tools, is made publicly available to support future research into personalized adaptation in LLMs."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD. md. [2] Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/introducing-claude. [3] Oren Barkan, Yonatan Fuchs, Avi Caciularu, and Noam Koenigstein. Explainable recommendations via attentive multi-persona collaborative filtering. In Proceedings of the 14th ACM Conference on Recommender Systems, pages 468473, 2020. [4] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. URL https://api.semanticscholar.org/CorpusID:125209808. [5] Konstantina Christakopoulou, Alberto Lalama, Cj Adams, Iris Qu, Yifat Amir, Samer Chucri, Pierce Vollucci, Fabio Soldo, Dina Bseiso, Sarah Scodel, et al. Large language models for user interest journeys. arXiv preprint arXiv:2305.15498, 2023. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. [7] Yijia Dai, Joyce Zhou, and Thorsten Joachims. Language-based user profiles for recommendation, 2024. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [9] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. [10] Nicolai Dorka. Quantile regression for distributional reward models in rlhf. arXiv preprint arXiv:2409.10164, 2024. [11] David Freedman, Robert Pisani, and Roger Purves. Statistics (international student edition). Pisani, R. Purves, 4th edn. WW Norton & Company, New York, 2007. [12] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. Assistgui: Task-oriented desktop graphical user interface automation. arXiv preprint arXiv:2312.13108, 2023. 11 [13] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4), December 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/ 2827872. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [15] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [16] Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo Taylor, and Dan Roth. Know me, respond to me: Benchmarking llms for dynamic user profiling and personalized responses at scale. arXiv preprint arXiv:2504.14225, 2025. [17] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. Do llms understand user preferences? evaluating llms on user rating prediction, 2023. [18] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. https://huggingface.co/spaces/allenai/reward-bench, 2024. [19] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. Teach llms to personalizean approach inspired by writing education. arXiv preprint arXiv:2308.07968, 2023. [20] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024. [21] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. Personal llm agents: Insights and survey about the capability, efficiency and security, 2024. [22] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [23] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. [24] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [25] Xiaoyu Tan Minghao Yang, Chao Qu. Inf-orm-llama3.1-70b, 2024. URL [https://huggingface.co/infly/ INF-ORM-Llama3.1-70B](https://huggingface.co/infly/INF-ORM-Llama3.1-70B). [26] Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers. arXiv preprint arXiv:2311.09180v1, 2023. [27] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 188197, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1018. URL https://aclanthology.org/D19-1018/. [28] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [29] Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. 12 [30] Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and Abhinav Sethy. Integrating summarization and retrieval for enhanced personalization via large language models. arXiv preprint arXiv:2310.20081, 2023. [31] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. Optimization methods for personalizing large language models through retrieval augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 752762, 2024. [32] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization, 2024. [33] Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research and future directions. arXiv preprint arXiv:2312.11518, 2023. [34] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. Democratizing large language models via personalized parameter-efficient fine-tuning. arXiv preprint arXiv:2402.04401, 2024. [35] Meiling Tao, Xuechen Liang, Tianyu Shi, Lei Yu, and Yiting Xie. Rolecraft-glm: Advancing personalized role-playing in large language models. arXiv preprint arXiv:2401.09432, 2023. [36] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288. [38] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multiobjective reward modeling and mixture-of-experts. In EMNLP, 2024. [39] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, et al. Weaver: Foundation models for creative writing. arXiv preprint arXiv:2401.17268, 2024. [40] Tiannan Wang, Meiling Tao, Ruoyu Fang, Huilin Wang, Shuai Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Ai persona: Towards life-long personalization of llms. arXiv preprint arXiv:2412.13103, 2024. [41] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023. [42] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024. [43] Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, and Jan Kocoń. Personalized large language models, 2024. [44] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. 2024. [45] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint, 2024. [46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, 13 Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [47] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [48] Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili Wang, Shusen Wang, and Hai Zhao. Refgpt: Dialogue generation of gpt, by gpt, and for gpt. arXiv preprint arXiv:2305.14994, 2023. [49] Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. Palr: Personalization aware llms for recommendation, 2023. [50] Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, and Xun Liang. Bookgpt: general framework for book recommendation empowered by large language model. arXiv preprint arXiv:2305.15673, 2023. [51] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [52] Wangchunshu Zhou, Qifei Li, and Chenle Li. Learning to predict persona information for dialogue personalization without explicit persona description. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 29792991, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.186. URL https: //aclanthology.org/2023.findings-acl.186. [53] Thomas Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, and Hongseok Namkoong. Personalllm: Tailoring llms to individual preferences. arXiv preprint arXiv:2409.20296, 2024."
        },
        {
            "title": "A Persona Selection Protocol",
            "content": "Selection Process To ensure that the personas we build are authentic and diverse, we implemented two rounds of quality screening for the initial 1,700 personas. 1. Round 1: Authenticity and Consistency Exclude personas whose professions, behaviors, or lifestyles are disconnected from modern society. Exclude overly idealized or internally contradictory personas. Exclude personas that are overly dramatized or reliant on stereotypes. 2. Round 2: Diversity and Balance Avoid over-concentration in specific profession categories. Avoid homogeneity in interests, habits, and other dimensions."
        },
        {
            "title": "B Question Filtering Protocol",
            "content": "After generating the set of questions, we apply the following manual filtering criterion to ensure the quality and naturalness of the final questions: Logical Inconsistency: Filtered out questions that were logically inconsistent or forced unrelated topics together. Unnatural Phrasing: Filtered out questions that were awkwardly phrased or unnatural. An Example of Filtered Questions Question: am currently optimizing the document processing process. Is there way to improve housework efficiency at the same time ? Reasons for filtering: This question awkwardly combines document processing workflows and house efficiency, two unrelated domains that lack natural coherence. An Example of Filtered Questions Question: How can gardening knowledge be used to optimize software architecture ? Reasons for filtering: The question bridges fields that are too unrelated, which goes against conventional logic. 15 Figure 5 screenshot of the human annotation."
        },
        {
            "title": "C Answer Selection Protocol",
            "content": "Task Description The human evaluator needs to simulate the user based on the persona profile provided and select the most appropriate answer based on the following criteria: Selection Criteria Helpfulness The answer should precisely address the users specific question, rather than being generic response or forced combination of information from the user configuration that results in strained answer. Personalization The answer should reflect attention to multiple fields of information in the users configuration and appropriately integrate relevant aspects to provide personalized assistance."
        },
        {
            "title": "D Reward Model Train Details and Results",
            "content": "Table 3 PersonaFeedback train results for reward model models. Specific General Model Qwen2.5-0.5B-Instruct(generative) +HelpSteer2 Trained +HelpSteer2 Personalized Trained Qwen2.5-3B-Instruct(generative) +HelpSteer2 Trained +HelpSteer2 Personalized Trained Gemma-2b-it(generative) +HelpSteer2 Trained +HelpSteer2 Personalized Trained Easy Medium Hard Avg. 50.6 49.8 52.0 33.6 68.4 85.9 63.2 70.1 62.5 59.6 73.1 88.7 49.8 51.1 63.5 71.7 71.8 88.5 50.5 54.7 57.1 57.6 62.7 64.8 50.7 57.3 61. 51.3 61.6 68.3 64.2 64.3 71.2 47.9 64.2 71.5 Easy Medium Hard Avg. 50.1 49.8 47.2 39.7 70.9 79.1 66.9 70.7 69.6 74.5 75.1 84.8 50.9 51.0 69.1 73.4 73.4 81.5 49.7 51.5 57.4 55.8 57.2 57.4 49.9 60.0 61.7 50.8 52.5 71.9 71.0 73.4 77.4 51.6 71.2 73.1 Total Avg. 50.4 49.8 69.6 64.9 65.8 74.0 50.3 66.1 72.5 * All settings (easy, medium, hard) in the table above are binary choices. Therefore, the random baseline is 50. Table 3 shows that base generative models (Qwen2.5-0.5B-Instruct and Gemma-2B-it) perform near random on personalized tasks, indicating that too small models cannot handle personalization out of the box, but Qwen2.5-3B-Instruct can get score of 65 initially. To enhance model performance, we selected 3632 pairs of data from Helpsteer2, These samples were selected based on criterion where the chosen response demonstrated helpfulness score at least 2 points higher than its rejected counterpart. Subsequently, we combined this helpfulness-oriented data with our custom dataset of 10,000 preference pairs, utilizing the RLHFlow/RLHF-Reward-Modeling framework [9]. The resulting BT reward models (shown in the \"+HelpSteer2 Personalized Trained\" rows) achieved substantial improvements across both specific and general question categories. While simple, our preference data effectively improves model performance on PersonaFeedback. Training details are as follows: For the \"+HelpSteer2 Trained\" setting, we use maximum sequence length of 4096, learning rate of 1e-5, and batch size of 32, and train for one epoch. In the \"+HelpSteer2 Personalized Trained\" setting, both models use maximum sequence length of 4096, learning rate of 1e-5, batch size of 64, and 1 epoch. We observe that varying the batch size between 64 and 256, or adjusting the learning rate between 1e-5 and 2e-5, has little impact on the results. Similarly, the choice of adjacent checkpoints has little effect on model performance."
        },
        {
            "title": "E List of Models Used",
            "content": "Table 4 List of all models used in our experiments, with Hugging Face links where available. Model Name QwQ-32B [36] R1-Distill-Qwen-32B [8] Qwen2.5-32B-Instruct [46] R1-Distill-Qwen-14B Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct Llama-3-8B-Instruct [1] INF-ORM-Llama3.1-70B [25] RM-Mistral-7B [45] LDL-Reward-Gemma-2-27B-v0.1 Llama-3-OffsetBias-RM-8B [29] Skywork-Reward-Llama-3.1-8B [23] QRM-Llama3.1-8B-v2 [10] ArmoRM-Llama3-8B-v0.1 [38] Qwen2.5-0.5-Instruct Gemma-2b-it Hugging Face Link https://huggingface.co/Qwen/QwQ-32B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B https://huggingface.co/Qwen/Qwen2.5-32B-Instruct https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B https://huggingface.co/Qwen/Qwen2.5-14B-Instruct https://huggingface.co/Qwen/Qwen2.5-7B-Instruct https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct https://huggingface.co/infly/INF-ORM-Llama3.1-70B https://huggingface.co/weqweasdas/RM-Mistral-7B https://huggingface.co/ShikaiChen/LDL-Reward-Gemma-2-27B-v0.1 https://huggingface.co/NCSOFT/Llama-3-OffsetBias-RM-8B https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B https://huggingface.co/nicolinho/QRM-Llama3.1-8B-v2 https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1 https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct https://huggingface.co/google/gemma-2b-it"
        },
        {
            "title": "F Examples",
            "content": "Table 5 Examples of three levels of difficulty (easy, medium, hard). Type Easy Query If want to focus on my health, what aspects do need to consider? How can uplift myself when Im feeling down? Medium How to stay healthy amidst busy schedule? Hard Response chosen: Every morning when you exercise in the park, you can do some proper warm-up to prevent injuries... Sitting for long periods while working can easily lead to fatigue. It is recommended that you stand up and move your shoulders and neck every 45 minutes... reject: Every morning, you can take 5 to 10 minutes to plan your day, ensuring that you prioritize important and urgent tasks before addressing non-urgent matters... chosen: You can try flipping through some philosophy books you found in old bookstores on Fuzhou Road, such as \"The Tibetan Book of the Dead.\" Such reading often provides emotional comfort and deeper sense of meaning in life. If the weather outside is nice, you could also take walk in the Guangfulin Ruins Park, where you can quietly observe and record the interactions between different people. This can help you discover simple and beautiful moments in life... reject: When feeling down, you might try browsing through some philosophical reading materials, such as related books or personal reflection notes from others. These words may bring certain spiritual strength... chosen: 1.Morning Planning: Continue to stick to the 5:30 am high-intensity interval training and incorporate health goals into your schedule using data from your Apple Watch.... 2. Healthy Eating: During work hours, choose your usual cold brew coffee paired with healthy light meal, such as organic salad and protein-rich sides. You might consider pre-ordering high-quality takeout for the week to reduce waiting time while ensuring the freshness and nutrition of the ingredients... 3. Weekend Recovery: Use weekends to try your favorite Spanish tapas restaurant and combine it with light hiking or cultural exploration in the area to rejuvenate both body and mind... reject: You can combine your morning running routine with exploring high-quality café near the hotel that offers healthy breakfast options. This not only energizes you but also adds sense of enjoyment to your workday. Make use of the space in the airport lounge to choose nutritious light snacks and fruits instead of fast food. Additionally, take advantage of fragmented time in the mornings or evenings to do some low-intensity stretching or meditation to help your body recover. Consider portable smart health devices (such as portable heart rate monitor or yoga practice guide) that align with your preference for efficient tools; these can easily fit into your work or travel environment while enhancing your health management efficiency. 19 Table 6 Examples of different types. Type Uncomfortable Query What methods can be used to better record the details during trip? Overpersonalization Are there any films that provide sense of thrilling adventure while also featuring creative action sequences? Response chosen: You could try combining recording with creative expression. Its recommended to carry lightweight film camera to capture delicate street scenes and cultural moments, such as steaming bowl in an old teahouse or the handwritten signs of flower shop on the corner. At the same time, keep Moleskine sketchbook handy to jot down inspirations and observations, capturing those unique moments... reject: You can carry lightweight film camera to capture changes in light and shadow, such as the dappled sunlight on bamboo chairs in the street or the reflections outside café window in an alley... These methods not only align with Chen Jialings pursuit of non-commercial recording but also help capture more delicate moments... Chen Jialing can also use her fragrance collection bottle to gather the scents from markets, cafés, or guesthouses as another form of sensory recording... chosen: If you want to experience the thrill of adventure while enjoying creatively packed action sequences, highly recommend the \"Agent Extreme\" series. This series is filled with extreme sports elements such as skydiving and downhill skateboarding, guaranteed to ignite your spirit of adventure... After watching, if youre interested, you can try to mimic some of the moves from the film and even use GoPro to capture your own creative extreme sports clips... reject: The \"Agent Extreme\" series hits all the right notes for youthe scene where the main character rides modified skateboard and dives down snowy mountain is so thrilling that it practically defies Newtons laws; the collision of physics and extreme sports will definitely get your DNA racing. recommend paying special attention to the part where Vin Diesel descends the steps of bridge; youll surely find yourself jotting down three pages of notes on ideas for upgrading your fixie bike. . . By the way, the parabolic trajectory of the motorcycle surfing at the end of the film is definitely worth writing next to your game sketches in your physics notebook as fun bonus..."
        },
        {
            "title": "G Prompts Used in PersonaFeedback Data Generation",
            "content": "Prompt for generating questions System Prompt: You are professional role-player. Based on the following character information, you need to generate one natural question that the character would ask an AI assistant. Important rules: 1. The question must be based on scene. Avoid purely factual queries. The question should focus on open-ended, exploratory, or subjective judgment types. 2. The question must sound natural, self-consistent and realistic, as if asked by real person. Do not awkwardly mash memory scenes and persona configuration. You can reference the persona configuration for inspiration, but the question should arise organically from the scene. Below is the persona config: {persona_description} Below is the list of memory scenes: {scenes} Output format example: {example} Prompt for naturally phrased questions System Prompt: Your task is to determine whether the following question is natural, coherent, and aligns with what user would typically ask an AI assistant. If you find any logical contradictions in the question, you should rewrite it to remove the unreasonable parts and rephrase it into natural question. Please format your output as follows: <conclusion > Answer whether to rewrite here, output True or False (True means it needs to be rewritten) </conclusion > <question > Output the rewritten question based on the conclusion of the conclusion. If the conclusion is True, output the rewritten content. if it is False, output the original question. </question > The following is an example of an unnatural question that needs to be rewritten: {example} The following is the question: {question} 21 Prompt for generating answers System Prompt: You need to simulate responses from an answer the users questions as the AI assistant based on the provided user configuration. You need to simulate responses from an AI assistant to user questions. Answer user questions as the AI assistant based on the provided user configuration. First, consider helpfulness: the answer should be more precise in addressing the users specific question. Eliminate unnatural or redundant phrases, ensuring that the answer is natural and fluent. Next, consider personalization: you can reference relevant personalized fields, but do not insert them awkwardly. Important rules: 1. The response should reflect the tone and style, the professional AI assistant, friendly and helpful. 2. Take into account the users preferences and characteristics to provide personalized response. 3. The answer should meet the users needs without overtly showcasing field information. Instead, infer what the user might want based on their existing persona configuration, rather than just combining field details. 4. The response should directly solve the users question while reflecting an understanding of their preferences. Here is the current persona config: {persona_description} Here is an example of output: {example} Here is the user question: {question} Generate personalized answer: 22 Prompt for answer improvement System Prompt: You are professional AI assistant who specializes in improving the quality of responses. You need to improve an existing answer to make it more personalized and helpful. Original question: {question} The answer to be improved: {original_answer} Please improve the response: The answer should more accurately address the users specific question, eliminating any unnatural or redundant parts in the original response. Make sure that the answer flows naturally without awkwardly inserting any information from the users configuration. Put the improved response within <improved_answer >tags. The revised answer should be noticeably more natural than the original one. Ensure that the word count does not increase or decrease! Output format: <reasoning > The changes to the answer are output here. </reasoning > <improved_answer > The modified answer content. </improved_answer > Prompt for answer degradation System Prompt: You are an export in modify the AI assistant answer. You are required to modify an existing answer to make it less personalized or less helpful. Original question: {question} The answer to be degraded: {original_answer} Please modify the previous answer by choosing one of the following directions to lower its quality: 1. Reduce personalization: Make the answer more generic. For example, change personalized element in the original response into something more universal, something that would be acceptable to anyone, or at least not fully aligned with the users specific interests and preferences. 2. Lower helpfulness: Make the answer vague or less precise, not solving the users specific problem well. For instance, add overly general or irrelevant information, or provide vague or overly broad advice. Place the degraded answer in the <degraded_answer >tag. The modified answer should be more generic and less helpful than the original, but still maintain basic logic and structure. Ensure that the word count does not increase or decrease! Output format: <reasoning > The changes to the answer are output here. </reasoning > <degraded_answer > The modified answer content. </degraded_answer >"
        },
        {
            "title": "H Limiations",
            "content": "Although PersonaFeedback significantly advances the research on personalized evaluation of large language models (LLMs), some limitations still exist. The binary choice evaluation method we use effectively quantifies differences in personalization capabilities, but human evaluators judgments are inevitably influenced by subjective factors, especially in more challenging cases. Moreover, despite our efforts to construct diverse and realistic user personas, the created characters may still contain certain biases or simplifications, failing to fully capture the complexity and nuances of real users."
        }
    ],
    "affiliations": [
        "OPPO",
        "South China Agricultural University",
        "The Chinese University of Hong Kong, Shenzhen",
        "University of Electronic Science and Technology of China"
    ]
}