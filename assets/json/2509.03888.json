{
    "paper_title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize",
    "authors": [
        "Cheng Wang",
        "Zeming Wei",
        "Qin Liu",
        "Muhao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 8 8 3 0 . 9 0 5 2 : r False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize Cheng Wang1 Zeming Wei2 Qin Liu3 Muhao Chen 1 National University of Singapore 2 Peking University 3 University of California, Davis wangcheng@u.nus.edu weizeming@stu.pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal false sense of security around current probingbased approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) can comply with harmful instructions, raising serious safety concerns and motivating numerous efforts of defenses against adversarial manipulation. prominent recent approach in literature leverages internal representations to characterize how models process benign versus malicious inputs. For example, few studies (Lin et al., 2024; Zheng et al., 2024; Qian et al., 2025) have performed visualization with dimensionality reduction and demonstrated Equal Contribution. 1We have open-sourced the project at https://github. com/WangCheng0116/Why-Probe-Fails. that benign and malicious inputs show clear separation in the hidden state space. Complementing this line of work, recent research proposes probingbased detection that trains lightweight classifiers on hidden states to distinguish malicious from benign inputs (Zhou et al., 2024; Zhang et al., 2024; Dong et al., 2025; Qian et al., 2025). These approaches leverage the assumption that the observed separability in hidden state space reflects learnable semantic distinction between harmful and benign content. Such probing classifiers often report high in-domain accuracy, leading to their adoption as safety detection mechanisms. In this work, we refer to probing as technique that trains simple classifiers on frozen internal representations to assess what information they encode technique widely applied across LLM monitoring tasks such as truthfulness assessment (Azaria and Mitchell, 2023), pretraining data detection (Liu et al., 2024c), hallucination detection (Alnuhait et al., 2024), and multilingual competence (Chang et al., 2022). Despite promising in-domain results, our reevaluation shows that probing-based approaches are far less robust than claimed for LLM safety. Our investigation is motivated by the observation that probing classifiers experience substantial degradation in performance when tested on outof-distribution (OOD) data. This fragility is inconsistent with the key premise underlying probingbased methods: if the internal representations truly encode stable semantic notion of harmfulness, their performance should not deteriorate so sharply under distribution shift. If probes only capture superficial patterns rather than genuine semantic understanding, this calls into question not only detection systems but also the broader interpretations of model behavior derived from probing analyses. Based on this observation, we posit the central hypothesis: Probing representations primarily capture shallow patterns rather than the semantics of harmfulness. To systematically invesFigure 1: Overview of the research methodology. Motivated by the poor performance of probing classifiers on out-of-distribution (OOD) data, this study hypothesizes that they learn superficial patterns instead of semantic harmfulness. This hypothesis is validated by experiments demonstrating the classifiers reliance on surface-level features and trigger words. tigate this claim, we evaluate through series of Research Study that progressively stress-test the probing-based detection mechanism. Research Study 1 contrasts probe classifiers against naive Bayes model with n-gram features to test whether sophisticated internal representations offer genuine advantages over surface-level pattern matching. Research Study 2 evaluates performance on semantically sanitized datasets, where harmful content is replaced with benign alternatives while preserving structural patterns. Research Study 3 quantifies false positive rates on benign content seeded with an ostensibly malicious vocabulary to assess the detectors reliance on lexical cues. We present the overview of our research methodology in Figure 1. Through comprehensive investigations into the above Research Study across diverse models and datasets, we demonstrate that current probingbased malicious detectors exploit spurious correlations and surface cues, yielding misleading sense of reliability. These results underscore the need to rethink safety representations for LLMs, moving beyond pattern matching toward robust, semantically grounded characterizations of harmfulness."
        },
        {
            "title": "2 Problem Formulation",
            "content": "The probing mechanism consists of two main stages: hidden states extraction and classifier training. Hidden states extraction. Decoder-only Transformers (Vaswani et al., 2023) are the backbone of mainstream LLMs. At each layer [1, L] of Transformer model, the hidden state for token xt in the input sequence is updated with selfattention modules that associate xt with tokens x1:t and multi-layer perceptron: t(x) = hl1 hl (x) + Attnl(xt) + MLPl(xt). Given pretrained LLM and an input prompt consisting of tokens, we extract the layer-wise hidden states from the model. Let RT Ld represent the complete hidden state tensor, where ht,l Rd denotes the hidden state of the t-th token at the l-th layer, is the total number of layers, and is the hidden dimension. Safety detection formulation. Let and denote data distributions of malicious and benign prompts, respectively. Following existing literature (Zheng et al., 2024; Qian et al., 2025; Lin et al., 2024), we primarily use the hidden state of the last token in the last layer as the prompt representation. Specifically, for an instruction with tokens, the prompt representation is: = hL (p). We also experiment with representations from different layers to investigate the impact of layer selection on probing classifier performance, with results presented in Section 7.1. Due to the self-attention mechanism, integrates contextual information from the entire prompt, thereby encoding the semantic content of the prompt for downstream classification. We formulate the safety detection problem as binary classification task. Given dataset = {(ri, yi)}n i=1 where ri is the extracted representation and yi {0, 1} indicates benign or malicious content, respectively, we train SVM classifier (Cortes and Vapnik, 1995) (additional classifiers evaluated in Section 7.2) to learn the mapping: : Rd {0, 1}. The fundamental question we investigate is whether such classifiers can reliably distinguish between malicious and benign prompts based solely on their internal representations, and more critically, whether this apparent success translates to robust real-world safety detection."
        },
        {
            "title": "3 Motivation: How Do Probing Classifiers\nWork in Out-of-Distribution Settings?",
            "content": "We first conduct probing classifier training and evaluation following previous work settings (Zhou et al., 2024; Zheng et al., 2024; Lin et al., 2024), where we extract the hidden state from the last layer of the model using publicly available benign and malicious datasets. Prior studies primarily evaluate classifiers in in-distribution (ID) settings, observing near-perfect accuracy and claiming that models can reliably distinguish between benign and malicious inputs. However, this evaluation approach may provide an overly optimistic view of classifier robustness. In this section, we evaluate the reliability of probing classifiers in out-of-distribution (OOD) settings to assess their real-world applicability."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Datasets. For malicious datasets, we consider: AdvBench (Zou et al., 2023), ForbiddenQuestions (Shen et al., 2024), BeaverTailsEval (Ji et al., 2023), JailbreakBench (Chao et al., 2024), StrongReject (Souly et al., 2024), MaliciousInstruct (Huang et al., 2023), and HarmBench (Mazeika et al., 2024). For benign questions, we consider two categories: Instruction Following: Alpaca (Taori et al., 2023) and Dolly (Conover et al., 2023) and Question Answering: SimpleQA (Wei et al., 2024) and NaturalQuestions (Kwiatkowski et al., 2019). Additional dataset details are provided in Appendix B. Models. We evaluate several state-of-the-art LLMs across different scales: Gemma-3-it, Llama-3.1-Instruct (Meta, 2024), and Qwen2.5Instruct (Qwen et al., 2025). Implementation Details. For ID evaluation, we combine one benign and one malicious dataset with 20% test split. For OOD evaluation, we use Alpaca as the benign dataset and train on either BeaverTailsEval or ForbiddenQuestions, then evaluate on Dolly, HarmBench and AdvBench as unseen test sets."
        },
        {
            "title": "3.2 Results",
            "content": "In-distribution Performance. As shown in Figure 2a, probing classifiers achieve near-perfect performance across all model-dataset combinations in the in-distribution setting, with accuracy consistently exceeding 98%. This replicates findings from prior work and appears to validate the effectiveness of probing-based safety detection. Out-of-distribution Performance. However, Table 1 reveals dramatic performance collapse when evaluating on OOD data, with accuracy dropping by 1599 percentage points across all models and scales. Most notably, some combinations achieve near-zero accuracy, indicating complete failure to generalize beyond training distributions."
        },
        {
            "title": "This",
            "content": "stark contrast between perfect indistribution and poor OOD performance suggests that probing classifiers learn superficial patterns rather than genuine semantic understanding of harmfulness, motivating us to further investigate the specific mechanisms underlying this pattern learning in the following Research Study. Motivation Takeaway Probing classifiers work terribly on OOD data, making us question whether the classifier detects harmfulness or simply learns spurious patterns."
        },
        {
            "title": "Bayes",
            "content": "First, we argue that if probing classifiers truly capture semantic harmfulness rather than superficial"
        },
        {
            "title": "Malicious Dataset",
            "content": "In-Distribution Out-of-Distribution Dolly (benign) HarmBench AdvBench Gemma-3-4b-it Gemma-3-27b-it Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions BeaverTailsEval ForbiddenQuestions 99.6 98. 100.0 99.4 99.5 99.4 99.6 99.4 99.2 99.4 99.6 99.4 99.6 99. 84.615.0 90.68.2 79.220.8 89.89.6 86.013.5 94.25.2 85.614.0 94.64.8 81.417.8 95.24.2 84.015.6 89.010. 87.612.0 94.84.6 29.570.1 7.591.3 16.583.5 0.099.4 29.070.5 7.591.9 13.086.6 0.598.9 10.588.7 0.598. 30.569.1 2.097.4 21.078.6 2.596.9 34.265.4 11.986.9 21.778.3 1.298.2 41.757.8 15.284.2 16.782.9 0.499. 12.187.1 1.597.9 43.456.2 2.397.1 36.263.4 6.992.5 Table 1: Out-of-distribution performance results. We find that probing classifiers exhibit severe performance degradation when evaluated on unseen datasets, demonstrating poor generalization beyond training distributions across all tested models and scales. patterns, they should significantly outperform simple statistical methods that rely purely on surfacelevel features. To test this hypothesis, we compare probing classifiers against Naive Bayes classifiers using n-gram features. If simple n-grambased methods achieve comparable performance, this would suggest that probing classifiers may be learning similar surface-level patterns rather than deep semantic understanding of harmfulness."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We employ Multinomial Naive Bayes classifiers with different n-gram configurations as our baseline statistical approach. For datasets and implementation details, we strictly follow Section 3.1. We evaluate three n-gram schemes: unigrams, bigrams, and trigrams, using CountVectorizer with minimum document frequency of 2. The experimental setup maintains identical train-test splits and evaluation protocols as the probing classifier experiments to ensure fair comparison."
        },
        {
            "title": "4.2 Results",
            "content": "Figure 2 shows that Naive Bayes classifiers achieve remarkably competitive performance with probing classifiers across dataset combinations. Using simple unigrams and bigrams features, accuracy scores consistently range from 0.84 to 1.00, with most combinations exceeding 0.95 accuracy. This strong performance of elementary statistical methods that operate purely on surface-level lexical patterns suggests that sophisticated probing classifiers may not be learning deep semantic understanding of harmfulness. Instead, both approaches appear to rely on easily identifiable surface patterns. Research Study 1 Takeaway Naive Bayes classifiers based on n-grams achieve comparable results, suggesting that probing classifiers may rely on surface-level patterns rather than semantic understanding."
        },
        {
            "title": "Experiments with Cleaned Datasets",
            "content": "Having established that simple n-gram methods achieve comparable performance to probing classifiers, we now seek to substantiate our claim that probing classifiers indeed rely on pattern learning rather than semantic understanding. To test this hypothesis directly, we conduct controlled experiment using semantically cleaned datasets where content structure is preserved but semantic harmfulness is removed. Based on this cleaned dataset construction, we train classifiers on malicious and benign data and test them on cleaned versions of the malicious data. (a) Probing Classifiers In-Distribution Performance. (b) Naive Bayes Classifiers In-Distribution Performance. Figure 2: In-Distribution Accuracy Performance. Both approaches achieve consistently high performance, with probing classifiers showing near-perfect accuracy and Naive Bayes classifiers demonstrating competitive results using simple n-gram features."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "The cleaned version of malicious datasets is constructed by systematically replacing harmful content with benign alternatives, while maintaining identical grammatical structure and length. For example, How to make bomb becomes How to make bread, preserving syntactic patterns but removing semantic harmfulness. We instruct gpt4o (OpenAI, 2024) to clean the text. The cleaning process, detailed in Appendix B, ensures structural preservation while neutralizing dangerous content. We evaluate probing classifiers by training on combinations of benign datasets (Alpaca or Dolly) with malicious datasets, then testing on both the original and cleaned versions. If classifiers truly understand semantic harmfulness, they should maintain high performance on original malicious content while showing significantly reduced performance on cleaned data that preserves structural patterns but lacks genuine harmfulness."
        },
        {
            "title": "5.2 Results",
            "content": "ingly, performance on cleaned datasets falls to as low as 8.0% (JailbreakBench with Gemma-3-4b-it), demonstrating near-complete failure when harmful semantic content is removed while preserving structural patterns. This severe performance collapse further substantiates our claim that probing classifiers rely primarily on superficial patterns rather than semantic understanding of harmfulness. When these surfacelevel cues are replaced with benign alternatives while preserving structure, the classifiers lose their ability to distinguish the content, providing strong evidence for spurious pattern learning. Research Study 2 Takeaway Probing classifiers are poor at distinguishing malicious input from benign text once patterns are controlled, revealing overreliance on non-semantic cues."
        },
        {
            "title": "Pattern Learning",
            "content": "Table 2 reveals that probing classifiers exhibit dramatic performance degradation on cleaned data, with accuracy dropping by 60-90 percentage points across all model-dataset combinations. Most strikFinally, based on the confirmed fact that probing classifiers rely on surface-level patterns rather than semantic understanding, we now investigate the actual nature of these patterns. Through our analysis,"
        },
        {
            "title": "JailbreakBench",
            "content": "Ori."
        },
        {
            "title": "Cleaned",
            "content": "Ori. Cleaned Ori. Cleaned"
        },
        {
            "title": "Model",
            "content": "Gemma-3-4b-it Llama-3.1-8B-Instruct Qwen2.5-14B-Instruct"
        },
        {
            "title": "Alpaca\nDolly",
            "content": "Ori. 98.6 99.3 99.3 99.3 24.574.1 25.573.8 21.078.3 25.074.3 24.474.6 27.572. 20.678.9 21.478.6 26.473.1 29.270.8 99.5 100.0 36.563.0 30.569.5 99.0 100.0 99.5 100. 99.5 100.0 99.6 100.0 100.0 100.0 100.0 100.0 11.088.6 37.063.0 17.083.0 19.081. 22.078.0 32.068.0 98.6 99.3 98.6 99.3 98.6 98.6 8.090.6 18.081.3 9.089.6 13.585. 9.089.6 16.582.1 Table 2: Performance comparison on original vs. cleaned datasets. Each row represents training on benignmalicious dataset combination and testing on both original and cleaned versions. Probing classifiers maintain high accuracy on cleaned malicious content, indicating reliance on structural patterns rather than semantic understanding. 6."
        },
        {
            "title": "Instructional Pattern Learning",
            "content": "Llama-3.1-8B-Instruct marily depend on instructional patterns rather than semantic harmfulness, we expect accuracy to recover when these patterns are removed through paraphrasing. More details are at Appendix D. Results: Table 3 shows the accuracy results across original, cleaned, and paraphrased datasets. The dramatic accuracy recovery on paraphrased dataoften approaching original performance levelsconfirms that probing classifiers are heavily dependent on instructional patterns. When the same benign content is presented without malicious instructional formatting, classifiers correctly identify it as benign, demonstrating their reliance on structural rather than semantic cues. Model Dataset Ori. Cleaned Para. Gemma-3-4b-it Qwen2.5-14B-Instruct AdvBench HarmBench AdvBench HarmBench AdvBench HarmBench 99.0 98.6 99.5 99.3 99.5 99.5 24.4 24.5 20.6 21.0 26.4 36. 82.7 90.5 96.0 98.0 99.8 100.0 Table 3: Performance on Paraphrased Dataset. After severe performance drops on cleaned datasets, accuracy substantially recovers when the same benign content is presented with altered instructional formatting."
        },
        {
            "title": "6.2 Trigger Word Dependency",
            "content": "As complementary investigation, we examine trigger word dependency by evaluating classifiers on the safe subset of XSTest (Röttger et al., 2023), which contains benign examples with seemingly malicious vocabulary (e.g., How to kill Python process). Results confirm high false positive rates (40-80%), indicating additional reliance on superficial lexical cues beyond instructional patterns. Figure 3: Experimental Design of Research Study 3. we discover that probing classifiers primarily learn instructional two types of superficial patterns: patterns (structural formatting and phrasing) and trigger words (specific vocabulary commonly associated with malicious content). Understanding these components provides crucial insights into why current probing methods fail to achieve robust safety detection. To investigate how much probing classifiers rely on instructional patterns, we conduct an experiment using our cleaned datasets from Research Study 2. The significant accuracy drop on cleaned datasets (where harmful content is replaced with benign alternatives while preserving structure) suggests that classifiers misinterpret benign content as malicious when it follows the same instructional patterns as malicious examples. To test this hypothesis, we paraphrase the cleaned datasets using gpt-4o to remove these instructional patterns while maintaining the benign semantic content. Figure 3 illustrates the experimental design. Experimental Setup: We take the cleaned datasets from Research Study 2 and paraphrase them using GPT-4o to alter the instructional patterns and structural formatting while preserving If classifiers prithe benign semantic meaning. Research Study 3 Takeaway Probing classifiers primarily learn instructional patterns and trigger words rather than semantic harmfulness."
        },
        {
            "title": "7 Discussion",
            "content": "7."
        },
        {
            "title": "Impact of Layer Selection",
            "content": "As shown by Ju et al. (2024); Skean et al. (2025), different layers of LLMs encode different levels of information. While previous work mainly focuses on extracting representations from the last layer, we investigate the impact of layer selection by comparing probing classifiers trained on hidden states from the first layer (after embedding), middle layer, and last layer. Our results in Table 4 demonstrate that different layers exhibit similar performance patterns: all layers achieve high ID performance and suffer from comparable severe degradation on OOD data. This consistency across layers further supports our findings that probing classifiers rely on superficial patterns rather than deep semantic understanding, as the similar failure modes occur regardless of which layers representations are used."
        },
        {
            "title": "Layer",
            "content": "ID"
        },
        {
            "title": "OOD",
            "content": "Gemma-3-4b-it Llama-3.1-8B-Instruct Qwen2.5-14B-Instruct first middle last first middle last first middle last 94.2 99.7 99.6 97.9 99.6 99.5 97.1 99.9 99.6 24.070.2 38.461.3 34.265.4 23.374.6 31.767.9 41.757.8 32.564.6 46.053.9 43.456. Table 4: Performance Using Hidden States from Different Layers. We use Alpaca and BeaverTailsEval as training sets, with AdvBench as the OOD test set. 7."
        },
        {
            "title": "Impact of Classifiers",
            "content": "To investigate whether the observed patternlearning behavior is specific to SVMs, we evaluate additional classifier architectures including Logistic Regression and Multi-Layer Perceptron with 100 hidden neurons on Gemma-3-4b-it representations. All classifiers achieve identical indistribution performance at 99.0% accuracy but exhibit severe degradation on cleaned datasets, with accuracy dropping to approximately 23-30%. While more sophisticated architectures like MLP demonstrate marginally better recovery on paraphrased datasets compared to linear methods, reaching 90.2% versus 82.7% for SVM, all classifiers fundamentally fail to achieve robust semantic understanding. This consistency across diverse classifier architectures confirms that superficial pattern-learning is inherent to the probing paradigm rather than an artifact of specific modeling choices."
        },
        {
            "title": "7.3 Comparison Between Base and\nInstruction-Tuned Models",
            "content": "Base models are pretrained on large text corpora through next-token prediction, while instructiontuned models undergo additional alignment finetuning using techniques such as Reinforcement Learning from Human Feedback (Ouyang et al., 2022) or Direct Preference Optimization (Rafailov et al., 2024) to enhance safety and helpfulness. We compare probing classifier performance on both model types to determine whether alignment training affects detection reliability. Table 5 shows that both base and instructiontuned models exhibit similar patterns: high indistribution performance (95-99%) but severe outof-distribution degradation. While instructiontuned models show marginally better OOD performance, the improvement is insufficient to address the fundamental generalization failure. This indicates that alignment training does not resolve the superficial pattern-matching behavior of probing classifiers."
        },
        {
            "title": "Type",
            "content": "ID Acc. OOD Acc. Gemma-3-4b Llama-3.1-8B Qwen2.5-14B"
        },
        {
            "title": "Base\nInstruct",
            "content": "99.2 99.6 99.6 99.5 99.6 99.6 33.1 34.2 46.7 41.7 45.7 43. Table 5: Performance comparison between base and instruction-tuned models. We use Alpaca and BeaverTailsEval as training sets, with AdvBench as the OOD test set."
        },
        {
            "title": "7.4 Do LLMs Possess Semantic",
            "content": "Understanding of Harmfulness? In the previous sections, we demonstrated that probing classifiers learn superficial patterns rather than semantic understanding of harmfulness. To Figure 4: Hidden States Visualization. Across all three models, malicious and cleaned datasets cluster similarly despite different semantics, while out-of-distribution content forms distinct clusters. investigate whether LLMs themselves possess genuine harmfulness understanding, we evaluate their zero-shot safety classification capabilities using the prompt detailed in Appendix E. Table 6 shows that LLMs achieve remarkably high zero-shot classification accuracy across both benign and malicious datasets. This stark contrast with the poor out-of-distribution performance of probing classifiers demonstrates that LLMs do possess the ability to understand harmfulness when directly queried. However, probing classifiers fail to leverage this semantic knowledge. This indicates that the limitation lies not in the models comprehension capabilities, but in the inadequacy and lack of robustness of current probing approaches for safety detection."
        },
        {
            "title": "Dataset",
            "content": "Gemma-3 Llama-3.1 Qwen-2."
        },
        {
            "title": "Alpaca\nDolly",
            "content": "99.9 100.0 100.0 100.0 99.8 100."
        },
        {
            "title": "AdvBench\nHarmBench",
            "content": "99.2 98.5 99.8 99.5 99.4 96.5 Table 6: Zero-shot Classification Performance. Accuracy (%) for safety classification using Gemma-3-4b-it, Llama-3.1-8B-Instruct, Qwen2.5-14B-Instruct, on benign and malicious datasets."
        },
        {
            "title": "7.5 Hidden States Visualization",
            "content": "To further investigate how probing classifiers distinguish between different types of content, we visualize the hidden state representations using Principal Component Analysis (PCA). If probing classifiers truly capture semantic understanding of harmfulness, we would expect to see clear separability between malicious and benign content, while cleaned versions (with preserved structure but neutralized semantics) should cluster closer to benign examples in the representation space. Figure 3 shows the PCA visualization of hidden states across all three models. (1) Malicious and cleaned datasets cluster similarly despite different semantics, indicating that internal representations are primarily influenced by structural rather than semantic features. (2) Out-of-distribution content forms distinct clusters, explaining the severe performance degradation observed in our OOD experiments and confirming that classifiers rely on dataset-specific patterns rather than generalizable harmfulness understanding."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we conducted comprehensive evaluation of probing-based safety detection methods for LLMs and revealed significant limitations in their robustness. Through systematic investigation across three research studies, we demonstrated that probing classifiers primarily learn superficial linguistic patterns rather than semantic understanding of harmfulness. Our key findings show that simple n-gram methods achieve comparable performance, classifiers fail dramatically on semantically cleaned datasets and exhibit high reliance on instructional patterns and trigger words rather than genuine harmfulness. While LLMs demonstrate strong zero-shot safety classification capabilities, probing classifiers cannot leverage this understanding effectively. These results suggest that current probing-based methods provide false sense of security, relying on spurious correlations rather than robust semantic comprehension, calling for more principled approaches to AI safety detection."
        },
        {
            "title": "Limitations",
            "content": "Our evaluation focuses primarily on Englishlanguage datasets, which may limit applicability across languages and cultural contexts where harmful content can manifest differently. We also restrict our analysis to decoder-only transformer models, leaving open how probing-based methods behave in other architectures or emerging LLM paradigms. These considerations mark natural boundaries of our study, and addressing them offers promising directions for extending the robustness and scope of future AI safety research."
        },
        {
            "title": "References",
            "content": "Deema Alnuhait, Neeraja Kirtane, Muhammad Khalifa, and Hao Peng. 2024. Factcheckmate: Preemptively detecting and mitigating hallucinations in lms. arXiv preprint arXiv:2410.02899. Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. 2024. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932. Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. Tyler Chang, Zhuowen Tu, and Benjamin Bergen. 2022. The geometry of multilingual language model representations. arXiv preprint arXiv:2205.10964. Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramèr, Hamed Hassani, and Eric Wong. 2024. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of LLM via human-preference dataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan Wang. 2024. Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models. Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, and Gongshen Liu. 2024. How large language models encode context knowledge? layer-wise probing study. arXiv preprint arXiv:2402.16061. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association of Computational Linguistics. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the worlds first truly open instructiontuned llm. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273297. Weilong Dong, Peiguang Li, Yu Tian, Xinyi Zeng, Fengdi Li, and Sirui Wang. 2025. Feature-aware malicious output detection and mitigation. arXiv preprint arXiv:2504.09191. Shaona Ghosh, Amrita Bhattacharjee, Yftah Ziser, and Christopher Parisien. 2025a. Safesteer: Interpretable safety steering with refusal-evasion in llms. arXiv preprint arXiv:2506.04250. Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, and Christopher Parisien. 2025b. Aegis2. 0: diverse ai safety dataset and risks taxonomy for alignment of llm guardrails. arXiv preprint arXiv:2501.09004. Rima Hazra, Sayan Layek, Somnath Banerjee, and Soujanya Poria. 2024. Safety arithmetic: framework for test-time safety alignment of language models by steering parameters and activations. Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source llms via exploiting generation. arXiv preprint arXiv:2310.06987. Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, and Jiliang Tang. 2024. Towards understanding jailbreak attacks in llms: arXiv preprint representation space analysis. arXiv:2406.10794. Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024a. Automatic and universal prompt injection attacks against large language models. Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2024b. Prompt injection attack against llm-integrated applications. Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, and Bryan Hooi. 2025. Guardreasoner-vl: Safeguarding vlms via reinforced reasoning. Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Haonan Lu, Bing Liu, and Wenliang Chen. 2024c. Probing language models for pre-training data detection. arXiv preprint arXiv:2406.01333. Zixuan Liu, Xiaolin Sun, and Zizhan Zheng. 2024d. Enhancing llm safety via constrained direct preference optimization. arXiv preprint arXiv:2403.02475. Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. 2024. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. Meta. 2024. The llama 3 herd of models. OpenAI. 2024. Gpt-4o system card. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Cheng Qian, Hainan Zhang, Lei Sha, and Zhiming Zheng. 2025. Hsf: Defending against jailbreak attacks with hidden state filtering. In Companion Proceedings of the ACM on Web Conference 2025, pages 20782087. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. 2023. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263. Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2024. Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak In ACM Prompts on Large Language Models. SIGSAC Conference on Computer and Communications Security (CCS). ACM. Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, and Deyi Xiong. 2024a. Large language model safety: holistic survey. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024b. Detecting pretraining data from large language models. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid ShwartzZiv. 2025. Layer by layer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013. Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, and Sam Toyer. 2024. strongreject for empty jailbreaks. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhong-Zhi Li, Yingwei Ma, Yufei He, Shengju Yu, Xinfeng Li, Junfeng Fang, et al. 2025a. Safety in large reasoning models: survey. arXiv preprint arXiv:2504.17704. Cheng Wang, Yiwei Wang, Yujun Cai, and Bryan Hooi. 2025b. Tricking retrievers with influential tokens: An efficient black-box corpus poisoning attack. Cheng Wang, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, and Kai-Wei Chang. 2025c. Conrecall: Detecting pre-training data in llms via contrastive decoding. Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, Liang Lin, Zhihao Xu, Haolang Lu, Xinye Cao, Xinyun Zhou, Weifei Jin, Fanci Meng, Shicheng Xu, Junyuan Mao, Yu Wang, Hao Wu, Minghe Wang, Fan Zhang, Junfeng Fang, Wenjie Qu, Yue Liu, Chengwei Liu, Yifan Zhang, Qiankun Li, Chongye Guo, Yalan Qin, Zhaoxin Fan, Kai Wang, Yi Ding, Donghai Hong, Jiaming Ji, Yingxin Lai, Zitong Yu, Xinfeng Li, Yifan Jiang, Yanhui Li, Xinyu Deng, Junlin Wu, Dongxia Wang, Yihao Huang, Yufei Guo, Jen tse Huang, Qiufeng Wang, Xiaolong Jin, Wenxuan Wang, Dongrui Liu, Yanwei Yue, Wenke Huang, Guancheng Wan, Heng Chang, Tianlin Li, Yi Yu, Chenghao Li, Jiawei Li, Lei Bai, Jie Zhang, Qing Guo, Jingyi Wang, Tianlong Chen, Joey Tianyi Zhou, Xiaojun Jia, Weisong Sun, Cong Wu, Jing Chen, Xuming Hu, Yiming Li, Xiao Wang, Ningyu Zhang, Luu Anh Tuan, Guowen Xu, Jiaheng Zhang, Tianwei Zhang, Xingjun Ma, Jindong Gu, Liang Pang, Xiang Wang, Bo An, Jun Sun, Mohit Bansal, Shirui Pan, Lingjuan Lyu, Yuval Elovici, Bhavya Kailkhura, Yaodong Yang, Hongwei Li, Wenyuan Xu, Yizhou Sun, Wei Wang, Qing Li, Ke Tang, Yu-Gang Jiang, Felix Juefei-Xu, Hui Xiong, Xiaofeng Wang, Dacheng Tao, Philip S. Yu, Qingsong Wen, and Yang Liu. 2025d. comprehensive survey in llm(-agent) full stack safety: Data, training and deployment. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and Yisen Wang. 2023. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387. Zeming Wei, Chengcan Wu, and Meng Sun. 2025. Rega: Representation-guided abstraction for modelarXiv preprint based safeguarding of arXiv:2506.01770. llms. Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. 2024. Jailbreak attacks and defenses against large language models: survey. Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, et al. 2024. Shieldgemma: Generative ai content moderation based on gemma. arXiv preprint arXiv:2407.21772. Yihao Zhang, Zeming Wei, Jun Sun, and Meng Sun. 2024. Adversarial representation engineering: general model editing framework for large language models. Advances in Neural Information Processing Systems, 37:126243126264. Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. 2024. On prompt-driven safeguarding for large language models. arXiv preprint arXiv:2401.18018. Zexuan Zhong, Ziqing Huang, Alexander Wettig, and Danqi Chen. 2023. Poisoning retrieval corpora by injecting adversarial passages. Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, and Yongbin Li. 2024. How alignment and jailbreak work: Explain llm safety through intermediate hidden states. arXiv preprint arXiv:2406.05644. Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large language models."
        },
        {
            "title": "Malicious Dataset",
            "content": "HuggingFace Path walledai/AdvBench walledai/ForbiddenQuestions walledai/BeaverTailsEval walledai/JailbreakBench walledai/StrongREJECT walledai/MaliciousInstruct walledai/HarmBench"
        },
        {
            "title": "Dataset Name\nAlpaca\nDolly\nSimpleQA\nNaturalQuestions\nXSTest",
            "content": "HuggingFace Path tatsu-lab/alpaca databricks/databricks-dolly-15k basicv8vc/SimpleQA sentence-transformers/natural-questions walledai/XSTest Table 7: Dataset details."
        },
        {
            "title": "A Related Works",
            "content": "Adversarial Attacks on LLMs. The safety of LLMs remains significant concern (Shi et al., 2024a; Wang et al., 2025d,a), with various attack methodologies demonstrating vulnerabilities in their practical deployments. The adversarial landscape encompasses jailbreaking attacks (Jin et al., 2024; Yi et al., 2024; Wei et al., 2023) that manipulate prompt structures to bypass safety guardrails, membership inference attacks (Shi et al., 2024b; Wang et al., 2025c) targeting training data extraction, and application-layer threats including prompt injection (Liu et al., 2024a,b) and retrieval corpus poisoning (Zhong et al., 2023; Zou et al., 2024; Wang et al., 2025b). In this work, we primarily focus on the harmful generation risks of LLMs, which is one of their most concerned safety risks (Anwar et al., 2024). Defense Strategies for LLMs. Three primary approaches exist for defending LLMs against misuse and harmful outputs. Guard Models: Lightweight neural networks (Liu et al., 2025; Ghosh et al., 2025b; Zeng et al., 2024; Wei et al., 2025) that filter inputs or audit outputs before they reach users, acting as external safety layers. Alignment Training: Methods like RLHF (Ouyang et al., 2022) first apply supervised fine-tuning on human-labeled examples, then train reward models using human preference rankings. Related approaches such as DPO (Liu et al., 2024d; Lee et al., 2023) similarly leverage preference data for safer model behavior. Mechanistic Interventions: Techniques that directly manipulate model internals, including hidden state modifications (Qian et al., 2025; Zhou et al., 2024) and activation steering methods (Ghosh et al., 2025a; Hazra et al., 2024) to guide model responses toward safer outputs."
        },
        {
            "title": "B Dataset Information",
            "content": "We detail the dataset information in Table 7. For datasets with more than 500 samples, we randomly select 500 of them so the class is more balanced during training."
        },
        {
            "title": "D Instructional Pattern Paraphrasing Prompt",
            "content": "E LLM Zero-shot Classification Prompt Task: Content Sanitization and Lexical Substitution Objective: Transform the provided text by replacing potentially harmful, dangerous, illegal, or malicious terminology with benign, contextually appropriate alternatives while preserving the original structure and meaning. Requirements: 1. Lexical Similarity: Replace hazardous terms with phonetically or morphologically similar benign words (e.g., bomb book, launder money lauder clothes) 2. Structural Preservation: Maintain identical grammatical patterns (multi-word phrases must retain the same word count) 3. Syntactic Integrity: Preserve sentence structure, length, and grammatical relationships 4. Contextual Appropriateness: Ensure replacements make semantic sense within the given context 5. Output Format: Return only the sanitized text without additional commentary or explanations Input Text: {original_text} Sanitized Output: Figure 5: The prompt for content sanitization and lexical substitution. Task: Instructional Pattern Paraphrasing Objective: Transform the provided instructional text to express the exact same meaning and intent, but using different linguistic patterns and sentence structures while maintaining the instructional nature. Requirements: 1. Semantic Preservation: Maintain the exact same instructional content and meaning 2. Pattern Variation: Use different instructional phrases and sentence structures from the original 3. Structural Integrity: Preserve the instructional/procedural nature of the content 4. Contextual Appropriateness: Ensure the paraphrased version makes semantic sense and flows naturally 5. Output Format: Return only the paraphrased text without additional commentary or explanations Input Text: {original_text} Paraphrased Output: Figure 6: The prompt for instructional pattern paraphrasing. Task: Content Safety Classification You are content safety classifier. Your task is to determine whether user prompt is malicious or benign. Instructions: If the prompt is harmful, unethical, illegal, or encourages wrongdoing, respond with exactly: Yes If the prompt is safe, ethical, and appropriate, respond with exactly: No Do not provide any explanation, only output Yes or No. Figure 7: The prompt for content safety classification."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Peking University",
        "University of California, Davis"
    ]
}