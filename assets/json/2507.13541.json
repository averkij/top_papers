{
    "paper_title": "PrefPalette: Personalized Preference Modeling with Latent Attributes",
    "authors": [
        "Shuyue Stella Li",
        "Melanie Sclar",
        "Hunter Lang",
        "Ansong Ni",
        "Jacqueline He",
        "Puxin Xu",
        "Andrew Cohen",
        "Chan Young Park",
        "Yulia Tsvetkov",
        "Asli Celikyilmaz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferences - yet current preference models typically treat human judgment as a black box. We introduce PrefPalette, a framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in a human-interpretable manner. PrefPalette operationalizes a cognitive science principle known as multi-attribute decision making in two ways: (1) a scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling and transparent, interpretable insights, and serves as a first step toward more trustworthy, value-aware personalized applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 4 5 3 1 . 7 0 5 2 : r PrefPalette: Personalized Preference Modeling with Latent Attributes Shuyue Stella Li1,2, Melanie Sclar1,2, Hunter Lang3, Ansong Ni1, Jacqueline He2, Puxin Xu3, Andrew Cohen3, Chan Young Park2, Yulia Tsvetkov2, Asli Celikyilmaz1 1Meta FAIR, 2University of Washington, 3Meta GenAI Personalizing AI systems requires understanding not just what users prefer, but the reasons that underlie those preferencesyet current preference models typically treat human judgment as black box. We introduce PrefPalette, framework that decomposes preferences into attribute dimensions and tailors its preference prediction to distinct social community values in human-interpretable manner. PrefPalette operationalizes cognitive science principle known as multi-attribute decision making in two ways: (1) scalable counterfactual attribute synthesis step that involves generating synthetic training data to isolate for individual attribute effects (e.g., formality, humor, cultural values), and (2) attention-based preference modeling that learns how different social communities dynamically weight these attributes. This approach moves beyond aggregate preference modeling to capture the diverse evaluation frameworks that drive human judgment. When evaluated on 45 social communities from the online platform Reddit, PrefPalette outperforms GPT-4o by 46.6% in average prediction accuracy. Beyond raw predictive improvements, PrefPalette also shed light on intuitive, community-specific profiles: scholarly communities prioritize verbosity and stimulation, conflict-oriented communities value sarcasm and directness, and support-based communities emphasize empathy. By modeling the attribute-mediated structure of human judgment, PrefPalette delivers both superior preference modeling and transparent, interpretable insights, and serves as first step toward more trustworthy, value-aware personalized applications. Date: July 21, 2025 Correspondence: stelli@cs.washington.edu Code: https://github.com/stellalisy/PrefPalette"
        },
        {
            "title": "1 Introduction",
            "content": "Accurately modeling human preferences remains fundamental challenge that resides at the intersection of artificial intelligence, cognitive science, and human-computer interaction (Doyle, 2004). As language models (LMs) are increasingly deployed within nuanced and complex social contexts, the precise prediction of human judgments is crucial for model development and trustworthy personalization. In this work, we define preference modeling as the task of predicting evaluative judgments over content. In the context of machine learning, modern preference modeling techniques have typically originated as components within recommendation systems or alignment pipelines (Schafer et al., 2007; Christiano et al., 2017; Ouyang et al., 2022). While such approaches perform adequately under controlled settings, they struggle in real-world contexts that require nuanced social understanding. Our work addresses this gap with PrefPalette, framework that explicitly models the attribute-mediated cognitive processes underlying human preference judgments, demonstrating improvements in prediction accuracy and interpretability across diverse social domains. Despite its central role in personalization and alignment, preference modeling has received limited attention as standalone research direction. Instead, most work embeds preference learning as component within RLHF pipelines without accounting for the cognitive mechanisms that govern human preference formation. Moreover, these methods rely on explicit, self-reported annotations(Christiano et al., 2017; Ouyang et al., 2022), suffer from social-desirability bias (Furnham, 1986; Paulhus, 1991) and fail to capture actual preferences from unobtrusive, real-world signals (e.g., engagement metrics). For instance, as Figure 1 (left) shows, annotators may opt for the more respectable, cultured\" option over the fun\" option due to response bias. 1 Figure 1 Comparison between conventional direct preference modeling (left) and PrefPalette, our attribute-mediated preference modeling approach (right). Attribute-mediated preference modeling better aligns with human cognitive evaluation processes by incorporating explicit attribute dimensions as interpretable intermediaries between content and preference. To mitigate annotation bias, some studies incorporate auxiliary information signals such as online engagement metrics, temporal metadata (Park et al., 2024a), or explicit user identifiers (Kumar et al., 2024). Such attempts still operate within direct content-to-preference paradigm and overlook the attribute-mediated structure documented in cognitive science. Another thread of research is multi-objective reward modeling, which involves decomposing rewards into interpretable dimensions (e.g., helpfulness, coherence, verbosity) and recombining them (Li et al., 2025a; Zhou et al., 2023; Wang et al., 2024a,b; Zhou et al., 2024). Yet by relying on broad, pre-annotated scalar ratings (Wang et al., 2024a,b), these approaches miss the more granular, latent attributessuch as core human valuesthat fundamentally drive preference judgments (Schwartz, 1992). We propose to tackle this challenge by leveraging insights from cognitive science research, which has established that human preference formation follows process known as multi-attribute decision making. In multi-attribute decision making, individuals decompose options into distinct attribute dimensions that serve as evaluation criteria (Slovic, 1995; Bettman et al., 1998; Kahneman, 2011). These attributes are dynamically weighted based on context (Fischer et al., 1999; Hsee et al., 2003). Here, we define attributes as interpretable characteristics of content that influence human evaluationsuch as the formality of language, the presence of humor, or the expression of cultural values such as empathy or achievement. These attributes are latent because they are not explicitly labeled in preference data; instead, they must be inferred from the content itself. For example, humor is highly valued in stand-up comedy but less so in news reporting. This multi-stage process creates an internal evaluative structure that cannot be discerned from final preference judgments alone. Current preference modeling approaches bypass this cognitive architecture, missing the underlying evaluative processes that are formative to human preferences (Stiennon et al., 2020). To this end, we introduce Preference Palette (PrefPalette), preference modeling framework that is grounded on the cognitively motivated structures that underlie human evaluative judgment and leverages unobtrusive preference signals from naturalistic interactions (Figure 1). Our approach is the first to bridge fundamental gap between black-box preference modeling systems and multi-attribute decision making. Specifically, PrefPalette employs two-stage process: (1) attribute representation learning, which involves generating scalable counterfactual data to train specialized attribute predictors, and (2) attribute-mediated preference modeling, which involves learning context-dependent attribute weights and incorporating them into preference models. We focus on two broad attribute categories that influence human preferences: 9 common sociolinguistic norms (e.g., toxicity, politeness, humor) that capture communication style (HernándezCampoy, 2016; Lapinski and Rimal, 2005), and 10 Schwartz value dimensions (Schwartz, 2012) that represent fundamental human values (e.g., benevolence, conformity). We instantiate PrefPalette on Reddit (Baumgartner et al., 2020) data, which is also one of the largest and most accessible open repositories of naturally-arising, community-specific interactions. The individual communities on Reddit, known as subreddits, offer distinct social niches with sharply differing norms, creating rich testbed for model robustness. PrefPalette outperforms GPT-4o by 46.6% across 45 diverse social contexts in preference accuracy and demonstrates strong temporal robustness. Beyond predictive improvements, PrefPalette also reveals intuitive community preference patterns (e.g., r/AskHistorians values verbosity, and r/MaliciousCompliance values sarcasm). Human evaluation validates that our PrefPalette models 2 Figure 2 PrefPalette framework overview. In this framework, we first train specialized attribute predictors using counterfactual data, then integrate their hidden representations via attention mechanisms to predict preferences, while learning context-dependent attribute importance weights. attention weights accurately reflect attribute-preference relationships; high-importance attributes show positive correlations with community preferences, while low-importance attributes show near-zero correlations. Our contributions include: computational framework that explicitly incorporates attribute-mediated evaluation processes into preference modeling, aligning AI systems with cognitive science theories of human judgment; counterfactual knowledge distillation technique that efficiently teaches attribute understanding capabilities from larger models to small, specialized predictors; Empirical evidence that shows superior preference prediction accuracy and temporal robustness across diverse social contexts compared to state-of-the-art baselines; Behavioral interpretability through per-instance, per-community attention weights, that reveals which sociolinguistic norms and cultural values govern preferences and facilitate transparent personalization."
        },
        {
            "title": "2 The Preference Palette Framework",
            "content": "PrefPalette combines preference modeling with multi-dimensional attribute structures adapted from cognitive science (Slovic, 1995; Bettman et al., 1998). As Figure 2 shows, given an instruction-response pair (x, y) and attributes, PrefPalette comprises: (1) attribute representation learningtraining specialized predictors for each attribute dimension (2.1), and (2) attribute-mediated preference modelingintegrating attribute embeddings with content representations via context attention to predict preferences (2.2). This architecture enables selective attention to context-relevant attribute dimensions and operationalizes the attribute-mediated nature of human preference judgment."
        },
        {
            "title": "2.1 Attribute Representation Learning\nFor each attribute a and response y, we can model the attribute intensity Aa(y) using a Bradley-Terry\nmodel (Bradley and Terry, 1952) for pairwise comparison. To acquire training data, we propose a novel\ncounterfactual attribute synthesis technique to generate counterfactual variations along the specified attribute\ndimension using a strong teacher model. This approach generates pairwise data to train small specialized\nattribute predictors for each attribute via contrastive attribute distillation (Figure 3), instead of relying on\neither real or synthetic scalar annotation, as used by prior work (Wang et al., 2024a,b).",
            "content": "Counterfactual Attribute Synthesis Traditional approaches to learning nuanced latent attributes face significant methodological limitations: (i) naturally occurring texts contain confounding variables via attribute correlations (Shah et al., 2019; Blodgett et al., 2020), (ii) inter-annotator inconsistency may lead to unreliable judgments (Sap et al., 2022; Mendelsohn and Budak, 2025), (iii) ambiguity in standard scalar ratings may introduce arbitrary granularities (Schuman and Presser, 1996; Schwarz, 1999), and (iv) distributional skew limits exposure to diverse attribute configurations (Paullada et al., 2021; Smith et al., 2023). Our proposed counterfactual attribute synthesis addresses all these challenges via controlled approach that replaces absolute scalar ratings with generative approach that systematically isolates attribute dimensions, while preserving semantic consistency. 3 Figure 3 Attribute predictor training pipeline. strong LM generator (Llama 3 405B) creates counterfactual variations of original content across attribute levels, generating paired training data where responses differ only along the target attribute dimension. The resulting data pairs enable contrastive attribute distillation, the training of small, specialized attribute predictors (Llama 3 1B) to distinguish attribute intensities. Formally, let s(y) represent the attribute-independent semantic content of response y. For an attribute and an intensity level L, we can generate counterfactual ya,l for that particular attribute level that preserves the original core semantic message: s(ya,l) s(y), Aa(ya,l) l, and Ak(ya,l) Ak(y) = i. Here, Aa(y) represents the intensity level of attribute in response (e.g., extreme formality), and Ak(y) represents the intensity level of any other attribute = in y. This isolates attribute effects while preserving semantic content, which, in practice, is nearly impossible to find in naturally occurring text. By explicitly controlling for confounding variables and enabling systematic coverage of the full attribute spaceincluding rare attributes underrepresented in common interactionsour approach establishes the ground truth through the generation process itself. For any counterfactual response pair (ya,l1 , ya,l2 ) where l1 < l2, note that ya,l2 , establishing scalable learning signal along the target attribute dimension. Our novel generative approach ensures that the pairwise differences only exist along the target attribute dimension. This synthetic data generation is efficient and cheap to scale without human annotations of attribute scores. exhibits higher level of attribute than ya,l1 Contrastive Attribute Distillation Using the counterfactual pairs generated by larger teacher model that possesses richer attribute understanding capabilities, we can establish contrastive knowledge distillation framework to train specialized, smaller attribute predictors. We distill to smaller specialized predictors for computational efficiency at inference time. For each attribute dimension a, the attribute function is Aa(y) = Sigmoid(ra(y)), where ra is reward model that outputs real-valued score and is trained on the counterfactual pairs using the following objective: Lattr = log σ(ra(ya,l2) ra(ya,l1 )). As Figure 3 shows, this approach enables the efficient distillation of nuanced attribute understanding from more powerful generative model to smaller attribute predictor, resulting in set of specialized models that can extract latent attributes from any response y. Overall, the attribute representation learning component provides scalable pipeline for extracting latent attributes that can be easily integrated into preference modeling pipelines."
        },
        {
            "title": "2.2 Attribute-Mediated Preference Modeling",
            "content": "The second stage of PrefPalette operationalizes the attribute-mediated decision process by integrating latent attributes into preference modeling. The main goal of this component is to learn context-aware importance weightings, α. The weighted attribute representation is augmented with the semantic representation via an aggregate function, , to compute final preference prediction. Given an instruction and response y, we model the preference function Ψ(x, y) as Ψ(x, y) = (x, y, {Aa(y)}n a=1), 4 i=1 where {Ai(y)}n represents the set of latent attribute representations computed by our specialized attribute predictors. We implement the aggregate function within Transformer-based architecture where the instruction and response are jointly encoded, and attribute dimensions are passed through self-attention mechanism that captures the context-specific importance of attributes. Our approach incorporates the attributes in latent space rather than using them as explicit prediction targets for the preference model. Attention-Based Attribute Integration We employ attention-based integration at the last hidden state of the last token: hintegrated = h(x,y) + Attn({hAa }n a=1), where h(x,y) represents the content encoding and hAa represents the attribute predictor hidden states. Unlike other two-stage preference prediction approaches (e.g., Wang et al., 2024a,b), PrefPalette uses attributes as supervision through the hidden representations of trained attribute predictors rather than explicit attribute scores. This design choice better matches the latent role of attributes in human cognitive decision-making processes, where attribute influences are implicit rather than explicitly computed. The attention mechanism, Attn(), computes importance weights α Rn for all attribute dimensions through parameterized function αi = , where ei = fattn(hAi, h(x,y)). (cid:80)n exp(ei) k=1 exp(ek) Using the importance weights α, we then compute the weighted sum of the attribute hidden states to obtain the final attribute representation.1 This formulation allows the model to adaptively prioritize different attributes in different contexts, similar to how humans may selectively weight attribute dimensions when evaluating different scenarios, and offers inherent interpretability of the predictive importance of each attribute. Gradual Feature Reduction The PrefPalette framework leverages the learned attribute representations at training time to guide preference modeling. PrefPalette eliminates the need for attribute predictors at inference time to enhance model generality and efficiency. To this end, we employ gradual feature reduction during training by stochastically masking attribute information with probability term λ that iteratively increases as training progresses: hintegrated = h(x,y) + β Attn({hAi}n i=1), β Bernoulli(1 λ), Gradual feature reduction encourages the preference model to internalize attribute-related patterns within its parameters, therefore enabling attribute-informed preference prediction even when explicit attribute signals are unavailable during deployment, while maintaining computational efficiency."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Datasets We define domain as distinct social context, operationalized as an online community centered around particular topic. We use subset of Reddit data collected from Pushshift.io in January 2023 (Baumgartner et al., 2020), which consists of 680 online communities, or subreddits, characterized by high user activity and diverse community norms.2 Following preprocessing protocols established in prior work (Park et al., 2024a), we sample up to 100,000 preference pairs per domain. We use all domains to train universal attribute predictors, and randomly sample 45 domains for evaluation (Appendix B). Attribute Selection Our attribute taxonomy encompasses 19 dimensions across sociolinguistic norms, and cultural values, two conceptual categories grounded in multi-attribute decision making. Sociolinguistic norms encompass stylistic dimensions that shape communicative effectiveness (Formality, Verbosity, Directness, Assertiveness) and interpersonal qualities that govern social acceptability (Supportiveness, Politeness, Sarcasm, Humor, Empathy). We also model cultural values grounded in Schwartzs theory of basic values (Schwartz, 2012), which identifies ten fundamental cross-cultural motivational goals: Self-Direction, Stimulation, Hedonism, Achievement, Power, Security, Conformity, Tradition, Benevolence, Universalism.3 1See Appendix for complete architectural details and choices of fattn. 2This is the last publicly available licensed Reddit dataset. 3Appendix provides further universal attribute predictor evaluation details. 5 While other attributes can be adapted to different domains, this particular attribute set captures both surfacelevel linguistic features and the deeper normative dimensions that are linked to preference formation (Tversky and Kahneman, 1981; Fischer et al., 1999). Importantly, note that PrefPalette remains attribute-agnostic: it provides generalizable framework for attribute-mediated preference modeling that is independent of the specific dimensions involved."
        },
        {
            "title": "3.1 Experiments",
            "content": "This section addresses the following questions: (1) Does attribute-mediated modeling improve preference prediction? (2) How do specific attributes contribute to improvements? (3) How robust is PrefPalette to temporal shifts? I. Comparative Performance Analysis. We first observe that PrefPalette meaningfully improves preference modeling compared to four baselines: GPT-4o-as-a-judge (Kumar et al., 2024; OpenAI et al., 2024), Dialog-RPT (Gao et al., 2020), ValueScope (Park et al., 2024a), and Direct Attribute Augmentation (PrefPalette-Score) across variety of domains. II. Mechanism Analysis. We examine the internal dynamics of PrefPalette by analyzing (i) attribute attention weights that reveal which dimensions most strongly influence preferences in different social contexts; (ii) attribute categories (linguistic norms, social norms, cultural values) that quantify their relative contribution to prediction accuracy; and (iii) integration strategies that determine the effect of attribute weights on end-task predictive performance. III. Robustness and Generalization. We evaluate the robustness of PrefPalette through temporally-shifted test sets on different domains, and show that attribute-mediated preference modeling exhibits stronger distributional robustness than baseline approaches. Baselines & Models We compare PrefPalette against the following baseline methods: directly prompting GPT-4o following Kumar et al. (2024) (as well as GPT-4o with chain-of-thought and o3-mini); Dialog-RPT (standard reward modeling using Bradley-Terry models trained from Llama 3 1B (Gao et al., 2020) with post content as instructions, and comments as responses); ValueScope (Park et al., 2024a) (which augments DialogRPT responses with temporal metadata); and PrefPalette-Scorea simplified version of PrefPalette that directly concatenates attribute scores with the response text, instead of using cross-attention integration.4 Evaluation Our main evaluation metric is preference accuracy: the proportion of test pairs where the model correctly predicts which comment received greater community approval, as measured by the net votes that the comment has received (computed as upvotes minus downvotes). For the interpretability analysis, we quantify the relative importance of attribute in domain Dk by averaging its learned weight across examples: αi(x, y). We also conduct qualitative case studies to show where baseline and = 1 Dk attribute-mediated predictions differ. (x,y)Dk (cid:80)"
        },
        {
            "title": "4.1 Preference Prediction Performance",
            "content": "We first show that PrefPalette outperforms existing preference modeling approaches and highlight few domains with distinctive preference patterns. Table 1 presents preference accuracy results that compare our approach against four representative baselines. PrefPalette consistently outperforms all baselines across the evaluated domains, with an average improvement of 1.4% over the SOTA baseline ValueScope (Park et al., 2024a), and 46.6% over GPT-4o. This comparative advantage demonstrates the effectiveness of attribute-mediated preference modeling. The performance difference between PrefPalette and PrefPalette-Score confirms that improvements stem 4Note that PrefPalette-Score is conceptually similar to ArmoRM (Wang et al., 2024a); both methods predict attribute dimension values before contextually combining them into scalar for pairwise comparison. We use PrefPalette-Score instead of ArmoRM for more controlled comparison to PrefPalette. 6 Table 1 Preference prediction accuracy (%) across select subreddit domains after training for three epochs over three random seeds. PrefPalette consistently outperforms both general preference models (GPT-4o, Dialog-RPT) and specialized approaches (ValueScope, PrefPalette-Score), with particularly strong performance in domains characterized by well-established community norms. Average prediction performance of additional SOTA models: GPT-4o CoT (57.8%), o3-mini (56.3%). Domain GPT-4o 56.4.13 r/unpopularopinion r/NoStupidQuestions 61.3.20 66.8.15 r/AskHistorians 62.4.20 r/explainlikeimfive 53.4.57 r/AskOuija r/ProgrammerHumor 55.6.27 59.1.38 r/confession Average (45 domains) 57.9.24 Temporal Robustness - Dialog-RPT 65.9.28 63.2.69 89.0.16 76.3.30 89.0.25 68.0.30 89.9.32 80.9.36 ValueScope 69.0.35 68.7.28 90.5.43 79.6.17 90.3.10 74.0.05 90.4.27 83.7.31 56.2.46 (-24.7%) 68.0.41 (-15.7%) PrefPalette-Score 69.4.29 68.7.28 91.0.39 79.5.32 90.4.23 74.8.48 90.5.67 84.0.38 68.0.44 (-16.0%) PrefPalette 70.2.08 69.4.06 91.6.11 80.2.30 90.6.18 75.7.66 91.8.27 84.9.23 69.3.30 (-15.6%) not only from attribute information, but also from the dynamic, context-sensitive weighting mechanism in latent space. Performance gains are most pronounced in domains with well-established community norms (r/AskHistorians: 91.6%, r/confession: 91.8%), supporting our hypothesis that attribute-mediated evaluation plays significant role in preference formation. These results quantitatively answer our first research question: attribute-mediated preference modeling demonstrably improves prediction accuracy across diverse social contexts. Qualitatively, we show examples where PrefPalette and ValueScope produce different predictions in Section 5."
        },
        {
            "title": "4.2 Examining Contextual Preference Dimensions’ Importance using PrefPalette",
            "content": "The PrefPalette framework provides interpretable attention weights over attributes, enabling direct examination of which dimensions influence preference judgments in specific contexts. Table 2 shows both the communities where specific attributes exert the strongest and weakest influence (top section), and the most and least influential attributes for selected communities (bottom section). Figure 4 shows the top dimensions in which three representative domains have the most variation. All together, these quantitative patterns reveal distinct evaluative frameworks that align with community purposes and established norms. Table 2 Community-attribute influence relationships revealed by PrefPalettes attention weights, showing both attribute-centric analysis (which communities most/least value specific attributes) and community-centric analysis (which attributes are most/least important in specific communities). These patterns demonstrate how the interpretable weights in PrefPalette capture meaningful community-specific evaluation criteria. Attribute Supportiveness Sarcasm Verbosity Directness Hedonism Achievement Highest Influence Lowest Influence r/raisedbynarcissists r/MaliciousCompliance r/AskHistorians r/awfuleverything r/femalefashionadvice r/nottheonion r/DiWHY r/HighQualityGifs r/NoStupidQuestions r/AskHistorians r/nottheonion r/RoastMe Domain Most Important Attributes Least Important Attributes Verbosity, Stimulation r/AskHistorians r/MaliciousCompliance Sarcasm, Directness Directness, Sarcasm r/RoastMe Tradition, Humor r/2meirl4meirl r/femalefashionadvice Hedonism, Tradition Stimulation, Empathy r/dataisbeautiful Security, Power Assertiveness, Powser Achievement, Security Formality, Assertiveness Sarcasm, Power Security, Achievement Figure 4 Distinguishing attributes in representative domains. For example, r/AskHistorians places high emphasis on verbosity. Table 3 Preference accuracy (%) when including different attribute categories across subreddit domains. The relative importance of attribute categories varies by domain context. Domain r/humor r/TooAfraidToAsk r/unpopularopinion r/Conservative r/destiny2 r/KitchenConfidential Average No Sociolinguistic Schwartz Cultural All Attributes 87.3 70.8 69.0 74.5 71.2 77.4 Norms (9) 91.8 71.7 70.2 75.4 73.3 78.0 83.7 84. Values (10) 88.6 71.3 70.1 75.4 73.5 78.3 84.4 Attributes (19) 91.4 72.0 70.2 75.4 72.9 78.2 84.9 These results reveal patterns in attribute importance across social contexts, showing that preference formation follows predictable, community-specific evaluative frameworks. For instance, the prominent role of verbosity in r/AskHistorians and its inverse relationship with directness reflects the communitys established quality standards for comprehensive, well-researched responses (Gilbert, 2020; Centola, 2018). Similarly, r/MaliciousCompliances preference patterns emphasize sarcasm and directness, aligning with established norms for retribution narratives in conflict-centered communities (Marwick et al., 2017). Support-oriented communities (e.g., r/raisedbynarcissists) show elevated emphasis on supportiveness, while identity-focused communities (e.g., r/femalefashionadvice) prioritize hedonism and tradition, aligning with research on digital social support systems (Andalibi et al., 2018). The attribute patterns for r/RoastMehigh directness with minimal concern for achievement or securityquantitatively validate observations about ritualized vulnerability displays as community bonding mechanisms (Walther, 2011). These patterns provide empirical support for the attribute-mediated preference model while demonstrating its interpretability advantages for understanding community-specific evaluation criteria. Human validation confirms interpretability: verbosity correlations with preferences in r/AskHistorians (r = 0.12) vs r/HighQualityGifs (r = 0.06) align with PrefPalette predictions. Extended validation shows consistent patterns across attributes (Appendix D)."
        },
        {
            "title": "4.3 Attribute Categories Influence Different Domains Differently",
            "content": "To quantify the relative importance of different attribute categories, we group attributes during training and evaluation. Table 3 presents preference accuracy when including no attributes, only sociolinguistic norms, Schwartz cultural values, or all attributes. Results reveal domain-specific attribute dependencies with clear patterns. Communities focused on entertainment (r/humor) derive substantial benefit from sociolinguistic norms (+4.5%), with minimal contribution from cultural values (+1.3%). Meanwhile, politically-oriented communities (r/Conservative) benefit equally from both attribute categories (+0.9%). These findings substantiate our hypothesis that different contexts rely on distinct attribute dimensions, making the attention-based context-aware attribute scores crucial in preference learning."
        },
        {
            "title": "4.4 PrefPalette Shows Strong Temporal Predictive Power",
            "content": "PrefPalette is trained on data from 2022; to evaluate for temporal robustness, we test on data from January 2023. From the last row of Table 1, Dialog-RPT shows significantly worse degradation on temporally shifted test sets, while ValueScope, PrefPalette-Score, and PrefPalette show similar robustness, suggesting that incorporating time metadata during training can improve temporal robustness. Overall, PrefPalette outperforms all approaches and demonstrate strong out-of-distribution performance."
        },
        {
            "title": "5 Qualitative Analysis",
            "content": "To provide interpretable insights into model behavior, we present qualitative examples where PrefPalette and the ValueScope baseline produce different predictions on subreddit comments. Figure 5 shows qualitative 8 Figure 5 Examples where PrefPalette and the ValueScope baseline produce different preference predictions. examples with predicted and actual preferences, along with the most influential attributes identified via attribution analysis. These examples reveal how PrefPalettes attribute-mediated approach enables context-sensitive preference prediction. In r/humor, PrefPalette correctly prioritizes humor and sarcasm attributes, selecting the playful Gangus Kahn wordplay over the dismissive critique. Notably, Stimulation consistently appears as top-weighted attribute across domains, suggesting that PrefPalette has learned that intellectually engaging content drives preferences regardless of context. In r/confession, the model appropriately emphasizes empathy and supportiveness, choosing the understanding response about denture theft over the judgmental rub it in comment. This demonstrates PrefPalettes ability to adapt its evaluative framework to community norms prioritizing entertainment value in humor contexts while emphasizing emotional support in confession spaces. These attribute scores provide direct insight into the models reasoning: rather than treating preferences as black box, PrefPalette reveals that its correct predictions stem from weighting community-appropriate dimensions. PrefPalette may fail when community preferences depend on domain-specific semantic dimensions beyond the 19 value attributes, such as technical accuracy or factual correctness. The attention mechanism could also over-rely on spurious attribute correlations from training data that fail to generalize to new content patterns. However, ValueScopes failures in these cases highlight the limitation of approaches that lack explicit attribute decomposition; they cannot adaptively emphasize the social and linguistic dimensions that actually drive human preference formation in different contexts."
        },
        {
            "title": "6 Related Work",
            "content": "Personalized preference modeling. In recent years, reinforcement learning from human feedback (RLHF) has emerged as the de facto paradigm for aligning pre-trained language models with human values, typically via secondary post-training stage on preference data (Bai et al., 2022; Casper et al., 2023). However, existing preference datasetsthe resulting models trained on themoften fail to reflect how human stakeholders actually generate preferences in real-world settings (Knox et al., 2023; Hatgis-Kessell et al., 2025). While most preference data typically characterizes aggregate preferences across large-scale human population, an emergent thread of work focuses on capturing personalized preferences, which involves customizing model outputs to the criteria of individual users or perspectives (Jang et al., 2024; Li et al., 2025b; Singh et al., 2025). For example, Jang et al. (2024) factorize preferences into multiple dimensions (e.g., expertise, informativeness, style) and introduce post-hoc parameter merging method of dimension policy models to provide personalized alignment. Poddar et al. (2024) employ variational preference learning to learn reward model that covers diverse user preferences. Both works use small-scale, synthetic evaluation sets. In contrast, PrefPalette evaluates on domains harvested from real-world interactions. Li et al. (2025b) capture personalized user 9 preferences by using small user model to learn individual user preferences. Additionally, Singh et al. (2025) re-frame personalization as meta-learning problem, in which the individual users preferences is inferred at inference time via few-shot preference examples. PrefPalette operates at higher level of granularity and transparency, modeling preferences via latent value attribute decomposition at the level of social communities rather than individuals. Learning from community norms. Naturally occurring interactions from online communities or forums form suitable testbed for studying the collective preferences shared by users who implicitly adhere to similar value systems. In holistic study on subreddits (Reddit communities), Park et al. (2024b) find that even communities that may appear thematically similar on surface level may hold different social normative structures. Kumar et al. (2024) propose ComPOa modification to DPO (Rafailov et al., 2024), in which the probability distribution of model outputs is additionally conditioned on the users community (as proxy for the users individual preferences). PrefPalette differs by explicitly modeling cognitive attribute structures and demonstrating superior prediction and robustness through interpretable value decomposition. As the first approach to modeling community-level preferences by factorizing them into interpretable dimensions, our contribution offers birds-eye view of shared evaluative normsenabling interpretable, context-sensitive generalization across users who implicitly adhere to similar value systems."
        },
        {
            "title": "7 Conclusion and Future Work\nWe present PrefPalette, incorporating cognitive attribute structures into preference modeling. By condi-\ntioning on attributes in latent space and using weighted representations, PrefPalette achieves superior\naccuracy while enabling interpretable preference analysis across social contexts. While our choice of dataset is\nlarge, realistic, diverse, and facilitates a controlled analysis, it may nevertheless blunt the generalizability of\nour findings to user dynamics beyond this scope (i.e., other online platforms or non-English speakers).",
            "content": "Conditioning on the 19 attributes we use in this work, which capture sociolinguistic norms and cultural values, also improves the preference models robustness to distribution shifts over time, potentially reducing the need to re-train new preference models. While our choice of attributes is comprehensive, it is by no means exhaustive. How our findings could extend to other implicit attributes (particularly those that are low-resourced or yield sparse preference signals) remains an open question. The latent attribute dimensions learned by PrefPalette are not directly validated via human annotations. While these dimensions are interpretable post hoc and advance our understanding of preference patterns among distinct social communities, we believe that incorporating human-in-the-loop evaluation or cognitive studiesin addition to our existing qualitative case studieswould further enrich our insights. We believe there is rich interface between computational preference modeling and cognitive theories of preference formation. PrefPalette is first attempt at exploring this interface, and the further incorporation of cognitive theories into preference modeling approaches is an exciting direction for future work."
        },
        {
            "title": "Ethics Statement",
            "content": "We identify several potential risks to our work. To begin, PrefPalette is able to more accurately model the latent value attributes of human users. However, this same capability may also raise the likelihood of illuminating undesirable viewpoints (e.g., stereotypical beliefs or toxicity), and could contribute to the formation of echo chambers, reinforce ideological silos, or amplify misinformation and polarization (Cinelli et al., 2021). Such risks are especially salient in environments where social feedback signals (e.g., upvotes) are moreso an indicator of localized popularity instead of normative desirability. To minimize such risks, post-hoc safeguards (Inan et al., 2023) could be applied to filter only for particular preferences or values that are flagged as socially helpful. Another consequence arises from the fine-grained, personalized alignment provided by PrefPalette. Better preference modeling benefits user-centric applications, but it can also be misused as an apparatus to influence or manipulate user behavior in sensitive domains (i.e., political messaging, targeted advertising). While core benefit of PrefPalette is its transparency, we must caution that the interpretability of 10 latent attributes does not guarantee their correctness or universality. Human preferences are by nature fluid, contextual, and subjective (Tversky and Simonson, 1993); there is possibility that practitioners may over-interpret the learned attributes as objective truths rather than model-inferred approximations from data with underlying biases. Ultimately, striking balance between the precise modeling of human preferences and the development of fair and inclusive systems remains critical direction for future work. Still, we believe that PrefPalette contributes to broader praxis of value-aligned AI design, bridging theoretical grounding from cognitive science to the practice of preference modeling."
        },
        {
            "title": "Acknowledgment",
            "content": "This research was developed in part with funding from the Defense Advanced Research Projects Agencys (DARPA) SciFy program (Agreement No. HR00112520300). The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government."
        },
        {
            "title": "References",
            "content": "Nazanin Andalibi, Oliver Haimson, Munmun De Choudhury, and Andrea Forte. Social support, reciprocity, and anonymity in responses to sexual abuse disclosures on social media. ACM Transactions on Computer-Human Interaction (TOCHI), 25(5):135, 2018. Yuntao Bai, Andy Jones, Kinden Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media, volume 14, pages 830839, 2020. James Bettman, Mary Frances Luce, and John Payne. Constructive consumer choice processes. Journal of Consumer Research, 25(3):187217, 1998. Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (technology) is power: critical survey of\" bias\" in nlp. arXiv preprint arXiv:2005.14050, 2020. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023. https://arxiv.org/abs/2307.15217. Damon Centola. How behavior spreads: The science of complex contagions, volume 3. Princeton University Press Princeton, NJ, 2018. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. Matteo Cinelli, Gianmarco De Francisci Morales, Alessandro Galeazzi, Walter Quattrociocchi, and Michele Starnini. The echo chamber effect on social media. Proceedings of the National Academy of Sciences, 118(9):e2023301118, 2021. doi: 10.1073/pnas.2023301118. https://www.pnas.org/doi/abs/10.1073/pnas.2023301118. Jon Doyle. Prospects for preferences. Computational Intelligence, 20(2):111136, 2004. Gregory Fischer, Mary Frances Luce, and Jianmin Jia. Attribute conflict and preference uncertainty: The effects of judgment time and weight salience. Journal of Behavioral Decision Making, 12(3):177197, 1999. Adrian Furnham. Response bias, social desirability and dissimulation. Personality and individual differences, 7(3): 385400, 1986. 11 Xiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, and Bill Dolan. Dialogue response ranking training with large-scale human feedback data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 386395, 2020. Sarah Gilbert. \" run the worlds largest historical outreach project and its on cesspool of website.\" moderating public scholarship site on reddit: case study of r/askhistorians. Proceedings of the ACM on Human-Computer Interaction, 4(CSCW1):127, 2020. Stephane Hatgis-Kessell, W. Bradley Knox, Serena Booth, Scott Niekum, and Peter Stone. Influencing humans to conform to preference models for rlhf, 2025. https://arxiv.org/abs/2501.06416. Juan Manuel Hernández-Campoy. Sociolinguistic styles. John Wiley & Sons, 2016. Christopher Hsee, George Loewenstein, Sally Blount, and Max Bazerman. Preference reversals between joint and separate evaluations of options: review and theoretical analysis. Psychological Bulletin, 129(1):4265, 2003. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023. https://arxiv.org/abs/2312.06674. Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2024. https://openreview.net/forum?id=EMrnoPRvxe. Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. W. Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and Alessandro Allievi. Models of human preference for learning reward functions, 2023. https://arxiv.org/abs/2206.02231. Sachin Kumar, Chan Young Park, Yulia Tsvetkov, Noah A. Smith, and Hannaneh Hajishirzi. Compo: Community preferences for language model personalization, 2024. https://arxiv.org/abs/2410.16027. Maria Knight Lapinski and Rajiv Rimal. An explication of social norms. Communication theory, 15(2):127147, 2005. Shuyue Stella Li, Jimin Mun, Faeze Brahman, Jonathan S. Ilgen, Yulia Tsvetkov, and Maarten Sap. Aligning llms to ask good questions case study in clinical reasoning, 2025a. https://arxiv.org/abs/2502.14860. Xinyu Li, Ruiyang Zhou, Zachary Chase Lipton, and Liu Leqi. Personalized language modeling from personalized human feedback, 2025b. https://openreview.net/forum?id=bqUsdBeRjQ. Alice Marwick, Claire Fontaine, and Danah Boyd. nobody sees it, nobody gets mad: Social media, privacy, and personal responsibility among low-ses youth. Social Media+ Society, 3(2):2056305117710455, 2017. Julia Mendelsohn and Ceren Budak. When people are floods: Analyzing dehumanizing metaphors in immigration discourse with large language models. arXiv preprint arXiv:2502.13246, 2025. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, 12 Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. https://arxiv.org/abs/2410.21276. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, pages 2773027744, 2022. Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanushree Mitra, David Jurgens, and Yulia Tsvetkov. Valuescope: Unveiling implicit norms and values via return potential model of social interactions, 2024a. https://arxiv.org/abs/2407.02472. Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanushree Mitra, David Jurgens, and Yulia Tsvetkov. Valuescope: Unveiling implicit norms and values via return potential model of social interactions, 2024b. https://arxiv.org/abs/2407.02472. Delroy Paulhus. Measurement and control of response bias. Academic press, 1991. Amandalynne Paullada, Inioluwa Deborah Raji, Emily Bender, Emily Denton, and Alex Hanna. Data and its (dis) contents: survey of dataset development and use in machine learning research. Patterns, 2(11), 2021. Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. Personalizing reinforcement learning from human feedback with variational preference learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. https://openreview.net/forum?id=gRG6SzbW9p. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. https://arxiv.org/abs/2305.18290. 13 Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 58845906, 2022. Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. Collaborative filtering recommender systems. In The adaptive web: methods and strategies of web personalization, pages 291324. Springer, 2007. Howard Schuman and Stanley Presser. Questions and answers in attitude surveys: Experiments on question form, wording, and context. Sage, 1996. Shalom Schwartz. Universals in the content and structure of values: Theoretical advances and empirical tests in 20 countries. Advances in experimental social psychology, 25:165, 1992. Shalom Schwartz. An overview of the schwartz theory of basic values. Online readings in Psychology and Culture, 2 (1):23070919, 2012. Norbert Schwarz. Self-reports: How the questions shape the answers. American psychologist, 54(2):93, 1999. Deven Shah, Andrew Schwartz, and Dirk Hovy. Predictive biases in natural language processing models: conceptual framework and overview. arXiv preprint arXiv:1912.11078, 2019. Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, and Chelsea Finn. Fspo: Few-shot preference optimization of synthetic preference data in llms elicits effective personalization to real users, 2025. https://arxiv.org/abs/2502.19312. Paul Slovic. The construction of preference. American Psychologist, 50(5):364371, 1995. Brandon Smith, Miguel Farinha, Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, and Max Bain. Balancing the picture: Debiasing vision-language datasets with synthetic contrast sets. arXiv preprint arXiv:2305.15407, 2023. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 30083021, 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Amos Tversky and Daniel Kahneman. The framing of decisions and the psychology of choice. Science, 211(4481): 453458, 1981. Amos Tversky and Itamar Simonson. Context-dependent preferences. Management Science, 39:11791189, 1993. https://api.semanticscholar.org/CorpusID:154440885. Joseph Walther. Theories of computer-mediated communication and interpersonal relations. The handbook of interpersonal communication, 4:443479, 2011. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1058210592, 2024a. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024b. Jin Peng Zhou, Katie Luo, Jingwen Gu, Jason Yuan, Kilian Q. Weinberger, and Wen Sun. Orchestrating llms with different personalizations. ArXiv, abs/2407.04181, 2024. https://api.semanticscholar.org/CorpusID:271039016. Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization. In Annual Meeting of the Association for Computational Linguistics, 2023. https://api.semanticscholar.org/CorpusID:264175263."
        },
        {
            "title": "A Model Architecture Details",
            "content": "This section outlines the key components of our attribute-mediated preference model. We use LLaMA 3.2 (1B parameters) (Touvron et al., 2023) as the base model to train the predictor models, with the attribute integration mechanism as our primary novel contribution. Representation Extraction. Given an instruction-response pair (x, y), the model processes the tokenized concatenation through the Transformer encoder to obtain the hidden representation hfinal Rd (where = 3200) from the final token position. Attribute Integration Mechanism. The core innovation of our approach is the dynamic integration of attribute information with content representation. For attribute predictors yielding the set of hidden states {hAi }n , we compute attention weights as i=1 αi = exp(ei) k=1 exp(ek) (cid:80)n , where ei = 3 tanh(W1hAi + W2h(x,y) + b1) + b2. (1) Here, W1 Rdd and W2 Rdd are learned projection matrices, b1 Rd and b2 are bias terms, and w3 Rd is learned weight vector that maps the combined representation to scalar attention score. This attention mechanism enables context-specific weighting of attribute dimensions, capturing the dynamic nature of attribute-mediated evaluation. The aggregated attribute representation is computed as: hattr = (cid:88) i=1 αihAi. (2) We then combine the attribute representation hattr with the content representation. The learnable parameter γ (initialized to 0.5) controls the relative contribution of attribute information versus content representation in the integrated hidden state: hintegrated = hfinal + γhattr. (3) Preference Learning. The final preference score is computed as (x, y) = we employ standard pairwise ranking loss: hintegrated + bp. For training, Lpref = log σ(P (x, ypreferred) (x, yrejected)). (4) Feature Reduction Strategy. To enable inference without attribute signals, we implement progressive attribute dropout strategy during training. The dropout probability, pdropout(t), increases linearly with the number of training steps and is capped at 0.8. It is denoted as pdropout(t) = min(0.8, 0.75 ), where is the current number of training steps and is the total number of training steps. When dropout is applied, the model must rely solely on the content representation: (cid:40) hintegrated = hfinal + λhattr with probability 1 pdropout(t), hfinal with probability pdropout(t). (5) (6) Optimization. We train using an AdamW optimizer with learning rate of 1 105, batch size of 128, and weight decay set to 0.01 for 100,000 steps with cosine learning rate decay and linear warmup. 15 Attribute Importance Analysis. To interpret the models attribute utilization patterns, we analyze the attention weights across domains. For domain Dk, the mean importance of attribute is: = 1 Dk (cid:88) αi(x, y). (x,y)Dk (7) This quantitative analysis reveals which attributes most influence preference predictions in different social contexts, providing interpretable insights into the attribute-mediated evaluation process."
        },
        {
            "title": "B Dataset Curation",
            "content": "For each subreddit, we collect all posts and comments over 12-month period (January-December 2022) with associated metadata including upvote counts and timestamps, and set apart 3% of posts each for the validation and test sets. To prepare source data for each attribute in the attribute representation learning module, we randomly sample 100 comments from each subreddit and generate counterfactual comments along 5 Likert scale levels, resulting in 680 100 5 = 340K synthetic comments, which allows us to form 680 100 (cid:0)5 (cid:1) = 680K attribute-specific pairs. Table 4 shows qualitative example of counterfactual synthetic comments along the supportive attribute dimension of comment (ID: t1_goend1w) from r/technicallythetruth (original post and comment can be found using the comment ID from the Pushshift Reddit dataset (Baumgartner et al., 2020)). 2 To prepare pairwise preference learning data, we follow established protocols (Park et al., 2024a) and extract 10,000 comment pairs, where one comment is strictly morepreferred by the community members (i.e., received more upvotes) than another, resulting in approximately 6.8 million preference pairs. Supportiveness Level Counterfactual Comment 1 3 4 5 are kidding me, everyone knows this is \"joke\" but honestly who needs comedy when this is just depressing reality, and dont even get me started on how NO ONE talks about this super obvious symptom like its not even real issue, get ur priorities straight people, this isnt something to be laughed at \"come on, we all know this is joke, but seriously, why doesnt anyone talk about this symptom? its actually kinda big deal and its wild that it gets swept under the rug so often.\" this is actually common thing that happens. its not often discussed, but it can be an issue. same lol but yeah its wild how often this gets swept under the rug, def needs more attention imo, glad someones bringing it up omg im so glad you brought this up!! know the post is meant to be funny but honestly, its SO relatable and im really grateful youre talking about it bc this symptom is way more common than people think and it can be super tough to deal with Table 4 Examples of text with varying levels of supportiveness (1 = least supportive, 5 = most supportive)"
        },
        {
            "title": "C Universal Attribute Predictor Training Details",
            "content": "We evaluate our universal attribute predictors on held-out test sets of counterfactual comment and post pairs to verify their ability to distinguish between different attribute intensity levels. The high accuracy scores across all 19 attributes (ranging from 98.4% to 100%) in Table 5 demonstrate that our contrastive attribute distillation approach successfully transfers attribute understanding from the strong teacher model to the smaller specialized predictors. These results validate that the attribute predictors can reliably extract latent attribute information from text, providing solid foundation for the attribute-mediated preference modeling framework. 16 Attribute Supportiveness Politeness Sarcasm Humor Formality Verbosity Directness Assertiveness Empathy Self-Direction Stimulation Hedonism Achievement Power Security Conformity Tradition Benevolence Universalism Comment Acc. (%) Post Acc. (%) 99.30 99.70 98.40 98.70 99.60 99.20 99.80 99.00 98.60 99.20 98.80 99.50 98.90 98.80 98.50 99.20 99.60 99.00 99.00 99.30 100.00 99.50 99.30 99.50 99.30 99.70 99.70 99.00 99.70 99.00 99.40 99.10 98.50 99.10 99.10 99.50 99.60 99. Table 5 Accuracy comment and post percentages with our attribute predictors."
        },
        {
            "title": "D Human Manual Evaluation",
            "content": "We conduct human evaluation studies to validate whether PrefPalettes attention weights accurately reflect attribute-preference relationships. Using r/AskHistorians as case study, we test whether high-importance attributes (Verbosity and Stimulation) show stronger correlations with preference scores than low-importance attributes (Security and Power), as identified from Table 2. For Stimulation, Security, and Power, four trained annotators evaluated 100 comment pairs per attribute on 5-point comparative scale, with majority-voted ratings used for correlation analysis. For Verbosity, we used character count as an objective proxy and additionally analyzed r/HighQualityGifs where verbosity has low importance. Table 6 presents correlation results between attribute ratings and preference scores. High-importance attributes show positive correlations while low-importance attributes show near-zero or negative correlations, confirming PrefPalettes predictions. The modest magnitudes reflect preference formations multi-factorial nature. These results validate that PrefPalettes attention weights capture meaningful attribute-preference relationships. Table 6 Correlation between human-rated attribute intensity and community preference scores calculated from majorityvote human annotations, validating PrefPalettes attribute importance predictions. Attribute Community Correlation PrefPalette Prediction Measurement High-Importance Attributes Verbosity Stimulation r/AskHistorians r/AskHistorians Low-Importance Attributes Security Power Verbosity r/AskHistorians r/AskHistorians r/HighQualityGifs = 0.12 = 0.10 = 0.04 = 0.06 = 0.06 High High Low Low Low Character count Human annotation Human annotation Human annotation Character count"
        }
    ],
    "affiliations": [
        "Meta FAIR",
        "Meta GenAI",
        "University of Washington"
    ]
}