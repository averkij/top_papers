{
    "paper_title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "authors": [
        "Egor Cherepanov",
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/."
        },
        {
            "title": "Start",
            "content": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning Egor Cherepannov 1 2 Daniil Zelezetsky 1 Alexey K. Kovalev 1 2 Aleksandr I. Panov 1 2 Project Page: avanturist322.github.io/KAGEBench 6 2 0 2 0 2 ] . [ 1 2 3 2 4 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying visual axis affects performance only through the induced state-conditional action distribution of pixel policy, providing clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, benchmark of six known-axis suites comprising 34 trainevaluation configuration pairs that isolate individual visual shifts. Using standard PPOCNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322. github.io/KAGEBench/. 1. Introduction Reinforcement learning (RL) agents trained from highdimensional pixel observations are brittle to changes in appearance, lighting, and other visual nuisance factors (Cetin et al., 2022; Yuan et al., 2023; Klepach et al., 2025). Policies that perform well in-distribution can degrade sharply under 1MIRIAI, Moscow, Russia 2Cognitive AI Systems Lab, Correspondence to: Egor Cherepanov Moscow, Russia. <cherepanov.e@miriai.org>. Preprint. January 21, 2026. 1 Figure 1. Representative observations from KAGE-Env illustrating controlled, known-axis visual variation. Each panel differs along one or more explicitly configurable axes, including background imagery and color, agent appearance and animation, moving distractors, photometric filters, and dynamic lighting effects, while task semantics and underlying dynamics are held fixed. purely visual distribution shifts, even when task semantics, transition dynamics, and rewards are unchanged (Staroverov et al., 2023; Kachaev et al., 2025; Mirjalili et al., 2025). This brittleness poses fundamental obstacle to real-world deployment, where observations inevitably vary due to viewpoint changes, illumination, surface appearance, and sensor noise while the control-relevant latent state remains fixed (Raileanu et al., 2020; Kostrikov et al., 2020; Kirilenko et al., 2023; Korchemnyi et al., 2024; Yang et al., 2024; Ugadiarov et al., 2026). As result, pixel-based RL policies that rely on incidental visual correlations can fail abruptly despite convergence, undermining reliability in robotics, autonomous navigation, and interactive environments (Stone et al., 2021; Yuan et al., 2023). More broadly, visual generalization is needed wherever models must robustly extract information from visual structure, even in scientific texts and figures (Sherki et al., 2025). Despite substantial progress in representation learning (Mazoure et al., 2021; Rahman & Xue, 2022; Ortiz et al., 2024) and data augmentation (Laskin et al., 2020; Raileanu et al., 2020; Hansen & Wang, 2021), understanding visual generalization failures remains challenging. central obstacle lies in evaluation benchmarks, which often entangle multiple KAGE-Bench: Known-Axis Visual Generalization Figure 2. KAGE-Bench: Motivation. Existing generalization benchmarks entangle multiple sources of visual shift between training and evaluation, making failures difficult to attribute. KAGEBench factorizes observations into independently controllable axes and constructs trainevaluation splits that vary one (or selected set) of axes at time, enabling precise diagnosis of which visual factors drive generalization gaps. The observation vector notation ψ is used for intuition only. visual and structural changes such as background appearance, geometry, dynamics, and distractors (Cobbe et al., 2020; Stone et al., 2021; Yuan et al., 2023). In these settings, trainevaluation performance gaps cannot be cleanly attributed to specific sources of shift, and failures may reflect visual sensitivity, altered task structure, or interactions between confounded factors. Compounding this issue, many pixel-based RL environments are computationally expensive to simulate, limiting large-scale ablations and slowing hypothesis testing. We address these limitations with KAGE-Bench (KnownAxis Generalization Evaluation Benchmark), visual generalization benchmark in which sources of distribution shift are isolated by construction. KAGE-Bench is built on KAGE-Env (Figure 1), JAX-native (Bradbury et al., 2018) 2D platformer whose observation process is factorized into independently controllable visual axes while latent dynamics and rewards are held fixed (see Figure 2). Under this known-axis design, each axis corresponds to well-defined component of the observation kernel, and any trainevaluation performance difference arises solely from how fixed observation-based policy responds to different renderings of the same latent states, enabling unambiguous attribution of visual generalization failures. Systematic analysis of visual generalization requires evaluating many controlled shifts at scale. KAGE-Env is implemented entirely in JAX with end-to-end jit compilation and vectorized execution via vmap and lax.scan, enabling efficient large-batch simulation on single accelIn practice, this design scales up to 216 parallel erator. environments on one GPU and achieves up to 33M environment steps per second (see Figure 3), making exhaustive sweeps over visual parameters and fine-grained diagnosis of generalization behavior feasible. 1https://colab.research.google.com/ (a) Easy configuration. (b) Hard configuration. Figure 3. Environment stepping throughput vs. parallelism. Environment stepping throughput (steps per second, higher is better) as function of the number of parallel environments nenvs for KAGE-Env across heterogeneous hardware backends. GPU results are shown for NVIDIA H100 (80 GB), A100 (80 GB), V100 (32 GB), and T4 (15 GB, Google Colab1), with CPU-only results on an Apple M3 Pro laptop. (a) Easy configuration: lightweight setup with all visual generalization parameters disabled. (b) Hard configuration: most demanding setup with all visual generalization parameters enabled at maximum values. Building on this environment, we construct six visual generalization suites comprising 34 trainevaluation configuration pairs, each targeting specific visual axis. Using these suites, we demonstrate that visual generalization is strongly axis-dependent and identify classes of visual shifts that reliably induce severe performance degradation, even for standard PPO-CNN baseline (Schulman et al., 2017). We summarize our main contributions as follows: 1. KAGE-Env, JAX-native RL environment with 93 explicitly controllable parameters, configurable via single .yaml file and vectorized to reach up to 33M environment steps per second with 216 parallel environments on single GPU. 2. KAGE-Bench, benchmark that isolates visual distribution shifts by construction via six knownaxis suites and 34 trainevaluation configuration pairs with fixed dynamics and rewards. 3. Empirical diagnosis of visual generalization: using PPO-CNN baseline, we quantify how visual generalization behavior differs across axes and identify classes of visual shifts that reliably induce severe performance degradation. 2. Related Work Visual generalization in RL. Visual generalization studies whether policies trained from pixel observations retain performance when the observation process changes while latent dynamics and rewards remain fixed. Prior work shows that agents often overfit incidental visual features, leading to substantial traintest gaps across wide range of environments and settings (Cobbe et al., 2019; Beattie et al., 2016; Xia et al., 2018; Ortiz et al., 2024). common expla2 KAGE-Bench: Known-Axis Visual Generalization Figure 4. Examples of visual generalization gaps. Success rate for three trainevaluation pairs showing (left) negligible, (middle) moderate, and (right) severe generalization gaps. nation is that standard architectures and objectives exploit spurious visual correlations, such as background textures or color statistics, rather than learning task-relevant invariances (Cobbe et al., 2020; Hansen & Wang, 2021; Stone et al., 2021). Accordingly, many approaches have been proposed to improve robustness, including data augmentation, auxiliary representation learning objectives, and regularization methods (Laskin et al., 2020; Raileanu et al., 2020; Mazoure et al., 2021; Raileanu & Fergus, 2021; Cobbe et al., 2021; Wang et al., 2020; Bertoin & Rachelson, 2022; Bertoin et al., 2022; Zisselman et al., 2023; Rahman & Xue, 2022; Jesson & Jiang, 2024). KAGE-Env and KAGE-Bench provide diagnostic infrastructure for this literature by enabling fast, controlled, axis-specific evaluation that isolates changes in the observation kernel. Benchmarks for visual generalization in RL. range of benchmarks study visual generalization in pixel-based RL, differing in task domains and in how explicitly they isolate sources of visual variation. RL-ViGen (Yuan et al., 2023) spans multiple domains, including locomotion, manipulation, navigation, and driving, with shifts in textures, lighting, viewpoints, layouts, and embodiments. Hansen & Wang (2021) evaluates continuous control under controlled appearance changes such as color randomization and dynamic video backgrounds. Obstacle Tower (Juliani et al., 2019) and LevDoom (Tomilin et al., 2022) consider 3D settings where many factors vary jointly, making attribution of failures to specific visual causes difficult. Related benchmarks such as DMC-VB (Ortiz et al., 2024) and Distracting MetaWorld (Kim et al., 2024) introduce task-irrelevant visual distractors while keeping task dynamics fixed. Among widely used benchmarks, Procgen (Cobbe et al., 2020) relies on procedural generation, so traintest gaps typically reflect entangled shifts in appearance and scene composition rather than isolated visual factors. The Distracting Control Suite (DCS) (Stone et al., 2021) introduces explicit distraction axes but is limited to small set of factors, and broad axis-wise sweeps are costly in its underlying continuous-control simulator. KAGE-Env and KAGEBench complement these benchmarks by explicitly factorizing the observation process into independently controllable visual axes. KAGE-Env uses simple platformer to reduce optimization and exploration confounds, while KAGEBench constructs trainevaluation splits that vary specified axes (e.g., backgrounds, sprites, distractors, filters, and lighting) with fixed dynamics and rewards, enabling systematic, axis-specific attribution of generalization failures. Fast and scalable evaluation in RL. Evaluating generalization in RL is sample intensive, as reliable conclusions require averaging over random seeds, environment instances, and distribution shifts. In visual generalization benchmarks, this leads to combinatorial scaling Nsteps Nseeds Nshifts, often compounded by checkpointing and hyperparameter sweeps, making evaluation costly in CPU-bound simulators. Recent work addresses this bottleneck through acceleratornative RL systems, where environment stepping is implemented as compiled, vectorized computation on GPUs or TPUs. Examples include JAX-based simulators such as Brax (Freeman et al., 2021), Jumanji (Bonnet et al., 2023), XLand-MiniGrid (Nikulin et al., 2024), CAMAR (Pshenitsyn et al., 2025), and Craftax (Matthews et al., 2024), as well as GPU-native platforms such as ManiSkill3 (Tao et al., 2024), MIKASA-Robo (Cherepanov et al., 2025), and WarpDrive (Lan et al., 2021). By eliminating host-side control flow, these systems achieve orders-of-magnitude throughput. However, high throughput alone does not yield diagnostic evaluation of visual robustness. Benchmarks such as Procgen and DCS do not support exhaustive, axis-isolated sweeps over rendering factors, limiting failure attribution. KAGE-Env combines the accelerator-native paradigm with explicit factorization of the observation process into independently controllable axes, enabling large-batch, reproducible evaluation of known-axis visual shifts under fixed latent dynamics and rewards. 3. Background Partially Observable Markov Decision Processes. We consider episodic control with horizon in partially observable Markov decision process (POMDP). Each environment instance is indexed by visual configuration ξ Ξ and defined as Mξ = (S, A, P, r, Ω, Oξ, ρ0, γ), where is the latent (control-relevant) state space, is the action 3 KAGE-Bench: Known-Axis Visual Generalization space, ( s, a) is the transition kernel, r(s, a) is the reward function, Ω is the observation space, Oξ( s) is the observation (rendering) kernel parameterized by ξ, ρ0 is the initial state distribution, and γ [0, 1) is the discount factor. At each timestep t, the environment occupies latent state st S. An observation is generated according to ot Oξ( st), ot Ω {0, . . . , 255}HW 3. Based on this observation, the agent selects an action at A, receives reward r(st, at), and transitions to st+1 ( st, at). key structural property enforced throughout this work is that the transition kernel and reward function are independent of the visual configuration ξ. All dependence on ξ is confined to the observation kernel Oξ. Consequently, the same latent state st may give rise to different observations under different values of ξ, while inducing identical dynamics and rewards. Visual generalization concerns the behavior of policies under such changes in the observation process, with the underlying control problem held fixed. Policies and return. We focus on reactive pixel-based policies that map observations directly to action distributions: π(a o). The expected discounted return of policy π in environment Mξ is J(π; Mξ) = s0ρ0, otOξ(st), atπ(ot), (cid:34)T 1 (cid:88) (cid:35) γtr(st, at) . (1) t=0 Visual generalization. We study generalization under shifts in visual parameters that affect observations but not the underlying control problem. Let Ξ denote the space of visual configurations, and let Dtrain and Deval be probability distributions over Ξ. Each ξ Ξ induces visual POMDP Mξ through its observation kernel Oξ, while sharing the same latent dynamics and reward function r. pixel policy π(a o) is trained using environments with ξ Dtrain and evaluated under ξ Deval. For any distribution over Ξ, we define the expected performance J(π; D) = EξD[J(π; Mξ)] . (2) We refer to this setting as visual generalization when the shift from Dtrain to Deval changes only the observation kernels Oξ, while preserving the latent state space, transition dynamics, and reward function. Known-axis visual shifts. KAGE-Bench focuses on known-axis visual generalization. Each visual configuration is decomposed as ξ = (ξaxis, ξrest), where ξaxis specifies designated axis of visual variation (e.g., background appearance, agent sprites, lighting, filters), and ξrest contains all remaining parameters. By construction, any performance difference between training and evaluation can therefore be attributed to changes in the observation process along the specified visual axis, rather than to changes in task structure, dynamics, or rewards. This intuition is formalized and justified in Section 4 and Appendix A. Evaluation metrics. Given Dtrain and Deval, we report in-distribution and out-of-distribution performance, J(π; Dtrain) and J(π; Deval), and define the return-based generalization gap (π) = J(π; Dtrain) J(π; Deval). (3) While (π) provides coarse measure of performance degradation under visual shift, it is insufficient to fully characterize generalization behavior. The discounted return aggregates multiple effects, including reward shaping, exploration inefficiency, and penalty terms, and may obscure whether an agent nearly solves the task or fails catastrophically. In particular, if policy fails under both training and evaluation configurations, the return gap can be small despite the absence of task competence. For this reason, we complement return-based evaluation with additional trajectory-level metrics that are measurable functions of the latent state trajectory, including distance traveled, normalized progress toward the goal, and binary task success. These metrics distinguish partial progress from complete failure and provide more fine-grained view of visual generalization behavior. Their precise definitions and empirical use are described in Section 6. 4. Known-axis visual generalization This section states the formal principle behind KAGEBench. In our construction (Section 3), the latent control problem is fixed and only the renderer changes: ξ affects performance only through the induced state-conditional action law obtained by composing the observation kernel with the pixel policy. The goal is to make this channel explicit and to justify the benchmark protocol: (i) constructing suites that intervene on single visual axis, and (ii) evaluating not only return but also trajectory-level metrics such as distance, progress, and success. From pixel policies to state-conditional behavior. reactive pixel policy π( o) maps observations to actions and does not directly specify an action distribution conditioned on the latent state s. However, in visual POMDP Mξ, the observation kernel Oξ( s) induces distribution over rendered observations for each latent state. Composing these kernels yields well-defined state-conditional action distribution by marginalizing the intermediate observation: Oξ(s) π(o) a. (4) Under our construction (and for reactive policies), this composition is the only mechanism by which the visual con4 KAGE-Bench: Known-Axis Visual Generalization figuration ξ can affect control, since and are invariant across ξ. Figure 5 illustrates this marginalization in concrete discrete example. Definition 4.1 (Induced state policy). Fix ξ Ξ, observation kernel Oξ( s), and reactive pixel policy π( o). The induced state policy πξ is defined by πξ(a s) := (cid:90) Ω π(a o) Oξ(do s), S, A. (5) For fixed pixel policy π, the map ξ (cid:55) πξ summarizes the effect of visual variation on state-conditional behavior. In particular, changing ξ changes πξ while leaving the latent control problem (S, A, P, r, ρ0, γ) unchanged. Visual shift is equivalent to induced policy shift. The next theorem formalizes the reduction used throughout KAGE-Bench: executing π in the visual POMDP Mξ induces the same latent stateaction law as executing πξ in the latent MDP M. Theorem 4.2 (Visual generalization reduces to induced policy shift). Fix any ξ Ξ and reactive pixel policy π( o), and let πξ be defined by Definition 4.1. Then: 1. (Conditional action law.) 0, A, PMξ,π(at = st) = πξ(a st) a.s. (6) 2. (Equality in law of stateaction processes.) The state action process (st, at)t0 induced by executing π in Mξ has the same law as the stateaction process induced by executing πξ in the latent MDP M. 3. (Return equivalence.) Consequently, J(π; Mξ) = J(πξ; M). (7) Theorem 4.2 is purely representational: it does not assume optimality and it does not modify the control problem. useful consequence is the identity, for any ξ, ξ Ξ, J(π; Mξ) J(π; Mξ) = J(πξ; M) J(πξ; M), (8) which states that visual trainevaluation gap for fixed pixel policy π is exactly performance difference between induced state policies in the same latent MDP. This is the formal basis for attributing failures to the observation process: since (P, r) are unchanged, any degradation under ξ ξ must be explained by how the renderer changes the induced state-conditional behavior πξ. Why known-axis suites enable axis-specific attribution. KAGE-Bench constructs axis-isolated suites by decomposing ξ = (ξaxis, ξrest) and pairing train and evaluation configurations that differ only in the designated axis: ξtrain = (ξtrain axis , ξrest) and ξeval = (ξeval axis, ξrest). (9) Figure 5. Induced state policy. The renderer Oξ( s) maps latent state to an observation distribution, and the pixel policy π( o) maps observations to actions. Their composition defines πξ( s) by marginalizing o. Equivalently, for all the paired renderers satisfy Oξtrain( s) = O( s; ξtrain axis , ξrest) and Oξeval( s) = O( s; ξeval axis, ξrest), so the only change in the observation process is along ξaxis. Under this controlled-intervention design, the induced policies πξtrain and πξeval differ only through this axis-dependent change in Oξ. Therefore, by Equation 8, the measured gap isolates how that visual axis perturbs the induced state-conditional behavior of π. Trajectory-level consequences and evaluation metrics. By Item 2 of Theorem 4.2, the latent stateaction trajectory has the same law under (Mξ, π) and (M, πξ), so the reduction applies to any measurable trajectory functional, not only return. We therefore report distance, progress, and success in addition to episodic return: these are functions of the latent trajectory exposed by KAGE-Env for evaluation, and their gaps under ξ ξ admit the same induced-policy interpretation. Unlike return, which can mask completion failures due to reward shaping, these metrics separate partial progress from task completion. Corollary 4.3 (Equivalence of trajectory-level evaluation metrics). Fix ξ Ξ and reactive π( o), and let πξ be the induced state policy. Let (st, at)t0 (Mξ, π) and (st, at)t0 (M, πξ). Then for any measurable functional : (S A)N R, (cid:0)(st, at)t0 (cid:1) d= (cid:0)(st, at)t0 (cid:1), and in particular EMξ,π[F ] = EM,πξ [F ] whenever the expectation is well-defined. Corollary 4.4 (Specialization to KAGE-Bench metrics). Assume the latent state contains one-dimensional position variable xt with initial position xinit and task completion threshold > 0. For fixed horizon (or terminal time), define Fdist := xT xinit, Fprog := xT xinit , and Fsucc := I{xT xinit D}. Then each metric has the same distribution under (Mξ, π) and (M, πξ), and in particular EMξ,π[F ] = EM,πξ [F ], {Fdist, Fprog, Fsucc}. All proofs are deferred to Appendix A. KAGE-Bench: Known-Axis Visual Generalization (cid:4) import jax from kage_bench import ( KAGE_Env, load_config_from_yaml, ) # Create environment with custom config env = KAGE_Env( load_config_from_yaml(\"custom_config.yaml\") ) # Vectorize and JIT compile reset_vec = jax.jit(jax.vmap(env.reset)) step_vec = jax.jit(jax.vmap(env.step)) # Initialize 65,536 parallel environments N_ENVS = 2**16 keys = jax.random.split( jax.random.PRNGKey(42), N_ENVS ) # Reset all at once obs, info = reset_vec(keys) states = info[\"state\"] # Parallel step: Samples one random discrete # action per env in [0, 7] (bitmask actions) actions = jax.random.randint( keys[0], (N_ENVS,), 0, 8 ) # obs.shape: (65536, 128, 128, 3) obs, rewards, terms, truncs, info = step_vec(states, actions) states = info[\"state\"] (cid:6) Code 1. Python (JAX) usage. The environment is configured from .yaml file (e.g., custom config.yaml); the code shows JAX-vmap/jit batched reset/step over 216 parallel envs. 5. KAGE-Environment KAGE-Env (Figure 1, Code 1) is JAX-native RL environment designed for controlled evaluation of visual generalization. It implements the visual-POMDP interface from Section 3: configurations ξ Ξ parameterize the renderer Oξ( s) while the latent control problem is held fixed. Task and interface. KAGE-Env is an episodic 2D sidescrolling platformer with horizon and push-scrolling camera. At each timestep t, the agent observes single RGB image ot {0, . . . , 255}HW 3, with default resolution = = 128, and selects an action at from discrete action space = {0, . . . , 7}. Actions are encoded as bitmask over three primitives: LEFT = 1, RIGHT = 2, and JUMP = 4. Policies interact with the environment exclusively through pixels; the latent simulator state is not available to the policy and is exposed only via the info dictionary for logging and evaluation. Reward and termination. Let xt denote horizontal position and xmax := max0kt xk the furthest position reached so far. The per-step reward is rt = α1 max{0, xt+1 xmax (cid:124) (cid:123)(cid:122) first-time forward progress } (cid:125) (cid:16) (cid:124) (cid:17) α2 I[JUMP(at)] + α3 + α4 I[idle(xt, xt+1)] (cid:123)(cid:122) penalties , (cid:125) (10) where I[JUMP(at)] indicates the jump bit is active in at, α3 is per-timestep time cost, and idle(xt, xt+1) flags lack of horizontal progress. Episodes terminate only by time-limit truncation at = episode length. Rendering assets and visual parameters. KAGE-Env provides library of visual assets and rendering controls for constructing visual variation. Assets include 128 background images (Appendix, Figure 43) and 27 animated sprite skins for the agent and non-player characters (Appendix, Figure 44); when sprites are disabled, entities can be rendered as geometric shapes (9 types) with palette of 21 colors. The renderer further exposes photometric and spatial transformations (e.g., brightness, contrast, gamma, hue, blur, noise, pixelation, vignetting) and lighting/overlay effects such as dynamic point lights with configurable count, intensity, radius, falloff, and color. (cid:5) (cid:13) Configuration interface. All parameters are specified through single .yaml configuration file (Code 2). configuration ξ Ξ is organized into groups background, character, npc, distractors, filters, effects, layout, and physics. These groups include rendering parameters (affecting only Oξ) as well as optional control parameters (affecting or r). KAGE-Env exposes both for extensibility; isolation of purely visual shifts is enforced by the KAGE-Bench pairing protocol (Section 6). 6. KAGE-Benchmark KAGE-Bench is benchmark protocol built on top of KAGE-Env. It specifies how environment configurations are selected and paired to evaluate known-axis visual generalization. Concretely, KAGE-Bench defines set of train evaluation configuration pairs (ξtrain, ξeval) such that the (cid:4) background: mode: \"image\" image_paths: - \"src/kage/assets/backgrounds/bg-1.jpeg\" - \"src/kage/assets/backgrounds/bg-64.jpeg\" - \"src/kage/assets/backgrounds/bg-128.jpeg\" parallax_factor: 0.5 switch_frequency: 0.0 character: mode: \"sprite\" sprite_paths: - \"src/kage/assets/sprites/clown\" - \"src/kage/assets/sprites/skeleton\" enable_animation: true animation_fps: 12.0 npc: mode: \"sprite\" sprite_dir: \"src/kage/assets/sprites\" filters: brightness: 0.0 hue_shift: 0. (cid:6) YAML configuration. Code 2. KAGE-Env is configured via single .yaml file; shown is small excerpt of custom config.yaml. We show only small part of all configuration parameters; for details, see Appendix G. (cid:5) (cid:13) KAGE-Bench: Known-Axis Visual Generalization Table 1. Axis-level summary of KAGE-Bench results (meanSEM). During training of each run, we record the maximum value attained by each metric. For each configuration, these per-run maxima are averaged across 10 random seeds, and the resulting per-configuration values are then averaged across all configurations within each generalization-axis suite. We report Distance, Progress, Success Rate (SR), and Return for train and eval configurations, along with the corresponding generalization gaps (meanSEM). Generalization gaps are color-coded: green indicates smaller gaps (better generalization), while red indicates larger gaps (worse generalization). Dist. = Dist.trainDist.eval 100%, Prog. = ProgresstrainProgresseval 100%, Ret. = Ret.train Ret.eval. 100%, SR = SRtrainSReval Progresstrain SRtrain Dist.train Evaluation on train config Evaluation on eval config Generalization gap Distance Progress SR Return Distance Progress SR Return Dist., % Prog., % SR, % Ret., (abs.) Agent Background Distractors Effects Filters Layout 396.526.8 463.410.4 413.522.9 426.315.6 431.219.3 452.30.0 0.810.05 0.950.02 0.840.05 0.870.03 0.880.04 0.920.00 0.760.06 0.900.02 0.810.05 0.820.03 0.830.04 0.860.00 -292.7377.21 -118.332.6 -178.041.5 -224.364.0 -204.859.6 -118.60.0 386.926.1 322.747.5 397.023.6 337.710.8 380.618.1 434.10. 0.790.05 0.660.10 0.810.05 0.690.02 0.780.04 0.890.00 0.600.06 0.420.13 0.560.11 0.160.06 0.110.04 0.320.00 -408.888.5 -935.8249.6 -307.078.1 -725.165.6 -652.470.5 -279.50.0 2.4 30.5 4.0 20.8 11.7 4.0 2.5 30.5 3.6 20.7 11.4 3.3 21.1 53.3 30.9 80.5 86.8 62. 116.1 691.0 129.0 500.8 447.6 160.9 underlying control problem is identical (P train = eval, rtrain = reval) and the two configurations differ only in designated subset of rendering parameters. Benchmark construction. We first conduct pilot sweep over KAGE-Envs rendering parameters using standard PPO-CNN, adopted from the CleanRL (Huang et al., 2022) library2, trained from single RGB frame. Hyperparameters are reported in Appendix, Table 4. This sweep measures how individual rendering parameters affect out-ofdistribution performance when the control problem is fixed. Based on these results, we curate 34 trainevaluation configuration pairs that exhibit range of generalization behavior, including both severe and mild gaps. The selected pairs are grouped into six suites corresponding to distinct visual axes: agent appearance, background, distractors, effects, filters, and layout. In each pair, exactly one parameter within the target axis is changed between train and evaluation, while all other parameters are held fixed. Easier pairs are intentionally retained as sanity checks, ensuring that the benchmark distinguishes lack of generalization from lack of task competence. Evaluation protocol and metrics. For each train evaluation configuration pair, we run 10 independent training seeds and periodically evaluate the current policy on both configurations. For each run and metric, we record the maximum value attained over training, average these maxima across seeds to obtain per-configuration results, and then average within each suite to produce the axis-level summaries in Table 1. We use the maximum-over-training statistic to assess whether visual generalization gap is in principle mitigable by given method. Because generalization performance can be non-monotonic and peak at different iterations across runs, this aggregation provides an upper envelope on achievable transfer and avoids confounding results with arbitrary checkpoint selection. Generalization gap. We define the visual generalization gap as the performance difference between the training and evaluation configurations of pair. Figure 4 illustrates three 2https://github.com/vwxyzjn/cleanrl characteristic regimes observed in KAGE-Bench: (i) negligible gap, where train and eval performance coincide; (ii) moderate gap, where partial transfer occurs; and (iii) severe gap, where evaluation performance collapses despite strong training performance. Full learning curves for all 34 configuration pairs and all suites are reported in Appendix C. 7. Results Table 1 reports axis-level results for PPO-CNN under our maximum-over-training protocol: for each seed we take the maximum of each metric over training checkpoints, then average across 10 seeds and finally across configuration pairs within an axis. Figure 6 complements this summary with representative difficulty-scheduled evaluations: (left) we train on black background and evaluate on progressively richer backgrounds (black, black+white, black+white+red, black+white+red+green, black+white+red+green+blue); (right) we train with no distractors and evaluate with increasing numbers of same-asagent distractors (0, 1, 2, 3, 5, 7, 9, 11), where distractors match the agents shape and color. Across suites, training success rises rapidly, while evaluation success often saturates substantially lower, revealing persistent traineval gaps under purely visual shifts with fixed dynamics and rewards. Generalization is strongly axis-dependent (meanSEM). Ranking axes by success-rate degradation, the largest gaps Figure 6. Visual generalization gaps in single-axis shifts. Each panel shows training success rate (blue) and evaluation on progressively harder visual variants (colored curves). (Left) Backgrounds: trained on black background, evaluated with cumulative color additions (black black+white black+white+red etc.). (Right) Distractors: trained without distractors, evaluated with increasing numbers of same-as-agent distractors. Full results are presented in the Appendix B, Figure 7. 7 KAGE-Bench: Known-Axis Visual Generalization Table 2. Per-configuration results for KAGE-Bench (meanSEM). Each row corresponds to train-evaluation configuration pair within known-axis suite. For each run, we record the maximum value attained by each metric during training; these maxima are then averaged across 10 random seeds. We report Distance, Progress, Success Rate (SR), and Return for both train and eval configurations, together with the resulting generalization gaps. Abbreviations: bg = background, ag = agent, dist = distractor, skelet = skeleton. Generalization gaps are color-coded: green indicates smaller gaps (better generalization), while red indicates larger gaps (worse generalization). The full version of this table with performance across train and eval configurations is presented in the Appendix B, Table 3. ID Train config Eval config 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 1 2 3 1 2 3 4 5 6 7 8 9 1 line teal ag circle pink ag line pink ag skelet ag clown ag noise bg purple bg purple, lime, indigo bg purple, lime, indigo bg 128 images bg another image bg another image bg purple bg, skelet ag teal circle ag circle teal ag circle teal ag circle teal ag skelet ag black bg black bg black bg red, green, blue bg black bg one image bg 3 images bg black bg, skelet ag one image bg, skelet ag another image bg, skelet ag another image bg, skelet ag 3 images bg, skelet ag NPC skelets, skelet ag no dist., skelet ag NPC 27 sprites, skelet ag no dist., skelet ag sticky NPC skelets, skelet ag no dist., skelet ag sticky NPC 27 sprites, skelet ag no dist., skelet ag 7 same-as-ag shapes, circle teal ag no dist., circle teal ag circle indigo dist., circle teal ag no dist., circle teal ag light intensity 0.5 no effects light fallof 4.0 no effects light count 4 no effects brightness 1 no filters contrast 128 no filters saturation 0.0 no filters hue shift 180 no filters color jitter std 2.0 no filters gaussian noise std 100 no filters pixelate factor 3 no filters vinegrette strength 10 no filters radial light strength 1 no filters red layout cyan layout Agent Background Distractors Effects Filters Layout Generalization gap Dist., % Prog., % SR, % Ret., (abs.) 2.8 2.1 3.1 3.0 1.0 72.8 59.6 61.2 2.0 50.2 1.4 0.0 55.9 1.3 -0.1 0.6 0.1 5.0 1.5 12.8 3.9 14.5 21.6 25.8 20.3 18.0 12.6 23.5 -3.4 6.4 6.6 2.2 17.1 4.0 2.6 2.0 3.6 2.7 1.4 73.1 59.8 61.3 2.4 50.5 2.0 0.0 56.4 2.0 0.00 0.0 0.0 5.4 1.1 13.3 3.9 14.1 21.7 25.8 20.4 18.6 12.8 23.7 -3.6 6.5 6.6 2.4 16.7 3.3 30.0 14.1 31.3 21.4 8.3 98.9 92.2 98.9 18.8 93.3 9.6 -1.3 99.0 8.3 -1.0 1.4 0.0 31.4 14.4 92.0 42.4 71.4 72.5 95.5 95.6 91.5 98.0 98.8 91.3 85.6 34.3 80.0 98.3 62.8 189.7 52.5 125.3 84.2 128.7 1867.5 1591.3 1611.1 50.6 1266.6 170.1 22.7 1463.7 167.9 9.0 14.4 0.6 176.7 48.9 418.4 116.0 384.6 479.2 638.5 506.9 523.6 593.3 727.9 283.1 210.3 166.8 526.0 490.6 160.9 arise from filters (SR = 86.8%) and effects (80.5%), followed by layout (62.8%) and background (53.3%); distractors (30.9%) and agent appearance (21.1%) are comparatively milder  (Table 1)  . Background shifts impair both motion and completion. Averaged across background pairs, distance and progress drop by 30.5% and SR drops from 0.90 to 0.42 (SR = 53.3%), accompanied by large absolute return gap. In Figure 6 (left), evaluation success decreases monotonically as additional colors are cumulatively introduced into the background, while training success on the black background remains high, yielding clear dose-response trend. Photometric and lighting perturbations primarily break completion. For filters and effects, distance degradation is moderate (Dist = 11.7% and 20.8%), yet SR collapses (0.83 0.11 and 0.82 0.16; SR = 86.8% and 80.5%), indicating that motion and shaped reward can persist while success fails under photometric/lighting shifts. Small motion gaps can mask large completion gaps. Distractors and layout show small distance/progress gaps (34%) but sizable SR drops (30.9% and 62.8%). In Figure 6 (right), increasing same-as-agent distractors (011) progressively suppresses evaluation success with unchanged training success. Per-configuration behavior is heterogeneous. Table 2 includes both negligible-gap sanity checks and near-failure pairs, e.g., blacknoise backgrounds (SR = 98.9%), hue shift 180 (98.8%), light count 4 (95.5%), and 7 same-asagent distractors (92.0%). Within Background, training with more visual diversity reduces SR gaps. Appendix 8 provides full learning curves for all 34 pairs; some small return gaps arise because both train and eval fail, motivating joint reporting of distance, progress, and SR. Overall, PPOCNN is strong in-distribution but brittle under controlled visual shifts, with failures concentrated in task completion rather than basic locomotion. 8. Conclusion We introduced KAGE-Env, JAX-native RL environment for controlled studies of visual generalization that factorizes the observation process into independently configurable visual axes while keeping the underlying control problem fixed, enabling high-throughput evaluation via end-to-end compilation and large-scale parallel simulation. Building on this environment, we presented KAGE-Bench, standardized benchmark comprising six known-axis suites and 34 trainevaluation configuration pairs that isolate specific sources of visual shift and allow precise attribution of performance changes. Empirically, we find that visual generalization difficulty varies substantially across axes: background changes and photometric or lighting perturbations induce the most severe failures, often collapsing task success despite nontrivial progress, whereas agent-appearance shifts are comparatively benign. Overall, KAGE-Bench provides fast, reproducible, and diagnostic framework for evaluating pixel-based RL under controlled visual variation, and we expect it to support more systematic analysis of visual robustness and future work on richer shifts, broader task families, and alternative learning algorithms. KAGE-Bench: Known-Axis Visual Generalization"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was inspired and motivated by the Naruto3 series and its emphasis on never giving up, which served as continual source of motivation throughout the project."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning by introducing fast, reproducible benchmark for studying visual generalization in RL. We do not anticipate immediate negative societal impacts from releasing an evaluation environment and configuration suites; however, as with most progress in robust perception and control, improved generalization methods could enable more capable autonomous systems, which may have downstream applications with safety and misuse considerations. We hope KAGE-Env and KAGE-Bench support more rigorous and transparent evaluation of robustness, helping the community identify failure modes early and develop safer learning systems."
        },
        {
            "title": "References",
            "content": "Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Kuttler, H., Lefrancq, A., Green, S., Valdes, V., Sadik, A., et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. Bertoin, D. and Rachelson, E. Local feature swapping for generalization in reinforcement learning. arXiv preprint arXiv:2204.06355, 2022. Bertoin, D. et al. Saliency-guided q-networks. arXiv preprint arXiv:2209.09203, 2022. Bonnet, C., Luo, D., Byrne, D., Surana, S., Abramowitz, S., Duckworth, P., Coyette, V., Midgley, L. I., Tegegn, E., Kalloniatis, T., et al. Jumanji: diverse suite of scalable reinforcement learning environments in jax. arXiv preprint arXiv:2306.09884, 2023. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. Cetin, E., Ball, P. J., Roberts, S., and Celiktutan, O. Stabilizing off-policy deep reinforcement learning from pixels. arXiv preprint arXiv:2207.00986, 2022. Cherepanov, E., Kachaev, N., Kovalev, A. K., and Panov, A. I. Memory, benchmark & robots: benchmark for solving complex tasks with reinforcement learning. arXiv preprint arXiv:2502.10550, 2025. 3https://en.wikipedia.org/wiki/Naruto Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. Quantifying generalization in reinforcement learning. In International conference on machine learning, pp. 1282 1289. PMLR, 2019. Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 20482056. PMLR, 2020. Cobbe, K. W., Hilton, J., Klimov, O., and Schulman, J. Phasic policy gradient. In International Conference on Machine Learning, pp. 20202027. PMLR, 2021. Freeman, C. D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O. Braxa differentiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021. Hansen, N. and Wang, X. Generalization in reinforcement learning by soft data augmentation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 1361113617. IEEE, 2021. Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., and Araujo, J. G. Cleanrl: High-quality singlefile implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274): 118, 2022. URL http://jmlr.org/papers/ v23/21-1342.html. Jesson, A. and Jiang, Y. Improving generalization on the procgen benchmark with simple architectural changes and scale. arXiv preprint arXiv:2410.10905, 2024. Juliani, A., Khalifa, A., Berges, V.-P., Harper, J., Teng, E., Henry, H., Crespi, A., Togelius, J., and Lange, D. Obstacle tower: generalization challenge in vision, control, and planning. arXiv preprint arXiv:1902.01378, 2019. Kachaev, N., Kolosov, M., Zelezetsky, D., Kovalev, A. K., and Panov, A. I. Dont blind your vla: Aligning visual representations for ood generalization. arXiv preprint arXiv:2510.25616, 2025. Kim, K., Lanier, J., Baldi, P., Fowlkes, C., and Fox, R. Make the pertinent salient: Task-relevant reconstruction for visual control with distractions. arXiv preprint arXiv:2410.09972, 2024. Kirilenko, D., Vorobyov, V., Kovalev, A. K., and Panov, A. I. Object-centric learning with slot mixture module. arXiv preprint arXiv:2311.04640, 2023. Klepach, A., Nikulin, A., Zisman, I., Tarasov, D., Derevyagin, A., Polubarov, A., Lyubaykin, N., and Kurenkov, V. Object-centric latent action learning. arXiv preprint arXiv:2502.09680, 2025. 9 KAGE-Bench: Known-Axis Visual Generalization Korchemnyi, A., Kovalev, A. K., and Panov, A. I. Symbolic disentangled representations for images. arXiv preprint arXiv:2412.19847, 2024. Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. Lan, T., Srinivasa, S., Wang, H., and Zheng, S. Warpdrive: Extremely fast end-to-end deep multi-agent reinforcement learning on gpu. arXiv preprint arXiv:2108.13976, 2021. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented data. Advances in neural information processing systems, 33: 1988419895, 2020. Matthews, M., Beukman, M., Ellis, B., Samvelyan, M., Jackson, M., Coward, S., and Foerster, J. Craftax: lightning-fast benchmark for open-ended reinforcement learning. arXiv preprint arXiv:2402.16801, 2024. Mazoure, B., Ahmed, A. M., MacAlpine, P., Hjelm, R. D., and Kolobov, A. Cross-trajectory representation learning for zero-shot generalization in rl. arXiv preprint arXiv:2106.02193, 2021. Mirjalili, R., Julg, T., Walter, F., and Burgard, W. Augmented reality for robots (arro): Pointing visuomotor policies towards visual robustness. arXiv preprint arXiv:2505.08627, 2025. Nikulin, A., Kurenkov, V., Zisman, I., Agarkov, A., Sinii, V., and Kolesnikov, S. Xland-minigrid: Scalable metareinforcement learning environments in jax. Advances in Neural Information Processing Systems, 37:43809 43835, 2024. Ortiz, J., Dedieu, A., Lehrach, W., Guntupalli, J. S., Wendelken, C., Humayun, A., Swaminathan, S., Zhou, G., Lazaro-Gredilla, M., and Murphy, K. P. Dmc-vb: benchmark for representation learning for control with visual distractors. Advances in Neural Information Processing Systems, 37:65746602, 2024. Pshenitsyn, A., Panov, A., and Skrynnik, A. Camar: Continuous actions multi-agent routing. arXiv preprint arXiv:2508.12845, 2025. Rahman, M. M. and Xue, Y. Bootstrap state representation using style transfer for better generalization in deep reinforcement learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 100115. Springer, 2022. Raileanu, R. and Fergus, R. Decoupling value and policy for generalization in reinforcement learning. In International Conference on Machine Learning, pp. 87878798. PMLR, 2021. Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., and Fergus, R. Automatic data augmentation for generalization in deep reinforcement learning. arXiv preprint arXiv:2006.12862, 2020. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Sherki, D., Merkulov, D., Savina, A., and Muravleva, E. Perelman: Pipeline for scientific literature meta-analysis. technical report. arXiv preprint arXiv:2512.21727, 2025. Staroverov, A., Gorodetsky, A. S., Krishtopik, A. S., Izmesteva, U. A., Yudin, D. A., Kovalev, A. K., and Panov, A. I. Fine-tuning multimodal transformer models for generating actions in virtual and real environIEEE Access, 11:130548130559, 2023. doi: ments. 10.1109/ACCESS.2023.3334791. Stone, A., Ramirez, O., Konolige, K., and Jonschkowski, R. The distracting control suitea challenging benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722, 2021. Tao, S., Xiang, F., Shukla, A., Qin, Y., Hinrichsen, X., Yuan, X., Bao, C., Lin, X., Liu, Y., Chan, T.-k., et al. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425, 2024. Tomilin, T., Dai, T., Fang, M., and Pechenizkiy, M. Levdoom: benchmark for generalization on level difficulty in reinforcement learning. In In Proceedings of the IEEE Conference on Games, 2022. Ugadiarov, L., Vorobyov, V., and Panov, A. Object-centric dreamer. In Senn, W., Sanguineti, M., Saudargiene, A., Tetko, I. V., Villa, A. E. P., Jirsa, V., and Bengio, Y. (eds.), Artificial Neural Networks and Machine Learning ICANN 2025, pp. 153165, Cham, 2026. Springer Nature Switzerland. ISBN 978-3-032-04558-4. Wang, K., Kang, B., Shao, J., and Feng, J. Improving generalization in reinforcement learning with mixture regularization. Advances in Neural Information Processing Systems, 33:79687978, 2020. Weiss, N., Holmes, P., and Hardy, M. Course in Probability. Pearson Addison Wesley, 2005. ISBN 9780321189547. URL https://books.google. ru/books?id=p-rwJAAACAAJ. 10 KAGE-Bench: Known-Axis Visual Generalization Xia, F., Zamir, A. R., He, Z., Sax, A., Malik, J., and Savarese, S. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 90689079, 2018. Yang, H., Zhu, W., and Zhu, X. Generalization enhancement of visual reinforcement learning through internal states. Sensors, 24(14), 2024. ISSN 1424-8220. doi: 10.3390/s24144513. URL https://www.mdpi. com/1424-8220/24/14/4513. Yuan, Z., Yang, S., Hua, P., Chang, C., Hu, K., and Xu, H. Rl-vigen: reinforcement learning benchmark for visual generalization. Advances in Neural Information Processing Systems, 36:67206747, 2023. Zisselman, E., Lavie, I., Soudry, D., and Tamar, A. Explore to generalize in zero-shot rl. Advances in Neural Information Processing Systems, 36:6317463196, 2023. 11 KAGE-Bench: Known-Axis Visual Generalization A. Reducing Visual Shifts to State-Policy Shifts A.1. Problem setup. KAGE-Bench is constructed to isolate purely visual distribution shift. Formally, each environment instance is indexed by visual configuration ξ Ξ (e.g., the YAML parameters controlling background, filters, lighting, sprites), and ξ determines how latent simulator state is rendered into pixel observation Ω. This rendering mechanism is modeled as an observation kernel Oξ( s), meaning that, given the same latent state s, different ξ may produce different distributions over images. Crucially, KAGE-Bench enforces that ξ does not alter the control problem itself: the transition kernel ( s, a) and reward function r(s, a) are identical for all ξ. Hence, when we observe traintest gap after changing ξ, it cannot be caused by different dynamics or rewards; it must be caused by the interaction between the same observation-based policy and different rendering process. The key point is that policy trained on pixels, π(a o), does not directly specify actions as function of the latent state s, but only as function of the rendered image o. Therefore, the action distribution conditioned on the latent state depends on ξ through the distribution of renderings Oξ( s). The definition below formalizes this dependence by defining, for each ξ, an induced state policy (Definition A.1): Definition A.1 (Induced State Policy). Given visual configuration ξ Ξ, observation kernel Oξ( s), and pixel policy π( o), the induced state policy πξ : [0, 1] is defined as: πξ(B s) := (cid:90) Ω π(B o) Oξ(do s), A, S. (11) where is any measurable subset of the action space (i.e., A, the σ-algebra of measurable action sets). This represents the conditional distribution over actions given latent state s, obtained by marginalizing over the intermediate observation variable o. Takeaway: Lets fix latent state s. The environment may render multiple different images due to background choices, filters, effects, etc., depending on the visual configuration ξ. The policy π maps each to an action distribution. πξ(s) is the mixture of those action distributions weighted by how likely each is under Oξ(s). Thus πξ( s) is the effective action distribution at latent state induced by the pair (renderer Oξ, pixel policy π). Induced state policy is the conditional distribution of the action after integrating out (marginalizing) the intermediate observation variable o. In this sense, changing ξ is equivalent to changing the induced state policy: even if the pixel policy π(a o) is fixed, the effective mapping from latent states to action distributions changes because the policy is evaluated on different renderings. This reduction is fundamental for analysis: it converts visual generalization under observation shifts into standard policy shift problem in fixed latent MDP. A.2. Setting and central objects We work with the following measurable objects. Measurable spaces. (S, S) is the latent state space equipped with σ-algebra of measurable subsets; (A, A) is the action space equipped with σ-algebra A; (Ω, O) is the observation (pixel). Measurability ensures that probabilities and integrals used below are well-defined. MDP primitives. γ [0, 1) is the discount factor and ρ0 is the initial distribution on (S, S). Transition kernel (Markov kernel). ( s, a) specifies the environment dynamics. For every stateaction pair (s, a) A, ( s, a) is probability distribution over next states in S. Operationally, this means that after taking action at in state st, the next state is sampled as st+1 ( st, at). Reward function. : is measurable and bounded: := sup(s,a)SA r(s, a) < . Boundedness guarantees the discounted return (cid:80) t0 γtr(st, at) is integrable. Visual configuration space. Ξ indexes renderers. For each ξ Ξ, Oξ( s) is an observation kernel (a Markov kernel from (S, S) to (Ω, O)). Operationally, given latent state s, an image is sampled as Oξ( s). Reactive pixel policy. π( o) is Markov kernel from (Ω, O) to (A, A) (memoryless policy): given observation o, an action is sampled as π( o). 12 KAGE-Bench: Known-Axis Visual Generalization Latent MDP and visual POMDP. Definition A.2 (Latent MDP). The latent MDP is the underlying control problem defined as: := (S, A, P, r, ρ0, γ). This represents the true decision process with latent states S, actions A, transition kernel , reward function r, initial distribution ρ0, and discount factor γ. Definition A.3 (Visual POMDP). For each visual configuration ξ Ξ, define the visual POMDP as: Mξ := (S, A, P, r, Ω, Oξ, ρ0, γ). By construction, ξ affects only the observation kernel Oξ; in particular, the transition kernel and reward function are invariant across all ξ Ξ. A.3. Main theorem. Theorem A.4 (Visual shift reduces to state-policy shift by marginalization). Fix any ξ Ξ and any reactive pixel policy π( o). Let πξ be defined by Definition A.1. Then: 1. (Conditional action law.) For every time 0 and every measurable action set A, PMξ,π(at st) = πξ(B st) a.s. (12) That is, after conditioning on the latent state, the intermediate observation variable can be integrated out and the resulting action distribution is exactly πξ( st). 2. (Equality in law of stateaction processes.) The stateaction process (st, at)t0 induced by executing π in Mξ has the same law as the stateaction process induced by executing πξ in the latent MDP M. 3. (Return equivalence.) Consequently, the expected discounted return is preserved: J(π; Mξ) = J(πξ; M), J(π; Mξ) := EMξ,π (cid:104) (cid:88) (cid:105) γtr(st, at) . t= (13) A.4. Proof of Theorem A.4 Step 0 (Generative dynamics in Mξ). By definition of the POMDP Mξ (Definition A.3) and the reactive policy π, the interaction at time is: ot Oξ( st) at π( ot) st+1 ( st, at) (14) Equation (14) (top) represents the rendering step: it formalizes that pixels are generated from the latent state via Oξ. Equation (14) (middle) is the policy step: the agent samples an action using only the pixels. Equation (14) (bottom) is the environment dynamics: the next state depends only on (st, at) through and is independent of ot given (st, at). Therefore, observations influence the future only through their effect on the chosen action. Step 1 (Show conditional action law (12)). Fix time 0 and an arbitrary measurable set A. We compute P(at st) by conditioning on the intermediate variable ot (the observation). (1a) Law of total probability (tower property). Recall the tower property of conditional expectation (Weiss et al., 2005): for any integrable random variable and σ-algebras G1 G2, E[X G1] = E[E[X G2] G1] . (15) 13 KAGE-Bench: Known-Axis Visual Generalization This identity states that conditioning can be performed in stages: one may first condition on finer information set G2 and then average again while conditioning on the coarser information set G1. We apply (15) to the indicator random variable := I{at B}, which is integrable since it is bounded between 0 and 1. Recall that conditional probabilities can be written as conditional expectations of indicator functions: P(at G) = E[I{at B} G] . Next, we specify the two σ-algebras: G1 := σ(st), the σ-algebra generated by the latent state st (i.e., conditioning on knowing st), G2 := σ(ot, st), the σ-algebra generated by the pair (ot, st) (i.e., conditioning on knowing both the observation and the latent state). Clearly, G1 G2, since knowing (ot, st) includes knowing st. Applying (15) with these choices gives PMξ,π(at st) = EMξ,π[I{at B} st] = EMξ,π (cid:2)EMξ,π[I{at B} ot, st] (cid:12) (cid:12) st (cid:3) . Finally, rewriting the inner conditional expectation again as conditional probability yields PMξ,π(at st) = EMξ,π (cid:2)PMξ,π(at ot, st) (cid:12) (cid:12) st (cid:3) . (16) This equality formalizes the intuitive idea that, to compute the probability of choosing an action in given the latent state st, one may first compute this probability given the more detailed information (ot, st) and then average over all possible observations ot that can occur when the state is st. (1b) Use the policy sampling rule. Recall from the interaction dynamics that, at time t, once the observation ot is generated, the action is sampled according to the policy: This means that the conditional distribution of at given ot is exactly π( ot). at π( ot). Formally, for any measurable action set A, PMξ,π(at ot) = π(B ot). Moreover, because the policy is reactive (memoryless), the action depends on the current observation ot but not directly on the latent state st once ot is known. Therefore, conditioning additionally on st does not change the conditional distribution: PMξ,π(at ot, st) = PMξ,π(at ot) = π(B ot). (17) This equality expresses the fact that the policy fully mediates the influence of the observation on the action, and no additional information about st is used once ot has been observed. (1c) Substitute (17) into (16). PMξ,π(at st) = EMξ,π[π(B ot) st] . (18) 14 KAGE-Bench: Known-Axis Visual Generalization At this point, the only remaining randomness inside the conditional expectation comes from ot given st. (1d) Use the observation sampling rule. Since ot st Oξ( st) by (14), the conditional expectation in (18) can be written as an integral with respect to the measure Oξ( st): EMξ,π[π(B ot) st] = (cid:90) Ω π(B o) Oξ(do st). (19) This step is precisely what averaging over renderings means: we are averaging the policys probability of selecting an action in over all images that can be rendered from st under configuration ξ. (1d) Use the observation sampling rule. At this point, the random quantity inside the conditional expectation in (18) is π(B ot), and the only remaining source of randomness is the observation ot given the latent state st. By the generative dynamics of the visual POMDP (14), the observation at time is sampled according to the observation kernel: ot st Oξ( st). Therefore, conditioning on st, the random variable π(B ot) is distributed according to the pushforward of Oξ( st) through the function (cid:55) π(B o). By the definition of conditional expectation with respect to Markov kernel, this conditional expectation can be written as an integral over the observation space: EMξ,π[π(B ot) st] = (cid:90) Ω π(B o) Oξ(do st). (20) This expression makes explicit what is meant by averaging over renderings: for fixed latent state st, we take all images that the renderer may produce under configuration ξ, weight the policys action probability π(B o) by how likely each image is under Oξ( st), and sum (integrate) these contributions. The result is the average probability of selecting an action in after accounting for all possible renderings of the same latent state. (1e) Recognize the induced policy definition. By Definition A.1, the right-hand side of (20) equals πξ(B st). Therefore, which is exactly (12). This completes Item 1. PMξ,π(at st) = πξ(B st), Step 2 (Equality in law of stateaction processes.). We now show that the stateaction process in Mξ under π evolves exactly as in the latent MDP under πξ. (2a) Effective action selection given st. Item 1 implies that, conditional on st, the action at has distribution πξ( st). Hence, if we are interested only in the joint process (st, at) (and not in ot), we may replace the two-step procedure by the single step ot Oξ( st), at π( ot) at πξ( st), without changing the conditional distribution of at given st. (2b) State transition given (st, at) is identical. Under Mξ, the next state satisfies st+1 ( st, at) by (14). This is exactly the same transition rule as in the latent MDP M, and it depends only on (st, at). (2c) Conclude identical recursion. Combining (2a) and (2b), the pair (st, at) evolves according to s0 ρ0, at πξ( st), st+1 ( st, at). This is precisely the generative definition of executing the state policy πξ in the latent MDP M. Therefore, the joint laws of (s0, a0, s1, a1, . . .) coincide under (Mξ, π) and (M, πξ), proving Item 2. Step 3 (Equality of expected discounted return). Define the discounted return random variable KAGE-Bench: Known-Axis Visual Generalization := (cid:88) t=0 γtr(st, at). We first verify that is integrable. By assumption, the reward function is bounded, meaning that for all (s, a) A, Therefore, for every time step t, r(s, a) < . γtr(st, at) γtr. Summing these bounds over and using that γ [0, 1) yields = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) t=0 (cid:12) (cid:12) γtr(st, at) (cid:12) (cid:12) (cid:12) (cid:88) t=0 γtr(st, at) (cid:88) t=0 γtr. The right-hand side is convergent geometric series: (cid:88) t=0 γtr = (cid:88) t=0 γt = 1 γ < . Hence, is almost surely finite and integrable. By Item 2, the stateaction processes (st, at)t0 have the same law under (Mξ, π) and (M, πξ). Since is measurable function of the entire stateaction trajectory and depends only on (st, at), it follows that has the same distribution under both constructions. In particular, their expectations coincide: J(π; Mξ) = EMξ,π[G] = EM,πξ [G] = J(πξ; M). This proves Item 3 and completes the proof. A.5. Interpretation for KAGE-Bench Theorem A.4 provides precise formal justification for how visual generalization should be interpreted in KAGE-Bench. Because the latent dynamics and reward function are identical across all visual configurations ξ, the theorem shows that changing ξ affects the learning problem only through the observation channel Oξ( s). For any fixed pixel policy π(a o), this change manifests exclusively as change in the induced state-conditional action distribution πξ( s). Crucially, the theorem establishes an exact equivalence in distribution at the level of latent stateaction trajectories: executing the observation-based policy π in the visual POMDP Mξ produces the same joint law over (st, at) as executing the induced state policy πξ in the latent MDP M. This result is purely representational. It does not claim that πξ is optimal, nor that marginalizing over observations improves performance. Rather, it shows that all effects of visual variation are captured entirely by the induced policy, without altering the underlying control problem. This equivalence is central to the design and interpretation of KAGE-Bench. It guarantees that any observed traintest performance gap under visual shift ξ ξ cannot be attributed to changes in dynamics, rewards, or task structure, but must correspond exactly to performance difference between two state policies πξ and πξ acting in the same latent MDP. As consequence, KAGE-Bench reduces visual generalization to well-defined policy shift problem in fixed MDP, enabling principled analysis using standard reinforcement learning tools and ensuring that benchmark results isolate perception-induced failures rather than confounding control effects. 16 KAGE-Bench: Known-Axis Visual Generalization A.6. Additional consequences: equivalence of trajectory-level metrics Theorem A.4 implies more than equality of expected return. Because it establishes equality in distribution of the latent stateaction process (st, at)t0, any performance metric that is measurable function of the latent trajectory inherits the same equivalence. We formalize this as corollary. Corollary A.5 (Equivalence of trajectory-level evaluation metrics). Fix any visual configuration ξ Ξ and reactive pixel policy π( o), and let πξ be the induced state policy. Let (st, at)t0 (Mξ, π) and (st, at)t0 (M, πξ)."
        },
        {
            "title": "Then for any measurable functional",
            "content": "it holds that and in particular whenever the expectation is well-defined. : (S A)N R, (cid:0)(st, at)t0 (cid:1) d= (cid:0)(st, at)t (cid:1), EMξ,π[F ] = EM,πξ [F ], Proof. By Item 2 of Theorem A.4, the joint laws of the stateaction trajectories coincide: LMξ,π (cid:0)(st, at)t0 (cid:1) = LM,πξ (cid:0)(st, at)t0 (cid:1). Applying any measurable function to two random elements with the same law yields random variables with the same law. Equality of expectations follows immediately. We now specialize Corollary A.5 to the concrete evaluation metrics used in KAGE-Bench. Corollary A.6 (Equivalence of distance, progress, and success metrics). Assume the latent state st contains onedimensional position variable xt R, with initial position xinit, and let > 0 denote the task completion threshold (e.g., = 490 in KAGE-Bench). Define the following trajectory-level metrics: Passed distance: for fixed horizon or terminal time. Normalized progress: Success indicator: Then, for each of these metrics, Fdist := xT xinit, Fprog := xT xinit . Fsucc := I{xT xinit D}. and moreover each metric has the same distribution under (Mξ, π) and (M, πξ). EMξ,π[F ] = EM,πξ [F ], {Fdist, Fprog, Fsucc}, Interpretation. Corollary A.6 shows that the equivalence established in Theorem A.4 applies not only to discounted return, but also to all trajectory-based evaluation metrics commonly reported in KAGE-Bench, including raw distance traveled, normalized progress, and binary success. These quantities depend only on the latent state trajectory (st)t0 and are therefore fully determined by the induced state policy πξ in the latent MDP. As result, differences in success rate, progress, or distance under visual shift ξ ξ are exactly differences between the induced state policies πξ and πξ acting in the same latent MDP. This further reinforces that KAGE-Bench isolates perception-induced failures: all reported metrics admit clean interpretation as properties of state-policy shift rather than changes in the underlying control task. 17 B. Extended figures and tables KAGE-Bench: Known-Axis Visual Generalization Figure 7 shows visual generalization gaps in single-axis shifts with training success rates and evaluation on progressively harder visual variants. Table 3 shows train and eval results across each reported metric across each config. Table 3. Per-configuration results for KAGE-Bench (meanSEM). Each row corresponds to train-evaluation configuration pair within known-axis suite. For each run, we record the maximum value attained by each metric during training; these maxima are then averaged across 10 random seeds. We report Distance, Progress, Success Rate (SR), and Return for both train and eval configurations, together with the resulting generalization gaps. Abbreviations: bg = background, ag = agent, dist = distractor, skelet = skeleton. Generalization gaps are color-coded: green indicates smaller gaps (better generalization), while red indicates larger gaps (worse generalization). ID Train config Eval config Evaluation on train config Evaluation on eval config Generalization gap Distance Progress SR Return Distance Progress SR Return Dist., % Prog., % SR, % Ret., (abs.) 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 1 2 3 1 2 3 4 5 6 7 8 1 Agent Background Distractors Effects Filters Layout teal circle ag circle teal ag circle teal ag circle teal ag skelet ag black bg black bg black bg red, green, blue bg black bg one image bg 3 images bg black bg, skelet ag one image bg, skelet ag 3 images bg, skelet ag no dist., skelet ag no dist., skelet ag no dist., skelet ag no dist., skelet ag no dist., circle teal ag no dist., circle teal ag line teal ag circle pink ag line pink ag skelet ag clown ag noise bg purple bg purple, lime, indigo bg purple, lime, indigo bg 128 images bg another image bg another image bg purple bg, skelet ag another image bg, skelet ag another image bg, skelet ag NPC skelets, skelet ag NPC 27 sprites, skelet ag sticky NPC skelets, skelet ag sticky NPC 27 sprites, skelet ag 7 same-as-ag shapes, circle teal ag circle indigo dist., circle teal ag no effects no effects no effects no filters no filters no filters no filters no filters no filters no filters no filters no filters light intensity 0.5 light fallof 4.0 light count 4 brightness 1 contrast 128 saturation 0.0 hue shift 180 color jitter std 2.0 gaussian noise std 100 pixelate factor 3 vinegrette strength 10 radial light strength 1 372.764.2 495.63.2 406.961.5 363.869.0 343.664. 455.543.3 452.746.1 456.642.2 415.355.9 455.743.2 497.71.1 411.957.2 493.05.8 498.00.8 498.00.6 352.674.5 407.061.4 360.570.7 456.642.3 405.460.9 498.50.3 416.154.7 406.162.1 456.941.5 457.141.7 497.61.3 498.90.1 453.344.0 407.261.5 455.843.1 371.564.1 416.155.3 323.271.9 0.760.13 1.010.01 0.830.13 0.740.14 0.700.13 0.930.09 0.920.09 0.930.09 0.850.11 0.930.09 1.020.00 0.840.12 1.010.01 1.020.00 1.020. 0.720.15 0.830.13 0.740.14 0.930.09 0.830.12 1.020.00 0.850.11 0.830.13 0.930.08 0.930.09 1.020.00 1.020.00 0.930.09 0.830.13 0.930.09 0.760.13 0.850.11 0.660.15 0.700.15 0.990.01 0.800.13 0.700.15 0.600.16 0.900.10 0.900.10 0.900.10 0.800.13 0.900.10 0.940.05 0.770.13 0.950.05 0.970.03 0.970.02 0.700.15 0.800.13 0.700.15 0.900.10 0.750.13 0.990. 0.770.13 0.800.13 0.890.10 0.900.10 0.940.06 1.000.00 0.860.10 0.800.13 0.900.10 0.670.15 0.800.13 0.600.16 -306.1151.3 -35.826.6 -225.3144.2 -454.8243.6 -441.8219.2 -111.2102.5 -116.8107.6 -104.294.7 -290.1186.6 -187.5178.6 -28.216.6 -277.7170.7 -25.716.3 -15.87.1 -25.714.6 -261.9129.0 -219.2143.6 -274.4135.4 -97.688.3 -199.9117.2 -14.96.0 -214.3127.5 -339.9221.8 -118.8101. -125.0115.2 -23.113.0 -9.30.5 -128.2104.3 -216.9138.9 -158.6149.3 -371.7178.6 -229.7146.8 -580.6257.6 362.461.9 485.08.0 394.160.0 353.067.5 340.163.1 123.929.7 182.952.4 177.140.0 406.854.8 226.838.9 490.52.2 411.857.6 217.552.5 491.61.9 498.40.4 350.575.1 406.761.3 342.368.0 449.741.6 353.653.6 479.22.8 355.646.2 318.446.8 339.030.4 364.333.9 408.012.5 435.84.8 346.934.4 421.129.0 426.840.5 346.959.4 407.244.9 268.058. 0.740.13 0.990.02 0.800.12 0.720.14 0.690.13 0.250.06 0.370.11 0.360.08 0.830.11 0.460.08 1.000.00 0.840.12 0.440.11 1.000.00 1.020.00 0.720.15 0.830.13 0.700.14 0.920.08 0.720.11 0.980.01 0.730.09 0.650.10 0.690.06 0.740.07 0.830.03 0.890.01 0.710.07 0.860.06 0.870.08 0.710.12 0.830.09 0.550.12 0.490.11 0.850.07 0.550.10 0.550.12 0.550. 0.010.00 0.070.06 0.010.00 0.650.12 0.060.02 0.850.04 0.780.13 0.010.00 0.890.03 0.980.02 0.690.15 0.800.13 0.480.11 0.770.09 0.060.01 0.570.04 0.220.04 0.220.04 0.040.01 0.040.01 0.080.01 0.020.01 0.010.00 0.070.01 0.130.03 0.440.10 0.160.04 0.010.00 -495.8158.2 -88.342.4 -350.6147.3 -539.0237.8 -570.5230.0 -1978.7111.3 -1708.1208.8 -1715.3164.1 -340.7181.0 -1454.1175.4 -198.230.8 -255.0150.0 -1489.4244.7 -183.632.6 -34.720. -276.2130.0 -218.6138.7 -451.1149.4 -146.582.5 -618.3156.8 -131.017.9 -598.9117.5 -819.1155.8 -757.250.8 -631.858.7 -546.750.7 -602.630.3 -856.195.5 -500.077.7 -368.9139.0 -538.5149.5 -755.791.1 -1071.2216.5 cyan layout red layout 452.342. 0.920.09 0.860.10 -118.689.1 434.139.3 0.890.08 0.320. -279.578.2 2.8 2.1 3.1 3.0 1.0 72.8 59.6 61.2 2.0 50.2 1.4 0.0 55.9 1.3 -0.1 0.6 0.1 5.0 1.5 12.8 3.9 14.5 21.6 25.8 20.3 18.0 12.6 23.5 -3.4 6.4 6.6 2.2 17. 4.0 2.6 2.0 3.6 2.7 1.4 73.1 59.8 61.3 2.4 50.5 2.0 0.0 56.4 2.0 0.00 0.0 0.0 5.4 1.1 13.3 3.9 14.1 21.7 25.8 20.4 18.6 12.8 23.7 -3.6 6.5 6.6 2.4 16. 3.3 30.0 14.1 31.3 21.4 8.3 98.9 92.2 98.9 18.8 93.3 9.6 -1.3 99.0 8.3 -1.0 1.4 0.0 31.4 14.4 92.0 42.4 71.4 72.5 95.5 95.6 91.5 98.0 98.8 91.3 85.6 34.3 80.0 98. 62.8 189.7 52.5 125.3 84.2 128.7 1867.5 1591.3 1611.1 50.6 1266.6 170.1 22.7 1463.7 167.9 9.0 14.4 0.6 176.7 48.9 418.4 116.0 384.6 479.2 638.5 506.9 523.6 593.3 727.9 283.1 210.3 166.8 526.0 490. 160.9 18 KAGE-Bench: Known-Axis Visual Generalization (a) Backgrounds (Distance) (b) Distractors (Distance) (c) Radial light (Distance) (d) Backgrounds (Progress) (e) Distractors (Progress) (f) Radial light (Progress) (g) Backgrounds (Return) (h) Distractors (Return) (i) Radial light (Return) (j) Backgrounds (Success Rate) (k) Distractors (Success Rate) (l) Radial light (Success Rate) Figure 7. Visual generalization gaps in single-axis shifts across all metrics. Each row shows different metric (Distance, Progress, Return, Success Rate), and each column shows different axis (Backgrounds, Distractors, Radial light effect). Training performance is shown in blue, evaluation on progressively harder visual variants in colored curves. Backgrounds: trained on black background, evaluated with cumulative color additions (black black+white black+white+red etc.). Distractors: trained without distractors, evaluated with increasing numbers of same-as-agent distractors. Radial light effect: trained without radial light effects, evaluated with increasing radial light strength. 19 C. Benchmark Training Details KAGE-Bench: Known-Axis Visual Generalization This appendix reports the full learning curves for the PPO-CNN baseline on all 34 KAGE-Bench train-evaluation configuration pairs. At each logging checkpoint, we evaluate the current policy on both the corresponding training configuration (in-distribution) and its paired evaluation configuration (out-of-distribution), and plot the resulting metrics over environment steps. Figures are grouped by generalization axis: Agent Appearance (Figure 8); Background (Figure 9, Figure 10); Distractors (Figure 12); Effects (Figure 13); Filters (Figure 14, Figure 15); and Layout (Figure 11). Config 1 Config 2 Config 3 Config Config 5 Figure 8. Agent appearance training metrics for Configs 15: covering passed distance, progress, success rate, and episodic return; curves are meansem across 10 independent runs. 20 KAGE-Bench: Known-Axis Visual Generalization Config 1 Config Config 3 Config 4 Config 5 Config 6 Figure 9. Background-only training metrics for Configs 16: showing passed distance, progress, success-once, and episodic return curves that represent meansem across 10 independent runs. KAGE-Bench: Known-Axis Visual Generalization Config 7 Config 8 Config 9 Config 10 Figure 10. Background-only training metrics for Configs 710: showing passed distance, progress, success-once, and episodic return curves that represent meansem across 10 independent runs. Figure 11. Layout training metrics for Config 1: plotting passed distance, progress, success rate, and episodic return; traces are meansem across 10 independent runs. Config 1 22 KAGE-Bench: Known-Axis Visual Generalization Config 1 Config Config 3 Config 4 Config 5 Config 6 Figure 12. Distractors training metrics for Configs 16: with passed distance, progress, success-once, and episodic return curves; each trace is meansem across 10 independent runs. KAGE-Bench: Known-Axis Visual Generalization Config 1 Config 2 Config 3 Figure 13. Effects training metrics for Configs 13: showing passed distance, progress, success rate, and episodic return; curves depict meansem across 10 independent runs. Config Config 2 Config 3 Figure 14. Filters training metrics for Configs 13: displaying passed distance, progress, success-once, and episodic return; curves show meansem across 10 independent runs. 24 KAGE-Bench: Known-Axis Visual Generalization Config Config 5 Config 6 Config 7 Config 8 Config 9 Figure 15. Filters training metrics for Configs 49: displaying passed distance, progress, success-once, and episodic return; curves show meansem across 10 independent runs. 25 D. Generalization axes review KAGE-Bench: Known-Axis Visual Generalization (a) H=128, W=128 (b) H=128, W=256 (c) H=128, W= Figure 16. Global Screen Settings. Representative renders under different screen configurations. YAML parameter: H: 128, W: 128. (a) black (b) cyan (c) lime (d) magenta (e) purple (f) noise Figure 17. Background Color Modes. Representative renders under different background color configurations. YAML parameter(s): background.mode (color/noise) and background.color names controlling the palette for the color mode. (a) Image 1 (b) Image 2 (c) Image 3 (d) Image (e) Image 5 (f) Image 6 (g) Image 7 (h) Image 8 (i) Image 9 (j) Image (k) Image 11 (l) Image 12 Figure 18. Background Image Modes. Representative renders under different background image configurations. YAML parameter(s): background.mode: \"image\", background.image paths. (a) Sprite 1 (b) Sprite (c) Sprite 3 (d) Sprite 4 (e) Sprite 5 (f) Sprite 6 Figure 19. Agent Sprites. character.use sprites: Representative renders showing different agent sprite configurations. YAML parameter(s): true, character.sprite paths. 26 KAGE-Bench: Known-Axis Visual Generalization (a) Circle (b) Cross (c) Diamond (d) Line (e) Square (f) Star Figure 20. Agent Shapes. character.use shape: Representative renders showing different agent shape configurations. YAML parameter(s): true, character.shape types. (a) Coral (b) Green (c) Magenta (d) Navy (e) Red (f) Teal Figure 21. Agent Colors. character.use shape: Representative renders showing different agent color configurations. YAML parameter(s): true, character.shape colors. (a) NPC Configuration 1 (b) NPC Configuration 2 Figure 22. NPCs. Representative renders showing different NPC configurations. YAML parameter(s): npc.enabled: npc.sprite dir, npc.min npc count, npc.max npc count. true, (a) Sprite 1 (b) Sprite 2 (c) Sprite 3 (d) Sprite 4 (e) Sprite 5 (f) Sprite Figure 23. npc.sticky enabled: Sticky NPCs. Representative renders showing different sticky NPCs configurations. YAML parameter(s): true, npc.min sticky count, npc.max sticky count, npc.sticky sprite dirs. (a) Sprite 1 (b) Sprite (c) Sprite 3 (d) Sprite 4 (e) Sprite 5 (f) Sprite 6 Shape Distractors. Figure 24. parameter(s): distractors.shape colors. distractors.enabled: Representative renders showing different shape distractors configurations. YAML distractors.shape types, true, distractors.count, KAGE-Bench: Known-Axis Visual Generalization (a) -1 (b) -0.5 (c) 0 (d) 0.5 (e) Figure 25. Brightness Levels. Representative renders showing different brightness configurations. YAML parameter(s): filters.brightness varied from -1 to 1, with other filters.* held at their base values. (a) 0.1 (b) 0.5 (c) 1 (d) 2 (e) (f) 128 Figure 26. filters.contrast varies from 0.1 to 128 while other filters stay at defaults. Contrast Levels. Representative renders showing different contrast configurations. YAML parameter(s): (a) 0. (b) 0.75 (c) 1 (d) 1.5 (e) 2 Figure 27. Gamma Levels. Representative renders showing different gamma configurations. YAML parameter(s): filters.gamma is swept from 0.5 to 2.0 (others default). (a) (b) 0.5 (c) 1 (d) 1.5 (e) 2 Figure 28. filters.saturation ranges from 0 to 2 with other filters unchanged. Saturation Levels. Representative renders showing different saturation configurations. YAML parameter(s): (a) -180 (b) -135 (c) -90 (d) -45 (e) 0 (f) (g) 90 (h) 135 (i) 180 Figure 29. Hue Shift Levels. Representative renders showing different hue shift configurations. YAML parameter(s): filters.hue shift sweeps through [-180, 180]. 28 KAGE-Bench: Known-Axis Visual Generalization (a) -1 (b) -0.5 (c) 0 (d) 0.5 (e) 1 Figure 30. Color Temperature Levels. Representative renders showing different color temperature configurations. YAML parameter(s): filters.color temp is varied between -1 and 1. (a) 0 (b) 1 (c) 5 (d) 10 (e) 100 Figure 31. Color Jitter Standard Deviation Levels. Representative renders showing different color jitter configurations. This is stochastic effect, and the jittering changes for each timestep. YAML parameter(s): filters.color jitter std. (a) 0 (b) 1 (c) 10 (d) 50 (e) 100 (f) Figure 32. Gaussian Noise Standard Deviation Levels. Representative renders showing different gaussian noise configurations. This is stochastic effect, and the noise changes for each timestep. YAML parameter(s): filters.gaussian noise std ranges from 0 to 200, with other filter noise terms disabled. (a) 1 (b) 2 (c) 3 (d) 4 (e) (f) 6 Figure 33. Pixelate Factor Levels. Representative renders showing different pixelate factor configurations. YAML parameter(s): filters.pixelate factor steps from 1 to 6 while other filters stay default. (a) 0 (b) 0.5 (c) 1 (d) (e) 5 (f) 10 Figure 34. Vignette Strength Levels. Representative renders showing different vignette strength configurations. YAML parameter(s): filters.vignette strength is increased from 0 to 10 (others default). 29 KAGE-Bench: Known-Axis Visual Generalization (a) (b) 0.25 (c) 0.5 (d) 1 (e) 2 Figure 35. Radial Light Strength Levels. Representative renders showing different radial light strength configurations. YAML parameter(s): filters.radial light strength spans 0 to 2. (a) none (b) vintage (c) retro (d) cyberpunk (e) horror (f) noir Figure 36. Pop Filter List Presets. Representative renders showing different pop filter preset configurations. YAML parameter(s): filters.pop filter list. (a) 0.1 (b) 0.5 (c) 1 (d) 2 (e) 5 Figure 37. Point Light Intensity Levels. Representative renders showing different point light intensity configurations. YAML parameter(s): effects.point light enabled: true, effects.point light intensity varies from 0.1 to 5. (a) 0.01 (b) 0.1 (c) 0.2 (d) 0.5 (e) 1 Figure 38. Point Light Radius Levels. Representative renders showing different point light radius configurations. YAML parameter(s): effects.point light radius sweeps from 0.01 to 1 (others fixed). 30 KAGE-Bench: Known-Axis Visual Generalization (a) 1 (b) 2 (c) 2.5 (d) (e) 4 Figure 39. Point Light Falloff Levels. Representative renders showing different point light falloff configurations. YAML parameter(s): effects.point light falloff varies between 1 and 4. (a) 1 (b) 2 (c) 3 (d) (e) 5 Figure 40. Point Light Count Levels. Representative renders showing different point light count configurations. YAML parameter(s): effects.point light count increases from 1 to 5. (a) gold (b) warm white (c) red (d) green (e) blue (f) fire Figure 41. Point Light Color Names. Representative renders showing different point light color configurations. YAML parameter(s): effects.point light color names lists the named lights (gold, warm white, red, green, blue, fire). (a) cyan (b) yellow (c) red (d) green (e) pink (f) lime Figure 42. Layout Colors. Representative renders showing different layout.layout colors selects the per-level palette. layout color configurations. YAML parameter(s): E. Backgrounds KAGE-Bench: Known-Axis Visual Generalization Figure 43. Background palette used in KAGE-Bench experiments (128 unique scenes). Images are 128128 pixels and located in src/kage bench/assets/backgrounds. 32 F. Agents Sprites KAGE-Bench: Known-Axis Visual Generalization boy chibi clown cowboy cowboy 2 dark knight dark knight 2 dark skeleton dog elder skeleton elf enchanter farmer girl girl 2 girl 3 knight man robot scientist skeleton spy thief warrior woman woman 2 woman wizard Figure 44. Representative idle states for each of the 27 animated agent sprites used in KAGE-Env. By default, sprite bounding box is 2416 pixels. Sprites are located in src/kage bench/assets/sprites. 33 (cid:4) G. YAML Configuration Details KAGE-Bench: Known-Axis Visual Generalization # ============================================================================== # JAX Platformer Environment Configuration # ============================================================================== # This configuration file allows precise control over all environment aspects. # Edit this file to customize backgrounds, characters, NPCs, physics, and usage. # # Load this config using: # # # # ============================================================================== from kage_bench import EnvConfig, load_config_from_yaml config = load_config_from_yaml(\"custom_config.yaml\") env = PlatformerEnv(config) # ------------------------------------------------------------------------------ # 0. Episode Settings # ------------------------------------------------------------------------------ episode_length: 500 forward_reward_scale: 0.2 jump_penalty: 10.0 timestep_penalty: 0.1 idle_penalty: 5.0 dist_to_success: 490.0 # Default: 500 Max steps per episode # Default: 0.2 Reward for moving forward # Default: 10.0 Penalty for jumping # Default: 0.1 Per-timestep reward penalty # Default: 5.0 Penalty when does not change # Default: 490.0 Passed distance needed for success # ------------------------------------------------------------------------------ # 1. Global Screen Settings # ------------------------------------------------------------------------------ H: 128 W: # Default: 128 Screen height in pixels # Default: 128 Screen width in pixels # 2. Background Settings # ------------------------------------------------------------------------------ background: # * Mode (Default: \"black\"): # \"black\" - just black background # \"image\" - image backgrounds # \"noise\" - white noise background (unique for each episode) # \"color\" - color backgrounds mode: \"image\" # * Image mode settings (ignired if mode != \"image\"): # * Please, choose only one of the following options image_dir, image_paths, image_path: # [Option 1] Specify directory to load all images from (randomly selected per episode) image_dir: \"src/kage_bench/assets/backgrounds\" # * Will use all the 128 images # [Option 2] Or specify list of explicit paths: # image_paths: # * Will use only listed images # # # - \"src/kage_bench/assets/backgrounds/bg-1.jpeg\" - \"src/kage_bench/assets/backgrounds/bg-64.jpeg\" - \"src/kage_bench/assets/backgrounds/bg-128.jpeg\" # [Option 3] Or force single image: # image_path: \"path/to/your/image.jpeg\" # Parallax/Tiling (ignored if mode != \"image\") parallax_factor: 0.5 # Default: 0.5 0.0 = static, 0.5 = slow scroll, 1.0 = locked to camera, <0 = moves (cid:44) left, >1 = moves fast tile_horizontal: true # Default: true Repeat image horizontally for infinite worlds # Dynamic switching (change background during the episode gif effect) switch_frequency: 0.0 # Default: 0.0 Probability per step (0.0 = never switch, 1.0 = every step) # * Color mode settings (ignored if mode != \"color\"): # List of colors to randomly select from per episode # Available colors: # \"black\", \"white\", \"red\", \"orange\", \"yellow\", \"green\", \"cyan\", \"blue\", # \"purple\", \"pink\", \"brown\", \"gray\", \"lime\", \"teal\", \"indigo\", \"magenta\" color_names: [\"purple\", \"teal\", \"indigo\"] (cid:6) Code 3. YAML excerpt showing reward structure configuration and background settings for the KAGE-Env. (cid:5) (cid:13) 34 KAGE-Bench: Known-Axis Visual Generalization # ------------------------------------------------------------------------------ # 3. Character Settings (Player) # ------------------------------------------------------------------------------ character: width: 16 Recommended to not change height: 24 # Default: 24 Character height in pixels Recommented to not change # Default: 16 Character width in pixels # * Sprite Settings (ignored if use_shape: true): use_sprites: true # Default: true Use sprites for character # * Please, choose only one of the following options sprite_dir, sprite_paths, sprite_path: # [Option 1] Directory containing sprite subdirectories (auto-discovers all subdirs as skins): sprite_dir: \"src/kage_bench/assets/sprites\" # Will auto-discover and use all subdirectories like clown/, (cid:44) robot/, skelet/, etc. # [Option 2] List of directories for multiple skins (randomly selected per episode): # NOTE: Each directory should contain .png animation frames (e.g., walk-1.png, walk-2.png) # sprite_paths: # - \"src/kage_bench/assets/sprites/clown\" # - \"src/kage_bench/assets/sprites/dark_knight\" # - \"src/kage_bench/assets/sprites/skeleton\" # Skin 1 # Skin 2 # Skin 3 # [Option 3] Single sprite directory: # sprite_path: \"path/to/your/folder/with/sprite\" # Animation settings (ignored if use_sprites: false): enable_animation: true animation_fps: 12.0 idle_sprite_idx: # Default: true Enable sprite animation (if false, use only first frame) # Default: 12.0 Frames per second for sprite animation # Default: 0 Which sprite to use when idle (typically first) # * Shape Settings (ignored if use_sprites: true): use_shape: false # Default: false Use shapes for character # Available shapes: # \"circle\", \"cross\", \"diamond\", \"ellipse\", \"line\", # \"polygon\", \"square\", \"star\", \"triangle\" shape_types: [\"circle\", \"star\"] # Available colors: # \"red\", \"green\", \"blue\", \"orange\", \"yellow\", \"violet\", \"magenta\", #\"cyan\", \"pink\", \"brown\", \"purple\", \"lime\", \"navy\", \"maroon\", # \"olive\", \"teal\", \"indigo\", \"coral\", \"gold\", \"silver\", \"white\" shape_colors: [\"teal\", \"indigo\"] shape_rotate: true shape_rotation_speed: 5.0 # Default: 5.0 Rotation speed in degrees per second # Default: true Enable shape rotation (cid:6) Code 4. Character configuration showing sprite directories, animation controls, and optional shape fallback settings. (cid:4) (cid:5) (cid:13) 35 KAGE-Bench: Known-Axis Visual Generalization (cid:4) # ------------------------------------------------------------------------------ # 4. NPC Settings (Non-Player Characters) # ------------------------------------------------------------------------------ npc: # Default: true Enable NPC system: # * World-Fixed NPCs (stand on platforms) enabled: true min_npc_count: 5 max_npc_count: 20 spawn_y_offset: 0 animation_fps: 12.0 # Default: 12.0 Animation speed for NPCs # Default: true Enable World-Fixed NPC system: # Default: 5 Minimum number of NPCs per level # Default: 20 Maximum number of NPCs per level # Default: 0 Offset from ground in pixels (positive = up, negative = down) # * Please, choose only one of the following options sprite_dir, sprite_paths, sprite_path: # [Option 1] Specify directory to auto-discover all sprite subdirectories: sprite_dir: \"src/kage_bench/assets/sprites\" # Will auto-discover all subdirs like robot/, girl/, skelet/, etc (cid:44) . # [Option 2] Or specify list of explicit paths: # sprite_paths: # # # - \"src/kage_bench/assets/sprites/clown\" - \"src/kage_bench/assets/sprites/dark_knight\" - \"src/kage_bench/assets/sprites/skeleton\" # Skin 1 # Skin 2 # Skin 3 # [Option 3] Or force single sprite directory: # sprite_path: \"path/to/your/folder/with/sprite\" # * Sticky NPCs (follow camera and always visible in observation): sticky_enabled: false min_sticky_count: 1 max_sticky_count: 5 # * Please, choose only one of the following options sticky_sprite_dir, sticky_sprite_dirs, sticky_sprite_path # Default: false Enable sticky NPC system: # Default: 1 Minimum number of sticky NPCs per level # Default: 5 Maximum number of sticky NPCs per level (cid:44) : # [Option 1] Specify directory to auto-discover all sticky sprite subdirectories: sticky_sprite_dir: \"src/kage_bench/assets/sprites\" # Will auto-discover all subdirs # [Option 2] Or specify list of explicit paths (backward compatibility): # sticky_sprite_dirs: # * Will use only listed sprite directories # # # - \"src/kage_bench/assets/sprites/clown\" - \"src/kage_bench/assets/sprites/dark_knight\" - \"src/kage_bench/assets/sprites/skeleton\" # Skin 1 # Skin 2 # Skin # [Option 3] Or force single sticky sprite directory: # sticky_sprite_path: \"path/to/your/folder/with/sprite\" # Sticky NPCs settings: sticky_can_jump: true sticky_jump_probability: 0.01 sticky_y_min_offset: -40 sticky_y_max_offset: -10 sticky_x_offsets: [] # Default: true Enable jumping for sticky NPCs # Default: 0.01 Probability of jumping per step # Default: -40 # Default: -10 # Default: [] Minimum offset from ground (negative = higher) Maximum offset from ground (0 = on ground) Camera-relative offsets for sticky NPCs (e.g., [-40, 0, (cid:44) 40]). If not provided, NPCs will be spread around agent sticky_x_min: -60 sticky_x_max: # Default: -60 # Default: 60 Minimum offset from center Maximum offset from center # ------------------------------------------------------------------------------ # 5. Distractors Settings # ------------------------------------------------------------------------------ distractors: # Whether to enable distractors, i.e. moving geometric shapes on background (always visible in observation) enabled: true # Default: false Enable distractors system: count: 5 # Default: 5 Number of distractors per level # Available shapes: # \"circle\", \"cross\", \"diamond\", \"ellipse\", # \"line\", \"polygon\", \"square\", \"star\", \"triangle\" shape_types: [\"circle\", \"star\", \"cross\"] # Available colors: # \"red\", \"green\", \"blue\", \"orange\", \"yellow\", \"violet\", \"magenta\", # \"cyan\", \"pink\", \"brown\", \"purple\", \"lime\", \"navy\", \"maroon\", # \"olive\", \"teal\", \"indigo\", \"coral\", \"gold\", \"silver\", \"white\" shape_colors: [\"red\", \"green\", \"blue\"] # Dynamics: can_move: true min_speed: 0.0 max_speed: 1.0 can_rotate: true min_rotation_speed: -0.3 # Default: -3.0 Minimum rotation speed (degrees per step) max_rotation_speed: 0.3 min_size: 4 max_size: # Default: true Whether distractors move around # Default: 0.0 Minimum movement speed (pixels per step) # Default: 2.0 Maximum movement speed (pixels per step) # Default: true Whether distractors rotate # Default: 3.0 Maximum rotation speed (degrees per step) # Default: 4 Minimum size # Default: 12 Maximum size Code 5. NPC, sticky NPC, and distractor parameters that govern entity counts, visuals, and motion cues. 36 (cid:6) (cid:5) (cid:13) KAGE-Bench: Known-Axis Visual Generalization (cid:4) # ------------------------------------------------------------------------------ # 6. Filter & Effect Settings # ------------------------------------------------------------------------------ filters: brightness: 0.0 contrast: 1.0 gamma: 1.0 saturation: 1.0 hue_shift: 0.0 color_temp: 0.0 # Default: 0.0 [-1, 1] additive exposure, scaled in code # Default: 1.0 >0, scales around mid-gray (128) # Default: 1.0 [0.5, 2.0] power-law on [0,1] # Default: 1.0 [0, 2] HSV multiplier # Default: 0.0 [-180, 180] degrees, HSV hue offset # Default: 0.0 [-1, 1] warm(+R,-B) vs cool(+B,-R) # Stochastic effects (require PRNG key): color_jitter_std: 0.0 gaussian_noise_std: 0.0 poisson_noise_scale: 0.0 # Default: 0.0 [0,1], shot noise with lambda = img*scale # Default: 0.0 >=0, std of 3x3 RGB mixing perturbation # Default: 0.0 >=0, pixelwise N(0, stdˆ2) in [0,255] # Spatial / detail transforms: blur_sigma: 0.0 sharpen_amount: 0.0 pixelate_factor: 1 # Default: 0.0 >=0, box-blur approximation strength # Default: 0.0 >=0, unsharp mask gain # Default: 1 int>=1, down/up nearest (1 disables) # Global shading / lighting overlays: vignette_strength: 0.0 # Default: 0.0 >=0, edge darkening factor (code expects [0,1]) radial_light_strength: 0.0 # Default: 0.0 >=0, additive center light (code expects [0,1]) # Optional preset stack: pop_filter_list: [] # Default: [] [\"vintage\",\"retro\",\"cyberpunk\",\"horror\",\"noir\"] # ------------------------------------------------------------------------------ # 7. Effects Settings # ------------------------------------------------------------------------------ effects: # Point light effect point_light_enabled: false point_light_intensity: 1.0 point_light_radius: 0.1 point_light_falloff: 2 # Multiple random lights point_light_count: 4 # Available colors: # Default: false Enable/disable point light effects # Default: 1.0 Light intensity in [0.1, 5.0] # Default: 0.1 Light radius as fraction of image size [0.01, 1.0] # Default: 2.0 Falloff exponent in [1.0, 4.0], higher = sharper # Default: 1 Number of lights in [1, 5] # warm_white, cool_white, yellow, orange, red, # green, cyan, blue, purple, pink, gold, fire point_light_color_names: [\"blue\", \"pink\", \"gold\"] # Default: [\"warm_white\"] List of color names from (cid:44) LIGHT_COLORS # ------------------------------------------------------------------------------ # 8. Level Layout # ------------------------------------------------------------------------------ layout: length: 2048 height_px: 128 base_ground_y: 96 pix_per_unit: 2 ground_thickness: 2 run_width: 25 p_change: 0. (cid:44) change # Default: 2048 Length of the level in pixels # Default: 128 Height of the level in pixels # Default: 96 [70, 127] Hight of the ground level. Higher - lower ground level # Default: 2 [0, 3] Pixels per height unit. 0 - flat, 3 - larger steps # Default: 2 [1, 10] Thickness of the ground band in pixels # Default: 20 [1, 60] Widths of the stair in pixels # Default: 0.7 [0, 1] Probability of height change. 0 -never change, 1 -always p_up_given_change: 0.5 # Default: 0.5 [0, 1] Probability of height increase given height change. 0 -always (cid:44) decrease, 1 -always increase min_step_height: 5 max_step_height: # Default: 5 [1, 17] Minimum height of the step in pixels # Default: 10 [1, 17] Maximum height of the step in pixels # Colors for platforms (randomly selected per episode). Available colors: # \"black\", \"white\", \"red\", \"orange\", \"yellow\", \"green\", \"cyan\", \"blue\", # \"purple\", \"pink\", \"brown\", \"gray\", \"lime\", \"teal\", \"indigo\", \"magenta\" layout_colors: [\"cyan\"] # Default: [\"cyan\"] List of color names from COLOR_PALETTE # ------------------------------------------------------------------------------ # 9. Physics Settings # ------------------------------------------------------------------------------ physics: gravity: 0.75 move_speed: 1 jump_force: -7.5 ground_friction: 0.8 air_resistance: 0.95 max_fall_speed: 8.0 # Default: 0.75 [0.1, 1.0] Gravity force # Default: 1 int and > 1 Move speed # Default: -7.5 [-10.0, 0.0] Jump force. Negative is upwards # Default: 0.8 [0.1, 1.0] Friction force # Default: 0.95 [0.1, 1.0] Air resistance force # Default: 8.0 [0.1, 10.0] Maximum fall speed (cid:6) Code 6. Filter, lighting, layout, and physics defaults that define the environment appearance and agent dynamics. 37 (cid:5) (cid:13) KAGE-Bench: Known-Axis Visual Generalization Table 4. PPO-CNN training and model hyperparameters. We report the exact settings used in our JAX/Flax PPO baseline (CNN encoder with separate actor/critic heads) for KAGE-Env."
        },
        {
            "title": "Category",
            "content": "Environment & rollout Parallel envs Rollout length Total timesteps Discount / GAE Observation Frame stacking Auto-reset Reward normalization Reward clipping Optimization & PPO"
        },
        {
            "title": "Setting",
            "content": "nenvs = 128 nsteps = 128 = 25,000,000 γ = 0.999, λ = 0.95 RGB frame, shape (H, W, 3), uint8 disabled (optional 4-stack) enabled disabled none ([, ]) Adam (ε = 105) 5 104 (linear anneal: off) = nenvs nsteps = 16,384 8 (minibatch size B/8 = 2,048) = 3 on ϵclip = 0.2 clipped (same ϵclip) cv = 0.5 cH = 0.01 (anneal: off) global norm 0.5 none Network (CNN encoder + heads) Input scaling Conv1 Conv2 Conv3 MLP trunk Actor head Critic head Initialization Action distribution"
        },
        {
            "title": "Eval frequency\nEval episodes\nEval parallelism",
            "content": "x x/255 32 channels, 88, stride 4, valid, ReLU 64 channels, 44, stride 2, valid, ReLU 64 channels, 33, stride 1, valid, ReLU FC 512 + ReLU linear 8 logits (orthogonal init, gain 0.01) linear 1 (orthogonal init, gain 1) 2), bias 0 orthogonal (trunk gain categorical; sampled via Gumbel-max every 300 iterations 128 episodes 32 envs"
        }
    ],
    "affiliations": [
        "Cognitive AI Systems Lab",
        "MIRIAI, Moscow, Russia"
    ]
}