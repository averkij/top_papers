{
    "paper_title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding",
    "authors": [
        "Langlin Huang",
        "Chengsong Huang",
        "Jixuan Leng",
        "Di Huang",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 6 5 3 0 . 6 0 5 2 : r POSS:Position Specialist Generates Better Draft for Speculative Decoding Langlin Huang1, Chengsong Huang1, Jixuan Leng2, Di huang1, Jiaxin Huang1 1Washington University in St. Louis 2Carnegie Mellon University h.langlin@wustl.edu, jiaxinh@wustl.edu"
        },
        {
            "title": "Abstract",
            "content": "Speculative decoding accelerates Large Language Model (LLM) inference by using small draft model to predict multiple tokens, and large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (POSS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that POSS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS."
        },
        {
            "title": "Introduction",
            "content": "Speculative decoding [1, 2] is an effective approach to accelerate the autoregressive decoding of Large Language Models (LLMs) through draft-then-verify framework. Specifically, it employs lightweight draft model to generate candidate tokens autoregressively, which are then verified by the larger target model in parallel to determine accepted tokens from proposed draft tokens, thereby reducing overall decoding time. The effectiveness of speculative decoding largely depends on the average acceptance length τ (accepted token counts per round) from the prediction depth (predicted token counts generated by the draft model per round). Recent efforts [3, 4, 5] in speculative decoding utilize the target model hidden states as input to enhance draft model prediction accuracy. EAGLE [4, 5] employs one-layer Transformer as the draft model and trains it to predict the next token with features from the target model. However, EAGLE exhibits traininginference discrepancy: during training, it predicts tokens using ground-truth features from the target model, whereas during autoregressive inference, ground-truth features at previous draft positions are unavailable. Instead, it must rely on features generated by the draft model, which deviate from the ground-truth. HASS [6] partially addresses this discrepancy by training the draft model to predict the next token with features from previous draft steps. However, both approaches suffer from relying on single draft model to predict tokens at multiple positions in the draft sequence. We hypothesize that effective draft model should be position-specialized within the prediction length L: early positions require accurate predictions with reliable target model features, while later positions must learn to mitigate the increasing levels of feature deviations. To evaluate the prediction quality across positions, we introduce the metric of position-wise acceptance rate (pos-acc) Preprint. Under review. to measure the conditional probability of accepting the ith token given the acceptance of its preceding (i 1)th token. Our analysis reveals that both EAGLE and HASS suffer from rapidly degrading pos-acc beyond the first few predicted tokens. This confirms our hypothesis that single draft model is limited by its generalization capability of various positions. To address this challenge, we propose Position Specialists (POSS), novel framework that consists of multiple position-specialized draft layers, called as position specialists. Each position specialist is trained for predicting tokens at its assigned position(s), and only needs to handle an expected level of feature deviation at that position, thus enabling more accurate draft token predictions than single draft model which needs to handle varying levels of feature deviation at different positions. We conduct extensive experiments on two model sizes (Llama-3-8B-Instruct and Llama-2-13Bchat) across six benchmark datasets, and demonstrate that POSS consistently outperforms baseline methods. POSS surpasses the strong baseline HASS on average acceptance length by up to 4.5% (from 4.62 to 4.83) and on speed-up ratio by up to 5.7% (from 2.97x to 3.14x). We also carry out comprehensive analysis and reveal that the efficiency of POSS comes from reduced rounds of speculative generation, as higher position-wise acceptance rate at deeper positions enables longer acceptance length τ per round. Our primary contributions include: We introduce position-wise acceptance rate (pos-acc) as crucial metric for analyzing the draft quality of speculative decoding approaches. We propose Position Specialists (POSS), novel framework that employs position-specialized layers to address the challenge of accumulated levels of feature deviation in draft predictions. We conduct extensive experiments and analysis to demonstrate that POSS outperforms baseline methods on both average acceptance length and speed-up ratio."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 Speculative Decoding Speculative decoding harnesses the principle of speculative execution [7] to achieve reduced latency through increased parallelism. In this framework, smaller, faster draft model θD works alongside larger target language model θT that we aim to accelerate. The standard speculative decoding [1] process operates in three key phases. First, the draft model θD autoregressively generates candidate sequence of length L. Next, the target model θT evaluates all draft tokens in parallel, computing their output distributions in single forward pass. Finally, specialized rejection sampling mechanism accepts tokens that align with the target distribution. This parallel evaluation significantly reduces inference latency compared to traditional token-by-token generation. 2.2 Hidden State Assisted Speculative Decoding Recent research efforts [3, 4, 5] discover the potential of the target models hidden state. Instead of using complete auxiliary model for drafting, researchers demonstrate that applying few extra layers to process the last-layer hidden states of the target model, referred to as features, suffices for effective draft generation. Medusa [3] uses multiple language model heads to project feature vector into different output spaces to predict several subsequent tokens simultaneously. EAGLE and EAGLE-2 [4, 5] represents significant breakthrough in speculative decoding through concatenating input embedding with feature vectors. It employs one-layer Transformer as the draft model θD and reuses LM head of the target model for token prediction. At generation step t, EAGLE-2s draft model θD predicts the next token xt+1 based on context xt and features f<t: (xt+1) = Head(θD([xt; (T ) t1], [xt1; (T ) t2], . . . , [x1; (T ) 0 ])) (1) Figure 1 provides an example of EAGLE-2 at inference stage. θD autoregressively generates draft tokens xt+1, xt+2, xt+3, where the subscripts represent the timesteps. Inputs are derived from different sources, denoted by superscripts: (T ) represents feature from the target model; (Di) represents feature from the ith draft step of the draft model D. (D) is used instead of (T ) when 2 Figure 1: The inference and training stages of EAGLE-2, HASS, and our POSS method. The dashed lines represent autoregressive decoding or training, and the solid lines represent parallel training. The input concatenates context word embeddings and features from previous step . During inference, EAGLE-2 and HASS use single draft model θD to generate features (Di) for each position recursively. For draft model training, EAGLE-2 uses the target model feature (T ) as input for training (teacher forcing), and HASS uses draft model predicted features (Di) to replace. Different from them, POSS introduces different position specialists θSj . During inference, the position-specialized draft models autoregressively generate features (Sj ), where position corresponds to the specialist θSj . At training stage, POSS applies position-specialized training: specialist θSj is trained on the ith position using the previous step specialist feature. the target model features are unavailable prior to the forward pass completion of subsequent tokens. Therefore, the prediction of the kth draft position is formulated as: (xt+k) = Head(θD([xt+k1; (Dk1) t+k2 ], . . . , [xt+1; (D1) ], [xt; (T ) t1], . . . , [x1; (T ) 0 ])) (2) Specifically, Equation (2) degenerates to Equation (1) when = 1. Although EAGLE-2 performs inference with Equation (2), it is solely trained on Equation (1), known as teacher forcing. This exhibits fundamental training-inference discrepancy: θD needs to predict the subsequent tokens (k > 1) with its own generated features during inference, but it never observes its own prediction errors during training, which impairs EAGLE-2s ability to effectively predict long draft sequences. HASS [6] explicitly addresses the discrepancy through recursive feature alignment in draft model training, where θD is trained to predict subsequent tokens with its own generated features from earlier timesteps. Figure 1 illustrates the training process of HASS. For the next > 1 token, HASS uses (Dk1) generated by the draft model from the previous step to substitute for the target model feature. Therefore, HASS is able to reuse each training token times, by considering it as the next 1st to Lth token in sequence of draft predictions. By effectively training on the draft model generated features 3 multiple times, HASS is able to improve the acceptance probabilities of tokens at later positions compared to EAGLE-2."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce our Position Specialist (POSS) approach for speculative decoding. We first introduce the concept of position-wise acceptance rate to reveal the fundamental limitations in existing approaches in Section 3.1. We then propose our POSS with position specialized training in Section 3.2 to address the limitation. Finally, we analyze potential computational cost in real implementation in Section 3.3. 3.1 Position-Wise Acceptance Rate Previous speculative decoding frameworks rely heavily on the generalizability of single draft layer for multi-position token generation. EAGLE-2 trains θD only on the immediate next position but expects it to generalize to subsequent positions at inference time. While HASS trains θD on both the immediate and later positions, it only uses one draft model to generalize across diverse feature sources and different token positions. Both EAGLE-2 and HASS use single Transformer layer as the draft model, which inherently constrain the generalizability due to model capacity. To demonstrate the generalization limitation of EAGLE-2 and HASS, we introduce position-wise acceptance rate (pos-acc), which measures the probability that token at position is accepted given that its preceding token at position 1 is accepted. Formally, the pos-acc at position is defined as: pos-acci = (Ai Ai1) = (Ai1 Ai) (Ai1) = (Ai) (Ai1) , > 1 (3) where Ai denotes the event that the token at position is accepted during the verifying process. Notice that the target model acceptance follows strict sequential dependency: if xi is accepted, its preceding tokens x[0:i1] must also have been accepted, and therefore Ai Ai1. We point out that higher pos-acc is crucial for achieving higher acceptance length τ at each draftverification round. For draft sequence of length L, the probability of accepting all draft tokens up to position (k L) is: (A1 A2 Ak) = (cid:40) (A1) (A1) (cid:81)k i=2 pos-acci if = 1 if > 1 (4) This chain rule decomposition reveals that the overall acceptance length depends on the multiplication of pos-acc, and is particularly sensitive to degradation in any single position. Notably, token prediction inherently becomes more challenging at later positions due to the accumulation of prediction errors and the increasing uncertainty in longer draft positions. In Figure 2, we demonstrate the empirical pos-acc of EAGLE-2 and HASS. EAGLE-2s pos-acc deteriorates rapidly beyond position = 1. This is because the draft model of EAGLE-2 is solely trained on predicting the next immediate token. HASS is able to maintain relatively higher pos-acc because its draft model is trained on multiple subsequent positions. However, since its single draft model needs to balance between multiple positions, the pos-acc drops by about 1 percent at position = 1, which critically impairs the overall acceptance length due to the multiplicative nature of the acceptance probability in Equation (4). 4 Figure 2: Position-wise acceptance rate (posacc) of the ith token on MT_Bench dataset by various speculative decoding methods. The pos-acc of EAGLE-2 and HASS decays fast as the draft sequence gets longer. Our proposed POSS method keeps stable and higher pos-acc even at the deepest position (draft model prediction depth = 6). Figure 3: This figure shows comparison of hidden state (feature) training between POSS and previous work. The draft model generated feature inevitably deviates from the target model, and the deviation level increases with error propagation in the draft model(s). In previous work, one draft model learns to mitigate varying levels of feature gaps. In POSS method, position specialists can focus on mitigating expected levels of feature deviation. 3.2 Position Specialists Improve Position-Wise Acceptance Rate To address the aforementioned limitation, we introduce Position Specialists (POSS) to preserve early-position acceptance rate while enhancing later position predictions. POSS consists of multiple position-specialized draft layers, called position specialists. Each specialist is trained for certain position(s) and generates draft tokens at its assigned position(s). The number of positions that specialist is assigned to can be pre-defined as n, and POSS-n means each specialist is responsible for positions. Figure 1 exhibits the training and inference of POSS-1. In the example, there are 3 position specialists {POSSi}3 i=1, with each assigned to predict the draft token xt+i. During training, each specialist POSSi learns to predict using input feature of draft model at previous step POSSi1. Figure 3 illustrates the training of POSS to explain why position specialists are better than single draft model. Despite being trained, the draft model inevitably generates hidden states (features) that deviate from the target model. The deviation (D) increases with position through error propagation in the autoregressive draft process, as visualized by the purple arrow in Figure 3. While previous approaches like HASS train single draft model to handle varying levels of input noise challenging task for single-layer architecture, our POSS method employs position-specific specialists to handle the narrow and expected levels of feature deviation , enabling more accurate draft sequence prediction by decomposing it into position-specific subtasks. (T ) We introduce the loss of POSS-n as follows. token-level loss is applied to guide draft model to generate the same token as target model does. Given verified context length t, the token-level loss on the ith(i 0) position of the draft sequence is Ltoken = Cross_Entropy(P (xt+i+1), ˆP (xt+i+1)) where the training target ˆP (xt+i+1) and the training variable (xt+i+1) are given by: and ˆP (xt+i+1) = Head(f (T ) t+i ) (xt+i+1) = Head(f (S(i+1)/n) t+i ) (5) (6) (7) t+i t+i and (S(i+1)/n) Here, (T ) represent the target feature and the specialist feature, where (i + 1)/n determines which specialist is responsible for position i. For example, with = 2, specialist S1 handles positions 0 and 1, specialist S2 handles positions 2 and 3, and so forth. The target and specialist features are computed as t+i = θT ([xt+i; (T ) (T ) t+i1], [xt+i1; (T ) t+i2], . . . , [x1; (T ) 0 ]) (8) and (S(i+1)/n) t+i = θ(Si/n)([xt+i; (Si/n) t+i1 ], . . . , [xt+1; (S1) ], [xt; (T ) t1], . . . , [x1; (T ) 0 ]) (9) Besides token-level loss, we also employ feature-level loss. This loss trains the specialist models to generate features that are close to the target features, which is given by: t+i , (S(i+1)/n) Lfeature = Smooth L1(f (T ) (10) t+i ) Since the specialist draft model takes in deviated features from preceding draft models during training, it effectively learns to mitigate the accumulated feature deviation at inference time. We also adopt the Top-K distillation loss LTopK, proposed by HASS [6], to ensure fair comparison with it. The loss is defined as: LTopK = Cross_EntropyTopK(P (xt+i+1), ˆP (xt+i+1)) (11) which calculates cross-entropy loss only on tokens of the highest probabilities from ˆP (xt+i+1). The overall loss is Ltotal = Lfeature + Ltoken +LTopK , where we follow previous works [4, 5, 6] and set weight = 0.1, = 10. 3.3 Computational Overhead of Position Specialists While POSS generates drafts closer to the target model and achieves longer acceptance length, we point out two types of additional computation overhead that POSS introduces. First, the GPU memory usage increases linearly with the number of position specialists. Fortunately, this additional cost is negligible compared to the target model size since each specialist costs only one transformer layer (around 218M parameters per specialist for an 8B target model). Second, the switching of position specialists brings little extra latency. Although each POSS specialist use the same structure with the single draft model of EAGLE-2 theoretically implying equivalent computation time per draft phase practical implementation of position specialists costs slightly more computation overhead for two reasons: (1) Non-shared KV cache across layers: Each position specialist computes key-value cache for draft tokens generated by its preceding specialist in addition to previously verified tokens. (2) Parameter switching overhead: Frequent parameter switching between specialists may introduce additional latency due to hardware-level parameter loading. We study the effect of extra computation overhead brought by POSS through comprehensive empirical analysis in Section 6.2. We demonstrate that POSS only brings minimal overhead compared to the overall computation time, and this overhead is largely outweighed by the increased average acceptance length, which reduces the overall drafting rounds needed."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Setup Metrics. We evaluate the performance of our approach using two key metrics: speed-up ratio and average acceptance length. Speed-up Ratio: The speed-up ratio measures the improvement in generation efficiency compared to the vanilla target model decoding, calculated as the ratio between throughputs (tokens generated per second) of speculative decoding approach to that of the target model autoregressive decoding. higher speed-up ratio indicates better performance. Average Acceptance Length τ : The average acceptance length represents the mean number of tokens accepted in each round of drafting positions (denoted as prediction length). It reflects how effectively the draft model can predict longer sequences that match the target model output. Longer acceptance lengths generally correlate with improved efficiency as they reduce the number of draft iterations needed. 6 Datasets. We conduct comprehensive experiments on six datasets, following EAGLE-2 [4]. This includes MT-Bench [8] for multi-turn conversation, Alpaca [9] for instruction following, GSM8K [10] for mathematical reasoning, Natural Questions [11] for question answering, CNN/Daily Mail (shortened to CNN/DM) [12] for summarization, and HumanEval [13] for code generation. Target Models. We evaluate our method on two model sizes: Llama-3-8B-Instruct and Llama-213B-chat. This allows us to evaluate how our approach performs across model sizes. Llama-3-8BInstruct serves as our primary model for ablation studies and detailed analysis, while Llama-2-13B demonstrates the scalability of our method to larger models. Implementations. Our implementation is built upon the open-source repositories of EAGLE-2 and HASS. We experiment with EAGLE-2, HASS, and our method with configurations of POSS-1, 2, 3, where POSS-3 adds the least extra layers and computation overhead. The training configurations are mostly aligned with HASS and are detailedly introduced in Appendix B. During inference, treedrafting [14] strategy is applied to generate multiple draft paths in one draft phase, where the width, depth, and total tokens are key controlling factors. We set the draft tree width to 10 and the number of draft total tokens to 60 for all experiments. We choose the draft tree depth that leads to the best performance. Table 4 and 6 suggest that the 8B target model setting achieves the best performance at depth=6, and the 13B target model reaches the best performance at depth=7. All experiments are conducted on NVIDIA A100 GPUs with 80GB of memory. We repeat all experiments 3 times and select the fastest one among each method for speed-up ratio calculation. This mitigates the random disturbance of the server and better reflects the real throughputs and speed-up ratio."
        },
        {
            "title": "5 Results",
            "content": "We introduce the main results in this section. Table 1 presents the average acceptance lengths of different models. Table 2 presents the speed-up ratio of these models. Our methods achieve the highest overall average acceptance length under different sampling temperatures, demonstrating the effectiveness of position specialists in making accurate draft predictions. When L3 8B serves as the target model, POSS achieves consistently higher speed-up ratio over the baselines. When L2 13B is the target model and generates stronger feature representations, POSS is less advantageous, but POSS-3 still achieves the highest speed-up ratio. Table 1: Average acceptance length τ of all methods. L3 8B represents Llama-3-8B-Instruct, L2 13B represents Llama-2-13B-Chat. Temperature=0 Model L3 8B L2 13B L3 8B L2 13B Method EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) MT-Bench Alpaca GSM8K Natural Questions CNN/DM HumanEval Avg. 4.06 3.38 4.39 3.54 3.65 4.54 4.55 3.63 4.54 3.64 4.79 4.15 5.32 4.55 5.34 4.57 5.36 4.59 5.37 4. 4.25 4.61 4.82 4.83 4.81 5.01 5.47 5.60 5.58 5.62 3.61 3.92 4.06 4.06 4.05 4.30 4.71 4.78 4.79 4.80 4.70 5.20 5.39 5.40 5.41 5.78 6.47 6.48 6.51 6.51 4.11 4.42 4.54 4.54 4.52 4.86 5.40 5.40 5.39 5.40 4.32 4.62 4.78 4.83 4.82 4.64 5.31 5.23 5.27 5.28 4.11 4.01 4.18 4.17 4.13 4.69 5.14 5.31 5.17 5. 4.32 4.39 4.46 4.47 4.46 4.44 5.21 5.19 5.05 5.18 Temperature=1 4.25 4.49 4.63 4.66 4.67 4.82 5.32 5.57 5.44 5.49 7 3.38 3.40 3.41 3.44 3.37 4.12 4.46 4.45 4.41 4.50 3.61 3.65 3.78 3.77 3.76 4.25 4.54 4.72 4.67 4. 4.70 5.00 5.20 5.13 5.12 5.54 6.16 6.20 6.22 6.30 4.06 4.16 4.28 4.27 4.25 4.64 5.14 5.24 5.16 5.23 Table 2: Speed-up ratios of all methods. L3 8B represents Llama-3-8B-Instruct, L2 13B represents Llama-2-13B-Chat. Temperature=0 Model L3 8B L2 13B L3 8B L2 13B Method EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) EAGLE-2 HASS POSS-1 (ours) POSS-2 (ours) POSS-3 (ours) MT-Bench Alpaca GSM8K Natural Questions CNN/DM HumanEval Avg. 2.68x 2.29x 2.89x 2.38x 2.49x 2.94x 2.98x 2.45x 2.95x 2.45x 3.01x 2.71x 2.96x 3.33x 3.21x 2.86x 3.30x 2.93x 3.34x 2.96x 2.27x 2.47x 2.50x 2.52x 2.50x 2.49x 2.72x 2.63x 2.72x 2.74x 2.87x 3.11x 3.19x 3.25x 3.17x 3.23x 3.52x 3.47x 3.54x 3.59x 2.77x 2.94x 2.96x 2.99x 2.96x 2.99x 3.28x 3.16x 3.22x 3.28x 3.08x 3.48x 3.52x 3.52x 3.53x 3.71x 4.15x 3.99x 4.11x 4.12x 2.79x 2.97x 3.00x 3.14x 3.10x 2.95x 3.34x 3.18x 3.26x 3.32x Temperature= 2.67x 2.77x 2.79x 2.81x 2.71x 2.95x 3.22x 3.16x 3.16x 3.22x 2.55x 2.79x 2.78x 2.79x 2.86x 2.88x 3.30x 3.19x 3.17x 3.23x 2.09x 2.14x 2.13x 2.17x 2.12x 3.13x 3.46x 3.44x 3.42x 3.49x 2.02x 2.09x 2.19x 2.18x 2.18x 2.76x 2.97x 2.86x 2.91x 2.96x 2.80x 3.03x 3.18x 3.10x 3.11x 2.51x 2.67x 2.65x 2.64x 2.73x 2.47x 2.56x 2.67x 2.55x 2.58x 3.48x 3.89x 3.75x 3.85x 3.92x 2.43x 2.56x 2.62x 2.60x 2.59x 2.95x 3.25x 3.17x 3.19x 3.26x"
        },
        {
            "title": "6 Analysis",
            "content": "6.1 Position-Wise Acceptance Rate In Section 3.1, we introduce the metric position-wise acceptance rate (pos-acc) to reflect the acceptance rate of specific position, which largely affects the overall acceptance length. Here we demonstrate that POSS largely improves pos-acc by mitigating the feature deviation at each position. In Figure 4, we show the pos-acc with draft depth of 8 on different models. EAGLE-2, with the least position generalization ability, has pos-acc lower than 65% from the 5th position on. HASS can only maintain adequate pos-acc at the first four positions, after which performance degrades significantly due to one single draft model. In contrast, all variants of our POSS method maintain substantially higher pos-acc until the last position. This demonstrates the effectiveness of POSS in mitigating position deviation and making accurate predictions. 6.2 Computational Efficiency Tradeoffs for Extra Position Specialists In Section 3.3, we analyze the computational overhead of position specialists. Here we conduct comprehensive analysis of computational costs and efficiency benefits brought by position specialists. Each complete round of speculative generation involves two primary phases: the draft phase and the verification phase. In this experiment, we quantitatively analyze the time cost through three key metrics: (1) per-round computation time, (2) total round counts for test set generation, and (3) total time cost for test set generation. We demonstrate comprehensive analysis in Figure 5 and present the following noteworthy observations. Figure 4: The position-wise acceptance rate of EAGLE-2, HASS, and variants of POSS. Experiments are conducted on MT-Bench dataset, with base model Llama-3-8B-Instruct and draft depth=8. Three variations of our method maintain relatively higher pos-acc even at the 8th position. Position specialists bring minimal overhead to overall computation time. As discussed in Section 3.3, position specialists cost additional computational overhead. We present in Figure 5(a) the sum of per-round computation time over 5,000 rounds across varying draft depths (bar chart), decomposed into draft phases and verification 8 Figure 5: Computation time of different phases on MT-Bench dataset on different models across varying draft depths. The bar plots present the decomposition of time spent on each phase of speculative decoding, where subfigure (a) measures the time spent on 5k rounds and subfigure (b) measures the time to complete an entire test set. The line plot presents the number of rounds needed to complete dataset. The lower the metrics are, the better the method is. phases. Empirical results show that POSS has lower per-round time than EAGLE-2. Compared to HASS, POSS only brings negligible fraction of time in the draft phase with positional specialists, and keeps similar verification phase costs. POSS achieves lowest overall computation time with reduced round counts. The line chart in Figure 5(a) illustrates the total round counts needed for test set generation. POSS-2 and POSS3 consistently require fewer rounds than baseline methods, benefitting from accurate draft token prediction from position specialists. The total time cost for decoding is primarily determined by both the per-round time cost and the total round counts. As shown in Figure 5(b), POSS-2 and POSS-3 achieve lower overall time costs compared to EAGLE-2 and HASS. This confirms that the efficiency gains from reduced round costs substantially outweigh the marginal per-round overhead brought by position specialists. 6.3 Ablation Study on Draft Model Prediction Depth Figure 6 presents the throughput and average acceptance length under different draft depths. The average acceptance length τ increases with the draft depth consistently, but the improvement diminishes at higher depth. The diminishing improvement, along with the linearly increasing time cost of draft depth, creates an optimal point for throughput. We empirically demonstrate that the throughput peaks at draft depth = 5 on the MT-Bench dataset for most models. We extend the ablation experiment on all six datasets in Appendix C. Results in Table 2 demonstrates that POSS methods achieve speed-up ratio peaks at the 8th position while HASS peaks at only the 7th position."
        },
        {
            "title": "7 Related Work",
            "content": "7.1 Linear Speculative Decoding Early works [15] introduce the fundamental concept of using draft model to predict multiple tokens in parallel. This is followed by various improvements in linear speculative decoding, including adaptive calibration techniques [16], dynamic candidate length adjustment [17], and methods to optimize the latency-throughput tradeoff [18]. Recent advances focus on multi-token prediction [19], efficient multi-sampling [20], and token recycling [21]. Some also explore parallel decoding strategies with adaptive n-gram techniques [22, 23, 24, 25]. 9 7.2 Tree Speculative Decoding Tree-based speculative decoding has advanced through several key works. GRIFFIN [26] and Sequoia [14] enhance token alignment methods, while SpecInfer [27]improves sampling techniques. Other notable approaches include dynamic tree pruning [28], early exit mechanisms [29], and hierarchical method [30]. 7.3 Efficient Inference Recent works apply other methods to improve the inference speed. Judge Decoding [31] uses small judge model to evaluate parallel reasoning paths, while SpecReason [32] and Speculative Thinking [33] leverage speculative computation for faster inference. Other efficient reasoning techniques include efficient chain-ofthought methods [34, 35], in-context learning methods [36], non-myopic generation [37] and system-level infra [38]."
        },
        {
            "title": "8 Conclusion",
            "content": "This paper proposes POSS, draft model consisting of several position specialists. This method mitigates feature deviation between the draft and target models, and reduces the deviation accumulation across draft positions. Experiments show that POSS maintains high position-wise acceptance rate at large positions, achieving larger acceptance length and faster generation speed than other methods."
        },
        {
            "title": "References",
            "content": "Figure 6: The throughput and average acceptance length of 4 models on different draft depths. The experiments are conducted on MT-Bench dataset. The acceptance length consistently increases as depth rises, while the throughput peaks at depth=5. [1] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, 2022. [2] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, L. Sifre, and John M. Jumper. Accelerating large language model decoding with speculative sampling. ArXiv, abs/2302.01318, 2023. [3] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [4] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative sampling requires rethinking feature uncertainty. In International Conference on Machine Learning, 2024. [5] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE-2: Faster inference of language models with dynamic draft trees. In Empirical Methods in Natural Language Processing, 2024. [6] Lefan Zhang, Xiaodan Wang, Yanhua Huang, and Ruiwen Xu. Learning harmonized representations for speculative sampling. arXiv preprint arXiv:2408.15766, 2024. [7] H. T. Kung and John T. Robinson. On optimistic methods for concurrency control. In TODS, 1979. [8] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [9] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. [11] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. [12] Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond, 2016. [13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. [14] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024. [15] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. arXiv preprint arXiv:2203.16487, 2022. [16] Aayush Gautam, Susav Shrestha, and Narasimha Annapareddy. Token-driven gammatune: Adaptive calibration for enchanced speculative decoding. arXiv preprint arXiv:2504.00030, 2025. [17] Kaixuan Huang, Xudong Guo, and Mengdi Wang. Specdec++: Boosting speculative decoding via adaptive candidate lengths. arXiv preprint arXiv:2405.19715, 2024. [18] Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latencythroughput tradeoff for long context generation with speculative decoding. arXiv preprint arXiv:2408.11049, 2024. [19] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. [20] Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, and Yunhe Wang. Ems-sd: Efficient multi-sample speculative decoding for accelerating large language models. arXiv preprint arXiv:2405.07542, 2024. [21] Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu, and Wanxiang Che. Turning trash into treasure: Accelerating inference of large language models with token recycling. arXiv preprint arXiv:2408.08696, 2024. 11 [22] Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model via adaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698, 2024. [23] Pengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, and Dongyan Zhao. Parallel decoding via hidden transfer for lossless large language model acceleration. arXiv preprint arXiv:2404.12022, 2024. [24] Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, and Winston Hu. Parallel speculative decoding with adaptive draft length. arXiv preprint arXiv:2408.11850, 2024. [25] Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, and Yu Meng. Fast and accurate language model In Adaptive Foundation Models: Evolving AI for decoding via parallel token processing. Personalized and Efficient Learning, 2024. [26] Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, and Pan Zhou. Griffin: Effective token alignment for faster speculative decoding. arXiv preprint arXiv:2502.11018, 2025. [27] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 932949, 2024. [28] Shuzhang Zhong, Zebin Yang, Ruihao Gong, Runsheng Wang, Ru Huang, and Meng Li. Propd: Dynamic token tree pruning and generation for llm parallel decoding. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, pages 18, 2024. [29] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layerskip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024. [30] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024. [31] Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, and Jonas Kohler. Judge decoding: Faster speculative sampling requires going beyond model alignment. arXiv preprint arXiv:2501.19309, 2025. [32] Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025. [33] Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329, 2025. [34] Jikai Wang, Juntao Li, Lijun Wu, and Min Zhang. Efficient reasoning for llms through speculative chain-of-thought. arXiv preprint arXiv:2504.19095, 2025. [35] Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031, 2025. [36] Chengsong Huang, Langlin Huang, and Jiaxin Huang. Divide, reweight, and conquer: logit arithmetic approach for in-context learning. arXiv preprint arXiv:2410.10074, 2024. [37] Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195, 2024. [38] Xi Huang, Yinxu Tang, Junling Li, Ning Zhang, and Xuemin Shen. Toward effective retrieval augmented generative services in 6g networks. IEEE Network, 38(6):459467, 2024. [39] Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, and Min Zhang. OPT-tree: Speculative decoding with adaptive draft tree structure. Transactions of the Association for Computational Linguistics, 13:188199, 2025."
        },
        {
            "title": "A Limitations",
            "content": "POSS requires slightly more memory to deploy extra position-specialized layers, which is quantitatively discussed in Appendix D. Besides, these layers cannot share their key-value cache. However, such computational overhead can be outweighed by the benefits from more accurate drafts generated by position specialists, as discussed in Section 6.2."
        },
        {
            "title": "B Training Configurations",
            "content": "In this work, all draft models are trained on the ShareGPT dataset and its distilled version generated by the corresponding target model. The former provides the token-level training objective, and the latter provides the feature-level training objective. When Llama-3-8B-Instruct serves as the target model, we notice that the open-sourced checkpoints of EAGLE-2 and HASS are inconsistent in structure1. To ensure the fairness of the comparison, we reproduce both methods with the model structure of HASS. While EAGLE-2 is trained for 20 epochs, HASS is trained for 40 epochs. To save the training time, we start training our method POSS from the reproduced EAGLE-2 model for another 20 epochs to fairly compare with HASS. When Llama-2-13B-Chat serves as the target model, we use the open-sourced checkpoints of EAGLE2 and HASS. Similarly, we train POSS from the EAGLE-2 model checkpoint for another 20 epochs. During the training of POSS models, we keep most configurations the same with HASS [6], including the loss-related hyperparameters and the learning rate."
        },
        {
            "title": "C Different Drafting Hyperparameters",
            "content": "Many factors influence the average acceptance length and speed-up ratio. Besides the prediction accuracy of draft models and computational overhead, the structure of draft trees also matters. We examine two key hyperparameters that affect the performance: depth and total tokens. We take Llama-2-13B-chat as the base model, and conduct experiments with depths from 6 to 9, and total tokens selected from {60, 80}. We evaluate the models on all six datasets and take the average of them. Table 5 presents the average acceptance length, and Table 6 presents the speed-up ratio. Interestingly, despite the consistent rise of average acceptance length as the number of total tokens increases from 60 to 80, the speed-up ratio shows sharp drop. This indicates the target model takes significantly more time to verify. This phenomenon results from the inner structure of the A100 GPU device that we use for experiments, which is also observed by OPT-Tree [39]. Table 3: Average acceptance length under different hyperparameters. Experiments use Llama-3-8BInstruct as the base model. We average the results on all six datasets. The largest average acceptance length within each column is bolded."
        },
        {
            "title": "Temperature",
            "content": "T=0 T=1 Depth Total Tokens HASS PosS-1 PosS-2 PosS-3 HASS PosS-1 PosS-2 PosS-3 6 7 9 60 4.39 4.54 4.55 4.50 4.16 4.28 4.27 4.28 80 4.49 4.64 4.67 4.62 4.24 4.37 4.37 4.35 60 4.49 4.65 4.68 4.61 4.22 4.35 4.37 4.30 80 4.62 4.78 4.81 4.75 4.34 4.48 4.53 4.49 60 4.54 4.74 4.74 4.69 4.26 4.44 4.43 4. 80 4.67 4.89 4.90 4.83 4.39 4.58 4.57 4.53 60 4.59 4.79 4.79 4.73 4.30 4.47 4.48 4.43 80 4.73 4.94 4.96 4.89 4.41 4.58 4.64 4.53 1In the configuration EAGLE-2 model, https://huggingface.co/yuhuili/ EAGLE-LLaMA3-Instruct-8B/blob/main/config.json, the \"bias\" is false. However, this is true in the configuration file of HASS, https://huggingface.co/HArmonizedSS/HASS-LLaMA3-Instruct-8B/ blob/main/config.json. file of 14 Table 4: Speed-up ratio under different hyperparameters. Experiments use Llama-3-8B-Instruct as the base model. We average the results on all six datasets. The largest number within each row is bolded to show the upper bound of each method. Temperature T=0 T=1 Depth Total Tokens HASS PosS-1 PosS-2 PosS-3 HASS PosS-1 PosS-2 PosS-3 6 7 9 60 2.89x 2.94x 2.98x 2.95x 2.63x 2.73x 2.66x 2.67x 80 2.83x 2.90x 2.92x 2.89x 2.54x 2.65x 2.60x 2.59x 60 2.84x 2.90x 2.93x 2.89x 2.56x 2.66x 2.63x 2.60x 80 2.78x 2.85x 2.87x 2.84x 2.50x 2.59x 2.57x 2.56x 60 2.76x 2.83x 2.84x 2.83x 2.47x 2.60x 2.55x 2.55x 80 2.71x 2.80x 2.81x 2.78x 2.44x 2.55x 2.51x 2.47x 60 2.67x 2.76x 2.77x 2.73x 2.41x 2.53x 2.48x 2.48x 80 2.65x 2.72x 2.74x 2.71x 2.35x 2.48x 2.45x 2.41x Table 5: Average acceptance length under different hyperparameters. Experiments use Llama-2-13Bchat as the base model. We average the results on all six datasets. The largest average acceptance length within each column is bolded."
        },
        {
            "title": "Temperature",
            "content": "T=0 T=1 Depth Total Tokens HASS PosS-1 PosS-2 PosS-3 HASS PosS-1 PosS-2 PosS-3 6 7 9 60 4.68 5.09 5.13 5.13 5.00 4.99 4.96 4.99 80 5.20 5.20 5.22 5.21 5.06 5.11 5.11 5.11 60 5.32 5.34 5.36 5.37 5.14 5.24 5.16 5.21 80 5.45 5.48 5.49 5.51 5.29 5.31 5.32 5.31 60 5.46 5.52 5.53 5.55 5.24 5.34 5.30 5. 80 5.62 5.66 5.68 5.70 5.45 5.49 5.49 5.50 60 5.57 5.63 5.65 5.66 5.35 5.43 5.44 5.43 80 5.75 5.79 5.82 5.83 5.52 5.52 5.61 5."
        },
        {
            "title": "D Extra Memory Usage During Inference",
            "content": "In Section 3.3, we admit the POSS method requires slight extra GPU memory usage. Figure 7 visualizes the memory usage of all methods mentioned in this paper. Here, EAGLE-2 and HASS cost the same GPU memory, and they are de facto POSS-. From left to right, the draft layers in the methods are 1, 2, 3, and 6. In both target model settings, POSS-3 and POSS-2 increase few extra memory usage. POSS-1, despite using 6 times draft layers than EAGLE-2, costs acceptable extra memory usage. 15 Table 6: Speed-up ratio under different hyperparameters. Experiments use Llama-2-13B-chat as the base model. We average the results on all six datasets. The largest number within each row is bolded to show the upper bound of each method. Temperature T=0 T= Depth Total Tokens HASS PosS-1 PosS-2 PosS-3 HASS PosS-1 PosS-2 PosS-3 6 7 8 9 60 3.28x 3.16x 3.26x 3.29x 3.24x 3.13x 3.17x 3.24x 80 3.02x 2.93x 3.00x 3.00x 2.94x 2.93x 2.94x 2.97x 60 3.33x 3.21x 3.30x 3.34x 3.25x 3.17x 3.19x 3.26x 80 3.08x 3.08x 3.06x 3.09x 3.00x 2.95x 2.98x 3.00x 60 3.31x 3.21x 3.31x 3.35x 3.20x 3.14x 3.18x 3.26x 80 3.09x 3.09x 3.09x 3.11x 3.01x 2.97x 2.99x 3.02x 60 3.28x 3.20x 3.27x 3.30x 3.18x 3.10x 3.17x 3.18x 80 3.09x 3.09x 3.07x 3.10x 2.98x 2.92x 2.98x 3.01x Figure 7: The Inference-time GPU memory usage of different speculative decoding methods. The memory usage is measured on the MT-bench test dataset. POSS methods require slightly more GPU memory than EAGLE-2, the baseline method."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Washington University in St. Louis"
    ]
}