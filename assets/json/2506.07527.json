{
    "paper_title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions",
    "authors": [
        "Lu Ma",
        "Hao Liang",
        "Meiyi Qiang",
        "Lexiang Tang",
        "Xiaochen Ma",
        "Zhen Hao Wong",
        "Junbo Niu",
        "Chengyu Shen",
        "Runming He",
        "Bin Cui",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved with Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 2 5 7 0 . 6 0 5 2 : r LEARNING WHAT REINFORCEMENT CANT: HARDEST QUESTIONS LEARNING INTERLEAVED ONLINE FINE-TUNING FOR Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui*, Wentao Zhang* Peking University maluqaq@163.com Project Page: https://github.com/TheRoadQaQ/ReLIFT"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the models original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for finetuning, and the training process alternates between RL and fine-tuning to enhance the models reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in large language model (LLM) reasoning, as demonstrated by OpenAIo1 Jaech et al. (2024), DeepSeek-R1 Guo et al. (2025), and Kimi-1.5 Team et al. (2025), highlight the potential to enhance reasoning capabilities by increasing test-time computational resources. These models typically generate long Chains-of-Thought (CoT Wei et al. (2022)) responses, exhibiting sophisticated behaviors such as reflection Kumar et al. (2024); Team et al. (2025) and planning Team et al. (2025). The primary factor driving this progress is large-scale Reinforcement Learning with Verifiable Rewards (RLVR), which does not depend on traditional scaffolding techniques like Monte Carlo Tree Search Xie et al. (2024); Li et al. (2025) or Process Reward Models Setlur et al. (2024). *Corresponding authors: Bin Cui, Wentao Zhang 1 In RLVR, rewards are assigned based on whether the models output matches ground-truth solution in mathematics or passes unit tests in code, thereby enabling scalability without the need for detailed demonstration data. In practice, RLVR is typically implemented using policy optimization algorithms such as Proximal Policy Optimization (PPO) Schulman et al. (2017) and Group Relative Policy Optimization (GRPO) Shao et al. (2024). Large-scale RLVR, characterized by its simplicity and directness, has enabled the emergence of long CoT reasoning and self-reflection capabilities. However, despite the empirical successes of RLVR, its current form is insufficient to incentivize capabilities that transcend the base models limitations Yue et al. (2025); Zhao et al. (2025). RL is inherently on-policy, learning from the LLMs self-generated responses through iterative rollouts and feedbacks. Essentially, RL primarily enhances performance by biasing the model toward reasoning paths it already knows are more likely to yield rewards. In other words, most RL methods optimize based on the models existing knowledge rather than facilitating the acquisition of new information. In contrast, supervised fine-tuning (SFT) offers complementary approach to enhancing the reasoning capabilities of LLMs. By leveraging high-quality demonstration data, SFT facilitates the incorporation of new knowledge and reasoning patterns Mecklenburg et al. (2024); Yue et al. (2025), and has been shown to achieve superior reasoning performance compared to RL, particularly in small-scale models Guo et al. (2025). Nevertheless, the effectiveness of SFT is highly contingent upon the availability of substantial amounts of high-quality demonstration data. Furthermore, models trained with SFT often exhibit limited generalization to out-of-distribution (OOD) scenarios, whereas RL-based approaches tend to demonstrate greater robustness Chu et al. (2025). These contrasting characteristics of RL and SFT highlight promising research direction: How can RL and SFT be effectively combined to improve LLM reasoning and OOD generalization, reduce dependence on expensive demonstration data, and achieve capabilities beyond current cognitive constraints? To achieve this goal, we analyze the training dynamics by examining how the accuracy of the questions change under both RL and SFT. In Section 2.1, we evaluate the accuracy of the model on the questions at various checkpoints and classify these questions into four levels of difficulty. Our findings show that RL is more effective for relatively easy questions, whereas SFT is more beneficial for most difficult ones. Furthermore, for simple questions, SFT can degrade the models existing performance, as accuracy on these questions decreases and response length increases. In contrast, for challenging questions, the improvement provided by RL is less pronounced compared to SFT. Overall, RL efficiently trains the model to generate correct answers for questions it can already solve, while SFT is crucial for enabling the model to address questions that surpass its current capabilities. Based on these analyses, we propose Reinforcement Learning Interleaved with Online Fine-Tuning (ReLIFT). As illustrated in Figure 2, during RL training, we collect highly challenging examples according to the accuracy observed during the rollout. When such challenging question is identified, we obtain high-quality CoT solution and subsequently filter out any incorrect answers. After constructing these examples, we add them to an SFT buffer. Once the number of challenging questions in the SFT buffer is sufficient, we perform SFT on these questions for one step. We conduct comprehensive experiments on five competition-level math reasoning benchmarks and one out-of-distribution (OOD) benchmark. Our proposed ReLIFT method achieves new state-ofthe-art accuracy of 51.1% based on Qwen2.5-Math-7B, outperforming previous zero-RL baselines by significant margin, surpassing the strongest zero RL baseline by +5.2 points. Notably, ReLIFT demonstrates clear advantages over both pure SFT and pure RL, while requiring only 13% amount of detailed demonstration data. In addition, ReLIFT generates much more concise solutions, reducing average response length by nearly 10x compared to SFT, thus improving both performance and efficiency. We further validate the generality of ReLIFT by extending it to smaller and weaker foundation models, consistently observing superior results over standard RL and SFT approaches."
        },
        {
            "title": "2.1 RL VS. SFT: TRAINING DYNAMICS ACROSS QUESTION DIFFICULTY LEVELS",
            "content": "We start our study by examining how the models accuracy evolves on questions of varying difficulty during both SFT and RL training. Specifically, we independently perform SFT and RL on the 2 (a) Average Accuracy (b) Average Response Length (c) Initial and Final Accuracy Figure 1: Accuracy and response length changes for Easy, Medium, Hard, and Hardest questions during RL and SFT training. (a) Average accuracy change for each difficulty category. (b) Average response length change for each difficulty category. (c) Number of questions transitioning between different initial and final accuracy categories in RL and SFT, respectively. The x-axis represents the initial difficulty category, and the y-axis represents the final difficulty category. Qwen2.5-Math-7B model, using subset of the Open-R1-math-220K Face (2025). Both SFT and RL are conducted for 120 steps, with model checkpoints saved every 30 steps. Detailed experimental settings are provided in Appendix A.2. At each checkpoint, we evaluate model performance on validation set comprising 1,000 questions. For more detailed analysis, we categorize the validation questions into four distinct difficulty levels: Easy, Medium, Hard, and Hardest. To determine the difficulty level of each question q, we prompt the Qwen2.5-Math-7B model with the same question eight times and calculate the average accuracy for that question, denoted as acc(q). Based on the value of acc(q), each question is assigned to difficulty level according to the following criteria: Easy: acc(q) 0.75 (i.e., 6, 7, or 8 correct out of 8) Medium: 0.375 acc(q) 0.625 (i.e., 3, 4, or 5 correct) Hard: 0.125 acc(q) 0.25 (i.e., 1 or 2 correct) Hardest: acc(q) = 0 (i.e., 0 correct) Questions answered correctly at least 6 out of 8 times (acc(q) 0.75) are labeled Easy, as the model shows high consistency on these items. Questions never answered correctly (acc(q) = 0) are labeled Hardest, exceeding the models capability. Intermediate thresholds (0.375 acc(q) 0.625 for Medium and 0.125 acc(q) 0.25 for Hard) are chosen to ensure each category has enough questions for reliable analysis. This classification allows clear and systematic evaluation of model performance across difficulty levels, with similar numbers of questions in each group. We monitor the changes in average accuracy during RL and SFT training for Easy, Medium, Hard, and Hardest questions. As shown in Figure 1a, RL outperforms SFT on Easy and Medium questions, while SFT yields better results than RL on Hardest questions. Notably, after SFT, the LLM fails to solve certain questions that were previously answered correctly by the initial model. In contrast, RL consistently improves accuracy across all difficulty levels and does not exhibit this issue. We also examine changes in the average response length during RL and SFT training, as shown in Figure 1b. SFT produces longer responses across all question difficulties, aiming to mimic the target long response. In contrast, RL starts with shorter responses, and for medium and hard questions, response length increases over time, indicating more exploration. For the hardest questions, response length remains stable, suggesting the model stops exploring further. Ideally, responses to difficult questions should be longer, while those to simple questions should be shorter. However, SFT and RL each only partially achieve this objective. We further analyze the evolution of accuracy during training by tracking both the initial and final accuracy for each question, as illustrated in Figure 1c. Our findings reveal substantial differences in how RL and SFT influence the models performance across questions of varying difficulty. For Easy and Medium questions, SFT results in decreased accuracy for some items, whereas RL generally preserves or enhances the models original ability to solve these questions. However, this pattern reverses for Hardest questions: SFT enables the model to learn and solve greater number of these challenging items, as evidenced by the increased transition of Hardest questions into other cate3 Figure 2: Overview of the ReLIFT Training Framework. The model is mainly trained with RL. When it encounters particularly hard questions, high-quality solutions are collected or generated, then stored in buffer. Once enough hard examples are gathered, fine-tuning (FT) step is performed using these examples. This process adaptively alternates between RL and FT to help the model learn from its mistakes and improve reasoning ability. In addition, denotes the number of hardest (q, s) pairs in the buffer, while represents predefined threshold, typically set to the batch size for FT. gories. In contrast, RL does not yield comparable improvements for Hardest questions, highlighting limitation in its ability to support learning beyond its current capabilities. Based on these observations, we conclude that accuracy trajectories across questions of varying difficulty do not increase uniformly during RL and SFT training. Instead, critical distinction emerges between these two paradigms: RL is more effective for Easy and Medium questions, whereas SFT proves more advantageous for the Hardest questions. These findings underscore the complementary strengths of RL and SFT: RL excels at maintaining and improving performance on questions within the models original capabilities, while SFT is more effective at facilitating progress on questions that extend beyond those capabilities. Consequently, by selectively incorporating the hardest questions during training and interleaving fine-tuning steps for these questions, it is possible to enhance the models overall performance and achieve more scalable and effective RL for LLMs."
        },
        {
            "title": "2.2 REINFORCEMENT LEARNING INTERLEAVED WITH ONLINE FINE-TUNING",
            "content": "Overview. Motivated by the complementary strengths of RL and SFT in addressing questions of varying difficulty, we propose novel training approach termed Reinforcement Learning Interleaved with Online Fine-Tuning (ReLIFT). ReLIFT dynamically integrates RL and SFT based on the characteristics of the training data and the current capabilities of the model, as illustrated in Figure 2. During RL training, we identify particularly challenging examples based on rollout accuracy. For each of these hardest questions, we obtain high-quality CoT solutions either by collecting them in advance before training or by consulting stronger model or human annotators, and filter out incorrect answers. These curated examples are added to an SFT buffer. When the buffer accumulates enough challenging questions to form training batch, we perform an SFT step using these examples. The underlying intuition is that questions difficult for the model to learn through RL alone may benefit from targeted SFT. The timing of SFT is adaptive: when the model performs poorly on training questions, SFT is applied more frequently; as the model improves, RL is emphasized to further enhance its reasoning ability. Below, we provide detailed description of our method. Reinforment Learning while Collecting Hard Questions. Owing to the success of Deepseek-R1, GRPO has become the de facto approach for RLVR training. GRPO leverages the reward scores of sampled solutions from query to estimate the advantage, thereby eliminating the need for an additional value model. Formally, we denote the policy model before and after the update as πθold and πθ. Given question q, set of solutions oi generated by πθold , and the reward function R(), the GRPO objective is defined as follows: 4 LGRPO(θ) = 1 i=1 oi (cid:80)N (cid:88) oi (cid:88) i=1 t=1 min [ri,t(θ)Ai, clip (ri,t(θ); 1 ϵ, 1 + ϵ) Ai] (1) where ri,t(θ) = πθ (oi,t q, oi,<t) πθold (oi,t q, oi,<t) , Ai = (oi) mean ({R (oi) oi πθold(o), = 1, 2, . . . , }) std ({R (oi) oi πθold(o), = 1, 2, . . . , }) . During the rollout phase of GRPO, for each question q, we generate set of solutions oi, which are evaluated by the reward function to obtain the accuracy acc(q). We identify particularly challenging questions as those for which acc(q) = 0. For these particularly hard questions, we solicit high-quality chain-of-thought (CoT) solutions from stronger reasoning model (e.g., R1) or human experts, filter out incorrect CoTs to obtain reliable QA pairs (q, s), and add them to buffer Bufferhardest. This process is performed online during training and can be formalized as: Bufferhardest = { (q, s) acc(q) = 0, = (q), extract(s) = a} , (2) BufferFT BufferFT Bufferhardest where (q) denotes the CoT response obtained either in advance or online from an external model or human annotator, extract(s) indicates the final answer extracted from s, and is the groundtruth answer for q. We only keep (q, s) pairs where extract(s) = a. Interleaved Online Fine-Tuning for Hardest Questions. When the size of the fine-tuning buffer exceeds predefined threshold (i.e., BufferFT ), we sample batch of (q, s) pairs from the buffer and perform fine-tuning step using these hard examples. The fine-tuning step minimizes the standard cross-entropy loss: LSFT(θ) = 1 s (cid:88) i=1 log πθ (si q, s<i) , where (q, s) BufferFT (3) Notably, the timing of interleaved fine-tuning is adaptive: at the early stage of training, when the model performs poorly on training questions, SFT is applied more frequently to help the model quickly learn better reasoning patterns. As training progresses and the model improves, RL is used more often to further incentivize and enhance the models existing reasoning abilities. Moreover, in most cases, it is unnecessary to prepare large amount of CoTs in advance; instead, it is possible to only generate CoTs online for the hardest questions during training."
        },
        {
            "title": "3.1 EXPERIMENTAL SETTING",
            "content": "Dataset Construction Our training set is subset of OpenR1-Math-220k Face (2025), with prompts sourced from NuminaMath 1.5 Li et al. (2024) and detailed demonstrations generated by DeepseekR1 Guo et al. (2025). We utilize the version of the dataset filtered by LUFFY Yan et al. (2025), resulting in 45,000 prompts and their corresponding CoT demonstrations. RL and SFT Implementation. For RL, we omit the KL divergence loss term and set the entropy loss coefficient to 0.001. Consistent with Dr.GRPO Liu et al. (2025) and LUFFY Yan et al. (2025), we exclude both length normalization and standard error normalization from the GRPO loss 1 in all experiments. The rollout batch size is set to 128, the rollout times is 8, the update batch size is 64, and the learning rate is 1 106. temperature of 1.0 is used during rollout generation. For SFT, we use training batch size of 128 and learning rate of 1 105. ReLIFT Implementation. The RL and fine-tuning configurations for ReLIFT are the same as those described above. During interleaved fine-tuning, we utilize the demonstrations collected in advance. For RL and interleaved SFT, the learning rates are both set to 1 106. Training. By default, we use Qwen2.5-Math-7B Yang et al. (2024), following previous works Yan et al. (2025); Cui et al. (2025); Liu et al. (2025); Zeng et al.; Hu et al. (2025). To encourage greater 5 exploration, and in line with LUFFY, we increase the RoPE theta from 10,000 to 40,000 and expand the window size to 16,384. We train for 500 steps for both RL and ReLIFT, and train 3 epochs for SFT. Further details are provided in Appendix A.1. Baseline. (1) Simple-RL Zeng et al.: Trains Qwen2.5-Math-7B using rule-based reward function. (2) Oat-Zero Liu et al. (2025): Also trains Qwen2.5-Math-7B with rule-based rewards, and further proposes removing the standard deviation in GRPO advantage calculation as well as omitting tokenlevel normalization in the policy loss. (3) PRIME-Zero Cui et al. (2025): Utilizes policy rollouts and outcome labels, leveraging implicit process rewards. (4) OpenReasonerZero Hu et al. (2025): Trains Qwen-2.5-7B with rule-based rewards. (5) LUFFY Yan et al. (2025): Incorporates off-policy reasoning traces into RL by combining them with on-policy rollouts. Evaluation. For our evaluation, we primarily consider five widely used mathematical reasoning benchmarks: AIME 2024, AIME 2025, AMC Li et al. (2024), OlympiadBench He et al. (2024), and MATH500 Hendrycks et al. (2021). Due to the relatively small test sets for AIME 2024, AIME 2025, and AMC, we report avg@32 for these datasets. For OlympiadBench and MATH500, we use pass@1 as the evaluation metric. Since our reinforcement learning training is mainly centered on mathematical reasoning, we also assess generalization performance on MMLU-Pro Wang et al. (2024). To prevent data contamination, we shuffle the multiple-choice options. Table 1: Overall performance on five competition-level math benchmarks and one out-ofdistribution benchmark based on Qwen2.5-Math-7B-Base. Bold and underline indicate the best and second-best results, respectively."
        },
        {
            "title": "Model",
            "content": "AIME-24 ACC LEN AIME-25 ACC LEN"
        },
        {
            "title": "AMC",
            "content": "ACC LEN MATH-500 LEN ACC"
        },
        {
            "title": "Olympiad\nACC",
            "content": "LEN MMLU-Pro LEN ACC"
        },
        {
            "title": "Overall",
            "content": "ACC LEN Qwen-Math Qwen-Math-Instruct SimpleRL-Zero OpenReasoner-Zero PRIME-Zero Oat-Zero LUFFY"
        },
        {
            "title": "RL\nSFT\nReLIFT",
            "content": "5.1 9.7 27.0 16.5 17.0 33.4 27.2 22.1 26.9 28.2 3146 1.4 16453 8.2 6.8 15.0 12.8 11.9 18.1 - - - - 3120 1131 13.9 16199 23.1 20.1 1666 3518 22.6 15715 39.9 54.9 52.1 54.0 61.2 61. - - - - 1852 1527 59.5 16609 58.9 64.9 1093 2611 37.2 11734 77.2 76.0 82.4 81.4 78.0 85.6 - - - - 1581 449 9397 943 84.2 85.0 85. 1811 7834 - - - - 1370 120 3717 405 19.0 34.1 34.7 47.1 40.3 43.4 53.6 44.7 50.7 55.9 2429 30.5 12424 33.0 34.5 58.7 32.7 41.7 52.6 - - - - 433 9769 796 48.3 48.6 52.5 938 20919 - - - - 1052 260 4906 1463 19.3 33.7 38.98 45.3 39.7 45.9 49.7 45.5 48.9 51. 2408 14179 - - - - 1762 653"
        },
        {
            "title": "3.2 MAIN RESULTS",
            "content": "Reasoning Performance. Our main results are presented in Table 1, where we compare ReLIFT with several representative zero-RL baselines, as well as direct SFT, on-policy RL, and off-policy RL method LUFFY Yan et al. (2025). Across five challenging competition-level reasoning benchmarks and one out-of-distribution benchmark, ReLIFT achieves the highest overall accuracy of 51.1%, outperforming all existing baselines and establishing new state-of-the-art for models based on Qwen2.5-Math-7B. Specifically, ReLIFT exceeds the strongest zero RL baseline (Oat-Zero) by substantial margin of +5.2 points in overall accuracy, and consistently achieves the best or secondbest results on all individual benchmarks. Furthermore, compared to SFT and RL trained on the same dataset, ReLIFT demonstrates clear superiority: it outperforms SFT by +2.2 points and RL by +5.6 points in overall accuracy. ReLIFT also outperforms LUFFY, which combines on-policy and off-policy reasoning traces, despite using only 13% off-policy guidance. These results underscore the effectiveness of integrating RL with interleaved fine-tuning for hardest questions, enabling ReLIFT to achieve both higher performance and better generalization. Reasoning Efficiency. In addition to accuracy, we evaluate the efficiency of different methods by measuring the average response length (LEN) across all benchmarks. ReLIFT demonstrates remarkable efficiency, generating shorter solutions (average LEN: 1,061) compared to SFT (average LEN: 10,166), Qwen2.5-Math-Instruct (average LEN: 14,179), and LUFFY, while still achieving superior accuracy. This indicates that ReLIFT not only produces more accurate answers but also does so with significantly less computational overhead and more concise reasoning chains. 6 (a) Training Rewards (b) Response Lengths (c) Number of Hardest (d) Training Entropy Figure 3: Training Dynamic of rewards, response lengths, number of Hardest questions and the training entropy during RL and ReLIFT training."
        },
        {
            "title": "3.3 TRAINING DYNAMICS",
            "content": "Training Dynamics of ReLIFT. Figure 3 presents the training dynamics of rewards, response lengths, and the number of hardest questions during RL and ReLIFT training. As shown in Figure 3a, both the RL and ReLIFT models exhibit an increasing trend in rewards, indicating progressive improvement in performance. Notably, the reward achieved by ReLIFT consistently surpasses that of standard RL, suggesting that ReLIFT is more effective in optimizing the target objective. Regarding response lengths, as depicted in Figure 3b, RL tends to generate shorter responses as training progresses, which may reflect diminishing capacity to address more challenging questions. In contrast, ReLIFT demonstrates gradual increase in response length, implying greater computational effort and higher potential to solve difficult problems. Furthermore, when examining the number of hardest questions encountered during training (Figure 3c), ReLIFT consistently faces fewer such instances compared to RL, and this number decreases over time. Maintaining Exploration. As shown in Figure 3d, the entropy of the RL model steadily declines throughout training, indicating reduced exploration. In contrast, the entropy of ReLIFT remains relatively high and exhibits persistent fluctuations, consistently surpassing that of RL. This sustained exploration allows ReLIFT to continue discovering novel solutions, thereby facilitating ongoing performance improvements even in the later stages of training. Actually, our method continues to exhibit upward trend in reward over the final 500 training steps, underscoring the benefits of sustained exploration for long-term optimization."
        },
        {
            "title": "3.4 ABLATION STUDY AND ANALYSIS",
            "content": "Table 2: Ablation study on ReLIFT. Bold indicate the best result."
        },
        {
            "title": "Model",
            "content": "AIME-24 ACC LEN AIME-25 ACC LEN"
        },
        {
            "title": "AMC",
            "content": "ACC LEN MATH-500 LEN ACC"
        },
        {
            "title": "Olympiad\nACC",
            "content": "LEN MMLU-Pro LEN ACC"
        },
        {
            "title": "Overall",
            "content": "ACC LEN ReLIFT ReLIFT(all) ReLIFT(uniform) ReLIFT(random) 28.2 7.8 23.2 22.1 20.1 1666 26583 9.0 19.3 3103 13.9 1131 64.9 1093 26704 31.6 62.3 2762 59.5 85.2 943 22025 54.2 86.0 1389 84.2 449 55.9 405 16984 26.4 52.4 962 44.7 120 52.5 796 21670 13.7 51.4 1426 48.4 433 1463 27938 655 22644 51.1 23.8 49.1 45.5 1061 23650 1716 We conduct an ablation study to assess the effectiveness of our ReLIFT approach. Specifically, we compare the following three ablation settings: ReLIFT(all): Alternates one fine-tuning step after each RL step on the same batch. ReLIFT(uniform): Applies fine-tuning at fixed, uniform intervals (every 8 RL steps), matching the number of demonstrations with ReLIFT. ReLIFT(random): Performs interleaved FT. The buffer is populated based on the number of Hardest questions, but instead of using the hardest questions, it is randomly filled with other, non-hardest questions. 7 Figure 4: Number of Demonstrations As summarized in Table 2, the results show that ReLIFT (all) quickly collapses, probably due to conflicting optimization objectives between RL and FT. ReLIFT (uniform) not only reduces accuracy but also leads to longer responses compared to ReLIFT. ReLIFT (random) causes dramatic increase in average response length during training, while still underperforming ReLIFT. We also analyze the number of demonstrations of different methods and SFT, as depicted in Figure 4. ReLIFT requires only 13% of the demonstrations used in SFT. Overall, these findings demonstrate that simply alternating RL and fine-tuning is ineffective; both the scheduling and the selection of fine-tuning training data are critical. ReLIFT applies fine-tuning more frequently in the early stages, when the models reasoning ability is relatively weak, and focuses on the hardest questions. This enables the model to efficiently acquire new knowledge without sacrificing performance or conciseness. In contrast, all, uniform, or random fine-tuning scheduling leads to suboptimal results and unnecessary inflation of response length."
        },
        {
            "title": "3.5 EXTENSION TO MORE MODELS",
            "content": "Table 3: Overall performance on five competition-level math benchmarks and one out-ofdistribution benchmark based on Qwen2.5-Math-1.5B and Qwen2.5-7B. Bold and underline indicate the best and second-best results, respectively."
        },
        {
            "title": "Model",
            "content": "Qwen-Math-1.5B Qwen-Math-1.5B-Instruct SFT RL ReLIFT Qwen-7B Qwen-7B-Instruct SFT RL ReLIFT AIME-24 MATH-500 ACC LEN ACC LEN ACC LEN ACC LEN AIME-"
        },
        {
            "title": "AMC",
            "content": "3.0 8.1 12.7 10.2 13.1 5.1 10.3 15.7 13.3 19.1 4105 1.4 5722 6.6 18060 13.0 1487 8.7 6294 8.8 3146 1.4 1299 6.5 19363 18.6 4371 4.2 12197 14.7 4291 19.4 4468 36.9 19271 40.9 1187 44.7 5866 42.5 3518 22.6 562 34.9 18070 49.8 3988 40.2 9284 51. 3480 44.4 4614 66.0 12538 71.8 796 71.4 3856 73.6 2611 37.2 1043 67.6 11349 80.8 2845 65.4 5909 81.6 2637 6148 5879 423 2750 1811 605 4453 1971 2395 Olympiad MMLU-Pro ACC LEN ACC LEN"
        },
        {
            "title": "Overall\nACC LEN",
            "content": "16.7 30.7 33.8 34.5 36.0 19.0 29.8 44.3 31.4 46.1 5.0 3487 5526 24.2 12906 23.5 28.7 832 30.1 3937 39.8 2429 907 53.3 11528 54.6 56.6 2852 56.4 6160 6260 8360 11767 756 3969 434 7413 5188 180 15.0 28.8 32.6 33.0 34.0 20.9 33.7 43.9 35.2 45.0 4043 5806 13403 913 4421 2324 1804 11658 2867 6415 To evaluate the general applicability of our method, we extend ReLIFT to both smaller base model, Qwen2.5-Math-1.5B Yang et al. (2024), and weaker base model, Qwen2.5-7B Team (2024). Table 3 presents the performance across five challenging competition-level math benchmarks and one out-of-domain benchmark. On both models, ReLIFT consistently achieves substantial improvements, surpassing both SFT and RL. Across all benchmarks, our method ranks first or second, while maintaining an acceptable response length. These results underscore the applicability of ReLIFT to achieve both higher performance and better generalization."
        },
        {
            "title": "4.1 RL-DRIVEN EMERGENCE OF REASONING ABILITIES",
            "content": "Reinforcement learning has recently emerged as promising approach for enhancing the reasoning capabilities of large language models (LLMs). Early breakthroughs, such as O1 Jaech et al. (2024), DeepSeek R1 Guo et al. (2025), and Kimi 1.5 Team et al. (2025), have demonstrated that applying Reinforcement Learning with Verifiable Rewards (RLVR) can lead to significant improvements in complex reasoning tasks. Notably, DeepSeek R1 further shows that applying RLVR directly to base modelwithout the need for intermediate supervised fine-tuningcan also effectively enhance the reasoning abilities of LLMs. Building on these foundations, Light-r1 Xie et al. (2025) incorporated curriculum learning to further refine reasoning skills, while Logic-rl Xie et al. (2025) adopted logic-driven reward functions to improve general reasoning ability. Deepscaler Luo et al. (2025) introduced Iterative Context Lengthening, method that trains models with progressively longer contexts as their performance improves, thereby reducing both monetary costs and end-to-end training time. In addition, Oat-Zero Liu et al. 8 (2025) was trained from Qwen2.5-Math-7B using rule-based rewards, and proposed removing the standard deviation in GRPO advantage computation as well as token-level normalization in policy loss computation. PRIME-Zero Cui et al. (2025) leveraged policy rollouts and outcome labels, utilizing implicit process rewards to further enhance model performance."
        },
        {
            "title": "4.2 TRAINING PARADIGMS",
            "content": "Despite significant advances in reinforcement learning (RL), Yue et al. Yue et al. (2025) argue that RL with value-based rewards (RLVR) does not enable large language models (LLMs) to acquire genuinely new reasoning abilities beyond those present in their base models. Instead, RL primarily enhances performance by biasing the model toward reasoning paths it already knows that are more likely to yield rewards. In other words, most RL methods optimize within the models existing knowledge rather than facilitating the acquisition of new information, thereby limiting the LLMs overall capabilities. In contrast, directly fine-tuning on high-quality demonstration data can introduce new knowledge and abilities to the model, and this approach is widely used to adapt LLMs to downstream reasoning tasks. However, supervised fine-tuning (SFT) requires high-quality, detailed demonstration data and tends to encourage memorization of the training data, often resulting in poor generalization to out-of-distribution scenarios Chu et al. (2025). Some approaches attempt to combine supervised fine-tuning (SFT) and reinforcement learning (RL). Typically, these methods first use SFT to stabilize the models output format, then apply RL to further improve performance and potentially unlock more advanced reasoning abilities in LLMs Guo et al. (2025); Wen et al. (2025). However, such combined methods remain limited: SFT is highly dependent on the quality of demonstration data, and the subsequent RL phase is still confined to the knowledge already present in the SFT-trained model, making it difficult for the model to acquire genuinely new information. LUFFY Yan et al. (2025) seeks to address these limitations by augmenting RL with off-policy reasoning traces, dynamically balancing imitation and exploration through combination of off-policy demonstrations and on-policy rollouts during training. However, this approach requires the preparation of off-policy data in advance and relies heavily on probability-sharpening techniques, which may restrict its applicability. In contrast, our work proposes simpler and more direct method that combines RL and SFT to leverage the strengths of both paradigms. Our approach encourages LLMs to explore when possible and to incorporate new information as needed, thereby pushing RL-driven reasoning beyond the boundaries of existing knowledge."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In summary, we systematically analyze the respective strengths of RL and SFT for LLM reasoning, finding that RL is more effective for learning easier questions while SFT is crucial for solving challenging questions. Building on these insights, we introduce ReLIFT, an approach that interleaves RL with online Fine-Tuning on the hardest questions, achieving state-of-the-art results with significantly less demonstration data and more concise solutions. Future work will focus on scaling ReLIFT to larger models and developing more effective strategies for coordinating SFT and RL to further advance LLM reasoning and generalization."
        },
        {
            "title": "REFERENCES",
            "content": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Hugging Face. Math-verify. https://github.com/huggingface/Math-Verify, 2024. Accessed: 2024-06-10. Hugging Face. Open r1: fully open reproduction of deepseek-r1, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Shuangtao Li, Shuaihao Dong, Kexin Luan, Xinhan Di, and Chaofan Ding. Enhancing reasoning through process supervision with monte carlo tree search. arXiv preprint arXiv:2501.01478, 2025. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, et al. Injecting new knowledge into large language models via supervised fine-tuning. arXiv preprint arXiv:2404.00213, 2024. 10 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Y. Wang, X. Ma, G. Zhang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eighth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. arXiv preprint Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv:2504.07912, 2025."
        },
        {
            "title": "A EXPERIMENT DETAILS",
            "content": "A.1 MAIN EXPERIMENT DETAILS Training. In addition to Qwen2.5-Math-7B, we extend ReLIFT to both Qwen2.5-Math-1.5B Yang et al. (2024) and Qwen2.5-7B Team (2024). Since the maximum sequence length for Qwen2.5Math-1.5B is 4096, which is insufficient for our tasks, we follow the approach used for Qwen2.5Math-7B by increasing the RoPE theta from 10,000 to 40,000 and expanding the window size to 16,384. To ensure fairness, we maintain 8 rollouts per prompt for all RL-trained models. The learning rate is fixed at 1 106. All training experiments are conducted on 2 8 A800 GPUs. We train all RL models for 500 steps and all SFT models for three epochs. For all experiments, we use VERL Sheng et al. (2024). Evaluation. All evaluations are conducted using VLLM Kwon et al. (2023), with the temperature set to 0.6 and the maximum number of tokens set to 8192. Results are verified using MATHVERIFY Face (2024). For the baselines, we independently reproduce and verify the results of the LUFFY Yan et al. (2025), whereas the results for the other zero-RL models are reported as in the LUFFY paper. Figure 5: Chat template for all experiments. Chat Template. To encourage the base model to reason more effectively, following Yan et al. (2025), we adopt complex chat template as shown in Figure 5. All training experiments and evaluation utilize the same chat template. Reward Function. To evaluate the impact of our method, we adopt simple reward function as below. All training experiments employ the same reward function. = (cid:26)1, if the answer is correct 0, otherwise A.2 MOTIVATION EXPERIMENT DETAILS Dataset. We randomly sample subset of 8,000 examples from the training dataset described in 3.1 as the training set, and select 1,000 examples as the validation set. RL and SFT are both conducted on this subset dataset for 120 steps, using the same parameters as in our main experiments."
        }
    ],
    "affiliations": [
        "Peking University"
    ]
}