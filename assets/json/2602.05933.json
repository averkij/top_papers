{
    "paper_title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
    "authors": [
        "Zhenghao Xu",
        "Qin Lu",
        "Changlong Yu",
        "Tuo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 3 3 9 5 0 . 2 0 6 2 : r Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training Zhenghao Xu1 Qin Lu2 Changlong Yu2 Tuo Zhao1 1Georgia Institute of Technology 2Amazon"
        },
        {
            "title": "Abstract",
            "content": "Policy mirror descent (PMD) provides principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate practical algorithm, termed PMD-MEAN, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-MEAN and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KLχ2 regularizer. This additional χ2 regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-MEAN achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-MEAN and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github. com/horizon-rl/OpenKimi."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become standard paradigm for enhancing post-training of large language models (LLMs) on reasoning tasks and agentic objectives (OpenAI, 2024; Guo et al., 2025). Despite diverse implementation approaches, most RL algorithms can be formalized as regularized policy improvement, an iterative method that updates policies to maximize rewards while maintaining proximity to reference policies. Policy mirror descent (PMD, Geist et al. 2019; Tomar et al. 2022) provides canonical formalization of this approach by iteratively solving KL-regularized improvement subproblems. In theory, these subproblems admit elegant closed-form solutions that reweight the current policy and renormalize using the partition function. In practice, however, reliably estimating this partition function and fitting the ideal target from finite rollouts presents significant challenges, particularly in the large action space of LLM post-training. common approach to solving KL-regularized subproblems involves applying policy gradient methods (Williams, 1992) directly to the regularized objective, either by incorporating regularization into the reward function or adding an explicit KL penalty. Methods such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024), and GRPO (Shao et al., 2024) Work done during internship at Amazon. Emails: {zhenghaoxu,tourzhao}@gatech.edu use rollout samples to construct surrogate losses and perform optimally when rollouts are from the current policy, i.e., on-policy. However, modern efficient RL implementations increasingly leverage large generation batches or asynchronous rollouts to prevent computational bottlenecks from long-tail generations (Noukhovitch et al., 2024; Fu et al., 2025). These approaches typically incur staleness tax: the sampling policy predates the policy being updated, creating fundamental training/inference mismatch. This mismatch introduces instability that practitioners attempt to mitigate through importance weighting with clipping or similar heuristics (Yao et al., 2025; Liu et al., 2025). While partially effective, these remedial techniques substantially complicate both implementation and theoretical analysis. This paper investigates an alternative minimalist approach popularized by Kimi K1.5/K2 (Team et al., 2025b,a) that fundamentally reframes the problem. Rather than attempting to mitigate off-policy-ness through complex correction mechanisms and heuristics, this algorithm adopts an off-policy regression perspective on PMD. Specifically, instead of fitting the exact partition-normalized target, the method approximates the log-partition term with the mean reward under the sampling policy and fits regression target directly in log-policy space. We refer to this practical algorithm as PMD-MEAN (or Kimi-style PMD). While this mean-reward approximation remains accurate under strong regularization conditions, it can diverge significantly from the partition-normalized update when using smaller regularization, typical in practice. This divergence raises fundamental question: What does PMD-MEAN optimize exactly, and what are the algorithmic consequences of that objective? Our results. We address this fundamental question by deriving closed-form characterization of the PMD-MEAN population solution. While the ideal KL-regularized PMD update produces standard Boltzmann reweighting, our analysis reveals that PMD-MEAN generates an update involving the Lambert-W function. Furthermore, we show that this update is mathematically equivalent to performing mirror descent with mixed KLχ2 regularizer, where the χ2 weight adapts dynamically based on the mean reward under the current policy. This additional χ2 term imposes stronger penalties on probability changes compared to KL alone, and the effect is particularly pronounced when the mean reward is low. This adaptive regularization effectively moderates the convergence rate during the early phases of training, providing principled explanation for the algorithms empirical stability. Our further analysis demonstrates that, compared to fitting the partition-normalized target (PMDPART), PMD-MEAN exhibits significantly reduced sensitivity to finite-sample errors when rollouts are limited. This characteristic substantially decreases the risk of overfitting to misestimated targets. The finding provides theoretical explanation for PMD-MEANs enhanced stability in practical applications: The implicitly induced χ2 regularization introduces additional robustness that is valuable in the data-constrained scenarios typical of LLM post-training. Contributions. We make the following contributions: Exact characterization of PMD-MEAN. We derive the closed-form PMD-MEAN solution in policy space and establish its equivalence to mirror-descent subproblem with an adaptive mixed KLχ2 regularizer. Regularization and stability mechanism. We demonstrate that the induced χ2 term provides additional control over probability ratios, offering substantial regularization even when the nominal KL coefficient is minimal. Convergence analysis. Under standard assumptions, we develop an inexact-PMD style convergence analysis that distinguishes PMD-MEAN from PMD-PART and characterizes their separations. Experimental validation. Through experiments on math reasoning tasks, we empirically confirm PMD-MEANs enhanced efficiency and stability while demonstrating its performance advantages over the standard GRPO."
        },
        {
            "title": "2 Preliminaries",
            "content": "For simplicity, we formulate RL for LLM post-training as contextual bandit problem. Let be an input prompt (state) and represent generated response (action), with r(x, y) [0, 1] denoting bounded reward function. language model policy π : (Y), which maps states to distributions over actions, induces the expected reward: J(π) := ExDEyπ(x) (cid:2)r(x, y)(cid:3), where is the distribution over prompts (e.g., dataset). In practice, LLM policies are implemented as compositions of token-wise softmax distributions, which assign non-zero probability to every token in the vocabulary. Consequently, we can reasonably assume that π has full support over the entire action space of possible responses, that is, π(y x) > 0 for all and Y. Policy mirror descent. At global step t, KL-regularized policy mirror descent (PMD; Geist et al. 2019; Tomar et al. 2022) updates policy πt by solving the following optimization problem for each state x: πt+1( x) = argmax Eyπ(x)[r(x, y)] τ KL (π( x) πt( x)) , (1) π(x)(Y) where τ > 0 is the regularization parameter that controls the strength of regularization. The KL divergence between distributions and over is KL(p q) := Eyp (cid:105) . (cid:104) log p(y) q(y) This KL-regularized optimization problem in (1) admits the following unique closed-form solution: πt+1(y x) = (cid:2)er(x,y)/τ (cid:3) is the partition function that ensures proper normalization. where Zt(x) := Eyπt(x) This update resembles Boltzmann distribution that exponentially reweights the previous policy according to the reward signal. (2) , πt(y x) exp(r(x, y)/τ ) Zt(x) Fitting the target by regression. The ideal update in (2) is computationally infeasible in large action spaces as it requires partition function evaluation and per-action probability assignment. direct off-policy approach to approximate this update is to fit the target in log-policy space using squared regression. After collecting rollouts πt( x) and evaluating rewards r(x, y), we define the state-dependent target log-ratio: part(x, y) := log πt+1(y x) πt(y x) = r(x, y) τ log Zt(x), where πt+1 is defined according to Equation (2). This leads to the following squared regression loss: (cid:16) (cid:17)2(cid:21) Lpart(π) := ExDEyπt(x) part(x, y) . (3) (cid:20) 1 2 log π(y x) πt(y x) If the policy class can represent the target distribution exactly (i.e., is realizable) and Zt(x) is known precisely, then minimizing (3) recovers the optimal policy update. However, in practice, Zt(x) must be estimated from the same finite set of rollouts, which introduces significant estimation error. For large action spaces and small regularization parameters τ , this estimation can be highly unstable, leading to pathological update behavior (see Section 5). We refer to this direct fitting approach as PMD-PART. PMD-MEAN: Log-partition approximation. Instead of fitting part, Team et al. (2025b) propose an alternative approach that approximates the log-partition function with the average reward, resulting in modified loss function. Specifically, we define the advantage function (x, y) under πt with mean reward baseline: With this definition, the regression objective becomes: (x, y) := r(x, y) Eyπt(x)[r(x, y)]. Lmean(π) := ExDEyπt(x) (cid:34) 1 2 (cid:18) log π(y x) πt(y x) (x, y) τ (cid:19)2(cid:35) . (4) We refer to this practical variant as PMD-MEAN, which has been adopted to train advanced models such as Kimi K1.5 and K2 (Team et al., 2025b,a). In practice, the expected reward Eyπt(x)[r(x, y)] can be efficiently estimated using per-prompt Monte Carlo average over sampled responses. 3 3 Implicit Regularization of PMD-MEAN The approximation adopted by PMD-MEAN is accurate when τ . However, large τ will significantly slow down convergence, and the average reward deviates significantly from the logpartition function when τ becomes small (Figure 1, left). Therefore, the solution of (4) may differ significantly from the ideal target πt+1 in (2), and thus no longer corresponds to the solution of the KL subproblem (1), even with infinite samples provided (Figure 1, right). 3.1 Exact Solution of PMD-MEAN To understand the actual objective of PMD-MEAN, we characterize the population minimizer of (4). For simplicity, we omit and consider single state, writing πt(y) and y. Theorem 3.1 (PMD-MEAN solution). Assume πt(y) > 0 for all Y. Let := r(y) Eyπt[r(y)] denote the mean-baseline advantage. Then the unique minimizer of (4) over the probability simplex satisfies πt+1(y) = πt(y) exp (cid:16) τ where () is the principal branch of the Lambert-W function (inverse of (w) = ew) and λ 0 is normalization constant chosen such that (cid:80) πt+1(y) = 1. Moreover, defining := Eπt[exp(y/τ )] and := Eπt[exp(2y/τ )], the normalization constant satisfies τ 2 A(A 1) (cid:16) λ τ 2 exp λ τ 2 log A. (cid:16) τ (cid:17)(cid:17)(cid:17) (5) (6) , For binary rewards {0, 1} with = Eπt[r(y)], λ (cid:26) 1 2 p(1 p), τ p(1 p), τ τ 0. (7) By Theorem 3.1, the solution of PMD-MEAN has its action probabilities heterogeneously normalized by the Lambert-W function, as opposed to the KL solution (2) where the normalization term is the log-partition function that is independent of the action y. Given the monotonicity of , actions with larger will get their probability suppressed compared to the KL solution, while actions with smaller will not be punished as hard. Therefore, PMD-MEAN update is less aggressive than PMD-PART. More precisely, suppose the reward is binary and the average reward under πt is p. We consider the ratio πt+1/πt on positive and negative actions when τ is small. For positive actions, r(y) = 1, PMD-MEAN yields while PMD-PART yields πmean t+1 (y) πt(y) = 1 1 ep/τ (1 + o(1)), πpart t+1 (y) πt(y) = 1 1 p2 e1/τ + O(e2/τ ). (8) (9) While both ratios approach 1/p from below when τ 0, the gap is much larger in PMD-MEAN when is small (e.g., early phase of training), hence giving more conservative update. For negative actions, the separation is clearer: For r(y) = 0, PMD-MEAN yields while PMD-PART yields πmean t+1 (y) πt(y) = ep/τ (1 + o(1)), πpart t+1 (y) πt(y) = 1 e1/τ + O(e2/τ ). (10) (11) When is small, the difference is significant, as illustrated in Figure 2. Full derivations are provided in Section B.2. 4 Figure 1: Left: Scaled log-partition function vs average reward assuming binary rewards. The gap is significant for moderate τ . Right: Illustration of PMD-MEAN and PMD-PART converging to different subproblem solutions in the probability simplex. 3.2 PMD-MEAN as Mirror Descent with Mixed KLχ2 Regularization (cid:2)( p(y) The Lambert-W closed form is useful for analysis, while more transparent insight is that PMDMEAN is exactly solving different regularized policy improvement problem. Let χ2(p q) := q(y) 1)2(cid:3) denote the χ2-divergence between two distributions and over Y. The following Eyq proposition shows that the PMD-MEAN update is equivalent to mirror descent with an additional χ2 penalty. Proposition 3.2 (Equivalent mixed KLχ2 subproblem). Fix state (omitted for brevity). Let πt+1 be the PMD-MEAN population solution in Theorem 3.1 and λ be the same normalization constant. Then πt+1 is the solution to πt+1 = argmax π(Y) Eyπ (cid:2)r(y)(cid:3) τ KL(π πt) λ 2τ χ2(π πt). (12) Proposition 3.2 follows directly from the KKT conditions of both problems (see Section B.3). The χ2 penalty directly suppresses large policy ratio spikes and provides stronger regularization compared to KL, as KL(p q) χ2(p q) for p, with full support. Moreover, Theorem 3.1 shows the effective strength λ/τ is adaptive. For binary rewards, λ/τ remains O(1) even as τ 0, which implies PMD-MEAN still regularizes updates when the nominal KL regularization is small. Remark 3.3 (Connection to χ2 preference optimization). The mixed KLχ2 regularizer has appeared in χ2-regularized preference optimization (χPO, Huang et al. 2024), which provides provable guarantees to avoid overoptimization in KL-regularized preference optimization. Our results show that PMD-MEAN can be interpreted as an online version of this idea, with an adaptive coefficient λ tied to the rollout reward distribution. Remark 3.4 (Policy ratios compared to Huang et al. 2024). Huang et al. (2024) show in their Proposition 4.2 that mixed KLχ2 regularized problem yields (in our notations) (cid:18) exp (cid:19) Rmax τ πmix t+1 πt 1 + Rmax τ , while the KL-regularized problem yields (cid:18) exp (cid:19) Rmax τ πKL t+1 πt exp (cid:18) Rmax τ (cid:19) , (13) (14) where [0, Rmax] is the bound of rewards. This highlights worst-case polynomial vs. exponential contrast in 1/τ for the upper policy ratio control. However, these bounds are uniform in Rmax and can be vacuous in regimes relevant to our binary reward analysis. Particularly, (14) neglects the partition normalizer that also grows with 1/τ . In contrast, our setting admits substantially sharper and distribution-dependent behavior for small τ . As shown in (8) and (9), both ratios share the 5 Figure 2: The (log) probability ratio of updates in PMD-MEAN is more conservative than that in PMD-PART for binary rewards. same constant upper bound 1/p, with different exponential rates of approaching this upper bound. Therefore, the gap between mixed and KL divergence in our setting is more of distinction in the distribution-dependent exponential rates, i.e., O(ep/τ ) vs. O(e1/τ ), instead of polynomial vs. exponential."
        },
        {
            "title": "Implications on Convergence",
            "content": "We present an inexact-PMD convergence analysis from the regression view to illustrate the implications of the implicit regularization on convergence. For clarity, we still suppress and analyze one state. The extension to averaging over the prompt distribution is standard. 4.1 One-Step Policy Improvement Recall J(π) := Eyπ[r(y)] where r(y) [0, 1]. At iteration t, let π update in PMD-PART or PMD-MEAN, and define its target log-ratio s(y) := log be the policy class and define, for each π Π, sπ(y) := log π(y) PMD-MEAN is to fit the ideal target with sπ by minimizing the population loss 1 2 (cid:2)(sπ(y) s(y))2(cid:3). t+1 denote the ideal target π t+1(y) πt(y) . Let Π πt(y) . The goal of PMD-PART and Lt(π) := Eyπt (15) In practice, π available, so one instead forms an estimated target update (cid:101)π samples y1, . . . , yn πt, we minimize the empirical loss t+1 depends on population quantities (e.g., Eπt[r] or log Zt) that are not exactly t+1 via finite MC samples. Given i.i.d. (cid:98)Lt(π) := 1 2n (cid:88) i=1 (sπ(yi) (cid:101)s i(yi))2, where the target at yi becomes the leave-one-out (LOO) estimated version (cid:101)s i(yi): (cid:101)s i(yi) = (cid:16) 1 τ r(yi) 1 1 (cid:88) j=i (cid:17) r(yj) r(yi) τ log (cid:16) 1 1 (cid:88) er(yj )/τ (cid:17) j=i (PMD-MEAN) (PMD-PART) (16) (17) (18) Let (cid:98)πt+1 be an approximate ERM solution and set πt+1 := (cid:98)πt+1. We make the following assumptions for analysis. Assumption 4.1 (Realizability). π t+1 Π. 6 Assumption 4.2 (Bounded optimization error). (cid:98)Lt((cid:98)πt+1) inf πΠ (cid:98)Lt(π) + ϵopt. Assumption 4.3 (Bounded log-ratio). There exist B, B+ > 0 such that for all π Π and Y, sπ(y) B+, Assumption 4.4 (Finite policy class). Π < . sπ(y) B. (19) Assumption 4.1 is standard in the sample-efficient RL literature (Foster & Rakhlin, 2023). Assumption 4.2 is generic assumption that allows focusing on statistical rates without involving overcomplicated subproblem optimization dynamics. Assumption 4.3 can be achieved by restricting the policy class via clipping. Assumption 4.4 is for simplicity, and one can extend it to general policy classes with Π replaced by other complexity metrics, e.g., covering number. Under these assumptions, we have the following lemma that characterizes the error of the approximate ERM solution (cid:98)πt+1. Lemma 4.5 (Empirical minimization). For global step t, suppose y1, . . . , yn πt are i.i.d. samples. Let Lt(π) and (cid:98)Lt(π) denote the population and empirical losses defined in (15) and (16), respectively. Define the target mismatch := (cid:101)s i(yi) s(yi), 2 := 1 (cid:88) i= 2 . Under Assumptions 4.1 to 4.4, for any δ (0, 1), with probability at least 1 δ, Lt((cid:98)πt+1) B2 log(Π /δ) + ϵopt + 2. (20) t+1) and J((cid:98)πt+1) that affects the one-step policy improvement. The proof is provided in Section C.1. The ERM solution error is then translated into the gap between J(π Theorem 4.6 (One-step policy improvement). Under Assumptions 4.1 to 4.4, suppose the ideal population update π t+1 satisfies, for some ηt (0, 1], (21) Let πt+1 := (cid:98)πt+1 and 2 be as in Lemma 4.5. Then for δ (0, 1), with probability at least 1 δ, (cid:16) eB+/2(cid:16) 1 J(πt+1) (1 ηt)(cid:0)1 J(πt)(cid:1) + ϵopt + t+1) (1 ηt)(cid:0)1 J(πt)(cid:1). 1 J(π (22) 2 (cid:17)(cid:17) (cid:114) (cid:112) + . log(Π /δ) 4.2 Instantiation and Separation We now specialize Theorem 4.6 to the binary reward model r(y) {0, 1} with pt := J(πt) (0, 1) in the small τ > 0 regime, and instantiate (i) the ideal improvement rate ηt in (21), (ii) the bounded log-ratio constants (B, B+) in Assumption 4.3, and (iii) the target estimation error 2. These quantities highlight separation between PMD-PART and PMD-MEAN at the early phase of training where pt is small: PMD-PART has faster ideal convergence rate, while PMD-MEAN can be more robust to statistical errors under finite rollouts. 4.2.1 Ideal Convergence Rate ηt The ideal improvement rate is direct consequence of our analysis in Section 3, and reveals the behavior when the rollout sample size is large. Proposition 4.7 (Ideal contraction for PMD-MEAN with small τ ). Let π with binary rewards. Then as τ 0 we have t+1 be the ideal update (5) ηmean = 1 exp (cid:16) pt τ (cid:17) (cid:0)1 + o(1)(cid:1). (23) Proposition 4.8 (Ideal contraction for PMD-PART). Let π rewards. Then (21) holds with t+1 be the ideal update (2) with binary ηpart = 1 1 1 pt + pte1/τ . (24) The proofs are provided in Sections C.3 and C.4. The ideal convergence rates in PMD-PART and PMD-MEAN both approach 1 when τ 0, leading to one-step convergence. Meanwhile, PMD-PART approaches this rate faster than PMD-MEAN when pt < 1 is small. 7 4.2.2 Log-ratio Bounds (B, B+) Compatible with Realizability In Theorem 4.6, the error of the inexact update depends on and B+. We compute the smallest (B, B+) compatible with the realizability of the ideal target in the binary model. Proposition 4.9 (Log-ratios for PMD-MEAN with small τ ). Consider binary rewards and the PMDMEAN target s(y) = log . Then for fixed pt (0, 1) and τ 0, the log-ratio bounds are t+1 (y) πt(y) πmean exp(Bmean +,t ) = 1 pt 1 pt pt ept/τ (1 + o(1)), Bmean = pt τ + o(1). Proposition 4.10 (Log-ratios for PMD-PART with small τ ). Consider binary rewards and the πpart t+1 (y) PMD-PART target s(y) = log πt(y) . Then for fixed pt (0, 1) and τ 0, the log-ratio bounds are exp(Bpart +,t ) = 1 pt 1 pt p2 e1/τ + O(e2/τ ), Bpart = 1 τ log 1 pt + o(1). The proofs follow from Proposition B.1 in Section B.2. By Propositions 4.9 and 4.10, when τ is small, the factors in PMD-PART are worse than PMD-MEAN, and the gap is significant when pt is small. 4.2.3 Target Estimation Error It remains to compare the target estimation error. Proposition 4.11 (Target estimation error for binary rewards). Assume r(y) {0, 1} and define pt := Eyπt[r(y)]. Let pi := 1 j=i r(yj) and := e1/τ 1, then Zt = 1 + apt. Fix δ (0, 1) n1 and define (cid:80) (cid:114) 2pt(1 pt) log(4n/δ) 1 Then with probability at least 1 δ, the LOO mean satisfies maxi pi pt εn(pt, δ), and consequently: 2 log(4n/δ) 3(n 1) εn(pt, δ) := + . (a) (PMD-MEAN) For (cid:101)s i(yi) in (17) and s(y) = log 2 εn(pt, δ)2 + τ 2 , πmean t+1 (y) πt(y) pt(1 pt)2 τ 2 . πpart t+1 (y) πt(y) , (b) (PMD-PART) For (cid:101)s i(yi) in (18) and s(y) = log (cid:26) (cid:18) 2 min εn(pt, δ) 1 + a(pt εn(pt, δ))+ (cid:27)(cid:19)2 . , 1 τ (25) (26) τ 2n (cid:1), while the other part O(cid:0) pt(1pt)2 The proof is provided in Section C.5. By Proposition 4.11, the target estimation error of PMD-MEAN consists of two parts. One part scales as O(cid:0) pt(1pt) (cid:1) is irreducible τ 2 due to the systematic mismatch between the estimated target (cid:101)s the ideal target that contains the Lambert-W term. In particular, for positive actions, the estimated target is systematically larger than the ideal target, resulting in more aggressive improvement on these actions. Meanwhile, for negative actions, the estimated target is closer to the ideal, thus inherits the more conservative shrinkage at rate exp(pt/τ ). For PMD-PART, there is critical regime where = Θ( 1 ). When is smaller than this threshold, pt }(cid:1), which is larger than the error of PMD-MEAN. the bound almost scales as O(cid:0)min{ 1 (cid:1). This suggests that for PMD-PART, the When exceeds the threshold, the bound scales as O(cid:0) 1pt ptn rollout sample size should be large enough, especially at the early phase of training where pt is small. τ 2 , e2/τ pt Figures 3 and 6 show simulations of target estimation errors, where PMD-PART suffers from larger error when pt and the rollout sample size are small. This explains that while PMD-PART has faster ideal convergence rate (almost in one step for small τ ), it can be very unstable in practice when the rollouts are limited. Meanwhile, PMD-MEAN suffers less from estimation error in this regime. 8 Figure 3: Target estimation error of PMD-MEAN and PMD-PART under τ = 0.05 and pt ranges from 0.01 to 0.2. Left: the target estimation error 2. Right: The scaled estimation error with corresponding prefactor eB+ in (22). The plot shows the average from 100 random seeds. When the rollout sample size is small, the error of PMD-PART is much larger for small pt. 4.2.4 Refined Analysis for PMD-MEAN While the gap between the ideal target and the estimated target of PMD-MEAN does not vanish as , the minimizer of the empirical loss (16) recovers the ideal target policy in this limit, as the constraint Eπt[esπ ] = 1 pulls back the log-ratios from exactly fitting the advantages. In that large regime, Proposition 4.11 is overly pessimistic. We provide refined analysis that eliminates the error floor. Lemma 4.12 (Refined analysis for PMD-MEAN). Suppose Assumptions 4.1 to 4.4 hold, r(y) {0, 1} and define pt := Eyπt[r(y)]. Let εn(pt, δ) be as in Proposition 4.11. Then for any δ (0, 1), with probability at least 1 δ, empirical PMD-MEAN yields (cid:98)πt+1 such that Lt((cid:98)πt+1) log(Π /δ) τ 2n + ϵopt + pt εn(pt, δ) + εn(pt, δ)2 τ 2 . (cid:1) term as PMD-PART in (20) (as Bpart (cid:1), which vanishes as . This is better than the O(cid:0) 1 τ 2 The complete proof is provided in Appendix D. Lemma 4.12 shows that PMD-MEAN now shares the (cid:1)), while the 2 term now becomes same O(cid:0) log(Π/δ) τ 2n O(cid:0) pt (cid:1) error of PMD-PART (cid:112) pt + 1 τ 2n2 τ 2 in the small and pt regime. On the other hand, when = Ω(cid:0) 1 (cid:1), the error of PMD-PART is pt O(1), while the error in PMD-MEAN is O(cid:0) p2 (cid:1). This suggests an adaptive regularization scheme of τ 2 PMD-MEAN that scales τ with per-prompt pass rate pt, which we leave for future investigation. = O(cid:0) τ"
        },
        {
            "title": "5 Experiments",
            "content": "We conduct experiments on math reasoning RL to validate the practical performance of PMD. Our implementation is based on verl (Sheng et al., 2025). We train on the DAPO-Math-17k dataset (Yu et al., 2025) with base models Qwen2.5-7B (Yang et al., 2024) and Qwen3-30B-A3B-Base (Yang et al., 2025). The 7B models are trained for 495 global steps (15 epochs), while the 30B models are trained for 300 global steps. We apply the same prompt formatting as in Yu et al. (2025). The reward is binary 1 based on the answer correctness only. We set the global batch size as 512 prompts with group size 16 and sampling temperature 1 for rollout. The maximum response length is 8192 for Qwen2.5-7B and 20480 for Qwen3-30B-A3B-Base. We train with mini-batch size of 32 prompts (512 sequences) and learning rate 1 106. We evaluate on AIME 2024 and AIME 2025. For each problem, we sample 32 solutions and report the average score. We mainly use GRPO (Shao et al., 2024) as the baseline. For efficiency comparison, we also include the on-policy gradient by setting the global batch size (rollout prompts) as 32 so that it equals the mini-batch size. More implementation details are provided in Section A. 9 Table 1: Overall evaluation scores (Avg@32). Staleness indicates the number of ministeps using rollouts from the same old policy. Method (τ ) Staleness AIME 24 AIME 25 Average Qwen2.5-7B GRPO (-) On-policy (-) PMD-MEAN (0.005) PMD-MEAN (0.01) PMD-MEAN (0.02) 16 1 16 16 16 17.08 18.65 19.69 17.60 22. Qwen3-30B-A3B-Base GRPO (-) PMD-MEAN (0.01) PMD-MEAN (0.1) 16 16 16 36.56 50.00 50.83 10.52 18.33 19.48 17.50 16.67 27.92 35.10 37. 13.80 18.49 19.58 17.55 19.58 32.24 42.55 44.01 Table 2: Comparing efficiency of on-policy gradient and PMD-MEAN. Timing is in milliseconds per token. While the actor update cost is comparable, larger global batch size (high staleness) amortizes the inference cost and reduces overall training time. Method Overall (ms/token) Generation Actor Update On-policy PMD-MEAN 0.0569 0.0126 0.0512 0.0062 0.0057 0.0064 5.1 Main Results The main results are shown in Table 1. PMD-MEAN significantly outperforms the GRPO baseline: τ = 0.005 achieves +2.6% AIME24 and +9.0% AIME25 absolute gains on 7B model, and τ = 0.1 achieves +14.6% AIME24 and +8.1% AIME25 absolute gains on 30B MoE model. Efficiency from larger global batch size. As shown in Tables 1 and 2, compared to the on-policy gradient with staleness 1, the off-policy PMD-MEAN achieves comparable performance with 4.6 speedup by leveraging larger global rollout batch size that amortizes the inference cost. Stability. As shown in Figure 4, PMD-MEAN remains stable during training, while PMD-PART is highly unstable and could collapse even with much larger τ . Figure 4: Training curves (smoothed) of PMD-MEAN (upper) and PMD-PART (lower) with baselines for Qwen2.5-7B on DAPO-Math-17k (left) and the averaged evaluation accuracy on AIME 2024 and AIME 2025 (right). The global step of on-policy gradient is divided by 16 to match other algorithms. Policy ratios. We record the log policy ratios between the actor policy πθ in the last mini-step and the old rollout policy πt of that global step, using it as an approximation of log πt+1 . As shown in πt Figure 5, the trend validates our theory in Section 4.2.2 that the policy decrease in PMD-MEAN is weaker than PMD-PART, and becomes stronger as training proceeds and accuracy improves. Beyond standard GRPO. Standard GRPO faces stability issues in training large MoE models. We further compare PMD-MEAN with GSPO (Zheng et al., 2025), which is an advanced variant of 10 Figure 5: The minimum of log-ratios log πt+1 πt last update mini-batch. in PMD-MEAN and PMD-PART, estimated from the GRPO that incorporates sequence-level importance sampling (IS) with clipping and geometric mean normalization to resolve this MoE stability issue and achieves superior performance to GRPO. The results are shown in Table 3. As shown, PMD-MEAN outperforms GSPO on the Qwen2.5-7B and achieves comparable performance on the Qwen3-30B-A3B-Base MoE model. More detailed results are provided in Section A.4. Table 3: Comparison of PMD-MEAN and GSPO (Avg@32). Method Model AIME 24 AIME 25 Average GSPO PMD-MEAN GSPO PMD-MEAN 7B 7B 30B 30B 15.52 19.69 53.33 50.83 11.98 19. 34.58 37.19 13.75 19.58 43.96 44."
        },
        {
            "title": "6 Related Work",
            "content": "RL for Post-Training of LLMs. Contemporary LLM post-training and alignment frameworks predominantly utilize reinforcement learning with human or AI feedback (RLHF/RLAIF, Ziegler et al. 2019; Ouyang et al. 2022; Bai et al. 2022) or verifiable rewards (RLVR, Lambert et al. 2024). This paradigm has demonstrated particular efficacy for mathematical reasoning, coding, and logical tasks, subsequently inspiring large-scale RL methodologies and architectural designs for increasingly complex agentic capabilities (OpenAI, 2024; Guo et al., 2025; Google, 2025; Team et al., 2025a). Policy gradient methods (Williams, 1992; Sutton et al., 1999), especially TRPO and PPO (Schulman et al., 2015, 2017), have established themselves as foundational approaches in reinforcement learning. However, within LLM post-training contexts, maintaining parameterized value networks (critic models) introduces substantial estimation biases and computational overhead. To mitigate these limitations, recent methods such as GRPO (Shao et al., 2024) and RLOO (Ahmadian et al., 2024) eliminate the critic component and instead leverage group-based relative baselines estimated from multiple Monte Carlo samples per prompt. More sophisticated approaches, such as DAPO (Yu et al., 2025), further refine performance through advanced optimization techniques. These methods fundamentally depend on sampling distributions that closely match the current policy distribution, necessitating off-policy correction mechanisms to maintain training stability. While PPO/GRPO implement token-level importance sampling (IS) with clipping, more recent algorithms such as GSPO (Zheng et al., 2025) and CISPO (Chen et al., 2025) employ sequence-level IS or detached clipping IS to enhance stability when training mixture-of-expert (MoE) models. Additional research addresses the training-inference distribution mismatch at the infrastructure level, proposing methodological refinements including truncated IS (Yao et al., 2025) and masked IS (Liu et al., 2025). Although partially effective, these techniques substantially increase algorithmic complexity and incorporate numerous empirical adjustments that resist theoretical analysis. In contrast, our investigation focuses on the minimalist PMD algorithm, which offers greater analytical transparency while delivering competitive empirical performance. 11 Policy Mirror Descent. The mirror descent framework (Nemirovski & Yudin, 1983) provides classical formulation for policy optimization in reinforcement learning (Geist et al., 2019; Tomar et al., 2022), with extensive literature analyzing its theoretical iteration and sample complexities (Xiao, 2022; Zhan et al., 2023; Lan, 2023; Yuan et al., 2023; Alfano et al., 2023; Xu et al., 2024). However, these analyses predominantly address tabular settings or function approximation scenarios with abundant samples, rather than the practical constraints of LLM post-training where rollouts are necessarily limited. Our work establishes novel connection between practical LLM posttraining methodologies and the choice of Bregman divergence, demonstrating that PMD-MEAN implicitly optimizes an adaptive mixed KLχ2 regularizer. This mixed regularization approach shares conceptual similarities with χ2PO (Huang et al., 2024), which employs mixed KLχ2 divergence to mitigate overfitting in KL-regularized DPO (Rafailov et al., 2023) under distribution shift conditions. While alternative regularization schemes have been explored in offline preference learning contexts (Wang et al., 2023), our investigation specifically addresses online policy optimization challenges. The literature offers various regression-based approaches to approximate the ideal KL solution in PMD-PART. Richemond et al. (2024) propose incorporating value network to estimate the logpartition function, which dates back to Nachum et al. (2017). Gao et al. (2024) develop technique for fitting pairwise relative rewards that eliminates the partition term entirely. Bartoldson et al. (2025) approximate the log-partition term in the loss using the group average of all other terms (with stop-grad). In contrast, PMD-MEAN (Team et al., 2025b) implements simpler strategy by directly approximating the log-partition function with the mean reward. Our analysis focuses on characterizing the theoretical properties of this practical approximation and establishing its mathematical foundations."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presents comprehensive analysis of PMD-MEAN, practical algorithm for large-scale LLM post-training that has been deployed in leading language models. The analysis provides an exact characterization of the algorithms population update through the Lambert-W function, establishing its mathematical equivalence to solving mirror descent subproblems with an adaptive mixed KLχ2 regularizer. This theoretical framework reveals concrete mechanism underlying the algorithms stability: the induced χ2 term systematically constrains large probability ratio changes, effectively preventing overly aggressive policy updates that often lead to training instability. The investigation deliberately focuses on the principled form of PMD-MEAN to enable clearer theoretical analysis and understanding. Advanced techniques such as oversampling strategies and importance sampling corrections for addressing training/inference engine mismatches could potentially enhance PMD-MEANs performance further, and these directions are reserved for future research. By elucidating the fundamental mathematical properties of PMD-MEAN, this work contributes to the development of theoretically grounded yet practically effective RL algorithms for LLM post-training, potentially enabling simpler, more robust, and scalable approaches to this increasingly critical task."
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Alfano, C., Yuan, R., and Rebeschini, P. novel framework for policy mirror descent with general parametrization and linear convergence. arXiv preprint arXiv:2301.13139, 2023. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Bartoldson, B., Venkatraman, S., Diffenderfer, J., Jain, M., Ben-Nun, T., Lee, S., Kim, M., ObandoCeron, J., Bengio, Y., and Kailkhura, B. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929, 2025. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Foster, D. J. and Rakhlin, A. Foundations of reinforcement learning and interactive decision making. arXiv preprint arXiv:2312.16730, 2023. Fu, W., Gao, J., Shen, X., Zhu, C., Mei, Z., He, C., Xu, S., Wei, G., Mei, J., Wang, J., et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Gao, Z., Chang, J., Zhan, W., Oertell, O., Swamy, G., Brantley, K., Joachims, T., Bagnell, D., Lee, J. D., and Sun, W. Rebel: Reinforcement learning via regressing relative rewards. Advances in Neural Information Processing Systems, 37:5235452400, 2024. Geist, M., Scherrer, B., and Pietquin, O. theory of regularized markov decision processes. In International conference on machine learning, pp. 21602169. PMLR, 2019. Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. Huang, A., Zhan, W., Xie, T., Lee, J. D., Sun, W., Krishnamurthy, A., and Foster, D. J. Correcting the mythos of kl-regularization: Direct alignment without overoptimization via chi-squared preference optimization. arXiv preprint arXiv:2407.13399, 2024. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Lan, G. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical programming, 198(1):10591106, 2023. Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Jiang, Z. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025. URL https://richardli. xyz/rl-collapse. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Bridging the gap between value and policy based reinforcement learning. NeurIPS, 30, 2017. Nemirovski, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983. Noukhovitch, M., Huang, S., Xhonneux, S., Hosseini, A., Agarwal, R., and Courville, A. Asynchronous rlhf: Faster and more efficient off-policy rl for language models. arXiv preprint arXiv:2410.18252, 2024. OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. NeurIPS, 35, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Richemond, P. H., Tang, Y., Guo, D., Calandriello, D., Azar, M. G., Rafailov, R., Pires, B. A., Tarassov, E., Spangher, L., Ellsworth, W., et al. Offline regularised reinforcement learning for large language models alignment. arXiv preprint arXiv:2405.19107, 2024. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. NeurIPS, 12, 1999. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025b. Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M. Mirror descent policy optimization. In International Conference on Learning Representations, 2022. Wang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Xiao, L. On the convergence rates of policy gradient methods. Journal of Machine Learning Research, 23(282):136, 2022. Xu, Z., Ji, X., Chen, M., Wang, M., and Zhao, T. Sample complexity of neural policy mirror descent for policy optimization on low-dimensional manifolds. Journal of Machine Learning Research, 25 (226):167, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, F., Liu, L., Zhang, D., Dong, C., Shang, J., and Gao, J. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao.notion.site/ off-policy-rl. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuan, R., Du, S. S., Gower, R. M., Lazaric, A., and Xiao, L. Linear convergence of natural policy gradient methods with log-linear policies. In ICLR, 2023. Zhan, W., Cen, S., Huang, B., Chen, Y., Lee, J. D., and Chi, Y. Policy mirror descent for regularized reinforcement learning: generalized framework with linear convergence. SIAM Journal on Optimization, 33(2):10611091, 2023. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 14 Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Experimental Details",
            "content": "We train on the DAPO-Math-17k2 dataset (deduplicated). The base models include Qwen2.5-7B3 and Qwen3-30B-A3B-Base4. A.1 Prompt Template Our prompt template follows Yu et al. (2025), with all questions problem_statement processed in the following form. Chain-of-Thought (CoT) Prompt Template Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem. {problem_statement} Remember to put your answer on its own line after \"Answer:\". A.2 Hyperparameters We summarize key hyperparameters in Table 4. Table 4: Key hyperparameters for 7B dense model and 30B MoE model experiments. Parameter trainer.nnodes trainer.n_gpus_per_node distributed strategy model.path data.train_batch_size (prompts) data.gen_batch_size (prompts) data.max_prompt_length data.max_response_length rollout.name rollout.n (group size) rollout.temperature rollout.top_p rollout.max_model_len val_kwargs.n (avg@k) val_kwargs.temperature val_kwargs.top_p actor.ppo_epochs actor.ppo_mini_batch_size (prompts) actor.clip_ratio_low / high (GRPO) actor.clip_ratio_low / high (GSPO) optim.lr optim.betas optim.weight_decay grad clip tensor_model_parallel_size pipeline_model_parallel_size expert_model_parallel_size 7B Dense 4 8 FSDP 30B MoE 8 8 Megatron Qwen/Qwen2.5-7B Qwen/Qwen3-30B-A3B-Base 512 512 2048 8192 vLLM 16 1.0 1.0 10240 32 1.0 0. 1 32 0.2 / 0.2 3e-4 / 4e-4 1e-6 [0.9, 0.999] 0.01 1.0 1 1 N/A 512 512 2048 20480 vLLM 16 1.0 1.0 22528 32 1.0 0.7 1 32 0.2 / 0.2 3e-4 / 4e-4 1e-6 [0.9, 0.999] 0.01 1.0 2 2 2https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k 3https://huggingface.co/Qwen/Qwen2.5-7B 4https://huggingface.co/Qwen/Qwen3-30B-A3B-Base 16 A.3 Implementation Details Our implementation is based on verl5 (Sheng et al., 2025). For GRPO and GSPO, we disable explicit KL penalty with respect to the base reference model, following the DAPO recipe (Yu et al., 2025). For each prompt x, we sample responses y1, . . . , yK πt( x) from the old policy πt( x) := πθt( x), and get rewards ri := r(x, yi) {1, +1} based on the correctness of answers. Let πθ be the trainable policy. We have log πθ(y x) = (cid:80)y Define the token probability ratio and geometric normalized sequence probability ratio as follows: j=1 log πθ(yj x, y<j). ρi,j(θ) := πθ(yi,j x, yi,<j) πt(yi,j x, yi,<j) , si(θ) := (cid:18) πθ(yi x) πt(yi x) (cid:19) 1 yi . Moreover, define the following advantages: (cid:98)Agrpo := ri mean(r1, . . . , rK) std(r1, . . . , rK) , (cid:98)Aloo := ri"
        },
        {
            "title": "1\nK − 1",
            "content": "(cid:88) j=i rj, (cid:98)Apart := ri τ log"
        },
        {
            "title": "1\nK − 1",
            "content": "(cid:88) j=i erj /τ . The GRPO (Shao et al., 2024) loss is defined as LGRPO(θ) = ExDEy1,...,yK πt(x) 1 (cid:88) i= 1 yi yi (cid:88) j=1 (cid:16) min ρi,j(θ) (cid:98)Agrpo , clip(ρi,j(θ), 1 ϵ, 1 + ϵ) (cid:98)Agrpo (cid:17) , where ϵ = 0.2 and we discard the KL penalty term. When the global batch size is set to be the same as the mini-batch size, GRPO reduces to the on-policy gradient with advantage estimator (cid:98)Agrpo yields similar performance, and thus we use the (length-normalized) RLOO (Ahmadian et al., 2024) loss for on-policy gradient experiments. . Empirically, we find that using (cid:98)Aloo LRLOO(θ) = ExDEy1,...,yK πt(x) (cid:34) 1 (cid:88) i=1 1 yi (cid:35) (cid:98)Aloo log πθ(yi x) . The GSPO (Zheng et al., 2025) loss is defined as follows: LGSPO(θ) = ExDEy1,...,yK πt(x) (cid:34) 1 K (cid:88) i=1 (cid:16) min si(θ) (cid:98)Agrpo , clip(si(θ), 1 ϵlow, 1 + ϵhigh) (cid:98)Agrpo (cid:35) (cid:17) , where ϵlow = 3 104 and ϵhigh = 4 104 as suggested. We implement PMD-MEAN (Team et al., 2025b) and PMD-PART using the following losses. Lmean(θ) = ExDEy1,...,yK πt(x) 1 (cid:88) i= τ yi (cid:32) log πθ(yi x) πt(yi x) (cid:98)Aloo τ Lpart(θ) = ExDEy1,...,yK πt(x) 1 (cid:88) i= τ yi (cid:32) log πθ(yi x) πt(yi x) (cid:98)Apart τ (cid:33)2 , (cid:33)2 . The factor τ in the loss ensures the gradient norm does not differ too much when tuning τ , and length normalization is consistent with the loss aggregation mode in other methods. A.4 Supplementary Results We provide supplementary experimental results in Figures 7 and 8. 5https://github.com/verl-project/verl 17 Figure 6: Target estimation error for positive and negative actions in PMD-MEAN and PMD-PART under τ = 0.05 and pt ranges from 0.01 to 0.2. Left: positive actions. Right: negative actions. The plot shows the average from 100 random seeds. The error in PMD-MEAN mainly comes from systematic mismatch between positive targets. Missing Proofs in Section 3 B.1 Proof of Theorem 3.1 Proof of Theorem 3.1. Define u(y) = log π(y) is written as πt(y) , then the Lagrangian of Lmean (in the policy space) L(u, λ) = 1 2 (cid:88) yY πt(y)(y τ u(y))2 + λ πt(y)eu(y) 1 , (cid:88) yY The KKT conditions yield (cid:40) τ πt(y)(y τ u(y)) + λπt(y)eu(y) = 0, Y, (cid:80) yY πt(y)eu(y) = 1. Assume πt(y) > 0 for all (which is true for LLMs without top-p/top-k constraints), then any stationary point of Lmean should satisfy τ (y τ u(y)) + λeu(y) = 0 τ (y τ u(y))eu(y) = λ (cid:19) u(y) τ u(y) = (cid:18) τ τ u(y) = u(y) = τ τ (cid:18) λ τ 2 (cid:18) λ τ 2 τ λ τ 2 (cid:19) τ (cid:19) . (27) The RHS of (27) is monotonically decreasing in λ, and Eπt[RHS] = 0 when λ = 0. Moreover, by Jensens inequality, 1 = Eyπt[eu(y)] eEyπt [u(y)] = Eyπt[u(y)] 0, thus there must be unique λ 0 (and hence u) such that the KKT conditions are satisfied. Moreover, the Hessian of Lagrangian in is positive definite under the assumption that πt(y) > 0, thus the point is indeed the minimizer of Lmean. To characterize λ, we invoke several properties of for all 0: (1) (z) is concave and monotonically increasing; (2) (z) (z) . Moreover, by the feasibility 1+z ; (3) eW (z) = Figure 7: Qwen2.5-7B training results for 15 epochs (495 global steps). Training reward, response length, and entropy are smoothed by EMA with an effective window size of 50. PMD-MEAN achieves superior performance not only in Pass@1 (measured in Avg@32) but also Pass@32 and Maj@32 (accuracy of majority voting answer). constraint, we have 1 = (cid:88) yY πt(y) = (cid:88) yY πt(y) (cid:17)(cid:17)(cid:17) (cid:16) τ (cid:17) (cid:16) τ exp (cid:16) λ (cid:16) (cid:16) τ τ 2 exp (cid:16) λ (cid:17) τ 2 exp (cid:17) (cid:16) τ λ τ 2 exp (cid:17)(cid:17) (cid:16) τ exp exp (cid:20) τ 2 exp For convenience, let := λ/τ 2 0, := Eπt[ey/τ ], := Eπt[e2y/τ ]. Then the target is to show = . τ 2 λ Eyπt (cid:18) λ (cid:19)(cid:19)(cid:21) (cid:18) τ A(A 1) log A. For the upper bound, by concavity and Jensens inequality, = Eπt[W (xey/τ )] (E[xey/τ ]) = (xA). Since is increasing, above inequality implies xex (xA)eW (xA) = xA = ex = log A, 19 Figure 8: Qwen3-30B-A3B-Base training results for 300 global steps. Training reward, response length, and entropy are smoothed by EMA with an effective window size of 20. thus proving the upper bound. On the other hand, by lower bound on , = Eπt[W (xey/τ )] Eπt (cid:21) (cid:20) xey/τ 1 + xey/τ (cid:0)Eπt[xey/τ ](cid:1)2 Eπt[xey/τ (1 + xey/τ )] = (xA)2 xA + x2B , where the second inequality is from Cauchy-Schwarz. Solving the inequality yields the lower bound. For binary rewards {0, 1} with = Eπt[r(y)], we have Eπt[2 y] = Var(r) = p(1 p) and = pW (xe(1p)/τ ) + (1 p)W (xep/τ ). (28) For large τ , we have (cid:34) = Eπt 1 + τ + (cid:34) = Eπt 1 + 2y τ + (cid:35) 2 2τ 2 + O(τ 3) 22 τ 2 + O(τ 3) = 1 + p(1 p) 2τ 2 + O(τ 3), (cid:35) = 1 + 2p(1 p) τ 2 + O(τ 3). (cid:19) (cid:0)1 + O(τ 2)(cid:1) (cid:0)1 + O(τ 2)(cid:1) λ τ 2 O(τ 1) λ. Thus, the lower bounds on = λ/τ 2 yield A(A 1) xB = (cid:18) p(1 p) 2τ 2 p(1 p) 2 On the other hand, the upper bound yields λ τ 2 log (cid:18) = = τ 2 log 1 + (cid:18) p(1 p) τ 2 p(1 p) (cid:19) 2τ 2 + O(τ 3) (cid:19) 2τ 2 + O(τ 3) + O(τ 1). 2 p(1 p) + O(τ 1) as τ . p(1 p) 2 Combine the two bounds, we have λ = 1 = For small τ 0, the bounds (6) are too loose. We define = λ Firstly, the Lambert-W function satisfies τ = τ and show that p(1 p). log log log (z) log for > e. Moreover, for 0, (z) = in τ 1, we have eW (z) z. Since ep/τ decays faster than any polynomial 0 (1 p)τ (xep/τ ) (1 p)τ xep/τ = (1 p)vep/τ 0. Therefore, the second term in (28) vanishes. For the remaining dominant term, let z1 = xe(1p)/τ = τ e(1p)/τ , then z1 when τ 0, and log z1 = 1p τ . In this case, our bound on the Lambert-W function gives τ + log and thus τ (z1) = (1 p) + o(1), = τ = τ (pW (z1) + o(1)) = p(1 p) + o(1) = λ = τ p(1 p)(1 + o(1)). B.2 Policy Ratio We formally state the policy ratios of PMD-MEAN and PMD-PART in Equations (8) to (11) in the following proposition. Proposition B.1 (Binary-reward ratios for small τ ). Assume r(y) {0, 1} and let = Eπt[r(y)] (0, 1). Consider the ratios ρ(y) := πt+1(y)/πt(y). PMD-MEAN. As τ 0, for any with r(y) = 1, and for any with r(y) = 0, ρmean + (y) = 1 1 ep/τ (1 + o(1)), ρmean (y) = ep/τ (1 + o(1)). PMD-PART. For any τ > 0, the partition update satisfies ρpart + (y) = 1 + (1 p)e1/τ , ρpart (y) = e1/τ + (1 p)e1/τ , and in particular as τ 0, ρpart + (y) = 1 1 p2 e1/τ + O(e2/τ ), ρpart (y) = 1 e1/τ + O(e2/τ ). 21 Proof of Proposition B.1. Throughout, let := λ/τ 2. For PMD-MEAN, start from the Lambert-W form (5) and use eW (z) = (z)/z to rewrite the ratio as πmean t+1 (y) πt(y) = exp (cid:16) τ (xey/τ ) (cid:17) = 1 (cid:0)xey/τ (cid:1). (29) In the binary case, = 1 when = 1 and = when = 0, so defining ρmean + := 1 (cid:16) xe(1p)/τ (cid:17) , ρmean := xep/τ (cid:17) (cid:16) , 1 we have the normalization identity (cid:88) 1 = t+1 (y) = pρmean πmean + + (1 p)ρmean . (30) By (7), λ τ p(1 p) as τ 0, hence = λ τ 2 p(1 p) τ = Θ (cid:19) . (cid:18) 1 τ Therefore xep/τ 0 as τ 0. Using the Taylor expansion (z) = + O(z2) as 0, we obtain and plugging into (29) yields (xep/τ ) = xep/τ (1 + o(1)), ρmean = 1 (xep/τ ) = ep/τ (1 + o(1)). For ρmean + , substituting the above into (30) gives ρmean + = 1 (1 p)ρmean = 1 1 ep/τ (1 + o(1)). This proves the PMD-MEAN claims. For PMD-PART, the update is explicit: πpart t+1 (y) = πt(y)er(y)/τ pe1/τ + (1 p) . Hence ρpart + = e1/τ pe1/τ + (1 p) = 1 + (1 p)e1/τ , ρpart = 1 pe1/τ + (1 p) = e1/τ + (1 p)e1/τ . Expanding (1 + u)1 = 1 + O(u2) with = 1p e1/τ yields ρpart + = 1 1 p2 e1/τ + O(e2/τ ), ρpart = 1 e1/τ + O(e2/τ ). B.3 Proof of Proposition 3. Proof of Proposition 3.2. Fix state and omit x. Let u(y) := log π(y) (cid:80) π(y) = 1 is equivalent to the single equality constraint (cid:2)eu(y)(cid:3) = 1. Eyπt Hence the mixed subproblem (12) is equivalent to max : Eπt [eu]=1 (cid:104) eur τ euu Eπt (eu 1)2(cid:105) . λ 2τ 22 πt(y) . Then the simplex constraint (31) Introduce Lagrange multiplier ν for the constraint (31) and define L(u, ν) := Eπt (cid:104) eur τ euu (eu 1)2(cid:105) + ν(cid:0)Eπt[eu] 1(cid:1). λ 2τ Stationarity w.r.t. u(y) gives, for all y, 0 = πt(y)eu(y)(cid:16) r(y) τ (u(y) + 1) (eu(y) 1) + ν (cid:17) . λ τ Dividing πt(y)eu(y) > 0 and rearranging the terms give u(y) r(y) τ + λ τ 2 eu(y) = c, := ν + λ/τ τ τ , (32) where is constant independent of y. Finally, adding constant baseline to does not change the optimizer over the simplex, thus by choosing := τ and writing := r(y) Eπt[r] (which differs from by constant), we may rewrite (32) equivalently as u(y) τ + λ τ 2 eu(y) = 0, Eπt [eu(y)] = 1. (33) These are exactly the same KKT conditions obtained for the PMD-MEAN population objective in Section B.1. Therefore the PMD-MEAN solution πt+1 also solves the mixed subproblem (12) with the same λ. Missing Proofs in Section 4.1 We first state simple self-bounding lemma from Bernsteins inequality. Lemma C.1 (Bernstein with self-bounding variance). Let Z1, . . . , Zn be i.i.d. random variables such that E[Zi] = µ 0, Zi R, and E[Z 2 ] vµ for some > 0. Then for any δ (0, 1), with probability at least 1 δ, µ 2 1 (cid:88) i=1 Zi + (2v + 4 3 R) log(1/δ) . (34) Proof. By Bernsteins inequality for bounded variables, with probability at least 1 δ, µ (cid:114) Zi + 1 n (cid:88) i=1 2Var(Zi) log(1/δ) + 2R log(1/δ) 3n . Using Var(Zi) E[Z 2 ] vµ yields µ (cid:114) Zi + 1 n (cid:88) i=1 2vµ log(1/δ) + 2R log(1/δ) 3n . Apply ab 1 2 + 1 2 with = µ and = 2v log(1/δ) : (cid:114) 2vµ log(1/δ) µ 2 + log(1/δ) . Substitute and rearrange to obtain (34). C.1 Proof of Lemma 4.5 Proof of Lemma 4.5. Fix iteration t. For each π Π, define the residual fπ(y) := sπ(y) s(y). 23 By Assumption 4.1, = sπ Y, sπ(y) [B, B+], t+1 for some π t+1 Π. Hence by Assumption 4.3, for all π Π and"
        },
        {
            "title": "Define the clean empirical loss",
            "content": "fπ(y) + B+ =: M. (cid:98)Lclean (π) := 1 2n (cid:88) i= fπ(yi)2 = 1 (cid:88) i=1 Xi(π), Xi(π) := 1 fπ(yi)2. Then 0 Xi(π) 1 union bound over π Π, we get that with probability at least 1 δ, for all π Π, 2 2 and E[Xi(π)] = Lt(π). Applying Lemma C.1 with = = 1 2 2 and Lt(π) 2 (cid:98)Lclean (π) + 5M 2 log(Π /δ) 3n < 2 (cid:98)Lclean (π) + 5M 2 log(2 Π /δ) 3n . (35) Next, relate (cid:98)Lclean ((cid:98)πt+1) to the target mismatch 2. For each i, write = (cid:101)s i(yi) = (cid:98)πt+1(yi) i. (cid:98)πt+1(yi) (cid:101)s i(yi) s(yi) so that Using Assumptions 4.1 and 4.2, we get (cid:98)Lt((cid:98)πt+1) inf πΠ (cid:98)Lt(π) + ϵopt (cid:98)Lt(π t+1) + ϵopt = 1 2n (cid:88) i=1 2 + ϵopt = 1 2 2 + ϵopt. On the other hand, the pointwise inequality (u v)2 1 2 u2 v2 implies (cid:98)Lt((cid:98)πt+1) = 1 2n (cid:88) i=1 (cid:0)f (cid:98)πt+1(yi) (cid:1)2 1 4n (cid:88) i=1 (cid:98)πt+1(yi)2 1 2n (cid:88) i=1 2 = 1 2 (cid:98)Lclean ((cid:98)πt+1) 1 2 2. Combining the last two displays yields (cid:98)Lclean ((cid:98)πt+1) 22 + 2ϵopt. Finally, apply (35) at π = (cid:98)πt+1 and substitute the above bound, we get Lt((cid:98)πt+1) 2 (cid:98)Lclean (π) + 5M 2 log(2 Π /δ) 3n 42 + 4ϵopt + 5M 2 log(2 Π /δ) 3n , which proves (20) as 2B. C.2 Proof of Theorem 4.6 Proof of Theorem 4.6. Since [0, 1], for any two policies p, we have J(p) J(q) TV(p, q). Moreover, TV(πt+1, π t+1) = Eyπt (cid:104)(cid:12) (cid:12) (cid:105) (cid:12)esπt+1 (y) es(y)(cid:12) max(cid:8)esπt+1 (y), es(y)(cid:9) (cid:12) (cid:12) (cid:12) (cid:104) (cid:105) (cid:12)sπt+1(y) s(y)(cid:12) (cid:12) (cid:113) (cid:2)e2s(y)(cid:3)(cid:17) Eπt (cid:2)e2sπt+1 (y)(cid:3) + (cid:113) Eπt (cid:113) (cid:16)(cid:112)1 + χ2(πt+1 πt) + 1 + χ2(π t+1 πt) Eπt (cid:17) (cid:2)(sπt+1(y) s(y))2(cid:3) (cid:112)2Lt(πt+1), Eyπt (cid:16)(cid:113) 1 2 1 2 1 2 1 2 (cid:12)ea eb(cid:12) = where we used (cid:12) (cid:12) max{ea, eb} b and CauchySchwarz. Since sπ B+ and Eπt[esπ ] = 1, we have Eπt[e2sπ ] eB+, hence the prefactor is at most eB+/2 up to constants. Combining the assumption on the ideal convergence rate (21) and the bound on Lt(πt+1) in Lemma 4.5, we obtain the result (22). 24 C.3 Proof of Proposition 4.7 Proof of Proposition 4.7. By (10) in Section 3.1, for r(y) = 0, πmean t+1 (y) πt(y) = exp (cid:16) pt τ (cid:17) (cid:0)1 + o(1)(cid:1). Therefore, the total probability mass on the negative set contracts as 1 t+1 = Eyπt (cid:20) π t+1(y) πt(y) (cid:16) (cid:21) 1{r(y) = 0} (cid:17) (cid:0)1 + o(1)(cid:1), pt τ = (1 pt) exp which yields (23). C.4 Proof of Proposition 4.8 Proof of Proposition 4.8. Under (2), the total probability mass on = 1 is t+1 = pte1/τ pte1/τ + (1 pt) , which implies 1 t+1 = 1pt pte1/τ +(1pt) . This is exactly (21) with (24). C.5 Proof of Proposition 4.11 Lemma C.2 (LOO mean concentration). Let U1, . . . , Un be i.i.d. Bernoulli random variables with mean and define pi = 1 n1 j=i Uj. Then for any δ (0, 1), with probability at least 1 δ, (cid:80) max i[n] pi (cid:114) 2p(1 p) log(4n/δ) 1 + 2 log(4n/δ) 3(n 1) . Proof. For any i, by Bernsteins inequality for (cid:80) j=i(Uj p), with probability at least 1 δ/(2n), (cid:115) pi 2p(1 p) log( 2n δ ) 1 + 2 log( 2n δ ) 3(n 1) . Applying union bound over [n] and both signs gives the stated bound. Proof of Proposition 4.11. Applying Lemma C.2 to Ui = r(yi), we obtain maxi pi pt εn(pt, δ) with probability at least 1 δ. PMD-PART. For PMD-PART, write = e1/τ 1 and Zt = 1 + apt. Since r(yi) {0, 1}, we have = log(1 + apt) log(1 + api) = 1 + aξi (pt pi) for some ξi between pt and pi by the mean value theorem. Therefore, 1 + a(pt pi pt)+ pi pt . Meanwhile, there is always τ log(1 + ap) [0, 1], and thus 1 In the event that τ . maxi[n] pi pt εn(pt, δ), combining the inequalities, squaring and averaging over yields (26). PMD-MEAN. Recall that (cid:101)s i(yi) is defined in (17) and s(y) = log πmean t+1 (y) πt(y) . By (5), we have s(y) = r(y) pt τ (cid:16) λ τ 2 exp (cid:16) r(y) pt τ (cid:17)(cid:17) . Therefore, for each [n], = (cid:101)s i(yi) s(yi) = pt pi τ + (cid:16) λ τ 2 exp (cid:16) r(yi) pt τ (cid:17)(cid:17) . Using (a + b)2 2a2 + 2b2 and averaging over gives 2 2 maxi pi pt2 τ 2 + 2 (cid:88) i=1 (cid:16) λ τ 2 exp (cid:16) r(yi) pt τ (cid:17)(cid:17)2 . On the event maxi pi pt εn(pt, δ), the first term is bounded by 2εn(pt,δ)2 term, since r(yi) {0, 1} it takes only two values: τ 2 . For the second Writing (cid:98)pt := (cid:80)n w+ := (cid:16) λ τ 2 exp i=1 r(yi), we have 1 (cid:16) λ τ 2 exp (cid:88) i=1 (cid:16) 1 pt τ (cid:17)(cid:17) , := (cid:16) (cid:16) λ τ 2 exp pt τ (cid:17)(cid:17) . (cid:16) r(yi) pt τ (cid:17)(cid:17) = (cid:98)ptw2 + + (1 (cid:98)pt)w2 . By the asymptotics used in the proof of Theorem 3.1 in Section B.1, as τ 0 with fixed pt (0, 1) we have τ w+ = (1 pt) + o(1) and τ = o(1), and hence for sufficiently small τ , (1 pt)2 τ 2 Since maxi pi pt εn(pt, δ), we have (cid:16) 1 τ 2 , w2 = w2 + (cid:17) . (cid:98)pt pt = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) (cid:12) (cid:12) (cid:12) (pi pt) (cid:12) (cid:12) 1 (cid:88) i=1 pi pt εn(pt, δ). i=1 Combining the above bounds yields 1 (cid:88) i=1 (cid:16) λ τ 2 exp (cid:16) r(yi) pt τ (cid:17)(cid:17)2 pt(1 pt)2 τ 2 (1 + εn(pt, δ)) . Substituting back proves (25). Refined Analysis for PMD-MEAN To refine the analysis of PMD-MEAN, we first connect the population squared loss Lt in (15) with ideal target s(y) = log πmean t+1 (y) πt(y) with the population objective of PMD-MEAN in (4). Lemma D.1 (Connection of losses for PMD-MEAN). Fix global step and write := r(y) Eyπt[r(y)]. Define the PMD-MEAN population objective, i.e., bandit specialization of (4): Lmean t+1 be the ideal PMD-MEAN update and = sπ Eyπt (π) := Let π 1 2 (cid:104)(cid:16) sπ(y) (cid:17)2(cid:105) . τ . Then for any π Π, Lmean (π) Lmean (π t+1) = Lt(π) + where λ 0 is the KKT dual multiplier in (33). t+1 λ τ 2 KL(π t+1 π) Lt(π), (36) (37) Using Lemma D.1, we can refine the ERM analysis for PMD-MEAN and eliminate the error floor in target estimation error. Lemma D.2 (Refined ERM for PMD-MEAN). Suppose Assumptions 4.1 to 4.4 hold, r(y) {0, 1} and define pt := Eyπt[r(y)]. Let εn(pt, δ) be as in Proposition 4.11. Then for any δ (0, 1), with probability at least 1 δ, Lt((cid:98)πt+1) (B + 1 τ )2 log(Π /δ) + ϵopt + (cid:16) εn(pt, δ) τ + (cid:17) pt τ + εn(pt, δ)2 τ 2 . (38) By substituting = pt τ for PMD-MEAN, we obtain Lemma 4.12 in Section 4. D.1 Proof of Lemma D.1 Proof of Lemma D.1. Denote g(y) := y/τ for brevity. For any π Π, we have"
        },
        {
            "title": "Lmean\nt",
            "content": "(π) Lmean (π t+1) = Eπt (cid:2)(sπ g)2 (s g)2(cid:3) 1 2 1 2 = Eπt = Lt(π) + Eπt (cid:2)(sπ s)2(cid:3) + Eπt (cid:2)(sπ s)(s g)(cid:3) (cid:2)(sπ s)(s g)(cid:3). By the KKT conditions (33) with = s, Combining the identities and es(y) = π s(y) g(y) = λ τ 2 es(y). t+1(y)/πt(y), we get Eπt (cid:2)(sπ s)(s g)(cid:3) = = λ τ 2 λ τ 2 Eπt (cid:2)(sπ s)es (cid:3) π(y) π t+1(y) (cid:104) log t+ (cid:105) Eyπ = λ τ 2 KL(π t+1 π) 0, which proves (37). D.2 Proof of Lemma D.2 Proof of Lemma D.2. Recall the PMD-MEAN leave-one-out target (17): (cid:101)s i(yi) = 1 τ (cid:0)r(yi) pi (cid:1), pi := 1 1 (cid:88) j=i r(yj). Also recall pt = Eyπt[r(y)] and yi = r(yi) pt. We first decompose the empirical targets as (cid:101)s i(yi) = yi τ + loo , loo := pt pi τ . Define the empirical loss with the population baseline target: (cid:98)Lmean (π) := 1 2n (cid:88) i=1 (cid:16) sπ(yi) (cid:17)2 . yi τ By (39), for each π Π, (cid:98)Lt(π) (cid:98)Lmean (π) = = 1 2n 1 2n (cid:88) (cid:104)(cid:0)ai loo (cid:105) (cid:1)2 a2 i=1 (cid:104) (cid:88) i=1 (loo )2 2ai loo (cid:105) , where ai := sπ(yi) yi τ . Thus we have (cid:12) (cid:12) (cid:98)Lt(π) (cid:98)Lmean (cid:12) (cid:12) (cid:12) (π) (cid:12) 1 (cid:88) i=1 ai (cid:12) (cid:12)loo (cid:12) (cid:12) + 1 2n (cid:88) (loo )2. i=1 (39) (40) (41) Let be the event from Proposition 4.11 that maxi pi pt εn(pt, δ/2). On we have (cid:12) (cid:12)loo (cid:12) (cid:12) εn(pt, δ/2)/τ , hence (cid:12) (cid:12) (cid:98)Lt(π) (cid:98)Lmean (cid:12) (cid:12) (cid:12) (cid:12) (π) sup πΠ εn τ sup πΠ 1 (cid:88) i=1 ai + ε2 2τ 2 , (42) 27 where we write εn := εn(pt, δ/2). We now bound the 1 since r(yi) {0, 1}, (cid:80)n i=1 ai term. By Assumption 4.3, sπ(y) for all π Π, Y. Also, yi = r(yi) pt = (cid:26) pt, 1 pt, r(yi) = 0, r(yi) = 1. Thus for any π and any i, (cid:12) (cid:12) sπ(yi) (cid:12) (cid:12) ai = yi τ (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "Averaging over i yields",
            "content": "sπ(yi) + yi τ + r(yi) pt τ . 1 n (cid:88) i=1 ai + 1 τ 1 n (cid:88) i=1 r(yi) pt . (43) Let := 1 (cid:80)n i=1 r(yi). For binary rewards, 1 (cid:88) i=1 Moreover, for any fixed i, hence r(yi) pt = pt + (1 2pt)r 2pt + pt . = (n 1)pi + r(yi) , pt 1 pi pt + 1 r(yi) pt max pj pt + 1 . On this gives pt εn + 1 , and therefore 1 (cid:88) i= r(yi) pt 2pt + εn + 1 . Combining (43) and (44) and substituting into (42) yields (cid:12) (cid:12) (cid:98)Lt(π) (cid:98)Lmean (cid:12) (cid:12) (cid:12) (π) (cid:12) sup πΠ (cid:16) (cid:16) εn τ εn τ + + pt τ pt τ + (cid:17) + + εn τ ε2 τ 2 . (cid:17) 1 nτ We now bound the population excess risk of PMD-MEAN. Recall that (44) (45) Lmean (π) := (cid:98)Lmean (π) := Eyπt (cid:88) (cid:16) 1 2 1 2n i=1 (cid:104)(cid:16) sπ(y) (cid:17)2(cid:105) , τ yi τ (cid:17)2 . sπ(yi) For π Π, define the pointwise loss ℓπ(y) := (cid:16) 1 2 sπ(y) (cid:17)2 , τ ℓ(y) := (cid:16) s(y) 1 2 (cid:17)2 . τ Let Zi(π) := ℓπ(yi) ℓ(yi), µ(π) := Eπt[Zi(π)] = Lmean (π) Lmean (π t+1), By Assumption 4.3 and 1, we have (cid:98)µ(π) := 1 (cid:88) i=1 Zi(π). 0 ℓπ(y) 1 2 2 τ , where Mτ := + 1 τ , and hence Zi(π) 1 2 2 τ . Moreover, 2 (π) = 1 4 (s(y) sπ(y))2 (cid:18) s(y) + sπ(y) (cid:19)2 2y τ (s(y) sπ(y))2M 2 τ , thus we have E[Z 2 (π)] 2 τ = 2M 2 2M 2 E[(sπ s)2] τ Lt(π) τ µ(π), where the last inequality comes from Lemma D.1. Apply Lemma C.1 with = 2M 2 then with probability at least 1 δ, for fixed π, τ and = 1 2 2 τ , µ(π) 2(cid:98)µ(π) + (cid:18) 2 τ log(1/δ) (cid:19) . union bound over Π with δ = δ 2Π yields that with probability at least 1 δ 2 , for all π Π, Lt(π) Lmean (π) Lmean (π t+1) (cid:98)Lmean (π) (cid:98)Lmean (π t+1) + 2 τ log(Π /δ) . (46) We now bound the empirical excess risk. (cid:98)Lmean = (cid:98)Lmean ((cid:98)πt+1) (cid:98)Lmean t+1) ((cid:98)πt+1) (cid:98)Lt((cid:98)πt+1) + (cid:98)Lt((cid:98)πt+1) (cid:98)Lt(π (π (cid:124) (cid:123)(cid:122) ϵopt t+1) (cid:125) + (cid:98)Lt(π t+1) (cid:98)Lmean (π t+1) ϵopt + 2 sup πΠ (cid:16) ϵopt + εn τ (cid:12) (cid:12) (cid:98)Lt(π) (cid:98)Lmean (cid:12) (cid:17) pt τ + ε2 τ 2 , + (cid:12) (cid:12) (π) (cid:12) where the first inequality uses Assumptions 4.1 and 4.2 and the second inequality uses (45). Combining the above inequality with (46), we get Lt(π) 2 τ log(Π /δ) + ϵopt + (cid:16) εn τ + (cid:17) pt τ + ε2 τ 2 , which is (38)."
        }
    ],
    "affiliations": [
        "Amazon",
        "Georgia Institute of Technology"
    ]
}