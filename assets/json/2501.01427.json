{
    "paper_title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
    "authors": [
        "Yuanpeng Tu",
        "Hao Luo",
        "Xi Chen",
        "Sihui Ji",
        "Xiang Bai",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning."
        },
        {
            "title": "Start",
            "content": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control Yuanpeng Tu1,2 Hao Luo2,3 Xi Chen1 1The University of Hong Kong 2DAMO Academy, Alibaba Group Sihui Ji1 Xiang Bai4 Hengshuang Zhao1, 4HUST 3Hupan Lab https://videoanydoor.github.io 5 2 0 2 2 ] . [ 1 7 2 4 1 0 . 1 0 5 2 : r Figure 1. Demonstrations for video object insertion. VideoAnydoor preserves the fine-grained object details and enables users to control the motion with boxes or point trajectories. Based on the robust insertion, users could further add multiple objects iteratively or swap objects in the same video. Compared with the previous works, VideoAnydoor demonstrates significant superiority."
        },
        {
            "title": "Abstract",
            "content": "Despite significant advancements in video generation, inserting given object into videos remains challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motion at the same time. In this paper, we propose VideoAnydoor, zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from text-to-video model, we utilize an ID extractor to inject the global identity and leverage box sequence to control the overall motion. To pre- *Work during DAMO Academy internship. Corresponding author. serve the detailed appearance and meanwhile support finegrained motion control, we design pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating In addition, we propose trainthe motion trajectories. ing strategy involving both videos and static images with reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., video face swapping, video virtual tryon, multi-region editing) without task-specific fine-tuning. 1 1. Introduction The booming of diffusion models [19, 30, 35] has spurred significant advancements in text-to-video generation [1, 12, 46] and editing [13, 21, 36, 45]. Some works [14, 37, 38, 52] learn to edit the video based on posture [14, 52] or styles [20] while other works [10, 17] explore modifying specific objects based on text descriptions. In this paper, we investigate video object insertion, which means seamlessly placing specific object (with reference image) into given video with the desired motion and location. This ability has broad potential for real-world applications, like video composition, video virtual try-on, video face changing, etc. Although strongly in need, this topic remains underexplored by existing works. We analyze that the challenge of video object insertion mainly lies in two folds: accurate ID preservation and precise motion control. Recently, some works have made initial attempts in this field. AnyV2V [17] and ReVideo [24] leverage image composition model [4] to insert the object in the first frame. Then, they propagate this modification to subsequent frames are under the guidance of text or trajectory control. However, this two-stage scheme may lead to suboptimal results if the first frame insertion is not satisfactory. Besides, as they do not inject ID information in the following frames, the objects identity and motion tend to collapse in the later frames. Faced with this challenge, we attempt to accurately preserve the objects identity and precisely control the objects motion throughout the whole video. Specifically, we propose an end-to-end framework termed VideoAnydoor. Starting from text-to-video diffusion model, the concatenation of random noise, object masks, and the masked video is utilized as input. Meanwhile, the reference image with no background is fed into the ID extractor to extract compact and discriminative ID tokens. Then these ID tokens are injected into the diffusion model together with the box sequence as coarse guidance of identity and motion to generate the desired composition. Additionally, pixel warper module is designed for joint modeling of the fine-grained appearance and precise motion. It takes the reference image with arbitrary key-points and the corresponding keypoint trajectories to warp the pixel details according to the desired motion. To further enhance motion alignment with the given condition, re-weighted reconstruction loss is introduced. Moreover, to address the scarcity of high-quality videos, we augment high-quality image data as videos to improve the alignment of appearance details. As shown in Fig. 1, with these techniques, users can edit specific regions in the video by providing target images and drawing boxes and trajectory lines. It should be noted that our inserted object is not constrained by shape or appearance, demonstrating robustness and generality in diverse scenarios. Our contributions can be summarized as follows: We construct the first end-to-end video object insertion framework that supports both motion and content editing. Our framework seamlessly supports diverse applications, e.g., multi-region editing, video virtual try-on, and video face changing, etc. We propose pixel warper to warp the pixel details according to the desired motion. It takes the reference image with arbitrary key-points and the trajectories as inputs for fine-grained modeling of identity and motion, enabling accurate ID preservation and motion control. We design multiple strategies to further enhance the capability of accurate insertion, including image-video mix training, training trajectory filtering, reweight reconstruction losses. Extensive experiments demonstrate their effectiveness in precise ID preservation and motion control. 2. Related Work Image-level object insertion. Generative object compositing [4, 32, 33, 42, 42, 49] focus on implanting subjects in diverse contexts. Among these methods, Paintby-Example [42] proposes an information bottleneck to avoid the trivial solution. CustomNet [48] incorporates 3D novel view synthesis capabilities. IMPRINT [33] decouples learning of identity preservation from that of compositing. AnyDoor [4] utilizes frequency-aware detail extractor to obtain detail maps. However, directly transferring similar insertion schemes as these to videos may result in imperfect performance as they fail to preserve fine-grained appearance details, while the quality of object insertion in videos is crucial for precise motion control. Nevertheless, these methods generally fail to insert objects with proper postures for motion control. Thus, to address these two issues, we conduct detailed investigation on object insertion with accurate ID preservation and proper posture control. Video editing. Early methods [2, 9, 26, 39] primarily adopt training-free or one-shot tuning schemes owing to the lack of proper training data. For example, Pix2Video [2] first edits the first frame and then produces followed frames with cross-frame attention. Recently, tuning-based methods [10, 17, 24, 40] have exhibited better results. Among them, text-prompt based schemes struggle to locate target regions. AnyV2V [17] uses an off-the-shelf image editing model to modify the first frame. Image-prompt based methods like ReVideo [24] design three-stage training scheme that decouples content and motion control. VideoSwap [10] uses semantic points to achieve video subject replacement. However, these methods either require extra fine-tuning, fail to keep the unedited region unchanged, or achieve poor motion/identity consistency with two-stage scheme. To address these issues, we aim to design an end-to-end zeroshot video insertion framework that precisely modifies both content and motion according to user-provided instructions while keeping the unedited content unchanged in zero-shot. 2 Figure 2. The pipelines of our VideoAnydoor. First, we input the concatenation of the original video, object masks, and masked video into the 3D U-Net. Meanwhile, the background-removed reference image is fed into the ID extractor, and the obtained features are injected into the 3D U-Net. In our pixel warper, the reference image marked with key points and the trajectories are utilized as inputs for the content and motion encoders. Then, the extracted embeddings are input into cross-attentions for further fusion. The fused results serve as the input of ControlNet, which extracts multi-scale features for fine-grained injection of motion and identity. The framework is trained with reweight reconstruction losses. We use blend of real videos and image-simulated videos for training to compensate for the data scarcity. 3. Method 3.1. Overview of Framework Task formulation. In this paper, we focus on high-fidelity video object insertion, with the goal of subject insertion with user-provided trajectories, where the unedited regions should remain the same as the source video. The primary challenge of this task lies in aligning the motion trajectory of the given one while preserving the identity of the target concept, particularly its appearance details. Overall pipeline. The VideoAnydoor pipeline is illustrated in Fig. 2. To reconstruct the background within the masked region, we build our method on 2D in-painting diffusion model. Specifically, following the latent diffusion model [31], we encode both the source video and the masked video with VAE encoder to obtain the latent space representations zori and zmask. The corresponding masks are 8 times down-sampled as the mask latent. Subsequently, DDIM inversion [5] is applied to transform the clean latent zori back to the noisy latent zT . Then we concatenate zT , zmask and zmask as 9-channel tensor for 3D U-Net. To utilize the priors of high-quality video generation, we integrate the motion layers [11] into the in-painting model as the 3D U-Net to ensure essential temporal consistency. For coarse-grained control, we leverage the powerful visual encoder DINOv2 [25] as the ID extractor for ID preservation and use the bounding boxes as motion guidance. Before feeding the reference image into the extractor, we remove its background with segmentor [16] and align the object to the image center to retain compact and ID-related representations. For fine-grained control, we adopt the interactionfriendly trajectory lines as the control signals and propose pixel warper to warp the pixel details according to the desired motion for joint modeling of appearance details and precise motion. Finally, we introduce reweight reconstruction loss to amplify the influence of key-points and design novel image-video mix-training strategy to address the scarcity of high-quality video data. For convenience, the trajectory map and correspondence reference image are denoted as cmot RN 3HW and cref key R3HW respectively. RN 3HW , RN 1HW and cref R3HW represent the original video, masks of the edited region and the reference image respectively. N, H, is the frame number, height, and width of the original video. The content and motion encoders are denoted as Ec and Em respectively. Inference configuration. For users, they only need to provide subject image, source video, and trajectory sequence. For the trajectory sequence, the users can directly use the trajectory of the object within the source video or just draw start box and an end box to flexibly generate edited videos precisely aligned with the given conditions. 3 Table 1. Statistics of datasets used for training our VideoAnydoor. quality particularly refers to the image resolution. Dataset Type # Samples Mask Quality Video Quality YouTubeVOS [44] Video YouTubeVIS [44] Video Video UVO [7] Video MOSE [6] Video VIPSeg [22] Video VSPW [23] Video SAM2 [28] Video Pexel Video MVImgNet [47] Video ViViD [8] Video CHDTF [53] Video CelebV-HQ [54] Image Pexel 4,453 2,883 10,337 2,149 3,110 3,536 51,000 6,000 219,188 9,700 362 35,666 95,000 High High High High High High High Medium High High High High Medium Low Low Low High High High High High High High High High High Then these features are added to the corresponding layers for fine-grained modeling of appearance details and precise motion. This procedure can be formulated as: yc = F(zt, t, cref ; Θ) + Z(F(zt + Z(fc), t, cref ; Θc)), (1) where yc denotes the new diffusion features. represents the function of zero-conv [51]. Θc and Θ are the parameters of the ControlNet and the diffusion model. Reweight reconstruction. To further enhance the finegrained modeling of identity and motion, we propose reweighted diffusion loss that differentiates the contributions of regions inside the bounding boxes and areas around the trajectories. Specifically, we amplify the contributions within bounding boxes and regions around trajectories to enhance subject and motion learning while preserving the original diffusion loss for regions outside these boxes. Denote the masks of the regions that cmot covers as Mmot, we perform 8 times down-sampling on Mmot and M, which and respectively. The reconstrucare denoted as Mmot tion loss can be formulated as: L(θ) = Ez,ϵ,c,t[(λL(M + mot) + (1 M)) ϵ ϵθ(zt, cref , cmot, crefkey, t)2 2], (2) where λL denotes the balancing loss weight. ϵθ, ϵ are the prediction of the 3D U-Net and the target. 3.3. Training Strategy Dataset preparation. The ideal training samples are video pairs for different objects in the same scene, which are hard to collect with existing datasets. As alternatives, we sample all the needed data from the same video. Specifically, for video, we pick video clip and frame that has the largest distance from the clip, which is assumed to contain the most dissimilar object from the one in the video clip. We take their masks for the foreground objects and remove the background for the select frame. Then we crop it around the mask as the target object. For the video clip, we generate the box sequence and remove the box region to get 4 Figure 3. Pipeline of trajectory generation for training data. We first perform NMS to filter out densely-distributed points and then select points with larger motion. The retained ones can be sparsely distributed in each part of the target and contain more motion information, thus inducing more precise control. 3.2. Pixel Warper Trajectory sampling. During training, it is essential to extract trajectories from videos to provide motion conditions. Previous works [43] show that the movement of objects can be controlled by general key-points. Thus, as shown in Fig. 3, we first input the first frame to X-Pose [43] to initialize the points for subsequent trajectory generation. For cases in which X-Pose fails to detect any key-points, we use grid to sparsify dense sampling points. We empirically find that points with larger motion are more helpful for trajectory control. However, these points are mostly densely distributed in certain regions, resulting in severe information redundancy. Therefore, to filter out the undesired points, we first perform non-maximum suppression (NMS) to filter out points that are densely distributed. Then we apply motion tracking on each point to obtain their path lengths, e.g., {l0, l1, ..., lNinit1}, where Ninit denotes the number of initial points. Then we retain points with the largest motion and use the corresponding trajectory map cmot as control signals. Different colors are assigned for points to represent different trajectories. Motion injection. naive implementation of motion injection is only training similar control module to inject the motion conditions as [10, 50]. However, such scheme may fail to accurately insert the objects with desired motion and appearance details, since it has no explicit semantic correspondence with the reference object. Thus the object may be inserted into the video with an undesired pose, leading to severe distortion in foreground regions. To address this issue, we input pair of trajectory maps cmot and correspondence reference image cref key as fine-grained guidance. As show in Fig. 2, cmot/cref key are first encoded by Ec/Em respectively. Then these two embeddings are input into two cross-attention modules respectively for semantic-aware fusion. Afterward, the fused two features are added and utilized as the input of ControlNet [50] to extract multi-scale intermediate features {f 0 }, where denotes the layer number of the diffusion model. , ..., c , 0 Figure 4. Comparison results between VideoAnydoor and existing state-of-the-art video editing works. Our VideoAnydoor can achieve superior performance on precise control of both motion and content. the scene video, where the unmasked video could be used as the training ground truth. Specifically, we use the expanded bounding box rather than the tightly-surrounded one in implementation. For boxes with small moving range, we use the union of the boxes as the final box to reduce the impact of the bounding box on the motion. The full data for training is shown in Tab. 1, which covers both videos from diverse domains and high-quality images to compensate for the scarcity of high-quality videos. Image-video mixed training. Different from previous works [3] that utilize high-quality images for two-stage disentangled training, we resort to employing them for joint training with videos. However, directly repeating images will impair the discriminative learning of temporal modules. Instead, we augment the images as videos with manual camera operation. Specifically, we either randomly translate the image at equal intervals from different directions, or gradually crop the original image at equal intervals to obtain an image sequence. Then the image sequence is processed with bilinear interpolation to enhance the video smoothness. Although the augmented videos would benefit the learning of appearance variation, the essential difference between them and real videos will potentially impair motion learning. Thus similar to [4], we adopt adaptive timestep sampling to enable different modalities of data to contribute to different stages of denoising training. 4. Experiments 4.1. Experimental Setup Implementation details. In this work, we choose Stable Diffusion XL with motion modules as the base generator. During training, we process the image resolution to 512512 and adopt Adam optimizer with an initial learning rate of 1e5. We use DDIM for 50-step sampling and classifier-free guidance with cfg of 10.0 for inference. The model is optimized for 120K iterations on 16 NVIDIA A100 GPUs with batch size of 32. We only use 8 points for trajectory generation of each sample. In actual use, these parameters can be adjusted by the user according to different subjects and the desired generation effect. Benchmarks. For comprehensive evaluation, we construct benchmark consisting of around 200 videos collected from Pexel, which includes ten different categories (e.g., 5 Figure 5. Demonstrations for precise motion control. VideoAnydoor can achieve precise alignment with the given trajectories and objects when using pair of reference images marked with key-points and corresponding trajectory maps as input. Table 2. Quantitative comparison between our VideoAnydoor and other related work. Six automatic metrics are employed for the performance evaluation of both content and motion. VideoAnydoor outperforms these methods across all the metrics. PSNR () CLIP-Score () DINO-Score () AJ () δvis ConsistI2V [29] I2VAdapter [41] AnyV2V [17] ReVideo [24] VideoAnydoor (ours) 25.1 24.3 30.1 33.5 37.7 64.7 67.1 70.2 74.2 81.2 40.6 42.2 47.2 51.7 58.9 49.3 51.2 54.1 79.2 88.0 avg () OA () 57.2 51.1 59.9 53.7 61.1 55.8 83.2 81.4 92.3 91. methods and it adopts semantic-unaware motion injection manner. AnyV2V [17] adopts similar two-stage generation scheme with text prompts. In Fig. 4, we can observe that AnyV2V [17] suffers from content distortions for both the edited and unedited areas. Moreover, it has poor motion consistency due to using texts as control signals. For ReVideo, there exists an obvious loss of edited content as well, especially for the cases with large motion. It exhibits inferior pose control over the inserted object owing to the lack of semantic information within motion signals. In comparison, our VideoAnydoor can effectively preserve the unedited content while allowing users to customize the motion in editing areas. We provide more examples in Fig. 5 and Fig. 6, where we insert the same object with different trajectories and different objects in diverse scenarios. 4.3. Quantitative Comparison ID preservation. We first conduct quantitative evaluation with CLIP-Score [27], DINO-Score [25], PSNR [34]. Previous approaches impose heavy reliance on the existing image customization methods to acquire the first frame, making them retain the distortions within the first frame for subsequent generation. Thus it can be observed in Tab. 2 that they generally achieve inferior results to our method. Moreover, since there is no explicit condition for AnyV2V [17], ConsistI2V [29], I2VAdapter [41] to keep the unedited regions unchanged, these methods perform much worse than our method and ReVideo on PSNR. Overall, our method achieves clear advantage over the compared methods across all the metrics. Figure 6. More visual examples of VideoAnydoor. It preserves fine-grained details (e.g., logos on the car) and achieves smooth motion control (e.g., the tail of the cat) with our pixel warper. persons, dogs). We also make qualitative analysis on the ViViD [8] and CHDTF [53] test set to evaluate the performance for virtual video try-on and video face swapping. Evaluation metrics. On our constructed benchmarks, for evaluation of ID preservation, we calculate both CLIPScore [27] and DINO-Score [25] to reflect the similarity between the edited region and the target subject, where PSNR [34] is employed to measure the reconstruction quality of unedited regions as well. Additionally, for customized concepts, we follow Custom Diffusion [18] to compute pairwise image alignment between each edited frame and each reference concept image. In addition, we feed the edited videos to Cotracker [15] to calculate the tracking metrics [15] with the points in original videos as ground truth labels. Finally, we organize user studies with group of 15 annotators to rate the edited results from the perspective of quality, fidelity, fluidity of movement, and diversity. 4.2. Qualitative Comparison Among existing methods, ReVideo [24] heavily relies on the first frame modified by existing image customization 6 Figure 7. Qualitative ablation studies on the core components of VideoAnydoor. When removing the pixel warper, it suffers from poor motion consistency due to the undesired posture. And it can be observed that all the components contribute to the best performance. Table 3. User study on the comparison between our VideoAnydoor and existing alternatives. Quality, Fidelity, Smooth, and Diversity measure synthesis quality, object identity preservation, motion consistency, and object local variation, respectively. Each metric is rated from 1 (worst) to 4 (best). Quality () Fidelity () Smooth () Diversity () ConsistI2V [10] AnyV2V [17] ReVideo [24] VideoAnydoor (ours) 1.80 1.90 2.65 3.75 1.75 1.85 2.55 3.80 2.30 1.50 2.50 3.65 1.50 2.10 2.25 3.70 Table 4. Quantitative evaluation of core components in VideoAnydoor on ID preservation. denotes removing the semantic points in the key-point image. PSNR () CLIP-Score () DINO-Score () Only Real-video Data Only Static-image Data FrozenDINOv2 w/o PixelWarper w/o Pixel Warper w/o Re-weighted Loss Ours-full 34.4 34.1 33.2 35.1 33.6 35.1 37.7 76.4 76.2 74.5 77.0 72.1 77.0 81.1 52.0 51.2 51.4 53.1 48.1 53.1 58.9 Motion consistency. We conduct further quantitative experiments with the metric in tracking tasks to evaluate the motion alignment. Specifically, we track the key-points in the first frame of the original video in the edited one with the Cotracker model [15] and adopt the original trajectories as ground truths. The results are summarized in Tab. 2. Due to the lack of explicit motion control, AnyV2V, I2VAdapter, and ConsistI2V usually generate static or distorted motion for the edited content. Compared with them, VideoAnydoor demonstrates the best performance. Besides these results, we provide evaluation from aesthetic and technical views in the Appendix as well. User study. We organize user study to compare ConsistI2V [29], ReVideo [24], AnyV2V [17], and our VideoAnydoor. Specifically, we let 20 annotators rate 20 groups of videos, where each group contains the original video and four edited videos. For each group, we provide one image edited by AnyDoor [4] as the first frame for the Table 5. Quantitative evaluation of core components in VideoAnydoor on motion consistency. denotes removing the semantic points in the key-point image. Only Real-video Data Only Static-image Data FrozenDINOv2 w/o PixelWarper w/o PixelWarper w/o Re-weighted Loss Ours-full AJ () 66.1 72.3 80.1 81.3 78.3 75.4 88.0 δvis avg () 67.0 72.6 82.2 82.2 81.7 84.2 91. OA () 69.6 75.1 85.1 85.0 84.0 85.1 92.3 compared methods. Besides, we provide detailed regulations to rate the generated videos for scores of 1 to 4 from four views: Quality, Smooth, Fidelity, Diversity. Fidelity measures ID preservation, and Quality counts for whether the result is harmonized without considering fidelity. Smooth assesses the motion consistency. We use Diversity to measure the differences among the synthesized results. The user-study results are shown in Tab. 3. It can be noted that our model demonstrates significant superiority, especially for Fidelity, and Smooth. Such results fully verify the effectiveness of our method. 4.4. Ablation Studies ID preservation. We first conduct an investigation of the core components on ID preservation. From Tab. 4, we can observe that training with fixed DINOv2 induces much inferior performance. Similarly, training only on videos suffers from severe accuracy degradation across all the metrics. Moreover, our pixel warper can effectively help inject the appearance details according to the motion. The results also show that our reweight reconstruction loss can bring performance boost to the baseline by making the model focus on the foreground regions. The best performance can be achieved by training with all the modules. Motion consistency. We present evaluation outcomes of the motion control for core components in Tab. 5. Results show that training with static images causes significant ac7 Figure 8. More applications of VideoAnydoor. Our framework seamlessly supports various tasks like video virtual try-on, talk head generation, multi-region editing, etc. The results show that VideoAnydoor could effectively preserve the structure and identity and impose precise control on movements of multiple objects in diverse scenarios. Table 6. Detailed quantitative evaluation of the pixel warper in VideoAnydoor on motion consistency. Tight box denotes training with tightly-surrounded boxes. Random X-Pose points Grid points w/o NMS Tight box Ours-full AJ () 80.4 82.6 82.3 83.2 88. δvis avg () 82.2 83.7 83.1 85.4 91.1 OA () 82.8 85.2 84.6 86.1 92.3 curacy drop due to impaired temporal module learning and inferior motion consistency when only using key-point trajectories in the pixel warper. Training with image-simulated videos aids precise motion control, likely due to facilitating fine-grained appearance reconstruction and making interframe key-point correspondence easier. Beyond this, we conduct more comparisons with different variants of pixel warper in Tab. 6. It can be observed that selecting points with larger motion gives significant performance boost. Moreover, using all grid-sampled points is inferior to extracted key-points. Selecting samples without considering distance leads to inferior performance as points are densely distributed in certain regions as well. Training with looselysurrounded boxes leads to precise motion control for keypoint trajectories. Qualitative comparisons in Fig. 7 show similar phenomenon to quantitative results. All modules contribute to the best performance. 4.5. More Applications Virtual video try-on. As shown in Fig. 8, without extra task-specific tuning, our VideoAnydoor demonstrates satisfactory performance for virtual try-on on the ViViD dataset [8], where diverse patterns of the target clothes can be well preserved across different frames. Such results underscore the strong generalization abilities of our method. Video face swapping. Besides video try-on, we further apply our method to video face swapping, which requires more precise control of tiny movements and preservation of face identities. Specifically, we conduct evaluations on the CHDTF dataset [53] and use 16 points with the largest movement in the face landmarks as the initial key-points. As demonstrated in Fig. 8, VideoAnydoor could give satisfactory performance for this task as well. Multi-region editing. In addition, we extend our VideoAnydoor to multi-region editing. As shown in Fig. 8, we can achieve precise control of multi-object insertion for both motion and content. Besides, it can be used for object swapping and inserting logos or ornaments as well. As shown in Fig. 8, we can precisely place the hat on the head and achieve smooth posture control. 5. Conclusion In this paper, we present VideoAnydoor for end-to-end video object insertion with precise motion control. Specifically, it can effectively characterize the reference target with an ID extractor when trained with combination of videos and high-quality images. Moreover, it can achieve smooth motion consistency and effective preservation of appearance details through the proposed pixel warper. Our VideoAnydoor has promising performance on diverse precise video editing applications, e.g., (1) object insertion, (2) virtual video try-on, (3) video face swapping, and (4) multiregion editing. Extensive qualitative and quantitative experimental results demonstrate its superiority over previous methods. It provides universal solution for general regionto-region mapping tasks as well. Limitations. Despite impressive results, our method still struggles with complex logos. This issue might be solved by collecting related data or using stronger backbones. Acknowledgements. This work was supported by DAMO Academy via DAMO Academy Research Intern Program."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Report, 2024. 2 [2] Duygu Ceylan, Chun-Hao Huang, and Niloy J. Mitra. Pix2video: Video editing using image diffusion. In ICCV, 2023. 2 [3] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, 2024. 5 [4] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024. 2, 5, 7 [5] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [6] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for In ICCV, video object segmentation in complex scenes. 2023. 4 [7] Yuming Du, Wen Guo, Yang Xiao, and Vincent Lepetit. 1st place solution for the uvo challenge on video-based openworld segmentation 2021. arXiv:2110.11661, 2021. 4 [8] Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and ZhengJun Zha. Vivid: Video virtual try-on using diffusion models. arXiv: 2405.11794, 2024. 4, 6, 8 [9] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. 2 [10] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In CVPR, 2024. 2, 4, 7 [11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 3 [12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022. 2 [13] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zeroshot grounded video editing using text-to-image diffusion models. In ICLR, 2024. [14] Johanna, Aleksander Karras, Ting-Chun Holynski, Ira Wang, and Kemelmacher-Shlizerman. Dreampose: Fashion imageto-video synthesis via stable diffusion. arXiv:2304.06025, 2023. 2 [15] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv:2307.07635, 2023. 6, 7 [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 3 [17] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv:2403.14468, 2024. 2, 6, 7 [18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. 6 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. [20] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Xintao Wang, Yujiu Yang, and Ying Shan. Stylecrafter: Enhancing stylized text-to-video generation with style adapter. arXiv:2312.00330, 2023. 2 [21] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, 2024. 2 [22] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: benchmark. In CVPR, 2022. 4 [23] Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. Vspw: large-scale dataset for video scene parsing in the wild. In CVPR, 2021. 4 [24] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. arXiv:2405.13865, 2024. 2, 6, 7 [25] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. TMLR, 2023. 3, [26] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In CVPR, 2024. 2 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 6 [28] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv:2408.00714, 2024. 4 [29] Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. TMLR, 2024. 6, 7 9 EVA: zero-shot accurate attributes and multi-object video editing. arXiv: 2403.16111, 2024. 2 [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv:2408.06072, 2024. [47] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet: large-scale dataset of multi-view images. In CVPR, 2023. 4 [48] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, and Ying Shan. Customnet: Object customization with variable-viewpoints in text-to-image diffusion models. In ACMMM, 2024. 2 [49] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv.2308.10040, 2023. 2 [50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 4 [51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. ICCV, 2023. 4 [52] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv:2406.19680, 2024. [53] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with highresolution audio-visual dataset. In CVPR, 2021. 4, 6, 8 [54] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 4 [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 2 [31] Jaskirat Singh, Stephen Gould, and Liang Zheng. Highfidelity guided image synthesis with latent diffusion models. In CVPR, 2023. 3 [32] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, and Daniel G. Aliaga. Objectstitch: Generative object compositing. arXiv.2212.00932, 2022. 2 [33] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, IMPRINT: generative object comand Daniel G. Aliaga. positing by learning identity-preserving representation. In CVPR, 2024. 2 [34] Yule Sun, Ang Lu, and Lu Yu. Weighted-to-sphericallyuniform quality evaluation for omnidirectional video. Signal Processing Letters, 2017. [35] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 2 [36] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In CVPR, 2024. 2 [37] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In NeurIPS, 2024. 2 [38] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, 2024. 2 [39] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning In of image diffusion models for text-to-video generation. ICCV, 2023. 2 [40] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv:2408.13239, 2024. 2 [41] Xun, Mingwu Guo, Liang Zheng, Yuan Hou, Yufan Gao, Pengfei Deng, Di Wan, Yufan Zhang, Weiming Liu, Zhengjun Hu, Haibin Zha, Chongyang Huang, and Ma. I2v-adapter: general image-to-video adapter for diffusion models. In SIGGRAPH, 2024. 6 [42] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv:2211.13227, 2022. [43] Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Xpose: Detection any keypoints. In ECCV, 2024. 4 [44] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019. 4 [45] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "HUST",
        "Hupan Lab",
        "The University of Hong Kong"
    ]
}