{
    "paper_title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "authors": [
        "Leszek Sliwko",
        "Jolanta Mizeria-Pietraszko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration."
        },
        {
            "title": "Start",
            "content": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing Leszek Sliwko1, Jolanta Mizeria-Pietraszko2 1Standard Chartered Bank, EC2V 5DD London, U.K. 2Department of Computer Science, Opole University of Technology, Opole, Poland Corresponding author: Leszek Sliwko (e-mail: lsliwko@gmail.com). This work was supported in part by the Research AI & Experiment department, Standard Chartered Bank. ABSTRACT Cluster workload allocation often requires complex configurations, creating usability gap. This paper introduces semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs Large Language Model (LLM) integrated via Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. prototype featuring cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration. INDEX TERMS Artificial Intelligence, Kubernetes, Load Balancing, Semantic Parsing, Soft-Affinity, Task Assignment I. INTRODUCTION In any large-scale computing environment, from cloud data center to supercomputing cluster, the scheduler is one of the most critical components. Its fundamental purpose is to act as master coordinator, intelligently assigning usersubmitted tasks, ranging from long-running services through short-lived data analysis jobs, to the most appropriate machines. The design of these schedulers has been central topic of research for decades, producing influential systems that define how the resources are managed nowadays. Representative examples include Googles Borg [9], which demonstrates how to orchestrate vast and diverse collection of applications at unprecedented scale; Apache Hadoop YARN [8], which decouples resource management from job execution and has become the standard for big data workloads; and the SLURM Workload Manager, which dominates in HighPerformance Computing (HPC) environments. Schedulers typically make decisions based on two categories of rules: hard and soft ones. Hard rules, also known as constraints, represent absolute, non-negotiable requirements that program needs to satisfy. For example, graphics-intensive application may require node equipped with specific type of GPU. The scheduler first filters out all the machines that do not satisfy these constraints. If none of them qualify, the job is not in scheduled, ensuring incompatible environments. that applications never run Once the scheduler identifies the machines satisfying all the hard rules, it applies the soft rules, or the preferences, to select the optimal candidate. These act as guidance rather than some strict requirements, i.e. for preferring nodes that are lightly loaded or located near data source in order to minimize latency. The scheduler assigns scores to these preferences, optimizing for metrics such as performance or energy efficiency. Considerable research has focused on fairness and preference models, including Dominant Resource Fairness resource (DRF), which ensures equitable allocation among heterogeneous users [1]. the valid machines based on These principles have been implemented in various systems. The Apache Mesos scheduler introduced resource offer mechanism to manage constraints [3], while Firmament adopted network-flow-based model to accelerate placement decisions [2]. Despite their significant architectural differences, these systems share common limitation: they require operators to express scheduling goals in rigid, machine-readable syntax. This leads to significant semantic gap between human intent and machine-level configuration. developer might express an intuitive request such as Run this machine learning job on powerful node, preferably one that isnt overloaded and is located in Europe. To achieve this in systems like Kubernetes, users must write complex YAML files specifying numerous fields and weighted values. This process is time-consuming, error-prone, and demands deep system expertise. This paper introduces new system designed to bridge that gap. The presented intelligent scheduler employs Large Language Model (LLM) as translation layer between natural-language user intent and the schedulers structured directives. Developers can specify scheduling preferences in plain English (or any other supported language), and the model automatically generates the corresponding configuration, enabling the scheduler to make human-aligned placement decisions. is contribution the design"
        },
        {
            "title": "The  primary",
            "content": "and implementation of semantic, intent-driven scheduling paradigm for Kubernetes, which leverages an LLM to translate natural-language requirements into low-level scheduling logic. Specifically, the other contributions are: Semantic Scheduling: shift from declarative, syntaxbound configuration (e.g., YAML with nodeAffinity) intent-based expressions. This toward semantic, reduces cognitive overhead for developers and supports more flexible workload specification. Dynamic and Extensible Logic: Traditional schedulers require writing and compiling new plug-ins in the Go programming language to incorporate custom logic. The presented approach allows new scheduling behaviors to be introduced dynamically by defining new Intent classes and updating the LLM prompt, with the aim of improving adaptability. Real-Time Contextual Awareness: By combining live cluster state cache with the reasoning capabilities of an LLM, the system enables nuanced soft-affinity (e.g., prefer_colocate_same_deployment, decisions spread_zones) that go beyond static affinity and antiaffinity mechanisms."
        },
        {
            "title": "III  details",
            "content": "The remainder of this paper is structured as follows: Section II reviews related work in cluster scheduling, highlighting existing approaches to affinity and resource management. Section the design and implementation of the prototype system and testbed environment, including the Kubernetes cluster simulation, the Cluster State Cache, the Score Extender Service, and the LLM-based Intent Analyzer module responsible for semantic parsing. Section IV presents the empirical evaluation, focusing first on the accuracy of intent recognition using various LLMs against an evaluation dataset and then outlining the methodology for assessing scheduling efficiency and placement quality. Finally, Section concludes the paper by summarizing the key findings, discussing the limitations of the current prototype, and proposing directions for future research. intersects contrasting conventional II. RELATED WORKS This section surveys foundational and contemporary research pertinent to the proposed system, structured across four principal domains. The first is AI in Cluster Scheduling, policy-driven schedulers with ML and LLM-assisted approaches. The resource topic of semantic soft affinity scheduling, ML-driven affinity prediction, and semantic analysis, drawing from workload trace analysis and AIbased scheduling. The second, Multi-Objective Scheduling (MOO), addresses the trade-offs in competing performance goals. The prototype's additive scoring model is weighted-sum strategy. The third, Intent-Based Systems (IBS), focuses on bridging the semantic gap between high-level intents and low-level configurations. Lastly, Parsing and Natural Language Interfaces covers semantic parsing for converting linguistic input into machineinterpretable policies. These research areas directly influence the systems architectural design. Insights from IBS and Natural Language Parsing (NLP) inform the Intent Analyzer. The foundations of MOO underpin the Score Extender Service, while advances in AI-assisted Scheduling guide the Cluster State Cache. A. AI IN CLUSTER SCHEDULING Cluster scheduling is central topic in distributed systems. Foundational systems like Google Borg [9], Apache Mesos [3], Quincy [4], and Firmament [2] established principles of fairness and resource sharing. Hadoop YARN [8] and SLURM are standards for big data and HPC. These schedulers rules and manual, declarative configuration. rely on hard-coded Real-world traces, like Google's Cluster Data, offer foundations for understanding workload patterns. [38] clustered workloads by resource usage, revealing job distributions and symmetries. This supports semantic soft affinity by showing how clustering can identify resource preferences without hard constraints, enabling NLP-based grouping. [42] also characterized Google workloads, noting elasticity and variability that inform soft affinity models. ML is increasingly used to predict node-task affinities. [40] proposed an ML ensemble to detect constraint operators, achieving 98% accuracy in affinity prediction. This predictive method could integrate NLP to parse metadata for softer, meaning-based affinities. Similarly, [6] introduced Decima, an RL learns scheduling policies, improving job completion times. Decima's approach offers basis for using NLP-derived semantic similarities. framework that Reviews highlight AI's role in overcoming traditional scheduling limitations. [39] surveyed DL workload scheduling in GPU datacenters, noting techniques to optimize utilization, which underscores the need for semantic tools like NLP to handle diverse workloads. survey by [44] on AI-driven job scheduling examines ML, RL, and hybrid models for dynamic allocation. [43] reviewed deep RL for cloud resource scheduling, emphasizing adaptive policies for NP-hard problems and suggesting semantic parsing extensions. ML-assisted scheduling, using models to guide placement, is seen in systems like DeepRM [5] and Decima [6]. However, these approaches use fixed quantitative inputs, lack natural language UI, and do not address the semantic gap between intent and configuration. The emergence of LLMs inspired research into semantic, intent-driven configuration. OpenDevin [10] shows LLMs can interpret natural-language DevOps commands into configurations, using for deterministic parsing. Research by [7] also shows models can translate high-level goals into actions, though often outside latency-critical paths. schema-guided extraction growing body of research explores LLM-assisted cluster management, using natural-language understanding for configuration or categorization. For example, [11] focuses on scheduling LLM inference tasks with KV cache constraints. These studies show an increasing intersection between natural-language resource management. The present work extends this by focusing on semantic intent translation for soft-affinity scheduling, using an LLM with live scheduler extender and reproducible testbed. It contributes to integrating naturallanguage infrastructure orchestration, emphasizing reproducibility, determinism, and quantitative evaluation. interfaces reasoning into and requiring trade-offs between B. MULTI-OBJECTIVE SCHEDULING The task of cluster scheduling is fundamentally MOO problem, conflicting objectives [33]. For example, maximizing fault tolerance by spreading pods (a spread_zones intent) conflicts with minimizing network latency by co-locating them on the same nodes (a prefer_colocate_same_deployment intent). Other conflicts include balancing resource utilization versus consolidating workloads for energy savings [31], or minimizing cost versus minimizing job completion time [34]. employs Formally, these problems seek Pareto optimal solutions, where no objective can be improved without degrading another. Research strategies: evolutionary algorithms (e.g., NSGA-II) to find the Pareto front, or scalarization. Scalarization, the more common approach, converts the multi-objective problem into single-objective one, often via weighted-sum method [34]. two main"
        },
        {
            "title": "A key challenge is that presenting an entire Pareto front \nto",
            "content": "impractical for real-time decisions. This led is preference-based optimization, using high-level user preferences to guide the selection of single best-fit solution. User preferences define the weights or constraints for the scalarization function [32]. The allocation hint proposed in this work serves this function; it is mechanism for capturing unstructured preferences and translating them, via an LLM, into the quantitative weights for the scheduler's multi-objective cost function. C. INTENT-BASED SYSTEMS Research in IBS addresses the semantic gap between high-level human goals and low-level system configuration [13]. The premise is that operators should specify what they want (e.g., ensure low latency) rather than how to achieve it (e.g., setting specific routing rules) [14]. This declarative, policy-driven model aims to replace imperative, errorprone manual configurations, reducing human error and cognitive overhead [16]. The core challenge is translating these abstract, human-centric goals into precise, machineexecutable instructions. loop intent includes: An IBS typically operates as continuous, closed-loop lifecycle [21]. This ingestion (capturing the objective); translation (converting the intent into formal policies); validation (ensuring policies are achievable and non-conflicting); actuation (orchestrating policies via controllers); and assurance (continuously monitoring the state to ensure the intent is met) [18]. If intent drift is detected, the system takes corrective action, completing the autonomous loop. The intent ingestion and translation phases are most relevant to this work. Traditionally, these use structured inputs like GUIs or, more commonly, Domain-Specific Languages (DSLs) [17]. DSLs allow operators to define complex intents formally [20]. While an improvement, DSLs still require specialized training and present steep learning curve, creating usability gap. IBS principles are most applied in Intent-Based Networking (IBN), particularly in SDN and managing 5G [24] and 6G [18] networks. Intents are used for traffic engineering, network slicing, and enforcing dynamic QoS [21]. The paradigm is also used in multi-cloud deployment [22] and defining dynamic security policies [19]. To address the usability limitations of formal DSLs, research has emerged to use natural language as the primary intent input. Early work explored NLP for translating user commands into network control actions [23] or verifiable policies [24]. The present work builds on this trajectory, proposing that modern LLMs can serve as more powerful universal translator for intent. By accepting unstructured allocation hints, this approach aims to eliminate specialized syntax, making intent-based scheduling accessible without the steep learning curve of traditional IBS. D. PARSING AND NATURAL LANGUAGE INTERFACES The Intent Analyzer's core functiontranslating an unstructured hint into structured JSON objectis semantic parsing challenge. This is extensively studied in Natural Language Interfaces to Databases (NLIDB), which aim to translate plain text into formal query language like SQL [12]. Traditional NLIDB systems struggled with ambiguity, but deep learning and LLMs have dramatically improved semantic parser performance [15]. This research applies these techniques to cluster scheduling, mapping text to scheduling policy rather than database query. from word To enable semantic soft affinity, NLP-driven clustering of textual workload descriptions (e.g., job requirements) is crucial. [41] apply graph-based clustering to semantic spaces revealing dense communities that capture contextual meaningsdirectly applicable to grouping similar jobs for soft allocation. This resource aligns with broader NLP applications management, though explicit integration with cluster scheduling remains underexplored. embeddings, in"
        },
        {
            "title": "Early",
            "content": "The advent of deep learning introduced new text-to-SQL sequence-to-sequence models architectures. struggled with rigid SQL syntax [30]. This led to sketchbased methods, where models like SQLNet learned to fill slots in predefined SQL sketch, improving accuracy [29]. Pre-trained transformers like BERT enabled models such as SQLova to consider both the question's context and the database schema's structure [27]. critical sub-task is schema linking: identifying relevant database tables and columns for question. In large databases, including the entire schema is inefficient; effective schema linking prunes the schema to only relevant parts. However, incorrectly omitting required table can impede correct query generation [26]. More recently, general-purpose LLMs have been applied to the NLIDB problem. Unlike earlier models needing extensive fine-tuning, LLMs can often perform text-toSQL translation in zero-shot or few-shot setting, generating correct queries for new schemas with little to no task-specific training [28]. This approach bypasses the costly data annotation process. and terminology) Despite progress, significant challenges remain [25]. Models still struggle with domain adaptation (handling grounding specialized (understanding abstract concepts). Generating highly complex SQL queries remains research frontier. These works form strong foundation, with gaps in NLP-specific semantic affinity offering opportunities for novel research could hybridize RL contributions. Future schedulers with NLP models for intent-based soft affinities. logic scheduling paradigm, III. PROTOTYPE AND TESTBED DESIGN To empirically evaluate the performance and efficacy of an intent-based and reproducible testbed is required. This section details the design and implementation of such an environment. The testbed is built around multi-node Kubernetes cluster simulated using Minikube, alongside custom scheduler extender. The extender intercepts scheduling requests and translates high-level, natural-language intents into concrete node-scoring decisions. robust The environment (see Figure 1 for detail) is designed as self-contained representation of topology-aware data center, enabling the evaluation of complex scheduling policies such as proximity preference, resource affinity, and workload spreading. The core components include: Local Kubernetes Cluster: multi-node Minikube cluster deployed via Minikube with the Docker driver, with nodes labeled to simulate multi-level physical topology. Cluster State Cache: thread-safe subsystem that maintains an up-to-date in-memory representation of cluster state. real-time, Score Extender Service: stateful scheduler extender microservice where the custom node-scoring logic resides. Intent Analyzer: An NLP module leveraging AWS Bedrock to access an LLM to translate user-provided hints into structured scheduling intents. Each of these modules, along with the testbed environment as whole, is described in detail in the following sections. A. LOCAL KUBERNETES CLUSTER The foundation of the experimental testbed is local Kubernetes version 1.31.4 cluster deployed using Minikube, configured with the Docker driver for node virtualization. This setup provides fully contained, reproducible environment suitable for rapid iteration and controlled experimentation. Figure 2 shows how the cluster is initialized with nine nodes: one dedicated control-plane node (minikube) and eight worker nodes (minikube-m02 through minikube-m09). This topology offers sufficient scale to evaluate scheduling behaviors across multiple failure domains while maintaining the manageability required for iterative testing. several advantages: The Minikube testbed is initialized using the Docker driver, providing containerized node lightweight, virtualization instead of full virtual machines. This approach offers it eliminates hypervisor configuration, reduces resource overhead compared to VM-based drivers, and shares the hosts networking stack for fast pod-scheduler communication. These containerized nodes can be managed via the Docker dashboard and programmatically scripted (e.g., labeled, tainted), which makes the testbed highly reproducible. This setup simplifies dependency management by using the host's OS and runtime, providing flexible platform for experimenting with topology-aware scheduling, custom intent analysis without extenders, and LLM-driven requiring production cluster. realistic The labeling scheme enables the prototype scheduler to interpret placement hints and make decisions based on synthetic but topology metadata, closely mimicking the conditions of multi-zone, multi-rack data center environment. To ensure isolation of control-plane operations, the control-plane node is tainted with noderole.kubernetes.io/control-plane=:NoSchedule, preventing the placement of user workloads and reserving the node exclusively for critical cluster management services such as etcd, kube-apiserver, and kube-scheduler. FIGURE 1. Prototype system architecture, illustrating the interaction between the main components: the Local Kubernetes Cluster (Minikube), Cluster State Cache, Score Extender Service, Intent Analyzer, and AWS Bedrock. critical feature of the testbed is its emulation of topology, necessary for validating physical network as intents scheduling such topology-aware spread_zones, and prefer_colocate_same_deployment, prefer_nearby_nodes_same_deployment. Immediately after cluster initialization, setup script programmatically labels the worker nodes to define hierarchical topology consistent with standard Kubernetes conventions. The hierarchy uses the following topology keys: Region (topology.kubernetes.io/region): All nodes share single region label, us-east-1. Zone (topology.kubernetes.io/zone): Two simulated availability zones, us-east-1a and us-east-1b, are assigned across the worker nodes. Rack (topology.kubernetes.io/rack): Five logical racks (rack-1 through rack-5) are distributed across the zones, with two nodes per racks 1, 3, and 4, and one node per rack 2 and 5."
        },
        {
            "title": "Table I details the topology structure used in the further",
            "content": "evaluation scenarios:"
        },
        {
            "title": "Node",
            "content": "minikube-m02 minikube-m03 minikube-m04 minikube-m05 minikube-m06 minikube-m minikube-m08 minikube-m"
        },
        {
            "title": "Region",
            "content": "us-east-1 us-east-1 us-east-1 us-east-1 us-east-1 us-eastus-east-1 us-east-"
        },
        {
            "title": "Zone",
            "content": "us-east-1a us-east-1a us-east-1b us-east-1b us-east-1b us-east-1b us-east-1b us-east-1b"
        },
        {
            "title": "Rack",
            "content": "rack-1 rack-1 rack-2 rack-3 rack-3 rackrack-4 rack-5 FIGURE 2. Minikube cluster startup sequence (Kubernetes version 1.31.4). The cluster consists of nine nodes: one control-plane node (minikube) and eight worker nodes (minikube-m02 through minikubem09). FIGURE 3. Minikube nodes displayed in the Docker Desktop dashboard. This GUI tool provided convenient way to monitor the state of the Kubernetes cluster nodes and manage associated persisted state (Volumes). The Score Extender Service, which encapsulates the semantic intent-based scheduling logic, is integrated directly into the Kubernetes control-plane via custom scheduler configuration file. This configuration modifies the kube-scheduler manifest to include the custom extender and adjust its scheduling behavior. Two configuration aspects are especially important: Extender Weight: The extenders weight in the scheduler configuration is set to 100, the maximum possible value. This ensures that the extenders node scores (ranging from 0-100) dominate those from Kubernetes native scoring plugins, giving the prototype primary control over node selection. NodeResourcesFit, Disabling Default Scoring Plugins: To isolate the effects of intent-based scheduling, several built-in Kubernetes scoring plugins are explicitly disabled, including TaintToleration, and DefaultPodTopologySpread. InterPodAffinity, Disabling these ensures that all node scoring and prioritization decisions originate from the custom NLPdriven scheduler extender, allowing for accurate evaluation of its independent behavior. This Minikube-based testbed thus provides controlled, topology-aware sandbox that replicates key aspects of realworld cluster scheduling while remaining lightweight and reproducible. It enables precise measurement of scheduling latency, accuracy, and correctness under various intent configurations, while isolating the impact of the LLMdriven logic from external Kubernetes behaviors. B. CLUSTER STATE CACHE The scheduler extender must make rapid decisions based on the current state of the entire cluster. Continuously querying the Kubernetes API server for each /prioritize request is computationally infeasible, as it would introduce prohibitive latency and place an unsustainable load on the API server. To address this, thread-safe state management subsystem the Cluster State Cache was implemented. This service maintains continuously updated, in-memory representation of all nodes and pods, effectively serving as high-speed local mirror of the clusters state. It employs hybrid update strategy designed to balance real-time accuracy with long-term consistency and fault tolerance: Real-time Watchers: The caches low-latency synchronization is achieved through set of real-time watchers. Two dedicated background threads establish persistent watch connections to the Kubernetes API server: the first for v1.list_node and the second for v1.list_pod_for_all_namespaces. These watchers are event-driven, receiving immediate notifications for ADDED, MODIFIED, and DELETED events. Upon receiving an event, the cache acquires lock and updates its internal dictionaries, ensuring that the inmemory state reflects cluster changes with sub-second for latency. This real-time stream is essential maintaining accurate, up-to-date context for schedulers scoring logic. the Periodic Refresh: To protect against transient failures, missed events, or watch disconnects, the watcher model is complemented by periodic reconciliation thread. This background process performs full re-list of all nodes and pods at regular intervals (e.g., every 60 seconds). This brute-force refresh ensures eventual consistency and acts as self-healing mechanism to recover from desynchronization or partial state loss. All modifications to shared data structures are protected by lock objects guaranteeing full thread safety under concurrent access. key performance optimization lies in the caches data normalization strategy. Rather than storing raw Kubernetes API objects, the system converts them into lightweight CachedNode and CachedPod data classes upon receipt. During this one-time conversion, all computationally complex parsing and normalization steps, such as converting resource strings (100m, 10Gi) into numeric values, and filtering topology labels, are performed. As result, the scoring logic can directly access pre-computed fields or cached_node.topology_labels, eliminating runtime parsing overhead and significantly accelerating the scoring loop. cached_node.cpu_count like C. SCORE EXTENDER SERVICE The core of the custom scheduling logic resides in an external microservice, implemented as Python Flask application, which exposes endpoints adhering to the Kubernetes scheduler extender webhook API [35]. This service is reachable by the kube-scheduler process and handles two types of requests: /filter: This verb is used by the kube-scheduler to prefilter nodes. The current implementation performs minimal validation, such as ensuring nodes do not possess NoSchedule taint. /prioritize: This verb is responsible for scoring. The kube-scheduler sends the pod specification and list of candidate nodes (those that passed the filter phase) to this endpoint. The service returns list of nodes with an integer score from 0 to 100. The extender service is stateful and integrates two key subsystems: the Cluster State Cache and the Intent Analyzer. significant challenge in this design is handling the high-velocity burst-like creation of pods, such as during deployment scale-up. The Kubernetes API-driven Cluster State Cache may not observe the placement of Pod 1 before Pod 2 is already being scheduled. This race condition would or prefer_colocate_same_deployment ineffective during the burst. To mitigate this, the service implements secondary, local cache of recent placements with very short time-tolive (10 seconds). When the /prioritize endpoint selects node, it immediately writes this decision into the local spread_nodes intents render like cache. Subsequent scoring requests query an \"effective\" list of pods, composite of the main Cluster State Cache and the recent placements cache, ensuring that affinity logic functions correctly even during successive scheduling events. rapid, iterates The scoring logic within the /prioritize endpoint is multi-step process. First, the service retrieves the allocation hint from the pods annotations and passes it to the Intent Analyzer. This module returns structured dictionary of detected Intent objects, along with their LLM-derived confidence and strength scores. Once the intents are parsed, the service through each candidate node, calculating final node score that starts at zero. base score is calculated for each intent (100 divided by the total number of intents) to provide an initial weight. The system then applies specific logic based on the Intent class: Resource Preferences: For intents like prefer_gpu or prefer_memory_gb, the service compares the metadata value from the intent (e.g., prefer_gpu_cores: 4.0) against the node's corresponding property (e.g., cached_node.gpu_count) retrieved from the Cluster State Cache. Topology Preferences: For intents like prefer_zones or avoid_racks, the service checks the nodes topology labels against the list of zones or racks provided in the intents metadata. Dynamic Affinity: For colocation like prefer_colocate_same_deployment, the service queries the \"effective\" pod list and counts existing pods from the same deployment on the node. intents Spread Logic: For spread_ intents (e.g., spread_zones, spread_nodes), the system builds histogram of existing pods from the \"effective\" pod list across the relevant topology (e.g., {'zone-a': 5, 'zone-b': 2}). least-loaded algorithm is applied, assigning the highest score to nodes in the least-populated domains. If node satisfies an intent, the calculated score is added to its node score. This score is multiplied by both the intent confidence and strength values provided by the Intent Analyzer. This weighting ensures that high-confidence, high-strength (e.g., must have) intent has greater impact than low-confidence, low-strength (e.g., maybe prefer) one. Conversely, avoid intents subtract weighted score. Finally, after all nodes are scored, the service normalizes the results: the highest-scoring node is deterministically assigned score of 100, and all other nodes are scaled between 1 and 99. This deterministic step ensures single, unambiguous node is selected by the kube-scheduler in the event of tie. The scoring functions are detailed in the Section IV. D. INTENT ANALYZER The core technical challenge Intent Analyzer solves is an NLP task known as semantic parsing [36]: translating an unstructured natural language utterance into structured, machine-readable scheduling policies - in this case: JSON object with intents and metadata. Intent Analyzer key functionalities are as follows: Reading and Sanitizing Allocation Hints: The Intent Analyzer retrieves natural language string from the allocation_hint annotation on the Pod object, passed via the /prioritize REST call. The hint is first processed through sanitization function to strip potentially malicious characters and mitigate prompt injection risks. LLM-Based Intent Translation: The sanitized hint is sent to an LLM endpoint as part of larger prompt (see Listing 1). In the testbed, AWS Bedrock with the Amazon Nova Pro model is used by default. The prompt instructs the LLM to analyze the hint and return JSON object mapping the users request to predefined list of intents along with accompanying metadata. To ensure deterministic behavior and repeatable results, the model parameters are set to temperature = 0.0 and nucleus sampling (top-p) = 1.0. The maximum response length is limited to 512 tokens; however, token encoding conventions vary between models, so this limit may be interpreted differently depending on the model used. Strength and Confidence Annotation: The LLM is prompted to return not only the detected intents but also confidence score (0.0-1.0) and strength multiplier (0.5, 1.0, or 1.5), reflecting the perceived importance of each request. For transparency and auditing purposes, the LLM also includes strength explanation field, briefly describing the rationale for the assigned strength value. To improve efficiency, the results of allocation hint analysis are cached. Since pods within single deployment frequently share identical allocation hints, analyzing each pod individually would be redundant and slow. The Intent Analyzer stores the structured JSON output keyed by the hint string, ensuring that the expensive LLM call is performed only once per unique allocation hint."
        },
        {
            "title": "Table  II  outlines",
            "content": "the specific scheduling intents recognized by the Intent Analyzer. Each row specifies unique intent, its semantic meaning, and the required metadata, including field names, data types, and example formats. The description content is incorporated directly into the LLM prompt during analysis to guide intent extraction. The intents are logically grouped according to their scope or type of scheduling preference. This hierarchical organization mirrors the operational scope of each scheduling rule: Colocation / Proximity: Placement of pods relative to others within the same deployment. Topological Constraints: Region-level, zone-level, and rack-level intents for preferring, avoiding, or distributing pods across these boundaries. Node-level: Target specific servers based on node characteristics. Deployment-level: Focus on proximity or separation relative to pods from other applications. Resource-based: Define to intrinsic node characteristics, such as CPU, memory, type, or network GPU/TPU availability, storage features. requirements related This structure enables the LLM to systematically map natural language hints to actionable scheduling policies while maintaining clarity, reproducibility, and operational relevance. Listing 1 presents the comprehensive prompt template engineered to direct the AI in its specialized capacity as semantic parser for the Kubernetes scheduler. The prompt explicitly establishes an accurate structured data extraction task, assigning the LLM the persona of an expert AI assistant. This role definition aims to prime the model for precision and adherence to instructions. Its articulated \"ONLY goal\" is to analyze the unstructured allocation hint annotation provided by users and translate it solely based on predefined list of intents into machine-readable JSON format, adhering strictly to the specified output format and extraction rules. is the prompt"
        },
        {
            "title": "The  core  of",
            "content": "the CRITICAL INSTRUCTIONS section. These rules guide the AIs behavior and were developed through testing: Defining the Input Text: The prompt clearly marks where the users request begins and ends using delimiters (---HINT START--- and ---HINT END---). This instruction ensures the LLM focuses precisely on the users text and does not misinterpret the surrounding instructions. Matching Requests to Defined Intents: The AI is provided with list of known scheduling requests, i.e., Intents. It must match phrases from the users hint to these predefined intents. The instructions emphasize selecting only obvious and clearly stated matches, minimizing guesswork or subjective interpretation by the AI. Extracting Details Correctly (Metadata): This is vital and carefully refined instruction. When an intent is matched, the AI must extract specific details accurately: (i) using exact names - the AI must use the exact predefined names for details (e.g., prefer_cpu_cores) so the scheduler program correctly interprets them; (ii) maintaining correct formats - details must adhere to specified formats, e.g., numbers as floats (like 16.0), lists as JSON arrays (like [\"us-east-1a\", \"us-east-1b\"]); (iii) listing all Items - if the user lists multiple items (like several regions), the AI is explicitly instructed to list all of them exactly as written, without summarizing or using wildcards (like us-east-1*). This prevents loss of information needed by the Score Extender Service; and (iv) applying defaults - if the AI identifies an intent but cannot confidently extract required detail from the text, it is instructed to use default value (1.0 for numbers, [] for lists) rather than omitting the field. This ensures the output JSON maintains consistent structure. Indicating Confidence: For each intent identified, the AI must provide confidence score (0.0 to 1.0). This score quantifies the AIs certainty about the match, allowing the scheduler logic to potentially weigh more confident interpretations more heavily. Assessing Importance (Strength): The AI assigns strength score based on specific keywords identified in the hint. simplified three-point scale (0.5 for weak words like prefer, maybe; 1.5 for strong words like must, critical; 1.0 otherwise) is used, as initial tests showed nuanced (i.e., float values between 0.0 to 1.0) scores were unreliable. If non-default strength is assigned, the AI must provide brief explanation quoting the keyword that justified it. Adhering to JSON Output Format: The prompt strictly mandates that the AI output ONLY single, valid JSON object, without any extra conversational text, explanations, or code formatting. This is essential so the scheduler program can directly parse the output without needing complex pre-processing. Handling Untrusted Input: The user's hint is explicitly labeled as untrusted, and the AI is instructed not to follow any instructions it might contain. This serves as safeguard, directing the AI to treat the hint purely as text for analysis according to the predefined rules, rather than as commands to execute. list of possible Below the main instructions, the prompt template also dynamically builds intents with descriptions and metadata fields. This section provides the definitive list (schema) of all valid scheduling requests the AI can identify. Including detailed descriptions and metadata requirements (see Table II) helps the AI perform the matching task accurately. Finally, the prompt also includes the JSON output example, reinforcing the required format, data types, and overall organization. Together, these designed sections and instructions aim to guide the LLM effectively, ensuring it reliably transforms user requests into the structured data needed for the intentbased scheduling system. The detailed nature of the prompt reflects the iterative refinement process informed by empirical evaluation on multiple LLMs. IV. SCORING MODEL The decision-making engine within the Score Extender Service employs deterministic, weighted additive utility function to evaluate candidate nodes. Unlike traditional binary filtering, this approach allows for nuanced trade-offs between conflicting soft preferences. Table III summarizes the symbols used in the following equations. Intent (grouped by scope) Description (used as part of LLM prompt template)"
        },
        {
            "title": "TABLE II \nINTENT CLASSES WITH REQUIRED METADATA",
            "content": "prefer_colocate_same_ deployment Prefer scheduling this pod on the SAME node as existing pods from the SAME deployment. No metadata required. prefer_nearby_nodes_ same_deployment Prefer scheduling this pod on node TOPOLOGICALLY CLOSE (same rack > zone > region) to existing pods from the SAME deployment. No metadata required. Required Metadata [] [] prefer_regions avoid_regions spread_regions prefer_zones avoid_zones spread_zones prefer_racks avoid_racks spread_racks prefer_nodes avoid_nodes spread_nodes Prefer scheduling in specific regions. Extract list of region names. Metadata field: 'prefer_regions' (MUST be JSON list of strings, e.g., ['us-east-1', 'eu-west-1']). ['prefer_regions'] Avoid scheduling in specific regions. Extract list of region names to avoid. Metadata field: 'avoid_regions' (MUST be JSON list of strings, e.g., ['us-east-1', 'eu-west-1']). ['avoid_regions'] Distribute pods of the same deployment across different REGIONS. No metadata required. [] Prefer scheduling in specific availability zones. Extract list of zone names. Metadata field: 'prefer_zones' (MUST be JSON list of strings, e.g., ['us-east-1a', 'us-east-1b']). ['prefer_zones'] Avoid scheduling in specific availability zones. Extract list of zone names to avoid. Metadata field: 'avoid_zones' (MUST be JSON list of strings, e.g., ['eu-central-1c']). ['avoid_zones'] Distribute pods of the same deployment across different availability ZONES. No metadata required. [] Prefer scheduling in specific server racks. Extract list of rack names. Metadata field: 'prefer_racks' (MUST be JSON list of strings, e.g., ['rack-a1', 'rack-b2']). Avoid scheduling in specific server racks. Extract list of rack names to avoid. Metadata field: 'avoid_racks' (MUST be JSON list of strings, e.g., ['rack-c3']). Distribute pods of the same deployment across different server RACKS. No metadata required. Prefer scheduling on specific nodes (servers/hosts). Extract list of node names. Metadata field: 'prefer_nodes' (MUST be JSON list of strings, e.g., ['node-101', 'node-102']). Avoid scheduling on specific nodes (servers/hosts). Extract list of node names to avoid. Metadata field: 'avoid_nodes' (MUST be JSON list of strings, e.g., ['node-maint']). ['prefer_racks'] ['avoid_racks'] [] ['prefer_nodes'] ['avoid_nodes'] Distribute pods of the same deployment across different NODES (servers/hosts). No metadata required. [] prefer_deployments Prefer scheduling near pods from specific other deployments/applications. Extract list of deployment names. Metadata field: 'prefer_deployments' (MUST be JSON list of strings, e.g., ['database', 'cache']). ['prefer_deployments'] avoid_deployments Avoid scheduling near pods from specific other deployments/applications. Extract list of deployment names to avoid. Metadata field: 'avoid_deployments' (MUST be JSON list of strings, e.g., ['batch-job']). ['avoid_deployments'] prefer_memory Prefer nodes with minimum amount of available RAM. Extract the amount in Gigabytes as float number. Metadata field: 'prefer_memory_gb' (MUST be float, e.g., 128.0 for 128GB). ['prefer_memory_gb'] Prefer nodes with minimum number of CPU cores. Extract the number of cores as float number. Metadata field: 'prefer_cpu_cores' (MUST be float, e.g., 16.0 for 16 cores). ['prefer_cpu_cores'] Prefer nodes with GPU hardware (CUDA cores). Extract the minimum number of GPUs required as float number. Metadata field: 'prefer_gpu_cores' (MUST be float, e.g., 4.0 for 4 GPUs). ['prefer_gpu_cores'] Prefer nodes with TPU hardware (Tensor Processing Unit). Extract the minimum number of TPU cores required as float number. Metadata field: 'prefer_tpu_cores' (MUST be float, e.g., 8.0 for 8 TPU cores). ['prefer_tpu_cores'] prefer_cpu prefer_gpu prefer_tpu prefer_ssd t l t x P / i s l g p e - N - m p e l s - u R Prefer nodes with Solid State Drive (SSD) storage. No metadata required. prefer_public_ip Prefer nodes that have public or external IP address. No metadata required. [] [] prefer_network_speed Prefer nodes with minimum network bandwidth. Extract the speed in Gigabits per second (Gbps) as float number. Metadata field: 'prefer_network_gbps' (MUST be float, e.g., 100.0 for 100Gbps). ['prefer_network_gbps'] prefer_network_type Prefer nodes with specific network interface type. Extract the network type name as string. Metadata field: 'prefer_network_type' (MUST be string, e.g., 'infiniband', 'ena'). ['prefer_network_type'] prefer_ephemeral_ storage Prefer nodes with minimum amount of local ephemeral storage. Extract the amount in Gigabytes as float number. Metadata field: 'prefer_ephemeral_storage_gb' (MUST be float, e.g., 500.0 for 500GB). ['prefer_ephemeral_storage_gb'] Formally, let ğ’© represent the set of candidate nodes filtered by the Kubernetes scheduler, and let â„ represent the set of unique scheduling intents extracted by the Intent Analyzer. The system calculates raw score, ğ‘†$%&(ğ‘›), for each node ğ‘› ğ’©. The fundamental scoring equation is defined as the summation of the weighted utility of all identified intents: ğ‘†$%&(ğ‘›) = , ğœ‡(ğ‘›, ğ‘–) 0â„ (1) where Âµ(ğ‘›, ğ‘–) represents the utility function of specific intent ğ‘– applied to node ğ‘›. This utility function is derived from the base weight of the intent relative to the total number of directives, modulated by the LLM's confidence and the user's semantic intensity. A. UTILITY FUNCTION COMPONENTS The utility function Âµ(ğ‘›, ğ‘–) is calculated as follows: ğœ‡(ğ‘›, ğ‘–) = ğ›½ ğ¶0 ğ‘Š0 ğœ™0(ğ‘›) (2) The components are defined as: Base Weight (ğ›ƒ): To ensure equal initial influence among multiple directives, base weight is calculated dynamically as Î² = 100/â„. Confidence (ğ‘ªğ’Š): scalar value ğ¶0 [0,1], provided by the Intent Analyzer, representing the probability that the intent was correctly identified. Strength (ğ‘¾ğ’Š): multiplier ğ‘Š0 {0.5,1.0,1.5} derived from linguistic modifiers, allowing the user's urgency to scale the impact of specific rules. Evaluation Logic (ğ›Ÿ_ğ’Š(ğ’)): domain-specific evaluation function that returns scalar value based on how well node ğ‘› satisfies intent ğ‘–. B. EVALUATION LOGIC CATEGORIES The evaluation logic Ï•0(ğ‘›) varies based on the semantic category of the intent: 1. Binary Preference Logic: For resource or topology attributes (e.g., prefer_gpu), the function acts as binary indicator: scores are scaled relative to the highest raw score observed in the current iteration: ğ‘†c]$g(ğ‘›) = maxi0, ğ‘†$%&(ğ‘›)j iğ‘†$%&(ğ‘š)j max gğ’© 100 (6) To eliminate non-determinism in tie-breaking scenarios, the system enforces an only-one-winner-node adjustment. The node ğ‘›mKdn with ties alphabetically) is assigned final score of 100, while all other nodes are clamped to [1,99]: the highest score (resolving ğ‘†L0c%p(ğ‘›) = 100 mini99, maxi1, ğ‘†c]$g(ğ‘›)jj if ğ‘› = ğ‘›mKdn otherwise (7) intents D. TOPOLOGICAL PROXIMITY QUANTIFICATION (e.g., affinity For topological requiring system the prefer_nearby_nodes_same_deployment), employs hierarchical proximity function. This function quantifies the \"gravitational pull\" of existing workloads on candidate node by aggregating pod counts across three topological boundaries: Rack, Zone, and Region. Let ğ’¯ = {ğ‘Ÿğ‘ğ‘ğ‘˜, ğ‘§ğ‘œğ‘›ğ‘’, ğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘›} be the set of topological domains. We define ğ‘˜c,n as the count of existing pods belonging to the target deployment within the specific domain ğ‘¡ of node ğ‘›. To prioritize tighter locality, the system assigns decaying weight vector Ï‰ to the domains: ğœ” = {ğœ”$%}: 2.0, ğœ”(cid:129)]cK: 0.5, ğœ”$K(cid:130)0]c: 0.2} (8) The raw proximity score ğ‘ƒ(ğ‘›) is calculated as the dot product of the weight vector and the topology counts: ğœ™J$KL(ğ‘›) = 1 if ğ‘› satisfies condition 0 otherwise (3) ğ‘ƒ(ğ‘›) = ğœ”$%} ğ‘˜c,$%} + ğœ”(cid:129)]cK ğ‘˜c,(cid:129)]cK + ğœ”$K(cid:130)0]c ğ‘˜c,$K(cid:130)0]c (9) 2. Avoidance Penalty Logic: For negative constraints (e.g., avoid_zones), the system applies weighted penalty. To enforce avoidance strongly, the prototype applies doubled magnitude subtraction: ğœ™%]0^(ğ‘›) = 2 if ğ‘› matches criteria 0 otherwise (4) 3. Distribution and Spreading Logic: For intents requiring the system high availability (e.g., spread_zones), calculates least-loaded score. Let ğ‘˜c be the count of pods belonging to the same deployment in the topology domain of node n, and be the maximum count of such pods in any domain. The spreading score is normalized as: ğœ™dJ$K%^(ğ‘›) = ğ‘€ ğ‘˜c + 1 ğ‘€ + 1 (5) C. NORMALIZATION AND DETERMINISTIC SELECTION To ensure compatibility with the Kubernetes scheduler extender API, raw scores are normalized to the range [0,100]. First, negative scores are clamped to zero. Then, is normalized against To prevent unbounded growth of this score in large clusters, ğ‘ƒ(ğ‘›) the maximum observed proximity score across all candidate nodes in the current scheduling cycle. The final evaluation logic Ï•J$](cid:132)(ğ‘›) is defined as: ğœ™J$](cid:132)(ğ‘›) = ğ‘ƒ(ğ‘›) iğ‘ƒ(ğ‘š)j max gğ’© (10) This hierarchical weighting ensures that nodes sharing rack with existing pods are mathematically favored over those sharing zone or region, aligning the scheduler's decisions with physical network topology constraints. E. TEMPORAL STATE CONSISTENCY To mitigate the latency inherent in asynchronous Kubernetes API updates which can lead to race conditions during highvelocity \"burst\" scheduling, the system implements hybrid state model. Let ğ’«ğ’¶ğ“…ğ’¾ represent the set of confirmed pods retrieved from the Kubernetes API (via the Cluster State Cache), and let ğ’«â„“â„´ğ’¸ğ’¶â„“ represent the set of tentative scheduling decisions recorded in the extender's local short-term memory. The local set is defined by Time-To-Live (TTL) constraint Ï„, set to 10 seconds: ğ’© â„ The set of candidate nodes filtered by the Kubernetes scheduler. The set of scheduling intents extracted from the allocation hint. ğ’«â„“â„´ğ’¸ğ’¶â„“ = {ğ‘ ğ’«ğ’¹â„¯ğ’¸ğ’¾ğ“ˆğ’¾â„´ğ“ƒğ“ˆ iğ‘¡c]& ğ‘¡d}(cid:147)K^(cid:148)pK^(ğ‘)j < ğœ} (11) The Effective Pod Set, ğ’«â„¯ğ’»ğ’», used for all affinity and antiaffinity calculations (such as the count ğ‘˜c in Eq. 5 and Eq. 9), is the union of these two sets, ensuring that subsequent scheduling requests within burst are aware of pending placements before they are persisted in etcd: ğ’«â„¯ğ’»ğ’» = ğ’«ğ’¶ğ“…ğ’¾ ğ’«â„“â„´ğ’¸ğ’¶â„“ (12) This consistency model ensures that for any dynamic scoring function ğ‘“(ğ‘›, ğ’«), the evaluation is performed against the most current state approximation: ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘›) = ğ‘“iğ‘›, ğ’«â„¯ğ’»ğ’»j. F. Multi-Objective Scalarization The scheduling problem is modeled as Multi-Objective Optimization (MOO) task, where the goal is to identify node that maximizes vector of objective functions Î¦(ğ‘›) = [ğœ™(cid:155)(ğ‘›), ğœ™(cid:156)(ğ‘›), , ğœ™(ğ‘›)]. in real-time, To resolve conflicts between objectives (e.g., spread vs. affinity) linear scalarization strategy. We define dynamic weight vector ğ‘¾ where each element ğ‘¤0 is the product of the base weight, confidence, and strength: the system employs ğ‘¤0 = ğ›½ ğ¶0 ğ‘Š (13) The scalarized global objective function ğ¹(ğ‘›), which corresponds to the code's implementation of the additive scoring loop, is the dot product of the weight and objective vectors: ğœ‡(ğ‘›, ğ‘–) The utility function for specific intent ğ‘– on node ğ‘›. ğ¶0 ğ‘Š ğ›½ ğ‘˜c ğ’«â„¯ğ’»ğ’» ğ‘†c]$g Î¦(ğ‘›) pK(cid:132) Confidence score [0,1] provided by the LLM for intent ğ‘–. Strength multiplier {0.5, 1.0, 1.5} derived from linguistic modifiers. Base weight calculated as ğ›½ = 100/â„. Count of relevant pods (same deployment) currently on node ğ‘›. The effective set of pods including API state and local cache. The normalized score scaled to [0,100] relative to the best node. The vector of all evaluation logic results for node ğ‘›. Lexicographical order (alphabetical sort) used for tie-breaking. V. SYSTEM EVALUATION To validate the efficacy and performance of the proposed semantic, intent-driven scheduling paradigm, series of quantitative experiments were conducted using the prototype testbed. The evaluation focuses on two critical aspects of the system: firstly, on the accuracy with which the Intent Analyzer translates natural language allocation hint annotations into structured, actionable intents and their associated metadata; and secondly, on the quality and efficiency of the resulting pod placements compared to the default Kubernetes scheduler configured with traditional i.e., using standard affinity and anti-affinity rules, directives and nodeSelector, podAntiAffinity to achieve similar outcome. These experiments aim to provide empirical evidence supporting the hypothesis that an LLM-based approach can effectively bridge the gap between high-level user requirements and low-level scheduling decisions, while also identifying the performance characteristics and limitations of the prototype implementation. podAffinity, like (14) A. INTENT RECOGNITION ACCURACY â„ ğ¹(ğ‘›) = ğ‘¾ Î¦(ğ‘›) = , ğ‘¤0ğœ™0(ğ‘›) 0(cid:160)(cid:155) Finally, to ensure strict determinism guaranteeing that the scheduler makes identical decisions for identical state inputs regardless of asynchronous race conditions, the system imposes Lexicographical Total Ordering on the set of optimal candidates. Let ğ’©â„´ğ“…ğ“‰ be the set of nodes achieving the maximum score. The selected node ğ‘› is defined mathematically as the minimum element under lexicographical order (pK(cid:132)) among the highest-scoring nodes: ğ‘› = min Æ’ { ğ‘› ğ’© ğ‘†L0c%p(ğ‘›) = max gğ’© ğ‘†L0c%p (ğ‘š)} (15) This formalization proves that the system avoids random tiebreaking, critical property for reproducibility in cluster orchestration."
        },
        {
            "title": "Definition",
            "content": "The first experiment tests the core of the semantic claim, i.e., how accurately does the LLM translate natural language into structured intents? The hypothesis is as follows: the LLM-based Intent Analyzer can accurately and consistently parse unstructured allocation hint text into the correct, structured Intent objects and their associated metadata. To validate the semantic parsing performance of the Intent Analyzer, benchmark ground-truth evaluation dataset was systematically constructed. The dataset v1, used to evaluate the LLM, can be found on the Model Hub [37] (file: schedule-intent-evaluation-v1.json). This dataset is foundational to this experiment, as it provides the means to quantify the accuracy, precision, and recall of the systems core function: translating unstructured, naturallanguage of allocation hint text into structured, machinereadable JSON objects. The 314-prompt evaluation dataset was created using structured four-category methodology. This approach ensures comprehensive test coverage across all 25 Intent types, while also testing the models robustness against complex, nuanced, and invalid inputs. Each of the prompts is paired with manually authored JSON object representing the expected ground truth. The dataset is segmented into four distinct categories, each designed to test specific aspect of the analyzers performance: Categorical & Paraphrasing: Tests in this category establish baseline accuracy for all 25 Intent types. For each individual intent, multiple prompts were authored using different paraphrasing and natural language variations. This component tests the models semantic equivalence detection (e.g., recognizing that needs GPU, want an NVIDIA card, and this is CUDA job all map to the prefer_gpu intent). Combinatorial (Complex): This category assesses the LLMs ability to parse complex prompts containing multiple, distinct intents. Prompts were authored to combine two or more directives, often mixing resource requirements with topology constraints (e.g., Highmemory pod (64GB), please spread across zones). This evaluates the models parsing completeness and its ability to handle non-trivial, realistic scheduling requests. Strength & Nuance: Those tests focus on the models interpretation of qualitative language. Prompts were authored to include specific qualifier words indicative of scheduling priority (e.g., must run, critical, strongly prefer vs. if possible, maybe, nice to have). This allows for quantitative evaluation of the generated strength and confidence scores, which are critical for effective, weighted scheduling decisions. Negative & Irrelevant (Noise): This category measures the models specificity and its resilience to false positives. It consists of prompts containing no valid scheduling intents, including empty strings, irrelevant technical metadata (pod image: nginx), ambiguous statements (this is test pod), or random gibberish. This validates the models ability to correctly identify and reject non-actionable input. The resulting 314-prompt evaluation dataset provides balanced mix of simple coverage, complex combinations, and realistic noise, enabling robust evaluation of the systems precision, recall, and overall effectiveness. The test dataset and evaluation results were also utilized to incrementally refine the prompt template for LLM models, intent descriptions, and to select the best performing model. Notable lessons learned are summarized below: Iterative refinement is an essential approach, and the initial prompt - model combination required several iterations to achieve satisfactory results. Systematic evaluation against ground-truth dataset - followed by failure analysis and targeted prompt adjustments - proved crucial for improving performance. Multiple iterations were necessary to reach the final results. experimented with the amazon.nova-micro-v1:0 model (128K token context research initially The window), but this model proved too small to capture the nuances required for detailed metadata extraction and strength interpretation. Subsequently, larger models such as Amazon Nova Lite / Pro (300K token context window), Amazon Nova Premium (1M token context window) were employed - the Pro model providing an effective balance between model size and cost. Large models especially excel at metadata identification and subsequent extraction into correct format. The most significant improvements came from making the prompt instructions highly specific and rule-based, i.e., by providing explicit guidance within the prompt. Adding explicit rules for data types (e.g., float, JSON list), precise naming conventions, handling of defaults, and especially negative constraints (e.g., DO NOT summarize lists, DO NOT use wildcards) dramatically improved metadata accuracy. However, overly strict prompt instructions (e.g., requiring perfect metadata extraction or omitting intent) led to an increase in missed metadata values when the model was uncertain. Modifying the prompt to allow reasonable defaults (e.g., 1.0 for counts, [] for lists) when extraction robustness without significantly compromising accuracy. improved failed Asking the model for nuanced strength interpretation, i.e., float values within ranges (e.g., 0.5-2.0), performed poorly. Simplifying the scale to three distinct values (0.5, 1.0, 1.5) and explicitly linking them to specific keywords in the prompt instructions led to much higher accuracy. Defining intent descriptions to serve as in-prompt guidance proved highly beneficial. Adding clear statements such as MUST be JSON list of strings or MUST be float directly within the descriptions reinforced the expected output format for each intents metadata. Including concrete examples within these descriptions (e.g., ['us-east-1a', 'us-east-1b'], 128.0 for 128 GB) further clarified the target format and improved consistency across outputs. The evaluation systematically compared the performance of eleven LLMs in parsing natural language scheduling hints into structured intents and metadata. The selection of the 11 LLMs for evaluation was aimed at robust comparison across provider diversity, model size, cost, and availability on the AWS Bedrock platform. The models were chosen from five different providers (Amazon, to assess varying Anthropic, Meta, AI21, Mistral) architectures. The inclusion of different model sizes within families (e.g., Nova Micro, Lite, Pro, Premier; Claude 3 Haiku, Sonnet) allowed for analysis of capability scaling, ultimately finding Nova Pro offered good balance. LISTING 1 PROMPT TEMPLATE You are an expert AI assistant performing highly accurate **structured data extraction** task for Kubernetes scheduler. Your **ONLY** goal is to analyze the user-provided text hint below and extract scheduling preferences based *solely* on the defined list of intents. Adhere strictly to the specified JSON output format and extraction rules. **CRITICAL INSTRUCTIONS:** 1. **Analyze the User Hint:** Carefully read the untrusted user hint provided between the '--- HINT START ---' and '--- HINT END ---' markers. 2. **Identify Intents:** Match phrases in the hint to the intents defined in the '--- LIST OF POSSIBLE INTENTS ---' section. Only include intents that are clearly and unambiguously expressed. 3. **Extract Metadata (VERY IMPORTANT):** For each identified intent, extract ALL required metadata fields specified in its description. - **Naming:** Use the *exact* metadata field names (e.g., `prefer_regions`, `prefer_cpu_cores`, `prefer_tpu_cores`). - **Types:** Ensure values match the expected type (list of strings, float). - **Numbers:** Extract numerical values precisely as floats (e.g., `128.0` for 128GB, `16.0` for 16 cores, `4.0` for 4 GPUs/TPUs). Extract the number *directly associated* with the preference. - **Lists:** If list of strings is expected (regions, zones, nodes, deployments), provide JSON list `[\"item1\", \"item2\"]`. **Crucially: List ALL specific items mentioned by the user individually.** - **DO NOT summarize list items.** (e.g., If user says \"avoid Asia regions like ap-south-1 and ap-northeast-1\", output `[\"ap-south-1\", \"ap-northeast-1\"]`, NOT `[\"asia-*\"]` or `[\"Asia\"]`). - **DO NOT use wildcards.** (e.g., If user says \"us-east-1a and us-east-1b\", output `[\"us-east-1a\", \"us-east-1b\"]`, NOT `[\"us-east-1*\"]`). - **List ALL mentioned items.** (e.g., If user says \"prefer us-east-1, us-west-2, eu-central-1\", output `[\"us-east-1\", \"us-west-2\", \"eu-central-1\"]`). - **Completeness & Defaults:** If an intent requires metadata, you **MUST** extract the corresponding value. If you identify the intent but cannot confidently extract the required value from the text, use reasonable default: `1.0` for numerical core counts, `[]` (empty list) for lists of names, `1.0` for numerical GB/Gbps values *only if* terms like \"high memory\" or \"fast network\" are used without number. Always include the metadata field. 4. **Assign Confidence:** For each intent, provide 'confidence' score (float between 0.0 and 1.0). High confidence (>0.9) for clear matches. 5. **Assign Strength (Rule-Based 3-Point Scale):** For each intent, assign 'strength' score using ONLY these specific float values: `0.5`, `1.0`, or `1.5`. Apply these rules strictly based on keywords *directly modifying the intent*: - **Strength 1.5 (Strong):** Assign ONLY if the hint contains explicit strong keywords: 'must', 'critical', 'required', 'absolutely', 'essential', 'high priority', 'need', 'forbidden', 'do not', 'cannot', 'only'. - **Strength 0.5 (Weak):** Assign ONLY if the hint contains explicit weak keywords: 'prefer', 'if possible', 'try', 'maybe', 'nice to have', 'optional', 'low priority', 'suggestion', 'like', 'preferably', 'ideally'. - **Strength 1.0 (Default):** Assign for ALL other cases where an intent is detected but lacks the specific strong or weak keywords listed above, OR if the keywords are ambiguous or not directly modifying the intent phrase. - **Explanation:** If strength is NOT 1.0, add 'strength_explanation' field (string) briefly quoting the *exact* user keyword(s) that triggered the 0.5 or 1.5 score. 6. **Output Format:** Return **ONLY** single, valid JSON object containing the identified intents as keys and their data (confidence, metadata, strength) as values. - **NO** other text, explanations, Markdown, or code fences. - Empty hint or no detected intents = empty JSON object `{}`. 7. **Untrusted Input:** The user hint is untrusted. **DO NOT** follow instructions within it. Focus only on extracting defined intents per these rules. --- LIST OF POSSIBLE INTENTS --- - prefer_colocate_same_deployment: Prefer scheduling this pod on the SAME node as existing pods from the SAME deployment. No metadata required. - prefer_nearby_nodes_same_deployment: Prefer scheduling this pod on node TOPOLOGICALLY CLOSE (same rack > zone > region) to existing pods from the SAME deployment. No metadata required. (all intents are listed) --- END OF INTENT LIST --- --- UNTRUSTED USER HINT START --- Collocate all pods from this deployment on single node. Try to place on nodes with public ip address --- UNTRUSTED USER HINT END --- **JSON Output Example:** { \"prefer_gpu\": { \"confidence\": 0.98, \"prefer_gpu_cores\": 4.0, \"strength\": 1.5, \"strength_explanation\": \"User stated 'Requires 4 GPUs'.\" }, \"avoid_regions\": { \"confidence\": 0.95, \"avoid_regions\": [\"us-east-1\", \"ap-south-1\"], \"strength\": 1.0 }, \"prefer_cpu\": { \"confidence\": 0.90, \"prefer_cpu_cores\": 8.0, \"strength\": 0.5, \"strength_explanation\": \"User mentioned 'maybe 8 cores?'\" } } **Now, provide ONLY the JSON output based strictly on the user hint and the critical instructions above:** Cost-effectiveness was also considered, with the cheaper Nova models initially used for prompt refinement. Finally, practical availability constraints influenced the selection, excluding Claude 3 Opus (enterprise-only access) and larger Llama 3 models (EU restrictions). This multi-faceted approach ensured comprehensive assessment of LLM performance for the specific task of semantic intent parsing. All models were evaluated against the 314-item ground-truth evaluation dataset using the final, highly refined prompt and intent descriptions to ensure fair comparison of their capabilities. The findings presented in Table IV indicate significant performance disparities between models but establish clear top tier of high-capability models. Amazon Nova Premier, Mistral Pixtral Large, Amazon Nova Pro, and Anthropic Claude 3 Sonnet all demonstrated exceptional performance, classification. Conversely, smaller or less-optimized models like Amazon Nova Micro, Claude 3 Haiku, Mistral 7B, and especially AI21 Jamba Mini and the Llama 3 (3B) variant, showed significant deficiencies, particularly in intent detection recall. particularly intent in The original intention of using Amazon Nova models in the early research was to refine the prompt template and intent descriptions and to determine the required model size while saving on cost, as Nova models are substantially cheaper per token. However, during the research, it was found that Amazon Nova models could stand on their own. As shown in Table IV, the performance of models like Amazon Nova Pro and Amazon Nova Premier was demonstrated to be comparable to other top-tier models, such as Anthropic Claude 3 Sonnet and Mistral Pixtral Large. Additionally, to establish non-AI baseline, Regular Expressions (Regex) Engine was implemented with set of hardcoded sentence and keywords matching expressions, and evaluated against the same dataset. As shown in Table IV, the Regex Engine offers virtually instantaneous performance, with average and P95 latencies under millisecond. Nevertheless, its ability to accurately parse the semantic nuances of the natural language hints proved significantly inferior to the top-tier LLMs. The Regex Engine achieved Subset Accuracy of only 29.62%, indicating it correctly identified the full set of intents in less than third of cases, compared to over 95% for models like Nova Premier and Mistral Pixtral Large. Its Macro F1-score (0.38) and Recall (0.30) were drastically lower than the best LLMs (0.98 and 0.98, respectively), highlighted by an extremely high number of False Negatives (240 missed intents). While Metadata Value Accuracy was somewhat better (78.67%), it still lagged behind the top LLMs and struggled (e.g., avoid_zones 0% accuracy). This demonstrates that while regular expressions-based logic is very fast, it is too brittle significantly with extraction list and lacks the semantic understanding required to reliably interpret the varied phrasing, complex combinations, strength indicators, and paraphrasing inherent in natural language scheduling requests, justifying the use of more sophisticated LLMs despite their higher latency. Regarding intent classification, the ability to identify the correct set of user directives, the top-tier models were outstanding. Amazon Nova Premier achieved the highest Subset Accuracy at 97.45%, closely followed by Mistral Pixtral Large (95.86%), Amazon Nova Pro (93.63%), and Anthropic Claude 3 Sonnet (93.63%). These models all achieved Macro F1-scores of 0.95 or higher, demonstrating near-perfect precision and recall with minimal errors (Aggregated False Positives 10-17, False Negatives 9-18). In stark contrast, smaller models like Nova Micro, Claude 3 Haiku, and Mistral 7B showed significant deficiencies, with Subset Accuracies between 68-78% and much higher False Negative counts (64-103). The Llama 3 (3B) and Jamba Mini models performed very poorly, failing to classify intents correctly in the vast majority of cases. In metadata value extraction, which assesses the accuracy of parsing quantities and entities, performance was strong among the top models. Claude 3 Sonnet achieved the highest accuracy at 95.22%, closely followed by Nova Micro (94.57%), Nova Pro (94.40%), and Mistral Pixtral Large (93.75%). This indicates that when models (even smaller ones like Micro) correctly identify an intent, the refined prompt is very effective at guiding data extraction. Notably, all high-performing models achieved perfect or near-perfect Metadata Completeness, successfully providing all required fields for the intents they detected. Still persistent minor weaknesses were observed across all models, particularly in accurately extracting complex lists without simplification (e.g., substituting specific regions with generic terms like 'Asia' or 'Europe') and in correctly applying default numerical values (often defaulting to 1.0) for prefer_network_speed and prefer_ephemeral_storage when only qualitative terms were used. B. SCHEDULING EFFICIENCY AND QUALITY With optimized prompt and selected model Amazon Nova the second experiment focused on Pro (pro-v1:0), evaluating the quality of placements, i.e., how the created solution compares with the default Kubernetes scheduler? The hypothesis was formulated as follows: the intentscheduler driven Score Extender Service-based produces pod placements that are quantifiably superior in fulfilling user intent compared to the default scheduler. In the Minikube testbed, the control-plane node runs within the kube-system namespace, and all core Kubernetes components, including the kube-apiserver, kube-scheduler, etcd, and kube-controller-manager, are deployed as individual pods (Figure 4). This design provides several practical advantages for testing and debugging. Since each system component is containerized and placed within unique namespaces, it is straightforward to list and inspect errors, monitor resource usage, and collect detailed scheduling logs. Administrators can access component-specific logs via standard Kubernetes tools such as kubectl logs, enabling fine-grained visibility into the schedulers decision-making process and making it easier to trace the impact of custom scheduler extenders, intent analysis, and topology-aware placement policies. This setup provides fully observable and debuggable control-plane environment without affecting the worker nodes running user workloads. In addition, for implementing low-level modifications required by this investigation, as well as for tracking the outcome, i.e., the resulting allocations for further evaluation. particularly convenient proved it Notably, matching the labels, i.e., the metadata.labels and selector.matchLabels fields (as specified in Table IV) ensures the Deployment controller can correctly identify, count, and manage the Pods it is responsible for, as labels are the link between the controller and the objects it controls. Table IV captures both the baseline Kubernetes manifests and manifests with allocation hints. The experiment utilized six distinct scenarios comparing the prototype's decisions against baseline Kubernetes configurations. The results for these scenarios are detailed in Table IV and summarized below. The relevant configuration sections in the deployment manifests were additionally bolded. Scenario - Topology Spreading: This test evaluated high-availability policy using the hint spread these pods evenly across all available zones for high the baseline scheduler (using availability. Both topologySpreadConstraints) and intent-driven the scheduler achieved perfect 3:3 split of the six replicas across the two simulated zones, successfully fulfilling the objective. Scenario - Resource Affinity: This scenario tested resource preferences with critical ML training job, it must run on nodes with GPUs hint. Both the baseline (using nodeAffinity) and the intent-driven scheduler successfully placed all 6 out of 6 pods (100%) on the minikube-m02 node, which was labeled as having GPU. This demonstrated that the prototype correctly interpreted the must run hint as strong preference and that its scoring logic functioned as intended."
        },
        {
            "title": "Model",
            "content": "2 ) 0 : 1 - 9 2 2 0 4 2 0 2 ( n 3 a ) k 0 0 2 ( ) 0 : 1 - 7 0 3 0 4 2 0 2 ( a 3 a ) k 0 0 2 ( 1 ) 0 : 1 - m ( m a n m ) k 1 ( 3 ) 0 : 1 - r i - 3 - 2 - 3 ( a t ) k 8 2 1 ( ) 0 : 1 - m - 5 - 1 ( M a 1 2 ) k 6 5 2 ( ) 0 : 1 - a - 5 - 1 ( a m 1 2 ) k 6 5 2 ( ) 0 : 1 - 2 0 5 2 ( r a i r M ) k 2 3 ( ) 2 : 0 - r i - 7 ( u n r M ) k 2 3 ( ) 0 : 1 - i ( i"
        },
        {
            "title": "M\na\nv\no\nN\nn\no\nz\na\nm\nA",
            "content": ") k 8 2 1 ( ) 0 : 1 - l ( L N a ) k 0 0 3 ( ) 0 : 1 - ( P N a ) k 0 0 3 ( ) l b - ( g e R 29.62% 78.03% 92.04% 93.63% 97.45% 93.63% 75.80% 22.61% 18.79% 90.45% 95.86% 68.79% 0.38 0.84 0.93 0.95 0. 0.96 0.85 0.30 0.26 0.93 0. 0.75 0.64 0.91 0.94 0.95 0. 0.96 0.91 0.58 0.80 0.94 0. 0.88 0.30 0.82 0.94 0.95 0. 0.97 0.82 0.34 0.16 0.94 0. 0.69 116 292 337 339 345 291 150 70 338 253 10 35 23 17 11 26 414 4"
        },
        {
            "title": "Notes",
            "content": "% of hints with perfect intent set match. Average F1 across all intents (balanced precision/recall). Average precision across all intents. Average recall across all intents. Correctly detected intents total. Incorrectly detected intents total. 240 64 20 18 9 65 206 286 18 10 Missed intents total. 78.67% 94.57% 89.60% 94.40% 91.89% 95.22% 93.36% 66.91% 84.21% 91.53% 93.75% 84.02% 44.83% 43.49% 35.91% 30.38% 25.07% 70.14% 68.04% 62.00% 28.57% 68.34% 58.09% 20.16% 59 / 75 209 / 221 224 / 236 / 250 238 / 259 239 / 251 197 / 211 91 / 136 48 / 227 / 248 240 / 256 163 / 194 73.28% 52.40% 67.95% 67.85% 71.79% 64.64% 63.92% 41.33% 62.86% 55.92% 71.79% 43.48% 0.0444 0. 0.0498 0.0480 0.0412 0.0761 0.0363 0. 0.0329 0.0761 0.0522 0.0497 % of correctly extracted metadata fields. % of non-default strength scores with explanation. Count of correct fields vs. total expected fields. % of correctly assigned strength scores (0.5, 1.0 or 1.5). Mean Absolute Error vs. ground truth (confidence). 0.0000s 0.4475s 0.5802s 0.7931s 1.0847s 2.0196s 1.1668s 1.8046s 1.4853s 2.0582s 2.8104s 0.9323s Average request time. 0.0014s 1.4120s 2.1121s 2.0866s 2.5670s 6.8001s 3.0732s 7.6656s 5.5956s 6.3754s 6.8237s 2.8802s 0.0001s 0.6942s 0.8732s 1.2442s 1.8416s 2.9403s 1.9125s 2.7433s 3.1769s 3.6644s 5.3434s 1.7795s Maximum (worst-case) latency. Tail latency, the slowest 5% of requests."
        },
        {
            "title": "Subset \nAccuracy",
            "content": "Macro Avg. F1Score Macro Avg. Precision Macro Avg. Recall"
        },
        {
            "title": "Aggregated \nFalse Negatives",
            "content": "n a i l e ) s e t n i n e ( a e i i s a i r ( Metadata Accuracy ) i , b e Strength Expl. Accuracy"
        },
        {
            "title": "Correct \nMetadata Fields",
            "content": "d t r c i C ) t r t ( ) o ( e ) i n e ("
        },
        {
            "title": "Overall \nConfidence \nMAE",
            "content": "Avg. Response Time"
        },
        {
            "title": "Max Response \nTime",
            "content": "P95 Response Time 1 Amazon Nova Premier does not support invocation with on-demand throughput; instead, the cross-region inference profile us.amazon.nova-premier-v1:0 is used. 2 At the time of writing, Claude 3 Opus was available on AWS Bedrock only to enterprise customers due to high demand. Thus, the research uses Sonnet and Haiku models. 3 At the time of evaluation, larger Llama 3 models, such as the 11B and 90B variants, were not available on the AWS Bedrock platform within the European Union [45] regions due to concerns related to data privacy regulations, including the GDPR and the EU AI Act. Consequently, this research employs the smaller 3B model instead. Scenario - Complex Co-location and Anti-Affinity: This scenario evaluated expressiveness with mixed intent: prefer to be in the same region as the 'database' and 'cache' deployments, but avoid being on the same 'logging-agent' pods. The baseline node as scheduler, and complex podAntiAffinity rules, successfully placed all 6 pods podAffinity using the correctly. It avoided the minikube-m02 node (running the logging-agent) and distributed the pods across minikube-m03, m06, m07, m08, and m09. The intentdriven scheduler also placed all 6 pods correctly, successfully avoiding minikube-m02. Furthermore, it fulfilled both objectives by placing all pods on minikube-m03, satisfying the anti-affinity rule while also adhering to the preference to be near the 'cache' deployment. Scenario - Rapid Burst Colocation: This test validated the recent placements caches effectiveness during high-velocity creation of 20 replicas with the hint Collocate all pods from this deployment on the baseline (using hard single node. Both podAffinity rule) and intent-based scheduler the successfully achieved the ideal outcome, placing all 20 pods onto single distinct node. the Scenario - Quantitative Resource Preference: This scenario tested the full pipeline: parsing quantitative metadata from the hint at least 100 Gbps network resulting placement speed and executing preference. The baseline scheduler, configured with an equivalent soft nodeAffinity preference, placed only 1 of 6 pods (16.7%) on the correct high-speed node. This the default Kubernetes failure occurred because scheduler's other plugins, such as those for topology spreading, outweighed the soft preference. In contrast, the intent-driven scheduler successfully parsed the numerical requirement and placed all 6 of 6 pods (100%) on the correct node labeled with 100 Gbps network speed. This result validates the systems ability to and demonstrates that its high extender weight effectively prioritizes the users intent over other default scoring logic. specific numerical metadata interpret the baseline contradictory hard Scenario - Conflicting Intent Resolution: This test evaluated the systems behavior when given logically contradictory hints (collocate all pods on single node and you must also spread these pods across all scheduler, zones). As expected, constraints configured with (podAffinity and topologySpreadConstraints), failed to schedule the pod, which remained in Pending state. The intent-driven scheduler, however, processed the contradictory soft preferences. Its additive scoring logic resolved the conflict to find best-fit solution, allowing the pod to be scheduled. This outcome highlights the prototypes ability soft preferences, which is known behavior of the additive scoring model. resolve conflicting to The strength of this experimental design lies in their strategic coverage. The set of six scenarios effectively validates the prototype from multiple critical perspectives: Core Effectiveness: Scenarios A, B, and test the schedulers primary function, i.e., correctly placing pods based on common topology and resource requirements. Scenario E, in particular, validates the full pipeline from quantitative metadata parsing to placement. Expressiveness Claim: Scenario directly supports the papers main hypothesis by testing complex, mixed-intent request (affinity and anti-affinity) that is notoriously cumbersome to express using traditional Kubernetes rules. Technical Robustness: Scenario is crucial stress test. It validates specific architectural component (the recent placements cache) that was explicitly designed to handle real-world performance challenge (burst scaling), which is non-obvious but critical detail. System Limitations: Scenario demonstrates the systems boundaries. By explicitly testing known limitation (the additive scorers handling of conflicting intents), the evaluation moves beyond simple proofof-concept and provides rigorous assessment of the prototypes current capabilities. This combination of scenarios provides the necessary evidence to validate the schedulers effectiveness and demonstrate view of the systems limitations and future work. FIGURE 4. Example output of kubectl get pods command used to list all pods across namespaces in the Minikube testbed. Similar listings were captured during evaluation to validate the correctness and quality of the resulting pod allocations. VI. CONCLUSIONS core premise of this work is the reduction of cognitive overhead, which became immediately apparent during the configuration of the baseline experiments (as presented in Section IV.B). For instance, achieving the baseline behaviors in experimental Scenarios and required complex, multi-level YAML structures. These baselines relied on Kubernetes default directives, such as combination of topologySpreadConstraints podAffinity and podAntiAffinity, respectively. Such configurations are verbose, difficult to author correctly, and prone to error. In contrast, the prototype required only single allocation-hint annotation (e.g., spread across zones), demonstrating significant and practical simplification in expressing user intent. and a"
        },
        {
            "title": "The  development  and  evaluation  of",
            "content": "the semantic scheduling prototype revealed several key insights into integrating LLMs within latency-sensitive infrastructure systems. First, while LLMs demonstrated strong capability in into structured scheduling directives, their decoding reliability remains central challenge. Even small misinterpretations of intents or metadata can lead to significant scheduling deviations translating natural-language hints"
        },
        {
            "title": "Kubernetes Manifest with Allocation Hint",
            "content": "Objective / Metric / Results"
        },
        {
            "title": "TABLE IV \nEVALUATION SCENARIOS",
            "content": "g a S apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-a-baseline} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: {labels: {app: web}} spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: web containers: [{name: nginx, image: nginx:latest}] apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-b-baseline} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: {labels: {app: web}} spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: hardware operator: In values: - gpu containers: [{name: nginx, image: nginx:latest}] apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-a-hint} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: spread these pods evenly across all available zones for high availability spec: containers: [{name: nginx, image: nginx:latest}] Objective: To evaluate the scheduler's ability to fulfill highavailability policy. Metric: The standard deviation of the pod count per zone. lower value signifies more even and superior distribution. Result: Both, the baseline and intent-driven schedulers, achieved perfect 3:3 split across the two zones, successfully fulfilling the spreading objective. apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-b-hint} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: this is critical ML training job, it must run on nodes with GPUs spec: containers: [{name: nginx, image: nginx:latest}] Objective: To test the system's handling of resource preferences, which aligns with the prototype's implementation of the softaffinity /prioritize verb. Metric: The percentage of pods successfully placed on nodes labeled as having GPUs. Result: Both, the baseline and intent-driven schedulers, placed 6 out of 6 pods (100%) on the minikube-m02 node. This demonstrates that it correctly interpreted the \"must run\" hint as strong preference and its scoring logic. apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-c-hint} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: prefer to be in the same region as the 'database' and 'cache' deployments, but avoid being on the same node as the 'logging-agent' pods. spec: containers: [{name: nginx, image: nginx:latest}] apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-c-baseline} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: {labels: {app: web}} spec: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - database - cache topologyKey: \"topology.kubernetes.io/region\" podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchLabels: app: logging-agent topologyKey: \"kubernetes.io/hostname\" containers: [{name: nginx, image: nginx:latest}] Objective: To evaluate the system's expressiveness and its ability to handle multiple, mixed intents, which is expected to be strength of the natural language approach. Metric: Evaluate of whether the placement decisions successfully adhered to both the region affinity and node anti-affinity objectives. Result (baseline): The baseline scheduler placed all 6 pods correctly, avoiding the logging-agent node (minikube-m02) and distributing them across minikube-m03, m06, m07, m08, and m09. Result (intent-driven): The intent-driven scheduler also placed all 6 pods correctly, avoiding minikube-m02. It successfully fulfilled both objectives by placing all pods on minikube-m03 (near the cache deployment). apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-d-baseline} spec: replicas: 20 selector: {matchLabels: {app: web}} template: metadata: {labels: {app: web}} spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: web topologyKey: \"kubernetes.io/hostname\" containers: [{name: nginx, image: nginx:latest}] apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-e-baseline} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: {labels: {app: web}} spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: network-gbps operator: Gt values: - \"99\" # (>100 Gbps) containers: [{name: nginx, image: nginx:latest}] t l 3 e e apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-d-hint} spec: replicas: 20 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: Collocate all pods from this deployment on single node. spec: containers: [{name: nginx, image: nginx:latest}] Objective: To test the effectiveness of the recent placements cache in ensuring colocation logic works correctly during high-velocity pod creation bursts, compensating for main cache update latency. Metric: The total number of distinct nodes used to place all replicas of the deployment. An ideal outcome is single node. Result: Both the baseline and intent-based schedulers successfully achieved the ideal outcome of single distinct node, placing all 20 pods onto the same node. apiVersion: apps/v1 kind: Deployment metadata: {name: test-scenario-e-hint} spec: replicas: 6 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: This is high-bandwidth job, please place on nodes with at least 100Gbps network speed. spec: containers: [{name: nginx, image: nginx:latest}] Objective: To evaluate the full pipeline, from the LLM's ability to parse specific numerical metadata (e.g., prefer_network_speed) to the scoring logic's ability to use that value correctly. Metric: The count of pods successfully placed on nodes that are labeled in the testbed as having network speed of 100Gbps or more. Result (baseline): The baseline scheduler placed only 1 of 6 pods on the preferred node. This demonstrates that the default Kubernetes scheduler's other plugins (like topology spreading) outweighed the soft nodeAffinity preference. (Notably, version of deployment manifest with requiredDuringSchedulingIgnoredDuringExecution works as expected.) Result (intent-based): The intent-driven scheduler placed 6 of 6 pods on the correct node with network Gbps set to 100. l T - r c 1 n A u R - i e 2 n A - A n a - e o - i e s d R - r c r e i i u - r c S"
        },
        {
            "title": "Kubernetes Manifest with Allocation Hint",
            "content": "Objective / Metric / Results t s n I t f - o n apiVersion: apps/v1 kind: Deployment metadata: name: test-scenario-f-hint spec: replicas: 1 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} annotations: allocation_hint: For high performance, collocate all pods on single node. For high availability, you must also spread these pods across all zones. spec: containers: - name: nginx image: nginx:latest apiVersion: apps/v1 kind: Deployment metadata: name: test-scenario-f-baseline spec: replicas: 1 selector: {matchLabels: {app: web}} template: metadata: labels: {app: web} spec: affinity: podAffinity: # Hard constraint for colocation (from Scenario D) requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app: web topologyKey: \"kubernetes.io/hostname\" topologySpreadConstraints: # Hard constraint for spreading (from Scenario A) - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: web containers: - name: nginx image: nginx:latest Objective: To test the behavior of the current simple additive scoring model when faced with directly conflicting user intents. Metric: The final placement distribution is observed to determine which of the two conflicting intents (colocation or spreading) the additive logic ultimately favored. Result (baseline): The pod is stuck in Pending state and this is the expected outcome. The baseline deployment manifest is configured with two contradictory hard constraints. Result (intent-based): The test-scenario deployments manifest provides contradictory soft preferences; however, the prototype's additive scoring logic is designed to resolve such conflicts by summing the weighted scores. It finds best-fit solution and schedules the pod. This is known limitation of the prototype, which future cost-function-based optimizer would address. 1 The GPU count in Kubernetes is specified as an extended resource nvidia.com/gpu as standard. The nodeAffinity mechanism (both preferredDuringScheduling... and requiredDuringScheduling...) only works with node labels, and not with node resources. Therefore, in order to simulate soft-affinity, custom hardware=gpu label has been applied to the node minikube-m02. 2 Scenario included the deployment of pods from the database (on minikube-m05), cache (on minikube-m03), and logging-agent (on minikube-m02) deployments. 3 Scenario involved setting the network bandwidth to 10 Gbps on all Minikube nodes, except for node minikube-m09, where it was set to 100 Gbps to meet the test scenario requirements. The need for deterministic, verifiable parsing underscores that LLMs should paired with strict schema validation, normalization layers, and human-readable audit trails to ensure operational safety in production environments. translating natural-language hints"
        },
        {
            "title": "The  development  and  evaluation  of",
            "content": "the semantic scheduling prototype revealed several key insights into integrating LLMs within latency-sensitive infrastructure systems. First, while LLMs demonstrated strong capability into structured in scheduling directives, their decoding reliability remains central challenge. Even small misinterpretations of intents or metadata can lead to significant scheduling deviations. The need for deterministic, verifiable parsing underscores that LLMs should paired with strict schema validation, normalization layers, and human-readable audit trails to ensure operational safety in production environments. The investigation has demonstrated that effective prompt design and schema constraints are as critical as model selection. Incremental improvements in prompt phrasing, output examples, and metadata formatting directly reduced false positives and extraction errors. This experience reinforced that LLM integration is as much an engineering discipline as modeling challenge; robustness depends on careful pipeline design, not model capability alone. The project also highlighted the importance of intent analysis and architectural separation between scheduling execution. The prototype exposed fundamental differences between research feasibility and production readiness: (i) embedding an LLM call within the introduced unacceptable synchronous latency for real-world use, and (ii) the single-threaded Flask server, static scoring logic, and lack of fault tolerance are acceptable for evaluation but unsuitable for real workloads. Transitioning to scalable, asynchronous /prioritize path and architecture incorporating caching, service concurrency, and adaptive fallback mechanisms will be essential for deployment in real clusters. The below list details the most crucial limitations in the current design: Single-threaded Python Flask Server: While the prototype performed well during controlled evaluations, its Python Flask-based implementation is inherently single-threaded, making it unsuitable for productionscale workloads. Under concurrent scheduling requests, this architecture would quickly become bottleneck, as each request must wait for the current one to complete before being processed. This serialization severely limits throughput and scalability, particularly in highload cluster environments where multiple scheduling operations occur in parallel. production-ready deployment would require migrating to multithreaded or asynchronous framework like FastAPI or aiohttp, and also to set up load balancer to provide resilience. Intent Classification Errors: The schedulers behavior depends critically on the correct identification of presence of an intent. While top-tier models such as Amazon Nova Premier and Mistral Pixtral Large achieved high subset accuracy (97.45% and 95.86%, respectively), their performance is not flawless. The 918 aggregated false negatives represent cases where users hint (e.g., spread_zones) was completely missed, leading to violations of the intended scheduling policy. Conversely, the 10-17 aggregated false positives correspond to hallucinated intents, where the model inferred preferences not expressed by the user. Such misclassifications can result in unpredictable or suboptimal scheduling outcomes, as the system might apply unintended placement preferences or ignore critical user directives. Metadata Extraction Errors: Even when an intent is correctly identified, its associated metadata can be misinterpreted or extracted incorrectly. As shown in Table IV, the best-performing models on this metric, i.e., Claude 3 Sonnet (95.22%) and Amazon Nova Pro (94.40%), still exhibit nontrivial error rates. This repeatedly during prompt limitation, observed refinement, becomes particularly pronounced with complex list extractions. For instance, hint such as avoid us-east-1a and us-east-1b might be erroneously parsed as ['us-east-1'], preventing the scheduler from excluding the specified zones. Similarly, an implicit hint like high memory may be incorrectly assigned default numeric value (e.g., 1.0), rendering the resulting prefer_memory_gb intent ineffective. Such metadata errors can silently degrade scheduling accuracy, as the scoring logic depends on precise, structured values. Unreliable Strength Interpretation: The most significant decoding weakness, as highlighted in Table IV, is unreliable strength interpretation. The Overall Strength Accuracy metric measuring how well models map linguistic modifiers like must (strength = 1.5) or prefer (strength = 0.5) proved the most challenging. Even the best-performing models, Nova Premier and Mistral Pixtral Large, achieved only 71.79% accuracy, while the chosen Nova Pro model reached 67.85%. Consequently, in roughly one-third of cases, the model misinterprets the users intended priority. critical directive (strength 1.5) misclassified as standard preference (strength 1.0) would cause the scheduler to treat it as weak suggestion, allowing less important factors to dominate the decision. This systematic degradation of the intent strength undermines user expectations and highlights the need for explicit strength calibration or post-processing normalization in future iterations. Overall, these lessons demonstrate that while LLMdriven intent recognition holds strong promise for advancing declarative scheduling, reliability, observability, and performance isolation must remain first-class design goals. Future iterations should focus on modularizing these components, enabling general-purpose, platform-agnostic scheduling augmentation built on stable, interpretable NLP foundations. The future works should cover the following: Intent Analysis Prior to Scoring: Although the prototype demonstrates the feasibility of translating natural-language hints into scheduling directives, its architecture introduces significant performance bottleneck. The current design performs synchronous LLM call within the /prioritize handler, adding P95 latencies ranging from 0.87s to 5.34s, depending on the model. In production, this delay would be unacceptable during scheduling. production-ready solution should to be effectively implemented via move the LLM call outside the scheduling loop, likely most MutatingAdmissionWebhook that analyzes intents at pod creation, stores the structured JSON output in an annotation, and allows the scheduler extender to read it locally during /prioritize. This would remove the synchronous dependency, eliminating latency and making semantic scheduling viable for high-throughput clusters. Expanding Evaluation Dataset with Compound Multi-Objective Prompts: The current 314-prompt evaluation dataset primarily covers cases translating into one or two intent objects. Future work should extend this dataset to include multi-level and conflicting scheduling intents, reflecting real-world scenarios where objectives such as co-location and fault tolerance interact. Incorporating such complex and contradictory prompts will better test the LLMs reasoning and consistency, highlighting the need for larger, contextaware models capable of resolving competing directives and handling richer semantic structures. This work has already commenced with the creation of the v2 dataset, available on the Model Hub [37] (file: schedule-intentevaluation-v2.json), which two new categories: Multi-Objective Optimization (focused on encoding multiple Intents) and Contradictory Prompt (providing opposing hints). introduces Evaluation in Parallel Execution Environment: notable limitation of the current prototype is its execution environment. While the Minikube testbed simulates multi-node cluster, the core scoring logic within the Extender Service operates sequentially due to Flask's single-threaded nature. This means that even if multiple pod scheduling requests arrive concurrently at the extender, they are processed one after another. This sequential processing does not accurately reflect production scenario where busy scheduler extender handles numerous /prioritize requests in parallel. Future work should involve reimplementing the extender logic using multi-threaded server capable of parallel request handling. Evaluating the system under such concurrent load is crucial, as it will likely necessitate the introduction of sophisticated locking mechanisms. Specifically, concurrent scoring of pods belonging to the same deployment (e.g., during scale-up) could lead to race conditions when updating or querying the recent placements cache or when calculating dynamic scores for intents. Therefore, implementing appropriate concurrency controls will be essential for ensuring the correctness and scalability of the intent-based scoring logic in realistic, highthroughput environment. spread_ and colocate_ General-Purpose Scheduling Grammar: key direction for future research is to decouple the core scheduling logic from its current tight integration with it defining should be format Kubernetes. In the prototype, Intent definitions and scoring logic depend on Kubernetes-specific topology labels (e.g., zone, rack) and its scheduler extender API. To generalize this concept, universal scheduling introduced - an abstract grammar intermediary soft-affinity for scheduling preferences. The LLM would translate this grammar, while natural-language hints into into platform-specific adapters would convert concrete directives for systems such as Kubernetes, Mesos, or SLURM. This approach would decouple NLP-based intent recognition from any single platform, enabling general-purpose framework for semantic workload orchestration across diverse environments. In summary, this work successfully validates the core premise that LLMs can bridge the semantic gap in cluster scheduling, demonstrating high accuracy in translating human intent into structured scheduling policies. The prototype proves the concept of semantic soft affinity, shifting the bottleneck from human configuration error to the engineering challenges of integration, specifically regarding synchronous latency and reliable metadata decoding. This research thus confirms the viability of intent-driven orchestration while clearly outlining the architectural and reliability engineering required to transition such system from novel prototype to production-grade infrastructure component. ACKNOWLEDGMENTS The authors wish to express their sincere gratitude to Standard Chartered Bank for its material support, which contributed significantly to the research and investigation. REFERENCES [1] Wang, Wei, Baochun Li, and Ben Liang. \"Dominant resource fairness in cloud computing systems with heterogeneous servers.\" In IEEE INFOCOM 2014-IEEE Conference on Computer Communications, pp. 583-591. IEEE, 2014. [2] Gog, Ionel, Malte Schwarzkopf, Adam Gleave, Robert NM Watson, and Steven Hand. \"Firmament: Fast, centralized cluster scheduling at scale.\" In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 99-115. 2016. [4] [3] Hindman, Benjamin, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D. Joseph, Randy Katz, Scott Shenker, and Ion Stoica. \"Mesos: platform for {Fine-Grained} resource sharing in the data center.\" In 8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11). 2011. Isard, Michael, Vijayan Prabhakaran, Jon Currey, Udi Wieder, Kunal Talwar, and Andrew Goldberg. \"Quincy: fair scheduling for distributed computing clusters.\" In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, pp. 261-276. 2009. [5] Mao, Hongzi, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. \"Resource management with deep reinforcement learning.\" In Proceedings of the 15th ACM workshop on hot topics in networks, pp. 50-56. 2016. [6] Mao, Hongzi, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh. \"Learning scheduling algorithms for data processing clusters.\" In Proceedings of the ACM special interest group on data communication, pp. 270-288. 2019. [7] Zeydan, Engin, and Yekta Turk. \"Recent advances in intent-based networking: survey.\" In 2020 IEEE 91st vehicular technology conference (VTC2020-Spring), pp. 1-5. IEEE, 2020. [8] Vavilapalli, Vinod Kumar, Arun C. Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans, Thomas Graves et al. \"Apache hadoop yarn: Yet resource negotiator.\" In Proceedings of the 4th annual Symposium on Cloud Computing, pp. 1-16. 2013. another [9] Verma, Abhishek, Luis Pedrosa, Madhukar Korupolu, David Oppenheimer, Eric Tune, and John Wilkes. \"Large-scale cluster management at Google with Borg.\" In Proceedings of the tenth european conference on computer systems, pp. 1-17. 2015. [10] Wang, Xingyao, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan et al. \"Opendevin: An open platform for ai software developers as generalist agents.\" arXiv preprint arXiv:2407.16741 3 (2024). [11] Jaillet, Patrick, Jiashuo Jiang, Konstantina Mellou, Marco Molinaro, Chara Podimata, and Zijie Zhou. \"Online Scheduling for LLM Inference with KV Cache Constraints.\" arXiv preprint arXiv:2502.07115 (2025). [12] Androutsopoulos, Ion, Graeme D. Ritchie, and Peter Thanisch. \"Natural language interfaces to databases - an introduction.\" Natural language engineering 1, no. 1 (1995): 29-81. [13] Silvander, Johan, Krzysztof Wnuk, and Mikael Svahnberg. intentdriven systems.\" IET review on \"Systematic literature Software 14, no. 4 (2020): 345-357. [14] Clemm, Alexander, Laurent Ciavaglia, L. Zambenedetti Granville, and Jeff Tantsura. \"Rfc 9315: Intent-based networking-concepts and definitions.\" (2022). [15] Pan Xuan, Xu Sihan, Cai Xiangrui, Wen Yanlong, Yuan Xiaojie. \"Survey on Deep Learning Based Natural Language Interface to Database.\" Journal of Computer Research and Development, 2021, 58(9): 1925-1950. [16] Tageldien, Marwa, Bassant Selim, and Lokman Sboui. \"Large Language Models in Intent-Based Networking: Comprehensive Survey Across International Intent Lifecycle.\" Telecommunications Conference (ITC-Egypt), pp. 810-817. IEEE, 2025. In 2025 the [17] Wang, Bailin, Zi Wang, Xuezhi Wang, Yuan Cao, Rif Saurous, and Yoon Kim. \"Grammar prompting for domain-specific language generation with in Neural Information Processing Systems 36 (2023): 65030-65055. language models.\" Advances large [18] Brodimas, Dimitrios, Kostis Trantzas, Besiana Agko, Georgios Christos Tziavas, Christos Tranoris, Spyros Denazis, and Alexios Birbas. \"Towards intent-based network management for the 6g system adopting multimodal generative ai.\" In 2024 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit), pp. 848-853. IEEE, 2024. [19] Chowdhary, Ankur, Abdulhakim Sabur, Neha Vadnere, and Dijiang Huang. \"Intent-driven security policy management for softwaredefined systems.\" IEEE Transactions on Network and Service Management 19, no. 4 (2022): 5208-5223. [20] Martini, Barbara, Molka Gharbaoui, and Piero Castoldi. \"Intent-based network slicing for SDN vertical services with assurance: Context, design and preliminary experiments.\" Future Generation Computer Systems 142 (2023): 101-116. [21] Pang, Lei, Chungang Yang, Danyang Chen, Yanbo Song, and Mohsen Guizani. \"A survey on intent-driven networks.\" Ieee Access 8 (2020): 22862-22873. [22] Singh, Abhishek. \"Intent-Based Networking in Multi-Cloud Environments.\" Journal of Engineering and Applied Sciences Technology. SRC/JEAST-411. DOI: doi. org/10.47363/JEAST/2025 (7) 288 (2025): 2-7. [23] Saha, Barun Kumar, Luca Haab, and Åukasz Podleski. \"Intent-based industrial network management using natural language instructions.\" In 2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT), pp. 1-6. IEEE, 2022. [24] Mcnamara, Joseph, Daniel Camps-Mur, Meysam Goodarzi, Hilary Frank, Lorena Chinchilla-Romero, FerrÃ¡n CaÃ±ellas, Adriana FernÃ¡ndez-FernÃ¡ndez, and Shuangyi Yan. \"Nlp powered intent based network management for private 5g networks.\" IEEE Access 11 (2023): 36642-36657. [45] \"Llama 4 Acceptable Use Policy.\" Meta.com. April 5, 2025. https://github.com/meta-llama/llamamodels/blob/main/models/llama4/USE_POLICY.md. [25] Deng, Naihao, Yulong Chen, and Yue Zhang. \"Recent advances in text-to-SQL: survey of what we have and what we expect.\" arXiv preprint arXiv:2208.10099 (2022). [26] Glass, Michael, Mustafa Eyceoz, Dharmashankar Subramanian, Gaetano Rossiello, Long Vu, and Alfio Gliozzo. \"Extractive schema linking for text-to-sql.\" arXiv preprint arXiv:2501.17174 (2025). [27] Hwang, Wonseok, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. \"A comprehensive exploration on wikisql with table-aware word contextualization.\" arXiv preprint arXiv:1902.01069 (2019). [28] Li, Boyan, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, and Yuyu Luo. \"Alpha-sql: Zero-shot text-to-sql using monte carlo tree search.\" arXiv preprint arXiv:2502.17248 (2025). [29] Xu, Xiaojun, Chang Liu, and Dawn Song. \"Sqlnet: Generating structured queries from natural language without reinforcement learning.\" arXiv preprint arXiv:1711.04436 (2017). [30] Zhong, Victor, Caiming Xiong, and Richard Socher. \"Seq2sql: language using Generating structured queries reinforcement learning.\" arXiv preprint arXiv:1709.00103 (2017). [31] Beloglazov, Anton, and Rajkumar Buyya. \"Energy efficient resource management in virtualized cloud data centers.\" In 2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing, pp. 826-831. IEEE, 2010. from natural [32] Chen, Jing, Tiantian Du, and Gongyi Xiao. \"A multi-objective optimization for resource allocation of emergent demands in cloud computing.\" Journal of Cloud Computing 10, no. 1 (2021): 20. [33] Khallouli, Wael, and Jingwei Huang. \"Cluster resource scheduling in cloud computing: literature review and research challenges.\" The Journal of supercomputing 78, no. 5 (2022): 6898-6943. [34] Sissodia, Rajeshwari, Man Mohan Singh Rauthan, and Kanchan Naithani. \"A survey on multi-objective tasks and workflow scheduling algorithms in cloud computing.\" International Journal of Cloud Applications and Computing (IJCAC) 12, no. 1 (2022): 1-16. [35] \"Kube-Scheduler Configuration V1.\" Kubernetes.io. September 4, 2025. https://kubernetes.io/docs/reference/config-api/kube-schedulerconfig.v1/. [36] Poon, Hoifung, and Pedro Domingos. \"Unsupervised semantic parsing.\" In Proceedings of the 2009 conference on empirical methods in natural language processing, pp. 1-10. 2009. [37] Sliwko, Leszek. \"Poc-Schedule-Intent-Evaluation.\" Huggingface.co. October 25, 2025. https://huggingface.co/datasets/lsliwko/pocschedule-intent-evaluation. [38] Alam, Mansaf, Kashish Ara Shakil, and Shuchi Sethi. \"Analysis and clustering of workload in google cluster trace based on resource usage.\" In 2016 IEEE Intl conference on computational science and engineering (CSE) and IEEE Intl conference on embedded and ubiquitous computing (EUC) and 15th Intl symposium on distributed computing and applications for business engineering (DCABES), pp. 740-747. IEEE, 2016. [39] Ye, Zhisheng, Wei Gao, Qinghao Hu, Peng Sun, Xiaolin Wang, Yingwei Luo, Tianwei Zhang, and Yonggang Wen. \"Deep learning workload scheduling in gpu datacenters: survey.\" ACM Computing Surveys 56, no. 6 (2024): 1-38. [40] Sliwko, Leszek. \"Cluster Workload Allocation: Predictive Approach Leveraging Machine Learning Efficiency.\" IEEE Access (2024). [41] Veremyev, Alexander, Alexander Semenov, Eduardo L. Pasiliao, and Vladimir Boginski. \"Graph-based exploration and clustering analysis of semantic spaces.\" Applied Network Science 4, no. 1 (2019): 104. [42] Mishra, Asit K., Joseph L. Hellerstein, Walfredo Cirne, and Chita R. Das. \"Towards characterizing cloud backend workloads: insights from clusters.\" ACM SIGMETRICS Performance google Evaluation Review 37, no. 4 (2010): 34-41. compute [43] Zhou, Guangyao, Wenhong Tian, Rajkumar Buyya, Ruini Xue, and Liang Song. \"Deep reinforcement learning-based methods for resource scheduling in cloud computing: review and future directions.\" Artificial Intelligence Review 57, no. 5 (2024): 124. [44] Sanjalawe, Yousef, Salam Al-Emari, Salam Fraihat, and Sharif Makhadmeh. \"AI-driven job scheduling in cloud computing: comprehensive review.\" Artificial Intelligence Review 58, no. 7 (2025): 197."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Opole University of Technology",
        "Standard Chartered Bank"
    ]
}