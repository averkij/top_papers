{
    "paper_title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models",
    "authors": [
        "Zae Myung Kim",
        "Chanwoo Park",
        "Vipul Raheja",
        "Dongyeop Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 7 5 1 0 2 . 4 0 5 2 : r Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models Zae Myung Kim1, Chanwoo Park2, Vipul Raheja3, Dongyeop Kang1, 1University of Minnesota, 2MIT, 3Grammarly {kim01756,dongyeop}@umn.edu, cpark97@mit.edu, raheja@grammarly.com"
        },
        {
            "title": "Abstract",
            "content": "Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), framework that addresses these challenges by integrating meta-reward model that dynamically refines the reward models prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward models prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPOs meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in rewardbased RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared."
        },
        {
            "title": "Introduction",
            "content": "Good thinkers must use another high-level thinking skill, namely, analysis and awareness of ones own thinkingor metacognition [Buckley et al., 2015, Lord et al., 1979]. Recent advancements in reinforcement learning (RL) for large language model (LLM) training have marked shift from tasks that prioritize human-like response generation [Ziegler et al., 2020, Stiennon et al., 2020, Ouyang et al., 2022a] to those that emphasize structured reasoning, such as mathematics and programming [Shao et al., 2024a, DeepSeek-AI et al., 2025]. Traditionally, human-aligned answering relies on reward models trained using binary comparison data, whereas structured reasoning tasks focus more on verifying the correctness of final answers or logical processes based on objective ground truth. However, many real-world challenges demand synthesis of both approachesrequiring models to integrate human-aligned judgment with rigorous reasoning. This introduces significant complexity, as such tasks often lack objectively verifiable golden answers, yet still necessitate coherent and justifiable reasoning. scalable approach to subjective evaluation is to use an LLM as judgean approach commonly referred to as reinforcement learning with AI feedback (RLAIF)where fixed prompt is used to assess model performance on specific tasks. However, this method introduces three key challenges. First, calibration: effective scoring requires evaluation criteria that are sufficiently detailed and carefully calibrated to the quality and variability of the policy models outputs. Providing highly 1https://github.com/minnesotanlp/mpo Preprint. Work in progress. granular feedback on poor responses may be unproductive, while overly coarse or simplistic scoring may fail to guide meaningful improvements in model behavior. Second, reward hacking: it is well-known that when LLMs are trained with imperfect reward functionsas is often the case with LLM-as-a-judge or reward modelsthey may learn to exploit shortcuts that maximize reward without genuinely improving output quality or alignment with human values [Amodei et al., 2016b, Everitt et al., 2021, Langosco et al., 2022, Pan et al., 2022]. These shortcuts can lead to responses that are formally rewarded yet misaligned with human intent or utility. Third, prompt-engineering overhead: substantial manual effort is often required for prompt engineering when generating training data for reward models or employing LLMs as proxy reward functions. This process introduces scalability bottlenecks and limits automation in alignment pipelines. To address these issues, this work introduces Meta Policy Optimization (MPO) (Figure 1), framework that augments existing reward-based RLAIF pipeline by adding meta reward model. Unlike traditional reward model that simply scores the policys output based on fixed prompt, the meta reward model monitors the evolving training landscape and adjusts or refines the prompt used by the standard reward model. Our design of MPO is inspired by the psychological concept of metacognitionthe process of becoming aware of and reflecting on ones own thinking [Flavell, 1979] and its central role in evaluative thinking, reflective, evidence-driven cognitive process that involves questioning, analyzing, and interpreting information to guide decision-making and continuous learning [Buckley et al., 2015]. Consider university classroom as an analogy: the policy model takes the role of student, while the standard reward model serves as junior instructor who follows fixed grading rubric. Periodically, senior instructor (the meta reward model) joins to assess the broader situation: the students performance, the junior instructors rubric, and the assigned grades. If the student discovers loopholes in the rubric and starts submitting subpar work for high marks with minimal effort, the senior instructorwho sees the whole picturecan identify this pattern and advise the junior instructor on adjusting the rubric for future assignments. Figure 1: In standard RLAIF, the reward model used during proximal policy optimization (PPO) remains fixed throughout RL alignment. In contrast, MPO framework (in green) introduces meta reward model that dynamically evolves the reward model based on the current training context, including the task prompt, sampled generations with associated scores, and the latest evaluation prompt. MPO leverages this contextual information to iteratively refine the evaluation prompt, enabling more adaptive and effective alignment. Metacognitive awareness and control are essential to this process, enabling individuals to monitor reasoning, detect biases, and refine strategies through task assessment and reflection. Cognitive science research further supports this by showing that deliberate reflection fosters deeper, more robust learning [McCormick, 2003, Metcalfe and Kornell, 2005, Veenman et al., 2006, Efklides, 2006]. By carrying these principles into RL-based alignment for LLMs, we unlock several advantages: Greater stability in RLAIF training: MPO dynamically adjusts reward model prompts to deliver context-sensitive scoring criteria based on the policy models performance, while also mitigating exploitation of static reward behaviors common in fixed-prompt setups. Reduced prompt engineering burden: MPO iteratively refines and expands existing prompts within single training cycle, eliminating repeated manual intervention. Flexible and general framework: MPO can be used across diverse tasks (see Section 3.3) without major modifications to the training procedure."
        },
        {
            "title": "2 Meta Policy Optimization",
            "content": "As noted in the introduction, our MPO approach draws inspiration from the fields of metacognition and evaluation. We elaborate on this conceptual motivation, then present formal reinforcement learning formulation of MPO, followed by detailed description of its implementation steps. 2 2.1 Evaluative Thinking Evaluative thinking (ET) [Buckley et al., 2015] is the intentional process of analyzing, interpreting, and assessing information to support thoughtful decision-making, playing critical role in evaluation capacity building (ECB). However, current reward models in RL for LLM lack such metacognitive control. These models are typically trained on static human preference datasets or guided by fixed rubric prompts, and they remain unchanged during training. As the policy improves, the static reward model tends to collapse nuanced improvements into coarse labelgood enoughallowing early blind spots to persist and go uncorrected. Motivated by the relationship between Evaluative Thinking (ET) and Evaluation Capacity Building (ECB)where ET supports ECB by enhancing metacognitionwe propose Meta Reward Model (MRM) that guides the reward model to develop evaluative metacognition and become more effective scorer. Specifically, the MRM follows the core principles of ET: evidence gathering, questioning, and reflective judgment (see Section 2.3 and Figure 3). Our MPO framework operationalizes ET by enabling the MRM to refine RMs observational partitions over time. In this setup, the reward model improves through on-policy learning driven by the metacognitive signals of the MRM. Figure 2: Dimensions of Evaluative Thinking: Depth and Breadth. Remark 1 (Depth and Breadth of ET). We posit that Evaluative Thinking (ET) in the context of reinforcement learning for large language models (LLMs) can be understood along two orthogonal dimensions: depth and breadth. This framework echoes Edward de Bonos celebrated distinction between vertical and lateral thinking [De Bono, 1971]. Intuitively, depth corresponds to sequential, instance-specific reasoningreflecting the degree of logical inference and deliberation required to evaluate single case. In contrast, breadth captures the ability to generalize across varied instances, recognizing recurring patterns or abstract principles that inform evaluation in novel contexts. As illustrated in Figure 2, tasks such as mathematical reasoning exemplify vertical (deep) thinking, as they involve multi-step, case-specific deductions. In contrast, open-ended tasks like essay writing align more with lateral (broad) thinking, requiring evaluative generalization across diverse prompts. To empirically explore these dimensions, we evaluate four representative tasksmathematical reasoning, ethical reasoning, summarization, and essay writingeach occupying distinct region of the ET depthbreadth space in our experiments. 2.2 Time-Varying Observations and Rewards by Evolving Reward Model While ECB through ET provides foundation for improving the RM, it does not fully capture the dynamics of RL, where learning is driven by signals from an ECB-enhanced RM. To address this gap, we introduce mathematical framework that formalizes how the RM evolves under the influence of ET and how this evolving RM can be integrated into the training process of LLMs. Formally, consider discrete state space S, an action space (both finite or countable), and transition kernel (s s, a) specifying the probability of transitioning to state given action from state S. We define golden reward function : assigning numerical reward to each state. Remark 2. In LLM-RL settings, state represents the textual history (e.g., the sequence of tokens generated so far), and an action is the selection of the next token. The subsequent state is thus naturally expressed as (s, a), the concatenation of the history and the chosen token. Practically, assigning precise numerical reward r(s) is challenging due to subjective criteria such as coherence or relevance. Hence, evaluators typically provide approximate feedback. This scenario aligns well with partially observable Markov decision process (POMDP) structure, where observations from evaluators form partial, aggregated view of the underlying states. Remark 3. Consider an LLM tasked with generating responses in conversational setting. At first, AI or human feedback might only broadly categorize responses as good, neutral, or bad. Over time, however, evaluators might introduce finer distinctions, such as coherent but impolite, polite but irrelevant, and relevant but verbose. Mathematically, this corresponds to refining the 3 granularity of observation sets that the LLM receives, providing increasingly precise and informative feedback. This is directly related to evolving reward model in our case - by ECB-ed RM by ET, they can provide finer score which will be closer to the golden reward model. 2.2.1 Refining Observation Partitions Over Time Let the set of possible observations at each discrete phase = 1, 2, 3, . . . be denoted by Ωt. Define collection of partitions {Oo,t}oΩt of the state space S, satisfying: (cid:91) Oo,t Oo,t = , for = o, and Oo,t = S. oΩt Each partition represents labeling of states by evaluators, where Oo,t contains states labeled as observation at time t. If the true state at phase is s, the agent deterministically observes label such that Oo,t. Refinement property. To formally capture increasingly precise feedback, assume each partition refines the previous one. Precisely, for every Oo,t, there exists some Oo,t1 satisfying: Oo,t Oo,t1. This means partitions at each subsequent phase are finer than the previous ones, never coarser, which is also consistent with our MPO setupmeta reward model guides the reward model with ET to have finer rubrics as training goes. 2.2.2 Observation-Based Rewards Rather than providing immediate rewards based on exact state information, define reward based on observation labels. When observing Ωt at phase t, the agent receives an averaged reward: Rt(o) = EsOo,t[r(s)] = 1 Oo,t (cid:88) r(s). sOo,t In our MPO setup, coarser rubric leads to rewards being averaged over all responses mapped to the same rubric category as well. 2.2.3 Meta Policy Optimization Framework Employing single static LLM scorer corresponds to fixed observation partition, potentially too coarse to accurately capture nuanced reward differences. Such coarse partitioning groups many distinct states (e.g., texts or dialog histories) into overly coarse categories, leading to averaged rewards of the form: R(o) = EsOo[r(s)], which obscure state-specific details crucial for precise policy optimization. Consequently, static scoring mechanism struggles to converge towards the ground-truth reward r(s), limiting its ability to capture subtle, high-dimensional, or evolving reward criteria. In contrast, meta policy framework addresses this limitation by adaptively refining observation partitions over successive iterations. Formally, the meta-rewarding process introduces progressively finer partitions: {Oo,t}oΩt, where Oo,t Oo,t1 for some o, > 1. This iterative refinement enables increasingly discriminative reward signals: Rt(o) = EsOo,t[r(s)], that better capture subtle variations in the state space. By adaptively partitioning the observation setssplitting larger, coarse categories into smaller, targeted subsets as the policys performance improves or as new dimensions of evaluation emergethe meta-rewarding evaluator progressively sharpens the granularity and efficacy of reinforcement signals. This dynamic refinement is particularly beneficial in complex LLM-driven tasks, ensuring that policy updates become more targeted and aligned with nuanced performance improvements, ultimately facilitating advanced policy learning. Figure 3: Meta Policy Optimization steps. 2.3 Implementation: Meta Reward Model The MPO framework is implemented by introducing meta reward model (MRM)conceptually, senior instructor in our academic analogy. As illustrated in Figure 3, the MRM oversees the broader training context and issues targeted refinements to the evaluation rubric (or prompt) used by the reward model (RM), which plays the role of junior instructor. These refinements are informed by inputs drawn from three sources: the prompt dataset, the policy model (the student), and the reward model itself. Specifically, at every fixed training batch steps, the MRM performs an MPO step by processing contextual input sources through general meta-level prompts, which are designed to be task-agnostic and applicable across all tasks. From the prompt dataset, the MRM processes the task description, set of task-specific prompt instructions, andwhen availablethe corresponding reference solutions. It also receives the policy models generated responses to these prompts, the current version of the evaluation rubric, and the scores assigned by the RM using that rubric. Leveraging this rich contextual input, the MRM identifies weaknesses or gaps in the current rubric and prescribes increasingly fine-grained and targeted evaluation criteria. This refinement process is triggered every fixed training batches and follows the three-stage procedure illustrated in Figure 3. At each stage, the MRM is prompted with meta-level instructions that are designed to be broadly applicable across range of tasks. 2.3.1 Meta-Analysis The first step of MPO involves processing the full input context to assess whether the RMs scoring is accurate and reliableparticularly in cases where the student LLM may exploit loopholes of the RM through reward hacking. The MRM evaluates whether the current scoring criteria are sufficiently comprehensive and detailed, and prescribes necessary adjustments to improve evaluation quality and robustness. This step is particularly crucial, as it serves to detect loopholes in the RMs evaluation logic early on. For instance, consider the following real example observed during RL training: reward hacking example User: You will act as an English writer and compose either an essay or story depending on the instruction given below. Your essay should be no more than 350 words. Instruction: negative impact on society that outweighs any benefits it provides. evidence and examples to support your argument. Student Response: Evaluation Score: Im ready when you are. Please provide the essay instruction. 3 Write an essay that argues the claim that the U. S. Drug Enforcement Agency has Your essay should include 5 In this example, the policy model discovers that generating degenerate responselike the one shownyields score of 3 out of 5 from the RM, despite being clearly misaligned with the task objective. This is an example of reward hacking, where the model exploits weaknesses in the scoring system to earn relatively high points without fulfilling the task faithfully. Meta-Analysis The junior instructors assigned score of 3 out of 5 points for the students response is misleading given the provided student generation and the task requirements. Lets break down the analysis. Students Response: The students response is entirely missing. The students generation reads, Im ready when you are. Please provide the essay instruction. This indicates that the student did not actually write the essay as instructed, but rather responded with statement that is irrelevant to the task. Task Requirements and Prompt Instructions: Junior Instructors Scoring Criteria Analysis: Discrepancies and Feedback: 1. Adherence to Task Requirements (0 points) - The students response does not reflect the task requirements at all. The student did not provide an essay on the given topic. - The junior instructor should assign 0 points for adherence to task requirements. 2. Content Accuracy and Relevance (0 points) 3. Argument Strength (0 points) 4. Counterarguments and Rebuttals (0 points) Meta-analysis is conducted every batch steps, identifying failure cases and proposing concrete remedies and feedback for rubric refinement. Without such intervention, continued RL training under fixed rubric would reinforce these degenerate outputs, potentially resulting in failed training. 2.3.2 Meta-Refinement Based on the results of the meta-analysis, the next step is to construct refined version of the rubric. Specifically, the MRM expands the rubric by introducing new evaluation criteria, enhances existing ones with greater detail and illustrative examples, and adjusts the scoring range to more effectively differentiate between varying levels of response quality. Continuing with the reward hacking example, proposed refinement to the rubric introduces essay length as an explicit evaluation criterion and expands the scoring range from 0 to 10 points. This adjustment enables more finegrained differentiation between responses and discourages degenerate outputs that exploit vague or underspecified scoring guidelines: The essay clearly and consistently addresses the specific claim made in the prompt and Meta-Refinement Task Alignment and Argument Clarity (10 points) - **10**: presents well-defined, coherent argument supported by relevant examples and evidence. The argument is fully developed and addresses the core issue of the prompt. count. - **8**: - **6**: - **4**: - **2**: - **0**: not related to the prompt and does not address the core issue. required word count. The essay does not address the specific claim or is completely off-topic. The argument is The essay is significantly below the The essay meets the required word 2.3.3 Meta-Merging As MPO samples student responses, it produces corresponding rubric refinements. The final step of MPO involves merging these multiple refined instances into single, coherent rubric prompt. This consolidated version becomes the updated evaluation rubric that the RM will use in the subsequent training steps."
        },
        {
            "title": "3 Experiments",
            "content": "To investigate the effectiveness of MPO and its influence on training dynamics, we conduct three core experiments. Section 3.1 outlines our experimental setup. Section 3.2 evaluates the performance of MPO-aligned LLMs on an argumentative essay writing task, exploring different pairings of junior 6 and senior instructors. Section 3.3 empirically shows that MPO generalizes to tasks demanding varying degrees of evaluative thinking. Finally, in Section 3.4, we analyze how the rubric prompts used by the RM (the junior instructor) evolve over successive MPO iterations. 3.1 Experimental Setup Policy Model. Throughout our experiments, we fix the policy model to relatively small LLM: Qwen2-1.5B-Instruct [Yang et al., 2024]. This choice is motivated by three factors. First, we require model with sufficient headroom for improvement across our target tasks. Since we use publicly available benchmarks with limited resources, we focus on smaller open-sourced models which could show clear effects and values of the proposed framework. Extending it to larger models and more variants of model families remains an interesting direction for future study. Second, some tasks such as mathematical reasoningrequire generating over 1000 tokens, which imposes significant memory load during the PPO step. Larger models exceed the capacity of our available GPUs, making Qwen2-1.5B-Instruct practical and scalable option. Third, to benchmark against the MPO-generated prompt, we manually engineered an RM prompt based on empirical insights gained from extensive PPO training runs on this task and model. (Meta) Reward Models. For reward modeling, we use Qwen2.5-32B-Instruct-AWQ and Qwen2.572B-Instruct-AWQ [Qwen et al., 2025], exploring all four junior-senior RMMRM size combinations: 32b_32b, 32b_72b, 72b_32b, and 72b_72b, where the first and second terms denote the sizes of RM and MRM, respectively. Using larger models as (M)RMs is feasible since only inference is required, which can be efficiently handled by an LLM-serving framework. We also include fixed-RM baseline without MPO, using the initial MPO prompt, denoted as {RM size}_iter0. Implementation. Our MPO framework relies on online (meta) reward models implemented as LLMs with an interchangeable prompt mechanism. To support this, we extend the TRL library [von Werra et al., 2020] by implementing prompt-based, online reward model, where LLM-based RMs are hosted using the SGLang framework [Zheng et al., 2024]. Additionally, we extend the PPOTrainer class in trl to customized PPOMetaTrainer, which integrates the MPO refinement steps directly into the training loop along with other necessary modifications. We note that the MPO framework is general and modular, and can be integrated into other RL optimization techniques that rely on reward modelssuch as GRPO [Shao et al., 2024b]to enable dynamic rubric refinement and more adaptive reward shaping. 3.2 Impact of MPO on Essay Writing Task Setup. We train four policy models using MPO with different RMMRM pairings as described in Section 3.1, and another four using vanilla PPO with fixed RM prompts. The essay writing dataset is compiled by Kim et al. [2025] and includes writing instructions drawn from diverse sources, such as English proficiency exams, persuasion corpus, and the Change My View (CMV) subreddit. The training set comprises 26,013 samples, and the test set includes 4,096 samples. Both MPO and PPO are trained for single epoch over the training set, with MPO refinement steps occurring every 10 batch steps during training. We utilize eight A100 80GB GPUs to RL-train the models with an effective combined batch size of 64, where each RL episode spans up to 400 tokens. This configuration yields 40 MPO refinement steps over the course of one-epoch training. Additional training details are provided in Appendix C.3. After single-epoch training, we generate essays for all test prompts using the final checkpoints and evaluate them through 10,000 head-to-head comparisons following the Elo-based Chatbot Arena framework [Chiang et al., 2024], with GPT-4o serving as the impartial judge and small K-factor of 4 applied after each match to maintain rating stability. The resulting Elo scores, summarized in Table 1, provide relative ranking of essay quality across the eight models. Training Curves. Figure 4 shows the training curves for eight models, each corresponding to different pairing of RM and MRM sizes in the essay writing task. Note that we use raw RL reward values for optimization, as they preserve fine-grained distinctions in reward signalsamplifying reward sensitivity and improving optimization effectivenessdespite slight deviation from our formal theoretical framework. The four models employing MPO exhibit increasing RL reward values 7 Figure 4: Training curves for eight essay-writing policy models, each pairing different-sized reward models (RM) and meta-reward models (MRM). The RL Reward and Normalized RL Reward plots show how reward values evolve over global batch steps, capturing the quality of generated responses as judged by the corresponding RM at each point in training. The normalized plot is obtained by dividing the RL reward values by the total attainable score defined by the current rubric, providing more consistent view of reward dynamics across evolving evaluation criteria. Kullback-Leibler (KL) divergence quantifies the extent of policy drift throughout training. The dotted vertical lines indicate MPO rounds, which occur every batch size MPO step640 steps in our setup. over time, driven by successive MPO steps that refine the evaluation prompt by (1) introducing greater number of evaluation criteria and (2) expanding the scoring ranges associated with those criteria as training progresses. While these curves suggest that training is progressing adaptively as intended, the reward scores are themselves adaptivereflecting evolving rubricsso differences in reward values across models (shown in the RL Reward plot) do not necessarily translate to corresponding performance gaps on the final test set. RM 32B MPO w/ MRM (ours) Table 1: Elo ratings for the essay writing task, evaluated through 10,000 pairwise comparisons across 7 LLMs. Each rating is accompanied by standard deviation, computed across 5 independent experiments, denoted by = value. Results. In Table 1, we observe that all MPO-aligned models outperform the PPOaligned baselines and the base LLM, with the 32b_32b model trained with 32B RM and 32B MRMachieving the best performance. We note that the 72b_iter0 model led to failed PPO run characterized by reward hacking, where the policy model often generated responses simply in the form of titlefor example, Title: The Myth of Reddits Inherent Badness: Case for Its Value as Platform for Discussionand still frequently received high scores of 4 or 5 out of 5. In contrast, the MPO-aligned model using the same 72B RM succeeded in meaningful training and achieved performance improvements over the Base LLM. This highlights that MPO can automatically discover effective evaluation rubrics, reducing reliance on manual prompt engineering and mitigating the risk of failed PPO training runs, thus saving both time and computational resources. We observe that the initial version of the evaluation prompt (i.e., iter0) used in MPO consistently yields the lowest Elo ratings, regardless of RM size. Figure 5 provides side-by-side comparisons of the initial and final evaluation prompts across different tasks. 1196s=20 1058s=18 1184s=18 1132s=17 966s=17 629s= Base LLM PPO iter0 836s=17 32B 72B 72B Interestingly, the best results arise when the RM and MRM share the same model size, for example, the 32B + 32B pairing outperforms asymmetric combinations. plausible explanation is that identically sized models develop similar token-usage and reasoning styles, making the MRMs rubric refinements clearer to the RM; when capacities diverge, mismatched vocabularies or reasoning depth may hinder this communication channel. Verifying this hypothesis and systematically exploring other pairings (e.g., capacity-matched but architecture-mixed RMs and MRMs) remain promising directions for future work. 8 Figure 5: Comparison between initial and last versions of MPO rubrics for all tasks. 3.3 Generalization of MPO across Different Tasks As discussed in Section 2.1, evaluating written essays requires form of evaluative thinking that is more breadth-focusedguided by broad set of generalized rubric items that can be applied across diverse prompts and writing styles. In this experiment, we apply the MPO framework to tasks that vary in their demands along the depth and breadth dimensions of evaluative thinking, examining whether the benefits of the proposed approach persist across this spectrum. As illustrated in Figure 2, we evaluate MPO across three additional tasks: summarization, ethical reasoning, and mathematical reasoning. For these three tasks, we compare performance across three models: Base LLM, the original Qwen21.5B-Instruct model prior to any alignment; 32b_iter0, vanilla PPO-aligned model using the initial evaluation prompt with the 32B Qwen model as the RM; and 32b_32b, an MPO-aligned model using the 32B Qwen model for both the RM and MRM. 3.3.1 Summarization Task For summarization, we train models on the BillSum benchmark [Kornilova and Eidelman, 2019] over one epoch, performing MPO refinements every 20 batch steps, and generate 4.5K summaries for evaluation. Performance is assessed using ROUGE scores against human-written references and Elo ratings derived from 5,000 GPT-4o-judged pairwise comparisons, with results shown in Table 2. ROUGE-1 ROUGE-2 ROUGE-L ROUGE-Lsum Elo Rating Base LLM PPO 32b_iter0 MPO 32b_32b 41.82 45.97 48.00 20.79 23.57 24. 27.81 30.29 30.97 27.80 30.29 30.98 849 1012 1139 Table 2: Performance of models was evaluated using ROUGE scores on the BillSum long-document summarization task and Elo ratings derived from 5,000 pairwise comparisons. Base LLM refers to the vanilla instruction-tuned model; 32b_iter0 is the Base LLM trained with PPO using 32B RM with fixed initial prompt; 32b_32b is trained with MPO using 32B LLMs for both RM and MRM. The MPO-aligned 32b_32b model outperforms the other two models across all evaluation metrics, including Elo ratings. Figure 5 presents an excerpt of the evaluation rubric used for the summarization task, showing how MPO expanded the rubric to incorporate criteria specifically relevant to assessing legislative bill summaries. 3.3.2 Ethical Reasoning Task Table 3: Accuracy on the ethical reasoning (ScruplesAnecdotes) and mathematical reasoning (MATH) benchmarks. Base LLM is the vanilla instruction-tuned model; 32b_iter0 is the same model aligned with PPO using fixed 32B RM; 32b_32b is our MPO-aligned model with 32B RMs for both RM and MRM. For ethical reasoning, we use the Anecdotes from the Scruples dataset [Lourie et al., 2020], collection of over 32,000 ethically complex real-world situations labeled with community judgments. We randomly sample 13K anecdotes for training and 4.7K for testing, running single training epoch with MPO steps performed every 10 batch steps, leading to 20 rubric refinements. Instead of relying on the binary ground truth labels, reward scores are based solely on the quality of ethical reasoning to encourage deeper reasoning development rather than optimizing for imbalanced label distributions. Base LLM PPO 32b_iter0 MPO 32b_32b (ours) 33.80 (1601/4736) 63.79 (3021/4736) 68.60 (3249/4736) 18.13 (928/5120) 23.93 (1225/5120) 24.18 (1238/5120) Accuracy (%, correct / total) ScruplesAnecdotes MATH Model Evaluation based on accuracy against the binary verdict labels is reported in Table 3. We observe that the MPO-aligned policy model generates ethical reasoning traces that result in higher degree of alignment with human-annotated verdicts. 3.3.3 Mathematical Reasoning Task For mathematical reasoning, we use the MATH dataset [Hendrycks et al., 2021], which contains 12,500 high school competition-style problems across seven subjects, each with detailed step-by-step solutions for evaluating both final answers and reasoning processes. We train on 7.5K samples and test on 5K, clustering problems into 21 subject-cluster groups and maintaining separate evaluation prompt for each, with MPO refinements triggered every 30 batch steps. The reward model follows plan-then-execute strategy adapted from Saha et al. [2025], formulating an evaluation plan before scoring student responses with rubric-guided assessment. Accuracy results based on the exact match of reference answers are reported in Table 3. As with the other tasks, applying the MPO framework through refinement of meta-level guidelines further enhances the performance of our plan-then-execute reward model. However, the relative improvement is more modest compared to the other tasks. We hypothesize that this is due to the highly instancespecific nature of mathematical reasoning, where scoring relies heavily on whether the sequential 10 (a) Rubric length during MPO refinements (b) Total score across rubric refinements Figure 6: (a) Mean length of rubric items for essay writing task across the MPO-aligned models. (b) Mean normalized total rubric score for 1,000 test essays (generated by the 32b_72b model) across successive evaluation prompt refinements. logic leads precisely to the correct answer. In contrast, the meta-level guidelinesconstructed from sampled instancestend to remain relatively general. Nonetheless, subject-specific refinements still contribute to performance gains. Developing more granular and tailored meta-guidelines could yield additional improvements, which we leave for future work. 3.4 Evolution of Evaluation Rubric While earlier results highlight how MPO-refined evaluation rubrics differ from their initial prompts (Figure 5), this experiment seeks to uncover more holistic patterns across rubric evolution, particularly focusing on essay writing task. Mean Lengths of Rubric Items. For the essay writing task, we track how the average length of each rubric item evolves over successive MPO refinements. Notably, the length increases sharply after the first refinement and continues to grow steadily over the next 5 to 10 iterations, before plateauing during the remaining stages of training (Fig. 6a). Manual inspection confirms that most meaningful rubric changes occur within the first 5 to 10 iterations, after which the refinements become relatively minor.2 This observation suggests that the current fixed MPO schedule could be improved through dynamic adjustment based on training dynamicsan avenue worth exploring in future work. Mean Total Scores Across Successive Rubrics Figure 6b plots the normalized mean total scores for 1,000 randomly sampled test essays, generated by the 32b_72b model and evaluated using both early and late-stage RM prompts evolved through MPO. The general pattern shows that mean total scores increase over the course of training, reflecting the policy models improving output quality. However, earlier versions of the evaluation prompts tend to assign higher scores to samples, largely because their coarser and less fine-grained criteria make it easier for responses to meet the rubrics standards. In contrast, later-stage prompts, which feature more detailed and discriminating rubric items, assign lower scores, as they capture subtler flaws and impose stricter evaluation standards. This trend highlights how rubric refinement not only tightens evaluation but also provides more accurate and demanding feedback to guide policy improvement. Changes in Discourse Motif Distribution. Thus far, we have shown that MPO-evolved rubrics not only lead to higher training rewards but also produce higher-quality generations across the four downstream tasks. To gain deeper insight into the linguistic structure of these rubrics, we apply hierarchical discourse parsing and analyze the resulting discourse subgraphsreferred to as discourse motifswhich capture pragmatic discourse relations between textual units ranging from phrases to full paragraphs. This analysis builds on the method introduced by Kim et al. [2024], who used Rhetorical Structure Theory (RST) [Mann and Thompson, 1987] to study discourse patterns in LLMand human-generated texts. In our setting, we compute the distribution of discourse motifs across three versions of the rubric: the initial version, the version after single MPO refinement, and the final version at the end of training. 2We provide the sequence of evolved rubrics in Appendix 11 Figure 7 presents bar plots comparing the three rubric versions in terms of discourse motif distributions, with motif types on the x-axis and their normalized frequency within the overall RST graph on the y-axis.3 The edge label / represents hyperedge relation, typically indicating transitions across textual levels and signaling that the text exhibits more hierarchically organized discourse structure. The plot shows that as MPO refinements progress, the evolved rubrics adopt increasingly hierarchical discourse structures, marked by decrease in Background relations and corresponding increase in more informative relations such as Contrast and Attribution. Notably, Attribution relations, which explicitly identify the source or ownership of presented information, become more frequentan important feature for rubric-based evaluation, where attributing claims, reasoning, and judgments clearly is critical for coherent assessment. These trends hint that MPO not only refines content criteria but also implicitly drives the development of richer, more structured evaluation language. We also note that an increase in hierarchical discourse structures is known characteristic of human-like writing, as reported in Kim et al. [2024, 2025]. Figure 7: Comparison of discourse motifs found in rubric prompts at the initial stage, after the first, and last MPO step of training for essay writing task. 3.5 Comparison against Human-Engineered Oracle Prompt Table 4: Elo ratings for the essay writing task, evaluated through 5,000 pairwise comparisons across 4 LLMs. Each rating is accompanied by standard deviation, computed across 5 independent experiments, denoted by = value. For the essay writing task, Kim et al. [2025] hand-engineered an evaluation prompt for the RM through an iterative process involving over 60 vanilla PPO training runs. The prompt was refined based on reward hacking behaviors observed from the same Qwen2-1.5B-Instruct model and adjusted to better align with human evaluation scores by modifying evaluation criteria. We treat this prompt as an oracle prompt, as it represents hand-evolved rubric distilled through extensive PPO training experience. Figure 8 compares the oracle evaluation prompt with the final version produced by the MPO framework. Unlike the oracle, the MPO-evolved prompt was generated within single training epoch and features diverse set of rubric items specifically tailored for essay evaluation. Each item is accompanied by detailed descriptions that support fine-grained point allocation across defined scoring range. MPO w/ MRM (ours) 952s=20 1042s=20 1001s=14 - 1005s=14 - PPO oracle 32B 72B 32B 72B RM Table 4 presents the Elo ratings for the 32b_32b and 32b_72b MPO-aligned models alongside the PPO-aligned models using the oracle prompt, based on 5,000 GPT-4o-judged pairwise comparisons. The two MPO-aligned models using 32B RM outperform the PPO-aligned model trained with the oracle prompt with 32B RM, although they fall behind the PPO model using the oracle prompt with 72B RM. Nevertheless, these results demonstrate that the MPO framework can automatically generate evaluation prompts approaching the quality of extensively hand-engineered oracle prompts, without requiring any task-specific manual prompt engineering. 3We provide the full version in Appendix D. Figure 8: Comparison between hand-crafted and MPO-evolved rubrics for essay writing task."
        },
        {
            "title": "4 Conclusion",
            "content": "This work introduces Meta Policy Optimization (MPO), novel framework that enhances reinforcement learning from human or AI feedback by dynamically evolving the evaluation rubrics used by reward models. Grounded in the cognitive principles of evaluative thinking and metacognition, MPO empowers reward models to not only evaluate policy outputs but also reflect on and refine their scoring criteria over time. Across diverse tasksincluding essay writing, summarization, ethical reasoning, and mathematical problem solvingMPO consistently improves alignment and outperforms models relying on static, manually crafted prompts. Beyond improved empirical performance, MPO offers new lens for thinking about reward modeling as an adaptive, self-improving process. Our analysis further reveals that the evolved rubrics exhibit deeper linguistic structure, suggesting the emergence of more principled evaluation schemas. For future work, several promising directions emerge: dynamically adjusting MPO frequency based on training dynamics, scaling to more granular rubric specializations, exploring multi-turn dialogues and interactive settings, and integrating MPO with advanced optimization algorithms beyond PPO. Finally, extending MPO to support multi-agent alignment or long-horizon tasks could open new pathways toward more generalizable and cognitively aligned learning systems."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was primarily funded by generous research gift from Grammarly. We are also grateful to the Minnesota NLP group members for their valuable feedback and constructive comments on our initial draft."
        },
        {
            "title": "References",
            "content": "A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer, O. Pietquin, A. Üstün, and S. Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1224812267, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.662. URL https://aclanthology.org/2024.acl-long.662. D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete problems in ai safety, 2016a. D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete problems in ai safety, 2016b. URL https://arxiv.org/abs/1606.06565. Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. J. Buckley, T. Archibald, M. Hargraves, and W. M. Trochim. Defining and teaching evaluative thinking: Insights from research on critical thinking. American Journal of Evaluation, 36(3): 375388, 2015. L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein, H. Huang, M. Shoeybi, and B. Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf, 2024a. URL https://arxiv.org/abs/ 2402.07319. Z. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning convertsweak language models to strong language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024b. W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/abs/2403.04132. T. Coste, U. Anwar, R. Kirk, and D. Krueger. Reward model ensembles help mitigate overoptimization. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=dcjtMYkpXx. E. De Bono. The Use of Lateral Thinking. Pelican books. Penguin Books, 1971. 9780140214468. URL https://books.google.com/books?id=2Fd-AAAAMAAJ. ISBN DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. 14 C. Denison, M. MacDiarmid, F. Barez, D. Duvenaud, S. Kravec, S. Marks, N. Schiefer, R. Soklaski, A. Tamkin, J. Kaplan, et al. Sycophancy to subterfuge: Investigating reward-tampering in large language models. arXiv preprint arXiv:2406.10162, 2024. M. Ding, S. Chakraborty, V. Agrawal, Z. Che, A. Koppel, M. Wang, A. Bedi, and F. Huang. SAIL: Self-improving efficient online alignment of large language models. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, 2024. URL https://openreview.net/forum? id=9m8dF6oAsd. A. Efklides. Metacognition and affect: What can metacognitive experiences tell us about the learning process? Educational research review, 1(1):314, 2006. J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. N. DAmour, K. D. Dvijotham, A. Fisch, K. A. Heller, S. R. Pfohl, D. Ramachandran, P. Shaw, and J. Berant. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=5u1GpUkKtG. K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. T. Everitt, M. Hutter, R. Kumar, and V. Krakovna. Reward tampering problems and solutions in reinforcement learning: causal influence diagram perspective. Synthese, 198(Suppl 27): 64356467, 2021. J. H. Flavell. Metacognition and cognitive monitoring: new area of cognitivedevelopmental inquiry. American psychologist, 34(10):906, 1979. L. Fluri, L. Lang, A. Abate, P. Forré, D. Krueger, and J. Skalse. The perils of optimizing learned reward functions: Low training error does not guarantee low regret. arXiv preprint arXiv:2406.15753, 2024. J. Fu, X. Zhao, C. Yao, H. Wang, Q. Han, and Y. Xiao. Reward shaping to mitigate reward hacking in rlhf. arXiv preprint arXiv:2502.18770, 2025. L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1083510866. PMLR, 2329 Jul 2023. URL https://proceedings.mlr. press/v202/gao23h.html. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_ files/paper/2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf. Z. M. Kim, K. Lee, P. Zhu, V. Raheja, and D. Kang. Threads of subtlety: Detecting machinegenerated texts through discourse motifs. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 54495474, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.298. URL https://aclanthology.org/2024. acl-long.298/. Z. M. Kim, A. Ramachandran, F. Tavazoee, J.-K. Kim, O. Rokhlenko, and D. Kang. Align to structure: Aligning large language models with structural information, 2025. URL https: //arxiv.org/abs/2504.03622. A. Kornilova and V. Eidelman. BillSum: corpus for automatic summarization of US legislation. In L. Wang, J. C. K. Cheung, G. Carenini, and F. Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 4856, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5406. URL https://aclanthology.org/ D19-5406/. 15 V. Krakovna, J. Uesato, V. Mikulik, M. Rahtz, T. Everitt, R. Kumar, Z. Kenton, J. Leike, and S. Legg. Specification gaming: the flip side of ai ingenuity. DeepMind Blog, 3, 2020. L. L. D. Langosco, J. Koch, L. D. Sharkey, J. Pfau, and D. Krueger. Goal misgeneralization in deep reinforcement learning. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1200412019. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/langosco22a.html. H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. H. Lee, C. Park, D. Abel, and M. Jin. hypothesis on black swan in unchanging environments. arXiv preprint arXiv:2407.18422, 2024. C.-Y. Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. H. Liu, C. Sferrazza, and P. Abbeel. Chain of hindsight aligns language models with feedback. In The Twelfth International Conference on Learning Representations, 2024a. URL https: //openreview.net/forum?id=6xfe4IVcOu. T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao, J. Shen, Z. Qin, T. Yu, D. Sohn, A. Makarova, J. Liu, Y. Liu, B. Piot, A. Ittycheriah, A. Kumar, and M. Saleh. Rrm: Robust reward model training mitigates reward hacking. ArXiv, abs/2409.13156, 2024b. URL https: //api.semanticscholar.org/CorpusID:272770255. C. G. Lord, L. Ross, and M. R. Lepper. Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence. Journal of personality and social psychology, 37(11):2098, 1979. N. Lourie, R. L. Bras, and Y. Choi. Scruples: corpus of community ethical judgments on 32,000 real-life anecdotes. arXiv e-prints, 2020. W. C. Mann and S. A. Thompson. Rhetorical structure theory: theory of text organization. University of Southern California, Information Sciences Institute Los Angeles, 1987. C. B. McCormick. Metacognition and learning. Handbook of psychology, pages 79102, 2003. Y. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with reference-free reward. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=3Tzcot1LKb. J. Metcalfe and N. Kornell. region of proximal learning model of study time allocation. Journal of Memory and Language, 52(4):463477, 2005. ISSN 0749-596X. doi: https://doi.org/10. 1016/j.jml.2004.12.001. URL https://www.sciencedirect.com/science/article/pii/ S0749596X04001330. Special Issue on Metamemory. Y. Miao, S. Zhang, L. Ding, R. Bao, L. Zhang, and D. Tao. InfoRM: Mitigating reward hacking in RLHF via information-theoretic reward modeling. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= 3XnBVK9sD6. Y. Miao, S. Zhang, L. Ding, Y. Zhang, L. Zhang, and D. Tao. The energy loss phenomenon in rlhf: new perspective on mitigating reward hacking, 2025. URL https://arxiv.org/abs/2501. 19358. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, 16 Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022b. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022c. Curran Associates Inc. ISBN 9781713871088. A. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating In International Conference on Learning Representations, 2022. URL misaligned models. https://openreview.net/forum?id=JYtwGwIL7ye. R. Y. Pang, V. Padmakumar, T. Sellam, A. Parikh, and H. He. Reward gaming in conditional In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the text generation. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 47464763, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.262. URL https://aclanthology.org/2023.acl-long.262/. R. Y. Pang, W. Yuan, K. Cho, H. He, S. Sukhbaatar, and J. Weston. Iterative reasoning preference optimization. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 116617116637. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/d37c9ad425fe5b65304d500c6edcba00-Paper-Conference.pdf. C. Park, M. Liu, D. Kong, K. Zhang, and A. Ozdaglar. Rlhf from heterogeneous feedback via personalization and preference aggregation. arXiv preprint arXiv:2405.00254, 2024. C. Park, S. Han, X. Guo, A. Ozdaglar, K. Zhang, and J.-K. Kim. Maporl: Multi-agent post-cotraining for collaborative large language models with reinforcement learning. arXiv preprint arXiv:2502.18439, 2025. E. Perez, S. Ringer, K. Lukosiute, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson, S. Kundu, S. Kadavath, A. Jones, A. Chen, B. Mann, B. Israel, B. Seethor, C. McKinnon, C. Olah, D. Yan, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, G. Khundadze, J. Kernion, J. Landis, J. Kerr, J. Mueller, J. Hyun, J. Landau, K. Ndousse, L. Goldberg, L. Lovitt, M. Lucas, M. Sellitto, M. Zhang, N. Kingsland, N. Elhage, N. Joseph, N. Mercado, N. DasSarma, O. Rausch, R. Larson, S. McCandlish, S. Johnston, S. Kravec, S. El Showk, T. Lanham, T. Telleen-Lawton, T. Brown, T. Henighan, T. Hume, Y. Bai, Z. Hatfield-Dodds, J. Clark, S. R. Bowman, A. Askell, R. Grosse, D. Hernandez, D. Ganguli, E. Hubinger, N. Schiefer, and J. Kaplan. Discovering language model behaviors with model-written evaluations. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1338713434, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.847. URL https://aclanthology.org/2023.findings-acl.847/. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. A. Rame, N. Vieillard, L. Hussenot, R. Dadashi-Tazehozi, G. Cideron, O. Bachem, and J. Ferret. WARM: On the benefits of weight averaged reward models. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 4204842073. PMLR, 2127 Jul 2024. URL https://proceedings.mlr. press/v235/rame24a.html. 17 S. Saha, X. Li, M. Ghazvininejad, J. Weston, and T. Wang. Learning to plan & reason for evaluation with thinking-llm-as-a-judge, 2025. URL https://arxiv.org/abs/2501.18099. K. Saito, A. Wachi, K. Wataoka, and Y. Akimoto. Verbosity bias in preference labeling by large language models, 2023. URL https://arxiv.org/abs/2310.10076. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a. URL https://arxiv.org/abs/2402.03300. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024b. URL https://arxiv.org/abs/2402.03300. M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, E. DURMUS, Z. HatfieldDodds, S. R. Johnston, S. M. Kravec, T. Maxwell, S. McCandlish, K. Ndousse, O. Rausch, N. Schiefer, D. Yan, M. Zhang, and E. Perez. Towards understanding sycophancy in language In The Twelfth International Conference on Learning Representations, 2024. URL models. https://openreview.net/forum?id=tvhaxkMKAn. P. Singhal, T. Goyal, J. Xu, and G. Durrett. long way to go: Investigating length correlations in RLHF, 2024. URL https://openreview.net/forum?id=sNtDKdcI1f. J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in reward gaming. Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= yb3HOXO3lX2. N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf. K. Tian, E. Mitchell, H. Yao, C. D. Manning, and C. Finn. Fine-tuning language models for factuality. In The Twelfth International Conference on Learning Representations, 2024. H. Tran, C. Glaze, nical new-benchmark-results-demonstrate-value-of-snorkel-ai-approach-to-llm-alignment. Snorkel report, URL and B. Hancock. 2023. AI, Iterative dpo alignment. Techhttps://snorkel.ai/ M. V. Veenman, B. H. Van Hout-Wolters, and P. Afflerbach. Metacognition and learning: Conceptual and methodological considerations. Metacognition and learning, 1:314, 2006. L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, N. Lambert, S. Huang, K. Rasul, and Q. Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/ trl, 2020. Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy, and H. Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=w4zZNC4ZaV. J. Wen, R. Zhong, A. Khan, E. Perez, J. Steinhardt, M. Huang, S. R. Bowman, H. He, and S. Feng. Language models learn to mislead humans via RLHF. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xJljiPE6dg. T. Wu, W. Yuan, O. Golovneva, J. Xu, Y. Tian, J. Jiao, J. Weston, and S. Sukhbaatar. Metarewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024a. Y. Wu, Z. Sun, H. Yuan, K. Ji, Y. Yang, and Q. Gu. Self-play preference optimization for language model alignment. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2024b. URL https://openreview.net/forum?id=Z1PDdGekgn. 18 W. Xiong, H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T. Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint, 2024. URL https://arxiv.org/abs/2312.11456. J. Xu, A. Lee, S. Sukhbaatar, and J. Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, X. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. W. Yuan, R. Y. Pang, K. Cho, X. Li, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language models, 2025. URL https://arxiv.org/abs/2401.10020. C. Zhang, C. Tang, D. Chong, K. Shi, G. Tang, F. Jiang, and H. Li. TS-align: teacher-student collaborative framework for scalable iterative finetuning of large language models. In Y. AlOnaizan, M. Bansal, and Y.-N. Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89268946, Miami, Florida, USA, Nov. 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.521. URL https:// aclanthology.org/2024.findings-emnlp.521/. S. Zhang, Z. Chen, S. Chen, Y. Shen, Z. Sun, and C. Gan. Improving reinforcement learning from human feedback with efficient reward model ensemble, 2024b. URL https://arxiv.org/abs/ 2401.16635. L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. H. Yu, S. Cao, C. Kozyrakis, I. Stoica, J. E. Gonzalez, C. Barrett, and Y. Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/ 1909.08593."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Reward Hacking in LLMs RL has been widely applied in the post-training of LLMs, enhancing areas such as factuality [Tian et al., 2024], code generation [Le et al., 2022], reasoning [DeepSeek-AI et al., 2025], and multi-agent decision-making [Park et al., 2025]. predominant strategy for incorporating RL into LLM training is reinforcement learning from human feedback (RLHF) [Ziegler et al., 2019, Ouyang et al., 2022b, Bai et al., 2022, Ahmadian et al., 2024, Park et al., 2024]. Reward hacking [Skalse et al., 2022] is phenomenon that is observed when an RL agent exploits flaws, ambiguities, or lack of specificity in the reward function (as also noted in Goodharts Law) to achieve high rewards in unintended ways, often showing coherent but unanticipated behavior [Amodei et al., 2016a]. This leads to the agent being misaligned with the human-intended behaviors, yet achieving high rewards. This has emerged as critical challenge in RLHF and RLAIF [Krakovna et al., 2020, Pan et al., 2022, Gao et al., 2023, Fluri et al., 2024, Lee et al., 2024]. variety of studies have highlighted the detrimental effects of reward in aligned LLMs [Pang et al., 2023]. Various undesirable side-effects, such as sycophancy [Perez et al., 2023, Sharma et al., 2024, Denison et al., 2024], verbosity [Saito et al., 2023, Singhal et al., 2024], and deception [Wen et al., 2025], among others. To address these challenges, recent work has proposed numerous reward modeling and mitigation strategies, such as such as Reward Ensembling [Eisenstein et al., 2024, Rame et al., 2024, Coste et al., 2024, Zhang et al., 2024b], and designing RL regularizations [Miao et al., 2025, Chen et al., 2024a, Liu et al., 2024b, Miao et al., 2024, Fu et al., 2025]. Although these strategies have demonstrated varying levels of success, challenges such as reward overfitting, misspecification, and misgeneralization still pose significant obstacles to robust and reliable reward model-based alignment in practice. We argue that effectively addressing reward hacking requires alignment mechanisms capable of broader contextual reasoning, as it often stems from the interaction between poorly defined reward signals, evolving policy behaviors, and the shifting dynamics of exploration versus exploitation during training. A. Iterative Alignment in LLMs As LLMs have scaled, despite advances in alignment techniques [Ouyang et al., 2022c, Bai et al., 2022], they have fallen short in handling complex, shifting failure modes [Xu et al., 2023, Meng et al., 2024, Liu et al., 2024a, Ethayarajh et al., 2024]. This has led to growing interest in iterative approaches to preference optimization for aligning LLMs [Tran et al., 2023, Xiong et al., 2024, Pang et al., 2024, Wu et al., 2024b, Chen et al., 2024b, Ding et al., 2024]. While these methods improve upon single-pass preference tuning by incorporating feedback into successive training rounds, allowing the model to refine its behavior based on evolving outputs, they remain dataset-bounded: relying on explicit preference comparisons or fixed prompt templates that are dependent on the initial design of reward prompts or training distributions and do not adapt during training with an evolving policy. They are also limited in their robustness to reward hacking as the policy shifts since the reward model does not evolve during training. Similarly, methods based on knowledge distillation via supervised fine-tuning (SFT) from reward model [Wang et al., 2023] encode reward judgments into static training target, which may no longer reflect optimal behavior as the model improves, further reinforcing non-adaptive biases in reward estimation. The self-rewarding LMs proposed in Yuan et al. [2025] share our motivation in leveraging LLM-as-a-Judge prompting to generate reward signals during training. However, their approach relies on iteratively applying Direct Policy Optimization (DPO), followed by generating new dataset for each DPO rounda process that is computationally intensive and resource-heavy. In contrast, our MPO framework introduces lightweight, prompt-based reward refinement via meta-reward model, enabling continuous alignment without the need for repeated dataset regeneration or full model retraining. A.3 Meta-Level Alignment for LLMs Our work is closely related to Wu et al. [2024a], who first introduced the idea of LLM-as-a-MetaJudge within self-rewarding pipeline, enabling single model to evaluate and refine its own judgments. Their method trains one LLM to serve as actor, judge, and meta-judge, using fixed 20 5-point rubric to generate and score responses, then iteratively refining both roles via DPO. While this reflects MPOs self-improvement philosophy and focus on mitigating reward hacking, the approaches differ: Meta-Judge updates model weights through preference optimization, whereas MPO introduces separate meta-reward model that rewrites the evaluation rubric itselfadapting the criteria, not just the model, in response to emerging behaviors like reward exploitation. Similarly, TS-Align [Zhang et al., 2024a] shares MPOs goal of scalable alignment with reduced human supervision but takes different route. It employs teacherstudent framework where strong teacher RM re-ranks preference pairs filtered by smaller student RM, followed by DPO-based fine-tuning. However, the reward prompt remains fixed throughout. In contrast, MPO operates with single RM (guided by meta RM) and dynamically evolves the evaluation rubric at regular intervals, allowing the reward function itself to adapt to policy drift and training-phase dynamics. To the best of our knowledge, MPO is the first to improve LLM alignment via meta-level rubric refinements under PPO, rather than DPOoffering lightweight, prompt-based alternative that reduces computational cost while enabling continual adaptation to the evolving training landscape."
        },
        {
            "title": "B Prompts for Meta Reward Model",
            "content": "Meta Analysis You are senior instructor tasked with evaluating junior instructors scoring of students generation based on specific task and prompt instruction. Your objective is to conduct meta-level analysis of the junior instructors evaluation approach, guiding them in refining their scoring criteria to ensure accurate, nuanced differentiation between high-quality and subpar generations. Emphasize strategies for assigning lower scores to undesirable responses and higher scores to responses that adhere closely to the overall objectives of the task. The information provided includes: Task Description: task_description Students Prompt Instructions: student_prompt Students Generation: student_generation Junior Instructors Scoring Criteria: junior_prompt Junior Instructors Assigned Score: junior_score Your task: Critically evaluate the junior instructors score and justification in relation to the students response, task requirements, and prompt instructions. 1. Accuracy of Scoring - Determine whether the students response is receiving an inflated score despite not fully meeting the task objectives in terms of quality and content. - Identify any elements where the response deviates from task expectations, such as misinterpretation, lack of depth, or overemphasis on irrelevant aspects. 2. Evaluation of Scoring Criteria - Assess whether the junior instructors criteria align with the tasks overarching purpose. Are critical aspects overlooked, or do the criteria require further breakdown for clarity? - Examine whether the distribution of points is logical and correctly sums to the total score. Flag any inconsistencies and suggest necessary adjustments. 3. Constructive Feedback for Refinement - Provide actionable recommendations to enhance the scoring framework, ensuring it is comprehensive and consistently applied. - Emphasize the need for strict penalization in cases of severe errors to maintain evaluation rigor. Present the analysis concisely within max_words words. Conclude the response with: <EOE>. Your Analysis:"
        },
        {
            "title": "Meta Refinement",
            "content": "Based on the meta-level analysis, refine the junior instructors scoring criteria by designing an explicit rubric-based framework with separate section items for awarding points and deducting points. This rubric must assign specific point values for meeting given criteria, with clear deductions for any shortcomings. Fill in any gaps in the existing criteria to cover all relevant aspects of the task. concrete example illustrating how the rubric would apply to typical student response. Adjust the total score to match the rubric items, ensuring the sum of all criteria equals the final total. Provide Use the following structure: <rubric> <item> Score Category Name - X1: (Description of the criterion for achieving this score X1, followed by an example.) - X2: (Description of the criterion for achieving this score X2, followed by an example.) ... </item> ... </rubric> Your generation should be no more than max_words words. End with <EOE>. Important: You must follow the <rubric> and <item> formatting as shown above. Junior Instructors Scoring Criteria (refined): Meta Merging Combine and refine the multiple sets of Junior Instructors Scoring Criteria into single, cohesive set that provides comprehensive guidelines for assessment. Here are multiple sets of Junior Instructors Scoring Criteria, delimited by ===: multiple_sets Combine the above concisely without repetition. The combined criteria should be no more than max_words words. Make sure that the points across criteria add up correctly to the total score. Use the following structure: <rubric> <item> Score Category Name - X1: (Description of the criterion for achieving this score X1, followed by an example.) - X2: (Description of the criterion for achieving this score X2, followed by an example.) ... </item> ... </rubric> Your generation should be no more than max_words words. End with <EOE>. Important: You must follow the <rubric> and <item> formatting as shown above. Junior Instructors Scoring Criteria (combined):"
        },
        {
            "title": "C Evolution of Evaluation Rubric",
            "content": "We present couple of examples of refined evaluation prompts produced during the 32b_32b model training. Full examples are available at https://github.com/minnesotanlp/mpo. C.1 Essay Writing Task MPO Iteration 1 Essay Writing <item> Task Alignment and Argument Clarity - 5: The essay clearly and consistently addresses the specific claim made in the prompt and presents well-defined, coherent argument supported by relevant examples and evidence. (Example: \"The essay argues that the U.S. is not responsible for social backwardness in Iran because it has not directly influenced Irans social structures and policies, supported by specific examples.\") - 4: The essay mostly addresses the specific claim and presents coherent argument, but there are minor inconsistencies or lack of depth. (Example: \"The essay argues that the U.S. is not primarily \"The essay (Example: \"The (Example: \"The essay argues that the U.S. has not \"The essay cites specific examples of U.S. policies that (Example: \"The essay mentions U.S. policies but does not provide specific examples or responsible for social backwardness in Iran, but it does not fully explain how other factors might influence this.\") - 3: The essay partially addresses the specific claim and presents an argument, but there are significant inconsistencies or lack of depth. directly caused social backwardness, but it does not fully explain how other factors might influence this.\") - 2: The essay addresses the specific claim but the argument is weak and poorly supported. (Example: \"The essay argues that the U.S. is not responsible, but it lacks supporting evidence or explanation.\") - 1: The essay fails to address the specific claim or presents an argument that is contradictory or irrelevant. (Example: \"The essay argues that the U.S. has historically been responsible for promoting social backwardness in Iran.\") - 0: The essay does not address the specific claim or is completely off-topic. (Example: \"The essay discusses the history of Iran without mentioning the U.S. or social backwardness.\") </item> <item> Evidence and Reasoning - 5: The essay provides strong, relevant evidence and reasoning to support the argument, with specific examples and data. (Example: did not directly influence Irans social structures and policies.\") - 4: The essay provides mostly relevant evidence and reasoning, but some points are weak or not fully supported. (Example: \"The essay cites examples of U.S. policies but does not fully explain how they do not contribute to social backwardness.\") - 3: The essay provides some relevant evidence and reasoning, but there are significant gaps or weak points. explanations.\") - 2: The essay provides weak or irrelevant evidence and reasoning. (Example: \"The essay mentions U.S. policies but does not explain how they are unrelated to social backwardness.\") - 1: The essay does not provide any evidence or reasoning to support the argument. essay makes claims without providing any supporting evidence.\") - 0: The essay provides evidence and reasoning that contradicts the argument. (Example: provides examples of U.S. policies that contributed to social backwardness.\") </item> <item> Counterarguments and Refutation - 5: The essay addresses potential counterarguments and provides strong refutations. (Example: essay acknowledges that U.S. policies might have some indirect influence but explains why this does not make the U.S. responsible for social backwardness.\") - 4: The essay addresses some counterarguments and provides mostly strong refutations. (Example: \"The essay acknowledges some counterarguments but does not fully refute them.\") - 3: The essay addresses some counterarguments but provides weak or incomplete refutations. \"The essay mentions counterarguments but does not fully address them.\") - 2: The essay addresses some counterarguments but does not provide any refutations. (Example: \"The essay mentions counterarguments without explaining why they do not undermine the argument.\") - 1: The essay does not address any counterarguments. (Example: \"The essay does not mention any counterarguments.\") - 0: The essay addresses counterarguments but fails to refute them or provides weak refutations. (Example: \"The essay mentions counterarguments but does not provide any refutations or provides weak refutations.\") </item> <item> Structure and Coherence - 5: The essay is well-organized and coherent, with clear structure and logical flow. \"The essay has an introduction, body paragraphs, and conclusion that are logically connected.\") - 4: The essay has mostly clear structure, but some points may be slightly disconnected or lack smooth transitions. (Example: \"The essay has clear structure but some points are not well-connected or lack transitions.\") - 3: The essay has somewhat clear structure, but the points are not well-connected, and the transitions are weak. (Example: \"The essay has structure but the points are disjointed and the transitions are weak.\") - 2: The essay lacks clear structure, and the points are disjointed and disconnected. \"The essay has no clear structure and the points are not well-connected.\") - 1: The essay is poorly structured, with no clear introduction, body, or conclusion. \"The essay lacks clear structure and the points are entirely disjointed.\") - 0: The essay is completely disorganized and lacks any structure. (Example: \"The essay is collection of unrelated points with no structure.\") </item> <item> Depth of Analysis - 5: The essay demonstrates deep and nuanced analysis of the issue, addressing complexities and providing specific examples and data. (Example: \"The essay discusses the historical roots of the conflict, the cultural identity of Palestinians, and the potential repercussions of the proposed solution.\") - 4: The essay provides some depth and nuance in the analysis but lacks specific examples or data. (Example: \"The essay mentions that the conflict is about identity and survival but does not provide substantial evidence or reasoning.\") - 3: The essay provides minimal depth and nuance in the analysis, lacking specific examples or data. (Example: \"The essay mentions the conflict is about identity but does not provide substantial historical or cultural context.\") - 2: The essay lacks depth and nuance in the analysis, providing vague statements without specific examples or data. (Example: \"The essay is vague and does not provide any substantial argument or (Example: (Example: (Example: \"The (Example: \"The essay is reasoning.\") - 1: The essay lacks any meaningful analysis or is superficial. (Example: \"The essay does not provide any meaningful analysis and remains superficial in its discussion.\") - 0: The essay is completely lacking in analysis and is purely superficial. (Example: vague and does not provide any substantial content or context.\") </item> <item> Language and Mechanics - 5: The essay demonstrates clear and effective use of language with minimal grammatical errors. (Example: \"The essay uses clear and precise language with no significant grammatical errors.\") - 4: The essay has some grammatical errors but is still generally clear. (Example: few grammatical errors but is still understandable.\") - 3: The essay has some grammatical errors that affect clarity but is still mostly comprehensible. (Example: \"The essay has some errors in grammar or style but is still generally clear.\") - 2: The essay has significant grammatical errors that affect clarity and coherence. (Example: \"The essay has multiple grammatical or stylistic errors that make it difficult to understand.\") - 1: The essay has significant grammatical errors that severely affect clarity and coherence. (Example: \"The essay has multiple grammatical or stylistic errors that make it difficult to understand.\") - 0: The essay is completely unclear and incoherent due to significant grammatical or stylistic errors. (Example: \"The essay is difficult to read due to numerous grammatical or stylistic errors.\") </item> \"The essay has MPO Iteration 40 Essay Writing The argument is weak and lacks substantial examples <item> Task Alignment and Argument Clarity (10 points) - 10: The essay clearly and consistently addresses the specific claim made in the prompt and presents well-defined, coherent argument supported by relevant examples and evidence. The argument is fully developed and addresses the core issue of the prompt. (Example: \"The essay argues that higher education is counterproductive by providing specific examples of student debt, income gaps, and over-specialization in certain fields.\") - 8: The essay mostly addresses the specific claim and presents coherent argument, but there are minor inconsistencies or lack of depth. The argument is somewhat developed but could benefit from more specific examples or deeper analysis. (Example: \"The essay argues that higher education is counterproductive but lacks substantial examples or clear connection to the context.\") - 6: The essay partially addresses the specific claim and presents an argument, but there are significant inconsistencies or lack of depth. or clear connection to the core issue. (Example: \"The essay argues that higher education is counterproductive but fails to address the specific aspects of student debt, income gaps, and over-specialization.\") - 4: The essay addresses the specific claim but the argument is weak and poorly supported. The argument lacks substantial examples or clear connection to the core issue. (Example: \"The essay argues that higher education is counterproductive but lacks supporting evidence or explanation.\") - 2: The essay fails to address the specific claim or presents an argument that is contradictory or irrelevant. The argument is not aligned with the prompt and does not address the core issue. (Example: \"The essay argues that higher education is beneficial, which contradicts the prompt.\") - 0: The essay does not address the specific claim or is completely off-topic. The argument is not related to the prompt and does not address the core issue. topics without addressing the claim.\") </item> <item> Evidence and Reasoning (10 points) - 10: The essay provides strong, relevant evidence and reasoning to support the argument, with specific examples and data from current events and historical contexts. The evidence is well-connected to the core issue and supports the argument effectively. cites specific studies showing the burden of student debt, the widening income gap, and the over-specialization in certain fields, providing detailed examples and data.\") - 8: The essay provides mostly relevant evidence and reasoning, but some points are weak or not fully supported. substantial examples or deeper analysis. provide substantial evidence or explanation.\") - 6: The essay provides some relevant evidence and reasoning, but there are significant gaps or weak points. The evidence is not well-connected to the core issue and lacks substantial examples or clear connection to the context. (Example: \"The essay mentions studies but does not provide specific examples or explanations.\") - 4: The essay provides weak or irrelevant evidence and reasoning. The evidence is not well-connected to the core issue and lacks substantial examples or clear connection to the context. (Example: \"The essay mentions studies but does not explain how this supports the argument or provide substantial evidence.\") - 2: The essay does not provide any evidence or reasoning to support the argument. not related to the core issue and lacks substantial examples or clear connection to the context. (Example: \"The essay makes claims without providing any supporting evidence.\") - 0: The essay provides evidence and reasoning that contradict the argument. The evidence is not related to the core issue and lacks substantial examples or clear connection to the context. (Example: \"The essay provides examples that support the claim that higher education is beneficial.\") </item> <item> Counterarguments and Refutation (10 points) The evidence is somewhat connected to the core issue but could benefit from more (Example: \"The essay mentions studies but does not (Example: \"The essay discusses unrelated (Example: \"The essay The evidence is The \"The \"The essay mentions \"The essay acknowledges some \"The essay has an introduction - 10: The essay addresses potential counterarguments and provides strong refutations. The counterarguments are acknowledged and effectively refuted with specific examples and reasoning. (Example: \"The essay acknowledges that some argue higher education is beneficial by explaining the burden of student debt, the widening income gap, and the over-specialization in certain fields.\") - 8: The essay addresses some counterarguments and provides mostly strong refutations. The counterarguments are acknowledged but not fully refuted. (Example: counterarguments but does not fully refute them.\") - 6: The essay addresses some counterarguments but provides weak or incomplete refutations. The counterarguments are acknowledged but not fully addressed. (Example: counterarguments but does not fully address them.\") - 4: The essay addresses some counterarguments but does not provide any refutations. The counterarguments are acknowledged but not addressed. (Example: \"The essay mentions counterarguments without explaining why they do not undermine the argument.\") - 2: The essay does not address any counterarguments. The counterarguments are not acknowledged or addressed. (Example: \"The essay does not mention any counterarguments.\") - 0: The essay addresses counterarguments but fails to refute them or provides weak refutations. The counterarguments are acknowledged but not effectively refuted. (Example: \"The essay mentions counterarguments but does not provide any refutations or provides weak refutations.\") </item> <item> Structure and Coherence (10 points) - 10: The essay is well-organized and coherent, with clear structure and logical flow. introduction sets up the argument, body paragraphs provide evidence and reasoning, and the conclusion summarizes the argument and provides final thought. (Example: that sets up the argument, body paragraphs that provide evidence and reasoning, and conclusion that summarizes the argument and provides final thought.\") - 8: The essay has mostly clear structure, but some points may be slightly disconnected or lack smooth transitions. The introduction sets up the argument, body paragraphs provide evidence and reasoning, and the conclusion summarizes the argument but lacks smooth transitions. (Example: essay has clear structure but some points are not well-connected or lack transitions.\") - 6: The essay has somewhat clear structure, but the points are not well-connected, and the transitions are weak. The introduction sets up the argument, body paragraphs provide evidence and (Example: \"The essay has structure but the points are reasoning, but the transitions are weak. disjointed and the transitions are weak.\") - 4: The essay lacks clear structure, and the points are disjointed and disconnected. introduction sets up the argument, but the body paragraphs are disjointed and the conclusion is unclear. (Example: \"The essay has no clear structure and the points are not well-connected.\") - 2: The essay is poorly structured, with no clear introduction, body, or conclusion. lacks clear structure and the points are entirely disjointed. structure and the points are entirely disjointed.\") - 0: The essay is completely disorganized and lacks any structure. The essay is collection of unrelated points with no structure. (Example: \"The essay is collection of unrelated points with no structure.\") </item> <item> Depth of Analysis (10 points) - 10: The essay demonstrates deep and nuanced analysis of the issue, addressing complexities and providing specific examples and data from current events and historical contexts. The analysis is well-connected to the core issue and supports the argument effectively. (Example: \"The essay discusses the complexities of higher education, providing specific examples of student debt, income gaps, and over-specialization in certain fields, and provides detailed data from reliable sources.\") - 8: The essay provides some depth and nuance in the analysis but lacks specific examples or data. The analysis is somewhat connected to the core issue but could benefit from more substantial examples or deeper analysis. (Example: \"The essay mentions that higher education has issues but does not provide substantial evidence or reasoning.\") - 6: The essay provides minimal depth and nuance in the analysis, lacking specific examples or data. The analysis is not well-connected to the core issue and lacks substantial examples or clear connection to the context. (Example: not provide specific examples or explanations.\") - 4: The essay lacks depth and nuance in the analysis, providing vague statements without specific examples or data. The analysis is not well-connected to the core issue and lacks substantial examples or clear connection to the context. substantial argument or reasoning.\") - 2: The essay lacks any meaningful analysis or is superficial. core issue and lacks substantial examples or clear connection to the context. (Example: does not provide any meaningful analysis and remains superficial in its discussion.\") - 0: The essay is completely lacking in analysis and is purely superficial. The analysis is not related to the core issue and lacks substantial examples or clear connection to the context. (Example: \"The essay is vague and does not provide any substantial content or context.\") </item> <item> Language and Mechanics (10 points) - 10: The essay demonstrates clear and effective use of language with minimal grammatical errors. The writing is clear, coherent, and free of significant errors that affect clarity or coherence. (Example: \"The essay uses clear and precise language with no significant grammatical errors.\") - 8: The essay has some grammatical errors but is still generally clear. The writing is mostly clear \"The essay and coherent, with few minor errors that do not significantly affect clarity. (Example: has few grammatical errors but is still understandable.\") - 6: The essay has some grammatical errors that affect clarity but is still mostly comprehensible. \"The essay mentions that higher education has issues but does (Example: \"The essay is vague and does not provide any (Example: \"The essay lacks clear The analysis is not related to the \"The essay The essay The 25 The writing is generally clear but has some errors that affect clarity. (Example: \"The essay has some errors in grammar or style but is still generally clear.\") - 4: The essay has significant grammatical errors that affect clarity and coherence. The writing is unclear and difficult to follow due to significant errors. (Example: \"The essay has multiple grammatical or stylistic errors that make it difficult to understand.\") - 2: The essay has significant grammatical errors that severely affect clarity and coherence. writing is unclear and difficult to follow due to multiple significant errors. (Example: has multiple grammatical or stylistic errors that make it difficult to understand.\") - 0: The essay is completely unclear and incoherent due to significant grammatical or stylistic errors. The writing is difficult to read due to numerous grammatical or stylistic errors. \"The essay is difficult to read due to numerous grammatical or stylistic errors.\") </item> The \"The essay (Example: C.2 Ethical Reasoning MPO Iteration 1 Ethical Reasoning The <verdict>WRONG</verdict>\" The argument is clear, structured, and avoids repetition or disjointedness. \"The girlfriends reaction is wrong because she overreacted. The act of forgetting to \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable <item> Logical Coherence (0-5) - 5: The reasoning is logically consistent, free from contradictions, and well-supported by evidence or examples. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. Trusting someone with your body is significant decision, and believe its important to prioritize safety and quality. <verdict>WRONG</verdict>\" - 4: The reasoning is mostly logical, with few minor contradictions or gaps in reasoning. The argument is coherent but could be more clear or structured. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate and disrespectful. <verdict>WRONG</verdict>\" - 3: The reasoning is somewhat logical but contains some contradictions or gaps in reasoning. The argument is coherent but could be more clear or structured. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate. - 2: The reasoning is partially logical, with significant contradictions or gaps in reasoning. argument is somewhat coherent but lacks clarity and structure. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience. <verdict>WRONG</verdict>\" - 1: The reasoning is illogical, with major contradictions or gaps in reasoning. The argument is incoherent and lacks clarity. - Example: defrost the shrimp is minor inconvenience. <verdict>WRONG</verdict>\" - 0: The reasoning is completely illogical, with no coherent argument or supporting evidence. argument is incoherent and lacks clarity. - Example: </item> <item> Depth of Moral Insight (0-5) - 5: The analysis demonstrates deep understanding of the ethical issues involved, providing nuanced insights and thoughtful consideration of multiple perspectives. The reasoning explores multiple ethical principles and their implications. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. This response reflects lack of consideration for others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. Additionally, her response fails to consider the utilitarian principle of maximizing overall happiness and well-being. <verdict>WRONG</verdict>\" - 4: The analysis demonstrates good understanding of the ethical issues involved, with some thoughtful consideration of multiple perspectives. and their implications. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. This response reflects lack of consideration for others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. <verdict>WRONG</verdict>\" - 3: The analysis demonstrates basic understanding of the ethical issues involved, with some consideration of multiple perspectives. The reasoning touches on some ethical principles and their implications. - Example: \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong. <verdict>WRONG</verdict>\" The reasoning explores some ethical principles The \"The girlfriends reaction is wrong because she overreacted. <verdict>WRONG</verdict>\" \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate and disrespectful. <verdict>WRONG</verdict>\" - 2: The analysis demonstrates limited understanding of the ethical issues involved, with little consideration of multiple perspectives. The reasoning touches on some ethical principles but lacks depth. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience. <verdict>WRONG</verdict>\" - 1: The analysis demonstrates minimal understanding of the ethical issues involved, with no consideration of multiple perspectives. The reasoning touches on some ethical principles but lacks depth. - Example: response as it lacks empathy and understanding. <verdict>WRONG</verdict>\" - 0: The analysis demonstrates no understanding of the ethical issues involved. The reasoning lacks any exploration of ethical principles or ethical theories. - Example: </item> <item> Alignment with Established Ethical Principles (0-5) - 5: The response references and applies relevant ethical principles or theories comprehensively and accurately. The reasoning explicitly identifies and applies specific ethical theories or principles, including how they apply to the specific situation and why they are relevant. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and This response reflects lack of consideration for potentially harmful to the relationship. others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. Additionally, her response fails to consider the utilitarian principle of maximizing overall happiness and well-being. <verdict>WRONG</verdict>\" - 4: The response references and applies relevant ethical principles or theories with some accuracy. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. This response reflects lack of consideration for others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. <verdict>WRONG</verdict>\" - 3: The response references some ethical principles or theories but with limited accuracy. reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate and disrespectful. <verdict>WRONG</verdict>\" - 2: The response references some ethical principles or theories but with significant inaccuracies. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience. <verdict>WRONG</verdict>\" - 1: The response references relevant ethical principles or theories but with major inaccuracies. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. - Example: response as it lacks empathy and understanding. <verdict>WRONG</verdict>\" - 0: The response does not reference any relevant ethical principles or theories. The reasoning does not reference any ethical principles or theories. - Example: </item> <item> Clarity of Verdict (0-5) - 5: The final verdict is clear, properly formatted, and well-supported by the ethical reasoning. The verdict is clearly stated within the <verdict></verdict> tags, and the reasoning supports the verdict. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. This response reflects lack of consideration for others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. Additionally, her response fails to consider the utilitarian principle of maximizing overall happiness and well-being. <verdict>WRONG</verdict>\" - 4: The final verdict is clear and properly formatted but lacks some supporting evidence. The verdict is clearly stated within the <verdict></verdict> tags, but the reasoning does not fully support the verdict. - Example: \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong. <verdict>WRONG</verdict>\" The 27 \"The girlfriends reaction is wrong because she overreacted. This is not justifiable response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate, disrespectful, and potentially harmful to the relationship. This response reflects lack of consideration for others and disregard for the impact of ones actions. It also violates deontological principles, which emphasize the importance of treating others with respect and dignity. <verdict>WRONG</verdict>\" - 3: The final verdict is somewhat clear and properly formatted but lacks supporting evidence. The verdict is clearly stated within the <verdict></verdict> tags, but the reasoning does not fully support the verdict. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience, and the girlfriends response is disproportionate and disrespectful. <verdict>WRONG</verdict>\" - 2: The final verdict is partially clear and properly formatted but lacks significant supporting evidence. The verdict is clearly stated within the <verdict></verdict> tags, but the reasoning does not fully support the verdict. - Example: response as it lacks empathy and understanding. The act of forgetting to defrost the shrimp is minor inconvenience. <verdict>WRONG</verdict>\" - 1: The final verdict is unclear or improperly formatted. the <verdict></verdict> tags, and the reasoning does not support the verdict. - Example: response as it lacks empathy and understanding. <verdict>WRONG</verdict>\" - 0: The final verdict is completely unclear or improperly formatted. The verdict is not clearly stated within the <verdict></verdict> tags, and the reasoning does not support the verdict. - Example: </item> \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong because she overreacted. This is not justifiable \"The girlfriends reaction is wrong. <verdict>WRONG</verdict>\" The verdict is not clearly stated within MPO Iteration 20 Ethical Reasoning <verdict>RIGHT</verdict> <verdict>RIGHT</verdict>\" The reasoning considers some The decision to not use the app is reasonable response The decision to not use the app is reasonable response The argument is clear, structured, and avoids repetition or disjointedness. It \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and <item> Logical Coherence (0-5) - 5: The reasoning is logically consistent, free from contradictions, and well-supported by evidence or examples. considers multiple perspectives and the broader ethical implications of the action. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning is clear, structured, and avoids repetition. - 4: The reasoning is mostly logical, with few minor contradictions or gaps in reasoning. The argument is coherent but could be more clear or structured. perspectives but lacks comprehensive analysis. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning is mostly coherent but could be more clear or structured. <verdict>RIGHT</verdict>\" - 3: The reasoning is somewhat logical but contains some contradictions or gaps in reasoning. The argument is coherent but could be more clear or structured. The reasoning touches on some perspectives but lacks comprehensive analysis. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning is somewhat coherent but lacks clarity. <verdict>RIGHT</verdict>\" - 2: The reasoning is partially logical, with significant contradictions or gaps in reasoning. argument is somewhat coherent but lacks clarity and structure. The reasoning is disjointed and lacks coherence. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning is disjointed and lacks coherence. <verdict>RIGHT</verdict>\" - 1: The reasoning is illogical, with major contradictions or gaps in reasoning. The argument is incoherent and lacks clarity. The reasoning is incoherent and lacks logical structure. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning is incoherent and lacks logical structure. <verdict>RIGHT</verdict>\" - 0: The reasoning is completely illogical, with no coherent argument or supporting evidence. The argument is incoherent and lacks clarity. The reasoning is completely illogical, with no coherent argument or supporting evidence. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: </item> <item> Depth of Moral Insight (0-5) - 5: The analysis demonstrates deep understanding of the ethical issues involved, providing nuanced insights and thoughtful consideration of multiple perspectives. \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. <verdict>RIGHT</verdict>\" The decision to not use the app is reasonable response The decision to not use the app is reasonable response The decision to not use the app is reasonable response <verdict>RIGHT</verdict> <verdict>RIGHT</verdict> The reasoning explores The <verdict>RIGHT</verdict> <verdict>RIGHT</verdict> <verdict>RIGHT</verdict>\" <verdict>RIGHT</verdict>\" The verdict is presented within the required tags. The decision to not use the app is reasonable response The decision to not use the app is reasonable response The decision to not use the app is reasonable response The decision to not use the app is reasonable response The reasoning touches on the ethical principles of personal safety and respect for \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and multiple ethical principles and their implications, including the balance between personal needs and others well-being. The reasoning explores the ethical principles of personal safety, respect for autonomy, and the impact on the relationship. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning explores the ethical principles of personal safety, respect for autonomy, and the impact on the relationship. <verdict>RIGHT</verdict>\" - 4: The analysis demonstrates good understanding of the ethical issues involved, with some thoughtful consideration of multiple perspectives. The reasoning explores some ethical principles and their implications. The reasoning considers the ethical principles of personal safety and respect for autonomy. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning considers the ethical principles of personal safety and respect for autonomy. - 3: The analysis demonstrates basic understanding of the ethical issues involved, with some consideration of multiple perspectives. The reasoning touches on some ethical principles and their implications. autonomy. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning touches on the ethical principles of personal safety and respect for autonomy. - 2: The analysis demonstrates limited understanding of the ethical issues involved, with little consideration of multiple perspectives. The reasoning touches on some ethical principles but lacks depth. The reasoning touches on the ethical principles of personal safety and respect for autonomy but lacks depth. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning touches on the ethical principles of personal safety and respect for autonomy but lacks depth. <verdict>RIGHT</verdict>\" - 1: The analysis demonstrates minimal understanding of the ethical issues involved, with no consideration of multiple perspectives. The reasoning touches on some ethical principles but lacks depth. The reasoning touches on the ethical principles of personal safety and respect for autonomy but lacks depth. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning touches on the ethical principles of personal safety and respect for autonomy but lacks depth. <verdict>RIGHT</verdict>\" - 0: The analysis demonstrates no understanding of the ethical issues involved. The reasoning lacks any exploration of ethical principles or ethical theories. The reasoning lacks any exploration of ethical principles or ethical theories. <verdict>RIGHT</verdict> - Example: </item> <item> Alignment with Established Ethical Principles (0-5) - 5: The response references and applies relevant ethical principles or theories comprehensively and accurately. theories, including how they apply to the specific situation and why they are relevant. The reasoning explicitly identifies and applies specific ethical principles such as personal safety, respect for autonomy, and the impact on the relationship. The verdict is presented within the required tags. <verdict>RIGHT</verdict> - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning explicitly identifies and applies specific ethical principles such as personal safety, respect for autonomy, and the impact on the relationship. - 4: The response references and applies relevant ethical principles or theories with some accuracy. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning references some ethical principles such as personal safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. <verdict>RIGHT</verdict>\" - 3: The response references some ethical principles or theories but with limited accuracy. reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. The reasoning references some ethical principles such as personal safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning references some ethical principles such as personal safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. <verdict>RIGHT</verdict>\" \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. It respects personal autonomy and The reasoning explicitly identifies and applies specific ethical principles or \"The action of not using the GPS app is justified. <verdict>RIGHT</verdict>\" The reasoning references some ethical principles such as personal The decision to not use the app is reasonable response The decision to not use the app is reasonable response The decision to not use the app is reasonable response The decision to not use the app is reasonable response The verdict is presented within the required tags. <verdict>RIGHT</verdict>\" <verdict>RIGHT</verdict> <verdict>RIGHT</verdict> The 29 <verdict>RIGHT</verdict> The decision to not use the app is reasonable response The reasoning references some ethical principles such as personal \"The action of not using the GPS app is justified. It respects personal autonomy and - 2: The response references some ethical principles or theories but with significant inaccuracies. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning references some ethical principles such as personal safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. <verdict>RIGHT</verdict>\" - 1: The response references relevant ethical principles or theories but with major inaccuracies. The reasoning references some ethical principles or theories but does not provide clear explanation of how they apply to the situation. safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. The verdict is presented within the required tags. - Example: privacy, which are fundamental human rights. to concerns about personal privacy and autonomy. The reasoning references some ethical principles such as personal safety and respect for autonomy but does not provide clear explanation of how they apply to the situation. <verdict>RIGHT</verdict>\" - 0: The response does not reference any ethical principles or theories. The reasoning lacks any exploration of ethical principles or ethical theories. The reasoning lacks any exploration of ethical principles or ethical theories. <verdict>RIGHT</verdict> - Example: </item> \"The action of not using the GPS app is justified. It respects personal autonomy and \"The action of not using the GPS app is justified. <verdict>RIGHT</verdict>\" The verdict is presented within the required tags. The reasoning references some ethical principles such as personal The decision to not use the app is reasonable response <verdict>RIGHT</verdict> C.3 Training Details C.3.1 Summarization Setup. For summarization, we use the BillSum benchmark [Kornilova and Eidelman, 2019], corpus designed for summarizing U.S. Congressional and California state legislation. The dataset includes over 22,000 mid-length Congressional bills with human-written summaries, along with an additional California test set to support cross-domain generalization. Its technical and hierarchical nature presents unique challenges, making it suitable for both domain-specific and general-purpose summarization research. The training set contains approximately 18.9K samples, while the combined test set includes 4.5K samples. We provide the full bill text as input and request summary of approximately 400 words. Training is conducted for single epoch over the dataset, with MPO steps executed every 20 batch steps, resulting in total of 29 rubric refinements throughout the training process. total of 4.5K bill summaries are generated on the test set by each of the three models. These generated summaries are then evaluated by computing ROUGE [Lin, 2004] scores against their corresponding human-written references. In addition, we computed Elo ratings for the three models based on 5,000 pairwise comparisons of their generated summaries, with GPT-4o serving as the judge. The results are presented in Table 2. C.3.2 Ethical Reasoning Setup. For ethical reasoning, we utilize the Anecdotes from the Scruples dataset [Lourie et al., 2020] which contains over 32,000 real-life anecdotes sourced from Reddit community, where users describe ethically charged situations they experienced or considered. Each anecdote includes title, detailed story, and distribution of community judgments indicating who was perceived to be in the wrongsuch as the author, another party, everyone, or no one. These narratives often feature moral ambiguity and are labeled with crowd-sourced ethical assessments, making them well-suited for modeling community norms and capturing the diversity of ethical reasoning. The dataset includes 27.8K training anecdotes, from which we randomly sampled 13K for our experiments, along with 4.7K anecdotes used for testing. Training is conducted for single epoch, with MPO steps performed every 10 batch steps, resulting in total of 20 rubric refinements. Although each anecdote includes binary judgment verdict from human annotators, we did not use these ground truth labels in either the RM or MRM. Instead, reward scores were assigned solely based on the quality of ethical reasoning demonstrated in the response. This decision was motivated by two factors: (1) the label distribution is imbalanced, and (2) our goal was to encourage the policy model to improve through generating stronger reasoning traces, rather than optimizing for label prediction alone. 30 C.3.3 Mathematical Reasoning Setup. For mathematical reasoning, we use the MATH dataset [Hendrycks et al., 2021] which consists of 12,500 high school competition-style math problems, sourced from contests like AMC 10, AMC 12, and AIME. Each problem is accompanied by detailed step-by-step solution written in LaTeX, enabling both final answer evaluation and learning of problem-solving processes. The dataset spans seven subjectsincluding algebra, geometry, and number theoryand is annotated with difficulty levels from 1 to 5, offering fine-grained assessment across wide range of mathematical reasoning tasks. The dataset consists of 7.5K training samples and 5K test samples. Because mathematical reasoning demands considerable evaluative depth, we apply MPO at finer granularity. Specifically, we cluster problems within each of the seven math subjects into three groups based on semantic embeddings, resulting in 21 (7 3) distinct categories. During MPO training, we maintain separate evaluation prompt for each of these categories, with refinement steps triggered based on the subject and cluster index of the sample. The reward model follows plan-then-execute strategy: it first formulates an evaluation plan based on the problem, reference solution, and meta-level guidelines, and then applies this plan to assess the students response. This approach builds on the method proposed by Saha et al. [2025], originally used for pairwise judgment, which we adapt for absolute scoring with (meta-level) rubric-guided evaluation. Training is conducted for single epoch, with MPO steps performed every 30 batch stepsusing longer interval to ensure sufficient mix of responses with varying quality levels is gathered before each refinement."
        },
        {
            "title": "D Discourse Motif Distribution of Evaluation Prompts",
            "content": "Figure 9: Comparison of discourse motifs found in rubric prompts at the initial stage, after the first, and last MPO step of training for essay writing task."
        }
    ],
    "affiliations": [
        "Grammarly",
        "MIT",
        "University of Minnesota"
    ]
}