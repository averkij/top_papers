{
    "paper_title": "BABE: Biology Arena BEnchmark",
    "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 7 5 8 5 0 . 2 0 6 2 : r BABE: Biology Arena BEnchmark Junting Zhou1,2,, Jin Chen1,, Linfeng Hao2,, Denghui Cao1, Zheyu Wang1, Qiguang Chen1, Chaoyou Fu1, Jiaze Chen1, Yuchen Wu1, Ge Zhang1, Mingxuan Wang1,, Wenhao Huang1,, Tong Yang2, 1ByteDance Seed, 2Peking University Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE (Biology Arena BEnchmark), comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides robust framework for assessing how well AI systems can reason like practicing scientists, offering more authentic measure of their potential to contribute to biological research. Date: February 6, 2026 Correspondence: Junting Zhou at juntingzhou@stu.pku.edu.cn; Tong Yang at yangtong@pku.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "The evolution of large language models (LLMs) has witnessed paradigm shift from basic conversational capabilities to advanced reasoning functionalities. Early-generation models excelled at generating coherent chatstyle responses, but modern foundation models have expanded into scientific research capabilitiesincluding hypothesis generation, data analysis, and experimental design. This shift has drawn significant attention to evaluating LLMs performance in specialized scientific domains, particularly biology, where complex experimental data and interdisciplinary knowledge demand more than trivial pattern recognition. critical yet underdeveloped aspect of assessing biological AI systems is their ability to reason based on experimental results and contextual background, which is core skill for biological researchers. For instance, interpreting Western blot image to infer protein expression changes requires integrating visual data (for example: band intensity, loading controls) with experimental context (for example: treatment conditions, cell lines) and domain knowledges. This kind of problem is challenge even for the strongest current SOTA models. However, existing benchmarks rarely test this integrated reasoning ability, instead focusing on isolated tasks like sequence classification or structure prediction. 1 Figure 1 Overview of the BABE (Biology Arena BEnchmark) construction and composition. (A) The multi-stage annotation pipeline for constructing the BABE benchmark. (B) The disciplinary distribution of questions in the BABE benchmark, covering 12 subfields of biology. (C) The proportion of strong-correlation (45%) and weak-correlation (55%) questions in the final BABE benchmark. To address these gaps, we develop BABE(Biology Arena BEnchmark1), benchmark specifically designed to evaluate biological AI systems experimental reasoning capabilities. Critically, all tasks in BABE are derived from peer-reviewed research papers and real-world biological studies. This ensures that the benchmark reflects the true complexity of biological research and tests models ability to reason like practicing scientists. In summary, Our work makes three primary contributions: Experimental Reasoning Focus: Unlike existing benchmarks, BABE centers on tasks that require models to integrate experimental results with contextual background to derivate biological conclusions. High-Difficulty, Research-Derived Tasks: All tasks are adapted from peer-reviewed papers and designed to demand causal reasoning and cross-scale inference. Broad Domain Coverage: We cover major biological domains, using tasks from diverse subfield studies, enabling evaluation of model generalization across real-world biological research areas."
        },
        {
            "title": "2.1 Deep Research Agents",
            "content": "The initial success of Large Language Models (LLMs) in natural language tasks was often hampered by inherent limitations in multi-step reasoning, knowledge cut-offs, and the propensity for hallucination [4]. This motivated critical shift from static generation toward the development of sophisticated Deep Research Agents. These agents are designed to tackle complex, multi-turn informational research tasks by equipping the LLM core with structured control loop, thereby enabling dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical 2 reports [2, 17, 20]. The introduction of agency relies on three primary functional pillars: (1) Planning and Control, typically achieved through mechanisms like iterative planning and self-reflection to maintain task alignment and correct errors [9]; (2) External Tool Utilization, encompassing code interpreters, specialized APIs, and web search to access computational and real-time resources [14]; and (3) Contextual Grounding, most commonly implemented via Retrieval-Augmented Generation (RAG) [7] to seamlessly incorporate up-to-date or proprietary domainspecific data. This composite framework allows agents to interact dynamically with complex environments and accumulate long-term memory, thereby achieving sustained logical reasoning and verifiable evidence retrieval necessary for accelerated research, literature review synthesis, and scientific hypothesis generation. However, evaluating the true scientific utility of these systems remains significant challenge. Crucially, the effective assessment of these domain-specific agents, particularly in high-stakes fields like Biology and Medicine, necessitates challenging benchmarks that test deep comprehension, multi-step causal reasoning, and faithful evidence extraction over highly specialized and voluminous literature [8], moving beyond general knowledge assessment."
        },
        {
            "title": "2.2 Scientific Benchmarks",
            "content": "General scientific benchmarks have emerged to evaluate AI systems across disciplines like physics, chemistry, and biology[10, 12, 23], with focus on testing domain knowledge and problem-solving skills. Initial efforts focused on high-difficulty, factual question-answering, exemplified by GPQA [13] and its scaled successor SuperGPQA [15], which feature expert-authored, graduate-level questions. Other highly challenging benchmarks include HLE [11], which assesses advanced STEM and humanities through short-answer and multimodal tasks, and R-Bench [19], which targets Graduate/Olympiad-level reasoning using bilingual, multimodal inputs. Furthermore, SciEval [18] aggregates academic datasets for broad STEM coverage, while OlympicArena [3] evaluates multidisciplinary cognitive reasoning with focus on process-level assessment."
        },
        {
            "title": "2.3 Biology-Specific Benchmarks",
            "content": "Biology-specific benchmarks have focused on subfield-specific tasks, but few address the integrated experimental reasoning that is critical for advancing the field. Table 1 summarizes key biology benchmarks and their limitations relative to BABE. Several benchmarks are Sequence-Centric. For example, benchmarks like BiologyInstructions [1] and ProteinBench [21] focus on sequence-based tasks (e.g., DNA sequence alignment, protein secondary structure prediction). Understanding structures of Bio-macromolecules is crucial task in biology. Hence, there are Structure-Centric Benchmarks, especially in protein area. For example, ProteinShake[6] and PepPCBench [22] evaluate models on protein structure, using PDB-formatted structures to assign proteins to fold families. While these benchmarks are valuable for assessing structural biology models [5]), they do not require models to interpret experimental data related to structures. Meanwhile, there is small number of biology benchmarks incorporate multiple modalities, but they lack the depth of experimental reasoning required for real research. For example, BioASQ [16] organizes challenges on biomedical semantic indexing and question answering relevant to hierarchical text classification, machine learning, information retrieval, QA from texts and structured data, multi-document summarization and many other areas. OlymBench is new benchmark designed to evaluate the reasoning ability of large language models, with challenges reaching competition difficulty. All existing biology-specific benchmarks fail to address one or more critical needs: Real experimental data: Most use simplified data or summarized data rather than figures and datasets from published papers. Integrated reasoning: Tasks do not require linking experimental results to contextual background; Broad domain coverage: Benchmarks are limited to single subfields rather than spanning multiple biological domains. BABE addresses these gaps by focusing on research-derived, multimodal tasks that require the same reasoning as practicing biologists. 3 Table 1 Key Biology Benchmarks and their Gaps Relative to BABE Benchmark ProteinBench ProteinShake PepPCBench BioASQ OlymBench Real Experimental Data No No No No No BABE Yes"
        },
        {
            "title": "3 Approach",
            "content": "Integrated Reasoning Domain Coverage Limited Limited Limited Medium High High Narrow Narrow Narrow Broad Broad Primary Focus Computational Metrics Standardized Structural Data Structure Prediction Accuracy Text/Sequence QA Corpus High-difficulty Logical Deduction Broad Research-Derived Multimodal Tasks"
        },
        {
            "title": "3.1 Problem Formulation\nThe Biology Arena Benchmark (BABE) is introduced as a novel diagnostic framework designed to rigorously\nevaluate Large Language Models (LLMs) on complex reasoning tasks within the biomedical domain, specifically\nover a single source research document D. Unlike existing benchmarks, which often emphasize factual recall\nor single-hop retrieval, fail to fully diagnose model robustness in compositional reasoning, our benchmark\nfocus on handling the complexity and quantitative content inherent in scientific literature. BABE addresses\nthis limitation by defining a structured question triplet:",
            "content": "QBABE = {Q1, Q2, Q3}, where the logical relationship between consecutive questions, R(Qi, Qi+1), is formally classified into two distinct diagnostic categories: Strong Correlation (RStrong) and Weak Correlation (RWeak). This structured methodology is essential for precisely measuring LLM depth (sequential inference) and breadth (parallel extraction) of understanding in domain-specific context. The problem formulation is centered on the interdependency within the question set QBABE. The overall type of benchmark instance is defined by the combined logical relations:"
        },
        {
            "title": "QType",
            "content": "BABE R(Q1, Q2) R(Q2, Q3). The Strong Correlation relation (RStrong) captures sequential, multi-hop reasoning, where the derived output of preceding question is necessary input for the subsequent one. This relationship is formally defined as: RStrong i, j(i < j), Aj requires Ai for optimal derivation from D. Conversely, the Weak Correlation relation (RWeak) captures parallel, independent extraction, where questions are logically uncoupled, testing the models ability to maintain multiple distinct contexts simultaneously. This is formalized as: RWeak Ci s.t. Ai = Extract(Ci), and = j, Ci Cj . The formal constraint of RStrong diagnoses failure modes related to error propagation and reasoning drift in Chain-of-Thought (CoT) processes, while RWeak diagnoses issues such as semantic interference during simultaneous knowledge retrieval."
        },
        {
            "title": "3.2 Data Collection",
            "content": "To construct high-quality benchmark tailored for rigorous evaluation of domain-specific reasoning, we adopted multi-stage data collection pipeline integrating frontier literature curation, expert-driven item development, and structured quality assurance. As shown in 1(A), We first curated corpus of cutting-edge scientific materials, including recently published peer-reviewed papers, domain-specific monographs, and authoritative review articles. The selection criteria 4 emphasized (i) recency of publication, (ii) relevance to the target scientific domain, and (iii) conceptual depth suitable for assessing multi-step reasoning. Only sources meeting all criteria were retained for downstream question construction. For each selected paper or book chapter, domain experts generated set of three assessment items. The items were designed to probe different cognitive dimensions, including conceptual understanding, methodological interpretation, and higher-order reasoning. All questions were required to be self-contained, unambiguous, and faithful to the source material while avoiding superficial fact-retrieval prompts. secondary panel of senior experts conducted rigorous review of all drafted items. The review served two purposes: Relevance Assessment: Each question was labeled as either strong correlation or weak correlation to the core knowledge unit extracted from the source text. Strongly related questions directly test key concepts and reasoning chains presented in the material, whereas weakly related questions assess peripheral or contextual understanding. Correctness Verification: Reviewers evaluated the factual fidelity, logical coherence, and answer correctness for every item. Items passing both relevance assessment and correctness verification were accepted into the benchmark. Items found to contain factual inaccuracies, ambiguous phrasing, or misalignment with the source were returned to the original authors for revision. After revision, questions underwent second-round review before being considered for final inclusion. With the help of LLMs, simple questions were removed by the reviewers in this round. Only items that successfully completed the full developmentreviewrevision cycle were incorporated into the released benchmark. This multi-layered pipeline ensures that the dataset captures high-quality, expert-vetted questions with explicit relevance annotations, enabling more fine-grained evaluation of model capabilities across varying levels of semantic alignment with the underlying scientific sources."
        },
        {
            "title": "4.1 Overall Performance Analysis",
            "content": "Table 2 Model Performance on BABE Comparison Model Name OpenAI-GPT-5.1-high Gemini-3-Pro-Preview-Exp OpenAI-o3-high.code Doubao-1.8-1228 Gemini-2.5-Pro Claude-Sonnet-4.5-thinking-azure Doubao-1.6-1015-high.foreval Doubao-1.6-thinking.0715.foreval Doubao-1.6-1015-high.foreval (A) Claude-Opus-4.1-thinking-azure OpenAI-gpt4.1-0414 Doubao-1.6-1015-high.foreval (B) QwenAPI-vl-max.latest Doubao-1.5-pro-thinking.vision.0428.eval Claude-Sonnet-4.5-nothinking-azure QwenAPI-3-max-0923 GPT4o-1120 Doubao-1.6-flash.0828.foreval GLM-4.5-V Average Score 52.31 52.02 51.62 43.27 42.17 41.93 39.84 39.54 39.34 37.35 36.86 36.64 32.81 31.71 31.66 26.54 23.93 22.04 20.83 5 Strong Correlation Weak Correlation 51.79 49.05 51.22 43.38 42.02 41.94 40.70 39.67 35.10 36.30 32.61 34.88 32.89 32.12 33.67 27.70 20.80 20.34 18. 52.86 55.16 52.05 43.16 42.33 41.91 38.94 39.40 43.81 38.46 41.34 38.49 32.72 31.28 29.55 25.32 27.22 23.83 23.72 Table 2 presents comprehensive comparison of different models on the BABE benchmark under both strong and weak correlation settings. Overall, OpenAI-GPT-5.1-high achieves the best performance, obtaining the highest average score of 52.31, and demonstrating consistently strong results across both strong (51.79) and weak (52.86) correlation subsets. This indicates robust reasoning capabilities that generalize well across varying dependency structures. Models in the lower performance tier generally struggle with both correlation settings, with notable variations. For example, OpenAI-gpt4.1-0414 (36.86) performs better in weak correlation (41.34) than strong correlation (32.61), while Claude-Sonnet-4.5-nothinking-azure (31.66) shows the opposite trend (33.67 for strong correlation vs. 29.55 for weak correlation), indicating divergent design trade-offs in handling explicit vs. implicit reasoning. The lowest-performing models, such as GLM-4.5-V (20.83) and Doubao-1.6-flash.0828.foreval (22.04), score consistently low across both subsets, suggesting fundamental limitations in reasoning capabilities for the BABE benchmarks task requirements."
        },
        {
            "title": "4.2 Strong vs. Weak Correlation",
            "content": "We observe notable performance differences between strong and weak correlation scenarios for several models. For example, Gemini-3-Pro-Preview-Exp exhibits clear advantage under weak correlation conditions (55.16), substantially outperforming its strong correlation score (49.05). This suggests that the model is particularly effective when explicit logical dependencies are reduced, potentially benefiting from broader contextual reasoning. In contrast, models such as Claude-Sonnet-4.5-thinking-azure and Gemini-2.5-Pro show highly balanced performance across the two settings, indicating stable behavior regardless of correlation strength."
        },
        {
            "title": "4.3 Reasoning Behavior Analysis on BABE",
            "content": "Figure 2 The Reasoning Behavior Distribution on BABE across four LLMs. BABE requires deeper reasoning. To better understand the reasoning demands of BABE, we compare models at the two extremes of performance: the two best-performing models and the two worst-performing models. Figure 2 shows clear association between success on BABE and the prevalence of Deep Reasoning behaviors during inference. Specifically, higher-performing models devote substantially larger portion of their inference steps to deep reasoning, whereas lower-performing models exhibit much smaller share of such behaviors. This pattern indicates that BABE is not primarily solved by shallow pattern matching; instead, it rewards deeper reasoning that resolves implicit or non-trivial dependencies in the input. Excessive self-reflection on BABE can lead to substantial degradation in reasoning performance. Figure 2 also reveals second, more subtle failure mode among the worst-performing models: they exhibit episodic Self-Reflection at notably higher rate, often exceeding their own proportion of deep reasoning. While limited self-reflection can be beneficial, these results suggest that frequent, repeated self-reflection without commensurate progress in deep reasoning is associated with worse outcomes on BABE. plausible interpretation is that weaker models fall into an overthinking loopspending many steps reconsidering intermediate thoughts or reformulating the approachyet failing to advance the core reasoning needed to 6 reach correct conclusions. In practice, this behavior consumes the inference budget and increases the chance of drifting away from relevant evidence on BABE, which ultimately harms accuracy. Strong performance on BABE depends on sustained, evenly applied deep reasoning. Beyond overall proportions, we observe that strong models tend to maintain deep reasoning consistently throughout the inference trajectory. In contrast, some models may begin with deep reasoning but gradually reduce such behaviors later in the process, yielding sparser and less stable reasoning pattern. Figure 2 suggests that this early burst is insufficient for BABE: correctly solving BABE examples often requires repeatedly revisiting earlier premises, integrating newly derived implications, and maintaining coherent multi-step constraints until final decision is justified. Therefore, strong performance on BABE depends not only on initiating deep reasoning, but on sustaining it in relatively uniform manner across the full inference process."
        },
        {
            "title": "4.4 Convergence Score Analysis",
            "content": "Figure 3 Performance gain from multi-trial inference. We define the gain of multi-trial inference as: Gain(n) = model-BoN(n) avg-score(n), where BoN is short for \"Best of N\", denotes the number of inference trials. This metric isolates the improvement obtained purely from repeated sampling and aggregation, independent of the models average single-run performance. To estimate the saturation behavior of gain, we fit parametric saturating function: Gain(n) = (cid:0)1 ebn(cid:1) , where represents the asymptotic (converged) gain and controls the convergence speed. Parameters are estimated using nonlinear least squares on observed gains at = {1, 2, 4, 8}. Figure 3 shows the gain achieved by increasing the number of inference trials. All models exhibit monotonic increase in gain, confirming that multi-trial inference consistently improves performance beyond expected single-run outcomes. However, the slope of the curves decreases as grows, indicating diminishing marginal returns and the onset of convergence. Strong reasoning models such as OpenAI-GPT-5.1-high and Gemini-3-Pro-Preview-Exp exhibit relatively fast convergence, with predicted asymptotic gains around 30 points. These models show limited additional improvement beyond moderate number of trials, suggesting that their reasoning quality is already robust in single or few-shot inference. In contrast, several mid-tier models (e.g., Gemini-2.5-Pro and Claude-Opus-4.1-thinking-azure) display higher estimated gain limits, exceeding 35 points. This indicates greater diversity in their generated reasoning trajectories, allowing aggregation to recover substantially better solutions despite weaker average performance. 7 Trial OpenAI-GPT-5.1 Gemini-3 Gemini-2.5-pro Claude-S-4.5 Doubao-1.6 Claude-O-4.1 Avg 52.31 54.14 53.87 55.09 1 2 4 8 Convergence BoN 52.31 63.87 69.06 76.01 81.90 Avg BoN Avg BoN Avg BoN Avg BoN Avg BoN 52.02 52.02 42.17 54.16 62.55 45.06 54.32 69.55 45.14 53.59 74.75 45.14 86. 42.17 56.33 62.39 69.63 81.31 41.93 43.18 42.23 42.60 41.93 52.19 59.99 64. 71.78 39.84 39.84 37.35 36.70 46.29 37.03 37.00 53.04 37.31 37.58 59.97 36.35 80.77 37.35 44.15 54.99 58. 73.73 Table 3 Raw performance under multi-trial inference. Rows correspond to different trial numbers and the predicted asymptotic convergence value. For each model, we report the average score (Avg) and round-based score (Round). The convergence value is estimated by fitting saturating exponential model using trial results at {1, 2, 4, 8}. Taken together, these results indicate that success on BABE generally requires at least 46 inference trials even for frontier models, and 8+ trials for most non-frontier models, highlighting the intrinsic difficulty of experimental reasoning tasks and the limitations of single-pass inference."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced BABE (Biology Arena BEnchmark), novel and rigorous benchmark designed to evaluate the experimental reasoning capabilities of large language models in biology. Unlike existing benchmarks that emphasize factual recall, sequence-level prediction, or isolated subtasks, BABE focuses on core competency of real biological research: integrating experimental results with contextual background to derive scientifically meaningful conclusions. BABE is constructed entirely from peer-reviewed research papers and real-world biological studies, ensuring that its tasks faithfully reflect the complexity, interdisciplinarity, and ambiguity inherent in actual scientific inquiry. By organizing questions into structured triplets with explicitly defined strong and weak correlation relationships, BABE provides fine-grained diagnostic framework for assessing both sequential multi-hop reasoning and parallel information extraction within single source document. This formulation enables more precise identification of reasoning failure modes that are often obscured in conventional benchmarks. Overall, BABE fills critical gap in the evaluation of biological AI systems by moving beyond task-level competence toward research-level reasoning. We believe BABE will serve as valuable testbed for future advances in biologically grounded LLMs, deep research agents, and multimodal scientific reasoning systems. Looking forward, we hope this benchmark will encourage the development of models that reason more like practicing scientistsgrounding their conclusions in evidence, maintaining coherence across experimental contexts, and ultimately contributing more reliably to real biological discovery."
        },
        {
            "title": "References",
            "content": "[1] Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, and Peng Ye. Biology-instructions: dataset and benchmark for multi-omics sequence understanding capability of large language models, 2025. URL https://arxiv.org/abs/2412.19191. [2] Jian Huang, Ming Li, and Wei Chen. systematic review of deep research agents: Architecture, applications, and challenges, 2024. Placeholder entry for very recent or forthcoming survey. [3] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. arXiv preprint arXiv:2406.12753, 2024. [4] Zexin Ji, Nayeon Lee, Siffi Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishigaki, Ying Liu, Huaxiu Wen, Yong Liu, et al. Survey of hallucination in large language models. ACM Computing Surveys, 56(7):138, 2023. doi: 10.1145/3610931. [5] J. Jumper, D. Hassabis, et al. Advanced protein structure prediction with AlphaFold3, 2024. Placeholder entry for the latest generation AlphaFold model. [6] Tim Kucera, Carlos Oliver, Dexiong Chen, and Karsten Borgwardt. Proteinshake: Building datasets and benchmarks Information Processing Systems, volume 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ b6167294ed3d6fc61e11e1592ce5cb77-Paper-Datasets_and_Benchmarks.pdf. These authors contributed equally. for deep learning on protein structures. In Advances in Neural [7] Patrick Lewis, Ethan Perez, Aleksandr Piktus, Fabio Fan, Timothee Humeau, Bartlomiej Oglaza, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. [8] Zhi Li, Xingyu Zhou, Hao Wang, Chen Zhao, et al. Biobench: comprehensive benchmark for evaluating large language models in biomedicine, 2024. Placeholder entry for biomedical reasoning benchmark. [9] Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023. [10] Yebo Peng, Zixiang Liu, Yaoming Li, Zhizhuo Yang, Xinye Xu, Bowen Ye, Weijun Yuan, Zihan Wang, and Tong Yang. Proof2hybrid: Automatic mathematical benchmark synthesis for proof-centric problems, 2025. URL https://arxiv.org/abs/2508.02208. [11] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, et al. Humanitys last exam. ArXiv, abs/2501.14249, 2025. [12] Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Jiaming Ji, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming xing Luo, Yaodong Yang, Muhan Zhang, and Hua Xing Zhu. Phybench: Holistic evaluation of physical perception and reasoning in large language models, 2025. URL https://arxiv.org/abs/2504.16074. [13] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. ArXiv, abs/2311.12022, 2023. [14] Timo Schick, Eric Sch\"utze, Vibhav Dwivedi, Joshua de Silva, Ed Wortsman, et al. Toolformer: Language models that can use tools, 2023. [15] M-A-P Team, Jiaheng Liu, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. ArXiv, abs/2502.14739, 2025. [16] George Tsatsaronis, Georgios Balikas, Paris Malakasiotis, et al. BioASQ: challenge on large-scale biomedical semantic indexing and question answering. BMC Bioinformatics, 16(1):116, 2015. [17] Litu Wang, Yu Chen, Wenlin Ding, Jun Zhu, Bo Wang, et al. survey on large language model based autonomous agents, 2023. 9 [18] Siyuan Weng, Yu Zhang, Yuntian Wang, et al. SciBench: An open-source framework for scientific evaluation of llms, 2023. [19] Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, and Rongrong Ji. Evaluating and analyzing relationship hallucinations in large vision-language models. In Proceedings of the 41st International Conference on Machine Learning, 2024. [20] Shunyu Yao, Dianbo Zhao, Mo Yu, Lai Po-Ting, Yejin Hazoom, Abbas Mirhoseini, Wen-tau Hao, et al. React: Synergizing reasoning and acting in language models, 2023. [21] Fei Ye, Zaixiang Zheng, Dongyu Xue, Yuning Shen, Lihao Wang, Yiming Ma, Yan Wang, Xinyou Wang, Xiangxin Zhou, and Quanquan Gu. Proteinbench: holistic evaluation of protein foundation models, 2024. URL https://arxiv.org/abs/2409.06744. [22] Silong Zhai, Huifeng Zhao, Jike Wang, Shaolong Lin, Tiantao Liu, Shukai Gu, Dejun Jiang, Huanxiang Liu, Yu Kang, Xiaojun Yao, and Tingjun Hou. Peppcbench is comprehensive benchmarking framework for proteinpeptide complex structure prediction. Journal of Chemical Information and Modeling, 65(16):8497 8513, 2025. ISSN 1549-9596. doi: 10.1021/acs.jcim.5c01084. URL https://doi.org/10.1021/acs.jcim.5c01084. [23] Zehua Zhao, Zhixian Huang, Junren Li, Siyu Lin, Junting Zhou, Fengqi Cao, Kun Zhou, Rui Ge, Tingting Long, Yuexiang Zhu, Yan Liu, Jie Zheng, Junnian Wei, Rong Zhu, Peng Zou, Wenyu Li, Zekai Cheng, Tian Ding, Yaxuan Wang, Yizhao Yan, Tingru Wei, Haowei Ming, Weijie Mao, Chen Sun, Yiming Liu, Zichen Wang, Zuo Zhang, Tong Yang, Hao Ma, Zhen Gao, and Jian Pei. Superchem: multimodal reasoning benchmark in chemistry, 2025. URL https://arxiv.org/abs/2512.01274."
        },
        {
            "title": "A Example Questions of BABE",
            "content": "Figure 4 Example Question 1 of BABE 11 Figure 5 Example Question 2 of BABE 12 Figure 6 Example Question 3 of BABE Figure 7 Example Question 4 of BABE 14 Figure 8 Example Question 5 of BABE 15 Figure 9 Example Question 6 of BABE Figure 10 Example Question 7 of BABE 17 Figure 11 Example Question 8 of BABE"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Peking University"
    ]
}