{
    "paper_title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "authors": [
        "Wei Du",
        "Shubham Toshniwal",
        "Branislav Kisacanin",
        "Sadegh Mahdavi",
        "Ivan Moshkov",
        "George Armstrong",
        "Stephen Ge",
        "Edgar Minasyan",
        "Feng Chen",
        "Igor Gitman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR). The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality. Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks. To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 9 8 4 5 1 . 2 1 5 2 : r 2025-12Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Wei Du, Shubham Toshniwal, Branislav Kisacanin, Sadegh Mahdavi, Ivan Moshkov, George Armstrong, Stephen Ge, Edgar Minasyan, Feng Chen, Igor Gitman Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR). The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the datasets quality. Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems, and incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math-competition benchmarks. To support efficient long-context training, we develop sequential bucketed strategy that accelerates 128K context-length fine-tuning by 23 without significant accuracy loss compared to the full-length training. To verify the scalability of our supervision, we further perform experiments on Qwen3-8B and Qwen3-30B-A3B, showing that both models converge to similar final accuracy under our full context training recipe. Overall, Nemotron-Math provides diverse, high-quality, and scalable reasoning supervision, enabling state-of-the-art performance, including 100% maj@16 accuracy on AIME 2024/2025 for both Qwen3-8B and Qwen3-30B-A3B with Python TIR. 1. Introduction Mathematical problem-solving remains key benchmark for evaluating the reasoning capabilities of large language models (LLMs). Unlike general natural language processing tasks, mathematical problems often require multi-step logical deduction, symbolic manipulation, and long-context understanding. Recent large-scale datasets, such as OpenMathInstruct-2 [21], Skywork-MathQA [24], and NuminaMath [11], have substantially advanced the study of mathematical reasoning in LLMs. Building on these efforts, subsequent datasets such as BackMATH [25] and OpenMathReasoning [15] extend this progress toward competition-level and olympiad-style reasoning. Despite these advances, most existing mathematical reasoning datasets are generated by single mode reasoning models, which produce relatively uniform solution styles and limited variation in reasoning depth or tool usage. At the same time, many recent efforts have focused primarily on increasing the difficulty of competition-style mathematical problems. While highly effective for constructing challenging reasoning data, these datasets tend to focus on formal, competition-style problems that cover relatively narrow range of mathematical domains. As result, current mathematical reasoning datasets capture correctness and complexity but only partially reflect the broader spectrum of reasoning behaviors encountered across diverse mathematical problems. The recently released GPT-OSS family of openweight reasoning models, with gpt-oss-120b [4] as the flagship, offers new opportunity to address these limitations. Unlike prior models, it provides three controllable reasoning modes, high, medium, and low, that produce solutions of varying depth and length, and it can generate exceptionally detailed toolintegrated reasoning traces through extensive Python calls. These capabilities make it possible to construct datasets that capture diverse reasoning styles, selfverification behaviors, and tool-usage patterns that were previously inaccessible. Building on this opportunity, we construct Nemotron-Math1, large-scale mathematical reasoning dataset designed to combine rich reasoning diversity with long-context supervision. On the data side, we curate high-quality mathematical problem set by combining structured, competition-style questions from OpenMathReasoning [15] with diverse, community-driven questions from Math Stack 1https://huggingface.co/datasets/nvidia/NemotronMath-v2 2025 NVIDIA. All rights reserved. Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Exchange [20] and MathOverflow [19], which together constitute the StackExchange-derived portion (StackExchange-Math) of our dataset. Following prior work, we remove proof-style items and further filter out problems that are too easy for gpt-oss-120b: for each question, the model generates 16 solutions with low reasoning mode (8 with and 8 without Python tool-integrated reasoning (TIR)), and any problem with high pass rate (>= 0.8) is discarded as trivial. This filtering reduces the Art of Problem Solving (AoPS) [1] portion, corresponding to the AoPS problems included in [15], from the original 175K problems to 85K, and reduces the StackExchange-Math portion from 651K to 262K, resulting in balanced collection of challenging and nontrivial tasks. From this filtered problem pool, we retain the multimode trajectories generated by gpt-oss-120b under high, medium, and low reasoning modes, each both with and without Python TIR. After removing trajectories that fail to reach the reference answer, the final Nemotron-Math corpus contains 7.5M highquality long-form reasoning traces up to 128K tokens in length, capturing diverse reasoning depths, selfverification styles, and tool-usage behaviors. These properties make Nemotron-Math rich resource for studying long-context and tool-augmented mathematical reasoning. To further enhance the efficiency of long-context fine-tuning on such large-scale data, we propose sequential bucketed training strategy that groups samples by sequence length and trains the model progressively from 16K to 128K tokens. This staged approach enables optimized parallelism configurations at each length scale, significantly improving training throughput and resource utilization. Although it may introduce minor accuracy trade-offs compared to fulllength joint training, the method achieves 23 faster training while maintaining strong overall performance. Building on these developments, our main contributions are summarized as follows: We present Nemotron-Math, large-scale mathematical-reasoning dataset that contains 7.5M long-form solution traces produced by gpt-oss-120b under three distinct reasoning modes (high, medium, and low), both with and without Python TIR. By integrating 85K curated AoPS problems with 262K diverse StackExchange-Math questions, the dataset provides broad domain coverage and rich variability in reasoning depth, style, and tool usage. Through controlled comparisons, we show that Nemotron-Math provides substantially higherquality supervision than prior datasets. Under the high reasoning mode setting without Python TIR, it improves AIME25 pass@1 for Qwen3-30B-A3B by 13.1% over the baseline. Incorporating StackExchange-Math further enhances robustness, especially on HLE-Math, without sacrificing competition performance. We propose sequential bucketed training strategy that progressively expands model context from short to long (128K) windows. This approach improves training throughput by 23 while maintaining accuracy within 13% of fulllength joint training, making ultra-long-context fine-tuning computationally practical. We conduct scaling studies on Qwen3-8B and Qwen3-30B-A3B, showing that both architectures benefit from Nemotron-Math and converge to similar final performance. Under the high reasoning mode with Python TIR, both models achieve 100% maj@16 accuracy on AIME24 and AIME25, demonstrating the strength and generality of the dataset. 2. Nemotron-Math Overview The Nemotron-Math dataset integrates diverse mathematical problems from multiple sources, combining problems from the OpenMathReasoning dataset [15], which is based on the AoPS community, with additional problems collected from the Stack Exchangemath [20, 19]. Each problem is used as prompt to the gpt-oss-120b model, which generates multiple solution trajectories under different reasoning modes, including both with and without Python TIR. In total, this process yields approximately 7.5M reasoning traces. Detailed descriptions of the data sources, generation pipeline, quality filtering, and dataset statistics are provided in the following subsections. 2.1. Problem Set The problem set of Nemotron-Math is constructed from two complementary sources, designed to balance curated mathematical rigor with real-world diversity. 2.1.1. AoPS Source The first source is adopted from the OpenMathReasoning dataset [15], which was constructed from problems collected on the AoPS community forum [1]. For convenience, we refer to this subset as the AoPS Source throughout the paper. To ensure that the tasks in our dataset admit verifiable final answers, we exclude items whose primary objective is theorem proving and retain only nontrivial, high-quality problems with checkable solutions. The resulting subset contains approximately 175K challenging mathematical questions spanning algebra, geometry, number 2 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Tool Usage Reasoning Mode AoPS StackExchange-Math Data Size (in K) Python TIR Without Python TIR High Medium Low High Medium Low 466 401 271 438 354 1106 945 719 1086 899 621 Table 1: Distribution of the 7.5M generated solutions across tool usage and reasoning modes for the AoPS and StackExchange-Math problem sets (in K). The AoPS set includes 85K problems, and the StackExchange-Math set includes 262K problems. theory, and combinatorics, and is further reduced to 85K by removing easy questions as described in Section 2.2. 2.1.2. StackExchange-Math Source The second component, StackExchange-Math Source2, is constructed from Math Stack Exchange [20] and MathOverflow [19]. These platforms, similar to AoPS in being user-generated and partially overlapping in audience, tend to feature more college-level and research-oriented problems. We apply the same preprocessing and filtering procedures as in Moshkov et al. [15]. Proof-style questions are filtered following the procedure described in Moshkov et al. [15], which employs the Qwen2.5-32B-Instruct model as classifier. In particular, the same decontamination process is applied to eliminate any overlap with public benchmarks, ensuring consistency with the previous setup. After all filtering and validation steps, the StackExchange-Math Source yields approximately 651K distinct mathematical problems, which are further reduced to 262K after removing easy questions as described in Section 2.2. 2.2. Solutions Generation Based on the prepared problem set, we prompt the gpt-oss-120b model to generate solutions in three reasoning modes, high, medium, and low, under both Python TIR and without Python TIR settings, yielding six configurations in total. For each configuration, eight solutions are generated by varying random seeds with temperature of 1.0 and top-p of 1.0. To ensure correctness, we follow Moshkov et al. [15] and use Qwen2.5-32B-Instruct [10] as an LLM-as-a-judge, comparing the expected answers with the final answers extracted from the generated solutions. All 2We use only the StackExchange data dumps released before the July 2024 policy change, when the content was distributed under CC BY-SA without additional usage restrictions. solution-generation and evaluation steps are orchestrated through the Nemo-Skills framework [3]. For each problem, we use model-generated solutions to verify the extracted answers from the source forums and to replace them when necessary, ensuring reliable reference answers. Specifically, we generate 16 high reasoning mode solutions (8 Python TIR and 8 without Python TIR) using gpt-oss-120b, as this mode provides the highest data quality. When problem has no extracted answer from the forum, we use the majority vote among these model solutions. When an extracted answer is available, we retain it as long as at least one model solution is judged consistent with it; if all model solutions disagree, we replace the extracted answer with the majority vote of the model-generated answers. Manual inspection of the cases where replacement occurs indicates that the extracted answers are typically noisy or incomplete, whereas the majority-vote gpt-oss-120b solutions are more reliable. To further filter out trivial problems, we computed the pass rate of each problem using low-reasoning solutions (8 Python TIR and 8 without Python TIR attempts). Problems with pass rate above 0.8 were removed, as such items are typically too easy for the model and contribute limited training signal. After applying this filtering, the AoPS portion was reduced from 175K to 85K problems, and the StackExchangeMath portion from 651K to 262K. Finally, we discard any generated solutions that fail to reach the expected answer, ensuring that the resulting dataset emphasizes challenging and high-quality problemsolution pairs. 2.3. Solutions Analysis The statistics of the 7.5M generated solutions is summarized in Table 1. The Python TIR variants are consistently larger than the variants without Python TIR, as code execution during reasoning enables the 3 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Data Source 16K 16K32K 32K64K 64K AoPS StackExchange-Math 175K 5022K 235K 254K 114K 82K 23K 20K Table 2: Distribution of the 7.5M generated solutions by reasoning trace length bucket across AoPS and StackExchange-Math sources. Bucket ranges (e.g., 16K32K) denote token lengths, and values indicate the number of reasoning traces (in K). model to solve more problems. Moreover, the overall data size increases from the low reasoning to the high reasoning mode, reflecting the fact that more reasoning effort can aid solution quality. These patterns arise directly from our final filtering step, in which we remove any solutions that do not lead to the expected answer. The data distribution statistics by bucket length are presented in Table 2, which indicates that the dataset is heavily skewed towards shorter sequences. 3. Experiments Setup 3.1. Training Details We fine-tune Qwen3-8B and Qwen3-30B-A3B [18] using consistent supervised training pipeline across all experiments. The model is optimized with AdamW [13] with fixed learning rate of 2e-4, selected based on learning-rate sweep (Appendix C), without warmup, and global batch size of 2048. Sequence packing is applied to improve efficiency on long-context reasoning data. All components of the pipeline, including problem extraction, data generation, training, and evaluation, are orchestrated using Nemo-Skills [3]. During training, Nemo-Skills leverages NeMo-RL [2] with the Megatron backend. and open-domain advanced mathematical problem solving. For each model, we evaluate the final fine-tuned checkpoint under six configurations: high, medium, and low reasoning modes, each in both with Python TIR and without Python TIR settings. For the benchmarks AIME24, AIME25, HMMT-24-25, we generate 16 solutions per problem using temperature 1.0, top-p 1.0, and maximum generation length of 120K tokens. For HLE-Math, we use the same decoding setup but generate 4 solutions per problem, reflecting its much larger number of questions (976 in total) and empirically low cross-seed variance (< 1%). We report both pass@1 and maj@k, where pass@1 measures the average accuracy across different independent runs and maj@k computes accuracy under majority voting over ð‘˜ generated solutions (with ð‘˜ = 16 for AIME24, AIME25, HMMT-24-25 and ð‘˜ = 4 for HLE-Math). For AIME24, AIME25, and HMMT-24-25, we use math-verify for automatic numeric/symbolic answer checking. For HLE-Math, we instead rely on an LLMas-a-judge protocol (Qwen2.5-32B-Instruct) using our evaluation prompt (Appendix D). For comparison, we also evaluate the baseline Qwen3-8B and Qwen3-30B-A3B models using their default decoding configuration [18] (temperature 0.6, top-p 0.95, maximum generation length 120K tokens). 3.2. Evaluation Setup We evaluate our models on two complementary benchmark suites. (1) Comp-Math-24-25 [15], which includes the HMMT-24-25, AIME24, and AIME25 subsets, covers competition-style mathematical reasoning problems emphasizing symbolic precision and multistep deduction. (2) the text-only Math subset of the recently released HLE (Humanitys Last Exam) benchmark [17], multi-domain evaluation suite featuring both text and multimodal problems. The HLE-Math subset consists of 976 diverse problems designed to assess high-level mathematical reasoning. It spans algebra, geometry, combinatorics, and calculus, and often demands long-context comprehension and verification of intermediate steps. Together, Comp-Math-24-25 and HLE-Math form balanced evaluation protocol that covers both formal competition-style reasoning 4. Experimental Results To evaluate the effectiveness of our dataset construction, we compare Nemotron-Math with the updated version of OpenMathReasoning [15]. The original OpenMathReasoning dataset was initially generated using DeepSeek-R1, and its generation pipeline was later updated to DeepSeek-R1-05-283. Our comparison is conducted against this updated version. In contrast, Nemotron-Math regenerates all reasoning trajectories using gpt-oss-120b and additionally incorporates problems from StackExchange-Math, providing broader coverage beyond AoPS. To ensure fair and controlled comparison, we con3https://huggingface.co/datasets/nvidia/NemotronPost-Training-Dataset-v1/viewer/default/math 4 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Configuration AIME AIME25 HMMT-24-25 Nemotron-Math OpenMathReasoning [15] Mixed 81.04 (90.00) 71.04 (82.50) 76.46 (88.33) 77.08 (90.00) 59.38 (71.67) 66.25 (86.67) 63.17 (73.43) 49.30 (63.21) 55.71 (66.70) Table 3: Accuracy of Qwen3-30B-A3B fine-tuned with different datasets on three benchmarks (without Python TIR, high reasoning mode). Metric: pass@1 (maj@16, %). struct two aligned datasets from the same pool of 50K unique AoPS problems shared across both datasets. We evaluate Nemotron-Math using its high-reasoning, without Python TIR subset, so that both datasets exhibit comparable reasoning depth and sequence length. For each selected problem, we collect all corresponding reasoning records from both datasets and retain an equal number of examples by matching the smaller count on each side. This yields two balanced datasets, each containing 264K examples. We also construct mixed dataset by randomly sampling half of the examples from each source, again totaling 264K examples, to examine possible complementarity between the two reasoning styles. This controlled setup isolates the effect of dataset design and reasoning behavior within the without Python TIR setting while keeping data scale and problem distribution identical. We evaluate the final models fine-tuned from Qwen3-30B-A3B without Python TIR, high reasoning mode setting, as both subset datasets used in this comparison contain only data of this type. Table 3 reports the final evaluation results, measured by pass@1 and maj@16 on the HMMT-24-25, AIME24, and AIME25 benchmarks. As shown in the table, models trained on Nemotron-Math consistently outperform those trained on the original OpenMathReasoning and on the Mixed dataset across all benchmarks. The Mixed dataset, which combines equal portions of Nemotron-Math and OpenMathReasoning, also surpasses the previous dataset OpenMathReasoning, indicating that the reasoning traces in Nemotron-Math enable models to exhibit stronger reasoning ability and more effective problem-solving behavior. 4.1. Effect of StackExchange-Math Integration To isolate the contribution of community-sourced problems, we construct two controlled subsets within Nemotron-Math, which we refer to as AoPS-only and AoPS+StackExchange-Math. AoPS-only contains all AoPS problems and their corresponding solution traces. AoPS+StackExchange-Math is obtained by randomly replacing half of the AoPS examples (problemsolution pairs) with examples drawn from StackExchange-Math, while keeping the total number of examples fixed. In this way, AoPS+StackExchangeMath differs from AoPS-only primarily through the inclusion of StackExchange-Math-based solutions, allowing us to assess the impact of community-sourced content. StackExchange-Math introduces broader linguistic variation, more informal phrasing, and richer real-world mathematical reasoning patterns compared to the structured, competition-style AoPS problems. This controlled setup therefore enables clean evaluation of whether incorporating community-driven supervision improves reasoning robustness and outof-distribution generalization, particularly on opendomain benchmarks such as HLE-Math, without compromising accuracy on competition-style tasks. Final results are presented in Table 4. Across all six reasoning configurations (high, medium, and low reasoning modes, each evaluated under both Python TIR and without Python TIR setting), models fine-tuned on the AoPS-only subset of Nemotron-Math and those trained on the AoPS+StackExchange-Math subset exhibit clear and consistent trends. For competitionstyle benchmarks (AIME24, AIME25, and HMMT-2425), the AoPS+StackExchange-Math variant achieves accuracy that is either comparable to or slightly higher than the AoPS-only variant across all modes, indicating that incorporating StackExchange-Math data does not hinder the models ability to solve highly structured olympiad-style problems. In contrast, the gains on HLE-Math are consistent across all configurations. Since HLE-Math contains communitydriven, open-domain mathematical questions with diverse linguistic expressions and less formalized reasoning structures, its distribution more closely aligns with StackExchange-Math. Training with the AoPS+StackExchange-Math subset therefore provides more suitable supervision for such tasks, leading to improved robustness overall. 4.2. Sequential Bucketed Training Strategy As shown in Table 2, the majority of reasoning traces in Nemotron-Math fall within short or medium context lengths, with relatively few examples exceeding 64K tokens. Training the full model directly with fixed 128K context window is therefore highly inefficient, as it forces all optimization steps to use the most 5 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Dataset Configuration AIME24 AIME25 HMMT-24-25 HLE-Math AoPS-only AoPS + StackExchangeMath 86.88 (95.00) High, w/o Python TIR High, Python TIR 92.67 (100.00) Medium, w/o Python TIR 61.88 (75.00) 79.11 (86.67) Medium, Python TIR 51.25 (63.33) Low, w/o Python TIR 66.00 (76.67) Low, Python TIR High, w/o Python TIR 90.00 (94.17) 94.58 (100.00) High, Python TIR Medium, w/o Python TIR 71.88 (83.33) 83.12 (88.33) Medium, Python TIR 52.50 (71.67) Low, w/o Python TIR 66.67 (80.00) Low, Python TIR 83.96 (90.00) 94.89 (100.00) 71.04 (81.67) 84.89 (100.00) 41.25 (54.44) 63.11 (76.67) 86.40 (96.67) 97.29 (100.00) 67.71 (80.83) 84.17 (100.00) 45.00 (59.44) 63.75 (76.88) 70.06 (77.99) 81.53 (86.19) 52.04 (64.01) 72.72 (80.39) 31.44 (39.48) 58.16 (67.94) 71.40 (77.85) 81.76 (85.28) 55.84 (66.22) 73.41 (77.55) 33.64 (43.99) 58.80 (68.02) 12.22 (12.22) 22.54 (22.54) 7.22 (7.22) 13.40 (13.40) 3.92 (3.92) 7.91 (7.91) 13.60 (13.60) 24.67 (24.67) 8.58 (8.58) 15.22 (15.22) 4.10 (4.10) 7.99 (7.99) Table 4: Accuracy comparison of AoPS-only and AoPS+StackExchange-Math subsets of Nemotron-Math under six reasoning configurations (high, medium, and low reasoning modes, each evaluated with both Python TIR and without Python TIR setting). Results are reported on AIME24, AIME25, HMMT-24-25, and HLE-Math. For AIME24/25 and HMMT-24-25, metrics are computed using 16 sampled solutions per problem (pass@1 (maj@16, %)). For HLE-Math, which contains substantially more questions (976 in total) and exhibits extremely low cross-seed variance (typically < 1%), results are averaged over 4 seeds while maintaining comparable stability (pass@1 (maj@4, %)). memory-intensive and communication-heavy parallelism settings, despite most samples not requiring such long contexts. To address this mismatch, we adopt sequential bucketed training strategy. The dataset is partitioned into buckets according to sequence length, and training proceeds in stages from short to long contexts (16K 32K 64K 128K). At each stage, parallelism configurations (tensor, pipeline, and context parallelism; see Appendix for per-bucket settings) are tailored to the current maximum sequence length, ensuring efficient memory usage and significantly reduced communication overhead. Sequence packing is applied throughout to maximize GPU utilization. This progressive approach offers two practical advantages. First, for the same short-context data, early stages can operate with extremely high throughput by using parallelism configurations optimized for short sequences. For example, when training on the 16K bucket, an optimized configuration runs at approximately 18 seconds per step, whereas forcing the same 16K data to use the parallelism setup required for 128K context increases the step time to around 25 seconds. This comparison isolates the effect of parallelism configuration, showing that most training tokens can be processed under substantially cheaper settings in the early stages. Second, only the final stage must operate with the full 128K context window, meaning that relatively few optimization steps require the expensive long-context parallelism setup. In practice, this yields 23 reduction in end-to-end training cost without altering the final target context length. Final results obtained using Qwen3-30B-A3B are summarized in Table 5. Across all benchmarks and reasoning configurations, the sequential bucketed training strategy achieves accuracy that is broadly comparable to full-length joint training. Many configurations match the full-data results almost exactly, while others show only minor 13% degradation. This demonstrates that the bucketed strategy preserves nearly the same level of accuracy while substantially reducing overall training cost. As expected, deeper reasoning modes consistently outperform shallower ones, and Python TIR models achieve the strongest results across all benchmarks. Under the high reasoning mode, the fine-tuned model substantially improves over the base Qwen3-30B-A3B. For example, on AIME25 (without Python TIR), accuracy improves 13.1% (from 71.67% to 84.79%), highlighting the benefit of supervised reasoning traces. Notably, the high reasoning, Python TIR configuration reaches maj@16 accuracy of 100% on both AIME24 and AIME25, demonstrating the combined effect of deeper reasoning depth and tool-augmented supervision. While these results confirm that sequential bucketed training preserves strong performance while delivering significant efficiency gains, we note that this strategy should be applied with care. In particular, the distribution of reasoning modes across buckets can become highly imbalanced at long context lengths. Since medium and low reasoning solutions rarely reach 128K tokens, naively training only on high-reasoning samples in the final stage can cause the models behavior to collapse toward uniformly long, high-depth 6 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Configuration AIME AIME25 HMMT-24-25 Bucket Training Full Data Bucket Training Full Data Bucket Training Full Data High, without Python TIR High, Python TIR Medium, without Python TIR Medium, Python TIR Low, without Python TIR Low, Python TIR 86.67 (93.33) 93.54 (100.00) 72.92 (80.00) 83.54 (87.78) 52.08 (70.00) 68.96 (80.00) 88.12 (97.50) 94.67 (100.00) 76.46 (88.33) 83.33 (88.33) 50.62 (64.00) 66.46 (76.67) 84.17 (96.67) 95.83 (100.00) 63.96 (78.33) 81.04 (96.67) 40.42 (58.82) 60.21 (78.76) 84.79 (95.00) 96.00 (100.00) 67.71 (76.67) 86.67 (100.00) 44.38 (61.11) 64.58 (77.50) 68.78 (77.78) 78.28 (85.68) 52.17 (65.98) 70.76 (79.85) 32.33 (42.11) 57.17 (67.30) 71.43 (79.78) 80.82 (85.05) 55.10 (64.54) 73.50 (79.16) 32.27 (41.49) 59.06 (66.50) Qwen3-30B-A3B (baseline) 81.25 (90.00) 71.67 (80.00) 57.33 (64.97) Table 5: Accuracy comparison between full-length joint training and sequential bucketed training on three benchmarks (AIME24, AIME25, and HMMT-24-25) using Qwen3-30B-A3B. Results are reported for three reasoning modes (high, medium, low) under both Python TIR and without Python TIR settings. The last row shows the accuracy of the pretrained Qwen3-30B-A3B model before any training. reasoning. In our experiments, such imbalance leads medium and low modes to generate increasingly long sequences and lose their intended distinction, even though overall accuracy may increase. To mitigate this effect, we explicitly balance the final long-context stage by sampling small proportion of medium and low reasoning data, ensuring that all reasoning modes remain visible throughout training. This practice preserves mode diversity while retaining the efficiency and performance benefits of the sequential bucketed strategy. 4.3. Scaling with Model size and Architecture To study how Nemotron-Math scales across model sizes and architectures, we compare Qwen3-8B and Qwen3-30B-A3B trained under identical data and training configurations. Figure 1 reports results under the high reasoning mode, with evaluations performed every half epoch. The left panel shows performance on Comp-Math24-25, while the right panel shows performance on HLE-Math. In each panel, results without Python TIR and with Python TIR are plotted together. Solid lines with markers denote Qwen3-30B-A3B, while dashed lines with matching marker shapes denote Qwen3-8B; blue curves correspond to the without Python TIR setting, and red curves correspond to the with Python TIR setting. For clarity, only the high reasoning mode is shown, as the medium and low reasoning modes exhibit the same qualitative trends. both competition-style and open-domain mathematical reasoning tasks. 5. Related Work 5.1. Long Chain-of-Thought Reasoning Data With the rapid advancement of open-source reasoning models, growing body of work focuses on enhancing the reasoning capabilities of LLMs by generating long chain-of-thought (CoT) traces, especially for mathematical problem solving. Early datasets such as OpenMathInstruct-1 [22], OpenMathInstruct-2 [21], Skywork-MathQA [24], and NuminaMath [11] were among the first to scale mathematical CoT supervision. While these datasets significantly improved mathematical reasoning performance, their solution traces remain relatively shallow, offering limited multistep deduction, self-correction, or verification. Recent advances in reasoning models, exemplified by DeepSeek-R1 [8] and the Qwen3 series [23], have made it possible to produce substantially longer, more coherent, and more systematically structured reasoning trajectories. These models can produce detailed multi-step derivations, engage in extensive selfreflection and consistency checking, leading to more reliable multi-step derivations. Leveraging such capabilities, OpenMathReasoning [15] released 540K olympiad-level problems paired with 3.2M long-form solutions, establishing the state-of-the-art and winning the AIMO-2 competition. Across both benchmarks and tool settings, the two models exhibit highly similar learning dynamics. They improve at comparable rates and converge to nearly identical final accuracy. The only noticeable deviation occurs on HLE-Math without Python TIR, where Qwen3-8B attains slightly higher accuracy. Overall, these results indicate that Nemotron-Math provides sufficiently strong supervision for both model scales, leading to aligned convergence behavior on As the field continued to advance, additional datasets emerged to further enrich the landscape of long-form mathematical reasoning. OpenR1-Math220K [16] provides two to four DeepSeek-R1 trajectories per problem across 220K challenging math questions. Zhao et al. [26] introduced AM-DeepSeek-R1Distilled, containing 1.4M questionresponse pairs with associated thinking traces for general reasoning tasks. DeepMath-103K [9] offers 103K mathe7 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Figure 1: Scaling with model size and architecture on Nemotron-Math. Each panel reports pass@1 as function of training progress (evaluated every half epoch), comparing Qwen3-8B and Qwen3-30B-A3B under the high reasoning mode. The left panel shows results on Comp-Math-24-25 and the right panel shows results on HLE-Math. Within each panel, both without Python TIR and with Python TIR settings are plotted. matically verifiable problems, each paired with three distinct DeepSeek-R1 reasoning paths. 5.2. Tool-Integrated Reasoning the Tool-Integrated Reasoning (TIR) enhances problem-solving capability of LLMs, particularly in mathematics, by enabling models to invoke external computational tools, such as Python execution or symbolic solvers, rather than relying solely on naturallanguage reasoning. This allows LLMs to handle tasks requiring high numeric precision, symbolic manipulation, or multi-step verification more reliably. pioneering effort in this direction is Program of Thoughts (PoT) [5], which expresses reasoning steps as executable programs and uses the resulting program outputs to derive final answers, achieving state-of-the-art results across several mathematical reasoning benchmarks. Following this idea, numerous studies have explored how to teach LLMs to use tools effectively. ToRA [7] integrates naturallanguage reasoning with external computational libraries and symbolic solvers, achieving significant improvements over open-source baselines. MathSensei [6] extends this direction by combining multiple tools, including web retrieval, Python execution, and symbolic solvers, and reports 13.5% accuracy improvement over GPT-3.5-turbo with standard CoT on the MATH dataset. MathCoder2 [14] introduces 19.2B-token tool-centric mathematical pretraining corpus that substantially improves downstream reasoning ability when used to train modern base models. More recently, Torl [12] proposed an RL-based framework that teaches base LLMs to autonomously employ computational tools, outperforming prior TIR models by large margin (17%). Despite advances in long-form and tool-integrated reasoning, existing datasets typically rely on singlemode solution generation, offering limited diversity in reasoning depth or tool-usage behavior. Nemotron-Math instead provides multi-mode supervision (high/medium/low) under both Python-free and Python-augmented settings, producing broad spectrum of reasoning styles, from concise heuristic chains to deeply structured, tool-driven solutions, yielding level of behavioral and tool-integration diversity that, to our knowledge, is not present in prior mathematical reasoning datasets. 6. Conclusion We introduce Nemotron-Math, large-scale mathematical reasoning dataset containing 7.5M long-form solution traces generated by gpt-oss-120b across multi-mode and tool-augmented settings, covering 347K curated problems. By combining structured AoPS problems with diverse StackExchange-Math questions, Nemotron-Math provides broad domain coverage and rich reasoning variability. Experiments show that it offers higher-quality supervision than prior datasets and improves both competitionlevel and open-domain reasoning. To enable efficient long-context training, we proposed sequential bucketed strategy that delivers 23 faster 128Kcontext fine-tuning with only 13% accuracy difference from full joint training. Models fine-tuned on 8 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Nemotron-Math also scale effectively: both Qwen3-8B and Qwen3-30B-A3B achieve 100% maj@16 accuracy on AIME24/25 under the Python TIR high reasoning mode. We will release all data, code, and trained models to support reproducibility and further open-source development."
        },
        {
            "title": "References",
            "content": "[1] Art of problem solving. //artofproblemsolving.com/, cessed: 2025-09-25. https: Ac2025. [2] Nemo rl: scalable and efficient post-training library. https://github.com/NVIDIA-NeMo/RL, 2025. GitHub repository. [3] NeMo-Skills. https://github.com/NVIDIANeMo/Skills/, 2025. GitHub repository. [4] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. [6] Debrup Das, Debopriyo Banerjee, Somak Aditya, tooland Ashish Kulkarni. Mathsensei: augmented large language model for mathematical reasoning. arXiv preprint arXiv:2402.17231, 2024. [7] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. [10] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [11] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. NuminaMath: The largest public dataset in AI4Maths with 860k pairs of competition math problems and solutions, 2024. [12] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. [13] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 69, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7. [14] Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathcoder2: Better math reasoning from continued pretraining on modeltranslated mathematical code. arXiv preprint arXiv:2410.08196, 2024. [15] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. [16] OpenR1 Team. OpenR1 Math 220k, FebruURL https://huggingface.co/ ary 2025. datasets/open-r1/OpenR1-Math-220k. Dataset available on Hugging Face. [17] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dan Hendrycks, et al. Humanitys last exam (hle). https: //arxiv.org/abs/2501.14249, 2025. preprint, arXiv:2501.14249. [18] Qwen-team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [19] Stack Exchange Community. MathOverflow. https://mathoverflow.net/, 2025. Accessed: 2025-10-16. 9 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision [20] Stack Exchange Community. Mathematics Stack Exchange. https://math.stackexchange.com/, 2025. Accessed: 2025-10-16. [21] Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. [22] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774, 2024. [23] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [24] Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, Han Fang, and Yahui Zhou. Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models The Story Goes On, 2024. [25] Shaowei Zhang and Deyi Xiong. BackMATH: Towards Backward Reasoning for Solving Math Problems Step by Step. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 466482, 2025. [26] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training. arXiv preprint arXiv:2503.19633, 2025. 10 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Model Context length TP CP PP ETP EMP Qwen3-30B-A3B 16k 32k 64k 128k 4 4 4 4 2 4 8 8 1 1 1 1 1 1 1 1 4 8 8 8 Table 6: Training configuration by bucket length. Model/Context Length 16K 32K 64K 128K Full context 128K Speedup Qwen3-30B-A3B 117248 58988 56801 22197 559802 2. Table 7: Training time breakdown by bucket length and stage, with full-context total and overall speedup. A. Training Configuration By Bucket Length Table 6 summarizes the parallelism settings used for each bucket length in the sequential training schedule. Because different sequence lengths place pressure on different parts of the training stack, we adapt the parallelism configuration at each stage. Tensor parallelism (TP) splits the large matrix multiplications inside each layer across devices, while context parallelism (CP) shards the sequence dimension to enable efficient long-context training. Pipeline parallelism (PP) is used only lightly, as most speedup comes from TP and CP in our experiment. For the MoE model Qwen3-30B-A3B, we additionally employ expert tensor parallelism (ETP) to parallelize computation within each expert, and expert model parallelism (EMP) to distribute different experts across devices. Short-context buckets (e.g., 16K) can be trained very efficiently when using parallelism configurations optimized for short sequences. For the Qwen3-30B-A3B model, 16K bucket runs at around 18 seconds per step under its own optimized TP/CP setup. In contrast, training the same data under the fixed parallelism configuration required for 128K context length slows execution to approximately 25 seconds per step. We report detailed breakdown of step time by bucket length and parallelism configuration in Table 7. By running most training steps under parallelism settings tailored to shorter contexts and reserving heavy configurations only for the final 128K stage, the sequential bucketed strategy achieves 23 end-to-end speedup compared to training the entire dataset with fixed 128K context window."
        },
        {
            "title": "With Python TIR Without Python TIR",
            "content": "pass@1 maj@k pass@1 maj@k AIME25 AIME24 HMMT2425 HLE-math 95.62% 100.00% 83.75% 94.38% 100.00% 88.75% 80.29% 80.29% 70.12% 10.32% 10.32% 10.96% 93.33% 98.33% 78.91% 10.96% Table 8: Performance of the Nemotron-3-Nano-30B-A3B SFT-only checkpoint (i.e., the pre-RL model used in the Nemotron-3 Nano post training pipeline) on Comp-Math-24-25 and HLE-Math, evaluated under both with and without Python TIR. This checkpoint is trained using only the high-reasoning-mode portion of Nemotron-Math. Metrics report pass@1 and maj@k (ð‘˜ = 16 for Comp-Math-24-25 and ð‘˜ = 4 for HLE-Math). 11 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Configuration lr=1ð‘’lr=5ð‘’-5 lr=7ð‘’-5 lr=1ð‘’-4 lr=2ð‘’-4 lr=5ð‘’-4 High, no tool High, with tool Medium, no tool Medium, with tool Low, no tool Low, with tool 38.75% 1.96% 50.46% 1.80% 50.46% 1.80% 53.98% 2.44% 54.39% 2.28% 48.22% 1.71% 50.85% 2.20% 65.50% 2.64% 65.50% 2.64% 66.20% 2.00% 68.21% 2.84% 67.07% 1.17% 26.15% 1.74% 36.18% 2.46% 36.18% 2.46% 43.95% 2.00% 42.75% 2.22% 32.64% 1.97% 49.34% 1.90% 61.69% 1.97% 61.69% 1.97% 66.43% 1.76% 68.68% 1.71% 60.52% 2.51% 17.29% 1.28% 22.00% 2.32% 22.00% 2.32% 27.54% 1.44% 25.24% 1.49% 18.31% 2.30% 40.26% 2.71% 48.14% 2.35% 48.14% 2.35% 53.25% 1.95% 53.76% 1.96% 46.34% 2.10% Table 9: Accuracy comparison on the Comp-Math-24-25 benchmark under different learning rates using Qwen3-30B-A3B. Results are reported for three reasoning modes (high, medium, low) under both Python TIR and without Python TIR settings. All results are reported as pass@1 accuracy (mean standard deviation) over 16 runs. B. Further Evidence via NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 Evaluation Table 8 reports results for NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 SFT-only checkpoint4 from the Nemotron-3 Nano post-training pipeline, before any reinforcement learning stages. This model is supervised fine-tuned using only the high-reasoning-mode trajectories from Nemotron-Math, and we evaluate it on both Comp-Math-24-25 and HLE-Math under with and without Python TIR settings. On Comp-Math-24-25, the SFT-only Nemotron checkpoint achieves accuracy comparable to our fine-tuned Qwen3-30B-A3B, suggesting that the highreasoning-mode subset already provides strong competition-style supervision. On HLE-Math, Qwen3-30B-A3B performs better, which we attribute to our use of the full Nemotron-Math including large-scale StackExchange-Math supervision that better matches the open-domain and linguistically diverse nature of HLE-Math. C. Learning Rate Grid Search We performed grid search for hyperparameter tuning on Qwen3-30B-A3B using subset of the training data, with the results summarized in Table 9. Overall, learning rate of 2e-4 generally performs best across different reasoning modes, and we fix this learning rate for all experiments. D. Answer Judgment Prompt Prompt: Answer Judgment r : You l be e l a problem and u whether . e two w t a q a t h h n t ( d e and e d ) math problem s e o Yes No your judgement on whether When comparing w o p o r a e a your s n a p f two w a h same . p i i . t e . Then p with Here a few examples . Example 1 : Problem : t $7x ^3 21 ^2 + 14 x$ . d e answer : $7x ( 2 ) ( 1 ) $ Expected answer : $7x ( x1) ( x2)$ Reaso ning : The e t a r e not matter , h s s t same . 4The final public release of NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 (https://huggingface.co/nvidia/NVIDIA-Nemotron-3Nano-30B-A3B-BF16) includes additional reinforcement learning stages; the results reported here are based on the pre-RL SFT checkpoint. 12 Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision Judgement : Yes Example 2 : Problem : c g has n o 6 e and width 2 e . u by 3 e and width halved , what g s f t new a t e n i u m r ? d e answer : 3/2 Expected answer : 1 . 5 Rea son ing : 3/2 Judgement : Yes same 1 . 5 Example 3 : Problem : p y e e o $ t { { 7 ! } } $ , where $n ! $ n d ( n2) t t 2 t 1 $ . d e answer : 71 Expected answer : 12 t { { 3 5 } } . r $n t ( n1) Reasoning : This Judgement : No non v t m f , h s s d e t . e e o $ t {{98 ^{{3}} ^{{5}} e p i form Example 4 : Problem : What }} ? i {{ g }} t {{A) }} & 2 q {{7 }} & t {{B) }} & 7 ^{{2}} ^{{2}} t {{2 }} t {{C) }} & 7 ^{{2}} t {{2 }} & t {{D) }} &49 ^{{2}} t {{2 }} end {{ g }} d e answer : 7 ^{{2}} t {{2 }} Expected answer : Rea soning : d e answer Judgement : Yes h same h p e answer i . Example 5 : Problem : n segment n i t $ ( 4 , ) $ . Find p i v e $b$ , d e answer : 2, 6 Expected answer : 6 , 2 g $5$ has one p t $ ( 1 , 2 ) $ and o r a e by commas . Rea soning : The e doesn mat ter Judgement : Yes h n t h problem . Example 6 : Problem : v $ tan = x$ $0 e 2 . $ Enter , a e by commas . d e answer : 0 , Expected answer : 0 , pi , 2 . s t s Rea soning : Number Judgement : No u n d e t . YOUR TASK Problem : { problem } d e answer : { d e _ w } Expected answer : { expected_answer }"
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}