{
    "paper_title": "FARMER: Flow AutoRegressive Transformer over Pixels",
    "authors": [
        "Guangting Zheng",
        "Qinyu Zhao",
        "Tao Yang",
        "Fei Xiao",
        "Zhijie Lin",
        "Jie Wu",
        "Jiajun Deng",
        "Yanyong Zhang",
        "Rui Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 8 5 3 2 . 0 1 5 2 : r FARMER: Flow AutoRegressive Transformer over Pixels Guangting Zheng1,3, Qinyu Zhao1,4, Tao Yang1, Fei Xiao1, Zhijie Lin2, Jie Wu1, Jiajun Deng5, Yanyong Zhang3, Rui Zhu1 1ByteDance Seed China, 2ByteDance Seed Singapore, 3USTC, 4ANU, 5NUS Project lead"
        },
        {
            "title": "Abstract",
            "content": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design one-step distillation scheme to significantly accelerate inference speed and introduce resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training. Date: October 28, 2025 Correspondence: zhurui.kim@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Explicitly modeling normalized likelihood P(x) over the high-dimensional data distribution is challenging. Popular generative paradigms such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion/score-based models do not provide tractable likelihoodsVAEs optimize lower bound, GANs learn implicit generators without likelihoods, and diffusion/score-based models offer likelihoods only via variational bounds or costly numerical estimation by probability-flow ODE. In contrast, Autoregressive (AR) models directly factorize sequence likelihoods via the chain rule and lead to the scaling successes of Large Language Models [1, 2, 16, 58, 59]. However, modeling the likelihood over continuous, high-dimensional image pixels remains notably challenging compared to the discrete text. Continuous AR over visual pixels has been explored for yearsfrom convolutional PixelRNN/PixelCNN [64, 65] to Image Transformer [45] and iGPT [5]. Despite these efforts, continuous AR suffers from extremely long sequences, making training and sampling costly and brittle to long-range dependencies. This gap motivates revisiting how we parameterize continuous densities over high-dimensional pixel spaces and how we couple them with scalable sequence models. 1 (a) Pixel Autoregressive models (b) Normalizing Flow (c) Flow Autoregressive Transformer (FARMER) Figure 1 Autoregressive (AR) models offer strong expressivity but struggle with pixel modeling and sampling due to the long sequences required for high-resolution images. Normalizing flows (NFs) employ invertible mappings to transform complex image distributions to standard Gaussian, but the substantial gap between two distributions leads to degraded sampling quality. FARMER unifies NF and AR within single framework, using the NF component to transform images into latent sequences, whose distribution is implicitly modeled by the AR component for easier modeling and controllable sampling. Furthermore, FARMER adopts self-supervised dimension reduction method to partition NF latent channels into distinct groups, making AR modeling feasible and scalable. At the same time, Normalizing Flow (NF) [18, 32, 78] has seen resurgence for image generation. By providing exact likelihoods via invertible and differentiable mappings, NF offers an attractive route for revitalizing continuous AR modeling and principled latent representation. For instance, JetFormer [63] and STARFlow [18] each design new NF Transformer as the visual tower: JetFormer employs Jet [32] to enable end-to-end continuous AR modeling over raw image pixels, while STARFlow extends TARFlow [78] and demonstrates that continuous Autoregressive Flow can achieve competitive generation quality. But recent NF works [12, 13, 18, 29, 32, 53, 78] predominantly map the data distribution to standard Gaussian. This is challenging objective, as forcing high-dimensional and highly dispersed data distribution onto simple isotropic Gaussian can introduce discontinuities or distortions, thus complicating the sampling process from the latent space and transforming back to the data space. Inspired by the great work of Jetformer [63], we propose framework named FARMER that leverages the strengths of both Normalizing Flows and Autoregressive models. As shown in Figure 1, rather than mapping the data distribution to fixed standard Gaussian, we employ an NF to transform images into latent sequence whose distribution is modeled implicitly by an AR model. Concretely, we implement the NF with an Autoregressive Flow (AF) architecture, ensuring causal modeling for NF/AR within FARMER. The two components are optimized jointly in an end-to-end fashion, preserving the tractable, exact likelihoods of NFs while endowing the target distribution with the expressivity of AR modeling. Beyond this design, two inherent challenges remain: (i) Continuous AR over pixels: Natural images are highly redundant. Without compression via VAEs [28, 54] or discrete tokenizers [50, 66], directly modeling all pixels forces the AR model to handle extremely long-range pixel dependencies, and thus results in unstable training and sample quality degrading. (ii) Slow reverse inference in AF: While AF substantially enhances the mapping capability via next-token modeling, they incur slow inference because the reverse inference process is strictly sequential. To mitigate the redundancy in pixel AR modeling, we introduce self-supervised dimension reduction mechanism that partitions NF latent channels into informative and redundant groups without information 2 loss. The key insight is to factorize the token likelihood (z c) as (z c) = (zR zI , c) (zI c) = (cid:104) (cid:89) i=1 PN +1 (cid:0)zR zI , c(cid:1)(cid:105) (cid:104) (cid:89) i=1 (cid:0)zI zI <i, c(cid:1)(cid:105) , Pi where zI denotes the informative channels and zR the redundant channels of each token. Concretely, the are modeled in the standard autoregressive manner, i.e., conditioned on the preceding informative channels zI informative tokens zu across all tokens are modeled jointly by and context c. The redundant channels zR <i shared distribution conditioned on the entire sequence of informative channels zI and context c. This construction allows us to treat the redundant channels of all tokens as single additional token, effectively converting high-dimensional tokens into +1 lower-dimensional tokens. Maximizing the resulting token likelihood encourages FARMER to disentangle information across channel groups, i.e., concentrating contour and structural features in zI , while assigning detail and color information to zR, as illustrated in Figure 7. For the slow reverse issue of AF, we propose one-step distillation scheme for efficient inference, which distills single-step student reverse path from the teachers forward path, thereby avoiding the causal reverse process of AF models. Finally, we present resampling-based Classifier-Free Guidance (CFG) algorithm that significantly improves generation quality in this framework. In summary, we summarize our contributions as follows: We introduce FARMER, an elegant and powerful framework that jointly optimizes Autoregressive Flow and Autoregressive Transformer for continuous image pixel likelihood estimation. We propose self-supervised dimension reduction approach that simplifies modeling of high-dimensional visual data. We develop one-step distillation method that accelerates AF reverse process by factor of 22 with only 60 additional training epochs, while maintaining comparable generation quality. We introduce novel resampling-based CFG algorithm that substantially enhances generation quality."
        },
        {
            "title": "2.1 Normalizing Flows\nNormalizing Flow [12, 13, 29, 30, 32, 43, 47, 53, 65, 78] maps a complex data distribution x ∼ pdata(x) into a\nsimple one z ∼ pZ(z). The target distribution pZ(z) is usually chosen as a standard Gaussian, which is easy\nfor density estimation and sampling. This transformation is achieved by applying a sequence of invertible\nfunctions F = fn ◦ fn−1 ◦ · · · ◦ f1. Accordingly, the forward and inverse mappings are:",
            "content": "z = (x) = fn fn1 f1(x), = 1(z) = 1 1 1 2 1 (z). (1) Using the change-of-variables formula, NFs can calculate the exact probability density of data point as: pdata(x) = pZ(z) (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) x (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) = pZ(F (x)) (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) (x) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) , (2) (cid:16) (x) (cid:17) where det denotes the determinant of the Jacobian matrix of the transformation . To facilitate training via maximum likelihood estimation, the learning objective is commonly formulated in terms of Negative Log-Likelihood (NLL): min log pZ(F (x)) log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) (x) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . (3) Previous works [18, 78] consider pZ as the standard Gaussian distribution (0, 1), so Eq. 3 can be written as: min 0.5 (x) 2 2 log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) (x) (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . (4)"
        },
        {
            "title": "2.2 AutoRegressive Models\nAutoRegressive models formulate the likelihood of a token sequence z = (z1, z2, . . . , zN ) by factorizing it into\na product of next-token conditional probabilities:",
            "content": "p(z) = (cid:89) i=1 p(ziz<i), (5) where z<i = (z1, . . . , zi1) conditions only on the previous tokens (z1, . . . , zi1) to predict the next token. Such AR paradigm has achieved remarkable scalability and tremendous success in language models [1, 2, 16, 58, 59]. Furthermore, it has also demonstrated promising capabilities in visual generation [21, 35, 38, 57, 61]."
        },
        {
            "title": "3.1 Mapping Image to AR Distributions via Invertible Flows",
            "content": "As aforementioned in Eq (4), mapping high-dimensional and highly dispersed image data distribution to simple isotropic Gaussian distribution via an NF can induce out-of-distribution issues and degrade the sampling quality [18]. Inspired by JetFormer [63], we propose framework that combines the strengths of NF and AR models. Rather than using fixed standard normal Gaussian, we employ an NF to transform images into latent sequence whose distribution is modeled implicitly by an AR model. Then the NF and AR components are optimized jointly in an end-to-end fashion, preserving the tractable, exact likelihoods of NFs while endowing the target distribution with the expressivity of AR modeling. The overall objective is to maximize the log-likelihood of the via the change-of-variables formula: log pdata(x) = (cid:88) i=1 log p(ziz<i) + log (cid:12) (cid:12) (cid:12) (cid:12) det( (x) (cid:12) (cid:12) ) (cid:12) (cid:12) , (6) where = (x) denotes the forward mapping of the NF. The target distribution over is parameterized autoregressively. To enhance the expressivity of the AR base, following JetFormer and GIVT [62], we model each conditional probability p(ziz<i) with Gaussian mixture model (GMM). The conditional log-likelihood for each token zi is: log p(ziz<i) = log( πi,kN (zi; µi,k, σ2 i,k)), (7) (cid:88) k= where the mixture weights πi,k, means µi,k, and deviations σ2 are predicted by the AR model conditioned on i,k preceding tokens z<i. Furthermore, different from Jetformer, we implement the NF model as an Autoregressive Flow (AF) [30, 43]. AF is powerful universal approximator for distributions that adopt an autoregressive structure: the transformation of each token zi is conditioned only on the preceding tokens z<i. Such AF architecture ensures that the entire pipeline maintains consistent and powerful causal formulation. Notably, when the number of mixture components in GMM is set to one (K = 1), the entire network, which composes an AF with an AR model, reduces to single and deeper Autoregressive Flow. We provide formal proof of this equivalence in Section A.1."
        },
        {
            "title": "3.2 Flow AutoRegressive Transformer",
            "content": "We devise Flow AutoRegressive transforMER models (FARMER) that unify an invertible autoregressive flow with an autoregressive model into single framework, which enables end-to-end training on raw image pixels by mapping the data onto an implicit target distribution modeled by the AR. Dequantize and Patchify. Specifically, given an input image RHW C, FARMER first adds Gaussian noise to I. It is common practice [46, 63, 78] to add small amount of noise to raw image to dequantize the discrete pixel values and create more continuous data distribution. Following Jetformer [63], we enhance this technique by employing noise augmentation strategy with annealed noise levels. During training, we add Gaussian noise with standard deviation (0, σ2) to I, where the noise level σ is annealed from 0.1 to 4 Figure 2 Overview of FARMER. Left, FARMER consists an autoregressive flow (AF) and an autoregressive (AR) model. The AF maps image patches to latent sequences, while the AR predicts Gaussian Mixture Models (GMMs) conditioned on these latents, optimizing their likelihood end-to-end. Middle, Each AF block performs an invertible next-token transformation of the input sequence to obtain new sequence. Right, AR splits latent channels into informative and redundant groups, modeling each informative tokens likelihood via GMM conditioned on its previous tokens, and redundant tokens jointly via shared GMM conditioned on all informative tokens. This separation enables disentangling structural and detailed information. 0.005 via cosine decay schedule. Then we patchify the noised image with downsampling factor to obtain the patch representation Rhwd, where = H/p, = W/p, and = p2. Finally, we reshape into sequence of = continuous-valued visual tokens = {x1, x2, . . . , xN }, with each token xi Rd. Notably, there is no dimension compression in the whole patchify process. Forward and Reverse of Autoregressive Flow. During training, FARMER utilizes an autoregressive flow to map token sequence RN to latents RN d, i.e., = (X). By design, is invertible (see Figure 2) and composed of invertible blocks: = fn fn1 f1. Letting 0 = and = Z, the forward transformation for the t-th AF block, = ft(Z t1), is defined for each token zt 1 as follows: zt = (cid:40) zt1 1 (cid:0)zt1 µt(zt1 <i )(cid:1) σt(zt1 <i ) if = 1, if > 1, (8) <i ) are predicted by t-th block conditioned on the preceding tokens zt1 represents the preceding tokens {zt1 where zt1 <i σt(zt1 the inverse of t-th block, 1 token, the inverse transformation t1 = 1 <i ) and the scaling factor in causal manner. Accordingly, , can be derived by algebraically solving for zt1 from the Eq (8). For each i1 }. The bias factor µt(zt1 , ..., zt1 (Z t) is defined as (see Figure 3a): <i 1 t zt1 = (cid:40) zt 1 (cid:0)zt σt(zt1 <i )(cid:1) + µt(zt1 <i ) if = 1, if > 1, (9) where denotes element-wise division. For training via the change-of-variables formula, it is essential that the Jacobian determinant of each block ft can be efficiently computable. Such autoregressive flow architecture enables that the Jacobian Zt Zt1 is lower triangular, so its determinant equals the product of its diagonal terms (i.e., the scaling factor σt). Consequently, the block-wise log-determinant is: log (cid:12) (cid:12) (cid:12)det Z (cid:12) (cid:12) (cid:12) = (cid:88) (cid:88) i=1 j=1 log(cid:12) (cid:12)[σt(zt1 <i )]j (cid:12) (cid:12) 1Subscripts denote indexing along the token sequence dimension, and superscripts denote the t-th AF block indices. 5 By the chain rule, the total log-det of is the sum over blocks, which in our case reduces to: log (cid:12) (cid:12) (cid:12)det X (cid:12) (cid:12) (cid:12) = (cid:88) t= log (cid:12) (cid:12) (cid:12)det Z t1 (cid:12) (cid:12) (cid:12) = (cid:88) (cid:88) (cid:88) t=1 i=1 j=1 log(cid:12) (cid:12)[σt(zt <i )]j (cid:12) (cid:12). (10) Permutation. To improve the expressiveness of AF, we follow TARFlow [78] and apply permutation to the token sequence as shown in Figure 2. Specifically, at the beginning of the t-th AF block, we apply the forward permutation πt to t1, which reverses the token order. After the forward AF transformation = ft(Z t1), we apply the corresponding inverse permutation π1 AR Modeling. After the AF forward mapping, we get the latent representation = {z1, z2, ..., zN } from the input image. Then we model its probability distribution with large causal AR Transformer. The AR Transformer is conditioned on an embedding R1D which encodes conditional information such as class label. To amplify its effect, we replicate condition embedding for times and prepend to the latent sequence Z. By the chain rule, to to restore the original ordering. N (cid:89) (Zc) = p(ziz<i, c). i=1 For each token, the AR Transformer predicts the parameters of K-component Gaussian Mixture Model (GMM) distribution Gi: p(ziz<i, c) = (cid:88) k=1 πk(z<i, c) (cid:0)zi ; µk(z<i, c), diag(σ2 k(z<i, c))(cid:1), (11) where πk R, µk Rd, σk Rd are the mixture weights, means, and standard deviations of the k-th GMM component. To highlight the conceptual link to the invertible flow, Eq (11) can be reformulated as: p(ziz<i, c) = (cid:88) k=1 πk(z<i, c) (cid:0)(zi µk(z<i, c)) diag( 1 σk(z<i, c) ) ; 0, Id (cid:1) (cid:12) (cid:12) (cid:12) (cid:12) 1 σk(z<i, c) (cid:12) (cid:12) (cid:12) (cid:12) , (12) This formulation reveals that each GMM component models zi by simple and invertible affine transformation (shifting by µk and scaling by 1 ) to random variable drawn from standard Gaussian distribution. This σk reveals that each GMM component performs an invertible affine normalization, i.e., (ziµk) (0, I). σk Learning Objective. As described in Eq (6), Eq (10) and Eq (11), the training loss of FARMER is the negative log-likelihood (NLL) of data and averaged over all dimensions: = 1 (cid:32) (cid:88) i=1 log p(ziz<i, c) + log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:33) . X (cid:12) (cid:12) (cid:12) (cid:12) (13)"
        },
        {
            "title": "3.3 Self-supervised Dimension Reduction",
            "content": "A fundamental challenge in pixel AR modeling is redundancy: natural images are intrinsically low-dimensional signals whose spectrum are dominated by low frequencies [63]. Although an invertible AF can faithfully map the data distribution, its bijective nature preserves dimensionality. For 256 256 3 image with patch size 16, the latent sequence has = (256/16)2 = 256 tokens, each of dimension = 768. This high-dimensional latent exacerbates two issues: (i) per-token AR modeling with K-component GMM in Rd becomes exceptionally challenging. (ii) The enlarged latent volume expands the sampling space, reducing efficiency and often degrading sample quality. Prior work like RealNVP [13] factors out half of the dimensions and model them with Gaussian priors. Jetformer [63] follows similar strategy: it models the informative dimensions autoregressively and assigns the redundant dimensions standard Gaussian prior, effectively assuming (Z c) = (Z R) (Z c), 6 i.e., is independent of both and c. This is strong assumption that is often violated in practice: informative and redundant parts typically remain correlated, so enforcing independence can discard information. Moreover, decoupling from and restricts how other modalities interact with the full latent, leading to suboptimal performance on multi-modal tasks. To this end, we propose novel self-supervised dimension reduction technique to address the above issues. It reduces the complexity of AR modeling, shrinks the sampling space, and lowers the computational cost, all while avoiding information loss. As shown in Figure 2 we split the latent RN channel-wise into an informative part RN dI and redundant part RN dR , with = dI + dR. Then we correctly factorize the joint probability via the chain rule: (Z c) = (Z c) (Z , c). Rather than assuming that is independent of (Z , c) in Jetformer, we explicitly condition on both and , where serves as the global image context. Furthermore, we constrain all tokens in to share GMM distribution, while modeling tokens in in token-by-token manner. This design encourages self-supervised disentanglement of distinct information across channel groups, without relying on standard Gaussian prior. autoregressively with an individual GMM distribution Gi For (Z c), we model each informative token zI predicted by the AR Transformer conditioned on and the preceding zI , thereby being capable of capturing <i complex distributions. In contrast, for (Z RZ , c), we use the entire informative sequence (global context) , By maximizing the together with to predict single shared GMM GN +1 for all redundant tokens zR combined likelihoods, our method successfully encourages complex contour and structural information to be reserved into , while the simple color and fine-detail information is relegated to R, as shown in Figure 7 and discussed in Section 4.3. After dimension reduction, the final training loss is rewritten as the sum of the NLL for both components: = 1 (cid:32) (cid:88) i=1 log p(zI zI <i, c) + log p(zR zI , c) + log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:33) . X (cid:12) (cid:12) (cid:12) (cid:12) (14) (cid:88) i="
        },
        {
            "title": "3.4 Resampling-based Classifier-Free Guidance",
            "content": "Classifier-Free Guidance (CFG) has become standard technique for improving sample quality in diffusion [39, 48, 54] and autoregressive models [35, 57, 61]. Conceptually, CFG steers the sampling process from base distribution towards target conditional distribution. For FARMER, the guided log-probability for latent token can be formulated as: log p(z) log pc(z) + (log pc(z) log pu(z)) = log pu(z) + (w + 1) (log pc(z) log pu(z)), (15) where pc(z) = p(zc) is the conditional GMM distribution, pu(z) = p(z) is the unconditional GMM, and is the guidance scale. However, the guided distribution p(z) is product and sum of GMMs, which does not correspond to any known tractable distribution, making direct sampling infeasible. To make it practical, we introduce novel Resampling-based CFG. The key insight is that the target distribution p(z) can be decomposed into two components as shown in Eq (15): the first term (blue) is tractable GMM distribution and can be sampled directly, while the second term (red) is not samplable but allows evaluation of the sample probability under such distribution. Therefore, we approximate the sampling from p(z) via three-step resampling scheme as detailed in Algorithm 1. For For each token zi, the procedure is: (i) Propose. Sample candidates from the conditional GMM pc(zi) and candidates from the unconditional GMM pu(zi) respectively. (ii) Weigh. Compute the corresponding log probability of each candidates as the second term in Eq (15), and normalize these weights. (iii) Resample. Resample from the categorical distribution that consists of the normalized weights of all candidates, to obtain the final sample. In summary, the probability where candidate is selected in the propose step is pc(z), and that in the (cid:17)w resample step is matches the target probability p(z). More details are provided in the Section A.2. . This resampling procedure ensures that the overall probability pc(z) (cid:16) pc(z) pu(z) (cid:16) pc(z) pu(z) (cid:17)w 7 (a) Autoregressive Flow Reverse Process (b) One-Step Distillation Process Figure 3 One-Step Distillation. (a) The autoregressive flow (AF) reverse process reconstructs tokens sequentially, conditioning each token on previous ones, which leads to slow inference. (b) Our method distills one-step student reverse path from the frozen teacher forward path in an end-to-end manner, approximating the reverse process of each AF block by the corresponding student AF blocks forward process, thereby enabling 22 faster AF reverse process and 4 overall inference speed-up."
        },
        {
            "title": "3.5 Fast Inferring via One-Step Distillation",
            "content": "A significant drawback of the Autoregressive Flow is the slow inference speed, caused by its sequential and token-by-token reverse process. As shown in Eq (9), during the inverse mapping (f 1 ) of AF block t, the calculation of each token zt1,i is conditioned on the preceding tokens zt1,<i, leading to complexity of O(N n). Such autoregressive dependency brings substantial inference speed bottleneck, and such limitation is also noted in recent AF-Transformer works like TARFlow [78] and STARFlow [18] whose token sequence length is 1024 with the patchsize of 8. Beneficial from the invertible nature of Normalizing Flow, whose forward and reverse paths are exact inverses, we can train new AF whose forward path mirrors the original AFs reverse path. Furthermore, because the forward/reverse path of NF consist of finite steps, we can invert the original AFs forward path (Z0, Z1, ..., Zn) to obtain its reverse path (Zn, Zn1, ..., Z0), and utilize such reverse path to supervise the new AF, thereby avoiding the original AF to perform slow reverse process to obtain its reverse path. As shown in Figure 3 and inspired by the generative distillation works [55, 67, 74], we propose one-step distillation scheme that learns single-step student reverse path from the trained teachers forward path while maintaining competitive sample quality. Algorithm 2 details the procedure: we first obtain teacher AF model, trained within the FARMER framework. Then, we initialize the student by copying the teacher AF and enable its attention bidirectional. At each distill iteration, we forward training data z0 to the latent zn by the teacher AF. In this way, we obtain teacher forward path (Z 0) = (Z 0, 1, ..., n). We use its reversal (Z n, n1, ..., 0) as the supervision target for the students forward path G( n) = ( n, n1, ..., 0). Specifically, to enhance the robustness of the student AF, we add small Gaussian noise to as and take as the input of the student AF. Then, the output latent t1 of each t-th student AF block is supervised by minimizing the Mean Squared Error (MSE) against the t1 from the teacher path. By distilling one such student AF model, we significantly accelerate the reverse process from 0.1689 to 0.0076 seconds per image. As discussed in Section 4.3 and Table 5, such one-step distillation brings 22 acceleration for AF reverse process while maintaining comparable generation quality. Notably, different from the progressive distillation for diffusion models, our approach offers three main advantages: it distills the entire AF model in an end-to-end manner, ensuring robustness to accumulative inference error; it eliminates the need for teacher models to run the inference process, thereby accelerating the distillation process; and it requires only 60 additional training epochs on the AF. Algorithm 1 Resampling-based CFG method Require: AR model Pθ and AF model Fθ Require: Guidance scale for [0, ..., + 1] do Sampel tokens <i ; c) <i ; ) # step1:Propose candidates Predict GMMc Gc,i = Pθ(zu Predict GMMu Gu,i = Pθ(zu zi,j Gc,i, [0, .., s] Sample from pc(z) zi,j Gu,i, [s + 1, .., + s] from pu(z) # step2:Weigh candidates if [0, ..., s] then Calculate weights πj = (log Gc,i(zi,j) log Gu,i(zi,j)) else πj = (w+1)(log Gc,i(zi,j)log Gu,i(zi,j)) end if π1, ..., πs+s = logsoftmax(π1, ..., πs+s) # step3:Resample from candidates if then For informative tokens else idx Categorical(π1, ..., πs+s) zu := zi,idx For redundant tokens for [0, ..., ] do s, larger in here idxk Categorical(π1, ..., πs+s) zd := zidxk end for end if end for = concat[[zu = 1 (z) θ 1 , zd ], ..., [zu , zd ]] Reverse to data Algorithm 2 One-step sampling distillation Require: Trained teacher AF (frozen) Fη = fηn fηn1 fη Require: Data set Require: Student AF Gθ = gθ1 gθ2 gθn for epochs do for iterations do = Patchify(x) 0 := for Teacher AF blocks do t, _ = fηt(Z t1) Teacher Transform end for := Zn ϵ (0, I) [0, 0.3] = + ϵ := # Distill one-step student reverse # path from the teacher forward path for Student AF reversed blocks do Sample noise Sample scale Add noise to latent t1, _ = gθt( t) Student Transform Lθt = t1 t12 MSE loss 2 end for Lθ = 1 Lθt θ θ γθLθ (cid:80)n end for end for"
        },
        {
            "title": "4.1 Experimental Settings\nDatasets. We empirically verify the merits of the proposed FARMER for image generation on ImageNet [10]\ndataset at 256 × 256 resolution, which consits of 1,281,167 training images from 1K different classes.",
            "content": "Network Architectures. We design two model scales: FARMER-1.1B/1.9B. The number of invertible AF blocks is set to 28 and 32 respectively. Each AF block contains 4/6 Transformer layers for FARMER-1.1B/1.9B. For the AR Transformer module, the number of Transformer blocks is 12 and 24 respectively. For the GMM prediction heads, the informative dimensions (dI ) are set to 128 with = 64 mixtures, while redundant dimensions (dR) are set to 640 with = 200 mixtures. Table 1 summarizes the detailed architectural configurations. Table 1 The architecture configurations of FARMER in two different scales (i.e., 1.1B and 1.9B). Model Autoregressive Transformer Layers Hidden size Params AF Blocks Layers Hidden size Params Invertible Autoregressive Flow Params FARMER-1.1B FARMER-1.9B 12 24 768 1024 295M 498M 28 32 4 768 768 828M 1.1B 1.9B 1.4B Training Setup. We train the models using AdamW optimizer (β1 = 0.9, β2 = 0.95) with weight decay of 0.03 for 320 epochs. cosine learning rate schedule is applied, starting from 1 104 to 1 106, with 5,000-step 9 Table 2 System performance comparison on ImageNet 256 256 class-conditioned generation. or indicate lower or higher values are better. Metrics include Fréchet inception distance (FID), inception score (IS), precision and recall. Resampling-based CFG is applied on FARMER. Types Models Params Epochs FID IS Pre. Rec. Latent Generative Models LDM-4 [54] DiT-XL [48] SiT-XL [39] FlowDCN [69] REPA [77] DDT-XL [71] GIVT [62] MAR-AR [35] MAR-L [35] 400M + 86M 675M + 86M 675M + 86M 618M + 86M 675M + 86M 675M + 86M 1.67B+86M 479M+86M 479M + 86M STARFlow [18] one-step denoise STARFlow [18] finetune decoder 1.4B+86M 1.4B+86M Diff. AR NF Pixel Generative Models GAN BigGAN [3] Diff. AR NF NF+AR ADM [11] CDM [23] SimpleDiffusion [24] PixelFlow-XL/4 [6] PixNerd-XL/16 [70] SiD2 patch 1 [25] FractalMAR-H [36] TARFlow [78] patch 8 STARFlow [18] patch 8 JetFormer [63] FARMER 1.1B patch 16 FARMER 1.1B patch 8 FARMER 1.9B patch 16 FARMER 1.9B patch 8 112M 554M - 2.0B 677M 700M - 844M 1.3B 1.4B 2.8B 1.1B 1.1B 1.9B 1.9B 170 1400 1400 400 800 400 500 800 800 320 320 / 400 2160 800 320 320 1280 600 320 320 500 320 320 320 320 3.6 2.27 2.06 2.00 1.42 1.26 2.59 4.69 1. 2.96 2.40 6.95 4.59 4.88 2.77 1.98 1.93 1.38 6.15 5.56 4.69 6.64 5.40 5.02 3.96 3. 247.7 278.2 270.3 263.1 305.7 310.6 - 244.6 296.0 - - 224.5 186.7 158.7 211.8 282.1 298 - 348. - - - 212.23 237.00 250.64 269.21 0.87 0.83 0.82 0.82 0.80 0.79 0.81 - 0.81 - - 0. 0.82 - - 0.81 0.80 - 0.81 - - 0.69 0.78 0.80 0.79 0.81 0.48 0.57 0.59 0.58 0.64 0.65 0.57 - 0. - - 0.38 0.52 - - 0.60 0.60 - 0.46 - - 0.56 0.45 0.45 0.50 0. linear warmup. Gaussian noise with cosine decay from 0.1 to 0.005 is added to the raw image. Evaluation Metrics. To assess sample quality, we use Fréchet Inception Distance (FID) [22], Inception Score (IS) [56] and Precision/Recall [33] on 50K generated samples to measure the image quality on ImageNet-256."
        },
        {
            "title": "4.2 Results\nSystem-level Comparison. As shown in Table 2, we compare FARMER with various generative models,\nincluding both latent and pixel-based approaches, on the class-conditional ImageNet 256×256 benchmark.\nNotably, FARMER significantly outperforms JetFormer [63], the most comparable baseline to our model,\nreducing the FID by 3.04. Furthermore, FARMER demonstrates superior generation quality compared to\nthe NF-based models, TARFlow [78] and STARFlow [18]. FARMER also achieves competitive performance\nand faster convergence speed against mainstream Generative Adversarial Networks (GANs), diffusion models,\nand AR models. While methods like PixelFlow [6] and PixNerd [70] employ complex multi-stage pipelines to\nachieve better results, our approach remains highly competitive by utilizing a simple, single-stage, end-to-end\ntraining strategy. Compared to latent generative models, our method maintains strong generative performance.\nLatent generative models often benefit from a well-structured continuous latent space, modeled by VAEs, that\nfacilitates high-quality sampling. However, by operating directly in pixel space, our model gains direct access\nto the raw data distribution. This approach can potentially capture more detailed data semantics without the",
            "content": "10 Figure 4 Qualitative Results. Images generated by FARMER on ImageNet 256x256. Figure 5 Qualitative Comparison. Images of class 0 in ImageNet generated by FARMER, MAR, and DiT. information bottleneck imposed by VAEs. Qualitative Results. To qualitatively evaluate FARMER, we show 28 generated images by FARMER-1.9B in Figure 4, sampling using resampling-based classifier-free guidance. As shown, our FARMER generates diverse images with high quality. key advantage of FARMER over latent generative models is its ability to preserve fine-grained details. This is because our end-to-end training directly accesses the raw data distribution, and the invertible nature of NFs prevents information loss. As shown in Figure 5, our FARMER can reconstruct intricate features, such as faces, which are often blurred or distorted by the compression of VAEs."
        },
        {
            "title": "4.3 Experimental Analysis\nAblation Study. Here we investigate the impact of each component within the FARMER framework on\noverall performance. Table 3 reports the performance of FARMER-1.1B with GMM components number\n(K=1024) across the ablated runs of different components on ImageNet 256×256 dataset for class-conditional\nimage generation. Natural images typically possess a high degree of redundancy, and low-dimensional signals\nwith low-frequency components dominating the spectrum [63]. Direct transformation of original images\nusing normalizing flows yields latent representations with unchanged dimensionality. Partitioning these\nhigh-dimensional latents into equal-length, high-dimensional tokens complicates AR modeling and sampling.\nBy introducing a self-supervised dimension reduction design as Eq (14), the FID notably decreases from 61.17\nto 49.29, and IS also improves from 22.10 to 30.61. Next, we repeat the class embedding 64 times to enhance",
            "content": "11 Table 3 Ablation study of FARMER. We demonstrate relative impact of various components on generation quality. Self-supervised Dim. Reduce Condition Repeat Final Permute CFG Method Naive Method Resampling-based FID 61.17 49.29 45.34 45.69 44.56 8.66 5.67 IS 22.10 30.61 33.87 33.73 33.17 233.84 215.53 NF Architectures FID IS Forward Speed (s/img) Reverse Speed (s/img) Table 4 Impact of Normalizing Flow Architectures. Jet AF AF+One-step Distll. 106.23 5.55 5.63 13.14 194.63 193. 0.0065 0.0066 0.0066 0.0099 0.1689 0.0076 the conditional guidance, the FID further decreases to 45.34. If we consider the AR model as block of AF, adding token permutation operation between AF and AR is beneficial to preserve the fixed dependency between token sequences. The FID further decreases to 44.56. CFG is essential for improving generation quality in modern generative models during sampling. We first adopt naive CFG sampling method from Jetformer [63], the FID score notably decreases to 8.66. Then, we upgrade the CFG sampling method to the resampling-based method described in Section 3.4, the FID score further decreases to 5.67. Together, these design choices enable FARMER to achieve strong performance across most evaluation metrics. Impact of Normalizing Flow Architectures. The architecture design of NF is an important research topic and has been extensively studied [12, 13, 29, 30, 32, 43, 53, 65, 78]. Different NF architectures exhibit distinct characteristics in terms of representational capacity, training speed, and inference efficiency. Here we primarily compare two architectures, Jet and AF, which have demonstrated strong performance in modern generative models Jetformer [63] and Tarflow [63], respectively. For fair comparison, we employ similar network parameters, same block numbers, the same layers per block, and the same AR models. Their representational capacity is evaluated using the FID metric, while forward and reverse speeds are also reported. Table 4 summarizes these results. Specifically, in each transformation of Jet, Jet first computes an affine transform from one half of the input latent channels by Jet block and then applies it to the other half of the input channels; this pattern applies to both forward and reverse passes. Jet is constructed by stacking such transformations. This simple and efficient design enables Jet to achieve fast forward and reverse computations, but it also limits its representational capacity, leading to failure to separate different information of the image into two channel groups. As described in Section 3.2, in each transformation of AFs, each token is updated based on preceding tokens through the block, resulting in reverse process where each token must be generated one by one. This enhances representation ability but leads to slow reverse speed. To address this, we introduce one-step distillation strategy. By distilling student AF model from the trained and frozen teacher AF model over only 60 additional training epochs on the NF, we significantly improve the reverse speed from 0.1689 seconds per image to 0.0076 seconds per image. This approach provides fast and expressive architecture for both training and inference. Dimension Reduction Method Comparison. We also compare our self-supervised dimension reduction method with the approach adopted in JetFormer [63]. JetFormer assumes that subset of channels is redundant and independent from the remaining channels and maximizes the likelihood of these redundant channels under the standard Gaussian prior. This assumption may result in information loss, thereby degrading generation quality. In contrast, our self-supervised method models redundant channels as being conditionally dependent on informative channels which encapsulate the global information of images. Our method achieves improved generative performance, reducing FID from 7.81 to 5.67, and increasing IS from 182.87 to 215.53. (a) Impact of GMM mixture component number (b) Impact of informative dimension Figure 6 The ablation study of different properties. Figure 7 The impact of redundant channels. The numbers above indicate scaling factors applied to the variance of the shared GMM distribution for redundant channels. Adjusting this variance controls sampling diversity: larger variance yields more diverse, potentially out-of-distribution samples, while smaller variance limits diversity. Visualization results demonstrate that the self-supervised dimension reduction effectively separates structural information from color details. Impact of GMM Mixture Component Number. We analyze the impact of the number of GMM mixtures predicted by the AR models, which reflects the complexity of the approximated distribution. larger number of mixtures enables the model to represent more complex distributions; however, it also increases sampling difficulty and computational cost during training. As shown in Figure 6a, the FID varies only slightly across different mixture numbers and attains its optimal value at 64 mixtures. Notably, reducing the number furtherto 32 mixturesprevents the model from performing effective dimension reduction, resulting in significant decline in generation quality. Therefore, we set the number of mixtures to 64 to balance generation quality and training cost. Impact of the Informative Dimension. We analyze the impact of the informative dimension, which reflects how information is separated and allocated by the NF models. As shown in Figure 6b, the FID initially decreases as the informative dimension increases and achieves the optimal value at 128. Further increasing the dimension leads to rise in FID. This phenomenon demonstrates trade-off: increasing the informative dimension allows capturing more information, but also makes AR modeling and sampling more challenging. Therefore, we set the informative dimension to 128. Information Separation of Different Dimension Groups. Here, we visualize the information contained in the informative and redundant channels. Specifically, during inference, we first predict all tokens of the informative channels in token-by-token manner. Subsequently, based on these tokens, we predict shared GMM distribution for the redundant channels. By adjusting the variance of each Gaussian component in the GMM, different distributions are obtained, from which we sample all tokens of the redundant channels. As shown in Figure 7, reducing the variance causes sampled tokens of redundant channels to concentrate around the means of the Gaussians, resulting in reduced diversity and smoother color regions, while the global structure of the images remains largely unaffected. Conversely, increasing the variance enhances diversity 13 but raises the risk of sampling out-of-distribution values, which can lead to color artifacts or, in extreme cases, failure to generate coherent images. These observations demonstrate that our self-supervised dimension reduction method successfully decouples structural contour information from fine color details. Table 5 Inference Speed Accelerate. Method Epochs FID IS AR infer. time (% in total) NF reverse time (% in total) Total time FARMER w/. One-step Distll. 280 280+ 5.55 5.63 194.63 193.49 0.0500s (22.8%) 0.0500s (88.2%) 0.1689s (77.2%) 0.0076s (13.4%) 0.2189s 0.0567s Inference Speed Acceleration. As shown in Table 5, the baseline FARMER requires 0.2189 seconds per image for inference, where the AR Transformer accounts for 0.0500 seconds and the NF reverse process dominates with 0.1689 seconds. By applying the proposed one-step distillation strategy, the NF reverse time is dramatically reduced from 0.1689 to 0.0076 seconds, yielding 22 acceleration for this component. Consequently, the total inference time decreases from 0.2189 to 0.0567 seconds per image, nearly 4 overall speed-up, while maintaining comparable image quality (FID 5.63 vs. 5.55, IS 193.49 vs. 194.63). These results demonstrate that one-step distillation effectively eliminates the sequential bottleneck of the reverse process, enabling FARMER to achieve both high fidelity and efficient generation. Impact of Logdet As defined in the training objective (see Eq (14)), the log-determinant (logdet) loss term quantifies the volume change induced by the transformation from the original space to the target latent space. As illustrated in Figure 8, samples with abnormal logdet values often exhibit blurred appearance and lack fine-grained details. Excessively large logdet values indicate that certain regions of the latent space are strongly compressed in the data space, which can lead to significant errors when reversing the transformation and reconstructing the data. This suggests that maintaining stable logdet values is crucial for high-fidelity and detail-preserving generation. Figure 8 The sample images with abnormal log-determinant values. High logdet values cause strong compression in parts of the data space, leading to blurred textures and missing fine-scale details in the generated images."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce FARMER, novel generative framework that integrate invertible AF with AR model, enabling end-to-end training directly on raw image pixels. FARMER learns by mapping the data distribution to an distribution modeled by the AR model and maximizing the negative log-likelihood of the raw images. This design permits both high-quality image synthesis and explicit likelihood estimation. Furthermore, we propose key techniques: self-supervised dimension reduction to alleviate the complexity of AR modeling/sampling, resampling-based CFG strategy to enhance image quality, and one-step distillation scheme to accelerate the inference speed. Through the contributions, FARMER demonstrates competitive performance in image generation relative to pixel-based and latent generative models. However, beyond the curse of high-dimensionality that we have addressed, two challenges persist in NFAR, i.e., (i) dequantization relying on noise injection and (ii) the complications arising from the log-determinant loss. We leave these for future works."
        },
        {
            "title": "6.1 Continuous AR",
            "content": "A common paradigm in autoregressive image generation is to quantize images into discrete tokens [4, 34, 49, 50, 57, 66, 76] and train autoregressive models over them, as exemplified by LlamaGen [57], Janus-Pro [7], and SimpleAR [68]. However, this design suffers from key bottleneck: quantization inevitably introduces information loss, which limits the fidelity of generated images [19, 35, 62]. To address this issue, GIVT [62] uses continuous latents obtained from VAE to encode images and trains an AR model to predict GMM parameters for approximating token distributions. ARINAR [80] further predicts GMM parameters of each token in Gaussian-to-Gaussian paradigm. Since GMMs have limited expressive power, Tschannen et al. further introduce NF to transform GMM samples into tokens, thereby improving generation quality. Jetformer [63] goes one step further by discarding the VAE and directly training AR and NF models in the pixel space. Another line of work explores continuous token modeling by combining AR with diffusion models. In MAR [35], the AR backbone first outputs conditioning vector for each token, and the diffusion head then generates the next tokens conditioned on this vector. Building on this idea, several other continuous-token approaches have been proposed [8, 9, 20, 27, 37, 40, 46, 51, 52, 60, 72, 73, 75, 81]. For example, FlowAR [51] employs VAR [61] backbone with flow matching as the generative head; Hi-MAR [81] pivots on low-resolution image tokens to trigger hierarchical autoregressive modeling in multi-phase manner; xAR [52] autoregressively generates next groups of tokens through flow matching. Although diffusion-based methods are effective at sampling continuous tokens, they require iterative noise-to-token denoising, which limits the model ability to perceive and understand images. In contrast, our model directly fits the token distribution without relying on noise sampling."
        },
        {
            "title": "6.2 Autoregressive Normalizing Flow",
            "content": "Normalizing flows (NF) [1215, 17, 29, 31, 41, 42, 44, 53, 65] provide powerful framework for density estimation, visual generation, and text generation [79], via invertible transformations, enabling exact likelihood computation and efficient sampling. However, the representational capacity of NFs is limited by the expressiveness of these invertible transformations. To address this limitation, autoregressive normalizing flows have been proposed, where each token is transformed conditioned on previous tokens. There has been long line of work on autoregressive normalizing flows, with representative approaches including IAF [30], MAF [43], neural autoregressive flows [26], and T-NAF [47]. More recently, the resurgence of NFs has attracted renewed interest. TARFlow [78] leverages causal Transformers and simplifies the log-determinant term in the loss function, leading to notable improvements in generation quality. STARFlow extends TARFlow into the VAE latent space and demonstrates that continuous AR flows can deliver competitive generative performance. Meanwhile, JetFormer [63] integrates Jet [32] to enable fully end-to-end continuous AR modeling directly over raw image pixels."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [3] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. [5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning. PMLR, 2020. 15 [6] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [9] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [12] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516, 2014. [13] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In International Conference on Learning Representations, 2017. [14] Felix Draxler, Peter Sorrenson, Lea Zimmermann, Armand Rousselot, and Ullrich Köthe. Free-form flows: Make any architecture normalizing flow. In International Conference on Artificial Intelligence and Statistics, pages 21972205. PMLR, 2024. [15] Felix Draxler, Stefan Wahl, Christoph Schnörr, and Ullrich Köthe. On the universality of volume-preserving and coupling-based normalizing flows. arXiv preprint arXiv:2402.06578, 2024. [16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Robert Giaquinto and Arindam Banerjee. Gradient boosted normalizing flows. Advances in Neural Information Processing Systems, 33:2210422117, 2020. [18] Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow: Scaling latent normalizing flows for high-resolution image synthesis. arXiv preprint arXiv:2506.06276, 2025. [19] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [20] Tiankai Hang, Jianmin Bao, Fangyun Wei, and Dong Chen. Fast autoregressive models for continuous latent generation. arXiv preprint arXiv:2504.18391, 2025. [21] Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1712317131, 2025. [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. [23] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. [24] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 1321313232. PMLR, 2023. 16 [25] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. [26] Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive flows. In International conference on machine learning, pages 20782087. PMLR, 2018. [27] Guolin Ke and Hui Xue. Hyperspherical latents improve continuous-token autoregressive generation. arXiv preprint arXiv:2509.24335, 2025. [28] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [29] Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. [30] Durk Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016. [31] Ivan Kobyzev, Simon JD Prince, and Marcus Brubaker. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):39643979, 2020. [32] Alexander Kolesnikov, André Susano Pinto, and Michael Tschannen. Jet: modern transformer-based normalizing flow. arXiv preprint arXiv:2412.15129, 2024. [33] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. [34] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern using residual quantization. Recognition, pages 1152311532, 2022. [35] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [36] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. [37] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. [38] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. [39] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [40] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. [41] Bálint Máté, Samuel Klein, Tobias Golling, and François Fleuret. Flowification: Everything is normalizing flow. Advances in Neural Information Processing Systems, 35:3547835489, 2022. [42] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. Advances in neural information processing systems, 33:25032515, 2020. [43] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density estimation. Advances in neural information processing systems, 30, 2017. [44] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):164, 2021. 17 [45] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning. PMLR, 2018. [46] Marco Pasini, Javier Nistal, Stefan Lattner, and George Fazekas. Continuous autoregressive models with noise augmentation avoid error accumulation. arXiv preprint arXiv:2411.18447, 2024. [47] Massimiliano Patacchiola, Aliaksandra Shysheya, Katja Hofmann, and Richard Turner. Transformer neural autoregressive flows. arXiv preprint arXiv:2401.01855, 2024. [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [49] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning. Pmlr, 2021. [50] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [51] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. [52] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. [53] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 15301538. PMLR, 2015. [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [55] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [56] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems, 29, 2016. [57] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [58] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [59] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [60] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. [61] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [62] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. GIVT: Generative infinite-vocabulary Transformers. arXiv:2312.02116, 2023. [63] Michael Tschannen, André Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024. [64] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. [65] Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning. PMLR, 2016. [66] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [67] Steven Walton, Valeriy Klyukin, Maksim Artemev, Denis Derkach, Nikita Orlov, and Humphrey Shi. Distilling normalizing flows. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. [68] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [69] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Exploring dcn-like architecture for fast image generation with arbitrary resolution. Advances in Neural Information Processing Systems, 37:8795987977, 2024. [70] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. [71] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. [72] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [73] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. [74] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [75] Hu Yu, Hao Luo, Hangjie Yuan, Yu Rong, and Feng Zhao. Frequency autoregressive image generation with continuous tokens. arXiv preprint arXiv:2503.05305, 2025. [76] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. [77] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. [78] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. [79] Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Yizhe Zhang, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Josh Susskind, and Navdeep Jaitly. Flexible language modeling in continuous space with transformer-based autoregressive flows. arXiv preprint arXiv:2507.00425, 2025. [80] Qinyu Zhao, Stephen Gould, and Liang Zheng. Arinar: Bi-level autoregressive feature-by-feature generative models. arXiv preprint arXiv:2503.02883, 2025. [81] Guangting Zheng, Yehao Li, Yingwei Pan, Jiajun Deng, Ting Yao, Yanyong Zhang, and Tao Mei. Hierarchical masked autoregressive models with low-resolution token pivots. arXiv preprint arXiv:2505.20288, 2025."
        },
        {
            "title": "A Discussions",
            "content": "A.1 FARMER reduces to Autoregressive Flow when = 1 When the number of components (K) in the Gaussian Mixture Model (GMM) predicted by FARMER is set to one (K = 1), FARMER reduces to an Autoregressive Flow (AF). In this case, each token zi in the sequence is modeled by conditional Gaussian distribution, where the mean and variance are functions of the preceding tokens z<i. The optimization objective for each token becomes: log p(ziz<i) = log (cid:0)N (zi; µ(z<i), σ2(z<i))(cid:1) (16) This can be further expressed as: log p(ziz<i) = log (cid:20) (cid:18) zi µ(z<i) σ(z<i) ; 0, Id (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) [zi µ(z<i)]/σ(z<i) zi (cid:12) (cid:21) (cid:12) (cid:12) (cid:12) , (17) where (; 0, Id) denotes the standard normal density, and the second term inside the log corresponds to the change of variables formula (the volume correction by the Jacobian determinant). Expanding the log yields two components: log p(ziz<i) = log (cid:18) zi µ(z<i) σ(z<i) (cid:19) ; 0, Id + log (cid:12) (cid:12) (cid:12) (cid:12) 1 σ(z<i) (cid:12) (cid:12) (cid:12) (cid:12) , (18) zi is transformed to new token ziµ(z<i) by the predicted results (µ(z<i), σ(z<i)) of the AR model conditioned σ(z<i) on preceding tokens z<i, and this transformation is invertible; the first term is the log-likelihood of new token under the standard Gaussian distribution, and the second term is the log-determinant of the Jacobian of the affine transformation. Thus, the AR model can be considered as the last block of AFs. This confirms that when = 1, FARMER reduces to an Autoregressive Flow. A.2 Resample-based CFG While the main text (Section 3.4) outlines the proposed Resampling-based Classifier-Free Guidance (CFG) method, we further elaborate on additional tunable parameters that enhance generation quality and control diversity. Each of the three stagesPropose, Weigh, and Resampleintroduces dedicated temperature coefficients and sampling numbers that can be adjusted. Propose stage. In the proposal step, candidate samples are drawn from the conditional GMM pc(zi) and the unconditional GMM pu(zi). To control the diversity at this stage, we introduce two distinct temperature coefficients: Weight temperature Tπ: applied multiplicatively to the mixture weights πk(z<i) of the GMM components before normalization. This modulates the relative selection probability among Gaussian components. Variance temperature Tσ: applied multiplicatively to the variance σk(z<i) of each Gaussian component, scaling the spread of proposals. Additionally, the number of samples drawn from pc and pu can differ; we denote these by sc and su. This allows balancing between strongly conditioned proposals and broader unconditional exploration. Weigh stage. Given candidate samples zi,j, their importance weights are computed as: log ωi,j = (cid:0) log pc(zi,j; Tπ,v, Tσ,v) log pu(zi,j; Tπ,v, Tσ,v)(cid:1), where Tπ,v and Tσ,v are temperature coefficients for the evaluation distributions in this stage (not necessarily equal to those used in the Propose stage). These temperatures control the sharpness or smoothness of the scoring in the log-probability space. 20 Resample stage. Finally, the normalized weights ωi,j define categorical distribution. To further modulate selection sharpness, we introduce resampling temperature Ts applied uniformly to all log-weights before normalization: pfinal(zi,j) exp (cid:0) log ωi,j Ts (cid:1). Higher Ts emphasizes high-weight proposals, while lower Ts encourages diversity. Summary table of parameter choices. Table 6 summarizes the temperature and sampling configurations used for different model variants evaluated in this work. Table 6 Temperature and sampling parameters for Resampling-based CFG in different models. Model FARMER 1.1B (patch 16) FARMER 1.1B (patch 8) FARMER 1.9B (patch 16) FARMER 1.9B (patch 8) Tπ 1.0 1.0 1.0 1.0 Tσ 0.9 1.0 0.9 1.0 sc 5 5 5 su 5 5 5 5 Tπ,v 0.2 0.2 0.2 0.1 Tσ,v 0.9 0.9 0.9 1.0 Ts CFG 2.5 1.1 2.0 1.1 3.5 1.1 1.5 1."
        }
    ],
    "affiliations": [
        "ANU",
        "ByteDance Seed China",
        "ByteDance Seed Singapore",
        "NUS",
        "USTC"
    ]
}