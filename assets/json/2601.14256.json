{
    "paper_title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "authors": [
        "Matthew Gwilliam",
        "Xiao Wang",
        "Xuefeng Hu",
        "Zhenheng Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr."
        },
        {
            "title": "Start",
            "content": "TikTok* 6 2 0 2 0 2 ] . [ 1 6 5 2 4 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn latent space that is useful for image generation. We seek to unify these two directions with first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at this https URL. 1. Introduction Vision encoders, which learn to compute useful image and video representations, are foundational components of modern computer vision systems. Although such encoders may be able to solve many different tasks, in practice, we typically use them in specialized manner according to their strengths. Models trained with text-image contrastive learning [78] lend themselves well to tasks such as retrieval and visual question answering. Models trained with imageonly contrastive learning [16, 67] or self-distillation [11, 74] perform well for fine-grained image understanding, such as semantic segmentation. Variational autoencoders, which reconstruct pixels, enable image and video synthesis with diffusion and auto-regressive models. Some prior and con- *This work is for research purposes only and is not currently integrated into any TikTok technology. Figure 1. (Top) Unified modeling, in terms of both tasks and token dimensions. We propose implicit neural Hyper-networks for Unified Visual Representation, HUVR, with good classification, reconstruction, and segmentation (shown for ViT-B/16 on ImageNet, ImageNet, and ADE20K, respectively). We design our model to generate not only standard-sized tokens, but also Tiny Tokens (TinToks). Here, the tiny embeddings of DINOv3 are generated via principle component analysis (PCA). (Bottom) Reconstruction. We unify recognition and generative task families. current works attempt to unify these recognition-focused and generative-focused models post-hoc [25, 57, 68, 117]. These works point to very promising synergy between pretraining methods for recognition and generative models. natively unified encoders features should have good high level (image classification), mid level (semantic segmentation), low level (depth estimation), and pixel level (reconstruction) information out-of-the-box. These requirements point to hyper-networks for implicit 1 neural representation (INR) [22, 44] as natural candidate. An INR hyper-network is network that takes image inputs, and predicts neural network weights (INR) as output. These output INRs take pixel coordinates as inputs, and give pixel color channel values as outputs. INR hyper-networks are similar to traditional autoencoders in the sense that their latents have good pixel representation. However, unlike convolutional VAEs, transformer-based INR hyper-networks can easily borrow encoder design, patch sizes, and latent dimensions from state-of-the-art Vision Transformer (ViT) encoders [27]. Furthermore, compared to other representation learning methods, INR hyper-networks perform compression along multiple axes, first by converting specific image to latents, and second by learning shared base INR to represent all possible images. We hypothesize this is helpful for learning high quality embeddings not only at the pixel-level, but also for low-, middle-, and high-level image information [34, 54, 84, 90]. Unified representation requires more than simply training model whose representations have suitable semantics for any task. In practice, tasks differ from each other not just in terms of the level of information or semantics they need, but also in terms of the amount of computational resources available to solve them. Tasks like retrieval become very difficult as the amount of data increases at scale, and reducing the embedding size introduces massive savings. Thus, we propose to unify representations not just in terms of tasks, but also by designing model that can generate both compressed and non-compressed representations. We design our Hyper-network for Unified Visual Representation (HUVR) with novel INR hyper-network design to natively unify disparate representation learning families, particularly recognition and reconstruction, in terms of both architectural design and pretraining. HUVR encodes images both in standard size representations, as well as smaller representation that we refer to as Tiny Tokens (TinToks). Figure 1 shows their superior recognition and novel reconstruction abilities. We build on the strengths of INR hyper-networks to design HUVR and its TinToks. INR hyper-networks do not natively learn good high level semantics, and the design of the latent tokens makes it difficult to recover patch-level information. We solve the gap in semantics via knowledge distillation from pretrained vision encoder. We refactor the design of the latent tokens to naturally support both mapping between input patches and patch tokens, as well as global cls token. To create the compressed representation, the TinToks, we introduce learnable feature downsampling and upsampling layers in between the transformer backbone and the layers that produce the final INR prediction. In summary, our key contributions are: We design and train an INR hyper-network, HUVR, which can match or outperform DINOv3 [87] with ViTB/16 we achieve +0.4% for ImageNet [24] classification and +1.2 mIoU for ADE20K [118] semantic segmentation, and +4.84 PSNR for reconstruction. The compressed representation TinToks, at 96x compression, can offer +48% ImageNet classification compared to DINOv3 PCA baseline, and +1.26 PSNR compared to the Stable Diffusion VAE [80] at equal embedding size. An improved design for hyper-networks for image implicit neural representations yields +2.34 PSNR even with only 10% training time on ImageNette. 2. Related Work 2.1. Implicit Neural Representation Implicit Neural Representation. An implicit neural representation (INR) is neural network that maps coordinates to signal (image, video, 3D, audio, etc.) [12, 66, 69, 83, 88, 94, 102]. Due to their small size, significant volume of research focuses on trying to leverage INRs for image and video compression [12, 29, 30, 45, 47, 50, 51, 64, 91, 114]. Methods in the NeRV [12] family use combination of linear layers, convolutional layers, and PixelShuffle [86] operations to map frame embeddings to the RGB values of the frame for video compression [13, 14, 36, 38, 46, 49, 53, 58, 82, 100, 103, 105, 113, 115, 116]. Other INR methods (including some in the NeRV family) use grid features for positional embedding [33, 47, 49, 53, 64]. Hyper-Networks. INRs require costly per-sample training, which limits their applicability. Some try to avoid the need to train per-sample by instead training hyper-network. The hyper-network learns to predict INR network weights that can reconstruct the input [15, 22, 44, 52, 112] or generate novel outputs [37, 89, 107]. Latent-INR redesigns INRs as per-video hyper-networks, and aligns an learnable latent for each frame with its corresponding CLIP embedding [65]. While we also pursue INR with semantics, unlike LatentINR, we learn general hyper-network that can predict INR network weights even for inputs not seen during training. 2.2. Image Representation Learning The first unsupervised deep learning methods for image representation focused on restoring withheld or corrupted information [67, 73, 76, 110], but they struggled to match the performance of supervised learning. These were followed by methods, mainly based on contrastive learning and clustering, which were able to compete with supervised learning [6, 911, 1618, 20, 21, 35, 56, 75, 95, 108, 120]. iBOT [119] was able to outperform these with masked image modeling [1, 4, 39, 43], and recent entries in the DINO family fuse these with self-distillation paradigm based on knowledge distillation [41] for state-of-the-art results [74, 87]. Some methods train an image representa2 Figure 2. INR Hyper-Network for unified visual representation. The standard and compressed encodings from our model have powerful recognition and reconstruction capabilities, enabling downstream tasks ranging from classification to image generation. tion jointly with text modeling [40, 78, 79, 93, 109], and these methods typically perform best as the vision encoders for multimodal large language models [3, 60, 61, 98, 104], while text-free methods (DINOv3) perform best for dense tasks. Some methods train models for both recognition and generation [19, 25, 26, 28, 57, 71, 101], while other recent works rely on pre-trained generative model, which is then adapted post-hoc for recognition [55, 68, 101]. In this work, we propose text-free method with design that allows it to natively perform recognition and reconstruction. 3. Methods 3.1. Hyper-Network for Implicit Neural Representation Background. Implicit neural representations store signals as neural network weights. function fθ : I, which maps coordinates to an image (or more generally, signal) I. In the case of images, we can represent these coordinates as = {(xi, yj0 xi < W, 0 yj < H} and images as color tuples = {(rxi,yj , gxi,yj , bxi,yj )0 xi < W, 0 yj < H}. In practice, these networks require long per-sample training time, with multiple iterations for every pixel, for fθ to memorize I. Additionally, there is no generalizability for every new I, we must train new corresponding fθ. To solve this, hyper-network approximates function, h, for mapping input signals to their corresponding instance-specific INR, : fθ [22]. Instead of fitting fθ separately for each sample, is trained on large dataset, and learns general mapping between data samples and θ. After this pre-training, encoding as θ requires only one forward pass of h. Prior INR hyper-networks are initialized with 4 learnable components: (i) transformer encoder, E, (ii) shared or base INR weights θb with layers, θb = {Wi}n i=1, (iii) input weight tokens corresponding to the number of layers n, and (iv) fully-connected (FC) linear layers to project between the transformer encoder output dimension and din of Wi. The transformer encoder, E, takes the image and weight tokens as input, and gives output image and weight tokens. The output image tokens are discarded. The output weight tokens corresponding to some Wi in θb are fed through an FC layer to change their token dimension to din, and then repeated = dout times to form modulation matrix Mi of shape dindout. Each Mi is multiplied elementwise with Wi to modulate it, forming the final weights θ In other words, for sample-specific fθ, we have of . θ = {Wi Mi}n i=1. For more details, we include tutorial in the Appendix. Overview. Figure 2 depicts our method. Unlike prior Instead of works, there are no learnable weight tokens. 3 predicting image INRs, our predicts fθ for each patch separately. Patch tokens are fed to the INR hyper-network, along with novel learnable global token. The INR hypernetwork itself is composed of 3 parts, which generate 3 sets of tokens in sequence: (i) standard Vision Transformer (ViT) output, (ii) compressed encoding (TinToks), (iii) an INR modulation encoding. The INR modulation encoding for each patch is projected and then multiplied by projection of the global token, which yields matrix Mi to modulate the corresponding Wi of θb. We use all modulated, per-patch INRs for given image to reconstruct the input patch. We concatenate the reconstructed patches to reconstruct the full I. Key Innovation #1: Patch Tokens as Weight Tokens. Existing INR hyper-networks discard the output image tokens, using them neither to calculate loss nor perform inference. This makes dense tasks like semantic segmentation very challenging, since is represented mainly in the weight tokens, but these lack clear correlation with spatial locations in the input. To resolve the inefficiency and lack of capability, we use the data tokens themselves as weight tokens. This presents challenge in the standard hyper-network formulation, the number of weight tokens must be factor of the din or dout for Wi in θb. This makes it difficult to formulate good hyper-network and INR settings. To solve this, we refactor the framework, predicting instead an fθ per-patch, rather than per-image. So, the output tokens of are also patch tokens. This leads to another issue. The output token has dimension dViT, but Wi has dimension dindout. Following the ideas from prior works [15, 22] we could simply make sure dViT = din and, and copy the output token dout times to yield an Mi to multiply with Wi. However, in practice this is suboptimal (see Section 4.5). Key Innovation #2: Global Tokens to Modulate and Summarize. Now, recall that the original INR hypernetwork formulation does not have cls token. We solve both of our problems at once. We add global token g, which can act as cls token for recognition tasks. We can project to have dimension dg = dout and project patch token to have dimension dp = din. Then, when we multiply pT , we yield the matrix Mi with shape dindout for modulating Wi. With this, we successfully modify the hyper-network architecture to have class token, patch tokens, and no wasted tokens, making it suitable for both INR prediction and image recognition. Key Innovation #3: Tiny tokens (TinToks). In practice, we want to be able to set the dimension of the transformer encoder, dViT separately from din and dout for any Wi. Additionally, compute-constrained applications require smaller tokens. To satisfy this, we introduce an intermediate representation, TinToks, which have their own dimension dt. To accomplish this, we use linear layer to downsample from dViT to dt, and another to upsample for decoding. We then process the tokens with transformer decoder to allow for better reconstruction. We use final linear projections to convert the tokens from the decoding dimension to din and dout for the patch and global tokens, respectively. Training. We train the model with distillation losses (described in Section 3.2) and visual quality losses. For the visual quality, we compute mean-squared error on pixels between the input image and the concatenated patch predictions of each patch fθ. We can also optionally train with SSIM [99] and LPIPS [111] losses to further improve the reconstruction. 3.2. Distillation to INR Hyper-Networks Key Innovation #4: Distillation for Unified Reprsentation. We align our representations to the pre-trained model (for most of our experiments, DINOv3) by computing loss between DINOv3 features and linear projection of our features. While we could hypothetically apply this to the outputs of any of ours layers, in practice we apply this loss to the outputs of the last encoder (enc) and decoder (dec) blocks. We learn separate transformations for each block, and within each block we learn separate transformations depending on whether token is global (typeg or patch (typep. So, considering token types typeg, typep and outputs of the final encoder and decoder blocks oenc, odec O, we get 4 components of our loss, which we weight individually with some α. We can compute an L2 distillation loss with features from some teacher as follows: Ldistillation = (cid:88) (cid:88) L2(θt,o(Ft,o)Ft,teacher)αt,o (1) Note that we do not perform distillation on the compressed tokens directly. Combined with the INR reconstruction objective, we find that distillation to the encoder and decoder is sufficient to imbue the compressed tokens with good semantics for downstream reconstruction tasks. 4. Results 4.1. Settings Training. For our models, we use modern version of the Vision Transformer (ViT) [27], with Rotary Positional Embeddings (RoPE) [92]. We focus on ViT-B/16 and ViT-L/16, due to the prevalence of practical adoption for these sizes, and to reduce our training cost. Unless otherwise indicated, we pre-train our models on mix of DataComp [32] (image-only) and ImageNet22k [81] data (without labels). We ensure at least 10% of all samples are from ImageNet22k (inspired by DINOv3 [87]) for the equivalent of 50 epochs on ImageNet22k. We parallelize according to resource availability by adjusting batch size and scaling the Table 1. Tiny token results. We compare our compressed tokens to principle component analysis (PCA) baseline. We compute the PCA transforms using the ImageNet-1k training set. We report linear probing classification accuracies for ImageNet (original and ReaL labels), ObjectNet, and some fine-grained datasets. We train decoder on frozen DINOv3 for reconstruction baseline. Enc. VAE ViT-B - 16 32 ViT-L 32 Settings Classification Reconstruction Dim. Pre-Training INet ReaL ONet Cars CUB DTD Flowers Food PSNR SSIM SD VAE DINOv3, PCA HUVR (ours) DINOv3, PCA HUVR (ours) - 16.1 64. 40.6 75.6 C-RADIOv3, PCA 71.8 62.3 SigLIP 2, PCA 64.1 DINOv3, PCA 79.4 HUVR (ours) C-RADIOv3, PCA 77.9 63.4 SigLIP 2, PCA 72.2 DINOv3, PCA 78.1 HUVR (ours) - 18.0 71.1 45.1 82. 79.2 67.8 70.3 85.6 83.8 67.9 77.5 82.9 - 8.2 34.0 19.5 43.7 42.1 31.3 35.7 47. 47.5 33.9 45.0 53.9 - 31.3 53.8 58.5 72.5 73.9 86.8 80.0 80.8 82.0 90.6 84.2 88. - 44.9 74.8 67.6 83.1 64.6 75.4 80.7 86.5 73.2 77.8 84.2 81.4 - 28.8 39.6 45.5 59.8 71.0 70.2 64.5 70.1 73.3 67.0 64.8 66.8 - 57.8 98. 86.7 99.4 93.7 98.6 97.3 99.6 98.6 99.3 99.1 99.5 - 24.99 0. 31.7 70.5 59.2 85.8 83.3 85.9 78.9 89.7 89.4 89.2 88.1 91.2 15.51 24.66 16.75 26. - - 17.68 27.83 - - 17.27 27.70 0.4975 0.6918 0.5228 0.7361 - - 0.5398 0.7845 - - 0.5311 0. Table 2. Diffusion results. We train class-conditional DiT-XL on ImageNet using the original VAE latents, and other DiT-XLs using our compressed tokens. We compare the DiTs in terms of FID, Inception Score, Precision and Recall. Autoencoder Latents FID sFID IS R SD VAE 32324 23.05 68.65 70. 0.4318 0.4775 Ours Ours 161616 16256256 24.72 24.53 76.09 68. 60.17 66.13 0.3850 0.4307 0.4645 0.4367 learning rate by linear factor. For data augmentation, we only use random resized cropping. For details on model and training settings, see the Appendix. Evaluation. Since we are mainly concerned with feature quality, we compute recognition evaluations with linear probing classification on ImageNet1k [24] using both the original labels and ReaL labels [7], and ObjectNet [5]. We do not perform any finetuning. We also perform linear probing with L-BFGS [59] for 5 FGVC datasets CaltechUCSD Birds 200 [97], Stanford Cars [48], Describable Textures Dataset [23], Oxford 102 Flower [72], and Food101 [8]. We perform linear probing on ADE20K [118] for semantic segmentation following the setup from AMRADIO [79] and use similar setup for depth probe [31] on NYUv2 [70]. For reconstruction, we report PSNR, SSIM, and LPIPS on the 50,000 images from the ImageNet1k validation set (which is never seen during training). For generation, we follow the protocol from DiT [77]. We also perform some comparison to prior INR hyper-networks by training and evaluating our reconstruction capability on comparable settings for the ImageNette subset of ImageNet [42], Celeb-A [62], and LSUN Churches [106] to prove that HUVR is the state-of-the-art INR hyper-network for images. For more details, see the Appendix. 4.2. Tiny Tokens are Unified Representation We show in Table 1 that our compressed vector representations simultaneously achieve good recognition and reconstruction. To our knowledge, our method is the first to attempt classification, segmentation, and depth estimation with such compressed features. So, for baseline, we fit principle component analysis (PCA) transform for DINOv3 [87], C-RADIOv3 [40], and SigLIP2 [96] at the corresponding compressed token dimension on the training set of ImageNet1k. We then use this transform for ImageNet, and all other datasets, and refer to it as PCA for each method. Note that none of these methods perform reconstruction natively, but as baseline we train decoder with roughly the same number of parameters as ours, on the frozen DINOv3 PCA features. Our compressed tokens offer the best unified modeling capability. The gap between ours and the baseline increases as the compression ratio increases notice the difference between ours and DINOv3 for 8-dim tokens. To show the promising generative potential of our latents, we train DiT [77] on the latents of our model instead of the SD VAE [80] latents. We show results in Table 2. While we fail to beat the performance of the existing SD VAE or achieve generative state-of-the-art results, we consider these results promising and hope they highlight the 5 Table 3. Standard classification results. Our model is competitive with state-of-the-arts for classification, including on fine-grained classification (FGVC) datasets. We set our TinTok = 32 but evaluate the performance of the full, standard-sized tokens."
        },
        {
            "title": "Encoder",
            "content": "Pre-Training"
        },
        {
            "title": "Food",
            "content": "ViT-B, Patch Size 16, = 768 ViT-L, Patch size 16, = 1024 C-RADIOv3 SigLIP 2 DINOv3 HUVR (ours) C-RADIOv3 SigLIP 2 DINOv3 HUVR (ours) 82.4 84.5 84.6 85.0 86.0 87.1 87.1 86. 87.6 89.0 88.9 89.2 89.6 90.1 90.0 90.1 54.5 68.4 59.4 62.0 66.0 75.5 71.0 70.7 88.6 92.8 93.4 93.1 92.8 94.9 93.7 91. 79.8 82.6 89.7 89.4 85.4 85.0 91.1 85.7 83.5 83.7 83.9 84.3 86.2 85.1 86.6 84.5 99.1 99.3 99.7 99.7 99.6 99.6 99.7 99. 91.5 94.2 93.7 94.3 94.4 96.1 95.6 95.4 potential our method as unified representation. Following insights from concurrent work [117], we try increasing the TinTok dimension to 256, and observe that increasing the TinTok dimension seems beneficial for generation. Applying further insights from the concurrent work, which uses frozen pre-trained vision encoders, would likely help further improve the performance, especially given that our model is natively superior for reconstruction. 4.3. HUVR beats DINOv3 for Classification The purpose of HUVR is not simply to generate powerful TinToks. Even HUVRs standard-size embeddings are quite useful. Table 3 shows that HUVR can match or beat prior works on ImageNet (both original and ReaL labels), and achieve good results on ObjectNet and the fine-grained datasets as well. Comparing our ObjectNet to SigLIP 2, it is worth keeping in mind that SigLIP 2 trains on 25 times more data. For FGVC datasets, DINOv3 mines pretraining data that is similar to many of these, whereas we use pretraining data that is not specifically curated for these [32]. 4.4. HUVR can Perform Dense Recogntion Tasks For fair comparison, since these tasks tend to evaluate at higher resolution (512512 for ADE20K semantic segmentation, 480480 for NYUv2 depth estimation), after we pre-train at 256256, we have second training stage. We further train for 3 epochs (following DINOv3) on mix of 256256 and 512512 resolution images, from ImageNet22k only. We achieve strong results, in Table 4, for both semantic segmentation and depth estimation, with both standard tokens and TinToks. Recall that the reconstruction performance for compressed DINOv3 tokens is quite poor  (Table 1)  . Keep in mind that for 32-dim TinTok, all information for both patch reconstruction (pixel-level) and segmentation (midlevel) must fit within 32 floating point values. Compression Table 4. Dense recognition results. We compare our distilled hyper-networks to Radiov3, SigLIP 2, and DINOv3 for semantic segmentation (ADE20K) and depth estimation (NYUv2). Model Size Method mIoU mAcc RMSE ADE20K NYUv2 ViT-B ViT-L 32 1024 32 SigLIP 2 C-RADIOv3 DINOv3 HUVR (ours) SigLIP 2 C-RADIOv3 DINOv3 HUVR (ours) SigLIP 2 C-RADIOv3 DINOv3 HUVR (ours) SigLIP 2 C-RADIOv3 DINOv3 HUVR (ours) 40.0 49.5 50.8 52.0 17.5 28.9 29.7 29.7 42.0 51.6 54.2 53.5 13.5 27.3 29.4 30.9 53.6 62.4 62.6 63. 25.0 40.3 38.6 39.1 55.8 63.8 66.2 65.2 19.3 37.0 39.2 41.1 0.5317 0.3359 0.3305 0.3263 0.8155 0.6017 0.7056 0.5980 0.5107 0.3239 0.3235 0. 0.7740 0.5430 0.6685 0.5726 helps the learning, but representation itself must be very In spite of these challenges, TinToks information-dense. achieve the overall best performance for dense tasks. We can further improve the dense task performance of the TinToks at the expense of reconstruction performance, and we investigate these trade-offs in Section 4.6 and Section 4.7. 4.5. Our Formulation is Ideal for Predicting Image"
        },
        {
            "title": "INRs",
            "content": "We show that our novel INR hyper-network design achieves state-of-the-art results compared to prior arts in Table 5. We set our hypernetwork here to have the same number of 6 Table 5. Hyper-network results. We compare our INR hyper-network to prior works on ImageNette (178178), LSUN (256256), and CelebA (178178), in terms of PSNR. Method Epochs ImageNette LSUN CelebA TransINR [22] IPC [44] LA-IPC [52] ANR [112] Ours 4000/12.67/300 4000/-/300 4000/-/300 -/12.67/- 400/12/100 29.01 38.46 46.10 - 48.44 24.21 - - 28.30 34.00 31.96 35.93 50.74 - 56.91 Table 6. Hyper-network design. We demonstrate how our changes to the original TransINR hyper-network impact the reconstruction performance on ImageNette. For each row, we keep all changes from prior rows, such that the last row contains our full method. Method # Params PSNR SSIM LPIPS TransINR + RoPE + only second layer [44] + patchwise + global token + compression + decoder 44.00M 43.21M 43.21M 43.41M 43.41M 43.40M 23.78 27.15 51.96 53.36 48.58 48.44 0.7041 0.8000 0.9974 0.9985 0.9962 0.9954 0.3866 0.2424 0.0026 0.0007 0.0019 0.0014 parameters or fewer, in terms of total encoder parameters, shared parameters, and predicted latents (details in the Appendix). Since our network generally requires more time to train, given equal epochs, we generally use significantly fewer epochs that prior works (400 on ImageNette instead of 4000). We thus achieve the best PSNR (the standard metric for these methods) with equal or shorter training time. We show how we achieve this in Table 6. The major driver of our good performance is our novel patch-wise design, which improves the performance at the cost of additional memory for forward computation. The global token, which we introduce mainly to facilitate tasks such as classification, also improves reconstruction performance. The compression and decoder are not necessary for reconstruction, but they are essential for good TinToks and wellbehaved distillation, respectively. 4.6. Recognition and Reconstruction can Improve"
        },
        {
            "title": "Together",
            "content": "In many ways the recognition and reconstruction performance can scale together. One of the most obvious ways is in terms of training time, shown in Figure 3, although the reconstruction performance saturates more quickly than classification. Another way is in terms of distillation teacher selection, which we explore via ablation an ablation in Table 7 where we train for 5 epochs on ImageNet22k with different teachers. For example, the RADIOv3 ViT-L improves all 5 metrics compared to the RADIOv3 ViT-B. In fact, for cases where the ViT-L seems worse, we find in practice that this Figure 3. Training time improves both classification and reconstruction performance, although longer training yields incrementally smaller gains. Table 7. Distillation teachers. We show the effect of distilling from different teachers in terms of main token classification (d=768), tiny token classification (d=32), and reconstruction (PSNR and SSIM). Teacher Classification Segmentation Reconstruction Enc. Method d=768 d=32 mIoU, d=768 PSNR SSIM ViT-B ViT-L DINOv2 C-RADIOv3 SigLIP 2 DINOv3 DINOv2 C-RADIOv3 SigLIP 2 DINOv3 82.7 78.3 81.3 83.2 80.9 78.8 82.6 81. 73.5 47.1 58.2 69.2 74.1 52.9 68.5 74.2 42.20 43.32 37.87 44.01 38.37 45.67 38.70 40.31 24.87 25.83 26.61 25.33 25.89 26.49 24.29 26. 0.6950 0.7240 0.7458 0.7129 0.7281 0.7433 0.6754 0.7439 is result of limited training time. Given sufficient training time (15 ImageNet22k epochs for DINOv3), there is crossover point where distilling from the larger teacher becomes superior (see Appendix), so in our main results we distill from ViT-L for our ViT-B, and from ViT-H for our ViT-L. 4.7. Some Design Decisions Involve Trade-offs We can revisit Table 7 in terms of some trade-offs between performance for different tasks. For example, the ViT-B DINOv2 teacher seems to give better tiny token accuracy, but this is counterbalanced by worse performance on other metrics. RADIOv3 gives the best overall segmentation performance, but the weakest ImageNet classification, by far. truly optimal method would probably distill from mixture of teachers, but we consider such engineering efforts out of scope. In Table 8 we compare our chosen distillation approach (where we compute distillation losses to both the global token and patch tokens) to alternatives where we distill only to the global token, only to patch tokens, or neither. For settings, we use DINOv3 ViT-B teacher and train for 5 In general, the impact of the ImageNet22k epochs only. 7 Table 8. Distillation token selection. We show the effect of distilling for the global (cls) token only, patch tokens only, and both in terms of main token classificiation (d=768), tiny token classification (d=32), and reconstruction (PSNR and SSIM). Table 10. Distillation design. We show the impact of increasing the tiny token size to match the original tokens, chaning the INR hidden dimension from 256 to 512, changing the number of INR layers from 4 to 3 or 5, reducing from 4 transformer decoder layers to 1, or removing the attention from the transformer decoder. Block Selection Classification Segmentation Reconstruction Global Patch d=768 d=32 mIoU, d=768 PSNR SSIM 11.4 82.9 81.9 83.2 1.9 71.3 65.5 69.3 4.18 39.29 45.23 43. 28.42 26.42 26.95 25.33 0.7973 0.7468 0.7580 0.7133 Table 9. Distillation block selection. We ablate selection of which block output we choose for the distillation. We try 2 blocks each for the Encoder and the Decoder and report performance of the standard token (d=768) and tiny token (d=32) for ImageNet linear probing classification, and of the tiny token for reconstruction (PSNR, SSIM). Recall that the encoder and decoder have 12 and 4 blocks, respectively."
        },
        {
            "title": "Encoder Decoder",
            "content": "d=768 d="
        },
        {
            "title": "SSIM",
            "content": "11 11 12 12 1 4 1 4 82.5 82.9 83.2 83.1 69.7 72.6 68.8 69.4 26.06 25.40 26.18 25.36 0.7370 0.7143 0.7454 0. distillation target tokens on patch performance makes sense. Distillation to the global token is helpful for classification, and distillation to patches is helpful for segmentation. Interestingly, the overall = 768 classification is best when we distill to all tokens. However, the reconstruction suffers substantially. We ultimately choose this setting anyway, and counterbalance the falling reconstruction metrics with longer training and distillation from larger teacher model. We can select any combination of blocks for distillation, and we perform limited investigation in Table 9, where we train for 5 ImageNet22k epochs, using DINOv3 ViTB teacher. The best setting for reconstruction and standard classification (12, 1) is the worst for tiny token classification. Distilling to (11, 4) leads to better TinTok classification, and (12, 1) yields the best reconstruction. However, we ultimately opt to distill to the last block of each network (12, 4), both as good middle ground and for the sake of simplicity. We finally examine certain decoder design choices in Table 10. We show how some choices that would increase computational cost, like increasing the INR hidden dimension from 256 to 512, actually hurt results. Increasing the token dimension in the decoder helps the reconstruction metrics, but at the expense of both speed and TinTok classification. The most significant decision involves removing Classification Segmentation Reconstruction Setting d= d=32 mIoU, d=768 PSNR SSIM Default Tiny dim=768 INR w/ dim=512 INR w/ 3 layers INR w/ 5 layers Trans. w/ 1 layer Trans. w/o att. 83.1 83.1 82.3 83.1 83.1 83.1 83.1 69.7 69.1 70.8 69.6 70.2 69.6 73. 43.85 44.00 23.36 43.36 44.44 43.65 43.83 25.35 25.65 25.30 25.08 25.48 24.78 24.90 0.7135 0.7243 0.7119 0.7015 0.7188 0.6962 0.6943 attention from the hyper-network decoder. This converts the decoder from transformer, to multi-layer perceptron with residual connections. This improves the TinTok classification, without affecting the standard-sized token classification or segmentation. However, it has significant negative impact on the reconstruction performance. In spite of this, for our TinTok experiments (such as in Table 1), we opt to use this setting, and we mainly to counter-balance the negative effect with more training time. 4.8. Limitations Our pre-training does not operate at the same scale or scope as prior image representation methods. Compared to SigLIP 2, we train with less data for less time. Compared to DINOv3, we do not train with specialized curated data. Compared to Radio, we focus more on the unified modeling aspect, and do not engineer distillation from multiple models. Due to this, while HUVR and TinyTokens have the best performance for many benchmarks, and are the only method with native reconstruction capability, there are some datasets and metrics for which other methods are superior. Potential application to tasks with Vision Language Models (VLMs) would require text-aligned pre-training, and could be future work. 5. Conclusion We propose HUVR, an INR hyper-network for unified universal image representation. Not only does HUVR compare favorably with prior works for unsupervised learning for image recognition, it also yields compressed TinToks which support recognition and reconstruction. We demonstrate the utility of the method for various datasets and embeddings sizes for tasks including classification, segmentation, and generation. We hope that this work enables further exploration in the areas of unified representation learning, compressed representations, and implicit neural representation as viable strategy for universal vision encoding."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning, 2022. 2 [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. 17 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 3 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers, 2022. 2 [5] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale biascontrolled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. [6] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. ArXiv, abs/2105.04906, 2021. 2 [7] Lucas Beyer, Olivier J. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet?, 2020. 5 [8] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 5 [9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuperIn Proceedings vised learning of visual features. of the European Conference on Computer Vision (ECCV), pages 132149, 2018. 2 [10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:99129924, 2020. [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision In Proceedings of the International transformers. Conference on Computer Vision (ICCV), 2021. 1, 2 [12] Hao Chen, Bo He, Hanyu Wang, Yixuan Ren, Ser Nam Lim, and Abhinav Shrivastava. Nerv: Neural representations for videos. Advances in Neural Information Processing Systems, 34:2155721568, 2021. 2, 17 9 [13] Hao Chen, Matt Gwilliam, Bo He, Ser-Nam Lim, and Abhinav Shrivastava. Cnerv: Content-adaptive neural representation for visual data, 2022. 2 [14] Hao Chen, Matthew Gwilliam, Ser-Nam Lim, and Hnerv: hybrid neural Abhinav Shrivastava. In Proceedings of the representation for videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1027010279, 2023. 2 [15] Hao Chen, Saining Xie, Ser-Nam Lim, and Abhinav Shrivastava. Fast encoding and decoding for implicit video representation, 2024. 2, 4 [16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597 1607. PMLR, 2020. 1, [17] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big selfsupervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:2224322255, 2020. [18] Xinlei Chen and Kaiming He. Exploring simple In Proceedings of siamese representation learning. the IEEE/CVF conference on computer vision and pattern recognition, pages 1575015758, 2021. 2 [19] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016. 3 [20] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. CoRR, abs/2003.04297, 2020. 2 [21] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision arXiv preprint arXiv:2104.02057, transformers. 2021. 2 [22] Yinbo Chen and Xiaolong Wang. Transformers as meta-learners for implicit neural representations, 2022. 2, 3, 4, 7, 15, [23] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5 [24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarIn 2009 IEEE Conference chical image database. on Computer Vision and Pattern Recognition, pages 248255, 2009. 2, 5 [25] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. Advances in neural information processing systems, 32, 2019. 1, 3 [26] Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016. 3 [27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers arXiv preprint for image recognition at scale. arXiv:2010.11929, 2020. 2, 4 [28] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016. 3 [29] Emilien Dupont, Adam Golinski, Milad Alizadeh, Yee Whye Teh, and Arnaud Doucet. Coin: Compression with implicit neural representations. arXiv preprint arXiv:2103.03123, 2021. [30] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and Arnaud Doucet. Coin++: Data agnostic neural compression. arXiv preprint arXiv:2201.12904, 1(2):4, 2022. 2 [31] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2179521806, 2024. 5 [32] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 4, 6 [33] Sharath Girish, Abhinav Shrivastava, and Kamal Gupta. Shacira: Scalable hash-grid compression for implicit neural representations, 2023. 2 [34] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression, 2016. 2 [35] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent: new approach to self-supervised learning. CoRR, abs/2006.07733, 2020. 2 [36] Matthew Gwilliam, Roy Zhang, Namitha Padmanabhan, Hongyang Du, and Abhinav Shrivastava. How to design and train your implicit neural representation for video compression. arXiv preprint arXiv:2506.24127, 2025. 2 [37] Kilichbek Haydarov, Aashiq Muhamed, Xiaoqian Shen, Jovana Lazarevic, Ivan Skorokhodov, Chamuditha Jayanga Galappaththige, and Mohamed Elhoseiny. Adversarial text to continuous image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 63166326, 2024. 2 [38] Bo He, Xitong Yang, Hanyu Wang, Zuxuan Wu, Hao Chen, Shuaiyi Huang, Yixuan Ren, Ser-Nam Lim, and Abhinav Shrivastava. Towards scalable neural In Proceedings representation for diverse videos. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61326142, 2023. 2 [39] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. 2 [40] Greg Heinrich, Mike Ranzinger, Hongxu, Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Radiov2.5: Improved baselines for agglomerative vision foundation models, 2024. 3, [41] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 2 [42] Jeremy Howard. Imagenette. 5 [43] Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners, 2022. 2 [44] Chiheon Kim, Doyup Lee, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Generalizable implicit neural representations via instance pattern composers. arXiv preprint arXiv:2211.13223, 2022. 2, 7, 15 [45] Hyunjik Kim, Matthias Bauer, Lucas Theis, Jonathan Richard Schwarz, and Emilien Dupont. C3: High-performance and low-complexity neural compression from single image or video, 2023. 2 [46] Jina Kim, Jihoo Lee, and Je-Won Kang. SNeRV: Spectra-Preserving Neural Representation for Video, page 332348. Springer Nature Switzerland, 2024. 2 10 [47] Subin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin. Scalable neural video representations with learnable positional features. In Advances in Neural Information Processing Systems, 2022. [48] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 5 [49] Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, and David Bull. Hinerv: Video compression with hierarchical encoding-based neural representation. In Advances in Neural Information Processing Systems, pages 7269272704. Curran Associates, Inc., 2023. 2 [50] Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, and David Bull. Nvrc: Neural video representation compression, 2024. 2 [51] Theo Ladune, Pierrick Philippe, Felix Henry, Gordon Clare, and Thomas Leguay. Cool-chic: Coordinatebased low complexity hierarchical image codec, 2023. 2 [52] Doyup Lee, Chiheon Kim, Minsu Cho, and WookShin Han. Locality-aware generalizable implicit neural representation, 2023. 2, 7, 17 [53] Joo Chan Lee, Daniel Rho, Jong Hwan Ko, and Eunbyung Park. Ffnerv: Flow-guided frame-wise neural representations for videos. In Proceedings of the 31st ACM International Conference on Multimedia, page 78597870. ACM, 2023. [54] Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive visual representations, 2021. 2 [55] Alexander Cong Li, Mihir Prabhudesai, Shivam Duggal, Ellis Langham Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023. 3 [56] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Efficient self-supervised vision transformers for representation learning, 2022. 2 [57] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis, 2022. 1, 3 [58] Zizhang Li, Mengmeng Wang, Huaijin Pi, Kechun Xu, Jianbiao Mei, and Yong Liu. E-nerv: Expedite neural video representation with disentangled spatial-temporal context, 2022. 2 [59] Dong Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1):503528, 1989. [60] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916, 2023. 3 [61] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tunIn Proceedings of the IEEE/CVF conference ing. on computer vision and pattern recognition, pages 2629626306, 2024. 3 [62] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 5 [63] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 17 [64] Shishira Maiya, Sharath Girish, Max Ehrlich, Hanyu Wang, Kwot Sin Lee, Patrick Poirson, Pengxiang Wu, Chen Wang, and Abhinav Shrivastava. Nirvana: Neural implicit representations of videos with adaptive networks and autoregressive patchIn Proceedings of the IEEE/CVF wise modeling. Conference on Computer Vision and Pattern Recognition, pages 1437814387, 2023. 2, [65] Shishira Maiya, Anubhav Gupta, Matthew Gwilliam, Max Ehrlich, and Abhinav Shrivastava. Latent-inr: flexible framework for implicit representations of videos with discriminative semantics. In European Conference on Computer Vision, pages 285302. Springer, 2024. 2 [66] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. 2 [67] Ishan Misra and Laurens van der Maaten. Selfsupervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67076717, 2020. 1, 2 [68] Soumik Mukhopadhyay, Matthew Gwilliam, Yosuke Yamaguchi, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Tianyi Zhou, Jun Ohya, and Abhinav Shrivastava. Do text-free diffusion models learn discriminative visual representations? In European Conference on Computer Vision, pages 253 272. Springer, 2024. 1, 3 [69] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 41(4):115, 2022. 2 [70] Pushmeet Kohli Nathan Silberman, Derek Hoiem Indoor segmentation and support and Rob Fergus. inference from rgbd images. In ECCV, 2012. 5 11 [71] Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney, Ankit Patel, and Anima Anandkumar. Semi-supervised stylegan for disentanglement learning. In Proceedings of the 37th International Conference on Machine Learning, pages 73607369, 2020. 3 [72] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. 5 [73] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 6984. Springer, 2016. 2 [74] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 1, 2 [75] Bo Pang, Yifan Zhang, Yaoyi Li, Jia Cai, and Cewu Lu. Unsupervised visual representation learning by synchronous momentum grouping, 2022. 2 [76] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 25362544, 2016. 2 [77] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 5 [78] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, [79] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1249012500, 2024. 3, 5 [80] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. 2, 5 12 [81] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large International Scale Visual Recognition Challenge. Journal of Computer Vision (IJCV), 115(3):211252, 2015. 4 [82] Jens Eirik Saethre, Roberto Azevedo, and Christopher Schroers. Combining frame and gop embedIn Proceeddings for neural video representation. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9253 9263, 2024. 2 [83] Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, and Richard G. Baraniuk. Wire: Wavelet implicit neural representations, 2023. 2 [84] Jurgen Schmidhuber. computer scientists view of In Foundations life, the universe, and everything. of computer science: Potentialtheorycognition, pages 201208. Springer, 2006. 2 [85] Noam Shazeer. Glu variants improve transformer, 2020. 16 [86] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient subpixel convolutional neural network, 2016. 2, 17 [87] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael arXiv preprint Ramamonjisoa, et al. Dinov3. arXiv:2508.10104, 2025. 2, 4, 5 [88] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020. 2, 15 [89] Ivan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. Adversarial generation of continuous images, 2021. 2 [90] Ray Solomonoff. formal theory of inductive inference. part i. Information and control, 7(1):122, 1964. 2 [91] Yannick Strumpler, Janis Postels, Ren Yang, Luc van Gool, and Federico Tombari. Implicit neural representations for image compression, 2022. 2 [92] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [93] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2024. tor quantized neural representation for videos, 2024. 2 [94] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020. 2, 15 [95] Nenad Tomasev, Ioana Bica, Brian McWilliams, Lars Buesing, Razvan Pascanu, Charles Blundell, and Jovana Mitrovic. Pushing the limits of self-supervised resnets: Can we outperform supervised learning without labels on imagenet?, 2022. 2 [96] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. 5 [97] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 5 [98] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at arXiv preprint arXiv:2409.12191, any resolution. 2024. [99] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from IEEE transerror visibility to structural similarity. actions on image processing, 13(4):600612, 2004. 4 [100] Chang Wu, Guancheng Quan, Gang He, Xin-Quan Lai, Yunsong Li, Wenxin Yu, Xianmeng Lin, and Cheng Yang. Qs-nerv: Real-time quality-scalable decoding with neural representation for videos. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 25842592, 2024. 2 [101] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are In Proceedings of unified self-supervised learners. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1580215812, 2023. 3 [102] Dejia Xu, Peihao Wang, Yifan Jiang, Zhiwen Fan, and Zhangyang Wang. Signal processing for implicit neural representations, 2022. 2 [103] Yunjie Xu, Xiang Feng, Feiwei Qin, Ruiquan Ge, Yong Peng, and Changmiao Wang. Vq-nerv: vec- [104] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 3 [105] Hao Yan, Zhihui Ke, Xiaobo Zhou, Tie Qiu, Xidong Shi, and Dadong Jiang. Ds-nerv: Implicit neural video representation with decomposed static and dynamic codes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2301923029, 2024. [106] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of large-scale image dataset using arXiv deep learning with humans in the loop. preprint arXiv:1506.03365, 2015. 5 [107] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks, 2022. 2 [108] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In Proceedings of the 38th International Conference on Machine Learning, pages 1231012320. PMLR, 2021. 2 [109] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language imIn Proceedings of the IEEE/CVF age pre-training. international conference on computer vision, pages 1197511986, 2023. 3 [110] Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. In European conference on computer vision, pages 649666. Springer, 2016. 2 [111] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [112] Shuyi Zhang, Ke Liu, Jingjun Gu, Xiaoxu Cai, Zhihua Wang, Jiajun Bu, and Haishuai Wang. Attention beats linear for fast implicit neural representaIn European Conference on Comtion generation. puter Vision, pages 118. Springer, 2024. 2, 7, 17 [113] Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang, Hongwei Qin, and Jun Zhang. Boosting neural representations for videos In Proceedings of the with conditional decoder. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 25562566, 2024. 2 13 [114] Yunfan Zhang, Ties van Rozendaal, Johann Brehmer, Implicit neural Markus Nagel, and Taco Cohen. video compression, 2021. 2 [115] Qi Zhao, M. Salman Asif, and Zhan Ma. Dnerv: Modeling inherent dynamics via difference neural In Proceedings of the representation for videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20312040, 2023. [116] Qi Zhao, M. Salman Asif, and Zhan Ma. Pnerv: Enhancing spatial consistency via pyramidal neuIn Proceedings of ral representation for videos. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1910319112, 2024. 2 [117] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders, 2025. 1, 6, 18 [118] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 2, 5 [119] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer, 2022. 2 [120] Pan Zhou, Yichen Zhou, Chenyang Si, Weihao Yu, Teck Khim Ng, and Shuicheng Yan. Mugs: multigranular self-supervised learning framework, 2022."
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implicit Neural Representation and Hypernetwork Tutorial 6.1. Image INR Basics Recall from Section 3.1 that an INR is function fθ : for coordinates and signal (image) I. In practice, this INR might be multi-layer perceptron, with feedforward layers separated by some non-linearity, such as ReLU. In addition, INRs perform best when they do not operate directly on coordinate inputs. Instead, the coordinate tuples (x, y) are first encoded with some sinusoidal or Fourier transformation [88, 94]. So, for some image with width = 1024 and = 1024, we might represent the image with an MLP with = 5 layers, with positional encoding dimension dpos = 128, hidden dimension dhid = 256, and output dimension doutput = 3 (the number of color channels). When training an INR, we use paradigm referred to as internal learning. Unlike typical machine learning, there is no distinction between training and testing data, and instead the goal is to overfit fθ to during training, such that fθ memorizes and can perfectly reconstruct it during inference. This is why it is called implicit representation is totally stored in θ. To train such network, we perform stochastic gradient descent with all pixel coordinates as inputs and all pixel color values as outputs. After iterating over all pixels multiple times (sometimes hundreds of times, depending on the complexity of the signal), an INR of sufficient size can achieve visually lossless reconstruction. Sometimes these networks are trained on 3D scenes (with many images from different viewpoints), and in such cases, they are expected to perform novel view synthesis (generate an image from an unseen viewpoint). In the image-specific regime, where each network represents single image (and is trained on that image, only), INRs can also perform some generative tasks. These include superresolution, where we can provide interpolated coordinates to the network as input, and it will give the corresponding color prediction as output. Following the same approach, we can give coordinates outside the training range, and the network will perform outpainting. 6.2. Hyper-networks As we mention previously, the main problem with these networks is they require extensive training time. While some approaches try to accelerate the training, hyper-networks try avoid the per-network training entirely. Instead, hypernetworks rely on pre-training stage to train network : fθ. While we already explain this to some extent in Section 3.1, we provide even more details here. The most counterintuitive aspect of hyper-networks is that they do not actually predict the entire implicit neural network at inference time. Instead, the actual INR is defined at the same time as the hyper-network, and the weights are learned during the hyper-network pre-training. However, since this INR is fit on the entire dataset, it actually does not represent an image. If one were to perform inference directly with that INR, they would get an image with noise resembling sort of dataset average image. Considering that we train the base INR weights, θb, on the entire dataset, the role of is obviously not to predict the entire INR weights, θ. Instead, simply needs to use input to modulate the weights of θb such that inference for with the resulting θ yields the actual input image I. In the original Trans-INR [22], predicts unique matrix to modify each layer of θb. However, IPC [44] shows that it is sufficient to modulate only the second layer, and since we adopt that practice in this paper, we will explain the process from the perspective that we only modulate single layer. In other words, if we define an INR with = 5, we only modulate W2, and θ uses W1, W3, W4, and W5 directly from θb. The modulation for θ proceeds as we explain in Section 3.1, with the second layer given by W2 M2. 7. Ablations 7.1. Distillation Teacher Size Crossover Point When we train the model with larger teacher, it requires more epochs before it outperforms the smaller teacher. However, given sufficiently long training time, larger teachers are superior. We illustrate this crossover effect for our ViT-B/16 in Table 11. We use the attention-free decoder for this experiment (hence the numbers do not exactly match with tables in the main paper). Note that the crossover effect impacts standard-size tokens (d = 768) more than TinToks (d = 32). The reconstruction and classification of the TinToks is better using large teachers, regardless of the number of training epochs. However, the standard tokens require many training epochs to justify using the DINOv3 ViT-L teacher instead of the DINOv3 ViT-B teacher (up to 40 epochs for classification, 10 epochs for segmentation). 7.2. Distillation Loss Weights We perform an ablation for different distillation loss weights (Equation 1) in Table 12. For this experiment, we train for only 5 ImageNet22k epochs. Evaluations follow the same linear probing and reconstruction settings as in our 15 Table 11. Distillation Teacher Size Crossover Effect. When distilling from DINOv3, training with teacher larger than the student initially yields poorer results than training with similar-size teacher. However, after training for enough time, training with the larger teacher is superior. For each metric, we highlight the value for the earliest epoch where the ViT-L teacher gives better result than the ViT-B at the same epoch. We give TinTok results (d = 32) for ImageNet only, but mostly focus on standard tokens. Settings Classification Segment. (ADE20K) Recon. (INet) Teacher Epochs INet (d = 32) INet ReaL ONet Cars CUB DTD Flowers Food mIoU mAcc PSNR SSIM ViT-B ViT-L 10 20 30 40 50 10 20 30 40 50 70.6 71.4 71.6 72.5 72.8 70.6 72.8 74.5 75.9 76.6 82.5 83.2 83.6 84.0 84. 80.3 82.1 83.1 84.1 84.6 87.7 88.2 88.3 88.5 88.6 86.2 87.4 88.0 88.7 88.9 53.3 54.6 56.0 57.8 58.3 47.3 51.4 55.2 58.9 60.8 92.8 92.9 93.1 93.3 93. 91.1 91.9 92.4 92.9 93.1 88.3 88.3 88.9 89.3 89.3 87.3 88.1 88.9 89.2 89.3 82.6 83.6 84.1 84.3 84.9 80.8 82.6 82.8 84.8 84.7 99.7 99.6 99.6 99.7 99. 99.5 99.6 99.7 99.8 99.7 91.9 92.4 92.7 93.2 93.2 90.4 91.6 92.7 93.7 94.2 45.74 46.11 46.85 46.87 47.17 44.82 46.33 47.35 48.41 48.71 58.28 58.52 59.44 58.69 58. 56.74 59.06 58.86 60.04 60.51 25.08 25.18 25.22 25.43 25.50 25.65 25.70 25.68 25.95 26.02 0.7001 0.7027 0.7036 0.7086 0.7108 0.7173 0.7194 0.7189 0.7250 0.7275 Table 12. Distillation loss weights. We show the effect of different distillation weights in terms of main token classification (d=768) and tiny token classification (d=32) on ImageNet, main token segmentation (d=768) on ADE20K, and reconstruction (PSNR) on ImageNet. We first show our main setting, then demonstrate the impact of independently increasing the decoder weights (αdec), the global token weights (αg), and the patch token weights (αp). Loss Weights Classification Segmentation Recon. αg,enc αp,enc αg,dec αp,dec d= d=32 mIoU (d=768) PSNR 2.0 1.0 2.0 4.0 2.0 2.0 1.0 2.0 2.0 4.0 0.5 0.25 1.0 1.0 0.5 0.5 0.25 1.0 0.5 1. 83.0 82.8 83.0 82.9 83.2 68.2 69.2 68.7 68.2 67.9 43.68 43.97 43.42 42.83 44.97 25.84 26.75 25.19 25.17 25.49 Table 13. Reconstruction objective. We train our system with no reconstruction loss (no INR prediction, and no image prediction), compared to our standard system, for 5 epochs on ImageNet22k. The network without INR prediction and associated loss is fundamentally incapable of reconstruction. Classification Recon. INR d=768 d=32 PSNR 83.3 83. 64.2 68.2 - 25.84 other experiments. 7.3. Hyper-network Helps Classification We can train HUVR without the loss for INR prediction and image reconstruction. The resulting model cannot reconstruct images, so its embeddings and TinToks cannot be used for image compression or generation. It does not learn unified representation. Additionally, as we show in Table 14. Encoder architectures. We show settings for our ViTB/16 and ViT-L/16. Backbone ViT-B/16 ViT-L/16 # Parameters # Blocks Position Embedding Register Tokens Token Dimension Feedforward Dimension Feedforward Type Attention Heads Attention Head Dimension 85.9M 12 RoPE None 768 304M 24 RoPE None 1024 3072 SwiGLU SwiGLU [85] 16 48 16 64 Table 13, the TinToks classification is significantly worse without the image reconstruction objective. This shows that the hyper-network prediction and image reconstruction are synergistic with the recognition task. 8. Detailed Settings 8.1. Main Experiments Our ViT-B and ViT-L have 85.9M and 304M parameters, respectively. We show full settings for our ViT-B and ViT-L in Table 14. In places where we change the design compared to prior works, we make sure to keep total parameters equivalent (for example, our ViT-B has fewer than 86M parameters). Our decoder parameters and designs vary depending on the experiment settings, but we show decoder settings for our main experiments in Table 15. Note that these are worst case sizes, where we exchange the increased parameter count for marginal improvement to reconstruction performance (see Table 10. Reducing from 4 layers to 1 (4 fewer parameters) does not incur penalty for classification or segmentation. Similarly, recall that even with Table 15. Decoder architectures. We show settings for sample decoders we can use with our ViT-B and ViT-L. For some ablations, we instead use token dimension 512 for the sake of speed. Backbone ViT-B/16 ViT-L/ # Parameters # Blocks Position Embedding Token Dimension Feedforward Dimension Attention Heads Attention Head Dimension 50.3M 4 Sinusoidal 1024 4096 16 64 68.2M 4 Sinusoidal 1280 4096 16 80 Table 16. INR architectures. We show settings for the base patchwise INR fθb ."
        },
        {
            "title": "Settings",
            "content": "# Parameters Position Embedding Embedding Dimension # MLP Layers MLP Dimension # Conv layers Kernel Size Padding Stride Upsample Activation 275k Sinusoidal 128 3 256 1 3 1 1 PixelShuffle [86] ReLU single layer decoder, we can have good reconstruction performance 5. Therefore, we hypothesize that future work that focuses on efficiency can reduce the parameter count and improve both training and inference efficiency. We also give settings for our INR with learnable weights θb in Table 16. Note that we mention convolutional layer and PixelShuffle. This is because we do not use pure MLP INR, nor do we take in every coordinate at inference. Instead, following intuitions from prior INR works [12, 64], we use strided coordinates (stride = 4) as inputs to fθ to reduce the computation by 16. We use the learnable convolutional layer with PixelShuffle to upsample from (H/4)(W/4) features to the HW image. We use base learning rate (lr) of 0.0005 which we rescale by multiplying lr = lrbase B/256 for some batch size, B. In our standard setting we train for 50 epochs (or the equivalent number of iterations, based on 13.7M training images in ImageNet22k). We use the AdamW [63] optimizer, cosine annealing learning rate after linear warm-up for 5 epochs, and we clip gradients with norm of 0.01. For data augmentations, we take random resized crop, with minimum rescale ratio of 0.2. We then resize to our input resolution, 256256 for pretraining. For our model, we use normalization with means (0.5, 0.5, 0.5) and standard deviations (0.5, 0.5, 0.5). When interacting with other models, either as teachers or when performing evaluations with them, we use their original image normalization settings. Aside from our encoder designing and training, we also have some other settings for the components of our pipeline that predict the INR, and for the INR itself. First, we have downsample layer that consists of LayerNorm [2] followed by fully-connected layer which projects from dViT to dt. Our upsample layer is the same but projects from dt to ddec where ddec is the token dimension of the transformer decoder that we use to predict matrices to modulate the INR. We also need learnable modules to project from ddec to din and these also have learnable LayerNorms. We learn two such layers, one for the global token, and another for the patch tokens. We also have layers to project between our tokens and the teacher representation space, which also feature single LayerNorm followed by linear layer to project from dViT or ddec to dteacher. For our ViT-B hyper-network, the ViT encoder itself has 85.9M parameters, the learnable θb has 275k parameters, and the remaining components (distillation projection layers, upsample/downsample, projection from decoder to INR) have 3.16M learnable parameters (of which only 305k are used at inference). 8.2. INR Hyper-network Comparisons When we indicate the training epochs in Table 5, we give the epochs for training on ImageNette/LSUN/CelebA, respectively. For fractional epochs, those papers train for set number of iterations rather than epochs, and we convert to epochs for comparison. We only train using whole-valued epochs, so we train for 12 instead of 12.67 epochs on LSUN Churches. The numbers in Table 5 come from multiple sources. For TransINR, we reproduce the results using the model settings from the original paper [22] and the indicated number of epochs, AdamW optimizer, batch size of 16, and learning rate of 0.0001. For IPC and LA-IPC we use the numbers from LA-IPC [52]. For INR, we use the number from ANR [112]. For ours, we use the indicated number of epochs and the same optimizer, batch size, and learning rate as with our baseline TransINR reproduction. We make the settings fair. With hypernetworks, there are several factors to consider, mainly (i) the size of the encoder E, (ii) the size of the learnable base INR weights θb, and (iii) the number of unique parameters, which is the smallest size of the portion of the weight tokens that is necessary to modulate θb to produce θ. The explain the latter with an example, consider an encoder that predicts 100 tokens with = 768, but only 50 are used to modulate the INR weights. In this case, the size of the unique parameters would be 38, 400 = 50 768. However, note that we said the smallest size. If the encoder, E, predicts the tokens at = 768, but these are downsampled to smaller dimension, such as = 32, before the modulation, then we would say the size of the unique parameters is 1600 = 50 32. Transformer Encoder Parameters. The prior works use the same design for the transformer encoder, and we match this design with one exception. Since our model has decoder, we must reduce the size of the encoder to maintain fairness. We accomplish this by removing single layer from E, and our decoder for the hyper-network comparisons consists of single layer with the exact same settings as (aside from the number of layers, which is 1). So, the encoders are all the same size, except for when we downsample for TinTok compression. In these cases, our model adds an extra 393k parameters for the learnable downsample from = 768 to = 256 and learnable upsample from = 256 to = 768. Base INR Parameters. Our θb consists of 230k learnable parameters, for all 3 datasets. We use this same exact design for our TransINR reproduction, and IPC and LA-IPC use these same settings for ImageNette and CelebA. ANRs learnable INR consists of 2757k parameters for LSUN. Our patch-wise INR is significantly smaller than the image-wise INRs, even though these ultimately represent the exact same data. However, our patch-wise INR formulation can result in less efficiency during forward computation, since our modulation procedure will yield unique INRs, where is the number of image patches (256 for LSUN, 400 for ImageNette/CelebA). This has slightly higher time costs and larger memory footprint, but due to our TinTok compression, our storage size is smaller (see Unique Parameters, next paragraph). Unique Parameters. ANR predicts 131k unique parameters to modulate their INRs for LSUN, while ours, IPC, and LA-IPC predict 65.5k (half as many). TransINR predicts 58.1k unique parameters, but uses portions of these to modulate each layer separately, compared to ours, IPC, and LA-IPC which modulate only the second layer. 9. Diffusion with HUVR We show visual diffusion examples in Figure 4. These initial results, combined with our comparisons to the VAE reconstruction  (Table 1)  and VAE-based DiT generation  (Table 2)  , are meant to show the potential of our method for generation. However, we acknowledge these results are significantly worse than the current generative state-of-thearts. In most respects, this is because we hypothesize we have optimized neither the diffusion model nor the diffusion process for our architecture. For example, we have both patch tokens and global token, and the global token is responsible for combining with the patch token to modulate every single patch INR. In spite of this, our implementation of 18 Figure 4. Generated samples with HUVR embeddings. We use DiT-XL train on HUVR embeddings with TinTok dt = 256. This HUVR is trained with LPIPS and SSIM losses in addition to the pixel-wise and DINOv3 MSE losses. Compared to our Table 2, this DiT is trained for 4500k steps instead of 400k steps. For reference, DiT-XL/2 trains their final model for 7000k steps. These results demonstrate many degradations and artifacts, but we hope they work as proof-of-concept to convey the promise of HUVR for generation. Future work could apply techniques, such as those in RAE [117], to improve the quality. DiT handles the global and patch tokens equivalently. Aside from this, there are many other strategies that could improve performance given our larger token size (we use dt = 256 here), larger patch size (16 for ours instead of 8 for VAE), and the semantic knowledge in our tokens. We leave these for future work, and hope our results here are sufficient to demonstrate this as compelling area for future research."
        }
    ],
    "affiliations": [
        "TikTok"
    ]
}