{
    "paper_title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "authors": [
        "Ganlin Yang",
        "Tianyi Zhang",
        "Haoran Hao",
        "Weiyun Wang",
        "Yibin Liu",
        "Dehui Wang",
        "Guanzhou Chen",
        "Zijian Cai",
        "Junting Chen",
        "Weijie Su",
        "Wengang Zhou",
        "Yu Qiao",
        "Jifeng Dai",
        "Jiangmiao Pang",
        "Gen Luo",
        "Wenhai Wang",
        "Yao Mu",
        "Zhi Hou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 2 0 1 1 . 0 1 5 2 : r Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning VLASER: VISION-LANGUAGE-ACTION MODEL WITH SYNERGISTIC EMBODIED REASONING Ganlin Yang1,2, Tianyi Zhang4,2, Haoran Hao5,2, Weiyun Wang6,2, Yibin Liu9,3, Dehui Wang3 Guanzhou Chen3,2, Zijian Cai10,3, Junting Chen8,2, Weijie Su2, Wengang Zhou1, Yu Qiao2 Jifeng Dai7,2, Jiangmiao Pang2, Gen Luo2, Wenhai Wang2, Yao Mu3,2, Zhi Hou2 1University of Science and Technology of China 3Shanghai Jiao Tong University 4Zhejiang University 5Nanjing University 6Fudan University 7Tsinghua University 8NUS 9Northeastern University 10Shenzhen University 2Shanghai AI Laboratory Project Page: Vlaser"
        },
        {
            "title": "ABSTRACT",
            "content": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser VisionLanguage-Action Model with synergistic embodied reasoning capability, which is foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across range of embodied reasoning benchmarksincluding spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark. The code, model and data are available at https://github.com/OpenGVLab/Vlaser/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Embodied artificial intelligence (AI) (Chrisley, 2003) aims to endow agents with the ability to perceive, understand, and act in the physical world. Achieving such intelligence requires not only accurate perception and language understanding but also embodied reasoning and effective control, which together define the paradigm of vision-language-action (VLA) models. Developing foundation models that possess strong reasoning and control capabilities is therefore an important advancement toward general-purpose embodied AI. In this context, vision-language models (VLMs) (OpenAI, 2023; Liu et al., 2023; Chen et al., 2024; Bai et al., 2025; Team et al., 2023) emerge as natural candidates to enhance embodied agents in perception generalization and reasoning ability. Following this paradigm, extensive embodied visionlanguage models (Azzolini et al., 2025; Team et al., 2025c) emerge from enhancing the key ability for an embodied agent in grounding, planning, and spatial reasoning. Meanwhile, significant body of work extends vision-language models (VLMs) into vision-language-action models (VLAs) (Kim et al., 2024; Intelligence et al., 2025; Driess et al., 2025) for robot control. While there are some approaches (Intelligence et al., 2025; Driess et al., 2025) that demonstrate the effectiveness of cotraining with web data for the generalization in robot manipulation, it remains poorly understood which multi-modal data streams/abilities are most critical for improving downstream VLA models. In this paper, we aim to construct Vlaser, an embodied vision-language model that possesses strong Equal contribution. Corresponding authors. 1 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 1: Overall framework, capabilities, and evaluation of Vlaser. Top-left: Composition of the Vlaser-6M dataset, featuring multi-task embodied dataincluding QA, grounding, spatial reasoning, and planningalong with in-domain simulation-sourced pairs. Top-right: LiDAR visualization illustrating the state-of-the-art embodied reasoning capability of the Vlaser VLM. Bottom-left: The pre-trained Vlaser VLM significantly accelerates convergence in downstream Vision-Language Action model (VLA) policy learning on WidowX platform (Walke et al., 2023a). Bottom-right: Successful closed-loop operation of an agent powered by Vlaser within the SimplerEnv benchmark (Li et al., 2024b). embodied reasoning capabilities, and subsequently answer this question based on the corresponding vision-language-action models. Despite advancements in vision-language models (Chen et al., 2024; Bai et al., 2025), the capabilities of operating as an embodied agent remain severely constrained. In particular, navigation and traditional manipulation approaches rely heavily on planning-based control (Huang et al., 2022; Gasparetto et al., 2015; Zhang et al., 2018), which requires strong foundational ability in grounding and planning. Planning and Grounding are cornerstones of the agents embodied in the physical world. Meanwhile, spatial understanding increasingly attracts the interest of the community in addressing the spatial perception ability of VLM. To this end, we firstly aim to introduce an embodied visionlanguage model specifically enhanced for the aboved embodied reasoning capabilities. Specifically, we construct the Vlaser data engine, which enables the systematic construction of the Vlaser-6M dataset by curating, reorganizing, and annotating public datasets from the Internet. As illustrated in Figure 1, the resulting dataset spans wide spectrum of embodied reasoning tasksincluding general embodied QA, visual grounding, spatial intelligence, task planning, and in-domain simulation data. Leveraging this comprehensive data foundation, Vlaser achieves state-of-the-art performance across variety of embodied reasoning benchmarks, demonstrating strong generalization in both open-loop inference and closed-loop control settings. Existing Vision-Language-Action (VLA) models (Black et al., 2024; Cheng et al., 2024; Kim et al., 2024; Intelligence et al., 2025) typically fine-tune pre-trained Vision-Language Models (VLMs) for robot control. However, the selection of an optimal VLM backbone one that accelerates convergence and improves success rates when used as initialization for end-to-end VLA policy learning, remains an under-explored research problem. To address this gap, we systematically investigate the VLM-toVLA adaptation paradigm using our enhanced embodied vision-language model and associated data engine. Our experiments reveal an important insight: although out-of-domain embodied reasoning 2 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 2: An illustration of Vlaser architecture. Vlaser includes two components and corresponding training phases: 1) the Multimodal Pretraining is for embodied reasoning enhancement based on the corresponding data engine; 2) VLA training is performed on the action expert module, which handles low-level control based on flow matching action generation. data significantly improve upstream reasoning capabilities as measured by standard benchmarks, these gains may not translate directly or prominently to downstream VLA performance. In contrast, indomain data annotated directly on robot interaction datasets such as Open X-Embodiment (ONeill et al., 2024) proves substantially more effective in accelerating convergence and increasing task success rates during VLA fine-tuning. We believe this observation provides significant insights for future embodied vision-language model construction: It is urgent to shrink the domain gap between current embodied perception and reasoning benchmarks to the real-world embodied tasks, and thus facilitate the closed-loop evaluation for the corresponding robot embodiment. In summary, the principal contributions of Vlaser are as follows. An open-source embodied vision-language model and dataset. We introduce Vlaser, an adaptable vision-language model that enhances InternVL with embodied reasoning capabilities and end-to-end robot control. The full model weights, modular data generation pipeline, training and evaluation code, and the accompanying Vlaser-6M dataset will be made publicly available to support reproducibility and future research. Systematic analysis of data effectiveness for VLA transfer. We conduct thorough investigation into which types of vision-language pretraining data contribute most effectively to downstream Vision-Language-Action (VLA) policy learning. Our findings offer practical insights for constructing task-aware data streams that bridge the gap between Internet-scale pretraining and embodied-specific fine-tuning. State-of-the-art performance across embodied benchmarks. Among models of comparable scale, Vlaser achieves top-tier results on comprehensive set of embodied reasoning benchmarksspanning visual grounding, task planning, spatial reasoning, and simulation-based robot evaluation, demonstrating its strong generalization and applicability to both open-loop inference and closed-loop control scenarios."
        },
        {
            "title": "2 METHOD",
            "content": "Vlaser aims to integrate embodied reasoning with end-to-end robot control for embodied agents, and identify the most crucial VLM data stream for VLA models. We first present the Vlaser structures 3 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning in Section 2.1. Then, we illustrate the data engine in Section 2.2. Section 2.3 discusses the training recipe that includes embodied reasoning pretraining and vision-language-action finetuning."
        },
        {
            "title": "2.1 MODEL STRUCTURE",
            "content": "The structure of Vlaser consists of two major components: the typical vision-language backbone (Chen et al., 2024; Liu et al., 2023) and the action expert for low-level control, as shown in Figure 2. We illustrate the two components respectively in this section. VLM Backbone Vision-language models (VLMs) are key candidates for embodied agents, providing both perception and reasoning abilities. Vlaser , built on InternVL3 (Zhu et al., 2025), integrates embodied reasoning with robot control for embodied agents. While InternVL3 excels in multimodal and linguistic tasks across various model sizes, Vlaser focuses on two sizes2B and 8Boptimized for the computational constraints of robots. These models utilize InternViT (Chen et al., 2024) as the vision encoder, paired with Qwen2.5-1.5B and Qwen2.5-7B LLMs (Qwen et al., 2025). Unlike typical multimodal MLLMs, Vlaser emphasizes embodied common-sense reasoning and end-to-end robot control capabilities. Action Expert There are large number of MLLMs (Team et al., 2025a; NVIDIA et al., 2025a) that enhance the ability of embodied common-sense reasoning for agents, while few approaches equip the embodied MLLMs with end-to-end robot control. Vlaser extends the MLLMs with low-level robot control and verifies the capability of different data streams in downstream VLA finetuning. Following (Intelligence et al., 2025), we design an action expert based on the opensource vision-language model (Chen et al., 2024; Zhu et al., 2025). Meanwhile, we utilize the flow matching (Lipman et al., 2023a) for action prediction based on the llava-like vision-language structure, while sharing the self-attention among the language model and action expert module. Specifically, we encode the robot state as state token and noised actions as action tokens, and input them into the action expert. Meanwhile, we utilize non-causal attention for the VLA stream. During inference, we denoise the actions based on the image observation, language instruction, as well as the current robot state. 2.2 VLASER DATA ENGINE This section outlines the composition of the Vlaser-6M data engine, cornerstone for the models embodied reasoning capabilities. Here we present the overall data scale and sources for each reasoning modality, while more details about the construction methodologies are provided in Appendix A.2. Embodied Grounding Data The Vlaser dataset incorporates two distinct 2D grounding formatsbounding boxes and center pointsboth normalized to the range [0, 1000] to ensure consistent and resolution-invariant grounding predictions across diverse image resolutions. Specifically, we collect 1.5 million high-quality question-answer pairs that support multiple grounding tasks: predicting bounding boxes from open-vocabulary descriptions, localizing object center points based on textual descriptions, and identifying objects from given spatial coordinates. The data is sourced from several open embodied grounding datasets, including RoboPoint (Yuan et al., 2024), ShareRobot (Ji et al., 2025), Pixmo-Points (Deitke et al., 2025), Paco-LaVIS (Ramanathan et al., 2023), and RefSpatial (Zhou et al., 2025a). To further enhance generalization capabilities for open-world and open-vocabulary scenarios, we also generate an additional 300k point and bounding box annotations derived from segmentation masks in the SA-1B dataset (Kirillov et al., 2023). This combination of curated human annotations and synthetically enriched data aims to bolster both the diversity and scalability of visual grounding under real-world embodied settings. General and Spatial Reasoning Data The Vlaser dataset integrates 1.2 million question-answer pairs dedicated to general Robotic Visual Question Answering (RoboVQA) tasks, along with an additional 500k data items specifically designed to enhance spatial intelligence. This comprehensive data composition substantially strengthens the models capabilities in general state perception and 3D spatial reasoning. For the RoboVQA component, data is aggregated from multiple established sources, including RoboVQA (Sermanet et al., 2024), Robo2VLM (Chen et al., 2025b), RoboPoint (Yuan et al., 2024), RefSpatial (Zhou et al., 2025a), OWMM-Agent (Chen et al., 2025a), among others. To support spatial understanding and reasoning, we incorporate datasets such as SPAR (Zhang et al., 4 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning 2025), SpaceR-151k (Ouyang et al., 2025), and VILASR (Wu et al., 2025). Furthermore, we augment these with 100k manually annotated spatial understanding samples generated from publicly available 3D scene datasetsincluding ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), CA-1M (Lazarow et al., 2025), and ARKitScenes (Baruch et al., 2021). The integration of these diverse and high-quality data sources effectively enhances the models spatial awareness and supports more robust performance in complex embodied reasoning tasks. Planning Data To tackle complex tasks, it is essential to decompose them into manageable sub-tasks and solve them step by step. This capability is commonly referred to as planning. Effective planning allows robots to combine basic skills and generalize to new scenarios. We collected 400k training data to strengthen the models planning ability, encompassing both language-based planning data and multimodal tasks. These include Alpaca-15k-Instruction (Wu et al., 2023) and MuEP (Li et al., 2024a). To further enhance environmental understanding and reasoning for complex decision-making, we incorporated training data with detailed reasoning processes from WAP (Shi et al., 2025). To improve the models ability to comprehend complex instructions and execute tasks, we followed the annotations of LLaRP (Szot et al., 2024) to initialize planning tasks in Habitat (Szot et al., 2021) and generate planning trajectories to accomplish these tasks. In addition, we integrated egocentric video datasets such as EgoPlan-IT (Chen et al., 2023) and EgoCOT (Mu et al., 2023), which closely align with the observational perspective of embodied agents and provide valuable planning examples. In-Domain Data for downstream VLAs To facilitate the end-to-end policy learning for VisionLanguage Action Models (VLAs), we further generate 2 million in-domain multimodal questionanswer pairs tailored for VLM pretraining. These data are specifically designed to align with the embodied reasoning context and enhance the models ability to perceive, reason, and plan in interactive environments. The in-domain data is sourced from simulation platforms SimplerEnv (Li et al., 2024c). Within SimplerEnv, data is generated for two distinct robotic embodiments: the Google Robot (Brohan et al., 2023b;a; ONeill et al., 2024) and the WidowX Robot (Walke et al., 2023a), ensuring broad morphological and kinematic coverage. The question-answer pairs encompass the specialized categories including embodied grounding, spatial intelligence, planning and general VQA for robot states as described above. The detailed methodology for constructing each of the in-domain data in simulation is described in Appendix A.2. 2.3 TRAINING RECIPE Vlaser adopts two-stage training recipe, designed to optimize both embodied reasoning and robot control. It includes VLM pretraining followed by VLA finetuning. In this section, we elaborate on the training recipe among all phrases. Vision-Language Pretraining Vlaser is developed by supervised fine-tuning (SFT) InternVL3 (Zhu et al., 2025) on embodied-related datasets, including those focused on grounding, planning, and spatial intelligence. In the first training phase, we fine-tune InternVL3 using auto-regressive language modeling loss. In particular, given the input images Rthw3 and textual prompt Rl, the language modeling loss Llm can be defined by Llm = log p(tN Fv(x; θv), Ft(y), t0:N 1; Θ), (1) where Rm is the next-token probability and denotes the vocabulary size. Here, Fv() denotes the ViT and the MLP, and θv is their parameters. Ft() is the textual tokenizer. Θ are the parameters of the LLM. ti denotes the i-th predicted word. Vision-Language-Action Finetuning For robot policy learning, we optimize the model using an action expert trained on robot-specific datasets. Vlaser integrates flow-matching-based action expert to predict sequence of future actions from single-frame observation. Specifically, denote the action chunk At = [at, at+1, . . . , at+H1], where at represents the action in the current timestep and represents the action horizon. Meanwhile, we encode each noisy action with an action encoder (i.e., an MLP projector) as single token for the action expert. For the action chunk, Aτ = τ At + (1 τ )ϵ is the corresponding noisy action chunk, and we train the network to output vθ(Aτ , ot) to match the denoising vector field u(Aτ At) = ϵ At, where ot indicates the observations (e.g., image camera and robot state) at action timestep t, θ represents the network and τ [0, 1] represents the flow matching timesteps. Therefore, the VLA optimization loss is as follows, Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 1: Comparison with existing close-sourced, open-sourced and embodied-related VLMs on 12 general embodied reasoning benchmarks, spanning from embodied QA, planning, embodied grounding to spatial intelligence and close-loop simulation evaluation. Avg denotes the normalized average performance of all the benchmarks. The best, second best and third best score among all the baselines are colored in red, orange and yellow. Model QA Planning Embodied Grounding Spatial Intelligence Simulation ERQA Ego-Plan2 Where2place Pointarena Paco-Lavis Pixmo-Points VSIBench RefSpatial MMSIBench VLABench EB-ALFRED EB-Habitat 47.0 35.5 55.0 34.3 31.5 35.3 36.0 37.3 35.8 Closed-source MLLMs: GPT-4o-20241120 Claude-3.7-Sonnet Gemini-2.5-Pro Small Size MLLMs: ChatVLA-2B InternVL3-2B Qwen2.5VL-3B Embodied-R1-3B RoboBrain2.0-3B Vlaser-2B Medium Size MLLMs: Magma-8B 29.3 Cosmos-Reason1-7B 39.3 38.3 VeBrain-7B 35.3 InternVL3-8B 39.3 Qwen2.5VL-7B 38.3 Embodied-R1-7B 42.0 RoboBrain2.0-7B 41.0 Vlaser-8B 41.8 41.3 42.9 25.3 30.9 30.3 36.0 41.8 38.3 27.9 26.9 27.3 40.0 29.7 37.1 33.2 53.4 29.1 25.6 39.9 3.7 5.2 31.0 35.1 64.2 74.0 10.9 11.4 33.1 10.0 31.1 69.5 63.6 69. 29.5 22.2 62.8 10.1 7.1 41.7 45.3 46.0 57.8 29.6 40.8 38.9 14.2 56.3 51.2 49.5 60.3 16.2 12.4 45.5 10.2 15.4 67.4 68.3 67.6 72.5 15.3 61.8 55.1 21.1 68.0 69.9 73.1 68. 10.8 7.2 25.8 2.1 1.4 36.6 36.6 36.9 44.6 10.1 23.6 20.1 5.7 43.5 39.2 37.8 40.5 42.5 47.0 43.4 2.4 31.5 27.9 28.0 28.8 57.5 12.7 33.9 39.9 42.1 38.2 38.6 36.1 60. 8.8 7.7 30.3 0.9 1.8 24.9 28.5 46.5 43.0 4.5 5.4 20.6 5.6 32.1 31.1 32.5 59.2 30.3 30.2 36.9 20.1 25.3 26.5 26.0 26.8 23.6 26.2 26.4 28.3 25.7 25.9 28.1 26.5 27. 39.3 41.7 34.8 0.0 19.4 31.3 24.6 18.1 23.1 8.5 35.5 25.9 24.7 36.4 35.5 6.6 45.6 56.3 67.0 62.7 0.0 1.3 6.7 7.0 0.0 42.3 0.0 4.0 5.7 19.0 10.0 10.0 14.0 50. 59.0 65.7 53.0 0.0 12.0 19.7 19.3 10.0 30.7 0.0 5.3 12.3 23.7 18.3 19.0 29.3 40.0 Lvla = vθ(Aτ , ot) u(Aτ At) Avg 34.2 33.6 44.4 9.1 15.2 31.6 32.5 35.3 45.3 14.6 26.2 28.8 22.3 35.7 38.9 37.0 51.3 (2) We sample the action chunks from the robot episodes and flow-matching timesteps to optimize the network. At inference, we generate actions by integrating the learned vector field from τ = 0 to τ = 1, starting with random noise A0 (0, I), as follows, Aτ +δ = Aτ + δvθ(Aτ , ot) (3) where δ is the integration step size. In our experiments, we set as 4, and δ as 10. We aim to identify the most effective VLMs for downstream VLA fine-tuning and bridge the gap between foundational VLMs and their performance in downstream VLA tasks, thus shedding light on the future construction of embodied VLMs. Currently, the SimplerEnv benchmark, including Bridge (Walke et al., 2023b) and Google Robot (Jang et al., 2022; Brohan et al., 2023b) datasets, provides numerous training episodes (Over 5M images) and corresponding Real-to-Sim benchmarks. We thus majorly analyze the most effective data stream for VLA finetuning based on SimplerEnv."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 PERFORMANCE ON EMBODIED REASONING CAPABILITY Evaluation Datasets We conduct comprehensive evaluation of embodied reasoning capabilities across total of 12 benchmarks, covering wide spectrum of tasks including embodied question answering, task planning, embodied grounding, spatial intelligence, and closed-loop simulation evaluation. The evaluated benchmarks consist of: ERQA (Team et al., 2025b), Ego-Plan2 (Qiu et al., 2024), Where2place (Yuan et al., 2024), Pointarena (Cheng et al., 2025), Paco-Lavis (Ramanathan et al., 2023), Pixmo-Points (Deitke et al., 2025), VSI-Bench (Yang et al., 2025b), RefSpatialBench (Zhou et al., 2025a), MMSI-Bench (Yang et al., 2025d), VLABench (Zhang et al., 2024), and EmbodiedBench (Yang et al., 2025c). For EmbodiedBench, we further assess performance in two simulation environments ALFRED (Shridhar et al., 2020) and Habitat (Szot et al., 2021). Baselines Since our method, Vlaser is trained at two model scales 2B and 8B parameters, we categorize the compared baseline methods into three groups for systematic evaluation: 1) State-ofthe-art closed-source models, including GPT-4o (OpenAI, 2025), Claude-3.7-Sonnet (Anthropic, 2025), and Gemini-2.5-Pro (Comanici et al., 2025); 2) Small-scale MLLMs (2B 3B parameters), comprising ChatVLA-2B (Zhou et al., 2025b), InternVL3-2B (Zhu et al., 2025), Qwen2.5-VL3B (Bai et al., 2025), Embodied-R1-3B (Yuan et al., 2025), and RoboBrain2.0-3B (Team et al., 6 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 2: SimplerEnv Evaluation on WidowX Robot Tasks. Avg indicates the average success rate among the four tasks. InternVL3-2B, Vlaser and Vlaser with Bridge Q&A indicates the base model we select to fine-tune on the WidowX Robot Tasks. Particularlly, Vlaser-QA refers to the model which is fine-tuned on the Bridge question-answer pairs dataset. Model sizes are indicated within parentheses. The result of RT-1-X (Brohan et al., 2023b), Octo-Base (Team et al., 2024), OpenVLA (Kim et al., 2024), RoboVLM (Liu et al., 2025) and SpatialVLA (Qu et al., 2025b) are from (Qu et al., 2025b) while the results of π0 (Black et al., 2024) is from (open-pi zero, 2025). Model RT-1-X (35M) (Brohan et al., 2023b) Octo-Base (93M) (Team et al., 2024) OpenVLA (7B) (OpenAI, 2023) RoboVLM (2B) (Liu et al., 2025) SpatialVLA (4B) (Qu et al., 2025b) π0 (3B) (Black et al., 2024) InternVL3-2B Vlaser (2B) Vlaser-QA (2B) 0% Carrot on plate Put eggplant in basket Spoon on towel Stack Cube Avg 1.1% 31.9% 16.0% 1.0% 0% 12.5 31.3% 62.5% 42.7% 21.3% 54.9% 11.3% 41.8% 20.0% 43.2% 33.3% 64.6% 0% 43.1% 4.1% 58.3% 100.0% 79.2% 57.1% 35.4% 83.3% 4.2% 8.3% 0% 25.0% 25.0% 55.8% 42.9% 60.8% 55.8% 0% 12.5% 0% 29.2% 16.7% 63.3% 55.8% 56.7% 77.9% 2025a); 3) Medium-scale MLLMs (7B 8B parameters), including Magma-8B (Yang et al., 2025a), Cosmos-Reason1-7B (NVIDIA et al., 2025a), VeBrain-7B (Luo et al., 2025), InternVL38B (Zhu et al., 2025), Qwen2.5-VL-7B (Bai et al., 2025), Embodied-R1-7B (Yuan et al., 2025), and RoboBrain2.0-7B (Team et al., 2025a). The overall experimental results are presented in Table 1. As shown in Table 1, compared to the base models InternVL3-2B and InternVL3-8B used as initialization for our supervised finetuning, our Vlaser yields substantial improvements across all embodied reasoning capabilities, with particularly notable gains in embodied grounding and simulation-based evaluation. For example, the average score increases from 15.2 to 45.3 for the 2B model, and from 22.3 to 51.3 for the 8B model. These significant performance gains underscore the high quality and effectiveness of the Vlaser-6M dataset in enhancing embodied reasoning abilities. An interesting observation emerges that when finetuning on the same Vlaser-6M dataset, smaller sized Vlaser-2B outperforms Vlaser-8B on simple point grounding tasks that require direct, short answers. Conversely, Vlaser-8B demonstrates superior performance on more complex tasks such as multi-step planning and closed-loop simulation evaluation, which often benefit from chain-of-thought (CoT) reasoning. This scaling behavior indicates the importance of appropriate model size selection based on target application requirements. including RoboWhen compared against current state-of-the-art embodied-specific VLMs, Brain2.0 (Team et al., 2025a) and Embodied-R1 (Yuan et al., 2025), our method, Vlaser still achieves superior performance on the majority of benchmarks while remaining highly competitive on the remainder, ultimately attaining the highest overall score (by +10% margin overall). These results indicate that Vlaser delivers well-balanced and robust capability set, performing strongly across multiple dimensions of embodied intelligence from embodied question answering and state estimation to future action planning, visual grounding, spatial reasoning, and closed-loop simulation. Such comprehensive competence highlights its suitability as versatile backbone for embodied AI brains. In the following section, we further examine how these enhanced reasoning capabilities, embedded within VLMs, translate into improved performance when fine-tuned for downstream Vision-Language Action models (VLAs) in simulation manipulation scenarios. 3.2 PERFORMANCE ON DOWNSTREAM CLOSE-LOOP ROBOT TASKS Finetuning Datasets We conduct extensive experiments on SimplerENV to evaluate the performance of Vlaser and Vlaser data engine on closed-loop robotic manipulation tasks. SimplerENV is an open-source suite of purpose-built simulated environments with nearly 150K episodes for evaluating real-world robot manipulation policies in scalable, reproducible way. It targets the key real-to-sim gaps control and vision so that simulated performance reliably tracks real-robot outcomes. Across Google Robot and WidowX/BridgeData V2 setups, SimplerEnv reports strong real-vs-sim correlations and faithfully reflects behavior under distribution shifts, enabling fast, comparable policy assessment without full digital twins. As result, SimplerENV has been widely adopted for evaluating VLA models and has proven to reliably reflect the performance of the models on the real robot platform. Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 3: Comparison with existing methods in SimplerEnv on Google Robot tasks. Avg indicates the average success rate among the three tasks. In the last five lines in the table, we use the name of base model to indicate different evaluation settings. Vlaser-QA indicates the model which is fine-tuned on the Fractal question-answer pair dataset. Vlaser-Spatial and Vlaser-Grounding represent the model fine-tuned on the Spatial Reasoning Data and Embodied Grounding Data separately. The details of different sub-datasets can be find in the AppendixA.2. Model sizes are indicated within parentheses. The results of TraceVLA (Zheng et al., 2024), RT-1-X (Brohan et al., 2023b), OctoBase (Team et al., 2024), OpenVLA (Kim et al., 2024), RoboVLM (Liu et al., 2025), Emma-X (Sun et al., 2024), Magma (Yang et al., 2025a), GR00T N1.5(NVIDIA et al., 2025b) and π0 (Black et al., 2024) are from (Lee et al., 2025). Model TraceVLA (7B) (Zheng et al., 2024) RT-1-X (35M) (Brohan et al., 2023b) Octo-Base (93M) (Team et al., 2024) OpenVLA (7B) (Kim et al., 2024) RoboVLM (2B) (Liu et al., 2025) Emma-X (7B) (Sun et al., 2024) Magma (8B) (Yang et al., 2025a) GR00T N1.5 (2.1B) (NVIDIA et al., 2025b) π0 (3B) (Black et al., 2024) InternVL3-2B Vlaser(2B) Vlaser-QA (2B) Vlaser-Spatial (2B) Vlaser-Grounding (2B) Visual Matching Pick Coke Can Move Near Drawer Avg Variant Aggregation Pick Coke Can Move Near Drawer Avg 28.0% 56.7% 17.0% 16.3% 77.3% 2.3% 56.0% 69.3% 72.7% 94.3% 85.0% 90.0% 83.0% 83.3% 53.7% 31.7% 4.2% 46.2% 61.7% 3.3% 65.4% 68.7% 65.3% 78.8% 76.3% 84.2% 77.9% 83.3% 57.0% 42.0% 59.7% 53.4% 22.7% 16.8% 35.6% 27.7% 43.5% 63.4% 18.3% 8.0% 83.7% 68.4% 35.8% 52.4% 38.3% 58.3% 19.0% 64.0% 44.9% 68.7% 44.4% 72.9% 56.0% 72.3% 54.2% 73.6% 60.0% 49.0% 0.6% 54.5% 75.6% 5.3% 53.4% 46.7% 75.2% 80.4% 74.4% 78.2% 77.7% 81.2% 56.4% 32.3% 3.1% 47.7% 60.0% 7.3% 65.7% 62.9% 63.7% 72.7% 69.2% 78.2% 73.2% 76.8% 31.0% 45.0% 29.4% 39.6% 1.1% 1.1% 17.7% 39.8% 10.6% 51.3% 20.5% 11.0% 68.8% 62.6% 17.5% 43.7% 25.6% 54.8% 11.1% 54.7% 10.3% 51.3% 13.0% 56.4% 13.2% 54.7% 17.0% 58.3% Baselines As mentioned before, we integrate Vlaser with an action expert utilizing flow matching for low-level control fine-tuning and evaluating the model on the WidowX and Google Robot datasets from SimplerEnv. We conduct experiments using InternVL3-2B, Vlaser, and the models based on Vlaser data engine for the in-domain robot data. Commonly, embodied reasoning can be marginally categorized into embodied QA (including planning), embodied grounding, and embodied spatial intelligence. We thus construct corresponding embodied VLMs based on the corresponding data stream: Vlaser-QA, Vlaser-Grounding, Vlaser-Spatial. Notably, all series of Vlaser models are based on the same architecture with 2B size. The full experimental results are presented in Table 2 and Table 3. We observe the proposed vision-language-action architecture based on InternVL3-2B is capable of low-level robot control and achieves comparable performance to many previous approaches, though we utilize 2B backbone. Particularly, we did not observe any clean improvement when we used the Vlaser-2B as our initial backbone on both WidowX and Google Robot, while we achieved significant performance promotion with the Vlaser-QA, although the architecture and model size are the same between the two models. This observation illustrates the effectiveness of our Vlaser data engine, and meanwhile identifies there is no positive correlation between common embodied reasoning benchmarks and the that performance of closed-loop control of the lower level for the specific embodiment of the robot. We reckon it is the domain shift between the internet data and the corresponding robot embodiment (e.g., WidowX or Google Robot), and we find that the enhanced abilities in the same observation domain effectively facilitate the closed-loop success rate. Therefore, it is urgent to shrink the domain gap between the foundational models and real-world robot embodiment for closed-loop task completion. In addition, we conducted simple ablation study on the Google Robot Tasks to evaluate the effectiveness of different data annotations. The experimental results indicate that incorporating all types of data leads to significant improvements, achieving performance comparable to the baseline. We attribute these improvements primarily to the reduction of the vision observation domain shift."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Vision-Language Model for Embodied Reasoning Enhancing the embodied reasoning capabilities of current state-of-the-art Vision-Language Models (VLMs) has emerged as critical research 8 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning direction. These capabilities encompass range of competencies, including grounding (Yuan et al., 2024; Deitke et al., 2025; Cheng et al., 2025) which identifies affordances that enable embodied agents to perform manipulations, spatial intelligence (Yang et al., 2025b;d), such as object counting and spatial relationship understanding, as well as task planning (Chen et al., 2023; Qiu et al., 2024), which involves assessing the current state and determining subsequent actions to be executed. Gemini Robotics-ER (Team et al., 2025b) integrates embodied reasoning into its core visual-language model (VLM), demonstrating strong generalization across variety of tasks such as 3D scene perception, visual pointing, state estimation, and affordance prediction. In parallel, number of data-driven methodologies have emerged to support such reasoning capabilities. For instance, Cosmos-Reason1 (NVIDIA et al., 2025a), VeBrain (Luo et al., 2025), MolmoAct (Lee et al., 2025), and EmbodiedOneVision (Qu et al., 2025a) each contribute curated datasets specifically designed for embodied reasoning tasks, emphasizing aspects such as multi-modal instruction following and action-aware visual-language alignment. Further advancing this direction, several frameworks including the RoboBrain series (Ji et al., 2025; Team et al., 2025a), Embodied R1 (Yuan et al., 2025), and Robix (Fang et al., 2025) incorporate Reinforcement Fine-Tuning (RFT) and synthesize spatiotemporal reasoning datasets enriched with structured thought traces. These approaches aim to enhance models capacity for causal reasoning and long-horizon task decomposition. Distinguished from these prior efforts, our work not only achieves competitive, and in some cases superior performance on established embodied reasoning benchmarks, but also provides an in-depth analysis of the synergistic relationship between pre-trained VLMs and downstream Vision-Language Action Models (VLAs), offering insights that bridge model capabilities and real-world deployment. Vision-Language-Action models. Developing generalist model remains central challenge Inspired by the strong generalization abilities of vision-language models (VLMs) in robotics. (OpenAI, 2023; Chen et al., 2024; Bai et al., 2025; Team et al., 2023) trained on large-scale internet data, researchers have proposed vision-language-action (VLA) models, which have demonstrated promising performance (Brohan et al., 2023a; Kim et al., 2024; Qu et al., 2025b; Hou et al., 2025). Compared to traditional robot policies, VLA models are pretrained on large-scale robotics datasets and exhibit improved generalization across object categories and visual observations. Building on recent progress, researchers have incorporated techniques such as diffusion (Ho et al., 2020; Rombach et al., 2022; Peebles & Xie, 2023), flow matching (Lipman et al., 2023b), and mixture-of-experts (MoE) (Shazeer et al., 2017) into VLA models, and have adopted larger, more capable VLMs as their backbones. These advances have enabled VLA models to tackle wider range of complex real-world manipulation tasks. Nevertheless, current VLA models remain limited in their generalization. In particular, they have not yet reached the level of general reasoning exhibited by VLMs, such as decompose complex task into manageable sub-steps and complete the task in zero-shot manner. Efforts have been made to enhance specific reasoning abilities in embodied scenarios (Team et al., 2025b; Ji et al., 2025; NVIDIA et al., 2025a). In parallel, several studies (Intelligence et al., 2025; Driess et al., 2025; Zhou et al., 2025b) explore unified training frameworks for VLMs and VLAs to leverage the reasoning capacity of VLMs. However, the relationship between high-level multimodal reasoning and low-level control performance remains largely unexplored. It is still unclear which specific multimodal abilities such as spatial understanding, grounding, or planning and which types of training data most effectively enhance the control capabilities of VLA model. In this work, we take an initial step toward analyzing this relationship by systematic evaluation, and also propose the latest foundational model with strong embodied multimodal understanding and action prediction."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "We introduce Vlaser, foundational vision-language-action model that extends vision-language models with embodied reasoning and end-to-end robot control capabilities. Powered by the Vlaser6M dataset, the model establishes new state of the art across wide range of embodied reasoning benchmarks, including planning, grounding, spatial reasoning, and simulation-based tasks. Moreover, Vlaser reveals the most effective data streams for downstream VLA through its curated data pipeline, achieving state-of-the-art performance on Bridge and competitive results on Google Robot for end-to-end robot control. In this work, we reveal that current embodied reasoning benchmarks exhibit significant domain gap when compared to real-world robots. This core domain shift arises from the observation that robots have fundamentally different viewpoint from that of internet datasets. Additionally, there 9 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning are inherent limitations due to the lack of sufficient data from the robots perspective, despite the abundance of vision datasets available. Therefore, we argue that it is essential to develop alignment techniques to bridge the domain gap in representations between the robots viewpoint and that of internet datasets."
        },
        {
            "title": "REFERENCES",
            "content": "Sonnet Anthropic. Claude 3.7 sonnet system card. 2025. URL https://www.anthropic. com/news/claude-3-7-sonnet. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. pi_{0}: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt2: Vision-language-action models transfer web knowledge to robotic control, 2023a. URL https://arxiv.org/abs/2307.15818. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. Robotics: Science and Systems XIX, 2023b. Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, Wenqi Shao, et al. Owmm-agent: Open world mobile manipulation with multi-modal agentic data synthesis. arXiv preprint arXiv:2506.04217, 2025a. Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Pannag Sanketi, and Ken Goldberg. Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. arXiv preprint arXiv:2505.15517, 2025b. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models. CoRR, 2023. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024. An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024. 10 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, et al. Pointarena: Probing multimodal grounding through language-guided pointing. arXiv preprint arXiv:2505.09990, 2025. Ron Chrisley. Embodied artificial intelligence. Artificial intelligence, 149(1):131150, 2003. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 91104, 2025. Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, et al. Internspatial: comprehensive dataset for spatial reasoning in vision-language models. arXiv preprint arXiv:2506.18385, 2025. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-languageaction models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instructionaligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, and Hang Li. Robix: unified model for robot interaction, reasoning and planning. arXiv preprint arXiv:2509.01106, 2025. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, and Renato Vidoni. Path planning and trajectory planning algorithms: general overview. Motion and operation planning of robotic systems: Background and practical approaches, pp. 327, 2015. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, et al. Dita: Scaling diffusion transformer for generalist vision-language-action policy. arXiv preprint arXiv:2503.19757, 2025. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pp. 91189147. PMLR, 2022. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi_{0.5}: vision-languageaction model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 9911002. PMLR, 2022. 11 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, and Shanghang Zhang. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 17241734, June 2025. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2222522233, 2025. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Kanxue Li, Baosheng Yu, Qi Zheng, Yibing Zhan, Yuhui Zhang, Tianle Zhang, Yijun Yang, Yue Chen, Lei Sun, Qiong Cao, Li Shen, Lusong Li, Dapeng Tao, and Xiaodong He. Muep: multimodal benchmark for embodied planning with foundation models. In Kate Larson (ed.), Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pp. 129138. International Joint Conferences on Artificial Intelligence Organization, 8 2024a. doi: 10.24963/ijcai.2024/15. URL https://doi.org/10.24963/ijcai.2024/15. Main Track. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024b. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024c. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023a. URL https://arxiv.org/abs/2210.02747. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=PqvMRDCJT9t. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action models. 2025. Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, et al. Visual embodied brain: Let multimodal large language models see, think, and control in spaces. arXiv preprint arXiv:2506.00123, 2025. 12 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. EmbodiedGPT: Vision-language pre-training via embodied chain of thought. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=IL5zJqfxAa. NVIDIA, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, YenChen Lin, Ming-Yu Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, and Zhe Zhang. Cosmos-reason1: From physical common sense to embodied reasoning, 2025a. URL https://arxiv.org/abs/2503.15558. NVIDIA, Nikita Cherniadev Johan Bjorck andFernando Castañeda, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. In ArXiv Preprint, March 2025b. open-pi zero. open-pi-zero, 2025. open-pi-zero. Accessed: 2025-08-25. URL https://github.com/allenzren/ OpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774. arXiv:2303.08774. OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2025. Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, et al. Embodiedonevision: Interleaved vision-text-action pretraining for general robot control. arXiv preprint arXiv:2508.21112, 2025a. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-languageaction model. arXiv preprint arXiv:2501.15830, 2025b. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 71417151, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 645652. IEEE, 2024. Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=B1ckMDqlg. Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, and Xipeng Qiu. World-aware planning narratives enhance large vision-language model planner, 2025. URL https://arxiv. org/abs/2506.21230. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1074010749, 2020. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, Tan, Deepanway Ghosal, Soujanya Poria, et al. Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning. arXiv preprint arXiv:2412.11974, 2024. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 251 266. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_ files/paper/2021/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, Devon Hjelm, and Alexander Toshev. Large language models as generalizable policies for embodied tasks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=u6imHU4Ebu. BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025a. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025b. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025c. 14 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023a. Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pp. 17231736. PMLR, 2023b. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2305.03716, 2023. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1420314214, 2025a. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025b. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025c. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025d. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: highfidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1222, 2023. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. arXiv preprint arXiv:2508.13998, 2025. Han-ye Zhang, Wei-ming Lin, and Ai-xia Chen. Path planning for the mobile robot: review. Symmetry, 10(10):450, 2018. Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, et al. From flatland to space: Teaching vision-language models to perceive and reason in 3d. arXiv preprint arXiv:2503.22976, 2025. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, et al. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. arXiv preprint arXiv:2412.18194, 2024. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. 15 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025a. Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint arXiv:2502.14420, 2025b. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TRAINING DETAILS Our Vlaser is optimized in fully supervised finetuning (SFT) manner based on InternVL3 series (Zhu et al., 2025)(InternVL3-2B and InternVL3-8B). To maximize adaptation to embodied reasoning tasks, we keep all parameters trainable, including those in the large language model, the vision-language projector, and the visual encoder, enabling comprehensive end-to-end learning. Further details regarding the training setup, including hyperparameters and optimization settings, are provided in Table 4. Table 4: Hyper-parameters used in the VLM pretraining of Vlaser. Configurations LLM sequence length Dynamic Resolution Patch Size Max Patch num Freeze vision tower Freeze multimodal projector Freeze language model Optimizer Optimizer hyperparameters Peak learning rate Learning rate schedule Training epochs Training steps Warm-up steps Global batch size Gradient accumulation Numerical precision Values 16, 384 True 448 12 False False False AdamW β1 = 0.9, β2 = 0.999, eps = 1e 8 2e-5 cosine decay 1 5, 000 150 128 2 bfloat16 Table 5: Hyper-parameters used in the VLA fine-tuning. Configurations Values LLM sequence length Image Size Freeze VLM Global batch size Training epochs VLM Peak Learning Rate Action Expert Peak Learning Rate Optimizer Optimizer hyperparameters Observation history length Action Chunk length Execute Action length 384 448 False 1024 10 5e-5 5e-5 AdamW β1 = 0.9, β2 = 0.999, eps = 1e 8 1 4 16 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning While using Vlaser as the base model for downstream VLA Policy fine-tuning, we optimize all parameters within both the VLM and the Action Expert. Additionally, we conduct comparative experiments using several different versions of base models, including InternVL3-2B, etc. Detailed information and related parameter settings can be found in Table 5. A.2 DATA GENERATION DETAILS Embodied Grounding Data To further enhance embodied grounding capabilities, we generate an additional 300k high-quality data samples from the SA-1B dataset (Kirillov et al., 2023). The data generation process consists of two main stages. First, we convert segmentation masks into bounding boxes and point annotations: bounding boxes are derived by computing the minimal axisaligned rectangle enclosing each mask, while point annotations are obtained by randomly sampling coordinate within the mask region. To ensure annotation quality, we apply an IoU threshold of 0.9 to select high-precision masks; masks with lower IoU values are either excluded or assigned reduced sampling weight. From the over 1 billion available masks, we initially sample 1 million candidate instances. In the second stage, we employ two-step captioning and refinement pipeline. Coarse captions are first generated using BLIP-2 (Li et al., 2023), followed by filtering and refinement process using Qwen2.5-VL-7B (Bai et al., 2025) to eliminate low-quality items and produce more accurate and detailed descriptions. This rigorous pipeline ultimately yields 300k high-quality data samples tailored for embodied grounding tasks, significantly expanding the diversity and precision of our training corpus. Spatial Reasoning Data To enhance spatial intelligence capabilities, we manually construct dataset of 100k 3D spatial perception samples derived from ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), and ARKitScenes (Baruch et al., 2021). Following methodologies established in prior data engines (Deng et al., 2025; Fan et al., 2025), we utilize both the 3D point cloud and video sequences of each scene to construct spatio-temporal scene graph. This graph encapsulates structural and semantic information such as overall scene dimensions, room center coordinates, object category counts, and precise 3D bounding boxes for every object instance. Based on this representation, we generate spatial reasoning questions that probe layout properties and inter-object relationships. These include queries about the object counts, absolute and relative distances, object and room sizes, relative directions between objects, and other spatial attributes, using the same question template in VSI-Bench (Yang et al., 2025b). Planning Data To improve the models ability to comprehend complex instructions and execute tasks with environmental feedback, we curate additional planning data within the Habitat simulator (Szot et al., 2021). Specifically, we initialize each planning task following the annotations of LLaRP (Szot et al., 2024), which specify both the task goals and the set of permissible actions. An LLM agent based on gpt-4o (OpenAI, 2025) is then deployed to roll out the task. During each rollout, we record the task instruction, the sequence of actions taken, and the environments feedback, including observations and success signal for each action. Both the executed action trajectories and the corresponding feedback are retained. Only trajectories that successfully accomplish the task are included in the final training set, providing rich paired data of instructions, execution processes, and environment responses for enhancing the models planning capabilities in complex environment. In-Domain Data for downstream VLAs We generate 2 million in-domain multimodal data samples to facilitate direct transfer learning for downstream Vision-Language Agents (VLAs) during finetuning. These data are collected from two distinct robotic platforms: the WidowX Robot (Walke et al., 2023a) and the Google Robot (Brohan et al., 2023b;a) within the SimplerEnv (Li et al., 2024c) simulation framework. The dataset mainly encompasses three systematically designed questionanswer types: 1) General QA, which queries the robots current state and requests next few action plans; 2) Grounding QA, which requires the robot to localize points and bounding boxes as the actionable affordances; 3) Spatial Reasoning QA, which probes relative spatial relationships and geometric properties of objects in the scene. Detailed prompt templates and representative examples for each data category are provided in Figure 3 (General QA), Figure 4 (Grounding QA), and Figure 5 (Spatial Reasoning QA), respectively. We use Qwen2.5VL-7B (Bai et al., 2025) as the base model to generate the above data items. 17 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning A.3 SIMULATION EVALUATION DETAILS We fine-tune and evaluate the VLA models using various base models within the SimplerEnv simulation environment. To ensure fair evaluation, we use checkpoints with the same number of iterations for the WidowX Robot Task and the Google Robot Task, respectively. Specifically, for the WidowX Robot Tasks, we use checkpoint after 45,390 iterations, while for the Google Robot Tasks, we use checkpoint after 36,970 iterations. During evaluation, we utilize single image and select an action chunk of size 2 for execution. In the flow matching configuration, we use 10 inference steps during the inference phase and apply Euler method as numerical integration method. We evaluate sufficient number of samples to ensure the reliability of the tests. The exact number of test samples for each task is shown in the Table 6. Table 6: The number of samples evaluated on SimplerEnv. Widowx Robot Tasks Google Robot Tasks Visual Matching Variance Aggregation Task Name Carrot on plate Put eggplant in basket spoon on towel stack cube Pick coke can Move Near Open/Close Drawer Pick coke can Move Near Open/Close Drawer Evaluation Samples 240 240 240 240 300 240 216 825 600 378 A.4 QUALITATIVE DEMONSTRATION Qualitative Samples for Embodied Reasoning. To gain in-depth insights into Vlaser, we provide extensive visualizations on embodied reasoning tasks including Embodied QA 7, grounding 8, spatial intelligence 9 and planning tasks in Simulation 10. These examples not only validate the generalizability of Vlaser, but also confirm the superior capabilities of Vlaser across all tasks. Qualitative Samples in SimplerEnv. Figure 6 and Figure 7 present qualitative examples from the evaluations conducted in the SimplerEnv. Specifically, Figure 6 showcases the performance of multiple models on the WidowX Robot Task, while Figure 7 demonstrates the results on the Google Robot Task. From these results, it is evident that, under the same test conditions, baseline models often fail the entire task due to slight positional errors. In contrast, our model shows significant improvements in this regard. Whether incorporating in-domain question-answer pair data, spatial intelligence data, or grounding data, the performance of the model shows clear enhancement. These results validate the effectiveness of our approach. 18 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 3: An illustration of Vlaser-6M data engine for in-domain general QA sample in SimplerEnv. 19 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 4: An illustration of Vlaser-6M data engine for in-domain embodied grounding QA sample in SimplerEnv. 20 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 5: An illustration of Vlaser-6M data engine for in-domain spatial reasoning QA sample in SimplerEnv. Figure 6: Qualitative samples in SimplerEnv on WidowX Robot Tasks 21 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Figure 7: Qualitative samples in SimplerEnv on Google Robot Tasks Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 7: Embodied QA examples. Embodied QA: Example #1 from ERQA. Question: What happened between these two frames? Choices: A. Robot arm lifted the cup. B. Robot arm poured all the nuts into cup. C. Robot arm poured some of the nuts into cup. D. Nothing happened. Please answer directly with only the letter of the correct option and nothing else. Vlaser-8B: C. Embodied QA: Example #2 from ERQA. Question: Select the best answer to the following multiple-choice question based on the video. Respond with only the letter (A, B, C, or D) of the correct option. Considering the progress shown in the video and my current observation in the last frame, what action should take next in order to weigh dough? A. put dough on table B. pick dough C. roll dough on table D. cut dough with dough cutter Vlaser-8B: A. 23 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 8: Embodied Grounding Examples. Embodied Grounding: Example #1 from PointArena. Question: Vlaser-8B: Point to the tool people use for writing. <point>[[701, 374]]</point>. Embodied Grounding: Example #2 from Where2Place. Question: Vlaser-8B: Please point out the free space between the black water bottle and the pot lid. <point>[[293, 560]]</point>. 24 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 9: Spatial Intelligence Examples. Spatial Reasoning: Example #1 from VSI. Question: You are robot beginning at the standing by the window and facing the window. You want to navigate to the white shoes. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): 1. [please fill in] 2. Go forward passing the bed 3. [please fill in] 4. Go forward until the white shoes. You have reached the final destination. A. Turn Right, Turn Left B. Turn Left, Turn Left C. Turn Left, Turn Right D. Turn Right, Turn Right. Answer with the options letter from the given choices directly. Vlaser-8B: B. 25 Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning Table 10: Simulation planning Examples. Simulation Planning: Example #1 from EmbodiedBench. Question: Vlaser-8B: You are robot operating in home. Given task, you must accomplish the task using defined set of actions to achieve the desired outcome. Now the human instruction is: Put microwaved tomato slice on the counter. You need to output the reasoning process and an action to execute. Reasoning-step-1: To put microwaved tomato slice on the counter, need to start by slicing tomato. To do this, require knife. In the image, there is knife visible on the counter near the toaster. Finding the knife is the logical next step because it is essential for cutting the tomato into slices. Once have the knife, can proceed with slicing the tomato. Therefore, my next action is to find the knife. Action: find knife. Reasoning-step-2: have identified the knife on the counter near the sink. Picking up the knife is the next logical step because it is necessary for slicing the tomato. Once have the knife, can proceed with cutting the tomato into slices. This aligns with the goal of preparing tomato slice for microwaving and then placing it on the counter. Action: pick up the knife. Reasoning-step-3: have successfully located the knife in the drawer. Now, need to find tomato to slice. The image shows kitchen counter with various items, but no tomato is immediately visible. Tomatoes are commonly stored in the refrigerator or on the counter. Since the refrigerator is partially visible on the left side of the image, it makes sense to check there first. If the tomato isnt in the fridge, can then look around the counter more thoroughly. Therefore, my next step is to find tomato. Action: find tomato. ... Reasoning-step-9: The current visual state shows the microwave with its door closed, and the tomato slice is on the counter. The task is to place microwaved tomato slice on the counter, which has already been accomplished. Given that the tomato slice is already on the counter, the next logical step is to confirm its placement by finding suitable counter top. This ensures that the tomato slice is securely and appropriately positioned for the task. Therefore, the next step is to find counter top. Action: find counter top. Reasoning-step-10: The current visual state shows microwave with its door closed, and two tomatoes are visible on the counter. One tomato is already sliced, indicating that the previous steps involving slicing and microwaving have been completed. Since the tomato is currently being held, the next logical step is to put it down on the counter. This action will free up the hand to handle the tomato slice and place it on the counter as required by the task. Therefore, the appropriate next action is to put down the tomato. Action: put down the tomato."
        }
    ],
    "affiliations": [
        "Fudan University",
        "NUS",
        "Nanjing University",
        "Northeastern University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shenzhen University",
        "Tsinghua University",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}