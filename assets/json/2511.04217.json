{
    "paper_title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
    "authors": [
        "Hikari Otsuka",
        "Daiki Chijiwa",
        "Yasuyuki Okoshi",
        "Daichi Fujiki",
        "Susumu Takeuchi",
        "Masato Motomura"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of $H$ heads and input dimension $d$ has the hidden dimension $O(d\\log(Hd^{3/2}))$ for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model."
        },
        {
            "title": "Start",
            "content": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms Hikari Otsuka1, , Daiki Chijiwa2, Yasuyuki Okoshi1, Daichi Fujiki1, Susumu Takeuchi2, Masato Motomura1 1Institute of Science Tokyo 2NTT, Inc. otsuka.hikari@artic.iir.isct.ac.jp 5 2 0 2 6 ] . [ 1 7 1 2 4 0 . 1 1 5 2 : r Abstract The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, core component of transformers. To address this gap, we introduce theoretical analysis of the existence of SLTs within MHAs. We prove that, if randomly initialized MHA of heads and input dimension has the hidden dimension O(d log(Hd3/2)) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model."
        },
        {
            "title": "Introduction",
            "content": "The lottery ticket hypothesis (Frankle and Carbin 2019) overparameterized networks contain subnetworks that achieve comparable accuracy to fully trained networks even if trained in isolationpresented new possibilities for compact and high-performing models inherent in recent deep neural networks. Later, stronger claim, which is formally defined as the strong lottery ticket hypothesis (SLTH), was proposed (Ramanujan et al. 2020; Malach et al. 2020): overparameterized networks contain subnetworks (called strong lottery tickets (SLTs)) that achieve comparable accuracy to the trained dense network even without any training. Whether such subnetworks exist is fascinating question in itself, and studying them can bring us closer to understanding the principles behind overparameterized models. The rigorous proof for the SLTH was firstly established in fully-connected networks. Early studies showed that randomly-weighted fully-connected network of sufficient width (a source network) contains an SLT, which approximates an arbitrary fully-connected network with half the Figure 1: Comparison of the approximation techniques in conventional theories of the SLTH (top) and in our attentionspecific approach (bottom). This work demonstrates that an arbitrary attention mechanism can be approximated by pruning randomly initialized one. depth (a target network) (Malach et al. 2020; Orseau, Hutter, and Rivasplata 2020; Pensia et al. 2020). These theories are built on the foundational argument called two-layers-forone approximation: two-layer source network with random weight matrices contains an SLT that approximates singlelayer target network with an arbitrary weight matrix (the top panel of Figure 1). Following this finding, subsequent studies have succeeded in proving the existence of SLTs in more complex networks, such as convolutional and equivariant networks (da Cunha, Natale, and Viennot 2022; Burkholz 2022a; Ferbach et al. 2023). However, the theoretical foundation of the SLTH for transformers, which form the basis of modern language models, remains unexploreddue to transformer-specific component, an attention mechanism. As shown in the bottom panel of Figure 1 (right side), one of the distinctive structures in transformers is the inner product between two vectors called query and key, obtained as linear projections of given inputs. This structure fundamentally differs from the conventional components of non-transformer architectures for which the SLTH has been established (the top panel of Figure 1); thus, it remains mystery whether transformers contain SLTs under existing theoretical insights. This gap motivates our key research question: does an attention mechanisman essential component of transformers contain an SLT? In this work, we prove the existence of SLTs within attention mechanisms, extending the SLTH to transformers. More precisely, we prove suitably pruned source attention mechanism with random weights can approximate any target attention mechanism with arbitrary weights: Theorem 1 (informal). Given inputs of length , suitably pruned randomly-initialized attention mechanism of the input dimension and hidden dimension = O(d log(d3/2/ϵ)) can approximate an arbitrary attention mechanism of the same input dimension with an approximation error ϵ, with high probability. Our key idea is to reinterpret the inner product between the query and key vectors in the attention mechanism as (linear) neural network weighted by the query and key projection matrices. Then, we can view the source and target inner products as neural networks with different numbers of layers: the source one has two layers with query and key projection matrices as its weights, while the target one has single layer with weight matrix obtained by merging these two projections. This reinterpretation makes it possible to apply variant of the two-layers-for-one approximation, leading to the SLT existence within attention mechanisms (Theorem 3). Note that, as can be seen by comparing the top and bottom panels of Figure 1, our arguments do not require additional layers in the MHA for approximation, in contrast to the previous two-layers-for-one argument for fully-connected networks. By exploiting this theorem, we further establish the SLTH for transformers without normalization layers: randomly-initialized transformer has an SLT that approximates an arbitrary transformer with similar structures (Theorem 6). We also empirically validate our theory and confirm its implications. Specifically, we show that 1) the approximation error between the source and target attentions (or, more generally, source and target transformers) decays exponentially as the hidden dimension increases; and 2) this approximation error does not diverge even when the input length increases. Also, based on our theoretical arguments, we derive new, practical weight initialization scheme, leading to better SLTs in our experiments. Our contributions are summarized as follows: We provide the first theoretical proof that SLTs exist within attention mechanisms and transformers by reinterpreting the inner product in attention mechanisms. We then empirically validate our theory under conditions that are close to our theoretical assumptions. More precisely, we carefully designed synthetic experiment to observe how the hidden dimension or input length affects the approximation error of SLTs. Furthermore, we demonstrate that our theory not only explains the empirical results, but also provides new insight into weight initialization for finding better SLTs in practical settings. Notation: In this paper, scalars, vectors, and matrices are denoted by lowercase, bold lowercase, and bold uppercase letters, respectively. We use the norm of matrices and vectors as the spectral norm unless otherwise specified by subscripts. We denote the uniform distribution on [a, b] by [a, b]. represents an element-wise multiplication (i.e., the Hadamard product). The superscript (i) denotes the layer index, and we write {x(i)}H i=1 to denote the set of elements x(i) indexed by from 1 to H."
        },
        {
            "title": "2.1 Strong Lottery Ticket Hypothesis\nThe strong lottery ticket hypothesis (SLTH) conjectured that\na randomly-initialized network inherently contains subnet-\nworks (strong lottery tickets (SLTs)) that achieve high ac-\ncuracy comparable to trained dense networks, without any\nweight updates (Ramanujan et al. 2020; Malach et al. 2020).\nThe first theoretical result of the SLTH was given by Malach\net al. (2020). They proved the existence of SLTs in a fully-\nconnected ReLU network. Subsequent studies relaxed the\nrequirements for source networks to contain SLTs that ap-\nproximate some target network (Orseau, Hutter, and Rivas-\nplata 2020; Pensia et al. 2020; Burkholz 2022b). In partic-\nular, Pensia et al. (2020) introduced a subset-sum approxi-\nmation technique (Lueker 1998) into the SLTH context and\nconcluded that the logarithmic overparameterization of the\nsource network to a given target is approximately optimal:\nLemma 2. Given x ∈ Rd1 , W ∈ Rd2×d1 , ˜W 1 ∈ Rn×d1,\nand ˜W 2 ∈ Rd2×n, we define the target and pruned source\nfully-connected networks as",
            "content": "FT(x) := x, FS(x) := ( 2 2)ReLU(( 1 1)x), where 1 {0, 1}nd1 and 2 {0, 1}d2n are binary pruning masks. Assume that 1, 1, and each entry of 1 and 2 is drawn i.i.d. from [1, 1]. Also, for 0 < ϵ < 1, suppose that the hidden dimension satisfies d1C log (2d1d2/ϵ), where > 0 is some universal constant. Then, with probability at least 1 ϵ, there exists choice of binary pruning masks 1 and 2 such that FT(x) FS(x) ϵ. This approach, which approximates single weight matrix by pruning two randomly initialized matrices (the top panel of Figure 1), is called the two-layers-for-one approximation, and is now the theoretical foundation of the SLTH for more complex architectures and problems (da Cunha, Natale, and Viennot 2022; Burkholz 2022a; Ferbach et al. 2023; Natale et al. 2024; Otsuka et al. 2025)."
        },
        {
            "title": "3.1 Setups\nWe consider two MHAs: a target MHA AttnT(·) with arbi-\ntrary (tuned) weights, and a pruned source MHA AttnS(·)\nwith randomly-initialized weights, denoted as follows:",
            "content": "AttnT(xi) = Attn(xi; X, (1:H) AttnS(xi) = Attn(xi; X, ( )(1:H) Q:O ), Q:O ). (1) (2) Here, similarly to the weight set (1:H) of pruned random weights as Q:O , we define the set ( )(1:H) Q:O := { (j) (j) (j) Q , (j) (j) , (j) , (j) (j) }H j=1, W (j) Rd1nV , (j) , (j) Rd1nK , , (j) , (j) , and (j) and where (j) RnVd2 are the randomly-weighted query, key, value, and output projections of the j-th head in AttnS(), respectively. Also, (j) are their corresponding binary pruning masks. Note that the target and source MHAs have different key and value hidden dimensions: dK and dV for the target, and nK and nV for the source. We assume that α max( d1, d2) for the , (j) , (j) , (j) inputs, and (j) 1 for the j-th head of the target MHA. The source MHA is initialized such that each entry of and is drawn i.i.d. from [n1/4 , n1/4 ], and each entry of and is drawn i.i.d. from [1, 1]."
        },
        {
            "title": "3.2 The Existence of SLTs Within an MHA\nNow, we prove the following SLT existence theorem:",
            "content": "Theorem 3. Let AttnT() and AttnS() be as defined in Equations (1) and (2). Then, with probability at least there exists choice of binary pruning masks 1 ϵ, , (j) (j) that satisfy , (j) , (j) max i[T ] AttnS(xi) AttnT(xi) ϵ, if the source hidden dimensions satisfy nK d1C log nV d1C log (cid:32) 1 8Hα3d3/2 ϵ (cid:18) 2Hαd1 ϵ (cid:33) , d2 (cid:19) , for some universal constant > 0. Figure 2: The structure of an MHA. By partitioning the output projection, the final result can be interpreted as the sum of outputs from all heads."
        },
        {
            "title": "2.2 Multi-head Attention Mechanisms\nLet X = [x1, ..., xT ]⊤ ∈ RT ×d1 be a sequence of T input\nvector embeddings. For each embedding xi, we define a\nbinary attention mask ai ∈ {0, 1}⊤, where ai,j = 1 indi-\ncates that the i-th embedding attends to the j-th one. We\nassume that each embedding attends to at least one other\n(i.e., ∥ai∥1 ≥ 1). Given such inputs, a multi-head attention\n(MHA) mechanism (Vaswani et al. 2017) is defined as a\nfunction that computes their pair-wise relationships at each\nof the H attention heads (the left panel of Figure 2). For the\ni-th embedding xi, the MHA is of the following form:\nQ , W (j)\nhead(1)\n(cid:32)",
            "content": "K , (j) }H , . . . , head(H) Attn(xi; X, {W (j) R1d2, j=1, O) (cid:33) := (cid:105) (cid:104) head(j) :=σ ; ai (j), q(j) K(j) dK q(j) := W (j) , K(j) := XW (j) , (j) := XW (j) , σ(xi; ai)j := (cid:80)T . ai,j exp(xi,j) k=1 ai,k exp(xi,k) , (j) K Rd1dK , (j) Here, we define (j) Rd1dV , and RHdVd2 as single-layer projections for the query q(j) , key K(j), value (j), and output of the MHA, respectively. The softmax function with the attention mask is defined as σ(). As shown in the right panel of Figure 2, by partitioning the output weight matrix into , . . . , (H) the form of Attn() can be represented as follows: , (j) Attn(xi; X, {W (j) := [W (1) RdVd2, ], (j) ,W (j) }H j=1, O) = Attn(xi; X, (1:H) Q:O ) = (cid:88) j=1 head(j) (j) . We denote the set of all weights as , (j) Q:O := {W (j) (1:H) , (j) , (j) }H j=1. Figure 3 shows an overview of our proof. To prove Theorem 3, we begin by focusing on the part before the softmax, the target and source inner products for the j-th head: 1 dK q(j)K(j) = (x 1 dK (j) ))(X( (j) 1 nK (x ( i (j) )(XW (j) ), (j) (j) )). (3) (4) (j) (j) and Since the only difference lies in the projection matrices, we consider the problem of pruning the source projections to approximate the target projections (j) and (j) . naive idea might be to approximate each target projection independently. In this case, single source random matrix must approximate each target matrix. However, pruning single random matrix cannot generally approximate arbitrary ones; thus, this approach is infeasible. To overcome this limitation, we revisit the structure of the target inner product. By closely examining the formulation of the target inner product (Equation (3)), we observe that the query and (transposed) key projections appear adjacently and can be merged into single joint projection (the right panel of Figure 3): 1 dK (x (j) )(XW (j) ) = W (j) QKX , (j) QK := 1 dk (j) (W (j) ). (5) This reformulation enables us to reinterpret the original problemnot as approximating two target matricesbut as approximating single merged projection matrix. We now approximate this merged matrix (j) QK by pruning the two source projections. On the source side (Equation (4)) as well, the query and key projections are adjacent. Thus, the source inner product can be viewed as computation that first calculates the query and key projections (the left panel of Figure 3): ))(X( (j) (j) )) 1 nK (x ( (j) (j) (cid:16) ( (j) = Q (j) )( W (j) := 1 n1/4 (j) , (j) := , (j) )(cid:17) (j) 1 n1/4 (j) , (j) (j) (j) where each entry of and is drawn i.i.d. from [1, 1] as per our assumption. Therefore, the task reduces and (j) to selecting masks (j) such that the source ma- (j) (j) (j) trix product ( )( ) closely approximates the target (j) QK. This allows us to draw an analogy to the conventional theoretical results of the SLTH, particularly the two-layers-for-one approximation (Lemma 2). We therefore establish and apply variant of Lemma 2, which guarantees the existence of binary pruning masks that achieve such an approximation (the bottom panel of Figure 3): Figure 3: The diagram of our proof. By merging the target projections and changing the calculation order of the source MHA, we can apply variant of the two-layers-for-one approximation technique and approximate the target MHA while keeping the original source and target structures. Lemma 4. Let Rd2d1 be target matrix with 1, and 1 Rnd1 and 2 Rd2n be source matrices whose entries are drawn i.i.d. from [1, 1]. Suppose that d1C log(d1d2/ϵ) for some universal constant > 0. Then, with probability at least 1 ϵ, there exists choice of binary pruning masks 1 and 2 such that (cid:13) (cid:13) (cid:13)W ( 2 2)( 1 1) (cid:13) (cid:13) (cid:13)max ϵ d1d2 . We now turn to the components after the softmax function: the value and output projections. Similar to the query and key case, the value and output projections appear adjacently and can also be merged into single composite transformation. Thus, we aim to approximate the target merged matrix (j) . This approximation follows the same principle as before: we leverage the matrix product (j) on the source side ( ) to approximate the merged matrix (j) VO. Lemma 4 ensures that, with high probability, this approximation is successful via appropriately chosen binary pruning masks (j) VO := (j) (j) (j) )( and (j) . Assuming that all weights in the target MHA are approximated by the above procedure, we next analyze the error of the entire attention mechanism by investigating the behavior of the softmax. As natural idea, one might consider exploiting the 1-Lipschitz continuity of the softmax (Gao and (j) (j) Pavel 2017), which enables internal errors to propagate linearly to the output. However, since the MHA subsequently multiplies the softmax output and the input matrix X, applying Lipschitz continuity results in loose upper bound of the error between MHAs: as can grow with in the worst case, the bound depends on the input length . In contrast to this general approach, we provide more precise analysis. In our setting, thanks to the accurate weight approximation technique mentioned earlier, the internal error of softmax is guaranteed to be finite and small. Leveraging this property, we analyze the softmax output and simultaneously to obtain -independent bound as follows: Lemma 5. Let ϵ Rd1 be an error vector with ϵmax ϵmax for some 0 ϵmax 1/2. Then, (cid:112) σ(xi; ai)X σ(xi + ϵ; ai)X 4 d1αϵmax. max i[T ] Since this lemma provides bound independent of ai, our theory holds for models with arbitrary attention masks, including encoder (Devlin et al. 2019) and decoder models (Radford et al. 2019). By applying these above analyses to each attention head, we complete the proof of Theorem 3. For the full proof, see Section A.3. We also empirically validate two main theoretical findings in Section 4.2: the accurate approximation of the target MHA becomes feasible with larger source hidden dimensions, and the the approximation error remains independent of the input length . Proof Sketch of Theorem 3: First, for each attention head, we reformulate the problem by merging the four original target projection matrices into two merged matrices: one combining the query and key projections, and the other the value and output projections. Applying Lemma 4 to these merged matrices enables us to prune each source head to produce an inner product that closely approximates the target one. Next, using Lemma 5, we bound how errors in approximating the query and key matrices propagate through the softmax operation. Lemma 5 ensures that the approximation error of the softmax depends on the approximation accuracy of the query-key projections and does not scale with the input length . Thus, provided the source hidden dimensions nK and nV are sufficiently large, there exists choice of binary masks for the source MHA which approximate the target MHA within an error ϵ. Also, by suitably setting lower bounds on nK and nV, union bound guarantees that the approximation succeeds across all heads with probability at least 1 ϵ."
        },
        {
            "title": "3.3 The Existence of SLTs Within a Transformer\nBy leveraging our main theorem, we now extend the SLTH\nto transformers. We consider a transformer without the nor-\nmalization layers for the original definition (Vaswani et al.\n2017). The target transformer of B blocks are of the follow-\ning form:",
            "content": "Tf T(xi) := Blk(B) ) := F(b) (x(b) (Blk(B1) (AttnT(x(b) Blk(b) . . . Blk(1) (xi)), ) + x(b) ) ) + x(b) , + AttnT(x(b) where Blk(b) is b-th target block, and x(b) Rd is the i-th input embedding of the b-th target block. We employ single-layer projection F(b) () for the fully-connected network of each target block. Similarly, we define the pruned source transformer as follows: Tf S(xi) := Blk(B) (x(b) (Blk(B1) (AttnS(x(b) ) := F(b) . . . Blk(1) ) + x(b) (xi)), ) Blk(b) + AttnS(x(b) ) + x(b) , S () as n(b) () is b-th source block and x(b) where Blk(b) is the i-th input embedding of the b-th source block. We set the hidden dimension of F(b) FC and assume the hidden dimensions of the MHA in the b-th source block as same value n(b) MHA for simplicity. Then, we prove the following theorem: Theorem 6. Assume 2. Then, with probability at least 1 ϵ for 0 < ϵ < 1, there exists choice of binary pruning masks that satisfies Tf S(xi) Tf T(xi) ϵ, if the hidden dimensions of b-th source MHA and fullyconnected network satisfy (cid:32) (cid:33) cf1(b,B) 1 f2(b,B)df3(b,B) 1 n(b) MHA d1C log ϵ n(b) FC d1C log (cid:32) cg1(b,B) g2(b,B)dg3(b,B) 1 ϵ , , (cid:33) for universal constants > 0 and c1, c2 > 0 including α. Here, f1, f2, f3, g1, g2, g3 are quadratic forms of and B. Proof Sketch of Theorem 6: From the existing work (Lemma 2) and Theorem 3, we already know that an MHA and FFN contain SLTs with high probability if each module has large hidden dimension; thus, by determining the lower bound of the hidden dimension of each module based on the error propagation from the input to output, we can prove that there exists an SLT, which approximates the output of an target transformer to an error of ϵ, within randomly initialized transformer. By the union bound, the probability that all approximations hold simultaneously is at least 1 ϵ. For simplicity, this theorem uses target and source fullyconnected networks as single-layer and two-layer ReLU networks FT and FS in Lemma 2. It can be generalized to an L-layer target fully-connected network by applying the multi-layer approximation by Pensia et al. (2020). We show that theorem and its proof in Section A.5."
        },
        {
            "title": "4 Experimental Results",
            "content": "This section empirically validates our SLTH theorems."
        },
        {
            "title": "4.1 Experimental Settings\nTo empirically validate the approximation guarantees estab-\nlished by our SLTH theorems, we evaluate the approxima-\ntion error on a synthetic dataset for angular velocity estima-\ntion. The input consists of a sequence of two-dimensional",
            "content": "Figure 4: The approximation error ϵ of SLTs within source MHA for the hidden dimensions nK = nV. This result shows that the error ϵ satisfies ϵ = O(exp(n)), consistently with Theorem 3. Figure 5: The approximation error ϵ of SLTs within an MHA for the sequence length . This result suggests that the error ϵ does not diverge as increases, as implied by Theorem 3. , n1/4 vectors arranged on the unit circle with fixed angular velocity. regression token is used to estimate this velocity, and the source model uses the same regression token as the target model to ensure input consistency. The source and target models are both implemented as either single-head attention mechanisms or single-head transformers as defined in Section 3.3. Both models are initialized according to our theoretical setup: the entries of the query and key projection weights are drawn i.i.d. from [n1/4 ], and those of the value and output projection weights from [1, 1]. To identify SLTs that approximate the target network, we implement the weight approximation technique described in Lemma 4, which is based on the subset-sum approximation of Pensia et al. (2020). The target MHA is approximated using 100 randomly initialized source MHAs, and we report the mean and standard deviation of the approximation error. We also investigate whether our theoretical insights generalize to practical settings. In this setting, we search for SLTs by the edge-popup algorithm (Ramanujan et al. 2020), which finds accurate subnetworks by backpropagation, instead of learning weights. We train models from the GPT-2 family (mini1, small, and medium) (Radford et al. 2019) on the WikiText-103 dataset (Merity et al. 2017). The weights of these models are initialized based on the GPT2 initialization scheme. For each model, we repeat training three times with different random seeds and report the mean and standard deviation of the final performance. See Section for further details on experimental settings."
        },
        {
            "title": "4.2 Verification of Main Theorems\nWe empirically verify our theoretical results by pruning a\nsource network to approximate the target network.",
            "content": "Varying the Hidden Dimensions: We validate Theorem 3 by showing that increasing the hidden dimensions leads to an exponential decrease in approximation error. When we fit the empirical results to the function ϵ = γ exp(δnK), we obtain ϵ = 0.8 exp(0.06nK), which closely matches 1A 4-layer GPT-2. For details, see the following repository: https://huggingface.co/erwanf/gpt2-mini (Wolf et al. 2020) Figure 6: The approximation error ϵ of SLTs within randomly initialized transformer for the source hidden dimensions nMHA = nFC. This result suggests that error accumulates as the number of blocks increases, while each error holds ϵ = O(exp(nMHA)), consistently with Theorem 3. the observations. This finding supports our theoretical claim: given target MHA, each source hidden dimension requires O(log(1/ϵ)) for the existence of SLTs. Varying the Sequence Length: Theorem 3 also implies that the existence of SLTs in MHAs is independent of the input length . In other words, with sufficiently large hidden dimensions, the approximation error has an upper bound that does not depend on . Figure 5 empirically supports this argument: even as increases, the error remains bounded, and the bound decreases with larger hidden dimensions. Varying the Number of Blocks: To validate Theorem 6, we analyze how the approximation error behaves across different numbers of transformer blocks. We set nMHA = nFFN and use an untrained target model to be close to our theoretical assumptions. As in the MHA experiment, we fit an exponential decay ϵ = γ exp(δnK) to the error of each block, using the same decay rate δ obtained from the first block, as predicted by Theorem 6. Figure 6 shows that, consistent with our theoretical implication, the approximation error decreases rapidly with increasing hidden dimensions for all numbers of blocks. Despite fitting only the coefficient Figure 7: Loss comparison between SLTs with and without query and key weight scaling. By introducing scale based on our theoretical assumptions, we can obtain better SLTs. Figure 8: Loss comparison with respect to the weight scaling factor applied to query and key weights. Interestingly, in all models, the loss reaches its minimum near the weight scaling of our theoretical assumptions. γ per block, the shared δ provides curves that closely match the empirical results, supporting our theoretical claim that only the scale factor varies across blocks."
        },
        {
            "title": "4.3 Behavior of SLTs in Practical Settings\nIn the theoretical analysis, we employ a non-conventional\ninitialization strategy: the query and key projection weights\nK , n1/4\nare initialized from U [−n1/4\nK ], scaled by a factor of\nn1/4\ncompared to the value and output weights, which are\nK\ninitialized from U [−1, 1]. This weight scaling was intro-\nduced to facilitate the application of the weight approxima-\ntion lemma in our analysis, and played an important role\nin establishing our theory. Its theoretical contribution moti-\nvates the following question: does this scaled initialization\nstrategy also benefit SLTs in realistic scenarios? We em-\npirically evaluate SLTs using the GPT-2 architectures and\nthe WikiText-103 dataset. Figure 7 compares the valida-\ntion loss of SLTs with and without scaling the query and\nkey weights by n1/4\nK ≃ 2.8, with respect to the number of\nnonzero parameters. We observe that SLTs with the weight\nscaling tend to exhibit lower loss, approaching the perfor-\nmance of trained models. Interestingly, this specific scal-\ning factor n1/4\nis nearly optimal for finding better SLTs: as\nshown in Figure 8, increasing the scale from 1 gradually de-\ncreases the loss up to a certain point, but further increasing it\nbeyond n1/4\nK results in increased loss. In all models, the low-\nest loss is consistently achieved around this scaling factor\nn1/4\nK . These findings suggest that our initialization strategy\nactually helps to ensure the existence of better SLTs within\nthe practical transformer models.",
            "content": "K"
        },
        {
            "title": "5 Related Work\nStrong Lottery Tickets: Zhou et al. (2019) and Ramanu-\njan et al. (2020) empirically found the subnetworks that\nachieve high accuracy without any weight training. The exis-\ntence of such high-performing subnetworks has been called\nthe strong lottery ticket hypothesis (SLTH), and its theoreti-\ncal proof was firstly provided in fully-connected ReLU net-\nworks (Malach et al. 2020; Orseau, Hutter, and Rivasplata",
            "content": "2020; Pensia et al. 2020; Burkholz 2022b). Based on these pioneering studies, the SLTH has been extended in three main directions. The first direction involves introducing additional flexibility for relaxing the overparameterization of the source network (Chijiwa et al. 2021; Xiong, Liao, and Kyrillidis 2023). The second direction, in contrast, imposes additional constraints on the source network (Gadhikar, Mukherjee, and Burkholz 2023; Otsuka et al. 2025; Natale et al. 2024). The third direction extends the SLTH to various architectures (Diffenderfer and Kailkhura 2021; Burkholz 2022b; Fischer and Burkholz 2021; da Cunha, Natale, and Viennot 2022; Da Cunha and dAmore 2023; Burkholz 2022a; Ferbach et al. 2023). Our work contributes to this third direction by proving the SLTH for attention mechanisms and transformers. Randomly Weighted Transformers: Several studies have empirically investigated the capabilities of randomly weighted transformers. Shen et al. (2021a) demonstrated that transformer with few randomly weighted layers achieves accuracy comparable to fully trained models on translation and language understanding tasks. Zhong and Andreas (2024) found that randomly weighted transformers can solve toy tasks with high accuracy as the hidden dimension increases. Some studies empirically showed the existence of SLTs within randomly weighted transformers (Shen et al. 2021b; Ito et al. 2025). Our analysis provides theoretical support for these empirical results about the SLT existence. Furthermore, it provides theoretical explanation for the improved performance of randomly weighted transformers as the hidden dimension increases, particularly when the pruning is used for optimization."
        },
        {
            "title": "6 Conclusion\nThis work investigated the existence of SLTs within a multi-\nhead attention (MHA) mechanism. We extended the exist-\ning theory of the SLTH to MHAs and proved that, if the\nsource MHA has logarithmically large hidden dimensions,\nit contains an SLT that approximates an arbitrary MHA with\nhigh probability. Our proof revealed that, for the SLTH in",
            "content": "MHAs, additional layers are not required for approximation, in contrast to the existing theories that rely on approximating single-layer structure by two-layer one. Furthermore, by exploiting our findings, we established the theory of the SLTH for transformers without normalization layers. We empirically validated our theory and confirmed that the results are consistent with the theoretical implications. Interestingly, our theoretical implication, which provides an appropriate weight scale for initializing query and key projection weights, contributed to improving the performance of SLTs in practical settings. Our results not only extend SLTH to transformers, but also indicate new research direction in the SLTH for practical transformer models. We hope these findings will lead to fundamental understanding of overparameterized models."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by JSPS KAKENHI Grant Number JP23H05489, JP25K03092, JP25KJ1236, and JSTALCA-Next Japan Grant # JPMJAN24F3. References Burkholz, R. 2022a. Convolutional and residual networks provably contain lottery tickets. In International Conference on Machine Learning, 24142433. PMLR. Burkholz, R. 2022b. Most activation functions can win the lottery without excessive depth. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds., Advances in Neural Information Processing Systems. Chijiwa, D.; Yamaguchi, S.; Ida, Y.; Umakoshi, K.; and Inoue, T. 2021. Pruning randomly initialized neural networks with iterative randomization. Advances in neural information processing systems, 34: 45034513. Da Cunha, A.; and dAmore, F. 2023. Polynomially overparameterized convolutional neural networks contain structured strong winning lottery tickets. Advances in Neural Information Processing Systems, 36: 2592925957. da Cunha, A.; Natale, E.; and Viennot, L. 2022. Proving the strong lottery ticket hypothesis for convolutional neural networks. In ICLR 2022-10th International Conference on Learning Representations. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for In Proceedings of the 2019 conlanguage understanding. ference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 41714186. Diffenderfer, J.; and Kailkhura, B. 2021. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning randomly weighted network. In International Conference on Learning Representations. Ferbach, D.; Tsirigotis, C.; Gidel, G.; and Bose, J. 2023. general framework for proving the equivariant strong lottery ticket hypothesis. In The Eleventh International Conference on Learning Representations. Fischer, J.; and Burkholz, R. 2021. Towards strong pruning for lottery tickets with non-zero biases. arXiv preprint arXiv:2110.11150. Frankle, J.; and Carbin, M. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations. Gadhikar, A. H.; Mukherjee, S.; and Burkholz, R. 2023. Why random pruning is all we need to start sparse. In International Conference on Machine Learning, 1054210570. PMLR. Gao, B.; and Pavel, L. 2017. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805. Glorot, X.; and Bengio, Y. 2010. Understanding the difficulty of training deep feedforward neural networks. In Teh, Y. W.; and Titterington, M., eds., Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, 249256. Chia Laguna Resort, Sardinia, Italy: PMLR. Gurobi Optimization, LLC. 2024. Gurobi optimizer reference manual. Ito, H.; Yan, J.; Otsuka, H.; Kawamura, K.; Motomura, M.; Chu, T. V.; and Fujiki, D. 2025. Uncovering strong lottery tickets in graph transformers: path to memory efficient and robust graph learning. Transactions on Machine Learning Research. Loshchilov, I.; and Hutter, F. 2017. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations. Loshchilov, I.; and Hutter, F. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Lueker, G. S. 1998. Exponentially small bounds on the expected optimum of the partition and subset sum problems. Random Structures & Algorithms, 12(1): 5162. Malach, E.; Yehudai, G.; Shalev-Schwartz, S.; and Shamir, O. 2020. Proving the lottery ticket hypothesis: Pruning is all you need. In International Conference on Machine Learning, 66826691. PMLR. Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017. In International ConferPointer sentinel mixture models. ence on Learning Representations. Natale, E.; Ferre, D.; Giambartolomei, G.; Giroire, F.; and Mallmann-Trenn, F. 2024. On the sparsity of the strong lottery ticket hypothesis. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Orseau, L.; Hutter, M.; and Rivasplata, O. 2020. Logarithmic pruning is all you need. Advances in Neural Information Processing Systems, 33: 29252934. Otsuka, H.; Chijiwa, D.; Garcıa-Arias, A. L.; Okoshi, Y.; Kawamura, K.; Chu, T. V.; Fujiki, D.; Takeuchi, S.; and Motomura, M. 2025. Partially frozen random networks contain compact strong lottery tickets. Transactions on Machine Learning Research. Zhong, Z.; and Andreas, J. 2024. Algorithmic capabilities of random transformers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Zhou, H.; Lan, J.; Liu, R.; and Yosinski, J. 2019. Deconstructing lottery tickets: Zeros, signs, and the supermask. Advances in neural information processing systems, 32. Pensia, A.; Rajput, S.; Nagle, A.; Vishwakarma, H.; and Papailiopoulos, D. 2020. Optimal lottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in neural information processing systems, 33: 2599 2610. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9. Ramanujan, V.; Wortsman, M.; Kembhavi, A.; Farhadi, A.; and Rastegari, M. 2020. Whats hidden in randomly weighted neural network? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1189311902. Shen, S.; Baevski, A.; Morcos, A.; Keutzer, K.; Auli, M.; and Kiela, D. 2021a. Reservoir transformers. In Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 4294 4309. Online: Association for Computational Linguistics. Shen, S.; Yao, Z.; Kiela, D.; Keutzer, K.; and Mahoney, M. 2021b. Whats hidden in one-layer randomly weighted transformer? In Moens, M.-F.; Huang, X.; Specia, L.; and Yih, S. W.-t., eds., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2914 2921. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Virtanen, P.; Gommers, R.; Oliphant, T. E.; Haberland, M.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.; Weckesser, W.; Bright, J.; van der Walt, S. J.; Brett, M.; Wilson, J.; Millman, K. J.; Mayorov, N.; Nelson, A. R. J.; Jones, E.; Kern, R.; Larson, E.; Carey, C. J.; Polat, I.; Feng, Y.; Moore, E. W.; VanderPlas, J.; Laxalde, D.; Perktold, J.; Cimrman, R.; Henriksen, I.; Quintero, E. A.; Harris, C. R.; Archibald, A. M.; Ribeiro, A. H.; Pedregosa, F.; van Mulbregt, P.; and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental algorithms for scientific computing in python. Nature Methods, 17: 261272. Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.; and Rush, A. 2020. HuggingFaces transformers: State-of-the-art natural language processing. In Liu, Q.; and Schlangen, D., eds., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 3845. Online: Association for Computational Linguistics. Xiong, Z.; Liao, F.; and Kyrillidis, A. 2023. Strong lottery ticket hypothesis with ε-perturbation. In International Conference on Artificial Intelligence and Statistics, 68796902. PMLR. Proofs of Main Theorems This section presents the detailed proofs of the main theorems in the manuscript. We first introduce two lemmas: one for approximating target weight matrix by pruning two random weight matrices, and another for bounding the effect of perturbations in the softmax function. These lemmas are then used to establish the SLTH for attention mechanisms. Then, leveraging the theory of the SLTH for attention mechanisms, we prove the existence of SLTs in transformer blocks and transformers without normalization layers. A.1 Weight Approximation Pensia et al. (2020) have shown that two-layer fully-connected ReLU network can approximate arbitrary matrices with high probability. Our problem setting can be viewed as simplified version of their construction, in which the ReLU nonlinearity is omitted. We follow their proof strategy and simplify it to the linear (non-activated) case. Lemma 7. Let Rd2d1 be target matrix with entries in [1, 1]. Let 1 Rnd1 and 2 Rd2n be source random matrices whose entries are drawn i.i.d. from [1, 1]. For any 0 < ϵ < 1, suppose that d1C log( d1d2 ϵ ) for some universal constant > 0. Then, with probability at least 1 ϵ, there exists choice of binary masks 1 {0, 1}nd1 and 2 {0, 1}d2n such that Proof. Firstly, we structurally prune the random weight matrix 1 by the pruning mask 1: (cid:13) (cid:13) (cid:13)W ( 2 2)( 1 1) (cid:13) (cid:13) (cid:13)max ϵ d1d2 . 1 1 = u1 0 0 u2 ... ... 0 0 , 0 0 ... . . . ud1 where ui Rn . Next, we decompose 2 2 as follows: 2 2 = (v1,1 m1,1) (v2,1 m2,1) ... . . . (vd2,1 md2,1) (vd2,2 md2,2) (v1,2 m1,2) (v2,2 m2,2) ... (6) (7) (v1,d1 m1,d1) (v2,d1 m2,d1) ... (vd2,d1 md2,d1) , where vi,j Rn and mi,j {0, 1}n ( 2 2)( 1 1) = . These operations enable us to rewrite the product of Equations (6) and (7) as follows: (v1,1 m1,1) u1 (v2,1 m2,1) u1 ... (vd2,1 md2,1) u1 (v1,2 m1,2) u2 (v2,2 m2,2) u2 ... (vd2,2 md2,2) . . . (v1,d1 m1,d1 ) ud1 (v2,d1 m2,d1 ) ud1 ... (vd2,d1 md2,d1) ud1 (8) We focus on the (i, j)-th entry of Equation (8). This entry can be rewritten as subset sum of element-wise products between the vectors vi,j and uj: (vi,j mi,j) uj = (cid:88) k=1 mi,j,kvi,j,kuj,k. Here, each mi,j,k determines whether the corresponding product vi,j,kuj,k is included in the subset sum. We aim to approximate the (i, j)-th entry of the target weight matrix with the subset sum (cid:80)n k=1 mi,j,kvi,j,kuj,k by appropriately choosing the binary mask mi,j. Since each entry of vi,j,k and uj,k is drawn i.i.d. from [1, 1], each product vi,j,kuj,k can be viewed as drawn from the distribution including some uniform distribution; thus, we can apply Corollary 3.3 of Lueker (1998), which states that if log (cid:0) d1d2 , there exists binary mask vector mi,j such that the subset sum (cid:80)n (cid:1), then with probability at least 1 ϵ d1d2 ϵ k=1 mi,j,kvi,j,kuj,k approximates the (i, j)-th entry of within an error of d1d2 all entries of the weight matrix are simultaneously approximated is at least 1 ϵ: . By the union bound, the probability that ϵ 1 d2(cid:88) d1(cid:88) i= j=1 ϵ d1d2 = 1 ϵ. Therefore, if = d1n d1C log (cid:0) d1d2 (cid:1), then with probability at least 1 ϵ, the following inequality holds: (cid:13) (cid:13) (cid:13)W ( 2 2)( 1 1) (cid:13) (cid:13) (cid:13)max . ϵ ϵ d1d2 A.2 Spectral Norm of Softmax Difference In addition to approximating target weights, we need to analyze the stability of the softmax output under small input perturbations, with respect to the spectral norm of the resulting attention-weighted output. Lemma 8. Given ϵ Rd1 as perturbation vector such that ϵmax ϵmax for some ϵmax 0, we have Proof. Let pi := σ(xi; ai) and := σ(xi + ϵ; ai). Then, for each coordinate j, we have σ(xi; ai)X σ(xi + ϵ; ai)X (cid:112) d1α (exp(2ϵmax) 1) . i,j = pi,j exp(ϵj) , = (cid:88) k=1 pi,k exp(ϵk). By the assumption ϵmax ϵmax, we have the following bound: (cid:12) (cid:12) (cid:12) (cid:12) 1 exp(ϵj) (cid:12) (cid:12) (cid:12) (cid:12) exp(2ϵmax) 1. (9) Now, we can bound the spectral norm for the i-th input embedding: piX iX (cid:112) d1 piX iXmax (cid:112) d1 max j[d1] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) k=1 (pi,k i,k)xk,j (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:112) d1 max j[d1] (cid:88) k= xk,j pi,k i,k (cid:112) d1 α (cid:112) d1 α (cid:88) k=1 (cid:88) k= pi,k i,k pi,k (cid:12) (cid:12) (cid:12) (cid:12) 1 exp(ϵk) (cid:12) (cid:12) (cid:12) (cid:12) (cid:112) d1 α (exp(2ϵmax) 1) (cid:88) k= pi,k (Using Equation (9)) d1 α (exp(2ϵmax) 1) . This upper bound is independent of i; thus, the upper bound of maxi[T ] pX pX is same as the final upper bound. = (cid:112) A.3 SLT Existence within Attention Mechanisms By leveraging these two lemmas, we prove the following theorem: Theorem 9. Let AttnS() and AttnT() be as defined in Equations (1) and (2). Assume α max( , (j) Then, with probability at least 1 ϵ, there exists choice of binary masks (j) , (j) , (j) d1, that satisfy d2) for the inputs. if the source dimensions satisfy max i[T ] AttnS(xi) AttnT(xi) ϵ, n1 d1C log n2 d1C log for some universal constant > 0. (cid:32) 1 8Hα3d3/2 ϵ (cid:18) 2Hαd1 ϵ (cid:33) , d2 (cid:19) , Proof. We prove the theorem in three steps. Step 1: Weight Merging. We begin by merging the weight matrices of the target and source MHAs. The target MHA weights are merged as (j) (j) QK := 1 dK (j) VO := (j) . (j) (W (j) ), This operation (Equations (10) and (11)) enables us to represent each head of AttnT() as AttnT(xi; X, (1:H) Q:O ) = (cid:16) σ (cid:88) j=1 (j) QKX ; ai (cid:17) XW (j) VO. From the assumption on the target weights, we have the following norm bounds: For the source MHA, we incorporate the scaling factor 1/ nK into the query and key weight matrices: (cid:112) dK, QK 1/ (j) (j) VO 1. (j) := (j) := (j) , (j) . 1 d1/4 1 d1/4 Assuming that each entry of W (j) is drawn i.i.d. from [1, 1]. (j) and (j) is drawn i.i.d. from [d1/4 , d1/4 ], each entry of the scaled matrices Step 2: Weight Approximation. From Lemma 7, for any 0 < ϵ < 1, if nK d1C log (cid:32) 8Hα3d3/2 ϵ 1 (cid:33) , then with probability at least 1 ϵ 8Hα3 d1 , there exists choice of binary masks (j) and (j) such that (cid:13) (cid:13) (j) (cid:13) (cid:13) QK (cid:16) (j) (j) (cid:17) (cid:16) (j) (j) (cid:17)(cid:13) (cid:13) (cid:13) (cid:13)max ϵ 8Hα3d3/2 1 . This inequality Equation (14) implies bound on the softmax input: (10) (11) (12) (13) (j) and (14) (cid:13) (cid:13) (j) QKX x (cid:13) (cid:13) (cid:12) (cid:12) (cid:12) (cid:12) (j) = max k[T ] (cid:13) (cid:13) (j) (cid:13) (cid:13) α2 QK (cid:16) (j) (j) (cid:17) (cid:16) (j) (j) (cid:17) QKxk (cid:16) (j) (j) (cid:17) (cid:16) (cid:16) (j) (j) (cid:17) (cid:16) (j) (j) (cid:16) (j) (j) (cid:17) (cid:16) (j) (j) (j) (j) (cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:17)(cid:13) (cid:13) (cid:13) (cid:13)max (cid:13) (cid:13) (cid:13) (cid:13) (cid:17) xk (cid:12) (cid:12) (cid:12) (cid:12) α2d1 α2d1 (cid:13) (cid:13) (j) (cid:13) (cid:13) QK ϵ 8Hα3d3/2 ϵ 8Hα d1 1 . = Let (cid:16) p(j) := σ (j) QKX ; ai (cid:17) , Applying Lemma 8, we obtain p(j) := σ (cid:16) ( x (j) (j) )( (j) (j) )X ; ai (cid:17) . (cid:13) (cid:13)p(j) (cid:13) p(j) (cid:112) (cid:13) (cid:13) (cid:13) (cid:18) (cid:18) d1α exp ϵ 2H . ϵ 4Hα d1 (cid:19) (cid:19) (Using 0 < ϵ 4Hα d1 < 1) For the value and output weights, from Lemma 7, if nV d1C log (cid:18) 2Hαd1 ϵ2 (cid:19) d2 , then with probability at least 1 (cid:13) (cid:13)W (j) (cid:13) d2ϵ 2Hα , there exists choice of binary pruning masks (j) and (j) ϵ VO ( 2Hαd1 (j) ) (j) (cid:13) (cid:13) (cid:13)max )( d2 (j) (j) . such that Step 3: Total Error Analysis. We now bound the difference between the outputs of the source and target MHAs: AttnT(xi) AttnS(xi) = (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) j=1 j=1 (cid:16) XW (j) p(j) VO p(j) X( (j) (j) )( (cid:16) (cid:13) (cid:13) (cid:13) XW (j) p(j) VO p(j) X( (j) (j) )( (j) (j) ) (cid:17) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (j) (j) ) (cid:17)(cid:13) (cid:13) (cid:13) . We apply the triangle inequality: (cid:13) XW (j) (cid:13)p(j) (cid:13) (cid:13) (cid:13)(p(j) (cid:13) p(j) For the first term, by using (j) VO p(j) (j) X( (cid:13) (cid:13) (cid:13) + (j) (cid:13) (cid:13)p(j) (cid:13) )( X(W (j) )XW (j) VO (j) (cid:13) (j) (cid:13) ) (cid:13) (j) (j) VO ( )( (j) (j) (cid:13) (cid:13) (cid:13) . )) (cid:13) (cid:13)(p(j) (cid:13) )XW (j) VO p(j) VO 1 (Equation (13)), we obtain (cid:13) (cid:13)(p(j) (cid:13) (cid:13) (cid:13)(p(j) (cid:13) ϵ . 2H (cid:13) (cid:13) (cid:13) p(j) p(j) (cid:13) (cid:13)W (j) (cid:13) VO (cid:13) (cid:13) (cid:13) )X )X (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (15) (16) For the second term, we obtain the following result by using Equation (15): (j) )( (j) (j) (j) (cid:13) (cid:13)W (j) (cid:13) (j) d1d (j) VO ( )( (j) (cid:13) (cid:13) )) (cid:13) (j) )( (cid:13) (j) (cid:13) ) (cid:13)max (j) (j) (j) ) (cid:13) X(W (j) (cid:13)p(j) VO ( (cid:13) (cid:13) (cid:13) (cid:112) (cid:112) (cid:13)p(j) (cid:13) (cid:13) d1 (cid:13) (cid:13) (cid:13)W (j) VO ( (cid:13) d2α ϵ 2Hαd1 d1 d1 d2α d2 (cid:112) (cid:112) = ϵ 2H . (cid:13) (cid:13) (cid:13)max (Using Equation (15)) (17) These results of Equations (16) and (17) do not depend on the input index i; thus, adding the two terms across heads gives max i[T ] AttnT(xi) AttnS(xi) Finally, using the union bound and the assumption α max( d1, (cid:88) j=1 (cid:16) ϵ 2H + (cid:17) ϵ 2H = ϵ. d2), the probability that all approximations hold is 1 ϵ 8Hα3 d1 d2ϵ 2Hα 1 ϵ. A.4 SLT Existence Within Transformer Blocks By combining the SLT existence theorem for attention mechanisms (Theorem 9) and for multi-layer fully-connected ReLU networks (FC) proven by Pensia et al. (2020) (Theorem 10), we prove the SLT existence theorem for transformer blocks. Theorem 10 (Theorem 1 in Pensia et al. (2020)). Let FT (xi) = LReLU(W L1 . . . ReLU(W 1xi)) be target FC with layers. Assume that each weight matrix Rdl+1dl satisfies l 1 for all = 1, . . . , L. Consider pruned source FC with 2L layers defined as FS (xi) = (cid:16) 2L 2L (cid:17)"
        },
        {
            "title": "ReLU",
            "content": "(cid:16)(cid:16) 2L1 2L1 (cid:17) . . . ReLU (cid:16)(cid:16) 2L1 2L1 (cid:17) xi (cid:17)(cid:17) , where 2l1 Rnldl and 2l Rdl+1nl for = 1, . . . , L, and each entry of is drawn i.i.d. from [1, 1]. Then, with probability at least 1 ϵ for any 0 < ϵ < 1, there exists choice of binary pruning masks 1, . . . , 2L that holds the following inequality: if each source dimension nl satisfies for some universal constant > 0. FS(xi) FT(xi) exp (cid:17) (cid:16) αϵ 2 1, nl dlC log 4Ldldl+1 ϵ , We now state the main result for transformer blocks, which follows from combining the two SLT existence theorems. Theorem 11. Let BlkT(xi) = FT(AttnT(xi) + xi) + AttnT(xi) + xi be target transformer block of an MHA AttnT() and FC FT() with layers. For simplicity, we assume each layer of target FC dimensions is all d1. Let BlkS(xi) = FS(AttnS(xi) + xi) + AttnS(xi) + xi be pruned random source transformer block of pruned random MHA AttnS() and FC FS() with 2L layers. Assume that the input dimension of each even layer of the source FC is nFC, and the input dimension of each odd layer is d1. Furthermore, for simplicity, we assume key and value dimensions of AttnS() are the same dimension nMHA. Then, with probability at least 1 ϵ for 0 < ϵ < 1, there exists choice of binary masks that satisfies if the hidden dimensions of the source MHA and FC satisfy max i[T ] BlkS(xi) BlkT(xi) ϵ, nMHA d1C log nFC d1C log (cid:32) (cid:32) (cid:33) 3 2 1 , 32α3Hd ϵ 5 24αLHd 2 1 ϵ (cid:33) , for some universal constant > 0. Proof. Our proof strategy is first to apply the SLT existence theorem to the attention mechanism, and then apply the result for FCs. From Theorem 9, with probability at least 1 ϵ 4 , there exists choice of binary masks so that AttnS() satisfies AttnS(xi) AttnT(xi) ϵ 4 . (18) This inequality Equation (18) implies AttnS(xi) AttnT(xi) = AttnS(xi) ϵ 4 ϵ 4 , + AttnT(xi) + αH (cid:112) d1. ϵ 4 Therefore, the norm of the input vector of the source FC satisfies AttnS(xi) + AttnS(xi) + α + α(H (cid:112) d1 + 1) ϵ 4 3αH (cid:112) d1. (19) Assume that this upper bound of Equation (19) holds. Now, applying Theorem 10 to the source FC, if nFC d1C log = d1C log (cid:18) 4Ld2 1 6αH ϵ (cid:19) d1 (cid:32) 5 24αLHd 2 1 ϵ (cid:33) , with probability at least 1 , there exists choice of binary pruning masks so that FS() satisfies ϵ 6αH d1 FS(AttnS(xi) + xi)FT(AttnS(xi) + xi) (cid:19) (cid:18) 3αH d1ϵ d1 1 exp = exp 2 6αH (cid:17) 1. (cid:16) ϵ 4 Finally, we bound the total error between the source and target transformer blocks: max i[T ] BlkS(xi) BlkT(xi) = max i[T ] FS(AttnS(xi) + xi) + AttnS(xi) + FT(AttnT(xi) + xi) AttnT(xi) (cid:16) max i[T ] FS(AttnS(xi) + xi) FT(AttnS(xi) + xi) + FT(AttnS(xi) + xi) FT(AttnT(xi) + xi) + AttnS(xi) AttnT(xi) (cid:17) (cid:17) 1 + max i[T ] 2AttnS(xi) AttnT(xi) (cid:16) ϵ 4 ϵ 2 + exp ϵ 2 = ϵ. From union bound, the probability that this approximation holds is at least 1 ϵ: 1 ϵ ϵ 6αH d1 1 ϵ. A.5 SLT Existence Within Transformers Without Normalization Layers By exploiting the SLT existence theorem for transformer blocks (Theorem 11), we prove the SLT existence theorem for transformers without normalization layers. We firstly prove the two lemmas used in the proof of the theorem. ] be perturbed input matrix, which satisfies maxi[T ] xi i ϵmax Then, an Lemma 12. Let = [x 1, . . . , arbitrary target MHA AttnT() holds the following inequality: (cid:112) AttnT(xi)AttnT(x i) d1(α (exp(4αϵmax) 1) + ϵmax). Proof. We begin by analyzing the upper bound of differences for different inputs: W (j) QKX W (j) QKX max = max k[T ] W (j) QKxk W (j) QKx x (j) QKxk W (j) QKxk + W (j) xi iW (j) QKxk + iW (j) QKxk k (j) QKxk (cid:17) QKx (cid:17) (cid:16) (cid:16) max k[T ] max k[T ] αϵmax + αϵmax = 2αϵmax. Applying Lemma 8, the following inequality holds: σ(x (j) σ(x = σ(x (cid:112) VO σ(x QKX ; ai)XW (j) (j) (j) QKX ; ai)X σ(x QKX ; ai)X σ(x (j) QKX ; ai)X (j) VO QKX ; ai)X + σ(x QKX ; ai)X + σ(x (j) (j) d1ϵmax. (cid:112) d1α (exp(4αϵmax) 1) + (j) (j) QKX ; ai)X σ(x QKX ; ai)(X ) (j) QKX ; ai)X Then, we have the following bound: AttnT(xi) AttnT(x) = (cid:88) σ(x (j) QKX ; ai)XW (j) VO (cid:88) j=1 σ(x (j) QKX ; ai)X (j) VO j=1 (cid:112) d1(α (exp(4αϵmax) 1) + ϵmax). Lemma 13. An arbitrary target Attention block BlkT() holds the following inequality: BlkT(xi)BlkT(x i) (cid:112) d1(α (exp(4αϵmax) 1) + 2ϵmax). Proof. From Lemma 12, we have the upper bound as follows: BlkT(xi) BlkT(x i) = FT(AttnT(xi) + xi) + AttnT(xi) + xi FT(AttnT(x i) + i) AttnT(x i) i i) + xi i i) + FT(AttnT(xi) + xi) FT(AttnT(x i) + 2xi 2AttnT(xi) AttnT(x i) + AttnT(xi) AttnT(x (cid:112) (cid:112) 2H 2H d1(α(exp(4αϵmax) 1) + ϵmax) + 2ϵmax d1(α(exp(4αϵmax) 1) + 2ϵmax) Theorem 14. Assume 2, and let Tf T(xi) := Blk(B) (Blk(B1) . . . Blk(1) (xi)) be target transformer with blocks. Let (Blk(B1) be pruned random transformer with layers. Then, with probability at least 1 ϵ for 0 < ϵ < 1, there exists choice of binary masks that satisfies Tf S(xi) := Blk(B) ...Blk(1) (xi)) if the hidden dimensions of the b-th source MHA and FC satisfy max i[T ] Tf S(xi) Tf T(xi) ϵ, n(b) MHA d1C log n(b) FC d1C log (cid:32) (cid:32) cf1(b,B) 1 f2(b,B)df3(b,B) 1 (cid:33) ϵ cg1(b,B) 2 LH g2(b,B)dg3(b,B) 1 (cid:33) ϵ for some universal constant > 0 and constants c1, c2 > 0 including α. Here, f1, f2, f3 and g1, g2, g3 are quadratic functions of B, b. Proof. We analyze the approximation errors in each block sequentially and identify the accumulated error in the last block. Notation for the Proof: Let x(b) be an input vector to the b-th target block: x(b) = (cid:40)xi if = 1, Blk(b1) (x(b1) ) if 2 B. Then, the final output of the target transformer is Tf T(xi) = BlkT(x(B) ). The spectral norm of these input vectors is x(b) = xi = α =: β1 Blk(b1) (x(b1) (cid:112) ) d1 + 1)x(b1) (cid:112) d1 + 1))b1 d1)b (cid:112) 2(H α(2(H α(4H =: βb if = 1, if 2 B. Similarly, let x(b) be an input vector to the b-th source block: x(b) = (cid:40)xi if = 1, Blk(b1) (x(b1) ) if 2 B. Then, the final output of the source transformer is Tf S(xi) = BlkS(x(B) First Block Error: From Theorem 11, if ). n(1) MHA d1C log n(1) FC d1C log 32β3 1 Hd 3 1 2B1 (cid:81)B j=2 16H ϵ d1β2 , 24β1LHd 5 2 1 2B1 (cid:81)B j=2 16H ϵ d1β2 , then with probability at least 1 2B1 (cid:81)B ϵ j=2 16H d1β2 , the following inequality holds independently of the input index i: x(2) x(2) = Blk(1) (xi) (xi) Blk(1) ϵ j=2 16H d1β2 2B1 (cid:81)B This inequality Equation (20) implies the upper bound of x(2) : x(2) x(2) = x(2) 2B1 (cid:81)B 2B1 (cid:81)B 2B1 (cid:81)B ϵ j=2 16H ϵ j=2 16H ϵ j=2 16H d1β2 d1β2 d1β2 + x(2) + β2 . (20) (From the triangle inequality.) Second Block Error: We assume the approximation of the first block is successful (i.e., Equation (21) holds). Then, from Lemma 13, the following bound holds: 2β2. (21) Blk(2) (x(2) (cid:32) (cid:112) d1 β exp ) Blk(2) (x(2) (cid:32) (cid:32) ) 4β2ϵ j=2 16H (cid:33) (cid:33) 1 + 2B1 (cid:81)B 2ϵ j=2 16H d1β2 (cid:33) d1β2 2B1 (cid:81)B = (cid:112) d1 (cid:32) (cid:32) β2 exp 1 d1β 4H 2B1 (cid:81)B ϵ j=3 16H d1β2 (cid:33) β2 + 1 d1β2 8H (cid:33) ϵ j=3 16H 2B1 (cid:81)B (cid:33) d1β2 + d1β2 8H 1 d1β2 2B1 (cid:81)B ϵ j=3 16H d1β2 (exp(x) 2x + 1 if 0 1.) (cid:112) d1 (cid:32) 1 2H = 1 2 ϵ 2B1 (cid:81)B j=3 16H ϵ j=3 16H 2B1 (cid:81)B ϵ j=3 16H 1 8β2 + 2B1 (cid:81)B d1β2 From Theorem 11, if 2B1 (cid:81)B ϵ j=3 16H d1β2 . d1β2 n(2) MHA d1C log n(2) FC d1C log 32(2β2)3Hd 3 2 1 2B1 (cid:81)B ϵ j=3 16H d1β2 24(2β2)LHd 5 2 1 2B1 (cid:81)B ϵ j=3 16H d1β2 , then with probability at least 1 2B1 (cid:81)B ϵ j=3 16H d1β2 , the following inequality holds: Blk(2) (x(2) Therefore, we have ) Blk(2) (x(2) ) 2B1 (cid:81)B ϵ j=3 16H . d1β2 x(3) x(3) ) Blk(2) ) Blk(2) ) Blk(2) (x(2) (x(2) (x(2) + T (x(2) ) ) + Blk(2) (x(2) ) + Blk(2) ϵ j=3 16H 2B1 (cid:81)B d1β2 ) Blk(2) (x(2) ) (x(2) ) Blk(2) 3 = Blk(2) = Blk(2) Blk(2) T (x(2) (x(2) (x(2) ϵ j=3 16H ϵ j=3 16H 2B1 (cid:81)B 2B2 (cid:81)B = d1β2 . d1β2 ) (22) This inequality Equation (22) implies the upper bound of x(3) : x(3) x(3) = x(3) 2B2 (cid:81)B 2B2 (cid:81)B 2B2 (cid:81)B ϵ j=3 16H ϵ j=3 16H ϵ j=3 16H d1β2 d1β2 d1β2 + x(3) + β3 (From the triangle inequality.) Third Block Error: We assume the approximation of second block is succeessful (i.e., Equation (23) holds). Then, from Lemma 13, the following bound holds: 2β3. (23) Blk(3) (x(3) (cid:32) (cid:112) d1 (x(3) ) Blk(3) (cid:32) (cid:32) ) β3 exp 4β ϵ j=3 16H (cid:33) (cid:33) 1 + 2 d1β2 2B2 (cid:81)B = (cid:112) d1 (cid:32) (cid:32) (cid:32) β3 exp 1 d1β3 4H 2B2 (cid:81)B ϵ j=4 16H d1β2 (cid:33) 2B2 (cid:81)B (cid:33) 1 + 8H ϵ j=3 16H 1 d1β3 (cid:33) d1β2 2B2 (cid:81)B ϵ j=4 16H d1β2 (cid:33) + d1β2 8H 1 d1β 2B2 (cid:81)B ϵ j=4 16H d1β2 (cid:33) (exp(x) 2x + 1 if 0 1.) (cid:112) d1 (cid:32) 1 2H = 1 2 ϵ 2B2 (cid:81)B j=4 16H ϵ j=4 16H 2B2 (cid:81)B ϵ j=4 16H 1 8β3 + 2B2 (cid:81)B d1β2 From Theorem 11, if 2B2 (cid:81)B ϵ j=4 16H d1β2 . d1β2 n(3) MHA d1C log n(3) FC d1C log 32(2β3)3Hd 3 2 1 2B2 (cid:81)B ϵ j=4 16H d1β2 24(2β3)LHd 5 2 1 2B2 (cid:81)B ϵ j=4 16H d1β2 , then with probability at least 1 2B2 (cid:81)B ϵ j=4 16H (x(3) Blk(3) d1β2 , the following inequality holds: ) Blk(3) (x(3) ) 2B2 (cid:81)B ϵ j=4 16H . d1β2 Therefore, we have i x(4) = Blk(3) = Blk(3) Blk(3) x(4) 3 (x(3) (x(3) (x(3) ϵ j=4 16H ϵ j=4 16H 2B3 (cid:81)B 2B2 (cid:81)B = d1β2 . d1β2 ) Blk(3) ) Blk(3) ) Blk(3) (x(3) (x(3) (x(3) + i (x(3) ) ) + Blk(3) (x(3) ) + Blk(3) ϵ j=4 16H 2B2 (cid:81)B d1β2 ) Blk(3) (x(3) ) (x(3) ) Blk(3) This inequality Equation (25) implies the upper bound of x(4) : x(4) x(4) = x(4) 2B3 (cid:81)B 2B3 (cid:81)B 2B3 (cid:81)B ϵ j=4 16H ϵ j=4 16H ϵ j=4 16H d1β2 d1β2 d1β2 + x(4) + β4 ) (24) (25) (From the triangle inequality.) 2β4. (26) (B 1)-th Block Error: By repeating the same proof procedure as above in each block, we can propagate the error to (B 1)-th block. We assume all first-to-(B 2)-th block approximations are successful. Then, from Lemma 13, the following bound holds: (x(B1) (cid:32) ) Blk(B1) (cid:32) (cid:32) (x(B1) ) Blk(B1) (cid:112) d1 βB1 exp (cid:18) (cid:18) (cid:18) (cid:112) = d1 βB exp (cid:112) d1 (cid:18) 1 d1 2H (cid:33) (cid:33) 1 + 4βB1 22 (cid:81)B 1 d1βB1 4H ϵ 22 16H d1β2 d1β2 ϵ j=B1 16H ϵ 22 16H 1 d1βB1 8H + d1β2 (cid:19) (cid:19) 1 + 8H (cid:19) ϵ 22 16H d1β2 ϵ 22 (cid:81)B j=B1 16H 1 d1βB1 d1β2 ϵ 22 16H (cid:33) (cid:19) d1β2 (exp(x) 2x + 1 if 0 1.) 1 2 ϵ 22 16H + 1 8βB1 ϵ 22 16H d1β2 d1β2 = ϵ 22 16H From Theorem 11, if . d1β2 n(B1) MHA d1C log n(B1) FC d1C log (cid:32) (cid:32) 32(2βB1)3Hd (cid:33) d1β2 3 2 1 22 16H ϵ 24(2βB1)LHd 5 1 22 16H ϵ d1β2 (cid:33) , then with probability at least 1 ϵ 2216H d1β2 , the following inequality holds independently of the input index i: Blk(B1) (x(B1) ) Blk(B1) (x(B1) ) ϵ 22 16H . d1β2 x(B) x(B) Therefore, we have the following inequality: (x(B1) (x(B1) (x(B1) = Blk(B1) = Blk(B1) Blk(B1) T 3 ) Blk(B1) ) Blk(B1) ) Blk(B1) (x(B1) (x(B1) (x(B1) ) ) + Blk(B1) ) + Blk(B1) (x(B1) (x(B1) ) Blk(B1) ) Blk(B1) (x(B1) ) (x(B1) ϵ 22 16H d1β2 = ϵ 22 16H ϵ 2 16H + d1β2 . d1β2 ) (27) This inequality Equation (27) implies the upper bound of x(B) : x(B) x(B) = x(B) ϵ 2 16H ϵ 2 16H ϵ 2 16H d1β2 d1β2 d1β2 + x(B) + βB (From the triangle inequality.) Final Block Error: We assume all first-to (B 1)-th block approximations are successful. Then, from Lemma 13, the following bound holds: 2βB. (28) (cid:19) (cid:19) 1 + 2 (cid:19) ϵ 2 16H (cid:19) d1β2 1 16H ϵ 2 16H 1 d1βB 1 (cid:19) ϵ ϵ 16H d1βB d1β2 (cid:19) 1 + (cid:19) ϵ d1βB (exp(x) 2x + 1 if 0 1.) Blk(B) (x(B) (cid:18) (cid:112) ) Blk(B) (x(B) (cid:18) (cid:18) ) βB exp 4βB (cid:112) = (cid:18) (cid:18) (cid:18) d1 βB exp 8H (cid:18) (cid:112) d1 1 4H ϵ + ϵ + 1 16βB ϵ . = 1 4 ϵ 2 From Theorem 11, if n(B) MHA d1C log (cid:32) 32(2βB)3Hd ϵ (cid:33) 3 2 1 2 , n(B) FC d1C log (cid:32) 24(2βB)LHd ϵ (cid:33) 5 1 2 , then with probability at least 1 ϵ 2 , the following inequality holds independently of the input index i: Blk(B) (x(B) ) Blk(B) (x(B) ) ϵ . Therefore, we finally obtain the following inequality: (x(B) (x(B) (x(B) ϵ 2 Tf T(xi) Tf S(xi) = Blk(B) = Blk(B) Blk(B) + i ϵ 2 = ϵ. ) Blk(B) ) Blk(B) ) Blk(B) (x(B) (x(B) (x(B) ) ) + Blk(B) ) + Blk(B) (x(B) ) Blk(B) (x(B) ) Blk(B) (x(B) ) (x(B) ) Success Probability of the Approximation: By the union bound, the probability that Tf T(xi) Tf S(xi) ϵ holds is at least 1 ϵ: d1β2 2B1 (cid:81)B ϵ j=3 16H d1β2 2B2 (cid:81)B ϵ j=4 16H d1β2 ϵ 22 16H d1β2 ϵ 2 1 2B1 (cid:81)B ϵ j=2 16H (cid:81)k = 1 2 + (cid:80)B k=2 2 (cid:81)B j=2 32H d1β2 ϵ j=2 32H d1β2 = 1 1 2 (cid:81)B j=2 32H 1 + (cid:80)B (cid:81)k j=2 32H d1β2 ϵ j=2 32H d1β2 ϵ d1β2 (cid:81)k j=2 32H k=2 2 (cid:81)B d1β2 ϵ j=2 32H k=2(32Hα2 d1β2 d1)k1 (cid:81)k 1 + (cid:80)B k=2 2 (cid:81)B 1 + (cid:80)B 2(32Hα2 d1)B1 (cid:81)k j=2(16H 2d1)j1 j=2(16H 2d1)j1 ϵ 1 = 1 = 1 1 1 1 = 1 1 = 1 = 1 ϵ ϵ ϵ ϵ ϵ ϵ 1 64 1 64 1 64 1 64 1 1 64 1 64 ϵ ϵ 1 64 1 64 1055 1984 ϵ 1 ϵ. d1(16H 2d1) 1 2 k)k1 2 B(B1) ϵ 2 B)k1 2 B(B1) ϵ k=2(32Hα2 1 + (cid:80)B 2(32Hα2 (cid:80)B k=1(32Hα2 d1)B1(16H 2d1) 1 d1(16H 2d1) 1 d1)B1(16H 2d1) 1 d1(16H 2d1) 1 d1(16H 2d1) 1 d1(16H 2d1) 1 d1(16H 2d1) 1 2 B)B 1 2 1) 2 B)B 2 1) 2(32Hα2 (32Hα2 (32Hα2 (32Hα2 (32Hα2 1 (cid:19) ϵ d1(16H 2d1) 1 2 ϵ (cid:18) 1 32Hα2 (cid:1) ϵ 1 2 (cid:0)1 1 16 32 ϵ 32 2(32Hα2 1 d1)B1(16H 2d1) 2 B(B1) ϵ 2(32Hα2 1 d1)B1(16H 2d1) 2 B(B1) ϵ Experimental Details This section describes the detailed experimental settings. All experiments can be verified with four NVIDIA H100 SXM5 94GB GPUs. B.1 Synthetic Data Experiment We construct synthetic dataset for angular velocity estimation, where each input sequence consists of two-dimensional vectors x1, ..., xT such that xt = (cos(ωt + θ0), sin(ωt + θ0)) for some angular velocity ω [π, π] and initial phase θ0 [0, π]. The task is to estimate ω given the full sequence. Each sequence includes special regression tokensimilar to the CLS token in BERT (Devlin et al. 2019)at the beginning, and the model is trained to predict angular velocity by the regression token initialized to zero. We generate 10, 000 samples each for training, validation, and test sets, and input sequence lengths vary from 4 to 256 during training. We experiment with MHAs and transformers. In the MHA experiment, both the source and target MHAs are configured as single-head attention modules, with input and output dimensions of 2 and 1, respectively. The networks are trained using the AdamW optimizer (Loshchilov and Hutter 2019) with batch size of 1024 and learning rate of 0.1. Each target MHA is trained for 25 epochs with weight decay set to 0.01. In the transformer experiment, both the source and target models follow the construction described in Section 3.3. Each MHA has single attention head, and both its input and output dimensions are set to 2. The same regression token is used for both the source and target models to ensure that the approximation quality reflects differences in the behavior of the models rather than token-level discrepancies. The query and key dimensions of the target models are set to 8. Target networks are initialized according to the assumptions of our theoretical results. Specifically, entries of the query and key projection matrices are drawn i.i.d. from [n1/4 ], and those of the value and output projection matrices from [1, 1]. The weights in fully-connected networks are also initialized with [1, 1]. Source networks are initialized with Xavier uniform distribution (Glorot and Bengio 2010). To identify SLTs, we use the weight approximation method in Lemma 4, based on the subset-sum approximation technique of Pensia et al. (2020). For each target network, we generate 100 source networks with random initialization and solve the associated subset-sum problem using Gurobis mixed-integer programming solver (Gurobi Optimization, LLC 2024). In the experiments varying the hidden dimension, the input length is fixed at 4. We report the mean and standard deviation of the approximation error over these 100 candidates. We also fit exponential decay curves to the approximation error using SciPy (Virtanen et al. 2020). , n1/4 B.2 Language Modeling Experiment We further evaluate our theoretical framework in practical language modeling setting. Here, we search for SLTs using the edge-popup algorithm (Ramanujan et al. 2020), which searches for accurate subnetworks by assigning scores to each connection and retaining only the top-k% entries during training. We set this as 30. We train models from the GPT-2 family (Radford et al. 2019; Wolf et al. 2020) on the WikiText-103 dataset (Merity et al. 2017), using maximum sequence length of 1024. The weights of these models are initialized based on the GPT-2 initialization scheme: they are drawn i.i.d. from normal distribution with mean 0 and standard deviation 0.02. For the output projection in MHAs and the second layer of the fully-connected ReLU networks, the standard deviation is further scaled by (2b)1/2, where is the number of transformer blocks. We train the models for 50 epochs, with 227 steps per epoch. The AdamW optimizer is used with an initial learning rate of 0.0001, which is decayed to 0.00001 via cosine annealing scheduler (Loshchilov and Hutter 2017). linear learning rate warm-up is applied during the first epoch. For each model size, we repeat training with three different random seeds and report the mean and standard deviation of the best performance."
        }
    ],
    "affiliations": [
        "Institute of Science Tokyo",
        "NTT, Inc."
    ]
}