{
    "paper_title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
    "authors": [
        "Qi Mao",
        "Tinghan Yang",
        "Jiahao Li",
        "Bin Li",
        "Libiao Jin",
        "Yan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication."
        },
        {
            "title": "Start",
            "content": "UNDER REVIEW 1 UniMIC: Token-Based Multimodal Interactive Coding for HumanAI Collaboration Qi Mao, Tinghan Yang, Jiahao Li, Bin Li, Libiao Jin, Yan Lu 5 2 0 2 6 ] . [ 1 0 7 5 2 2 . 9 0 5 2 : r AbstractThe rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming humanAI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compresstransmitreconstruct pipelines. To address this limitation, we propose UniMIC, Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designsgeneric, masked, and text-conditionedeffectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (< 0.05 bpp), without compromising downstream task performance. These results establish UniMIC as practical and forward-looking paradigm for next-generation multimodal interactive communication. Index TermsMultimodal Interactive Coding, Ultra-Low Bitrate Compression, HumanAI Collaboration Compression, Token-Based Transmission. I. INTRODUCTION ecent advancements in artificial intelligence (AI), particularly Large Multimodal Models (LMMs) and autonomous AI agents, are fundamentally reshaping the paradigm of humanAI collaboration. As AI systems typically deployed in the cloudevolve from passive analytical tools to interactive collaborators (e.g., generative design assistants that co-create content with users or diagnostic agents that conduct multimodal consultations), the interaction pattern between humans and machines is shifting from one-way send receive communication to iterative, bidirectional, and multimodal dialogue. In such collaborative settings, edge devices (i.e., user terminals) transmit instructions and upload images for analysis, while cloud-based AI agents respond by generating new content (e.g., text or images) or performing multimodal reasoning according to the given instructions. This process inherently involves both humans and AI as senders and text Qi Mao, Tinghan Yang, and Libiao Jin are with the State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing 100024, China (e-mail: qimao@cuc.edu.cn; yangtinghan@cuc.edu.cn; libiao@cuc.edu.cn). Jiahao Li, Bin Li, and Yan Lu are with Microsoft Research Asia, Beijing 10080, China (e-mail: li.jiahao@microsoft.com; libin@microsoft.com; yanlu@microsoft.com). This work was done when Qi Mao was visiting scholar at Microsoft Research Asia. Fig. 1. Comparison of pixeland token-level transmission in human AI interaction. Illustrated with the inpainting task (text prompts omitted for simplicity). (a) Pixel-based pipelines accumulate distortion due to repeated image compressiondecompression. (b) Token-based pipelines exchange losslessly compressed tokens, preserving fidelity even at ultra-low bitrates. receivers, with repeated multimodal exchanges over multiple rounds, thereby demanding new communication paradigm for humanAI collaboration. However, existing compression paradigms are not wellsuited to this interactive setting. Most traditional frameworks are designed either for human perception (e.g., minimizing visual distortion) [1][5] or for machine understanding [6], [7] in limited recognition tasks, but they typically treat humans and machines as isolated endpoints. Recent works [8] [14] have begun to consider both humans and machines these designs still assume unidirectional, as receivers, yet unimodal transmission and fail to support iterative multimodal interaction. As result, conventional pipelines often compress raw pixels before sending them to cloud LLMs for processing, and then re-compress the generated results before delivering them back to users, causing repeated degradation and latency (see Fig. 1(a)). To address these limitations, we propose UniMIC, unified token-based multimodal interactive coding framework tailored for humanAI collaboration. Different from existing codecs that only optimize ratedistortion in pixels or features, UniMIC establishes an AI-native communication protocol where tokens serve as the native medium of exchange. This protocol enables edge devices and cloud agents to exchange only the task-relevant token subsets. For instance, as illustrated in Fig. 2, in inpainting, the edge transmits unmasked tokens and editing instructions, while the cloud returns only the generated tokens for masked regions; in outpainting, the edge uploads full tokens and the cloud transmits only extrapolated UNDER REVIEW 2 Fig. 2. Application scenarios of the proposed UniMIC framework. All tasks share the same token-based transmission pipeline but transmit different token subsets, enabling efficient and flexible multimodal communication. tokens back. Such task-adaptive and asymmetric transmission is fundamentally different from traditional compresstransmit reconstruct loops. UniMIC incurs only one-time tokenization thereby loss, while all subsequent exchanges are lossless, avoiding cumulative degradation and preserving semantic fidelity even at ultra-low bitrates, as shown in Fig. 1(b). Building on this protocol, we further develop lightweight Transformer-based entropy models specialized for different scenarios, including autoregressive, masked-token, and textconditional variants. These models reduce token redundancy and align probability estimation with generative statistics, ensuring efficiency across diverse multimodal tasks. Consequently, UniMIC supports broad range of downstream applications, including text-to-image (T2I) generation, textguided inpainting, outpainting, and visual question answering (VQA), with consistent gains in both compression efficiency and task performance."
        },
        {
            "title": "The main contributions of this work are summarized as",
            "content": "follows: We establish UniMIC, unified token-based multimodal interactive coding framework that treats tokens as the native medium of exchange between edge devices and cloud AI agents. This paradigm enables bidirectional, task-adaptive communication while avoiding cumulative degradation inherent in pixel-based codecs. We develop family of lightweight Transformer-based entropy models, including autoregressive, masked-token, and text-conditional variants, together with scenariospecific training strategies to minimize redundancy and adapt to diverse interaction settings. We extensively validate UniMIC on T2I generation, textguided inpainting, outpainting, and VQA, showing consistent bitrate savings and improved task performance over existing pixel-based baselines at ultra-low bitrates (< 0.05 bpp). II. RELATED WORK A. Image/Video Compression Classical image and video compression has evolved over several decades, resulting in standardized formats such as JPEG [1], JPEG2000 [2], MPEG-4, AVC/H.264 [3], AVS [4], and HEVC [5]. The latest standard, VVC [15], further improves ratedistortion efficiency through more flexible coding tools. However, classic codecs are built upon modular transpipeline in which each stageincluding prediction, form, quantization, and entropy codingrelies on handcrafted design choices. These modules are typically optimized in isolation, leading to locally optimal solutions that cannot fully exploit cross-module dependencies. The emergence of deep learning has enabled new paradigm in which the entire compression pipeline can be learned end-to-end. Deep neural network (DNN)-based codecs [16][18] replace handcrafted transforms and entropy models with trainable components, jointly optimized for global ratedistortion objectives. This approach offers better adaptability to image content and supports flexible bitrate control [19]. However, most DNN-based codecs remain primarily distortion-oriented and may struggle to preserve perceptual quality at extremely low bitrates. To address this, deep generative models have been integrated into learned compression frameworks. Generative adversarial networks (GANs) [20] excel at synthesizing highresolution details from compact latent codes [21][23], while diffusion-based models further improve perceptual fidelity in ultra-low bitrate regimes [24][26]. Nevertheless, both DNNand generative-based codecs largely follow compress-thenanalyze (CTA) paradigm [27], requiring full image reconstruction before any downstream visual analysis. This design introduces unnecessary decoding overhead and is suboptimal for machine-oriented applications. In contrast, our work departs from the CTA paradigm by leveraging token-based transmission to jointly support efficient machine analysis and high-quality human perception. B. HumanMachine Collaborative Compression"
        },
        {
            "title": "As outlined in the previous",
            "content": "subsection, conventional machine-oriented codecs often follow CTA paradigm, which requires full image decoding before downstream analysis and is inefficient for many AI-driven applications. An alternative, the analyze-then-compress (ATC) paradigm [27], reduces transmission costs in machine vision tasks by transmitting task-specific features [6], but this approach severely limits reconstruction quality, making results unsuitable for human inspection. UNDER REVIEW 3 Fig. 3. Overall architecture of the proposed UniMIC framework. (a) In edge-to-cloud transmission, multimodal inputs are tokenized and entropy-coded before being sent to the cloud. (b) In cloud-to-edge transmission, the Unified Transformer processes the tokens, and the generated outputs are entropy-coded and reconstructed at the user side. To address this limitation, the HumanMachine Collaborative Compression (HMCC) paradigm [8] jointly optimizes for both human and machine receivers. Existing works adopt scalable coding architectures that decompose content into semantic layers. Examples include face-recognitionand perceptual based residual enhancement [9], unified decoders for gradual reconstruction [28], task-specific latent partitions [11], and semantics-driven hierarchical schemes for progressive decoding [12], [13], [29]. Despite these advances, two major challenges remain: (1) current HMCC frameworks assume fixed senderreceiver roles, lacking flexibility for interactive bidirectional communication, in which both human and machine serve as sender and receiver; (2) they largely focus on visual-only data, offering limited support for multimodal tasks that involve cross-modal reasoning or generation. Our work addresses these gaps by introducing discrete token-based coding framework that unifies multimodal information. This design supports flexible role reversals and enables efficient, high-quality reconstructions for both human and machine receivers in diverse humanAI collaborative tasks. C. Large Multimodal Models LMMs have recently demonstrated unprecedented capabilities in both understanding and generation, enabled by unified representation learning across visual, textual, and auditory modalities [30][32]. By mapping heterogeneous inputs into shared embedding space, they can seamlessly perform diverse cross-modal taskssuch as image captioning and visual question answering [33][35]without relying on separate taskspecific models. foundation, recent works"
        },
        {
            "title": "Building on this",
            "content": "such as Chameleon [36], Transfusion [37], Show-o [38] and UniTok [39] employ discrete tokenization to represent multimodal data streams within unified vocabulary, enabling simultaneous comprehension and generation in Transformer-based architectures. This establishes discrete tokens as fundamental unit for scalable multimodal reasoning and generation. Motivated by these advances, we introduce the first multimodal interactive coding framework that operates directly on LMM tokens. By compressing discrete token sequencesrather than raw pixels or textwe achieve lowbitrate transmission while fully preserving the semantic fidelity and generative capabilities of large cloud-based models, thus enabling efficient, high-quality humanAI collaboration. III. UNIMIC: TOKEN-BASED MULTIMODAL INTERACTIVE CODING We present UniMIC, an efficient and unified multimodal interactive coding framework that enables seamless edge cloud communication between humans and AI agents. Unlike traditional pixel-based pipelines, UniMIC adopts tokenbased paradigm that is natively compatible with LMMs, yielding substantial gains in both compression efficiency and downstream task performance. On the edge side, multimodal inputsincluding images and textare converted into discrete token sequences via modality-specific tokenizers. These tokens are entropy-coded and transmitted to the cloud, where unified Transformer-based model directly processes them for diverse UNDER REVIEW generation and understanding tasks, such as text-to-image synthesis, text-guided image editing, and VQA. After cloudside processing, only the resulting tokens are entropy-coded and sent back to the edge, where they are reconstructed into final images or text outputs. Fig. 3 provides an overview of the UniMIC architecture. In Section III-A, we describe the end-to-end workflow of UniMIC. We then introduce lightweight Transformer-based entropy model in Section III-B, designed to accurately estimate token probabilities and fully exploit redundancy across different transmission scenarios. Finally, Section III-C details how UniMIC adapts to various multimodal tasks, demonstrating its flexibility and efficiency in real-world humanAI collaboration. A. Overview of the UniMIC Framework We refer to the human-operated device as the edge side and the AI-agent host as the cloud side. The edge side is assumed to have limited computational and memory resources, whereas the cloud side runs large-scale multimodal models, such as Transformer-based LMMs. Depending on the task, the edge transmits different multimodal inputs. For T2I generation, only text prompt Ts is sent; for tasks such as text-guided image editing or VQA, both text prompt Ts and an image Is are required. We define the edge-side input as Xs = {Ts, Is}, (1) where either element may be absent depending on the task."
        },
        {
            "title": "These inputs are mapped into a unified token space via",
            "content": "modality-specific tokenizers: = Etext(Ts) = {t1, . . . , tN }, = Eimg(Is) = {u1, . . . , uM }, (2) (3) The resulting text tokens and image tokens are concatenated as = [t; u], (4) then entropy-encoded into bitstream Bup for edge-to-cloud transmission (with text and image tokens coded separately; see Section III-B). On the cloud side, Bup is decoded to recover ˆz = [ˆt; ˆu], to which task token Ttask is prepended, yielding ˆz = [Ttask; ˆz], (5) (6) the input to the Unified Transformer F(). The model outputs token sequence = F(ˆz), (7) which is entropy-encoded as Bdown for cloud-to-edge transmission. Upon receiving Bdown, the edge decodes ˆo and reconstructs either an image ˆIt = Dimg(ˆo) or text ˆTt = Dtext(ˆo), depending on the task. Fig. 4. Entropy modeling strategies for image tokens in UniMIC, including (a) autoregressive, (b) masked-token, and (c) text-conditional mode. This token-centric pipeline avoids repeated pixel-level compressiondecompression, preserves structural and semantic integrity, and minimizes transmission overhead while maintaining fidelity for both human viewing and downstream machine analysis. B. Entropy Modeling in Unified Token Space Although UniMIC represents all multimodal inputs in unified token space, the statistical properties of text tokens and image tokens differ significantly, necessitating modalityspecific entropy coding strategies on both the edge and cloud sides. Text Tokens. Unlike conventional approaches that directly compress textual prompts, UniMIC first tokenizes text prompts into discrete tokens using text tokenizer Etext(). The resulting token sequence is then compressed losslessly using the Brotli [40] algorithm, which is lightweight and well-suited for the typically small size of text token sequences: Btext = Brotli(t). (8) Image Tokens. Image tokens constitute the majority of the bitstream and exhibit much higher entropy, requiring specialized modeling to achieve efficient compression. To this end, transformer-based entropy model, we design lightweight denoted as Tlite, which estimates the probability distribution of image tokens to support arithmetic coding. Importantly, Tlite is designed to be sufficiently lightweight to be deployed on both the edge side and the cloud side, ensuring symmetric probability estimation and exact reconstruction. Depending on the nature of the downstream task, we employ three variants of image-token entropy modeling, as illustrated in Fig. 4 : Autoregressive Mode Tlite. For standard image token sequences, the joint probability is factorized autoregressively as: p(u) = (cid:89) i=1 p(ui u<i; θlite), (9) UNDER REVIEW 5 where is the image token sequence and θlite denotes the parameters of Tlite. Masked-Token Mode mask lite . For tasks that require localized image editing, such as inpainting, only subset of the image tokens needs to be updated. To efficiently handle these scenarios, we adopt masked image transformer, mask , pre-trained using random masking strategies to lite learn the distribution of partially visible image token sequences. During training, we randomly select subset of tokens in the original image token sequence = {u1, . . . , uM } to be masked, producing binary mask = {m1, . . . , mM }, where mi = 1 denotes masked position. These positions are replaced with special mask token MASK, yielding the masked sequence u: (cid:40) ui = MASK, ui, if mi = 1, otherwise. (10) Importantly, although the masked tokens are not themselves targets for prediction, they remain in the context sequence as placeholders, providing positional and semantic cues to the transformer. Thus, the joint probability of all unmasked tokens is modeled as: p(uvalid u) = (cid:89) iU p(cid:0)ui u<i; θmask lite (cid:1), (11) where = {i mi = 0} denotes the indices of unmasked tokens. Accordingly, the entropy modeling loss is defined as: Lentropy = (cid:88) log p(cid:0)ui u<i; θmask (cid:1). lite TABLE TASK-SPECIFIC TOKEN-BASED CODING PROTOCOLS IN UNIMIC. AR: AUTOREGRESSIVE MODE. MASK-T: MASKED-TOKEN MODE. TEXT-T: TEXT-CONDITIONAL MODE. BROTLI: LOSSLESS COMPRESSION FOR TEXT TOKENS. EPAC: EQUI-PROBABLE ARITHMETIC CODING. Task T2I Inpainting Outpainting VQA Edge-to-Cloud (Brotli) umask (Mask-T), (Brotli), mask (EPAC) (Text-T), (Brotli) (AR), (Brotli) Cloud-to-Edge ˆu (Text-T) ˆuinpaint (Text-T) ˆuoutpaint (Text-T) ˆtans (Brotli) contextual priors for modeling the image token distribution. To exploit these cross-modal relationships and reduce redundancy between the textual and visual modalities, we propose text-conditional entropy modeling strategy, where the probability of each image token ui is conditioned not only on its preceding image tokens u<i, but also on the full text token sequence t: p(u t) = (cid:89) i= p(cid:0)ui u<i, t; θtext lite (cid:1), (15) where θtext lite denotes the parameters of the text-conditional lightweight transformer. This formulation allows the model to leverage semantic cues from the text tokens to more accurately predict the probabilities of image tokens, effectively reducing entropy and achieving better compression efficiency in multi-modal scenarios such as T2I generation. (12) C. Task-Adaptive Transmission Protocols iU During encoding, only the unmasked tokens uvalid are entropy-coded and transmitted: uvalid = {ui mi = 0}, (13) On the decoder side, the binary mask is reconstructed from the bitstream and used to re-insert mask tokens MASK at the masked positions, forming the reconstructed masked sequence umask: (cid:40) umask(i) = ui, MASK, if mi = 0, if mi = 1. (14) This masked sequence is then used as context for decoding the bitstream, ensuring that all unmasked tokens can be reconstructed precisely while avoiding the need to transmit unchanged image regions. This approach significantly improves compression efficiency by leveraging partial information while preserving spatial and semantic consistency. Text-Conditional Mode text lite . In many multimodal applications, textual information is semantically correlated with visual content. prime example is T2I generation, where the textual prompt Ts serves as direct description of the visual scene to be synthesized. During cloud-toedge transmission, returning generated images from the cloud side to the edge side, both transmission directions contain textual information that can serve as powerful In this section, we describe how UniMIC adapts its tokenbased coding framework to different multimodal humanAI collaborative tasks. Each task requires transmitting different combinations of tokenized modalities and employs corresponding entropy modeling strategies, yet all share the same unified token-based pipeline. Table summarizes the tokenbased transmission protocols in UniMIC for different tasks, combining transmitted modalities with the corresponding entropy models used in both edge-to-cloud and cloud-to-edge directions. T2I Generation. The edge side transmits only text tokens t, compressed losslessly using the Brotli [40] algorithm. On the cloud side, the Unified Transformer takes as input and generates the corresponding image token sequence ˆu. Since the generated image tokens are strongly conditioned on the text prompt, the output sequence ˆu is entropy-coded using the text-conditional transformer text lite before transmission back to the edge side. Upon receiving ˆu, the edge side decodes and reconstructs the final synthesized image It via the image detokenizer. Text-Guided Image Inpainting. Only the image tokens corresponding to regions that do not require editing (i.e., unmasked tokens with mi = 0), together with binary mask indicating the positions of masked regions (mi = 1), and the text tokens describing the desired edits are transmitted on the edge side. The text tokens are compressed using the Brotli [40] algorithm. The unmasked image tokens are entropy-coded via the masked-token transformer mask , since their encoding context lite UNDER REVIEW includes placeholder mask tokens for the masked regions. On the cloud side, the Unified Transformer processes these inputs and predicts the token sequence ˆuinpaint corresponding to the masked regions. These predicted tokens ˆuinpaint are entropycoded using the using the text-conditional transformer text lite before being transmitted back to the user on the edge side. Finally, the user merges the received inpainted tokens into the original sequence according to the binary mask, reconstructing the complete inpainted image. Text-Guided Image Outpainting. Both text tokens and full image tokens are transmitted from the edge to the cloud. The text tokens are compressed losslessly with Brotli [40], while the image tokens are entropy-coded using the textconditional transformer text lite , which leverages semantic cues from to improve compression efficiency. The Unified Transformer then generates additional tokens ˆuoutpaint representing the extrapolated regions. These new tokens ˆuoutpaint are likewise compressed using text lite and transmitted back to the user. Finally, the user merges the received tokens with the original image tokens to reconstruct the outpainted image. VQA. The user on the edge side transmits both text tokens (representing the question) and image tokens u. The text tokens are compressed using Brotli [40], while the image tokens are entropy-coded via the autoregressive model Tlite. On the cloud side, the Unified Transformer jointly processes these multimodal tokens and generates answer text tokens ˆtans. The answer tokens are compressed with Brotli [40] for efficient transmission. Once received, the user decodes the tokens to reconstruct the natural-language answer Tt. IV. EXPERIMENTS We conduct extensive experiments to comprehensively validate the effectiveness of the proposed UniMIC framework across four representative multimodal humanAI collaborative tasks: T2I generation, text-guided image inpainting, textguided image outpainting, and VQA. Although these tasks share the same unified token-based communication infrastructure, they differ in terms of token compositions and entropy coding strategies, offering diverse evaluation setting for our method. A. Implementation Details Tokenizers and Unified Transformer Backbone. UniMIC is designed as modular framework that can interface with unified multimodal transformer models capable of processing tokenized text and image inputs. In our experiments, we instantiate the backbone using the pretrained Show-o model [38] for empirical validation, as it provides strong offthe-shelf autoregressive transformer supporting both text-toimage generation and understanding. Text inputs are tokenized via GPT-style Byte Pair Encoding (BPE) tokenizer [41], while images are encoded using MagViT-v2 [42], which maps latent visual features into discrete tokens via an 8,192-entry codebook and Lookup-Free Quantization. While Show-o [38] is adopted here for validation, the framework is not limited to this specific choice. It can be integrated with other unified multimodal architectures with minimal adaptation, typically TABLE II QUANTITATIVE COMPARISONS ON THE T2I TASK. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD AND THE SECOND-BEST RESULTS ARE UNDERLINED. LOSSLESS INDICATES PERFECT RECONSTRUCTION, CORRESPONDING TO INFINITE PSNR. Stage Metric Cloud-to-Edge Overall Process BPP PSNR LPIPS FID CLIP-T BPG [46] VVC [15] MS-ILLM [47] VQ-Kmeans [48] DiffEIC [49] UniMIC 0.0414 0.0337 0.0647 0.0431 0.0587 0.0296 23.13 23.12 25.76 22.03 20.13 lossless 0.354 0.343 0.075 0.102 0.179 0 176.74 180.19 85.07 82.53 81.95 80.61 0.286 0.286 0.310 0.314 0.308 0.315 requiring retraining of the entropy modeling component to match the token distributions of new backbone. Entropy Models and Training. key innovation of UniMIC lies in its entropy modeling strategy, which enables efficient compression of multimodal token sequences while preserving semantic fidelity. Text tokens are directly compressed using the Brotli [40] lossless algorithm, while image tokens are handled by lightweight autoregressive Transformer entropy estimator (0.6B parameters) combined with arithmetic coding, making it practical for deployment on both edge and cloud sides. To support diverse transmission scenarios, we employ three variants of this model: standard Image Transformer Tlite for generic sequences, Masked Image Transformer mask for localized editing, and Text-Conditional Image lite Transformer text for text-guided generation. All models are lite implemented in PyTorch and trained on two NVIDIA A100 GPUs using two-stage strategy: (1) Pretraining on largescale datasets, where Tlite and mask are trained on ImageNet2012 [43] (with random masking for the masked variant), and text is pretrained on MS COCO [44] and CC3M [45] to lite capture textimage dependencies; (2) Domain adaptation to the Show-o token distribution via the synthetic Show-COCO dataset, where images are re-tokenized by Show-o using their captions. The pretrained text is fine-tuned on this dataset to lite yield ˆT text lite , ensuring the entropy model aligns with the actual token statistics in cloud-to-edge transmission. lite B. Evaluation Details Baseline Compression Methods. We benchmark UniMIC against set of representative codecs spanning both conventional and generative paradigms. For conventional compression, we adopt BPG [46] (an HEVC-Intra-based image codec) and the latest VVC standard [15]. For learned generative approaches, we include three representative methods: MSILLM [47], GAN-based codec; VQ-Kmeans [48], based on VQGAN representations; and DiffEIC [49], which leverages diffusion models for image compression. All baselines are evaluated using publicly available pretrained weights and are configured to achieve bitrate levels comparable to UniMIC, ensuring fair and consistent comparison. Common Compression Metrics. We adopt standard metrics to assess ratedistortion performance. Compression efficiency is measured in bits per pixel (bpp). Reconstruction quality is evaluated using Peak Signal-to-Noise Ratio (PSNR) for pixellevel fidelity and Learned Perceptual Image Patch Similarity (LPIPS) for perceptual similarity. These metrics are applied UNDER REVIEW 7 Fig. 5. Qualitative comparison of T2I generation. Compared with traditional codecs and recent generative baselines, UniMIC preserves semantic details and visual fidelity while achieving lower bitrates (bpp values shown under each result). consistently across all tasks to provide an objective assessment of the underlying compression schemes. C. Compression Evaluation for Text-to-Image Generation We evaluate UniMIC on the T2I generation task, focusing on (i) the efficiency of image token compression during the cloud-to-edge transmission stage and (ii) the overall impact on text-conditioned image generation quality. Evaluation Protocol. We randomly sample 1,000 humanannotated text prompts from the MS COCO 2017 validation set [44] as inputs to the T2I generation pipeline. In the UniMIC pipeline, the edge side transmits only the tokenized instructions, which are compressed using Brotli [40]. text On the cloud side, the Unified Transformer synthesizes the corresponding image tokens, which are then entropycoded and transmitted back to the edge for reconstruction. For comparison, we follow the baseline selection in Section IV-B and benchmark UniMIC against both conventional codecs (BPG [46], VVC [15]) and generative codecs (MSILLM [47], VQ-Kmeans [48], DiffEIC [49]). Unlike UniMIC, these codecs cannot operate directly on tokens. Instead, the cloud-generated image tokens must first be decoded into fullresolution images before pixel-level compression is applied, and the compressed bitstream is then transmitted back to the edge side. Task-Specific Metrics. In addition to the common compression metrics introduced in Section IV-B (bpp, PSNR, LPIPS), we report two task-specific measures for T2I generation. Frechet Inception Distance (FID) [50] evaluates the perceptual realism of generated images by measuring distributional divergence from real images, while CLIP-T [51] quantifies semantic alignment between generated images and input text prompts. Results and Analysis. Table II reports quantitative results under comparable bitrate conditions. UniMIC achieves nearlossless reconstruction even at extremely low bitrates (0.03 bpp), consistently outperforming all baselines in PSNR and LPIPS. This advantage stems from directly entropy-coding discrete image tokens, which avoids pixel-domain re-encoding and the quantization losses inherent to other codecs. In addition, UniMIC obtains the lowest FID and highest CLIPT scores, indicating superior preservation of both perceptual realism and semantic alignment with input prompts. As shown in Fig. 11, traditional codecs exhibit blurring and blocking artifacts, while generative codecs often introduce semantic shifts or structural distortions. In contrast, UniMIC reconstructs images that remain visually indistinguishable from their original cloud-generated counterparts, delivering perceptually lossless quality while maintaining high compression efficiency. D. Compression Evaluation for Text-Guided Image Inpainting We evaluate UniMIC on the text-guided image inpainting task with two primary objectives: (i) assessing the compression efficiency of image tokens across both edge-to-cloud and cloud-to-edge transmissions, and (ii) evaluating its effectiveness in text-conditioned editing. Evaluation Protocol. We employ the MagicBrush dataset, which provides 535 raw images and 1,053 edited samples with semantically masked regions and corresponding text instructions [52], as inputs to the text-guided inpainting pipeline. UNDER REVIEW Fig. 6. Qualitative comparison of text-guided inpainting. Baseline codecs suffer cumulative degradation: compression loss from Edge-to-Cloud is amplified during Cloud-to-Edge retransmission. In contrast, UniMIC incurs only one-time tokenization loss, avoiding repeated degradation while preserving fidelity at ultra-low bitrates. For clarity, note that the Cloud Rec and Cloud Gen stages do not exist in UniMICs actual pipeline; we reconstruct them here using original tokens and edited tokens solely to enable fair visual comparison with baselines. TABLE III QUANTITATIVE COMPARISON RESULTS ON TEXT-GUIDED IMAGE INPAINTING. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD AND THE SECOND-BEST RESULTS ARE UNDERLINED. LOSSLESS INDICATES PERFECT RECONSTRUCTION, CORRESPONDING TO INFINITE PSNR. Stage Metric Edge-to-Cloud Cloud-to-Edge Overall Process BPP PSNR LPIPS BPP PSNR LPIPS Total BPP FID CLIP-T R-CLIP-I BPG VVC MS-ILLM VQ-Kmeans DiffEIC UniMIC 0.0514 0.0370 0.0734 0.0391 0.0493 0.0369 22.35 21.88 23.68 19.85 15.53 20.42 0.440 0.481 0.120 0.149 0.282 0.134 0.0381 0.0160 0.0230 0.0118 0.0306 0.0063 25.98 20.52 19.24 16.90 12.71 lossless 0.245 0.486 0.381 0.284 0.534 0.0895 0.0530 0.0964 0.0509 0.0799 0.0432 209.05 254.76 184.76 95.51 248.71 57.30 0.237 0.206 0.227 0.261 0.206 0.298 0.691 0.662 0.711 0.779 0.675 0.903 In UniMIC, the edge side transmits only the unedited image tokens, along with the binary mask and text prompt. Conditioned on these inputs, the Unified Transformer generates tokens for the masked regions, which are entropy-coded and returned to the edge, where they are merged with the unedited tokens to reconstruct the final inpainted image. We also follow the baseline selection in Section IV-B and evaluate UniMIC against conventional codecs and generative codecs. Since these codecs cannot process tokens directly, the full image must be compressed and transmitted to the cloud for decompression UNDER REVIEW 9 and editing. After editing, the result is re-compressed and sent back to the edge. Task-Specific Metrics. We adopt the general compression metrics defined in Section IV-B, including bpp, PSNR, and LPIPS, to evaluate reconstruction fidelity across both transmission stages. We compare reconstruction quality at two stages: edge-to-cloud reconstruction is evaluated against the original input image, while cloud-to-edge reconstruction is evaluated against the generated edited image. In UniMIC, the edge-tocloud stage transmits only the tokens of unedited regions rather than the full image. To ensure fair evaluation, we reconstruct the complete image on the edge side from the full token sequence and report PSNR/LPIPS on this reconstruction, while the bpp is computed using only the transmitted uneditedregion tokens. For the cloud-to-edge stage, we further assess the quality of the inpainted outputs by reporting FID [50] for perceptual realism, CLIP-T [51] for semantic alignment with text prompts, and R-CLIP-I [51] for structural consistency with the original unedited regions. Results and Analysis. Table III reports the quantitative results on the text-guided image inpainting task under comparable bitrate conditions. UniMIC achieves the lowest transmission cost in both directions (0.0369 bpp for edge-to-cloud and 0.0063 bpp for cloud-to-edge), while also attaining the best overall performance on semantic and perceptual metrics (FID = 57.30, CLIP-T = 0.298, R-CLIP-I = 0.903). Although the reconstructed images in the edge-to-cloud stage inevitably show some degradation due to the tokenizer, this loss is fixed and does not propagate further, since token sequences are transmitted and compressed losslessly. In contrast, baseline codecs must process pixel-level images: compression artifacts are introduced during edge-to-cloud transmission, which then propagate through the pipeline and significantly impair the quality of inpainting and final reconstructions. Fig. 12 provides qualitative comparisons under similar bitrates. For fair visualization, we additionally reconstruct the intermediate stages (Cloud Rec, Cloud Gen) for UniMIC by using the complete token set, even though in the actual pipeline the cloud side does not have access to all tokens and only the final edge-side output is observable. For baseline methods, these stages are naturally available since they operate on pixel-level images. In all cases, the reported bitrates strictly correspond to the actual tokens transmitted in UniMIC and the actual compressed images in baselines. The visual results further confirm the quantitative findings: traditional codecs (VVC [15] and BPG [46]) already suffer from severe blurring and blocking artifacts in the edge-to-cloud reconstruction, and these distortions are further amplified during the cloud-toedge transmission, leading to heavily degraded final outputs. Generative codecs (DiffEIC [49], MS-ILLM [47], and VQKmeans [48]) produce more natural details, but the artifacts and inconsistencies introduced in the first stage also propagate into the second, often causing semantic shifts or unintended changes in unedited regions, resulting in noticeable divergence from the original content. In contrast, UniMIC avoids this cumulative degradation since tokens are transmitted losslessly after the one-time tokenizer step, ensuring that the inpainted content generated in the cloud is faithfully preserved in the final edge-side reconstruction. E. Compression Evaluation for Text-Guided Image Outpainting We evaluate UniMIC on the text-guided image outpainting task, focusing on (i) the compression efficiency of image tokens during both edge-to-cloud and cloud-to-edge transmissions, and (ii) its effectiveness in generating high-quality extrapolated content conditioned on textual instructions. Evaluation Protocol. We construct large-scale test set based on the Flickr Scenery dataset [53], from which 1,000 highresolution landscape images are randomly selected. To enable text-guided outpainting, we employ Qwen-VL [54] to generate descriptions for extrapolated regions, forming paired inputs of original images and text prompts. Following the baseline selection in Section IV-B, we compare UniMIC against conventional codecs (BPG, VVC) and generative codecs (MSILLM [47], VQ-Kmeans [48], DiffEIC [49]) under unified two-stage transmission pipeline comprising edge-to-cloud and cloud-to-edge communication. In UniMIC, the edge side compresses and transmits image tokensrather than pixel-level imagesto the cloud, where the Unified Transformer performs outpainting directly in the token space conditioned on the received tokens and text prompt. The generated tokens for the extrapolated regions are then entropy-coded and transmitted back to the edge, where they are seamlessly merged with the original tokens to reconstruct the final image. In contrast, baseline codecs cannot operate on tokens. The edge-side image must first be compressed and sent to the cloud, where it is decompressed, outpainted based on the input prompt, and the resulting full-resolution image is re-compressed for cloud-toedge transmission. We report reconstruction quality at both stages: edge-to-cloud reconstruction is measured against the original input image, while cloud-to-edge reconstruction is evaluated against the generated outpainted image. Task-Specific Metrics. We adopt the same evaluation metrics as in the T2I generation task (cf. Section IV-C), including bpp, PSNR, LPIPS, FID, and CLIP-T. While bpp, PSNR, and LPIPS jointly reflect the reconstruction fidelity during both transmission stages, FID specifically quantifies the semantic and perceptual quality of the extrapolated content conditioned on textual prompts. CLIP-T further evaluates the alignment between the extrapolated image regions and the guiding text, measuring whether the generated visual content faithfully follows the textual instructions. Results and Analysis. Table IV reports the quantitative results for text-guided image outpainting under comparable bitrate settings. In the edge-to-cloud stage, UniMIC achieves the lowest bitrate while retaining competitive perceptual quality and distortion performance. Unlike conventional codecs that must decode the original image before compression, UniMIC directly operates on discrete tokens without reconstructing pixel-level images. For fairness, we employ an auxiliary image detokenizer to visualize cloud-side reconstructions; the slight degradation observed arises from the inherent lossy tokenizerdetokenizer process, than the token-level compression itself. In the cloud-to-edge stage, UniMIC delivers lossless reconstruction even below 0.03 bpp, its discrete rather UNDER REVIEW 10 Fig. 7. Qualitative comparison of text-guided image outpainting. Baseline methods accumulate distortion across transmission stages and often lose semantic consistency in the extended regions. UniMIC avoids repeated degradation and generates coherent outpainting at ultra-low bitrates. TABLE IV QUANTITATIVE COMPARISONS ON THE TEXT-GUIDED IMAGE OUTPAINTING TASK. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD AND THE SECOND-BEST RESULTS ARE UNDERLINED. LOSSLESS INDICATES PERFECT RECONSTRUCTION, CORRESPONDING TO INFINITE PSNR. Stage Edge-to-Cloud Cloud-to-Edge Overall Process BPP PSNR LPIPS BPP PSNR LPIPS Total BPP FID CLIP-T BPG [46] VVC [15] MS-ILLM [47] VQ-Kmeans [48] DiffEIC [49] UniMIC 0.0610 0.0576 0.0622 0.0431 0.0437 0.0416 24.39 24.84 24.50 20.12 16.37 22.07 0.484 0.524 0.126 0.139 0.228 0.127 0.0387 0.0387 0.0396 0.0313 0.0401 0. 28.69 29.69 27.60 23.45 19.69 lossless 0.274 0.184 0.083 0.105 0.187 0 0.0997 0.0963 0.1018 0.0744 0.0838 0.0689 213.36 216.88 56.41 39.16 66.73 30.04 0.232 0.252 0.280 0.290 0.287 0.291 token design allows transmission of only the newly generated tokens for extrapolated regions, which are seamlessly merged with unedited tokens on the edge side, substantially reducing bandwidth while preserving semantic coherence. As result, UniMIC also achieves the lowest FID score, demonstrating superior perceptual realism of the extrapolated content. Beyond these quantitative improvements, qualitative results further highlight UniMICs advantages. Fig. 13 compares different methods across edge-to-cloud reconstruction, cloudside outpainting, and cloud-to-edge reconstruction. Baseline codecs introduce compression artifacts during edge-to-cloud transmission, which degrade the cloud input and subsequently impair the extrapolated content despite identical text prompts. Pixel-level re-compression of the outpainted image further accumulates distortions, producing visible inconsistencies in the final reconstructions. In contrast, UniMIC maintains structural fidelity and stylistic consistency, yielding natural and coherent extensions aligned with both the original image and the guiding prompt. Among conventional codecs, VVC [15] and BPG [46] often cause blurring, color shifts, and discontinuities between original and extrapolated regions, while generative methods such as DiffEIC [49], MS-ILLM [47], and VQKmeans [48] improve perceptual quality but still suffer from semantic drifts and texture inconsistencies, leading to stylistic deviations from the source image. F. Compression Evaluation for Visual Question Answering We evaluate UniMIC on the VQA task, focusing on the efficiency of image token compression during the edge-tocloud transmission stage and the overall performance of answer generation. Evaluation Protocol. We evaluate UniMIC on the VQA task using four benchmark datasets: POPE [56], built on MS COCO with 9,000 questions requiring precise object localization alongside adversarial distractors; GQA [57], comprising 20M reasoning questions paired with real-world images annotated with detailed scene graphs; Flickr30k [55], containing 31,783 images with descriptive captioning tasks; and Vizwiz-val [58], featuring 4,319 real-world imagequestion pairs collected from blind users, covering recognition, reading, UNDER REVIEW Fig. 8. Qualitative comparison of VQA capability on examples from the Flickr30k dataset [55]. We show two representative cases with input questions and the corresponding answers generated by different methods. Baseline codecs (BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC) either produce generic or semantically incorrect responses, whereas UniMIC generates contextually rich and accurate captions that align closely with the visual content. TABLE QUANTITATIVE COMPARISON RESULTS ACROSS MULTIPLE VQA DATASETS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD AND THE SECOND-BEST RESULTS ARE UNDERLINED. LOSSLESS INDICATES PERFECT RECONSTRUCTION, CORRESPONDING TO INFINITE PSNR. Stage POPE [56] GQA [57] Flickr-30k [55] Vizwiz-val [58] BPP ACC BPP EM BPP BLEU CIDEr ROUGE-L BPP EM BPG [46] VVC [15] MS-ILLM [47] VQ-Kmeans [48] DiffEIC [49] UniMIC 0.0565 0.0465 0.0752 0.0470 0.0740 0.0424 0.5290 0.5348 0.5267 0.5312 0.5309 0.7710 0.0569 0.0529 0.0754 0.0470 0.0747 0.0422 0.3357 0.3318 0.3312 0.3348 0.0339 0.4915 0.0535 0.0500 0.0793 0.0470 0.0770 0.0429 0.1072 0.0968 0.0969 0.1022 0.1010 0. 0.0193 0.0166 0.0167 0.0176 0.0177 0.3398 0.2232 0.2142 0.2145 0.2203 0.2189 0.4200 0.0470 0.0429 0.0578 0.0470 0.0620 0.0394 0.0156 0.0125 0.0107 0.0128 0.0110 0.0674 and reasoning tasks. In UniMIC, the edge compresses and sends image and question tokens to the cloud, where VQA is performed directly in the token space. The cloud then returns entropy-coded answer tokens, which are decoded on the edge into natural-language responses. In contrast, baseline codecs operate at the pixel level: the original image must first be compressed and transmitted to the cloud, decompressed for VQA, and the resulting answer subsequently transmitted back. Task-Specific Metrics. In line with the previous tasks, we report the edge-to-cloud image bpp to measure compression efficiency. For task performance, different metrics are adopted depending on the dataset and the nature of the text responses: POPE [56]: Accuracy is used to assess the models ability to correctly recognize objects and their spatial relationships in the presence of distractors. Higher accuracy reflects stronger fine-grained visual grounding. GQA [57]: We use Exact Match (EM) to measure the proportion of generated answers that exactly match the reference answers, where higher EM indicates stronger multimodal reasoning and semantic faithfulness. Flickr30k [55]: Captioning performance is evaluated with BLEU, CIDEr,and ROUGE-L. Higher scores reflect better semantic alignment and descriptive quality relative to human-written captions. Vizwiz-val [58]: Similar to GQA [57], EM is employed to evaluate answer quality on this real-world dataset, where higher scores indicate stronger robustness to diverse and noisy visuallinguistic inputs. Results and Analysis. Table reports quantitative comparisons across all datasets under comparable bitrate settings, while Fig. 10 illustrates representative qualitative results. Baseline codecs significantly impair the semantic understanding of the cloud model after decompression, leading to degraded reasoning and inaccurate answers to textual queries. In contrast, UniMIC preserves the token-level information required by the cloud-based model, maintaining robust performance. Notably, UniMIC consistently achieves higher task-specific scores at substantially lower bitrates (0.04 bpp), demonstrating that operating directly on discrete tokens eliminates the information loss caused by pixel-level compression and ensures stable end-to-end VQA performance. These results further validate the generality of UniMIC, showing that its tokenbased interactive coding framework extends beyond generation and editing tasks to support complex multimodal reasoning in real-world scenarios. G. Ablation Studies We conduct ablation experiments to quantify the contribution of key components in UniMIC. Specifically, we first evaluate different Transformer-based entropy modeling strategies and then investigate the effect of text tokenization on transmission efficiency. Impact of Transformer-Based Entropy Models. We conduct the ablation study in two stages to validate the effectiveness of Transformer-based entropy models in UniMIC. First, compared to the baseline without Transformer modeling, UNDER REVIEW 12 TABLE VI ABLATION STUDY RESULTS OF TRANSFORMER-BASED ENTROPY MODELS. RESULTS FOR OUR FINAL SCHEME ARE SHOWN WITH BLUE BACKGROUND. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD AND THE SECOND-BEST RESULTS ARE UNDERLINED. Methods w/o Transformer Tlite mask lite text lite ˆT text lite T2I Inpainting Outpainting VQA Cloud-to-Edge Edge-to-Cloud Cloud-to-Edge Edge-to-Cloud Cloud-to-Edge Edge-to-Cloud 0.0509 0.0347 0.0368 0.0336 0.0296 0.0429 0.0385 0.0369 0.0384 0.0398 0.0081 0.0064 0.0065 0.0063 0. 0.0509 0.0423 0.0432 0.0416 0.0432 0.0382 0.0274 0.0276 0.0273 0.0277 0.0509 0.0424 0.0438 0.0425 0.0440 TABLE VII ABLATION STUDY OF TEXT COMPRESSION METHODS ACROSS DIFFERENT MULTIMODAL TASKS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD."
        },
        {
            "title": "Text Type",
            "content": "Brotli (CR) Ours (CR) T2I Inpainting Outpainting VQA VQA"
        },
        {
            "title": "Prompt\nPrompt\nPrompt\nQuestion\nAnswer",
            "content": "0.905 0.824 0.728 0.733 0.843 0.745 0.656 0.548 0.632 0.691 their the Masked model lowering all variants achieve clear bitrate reductions, e.g., T2I (Cloud-to-Edge) from 0.0509 to 0.0296, confirming the advantage of probabilistic modeling over uniform token distribution. Second, we evaluate the three proposed variantsImage Transformer, Masked Image Transformer, and Text-Conditional Transformerunder respective task settings. Each shows superior performance in its intended scenario: the Text-Conditional model excels in text-to-image and outpainting, is most effective for inpainting (Edge-to-Cloud), and the Image Transformer performs best on VQA. These results verify that our task-specific designs provide consistent gains across diverse transmission directions. Finally, domain adaptation of the Text-Conditional Transformer on the Show-COCO dataset further reduces the bitrate for T2I (from 0.0336 bpp to 0.0296 bpp, an 11.9% improvement), highlighting the importance of adapting entropy models to the token distributions of generative models. Effect of Text Tokenization on Compression Efficiency. UniMIC transmits textual inputs by first converting them into discrete tokens and then applying entropy coding, rather than compressing raw text directly. To assess the effectiveness of this design, we compare UniMIC with baseline where Brotli [40] is directly applied to raw text sequences without tokenization. We evaluate text compression efficiency using Compression Ratio (CR) across different multimodal tasks. As shown in Table VII, UniMIC achieves consistently better compression than the Brotli baseline, with improvements ranging from 13.8% to 24.7% across tasks. These results validate that tokenizing text before entropy coding yields more compact representations and thus more efficient transmission in multimodal communication pipelines. V. CONCLUSIONS In this paper, we presented UniMIC, unified token-based multimodal interactive coding framework for next-generation humanAI collaboration. By replacing conventional pixelor text-based transmission with compact tokenized representations, UniMIC enables efficient bidirectional communication while preserving structural alignment with LMMs. To further enhance efficiency, we designed lightweight Transformerbased entropy models with scenario-specific strategies that to diverse tasks. effectively reduce redundancy and adapt Extensive experiments across T2I generation, text-guided inpainting, outpainting, and VQA validate the robustness of UniMIC, demonstrating substantial bitrate savings and stable performance even under ultra-low bitrate conditions. Beyond compression efficiency, UniMIC establishes paradigm shift from pixel-centric coding to token-based communication protocols, paving the way for future AI-native multimedia transmission systems. Nevertheless, UniMIC still relies on the quality and compatibility of upstream tokenizers, and adapting the entropy models to different LMM backbones may require lightweight retraining to mitigate domain gaps. We leave these aspects as promising directions for future work."
        },
        {
            "title": "REFERENCES",
            "content": "[1] William Pennebaker and Joan Mitchell, JPEG: Still image data compression standard, Springer Science & Business Media, 1992. [2] Majid Rabbani, JPEG2000: Image compression fundamentals, standards and practice, JEI, vol. 11, no. 2, pp. 286, 2002. [3] Thomas Wiegand, Gary Sullivan, Gisle Bjontegaard, and Ajay Luthra, Overview of the H.264/AVC video coding standard, IEEE TCSVT, vol. 13, no. 7, pp. 560576, 2003. [4] Wen Gao and Siwei Ma, An overview of AVS2 standard, in Advanced Video Coding Systems, pp. 3549. 2014. [5] Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand, Overview of the high efficiency video coding (HEVC) standard, IEEE TCSVT, vol. 22, no. 12, pp. 16491668, 2012. [6] Ling-Yu Duan, Vijay Chandrasekhar, Jie Chen, Jie Lin, Zhe Wang, Tiejun Huang, Bernd Girod, and Wen Gao, Overview of the mpegcdvs standard, IEEE TIP, vol. 25, no. 1, pp. 179194, 2015. [7] Ling-Yu Duan, Yihang Lou, Yan Bai, Tiejun Huang, Wen Gao, Vijay Chandrasekhar, Jie Lin, Shiqi Wang, and Alex Chichung Kot, Compact descriptors for video analysis: The emerging mpeg standard, IEEE MM, vol. 26, no. 2, pp. 4454, 2018. [8] Lingyu Duan, Jiaying Liu, Wenhan Yang, Tiejun Huang, and Wen Gao, Video coding for machines: paradigm of collaborative compression and intelligent analytics, IEEE TIP, vol. 29, pp. 86808695, 2020. [9] Shurun Wang, Shiqi Wang, Wenhan Yang, Xinfeng Zhang, Shanshe Towards analysis-friendly face Wang, Siwei Ma, and Wen Gao, representation with scalable feature and texture compression, TMM, vol. 24, pp. 31693181, 2021. [10] Yueyu Hu, Shuai Yang, Wenhan Yang, Ling-Yu Duan, and Jiaying Liu, Towards coding for human and machine vision: scalable image coding approach, in ICME, 2020, pp. 16. [11] Hyomin Choi and Ivan Bajic, Scalable image coding for humans and machines, IEEE TIP, vol. 31, pp. 27392754, 2022. UNDER REVIEW 1 [12] Ning Yan, Changsheng Gao, Dong Liu, Houqiang Li, Li Li, and Feng Sssic: semantics-to-signal scalable image coding with learned Wu, structural representations, IEEE TIP, vol. 30, pp. 89398954, 2021. [13] Hanyue Tu, Li Li, Wengang Zhou, and Houqiang Li, Semantic scalable image compression with cross-layer priors, in ACM MM, 2021, pp. 40444052. [14] Kang Liu, Dong Liu, Li Li, Ning Yan, and Houqiang Li, Semanticsto-signal scalable image compression with learned revertible representations, IJCV, vol. 129, no. 9, pp. 26052621, 2021. [15] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary Sullivan, and Jens-Rainer Ohm, Overview of the versatile video coding (VVC) standard and its applications, IEEE TCSVT, vol. 31, no. 10, pp. 37363764, 2021. [16] Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszar, Lossy image compression with compressive autoencoders, arXiv:1703.00395, 2017. [17] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto, Learned image compression with discretized gaussian mixture likelihoods and attention modules, in CVPR, 2020, pp. 79397948. [18] Jun-Hyuk Kim, Byeongho Heo, and Jong-Seok Lee, Joint global and local hierarchical priors for learned image compression, in CVPR, 2022, pp. 59926001. [19] Chenjian Gao, Tongda Xu, Dailan He, Yan Wang, and Hongwei Qin, Flexible neural image compression via code editing, in NIPS, 2022, vol. 35, pp. 1218412196. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets, in NeurIPS, 2014, pp. 26722680. [21] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool, Generative adversarial networks for extreme learned image compression, in ICCV, 2019, pp. 221231. [22] Jianhui Chang, Qi Mao, Zhenghui Zhao, Shanshe Wang, Shiqi Wang, Hong Zhu, and Siwei Ma, Layered conceptual image compression via deep semantic synthesis, in ICIP, 2019, pp. 694698. [23] Jianhui Chang, Zhenghui Zhao, Chuanmin Jia, Shiqi Wang, Lingbo Yang, Qi Mao, Jian Zhang, and Siwei Ma, Conceptual compression via deep structure and texture synthesis, IEEE TIP, vol. 31, pp. 28092823, 2022. [24] Oren Rippel and Lubomir Bourdev, Real-time adaptive image compression, in ICML, 2017, pp. 29222930. [25] Jooyoung Lee, Donghyun Kim, Younhee Kim, Hyoungjin Kwon, Jongho training method for image compression Kim, and Taejin Lee, networks to improve perceptual quality of reconstructions, in CVPR Workshops, 2020, pp. 144145. [26] Shoma Iwai, Tomo Miyazaki, Yoshihiro Sugaya, and Shinichiro Omachi, Fidelity-controllable extreme image compression with generative adversarial networks, in ICPR, 2021, pp. 82358242. [27] Alessandro Redondi, Luca Baroffio, Matteo Cesana, and Marco Tagliasacchi, Compress-then-analyze vs. analyze-then-compress: Two paradigms for image analysis in visual sensor networks, in MMSP, 2013, pp. 278282. [28] Shuai Yang, Yueyu Hu, Wenhan Yang, Ling-Yu Duan, and Jiaying Liu, Towards coding for human and machine vision: Scalable face image coding, IEEE TMM, vol. 23, pp. 29572971, 2021. [29] Qi Mao, Chongyu Wang, Meng Wang, Shiqi Wang, Ruijie Chen, Libiao Jin, and Siwei Ma, Scalable face image coding via stylegan prior: Toward compression for human-machine collaborative vision, IEEE TIP, vol. 33, pp. 408422, 2023. [30] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al., Palm: Scaling language modeling with pathways, JMLR, vol. 24, no. 240, pp. 1113, 2023. [31] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., Language models are few-shot learners, in NeurIPS, 2020, vol. 33, pp. 18771901. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, MarieAnne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al., Llama: Open and efficient foundation language models, arXiv:2302.13971, 2023. [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, Visual instruction tuning, in NeurIPS, 2023, vol. 36, pp. 3489234916. [34] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, in NeurIPS, 2023, vol. 36, pp. 4925049267. [35] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua, Next-gpt: Any-to-any multimodal llm, in ICML, 2024. [36] Chameleon Team, Chameleon: Mixed-modal early-fusion foundation models, arXiv:2405.09818, 2024. [37] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy, Transfusion: Predict the next token and diffuse images with one multi-modal model, arXiv:2408.11039, 2024. [38] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou, Show-1: Marrying pixel and latent diffusion models for text-to-video generation, IJCV, pp. 115, 2024. [39] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi, Unitok: unified tokenizer for visual generation and understanding, arXiv:2502.20321, 2025. [40] Jyrki Alakuijala, Andrea Farruggia, Paolo Ferragina, Eugene Kliuchnikov, Robert Obryk, Zoltan Szabadka, and Lode Vandevenne, Brotli: general-purpose data compressor, TOIS, vol. 37, no. 1, pp. 130, 2018. [41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, pp. 9, 2019. [42] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al., Language model beats diffusiontokenizer is key to visual generation, arXiv:2310.05737, 2023. [43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009, pp. 248255. [44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick, Microsoft coco: Common objects in context, in ECCV, 2014, pp. 740755. [45] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut, Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning, in ACL, 2018. [46] Fabrice Bellard, Bpg image format, URL https://bellard.org/bpg, 2015. [47] Matthew J. Muckley, Alaaeldin El-Nouby, Karen Ullrich, Herve Jegou, Improving statistical fidelity for neural image and Jakob Verbeek, compression with implicit local likelihood models, in ICML, 2023. [48] Qi Mao, Tinghan Yang, Yinuo Zhang, Zijian Wang, Meng Wang, Shiqi Wang, Libiao Jin, and Siwei Ma, Extreme image compression using fine-tuned vqgans, in DCC, 2024, pp. 203212. [49] Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, and Jingwen Jiang, Toward extreme image compression with latent feature guidance and diffusion prior, IEEE TCSVT, vol. 35, no. 1, pp. 888899, 2025. [50] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter, GANs trained by two time-scale update rule converge to local nash equilibrium, in NeurIPS, 2017. [51] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi, Clipscore: reference-free evaluation metric for image captioning, arXiv:2104.08718, 2021. [52] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su, Magicbrush: manually annotated dataset for instruction-guided image editing, in NeurIPS, 2023. [53] Chieh Hubert Lin, Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, InfinityGAN: Towards infinite-pixel image and Ming-Hsuan Yang, synthesis, in ICLR, 2022. [54] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al., Qwen2vl: Enhancing vision-language models perception of the world at any resolution, arXiv:2409.12191, 2024. [55] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, TACL, vol. 2, pp. 6778, 2014. [56] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen, Evaluating object hallucination in large vision-language models, in EMNLP, 2023. [57] Drew Hudson and Christopher Manning, Gqa: new dataset for real-world visual reasoning and compositional question answering, in CVPR, 2019, pp. 67006709. [58] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham, Vizwiz grand challenge: Answering visual questions from blind people, in CVPR, 2018, pp. 36083617. UNDER REVIEW 2 various baseline approaches in terms of CLIP-T, CLIP-I, and FID metrics. C. Additional Results Fig. 10, Fig. 11, Fig. 12, and Fig. 13 provide further visual comparisons between UniMIC and baseline methods on VQA, T2I generation, text-guided image inpainting, and outpainting tasks. VI. SUPPLEMENTARY In this supplementary material, we provide detailed implementation details, additional analyses, limitations, and qualitative results as follows: In Section VI-A, we describe the baseline configurations used in our experiments. In Section VI-B, we demonstrate how our method enables flexible rate control without compromising performance. In Section VI-C, we provide additional qualitative results, including comparisons on visual question answering (VQA), text-to-image (T2I) generation, text-guided image inpainting, and outpainting tasks. A. Details of Comparisons with Baselines"
        },
        {
            "title": "For",
            "content": "fair comparison, we configure traditional codecs (BPG [46], VVC [15]) and generative codecs (MS-ILLM [47], VQ-Kmeans [48], DiffEIC [49]) under settings that yield bitrates comparable to UniMIC across different tasks. Specifically: 1) Text-to-Image Generation: BPG uses HEVC-Intra 18.0 with QP = 51, while VVC adopts VVC-Intra 11.0 with QP = 52. Generative codecs are tested with publicly available pretrained models: MS-ILLM with msillm_quality_1, VQ-Kmeans with 2048-entry codebook, and DiffEIC with the 1_2_4 configuration. 2) Text-Guided Image Inpainting: BPG is evaluated at QP = 50 (edge-to-cloud) and QP = 51 (cloudto-edge), while VVC is evaluated at QP = 52 and QP = 69, respectively. For generative codecs, MSILLM uses msillm_quality_1 (edge-to-cloud) and msillm_vlov1_1 (cloud-to-edge); VQ-Kmeans employs 2048-entry codebook for edge-to-cloud and 256entry codebook for cloud-to-edge; DiffEIC is tested with 1_2_8 and 1_2_16, respectively. 3) Text-Guided Image Outpainting: BPG is evaluated at QP = 47 (edge-to-cloud) and QP = 46 (cloud-to-edge), while VVC is evaluated at QP = 47 and QP = 45, respectively. For generative codecs, MS-ILLM employs msillm_quality_1 for both directions; VQ-Kmeans uses 2048-entry codebook (cloud-to-edge) and 256entry codebook (edge-to-cloud); DiffEIC is evaluated with the 1_2_8 configuration for both directions. 4) Visual Question Answering: BPG is evaluated at QP = 51, while VVC version 11.0 (intra coding) is evaluated at QP = 49. Generative codecs are tested using MS-ILLM (msillm_quality_1), VQ-Kmeans (2048entry codebook), and DiffEIC (1_2_4). B. Flexible Bitrate Control The proposed Uni-MIC framework enables flexible bitrate control without the need for additional training. Specifically, we can mask image tokens corresponding to less informative regions of the original image based on predefined ratio, thereby effectively reducing the transmission bitrate. As illustrated in the Fig. 9, even when most image tokens are masked during the user-to-cloud stage in the text-guided inpainting task, our method still significantly outperforms UNDER REVIEW 3 Fig. 9. The R-D performance of BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC, and our method on the MagicBrush dataset. (a) higher CLIP-T score indicates better semantic consistency between the generated or edited images and their corresponding text descriptions; (b) higher R-CLIP-I score reflects better structural preservation between the original and edited images; (c) lower FID score suggests improved overall image quality and closer match between the distributions of the generated (or edited) images and real images. Fig. 10. Qualitative comparison of VQA capability on examples from the Flickr30k dataset [55]. We show two representative cases with input questions and the corresponding answers generated by different methods. Baseline codecs (BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC) either produce generic or semantically incorrect responses, whereas UniMIC generates contextually rich and accurate captions that align closely with the visual content. UNDER REVIEW Fig. 11. Qualitative comparison of T2I generation. Compared with traditional codecs and recent generative baselines, UniMIC preserves semantic details and visual fidelity while achieving lower bitrates (bpp values shown under each result). UNDER REVIEW 5 Fig. 12. Quantitative comparison results on text-guided image inpainting. UNDER REVIEW Fig. 13. Qualitative comparison results of text-guided image outpainting. Specifically, the encoding method used to generate each image is indicated above, and the white box at the bottom denotes the decoding bitrate (bits per pixel, bpp) for each image. The red box highlights the reconstructions of the original image at different stages."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia, Beijing 10080, China",
        "State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing 100024, China"
    ]
}