{
    "paper_title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "authors": [
        "Jin-Ting He",
        "Fu-Jen Tsai",
        "Yan-Tsung Peng",
        "Min-Hung Chen",
        "Chia-Wen Lin",
        "Yen-Yu Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM."
        },
        {
            "title": "Start",
            "content": "BlurDM: Blur Diffusion Model for Image Deblurring Jin-Ting He1 Fu-Jen Tsai2 Yan-Tsung Peng3 Min-Hung Chen4 Chia-Wen Lin2 Yen-Yu Lin1 1National Yang Ming Chiao Tung University 2National Tsing Hua University 3National Chengchi University 4NVIDIA 5 2 0 2 3 ] . [ 1 9 7 9 3 0 . 2 1 5 2 : r jinting.cs12@nycu.edu.tw fjtsai@gapp.nthu.edu.tw ytpeng@cs.nccu.edu.tw minhungc@nvidia.com cwlin@ee.nthu.edu.tw lin@cs.nycu.edu.tw"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through dual-diffusion forward scheme, diffusing both noise and blur onto sharp image. During the reverse generation process, we derive dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM."
        },
        {
            "title": "Introduction",
            "content": "Camera shake or moving objects frequently introduce unwanted blur artifacts in captured images, severely degrading image quality and hindering downstream vision applications, such as object detection [11, 41], semantic segmentation [1, 43], and face recognition [12, 23]. Dynamic scene image deblurring aims to restore sharp details from single blurred image, highly ill-posed problem due to the directional and non-uniform nature of blur. With the advancement of deep learning, CNN-based models [6, 24, 26, 39, 50, 52] have demonstrated remarkable success in data-driven deblurring. Additionally, Transformer-based approaches [2, 13, 21, 40, 42, 49] have been introduced to effectively capture long-range dependencies, further enhancing deblurring performance by leveraging global contextual information. Although previous methods have successfully improved deblurring performance, the inherent constraints of regression loss [4] typically lead to over-smoothed results with limited high-frequency details. Recent advances in diffusion models [10, 32, 37] have demonstrated remarkable success in image generation, producing high-quality images with rich details and sharp textures through forward noise diffusion followed by reverse denoising. Building on the success of diffusion models, several studies [4, 17, 28, 29, 47] have incorporated them into deblurring models to produce restored images. However, standard diffusion models are not specifically designed for deblurring. Thus, directly applying them to deblurring networks limits their potential, leading to suboptimal performance. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: BlurDM is diffusion-based network that leverages inductive bias of blur formation for dynamic scene deblurring. It progressively adds noise and blur in the forward process and iteratively estimate and removes them in the reverse process to recover sharp images. The limitation arises from fundamental discrepancy between the diffusion process and the motion blur formation process. Unlike random noise in the standard diffusion process, motion blur results from continuous exposure process during image capture, where blur intensity accumulates progressively along motion trajectory. As result, motion blur exhibits structured and directed patterns, rather than the random noise perturbations modeled in conventional diffusion processes. To bridge this gap, we propose diffusion model that mimics the physical formation of motion blur, highlighting the continuous and progressive characteristics of blur formation. Instead of solely relying on standard noise diffusion, our approach incorporates blur diffusion mechanism, which gradually introduces motion blur to sharp images in structured manner. By leveraging the iterative nature of diffusion models, the proposed framework integrates the inductive bias of continuous blur formation, enhancing its ability to recover fine details and preserve image structures. In this paper, we propose the Blur Diffusion Model (BlurDM), novel approach that aligns the diffusion process with the physical principles of blur formation to enhance deblurring performance. BlurDM adopts dual diffusion strategy, combining noise and blur diffusion to truly reflect the progressive nature of blur formation, as illustrated in Fig.1. In the forward diffusion process, BlurDM progressively adds both noise and blur to sharp images to generate blurred and noisy images. To achieve gradual increase in blur, it is crucial to gauge the blur residual, representing the incremental blur added as the exposure time extends. However, existing deblurring datasets primarily consist of blurred-sharp image pairs without ground-truth blur residuals. To address this, BlurDM employs continuous blur accumulation formulation to implicitly represent the blur residual without relying on ground-truth blur residuals. This enables BlurDM to gradually blur images to align with the principles of the blur formation process. In the reverse generation process, BlurDM aims to simultaneously remove noise and blur to restore sharp images. To overcome the challenge of unavailable ground-truth blur residuals, we derive dual denoising and deblurring formulation that follows the principles of the blur formation process to implicitly approximate noise and blur residuals through dedicated noise and blur residual estimators. By effectively reversing the blur formation process, BlurDM can generate high-quality, realistic, sharp images. However, when applied to image deblurring, diffusion models may struggle to accurately reconstruct details with high content fidelity due to their inherent stochastic nature [48]. To address the limitations, inspired by [4, 47], we utilize BlurDM as prior generation network to flexibly and efficiently enhance existing deblurring models. Guided by the priors learned via BlurDM, deblurred models can achieve more accurate and visually consistent results. Key contributions of this work are summarized as follows: We present BlurDM, novel diffusion-based network that incorporates the inductive bias of blur formation to enhance dynamic scene image deblurring. We propose dual noise and blur diffusion process and derive dual denoising and deblurring formulation, which allows BlurDM to implicitly estimate blur residuals instead of relying on the ground truth. Extensive experiments demonstrate that BlurDM significantly and consistently improves four deblurring models on four benchmark datasets."
        },
        {
            "title": "2 Related Work\n2.1 Image Deblurring\nImage deblurring has made substantial progress with the development of deep learning. Numerous\nstudies have explored CNN-based deblurring using recurrent architectures, such as multi-scale [24,",
            "content": "2 39], multi-patch [51, 52], and multi-temporal [26] recurrent networks. For example, Tau et al. [39] develop scale-recurrent network accompanied by coarse-to-fine strategy for deblurring. Zamir et al. [51] introduce multi-stage patch-recurrent network that splits an image into non-overlapping patches for hierarchical blurred pattern handling. Park et al. [26] designs temporal-recurrent network that progressively recovers sharp images through incremental temporal training. Transformer-based methods [2, 13, 21, 40, 42, 49] have recently garnered considerable attention for deblurring due to their capacities to model long-range dependencies. However, the substantial training data and memory requirements of Transformers motivate the development of efficient variants [13, 21, 40, 42, 49] specifically tailored for deblurring. For instance, Zamir et al. [49] introduced channel-wise attention mechanism to reduce memory overhead. Tsai et al. [40] proposed strip-wise attention mechanism to handle blurred patterns with diverse orientations and magnitudes. Kong et al. [13] presented frequency attention to replace dot product operations in the spatial domain with element-wise multiplications in the frequency domain. Mao et al. [21] incorporated local channel-wise attention in the frequency domain to capture cross-covariance in the attention mechanism. Although the aforementioned advances have improved deblurring performance through various architectural and algorithmic designs, the inherent constraints of using the regression loss [4] frequently lead to over-smoothed results with limited high-frequency details, producing suboptimal deblurred images. 2.2 Diffusion Models Diffusion models [10, 38] have demonstrated remarkable capability in generating high-fidelity images with rich details through forward noise diffusion and reverse denoising. They have been leveraged in numerous studies [20, 22, 25, 27, 33, 34, 45, 46, 53] to synthesize high-quality images under variety of conditioning schemes. Diffusion models have been applied to low-level vision tasks [7, 9, 14, 18, 19, 47, 54]. For instance, Xia et al. [47] employed diffusion models to generate prior representations for clean image recovery. Liu et al. [19] utilized text prompts to compile task-specific priors across various image restoration tasks. Zheng et al. [54] proposed selective hourglass mapping strategy to learn shared information between different tasks for universal image restoration. Recognizing the advances of diffusion models in low-level vision, researchers have extended their use to image deblurring [3, 4, 15, 16, 17, 28, 30, 44]. Specifically, Whang et al. [44] introduced stochastic refinement diffusion model for deblurring. Ren et al. [30] incorporated multi-scale structure guidance network within the diffusion model to recover sharp images. Furthermore, several studies [3, 4, 16, 28] employed diffusion models as prior generation networks and perform diffusion in the latent space to improve deblurring efficiency. For instance, Chen et al. [4] proposed hierarchical integration module to fuse diffusion priors for deblurring, while Chen et al. [3] incorporated these priors into window-based transformer blocks. While these methods effectively reduce diffusion model latency for deblurring, they overlook the intrinsic characteristics of the blurring process within the diffusion framework, limiting their full potential. Although Liu et al. [17] proposed residual diffusion by computing the difference between sharp and blurred images using subtraction operation, the blur formation process is inherently convolutional process rather than direct additive difference, making this approach insufficient for accurately capturing blur characteristics. In contrast, we propose novel framework that incorporates the blur formation process into the diffusion model, leading to significant deblurring performance improvements."
        },
        {
            "title": "3 Proposed Method",
            "content": "We propose Blur Diffusion Model (BlurDM), novel diffusion framework for image deblurring. Unlike existing methods [4, 28, 29, 47], which rely only on noise diffusion, BlurDM integrates blur diffusion process, incorporating blur formation into diffusion to improve deblurring performance. As shown in Fig. 1, we progressively add both noise and blur to sharp image through dual noise and blur diffusion process during forward diffusion. In the reverse process, BlurDM jointly denoises 3 Figure 2: Overall framework of the proposed method. (a) Stage 1: Pre-train the Sharp Encoder (SE), Prior Fusion Module (PFM), and the deblurring network to obtain the sharp prior S. (b) Stage 2: Optimize the Blur Encoder (BE) and BlurDM to learn the diffusion prior 0 from blurred image. (c) Stage 3: Jointly optimize the BE, PFM, BlurDM, and deblurring network to generate the final deblurred image. and deblurs the image, starting from Gaussian noise conditioned on the blurred input. Ultimately, we use BlurDM as prior generation network to retain the diffusion models ability to learn high-quality, realistic image content while embedding the learned prior into the latent space of deblurring network for effective and high-fidelity restoration, as shown in Fig. 2. Next, we detail the key components of BlurDM, including the dual noise and blur diffusion process, dual denoising and deblurring formulation, and network architecture. 3.1 Dual Noise and Blur Diffusion Process Motion blur in the image capture process is introduced from continuous exposure, where the camera sensors accumulate light over the exposure duration, causing the blending of moving elements along the motion trajectories and leading to gradual build-up in blur. This process can be mathematically (cid:82) αT τ =0 H(τ ) dτ, where RHW 3, H(τ ) RHW 3, and αT denote represented as = 1 αT the blurred image, the instantaneous scene radiance at each moment τ , and the total exposure time, respectively. This models the blur formation process, showing how continuous light integration during exposure results in the accumulation of motion blur. Building on the understanding of the blur formation process, we propose dual-diffusion framework that incorporates the blur formation framework into noise diffusion. That is, sharp image is progressively corrupted by both noise and blur in the forward diffusion, capturing the concept of blur degradation introduced during continuous exposure. To differentiate between images captured at varying exposure periods, we define sharp and clean (cid:82) α0 image I0, obtained within short, proper exposure time α0 (α0 < αT ), as I0 = 1 τ =0 H(τ ) dτ, α0 where I0 RHW 3 represents the sharp image captured with minimal blur. The contrast between and I0 indicates the effect of exposure duration on motion blur, meaning longer exposure αT introduces more blur, whereas proper exposure α0 yields sharp image. Our objective is to progressively add noise and blur to I0 based on the blur formation process. The dual noise and blur diffusion process at the next time step can be defined as I1 = = (cid:90) α1 τ = I0 + 1 α1 α0 α1 H(τ ) dτ + β1ϵ1 = 1 α1 (cid:18)(cid:90) α0 τ = H(τ )dτ + (cid:90) α1 τ =α0 (cid:19) H(τ )dτ + β1ϵ 1 α1 (cid:90) α1 τ =α0 H(τ )dτ + β1ϵ1 = α0 α1 I0 + 1 α1 e1 + β1ϵ1, (1) (2) where I1 represents the intermediate blurred and noisy image, corresponding to the exposure period from 0 to α1 (α0 < α1 < αT ), ϵ1 is pure Gaussian noise, β1 denotes the noise scaling coefficient, and e1 = (cid:82) α1 H(τ )dτ is the blur residual that accumulates from α0 to α1. Based on (2), the forward transition at time is defined as τ =α It = αt1 αt It1 + 1 αt et + βtϵt, (3) where ϵt (0, I) and et = (cid:82) αt αt during the exposure process. τ =αt1 H(τ )dτ denotes the blur residual accumulating from αt1 to From (3), each forward step from It1 to It is Gaussian transition, where the blur residual et introduces deterministic mean shift to the distribution. Specifically, the transition distribution is q(It It1, et) = It; (cid:18) αt1 αt It1 + (cid:19) et, β2 . 1 αt (4) By iterating (4), we generate sequence of progressively blurred and noisy images {I1, I2, . . . , IT } through -step diffusion process. The complete forward sampling probability is therefore given by q(I1:T I0, e1:T ) = (cid:89) t=1 q(It It1, et). (5) However, existing deblur datasets typically consist of blurry-sharp image pairs without providing the corresponding blur residuals. To address this limitation, we reparameterize (5) to obtain the conditional probability distribution q(IT I0, e1:T ) [10], as shown in (6). The full derivation is provided in Appendix A.1. q(IT I0, e1:T ) = IT ; (cid:32) α0 αT I0 + 1 αT (cid:88) t= (cid:33) et, β2 , where βT = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) t=1 (cid:19)2 (cid:18) αt αT β2 . (6) The final blurred and noisy image IT can be sampled from the distribution q(IT I0, e1:T ) as IT = α0 αT I0 + 1 αT (cid:88) t=1 et + βT ϵ = 1 αT (cid:90) αT τ = H(τ ) dτ + βT ϵ = + βT ϵ, (7) where the final blurred and noisy image IT can be generated in single step by adding noise to the input blurred image B. This formulation preserves the Gaussian nature of the diffusion process while embedding the blur information directly into the mean of the distribution through physically grounded shift. Next, we detail the dual denoising and deblurring formulation for the reverse generation process. 3.2 Dual Denoising and Deblurring Process In the reverse generation process, we aim to progressively remove both noise and blur from the degraded image IT to recover the sharp image I0, based on our dual denoising and deblurring framework. Unlike standard diffusion models that start from pure Gaussian noise, our method samples the terminal observation IT from Gaussian distribution (IT ; B, β2 I), where is fully blurred input image. To reconstruct I0, we use blur residual estimator eθ(It, t, B) and noise estimator ϵθ(It, t, B) to approximate the respective components, et and ϵt, at each step. Inspired by the deterministic sampling formulation in DDIM [38], we define the reverse transition distribution as pθ(It1 It) = qσ(It1 It, θ 0 , eθ 1:t), where The transition probability qσ is defined as 0 , eθ 1:t) qσ(It1 It, θ (cid:32) = It1; α0 αt1 θ 0 + 1 αt1 t1 (cid:88) i= eθ + (cid:113) β2 t1 σ2 θ 0 = αt α0 It 1 α0 (cid:88) i=1 eθ αt α0 βtϵθ. (8) (9) It (cid:16) α0 αt (cid:17) (cid:80)t i=1 eθ (cid:33) , , σ2 0 + 1 θ αt βt with variance term σ2 = η simplifies the reverse step to β2 t1 β2 β2 . When η = 0, this yields deterministic sampling, which It1 = αt αt1 It 1 αt1 eθ(It, t, B) ( βt αt αt1 βt1)ϵθ(It, t, B). (10) Figure 3: Architecture of the Sharp/Blur Encoders (a), Blur/Noise Estimators (b), and the Prior Fusion Module (c). Complete derivations of the variational lower bound, optimization objectives, and sampling formulation are provided in Appendix A.2 and Appendix A.3. In the following, we detail the optimization of the blur residual estimator eθ and the noise estimator ϵθ in the latent space for BlurDM. 3.3 Latent BlurDM To efficiently integrate BlurDM into deblurring networks, we develop it in the latent space, where it serves as prior generator to enhance existing deblurring methods. Inspired by previous work [4, 47], we adopt three-stage training strategy for effective integration, as illustrated in Fig. 2. This process guides the latent features to capture physically meaningful representations for blur residuals, allowing the model to encode exposure-aware information in the latent space. First Stage. We begin by pre-training the deblurring networks with the Sharp Encoder (SE) and the Prior Fusion Module (PFM). Specifically, given blurred image RHW 3 and its sharp counterpart RHW 3, we concatenate them to feed into the SE to obtain the sharp prior as = SE(Concate(B, S)) R11C. Subsequently, we fuse with the decoder features Fi Rhiwici at each scale of deblurring network using PFM, generating the fused features of the i-th scale. Specifically, PFM generates the affine parameters S,αi R11ci and S,βi R11ci from by linear transformation, and modulate as (Z S,αi, S,βi) = Linear(Z S), = S,αi Fi + S,βi, (11) where and + denote channel-wise multiplication and addition, respectively. Thus, we can generate deblurred image RHW 3, enhanced by the sharp prior S, by supervising with the sharp image S. Second Stage. Since sharp images are unavailable during testing, we estimate the sharp prior from the blurred image using the proposed BlurDM, treating as the ground-truth prior at this stage. To achieve this, we employ Blur Encoder (BE), structurally identical to SE, to generate R11C from B. Next, we introduce noise into following (6) to obtain , defined as = + βT ϵ, aligning the diffusion process with blur formation. Finally, we iteratively remove using (10) to generate the diffusion prior both noise and blur from 0 via t1 = αt αt1 1 αt1 eθ(Z t , t, B) ( βt αt αt1 βt1)ϵθ(Z , t, B), (12) which is used to estimate the sharp prior S. Recent studies [4, 8, 35, 47] reveal that supervision on the final output can effectively influence the entire diffusion trajectory. We define latent-prior loss Lprior = (cid:13) 0 is obtained by recursively removing estimated blur and noise residuals from IT via the shared estimators eθ and ϵθ. By back-propagating Lprior, gradients are distributed across all reverse steps, furnishing amortised trajectory-level supervision without step-wise labels. Further details are provided in Appendix A.2. (cid:13)1 , where 0 S(cid:13) (cid:13)Z Third Stage. We jointly optimize the pre-trained BE, BlurDM, PFM, and deblurring networks from the first and second stages to generate the deblurred image O, ensuring that the learned diffusion prior 0 effectively enhances deblurring performance. We supervise with using the loss function originally designed for the given deblurring network. After this stage, the final model we obtain consists of BE, BlurDM, and the deblurring network for inference. We further provide the theoretical justification of latent BlurDM in Appendix A.4. 6 Table 1: Quantitative results on GoPro, HIDE, RealBlur-J, and RealBlur-R datasets, where Baseline and BlurDM denote the image deblurring performances without and with BlurDM, respectively. Arrows indicate the direction of improvement (PSNR, SSIM, LPIPS). GoPro HIDE RealBlur-J RealBlur-R Method MIMO-UNet Stripformer FFTformer LoFormer Baseline 32.44 BlurDM 32.93 Baseline 33.09 BlurDM 33.53 Baseline 34.21 BlurDM 34.34 Baseline 33.54 BlurDM 33.70 PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.968 0.0215 0.972 0.0172 0.974 0.0138 0.977 0.0115 0.973 0.0149 0.975 0.0136 0.974 0.0148 0.976 0.0127 +0.31 +0.003 -0.0013 +0.32 +0.004 -0.0025 +0.78 +0.008 -0.0047 +0.69 +0.003 -0.0025 0.918 0.0345 39.03 0.926 0.0264 39.63 0.929 0.0222 39.84 0.938 0.0175 41.00 0.933 0.0220 40.11 0.939 0.0195 40.55 0.932 0.0223 40.36 0.941 0.0189 40. 0.930 0.0217 31.59 0.939 0.0168 32.13 0.940 0.0147 32.48 0.944 0.0122 33.53 0.946 0.0153 32.62 0.947 0.0145 32.92 0.943 0.0176 32.23 0.944 0.0158 33.47 0.957 0.0115 30.00 0.961 0.0091 30.73 0.962 0.0085 31.03 0.966 0.0074 31.36 0.969 0.0067 31.62 0.970 0.0060 31.76 0.966 0.0084 31.18 0.967 0.0073 31.27 Average Gain"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Implementation Details. Fig. 3 illustrates the architectural design of the four components in BlurDM: the Sharp Encoder (SE), Blur Encoder (BE), BlurDM, and Prior Fusion Module (PFM). Specifically, SE and BE have the same network architecture, each with six residual blocks, four CNN layers, and two MLP layers. BlurDM contains noise and blur residual estimators, each comprising six MLP layers. PFM consists of one MLP layer. We empirically set = 5 in BlurDM, with β1:T increasing uniformly from 0 to 0.02 and α0:T increasing uniformly from 0 to 1. The overall framework (Third Stage) is optimized using the default training settings of each deblurring model, including learning rate, number of epochs, batch size, optimizer, etc., to ensure fair comparisons. Deblurring Models and Datasets. We adopt four prominent deblurring models, including MIMOUNet [5], Stripformer [40], FFTformer [13], and LoFormer [21], to validate the effectiveness of BlurDM. Following previous work [5, 13, 21, 40], we adopt the widely used GoPro [24] and HIDE [36] datasets. The GoPro dataset contains 2, 103 image pairs for training and 1, 111 image pairs for testing, while the HIDE dataset contains 2, 025 image pairs used only for testing. Additionally, we utilize the real-world RealBlur [31] dataset, which contains RealBlur-J and RealBlur-R subsets. Each subset contains 3, 758 training pairs and 980 testing pairs, with RealBlur-J in JPEG and RealBlur-R in Raw format. 4.2 Experimental Results Quantitative Analysis. As shown in Tab. 1, we compare the deblurring performance of four baselines and their BlurDM-enhanced versions, where Baseline and BlurDM refer to the deblurring performance without and with BlurDM, respectively. The results indicate that BlurDM consistently and significantly enhances deblurring performance, yielding average PSNR improvements of 0.31 dB, 0.32 dB, 0.78 dB, and 0.69 dB on the GoPro, HIDE, RealBlur-J, and RealBlur-R test sets, respectively. Additionally, BlurDM achieves average PSNR improvements of 0.59 dB, 0.75 dB, 0.25 dB, and 0.51 dB on MIMO-UNet, Stripformer, FFTformer, and LoFormer, respectively. Notably, BlurDM achieves substantial performance gains, up to 0.73 dB, 1.16 dB, 0.44 dB, and 1.24 dB for MIMO-UNet, Stripformer, FFTformer, and LoFormer, respectively. On average across all backbones and datasets, BlurDM achieves an overall gain of 0.53 dB in PSNR, 0.004 in SSIM, and reduction of 0.0028 in LPIPS. These comprehensive quantitative results demonstrate that BlurDM substantially enhances the performance of deblurring models across diverse datasets, highlighting BlurDMs effectiveness and robustness as flexible prior generation network for image deblurring. Qualitative Analysis. We provide qualitative comparisons of four baselines and their BlurDMenhanced versions on the GoPro and HIDE test sets in Fig. 4 and RealBlur-J test set in Fig. 5. The results show that BlurDM consistently produces sharper and more visually appealing deblurred results than \"Baseline.\" By integrating BlurDM into the latent space of deblurring network, we leverage its ability to learn rich and realistic image priors while preserving the networks fidelity to sharp image contents. 7 Figure 4: Qualitative results on the GoPro (left) and HIDE (right) datasets. Figure 5: Qualitative results on the RealBlur-J dataset. Figure 6: Deblurred results I5 to I0 from latent features steps increase. 5 to 0 , showing reduced blur as reverse 4.3 Ablation studies To evaluate the effect of the proposed components in BlurDM, we adopt MIMO-UNet as the baseline deblurring model and analyze its performance under various ablation settings. Specifically, we analyze the effectiveness of noise and blur residual estimators, compare different prior generation methods, analyze blur residual modeling in the latent space, inspect the effect of iteration counts, i.e., , in BlurDM, compare different diffusion-based methods, analyze the effectiveness of each training stage, and measure the computational overhead introduced by BlurDM. All experiments are conducted with 1, 000 training epochs used. Effectiveness of Noise and Blur Estimators. We evaluate the effectiveness of the noise and blur estimators in BlurDM through an ablation study shown in Tab. 2. Net1 denotes the baseline deblurring model. Net2 represents conventional DDPM-based design using only the noise estimator. Net3 denotes BlurDM variant that estimates blur residuals but omits the noise component. Net4 is our complete BlurDM design incorporating both estimators. As can be seen, both Net2 and Net3 improve performance over the baseline, which demonstrates the individual benefit of noise and blur estimation. Moreover, Net4 achieves the best result, showing that combining both estimators yields complementary gains. These findings further confirm the importance of explicitly modeling blur residuals to improve deblurring effectiveness. Comparison of Prior Generation Methods. We evaluate the performance of the baseline deblurring model enhanced by various prior generation methods, including MLP, DDPM [10], RDDM [17], and our proposed BlurDM, on the GoPro and RealBlur-J datasets (see Tab. 3). Net1 denotes the baseline model without guidance by prior generation network. Net2 denotes the deblurring model enhanced with MLP layers, without using diffusion process. Net3, Net4, and Net5 corre8 Table 2: Effectiveness of the noise estimator and the blur estimator on the GoPro test set. Noise Estimator Blur Estimator PSNR 31.78 31.91 32.20 32.28 Net1 Net2 Net3 Net4 Table 3: Comparison of different prior generators on GoPro and RealBlur-J datasets in PSNR. Prior Generators GoPro RealBlur-J Net1 Net2 Net3 Net4 Net5 N/A MLP DDPM RDDM BlurDM 31.78 31.90 31.91 32.03 32.28 31.59 31.84 31.85 31.90 32.13 spond to the deblurring models enhanced by different diffusion-based priors, including DDPM [10], RDDM [17], and the proposed BlurDM, respectively. While integrating the standard diffusion process (DDPM) into the deblurring model improves performance compared to the baseline (Net3 vs. Net1), the gain is comparable to that of Net2, which uses the same MLP structure without diffusion. This suggests that the standard diffusion process alone contributes little to deblurring performance. In contrast, BlurDM explicitly incorporates the blur formation process into diffusion, leading to superior performance over both the standard diffusion-based prior (Net5 vs. Net3) and the residual diffusion prior (RDDM), which lacks the proposed blur-aware diffusion mechanism (Net5 vs. Net4). Analysis of Blur Residual Modeling in Latent Space. To verify whether BlurDM models blur formation in the latent space, we analyze outputs at different reverse diffusion steps during inference. While the model is trained with = 5 steps, we evaluate intermediate latent representations by performing = [0, 1, 2, 3, 4, 5] reverse steps from the fully blurred latent 5 , yielding sequence [Z is decoded into an image It via the deblurring network. As illustrated in Fig. 6, the outputs transition progressively from blurred (I5) to sharp (I0), confirming that BlurDMs latent representation captures progressive blur-to-sharp structure and enables interpretable modeling in latent space. 0 ]. Each 4 , . . . , 5 , Effect of Iteration Counts in BlurDM. Compared to the standard diffusion model in [10], which requires thousands of iterations in the reverse generation process, applying diffusion networks in the latent space has proven effective in reducing the number of iterations [4, 47]. Therefore, we examine the effect of different iteration counts used in BlurDM on deblurring performance, as shown in Fig. 7. Specifically, we test eight iteration settings {0, 1, 2, 4, 5, 6, 8, 10} and evaluate their deblurring performances on the GoPro test set. The results show that BlurDM significantly improves performance with two iterations and reaches peak performance after five, showcasing its ability to achieve substantial and stable performance gains with only few iterations. Analysis of BlurDMs Computational Overhead. We present the computational overhead introduced by BlurDM in Tab. 4, measuring FLOPs and inference time on 256 256 image using an NVIDIA GeForce RTX 3090. The results show that BlurDM introduces only slight increase in computational complexity while significantly improving deblurring performance. Specifically, BlurDM adds an average of just 4.16G FLOPs, 3.33M parameters, and 9 milliseconds across four deblurring models, demonstrating its effectiveness with minimal overhead. Note that the number of parameters varies across different deblurring models, as PFM must adapt to the varying channel dimensions of each decoder. Comparison of different diffusion-based methods. We compare BlurDM with two recent diffusion-based approaches, HI-Diff [4] and RDDM [18], in Tab. 5. HI-Diff follows the conventional diffusion process in the latent space, where Gaussian noise is progressively injected directly into the latent until it becomes pure noise, and learned reverse process is used to reconstruct clean latent for deblurring. RDDM first forms an image space residual by subtracting the clean image from the degraded one, then performs diffusion on the clean image that jointly models this residual and Gaussian noise. Both HI-Diff and RDDM neglect the physics of blur formation. BlurDM addresses this gap by explicitly integrating the blur formation process with diffusion, executing dual noise and blur diffusion that matches the physics of blur accumulation. With comparable parameter counts and FLOPs, BlurDM consistently delivers higher PSNR and SSIM. As plug-and-play module, it integrates seamlessly with diverse backbone architectures, demonstrating both strong performance and broad generalizability. 9 Table 4: Computational overhead comparison between baseline deblurring models and their BlurDMenhanced versions. Method FLOPs (G) Params (M) Time (ms) Figure 7: Effect of the number of iterations in BlurDM on deblurring performance in PSNR on the GoPro dataset. Table 5: Comparison of different diffusion-based methods on the GoPro dataset. Method HI-Diff RDDM BlurDM (Stripformer) BlurDM (FFTformer) BlurDM (LoFormer) PSNR SSIM Params (M) 33.33 32.40 33.53 34.34 33.70 23.99 15.49 24.33 18.66 19. 0.955 0.963 0.966 0.970 0.967 FLOPs (G) 125.47 134.20 174.18 135.69 56.35 MIMO-Unet+ Stripformer FFTformer LoFormer-S Baseline +BlurDM 158.10 (+4.17) 18.29 (+2.18) 42 (+11) 153.93 16.11 31 Baseline +BlurDM 174.18 (+4.16) 24.33 (+4.62) 55 (+7) 170. 19.71 48 Baseline +BlurDM 135.69 (+4.16) 18.66 (+3.78) 141 (+10) 131.53 14.88 Baseline +BlurDM 56.35 (+4.16) 52.19 16.35 19.08 (+2.73) 99 (+6) 93 Table 6: Effect of each training stage on the GoPro dataset. Model Net1 Net2 Net3 Net4 Net5 Net Stage 1 Stage 2 Stage 3 PSNR 31.78 (baseline) 32.69 (upper bound) 31.80 32.01 31.95 32.28 Effectiveness of Each Training Stage. We evaluate the effectiveness of the three-stage training strategy, as shown in Tab. 6. Net1 denotes the baseline deblurring performance without incorporating BlurDM. In Net2, the ground-truth sharp image is passed through the Sharp Encoder to obtain the sharp prior S, which is then used by BlurDM. This setting serves as an upper bound on achievable deblurring performance with an ideal prior. In Net3, we jointly optimize BlurDM and the deblurring model without pretraining through Stage 1 and Stage 2, serving as baseline for purely data-driven approach. In Net4 and Net5, after completing pre-training in Stage 1, we apply either Stage 2 or Stage 3 alone to optimize BlurDM and the deblurring model. Net6 employs the full three-stage training pipeline, achieving the highest PSNR among all settings. These results clearly demonstrate the effectiveness and necessity of the proposed three-stage training strategy in improving deblurring performance."
        },
        {
            "title": "5 Limitations",
            "content": "Since BlurDM is designed based on the motion blur formation process, it effectively handles blur caused by camera motion and moving objects. However, it may not be well-suited for handling defocus blur, which arises from optical aberrations due to out-of-focus issues. Unlike motion blur, defocus blur is depth-dependent and does not exhibit the same temporal accumulation properties, making it fundamentally different in nature. Addressing defocus deblurring would require distinct approach, potentially incorporating depth estimation or optical defocus modeling, which remains an open direction for future research."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed Blur Diffusion Model (BlurDM), novel diffusion-based framework for image deblurring. BlurDM integrates the blur formation process into the diffusion framework, simultaneously performing noise diffusion and blur diffusion for more effective deblurring. In the forward process, BlurDM progressively degrades sharp image by introducing both noise and blur through dual noise and blur diffusion process. Conversely, in the reverse process, BlurDM restores the image by removing noise and blur residuals via its dual denoising and deblurring process. To enhance the performance of existing deblurring networks, we incorporated BlurDM into their latent spaces as prior generator, seamlessly integrating the learned prior into each decoder block via our proposed Prior Fusion Module (PFM) to generate higher-quality deblurring results. Extensive experimental results have demonstrated that our method effectively improves deblurring performance across four deblurring models on four deblurring datasets."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work was supported in part by the National Science and Technology Council (NSTC) under grants 114-2221-E-A49-038-MY3, 112-2221-E-A49-090-MY3, 113-2221-E-004-001-MY3, 113-2634-F002-008, and 114-2221-E-007-065-MY3, and by the NVIDIA Taiwan AI Research & Development Center (TRDC)."
        },
        {
            "title": "References",
            "content": "[1] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and Stéphane Lathuilière. Collaborating foundation models for domain generalized semantic segmentation. In CVPR, 2024. [2] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021. [3] Kang Chen and Yuanjie Liu. Efficient image deblurring networks based on diffusion models. arXiv, 2024. [4] Zheng Chen, Yulun Zhang, Liu Ding, Xia Bin, Jinjin Gu, Linghe Kong, and Xin Yuan. Hierarchical integration diffusion model for realistic image deblurring. In NeurIPS, 2023. [5] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung, and Sung-Jea Ko. Rethinking coarse-to-fine approach in single image deblurring. In ICCV, 2021. [6] Hongyun Gao, Xin Tao, Xiaoyong Shen, and Jiaya Jia. Dynamic scene deblurring with parameter selective sharing and nested skip connections. In CVPR, 2019. [7] Tomer Garber and Tom Tirer. Image restoration by denoising diffusion models with iteratively preconditioned guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [8] Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, and Zico Kolter. Consistency models made easy. In ICLR, 2025. [9] Jin-Ting He, Fu-Jen Tsai, Jia-Hao Wu, Yan-Tsung Peng, Chung-Chi Tsai, Chia-Wen Lin, and Yen-Yu Lin. Domain-adaptive video deblurring via test-time blurring. In ECCV, 2024. [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [11] Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, and Seungryul Baek. Sddgr: Stable diffusion-based deep generative replay for class incremental object detection. In CVPR, 2024. [12] Minchul Kim, Yiyang Su, Feng Liu, Anil Jain, and Xiaoming Liu. Keypoint relative position encoding for face recognition. In CVPR, 2024. [13] Lingshun Kong, Jiangxin Dong, Jianjun Ge, Mingqiang Li, and Jinshan Pan. Efficient frequency domain-based transformers for high-quality image deblurring. In CVPR, 2023. [14] Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, and Lei Zhao. Rethinking diffusion model for multi-contrast mri super-resolution. In CVPR, 2024. [15] Xiaopan Li, Shiqian Wu, Xin Yuan, Shoulie Xie, and Sos Agaian. Hierarchical wavelet-guided diffusion model for single image deblurring. The Visual Computer, 2024. [16] Hanyan Liang, Shuyao Chai, Xixuan Zhao, and Jiangming Kan. Swin-diff: single defocus image deblurring network based on diffusion model. Complex & Intelligent Systems, 2025. [17] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, and Liangqiong Qu. Residual denoising diffusion models. In CVPR, 2024. [18] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, and Liangqiong Qu. Residual denoising diffusion models. In CVPR, 2024. [19] Yuhao Liu, Zhanghan Ke, Fang Liu, Nanxuan Zhao, and Rynson W.H. Lau. Diff-plugin: Revitalizing details for diffusion-based low-level tasks. In CVPR, 2024. [20] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH, 2024. [21] Xintian Mao, Jiansheng Wang, Xingran Xie, Qingli Li, and Yan Wang. Loformer: Local frequency transformer for image deblurring. In ACM MM, 2024. [22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. [23] Yuxi Mi, Zhizhou Zhong, Yuge Huang, Jiazhen Ji, Jianqing Xu, Jun Wang, Shaoming Wang, Shouhong Ding, and Shuigeng Zhou. Privacy-preserving face recognition using trainable feature subtraction. In CVPR, 2024. [24] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, 2017. [25] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2022. [26] D. Park, D. U. Kang, J. Kim, and S. Y. Chun. Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training. In ECCV, 2020. [27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. [28] Chen Rao, Guangyuan Li, Zehua Lan, Jiakai Sun, Junsheng Luan, Wei Xing, Lei Zhao, Huaizhong Lin, Jianfeng Dong, and Dalong Zhang. Rethinking video deblurring with waveletaware dynamic transformer and diffusion model. In ECCV, 2024. [29] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. Multiscale structure guided diffusion for image deblurring. In ICCV, 2023. [30] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. Multiscale structure guided diffusion for image deblurring. In ICCV, 2023. [31] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In ECCV, 2020. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, June 2022. [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. [35] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. [36] Ziyi Shen, Wenguan Wang, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. Humanaware motion deblurring. In ICCV, 2019. [37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [39] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep image deblurring. In CVPR, 2018. [40] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, and Chia-Wen Lin. Stripformer: Strip transformer for fast image deblurring. In ECCV, 2022. [41] Jiabao Wang, Yuming Chen, Zhaohui Zheng, Xiang Li, Ming-Ming Cheng, and Qibin Hou. Crosskd: Cross-head knowledge distillation for object detection. In CVPR, 2024. [42] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In CVPR, 2022. [43] Simon Weber, Bar Zöngür, Nikita Araslanov, and Daniel Cremers. Flattening the parent bias: Hierarchical semantic segmentation in the poincare ball. In CVPR, 2024. [44] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros Dimakis, and Peyman Milanfar. Deblurring via stochastic refinement. In CVPR, 2022. [45] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform deblurring for shaken images. In CVPR, 2010. [46] Jia-Hao Wu, Fu-Jen Tsai, Yan-Tsung Peng, Chung-Chi Tsai, Chia-Wen Lin, and Yen-Yu Lin. Id-blau: Image deblurring by implicit diffusion-based reblurring augmentation. In CVPR, 2024. [47] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In ICCV, 2023. [48] Tian Ye, Sixiang Chen, Wenhao Chai, Zhaohu Xing, Jing Qin, Ge Lin, and Lei Zhu. Learning diffusion texture priors for image restoration. In CVPR, 2024. [49] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. [50] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In CVPR, 2021. [51] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In CVPR, 2021. [52] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Koniusz. Deep stacked hierarchical multi-patch network for image deblurring. In CVPR, 2019. [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [54] Dian Zheng, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, and Wei-Shi Zheng. Selective hourglass mapping for universal image restoration based on diffusion model. In CVPR, 2024."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly describe the key contribution and scope of the paper. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the work in the Section 5 of the paper. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: The full set of assumptions and complete proof are provided in Section 3, Appendix A.1 A.2 A.3 A.4 Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All information needed to reproduce the main experimental results of the paper are provided in Section 4 of the paper. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 15 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our code in supplementary material. We used only open-source datasets for all our experiments. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and testing details are provided in Section 4. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our paper does not include error bars or statistical significance tests because it would be too computationally expensive. However, we report consistent PSNR and SSIM improvements across four benchmark datasets (GoPro, HIDE, RealBlur-J, and RealBlurR) and four different deblurring models. The results in Table 1 show stable and clear performance gains, indicating the robustness and general applicability of BlurDM, even without formal statistical analysis. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 16 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The information on the computer resources need to reproduce the experiments are provided in Section 4. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics in every respect. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The discussion of broader impacts are provided in Appendix A.10. Guidelines: The answer NA means that there is no societal impact of the work performed. 17 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use publicly available datasets (GoPro, HIDE, RealBlur-J/R) and existing models (e.g., MIMO-UNet, Stripformer), all of which are properly cited in our paper with corresponding references. The datasets are commonly used in the community and their licenses are respected as per the information provided by the original sources. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. 18 For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code, inference results, and the model weights introduced in the paper are well documented. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification:The paper does not involve any human subjects or crowdsourcing experiments. All results are derived from objective evaluations on publicly available datasets. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve any human subjects or crowdsourcing, and therefore no IRB approval was required. Guidelines: 19 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method development in this research does not involve LLMs as any important, original, or non-standard components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "A Appendices",
            "content": "A.1 One-Step Diffusion Derivation for BlurDM In the dual noise and blur diffusion process of BlurDM, the forward process is defined as: q(I1:T I0, e1:T ) := (cid:89) t=1 q(It It1, et); q(It It1, et) := It; (cid:18) αt1 αt It1 + (cid:19) , et, β2 1 αt (13) where et = (cid:82) αt τ =αt H(τ ) dτ is the blur residual accumulated during the exposure interval [αt1, αt]. We now expand the full forward process by recursively substituting the previous states. Starting from the last time step as IT = = = ... = αT 1 αT αT 1 αT αT 2 αT IT 1 + (cid:18) αT 2 αT 1 IT 2 + 1 αT eT + βT ϵT IT 2 + 1 αT 1 eT 1 + βT 1ϵT 1 + (cid:19) 1 αT eT + βT ϵT 1 αT (eT 1 + eT ) + αT 1 αT βT 1ϵT 1 + βT ϵT α0 αT I0 + (cid:88) t=1 1 αT et + (cid:88) t=1 αt αT βtϵt. (14) Since ϵt (0, I) are independent for all t, their weighted sum remains Gaussian with zero mean. The resulting variance of this sum is β2 = (cid:88) t=1 (cid:18) αt αT (cid:19)2 β2 , which allows us to reparameterize as: (cid:88) t=1 αt αT βtϵt = βT ϵ, ϵ (0, I). Moreover, combining the clean image component and the accumulated blur residuals, we observe that α0 αT I0 + 1 αT (cid:88) t=1 et = α0 αT 1 α0 (cid:90) α0 0 H(τ ) dτ + 1 αT (cid:90) αT α0 H(τ ) dτ = 1 αT (cid:90) αT 0 H(τ ) dτ = B, where denotes the fully blurred image formed by integrating the instantaneous scene radiance H(τ ) over the total exposure time interval [0, αT ]. Thus, the forward process simplifies to single-step form: IT = + βT ϵ, with the corresponding marginal distribution (cid:32) q(IT I0, e1:T ) = IT ; 1 αT I0 + 1 αT (cid:88) t= (cid:33) et, β2 . (15) (16) This one-step form is mathematically equivalent to the full forward process, while providing more computationally efficient approximation that captures both accumulated blur and noise in single Gaussian transition. 21 A.2 ELBO and Optimization for BlurDM To reconstruct the sharp image I0 from the degraded observation IT , we adopt the variational inference framework of DDPM [10] and derive an evidence lower bound (ELBO) that explicitly incorporates the blur residuals e1:T . The joint ELBO is expressed as (cid:105) (cid:104) log pθ(I0) Eq(I1:T I0,e1:T ) log =: LELBO. pθ(I0:T ) q(I1:T I0,e1:T ) (17) By unrolling the Markov chain formulation in DDPM, we can rewrite the objective as: LELBO = Eq (cid:20) log pθ(I0:T ) q(I1:T I0, e1:T ) (cid:21) = Eq log pθ(IT ) (cid:88) t1 log pθ(It1 It) q(It It1, et) log pθ(IT ) (cid:88) t> log pθ(IT ) (cid:88) t>1 = Eq = Eq (cid:34) (cid:34) (cid:34) log pθ(It1 It) q(It It1, et) log pθ(I0 I1) q(I1 I0, e1) (cid:35) log pθ(It1 It) q(It1 It, I0, e1:t) q(It1 I0, e1:t) q(It I0, e1:t) log pθ(I0 I1) q(I1 I0, e1) (cid:35) = Eq log pθ(IT ) q(IT I0, e1:T ) (cid:88) log t>1 pθ(It1 It) q(It1 It, I0, e1:t) (cid:35) log pθ(I0 I1) . (18) (19) (20) (21) (22) (23) Rewriting (22) in terms of KL-divergence yields: LELBO . = Eq DKL(q(IT I0, e1:T ) pθ(IT )) + (cid:88) t1 DKL(q(It1 It, I0, e1:t) pθ(It1 It)) log pθ(I0 I1) (24) Unlike standard diffusion models, which assume standard Gaussian prior at the terminal state, our model defines the prior distribution as: pθ(IT ) = (IT ; B, β2 I), where denotes the physically blurred image obtained from full exposure integration. This formulation ensures that the terminal distribution is aligned with the mean of the forward marginal q(IT I0, e1:T ). Therefore, our prior is not an arbitrary isotropic Gaussian but noise-perturbed version of blurry observation, consistent with the forward process structure. Given this structural alignment between the forward marginal and the prior, we follow the standard approach of DDPM [10] and DDIM [38], and omit the terminal KL divergence DKL(q(IT I0, e1:T ) pθ(IT )) during training. Instead, we retain only the stepwise KL divergence terms, which capture the discrepancy between the reverse and forward transitions at each timestep as: (cid:88) t1 DKL(q(It1 It, I0, e1:t) pθ(It1 It)). To compute these terms, we derive the posterior q(It1 It, I0, e1:t) using Bayes rule: q(It1 It, I0, e1:t) = q(It It1, I0, e1:t) q(It1 I0, e1:t) q(It I0, e1:t) . From (6), we have q(It1 I0, e1:t) = It1; (cid:32) 1 αt1 I0 + 1 αt1 t1 (cid:88) i= (cid:33) ei, β2 t1I . (25) (26) 22 From (5), the forward transition is expressed as (cid:18) q(It It1, et) = It; αt1 αt It1 + et, β2 (cid:19) . 1 αt Combining the above, we obtain the posterior q(It1 It, I0, e1:t) = (cid:0)It1; µt(It, I0, e1:t), σ2 exp (cid:18) 1 2 (cid:18) (It αt1 αt It1 1 αt β2 et) + (It1 1 αt1 (It, I0, e1:t)I(cid:1) , i=1 ei)2 I0 1 β2 (cid:80)t1 αt t1 (It 1 αt I0 1 αt β2 (cid:80)t i=1 ei) which can be further simplified to (cid:32) (cid:32) (cid:32) exp 1 2 β2 β2 β2 t1 2 t1 2 αt1 αtβ2 It αt1 β2 α2 et + 1 αt1 β2 t1 I0 + 1 αt1 β2 t1 t1 (cid:88) i=1 where = C(It, I0, e1:t) denotes the terms not involving It1. From (29), the posterior parameters are given by It αt1 β2 α2 αt1 αtβ2 et + µt(It, I0, e1:t) = I0 + 1 αt1 β2 t1 (cid:80)t1 i=1 ei (cid:33) ei It1 + , 1 αt1 β2 t1 β2 β2 β2 βt β2 αt αt t1 ϵ, = σ2 (It, I0, e1:t) = 1 αt et It αt αt1 β2 β2 t1 β2 , (27) (cid:19)(cid:19) , (28) (cid:33)(cid:33) (29) (30) (31) (32) where µt(It, I0, e1:t) denotes the posterior mean, which is derived by combining (6) and (4) through the product of Gaussian densities. σ2 (It, I0, e1:t) represents the corresponding posterior variance. We model the reverse process beginning at pθ(IT ) = (IT ; B, β2 I), and define pθ(It1 It) = q(It1 It, θ 0 , θ res). In our setting, since the variances of the two Gaussian distributions are matched exactly, the KL divergence reduces to squared difference between their means, as is standard in DDPM [10]. Accordingly, the KL divergence term in (25) reduces to DKL(q(It1 It, I0, e1:t) pθ(It1 It)) = (cid:104)(cid:13) (cid:13)µt µθ 2(cid:105) (cid:13) (cid:13) , (33) where the mean of the true posterior is given by µt = αt αt1 It 1 αt et αt αt1 β2 βt ϵ, and the model-predicted mean is αt αt1 = µθ It 1 αt eθ(It, t, B) αt αt1 β2 βt ϵθ(It, t, B), where eθ(It, t, B) and ϵθ(It, t, B) denote the learned blur residual and noise estimators, respectively. Based on (4) and (33), we can derive the following optimization objectives Let(θ) = (34) 2(cid:105) , (cid:104) λe (cid:104) λϵ (cid:13) (It, t, B)(cid:13) (cid:13)et eθ (cid:13) 2(cid:105) (cid:13)ϵ ϵθ(It, t, B)(cid:13) (cid:13) (cid:13) Lϵ(θ) = . (35) Thus, the optimization of BlurDM reduces to minimizing the combined loss in (34) and (35), which directly supervises both the blur residual estimator and the noise residual estimator through their respective ground-truth signals. 23 End-to-End Trajectory Supervision via Final Reconstruction Although per-step ground-truth blur residuals are unavailable, recent diffusion-based research [4, 8, 35, 47] has shown that supervising the generated results is sufficient to train the diffusion model. Following this line of evidence, we train BlurDM with the reconstruction objective Lrec = (cid:13) (cid:13)I θ 0 I0 (cid:13) (cid:13) , (36) 0 is obtained by successively denoising the degraded observation IT through learned = eθ(It, t, B) and noise where θ reverse steps. At each step {T, . . . , 1}, the network predicts blur residual ˆeθ = ϵθ(It, t, B), then reconstructs the previous latent state via residual ˆϵθ αt αt1 1 αt1 αt αt1 θ t1 = ˆeθ It β2 βt ˆϵθ ."
        },
        {
            "title": "This yields the unrolled trajectory",
            "content": "I θ = IT , t1 = gθ θ 0 = gθ θ (I θ ), 1 gθ (IT ), = T, . . . , 1, (37) (38) (39) (40) where every step operator gθ shares parameters θ. Backpropagating Lrec supplies gradients to all intermediate residual predictions {ˆeθ t=1, allowing amortized trajectory level optimization despite the lack of explicit stepwise supervision. This strategy mirrors the unrolled inference paradigm in generative models based on variation and scores and empirically produces stable convergence with strong deblurring fidelity. , ˆϵθ }T A.3 Deterministic Implicit Sampling for BlurDM In this section, we provide formal derivation to demonstrate that our deterministic reverse process preserves the forward process distribution defined in (6), i.e., qσ(It1 It, I0, e1:t), q(It I0, e1:t) = It; (cid:32) α0 αt I0 + 1 αt (cid:88) i=1 (cid:33) ei, β2 . We follow the approach of DDIM [38] and proceed by mathematical induction from = to = 1. Assuming that the marginal distribution q(It I0, e1:t) is valid at step t, we aim to prove that sampling It1 from qσ(It1 It, θ 0 , e1:t) yields distribution consistent with q(It1 I0, e1:t1) = It1; (cid:32) α0 αt1 I0 + 1 αt1 t1 (cid:88) i= (cid:33) ei, β2 t1I . Let us begin by rewriting the marginal at time q(It I0, e1:t) = It; (cid:32) α0 αt I0 + 1 αt (cid:88) i=1 (cid:33) ei, β2 . (41) We define the reverse transition distribution using the deterministic implicit sampling formulation: qσ(It1 It, I0, e1:t) = It1; α0 αt1 (cid:124) I0 + 1 αt1 t1 (cid:88) i=1 (cid:113) ei + β2 t1 σ2 It (cid:16) α0 αt (cid:17) (cid:80)t i=1 ei , σ2 0 + 1 θ αt βt (cid:123)(cid:122) µt1 24 (cid:125) (42) . (43) (44) (45) (46) (47) (48) We now compute the implied marginal distribution over It1 by integrating out It, using the properties of marginal and conditional Gaussians. Let the conditional be p(It1 It) and the marginal q(It), then the marginal of It1 becomes q(It1 I0, e1:t) = (cid:90) qσ(It1 It, I0, e1:t) q(It I0, e1:t) dIt. By applying the formula for Gaussian marginalization over linear Gaussian transformations, we obtain (cid:113) ei + β2 t1 σ2 (cid:16) α0 αt I0 + 1 αt (cid:80)t i=1 ei (cid:17) βt (cid:16) α0 αt I0 + 1 αt (cid:80)t i=1 ei (cid:17) Mean: µt1 = α0 αt1 I0 + 1 αt1 = α0 αt1 I0 + 1 αt1 t1 (cid:88) i=1 t1 (cid:88) i=1 ei, with variance term σ2 = η β2 t1 β2 β2 . When η = 0, this yields deterministic sampling. Variance: t1I = σ2 σ2 + (cid:113) β t1 σ2 βt 2 β2 (cid:19) (cid:18) β t1 σ2 β2 β2 = σ2 + = β2 t1I. Hence, the marginal distribution at step t1 becomes q(It1 I0, e1:t1) = It1; (cid:32) α0 αt1 I0 + 1 αt1 t1 (cid:88) i=1 (cid:33) ei, β2 t1I , which confirms that (6) holds at step t1. By induction, the deterministic sampling formulation maintains consistency with the original forward process distribution at every timestep. Derivation from (9) to (10) Based on (9), we can sample It1 from qσ(It1 It, θ 0 , eθ 1:t) as It1 = α0 αt1 θ 0 + 1 αt1 t1 (cid:88) i= (cid:113) eθ + β2 t1 σ2 It ( α0 αt (cid:80)t i=1 eθ ) 0 + 1 θ αt βt + σt, (49) where σ2 with (8). Therefore, (49) can be rewritten as = η β2 β2 β2 , we set η = 0 for the deterministic sampling. In addition, we substitue θ 0 It1 = α0 αt1 ( αt α0 It 1 α0 It ( α0 αt ( αt α0 + βt = αt αt1 It 1 αt1 (cid:88) eθ i=1 It 1 α0 (cid:80)t αt α0 βtϵθ) + αT i=1 eθ α0 βt βt αt αt1 eθ t1 (cid:88) 1 αt1 i=1 βtϵθ) + 1 αt (cid:80)t i=1 eθ ) , eθ(It, t, B) ( βt1)ϵθ(It, t, B). (50) (51) (52) 25 Table 7: Comparison of training cost and performance between baseline and BlurDM. Method Training epoch Training time [h] PSNR [dB] 3000 6000 Baseline 1 Baseline 2 BlurDM 1 1500 (Stage 1) + 500 (Stage 2) + 1500 (Stage 3) 3000 (Stage 1) + 500 (Stage 2) + 1500 (Stage 3) BlurDM 2 3000 (Stage 1) + 500 (Stage 2) + 3000 (Stage 3) BlurDM 3 66.7 133.4 70.7 104.1 141. 32.44 32.51 32.62 32.71 32.93 A.4 Theoretical justification of latent BlurDM. Let zt = E(It) be the latent feature produced by an encoder E() from the blurred image It at exposure step t. firstorder Taylor expansion of around It1 gives zt zt1 + JE(It1) (It It1), where JE(It1) is the Jacobian of the encoder at It1. From the imagespace exposure model, It It1 = (cid:16) αt1 αt (cid:17) 1 It1 + 1 αt et + βt εt, with et the blur residual and εt the stochastic noise at step t. Substituting yields (cid:104)(cid:16) αt1 αt When exposure increments are small so that αt1 αt we obtain the latentspace dynamics zt zt1 + JE(It1) zt (cid:17) (cid:16) αt1 αt zt1 + 1 αt + βt εθ eθ , (cid:17) 1 It1 + 1 αt et + βtεt (cid:105) . 1, the term proportional to It1 vanishes and := JE(It1) et, εθ eθ := JE(It1) εt, (56) (53) (54) (55) where θ denotes the learnable parameters of E. Thus, blur residuals and stochastic noise in image space are projected into the latent space with the same coefficients, justifying that the two estimators in BlurDM can learn these terms directly in the latent domain. A.5 Training cost of the three-stage training strategy In Tab. 7, we report the training complexity introduced by BlurDM in terms of training time, using MIMO-UNet as the backbone on GoPro dataset. The default number of training epochs for MIMOUNet is 3, 000, denoted as Baseline 1. Increasing the training epochs to 6, 000, denoted as Baseline 2, results in only marginal improvement of 0.07 dB in PSNR. For BlurDM, we experiment with different training configurations, including using 1, 500 or 3, 000 epochs for Stage 1 and Stage 3, with 500 epochs for Stage 2. Both BlurDM 1 and BlurDM 2 outperform Baseline 2 while requiring less training time, demonstrating the effectiveness of the proposed three-stage training strategy. Finally, we adopt BlurDM 3 as our final model, which increases training time by only 8% while achieving 0.42 dB improvement in PSNR compared to Baseline 2. A.6 Performance comparison of the number of steps used in RDDM and BlurDM Tab. 8 presents comparison of RDDM and BlurDM under varying numbers of diffusion steps, where PSNR and LPIPS are used to assess performance. RDDM achieves its peak PSNR of 32.08 dB at step 4 and the best LPIPS score of 0.0121 at step 10. In comparison, BlurDM achieves higher peak PSNR of 32.28 dB at step 5 and lower LPIPS score of 0.0113 at step 10. These results demonstrate that BlurDM consistently outperforms RDDM in terms of both distortion-based (PSNR) and perceptual (LPIPS) quality metrics. A.7 Visualizations of Blur Residual In Fig. 9, we compare blurred images at different exposure times synthesized from the GoPro [24] dataset and the corresponding blur residuals obtained using BlurDM. Here, we depict = 0 as sharp images. As increases (i.e., as the exposure time increases), the images gradually become more blurred, and the corresponding blur residuals evolve accordingly. Eventually, at = 1, the images correspond to the fully blurred versions. 26 Table 8: Performance under different step counts on the GoPro dataset. Method 2 4 5 6 8 PSNR / LPIPS PSNR / LPIPS PSNR / LPIPS PSNR / LPIPS PSNR / LPIPS PSNR / LPIPS RDDM 32.07 / 0.0130 BlurDM 32.21 / 0.0116 32.08 / 0.0131 32.27 / 0.0114 32.03 / 0.0125 32.28 / 0.0114 32.01 / 0.0122 32.22 / 0.0115 32.02 / 0.0122 32.24 / 0.0113 32.02 / 0.0121 32.24 / 0. Figure 8: Comparison of blur residuals between RDDM and BlurDM at = 1. A.8 Comparison between BlurDM and RDDM in Blur Residuals. We visualize and compare blur residuals generated by BlurDM and RDDM [18] in Fig. 8. RDDM computes blur residuals using simple subtraction operation followed by linear scaling to adjust intensity, resulting in residuals that primarily capture direct differences between blurred and sharp images, progressively magnified over time steps (et to et+2). In contrast, BlurDM addresses the inductive bias of blur formation process to estimate blur residuals, capturing the non-linear progressive accumulation of blur. Unlike RDDM, BlurDM models how blurred residuals spatially diffuse and evolve over time, simulating the blur spread as exposure time extends. As result, BlurDM more accurately represents the physical characteristics of motion blur within the diffusion process, leading to significantly improved deblurring performance. A.9 Deblurred Results on Real-world Datasets We provide additional deblurred results for deblurring models using BlurDM, compared to those without using our method, referred to as Baseline. These deblurring models are trained on the GoPro [24] and RealBlur-J [31] training sets, and tested on the RealBlur-J testing set. We demonstrate qualitative comparisons based on four image deblurring models, including MIMO-UNet [5] in Fig. 10, Stripformer [40] in Fig. 11, FFTformer [13] in Fig. 12, and LoFormer [21] in Fig. 13. A.10 Broader Impacts Our work improves the capability of image deblurring by introducing diffusion-based framework that mimics the physical formation of motion blur. This has potential benefits in applications such as autonomous driving, medical imaging, and restoration of historical media. However, as with many image enhancement technologies, there exists risk of misuse, such as reconstructing intentionally blurred faces or sensitive content, which may raise privacy concerns. We encourage the responsible and ethical use of deblurring models and suggest deploying them in contexts with appropriate privacy safeguards and user consent. 27 Figure 9: Visualizations of blur residual from GoPro [24] dataset 28 Figure 10: Qualitative results of MIMO-UNet [5] on the RealBlur-J [31] dataset. Figure 11: Qualitative results of Stripformer [40] on the RealBlur-J [31] dataset. 30 Figure 12: Qualitative results of FFTformer [13] on the RealBlur-J [31] dataset. 31 Figure 13: Qualitative results of LoFormer [21] on the RealBlur-J [31] dataset."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National Chengchi University",
        "National Tsing Hua University",
        "National Yang Ming Chiao Tung University"
    ]
}