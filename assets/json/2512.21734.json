{
    "paper_title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
    "authors": [
        "Steven Xiao",
        "Xindi Zhang",
        "Dechao Meng",
        "Qi Wang",
        "Peng Zhang",
        "Bang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs."
        },
        {
            "title": "Start",
            "content": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation Steven Xiao* Xindi Zhang* Dechao Meng*"
        },
        {
            "title": "Peng Zhang Bang Zhang",
            "content": "Tongyi Lab, Alibaba Group 5 2 0 2 9 2 ] . [ 2 4 3 7 1 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) running ahead\" mechanism that dynamically updates the reference frames temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs. Project page: https://humanaigc.github. io/knot_forcing_demo_page/. 1. Introduction Real-time portrait animation [10, 24, 33, 40] has become cornerstone of interactive digital experiences, powering applications such as virtual assistants, live avatars, and immersive communication systems. Recent advances in diffusion transformer (DiT)-based video generation [9, 11, 17, 21, 30, *Equal contribution. 31] have provided powerful engine for high-fidelity, photorealistic, and expressive digital human synthesis. Equipped with strong generative priors and sophisticated temporal modeling, these models support flexible conditioning on diverse user inputssuch as reference images, human poses, facial expressions, audio, or textenabling highly controllable and personalized portrait animation [6, 8, 15, 18, 24]. However, these models typically operate on long token sequences and rely on iterative denoising, leading to high computational cost and slow generation speed. As result, they struggle to meet the real-time demands of interactive applications, where low latency and immediate response are critical. This limits their practicality for on-the-fly portrait animation, despite their high visual quality. Recently, causal autoregressive (AR) video generation model has emerged as promising alternative for low-latency inference. By leveraging score distillation [37, 39], these works [7, 13, 19, 22, 35, 39] effectively transfer the rich contextual understanding of bidirectional teacher models into causal AR generators. Furthermore, by employing key-value (KV) caching and reducing the number of denoising steps, these models generate video frames in streaming fashion (illustrated in Fig. 1(a)), significantly reducing real-time latency while preserving temporal coherence. However, these methods primarily focus on open-ended tasks such as textto-video generation, and still suffer from noticeable visual degradation, flickering artifacts, and temporal inconsistency. In this work, we present Knot Forcing, novel streaming framework tailored for real-time portrait animation, designed to balance quality, efficiency, and interactivity. Our approach integrates three key innovations: (1) We propose chunk-wise causal generation framework that autoregressively produces video in manageable segments, balancing computational latency and temporal coherence, while supporting seamless integration of streaming control signals (e.g., poses or audio) for real-time controllability. To maintain identity consistency, we encode the user-provided reference image using the video DiT and cache its KV states as global semantic anchor, which is fused with streaming video features throughout generation. short sliding Figure 1. Streaming video generation setup. (a) T2V causal video diffusion. (b) Our approach for portrait animation: given reference frame, we generate video autoregressively with short sliding attention window, ensuring low latency, balanced computation, and stable identity preservation. window attention is further applied to model local temporal dependencies between adjacent chunks (shown in Fig. 1(b)). (2) We observe that, unlike bidirectional models which provide each token with consistent, full-sequence attention context, causal autoregressive video models suffer from periodic shifts in attention context between adjacent frames. This leads to visual drift and motion discontinuities at chunk boundaries, resulting in noticeable degradation in temporal stability and visual fidelity compared to the bidirectional teacher. To address this, we propose the Temporal Knot module, which introduces temporal overlap between the tail frames of the previous chunk and the head frames of the current chunk. By leveraging image-to-video (I2V) conditioning, it propagates fine-grained spatio-temporal cues across chunk boundaries, effectively bridging semantic gaps and ensuring smooth motion transitions. This design enhances both local and global temporal consistency, enabling the causal model to better approximate bidirectional dynamics while preserving streaming efficiency and delivering high-fidelity portrait animation. (3) To address error accumulation in infinite portrait animation, we propose global context running ahead, which maintains forward-looking goal state during synthesis. During training, the model learns from short clips with the final frame serving as future-facing reference. At inference, we treat the ground-truth reference frame as moving pseudo-final\" frame, dynamically updating its temporal position according to the current generation stepby adjusting its rotary positional encoding (RoPE) and re-caching its KV states. This ensures the reference remains temporally ahead, providing stable structural and identity prior that continuously guides streaming predictions toward the target trajectory. The approach effectively suppresses error propagation, preserves motion diversity, bridges the short-to-long gap between training and inference, and enhances visual fidelity in long-term generation. We validate our method on diverse real-time, controllable portrait animation tasks under long-horizon setting. Experiments show that Knot Forcing outperforms existing approaches in visual stability, temporal coherence, and generation quality, with significantly reduced flickering, motion jitters, and identity drift. The framework maintains low latency and strong responsiveness to streaming controls, enabling high-fidelity, infinite-duration animation. Ablation studies confirm the contribution of each component, and overall, our method advances the state of the art in real-time, controllable portrait animation. 2. Preliminaries 2.1. Autoregressive Video Diffusion Current video diffusion transformers [9, 11, 17, 21, 31] represent videos as extended token sequences, applying denoising across the full sequence. Although this yields videos with high coherence and quality, the high inference latency makes it difficult to adapt for real-time generation demands. Recently, the development of autoregressive (AR) video diffusion models [3, 7, 13, 19, 27, 35, 39] offers promising avenue for streaming generation. These models represent hybrid approach, effectively combining the autoregressive chain-rule decomposition with the power of denoising diffusion models for video synthesis. Given sequence of video frames x1:N = (x1, x2, ..., xN ), AR models factorize the joint distribution into product of conditionals using the chain rule: p(x1:N ) = (cid:81)N i=1p(xix<i), (1) where each conditional distribution p(xix<i) is modeled via diffusion process, wherein each frame xi is synthesized by progressively denoising Gaussian noise, conditioned on the preceding frames x<i. Practically, many models [27, 39] choose to model conditional distributions chunk by chunk, predicting future frames concurrently based on the current generated frames. During inference, AR video diffusion models leverage KV caching to store the historical context x<i (shown in Fig. 1(a)). This allows for efficient reuse of past information when predicting subsequent frames, enabling streaming generation with reduced computational overhead. From Bidirectional to Autoregressive Video Diffusion. To build robust Autoregressive (AR) video diffusion model, one promising approach is to distill knowledge from pretrained bidirectional teacher model Fϕ(xt, t) into few-step causal model Gθ(xt, t) via score matching [13, 39]. Distribution Matching Distillation (DMD) loss [23, 38] is adopted to minimize the KL divergence between the target distribution preal and the efficient generator output distribution pfake. The gradient of DMD objective w.r.t. θ is: θLDMD = z,t ,t,xt [(sreal(xt) sfake(xt)) dGθ(z) dθ ] (2) where (0, I), (0, ), and xt = Ψ(Gθ(z), t) is obtained by applying the forward diffusion process Ψ to the output of Gθ. As formulated in Eq. (2), sreal(xt) = xtlog preal(xt) and sfake(xt) = xtlog pfake(xt) are score functions that point towards higher density for preal and pfake, respectively. Practically, xtlog preal is estimated by the bidirectional teacher model Fϕ(xt, t), and xtlog pfake is estimated by fψ(xt, t), which is initialized the same as Fϕ(xt, t), parameterized as shadow model of Gθ within the continuous-time schedule, in order to provide distillation scores at multiple noise levels. Solving Train-inference Gap for AR Video Models. During the inference phase, AR video diffusion models denoise each frame conditioned on their own generated past frames. Previous works such as Teacher Forcing and Diffusion Forcing [2] train AR diffusion models by leveraging ground-truth prefixes (either clean or noised) to condition the denoising process of the next frame. However, such approaches introduce significant train-inference gap. The mismatch between the training conditions (ground truth) and the inference conditions (model-generated history) results in significant video degradation and temporal inconsistency. To alleviate this, Self Forcing [13] directly sample training videos from the distributions estimated by the few-step Gθ: θ = pθ(x1:N ) = (cid:81)N i=1pθ(xix<i), where each frame x1:N xi θ is generated by iterative denoising, conditioned on KVcached self-generated past clean frames and the current noisy frame. This approach ensures consistent exposure to selfgenerated prefix information during training, and drastically reduces the train-inference gap and minimizes artifacts in the generated videos. 2.2. Controllable Portrait Animation Controllable portrait animation [5, 8, 12, 15, 18, 28, 32, 34, 36] is challenging task that aims to generate realistic and consistent video sequences of target person, driven by user-provided inputs. Achieving high visual fidelity in controllable portrait animation relies on two key aspects: maintaining the identity of the reference portrait image, and accurately reflecting the provided control signals (e.g., poses, audio, and motion dynamics). In this work, we adopt the Diffusion Transformer (DiT) architecture [31] for portrait animation. Below we detail the fusion mechanisms for identity and driving signals injection respectively. ID Injection. To inject the ID information from the userprovided reference image into the generated video, we encode the reference image using the video VAE. This static latent is then concatenated with the video latents along temporal dimension, enabling consistent identity fusion during generation [15, 17, 18]. We fine-tune text-to-video diffusion model [8] with masked inpainting to preserve identity, where an additional mask channel controls the visibility of the reference image. During training, we randomly expose reference image and past frames through the masks, teaching the model to reconstruct the target identity under varying conditions. At inference, only the reference image is visible. Driving Signals Injection. Most user-provided control signals are abstract (e.g., audio, expression parameters, motion strength) and lack spatial alignment with video semantics. To ensure faithful adherence to such inputs, we introduce crossattention layers [8, 15, 18, 24] after selected DiT blocks, enabling frame-wise, content-aware fusion of driving signals with video features. The model learns the alignment between sequential video and corresponding driving signals from paired data, allowing it to condition on diverse control inputs and generate appropriately driven portrait animations. This design supports flexible, temporally coherent conditioning while maintaining identity and structural consistency. 3. Method 3.1. Live Portrait Animation with Streaming Video"
        },
        {
            "title": "Generation",
            "content": "In practical applications, real-time portrait animation is highly desirable for scenarios such as virtual live streaming and visual dialogue, etc. In this work, we focus on devising streaming generation method that achieves interactive portrait animation in real-time without compromising video fluency or quality. Stable AR Inference with Short Sliding Window and Global Context. One straightforward approach is to apply Self Forcing for streaming portrait animation. Yet, the accumulation of memory tokens over time causes inference latency to grow, posing challenge for real-time interactive generation that demands consistent and low latency. To Figure 3. Attention masks of different causal designs. IoU of attention contexts between time steps and + 1 is computed to quantify the change in contextual coherence. (a): Causvid and Self Forcing. (b): LongLive. (c): Ours. sudden deformations in object shapes, and abrupt shifts in the periodicity of subject motion (shown in Fig. 2). These artifacts severely degrade the visual coherence and realism of synthesized videos. Temporal Drift from Context Mismatch. We identify the root cause of this issue as abrupt shifts in attention context between adjacent frame latents in causal generation. As shown in Fig. 3, we visualize attention masks under various causal architectures and measure the similarity of attention contexts between neighboring frames. Our analysis reveals that all existing causal designs exhibit periodic shifts in attention context. This structural behavior fundamentally differs from the bidirectional teacher model Fϕ(xt, t), which leverages full temporal context to ensure stable and coherent latent transitions. In contrast, the autoregressive nature of the causal student Gθ limits each tokens receptive field to past frames only, making it difficult to approximate the teachers denoising distribution accurately. These limitations induce feature misalignments that manifest as visible artifacts, such as flickering, shape warping, or motion jitters even between consecutive frames. Unlike discrete token prediction in language models [29], video diffusion operates in continuous latent space [26] requiring fine-grained spatial and temporal precision. The context mismatches amplify small errors into visible artifacts, and without future context, smooth frame-to-frame transitions become difficult to maintain, degrading temporal coherence. Temporal Knot as Semantic Bridge. To address this, we propose Knot Forcing to fix inter-frame coherence: at each step, the model simultaneously denoises the current chunk and the first frames of the subsequent chunk, explicitly aligning their semantic and motion context. This enables the model to incorporate future motion cues when predicting the current chunk, enhancing temporal coherence through locally aware, cross-chunk context fusion. We refer to these shared boundary frames as temporal knots, which serve as anchor points that bind adjacent chunks together and stabilize transitions by maintaining consistent dynamics and semantics across generation steps. As shown Figure 2. The video clips generated by Rolling Forcing, LongLive, and Self Forcing are presented from top to bottom, respectively. Significant temporal artifacts can be observed between adjacent frames, such as inconsistent object motion (first two rows) and abrupt changes in color tone (last row). Zoom in for details. achieve stable, low-latency inference, we introduce short sliding window (Swin) of fixed length L. The sliding window limits attention to local temporal context, ensuring constant per-chunk latency. However, restricting context may degrade long-range coherence and cause visual drift. To address this, we cache the KV pairs from the user-provided reference frame as global anchor, preserving visual consistency. Under the noise schedule {t0 = 0, t1, ..., tT = 1000} of the few-step Gθ, generation of each chunk can be factorized into multi-step denoising process, hence we rewrite the conditional distribution in Eq. 1 as follows: tj1 xi:i+c pθ(xi:i+c tj =Ψ(Gθ(xi:i+c , xi+cL:i 0 , tj, xi+cL:i 0 tj , xref 0 ) , xref 0 ), tj1), (3) where tj1, tj {t0, t1, ..., tT }. At each denoising step for current chunk, generator Gθ predicts the clean frames conditioned on the noisy frames xi:i+c , the local context within the sliding window, and the global context xref 0 from the reference image. Then the predicted output is converted to frames xi:i+c tj1 with reduced noise level via the forward process Ψ [20]. tj 3.2. Forcing Inter-frame Coherence with Temporal"
        },
        {
            "title": "Knot",
            "content": "A critical challenge observed in the implementation of causal video diffusion models is the emergence of temporal drift between consecutively generated frames. This instability manifests as periodic oscillations in background color tones, Figure 4. Key components of Knot Forcing. (a) illustrates the proposed temporal knot module. (b) illustrates the rollout inference pipeline with global context running ahead. in Fig. 4(a), inspired by Image-to-Video (I2V) video diffusion models [16], we further propagate the early-predicted frames from the previous iteration to the next chunk via mask inpainting, ensuring identity-preserving and smooth frame transitions at chunk boundaries. Equipped with temporal knots, we reformulate the denoising process of Gθ to explicitly incorporate future context through shared boundary predictions. Specifically, we rewrite the denoising distribution in Eq. 3 as: pθ(xi:i+c tj1 ; xi+c:i+c+k tj1 =Ψ(Gθ(xi:i+c+k tj , tj, xi:i+k tj xi:i+c+k , xi+cL:i 0 , xi+cL:i , xref , xref 0 ) 0 ), tj1), (4) where ˆxi:i+k represents the temporal knots generated by the model during the noise prediction of the prefix chunk la0 tents. Eq. (4) enables hinge-style latent propagation, where local chunks maintain bidirectional context awareness, while the temporal knots bridge the semantic gap between adjacent chunks via mask inpainting. This restores the interrupted inter-chunk information flow at boundaries and mimics the full temporal modeling of the bidirectional teacher. In this way, Knot Forcing greatly improves the quality and temporal coherence of generated video flow. Practically, each temporal knot introduces an additional frames of context for chunk denoising, which causes extra latency overhead. As shown in Fig. 5, we quantitatively analyze the mutual information between video frames and their contextual neighbors. The results clearly show that the most informative context comes from adjacent frames in terms of attention modeling. To balance performance and latency, we set the hyperparameter = 1, ensuring effective context propagation while minimizing computational delay. Essentially, the streaming latents at chunk boundaries are denoised twice, once as the suffix xi:i+k of the preceding chunk and once as the prefix xi:i+k of the subsequent chunk. 0 To improve consistency at chunk boundaries, we fuse the two predictions by taking their average as the final output for the knot regions (shown in Fig. 4(b)): + xi:i+k 0 2 xi:i+k 0 xi:i+k 0 (5) 0 Figure 5. We assess inter-frame dependency by ablating each context frame and computing the L2 difference in attention outputs (relative to the original), normalized by the L2 norm of the unmodified output. The 10th frame is used as the anchor, the resulting scores indicate each frames contribution to the current frames attention output. 3.3. Mitigating Error Accumulation in Long-Term"
        },
        {
            "title": "Streaming Generation",
            "content": "Existing causal video generation methods suffer from severe error accumulation, as their attention receptive fields are relatively limited. During streaming generation, small prediction errors propagate and accumulate over time, leading to progressive degradation in visual details such as texture and structure. This ultimately results in breakdown of temporal Algorithm 1 Knot Forcing Inference with Global Context Running Ahead Require: KVpre cache of prefix context Require: KVref cache of global context Require: Denoise timesteps {t0, . . . , tT } Require: Number of generated frames , chunk size c, local window length L, reference image xref, RoPE index for xref, running ahead interleave Require: AR diffusion model Gθ (returns KV embeddings via GKV θ ) θ (xref; 0) if + + 1 > then 1: Initialize model output Xθ [] 2: Initialize KV cache KVpre [] 3: Initialize KV cache KVref GKV 4: Initialize Temporal Knot [] 5: while < do 6: 7: 8: 9: 10: 11: 12: 13: 14: end if Initialize xi:i+c+1 for = T, . . . , 1 do Set ˆxi:i+c+1 0 if = 1 then + KVref GKV if > 0 then θ (xref; 0) (0, I) tT tj Update RoPE Index for xref Running Ahead Gθ(xi:i+c+1 ; x, tj, KVpre, KVref) Update Temporal Knot Fused Prediction 0 2 0 x+ˆxi ˆxi end if ˆxi+c Xθ.append(ˆxi:i+c KVpre GKV 0 ) θ (ˆxi+2cL:i+c 0 Sample ϵ (0, I) Set xi:i+c+1 tj1 Ψ(ˆxi:i+c+1 0 ; 0, KVpre, KVref) , ϵ, tj1) else 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end while 27: return Xθ end if end for + consistency in long-term sequences. Although existing methods [19, 35] propose using the initially generated frames as attention sinks to preserve global semantic consistency, the generated content still gradually drifts away from the global context as the temporal index increases. Global Context Running Ahead. To address this issue, inspired by previous work [15], we propose global context running ahead strategy. During training, we consistently treat the last frame of each sampled video clip as the global context. During inference, we leverage the ground-truth reference image as the pseudo last frame\" and ensure that its RoPE index is always placed beyond the current generation chunk, i.e., in the future relative to the generated frames. An overview of the inference pipeline is illustrated in Fig. 4(b). As result, the model learns to perceive the reference as future-appearing anchor, which provides consistent directional signal throughout the streaming generation process. This effectively guides the generation toward long-term coherence and mitigates error accumulation by continuously aligning predictions with fixed semantic target. To facilitate clearer understanding of our approach, we present the pseudo code for causal inference in Algorithm 1. 4. Experiments 4.1. Implementation We implement Knot Forcing based on Wan2.1-T2V1.3B [31]. We first incorporate mask inpainting module into the model and finetune on dataset of 70k collected portrait videos, thereby obtaining reference-based external signal-driven video generation model. Subsequently, following Self Forcing [13], we initialize the causal model with the pretrained weights and distill the bidirectional model into 4-step autoregressive video diffusion model. For hyperparameters, we set the chunk size to 3, the local window size to 6, and the temporal knot length to 1. 4.2. Long-term Portrait Animation As shown in Fig. 6, we evaluate the performance of our model on infinite portrait animation. As demonstrated, Knot Forcing effectively maintains both visual vividness and temporal stability during generation without introducing error accumulation. The proposed temporal knot successfully bridges the temporal gap between consecutive chunks in streaming generation. Furthermore, the proposed global context running ahead further provides the model with effective lookahead guidance, correcting temporal drift that may occur over extended sequences. 4.3. Comparisons 4.3.1. Compare with Autoregressive Portrait Animation We compare our proposed approach with two portrait animation baselines based on autoregressive video generation. (1) MIDAS [4] trains unified multimodal autoregressive model [1] to fuse textual, audio, and visual information, thereby generating audio-aligned portrait animations. (2) TalkingMachines [22] builds upon CausVid [39] distilling bidirectional image-to-video (I2V) model into few-step causal video diffusion model. Since neither method has been open-sourced, we conduct visual comparison based on the demo videos provided on their respective project websites. Qualitative results are shown in Fig. 7, the top row shows visualizations of the baseline methods, while our corresponding results are presented in the bottom row. Visible artifacts in MIDAS stem from its limited visual detail modeling, as it decomposes frames into discrete tokens mixed with other modalities, compromising both texture fidelity and temporal coherence. TalkingMachines demonstrates strong visual stability and ID consistency, benefiting from adopting Wan2.1-14B as its base model, which provides more powerFigure 6. Infinite portrait animation results utilizing the proposed Knot Forcing. Given reference frame, our approach produces videos exhibiting smooth motion, consistent identity, and high vividness across long-horizon without drift during stream-based generation. The generated timestamps are annotated beneath the corresponding video frames. ful prior. In comparison, Knot Forcing achieves comparable performance with significantly lower computational cost. 4.4. Compare with Causal Video Diffusion Models We also compare with state-of-the-art causal video diffusion models [13, 19, 35, 39] for portrait generation. All of these models are few-step causal generators distilled from bidirectional teacher models. Qualitative Comparison. As illustrated in Fig. 8, we conduct visual comparisons with Rolling Forcing and LongLive on portrait animation to evaluate the quality, temporal conFigure 7. Comparisons with streaming portrait animation models. Phonemes corresponding to the video frames are annotated below. sistency, and ID preservation of the generated videos. Since both methods are text-to-video models and cannot generate videos based on specified reference image, we first use them to generate long videos from text prompts. We then leverage the first frame of generated video as the reference input for our model to produce comparable results. As can be observed, although both methods adopt attention sink to mitigate error accumulation, they still suffer from color drifting, identity shifts, and local distortions in long-horizon generation. In contrast, our method produces more stable results without error accumulation, preserving structural integrity without liquefaction. This is because (1) the introduced temporal knot establishes strong inter-frame temporal coherence and enables better imitation of the bidirectional teachers generation distribution, (2) while the global context running ahead provides stable semantic target for future frames, preventing overall visual drifting. Quantitative Comparison. We also conduct numerical comparisons with open-source autoregressive methods. We use VBench [14] quality metrics to evaluate generation quality on set of 300 portrait-related prompts selected from MovieGen [25]. All videos are generated at 832 480 resolution. For our method, we first generate video using the bidirectional base model conditioned on the text prompt, then use its first frame as input for portrait animation in the evaluation. Quantitative results can be found in Tab. 1. As analyzed in Sec. 3.2, existing AR diffusion methods inevitably suffer from attention context mismatch, leading to uncontrollable visual distortions between consecutive frames, which severely degrades video stability and generation quality. In contrast, our method bridges the semantic gap across chunks, producing smoother, more stable, and higher-quality videos. Model Throughput (FPS) Evaluation Scores Temporal Subject Background Aesthetic Imaging Flickering Consistency Consistency Quality Quality CausVid [39] Self Forcing [13] Rolling Forcing [19] LongLive [35] Knot Forcing (Ours) 15.38 15.38 15.79 20.70 17. 96.02 97.23 96.91 97.82 98.50 86.20 84.97 90.89 91.80 94.05 88.15 89.47 93.01 93.42 96.26 58.93 65.50 57.74 66.21 63.11 70.53 62.56 72.01 63.09 74.96 Table 1. We compare Knot Forcing with autoregressive video generation models on generative quality. Best results are bold. 5. Conclusion This work proposes Knot Forcing, new causal video diffusion approach for infinite real-time portrait animation. To balance efficiency and identity consistency, we use short sliding attention window and cache the reference image as global context. To address two key issues in causal generation: temporal discontinuities between chunks and long-term visual drift, we introduce (1) temporal knot and (2) global context running ahead respectively. The former improves coherence by jointly denoising overlapping frames across chunks, while the latter dynamically updates the references temporal position to guide long-term predictions. Together, they enable stable, high-fidelity, and temporally consistent animation. Future work will explore (1) theoretical analysis of the gap between causal students and bidirectional teachers, and (2) extending the framework to more general controllable generation tasks, such as world models and game environment simulations. Figure 9. Visual effects of different components. From (a) to (c), three modules are integrated progressively: sliding window with global context, temporal knot, and global context running ahead. Figure 8. Comparison with causal-based video diffusion models. Top row shows results from the baseline methods, bottom row presents competing results generated by our method. The first frame from the videos generated by each baseline is used as the reference image for our model and is displayed in the leftmost column. Zoom in for better visual detail. 4.5. Ablation Studies We conduct an ablation study to better understand the contribution of each designed component, and present the visual results in Fig. 9. From (a) to (c), we gradually add the three designed modules: (1) sliding window with global context, (2) temporal knot, (3) global context running ahead. As can be seen, when using only sliding attention window with the reference image as global context (1st row), model tends to learn suboptimal solutions due to the relatively homogeneous data distribution in portrait animation. It simply replicates patterns from the reference image and focuses on intra-chunk semantic continuity. Consequently, when target motion pattern significantly deviates from the reference, the generation quality degrades noticeably, and undesirable motion jumps frequently occur. By introducing the temporal knot (2nd row), the semantic discontinuity between chunks is mitigated, and the model strengthens contextual coherence across frames. However, it tends to drift gradually from the global context over time, leading to noticeable semantic deviation in the overall video (last column). Further, the global context running ahead provides stable semantic anchor for future frames (last row). The model maintains inter-frame continuity while staying on the correct semantic trajectory during rollout generation, effectively preventing visual drift."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [2] Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 3 [3] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. 2 [4] Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, et al. Midas: Multimodal interactive digitalhuman synthesis via real-time autoregressive video generation. arXiv preprint arXiv:2508.19320, 2025. 6 [5] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 24032410, 2025. 3 [6] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. 1 [7] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 1, [8] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. 1, 3 [9] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 1, 2 [10] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 1 [11] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 1, 2 [12] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 3 gressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 1, 2, 3, 6, 7, [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 7 [15] Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Omnihuman-1.5: Instilling an active mind in avatars via cognitive simulation. arXiv preprint arXiv:2508.19209, 2025. 1, 3, 6 [16] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 5 [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2, 3 [18] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of onestage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 1, 3 [19] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. 1, 2, 6, 7, [20] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 4 [21] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 1, 2 [22] Chetwin Low and Weimin Wang. Talkingmachines: Realtime audio-driven facetime-style video via autoregressive diffusion models. arXiv preprint arXiv:2506.03099, 2025. 1, 6 [23] Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy Ma, Xiaohua Xie, and Jian-Huang Lai. Adversarial distribution matching for diffusion distillation towards efficient image and video synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1681816829, 2025. 3 [24] Dechao Meng, Steven Xiao, Xindi Zhang, Guangyuan Wang, Peng Zhang, Qi Wang, Bang Zhang, and Liefeng Bo. Mirrorme: Towards realtime and high fidelity audio-driven halfbody animation. arXiv preprint arXiv:2506.22065, 2025. 1, 3 [25] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [13] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autore- [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image liang Cai, Ran He, et al. Infinitetalk: Audio-driven video generation for sparse-frame video dubbing. arXiv preprint arXiv:2508.14033, 2025. 3 [37] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:47455 47487, 2024. 1 [38] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 3 [39] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. 1, 2, 3, 6, 7, [40] Dingcheng Zhen, Shunshun Yin, Shiyang Qin, Hou Yi, Ziwei Zhang, Siyuan Liu, Gan Qi, and Ming Tao. Teller: Real-time streaming audio-driven portrait animation with autoregressive motion generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2107521085, 2025. 1 synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 [27] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 2, 3 [28] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 3 [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [31] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 6 [32] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion In Proceedings of the 33rd ACM International synthesis. Conference on Multimedia, pages 98919900, 2025. 3 [33] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. 1 [34] Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll. Infinihuman: Infinite 3d human creation with precise control. arXiv preprint arXiv:2510.11650, 2025. 3 [35] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 1, 2, 6, 7, 8 [36] Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xun-"
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}