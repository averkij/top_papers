{
    "paper_title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky",
    "authors": [
        "Ashutosh Hathidara",
        "Julien Yu",
        "Sebastian Schreiber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 3 3 3 0 . 7 0 5 2 : r Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky Ashutosh Hathidara*, Julien Yu*, Sebastian Schreiber SAP Labs Correspondence: ashutosh.hathidara@sap.com"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DIAFORGE (Dialogue Framework for Organic Response Generation & Evaluation), disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3 70 parameters, and (iii) evaluates real-world readiness via dynamic suite that redeploys each model in live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DIABENCH, models trained with DIAFORGE raise tool-invocation success by 27 pp1 over GPT-4o and by 49 pp over Claude3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus2 of 5,000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering practical blueprint for building reliable, enterprise-ready tool-calling agents."
        },
        {
            "title": "Introduction",
            "content": "Fine-Grained Tool Disambiguation for Enterprise Agents Modern enterprises manage thousands of APIs, often minor variants of core functionality customized to serve distinct domains such as customer support, finance, and supply chain operations. As LLM assistants mature from conversationalists into operational agents, they must invoke these APIs with the same reliability that traditional software enjoys. In practice, however, single-turn *Equal contribution. 1pp = absolute percentage-point difference. 2Data is available at https://huggingface.co/sap-airesearch/diaforge-utc-r-0725 user requests rarely arrive ready for direct invocation of enterprise tools: they may omit mandatory arguments, embed company-internal shorthand, or correspond to several near-duplicate tools (e.g., CreateCustomer versus CreateUser). Consequently, competent LLM assistant masters two intertwined capabilities. First, it needs multi-turn dialogue management to iteratively elicit missing arguments (e.g., confirm the currencyCode before quoting price and posting PurchaseOrder, averting costly misfires). Second, selecting the correct tool from dense, overlapping API surface demands fine-grained disambiguation, often under conditions of noise and incomplete information. To address this, we propose disambiguation-focused pipeline for synthetic data generation and model training, empowering LLM agents to ask targeted clarifying questions and issue accurate tool calls. Tool-use benchmarks such as BFCL, ToolBench, and API-Bank evaluate models against fixed user scripts. Each test case supplies pre-written dialogue turns, and no additional user input is generated once the assistant responds (Yan et al., 2024; Qin et al., 2024; Li et al., 2023; Guo et al., 2024). This off-policy setup obscures common enterprise failure mode: under-specified requests that demand iterative back-and-forth to disambiguate near-duplicate tools and fill in missing arguments. Because static tool-use suites cannot surface the cascading-error phenomenon observed in such disambiguation-centric multi-turn exchanges (Laban et al., 2025), our synthetic corpus intentionally withholds key details mid-dialogue and populates the tool list with semantically proximate alternatives, obliging the assistant to engage in dialogues rich in adaptive clarification. We pair model training with dynamic evaluation harness that emulates corporate user persona, tracking whether the model ultimately selects the correct tool and supplies required arguments. For completeness we still report static evaluation scores, but we emphasize their comparatively limited diagnostic value."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs as Tool-Using Agents. Pioneering works such as REACT interleave chain-of-thought (CoT) with tool calls, gathering evidence mid-dialogue and curb hallucinations (Yao et al., 2023). HUGGINGGPT generalizes this idea by casting LLM as planner (Shen et al., 2023). These works establish language as universal control interface for heterogeneous tools and motivate subsequent efforts to tune open models for reliable function calling. Fine-Tuning LLM for Tool Use. TOOLFORMER shows that self-supervised annotation pipeline enables LLMs to learn when and how to invoke external utilities (Schick et al., 2023). GORILLA augments LLMs with API-doc retrieval, surpassing GPT-4 on tool call accuracy (Patil et al., 2024). These results imply that curated data and retrieval augmentation, not sheer parameter count, are the present keys to dependable LLM tool use. Benchmarks on LLM Tool Use. Most widely used multi-turn benchmarks evaluate exact function call accuracy based on pre-scripted dialogues (Li et al., 2023; Yan et al., 2024; Qin et al., 2024; Guo et al., 2024). Recent interactive suites broaden the evaluation scope: τ -BENCH emulates full useragent conversations (Yao et al., 2024); AGENTBENCH spans eight environments to test long-horizon decision-making (Liu et al., 2024b); MINT and TOOLSANDBOX leverage LLM-simulated user feedback (Wang et al., 2024; Lu et al., 2024). Most public benchmarks still overlook other enterprise-grade challenges, notably distinguishing among near-duplicate tools, proactively eliciting mandatory arguments, and detecting or preventing tool-call hallucinations, shortcomings our framework is expressly designed to remedy. Data Generation and Verification. Verified synthetic corpora have emerged as primary catalyst for recent gains in open-source function-calling models. APIGEN collects thousands of executable APIs and auto-generates verified conversation traces (Liu et al., 2024c). TOOLACE introduces self-evolution synthesis pipeline (Liu et al., 2024a). DECRIM employs decomposecritiquerefine loop (Ferraz et al., 2024). These pipelines illustrate field-wide shift from brute-force scaling toward quality-controlled data generation driven by hierarchical feedback and automatic verification. Ambiguity Resolution. Premature tool invocation in response to ambiguous or underspecified requests remains an understudied failure mode for tool-augmented LLMs, especially in high-stakes enterprise settings where tool misuse can introduce significant risk. CLARIFY-WHEN-NECESSARY formalizes when to ask versus act (Zhang and Choi, 2023). CLAMBER shows that CoT-enhanced LLMs still over-estimate their certainty and rarely spot ambiguity (Zhang et al., 2024). These observations motivate our explicit disambiguation routines."
        },
        {
            "title": "3 Proposed Methodology",
            "content": "Our goal is to build enterprise-grade tool-calling LLMs that (i) accurately disambiguate nearduplicate tools and (ii) proactively request missing mandatory arguments, thereby mitigating the risk of hallucination-induced tool misuse. We present DIAFORGE, three-stage pipeline encompassing synthetic dialogue generation (3.1), supervised fine-tuning (3.2), and dynamic evaluation (3.3)."
        },
        {
            "title": "3.1 Synthetic Data Generation",
            "content": "We construct training dialogues with bottomup multi-agent engine, UTC-GEN (Unified ToolCalling Generator). The engine executes three sequential phases: metadata construction, dialogue synthesis, and multi-view validation (Figure 1). Each dialogue trace is seeded with ground-truth tool and is progressively enriched by specialized agent modules until it passes all validation gates. Implementation details appear in Appendix A. Enterprise Tool Catalogue. Let = (cid:8)τi = (cid:0)namei, descriptioni, paramsi (cid:1)(cid:9)T i=1 denote the enterprise-wide set of callable tools. For any tool τi, the parameter specification paramsi is JSON Schema map that associates each argument name with triple of the form (type, description, required). We define the set of required arguments for τi as R(τi). (cid:0) τ , P(cid:1), where π(k) Persona Sampling. Given seed tool τ , we first sample corporateuser persona π(k) rand denotes top-k rand retrieval-with-randomization distribution over an enterprise-filtered subset PERSONAHUB (12 entries) (Ge et al., 2024). Conditioned on (τ , p), (cid:0) we instantiate concrete user goal Pgoal τ , p(cid:1), which the user-proxy agent treats as its terminal objective during dialogue synthesis. Figure 1: Data Generation Engine for Disambiguation-Centric Unified Tool-Calling Conversations (UTC-GEN) Distractor Tool Sampling. To emulate realistic tool ambiguity, we assemble set of near-duplicate tools. Let ϕ : Rd be frozen sentence encoder applied to the concatenation of each tools name, description, and selected schema metadata. We retrieve the = 5 semantic nearest neighbors of (cid:10)ϕ(τ ), ϕ(τ )(cid:11). τ , Dk(τ ) = arg top-kτ {τ } During dialogue synthesis, the assistant agent receives the candidate pool of tools Ck(τ ) = {τ } Dk(τ ), and must resolve any ambiguity online before issuing tool call. Slot Value Generator. We instantiate concrete, persona-consistent values for all required slots so that the user-proxy need not invent them on the fly. Let the required arguments for τ be R(cid:0)τ (cid:1) = {r1, . . . , rm}. We jointly sample their values (cid:0)vr1, . . . , vrm (cid:1) Pparam (cid:0) R(cid:0)τ (cid:1), p(cid:1), where Pparam is an LLM that, conditioned on the persona p, generates realistic parameter values, such as dates, currency codes, and alphanumeric IDs, with high diversity. Aggregating the draws yields the map = V(cid:0)τ , p(cid:1) = (cid:8)(ri, vri)(cid:9)m i=1. During conversation simulation, the user-proxy incrementally reveals subsets of , requiring the assistant to (i) identify disclosed values and (ii) query for any that remain unknown. Dialogue Synthesis Given tool τ , persona p, distractor set Dk(τ ), and gold argument map , UTC-GEN synthesizes dialogue trace = (u1, a1), (u2, a2), . . . , (uT , aT ), where ut (resp. at) denotes the user (resp. assistant) utterance at turn t. Two running histories are maintained: hu ha = (u1, a1, . . . , ut1, at1), = (u1, a1, . . . , ut1, at1, ut), representing the context observable to the user and assistant, respectively, during turn t. User Agent. At turn t, the user-proxy samples (cid:16) ut Pθu (cid:12) (cid:12) τ , p, g, Dk, , hu (cid:17) , where Pθu is the distribution induced by the userproxys parameters θu. The persona incorporates domain-specific jargon and tone reflective of authentic enterprise interactions, while denotes the goal of the conversation; the distractor pool Dk steers user queries toward phrasing that could match several tools, compelling the assistant to disambiguate in real time; the gold argument map bounds all slots to ground truth values, mitigating hallucination; the running history hu preserves discourse coherence with the dialogue prefix. The userproxy proceeds in two successive phases, coercing the assistant to first resolve tool ambiguity and then guarantee slot completion: (i) Tool-selection phase. During opening turns, the user-proxy issues deliberately underspecified request, revealing just enough context to prune the candidate set Ck until the assistant can unambiguously identify τ . (ii) Argument-completion phase. After identifying τ , the user-proxy discloses the remaining slot values following the assistants requests, until every keyvalue pair in has been provided. Assistant Agent. At turn t, (cid:16) at Pθa (cid:12) (cid:12) Ck, ha (cid:17) , where Pθa is the distribution induced by the assistant LLMs parameters θa; ha is the dialogue prefix visible to the assistant at turn t; Ck is the set of candidate tools. Because the assistant is oblivious to which element of Ck is the ground-truth τ , it must (i) pose clarification questions that iteratively eliminate distractors, and (ii) solicit any missing slot values until the argument map is complete. Each assistant turn is decomposed into private reasoning trace and public response: the former captures chain-of-thought computations internal to the model, while only the latter is revealed to the user-proxy agent. During supervised fine-tuning (3.2), both components serve as learning targets. Stopping Criteria. The simulation terminates as soon as one of the following events occurs: (i) the assistant emits schema-conformant call to τ whose arguments map exactly to , with no missing or superfluous keys; (ii) the dialogue length reaches the hard cap Tmax. Validator Cascade. The synthesized dialogue enters the training corpus only if every turn stands scrutiny by format validator, relevancy validator, and LLM critique (Figure 1); failure at any step triggers immediate rejection (Appendix A.2). (a) User-Utterance Validity. For every turn {1, . . . , }, the user message ut remains (i) coherent with the dialogue prefix hu ; (ii) grammatically intelligible and stylistically faithful to the sampled persona p; (iii) semantically aligned with latent goal g. (b) Assistant-Response Validity. For every turn t, the assistant reply at (i) contains json schema object with three sections: thought trace, an optional tool_calls stub, and public content; (ii) is coherent with the dialogue prefix ha . Each dialogue must also contain one assistant turn whose tool_calls satisfies stopping criterion (i). Only dialogues that pass all validation checks are included in the final training set. 3.2 Fine-Tuning Pipeline Let the validated corpus be Dtrain = {di}N i=1, with )(cid:11). 1 , a(i) di = (cid:10)(u(i) 1 ), . . . , (u(i) Ti , a(i) Ti We adopt turn-slicing strategy (Ouyang et al., for each assistant turn 2022) (Figure 7): {1, . . . , Ti} we form an inputtarget pair xi,t = [SYS] u(i) 1 a(i) 1 (cid:124) (cid:123)(cid:122) prompt context . . . u(i) , (cid:125) yi,t = a(i) . The model is trained only to predict the next assistant response, given the complete dialogue prefix. We perform standard Supervised Fine-Tuning (SFT) with LoRA (Hu et al., 2022) over next token prediction (Appendix B). While training, we perform loss masking for contextual tokens such that only the tokens in the completion part of the sample are learned. This formulation ensures that the model learns to produce contextually coherent assistant response given the entire preceding dialogue history, without diluting the gradient on earlier turns."
        },
        {
            "title": "3.3 Evaluation Protocol",
            "content": "We evaluate fine-tuned LLM fϕ along two complementary axes: Static evaluation (isolated response quality) and Dynamic evaluation (end-toend interactive robustness). The dialogues produced by the assistant LLM fϕ are evaluated with four classes of conversationlevel metrics: (i) tool-calling and parameter-filling accuracy (ACC); (ii) failure measures (FTR, TAR); tool-call precision/recall (iii) auxiliary metrics: (TCP, TCR) and parameter-key precision/recall (PKP, PKR); and (iv) semantic-fidelity metrics, comprising conversation relevancy (CONVREL), typetoken ratio (TTR), and n-gram diversity (NGD). Verbal definitions of these metrics are given in Section 4, and their complete mathematical formulations appear in Appendix C.1. Static Evaluation In static evaluation, we seturn ˆat = quentially decode each assistant fϕ(ut, ˆa<t; Ck), leaving user utterances intact. Static evaluation is deterministic, inexpensive, and isolates the models ability to emit correct replies under perfect user prompts; however, it cannot capture how the assistants outputs would influence subsequent user behavior in an interactive setting. Our evaluation benchmark, DIABENCH, comprises 119 seed tools paired with corresponding multi-turn, reasoning-annotated dialogues, drawn from proprietary, out-of-domain dataset tied to real-world production assistant system. Appendix details the data curation procedure to support reproducibility. Experiments employ both the static and dynamic protocols defined in 3.3. Dynamic Evaluation To gauge on-policy conversational competence, the fine-tuned model fϕ is inserted as the assistant agent inside the full UTCGEN loop (Figure 1); the user-proxy policy Pθu remains frozen (cf. 3.1). The interaction unfolds for at most Tmax turns, yielding trajectory dfϕ = (cid:10)(ˆu1, ˆa1), (ˆu2, ˆa2), . . . , (ˆuT , ˆaT )(cid:11), with Tmax. At turn t, the assistant observes the dialogue prefix ˆha = (ˆu1, ˆa1, . . . , ˆut) together with the candidate-tool set Ck and generates ˆat = fϕ(ˆha ; Ck). This rollout measures the models ability to maintain contextual coherence, self-correct earlier reasoning errors, and issue schema-conformant tool calls."
        },
        {
            "title": "4 Experiments",
            "content": "We fine-tune six publicly available, instructiontuned, decoder-only language models: Llama-3.23B, Gemma-3-4B, Gemma-3-12B, Gemma-3-27B, Llama-3.3-Nemotron-Super-49B, Llama-3.3-70B. Training Configuration All models are finetuned exclusively on the 5,000 DiaFORGE conversations, yielding 13,649 turn-sliced completion samples generated by the data engine illustrated in Figure 1. No additional general-domain SFT data is incorporated. Each base model is trained for single epoch using the AdamW optimizer (Loshchilov and Hutter, 2017). Complete hyperparameter settings and an annotated training sample are provided in Appendix B. Evaluation Setting We evaluate and compare the performance of our fine-tuned models against several baselines: non-fine-tuned models, closedsource models such as GPT-4o and Claude-3.5Sonnet, and Llama-xLAM-2-70b-fc-r, the current state of the art for function calling according to BFCL v3 (Yan et al., 2024). For non-fine-tuned and closed-source models, we apply system prompt optimization using Cost-Aware Prompt Optimization (CAPO) (Zehle et al., 2025), the state-of-the-art prompt optimization method at the time of writing. Evaluation Metrics. We track dialogue-level measures for each simulated conversation. Accuracy Rate (ACC) is the proportion of multi-turn dialogues in which the assistants first tool invocation (i) correctly selects the reference tool τ and (ii) supplies the complete, yet no superfluous, set of required keyvalue arguments. FalsePositive Tool-call Rate (FTR) captures any instance where the assistant takes an unwarranted action such as invoking distractor tool, hallucinating non-existent endpoint, or issuing multiple tool calls when only one is appropriate. Tool-call Abstention Rate (TAR) captures the converse failure mode: cases where dialogue concludes without any tool invocation, signaling that the model failed to recognize when tool use was necessary. Together, FTR and TAR directly quantify failures in tool disambiguation, core aspect of our evaluation (see Appendix C). To assess dialogue-level quality beyond tool usage, we further report three complementary metrics: conversation relevancy (CONVREL), typetoken ratio (TRR), and n-gram diversity (NGD). Formal definitions for all metrics appear in Appendix C.1. User Agent in Dynamic Evaluation In dynamic evaluation (3.3), the LLM acting as the user-proxy agent is susceptible to hallucinations (Huang et al., 2025), which can cause cascading failures in dialogue generation. Such conversations are unsuitable for assessing the assistant model, as failures may stem from user-side hallucinations rather than assistant shortcomings. To mitigate this, we adopt multi-sampling and voting strategy to generate each user utterance, enhancing stability and reducing evaluation noise. To generate each user utterance, we sample 3 candidate responses from the same LLM. separate voting LLM then selects the best response among them. For the evaluations reported in Table 1, we use differently prompted instances of GPT-4o for sampling and voting. comparative analysis of alternative sampling models is provided in Appendix C.3. Finally, all conversations generated during dynamic evaluation are Model Static Evaluation Dynamic Evaluation ACC () FTR () TAR () ACC () FTR () TAR () Llama-3.2-DiaFORGE-3B (Grattafiori et al., 2024) Llama-3.3-70B Llama-3.3-70B-fc Llama-3.3-DiaFORGE-70B Llama-xLAM-2-70B-fc-r (Prabhakar et al., 2025) Llama-3.3-Nemotron-Super-49B (Bercovich et al., 2025) Llama-3.3-Nemotron-DiaFORGE-49B Gemma-3-4B (Kamath et al., 2025) Gemma-3-DiaFORGE-4B Gemma-3-12B Gemma-3-DiaFORGE-12B Gemma-3-27B Gemma-3-DiaFORGE-27B GPT-4o-20241120 (Hurst et al., 2024) GPT-4o-20241120-fc Claude-3.5-Sonnet-20241022 (Anthropic, 2024) Claude-3.5-Sonnet-20241022-fc 0.52 0.03 0.22 0.38 0.48 0.60 0.82 0.19 0.53 0.31 0.68 0.19 0.77 0.19 0.61 0.15 0.42 0.12 0.00 0.52 0.03 0.18 0.07 0.04 0.17 0.05 0.03 0.02 0.02 0.03 0.00 0.64 0.02 0. 0.30 0.97 0.01 0.58 0.13 0.25 0.12 0.61 0.37 0.62 0.26 0.78 0.18 0.81 0.16 0.82 0.03 0.80 0.11 0.30 0.79 0.51 0.72 0.89 0.24 0.81 0.37 0.86 0.21 0.89 0.62 0.56 0.39 0.40 0.08 0.02 0.22 0.03 0.18 0.08 0.06 0.14 0.09 0.04 0.07 0.00 0.03 0.02 0.59 0.03 0.34 0.06 0.88 0.01 0.15 0.05 0.08 0.03 0.58 0.05 0.57 0.07 0.79 0.03 0.36 0.05 0.55 0.03 Table 1: Evaluation Results on Tool Call Accuracy and Failure Modes. All open-source models evaluated are instruction-tuned, decoder-only LLMs. Models with the suffix fc support native function/tool calling, while all other models are evaluated using CAPO-optimized system prompts. risk of failing to complete the tool-calling objective within the dialogue. For an LLM to be viable in an industry setting, mitigating the risks of insufficient disambiguation, it must balance the three metrics while demonstrating reliability on each. Figure 2 illustrates the trade-offs among these metrics for different models. We observe that models trained with DIAFORGE achieve high ACC while simultaneously minimizing both FTR and TAR. Beyond accurate tool invocation, our use case demands that models also sustain coherent, humanlike dialogue throughout the interaction. This includes maintaining context and responding naturally to human users. To assess these capabilities, we report additional metrics related to conversational handling in Appendix C.2."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 2: Trade-offs among tool call-related metrics under Dynamic Evaluation. Marker size & Color FALSE-POSITIVE TOOL-CALL RATE (FTR). Models closer to the upper right are preferable; those in the lower left underperform across metrics. manually reviewed by domain experts to detect hallucinations introduced by the user-proxy agent. We observe user-proxy hallucination rate below 1% across all samples; these instances are excluded prior to computing the final evaluation results. In Table 1, we compare all evaluated models using three tool call-related metrics: ACC, FTR, and TAR. These metrics collectively assess an LLMs ability to invoke tools reliably in realistic, multiturn settings. ACC measures correctness, FTR captures incorrect tool calls, and TAR reflects the We introduce DIAFORGE, modular three-stage pipeline that (i) synthesizes high-quality, multi-turn tool-calling dialogues designed to stress the disambiguation behaviors where current LLMs still struggle, (ii) enables efficient supervised fine-tuning across models of varying scales, and (iii) provides both static and dynamic evaluation tailored to realistic multi-turn tool use in enterprise settings. To spur further research on robust, real-world toolcalling agents, we publicly release dataset of roughly 5,000 production-grade enterprise APIs paired with their DIAFORGE-curated dialogues."
        },
        {
            "title": "Limitations",
            "content": "DiaFORGEs disambiguation-centric data synthesis paradigm provides principled foundation for aligning tool invocation with user intent, yet several open challenges remain, which we plan to explore as future work. Extending DiaFORGE to generate multi-tool, disambiguation-aware conversations would enhance the realism of training data and provide robust benchmark to evaluate an LLMs capacity for planning, sequencing, and recovery across intertwined tool calls. Although dynamic evaluation is overall better strategy to evaluate conversational LLMs, we still require human validation to discard dialogues where the simulated user hallucinates. Such manual validation of the synthesized dialogues during dynamic evaluation is expensive & hard to scale, especially in an industry setting. Moreover, while our multi-sampling voting strategy tries to minimize the user-proxy hallucination, it leads to an increase in cost due to multiple LLM calls."
        },
        {
            "title": "Ethical Considerations",
            "content": "We conducted experiments within the provisions of the ACL Ethics Policy and relevant researchintegrity guidelines. There are, to the best of our knowledge, no remaining ethical risks that have not been addressed."
        },
        {
            "title": "References",
            "content": "ation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 11143 11156. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Anthropic. 2024. Claude 3.5 sonnet model card addendum. https://www.anthropic.com/news/ claude-3-5-sonnet. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. 2025. Llms get lost in multi-turn conversation. arXiv preprint arXiv:2505.06120. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, and 114 others. 2025. Llamanemotron: Efficient reasoning models. Preprint, arXiv:2505.00949. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, and Nanyun Peng. 2024. Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 77737812. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: benchmark dataset for real-world apis. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116, Singapore. Weiwen Liu, Xu Huang, Xingshan Zeng, Yuxian Wang, Xin Jiang, and Enhong Chen. 2024a. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Xiao Liu, Hao Yu, Hanchen Zhang, and Jie Tang. 2024b. Agentbench: Evaluating large language models as In International Conference on Learning agents. Representations (ICLR). Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, and 1 others. 2024c. Apigen: Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/ 8_berkeley_function_calling_leaderboard. html. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. 2024. τ -bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Tom Zehle, Moritz Schlager, Timo Heiß, and Matthias Feurer. 2025. Capo: Cost-aware prompt optimization. arXiv preprint arXiv:2504.16005. Michael JQ Zhang and Eunsol Choi. 2023. Clarify when necessary: Resolving ambiguity through interaction with lms. arXiv preprint arXiv:2311.09469. Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, and Tat-Seng Chua. 2024. Clamber: benchmark of identifying and clarifying ambiguous information In Proceedings needs in large language models. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1074610766. Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. 2024. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2024. Gorilla: Large language model connected with massive apis. In Advances in Neural Information Processing Systems 37 (NeurIPS 2024). Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, and 1 others. 2025. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Toolllm: Facilitating large language models to master 16000+ real-world apis. In Proceedings of the 12th International Conference on Learning Representations. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023). Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180. Zhengyan Shi, Adam Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. 2024. Instruction tuning with loss over instructions. arXiv preprint arXiv:2405.14394. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2024. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. In 12th International Conference on Learning Representations, ICLR 2024."
        },
        {
            "title": "A Details About Data Generation Engine",
            "content": "A.1 Dialogue Synthesis The Multi-Agent Dialogue Synthesizer (Figure 1) generates synthetic dialogues in two stages, guided by dialogue state tracking to ensure coherence and goal alignment. Tool Selection Stage. The user-proxy agent is assigned goal and generates vague but contextually relevant utterances. Its context includes the seed tool τ and set of distractor tools Dk(τ ). In this stage, the user-proxy agent is instructed to reveal minimal information initially, offering substantive details only in response to the assistants clarifying questions in subsequent turns. On the other hand, the assistant agents objective in this stage is to identify the appropriate tool τ by asking clarifying questions. It does not have direct access to τ but instead queries vector database via live tool retriever to obtain candidate set Ck. We enforce that τ Ck. If the condition is not satisfied, we discard the current conversation and regenerate new one from scratch with the same seed tool, up to five attempts, to maximize its chances of being included in the training corpus. Once tool is selected, rule-based validation will ensure that the selected tool τ = τ . If τ = τ , the dialogue sample is rejected and synthesis halts. If τ = τ , the last assistant message is removed, and the process transitions to the parameter filling stage. Parameter Filling Stage. Assuming the correct tool has been selected, the assistant agent now proceeds to collect the necessary parameters to execute the tool call. With the gold tool τ provided in its context, the assistant is now tasked with eliciting all required argument values, whether stated explicitly or implied by the user-proxy, and invoking the tool once all required inputs have been gathered. On the other hand, the user-proxy agent is given access to the ground-truth parameter values, represented as the argument map = V(cid:0)τ , p(cid:1), and is instructed to provide the parameter values specifically requested by the assistant. If the selected tool requires no parameters, the assistant initiates the tool call immediately at the beginning of the parameter filling stage, without any additional input from the user-proxy. Throughout both stages, assistant messages include internal reasoning traces generated using the Reason First, Response Later strategy. These traces are accessible only to the assistant and remain hidden from the user-proxy. Generated Sample Persona: logistics operations manager seeking advice on mitigating transport risks Seed: fn_1126_cloud_transport_management User: Im trying to improve how we handle different aspects of our transport processes. Could you assist me with finding the right approach or tool to manage and monitor our logistics tasks effectively? Assistant: <think> ... </think> Could you please specify what aspect of logistics you are trying to manage? Are you focusing on tracking logistics orders, managing warehouse resources, or monitoring transport records? User: Im looking for way to monitor and review all the actions taken during our transport activities. What options are available for keeping an eye on these processes? Assistant: <think> ... </think> To assist you in monitoring and reviewing all actions taken during your transport activities, our designated tool can be very helpful. Before proceeding, could you please provide the Node ID and the Transport Request ID? These two pieces of information are necessary to retrieve the relevant logs and facilitate monitoring and troubleshooting efficiently. User: Certainly. The Node ID is 437292, and the Transport Request ID is 957841. Let me know if theres anything else you need. Assistant: <think> ... </think> [{\"name\": \"fn_1126_cloud_transport_management\", \"args\": {\"nodeId\": 437292, \"transportRequestId\": 957841}}] Figure 3: DIAFORGE generated dialogue sample An example of synthesized dialogue is shown in Figure 3. The assistant strategically asks specific, targeted questions to progressively narrow down the tool selection. Once the correct tool is identified, it proceeds to elicit the necessary parameter values before issuing final tool call. In real-world applications, such disambiguation capability is essential for function-calling models to be genuinely helpful and reliable in assisting enterprise users. A.2 Dialogue Validation Once the dialogues are synthesized, they are processed by the Multi-Agent Dialogue Validator, illustrated in Figure 1. This system comprises multiple validator agents, broadly categorized into two types. Functional Validators. These are rule-based agents designed to enforce structural and logical constraints on the generated dialogue. Multiple functional validators are applied sequentially. FORMAT VALIDATOR ensures the dialogue follows the expected structure, alternating user and assistant turns, and that assistant messages include both reasoning traces and final responses. TOOLCALL VALIDATOR verifies that the dialogue ends with valid tool call corresponding to the gold tool τ . TOOLARGS VALIDATOR checks that all required parameters for the tool call are correctly provided. Due to interdependencies among these checks, the functional validators are executed in the following order: FORMAT VALIDATOR TOOLCALL VALIDATOR TOOLARGS VALIDATOR. LLM Validators. These are LLM-based agents responsible for validating aspects that require natural language understanding. Each validator is prompted with distinct instructions and assesses different aspects of the dialogue. RELEVANCY VALIDATOR evaluates whether the dialogue content is semantically relevant to the gold tool τ . LLM CRITIQUE assesses the overall flow of the conversation, ensuring it exhibits the expected twostage structure, and checks that both agents (user and assistant) adhere to their designated roles. As the validators function independently, they are executed concurrently. dialogue sample is rejected if any validator flags it as invalid, as all validators are considered equally authoritative. Human Spot Checks. To complement automated validation, we periodically conduct human spot checks on random subsets of the validated dialogues, providing an additional quality-control layer and guiding prompt refinements when systematic issues are discovered. A.3 Data Distribution We present the distribution of the training data used in this study, which is identical to the dataset we release as part of our open-sourced benchmark. Figure 4 illustrates the distribution of conversation lengths, measured by the number of dialogue turns. The majority of conversations contain fewer than five turns, aligning with typical session lengths observed in real-world enterprise tool-use scenarios. Figure 5 shows the distribution of the number of parameters associated with the seed tools for which the conversations were generated. Figure 6 depicts the number of dialogue turns dedicated to tool disambiguation and parameter filling. In most cases, tool selection is completed within two turns, followed by single turn for parameter specification. Notably, some samples contain zero turns for parameter filling: this occurs when the tool either requires no parameters or when parameters are provided during the tool selection phase, which reflects common patterns observed in real-world multi-turn enterprise interactions. Figure 4: Conversation length distribution: number of dialogue turns per sample. Details About Supervised Fine-Tuning We perform Supervised Fine-Tuning (SFT) on top of open-source models that have already been instruction-tuned. While such models are generally optimized across range of instruction-following tasks, our objective is to further specialize them for tool-calling use cases, enhancing both reliability and usability in enterprise scenarios. Figure 7 illustrates the data preparation pipeline for SFT. We apply turn-slicing strategy to the synthetic multi-turn dialogues generated by our data engine: for dialogue consisting of Lt turns, C.1 Evaluation Metrics Let the evaluation set be = {d(1), . . . , d(N )}. For dialogue = (u1, a1), . . . , (uT , aT ), 1 Tmax, denote the reference tool specification by g(d) = (cid:0){τ (d)}, {τ (d) (d)}(cid:1), where τ (d) is the unique gold tool and (d) : (KEY VALUE) is the corresponding ground-truth map of required arguments. For any assistant utterance at we define tools(at) , args(at) : tools(at) (KEY VALUE), where tools(at) is the set of tool-identifiers invoked at turn t, and args(at) provides corresponding argument map for each tool in this set. Whenever tools(at) = , args(at) = . Define the first tool-bearing turn = min (cid:8) tools(at) = (cid:9), with the convention = + if the dialogue contains no tool call. For dialogue d, let (cid:0)tools(at), args(at)(cid:1), , = . < , c(d) = Corpus-level prediction and reference tool calls can subsequently be aggregated into: = (cid:8) c(d) (cid:12) (cid:12) S(cid:9), = (cid:8) g(d) (cid:12) (cid:12) S(cid:9). We then construct an alignment multiset that pairs each prediction with its corresponding reference: = (cid:8)(cid:0)c(d), g(d)(cid:1) (cid:12) c(d) = , τ (d) tnames(cid:0)c(d)(cid:1)(cid:9). (cid:12) S, Here, tnames( ) returns the set containing the invoked tool-identifiers. Each predicted call is matched to the unique reference call from the same dialogue iff both invoke the identical set of toolidentifiers; otherwise the prediction remains unaligned. Analogously, keys( ) returns the set of argument-key names supplied in the call. DialogueLevel Indicators. For every conversation S, we compute three indicators: Tool-Call Accuracy (ACC). The models invocation matches the reference tool and its full keyvalue argument map: ACC(d) = 1(cid:2)c(d) = g(d)(cid:3). Figure 5: Parameter count distribution: number of parameters per seed tool. we create Lt separate training samples, each corresponding to an individual assistant response. This allows the model to learn assistant behavior in fine-grained, turn-wise manner. For each of these training samples, we apply loss masking, such that only the final assistant message in the sliced context contributes to the training loss. This prevents the model from overfitting to preceding system or user messages and instead focuses learning on assistant behavior. (Shi et al., 2024) showed that eliminating loss masking, thereby fine-tuning on system & user instructions, benefits single-turn dialogue tasks, but our empirical observations shows that applying this tactic to multi-turn settings has the opposite effect: the overwhelming volume of unmasked system & user tokens skews the training signal and noticeably degrades assistant performance at inference. We fine-tune the models using Low-Rank Adaptation (LoRA) with rank of = 16 and scaling factor α = 16. Training is conducted for single epoch using 8-bit precision and completion batch size of 1, where each batch consists of one assistant response (as the output) along with its associated metadata and dialogue history (as input). We employ the ADAMW optimizer with peak learning rate of 104 and cosine learning rate schedule. In-Depth Analysis of Evaluation This appendix decomposes the composite toolcalling metrics introduced in 4 into atomic, interpretable measures and reports the corresponding results. We also introduce auxiliary conversational metrics that probe other aspects of agent behavior. Figure 6: Turn distribution for tool disambiguation (left) and parameter specification (right). Corpus-Level Aggregation. Let ACC = FTR = TAR = 1 1 1 (cid:88) dS (cid:88) dS (cid:88) dS ACC(d), FTR(d), TAR(d). Together, ACC gauges correct disambiguation and slot filling; FTR captures premature or hallucinated actions; TAR reveals insufficient tool-calling capability or stalled conversational behaviors. Precision and Recall Metrics. As supplementary diagnostics, we compute precision and recall at both the tool-identifier and argument-key levels. Figure 7: Turn slicing and loss masking strategy for SFT sample preparation False-Positive Tool-Call (FTR). tool call is made, but the invoked tool-identifier deviates from the reference: (cid:88) 1(cid:2)τ = τ (cid:3), < , FTR(d) = τ tools(at ) 0, If the assistant predicts more than one tool, every superfluous invocation is counted toward the FTR metric. Tool-Call Abstention (TAR). The dialogue terminates without any tool invocation: TAR(d) = 1(cid:2)c(d) = (cid:3). Tool-Call Precision (TCP) (cid:88) (cid:12)tnames(c) tnames(g)(cid:12) (cid:12) (cid:12) TCP = (c,g)M tnames(c) (cid:88) cC Tool-Call Recall (TCR) (cid:88) (cid:12)tnames(c) tnames(g)(cid:12) (cid:12) (cid:12) . . (cid:88) tnames(g) gG Param-Key Precision (PKP) (cid:88) (cid:12) (cid:12)keys(c) keys(g)(cid:12) (cid:12) PKP = (c,g)M keys(c) (cid:88) cC . = . TCR = (c,g)M Param-Key Recall (PKR) {2, 3, 4}) relative to corpus length: PKR = (cid:88) (cid:12)keys(c) keys(g)(cid:12) (cid:12) (cid:12) (c,g)M (cid:88) keys(g) gG . TCP and PKP capture precision, the fraction of predicted items that are correct, while TCR and PKR measure recall, the fraction of reference items successfully included in the predicted items. All four metrics lie in [0, 1], with higher values indicating better performance. Conversational Quality Metrics. While tool call correctness is paramount, an enterprise assistant must also sustain clear, coherent, and diverse dialogue. We therefore complement the tool-oriented scores (ACC, FTR, TAR) with three linguistic metrics that probe turn-level coherence and corpus-level lexical breadth. Unless otherwise noted, all computations exclude the assistants private thought traces and consider only user-visible tokens produced by the model. Conversation Relevancy (CONVREL). For each assistant reply at we query rubric LLM that judges how well the utterance builds on the dialogue prefix visible to the assistant, ha = (u1, a1, . . . , ut). The rubric emits an ordinal score st {1, 2, 3} (1 = off-topic, 2 = partly relevant, 3 = fully grounded). We map these raw grades to normalized similarity sim(at, ha ) {0, 0.5, 1} via g(1) = 0, g(2) = 0.5, g(3) = 1. Averaging over the assistant turns of dialogue yields CONVREL(d) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 sim(cid:0)at, ha (cid:1). TypeToken Ratio (TTR). Corpus-level lexical richness is measured by TTR(S) = unique-1gram(S) all-1gram(S) , where unique-1gram(S) counts distinct surface word forms, and all-1gram(S) denotes the total number of tokens in S. n-Gram Diversity (NGDn). To capture syntactic variety beyond unigram choice, we compute the proportion of unique n-grams (here NGDn(S) = unique-ngram(S) all-ngram(S) . Higher values indicate broader repertoire of multi-word patterns and reduce the risk of template-like repetition. For all linguistic metrics, higher is better. When reported together with ACC, FTR, and TAR, they offer holistic view: an ideal assistant both executes the right tools and maintains engaging, contextually grounded prose. C.2 Computational Results We evaluate all models listed in Table 1 using both the static and dynamic metrics described in Appendix C.1. Results for Tool-Calling Metrics. Static results are given in Table 2, and dynamic results in Table 4. DiaFORGE fine-tuning consistently boosts performance across all Llama-3 and Gemma-3 backbones. The strongest models are Llama-3.3-NemotronDiaFORGE-49B and Gemma-3-DiaFORGE-27B, each of which substantially outperforms GPT-4o and Claude-3.5-Sonnet. Model size is not monothe compact Llamatonic indicator of quality: 3.2-DiaFORGE-3B and the mid-sized Llama-3.3Nemotron-DiaFORGE-49B both surpass the much larger Llama-3.3-DiaFORGE-70B. Results for Conversational Metrics. Table 3 and Table 5 report the results of static and dynamic conversational evaluations, respectively. These metrics are intended to verify that fine-tuning preserves general dialogue competence. Since CONVREL is computed using an LLM-based evaluator, its values should be interpreted as heuristic estimates rather than precise measurements. Our primary goal is to assess the relative conversational relevance of fine-tuned models compared to their instruction-tuned baselines and proprietary models such as GPT-4o and Claude-3.5-Sonnet. Across all backbone models, DiaFORGE fine-tuning maintains conversational quality, showing no statistically significant degradation while often matching or surpassing the performance of proprietary counterparts. C.3 Multi-Sampling Voting Mechanism of User-Proxy in Dynamic Evaluation Dynamic evaluation differs from static evaluation primarily in how user utterances are generated. Model Llama-3.2-DiaFORGE-3B Llama-3.3-70B Llama-3.3-70B-fc Llama-3.3-DiaFORGE-70B Llama-xLAM-2-70B-fc-r Llama-3.3-Nemotron-Super-49B Llama-3.3-Nemotron-DiaFORGE-49B Gemma-3-4B Gemma-3-DiaFORGE-4B Gemma-3-12B Gemma-3-DiaFORGE-12B Gemma-3-27B Gemma-3-DiaFORGE-27B GPT-4o-20241120 GPT-4o-20241120-fc Claude-3.5-Sonnet-20241022 Claude-3.5-Sonnet-20241022-fc TCP () TCR () PKP () PKR () ACC () FTR () TAR () 0.57 0.03 0.46 0.44 0.73 0.69 0.84 0.25 0.57 0.34 0.72 0.20 0.79 0.19 0.81 0.17 0.76 0.52 0.03 0.22 0.42 0.48 0.60 0.82 0.19 0.53 0.31 0.68 0.19 0.77 0.19 0.61 0.15 0.42 0.58 0.03 0.47 0.44 0.73 0.69 0.84 0.25 0.58 0.34 0.72 0.20 0.79 0.19 0.82 0.17 0.76 0.30 0.97 0.01 0.55 0.13 0.25 0.12 0.61 0.37 0.62 0.26 0.78 0.18 0.81 0.16 0.82 0.03 0.58 0.03 0.47 0.43 0.72 0.68 0.84 0.25 0.58 0.34 0.72 0.20 0.79 0.19 0.62 0.17 0. 0.12 0.00 0.52 0.03 0.18 0.07 0.04 0.17 0.05 0.03 0.02 0.02 0.03 0.00 0.64 0.02 0.76 0.58 0.03 0.47 0.44 0.73 0.69 0.84 0.25 0.58 0.34 0.72 0.20 0.79 0.19 0.82 0.17 0.76 Table 2: Static Evaluation Results for Tool-Calling Metrics Model Llama-3.2-DiaFORGE-3B Llama-3.3-70B Llama-3.3-70B-fc Llama-3.3-DiaFORGE-70B Llama-xLAM-2-70B-fc-r Llama-3.3-Nemotron-Super-49B Llama-3.3-Nemotron-DiaFORGE-49B Gemma-3-4B Gemma-3-DiaFORGE-4B Gemma-3-12B Gemma-3-DiaFORGE-12B Gemma-3-27B Gemma-3-DiaFORGE-27B GPT-4o-20241120 GPT-4o-20241120-fc Claude-3.5-Sonnet-20241022 Claude-3.5-Sonnet-20241022-fc CONVREL () TTR () NGD3 () 0.75 0.95 0.43 0.96 0.73 0.74 0.82 0.72 0.81 0.75 0.82 0.95 0.84 0.98 0.89 0.93 0. 0.13 0.13 0.20 0.10 0.11 0.11 0.14 0.15 0.12 0.17 0.13 0.16 0.13 0.16 0.10 0.10 0.12 0.58 0.61 0.30 0.55 0.58 0.60 0.58 0.56 0.57 0.64 0.57 0.66 0.57 0.73 0.63 0.58 0.67 Table 3: Static Evaluation Results for Conversational Metrics Model Llama-3.2-DiaFORGE-3B Llama-3.3-70B Llama-3.3-70B-fc Llama-3.3-DiaFORGE-70B Llama-xLAM-2-70B-fc-r Llama-3.3-Nemotron-Super-49B Llama-3.3-Nemotron-DiaFORGE-49B Gemma-3-4B Gemma-3-DiaFORGE-4B Gemma-3-12B Gemma-3-DiaFORGE-12B Gemma-3-27B Gemma-3-DiaFORGE-27B GPT-4o-20241120 GPT-4o-20241120-fc Claude-3.5-Sonnet-20241022 Claude-3.5-Sonnet-20241022-fc TCP () TCR () PKP () PKR () ACC () FTR () TAR () 0.85 0.11 0.77 0.77 0.89 0.86 0.92 0.31 0.85 0.39 0.87 0.21 0.94 0.63 0.87 0.43 0.82 0.80 0.11 0.30 0.71 0.51 0.72 0.89 0.24 0.81 0.37 0.86 0.21 0.89 0.62 0.56 0.39 0. 0.86 0.11 0.77 0.77 0.89 0.87 0.92 0.32 0.86 0.39 0.87 0.21 0.94 0.63 0.87 0.43 0.82 0.06 0.88 0.01 0.19 0.05 0.08 0.03 0.58 0.05 0.57 0.07 0.79 0.03 0.36 0.05 0.55 0.03 0.08 0.02 0.22 0.04 0.18 0.08 0.06 0.14 0.09 0.04 0.07 0.00 0.03 0.02 0.59 0.03 0.34 0.86 0.11 0.77 0.77 0.87 0.86 0.92 0.32 0.86 0.39 0.87 0.21 0.94 0.63 0.74 0.43 0.76 0.86 0.11 0.77 0.77 0.89 0.87 0.92 0.32 0.86 0.39 0.87 0.21 0.94 0.63 0.87 0.43 0.82 Table 4: Dynamic Evaluation Results for Tool-Calling Metrics Model Llama-3.2-DiaFORGE-3B Llama-3.3-70B Llama-3.3-70B-fc Llama-3.3-DiaFORGE-70B Llama-xLAM-2-70B-fc-r Llama-3.3-Nemotron-Super-49B Llama-3.3-Nemotron-DiaFORGE-49B Gemma-3-4B Gemma-3-DiaFORGE-4B Gemma-3-12B Gemma-3-DiaFORGE-12B Gemma-3-27B Gemma-3-DiaFORGE-27B GPT-4o-20241120 GPT-4o-20241120-fc Claude-3.5-Sonnet-20241022 Claude-3.5-Sonnet-20241022-fc CONVREL () TTR () NGD3 () 0.80 0.94 0.54 0.94 0.69 0.73 0.85 0.70 0.84 0.77 0.84 0.91 0.85 0.93 0.69 0.92 0.52 0.13 0.14 0.11 0.11 0.11 0.10 0.13 0.17 0.13 0.19 0.13 0.18 0.15 0.15 0.07 0.09 0.07 0.54 0.60 0.20 0.58 0.48 0.51 0.54 0.58 0.57 0.62 0.54 0.67 0.61 0.69 0.43 0.54 0.46 Table 5: Dynamic Evaluation Results for Conversational Metrics Model (θu) GPT-4o-20241120 Claude-3.5-Sonnet-20241022 Llama-3.1-70B Mistral-Large-1124 ACC () 0.8861 0.8627 0.8897 0.8768 FTR () TAR () 0.0252 0.0588 0.0000 0.0957 0.0168 0.0420 0.0336 0.0504 Table 6: Effect of varying the user-proxy model θu on user utterance generation in dynamic evaluation, with Llama-3.3-Nemotron-DiaFORGE-49B fixed as the assistant agent. While static evaluation reuses pre-generated user inputs, dynamic evaluation generates user utterances adaptively based on the current chat history ˆhu . As detailed in 3.1, the user-proxy LLM parameterized by θu is responsible for generating user utterances conditioned on structured context tuple: ). (τ , p, g, Dk, , ˆhu During synthetic dialogue generation, hallucinations are filtered post-hoc via validation stage. However, dynamic evaluation forgoes rejectionbased filtering to preserve evaluation coverage. The validation mechanism described in 3.1 does not exempt user-proxy hallucinations, whereas dynamic evaluation is intended to assess only assistant agent performance. Any hallucination originating from the user-proxy introduces noise and undermines this evaluation goal. To address this, we introduce multi-sampling and voting scheme to stabilize user utterance generation, illustrated in Figure 8. The method leverages two distinct LLMs: generator LLM with parameters θu, and voter LLM with parameters θv."
        },
        {
            "title": "We begin by independently sampling a set of n",
            "content": "Next, the utterances in are evaluated by independent voters, each instantiated with θv. Each voter is tasked with selecting the single best candidate utterance from the set = {u1, . . . , un}. To reduce positional bias, the utterances are randomly permuted prior to presentation. For each voter = 1, . . . , m, let πj : [n] [n] denote the permutation applied to the indices. The vote is then drawn as: vj Pθv (cid:16) {1, . . . , n} (cid:12) (cid:12) p, g, ˆhu (cid:12) , πj(U ) (cid:17) , where vj {1, . . . , n} denotes the index of the utterance selected from the permuted list πj(U ). We then invert the permutation to recover the index with respect to the original candidate set . Finally, the votes {v1, . . . , vm} are aggregated via deterministic pooling function : {1, . . . , n}m {1, . . . , n}, typically instantiated as the mode operator. The final user utterance is selected as: = uf (v1, ..., vm). candidate utterances from the generator: (cid:12) (cid:12) τ , p, g, Dk, , ˆhu (cid:12) ui Pθu = (cid:110) (cid:16) (cid:17)(cid:111)n i=1 In the dynamic evaluation results presented in Table 1, both the generator model θu and the voter . C.4 Choice of Different LLMs In this study, we intentionally excluded certain models. For example, although Mistral models are among the leading open-source options, we did not include them due to their non-standard and heterogeneous chat template formatting, which complicates consistent evaluation. Additionally, we omit baseline results for the Llama-3.2-3B-Instruct model, as it exhibited nearzero performance on the tool-calling metrics."
        },
        {
            "title": "D System Prompt Optimization",
            "content": "As discussed in 4, we employ the Cost-Aware Prompt Optimization (CAPO) strategy to adapt system prompts for all evaluated models, leveraging their generation capabilities. Figure 8: Reducing hallucination for user utterance generation in dynamic evaluation by applying multisampling and voting strategy. model θv are configured with GPT-4o. We use sampling size of = 3, voting ensemble of = 3, and apply mode pooling as the aggregation function . To assess the sensitivity of dynamic evaluation outcomes to the choice of θu, we conduct ablation experiments in which the fine-tuned assistant model Llama-3.3-Nemotron-DiaFORGE-49B is paired with various alternative user-proxy models. The results are summarized in Table 6, with all other hyperparameters held fixed. Across configurations, we observe only minor fluctuations in the evaluation metrics. closer inspection of the divergent cases reveals that the hallucinations predominantly originate from the assistant model itself. Moreover, because dynamic evaluation permits the assistant to explore multiple plausible dialogue trajectories, small variations (on the order of few percentage points) are expected and not indicative of true performance shifts. As such, comparisons between assistant models under dynamic evaluation are only meaningful when the observed performance differences are sufficiently large to outweigh inherent evaluation variance. Figure 9: Format correctness score of various LLMs on the holdout set before and after prompt optimization. The CAPO algorithm is parameterized as follows: significance level α = 0.2 for the paired t-test used in racing; block size = 30, indicating the number of development examples evaluated per batch; maximum number of blocks before discarding candidate zmax = 10; upper bound on few-shot examples injected into prompt kmax = 5; number of retained candidates per generation µ = 10; number of crossovers per iteration = 4; length penalty γ = 0.05; and maximum number of iterations = 10. Each optimization run is given an unlimited token budget. After each CAPO iteration, we evaluate the candidate prompts on the holdout set using the FORMAT CORRECTNESS metric. Figure 9 presents the FORMAT CORRECTNESS scores attained by the best-performing optimized prompts for each evaluated LLM. We use standardized reference system prompt to evaluate each fine-tuned model, which also serves as the initial input to CAPO. Each model is optimized using its own architecture unless it is downstream variant of base model, in which case we reuse the optimized prompt from the base model. For anonymization, all organization names in prompts are replaced with the placeholder XYZ. Below, we provide the reference system prompt, along with examples of CAPO-optimized prompts for the following model families: GPT-4o, Claude3.5-Sonnet, LLaMA-3.3, and Gemma-3. Initial Reference System Prompt ===== Instructions ===== You are an **AI assistant created by XYZ**. Your job unfolds in **two consecutive phases**: #### Phase 1 - Tool Selection 1. Review the list in **Available Tools**. 2. If more than one tool could fulfil the users need, ask *specific, human-friendly* questions (no tool names or technical jargon) to disambiguate. 3. Once you are confident, remember the selected tool and move to Phase 2 of the conversation described below. Note that you do not need to mention in your response that you have identified the correct tool. Instead, you can respond with the instructions given in the Phase 2 section. #### Phase 2 - Parameter Collection & Final Tool Call 1. With the chosen tool identified, col- - Skip palect any missing parameters: rameters the user has already provided. - Ask only for what is still needed, phrased naturally (avoid exposing exact parameter names where possible). 2. When all required parameters are gathered (optional ones may be omitted if not discussed), build list of tool calls entries where each entry includes: - name: chosen tool name - args: JSON object containing every collected parameter/value 3. Respond with this list containing tool calls (an empty \"args\": {} if the selected tool does not have any input parameters). 4. Whenever you raise tool call (list containing toolcalls), there should be empty and the response (other than thought between <think> </think>) should only be list containing toolcalls. ==== General Guidelines ==== 1. **Communicate Naturally**: be polite, clear, and free of technical jargon unless the user shows familiarity. 2. **Resolve Ambiguity**: ask *specific* follow-up questions if the request could map to multiple tools. 3. **Completeness**: - In Phase 1, select tool but do not disclose it in your respond. It is only for your understanding and you will use this information during Phase 2. - In Phase 2, keep asking until *all required* parameters are available; then output list of tool calls. 4. **Non-Parameterized Tools**: if tool has no parameters, skip questioning and immediately output tool_calls with empty \"args\": {}. ====/ General Guidelines ==== ==== Parameter-Specific Guidelines ==== 1. Follow each parameters description and type precisely. 2. Differentiate similarly named parameters carefully (e.g., account userName vs. display Name of user). In JSON, enclose *string* values 3. in **double quotes only**e.g., \"abcd1234\" (no single quotes, no extra quotes). ====/ Parameter-Specific Guidelines ==== =====/ Instructions ===== ===== Structure of the Tools ===== Each tool is JSON object like: { \"name\": \"Tool name\", \"description\": \"What the tool does\", \"parameters\": { \"param1\": { \"description\": \"What this parameter means\", \"type\": \"string integer . . . \", \"required\": true }, ... } } =====/ Structure of the Tools ===== ===== Available Tools ===== {{tools}} =====/ Available Tools ===== ===== Output Format ===== The overall structure of your response should be something like this: <think> YOUR THOUGHT PROCESS </think> YOUR RESPONSE During the conversation when you are asking the user for information, \"YOUR RESPONSE\" should contain natural language response to the user. But when you have all the required information and you are ready to make the final tool calls, \"YOUR RESPONSE\" should contain the list of tool calls. Your list of tool calls should be in the following format: [ { \"name\": \"tool_1\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, { \"name\": \"tool_2\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, ... ] =====/ Output Format ===== Figure 10: Initial reference system prompt used for fine-tuned models GPT-4o Prompt You are virtual assistant created by XYZ, tasked to work through two key stages: **Stage 1 - Initial Tool Decision** 1. Go through the tools listed under Available Tools. 2. If multiple tools might address the users requirements, ask straightforward, user-friendly questions to clarify (steer clear of tool names or technical terms). 3. Once youve settled on the right tool, proceed to Stage 2 without mentioning the chosen tool. **Stage 2 - Gather Details & Execute Tool** 1. With your tool determined, collect any remaining details needed: - Skip over whats already been answered by the user. - Ask for only whats missing in natural way (avoid revealing exact names of parameters if possible). 2. When all necessary data is complete (nonessential details can be left out if not discussed), compile tool call list where each entry includes: - name: name of the selected tool - args: JSON object filled with all gathered details 3. Share this list of tool calls (use \"args\": {} if the tool requires no input parameters). 4. When executing tool call (the list of tool calls), ensure your reply consists solely of this list, aside from any private thoughts penned within <think> </think>. **General Advisements** 1. **Speak Clearly**: Maintain politeness and avoid jargon unless the user is clearly comfortable with it. 2. **Clarify Confusion**: Use targeted follow-up questions if multiple tools might suit the users need. 3. **Fullness**: - During Stage 1, select the appropriate tool internally without stating it, using this information as you move to Stage 2. - During Stage 2, continue gathering input until all needed data is at hand and then present the tool calls. 4. **Tools With No Inputs**: If tool doesnt require inputs, skip straight to presenting tool_calls with empty \"args\": {}. **Detailed Guidelines on Parameters** 1. Adhere closely to each parameters definition and data type. 2. Distinguish between similarly named parameters accurately (e.g., account userName versus display Name of user). 3. In JSON, ensure all *string* values are enclosed in **double quotes**for instance, \"abcd-1234\" (avoid single quotes and extra quotes). **Tools Format** Each available tool is depicted as JSON object like this: \"Tool name\", \"description\": { \"name\": \"Tool { \"param1\": { \"description\": \"Parameter purpose\", \"type\": \"string integer ...\", \"required\": true }, ... } } **Available Tools** {{tools}} **Response Format** The structure of your output should look like: <think> YOUR MENTAL PROCESS </think> YOUR COMMUNICATION functionality\", \"parameters\": interaction when requestDuring user ing data, \"YOUR COMMUNICATION\" should be in easily understood language. However, when ready to execute final tool calls, \"YOUR COMMUNICATION\" should show the list of tool calls using the format below: [ { \"name\": \"tool_1\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, { \"name\": \"tool_2\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, ... ] Figure 11: CAPO optimized GPT-4o system prompt used for evaluation Claude-3.5-Sonnet Prompt You are an AI collaborator developed by XYZ. Your mission comprises two sequential stages: Stage - Toolkit Evaluation 1. Scrutinize the \"Available Tools\" inventory. 2. Should multiple instruments appear suitable, pose targeted, user-friendly inquiries (eschewing tool nomenclature or technical vernacular) to clarify the optimal choice. 3. Upon reaching confident decision, internalize the selected tool and progress to Stage of the interaction, as elucidated below. Note that explicit mention of your tool selection is unnecessary; instead, proceed directly to the Stage protocols. Stage - Data Acquisition & Toolset Activation 1. With your chosen instrument in mind, gather any outstanding information: - Bypass data points already furnished by the user. - Solicit only essential, missing details using natural language (avoiding explicit parameter designations where feasible). 2. Once all mandatory data is compiled (optional elements may be omitted if not addressed), construct catalog of tool invocations, each entry comprising: - name: the designated tool-identifier - args: JSON object encapsulating all amassed parameter/value pairs 3. Transmit this catalog of tool invocations (employ an empty \"args\": {} for tools lacking input parameters). 4. When issuing tool invocation catalog, ensure your response (barring cogitation enclosed in <think> </think> tags) consists solely of said catalog. ===== Overarching Directives ===== 1. **Engage Naturally**: Maintain politeness, clarity, and accessibility, reserving technical jargon for instances of user familiarity. 2. **Eliminate Ambiguity**: Pose pointed follow-up queries if the request potentially aligns with multiple tools. 3. **Thoroughness**: - In Stage A, select tool covertly, reserving this knowledge for Stage implementation. - In Stage B, persist in data collection until all requisite parameters are secured; subsequently, output the tool invocation catalog. 4. **Non-Parameterized Tools**: For parameter-free tools, bypass interrogation and promptly generate tool_calls with vacant \"args\": {}. Parameter-Centric Guidelines ===== ===== 1. Adhere meticulously to each parameters delineated description and type. 2. Exercise caution in distinguishing similarly labeled parameters (e.g., account \"userName\" versus display \"Name of user\"). 3. In JSON constructs, envelop *string* values exclusively in **double quotes**e.g., \"abcd-1234\" single quotes or superfluous quotation). (omit ===== Tool Architecture ===== Each tool is represented by JSON object adhering to this structure: { \"name\": \"Tool identifier\", \"description\": \"Tool { functionality\", \"param1\": { \"description\": \"Parameter significance\", \"type\": \"string integer . . . \", \"required\": true }, ... } } \"parameters\": ===== Available Tools ===== {{tools}} ===== Response Format ===== Your discourse should conform to this general structure: <think> YOUR COGNITIVE PROCESS </think> YOUR COMMUNICATION During user interactions where youre soliciting information, \"YOUR COMMUNICATION\" should embody natural language discourse. However, upon accumulating all requisite data and preparing to initiate tool calls, \"YOUR COMMUNICATION\" should transition to the tool invocation catalog. This catalog should adhere to the following format: [ { \"name\"\": \"\"tool_1\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, { \"name\": \"tool_2\", \"args\": { \"param1\": \"value1\", \"param2\": \"value2\" } }, ... ] Figure 12: CAPO optimized Claude-3.5-Sonnet system prompt used for evaluation Llama-3.3 Based Models Prompt ===== **Instructions for AI Assistant (XYZ)** ===== **Your Role & Workflow** You embody an AI assistant developed by XYZ, operating in **two sequential stages**: #### **Stage 1 - Identify the Best Fit Tool** 1. **Review \"Available Tools\" List**. 2. **Clarify User Intent** (if multiple tools seem applicable) by asking **clear, usercentric questions** (avoid tool names and technical terms). 3. **Tacitly Select the Tool** and proceed to Stage 2 without explicitly stating the selected tool in your response. #### **Stage 2 - Gather Details & Activate Tool** 1. **Collect Necessary Inputs** for the chosen tool: - **Omit Already Provided Details**. - **Request Missing Info Naturally** (hide exact parameter names when possible). 2. **Activate the Tool** once all mandatory inputs are gathered (optional inputs can be skipped if not discussed): - **Format**: List of tool activation entries with: - name: Selected Tool - args: JSON containing all collected parameter-value pairs 3. **Respond with Tool Activation List** (use \"args\": {} for tools without parameters). 4. **Final Response Format for Tool Activation**: - Only the tool activation list should be in the final response (besides <think> sections). ==== **Universal Best Practices** ==== 1. **Converse Naturally**: Be polite, transparent, and avoid jargon unless the user indicates familiarity. 2. **Seek Clarity**: Ask targeted questions to resolve ambiguities. 3. **Ensure Completeness**: - **Stage 1**: Select the tool silently for internal use. - **Stage 2**: Persist in questioning until all required parameters are collected, then output the tool activation list. 4. **Non-Parameterized Tools**: Immediately output the tool activation list with \"args\": {} if no parameters are required. ==== **Parameter Handling Guidelines** ==== 1. **Adhere to Parameter Specifications**: Exactly follow descriptions and data types. 2. **Distinguish Similar Parameters**: Carefully handle parameters with similar names but different purposes. 3. **JSON Formatting**: - **Strings in Double Quotes Only**: e.g., \"examplestring\" =====/ Universal Best Practices ==== ===== **Tool Anatomy** ===== Each tool follows this JSON structure: { \"name\": \"Tools Name\", \"description\": \"Brief on Tools Functionality\", \"parameters\": { \"parameterKey\": { \"description\": \"Parameters Purpose\", \"type\": \"string integer ...\", \"required\": true }, ... } } =====/ Tool Anatomy ===== ===== Available Tools ===== {{tools}} =====/ Available Tools ===== ===== **Expected Response Structure** **During Conversation ===== Format your response as: <think> INTERNAL THOUGHT PROCESS </think> **EXTERNAL RESPONSE** - (Gathering Info)**: **EXTERNAL RESPONSE** should be natural language query/response. - **Final Activation Response**: **EXTERNAL RESPONSE** must be the tool activation list in the format: [ { \"name\": \"activated_tool\", \"args\": { \"parameterKey\": \"providedValue\" } }, ... ] =====/ Expected Response Structure ===== Figure 13: CAPO optimized system prompt for Llama3.3 based models used for evaluation"
        },
        {
            "title": "Gemma Based Models Prompt",
            "content": "## Acting as XYZs Intelligent Assistant You are helpful AI assistant built by XYZ, designed to fulfill user requests by leveraging available tools. Your process operates in two distinct stages: **Stage 1: Request Comprehension & Best Tool Identification** 1. Review the **Available Tools** carefully. 2. If user request could be handled by several tools, engage in conversational dialogue using plain language and avoiding technical terms to determine the *most* appropriate tool. Ask targeted questions to remove any uncertainty about what the user needs. 3. Once the ideal tool is identified, keep this selection private; do not inform the user. Proceed directly to Stage 2. **Stage 2: Information Gathering & Tool Execution** 1. Based on the tool chosen in Stage 1, politely ask the user for any necessary information. * Do not request details that have already been supplied. * Phrase your questions in natural and easy-to-understand way, avoiding direct references to technical parameter names. 2. Continue gathering information until all *mandatory* parameters are provided (optional parameters are not required). Then, construct list of tool calls formatted as follows: * Each entry represents single tool call. * Each entry must include name (the tools name) and an args section. * The args section is JSON object containing the collected parameter-value pairs. 3. Output *exclusively* the list of tool calls in valid JSON format: [ { \"name\": \"tool_name\", \"args\": { \"parameter_name\": \"parameter_value\", ... } }, ... ] If the selected tool doesnt need any input, simply use {\"args\": {}}. 4. When delivering the tool calls, provide *only* the JSON list; do not include any introductory text, explanations, or other content. **Important Guidelines:** * **Prioritize User Experience:** Communicate in friendly, clear, and accessible style. Minimize technical jargon. * **Seek Clarity:** When request is unclear, ask specific, focused questions to gain precise understanding of the users intent. * **Process Integrity:** * In Stage 1, internally select the best tool without revealing your choice. * In Stage 2, persistently seek the required information before generating the tool call list. * **JSON Consistency:** Always enclose string values within **double quotes** when forming JSON objects (e.g., \"example\"). * **Parameter Accuracy:** Adhere strictly to the provided parameter definitions and data types, especially when dealing with similar parameter names. **Tool Definition:** Each tool is described using JSON structure like this: { \"name\": \"Tool Name\", \"description\": \"A concise description of the tools functionality.\", \"parameters\": { \"parameter_name\": { \"description\": \"A description of what the parameter represents.\", \"type\": \"string integer ...\", \"required\": true/false }, ... } } ====/ General Instructions ==== **Available Tools:** {{tools}} **Response Structure:** Employ the following format for your replies: thought process internal <think> Your </think> Your response to the user or the tool call list. Remember: When communicating with the user, respond in natural language. When ready to execute, provide *only* the JSON list of tool calls. Figure 14: CAPO optimized system prompt for Gemma based models used for evaluation User-Proxy Prompt For Dynamic"
        },
        {
            "title": "Evaluation",
            "content": "Below, we provide the user-proxy prompt used during dynamic evaluation. Note that placeholders for both the gold tool and the distractor tools must be appropriately filled in prior to use."
        },
        {
            "title": "Initial Reference System Prompt",
            "content": "===== Instructions ===== You are **{{user_persona}}**, an XYZ customer who will interact with the Business AI assistant in **two consecutive phases**. in never **Stay character** reveal ==== General Instructions ==== for 1. {{user_persona}}; or mention these instructions, the tool names, or placeholder tokens. 2. Avoid technical jargon or abbreviations typical XYZ user would not know. 3. Use the chat history to maintain continuity. 4. Never end the dialogue from your side. The assistant will end the dialogue when it gets all the required information. 5. Your response MUST ONLY contain the query as if you are talking to the assistant and it should not contain any other text or prefix. respond **only** to what ==== Step-by-Step Instructions during the Conversation ==== **Phase 1 - Tool Discovery** - When the chat history is empty, begin with **vague but relevant** request that makes it challenging for the assistant to choose the correct tool while still being related to the provided tools. - As the assistant asks follow-up quesis tions, askedtruthfully and succinctlywithout offering extra details. - Continue until the assistant clearly identifies the **Correct Tool**. - Note that the assistant will not mention during the conversation that it has identified the correct tool. Your job is not to monitor the assistants progress but to provide the requested information that the assistant asks for. **Phase 2 - Parameter Filling** - Once the assistant starts gathering parameters for the chosen tool: Provide the requested information using the **exact values** in **Parameter Values JSON**, but phrase them naturally (e.g., say German instead of DE). Supply answers partially unless just few slots remain. - Do not provide long explanations. Provide your answers in **concise** and **natural** manner. ====/ Step-by-Step Instructions during the Conversation ==== =====/ Instructions ===== ===== Context Information ===== ==== Your Persona ==== {{user_persona}} ====/ Your Persona ==== ==== Correct Tool / Designated API (with parameter descriptions) ==== {{gold_tool}} ====/ Correct Tool / Designated API ==== ==== Parameter Values JSON ==== {{parameter_values}} ====/ Parameter Values JSON ==== ==== Distractor Tools ==== {{distractor_tools}} ====/ Distractor Tools ==== =====/ Context Information ===== Figure 15: System prompt for user-proxy agent used during dynamic evaluation"
        }
    ],
    "affiliations": [
        "SAP Labs"
    ]
}