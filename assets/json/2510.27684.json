{
    "paper_title": "Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals",
    "authors": [
        "Xiangyu Fan",
        "Zesong Qiu",
        "Zhuguanyu Wu",
        "Fanzhou Wang",
        "Zhiqian Lin",
        "Tianxiang Ren",
        "Dahua Lin",
        "Ruihao Gong",
        "Lei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 4 8 6 7 2 . 0 1 5 2 : r Phased DMD PHASED DMD: FEW-STEP DISTRIBUTION MATCHING DISTILLATION VIA SCORE MATCHING WITHIN SUBINTERVALS Xiangyu Fan1, Zesong Qiu1, Zhuguanyu Wu1, Fanzhou Wang1, Zhiqian Lin1, Tianxiang Ren1, Dahua Lin1, Ruihao Gong1,2, Lei Yang1,(cid:0) 1SenseTime Research, 2Beihang University"
        },
        {
            "title": "ABSTRACT",
            "content": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, state-of-the-art (SOTA) diffusion models have made significant progress in image and video generation. In image generation, SOTA models (Wu et al., 2025; OpenAI, 2025; Team, 2025b; Cao et al., 2025; GoogleAI, 2025a; Seedream et al., 2025) demonstrate precise prompt control, enabling complex text-to-image rendering and accurate layout specification. In video generation, these models (Wan et al., 2025; Kong et al., 2024; GoogleAI, 2025b; OpenAI, 2024) exhibit substantial improvements in dynamic scene generation, such as fast-moving objects in sports and complex camera movements like ego-centric videos. Simultaneously, the increasing parameter sizes and computational demands of base models highlight the importance of accelerating diffusion model sampling. Several techniques have been proposed to accelerate diffusion models, including classifier-free guidance (CFG) distillation (Meng et al., 2023), step distillation (Song et al., 2023; Wang et al., 2024; Salimans & Ho, 2022; Yin et al., 2024a; Luo et al., 2023; Luo, 2024; Zhou et al., 2024; Huang et al., 2024a; Lin et al., 2024; 2025a;b; Frans et al., 2024; Geng et al., 2025), SVDQuant (Li* et al., 2025), Mixture-of-Expert (MoE) models (Balaji et al., 2022; Feng et al., 2023; Wan et al., 2025), and parallel computation (Fang et al., 2024). Among these, step distillation methods based on Variational Score Distillation(VSD), including diff-instruct (Luo et al., 2023), DMD (Yin et al., 2024a), SID (Zhou et al., 2024), achieve high-quality generation by distilling models into single-step generators. However, the limited network capacity (Lin et al., 2024) of single-step distilled models hinders their 1 Phased DMD Figure 1: Schematic diagram of (a) Few-step DMD (Yin et al., 2024a), (b) Few-step DMD with stochastic gradient truncation strategy (SGTS) (Huang et al., 2025), (c) Phased DMD and (d) Phased DMD with SGTS . ability to handle complex tasks like intricate text rendering or dynamic scene generation, which are critical for the widespread adoption of these foundational models. Few-step distillation balances computational cost and generation quality (Luo et al., 2025). Yet, as shown in Fig. 1a, directly applying VSD to few-step distillation (Yin et al., 2024a) introduces challenges such as increased computational graph depth and higher memory overhead. Furthermore, the lack of explicit constraints on intermediate generator steps reduces training stability and leads to suboptimal performance in few-step models. To address these issues, Huang et al. (2025) proposed stochastic gradient truncation strategy (SGTS), where multi-step sampling may terminate at random step and the gradient backpropagation is restricted to the final denoising step (see Fig. 1b). This approach improves training convergence and stability by supervising all intermediate steps while enhancing memory efficiency via gradient detachment for non-final steps. However, SGTS can terminate sampling after just one step during training, distilling one-step generator for that iteration. Consequently, the generative diversity of few-step generators trained with SGTS is reduced to level akin to that of one-step generators. The diffusion theory (Song et al., 2020) suggests the existence of infinitely many neural networks as score estimators across range of signal-to-noise ratios (SNR), spanning from zero to infinity. During the generation process, diffusion models exhibit distinct temporal dynamics (Balaji et al., 2022; Ouyang et al., 2024). Specifically, the low-SNR stage focuses on modeling image structures and video dynamics, while the high-SNR stage refines visual details. In practice, single neural network is typically employed throughout the denoising process, requiring the model to simultaneously learn and perform variety of denoising tasks. Recent studies (Balaji et al., 2022; Feng et al., 2023; Wan et al., 2025) have introduced an MoE architecture into diffusion models. By assigning specialized experts to different SNR levels, MoE enhances model capacity and generative performance without increasing inference cost. The performance improvement is particularly pronounced in video generation (Wan et al., 2025), where the low-SNR expert excels at capturing dynamic content. In this work, we propose Phased DMD, novel distillation framework for few-step generation. Our approach is inspired by broader vision: By decomposing complex task into learnable phases, each phase naturally forms an expert, collectively enhancing the models capacity in MoE manner. Our method is built upon two key components: Progressive distribution matching: Conceptually similar to ProGAN (Karras et al., 2017), which progressively trains generator to handle higher resolutions, Phased DMD divides SNR into subintervals and progressively distills models toward higher SNR levels. Score matching within SNR subintervals: As each phase is trained within subinterval, the training objective undergoes transformation. To ensure theoretical rigor, we derive the training objective for the fake score estimator within each subinterval. As illustrated in Fig. 1c, Phased DMD offers several advantages: First, by partitioning SNR into subintervals, the model learns complex data distributions incrementally, improving training stability and generative performance. Second, each phase involves only single gradient-recorded sampling 2 Phased DMD step, avoiding additional computational and memory overhead. Third, notably, Phased DMD naturally produces few-step MoE generative model, regardless of whether the teacher model adopts an MoE architecture. Last, as shown in Fig. 1d, Phased DMD can be combined with SGTS , enabling 4-step inference across 2 phases while simplifying the complexity of both training and inference. We validate Phased DMD by distilling SOTA image and video generation models, including QwenImage (Wu et al., 2025) with 20B parameters and Wan2.1/Wan2.2 (Wan et al., 2025) with 14/28B parameters. Experimental results demonstrate that Phased DMD better preserves output diversity compared to standard DMD while maintaining the base models key capabilities, such as faithful text rendering in Qwen-Image and realistic dynamic motion in Wan2.2. Our contributions are summarized as follows: We propose Phased DMD, data-free distillation framework for few-step diffusion models. This framework combines ideas from DMD and MoE, achieving higher performance ceilings while maintaining memory usage similar to single-step distillation. We derive the theoretical training objective for subinterval diffusion models without relying on external information, such as clean samples. We highlight the necessity of this correctness for DMD distillation. Without requiring GAN loss or regression loss, Phased DMD achieves SOTA results on textto-image and text-to-video generation models. To the best of our knowledge, this is the largest reported distillation validation. Experimental results show that our method effectively reduces diversity loss while preserving the base models key capabilities, including complex text rendering and high-dynamic video generation."
        },
        {
            "title": "2 METHOD",
            "content": "To clarify the principle of phased DMD, we begin by introducing the theoretical background and notations related to diffusion models (Kingma et al., 2023; Zhang et al., 2024), score matching (Song et al., 2020; Karras et al., 2022), and distribution matching distillation (Yin et al., 2024b;a). We explicitly highlight why the principle of DMD is applicable only to score-based generative models. Building on this foundation, we present the motivation behind Phased DMD and explain how it inherently achieves improved generative diversity. Following this, we detail the two key components of Phased DMD : progressive distribution matching and score matching within subintervals. 2.1 PRELIMINARY 2.1.1 DIFFUSION MODELS AND SCORE MATCHING Consider continuous-time Gaussian diffusion process defined over the interval 0 1. The ground-truth distribution is denoted p(x0). For any 0 1, the forward diffusion process is described by the following conditional distribution: p(xtx0) = (xt; αtx0, σ2 I) (1) where αt and σ2 are positive, scalar-valued functions of t. The signal-to-noise ratio (SNR) is defined as SNR(t) = α2 /σ2 . It is assumed that SNR(t) is strictly monotonically decreasing over time. No additional constraints are imposed on the relationship between αt and σt, ensuring the notations are compatible with different kinds of diffusion models (Ho et al., 2020; Karras et al., 2022; Song et al., 2022; Podell et al., 2023) and flow models (Liu et al., 2022; Esser et al., 2024). The diffusion process is Markovian (Kingma et al., 2023), meaning that p(xtxs, x0) = p(xtxs). Furthermore, p(xtxs) is also Gaussian, and can be expressed as: p(xtxs) = (xt; αtsxs, σ2 tsI) (2) where αts = αt/αs and σ ts = σ2 (cid:90) α2 tsσ2 . For any 0 < 1, the marginal distribution of (cid:90) xs and xt are given by p(xs) = p(xsx0)p(x0)dx0 and p(xt) = p(xtx0)p(x0)dx0. If only p(xs) is observed and not p(x0), the marginal distribution of xt can alternatively be expressed as: 3 Phased DMD (cid:90) p(xt) = p(xtxs)p(xs)dxs. Thus, we have the following equivalence: (cid:90) p(xt) = p(xtx0)p(x0)dx0 = (cid:90) p(xtxs)p(xs)dxs (3) In the training process, αt and σt are predefined functions of t, while x0 is sampled from the dataset distribution x0 p(x0). Timestep is sampled from predefined distribution over the interval [0, 1], such as uniform or logit-normal distribution (Esser et al., 2024), i.e., (t; 0, 1). The sample xt is then given by xt = αtx0 + σtϵ, where ϵ (ϵ; 0, I). We use and ϵ for brevity in later paragraphs unless otherwise specified. Song et al. (2020) unified diffusion models under the theoretical framework of score-based generative models and demonstrated that the continuous diffusion process is fundamentally governed by Stochastic Differential Equation (SDE). Here, we adopt flow velocity prediction as an example and demonstrate its connection to score matching. Let ψθ denote diffusion model parameterized by θ. The relationship between flow matching and score matching is expressed below. Jf low(θ) = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[ψθ(xt) (ϵ x0)2] = Ex0p(x0),tT ,xtp(xtx0)[ψθ(xt) + xt/αt + (σt + σ2 /αt)xt log(p(xtx0))2] = EtT ,xtp(xt)[ψθ(xt) + xt/αt + (σt + σ2 /αt)xt log(p(xt))2] (4) (5) Eq. 5 is derived based on the equivalence between denoising score matching (DSM) and explicit score matching (ESM), as originally proven in Vincent (2011). In Supp. A, we provide the detailed derivation of Eq. 5. Additionally, we demonstrate the connection between sample prediction (a.k.a. x-prediction) and score matching in Appendix A. 2.1.2 DISTRIBUTION MATCHING DISTILLATION Let Gϕ denote the generator parameterized by ϕ. The objective of DMD is to minimize the reverse Kullback-Leibler (KL) divergence between the real data distribution preal(x0) and the generated data distribution pf ake(x0), produced by Gϕ. DKL(pf akepreal) = EϵN ,x0=Gϕ(ϵ)[log pf ake(x0) log preal(x0)] (6) We use DKL to abbreviate DKL(pf akepreal) in later paragraphs. To leverage the pretrained diffusion models as score estimators, the generated samples are diffused and the objective becomes: DKL = EϵN ,x0=Gϕ(ϵ),tT ,xtp(xtx0)[log pf ake(xt) log preal(xt)] By combining Eq. 5 and Eq. 7, we can approximate the objective as: DKL EϵN ,x0=Gϕ(ϵ),tT ,xtp(xtx0)[λt(T ˆθ(xt) Fθ(xt))] (7) (8) where λt = 1/(σt + σ2 /αt), Fθ denotes the fake diffusion model and ˆθ denotes the teacher diffusion model. θ is initialized from ˆθ and Fθ is updated on pf ake(x0) according to Eq. 4. The derivation from Eq. 7 to Eq. 8 is valid under the condition that the models are score-based generative models. Formally, this approximation holds if Fθ(xt) atxt log(pf ake(xt)) + btxt and ˆθ(xt) atxt log(preal(xt))+btxt, where at is any non-zero function of and bt is any function of t. Taking the gradient of Eq. 8 with respect to the generator parameters, we have: ϕDKL EϵN ,x0=Gϕ(ϵ),tT ,xtp(xtx0)[wt(T ˆθ(xt) Fθ(xt))]dG/dϕ (9) where wt = λtαt. Similar to GANs (Goodfellow et al., 2014), DMD employs an adversarial training process consisting of two stages in each iteration. In the fake diffusion optimization stage, Fθ is optimized on the generated distribution using Eq. 4, allowing it to serve as score estimator for pf ake(xt). In the generator optimization stage, Gϕ is updated according to Eq. 9, encouraging the generated distribution to more closely approximate the real distribution. For training stability, Fθ receives more frequent updates, enabling it to accurately estimate the score of the evolving generated distribution (Yin et al., 2024a). 4 Phased DMD 2.2 FROM ONE-STEP DISTILLATION TO FEW-STEP DISTILLATION In -step distillation, we have scheduler with + 1 timesteps, = {t0, t1, t2, ..., tN }, where 0 = tN < ti < ti1 < t0 = 1 for any {2, ..., 1}. The sampling process begins with xt0 = ϵ (ϵ; 0, I). The sample x0 is then generated iteratively: for = 0, 1, ..., 1, we compute xti+1 = S(Gϕ(xti ), xti, ti, ti+1). Let pipeline(Gϕ, t, ϵ, S) denote this iterative sampling procedure. Eq. 9 is thus adapted as follows: ϕDKL EϵN ,x0=pipeline(Gϕ,t,ϵ,S),tT ,xtp(xtx0)[wt(T ˆθ(xt) Fθ(xt))]dG/dϕ (10) As shown in Fig. 1a, the depth of the computational graph during generator optimization increases linearly with , which reduces training stability and increases memory overhead. To address this issue, Huang et al. (2025) introduced stochastic gradient truncation strategy (SGTS), depicted in Fig. 1b. In this strategy, an index is randomly selected from {1, 2, ..., }, the corresponding timestep tj is set to 0. The sampling pipeline is then executed only for steps = 0, 1, ..., 1. Crucially, when = 1, the training iteration reduces to one-step distillation. Consequently, while SGTS improves memory efficiency and training stability, it reduces the generative diversity of the few-step models, as the generated distribution is biased toward that of one-step generator. 2.3 PHASED DMD In contrast to DMD with SGTS, which can degenerate into one-step distillation in certain iterations, Phased DMD avoids this issue by partitioning the distillation process into distinct phases and applying supervision at intermediate timesteps. In each phase except the last, the generator is optimized to minimize the reverse KL divergence at an intermediate timestep, while the fake diffusion model is updated via score matching within subinterval of the diffusion process. 2.3.1 DISTRIBUTION MATCHING AT INTERMEDIATE TIMESTEPS The motivation for Phased DMD can be understood by revisiting Eq. 10. To sample xt, prior methods (Yin et al., 2024a; Huang et al., 2025) first generate x0 and then diffuse it to xt according to Eq. 1. In phased DMD, the pipeline is modified to generate intermediate samples xtk , where 0 < , instead of x0. The sample xtk is then diffused according to Eq. 2, with = tk and is sampled from the subinterval (tk, 1), i.e., (t; tk, 1). As illustrated in Fig. 1c, Phased DMD progressively distills the generator toward higher SNR levels. In each phase k, only single expert Gϕk is trained. This expert maps the distribution p(xtk1 ) to p(xtk ). The generator optimization objective for the k-th phase is given by: ϕkDKL EϵN ,xtk =pipeline(Gϕ1 ,Gϕ2 ,...,Gϕk ,{t1,t2,...,tk},ϵ,S),tT (t;tk,1),xtp(xtxtk ) [wts(T ˆθ(xt) Fθi (xt))]dG/dϕk where wts = λtαts. Empirically, we find that sampling (t; tk, 1) instead of (t; tk, tk1), aligns better with the progressive design of Phased DMD and yields superior performance (Appendix E.2). At the onset of each phase, the fake diffusion model Fθk is re-initialized from the pretrained teacher model ˆθ and is trained independently of the models from previous phases. (11) Although the resulting MoE generator requires more GPU memory than single-network generator, the overhead is manageable for three reasons. First, an optimizer is required only for the k-th trainable expert. Second, this overhead can be substantially reduced using Low-Rank Adaptation (LoRA) (Hu et al., 2021). Specifically, all experts can share common backbone network, with individual experts activated by switching their respective LoRA weights. Finally, Phased DMD can be combined with SGTS (as shown in Fig. 1d), and the number of distillation phases can be less than the number of sampling steps. 2.3.2 SCORE MATCHING WITHIN SUBINTERVALS key challenge in Phased DMD is that clean data samples x0 are inaccessible in all but the final phase. Consequently, the training objective for the fake diffusion model Fθk in Eq. 4 is no longer applicable. To address this, we derive training objective based on score matching within subintervals. Assume we have observations xs p(xs) at an intermediate timestep where 0 < < 1. Phased DMD (a) Flow Match (b) Unbiased within Subinterval (c) Biased within Subinterval (a) Training with the Figure 2: Sampling trajectories for 200 samples in 1D toy experiment. full-interval objective (Eq. 4). (b) Training on 0.5 < < 1 with the correct subinterval objective (Eq. 13). (c) Training on 0.5 < < 1 with an incorrect target: (ψθ(xt) (ϵ xs)2. The diffusion model ψθ can be optimized within the subinterval (s, 1) using the following objective, derived from Eq. 5: Jf low(θ) = EtT (t;s,1),xtp(xt)[ψθ(xt) + xt/αt + (σt + σ2 = Exsp(xs),tT (t;s,1),xtp(xtxs)[ψθ(xt) + xt/αt + (σt + σ2 /αt)xt log(p(xt))2] /αt)xt log(p(xtxs))2] = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[ψθ(xt) ((α2 sσt + αtσ2 )/(α2 sσts)ϵ (1/αs)xs)2] (12) In the k-th phase of Phased DMD, the distribution p(xs) is approximated using the output of the MoE generator pipeline Gϕ1 , Gϕ2 , ..., Gϕk. As σts 0 when s, the formulation in Eq. 12 encounters singularity and numerical instability. To mitigate this, we apply clamping function, resulting in the final objective: Jf low(θ) = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ sσt + αtσ2 [clamp(1/(σts)2)σtsψθ(xt) ((α2 s)ϵ (σts/αs)xs)2] Here, clamp(1/(σts)2) restricts the value within predefined range to prevent overflow. We design one-dimensional toy experiment to validate the effect of this training objective, as shown in Fig. 2. The close overlap of the sampling trajectories in Fig. 2b demonstrates that, within the defined subinterval, the flow model trained with Eq. 13 is equivalent to one trained with the standard objective in Eq. 4. Conversely, Fig. 2c illustrates how an incorrect formulation of the objective leads to biased estimation. Refer detailed settings of toy example to Appendix D. )/α (13)"
        },
        {
            "title": "3 EXPERIMENTS AND RESULTS",
            "content": "We apply Phased DMD to state-of-the-art (SOTA) image and video generative models. All experiments are conducted using 4-step, 2-phase configuration, as illustrated in Fig. 1d. Consequently, each base model is distilled into two expert networks. To demonstrate that the performance improvement stems primarily from our novel distillation paradigm rather than merely an increase in trainable parameters, we include the Wan2.2-T2V-A14B model (Wan et al., 2025) in our experiments. This model already features an MoE structure, and both standard DMD and our Phased DMD distill it into two experts. This allows for direct comparison under equivalent parameter budgets. Owing to its computational demands, the vanilla DMD (Yin et al., 2024a) method was applied only to the smallest model configuration, namely the Wan2.1-T2V-14B. An overview of the experimental configurations is provided in Tab. 1, with detailed descriptions available in Appendix C. 3.1 PRESERVATION OF GENERATIVE DIVERSITY To evaluate generative diversity, we constructed text-to-image test set comprising 21 prompts. Each prompt provides short description of the image content without detailed specifications. For 6 Phased DMD Table 1: Overview of Experimental Setup. Base Model Task DMD DMD with SGTS Wan2.1-T2V-14B Wan2.2-T2V-A14B T2I, T2V, I2V Qwen-Image-20B T2I T2I Phased DMD (Ours) each prompt, we generated 8 images using seeds from 0 to 7. For the base model, images are sampled using 40 steps with CFG scale of 4. All distilled models are sampled using 4 steps and CFG scale of 1. As shown in Fig. 3b, images generated by the 4-step DMD model exhibit loss of fine details. While the 4-step DMD model with SGTS improves image quality, this comes at the cost of reduced diversity. Fig. 3c reveals that the generated images often adopt similar close-up view and demonstrate limited variation in composition across different random seeds. In contrast, Phased DMD better preserves diversity, producing images with wider range of natural compositions, as illustrated in Fig. 3d. Generative diversity is evaluated using two complementary metrics: (1) the mean pairwise cosine similarity of DINOv3 features (Simeoni et al., 2025), where lower values indicate higher diversity, and (2) the mean pairwise LPIPS distance (Zhang et al., 2018), where higher values denote greater diversity. Both metrics are computed across images generated from the same prompt using different seeds. The quantitative results are presented in Tab. 2. As expected, the base models achieve the highest diversity. Notably, DMD with SGTS yields slightly lower diversity than vanilla DMD. Our Phased DMD outperforms both distillation baselines, demonstrating its superior capability for preserving the generative diversity of the original model. The diversity improvement on Qwen-Image is marginal. We argue this stems from the base models own limited output diversity. Table 2: Two metrics for quantitative diversity evaluation: average pairwise DINOv3 cosine similarity (lower is better) and LPIPS distance (higher is better). Phased DMD outperforms the vanilla DMD and DMD with SGTS in preserving generative diversity of the base models. Method Base model DMD DMD with SGTS Phased DMD (Ours) Wan2.1-T2V-14B Wan2.2-T2V-A14B DINOv3 LPIPS DINOv3 LPIPS DINOv3 LPIPS Qwen-Image 0. 0.825 0.826 0.782 0.607 0.522 0.521 0.544 0.732 - 0.828 0.768 0. - 0.447 0.481 0.907 - 0.941 0.958 0.483 - 0.309 0.322 3.2 RETAIN BASE MODELS KEY CAPABILITIES Wan2.2 video generation models exhibit remarkable capabilities in motion dynamics and camera control. However, we observe that DMD with SGTS degrade these properties, as they do not specifically address the low-SNR base expert. Phased DMD inherently resolves this issue by dividing distillation into phases and explicitly eliminating dependency on x0 except in the final phase. In the first phase, only the low-SNR expert attends and is distilled according to Eq. 11 and Eq. 13. Since the pre-trained low-SNR expert is also trained on the low-SNR subinterval, this alignment better preserves its capabilities. As shown in Fig. 6, DMD with SGTS generates slower motion Table 3: Comparison of motion dynamics preservation across distillation methods, measured by mean absolute optical flow and VBench (Huang et al., 2024b) dynamic degree. Phased DMD outperforms in retaining the base models motion quality for both T2V and I2V tasks. Method T2V I2V Optical Flow Dynamic Degree Optical Flow Dynamic Degree Base model DMD with SGTS Phased DMD(Ours) 10.26 3.23 7.57 9. 7.87 9.84 82.27 % 80.00 % 83.64 % 79.55 % 65.45 % 74.55 % Prompt: chef meticulously plating dish. Phased DMD seed 0 seed 1 seed 2 seed Prompt: mother braiding her daughter hair, sunlight warming the room. seed 0 seed 1 seed 2 seed 3 (a) Base (b) DMD (c) DMD with SGTS (d) Phased DMD Figure 3: Samples (seeds 0-3) from the Wan2.1-T2V-14B base model (40 steps, CFG=4) and its distilled variants (4 steps, CFG=1): (a) Base, (b) DMD, (c) DMD with SGTS, (d) Phased DMD. dynamics compared to the base model and Phased DMD. Similarly, Fig. 7 show that DMD with SGTS tends to produce close-up views, while Phased DMD and the base model better adhere to the prompts camera instructions. We evaluate motion quality using set of 220 text prompts for T2V and 220 image-prompt pairs for I2V, generating one video per prompt with fixed seed 42. Motion intensity is quantified using the mean absolute optical flow computed with Unimatch (Xu et al., 2023) and the dynamic degree metric from VBench (Huang et al., 2024b). As Tab. 3 shows, Phased DMD produces significantly stronger motion dynamics than DMD with SGTS, confirming its superior ability to preserve the base models motion capabilities. Additional comparative videos are provided in the supplementary material. Figure 4: Examples generated by the Qwen-Image distilled with Phased DMD. 8 Phased DMD Qwen-Image is recognized for its faithful adherence to prompts and high-quality text rendering. To evaluate the preservation of these capabilities after distillation, we applied Phased DMD to QwenImage and generated images using prompts from its official website (Team, 2025a). As shown in Fig. 4, the model distilled with Phased DMD exhibits well-preserved capabilities, producing highquality images with accurate text rendering. 3.3 MERIT OF MOE Our empirical findings reveal that during the distillation process, DMD initially captures structural information before learning finer textural details. Before the complete acquisition of textural details, the generated images and videos tend to exhibit overly smooth features, such as blurry hair and plastic-like skin textures. On the other hand, the mode-seeking nature of reverse KL divergence leads to decline in generative diversity as training iterations increase. Phased DMD addresses the trade-off between quality and diversity by dividing DMD into distinct training phases. In the low-SNR phases, the composition of images and videos is effectively established. During the subsequent high-SNR phases, the low-SNR expert is frozen, allowing for extended training to enhance generation quality without degrading the structural composition of the outputs. As illustrated in Fig. 5, extending the training of high-SNR experts primarily affects lighting and textural details, while leaving the overall structural composition of the images unchanged. Figure 5: Samples generated with high-SNR experts from different training stages (top: 100 iterations; bottom: 400 iterations) and shared low-SNR expert. Each column uses identical prompts and seeds."
        },
        {
            "title": "4 RELATED WORKS",
            "content": "Our work builds on Variational Score Distillation (VSD), comprising trainable generator, fake score estimator, and pretrained teacher score estimator. The closest related work is TDM (Luo et al., 2025), which also extends DMD to few-step distillation. Yet, Phased DMD differs in three key ways: (a) TDM lacks theoretical grounding, leading to incorrect fake flow training; (b) our framework inherently produces MoE models; and (c) we use reverse nested SNR intervals, unlike TDMs disjoint intervals. Full discussions about related work are presented in Appendix B."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "Phased DMD primarily enhances structural aspects of generation, such as image composition diversity, motion dynamics, and camera control. However, for base models like Qwen-Image, whose outputs are inherently less diverse, the improvement is less pronounced. While this work demonstrates phased distillation within the DMD framework, the approach is generalizable to other objectives like Fisher divergence in SiD (Zhou et al., 2024), which we leave for future exploration. It is conceivable that other methods for enhancing diversity and dynamics, such as incorporating trajectory data pregenerated by the base model, could be integrated. However, this would compromise the data-free advantage central to DMD. While we may explore such directions in the future, this work prioritizes the data-free paradigm. 9 Phased DMD"
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work complies with the ICLR Code of Ethics. The proposed method follows DMD and is data-free distillation framework. However, the base model used for distillation may generate human figures due to the presence of human data in the training set, potentially raising concerns about privacy and consent. To address this, we focus solely on human motion dynamics, with no use of personally identifiable information. Regarding the video generation model, while it offers positive applications in content creation, it also carries risks of misuse for deceptive content or surveillance. We acknowledge these risks and emphasize that our model is intended strictly for scientific research and positive use cases."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "We have taken extensive measures to ensure reproducibility. To reproduce Phased DMD, the core equations are provided in Sec.2 of the main text, with detailed derivations in Appendix A. For the toy example to verify the effectiveness of score matching with subintervals, relevant details can be found in the Appendix D. To replicate our experiments, details of the experimental setup, hyperparameters, evaluation metircs and implementation choices are available in Sec. 3 of the main text and Appendix 3. Code and models will also be released."
        },
        {
            "title": "REFERENCES",
            "content": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv.org/abs/ 2403.03206. Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1013510145, 2023. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https: //arxiv.org/abs/1406.2661. GoogleAI. Image generation with gemini (aka nano banana), 2025a. URL https://ai. google.dev/gemini-api/docs/image-generation. GoogleAI. Generate videos with veo 3 in gemini api, 2025b. URL https://ai.google.dev/ gemini-api/docs/video?example=dialogue. 10 Phased DMD Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Zemin Huang, Zhengyang Geng, Weijian Luo, and Guo-jun Qi. Flow generator matching. arXiv preprint arXiv:2410.19310, 2024a. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024b. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023. URL https://arxiv.org/abs/2107.00630. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025a. Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Weijian Luo. Diff-instruct++: Training one-step text-to-image generator model to align with human preferences. arXiv preprint arXiv:2410.18881, 2024. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, and Jing Tang. Learning few-step diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and In Proceedings of the IEEE/CVF Tim Salimans. On distillation of guided diffusion models. conference on computer vision and pattern recognition, pp. 1429714306, 2023. 11 Phased DMD OpenAI. Video generation models as world simulators, 2024. URL https://openai.com/ index/video-generation-models-as-world-simulators/. OpenAI. Introducing 4o image generation, 2025. URL https://openai.com/index/ introducing-4o-image-generation/. Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via image-to-video diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. URL https://arxiv.org/abs/2307.01952. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. URL https://arxiv.org/ abs/2508.10104. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. URL https://arxiv.org/abs/2303.01469. Qwen Team. Qwen-image: Crafting with native text rendering, 2025a. URL https://qwenlm. github.io/blog/qwen-image/. Tencent Hunyuan Team. for highresolution (2k) text-to-image generation. https://github.com/Tencent-Hunyuan/ HunyuanImage-2.1, 2025b. Hunyuanimage 2.1: An efficient diffusion model Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. 12 Phased DMD Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/ 2508.02324. Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2024b. URL https://arxiv.org/abs/2311.18828. Pengze Zhang, Hubery Yin, Chen Li, and Xiaohua Xie. Tackling the singularities at the endpoints of time intervals in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 69456954, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 13 Phased DMD"
        },
        {
            "title": "A DETAILED DERIVATION OF METHOD",
            "content": "We show the detailed derivation of Eq. 5 as follows: Jf low(θ) = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[ψθ(xt) (ϵ x0)2] = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[ψθ(xt) (ϵ (xt σtϵ)/αt))2] = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[ψθ(xt) + xt/αt (1 + σt/αt)ϵ2] = Ex0p(x0),tT ,xtp(xtx0)[ψθ(xt) + xt/αt + (σt + σ2 = EtT ,xtp(xt)[ψθ(xt) + xt/αt + (σt + σ2 /αt)xt log(p(xt))2] /αt)xt log(p(xtx0))2] In the derivation, we use the the score of p(xtx0), i.e., xt log(p(xtx0)) = (1/σt)ϵ, and the equivalence between DSM and ESM (Vincent, 2011). We show the detailed derivation of Eq. 12 as follows: Jf low(θ) = EtT (t;s,1),xtp(xt)[ψθ(xt) + xt/αt + (σt + σ2 = Exsp(xs),tT (t;s,1),xtp(xtxs)[ψθ(xt) + xt/αt + (σt + σ2 = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[ψθ(xt) + xt/αt ((σt + σ2 /αt)/σts)ϵ2] = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[ψθ(xt) + (αtsxs + σtsϵ)/αt ((σt + σ2 = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[ψθ(xt) ((α /αt)xt log(p(xt))2] /αt)xt log(p(xtxs))2] sσt + αtσ2 )/(α2 /αt)/σts)ϵ2] sσts)ϵ (1/αs)xs)2] The relationship between sample prediction (x-prediction) and score matching is derived as follows: Jsample(θ) = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[µθ(xt) x02] = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[µθ(xt) (xt σtϵ)/αt)2] = Ex0p(x0),ϵN ,tT ,xt=αtx0+σtϵ[µθ(xt) xt/αt + (σt/αt)ϵ2] = Ex0p(x0),tT ,xtp(xtx0)[µθ(xt) xt/αt (σ2 = EtT ,xtp(xt)[µθ(xt) xt/αt (σ2 /αt)xt log(p(xt))2] /αt)xt log(p(xtx0))2] (14) The training objective for x-prediction diffusion models within subinterval is as follows: Jsample(θ) = EtT ,xtp(xt)[µθ(xt) xt/αt (σ2 = Exsp(xs),tT (t;s,1),xtp(xtxs)[µθ(xt) xt/αt (σ2 = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[µθ(xt) xt/αt + ((σ2 /αt)/σts)ϵ2] = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[µθ(xt) (αtsxs + σtsϵ)/αt + ((σ2 = Exsp(xs),ϵN ,tT (t;s,1),xt=αtsxs+σtsϵ[µθ(xt) ((1/αs)xs (αtσ2 /αt)xt log(p(xt))2] /αt)xt log(p(xtxs))2] /α2 /(αtσts)ϵ2] (15) sσts)ϵ)2] Optimizing within the subinterval according to Eq. 15 gives an unbiased estimation of x-prediction. In contrast, the objective [µθ(xt) xs2] yields biased estimation."
        },
        {
            "title": "B RELATED WORKS",
            "content": "Our work is situated within the framework of Variational Score Distillation (VSD) (Wang et al., 2023). VSD involves three components: trainable generator, fake score estimator, and pretrained teacher score estimator. The generator is optimized to produce distribution that approximates the real data distribution. Concurrently, the fake score estimator learns to estimate the score of the generators output distribution. The update direction for the generator is then determined by the discrepancy between the teachers score (for the real distribution) and the fake score estimators score. Similar to GANs, the VSD framework is adversarial. The fake score estimator must be precisely optimized to learn the score of the current generated distribution. This accurate estimation is crucial, 14 Phased DMD as it combines with the fixed teacher model (which provides the score for the real data) to produce correct guidance signal for the generator. This principle explains why DMD2 (Yin et al., 2024a) operates successfully without external real data, in contrast to its predecessor DMD (Yin et al., 2024b). key advantage of VSD over GANs for distilling pre-trained diffusion models is initialization. The pre-trained model serves dual role: it is powerful multi-step generator and an accurate estimator of the real data distributions score. This allows it to effectively initialize all three components in the VSD framework, leading to significantly enhanced training stability. Several methods are built upon the VSD framework, including Diff-Instruct (Luo et al., 2023), DMD (Yin et al., 2024a), SID (Zhou et al., 2024), and FGM (Huang et al., 2024a). The fundamental distinction between these approaches lies in the specific divergence they minimize. DMD, for instance, optimizes the reverse KL divergence between the real and generated distributions. key advantage of this choice is its computational efficiency compared to alternatives like the Fisher divergence used in SID (Zhou et al., 2024). Specifically, during generator optimization, DMD does not require gradients to be backpropagated through the fake and teacher score estimators, whereas SID does. This does not imply the two estimators are trainable in this stage for SID, but rather reflects difference in the computational graph. This property makes DMD more amenable to engineering implementation and scalable to large base models. Similar to our work, TDM (Luo et al., 2025) also aimed to extend DMD to few-step distillation. However, our approach differs from TDM in three key aspects: (a) The lack of proper theoretical grounding in TDM renders its fake flow training formulation incorrect, undermining the foundations of DMD. (b) Our framework inherently produces MoE models for few-step generation. (c) While TDM uses disjoint SNR intervals, our method employs reverse nested intervals, where each interval is subset of the subsequent one."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "We conduct experiments on three tasks: text-to-image, text-to-video and image-to-video generation. The following global settings are applied across all experiments: batch size of 64; fake diffusion model learning rate of 4e-7 with full-parameter training; generator learning rate of 5e-5 using LoRA with rank of 64 and an alpha value of 8. The AdamW optimizer is used for both the fake diffusion model and the generator, with hyperparameter β1 = 0, β2 = 0.999. The fake diffusion model is updated five times for every generator update. For the Wan2.x base models, distillation for the text-to-image task is performed at data resolution of frame = 1, width = 1280, height = 720. For the Wan2.2-T2V-A14B model, distillation for the text-to-video and image-to-video task uses mixture of data resolutions: (81, 720, 1280), (81, 1280, 720), (81, 480, 832), (81, 832, 480). For the Qwen-Image model, distillation for the text-to-image task uses mixture of data resolutions: (1, 1382, 1382), (1, 1664, 928), (1, 928, 1664), (1, 1472, 1104), (1, 1104, 1472), (1, 1584, 1056), (1, 1056, 1584)."
        },
        {
            "title": "D TOY EXAMPLE DETAILS",
            "content": "We construct toy example where x0 takes only four values: {-1, 0, 1, 2}. minimal model is designed, consisting of four MLPs with dim=512, conditioned solely on t. Fig. 1a shows training on the full interval using Eq. 4, Fig. 1b shows training on subintervals using Eq. 13, and Fig. 1c shows training on subintervals using Eq. 4, simply replacing x0 with xs. As shown in Fig. 4b, when the correct objective is used, the trajectories on subintervals perfectly align with those on the full interval. In contrast, using an incorrect objective introduces trajectory deviations, as illustrated in Fig. 4c. Such trajectory deviation signifies that the trained model no longer satisfies the scorematching objective (i.e., Eq. 5 is violated), thus contravening core principle of DMD. 15 Phased DMD"
        },
        {
            "title": "E MORE RESULTS",
            "content": "E.1 MOTION DYNAMICS AND CAMERA CONTROL As shown in Fig. 6, DMD with SGTS generates slower motion dynamics compared to the base model and Phased DMD. Similarly, Fig. 7 show that DMD with SGTS tends to produce close-up views, while Phased DMD and the base model better adhere to the prompts camera instructions. E.2 ABLATION ON DIFFUSION TIMESTEP SUBINTERVALS Empirically, we observe that sampling (t; tk, 1) outperforms sampling (t; tk, tk1) in terms of generation quality. Fig. 8 illustrates the results of these two methods in the Wan2.2 T2V distillation task. Specifically, sampling (t; tk, 1) yields normal color tones and accurate structures, whereas sampling (t; tk, tk1) results in low-contrast tones and degraded facial structures. At the beginning of each phase in Phased DMD, there is substantial gap between the distribution of samples generated by the few-step generator and the distribution of real samples. The generated samples fall outside the domain of the teacher model, leading to inaccurate score estimations. This discrepancy is particularly pronounced in the high-SNR (low noise level) range, where samples are less corrupted by noise. In contrast, in the low-SNR (high noise level) range, the diffused generated distribution overlaps more significantly with the diffused real distribution, enabling the teacher model to provide more accurate score estimations. Consequently, noise injection at high noise levels plays crucial role in DMD training. To validate this analysis, we perform ablation studies on vanilla DMD for the Wan2.1 T2I task. Specifically, the diffusion timestep is fixed at 0.357 for one experiment and at 0.882 for another. Wang et al. (2023) has proven that DKL(pf ake(xt)preal(xt) = 0 DKL(pf ake(x0)preal(x0) = 0 for any 0 < < 1. Thus, both experiments are theoretically valid. However, the experiment with diffusion timestep = 0.357 fails to converge, as illustrated in Fig. 9, while the experiment with = 0.882 demonstrates correct results. This controlled experiment highlights that incorporating high noise levels is essential for effective DMD training. 16 Phased DMD (a) Base (c) Phased DMD (b) SGTS DMD Figure 6: Comparison of video frames generated by the Wan2.2-T2V-A14B base model and its distilled versions using DMD with SGTS and Phased DMD. Each video consists of 81 frames and frames with indices {0, 10, ..., 80} are combined as preview. The base model was sampled with 40 steps and CFG of 4, while the distilled models used 4 steps and CFG of 1 (seed fixed at 42). The prompt is parkour athlete swiftly runs horizontally along brick wall in an urban setting. Pushing off powerfully with one foot, they launch themselves explosively into twisting front flip. The camera tenaciously stays with them in mid-air as they tuck their legs tightly to their chest to rapidly accelerate the rotation, then extend them forcefully outwards again, precisely spotting their landing on the concrete below. The dynamic movement is vividly captured against backdrop of city lights and shadows. Phased DMD (a) Base (c) Phased DMD (b) SGTS DMD Figure 7: Comparison of video frames generated by the Wan2.2-T2V-A14B base model and its distilled versions using DMD with SGTS and Phased DMD. Each video consists of 81 frames and frames with indices {0, 10, ..., 80} are combined as preview. The base model was sampled with 40 steps and CFG of 4, while the distilled models used 4 steps and CFG of 1 (seed fixed at 42). The prompt is Day time, sunny lighting, low angle shot, warm colors. dynamic individual in vibrant, multi-colored outfit and red helmet executes fast-paced slalom on roller skates through bustling urban park. The camera starts focused on the skates carving sharp turns on the pavement and tilts up to reveal their entire body leaning into the motion. Their face shows mix of joy and deep concentration. The warm afternoon sun filters through the lush greenery, with the azure sky visible above, creating scene bursting with energy. 18 Phased DMD (a) Disjoint Intervals (b) Reverse Nested Intervals Figure 8: The effect of noise injection intervals. Luo et al. (2025) employs disjoint noise injection timestep intervals for different generation steps, where the intervals do not overlap. In contrast, we adopt reverse nested intervals, where the diffusion timestep interval in each phase terminates at 1.0. Integrating disjoint intervals into Phased DMD leads to unnatural colors and deteriorated facial structures, as illustrated on the left. Conversely, adopting reverse nested intervals yields correct results. 19 Phased DMD (a) = 0. (b) = 0.882 Figure 9: The effect of noise injection timestep in DMD training. In DMD training, noise is injected into the generated samples at low noise level (left) and high noise level (right). The training fails to converge correctly when noise is injected exclusively at low noise level."
        }
    ],
    "affiliations": [
        "Beihang University",
        "SenseTime Research"
    ]
}