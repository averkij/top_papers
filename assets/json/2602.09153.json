{
    "paper_title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
    "authors": [
        "Nicholas Pfaff",
        "Thomas Cohn",
        "Sergey Zakharov",
        "Rick Cory",
        "Russ Tedrake"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\\unicode{x2013}$from architectural layout to furniture placement to small object population$\\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation."
        },
        {
            "title": "Start",
            "content": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Nicholas Pfaff 1 Thomas Cohn 1 Sergey Zakharov 2 Rick Cory 2 Russ Tedrake 1 https://scenesmith.github.io/ 6 2 0 2 9 ] . [ 1 3 5 1 9 0 . 2 0 6 2 : r Figure 1. Fully automated text-to-scene generation. This entire community center was generated by SceneSmith without any human intervention, from single 151-word text prompt (full prompt in Appendix O.6). Beyond explicitly specified elements, SceneSmith places additional objects from inferred contextual information, such as ping pong paddles and balls placed near ping pong table. Objects are generated on-demand, are fully separable (non-composite), and include estimated physical properties, enabling direct interaction within simulation. The resulting scenes are immediately usable in arbitrary physics simulators (robots added for demonstration). Abstract Simulation has become key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely 1Massachusetts Institute of Technology 2Toyota Research Institute. Correspondence to: Nicholas Pfaff <nepfaff@mit.edu>. Preprint. February 11, 2026. furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stagesfrom architectural layout to furniture placement to small object populationeach implemented as an interaction among VLM agents: designer, critic, and orches1 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes trator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation. 1. Introduction Recent progress in general-purpose robotics has been driven by large-scale foundation models that promise broad generalization across tasks, embodiments, and environments (Black et al., 2025; Kim et al., 2024; Barreiros et al., 2025). Companies such as 1X and Sunday are now explicitly targeting the deployment of robots into arbitrary human homes. Achieving this vision requires robots that can robustly perceive, reason, and act across the long tail of real-world indoor environmentsspaces that vary widely in layout, object composition, clutter, articulation, and physical interaction affordances. central challenge in developing such robots is how to train and evaluate them at scale before deployment. While real-world data collection is essential, it is costly, slow, and difficult to scale across the diversity of homes robots are expected to operate in. As result, simulation has emerged as key tool for scalable robot training and evaluation, enabling rapid iteration, controlled experimentation, and safe testing of failure modes (Wei et al., 2025; Maddukuri et al., 2025; Barreiros et al., 2025). However, most existing simulation environments remain overly simplistic and poorly matched to real human indoor spaces. Typical simulated environments consist of sparsely furnished rooms, limited object diversity, and largely static scenes (Maddukuri et al., 2025; Barreiros et al., 2025). In contrast, real homes exhibit dense object arrangements, articulated furniture, and fine-grained clutter. For example, even modest one-bedroom apartment may contain kitchen cabinet densely packed with plates, bowls, and glassesall individually manipulable and physically plausible. Such clutter is central challenge in robotic manipulation (Zeng et al., 2022; Jia & Chen, 2024), yet sparse simulated environments may fail to prepare robots for these conditions. This gap between simulated environments and real-world indoor scene distributions limits the effectiveness of simulation-based training and evaluation for general2 purpose home robots. While manual environment design can produce high-quality scenes, this approach is costly, time-consuming, and does not scale easily. We aim to close this gap by enabling the scalable generation of realistic, simulation-ready indoor environments that reflect the diversity and physical complexity of real homes. Specifically, we seek framework that takes naturallanguage description of an environment or task, and constructs roomor house-level indoor environment through sequence of grounded decisions, resulting in scenes that are immediately simulatable and suitable for robotic interaction. Samples from this framework should reflect real-world indoor scene distributions while matching the prompt and being physically feasible. This problem spans both asset generation, producing individual objects with geometry and physical properties, and scene generation, where those assets are assembled into multi-room indoor environments with realistic layouts and object arrangements. Prior work has largely addressed these aspects in isolation. Asset-centric approaches focus on reconstructing or synthesizing individual objects with realistic geometrical and physical properties (Pfaff et al., 2025a). Scene-centric approaches generate layouts or object arrangements assuming fixed library of assets (Yang et al., 2024b; Pfaff et al., 2025b). In contrast, we aim to jointly generate simulation-ready assets and assemble them into complete house-level scenes, enabling end-to-end generation of environments suitable for robotics. Prior scene synthesis methodswhether procedural, data-driven, or LLM-based primarily target furniture-level layout and visual realism, treating small objects, articulated assets, and physical properties as secondary (Deitke et al., 2022b; Tang et al., 2024; Yang et al., 2025). This is misaligned with robotics requirements, where dense arrangements of manipulable objects, hierarchical support relationships, and physically valid configurations are essential. We propose SceneSmith, hierarchical, agentic framework for generating simulation-ready indoor environments from natural language. SceneSmith constructs scenes through sequence of stagesfrom architectural layout to furniture placement to small object populationorganized as tree where rooms and supporting surfaces form independent branches. Building upon recent advances in agentic AI, each stage is implemented as an interaction between three VLM agents: designer that proposes scene modifications, critic that evaluates feasibility and alignment, and an orchestrator that manages iterative refinement. Asset generation is tightly integrated through routing mechanism that combines modern text-to-3D synthesis for static objects, dataset retrieval for articulated furniture, and physical property estimation, ensuring generated scenes are immediately usable for robotics simulation. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes We evaluate SceneSmith across 210 diverse roomand house-level prompts, demonstrating its ability to generate dense, articulated, and physically feasible indoor environments. In user study with 205 participants, SceneSmith achieves 92.2% average realism win rate and 91.5% average prompt faithfulness win rate against baselines. SceneSmith generates 3-6x more objects than baselines (71.1 vs 11-23 objects per room on average) while maintaining <2% inter-object collisions and 95.6% of objects remaining stable under physics simulation, compared to 3-29% collisions and 8-61% stability for baselines. In addition, we demonstrate an end-to-end robot policy evaluation pipeline in which natural-language task descriptions are converted into diverse scene prompts, simulated robots execute policies in the generated environments, and an evaluator agent verifies task completion by jointly reasoning over symbolic scene state and visual observations. In summary, our key contributions are: We introduce SceneSmith, hierarchical, agentic framework for constructing simulation-ready indoor environments from natural language, designed to support scalable robot training and evaluation. We develop an integrated asset generation and routing pipeline combining text-to-3D synthesis with retrieval for articulated objects, augmenting all assets with collision geometry and physical properties. We show that SceneSmith outperforms all baselines in user studies and automated metrics, generating denser, collision-free, and physically stable scenes. We demonstrate SceneSmith in an end-to-end robotics evaluation pipeline, from natural-language task descriptions to automatic success verification. 2. Related Work Indoor Scene Synthesis. Indoor scene synthesis has been approached through procedural, data-driven, and Procedural methods like language-guided paradigms. ProcTHOR (Deitke et al., 2022b) and Infinigen Indoors (Raistrick et al., 2024) encode object relationships via hand-crafted rules. While scalable, they have limited semantic expressivity and extensibility across scene types. Data-driven generative models learn spatial patterns from 3D scene datasets (Tang et al., 2024; Yang et al., 2024a; Hu et al., 2024). However, they typically operate under restrictive assumptions such as floor-aligned SE(2) layouts, producing sparse scenes with limited object diversity. Recent work leverages LLMs and VLMs to provide openvocabulary semantic priors and natural-language controllability (Yang et al., 2024b; elen et al., 2025; Sun et al., 2024; Zhou et al., 2025). While these approaches capture highlevel semantics, they often struggle with fine-grained spatial reasoning and physical feasibility, resulting in implausible placements or object interpenetrations. More broadly, none of the above methods produce scenes with the collision geometry and physical properties required for robotics simulation. HSM (Pun et al., 2026) decomposes scene generation hierarchicallyplacing furniture before small objectsand identifies support surfaces for manipuland placement. We use their support surface detection and adopt similar hierarchical structure. Our extensions include hierarchical prompt refinement, where placement prompts derive constraints from the global scene description, and joint surface population, where related surfaces are populated together for consistency. This enables coordinated placement across surfaces (e.g., books on one shelf, plants on the other). Agentic Scene Synthesis. Recent work has moved from single-shot generation to iterative refinement through agentic systems. SceneWeaver (Yang et al., 2025) employs reason-act-reflect paradigm where single LLM planner selects one tool per turn from an extensible toolkit, guided by physical and semantic evaluations. LL3M (Lu et al., 2025) introduces iterative refinement between designer (coding agent) and critic for 3D asset generation; we adapt this pattern for scene-level generation. We build on these agentic approaches but allow unlimited tool invocations per agent turn, including visual observations and state feedback, enabling agents to self-verify before handoff. 3. Agentic Scene Construction for Simulation-Ready Environments We present SceneSmith, hierarchical agentic system for constructing simulation-ready indoor environments from natural language prompts. SceneSmith decomposes scene creation into sequence of decisions over layout, furnishing, object population, and refinement. Each stage is implemented as an agentic interaction between designer, critic, and an orchestrator, each equipped with specialized tools. We begin by describing the scene representation and hierarchical construction process (Section 3.1). We then detail the agentic interactions and tool abstractions (Section 3.2). Next, we introduce the asset generation and routing pipeline used to produce simulation-ready objects (Section 3.3), followed by physical feasibility post-processing (Section 3.4). Finally, we describe an application to automatic robot policy evaluation using our generated scenes (Section 3.5). 3.1. Scene Hierarchy We represent scene as set of rooms, = {Rj {1, . . . , }}, constructed from natural-language prompt . Each room Rj = (Gj, Oj) consists of architectural geometry Gj (walls, floor, doors, windows) and objects Oj = {(Ai, Xi)}. Each object pairs simulation-ready asset Ai, comprising visual geometry, collision geometry, 3 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 2. SceneSmiths hierarchical scene construction pipeline. scene prompt is processed by layout agent to generate architectural geometry for rooms. Each room is then independently populated through furniture, wall-mounted, and ceiling-mounted stages using room-specific prompts Tj. In each room, Kj supporting entities subsequently form additional branches populated with manipulable objects using entity-specific prompts Tj,k. Colored highlights indicate objects added at each stage. Each stage (colored triangle) is implemented as an agentic interaction between Designer, Critic, and Orchestrator. Stacked frames indicate parallel branches. physical properties, and joint definitions if it is articulated, with pose Xi SE(3). SceneSmith constructs scenes through tree of stages (Figure 2). The root stage generates architectural layout, determining the number of rooms and producing geometry Gj for each. This geometry specifies room extents and structural elements. Each room Rj is then populated independently by adding objects Oj through successive stages: furniture, wall-mounted objects, and ceiling fixtures. Each stage is guided by room-specific prompt Tj derived from . Finally, selected supporting entities (furniture surfaces, wall shelves, floor regions) spawn additional branches for adding small manipulable objects to Oj, each guided by an entity-specific prompt Tj,k. All stages are implemented as agentic interactions (Section 3.2). This hierarchical prompt refinement enables local decisions to be made independently while remaining coherent with scene intent. Finally, all room and manipuland branches are assembled into the flat scene representation S, suitable for direct export to robot simulators. 3.2. Agentic Trio: Designer, Critic, and Orchestrator Each stage of SceneSmiths hierarchical construction process is implemented as an interaction between three agents with complementary roles: designer, critic, and an orchestrator. This decomposition separates scene proposal, evaluation, and control flow, enabling structured refinement while keeping individual agent responsibilities simple. This separation reduces self-evaluation bias: an independent critic is better positioned to identify errors or omissions that proposal-focused designer may overlook. The designer proposes modifications to the scene state at the current stage using structured tools. The critic evaluates the resulting scene with respect to factors such as semantic plausibility, physical feasibility, and alignment with the stage objective, providing scalar scores and naturallanguage feedback. The orchestrator coordinates this interaction, tracking scores and determining when to accept proposal, request further refinement, or terminate the stage. To prevent degradation during iterative refinement, the orchestrator maintains checkpoints of prior scene states and can revert changes when critic scores decrease. Agent Tools. Agents interact with the scene exclusively through tools that provide structured observation and modification operations. We organize tools into functional categories that are shared across stages, including state observation tools (e.g., querying object metadata and poses), visual observation tools (e.g., rendering scene views), scene modification tools (e.g., placing or adjusting assets), asset acquisition tools for generating or retrieving simulation assets, and feasibility verification tools (e.g., collision and reachability checks). Object placement is performed relative to supporting surfaces. Agents specify SE(2) poses within surface coordinate frame (e.g., on floors, walls, or shelves). Full SE(3) object poses arise by lifting these placements through the known pose of the supporting surface. In addition, certain stages expose specialized tools tailored to their construction context. For example, furniture placement stages include snapping tools that translate objects into contact-aligned relative configurations (e.g., chairs snapped toward tables or cabinets snapped against walls), as well as relational facing queries that evaluate whether an object is oriented toward or away from another object or architectural element. Manipuland population stages additionally include tools for assembling multiple assets into composite object groups that are placed jointly (e.g., constructing fruit bowl by placing individual fruit relative to 4 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes bowl and then placing the assembled group on supporting surface). Agents may invoke arbitrarily many tools within single turn, making complex edits via multiple atomic operations. complete description of available tools is provided in Appendix A. Tool access is role-dependent: the designer has access to scene modification tools, the critic is restricted to observation and verification tools, and the orchestrator invokes designer and critic agents as tools while managing global control operations such as state rollback. Agent Memory. SceneSmith maintains persistent, agentspecific session memory within each construction stage, allowing agents to retain context across turns during iterative refinement. To bound context growth, each agent uses turn-based memory in which the current and immediately preceding turn are retained in full, while earlier agent turns are replaced by LLM-generated summaries. Visual observations are stored in bounded sliding window and discarded once no longer needed. Memory is reset between stages. 3.3. Asset Generation and Routing SceneSmith integrates asset generation directly into the scene construction process, enabling open-set object vocabularies while ensuring simulation readiness. Given an asset request from designer agent, an asset router resolves the request into one or more atomic assets and selects an appropriate acquisition strategy, returning validated simulationready objects. Generative Asset Synthesis. For static objects, SceneSmith primarily relies on generative text-to-3D synthesis rather than retrieval from existing asset libraries. Generating assets on demand also avoids training data contamination, enabling fair evaluation of robot policies on truly unseen objects. Given textual object description, we generate reference image using text-to-image model (GPT Image 1.5), segment the foreground object using SAM3 (Carion et al., 2025), and reconstruct textured 3D mesh from the segmented image using SAM3D (Chen et al., 2025). The resulting mesh is canonicalized to standard orientation, scaled to target dimensions specified in the asset request, and augmented with collision geometry and estimated physical properties (Figure 3). Articulated Object Library. For objects with movable parts, such as cabinets, drawers, or appliances, SceneSmith retrieves assets from ArtVIP (Jin et al., 2025), an articulated object library containing pre-authored multi-link models with joint definitions. We augment these with estimated physical properties (Appendix B.5). While generative methods are effective for static objects, in our experience, current text-to-3D approaches do not yet reliably produce articulated structure and kinematics suitable for robotics simulation (Chen et al., 2024; Liu et al., 2024). Thin Coverings. To represent flat decorative elements such as rugs, posters, or tablecloths, we introduce thin coverings: lightweight geometric surfaces paired with physically based materials. Thin coverings capture visual detail and clutter while avoiding unnecessary rigid-body complexity. We retrieve materials from ambientCG1 and fall back to imagegenerated materials when library assets cannot satisfy the request (Appendix B.4). Asset Routing and Validation. The asset router decomposes composite requests into individually manipulable assets when needed (e.g., representing fruit bowl as bowl plus multiple fruit objects) and selects among generation, articulated retrieval, or thin covering strategies based on object type and placement context. All candidate assets undergo validation, including mesh integrity checks and VLM-based semantic verification. Assets that fail validation are regenerated or rerouted up to fixed retry budget, after which failure feedback is returned to the agent. This routing and validation loop enables robust, scalable asset acquisition without manual curation (Appendices B.1, B.7). 3.4. Simulation Readiness Architectural elements use volumetric geometry (e.g., walls with finite thickness) rather than planes, improving robustness to penetration under discrete time-stepping physics simulation. All objects are augmented with estimated physical properties (mass, center of mass, inertia, friction) during asset generation, enabling realistic dynamics (Appendix B.5). While SceneSmiths agentic construction process encourages semantically plausible and physically reasonable placements through iterative feedback, agents are not required to satisfy physical constraints exactly during generation. As result, generated scenes may contain inter-object penetrations or objects placed in configurations that are not statically stable. To ensure that environments are directly simulatable, we apply lightweight post-processing step after both furniture and manipuland placement stages that enforces physical feasibility. We first resolve inter-object penetrations by projecting object positions to the nearest collision-free configuration using nonlinear optimization, while preserving orientations, as in (Pfaff et al., 2025b). We then simulate the scene under gravity in Drake (Tedrake et al., 2019) to allow unstable objects to settle into statically stable configurations. This minimizes penetrations and ensures static equilibrium without requiring manual intervention or object welding. 3.5. Application: Robot Policy Evaluation As one application of SceneSmith, we present an end-toend evaluation pipeline that connects natural-language robot 1https://ambientcg.com/ 5 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 3. Text-to-3D asset generation pipeline. Given an object description, we generate an image, segment the foreground, and reconstruct textured 3D mesh. The mesh is augmented with collision geometry (gray convex pieces) and physical properties estimated by VLM, including mass, center of mass, friction, and inertia (blue ellipsoid). The mesh is also scaled to target dimensions. tasks to generated environments, robot execution, and automated task verification. This enables scalable evaluation of robot policies in diverse, simulation-ready scenes without manual environment or evaluation predicate design. Given natural-language task description (e.g., find fruit and place it on the table), we first use language model to generate set of diverse scene prompts consistent with the task requirements. These prompts are passed to SceneSmith to produce multiple task-relevant indoor environments, allowing evaluation across varied layouts, object configurations, and clutter conditions. Robots then execute policies directly in the generated environments. As an example instantiation, we demonstrate this pipeline using model-based pick-and-place policy that operates on the generated simulator scenes. While we use Drake for demonstration, our scenes can be exported to other major robotics simulators (e.g., MuJoCo (Todorov et al., 2012), Isaac Sim, Genesis; see Appendix J). Finally, task completion is verified by an evaluator agent that jointly reasons over symbolic scene state and visual observations rendered from the simulator. The evaluator does not rely on fixed success predicates; instead, it uses structured tools to gather evidence for task completion, including querying object poses and rendering selected objects for visual inspection. This avoids hand-crafted success metrics and supports open-ended tasks, though at the cost of determinism. See Appendix for details. 4. Evaluation 4.1. Experimental Setup Baselines. We compare against five external baselines: HSM (Pun et al., 2026), hierarchical framework using learned scene motifs; Holodeck (Yang et al., 2024b), which uses constraint satisfaction for layout optimization; I-Design (C elen et al., 2025), multi-role LLM pipeline; LayoutVLM (Sun et al., 2024), combining visual prompting with differentiable optimization; and SceneWeaver (Yang et al., 2025), single-agent framework using iterative refinement. We evaluate LayoutVLM with both its original curated asset library and the larger Objaverse (Deitke et al., 2022a) library used by Holodeck for fair comparison. Ablations. We evaluate six ablations: NoCritic (initial design only), NotGenerated (HSSD (Khanna et al., 2023) assets instead of generated), NoAssetValidation (no asset validation), NoSpecializedTools (no specialized furniture or manipuland tools), NoObserveScene (no visual observations), and NoAgentMemory (no session memory). Input Text Descriptions. We evaluate on 210 prompts across five categories: SceneEval-100 (Tam et al., 2025) room prompts, Type Diversity prompts covering underrepresented room types like pet stores and yoga studios, Object Density prompts for high object count scenarios, Themed Scenes with stylistic constraints, and House-Level multiroom prompts. House-level prompts are limited to SceneSmith and Holodeck, the only multi-room baseline. See Appendix for the complete prompt corpus. Human Study. We conducted pairwise comparison study with 205 crowdsourced participants, collecting 3,051 valid responses. Each comparison presents two scenes side-byside with an interactive 3D viewer and asks two questions: (1) which scene looks more realistic (forced choice), and (2) which scene better follows the prompt requirements (with Equal option). See Appendix M.1 for details. Automatic Evaluation. We use SceneEval (Tam et al., 2025) with the following metrics: CNT (object count), ATR (object attributes), OOR (object-object relationships), OAR (object-architecture relationships), ACC (accessibility), NAV (navigability), and OOB (out-of-bounds). We note that these VLM-based metrics have limitations including false positives and negatives (Appendix L.2). We add two physics metrics using Drake (Tedrake et al., 2019) to evaluate simulation-readiness: COL (collision rate) and STB (static equilibrium). Since baseline methods do not produce simulation-ready scenes, we augment their outputs with collision geometry and physical properties to enable fair comparison (Appendix L.2). SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 4. Qualitative comparison with HSM and Holodeck, the two strongest baselines in our user study. SceneSmith produces denser scenes that better satisfy prompt requirements. See Appendix for additional examples. 4.2. Results Figure 4 shows qualitative comparisons with baselines. Table 1 presents human study results. SceneSmith achieves 92.2% average realism win rate and 91.5% average prompt faithfulness win rate against all room-level baselines (all < 0.001). The largest margins are against the LayoutVLM variants, while the smallest is against HSM (88.5% realism, 85.2% faithfulness). For house-level scenes, SceneSmith achieves 80.3% realism and 84.7% faithfulness win rates against Holodeck, the only multi-room baseline. Among ablations, the most impactful are NotGenerated (63.8% realism, 67.0% faithfulness), NoAssetValidation (63.0%, 62.2%), and NoObserveScene (61.5% realism), all showing significant effects. These results demonstrate that generative asset acquisition, asset validation, and visual feedback each contribute meaningfully. NoCritic, NoSpecializedTools, and NoAgentMemory show smaller effects (5155% realism) that did not reach significance with our study power; detecting these would require 6-18x more comparisons. Notably, NoCritic achieves similar preference scores while being 70% cheaper (Appendix N.2), though it produces 24% fewer objects. While higher object density enables richer manipulation scenarios for robotics, NoCritic offers cost-efficient alternative for applications where this trade-off is acceptable. Table 2 presents automated metrics. SceneSmith achieves the best performance on CNT, ATR, OOR, OAR, COL, and STB. Notably, we achieve 2.2x improvement on OOR over the best baseline. The lower ACC and NAV scores are expected given our 3-6x higher object density (71.1 vs 11-23 objects), which inherently reduces free space. The most striking difference is physics quality: SceneSmith achieves 1.2% collision rate versus 3-29% for baselines, and 95.6% stability versus 8-61%. The remaining collisions are slight penetrations (3.8mm mean depth) that are 3-12x shallower than baselines (Appendix N.1). This is critical differentiator for robotics as our scenes are simulation-ready without post-hoc correction. House-level results are similar: SceneSmith generates 2.6x more objects (214 vs 81) while maintaining 0.9% collisions and 79.8% stability versus Holodecks 3.8% and 17.9%. Beyond quantitative metrics, SceneSmith produces houses with realistic room connectivity. For example, generated hotel scene has the entrance leading through the reception, en suite bathrooms accessible only through their associated bedrooms, and all rooms connected via central hallway. In contrast, Holodeck often generates implausible layouts where an entire hotel might only be reachable through guest room, or second bedroom is accessible only through the first bedrooms bathroom (Figure 23). See Appendices and for additional quantitative and Appendix for additional qualitative results. Finally, we demonstrate the agentic robot policy evaluation pipeline from Section 3.5. We evaluate across 100 generated scenes spanning four pick-and-place tasks (three room-level, 7 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Table 1. User study results (179 room-level, 31 house-level prompts): SceneSmith vs baselines and ablations. #Obj shows the comparison methods average object count with 95% CI. Win rates show percentage preferred (excluding ties for faithfulness), with 95% Wilson score CIs. Cohens measures effect size. Significance: ** < 0.001, * < 0.01, 0.05 (FDR-corrected). Comparison #Obj Win% 95% CI Sig. Win% 95% CI Sig. Realism Faithfulness External Baselines (SceneSmith averages 71.1 13.0 objects) vs HSM vs Holodeck vs SceneWeaver vs I-Design vs LayoutVLM (Curated) vs LayoutVLM (Objaverse) 22.7 2.6 23.0 1.4 13.5 1.0 13.0 1.0 11.2 1.6 14.2 1.7 [83.5, 92.1] 0.88 [83.8, 92.2] 0.88 [87.3, 94.7] 0.99 [90.1, 96.5] 1.08 [91.1, 97.1] 1.12 [91.7, 97.5] 1. 88.5 88.6 91.7 94.1 94.9 95.4 Ablations vs NotGenerated vs NoAssetValidation vs NoObserveScene vs NoSpecializedTools vs NoAgentMemory vs NoCritic 57.7 9.9 72.7 16.3 69.7 15.4 61.5 14.1 78.9 19.7 54.0 8.3 63.8 63.0 61.5 54.8 53.4 51.8 [57.2, 69.9] 0.28 [56.3, 69.1] 0.26 [54.9, 67.7] 0.23 [48.2, 61.2] 0.10 [46.8, 59.9] 0.07 [45.2, 58.4] 0.04 House-Level (SceneSmith averages 214.1 60.9 objects) vs Holodeck 81.3 18.3 80.3 [63.5, 91.0] 0.65 ** ** ** ** ** ** ** ** * 85.2 90.6 92.9 90.6 95.8 93. 67.0 62.2 53.2 53.2 55.1 47.5 [79.7, 89.3] [85.9, 93.8] [88.6, 95.6] [85.9, 93.8] [92.3, 97.8] [89.9, 96.4] 0.78 0.95 1.03 0.95 1.16 1.07 0.35 [60.0, 73.3] 0.25 [54.8, 69.1] 0.06 [45.7, 60.5] 0.06 [45.7, 60.5] [47.5, 62.4] 0.10 [40.4, 54.8] 0.05 ** ** ** ** ** ** ** * ** 84.7 [67.5, 93.8] 0.76 ** Table 2. Automated evaluation (179 room-level, 31 house-level scenes). Values are mean 95% CI. = higher is better, = lower is better. Bold = best within section, underline = second best. See Appendix N.1 for full results with ablations. Method #Obj CNT ATR OOR OAR ACC NAV COL STB OOB HSM Holodeck SceneWeaver I-Design LayoutVLM (Cur.) LayoutVLM (Obj.) SceneSmith (Ours) 22.72.6 23.01.4 13.51.0 13.01.0 11.21.6 14.21.7 71.113.0 60.64.2 44.24.3 41.83.9 69.34.0 41.04.0 55.54.7 82.93.4 61.57.3 44.47.2 29.16.7 50.37.2 25.66.6 34.37.0 74.46.2 30.65.7 16.84.6 15.34.2 28.65.5 14.14.0 20.75.0 67.65.8 66.26.6 88.91.7 99.40.4 20.63.0 45.23.9 5.51.1 38.07.1 84.91.4 99.60.2 12.32.3 31.92.7 0.80.4 31.66.5 77.52.7 98.11.2 12.52.5 37.34.4 0.00.0 66.66.7 70.12.8 95.91.4 60.84.6 4.32.0 22.46.1 93.81.9 99.70.2 25.94.4 19.43.6 6.22.1 8.11.7 17.45.6 91.51.6 98.70.8 28.93.5 5.11.4 95.61.7 0.20.4 1.20.6 80.65.5 83.41.5 97.61.1 3.01. House-Level Holodeck SceneSmith (Ours) 214.160.9 83.36.8 66.824.6 81.318.3 64.66.8 43.227.6 30.110.1 58.46.3 79.43.0 99.60.2 82.65.8 79.92.0 96.52.1 76.69.2 3.83.1 0.90.5 17.95.6 0.30.2 79.89.1 0.60. one house-level). To validate that the pipeline can discriminate between policies of varying quality, we compare simple model-based policy against deliberately degraded variant with relaxed motion constraints. The standard policy achieves 16% success versus 12% for the degraded variant, demonstrating that the automatic evaluation system can detect meaningful differences in policy quality. We manually verified all 300 evaluator judgments (100 scenes 3 states: initial, standard policy, degraded policy) and found 99.7% agreement with human labels. The single disagreement was an ambiguous case where fruit landed on the edge of plate. See Appendix for details. 5. Conclusion We present SceneSmith, hierarchical agentic framework for generating simulation-ready indoor environments from natural language prompts. SceneSmith decomposes scene construction into hierarchical stages, each driven by 8 designer-critic-orchestrator interactions, producing dense, physically valid scenes that capture the complexity of real homes. Our integrated asset pipeline combines generative text-to-3D synthesis with retrieval for articulated objects, enabling open-vocabulary generation while ensuring simulation readiness. Experiments demonstrate dramatic improvements over baselines: SceneSmith generates 3-6x more objects while achieving <2% inter-object collisions and 95.6% static stability, compared to 3-29% collisions and 8-61% stability for baselines. Human evaluators preferred SceneSmith with 92% average win rate for realism and 91% average for prompt faithfulness. We believe these results mark point where environment generation is no longer the primary bottleneck for scalable robot training and evaluation in simulation. We hope SceneSmith proves useful for robotics and beyond. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially supported by Amazon.com (PO No. 2D-19158853), the Office of Naval Research (ONR) under Grant No. N00014-22-1-2121, the Toyota Research Institute (TRI), and the National Science Foundation Graduate Research Fellowship Program under Grant No. 2141064. This article reflects only the views of the authors and not those of TRI or any other Toyota entity. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or other funding organizations. We thank Hongkai Dai, Jeremy Binagia, Peter Werner, and Tom Erez for helpful discussions. We also thank Patrick Farrell for assistance with the videos."
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance robot development through improved simulation environments, enabling safer evaluation before real-world deployment. We do not foresee direct negative societal impacts from this research."
        },
        {
            "title": "References",
            "content": "Barreiros, J., Beaulieu, A., Bhat, A., Cory, R., Cousineau, E., Dai, H., Fang, C.-H., Hashimoto, K., Irshad, M. Z., Itkina, M., Kuppuswamy, N., Lee, K.-H., Liu, K., McConachie, D., McMahon, I., Nishimura, H., PhillipsGrafflin, C., Richter, C., Shah, P., Srinivasan, K., Wulfe, B., Xu, C., Zhang, M., Alspach, A., Angeles, M., Arora, K., Guizilini, V. C., Castro, A., Chen, D., Chu, T.-S., Creasey, S., Curtis, S., Denitto, R., Dixon, E., Dusel, E., Ferreira, M., Goncalves, A., Gould, G., Guoy, D., Gupta, S., Han, X., Hatch, K., Hathaway, B., Henry, A., Hochsztein, H., Horgan, P., Iwase, S., Jackson, D., Karamcheti, S., Keh, S., Masterjohn, J., Mercat, J., Miller, P., Mitiguy, P., Nguyen, T., Nimmer, J., Noguchi, Y., Ong, R., Onol, A., Pfannenstiehl, O., Poyner, R., Rocha, L. P. M., Richardson, G., Rodriguez, C., Seale, D., Sherman, M., Smith-Jones, M., Tago, D., Tokmakov, P., Tran, M., Hoorick, B. V., Vasiljevic, I., Zakharov, S., Zolotas, M., Ambrus, R., Fetzer-Borelli, K., Burchfiel, B., Kress-Gazit, H., Feng, S., Ford, S., and Tedrake, R. careful examination of large behavior models for multitask dexterous manipulation, 2025. URL https: //arxiv.org/abs/2507.05331. Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Galliker, M. Y., Ghosh, D., Groom, L., Hausman, K., Ichter, B., Jakubczak, S., Jones, T., Ke, L., LeBlanc, D., Levine, S., Li-Bell, A., Mothukuri, M., Nair, S., Pertsch, K., Ren, A. Z., Shi, L. X., Smith, L., Springenberg, J. T., Stachowicz, K., Tanner, J., Vuong, Q., Walke, H., Walling, π0.5: A., Wang, H., Yu, L., and Zhilinsky, U. vision-language-action model with open-world generalization, 2025. URL https://arxiv.org/abs/ 2504.16054. Carion, N., Gustafson, L., Hu, Y.-T., Debnath, S., Hu, R., Suris, D., Ryali, C., Alwala, K. V., Khedr, H., Huang, A., Lei, J., Ma, T., Guo, B., Kalla, A., Marks, M., Greer, J., Wang, M., Sun, P., Radle, R., Afouras, T., Mavroudi, E., Xu, K., Wu, T.-H., Zhou, Y., Momeni, L., Hazra, R., Ding, S., Vaze, S., Porcher, F., Li, F., Li, S., Kamath, A., Cheng, H. K., Dollar, P., Ravi, N., Saenko, K., Zhang, P., and Feichtenhofer, C. Sam 3: Segment anything with concepts, 2025. URL https://arxiv.org/abs/ 2511.16719. Chen, X., Chu, F.-J., Gleize, P., Liang, K. J., Sax, A., Tang, H., Wang, W., Guo, M., Hardin, T., Li, X., Lin, A., Liu, J., Ma, Z., Sagar, A., Song, B., Wang, X., Yang, J., Zhang, B., Dollar, P., Gkioxari, G., Feiszli, M., and Malik, J. Sam 3d: 3dfy anything in images, 2025. URL https: //arxiv.org/abs/2511.16624. Chen, Z., Walsman, A., Memmel, M., Mo, K., Fang, A., Vemuri, K., Wu, A., Fox, D., and Gupta, A. Urdformer: pipeline for constructing articulated simulation environments from real-world images, 2024. URL https://arxiv.org/abs/2405.11656. Coumans, E. and Bai, Y. Pybullet, python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 20162021. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and Farhadi, A. Objaverse: universe of annotated 3d objects, 2022a. URL https://arxiv.org/abs/ 2212.08051. Deitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Ehsani, K., Salvador, J., Han, W., Kolve, E., Kembhavi, A., and Mottaghi, R. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:59825994, 2022b. Gill, P. E., Murray, W., and Saunders, M. A. Snopt: An sqp algorithm for large-scale constrained optimization. SIAM Journal on Optimization, 12(4):9791006, 2002. doi: 10.1137/S1052623499350013. URL https:// doi.org/10.1137/S1052623499350013. Hu, S., Arroyo, D. M., Debats, S., Manhardt, F., Carlone, L., and Tombari, F. Mixed diffusion for 3d indoor scene synthesis. arXiv preprint arXiv:2405.21066, 2024. Jia, Y. and Chen, B. Cluttergen: cluttered scene generator for robot learning. In 8th Annual Conference on Robot Learning, 2024. 9 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Jin, Z., Che, Z., Zhao, Z., Wu, K., Zhang, Y., Zhao, Y., Liu, Z., Zhang, Q., Ju, X., Tian, J., Xue, Y., and Tang, J. Artvip: Articulated digital assets of visual realism, modular interaction, and physical fidelity for robot learning, 2025. URL https://arxiv.org/abs/2506. 04941. Khaled Mamou, E. L. and Peters, A. Volumetric hierarchical approximate convex decomposition. In Game Engine Gems 3, pp. 141158. AK Peters, 2016. doi: https://doi. org/10.1201/b21177. Khanna, M., Mao, Y., Jiang, H., Haresh, S., Shacklett, B., Batra, D., Clegg, A., Undersander, E., Chang, A. X., and Savva, M. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation, 2023. URL https://arxiv. org/abs/2306.11290. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., Vuong, Q., Kollar, T., Burchfiel, B., Tedrake, R., Sadigh, D., Levine, S., Liang, P., and Finn, C. Openvla: An open-source vision-language-action model, 2024. URL https://arxiv.org/abs/2406.09246. Kuffner, J. J. and LaValle, S. M. Rrt-connect: An efficient approach to single-query path planning. In Proceedings 2000 ICRA. Millennium conference. IEEE international conference on robotics and automation. Symposia proceedings (Cat. No. 00CH37065), volume 2, pp. 9951001. IEEE, 2000. Liu, J., Iliash, D., Chang, A. X., Savva, M., and MahdaviAmiri, A. SINGAPO: Single image controlled generation of articulated parts in object. arXiv preprint arXiv:2410.16499, 2024. Lu, S., Chen, G., Dinh, N. A., Lang, I., Holtzman, A., and Hanocka, R. Ll3m: Large language 3d modelers, 2025. URL https://arxiv.org/abs/2508.08228. Maddukuri, A., Jiang, Z., Chen, L. Y., Nasiriany, S., Xie, Y., Fang, Y., Huang, W., Wang, Z., Xu, Z., Chernyadev, N., Reed, S., Goldberg, K., Mandlekar, A., Fan, L., and Zhu, Y. Sim-and-real co-training: simple recipe for In Proceedings of vision-based robotic manipulation. Robotics: Science and Systems (RSS), Los Angeles, CA, USA, 2025. Pfaff, N., Fu, E., Binagia, J., Isola, P., and Tedrake, R. Scalable real2sim: Physics-aware asset generation via robotic pick-and-place setups. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 62966303, 2025a. doi: 10.1109/IROS60139.2025. 11246653. Pfaff, N. E., Dai, H., Zakharov, S., Iwase, S., and Tedrake, R. Steerable scene generation with post training and inference-time search. In Lim, J., Song, S., and Park, H.-W. (eds.), Proceedings of The 8th Conference on Robot Learning, volume 305 of Proceedings of Machine Learning Research, pp. 16901702. PMLR, 27 30 Sep 2025b. URL https://proceedings.mlr. press/v305/pfaff25a.html. Pun, H. I. D., Tam, H. I. I., Wang, A. T., Huo, X., Chang, A. X., and Savva, M. HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation. In Proceedings of the IEEE Conference on 3D Vision (3DV), 2026. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Raistrick, A., Mei, L., Kayan, K., Yan, D., Zuo, Y., Han, B., Wen, H., Parakh, M., Alexandropoulos, S., Lipson, L., et al. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2178321794, 2024. Sekhavat, S., Svestka, P., Laumond, J.-P., and Overmars, M. H. Multilevel path planning for nonholonomic robots using semiholonomic subsystems. The international journal of robotics research, 17(8):840857, 1998. Sun, F.-Y., Liu, W., Gu, S., Lim, D., Bhat, G., Tombari, F., Li, M., Haber, N., and Wu, J. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. arXiv preprint arXiv:2412.02193, 2024. Tam, H. I. I., Pun, H. I. D., Wang, A. T., Chang, A. X., and Savva, M. SceneEval: Evaluating semantic coherence in text-conditioned 3D indoor scene synthesis. 2025. Tang, J., Nie, Y., Markhasin, L., Dai, A., Thies, J., and Nießner, M. Diffuscene: Denoising diffusion models for gerative indoor scene synthesis. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, 2024. Tedrake, R. Robotic Manipulation. 2024. URL http: //manipulation.mit.edu. Tedrake, R. and the Drake Development Team. Drake: Model-based design and verification for robotics, 2019. URL https://drake.mit.edu. Todorov, E., Erez, T., and Tassa, Y. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 50265033, 2012. doi: 10.1109/IROS.2012.6386109. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes elen, A., Han, G., Schindler, K., Van Gool, L., Armeni, I-Design: PersonalI., Obukhov, A., and Wang, X. ized LLM Interior Designer, pp. 217234. Springer Nature Switzerland, 2025. ISBN 9783031923876. doi: 10.1007/978-3-031-92387-6 17. URL http://dx. doi.org/10.1007/978-3-031-92387-6_17. Verscheure, D., Demeulenaere, B., Swevers, J., De Schutter, J., and Diehl, M. Time-optimal path tracking for robots: convex optimization approach. IEEE Transactions on Automatic Control, 54(10):23182327, 2009. Wei, A., Agarwal, A., Chen, B., Bosworth, R., Pfaff, N., and Tedrake, R. Empirical analysis of sim-and-real cotraining of diffusion policies for planar pushing from pixels. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 56255632, 2025. doi: 10.1109/IROS60139.2025.11246304. Wei, X., Liu, M., Ling, Z., and Su, H. Approximate convex decomposition for 3d meshes with collision-aware concavity and tree search. ACM Transactions on Graphics (TOG), 41(4):118, 2022. Xiang, F., Qin, Y., Mo, K., Xia, Y., Zhu, H., Liu, F., Liu, M., Jiang, H., Yuan, Y., Wang, H., Yi, L., Chang, A. X., Guibas, L. J., and Su, H. SAPIEN: simulated partbased interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023. URL https://arxiv. org/abs/2310.11441. Yang, Y., Jia, B., Zhi, P., and Huang, S. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2024a. Yang, Y., Sun, F.-Y., Weihs, L., VanderBilt, E., Herrasti, A., Han, W., Wu, J., Haber, N., Krishna, R., Liu, L., et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1622716237, 2024b. Yang, Y., Jia, B., Zhang, S., and Huang, S. Sceneweaver: All-in-one 3d scene synthesis with an extensible and selfreflective agent. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Zeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan, F. R., Bauza, M., Ma, D., Taylor, O., Liu, M., Romo, E., et al. Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. The International Journal of Robotics Research, 41(7):690705, 2022. Zhou, M., Wang, X., Wang, Y., and Zhang, Z. Roomcraft: Controllable and complete 3d indoor scene generation, 2025. URL https://arxiv.org/abs/2506. 22291. 11 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes"
        },
        {
            "title": "Contents",
            "content": "A. System Architecture B. Asset Acquisition C. Layout Generation D. Furniture Placement E. Wall Object Placement F. Ceiling Object Placement G. Manipuland Placement H. Physical Feasibility Post-Processing I. Stochastic Placement J. Export and Simulator Compatibility K. Task-Driven Robot Evaluation L. Evaluation Methodology M. Human Evaluation N. Automated Evaluation O. Evaluation Prompts P. User Study Interface Screenshots Q. Additional Qualitative Results A. System Architecture 12 14 17 21 23 26 27 33 34 34 34 37 39 40 43 50 55 SceneSmith employs hierarchical multi-agent architecture to construct scenes = {Rj} through five sequential stages: layout generation, furniture placement, wall-mounted object placement, ceiling-mounted object placement, and manipuland placement. Each stage follows unified Designer-Critic-Orchestrator pattern. A.1. Agent Roles Designer Agent. Executes scene modifications through tool calls when invoked by the orchestrator. The designer can observe the scene (via visual renders or structured state queries) and uses placement, modification, and validation tools. Each stage provides specific tool set; see Tables 48 for complete tool descriptions. Critic Agent. Evaluates scene quality across multiple dimensions, providing categorical scores (010) with written justifications. The critic identifies specific issues and suggests improvements without directly modifying the scene. Orchestrator. Manages the critique-and-improve loop, coordinating between designer and critic using the tools in Table 3. It handles checkpoint management, enabling rollback when quality degrades, and determines when to terminate based on score thresholds or iteration limits. Figure 5 illustrates this iterative refinement process during furniture placement, showing how scenes evolve through multiple critique-and-improve cycles. Table 3. Orchestrator tools (shared across all stages). Tool Description request initial design request design change request critique reset scene to checkpoint Revert to previous checkpoint select placement style Invoke designer for initial scene Invoke designer for refinement Invoke critic for evaluation Set natural/perfect placement mode 12 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 5. Designer-critic iteration during furniture placement. Each column shows different room evolving through two rounds of critic feedback and designer refinement. Top row: Initial designs after the first designer pass, with scene prompts shown above. Middle row: Scenes after the first critique-and-improve cycle. Bottom row: Scenes after the second cycle. Text annotations describe the changes made at each step. The bedroom (left) progressively improves bunk bed placement. The dining room (center) refines chair orientation and adds furnishings. The pharmacy (right) illustrates checkpoint rollback: when the designers additions degrade the critic score, the orchestrator resets to the previous checkpoint and prompts different approach. 13 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes A.2. Model Configuration All agents and VLM calls use GPT-5.2. Designers and critics use high reasoning effort for thorough analysis, while planners use low reasoning for efficient coordination. Auxiliary VLM tasks (asset routing, physics estimation, validation) use medium reasoning. A.3. Scene Observation Both designer and critic agents observe scenes through two complementary channels: structured state queries and visual renders. Structured State. State query tools return spatial metadata for precise reasoning without image interpretation. Object placement stages query the object set Oj via get current scene state, returning object IDs, surface-local poses, surface assignments, bounding boxes, dimensions, and object descriptions. The layout stage uses render ascii to query room layouts and wall segments within Gj. Visual Renders. The observe scene tool produces annotated multi-view renders tailored to each construction stage. Figures 713 in subsequent sections show representative outputs with stage-specific annotations. A.4. Checkpoint and Rollback After each critique, the system saves checkpoint containing complete scene state and scores, maintaining both current and previous checkpoints. If scores decrease beyond thresholds, the orchestrator rolls back to the previous checkpoint and instructs the designer to try an alternative approach (Figure 5, right column). Scene state is restored but agent context is preserved, allowing the designer to learn from the failed attempt and pursue different strategy. A.5. Parallel Scene Generation Scene generation parallelizes across multiple GPUs to achieve practical throughput for large-scale dataset creation. GPU worker pool spawns one text-to-3D worker per available GPU. The key design enabling throughput is batch request pipelining: scene workers submit multiple asset generation requests simultaneously rather than sequentially. The geometry server processes these requests across available GPUs and streams results back, allowing the scene worker to perform mesh post-processing (collision geometry, physics estimation) on completed assets while the GPUs process remaining requests. This overlap prevents text-to-3D from becoming bottleneck. Multiple scenes generate in parallel. In our experiments, we generate 25 scenes concurrently across 8 NVIDIA L40S GPUs; per-scene generation time remains nearly constant as parallel scene count increases, since the shared geometry server efficiently distributes requests across all available GPUs. B. Asset Acquisition Before placement, simulation-ready assets Ai are acquired through generation or retrieval, then processed to extract collision geometry and physical properties. B.1. Asset Router An intelligent router uses VLM to analyze each asset request and performs three tasks: Stage Filtering. The router rejects items that belong to different construction stage. For example, coffee mug requested during furniture placement is flagged as manipuland and rejected with feedback to the designer. Composite Decomposition. Composite requests are split into individually manipulable assets. fruit bowl becomes bowl plus individual fruits; desk with monitor separates into furniture (desk) and manipuland (monitor) items for their respective stages. This decomposition is essential for robotic manipulation: fruit bowl generated as single fused mesh would be neither realistic nor interactable. Strategy Selection. For each valid item, the router selects an acquisition strategy: Generated: Text-to-3D generation for standard furniture and objects 14 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Articulated: Retrieval from curated datasets for objects with moving parts (doors, drawers) Thin covering: Procedural generation for flat textured surfaces (floor rugs, tablecloths, posters) B.2. Text-to-3D Generation Standard assets are generated using text-to-image-to-3D pipeline: 1. Generate reference image from text description using GPT Image 1.5 (10241024, solid opaque background for clean segmentation) 2. Segment foreground object using SAM3 (Carion et al., 2025) 3. Reconstruct textured 3D surface mesh from single image using SAM3D (Chen et al., 2025) B.3. Articulated Object Retrieval For objects with movable parts (cabinets, drawers, appliances), we retrieve from the ArtVIP library (Jin et al., 2025) containing pre-authored multi-link models with joint definitions. We evaluated PartNet-Mobility (Xiang et al., 2020) but found its visual and joint quality insufficient for realistic scene generation. Embedding Pre-computation. To enable semantic similarity search, we pre-compute visual embeddings for each object in the library. Each object is rendered from 8 views (4 upper + 4 lower at 30 elevation) with joints at zero positions. We compute CLIP (Radford et al., 2021) embeddings using ViT-H-14 and average across views, producing 1024-dimensional vectors stored for retrieval. Two-Stage Retrieval. Following (Pun et al., 2026), we use two-stage retrieval process. Given text query and optional target dimensions: 1. Filter by object type (e.g., floor-standing furniture, wall-mounted, ceiling-mounted, or manipuland) 2. Rank candidates by CLIP similarity to the text query, selecting top-k 3. If target dimensions specified, re-rank by L1 distance to desired bounding box B.4. Thin Covering Generation Thin coverings represent flat textured surfaces such as floor rugs, tablecloths, and wall posters. Floor and manipuland thin coverings are purely decorative elements without collision geometry. The system supports both rectangular and circular shapes, automatically inferred from the object description (e.g., round rug circular). Texture Modes. Two texture modes address different use cases: Tileable: Repeating patterns (rugs, fabrics) where the texture tiles across the surface Single image: Artwork spanning the full surface (posters, paintings) with no repetition The routers VLM selects the appropriate mode based on semantic understanding of the description (e.g., persian rug tileable, poster depicting detailed dog washing instructions single image). Texture Retrieval. The system first attempts to retrieve matching PBR texture from material library (ambientCG2) using CLIP (Radford et al., 2021) similarity search. Pre-computed embeddings (ViT-H-14, 1024-dimensional) are generated from material preview images. AI-Generated Textures. When retrieval fails, the system falls back to AI image generation (GPT Image 1.5). Tileable textures are prompted for seamless repeating patterns, while artwork is prompted for full edge coverage without background. Generated images are supplemented with flat normal maps and uniform roughness to form complete PBR materials. For tileable textures, an edge-blending algorithm ensures seamless repetition by linearly blending opposing edges (left-right and top-bottom) over transition zone. B.5. Physical Property Estimation vision-language model (VLM) analyzes multi-view renders to estimate physical properties. 2https://ambientcg.com/ 15 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes B.5.1. RIGID BODY ESTIMATION The VLM receives 6 views (top and bottom + 4 side views at 30 elevation) and outputs: Dominant material category (from 19 predefined types), used to determine friction coefficients Mass estimate with confidence range [mmin, mmax]. The confidence range enables domain randomization over plausible masses Canonical orientation: The VLM identifies the up axis and selects which numbered image shows the functional front face (e.g., the side users face for furniture, the decorative face for wall art). This image-selection approach is more robust than directly predicting axis labels. The moment of inertia S3 (the set of 3 3 symmetric matrices) is computed assuming uniform density: ρ = Vmesh , = ρ unit (1) where Vmesh is the mesh volume and unit is the unit-density inertia tensor from mesh geometry. We found SAM3D meshes to be of sufficient quality that mesh repair operations are not required for these computations. Based on the VLM-predicted axes, meshes are transformed to canonical Z-up, Y-forward orientation with object-typespecific positioning: floor and manipuland objects have their bottom at = 0, ceiling objects have their top at = 0, and wall objects have their back at = 0. This canonicalization enables SE(2) placement on support surfaces. B.5.2. ARTICULATED BODY ESTIMATION For objects with multiple parts (e.g., cabinets with doors), properties are estimated per-link. The VLM receives combined views of the complete object, isolated views for each articulated link, and overall and per-link bounding box dimensions. It first estimates the total object mass, then estimates per-link materials (for friction coefficients) and masses constrained to sum to this total. Estimating the total first grounds the part estimates in quantity that is easier to reason about. Per-link moments of inertia are computed independently using each links mesh geometry and assigned mass. B.6. Collision Geometry Visual meshes cannot be used directly for simulation of interactable objects, as most physics engines (Tedrake et al., 2019) (Todorov et al., 2012) require convex geometry for efficient contact resolution. We decompose visual meshes into convex pieces using V-HACD (Khaled Mamou & Peters, 2016). We limit the number of convex pieces per object: 128 for furniture (larger objects requiring more detail), 64 for wall objects and manipulands, and 16 for ceiling objects (rarely involved in collisions). We also evaluated CoACD (Wei et al., 2022), which produces fewer convex pieces due to its tree search and thus can result in faster simulations. However, CoACD tends to inflate collision geometry, causing visual meshes to appear floating when objects are stacked. V-HACD does not exhibit this inflation effect. Additionally, we found CoACD to be one to two orders of magnitude slower to compute than V-HACD, which can become bottleneck when generating scenes with hundreds or thousands of objects. B.7. Asset Validation Generated and retrieved assets undergo VLM-based validation to verify suitability for the scene. The VLM checks: Object type: Mesh matches request (e.g., rejecting stool when chair was requested) Style consistency: Colors and materials match specifications (e.g., rejecting brown chair when red was requested) Single object: No multiple objects or furniture with manipulands attached (e.g., rejecting dining table generated with chairs) Completeness: No missing parts (e.g., rejecting chair missing leg) Reasonable proportions: Not severely distorted Closed state: Doors and drawers are closed, since generated assets cannot articulate and open doors would remain frozen in place The validator is lenient toward text and labels (which current 3D generation cannot reliably produce) and metallic or transparent materials (which require PBR properties beyond SAM3D). For retrieved assets such as articulated objects or the HSSD asset ablation, we use more lenient validator that focuses on object type and basic structure while ignoring SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes cosmetic details (color, style, finish) and minor structural variations (e.g., accepting 4-drawer dresser when 6 drawers were requested). Limited library sizes necessitate accepting close matches rather than exact specifications. Assets that fail validation are regenerated or re-retrieved up to configurable retry budget. If all retries fail, failure feedback is returned to the designer agent, which can request an alternative object or adjust the scene. Figure 6 shows examples of accepted and rejected assets with failure reasons. Figure 6. Asset validation examples. Each row shows text-to-3D request with rejected (left) and accepted (right) generation. Top: The validator rejects media console generated with TV attached, violating the single-object requirement. Bottom: The validator rejects bookshelf with closed cabinet doors, which does not match the open bookshelf specification. C. Layout Generation The layout agent receives the global scene prompt and generates architectural geometry {Gj}M j=1 for rooms, including walls, doors, windows, and material assignments. j=1 that propagate to downstream agents. The designer has access to all tools in Table 4 and can invoke them in any order, iterating freely based on its own judgment or critic feedback. This flexibility allows the designer to regenerate the entire layout, adjust individual rooms, or refine architectural details as needed. It also derives room-specific prompts {Tj}M C.1. Room Layout Creation When creating the room layout via generate room specs, the designer provides room types, dimensions, adjacency constraints, and room-specific prompt Tj for each room j. These prompts are derived from the global scene prompt by extracting constraints relevant to each room: Room-specific furniture, colors, and features (e.g., living room with red sofa) House-wide style applied to all rooms (modern, rustic, minimalist) Resolved ambiguous items: furniture mentioned without clear room assignment is allocated based on room types present (e.g., dining table kitchen when no dining room exists) The room prompts Tj propagate to all downstream agents (furniture, wall, ceiling, manipuland) to ensure placed objects remain consistent with the original scene description. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Table 4. Layout agent tools. Access: = Designer, = Critic. Category Access Description Tool observe scene render ascii Visual Visual generate room specs Modification resize room Modification add adjacency Modification remove adjacency Modification add open connection Modification remove open connection Modification set wall height Modification add door / remove door Modification add window / remove window Modification get material set room materials validate Asset Modification Feasibility D, D, D D D D, Top-down render with room labels ASCII floor plan with wall labels Create rooms with dimensions Adjust room dimensions Add room connection Remove room connection Create open floor plan Remove opening Set wall height Manage doors Manage windows Semantic material search Assign floor/wall materials Check placement validity C.2. Room Placement Algorithm Given room specifications with adjacency constraints, generate room specs invokes placement algorithm that finds valid spatial arrangements. The algorithm uses best-first backtracking search: 1. Sort rooms so adjacency constraints can be evaluated incrementally 2. Place the first room at the origin 3. For each subsequent room: generate candidate positions along edges of already-placed rooms, score candidates, and explore in best-first order 4. On completing layout, evaluate global score and track the best found 5. Backtrack to try alternative placements; return best layout found within timeout The algorithm is complete and optimal within the fixed room ordering and discretized position space. Different orderings may yield different layouts; exploring all orderings is intractable. As an anytime algorithm, it can be run with timeout and will return the best layout found so far. The following subsections detail each component. C.2.1. ROOM ORDERING Rooms are sorted so that when placing each room, at least one of its required neighbors (if any) is already placed, allowing adjacency constraints to guide placement. Rooms with no adjacency requirements (anchors) are placed first to establish the layout foundation. Rooms with requirements (connectors) are placed once at least one required neighbor exists. Within each category, rooms are sorted by area (largest first) to place more constrained rooms earlier. C.2.2. EDGE-BASED ATTACHMENT New rooms are placed by attaching them to edges of already-placed rooms. The algorithm samples 11 candidate positions along each edge and tests both original and 90 rotated orientations for non-square rooms. C.2.3. PLACEMENT SCORING The algorithm uses two levels of scoring. Local scores rank candidate positions to guide the best-first search, while global scores evaluate complete layouts. Local scoring. Each candidate is scored to prioritize placements that satisfy adjacency constraints while keeping the layout compact: Base score Sbase for all valid placements Adjacency bonus wadj per required neighbor that is already placed and adjacent Centroid penalty wdist per meter from the centroid of already-placed rooms SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Candidates violating required adjacencies to already-placed rooms are rejected. Global scoring. Complete layouts are evaluated on compactness and stability. Compactness measures space efficiency as the ratio of total room area to bounding box area: Stability rewards preserving room positions during iterative editing: Scompact ="
        },
        {
            "title": "Arooms\nAbbox",
            "content": "Sstable = (cid:18) exp pr pprev 2 (cid:19) (cid:88) (2) (3) where pr, pprev R2 are current and previous room centers. This enables incremental refinement. For example, when the designer narrows central hallway, the modified room must be re-placed and surrounding rooms must shift to maintain adjacency. The stability score ensures all rooms remain as close as possible to their original positions rather than triggering complete layout redesign. C.3. Architectural Elements After establishing room layout, the designer adds architectural elements: wall height, doors (interior connections between rooms and exterior access), windows (on exterior walls for natural lighting), open connections (removing the entire shared wall between adjacent rooms for open floor plans), and materials (floor and wall surfaces per room retrieved via semantic search). For doors and windows, the designer specifies the wall segment, discrete position (left, center, or right), and dimensions; windows additionally require sill height. The exact position is sampled uniformly within the specified segment, creating natural variation. The designer can adjust any element at any time, such as resizing room after adding doors, changing materials after critique feedback, or regenerating the entire layout if the current design is unsatisfactory. C.4. Layout Validation The validate tool checks two properties. First, it verifies that room placement completed successfully and rooms exist. Second, it validates connectivity: starting from rooms with exterior doors, breadth-first search traverses interior doors and open connections to find all reachable rooms. At least one exterior door must exist, and any room not reachable from the exterior is reported as an error. C.5. Critic Evaluation The critic evaluates layouts across five categories, each scored 010: Room Proportions (appropriate dimensions for room types), Spatial Flow (logical connections and traffic patterns), Natural Lighting (window placement and exterior wall exposure), Material Consistency (materials matching room purposes), and Prompt Following (adherence to the scene description). C.6. Observation Figure 7 shows the layout agents observation output, including rendered perspective view and ASCII layout with labeled wall segments for precise door and window placement. 19 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 7. Layout agent observation tools. Left: The observe scene tool renders top-down view with room labels and displays assigned materials with texture swatches. Right: The render ascii tool provides an ASCII floor plan with labeled wall segments (AJ) and structured metadata including room dimensions, connectivity validation, and door/window specifications. 20 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes D. Furniture Placement The furniture agent populates each room Rj with floor-standing furniture. It receives room scene containing the architectural geometry Gj from the layout agent and the room-specific prompt Tj describing the intended furnishings. Each room is processed independently with fresh agent instance. This decomposition enables parallel generation, simplifies the task for the agent by limiting scope to single room, and reduces inference costs by keeping context sizes smaller. Table 5 lists the available tools. Coordinate System. The designer specifies furniture poses as (x, y, θ) tuples in floor coordinates, where θ is the yaw rotation in degrees. The room coordinate system is centered at the floor origin, with [ℓ/2, ℓ/2] and [w/2, w/2] where ℓ and are the room length and width respectively. Since floor-standing furniture rests on the floor surface, the full SE(3) pose Xi is constructed by setting = 0 with roll and pitch fixed to zero. Table 5. Furniture agent tools. Access: = Designer, = Critic. Tool Category Access Description observe scene Visual get current scene state State generate assets list available assets add furniture move furniture remove furniture rescale furniture check facing snap to object check physics check reachability Asset State Modification Modification Modification Modification Specialized Specialized Feasibility Feasibility D, D, D, D D, D, D, Multi-view renders with annotations Object positions and bounding boxes Create 3D furniture from text List generated assets Place at floor position Reposition existing furniture Remove from scene Uniform scaling Verify orientation relationships Align and eliminate gaps Detect collisions Verify traversability The following subsections detail the algorithms behind the specialized and feasibility tools. D.1. check facing Algorithm Many furniture pieces have functional orientations: chairs should face tables, sofas should face entertainment centers, and storage furniture (wardrobes, dressers) should face away from walls so doors and drawers remain accessible. Visual assessment from rendered views can be unreliable due to camera angle ambiguities. The check facing tool provides precise orientation verification and returns the exact rotation needed for alignment. Given source object and target B, the algorithm determines if faces toward/away from B: 1. Extract yaw θA from object As transform 2. Compute forward direction: = Rz(θA) [0, 1, 0], where Rz denotes rotation about the vertical axis 3. Select target point on B: center for circular objects, closest axis-aligned bounding box (AABB) point to otherwise 4. Perform 2D ray-AABB intersection to check if As forward direction intersects 5. Compute optimal rotation: θ = atan2(y, x) Circular objects are detected via volume ratio: Vmesh/VAABB < 0.80, where Vmesh is the mesh volume and VAABB is the axis-aligned bounding box volume. D.2. snap to object Algorithm Placing furniture against walls or adjacent to other furniture typically requires multiple operations: approximate placement, orientation check, rotation correction, and position adjustment to eliminate gaps. The snap to object tool combines these into single operation. It also handles cases like pushing chair under desk, where the correct position is difficult to determine from vision alone: 2D bounding boxes appear to intersect in the top-down view even though the 3D geometries do not collide. The tool supports three orientation modes: toward rotates the source to face the target (e.g., chairs facing tables), away 21 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes rotates the source to face away from the target (e.g., wardrobes with doors facing into the room), and none performs gap elimination without orientation change. Given source object to be moved and stationary target, the algorithm proceeds in three phases: Phase 1: Collision Resolution. If objects overlap, push the source out along the axis with minimum overlap. The sources bounding box is treated as square using max(width, depth) to ensure subsequent rotation will not reintroduce collision. Phase 2: Orientation Alignment. For toward and away modes, use check facing to compute and apply the optimal yaw rotation. Skipped for none mode. Phase 3: Gap Elimination. Iteratively move the source toward the target in small steps, checking for collision at each step. Stop when collision is detected and revert to the last collision-free position, leaving small margin between objects. D.3. check reachability Algorithm For generated scenes to be useful in robotics applications, all room areas must remain traversable by the target robot. The robot footprint half-width hr is configurable parameter that users set based on their robot platform. This configurability is essential because mobile robots vary significantly in size: humanoid robot may need only 20cm of clearance, while larger mobile manipulators require substantially more; for example, the Rainbow Robotics RB-Y13 has 6069cm base, requiring approximately 35cm clearance. By specifying the appropriate footprint, users ensure that generated scenes are actually navigable by their specific robot. To verify traversability, the algorithm models the robot as disk Bhr of radius hr and computes the walkable area via Minkowski sum operations: 1. Shrink floor polygon: = Bhr 2. Expand each furniture OBB: 3. Compute walkable area: = (cid:83) 4. Verify is single connected region = Oi Bhr where denotes Minkowski difference (erosion) and denotes Minkowski sum (dilation). Erosion shrinks the floor boundary inward so the robot center cannot get too close to walls, while dilation expands furniture outward to account for the robots physical extent. Blocking Identification: For each furniture piece, the tool tests if removing it reduces the number of disconnected regions. Pieces that block connectivity are identified as potential repositioning candidates. Feedback to Agent: The tool returns whether the room is fully reachable, the number of disconnected regions, the reachability ratio (largest region area divided by total walkable area), and the list of blocking furniture IDs. This feedback enables the designer to identify and reposition problematic pieces. D.4. check physics Tool The check physics tool detects geometric and functional issues that would make the scene infeasible. During furniture placement, the tool reports: Furniture collisions: Interpenetration between furniture pieces or with walls, detected via signed distance queries on collision geometry using Drake (Tedrake et al., 2019). Floor covering overlaps: Overlapping rugs or mats, detected via 2D oriented bounding box intersection since these objects lack collision geometry. Floor covering boundary: Rugs extending beyond room walls. Door blockage: Furniture blocking the clearance zone in front of doors, which would prevent passage. Open connection blockage: Furniture blocking open connections between rooms, verified by checking if robot-sized passage remains clear. Window clearance (warning): Furniture placed in front of windows above sill height. Reported as warning for the agent to judge whether the placement is acceptable (e.g., desk slightly overlapping window may be fine, while tall wardrobe fully blocking it is problematic). 3https://www.rainbow-robotics.com/en_rby 22 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Issues are reported with penetration depths and object identifiers, enabling the designer to make targeted corrections. D.5. Critic Evaluation The critic evaluates furniture placement across six categories, each scored 010: Realism (natural placement patterns, scene collisions), Functionality (items support intended activities with correct facing relationships), Layout (logical arrangement with proper spacing and clear traffic flow), Holistic Completeness (room feels appropriately furnished for its type and size), Prompt Following (literal adherence to specified furniture and quantities), and Reachability (all areas remain traversable by the robot). Holistic Completeness and Prompt Following are complementary: the former evaluates whether the room is appropriately furnished based on common expectations for the room type, while the latter checks whether explicitly requested items are present. For example, prompt living room with chair would score high on Prompt Following if the chair is placed, but low on Holistic Completeness since living rooms typically contain more furniture than single chair. D.6. Observation The observe scene tool renders the room from multiple viewpoints: one top-down view and four corner views. For corner views, walls between the camera and room center are hidden to reveal the interior. Inspired by set-of-mark prompting (Yang et al., 2023) and SceneWeaver (Yang et al., 2025), the top-down view includes annotations to support spatial reasoning: Coordinate grid with (x, y) position markers and coordinate frame Bounding boxes around each furniture piece Object labels with unique identifiers Direction arrows showing the forward-facing direction of each object Door, window, and open connection labels on architectural elements Corner views include coordinate markers for spatial reference. Object identifiers combine human-readable name with base-36 sequential suffix (e.g., chair 0, chair a, chair 10), reducing visual clutter while remaining easy for the agent to reference. Figure 8 shows an example observation. Figure 8. Furniture agent observe scene output. Left: Top-down view with coordinate grid, coordinate frame, and labeled furniture showing bounding boxes and direction arrows. Walls, doors, and windows are labeled for placement reference. Right: Four corner views with coordinate markers for depth perception and alignment. E. Wall Object Placement The wall agent adds wall-mounted objects (Ai, Xi) to Oj, including art, mirrors, shelves, and clocks. It receives the room-specific prompt Tj and the room with furniture already placed. Each room is processed independently. Table 6 lists 23 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes the available tools. Coordinate System. Each wall surface in the room has its own local coordinate frame and unique identifier. Wall objects use SE(2) coordinates (x, z, θ) within specified walls frame: runs along the wall from start (0) to end (wall length in meters), runs vertically from floor (0) to ceiling height, and θ specifies rotation in degrees around the wall normal. Since objects mount flush to the wall surface, the y-coordinate is implicitly zero. The agent populates all walls jointly during single session, specifying both the target wall identifier and the local pose for each placement. The system automatically excludes regions occupied by doors, windows, and open connections, preventing invalid placements. Table 6. Wall agent tools. Access: = Designer, = Critic. Tool Category Access Description observe scene Visual D, Multi-view renders with annotations get current scene state list wall surfaces generate wall assets list available assets place wall object move wall object remove wall object rescale wall object check physics State State Asset State Modification Modification Modification Modification Feasibility D, Wall surfaces and placed objects D, Walls with excluded regions Create wall-mounted assets List generated assets Place using wall-local SE(2) Reposition (can change walls) Remove from scene Uniform scaling Detect collisions D, D D, E.1. Physics Validation The check physics tool detects geometric issues specific to wall placements: Wall object collisions: Interpenetration between wall-mounted objects Boundary violations: Objects extending beyond wall edges or above ceiling height Excluded region violations: Placement overlapping door, window, or open connection regions E.2. Critic Evaluation The critic evaluates wall object placement across five categories, each scored 010: Realism (objects placed at realistic heights for their purpose), Functionality (objects accessible and not blocked by furniture), Layout (distribution across walls with appropriate spacing from openings), Holistic Completeness (walls appropriately decorated for room type), and Prompt Following (requested wall items present). E.3. Observation The observe scene tool provides two complementary views: top-down context view showing the full room layout, and per-wall orthographic views for precise placement. Each wall view includes: Coordinate grid with position markers showing wall-local (x, z) coordinates and coordinate frame Wall surface identifier for unambiguous wall references Doors, windows, and open connections visible as openings in the wall Wall object labels with unique identifiers Nearby furniture rendered for spatial context (unlabeled to reduce clutter) Figure 9 shows an example observation. 24 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 9. Wall agent observe scene output. Top-right: Top-down context view showing the full room with labeled wall objects. Remaining panels: Orthographic views of each wall surface with coordinate grids, wall identifiers, door/window openings, and object labels. Nearby furniture is rendered for spatial context. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes F. Ceiling Object Placement The ceiling agent adds ceiling-mounted objects (Ai, Xi) to Oj, including lights, fans, and chandeliers. It receives the room-specific prompt Tj and the room with furniture and wall objects already placed. Each room is processed independently. Table 7 lists the available tools. Coordinate System. Ceiling objects use room-local SE(2) coordinates (x, y, θ) where and specify position within the room bounds and θ is the yaw rotation in degrees. Since fixtures mount to the ceiling surface, the z-coordinate is implicitly fixed at ceiling height. Table 7. Ceiling agent tools. Access: = Designer, = Critic. Tool Category Access Description observe scene Visual get current scene state State generate ceiling assets Asset list available assets State place ceiling object Modification move ceiling object Modification remove ceiling object Modification rescale ceiling object Modification check physics Feasibility D, D, D, D D, Multi-view renders with annotations Room bounds and ceiling objects Create ceiling-mounted assets List generated assets Place using room-local SE(2) Reposition fixture Remove from scene Uniform scaling Detect collisions F.1. Physics Validation The check physics tool detects collisions between ceiling fixtures and other objects in the scene. F.2. Critic Evaluation The critic evaluates ceiling fixture placement across four categories, each scored 010: Realism (natural fixture placement centered over functional areas), Functionality (adequate lighting coverage for the room), Layout (symmetry, spacing, and clearance from tall furniture), and Prompt Following (requested fixtures present). F.3. Observation The observe scene tool renders one top-down view and two side views. Annotations include: Coordinate grid at ceiling height with (x, y) position markers Furniture and wall-mounted objects rendered for spatial context (unlabeled to reduce clutter) Placed ceiling fixtures labeled with unique identifiers Figure 10 shows an example observation. 26 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 10. Ceiling agent observe scene output. Left: Top-down view with coordinate grid and labeled ceiling fixtures. Right: Two corner views with coordinate markers for verifying fixture heights and clearance from tall furniture. G. Manipuland Placement The manipuland agent adds small manipulable objects (Ai, Xi) to Oj, creating the dense arrangements and fine-grained clutter characteristic of real indoor environments. Specialized tools enable realistic object configurations including vertical stacks, random piles, and filled containers (Section G.6). Objects are placed on support surfaces of existing furniture, wall-mounted objects (e.g., shelves), and architectural elements (e.g., floors). Before agent execution, VLM-based analysis step (Section G.1) selects which entities should receive manipulands and derives entity-specific prompts Tj,k for each supporting entity k. Each supporting entity is then processed independently with fresh agent instance. As described in the main text (Section 3.1), this forms the second level of branching in the tree-structured construction process (the first being the house-to-room decomposition used by furniture, wall, and ceiling agents), enabling parallel generation and reducing context size. Table 8 lists the available tools. Table 8. Manipuland agent tools. Access: = Designer, = Critic. Category Access Description Tool observe scene get current scene state Visual State list support surfaces State generate manipuland assets Asset list available assets State place manipuland Modification move manipuland Modification remove manipuland Modification rescale manipuland Modification create stack Specialized fill container Specialized create arrangement Specialized create pile check physics Specialized Feasibility D, D, D, D, D D D, Multi-view renders with annotations Surface bounds and placed objects Surfaces with area and clearance Create small object assets List generated assets Place using surface-local SE(2) Reposition object Remove from scene Uniform scaling Vertical stacking Fill cavity with objects Controlled placement on container Random scattered arrangement Detect collisions Coordinate System. Each support surface on an entity has its own local 2D coordinate frame and unique identifier (e.g., 0, 1, a). Manipulands use SE(2) coordinates (x, y, θ) within the specified surfaces frame: runs left-to-right across the surface, runs front-to-back, and θ specifies rotation in degrees around the surface normal. The origin (0, 0) is located at the surface center. Since objects rest on the surface, the z-coordinate is determined by the surface height. The agent populates all surfaces of an entity jointly during single session (e.g., all shelf levels of bookcase), specifying both the target surface identifier and the local pose for each placement. Surface boundaries are validated automatically, ensuring SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes objects remain within the support region. G.1. Entity Selection and Prompt Derivation VLM analyzes the room prompt Tj and rendered furniture and wall-mounted objects to determine which entities should receive manipulands. For each selected entity, the VLM derives three structured outputs that together form the entity-specific prompt Tj,k: Suggested items: Objects to place, distinguished as required (explicitly mentioned in Tj) or optional (contextually appropriate) Prompt constraints: Explicit requirements extracted from Tj, including quantity constraints (e.g., three books) and negative constraints (e.g., only laptop) Style notes: Placement guidance affecting density and aesthetic (e.g., minimalist, cluttered, organized) The same VLM call also identifies nearby context furniture (e.g., chairs around dining table) that informs placement decisions, such as positioning place settings in front of chairs or orienting user-facing objects toward seating. This context furniture is included in visual observations to help guide manipuland placement. G.2. Support Surface Detection We use the support surface identification algorithm from HSM (Pun et al., 2026) to extract horizontal placement surfaces from furniture and wall object meshes. For articulated objects, we merge all link geometries into single mesh at zero joint positions (closed drawers/doors), ensuring correct clearance computation. G.3. Physics Validation The check physics tool detects collisions relevant to the current placement session: Manipuland-manipuland collisions: Interpenetration between manipulands on the current entity Manipuland-furniture collisions: Manipulands extending into nearby furniture The tool applies context-aware filtering to report only violations the agent can address, excluding collisions involving objects from other entities. Violations are reported with object identifiers, enabling targeted corrections. G.4. Critic Evaluation The critic evaluates manipuland placement across five categories, each scored 010: Realism (natural placement patterns on surfaces), Functionality (items support intended activities), Layout (logical arrangement with visual balance), Holistic Completeness (surfaces appropriately populated for the furniture type), and Prompt Following (required items placed with constraints respected). G.5. Observation The observe scene tool renders focused views of the current entity (furniture or wall-mounted object). The base output includes four side views with coordinate markers. Additional views depend on entity type: Single-surface: One top-down view with coordinate grid, surface-local (x, y) position markers, and coordinate frame. Multi-surface: One top-down render per surface, each with coordinate grid and surface identifier. The agent populates all surfaces jointly in single session. Articulated: Doors are opened during rendering to reveal interior surfaces. For entities with drawers, one render per drawer shows only that drawer open to avoid occlusion from stacked drawers. Context furniture identified during entity selection (Section G.1) may be included to inform object orientation (e.g., chairs around dining table). Placed manipulands are labeled with unique identifiers, bounding boxes, and direction arrows. Figures 11, 12, and 13 show example observations. 28 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 11. Manipuland agent observation for single-surface entity. Top: Top-down view with coordinate grid, surface-local position markers, and coordinate frame. Manipulands are labeled with bounding boxes. Bottom: Four side views with coordinate markers. The office chair is included as context furniture to inform object orientation. G.6. Composite Object Tools The specialized composite tools  (Table 8)  create multi-object arrangements using physics simulation. These tools are essential for creating interesting arrangements, as the standard placement tool only supports SE(2) poses on surfaces. Each tool assembles objects into composite that is placed at an SE(2) pose on the support surface. In the final scene, individual objects are stored independently with full SE(3) poses computed from the surface transform and each objects pose within the composite. Figures 14, 15, 16, and 17 show examples of each tools output. G.6.1. A T Vertically stacked objects (e.g., books, plates, mug on coaster, candle on plate). Takes list of objects (same or different types) and stacks them bottom-to-top in input order: 1. Initial placement: Compute initial transforms using collision geometry bounding box heights. Objects are positioned sequentially along the Z-axis based on their vertical extents. 2. Physics simulation and stability check: Run Drake simulation to settle objects into static equilibrium. Objects may nest (e.g., stacked bowls) or shift laterally. Objects exceeding displacement threshold are marked unstable; if any object falls, the entire stack is rejected and the tool reports how many items remained stable, enabling informed retry with fewer items. 3. Clearance validation: Verify the final stack height fits the available surface clearance. G.6.2. L T E Fills containers with multiple objects using physics simulation (e.g., fruit bowls, pencil cups, utensil crocks, vases with flowers, baskets). Takes container asset and list of fill assets: 1. Cavity detection: Identify the container interior using top-rim heuristic. Vertices in the top fraction of the container height are projected to the XY plane, and their convex hull defines the opening. The hull is scaled inward to provide safety margin from container walls. 2. Object rotation: Objects are rotated based on aspect ratio. Elongated objects (aspect ratio above threshold) are oriented to stand upright with their longest axis vertical, aligned with the containers length direction. Asymmetric objects like 29 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 12. Manipuland agent observation for multi-surface entity. Top: Four side views of shelving unit with color-coded surface identifiers (S 4 through 8). Bottom: Top-down renders for individual surfaces (two of five shown), colored to match the side views. The agent populates all surfaces jointly in single session. Figure 13. Manipuland agent observation for articulated entities. Left: Doors are opened during rendering to reveal interior surfaces. Right: For entities with drawers (prismatic joints), separate renders show each drawer individually to avoid occlusion from stacked drawers (one of multiple drawer renders shown). 30 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 14. Examples of the create stack tool. Stacks include same-type objects (plates, bowls, cups), mixed objects (books, storage containers), and object-on-object arrangements (cup on saucer, candle on holder, croissant on plate). Physics simulation settles objects into stable configurations. utensils undergo thick-end-up check that compares top and bottom half footprints, flipping the object if the bottom is thicker (ensuring spatula heads face up in utensil crocks). 3. Spawn position generation: For each fill object, sample an XY position within the interior hull using rejection sampling. Objects are spawned at staggered heights above the rim, with spacing based on each objects rotated height to prevent initial overlap. 4. Physics simulation: Run Drake simulation to let objects fall and settle naturally within the container. Objects may shift, nest, or fall out depending on geometry and packing. 5. Inside/outside classification: After simulation, objects are classified based on their final position relative to configured threshold. Objects below the threshold are considered to have fallen out. 6. Iterative retry: Objects that fall out are respawned in subsequent iterations with new random positions. The loop continues until all objects settle inside or the iteration limit is reached. Previously settled objects remain in place during retries but may be pushed out by new objects, in which case they are added back to the retry list. The tool succeeds as long as at least one object remains inside; objects that cannot settle after the maximum iterations are removed from the final composite. The tool reports which objects remained and which were removed, enabling the agent to retry with different object combinations if needed. Figure 15. Examples of the fill container tool. Containers include fruit bowls, utensil crocks, pencil cups, toy bins, vases with plants, and bread baskets. Physics simulation with iterative retry settles objects naturally within container cavities. G.6.3. A R G N Places items at agent-specified positions on flat containers (e.g., place settings on placemats, cheese on cheeseboards, items on serving trays, toiletries on bathroom trays). Unlike fill container which uses random positions, this tool provides precise control over item placement. Takes container asset and list of items with their local (x, y, θ) coordinates relative to the container center: 1. Position validation: Determine whether the container is circular or rectangular using mesh volume ratio analysis, then verify each items center position lies within bounds. For circular containers, check Euclidean distance from center against the radius; for rectangular containers, check axis-aligned containment against half-extents. Only the center position is validated, allowing elongated items (e.g., forks, knives) to be placed near edges when oriented parallel to the edge. 2. Pre-collision check: Before physics simulation, detect item-item collisions using Drakes signed distance queries. If 31 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes any items overlap, the tool fails immediately with feedback identifying the colliding pair and penetration depth. 3. Physics Z-settling: Spawn items above the container surface at their specified XY positions, then run Drake simulation to let them settle under gravity. Items may shift slightly as they come to rest. 4. All-or-nothing validation: After simulation, classify items by final position. If any item falls off the container (Z below threshold), the entire arrangement fails. The tool reports which items fell and the container bounds, enabling the agent to adjust positions closer to the center. This all-or-nothing semantic ensures complete, coherent arrangements. There is no retry mechanism; if items fall off, the agent must explicitly retry with adjusted positions. Figure 16. Examples of the create arrangement tool. Items are placed at agent-specified positions on flat containers: baking trays, cheese boards, condiment caddies, candle trays, bathroom trays, and cutting boards. Unlike fill container, this tool provides precise control over item placement. G.6.4. A I Serves two primary use cases: (1) random scattered arrangements on surfaces (e.g., paperclips on desk, scattered toys, piles of laundry), and (2) placing objects into built-in furniture cavities that cannot be used with fill container (e.g., dirty dishes in sink basin), since fill container requires separate container asset. For built-in cavities, the cavity geometry naturally keeps objects inside during physics simulation. Takes list of assets (minimum 2, same or different types): 1. Spawn radius computation: Compute the average bounding box diagonal across all objects and multiply by configured factor to determine the spawn radius, ensuring objects spread naturally without excessive gaps. 2. Position and rotation generation: For each object, sample an XY position uniformly from disk of the spawn radius, with square root transform on the radial component ensuring uniform area coverage. Orientations are sampled uniformly at random over SO(3). 3. Staggered spawning: Objects spawn at staggered heights above the surface, with spacing based on each objects bounding box diagonal. This prevents initial overlap before physics settles the arrangement. 4. Physics simulation: Run Drake simulation to let objects fall and settle under gravity. Objects may shift, tumble, nest, or fall off the surface depending on geometry and initial positions. 5. Surface classification: After simulation, classify objects by final position. Objects below configured threshold are considered to have fallen off the placement surface. 6. Minimum object validation: The tool requires at least 2 objects to remain on the surface. If fewer than 2 stay on, the tool fails with feedback suggesting the agent try placing the pile further from edges or using fewer objects. Unlike fill container, there is no iterative retry. The tool returns which objects remained on the surface and which fell off, enabling informed retry with different configurations. 32 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 17. Examples of the create pile tool. Left: Floor piles with random positions and orientations: scattered toys and dish mess on kitchen floor. Right: Dirty dishes in sink basin. The sink is built into the counter, so fill container cannot be used since it requires separate container asset. H. Physical Feasibility Post-Processing While SceneSmiths agentic construction process encourages physically reasonable placements through iterative feedback, agents are not required to satisfy physical constraints exactly during generation. Generated scenes may contain inter-object penetrations or objects in configurations that are not statically stable. To ensure environments are directly simulatable, we apply two-stage post-processing pipeline after furniture placement and again after manipuland placement. Wallmounted and ceiling-mounted objects do not require post-processing as they are welded to the world and remain fixed under simulation. H.1. Non-Penetration Projection We resolve inter-object penetrations by projecting object poses to the nearest collision-free configuration using nonlinear optimization. Following Pfaff et al. (2025b), we solve: min p02 s.t. d(i, j) ϵ, = j, (4) where and p0 denote the optimized and original object translations, d(i, j) is the signed distance between objects and computed via Drakes collision checker, and ϵ = 105m provides small separation tolerance. Orientations remain fixed during projection to prevent rotational adjustments from destabilizing objects that would otherwise tip over during subsequent gravity simulation. The optimization is solved using SNOPT (Gill et al., 2002) with Drakes inverse kinematics framework. H.2. Gravity Settling After projection resolves penetrations, we simulate the scene under gravity using Drake. Objects settle into statically stable configurations. H.3. Pipeline Integration The pipeline applies projection followed by gravity settling at three points: (1) after furniture placement, where all furniture is free to translate; (2) after each supporting entitys manipulands are placed, using subset scene containing only that entity and its manipulands with the entity welded; and (3) after all manipuland placement completes, where all furniture is welded and only manipulands are optimized. H.4. Fallen Object Removal Physics simulation may cause objects to tip over or fall off surfaces. The system automatically detects and removes fallen objects using type-specific criteria: Furniture: Compute tilt angle between the objects Z-axis and world vertical. Objects exceeding threshold are marked as fallen and removed. Manipulands: Two detection methods identify fallen small objects: (1) floor penetration detection removes objects that clip through the floor, indicating physics instability; (2) surface departure detection removes manipulands that started on 33 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes furniture but ended near floor level with significant downward displacement, indicating they fell off their intended support surface. I. Stochastic Placement We observed that agents tend to select round position and rotation values when placing objects (e.g., exactly 1.0m from wall, precisely 90 rotation), resulting in unnaturally precise arrangements compared to real-world scenes. To address this, we apply Gaussian noise to object placements, effectively treating placement as noisy actions, creating realistic variation. I.1. Mode Selection The orchestrator analyzes prompt keywords to select placement style: Natural: Keywords like lived-in, cozy, casual Perfect: Keywords like pristine, showroom, gallery Default is natural mode if no keywords match. In practice, we use small noise values (13cm position, 0.53 rotation for natural mode) to introduce subtle imperfections without disrupting functional arrangements; see Table 9 for the full noise profiles. Table 9. Gaussian noise standard deviations (σ) for stochastic placement by object type and placement style. Object Type Parameter Natural σ Perfect σ Furniture Furniture Wall Wall Wall Ceiling Ceiling Manipuland Manipuland Position XY Yaw Position (along wall) Position (height) Rotation Position XY Yaw Position XY Yaw 3 cm 1.0 2 cm 1 cm 0.5 2 cm 0.5 1 cm 3.0 1 mm 0.1 1 mm 1 mm 0.1 1 mm 0.1 1 mm 0.1 I.2. Style-Aware Evaluation The critic receives placement style to calibrate expectations. Natural style accepts subtle imperfections (e.g., dining chairs at slightly varied angles). Perfect style expects precise alignment within tolerances. J. Export and Simulator Compatibility SceneSmith performs simulator-in-the-loop scene creation using Drake as its native simulation backend. Collision detection, non-penetration projection, and gravity settling are all performed in Drake throughout the generation pipeline, ensuring physical consistency during construction. Scenes are natively represented using SDFormat with Drake model directives. For compatibility with other simulators, SceneSmith can export scenes to MJCF and USD formats. These exports include visual geometry, collision geometry, physical properties (mass, center of mass, moment of inertia, friction), and articulated joints for articulated objects. The exported formats work with most common robotics simulators including MuJoCo (Todorov et al., 2012), PyBullet (Coumans & Bai, 20162021), Isaac Sim4, and Genesis5. Blender export is also supported for visualization and rendering, containing visual geometry only. K. Robot Policy Evaluation This section provides implementation details for the robot policy evaluation pipeline described in Section 3.5 of the main text. Figure 18 illustrates the pipeline. 4https://developer.nvidia.com/isaac/sim 5https://genesis-embodied-ai.github.io/ 34 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 18. Robot manipulation evaluation pipeline. Given manipulation task (e.g., Pick fruit from the fruit bowl and place it on plate), an LLM generates diverse scene prompts specifying scene constraints implied by the task. SceneSmith generates scenes from each prompt. robot policy attempts the task in simulation, and an evaluation agent verifies success using simulator state queries and visual observations. This enables scalable policy evaluation without manual environment or success predicate design. K.1. Scene Prompt Generation Given natural-language task description (e.g., pick fruit and place it on plate), an LLM generates diverse scene prompts that enable the task without pre-solving it. The generator first analyzes the task to extract: Room requirement: Whether specific room types are required (explicit: in the kitchen), implied (dining table suggests dining room), or flexible. House-level tasks may require multiple rooms. Required objects: Objects that must be present (e.g., fruit, plate). Initial state constraint: What must not be true initially. For example, for place fruit on plate, the fruit must not already be on the plate. Flexible dimensions: Aspects that can vary across prompts (style, object variants, additional furniture). The generator then produces diverse scene prompts describing room with the required objects in non-goal positions plus contextual objects for diversity. Prompts vary across style (modern, rustic, minimalist), object variants (apple vs. banana vs. orange), and placement configurations. Each prompt is passed to SceneSmith to generate complete scene. K.2. Evaluator Agent After each policy execution, an evaluator agent determines task success. The agent is provided with the task description and final scene state, then uses two categories of tools to gather evidence: State tools: Geometric queries including object listing, signed distance computation between object pairs, and support verification (contact detection and surface overlap percentage). Vision tools: Multi-view renders of the scene or specific objects, providing top-down and side views for visual verification of spatial relationships. These reuse the rendering infrastructure from SceneSmiths scene observation tools. The agent exhaustively tests candidate object pairs that could satisfy the task requirement, using state tools for efficiency and falling back to vision tools for ambiguous cases such as containment relationships. This approach avoids hand-crafted 35 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes success predicates and generalizes across diverse task specifications. Table 10 lists the available tools. Table 10. Evaluator agent tools. Tool Category Description list objects get object info get distance get spatial relation get support observe scene observe objects State State State State State Vision Vision List all objects with positions Object dimensions and orientation Signed distance between object pair Vertical/horizontal gaps, overlap Contact detection and surface overlap Multi-view renders of entire scene Focused renders of specific objects K.3. Example Policy To demonstrate the pipeline, we implement model-based pick-and-place policy using KUKA LBR iiwa14 mounted on mobile base with telescoping lift joint. The policy first uses VLM to parse the task into structured components: goal predicate (on, inside, or near), target and reference object categories, and optional preconditions. The VLM then matches these categories to specific objects in the scene, producing ranked candidate bindings that are attempted in order. true generalist policy could operate directly from the natural language task without this structured interface. Our policy is given segmented point cloud of each candidate object, and iteratively samples antipodal grasps (Tedrake, 2024, 5.4) until one is found such that the gripper is collision-free. It then solves an inverse kinematics optimization problem with collision-avoidance constraints to determine complete robot posture for the pick configuration (Tedrake, 2024, 6.1). To determine the place configuration, gripper poses are sampled within the target region and similar inverse kinematics problem is solved. It also solves for heuristic pre-grasp and pre-place configuration, where the gripper pose is moved away from the manipuland along the direction of grasping. These steps are interleaved and run repeatedly until success or timeout. Planning is performed in segments, connecting (in order) the robots start configuration, pre-grasp configuration, the grasp configuration, the post-grasp (same as pre-grasp) configuration, the pre-place configuration, and the place configuration. Planning is performed using RRT-Connect (Kuffner & LaValle, 2000) with randomized shortcutting (Sekhavat et al., 1998, 6.4.1). During the post-grasp to pre-place segment, we require that the trajectory have greater clearance of obstacles around the gripper, to help prevent dropping the manipuland. These path segments are post-processed with time-optimal path parameterization (Verscheure et al., 2009) to ensure the resulting trajectory satisfies the velocity, acceleration, and torque limits of the robot. We additionally enforce spatial velocity and acceleration limits on the pose of the gripper, to prevent sudden movements that may cause the robots grasp to slip. Finally, we create deliberately-inferior policy to demonstrate that our agentic policy evaluation can be used for performance comparisons. The inferior policy does not have the gripper pose velocity and acceleration limits, making it more likely to drop the object. It also does not have the increased gripper clearance enforced in the planning stage, making the manipuland more likely to collide with the environment. K.4. Experimental Results Tasks. We evaluate on four pick-and-place tasks, with 25 generated scenes per task: 1. Pick fruit from the fruit bowl and place it on plate on the dining table. 2. Pick coke can from the shelf and place it on the table. 3. Pick cup from the floor and place it in the sink. 4. Bring me the water bottle from the kitchen and place it on the coffee table in the living room. (house-level) Evaluator Accuracy. To validate the evaluator agent, we manually labeled all 300 evaluations (100 scenes 3 states: initial condition, standard policy result, degraded policy result). The evaluator agreed with human judgment in 299/300 cases (99.7%). The single disagreement was an ambiguous case: fruit landed on the edge of plate, which the human labeled as success but the evaluator judged as failure. Policy Comparison. Table 11 presents per-task success rates. The standard policy achieves 16% overall success compared 36 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes to 12% for the degraded variant, confirming the two policies differ in performance as intended. This demonstrates that our automatic evaluation system can detect meaningful differences in policy quality. Table 11. Robot policy evaluation results across 100 generated scenes (25 per task). Task Standard Degraded Fruit plate Coke can table Cup sink Water bottle coffee table 3/25 (12%) 6/25 (24%) 4/25 (16%) 3/25 (12%) 3/25 (12%) 4/25 (16%) 3/25 (12%) 2/25 (8%) Overall 16/100 (16%) 12/100 (12%) 0% +8% +4% +4% +4% L. Evaluation Methodology This section provides details on the experimental setup and evaluation metrics. L.1. Experimental Setup L.1.1. BASELINES We compare against five external baselines, all of which use LLMs or VLMs for scene generation but differ in their architectural approaches. Holodeck is the only baseline that supports house-level generation. LayoutVLM is evaluated with two different asset libraries. All baselines are evaluated using their open-source codebases. HSM (Pun et al., 2026) is hierarchical scene generation framework that recursively decomposes scene generation from room-level furniture to small objects on surfaces. It uses learned scene motifs (recurring spatial patterns like dining chairs around table or stacked books) to capture arrangement structures. Assets are retrieved from the HSSD library (Khanna et al., 2023) using CLIP similarity, and collisions are resolved through spatial optimization. Holodeck (Yang et al., 2024b) prompts an LLM to generate spatial relations between objects, then uses constraint satisfaction solver to convert these into physically plausible layouts. It decouples semantic understanding from geometric optimization to avoid collisions and boundary violations. Holodeck is the only baseline that supports both room-level and house-level generation. I-Design (C elen et al., 2025) is an LLM-based pipeline that uses sequence of specialized prompts (designer, architect, engineer, corrector, refiner) to progressively build scene graph from user preferences. Each role refines the scene description: the designer selects objects, the architect specifies spatial relationships, and correction stages resolve conflicts. The scene graph is converted to object poses using backtracking algorithm that samples valid positions from constraint intersections. LayoutVLM (Sun et al., 2024) leverages vision-language models for layout generation by combining visual prompting with differentiable optimization. It generates both numerical pose estimates and spatial relations from visually marked images, filtering for self-consistency before jointly optimizing for semantic coherence and collision avoidance. Originally designed for placing given set of objects, we implement the open-vocabulary extension from their Appendix B.1, which first retrieves objects from library. We evaluate two asset libraries: LayoutVLM (Curated) uses the small curated library provided by the authors, while LayoutVLM (Objaverse) uses the Objathor subset of Objaverse (Deitke et al., 2022a;b), curated collection of assets used by ProcTHOR and Holodeck. SceneWeaver (Yang et al., 2025) is single-agent LLM framework that synthesizes 3D scenes through iterative refinement using reason-act-reflect paradigm. separate VLM evaluates rendered scenes using physical and perceptual metrics. The agent then uses these evaluation results to select tools for progressive refinement. It uses standardized tool interface with feedback-driven planning to maintain spatial consistency. L.1.2. ABLATIONS We evaluate six ablations to understand component contributions: NoCritic: No critic iterations (initial design only) 37 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes NotGenerated: HSSD (Khanna et al., 2023) retrieved assets instead of generated assets NoAssetValidation: No VLM-based asset validation (routing components retained) NoSpecializedTools: No specialized placement tools (snap, facing, stack, fill, arrangement, pile) NoObserveScene: No visual observations (structured state only) NoAgentMemory: No memory of previous iterations for designer and critic L.1.3. PROMPT CORPUS We evaluate on 210 total prompts organized into five categories: SceneEval-100 (Room): 94 room-level prompts from the SceneEval benchmark (Tam et al., 2025), primarily bedrooms, living rooms, and dining rooms with varying complexity (the remaining 6 SceneEval prompts are house-level and included in the House-Level category) Type Diversity: 50 prompts each with different room type not well-represented in SceneEval-100 (pet stores, pharmacies, yoga studios, wine cellars, etc.) Object Density: 25 prompts specifically designed to stress-test high object count scenarios (e.g., kitchen shelf with at least 10 plates, 8 bowls, and 8 cups) Themed Scenes: 10 prompts with specific stylistic constraints (Star Wars themed bedroom, steampunk styled home office, Harry Potter library, etc.) House-Level: 31 prompts for multi-room scenes (SceneEval-100 house subset plus additional house prompts) Since most baselines only support room-level generation, we evaluate room-level prompts (179 total) and house-level prompts (31 total) separately. Room-level results comparing all methods are presented in Sections and N. House-level results comparing SceneSmith against Holodeck (the only baseline with multi-room support) are presented in Tables 1 and 2 in the main paper. L.2. SceneEval Metrics We use the following metrics from the SceneEval benchmark (Tam et al., 2025): CNT (Object Count): Satisfaction rate for object count requirements. VLM maps scene objects to objects specified in the prompt, then compares instance counts to annotated quantities (exact or relative). ATR (Object Attribute): Satisfaction rate for object attributes (e.g., colors, materials). Renders front view and reference view with human figure for scale, then uses VLM to evaluate if objects satisfy the specified attributes. OOR (Object-Object Relationship): Satisfaction rate for spatial relationships between objects (e.g., chair in front of desk). Evaluates predefined relationship types. OAR (Object-Architecture Relationship): Satisfaction rate for object-architecture relationships (e.g., sofa against wall, rug in middle of room). Checks relationship types between objects and architectural elements. ACC (Accessibility): Measures whether functional sides of objects are accessible. VLM identifies functional sides (e.g., front of sofa, sides of bed), then 2D occupancy analysis checks if those sides are unobstructed. NAV (Navigability): Ratio of the largest connected free floor space to the total free space. Uses 2D occupancy projection and connected component analysis to measure if object placements create isolated regions. OOB (Out of Bounds): Fraction of objects outside the floor plan boundary. Samples surface points and casts rays toward the floor; objects with <99% of rays hitting the floor are considered out-of-bounds. L.2.1. PHYSICS METRICS FOR SIMULATION READINESS In addition to the semantic metrics above, we propose two physics-based metrics using Drake (Tedrake et al., 2019) to evaluate whether generated scenes are ready for robotic simulation. None of the baselines produce scenes that can be directly simulated in physics simulators, as they lack collision geometry and physical properties (mass, center of mass, inertia, friction). To enable evaluation, we first make baseline scenes simulator-compatible: we generate collision geometries using V-HACD convex decomposition (128 max hulls, matching our pipeline) and estimate mass, center of mass, and inertia assuming uniform density of 1000 kg/m3 (water density). We exclude carpets from these metrics as they are mainly visual elements, following SceneWeaver (Yang et al., 2025). This postprocessing step reflects the work required to use baseline outputs in robotic simulation. SceneSmith scenes are simulation-ready out of the box and require no such postprocessing. 38 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes COL (Collision Rate): Uses Drakes signed distance queries on collision geometry. Reports the fraction of objects in collision with penetration depth exceeding 1mm. This metric reflects whether scenes are physically feasible. Deep penetrations can cause objects to explode apart due to high contact forces when simulating scene, while slight penetrations are generally tolerable. STB (Static Equilibrium): Runs 5-second physics simulation with gravity (1ms time step). Objects are marked stable if displacement < 1cm and rotation < 0.1rad after settling. Wall and ceiling objects are welded to the world frame since they are mounted fixtures. For fairness, support types (ground, wall, ceiling, or object-supported) are determined by SceneEvals VLM classification for all methods, including ours, rather than using ground-truth labels from our pipeline. M. Human Evaluation This section presents the methodology of our human evaluation study. See Table 1 in the main paper for complete pairwise comparison results. P-values were computed using two-sided binomial tests against null hypothesis of 50% win rate, with Benjamini-Hochberg FDR correction for 28 simultaneous tests (14 comparisons 2 dimensions). Effect sizes are reported using Cohens (small 0.2, medium 0.5, large 0.8). Our study was powered to detect win rates 63% with 95% power; detecting smaller effects (5355%) would require 6-18x more comparisons. M.1. Study Setup We conducted human evaluation study with the following setup: Platform: Prolific6, USA-based participants only Participants: 210 recruited, 205 with complete data Compensation: $12.50 per participant (32 minutes mean completion time) Scenes: 179 room-level + 31 house-level prompts (same as SceneEval, see Section L) Design: Pairwise comparison where each comparison includes SceneSmith vs one baseline or ablation Comparisons per participant: 15 scene pairs Total valid responses: 3,051 M.2. Interface Design The interface was iteratively refined through multiple rounds of feedback from 12 independent testers who completed the study and provided detailed suggestions. The final design prioritizes accessibility for non-specialists, with an interactive tutorial, contextual tooltips, and multiple viewing modes to accommodate different preferences and device capabilities. We optimized for various internet speeds by compressing GLB files for fast download, implementing lazy loading, and adding loading indicators for both 3D viewers and images. See Appendix for screenshots. Each comparison presents two scenes side-by-side with the following components: Primary view: Custom Babylon.js 3D viewer with full interactivity (orbit, pan, zoom) Object inspection: Click to select objects with visual highlight Controls: Hide Selected, Show All, Frame (focus camera on object), Reset view, Fullscreen Alternative views: Click-Through carousel (9 camera angles) and Grid View for image-based viewing Prompt display: Text description shown prominently above the comparison The evaluation was fully blinded: method names were hidden, and scene positions (left/right) were randomized. M.3. Questions Participants answered two questions per comparison. Each question included short hint visible below the question text and an expandable tooltip with detailed guidance. The welcome page also provided context for interpreting each question. Q1: Realism (forced choice: Scene / Scene B) Which scene looks more realistic? 6https://www.prolific.co/ 39 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Hint: Consider: realism, object placement, layout coherence Tooltip: Consider the realism of each scene: Does it look like realistic room? Are the objects and furniture well-placed and appropriately sized? Does the overall layout feel coherent and plausible? Welcome page: Which scene looks more realistic? Consider whether the room looks realistic, objects are well-placed and appropriately sized, and the layout feels coherent and plausible. Q2: Prompt Faithfulness (three options: Scene / Scene / Equal) Which scene better follows the requirements in the text description? Hint: Consider: requested objects, room type, described details Tooltip: Consider how well each scene matches the specific requirements mentioned in the prompt: Does the scene include the requested objects? Is the room type correct? Are details like colors, quantities, and arrangements as described? Additional objects that fit the room are fine. Welcome page: Which scene better follows the requirements in the text description? Consider whether the requested objects are present, the room type is correct, and details like colors, quantities, and arrangements match what was described. Additional objects that fit the room are fine. Welcome page example: If the prompt says living room with sofa and coffee table with book on it, the scene should include sofa and coffee table with book on top. Additional items typical of living room (like TV, rug, or lamp) are fine. We dont expect an empty room with just the mentioned objects. Equal confirmation: When selecting Equal, participants saw: Are you sure both scenes follow the description equally well? Select Equal only if you genuinely cannot distinguish which scene better matches the prompt. M.4. Randomization and Counterbalancing Left/right position: Randomized per comparison to prevent positional bias Baseline assignment: Each baseline is evaluated by an equal number of participants Comparison order: The 15 comparisons are presented in random order per participant Question order: Randomized per comparison to prevent order effects between Q1 and Q2 No prompt repeats: Each participant sees each prompt at most once M.5. Results by Prompt Category Table 12 shows win rates broken down by prompt category for room-level scenes. Object Density prompts show the highest realism win rate (94.7%), validating our methods ability to handle complex, object-rich scenes. Themed Scenes show the highest faithfulness win rate (94.9%), indicating strong adherence to specific stylistic constraints. All room-level categories exceed 89% on both metrics, demonstrating consistent performance across diverse scene types. House-level results are reported in Table 1 in the main paper. Table 12. User study win rates by prompt category (room-level scenes, average across all external baselines). denotes the number of pairwise comparisons (excluding ties for faithfulness). Category Realism Win% Faith. Win% (Real.) (Faith.) SceneEval-100 Type Diversity Object Density Themed Scenes 92.2% 90.3% 94.7% 89.8% 92.0% 89.8% 89.7% 94.9% 831 309 151 59 808 304 146 59 N. Automated Evaluation This section presents results from automated evaluation metrics including SceneEval benchmark scores and generation costs. 40 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes N.1. SceneEval Benchmark Results Table 13 presents SceneEval metrics and our physics metrics for all methods on the 179 room-level prompts. Error bars. All values are reported as mean 95% confidence interval (CI), computed using the t-distribution: CI = t0.975,n1 (s/ n), where is the sample standard deviation and is the number of scenes. Non-overlapping CIs suggest statistically significant differences between methods. Table 13. SceneEval benchmark results (179 room-level scenes). Metrics: CNT = object count, ATR = attributes, OOR = object-object relationships, OAR = object-architecture relationships, ACC = accessibility, NAV = navigability, COL = collision rate, STB = stability, OOB = out-of-bounds. Values are mean 95% t-distribution CI. = higher is better, = lower is better. Non-overlapping CIs suggest statistically significant differences. Method #Obj CNT ATR OOR OAR ACC NAV COL STB OOB External Baselines HSM Holodeck IDesign LayoutVLM (Curated) LayoutVLM (Objaverse) SceneWeaver Ablations NoCritic NoObserveScene NoAgentMemory NoSpecializedTools NoAssetValidation NotGenerated 22.72.6 23.01.4 13.01.0 11.21.6 14.21.7 13.51.0 54.08.3 69.715.4 78.919.7 61.514.1 72.716.3 57.79.9 60.64.2 44.24.3 69.34.0 41.04.0 55.54.7 41.83. 83.53.0 81.13.0 82.53.1 83.12.9 81.63.0 75.73.1 61.57.3 44.47.2 50.37.2 25.66.6 34.37.0 29.16.7 72.46.3 75.96.0 72.66.5 75.26.1 72.45.9 64.26.9 30.65.7 16.84.6 28.65.5 14.14.0 20.75.0 15.34.2 64.55.8 59.65.8 66.05.8 66.05.5 65.55.5 53.76.2 66.26.6 38.07.1 66.66.7 22.46.1 17.45.6 31.66. 73.56.2 71.76.7 81.55.3 81.25.4 79.55.4 72.46.3 88.91.7 84.91.4 70.12.8 93.81.9 91.51.6 77.52.7 85.81.6 84.71.7 84.51.6 83.81.5 83.61.4 83.91.6 99.40.4 99.60.2 95.91.4 99.70.2 98.70.8 98.11.2 99.00.8 98.80.8 97.61.2 97.71.1 97.51.2 97.31.3 20.63.0 12.32.3 3.01.5 25.94.4 28.93.5 12.52. 0.80.4 0.90.4 1.70.7 0.50.3 0.80.4 7.41.3 45.23.9 31.92.7 60.84.6 19.43.6 8.11.7 37.34.4 96.11.5 95.31.5 94.11.9 97.61.2 95.61.5 88.42.4 5.51.1 0.80.4 4.32.0 6.22.1 5.11.4 0.00.0 0.10.1 0.00.0 0.10.1 0.00.0 0.10.1 0.10.1 SceneSmith (Ours) 71.113.0 82.93.4 74.46.2 67.65.8 80.65.5 83.41. 97.61.1 1.20.6 95.61.7 0.20.4 See Table 2 in the main paper for baseline comparison results. For ablations, most semantic metrics have overlapping CIs with SceneSmith, indicating these automated metrics lack sensitivity to differentiate system components. The exception is NotGenerated, which shows statistically significant degradation on CNT (75.7% vs 82.9%), OOR (53.7% vs 67.6%), and STB (88.4% vs 95.6%), all with non-overlapping CIs. This suggests that generated assets provide meaningful improvements over fixed asset library. NoObserveScene and NoAssetValidation show significant user study differences  (Table 1)  not captured by these automated metrics. Other ablations (NoCritic, NoAgentMemory, NoSpecializedTools) show user study win rates near 50% (5255% realism) that did not reach significance with our study power, and similarly indistinguishable automated metrics. N.1.1. EQUILIBRIUM STATISTICS While the STB metric reports the fraction of stable objects, it does not capture the severity of instability. Table 14 presents detailed equilibrium statistics from our 5-second physics simulation, showing how much objects displace and rotate after settling. SceneSmith achieves 12.3 4.5mm mean displacement with tight CI, indicating consistent stability across scenes: objects reliably settle within 1cm. Baselines show 30-75x higher mean displacement (382932mm) with non-overlapping CIs, confirming statistically significant differences. The contrast in variance is also informative: ablations like NoAgentMemory (795 1079mm) have CIs larger than their means, indicating inconsistent behavior where some scenes work well while others contain objects that fail significantly. Note that some instability stems from VLM misclassification of object support types (e.g., wall-mounted objects incorrectly classified as floor-supported, which then fall). This is expected to affect all methods equally. N.1.2. COLLISION DEPTH STATISTICS While the COL metric reports the fraction of colliding objects, it does not capture how severely objects interpenetrate. Table 15 presents penetration depth statistics, showing the mean depth among colliding objects per scene. SceneSmiths collisions are not only rare (1.2%) but shallow: colliding objects penetrate by only 3.77 1.28mm on average, consistent with slight overlaps among densely packed manipulands (e.g., fruit in bowls) that do not cause simulator instability, SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Table 14. Equilibrium statistics for room scenes (179 prompts). Values are mean 95% t-distribution CI, averaged across scenes. MD = mean displacement across non-welded objects per scene; XD = max displacement per scene; MR = mean rotation per scene. Lower values indicate objects settle with minimal movement, critical for simulation-ready scenes. Non-overlapping CIs suggest statistically significant differences. CIs are symmetric approximations; actual values are bounded at zero. Method MD (mm) XD (m) MR (rad) SceneSmith + Ablations SceneSmith (Ours) NoCritic NoObserveScene NoAgentMemory NoSpecializedTools NotGenerated NoAssetValidation Baselines HSM Holodeck IDesign LayoutVLM (Curated) LayoutVLM (Objaverse) SceneWeaver 12.34.5 28.534.0 61.983.7 795.11078.8 22.624.4 45.319.4 33.331.5 654.8164.1 932.2233.6 382.3106.1 684.5117.1 906.3114.7 575.8103.2 0.350.13 1.311.40 1.141.31 1.921.94 0.670.83 0.940.59 1.191.31 5.021.88 7.952.69 2.060.50 2.490.46 3.510.44 3.300. 0.0140.004 0.0110.005 0.0200.011 0.0200.006 0.0070.003 0.0490.015 0.0110.003 0.1930.037 0.2170.032 0.1140.027 0.2080.040 0.2990.049 0.1930.033 Table 15. Collision depth statistics for room scenes (179 prompts). Values are mean 95% t-distribution CI, averaged across scenes. COL = fraction of objects in collision (>1mm penetration); MPD = mean penetration depth among colliding objects per scene. Lower values indicate less severe interpenetration. Non-overlapping CIs suggest statistically significant differences. CIs are symmetric approximations; actual values are bounded at zero. Method COL (%) MPD (mm) SceneSmith + Ablations SceneSmith (Ours) NoCritic NoObserveScene NoAgentMemory NoSpecializedTools NotGenerated NoAssetValidation Baselines HSM Holodeck IDesign LayoutVLM (Curated) LayoutVLM (Objaverse) SceneWeaver 1.20.6 0.80.4 0.90.4 1.70.7 0.50.3 7.41.3 0.80.4 20.63.0 12.32.3 3.01.5 26.14.4 31.43.5 12.62.6 3.771.28 3.031.44 4.443.67 3.991.27 4.483.13 6.141.91 2.320.70 14.543.46 12.943.88 47.0525.31 15.004.01 12.781.78 10.873.21 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes as contact forces are proportional to penetration depth. Baselines exhibit 3-12x deeper penetrations (10.8747.05mm), with IDesign showing particularly severe interpenetration (47.05 25.31mm) despite relatively low collision rate (3.0%), indicating that its few collisions involve deeply overlapping objects. The NotGenerated ablation stands out among ablations with both the highest collision rate (7.4%) and deepest penetrations (6.14mm), suggesting that retrieved HSSD assets have visual geometry that produces worse V-HACD collision decompositions, leading to more penetrations during physics post-processing. N.2. Generation Costs N.2.1. COST AND TIME BREAKDOWN Table 16 presents generation cost and time statistics. For baseline cost estimates, we refer readers to the respective papers. Table 16. Generation cost and time statistics per scene. Method Mean Std Min Max Cost per scene ($) SceneSmith (Ours) NoCritic NoObserveScene NoSpecializedTools NoAssetValidation NoAgentMemory NotGenerated 13.98 4.24 9.62 14.31 14.85 16.41 15.70 Time per scene (hours:minutes) SceneSmith (Ours) NoCritic NoObserveScene NoSpecializedTools NoAssetValidation NoAgentMemory NotGenerated 3:26 1:40 2:10 3:14 3:11 4:14 3:33 9.98 2.15 7.46 9.48 9.24 10.76 9.99 1:57 1:04 0:49 1:41 1:52 2:01 1:49 4.59 1.73 3.94 5.07 5.52 5.56 4.42 1:17 0:42 0:48 1:20 1:19 1:48 1:11 101.93 22.70 75.56 72.54 81.99 96.62 104. 15:45 12:17 5:24 17:43 20:22 15:35 13:55 NoCritic achieves 70% cost reduction ($4.24 vs $13.98) and 51% faster generation (1:40 vs 3:26) but produces sparser scenes (54.0 vs 71.1 objects on average), presenting cost-density trade-off discussed in the main paper. NoObserveScene is 31% cheaper ($9.62 vs $13.98) and 37% faster (2:10 vs 3:26) because images consume many tokens. Conversely, NoAgentMemory is 17% more expensive ($16.41 vs $13.98) and 23% slower (4:14 vs 3:26) because designers and critics retry previously failed approaches without remembering what was attempted. N.2.2. COST BREAKDOWN BY AGENT Table 17 shows how costs are distributed across agent components. Table 17. Cost breakdown by agent component for SceneSmith. Designer includes Initial, Change, and Layout sub-agents. Orchestrator includes Manipuland planner. Other includes VLM calls for asset physics analysis and furniture selection. Component Cost ($) % of Total Designer Critic Orchestrator Other Total 10.61 2.79 0.24 0.34 13.98 75.9% 20.0% 1.7% 2.4% 100% The Designer agent dominates costs at 76%, reflecting the iterative refinement process. O. Evaluation Prompts This section lists all 210 evaluation prompts organized by category. 43 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes O.1. SceneEval-100 (Room) (Tam et al., 2025) (94 prompts) 1. bedroom with bed, two nightstands, and wardrobe in the corner of the room. 2. bedroom with bed and wardrobe. 3. bedroom with king-size bed in the corner of the room, two large blue armchairs, and floor lamp near armchair. 4. bedroom with double bed and mini fridge near the bed, table across from the door, and painting on the wall above the bed. 5. bedroom with two bunk beds in the corners of the room, desk placed against the wall far from the door, and light hanging from the ceiling in the center of the room. 6. simple bedroom featuring twin bed against the wall, with wardrobe positioned in the corner of the room, and desk next to the window. 7. minimal bedroom with queen bed against the wall, flanked by two nightstands on each side. 8. living room with TV, sofa, and bookshelf. There is no coffee table in the room. 9. living room with TV, sofa, bookshelf, and coffee table. 10. living room with sofa in front of wall-mounted TV, lamp behind the sofa, and stool near the TV against the wall. 11. living room with two bookcases along the wall far from the door, wooden chair near the bookcases, and painting on the wall opposite the bookcases. 12. living room with two-seater sofa against the wall, square rug in the middle in front of the sofa, and two large plants on the floor near the sofa. 13. living room with bean bag near wall cabinet, which is across from the window. There is stool next to the bean bag. 14. living room with sofa against the wall across from the door, painting on the wall above the sofa, and pendant light hanging in the middle of the room. 15. dining room with two bar stools at the short sides of bar table. 16. dining room with three dining chairs surrounding dining table. 17. small dining room with dining table next to window, chair on the long side of the table, and rug in the middle of the room. 18. dining room with cabinet next to the door against the wall, and two wine cabinets against the wall near bar table. 19. dining room with table in the corner of the room and chair on the long side. 20. dining room with ceiling light hanging above the table and two chairs on the long side of the table. 21. bedroom with bed with two nightstands on each side, wardrobe, and TV stand positioned in front of the bed. There is toy box with two dolls outside the box. 22. bedroom with bed flanked by two nightstands, and an office chair in front of an office desk located in the corner of the room. There is no wardrobe in the room. 23. bedroom with bed against the wall and at least one nightstand next to it, featuring lamp on the nightstand. There is an office desk with monitor on top and sofa chair in front of it. 24. teenagers bedroom featuring twin bed with an adjacent nightstand, wardrobe for clothes, small desk with chair, and bookshelf positioned next to the wardrobe. There are two books outside the bookshelf. 25. teenagers bedroom designed for both comfort and functionality includes twin bed with an adjacent wooden nightstand, large wardrobe with multiple drawers for clothes, and small desk and chair equipped with an electronic setup featuring monitor and peripherals. large oak bookshelf is positioned next to the wardrobe. 26. bedroom with king-size bed positioned against the wall across from the window. Two stools are placed in front of the bed. painting hangs above the bed, flanked by wall light on each side. There is bookcase with two books on the left side. 27. bedroom with three extendable beds, one of which has doll placed on top. Two of the beds are positioned in the corners of the room, and dresser is located next to the door. There are two dolls outside toy box. 28. living room with coffee table in front of large sofa, TV on TV stand facing the sofa, and chair on the short side facing the coffee table. There is cup on the right side of the TV stand. 29. living room with coffee table in front of large sofa, TV on TV stand facing the sofa, and two small tables on each side of the sofa. 30. living room featuring two bookshelves along the wall, with only one book on the bookshelf nearest the door. Across from the bookshelves, painting is mounted on the wall. In the center of the room, table with tray holding potted plant sits near an L-shaped sofa. 31. living room with marble table in front of four-seater sofa. The sofa is positioned in the corner of the room, with two bean bags placed next to the table. lamp hangs from the ceiling. 32. large living room with three display shelves against the wall, with long sofa in front. There are two tables in front of the sofa and another table right of the sofa with lamp on top. There is no TV in the room. 33. Japanese-style living room featuring coffee table next to the window with two floor cushions placed beside it. sofa is positioned across from the window, and in front of the sofa is table with teapot on top, completing the serene and minimalist setup. 34. living room featuring an irregular-shaped table in the middle of the room with sofa positioned in front of it. Across the table are two sofa chairs with small wooden coffee table placed between them. clock is mounted on the wall far from the door. 35. dining room with bar table positioned in the middle of the room. wooden shelf is mounted on the wall, holding potted plant and jug, adding touch of decor and functionality. Along the wall, there is fridge and wine cabinet, providing ample storage and cohesive design. 36. dining room with fridge positioned near the door against the wall. table is placed in the middle of the room with two chairs on its long sides. In the corner of the room across from the door, there is cabinet. 37. dining room with six wooden dining chairs surrounding round wooden table in the middle of the room. There is no coffee table in the room. 38. dining room with square dining table, featuring two chairs on each long side and one chair on each short side of the table. 44 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes 39. dining room with dining table featuring four chairs on the long side and one on short side. vase is placed in the middle of the table. display cabinet is positioned next to window against the wall. 40. Generate dining room. The room should have rectangular, wooden dining table with three chairs surrounding it. There is utility cart near the table with three plates inside. 41. spacious master bedroom featuring king-size bed against the wall, flanked by two nightstands, one holding modern table lamp and small book. large wardrobe is positioned in the corner of the room, and wooden dresser with three picture frames on it. Near the window, plush armchair with throw blanket provides cozy seating area. round area rug is placed at the foot of the bed, adding warmth and style to the space. 42. teenagers bedroom features comfortable twin bed with backboard in the far corner, with boxes underneath it. At the foot of the bed is small desk equipped with monitor, an external keyboard and mouse, and desk lamp on the right for visibility, accompanied by rolling chair. Next to the bed, nightstand with an additional floor lamp nearby provides space for phone and other valuables. sizable wooden wardrobe with multiple drawers offers ample storage for clothes, while coffee table beside it holds books and board games. In the center of the room, tan-colored rug creates cozy spot to sit, and the walls are adorned with various posters and pictures. 43. bedroom featuring bunk bed positioned next to the window, adorned with plush toys on the bed. The room includes two desks placed against the wall, each accompanied by chair. Over ten plush toys are scattered across the desks, adding playful and cozy touch to the space. 44. modern living room featuring an L-shaped sofa facing wall-mounted TV above floating media cabinet. Two leather accent chairs face glass coffee table in the center, which displays two art books. bar cart stocked with glasses and bottles stands in the corner, while three pendant lights hang gracefully from the ceiling. The sofa is flanked by two side tables, one of which holds table lamp, and large abstract painting decorates the wall. 45. The room is dimly lit, creating somber atmosphere in cozy and well-furnished living room. In the center of the room, there is dining table with four wooden dining chairs arranged neatly around it. The table appears to be set, indicating recent meal or gathering. Adjacent to the dining area, There is glass coffee table serving as centerpiece for the seating arrangement. It is both stylish and practical, providing surface for drinks or decor. multi-seat sofa is in front of the coffee table, providing ample seating for guests. Completing the seating options, two comfortable armchairs also face the coffee table on opposite sides near the sofa. Next to the sofa and each armchair is corner side table, adding touch of functionality and convenience. Each table has lamp on it for lighting. The sofa and seating arrangement should face an opposite wall, against which someone in the scene could view large flatscreen TV. The TV is supported by TV stand with storage underneath for various books. The room appears to reflect the inhabitants taste for warm and inviting environment, despite the unsettling news program playing on the TV in the background. The juxtaposition of the serene living room with the chaos and screams on the TV screen creates sense of tension and unease. 46. modern living room featuring four sofa chairs surrounding circular ceramic table, with two floor lamps positioned adjacent to the table. All the furniture is set on large rectangular rug in the middle of the room. large wooden tray with two glass bowls rests on top of the table. mirror is mounted on the wall next to the window, flanked by wall lamp on each side. 47. An open-plan dining area featuring round table surrounded by six mid-century modern chairs. grand chandelier illuminates the table, while sideboard with four drawers rests against the wall, topped with three potted plants. Two wall sconces provide additional lighting, flanking large mirror that hangs between them. wine cabinet is positioned in the corner, with serving cart placed nearby for added convenience. 48. cozy bedroom featuring queen-sized bed against soft pastel wall, with small desk and bookshelf tucked neatly into the corner. 49. simple bedroom featuring full bed flanked by two bedside lamps, with large wardrobe positioned directly facing the bed. 50. bright bedroom with king-sized bed, plush area rug under it, and dresser along the wall. 51. large bedroom with California king bed, reading table in the corner with comfy chair, and wall-mounted TV for entertainment. 52. living room featuring an overstuffed sofa, vintage wooden table, alongside bicycle hung decoratively on the wall. 53. chic living room with bright rug, gallery wall of family photos, and comfortable hammock chair. 54. contemporary living room with leather sofa, small coffee table, and vintage typewriter. 55. sunny dining room with large picnic-style table, colorful tablecloth, and lovely herb garden next to the window. 56. simple dining room featuring farmhouse table, rustic chandelier, and small cart placed against the wall. 57. minimalist dining room with glass table and an industrial-style metal chair next to it, accented by unique wall-mounted wine rack. 58. In cheerful playroom, spacious foam play mat lies in the middle, with small climbing structure positioned nearby for active play, and cozy bench situated against the wall. 59. This playroom features large colorful rug at its center, with play kitchen set against one wall and rocking horse placed on the rug. 60. compact gaming room is designed with an adjustable computer desk and chair for PC gaming, positioned next to large TV and comfortable couch. 61. minimalist gaming room features wall-mounted screen paired with low-profile media console below, while compact gaming desk with an ergonomic chair is positioned nearby. 62. kitchen featuring bistro table with two chairs in corner and refrigerator positioned nearby. 63. stylish kitchen with an elegant pendant light hanging over marble-topped kitchen counter. single cabinet is positioned against the wall, providing ample storage space. 64. bathroom with freestanding bathtub positioned beside frosted window and double vanity located across from it. 65. In classic bathroom, double-sink vanity is set against one wall with two round mirrors hanging above, while toilet is positioned in the corner. 66. In this basement, pool table occupies the center of the room, with two bar stools lined up at nearby bar table, allowing for social gatherings and entertainment. 67. gym-purpose basement featuring treadmill and an exercise bike positioned against one wall, with tall mirror mounted on wall SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes directly facing the exercise bike. 68. This is small, plain bedroom. Upon entering through the door, white desk with black rolling chair in front of it is positioned against the wall to the right. queen bed is centered against wall, with two modern nightstands beside it, one of which holds stylish lamp. 69. large bedroom with lofted twin bed in one corner, freeing up the floor space below for comfy armchair and small side table. bookcase stands against the wall next to the armchair, while small desk, equipped with pen and some papers along with lamp for studying, is positioned near the window. 70. bedroom with modern queen bed against the main wall. Two bedside tables stand on either side, while small writing desk is positioned in the corner next to the window, equipped with cute desk lamp. wicker basket lies at the foot of the bed. 71. bedroom featuring queen-sized bed against one wall, with whimsical painting hung above it. small bedside table sits to the left of the bed, holding cute alarm clock, while dresser stands across from the door. 72. living room with sectional sofa, wooden coffee table in the middle, and retro record player in the corner with some records nearby. 73. living room with large wall-mounted screen, several chairs arranged in front of it, and collection of board games on shelf in the corner. 74. cozy living room with an L-shaped oversized sofa against one wall, adorned with three colorful pillows. In front of the sofa, wooden coffee table holds small vase with fresh flowers. 75. dining room featuring simple rectangular table, corner shelf filled with cookbooks, and pendant light hanging from the ceiling. 76. simple dining room with picnic table, illuminated by string lights overhead, and small cooler holding utensils and napkins. 77. dining room with round wooden table in the middle of the room, surrounded by four wooden chairs, and large vintage map displayed on the wall. 78. bright playroom featuring low table with board games in the center, surrounded by three small chairs for group activities, while an adjacent puppet theater is positioned against the wall. 79. This engaging playroom features small painting stand positioned next to the window, bookshelf full of toys standing against the nearby wall, and cozy reading chair set across from the door. 80. tech-styled gaming room featuring glass gaming desk with dual monitors and powerful gaming PC, with an ergonomic chair positioned directly in front. 81. welcoming gaming room featuring U-shaped sofa with coffee table in front, facing large TV on the wall. In the corner, there is mini arcade machine and PlayStation. 82. In this modern kitchen, large bar table with four high-backed stools stands in the center, facilitating social gatherings, while stainless steel fridge is positioned against the far wall, next to sleek pantry shelf. small bowl of fruit sits on the tables surface. 83. This cozy kitchen features rustic wooden dining table positioned against the window, surrounded by four chairs. sideboard against the wall is equipped with coffee maker and small herb planter. 84. In this bathroom, wide vanity sits against the wall, paired with tall mirror mounted above. Nearby, stylish laundry hamper is positioned next to the door, while small basket containing bath essentials rests beside the tub. 85. bright bathroom features spacious bathtub positioned beside large window, while stylish shelving unit stands against the wall, displaying towels and small decorative items. The toilet is placed across from the tub, with small basket beside it. 86. This versatile basement features large sofa and an area rug in the center, creating cozy movie area, while small treadmill is positioned in the corner for quick exercise sessions. mini chalkboard hangs on the wall next to wall clock. 87. This entertaining basement layout features large gaming setup with two monitors on desk against one wall, while comfy bean bag chair is positioned nearby for casual seating. Across from the gaming area, small cabinet with mini fridge and popcorn machine on top completes the setup. 88. welcoming bedroom with full bed placed against the wall, flanked by two nightstands topped with bedside clock and decorative candle. reading chair sits in the corner next to tall bookshelf filled with novels. 89. bedroom featuring vintage double bed dressed in soft linens, with painting hanging on the wall above it. On either side of the bed are wooden nightstands, each with small decorative vase. tall chest of drawers stands against the opposite wall. small round table in the corner holds lamp, colorful cobblestones, and photo frames. 90. living room featuring comfortable recliner chair placed next to tall bookshelf filled with books and small sculptures. Across from the chair, table next to large window holds collection of candles. In the center of the room, round coffee table is surrounded by simple sofa and two cube ottomans. 91. living room with comfortable loveseat against the wall, adorned with plush cushions. Next to the loveseat, small table holds cup of tea and remote control. mid-century style coffee table sits in front of the loveseat with art magazines spread out. large bookshelf containing books and newspapers lines the wall across from the door. 92. dining room with circular table surrounded by six vintage chairs, and an old wooden ladder against the wall displaying plants and decorative jars. 93. In this playroom, large toy chest sits against the wall, overflowing with colorful building blocks, while small craft table nearby is equipped with papers, crayons, and glue sticks for hands-on projects. 94. This gaming room features powerful speakers positioned in every corner to create an immersive audio experience, with large screen mounted opposite plush sofa. small table beside the sofa holds various game controllers. O.2. Type Diversity (50 prompts) 1. pet store with aquariums along one wall, exactly two shelf units where one displays pet food bags and the other displays pet toys, pet beds on the floor, and checkout counter near the entrance. 46 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes 2. bookstore with at least four tall bookshelves, each containing at least five books. central display table showcases featured titles, and two comfortable reading chairs sit in one corner. 3. small grocery store with two refrigerated display cases along one wall. Two tables near the entrance hold produce bins, each containing at least three fruits. At least four shopping baskets are stacked by the door. 4. pharmacy with pharmacy counter in the back of the room. Two shelves stand against the walls, each holding at least four medicine bottles. Three waiting chairs are arranged near the counter. 5. toy store with two large toy bins in the middle of the room, each containing at least five stuffed animals. Three shelves along the walls display multiple board games each, and rocking horse sits in one corner. 6. An electronics store with two display tables in the center, one showcasing three laptops and the other holding four tablets. Two large televisions are mounted on the wall, and headphone display stand holds headphones. 7. furniture showroom with sofa and coffee table arrangement in one corner, dining table surrounded by four chairs in the center, and two floor lamps positioned near the walls. large rug lies under the dining set. 8. flower shop with three flower buckets on the floor, each holding flowers. wooden table displays potted plants. potted bonsai tree sits in one corner, and two Christmas trees stand in another corner. 9. hotel lobby with reception desk in the back of the room. seating area in the center features two sofas facing each other with coffee table between them. luggage cart stands near the entrance, and large chandelier hangs from the ceiling. 10. hotel room with queen bed flanked by two nightstands, desk with chair, and luggage rack in the corner. 11. restaurant with four dining tables, each surrounded by four chairs. bar counter runs along one wall with six bar stools in front. Two potted plants flank the entrance, and pendant lights hang above each table. 12. cafe with bar counter along one wall featuring coffee machine, filter coffee stand, and stacks of stone coffee cups. Three small round tables with two chairs each are arranged in the center. chalkboard menu hangs on the wall behind the counter. 13. bar with long wooden bar counter and five bar stools. Shelves behind the counter display bottles. Two high tables with two stools each are near the window. 14. diner with three booth tables against the wall, jukebox in the corner, and hostess stand near the entrance. 15. private office with desk and office chair, shelf with books against the wall, and two guest chairs facing the desk. 16. An open office with four desks arranged in pairs facing each other. Each desk has monitor and an office chair. water cooler stands in the corner. 17. conference room with long table surrounded by eight chairs. projector screen hangs on the wall, and whiteboard is mounted on another wall. 18. reception area with reception desk, at least three waiting chairs, and potted plant in the corner. 19. classroom with six student desks, each with chair. teachers desk sits at the front near the chalkboard, which hangs on the wall. 20. library with shelves containing books, reading tables with chairs, and commercial printer. 21. computer lab with six desks, each with monitor, keyboard, and chair. printer sits in the corner. 22. An art studio with easels, stools, and supply cabinet in the corner. 23. music room with piano, drum set, and music stands with chairs. 24. medical exam room with an exam table, doctors stool, and cabinet with medical supplies and blood pressure monitor. 25. hospital room with hospital bed, an IV stand, bedside table, and visitor chair. 26. dental office with dental chair, dentists stool, dental light overhead, and cabinet with dental tools. 27. waiting room with chairs along the walls, coffee table with magazines, and water dispenser in the corner. 28. physical therapy room with treatment table, exercise mats on the floor, dumbbells, and full-length mirror on the wall. 29. gym with two treadmills and two exercise bikes along the wall. weight rack holds dumbbells. squat rack has five 45-pound weight plates stacked next to it. Exercise balls are in the corner, and large mirror covers one wall. 30. yoga studio with yoga mats on the floor, mirror on the wall, and meditation cushions in the corner. 31. dance studio with ballet barre along the wall, large mirror, and speaker in the corner. 32. spa treatment room with massage table, cabinet with towels, and candles on round side table. 33. robotics lab with two workbenches with robotic arms. Tool shelves on the wall hold tools, and 3D printer sits in the corner. 34. workshop with wooden workbench, stool, and model airplane on the workbench. tool box contains tools including drill, hammer, screwdriver, and tape. 35. garage with car, workbench, tires stacked in the corner, and ladder against the wall. 36. server room with three server racks, fire extinguisher in the corner, and desk with two monitors. 37. An art gallery with at least five paintings on the walls, sculpture on pedestal in the center, and bench for viewing. 38. commercial kitchen with large stainless steel prep table, an industrial stove, commercial refrigerator, and shelves with pots and pans. utensil crock holds utensils including spatula, tongs, and whisk. cutting board with chefs knife on top sits on the table. 39. bakery with display case showing pastries, counter with cash register, and bread rack with loaves of bread. 40. nail salon with two manicure tables, each with chair and stool, and shelf with nail polish bottles. 41. barber shop with two barber chairs facing mirrors on the wall, and waiting bench near the entrance. 42. tattoo parlor with tattoo chair, rolling cart with ink bottles, and framed artwork on the walls. 43. pet grooming salon with grooming table, bathtub, and cage for pets in the corner. 44. laundromat with four washing machines along the wall, two dryers, folding table, and row of plastic chairs. 45. nursery with crib, rocking chair, changing table, baby mobile hanging from the ceiling, and pile of wooden play blocks on the floor. 46. daycare room with small tables and chairs, storage box with toys in the corner, and colorful bins on shelf. 47. wine cellar with wooden shelves along the walls holding at least ten wine bottles. tasting table with four stools sits in the center, with four wine glasses and plate with cheese on it. barrel is in the corner, and an antique chandelier hangs from the ceiling. 48. science lab with two lab benches, each with microscope, glass beaker, and test tube stand. fume hood hangs from the ceiling, 47 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes and safety shower is in the corner. 49. An Egyptian museum room with sarcophagus in the center. One display table holds golden scarab beetle and canopic jar. Another display table has an ancient scroll, clay tablet, and bronze amulet. Hieroglyphic panels are mounted on the walls, and two stone statues flank the entrance. 50. An escape room with locked chest, bookshelf with books, clock on the wall, and mysterious painting. O.3. Object Density (25 prompts) 1. pantry with single three-level shelf. The first level holds at least 10 cans, the second level holds at least 10 pasta packs, and the third level holds at least 10 potatoes. 2. kitchen with table. There is fruit bowl on the table with mix of apples and oranges. The bowl contains at least 6 fruits. 3. kitchen with single shelf. The shelf holds at least 8 plates, 8 bowls, and 8 cups. 4. kitchen with spice rack on the wall. The rack holds at least 10 spice jars. 5. kitchen with an open refrigerator. The refrigerator contains at least 6 bottles and 6 food containers. 6. kitchen. There is utensil holder on the counter. The holder contains at least 4 forks, 4 knives, and 4 spoons. 7. dining room with table set for 4 people. Each setting has plate, glass, fork, and knife. 8. dining room with table set for 8 people. Each setting has plate, glass, fork, and knife. 9. dining room with long table set for 12 people. Each setting has small plate stacked on large plate, glass, fork, and knife. 10. kitchen with mug rack on the wall holding at least 8 mugs. 11. bedroom with breakfast tray on the bed. The tray has plate, cup, bowl, and glass. 12. living room with coffee table. On the table is tea set with teapot, 4 teacups on saucers. 13. bar with counter. On the counter are at least 8 glasses and 6 bottles. 14. dining room with wine shelf holding at least 12 wine bottles. 15. pottery store with shelves along the walls. The shelves hold at least 30 cups and 30 bowls. There is table with large painted bowl. 16. bathroom with counter. On the counter are at least 4 bottles, 2 tubes, and soap dispenser. 17. bathroom with medicine cabinet. The cabinet contains at least 6 medicine bottles, 4 pill boxes, 2 tubes, and pair of scissors. 18. kitchen with three-level shelf. The top level holds at least 10 jars. The other two levels hold stacks of plates with at least 20 plates in total. 19. closet with shoe rack holding exactly 5 pairs of shoes. 20. bedroom with vanity table. On the table are at least 6 makeup bottles, at least 2 brushes, and small mirror. There is also nightstand with stack of 8 books. 21. An office with desk. On the desk is pen holder with at least 5 pens and laptop. There is shelf with stacks of books, at least 15 books in total. 22. kitchen with sink. The sink contains mess of dishes with at least 5 plates, 3 glasses, and 5 bowls. 23. playroom with two bins. One bin contains 2 teddy bears. The other bin contains at least 5 wooden blocks. There is carpet with toy car and toy train. There is pile of lego blocks on the carpet containing at least 5 blocks. 24. restaurant with exactly 4 tables and 8 chairs. Each table is set for 2 people with plate, glass, fork, and knife. 25. bookstore with at least 50 books. O.4. Themed Scenes (10 prompts) 1. Star Wars themed teen bedroom. The room features bed with Star Wars bedding showing characters and starships, and Star Wars themed desk lamp with Darth Vader or Death Star designs. 2. playroom inspired by The Incredibles movie. The room features an Incredibles themed rug with the superhero family logo and two Incredibles themed bean bags for seating. 3. An Art Deco styled living room. The room features an Art Deco style sofa with geometric patterns, an Art Deco style coffee table positioned in front of the sofa, and an Art Deco style floor lamp with geometric shade. 4. Minecraft themed gaming room. The room features Minecraft themed chair with Creeper designs and Minecraft themed poster on the wall. 5. Picasso-inspired dining room. The room features dining table with four chairs surrounding it. Picasso-style cubist painting hangs on the wall, and tablecloth with abstract geometric patterns covers the table. 6. steampunk styled home office. The room features steampunk desk with brass fittings and exposed gears, steampunk style desk lamp with copper pipes and vintage bulb, steampunk themed wall clock with visible clockwork mechanisms, and steampunk styled office chair with leather and metal accents. 7. Harry Potter themed home library. The room features bookshelf styled like Hogwarts library, Harry Potter themed reading chair with Harry Potter themed floor lamp next to it, and Hogwarts trunk for storage. 8. Pop Art styled artist studio. The room features Pop Art style desk with bold colors, Pop Art style stool positioned in front of the desk, large Pop Art style painting with comic-style imagery on the wall, and Pop Art themed floor lamp. 9. Jurassic Park themed kids bedroom. The room features bed with Jurassic Park dinosaur bedding, Jurassic Park themed rug on the floor, and Jurassic Park poster on the wall. Two dinosaur stuffed animals sit on the bed, and dinosaur themed desk lamp rests on the nightstand. 10. Frozen themed nursery. The room features Frozen themed crib with Elsa and Anna bedding, Frozen themed rocking chair, Frozen themed rug with snowflake patterns on the floor, Frozen themed lamp on side table, Frozen poster on the wall, and Frozen themed toy box for storage. 48 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes O.5. SceneEval-100 (House) (Tam et al., 2025) (6 prompts) 1. medium-sized bedroom featuring queen-size bed with nightstand on each side. TV is mounted on the wall directly across from the bed, providing convenient viewing. The room includes walk-in closet equipped with three wardrobes for clothes and two large mirrors. 2. master bedroom with king-size bed, an attached walk-in closet, and master bathroom. desk is positioned in one corner of the room for working, there is laptop on the right side of the desk and wallet on the left side. There are also two sofa chairs next to the window. There is sink mounted in the bathroom with soap on the left side. 3. The space includes three rooms: spacious living room with an L-shaped sectional sofa facing wall-mounted TV and rectangular coffee table in front, holding small serving tray with napkins. The dining room features an extending dining table surrounded by six chairs, sleek sideboard with cups and sodas on top, two wall sconces providing soft lighting, and small bar cart loaded with bottles in the corner. The modern kitchen boasts central bar table with four bar stools on the long side, bar table holding charcuterie board with plates beside it, and wine cooler in the corner. 4. This is modern bedroom with spa-like bathroom. The bedroom has queen-size bed with vibrant pillows. sleek dresser sits against one wall, adorned with small jewelry box and decorative mirror. reading corner includes an armchair and small side table. Adjacent to the bedroom, the bathroom features large bathtub next to frosted window. vanity opposite the tub holds two sinks with skincare products. 5. This cozy bedroom features full-size bed against wall with two nightstands on each side where the left one has small clock. small desk sits in the corner with comfortable chair for studying or working. Opposite the bed, spacious dresser provides additional storage. Adjacent to the bedroom, the living room has recliner and an ottoman facing low coffee table with small vase and magazines. large bookshelf in the corner holds books and board games, while wide couch provides space for gatherings. Just off the living room, small gaming room with gaming console and two beanbag chairs offers dedicated space for entertainment, complete with small shelf for controllers and headsets. 6. This bedroom has full-size bed against wall with cartoon painting above. Two nightstands on each side of the bed provide storage. writing desk in the corner holds modern lamp for task lighting. large wardrobe stands next to the door for additional storage. Beside the bedroom is cozy living room with large sofa facing sleek coffee table where puzzles and decorative coasters reside. bookshelf in the corner displays books and family pictures. The stylish bathroom includes bathtub, with toilet in the corner. The vanity features double sink, with toiletries and white candles on top. wicker basket at the foot of the bathtub holds towels. O.6. House-Level (25 prompts) 1. compact studio apartment with single open-plan room and bathroom. The studio contains bed against the wall, sofa, and desk with chair. The bathroom has toilet, sink, and blue bathtub. 2. hotel room with bedroom and an en-suite bathroom. The bedroom has bed with two nightstands on each side, desk with chair, and TV mounted on the wall. The bathroom has toilet, sink, and shower. 3. one-bedroom apartment with bedroom, living room, and bathroom. The bedroom has bed and wardrobe. The living room has sofa, coffee table, and TV on TV stand. The bathroom has toilet, sink, and bathtub. 4. cozy guest cottage with bedroom, sitting area, and bathroom. The bedroom has bed with nightstand and lamp on top. The sitting area has two armchairs and wooden bookshelf full of books. skull sits on the bookshelf. The bathroom has toilet and sink. 5. small office suite with reception, private office, and bathroom. The reception has reception desk with computer on it and two chairs. The private office has an office desk with chair and filing cabinet. The bathroom has toilet and sink. 6. massage parlor with treatment room, waiting area, and bathroom. The treatment room has massage table and shelf with bottles. The waiting area has sofa and coffee table with books on it. The bathroom has toilet and sink. 7. tiny house with bedroom, living-kitchen, and bathroom. The bedroom has bed and wooden dresser. The living-kitchen has sofa, dining table with two chairs, and refrigerator. The bathroom has toilet and sink. 8. hair salon with salon floor, waiting area, storage room, and bathroom. The salon floor has two salon chairs positioned in front of two mirrors on the wall, facing them. The waiting area has sofa and coffee table. The storage room has shelves with bottles. The bathroom has toilet and sink. 9. dorm suite with bedroom, study room, common room, and bathroom. The bedroom has bed and wardrobe. The study room has desk with chair and bookshelf with books. The common room has refrigerator and microwave on counter. The bathroom has toilet and sink. 10. small retail shop with showroom, fitting room, storage room, and bathroom. The showroom has display table with clothes on it and cash register on counter. The fitting room has mirror and bench. The storage room has shelves with boxes. The bathroom has toilet and sink. 11. two-bedroom apartment with master bedroom, second bedroom, living room, hallway, and bathroom. The master bedroom has bed and nightstand. The second bedroom has bed and desk with chair. The living room has sofa, coffee table, and TV. The hallway has coat rack. The bathroom has toilet, sink, and bathtub. 12. boutique hotel suite with bedroom, living area, walk-in closet, dressing room, and bathroom. The bedroom has king-size bed with two nightstands. The living area has sofa and coffee table. The walk-in closet has wardrobe and luggage rack. The dressing room has vanity with mirror. The bathroom has toilet, sink, and bathtub. 13. small restaurant with dining area, kitchen, bar, storage room, and bathroom. The dining area has four dining tables, each with two chairs. The kitchen has stove and refrigerator. The bar has bar counter with three bar stools. The storage room has shelves with supplies including plates and cups. The bathroom has toilet and sink. 14. small family home with master bedroom, kids room, living-dining room, kitchen, hallway, and bathroom. The master 49 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes bedroom has bed and dresser. The kids room has bunk bed and toy box. The living-dining room has sofa, dining table with four chairs, and TV. The kitchen has refrigerator and stove. The hallway has coat rack and shoe cabinet. The bathroom has toilet, sink, and bathtub. 15. dental office with reception-waiting area, two exam rooms, an X-ray room, corridor, and bathroom. The reception-waiting area has reception desk with computer and three chairs. The first exam room has dental chair and cabinet. The second exam room has dental chair and sink. The X-ray room has an X-ray machine. The corridor has bench. The bathroom has toilet and sink. 16. An art gallery with three exhibition rooms, an office, storage room, and bathroom. The first exhibition room has two paintings on the wall and bench. The second exhibition room has three sculptures on pedestals. The third exhibition room has painting on the wall with bench in front for viewing. The office has desk with chair and computer. The storage room has shelves with crates. The bathroom has toilet and sink. 17. co-working space with an open office, two meeting rooms, lounge, kitchen, reception, and bathroom. The open office has four desks with chairs and computers. The first meeting room has conference table with six chairs. The second meeting room has whiteboard and four chairs. The lounge has two sofas and coffee table. The kitchen has refrigerator and microwave on counter. The reception has reception desk with computer. The bathroom has toilet and sink. 18. photography studio with shooting area, an editing room, client lounge, dressing room, props storage, an office, and bathroom. The shooting area has two studio lights and backdrop. The editing room has desk with two monitors and chair. The client lounge has sofa and coffee table. The dressing room has vanity with mirror on top and clothing rack. The props storage has shelves with props including hats and bags. The office has desk with chair and filing cabinet. The bathroom has toilet and sink. 19. small fitness studio with workout area, yoga room, locker room, reception, an office, storage room, and bathroom. The workout area has treadmill, weight rack with dumbbells, and squat rack with stack of weight plates next to it. The yoga room has yoga mats and mirror on the wall. The locker room has lockers and bench. The reception has reception desk with computer. The office has desk with chair. The storage room has shelves with towels. The bathroom has toilet and sink. 20. pet clinic with reception, waiting room, two exam rooms, surgery room, corridor, and bathroom. The reception has reception desk with computer. The waiting room has four chairs and water dispenser. The first exam room has an exam table and cabinet. The second exam room has an exam table and scale. The surgery room has surgery table and surgical light. The corridor has bench. The bathroom has toilet and sink. 21. large family home with master bedroom, two kids bedrooms, living room, kitchen, dining room, hallway, and two bathrooms. The master bedroom has bed with two nightstands and dresser. The first kids bedroom has bed and desk with chair. The second kids bedroom has bunk bed and bookshelf with books. The living room has sofa, coffee table, and TV. The kitchen has refrigerator, stove, and dining table with four chairs. The dining room has dining table with six chairs and sideboard. The hallway has coat rack and mirror on the wall. The first bathroom has toilet, sink, and bathtub. The second bathroom has toilet and sink. 22. medical clinic with reception, waiting room, four exam rooms, lab, pharmacy, corridor, and two bathrooms. The reception has reception desk with computer. The waiting room has six chairs and water dispenser. The first exam room has an exam table and cabinet. The second exam room has an exam table and scale. The third exam room has an exam table and desk with chair. The fourth exam room has an exam table and stool. The lab has lab bench and microscope. The pharmacy has shelves with bottles. The corridor has bench. The first bathroom has toilet and sink. The second bathroom has toilet and sink. 23. small office building with reception, five offices, two conference rooms, break room, server room, corridor, and bathroom. The reception has reception desk with computer and two chairs. The first office has desk with chair and computer. The second office has desk with chair and filing cabinet. The third office has desk with chair and bookshelf. The fourth office has desk with chair and printer. The fifth office has desk with chair and whiteboard. The first conference room has conference table with eight chairs. The second conference room has conference table with six chairs, projector, and whiteboard on the wall. The break room has refrigerator, microwave on counter, and table with four chairs. The server room has two server racks. The corridor has water dispenser. The bathroom has toilet and sink. 24. community center with main hall, four activity rooms, kitchen, an office, reception, locker room, hallway, two bathrooms, and storage room. The main hall has stage and rows of chairs. The first activity room has tables and chairs for crafts. The second activity room has yoga mats and mirror on the wall. The third activity room has ping pong table. The fourth activity room has easels and stools for art class. The kitchen has refrigerator, stove, and counter. The office has desk with chair and computer. The reception has desk with computer. The locker room has lockers and bench. The hallway has bulletin board on the wall. The first bathroom has toilet and sink. The second bathroom has toilet and sink. The storage room has shelves with boxes. 25. boutique hotel with reception, four guest bedrooms with en-suite bathrooms, restaurant, kitchen, gym, lounge, and corridor. The reception has desk with computer and two armchairs. The first guest bedroom has bed with two nightstands and its bathroom has toilet, sink, and shower. The second guest bedroom has bed and desk and its bathroom has toilet, sink, and bathtub. The third guest bedroom has bed and an armchair and its bathroom has toilet, sink, and shower. The fourth guest bedroom has bed and wardrobe and its bathroom has toilet, sink, and bathtub. The restaurant has four dining tables with chairs. The kitchen has refrigerator, stove, and counter. The gym has treadmill and weight rack with dumbbells. The lounge has two sofas and coffee table. The corridor has console table with vase on top. P. User Study Interface Screenshots This section provides screenshots of the user study interface, illustrating the viewing modes, interactive features, and question format. 50 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 19. User study interface: 3D viewer mode. Top: Side-by-side comparison with interactive Babylon.js 3D viewers. Participants can orbit, pan, and zoom each scene independently. Controls include Hide Selected, Show All, Reset, and Fullscreen. Bottom: Fullscreen mode with object selection. Clicking an object highlights it and enables the Frame button, which focuses the camera on the selected object and zooms in. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 20. User study interface: Alternative viewing modes for participants who prefer static images or have slower devices. Top: Click-Through mode with carousel navigation through 9 pre-rendered camera angles. Bottom: Grid View mode displaying all angles simultaneously for quick comparison. 52 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 21. User study questions. Top: The two evaluation questions. Q1 (Realism) is forced-choice between Scene and B; Q2 (Faithfulness) includes an Equal option. Bottom: Confirmation dialog shown when participants select Equal, encouraging them to reconsider and only confirm if they genuinely cannot distinguish which scene better matches the prompt. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 22. User study tutorial tooltips (selection). The interface includes an interactive onboarding sequence with contextual tooltips. Shown here: display mode switching (3D View, Click-Through, Grid View), the help button for 3D viewer controls, and the Hide Selected/Show All functionality for inspecting occluded objects or verifying object composition. 54 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Q. Additional Qualitative Results Q.1. Baseline Comparisons Figure 23 compares room connectivity between SceneSmith and Holodeck for hotel scene. Figures 24 and 25 provide qualitative comparisons between SceneSmith and baseline methods for house-level and room-level prompts respectively. Q.2. Additional SceneSmith Examples Figures 2630 show additional scenes generated by SceneSmith across diverse room types and complexity levels. Q.3. Manipuland and Themed Generation Figure 31 demonstrates the manipuland agents ability to populate furniture with contextually appropriate objects. Figure 32 shows SceneSmiths capability to generate rooms matching highly specific visual themes. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 23. Room connectivity comparison for hotel scene. Both SceneSmith (top) and Holodeck (bottom) contain all the requested rooms (House-level prompt 25; Appendix O.6): reception (a), lounge (b), restaurant (c), kitchen (d), corridor (e), gym (f), four guest rooms (g, i, k, m), and four bathrooms (h, j, l, n). Cyan lines show room connectivity paths. SceneSmith generates realistic floor plans where the entrance leads through the reception, en suite bathrooms are only accessible through associated guest rooms, the kitchen is connected to the restaurant, and all rooms connect via central corridor. Holodeck produces implausible layouts where rooms may only be reachable through other guest rooms or bathrooms. For example, the kitchen (d) is only reachable through bathroom (j), and guest room (m) is only reachable by traversing through guest rooms (i) and (k) and bathroom (l). 56 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 24. House-level qualitative comparison. SceneSmith (top) vs Holodeck (bottom) for multi-room prompt. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 25. Room-level qualitative comparison. SceneSmith compared to six baselines across four diverse prompts: bunk bed bedroom, Japanese-style living room, grocery store, and nail salon. Baselines are ordered by their user study realism score. SceneSmith generates scenes with appropriate object density, realistic arrangements, and faithful prompt following. 58 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 26. House-level generation examples. Two multi-room scenes generated by SceneSmith: dental office with reception area, exam rooms, X-ray room, and bathroom; and boutique hotel suite with bedroom, living area, walk-in closet, dressing room, and bathroom. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 27. Additional room-level generation examples. Each panel shows text prompt and the corresponding scene generated by SceneSmith. 60 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 28. Additional room-level generation examples. Each panel shows text prompt and the corresponding scene generated by SceneSmith. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 29. Additional room-level generation examples. Each panel shows text prompt and the corresponding scene generated by SceneSmith. 62 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 30. Additional room-level generation examples. Each panel shows text prompt and the corresponding scene generated by SceneSmith. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 31. Manipuland placement examples. Diverse furniture pieces populated with contextually appropriate manipulands: pottery store display table, produce stand with fruits, store shelf with products, diner table with menus, bar cart with bottles and glasses, library table with books, and gaming desk setup. Each object is separate simulation asset that can be individually manipulated (e.g., each book in the stacks). 64 SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes Figure 32. Themed room generation. SceneSmith generates rooms with specific visual themes: Jurassic Park kids bedroom, Art Deco living room, steampunk home office, Star Wars teen bedroom, Frozen nursery, and Incredibles playroom. Generated assets and retrieved materials match the requested themes and styles."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Toyota Research Institute"
    ]
}