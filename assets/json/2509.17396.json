{
    "paper_title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering",
    "authors": [
        "Minsoo Kim",
        "Arnav Kundu",
        "Han-Byul Kim",
        "Richa Dixit",
        "Minsik Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 6 9 3 7 1 . 9 0 5 2 : r EPICACHE: EPISODIC KV CACHE MANAGEMENT FOR LONG CONVERSATIONAL QUESTION ANSWERING Minsoo Kim1,2, Arnav Kundu1, Han-Byul Kim1, Richa Dixit1, Minsik Cho1 1Apple, 2Hanyang University minsoo2333@hanyang.ac.kr, {a kundu,hanbyul,r dixit,minsik}@apple.com Work done during an internship at Apple."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to single query, leading to degraded accuracy in multi-turn conversations. We introduce EPICACHE, training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EPICACHE bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layers sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EPICACHE improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 46 compression, and reduces latency and memory by up to 2.4 and 3.5, thereby enabling efficient multi-turn interaction under strict resource constraints."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) (Brown et al., 2020; Yang et al., 2025; Touvron et al., 2023; Jiang et al., 2023) have significantly extended their context lengths, with LLM-based chat assistants now capable of processing hundreds of thousands to millions of tokens (Reid et al., 2024; Meta, 2025). This capability enables assistants to leverage extensive dialogue histories when generating responses, producing personalized and contextually coherent outputs (OpenAI, 2024; Anthropic, 2024), which are central requirements for real-world conversational AI applications. Long Conversational Question Answering (LongConvQA) is the task of answering sequence of user questions grounded in extended interaction histories, where sessions span hundreds of turns and multi-session interactions unfold over days or weeks. Recent work has formalized LongConvQA in both human-human conversations and user-AI assistant interactions (Maharana et al., 2024; Lee et al., 2025; Wu et al., 2025a). While most current systems rely on external memory modules (Zhong et al., 2024; Chhikara et al., 2025), the challenge of sustaining such long contexts under strict memory budgets remains largely unaddressed. To bridge this gap, we study how to enable LongConvQA under constrained memory by designing Key-Value (KV) cache control framework. The KV cache stores the Key and Value states of each token for reuse in auto-regressive generation, but its size grows linearly with context length, creating severe challenges in extended conversations. For instance, in multi-day dialogues between user and assistant (Wu et al., 2025a), the KV cache of LLaMA3.2-3B exceeds 7GB after only 30 sessionslarger than the size of the model parameters. This underscores the importance of cache management for deploying conversational AI system under resource-constrained environments. 1 Prior work has attempted to mitigate the growing memory footprint of KV caches in long conversations through various compression techniques (Zhang et al., 2023; Li et al., 2024; Cai et al., 2025). Yet two major limitations remain under memory-constrained settings. First, most methods apply compression after full context prefill (post-prefill), causing peak memory usage to scale linearly with input length. Second, query-dependent eviction (Li et al., 2024) optimizes for the current question but ties the cache closely to it, neglecting information needed for future queries and degrading accuracy in multi-turn conversations. We propose EPICACHE, training-free KV cache management framework that enforces constant memory footprint through block-wise prefill. After each block, less critical KV entries are evicted, ensuring memory consumption remains bounded. Beyond block prefill, EPICACHE incorporates an episodic clustering method inspired by conversation segmentation studies (Joty et al., 2013; Galley et al., 2003). Specifically, we apply semantic clustering to group conversation history into coherent episodes, and perform episodic KV cache compression, yielding topic-specific caches within the fixed budget. Finally, we find that LLMs exhibit different sensitivities to block prefill eviction across layers. Building on this observation, we propose an adaptive layer-wise budget allocation strategy that distributes KV cache budget proportionally to each layers sensitivity. Together with episodic eviction, this enables EPICACHE to preserve long-range conversational context while operating under strict memory limits, yielding up to 40% higher scores than recent baselines and sustaining accuracy comparable to full KV under 46 cache compression. In addition, our block-wise cache control framework reduces peak memory usage by up to 3.5, while cache eviction accelerates decoding, cutting latency by as much as 2.4 compared to full KV."
        },
        {
            "title": "2 BACKGROUND",
            "content": "We begin by formalizing Long Conversational Qustion-Answering (LongConvQA) in Section 2.1. We then discuss memory-constrained KV cache management in Section 2.2, comparing postand block-prefill eviction and discussing the resulting accuracy-memory trade-offs. Finally, in Section 2.3, we review attention-guided cache compression with patched prompts and present analyses that motivate our method, EPICACHE."
        },
        {
            "title": "2.1 LONG CONVERSATIONAL QA FORMULATION",
            "content": "We formalize LongConvQA as answering sequence of user queries = {q1, . . . , qNq }, with Nq denoting the total number of queries, conditioned on long conversational history (Maharana et al., 2024; Lee et al., 2025; Wu et al., 2025a). Let the dialogue history be represented as an ordered sequence of Nu utterances where each utterance uj pairs role rj with text tj: = {u1, u2, . . . , uNu }, uj = (rj, tj), rj {speaker1, speaker2}, (1) Given long conversation H, an LLM encodes it into Key-Value (KV) cache KVH. For layers and KV heads, encoding tokens produces KV entries, growing linearly with the conversation length. In this work, we focus on token-level cache compression, where KV entries of less important tokens are evicted; the resulting compressed cache is denoted as (cid:103)KVH KVH, and we use the terms compression and eviction interchangeably. Our goal is to generate accurate answers for all queries q1, . . . , qNq grounded in the dialogue history H, using compressed cache (cid:103)KVH that satisfies memory budget , i.e., with size , while preserving answers comparable to full KV cache (KVH) based generation: fLM(qi (cid:103)KVH) fLM(qi KVH), = 1, . . . , Nq. (2) This formulation serves as an evaluation of multi-turn conversational accuracy, where multiple query-answer pairs are grounded in the same dialogue history."
        },
        {
            "title": "2.2 KV CACHE MANAGEMENT: POST PREFILL VS BLOCK PREFILL",
            "content": "Most existing KV compression approaches reduce cache size in the decoding stage by performing eviction after the full context has been prefilled, i.e., Post Prefill Eviction (Li et al., 2024; Feng et al., 2 (a) Post Prefill Eviction (b) Block Prefill Eviction (c) Top: Peak GPU Memory Bottom: LongConvQA Score Figure 1: KV Cache Management Analysis. (a) Post prefill eviction: eviction after full-context prefill, reducing KV size at decoding but causing unbounded memory usage. (b) Block prefill eviction: input processed in 3-token blocks with patched prompts for scoring, then evicted to 1 token. (c) Top: Peak GPU memory vs. input length on LLaMA-3.2-3B with A100. Bottom: LongConvQA accuracy of KV compression methods under post vs. block prefill on LLaMA-3.2-3B. 2024; Cai et al., 2025; Kim et al., 2025). As shown in Figure 1a, this design causes peak memory usage to grow linearly with input length, since the entire context must be cached before any eviction takes place. With optimized attention kernels (Dao, 2024), the prefill stage remains unbounded in memory demand, as observed in Figure 1c top. To bound memory growth, Block Prefill Eviction (Kim et al., 2024; Corallo & Papotti, 2024; Park et al., 2025) processes the input in block-wise way, handling one segment at time under fixed budget . Each step adds Mblock tokens, after which eviction reduces KV cache entries back to . For example, in Figure 1b, the budget is = 1, and each block adds Mblock = 3 tokens that are evicted to . This design ensures the cache never exceeds + Mblock, keeping peak GPU memory usage flat with input length as highlighted in Figure 1c top. However, this bounded memory comes with steep accuracy trade-off: when the state-of-the-art post prefill eviction method KVzip, (Kim et al., 2025; NVIDIA, 2025) is applied in the block prefill setting, LongConvQA scores (Maharana et al., 2024) degrade sharply across all budget levels. This underscores central challengewhile block prefill guarantees constant memory usage, adapting post-prefill eviction methods to this setting severely undermines answer quality in LongConvQA."
        },
        {
            "title": "2.3 ATTENTION-GUIDED KV CACHE COMPRESSION",
            "content": "To address the accuracy degradation of block prefill eviction, prior work employs attention-based token scoring with patched prompt. Here, token importance is quantified by the cross-attention it receives from query tokens: Attn(xt xi) which denotes the attention weight from query token xt to key token xi. Tokens that receive higher attention from queries are considered more important, while those with lower scores are evicted to satisfy the memory budget . To provide guidance for cache eviction, the patched prompt strategy (Kim et al., 2024; Bhaskar et al., 2025) appends an auxiliary prompt of length after each block ending at token n. These queries xn+1, . . . , xn+p attend back to the preceding block tokens x1, . . . , xn, as illustrated in Figure 1b. The resulting importance score si of token is aggregated either by averaging or by taking the maximum, as defined in Equation (3) (the patched prompt is only used for scoring and not retained in the KV cache; all experiments in this paper adopt the maximum aggregation). savg = 1 n+p (cid:88) t=n+ Attn(xt xi), smax = max t[n+1,n+p] Attn(xt xi). (3) In block prefill, accuracy is highly dependent on the content of the patched prompt. To study this, Figure 2 presents controlled experiment where we assume oracle access to the future user query, with inserting it as the patched prompt yielding the highest accuracy (Exact-Question)1. Since the dialogue history in Equation (1) consists of question-answer turns, it offers an opportunity to approximate the future query with semantically related utterances. To test this idea, we embed both user queries q1, . . . , qNq and conversation utterances u1, . . . , uNu into shared embedding space, compute semantic similarity scores, and construct patched prompts using the top-ranked segments. As expected, the results confirm that conversation utterances semantically aligned with the future question can serve as effective proxies, with the closest 10% achieving accuracy nearly matching the Exact-Question case in Figure 2. Accuracy then declines as the selected segments become less related, showing that the degree of semantic alignment directly determines the quality of answers generated from the compressed KV cache. These findings narrow our objective: during block prefill eviction, the central challenge is to identify patched prompt that best approximates unseen future questions. To this end, we employ unsupervised methods to discover dialogue segments aligned with future queries, motivating the clustering-based approach that will be introduced in Section 3.1."
        },
        {
            "title": "3 METHOD",
            "content": "Figure 2: Patched-prompt analysis: LoCoMo results with LLaMA3.1-8B under block prefill. Patched prompts are formed by selecting the top 10%90% similar conversation utterances to qi."
        },
        {
            "title": "3.1 EPISODIC KV CACHE MANAGEMENT WITH CONVERSATION CLUSTERING",
            "content": "EPICACHE consists of three stages as illustrated in Figure 3. Stage 1 (Clustering) is performed offline, where the conversation history is clustered into topical episodes, and representative segment is identified for each episode. Stage 2 (Prefilling) leverages representative segments as patched prompt to guide block prefill eviction, building episode-specific KV cache while keeping GPU memory bounded. Stage 3 (Decoding) embeds the incoming query, matches it to the most relevant episode, and retrieves the corresponding episodic KV cache for answer generation. Stage 1. Conversation Clustering and Selecting Medoids. For clustering conversation, we first divide the dialogue history into segments of wembed utterances, denoted as = S1, . . . , SK. Sk = u(k1)wembed+1, . . . , umin(kwembed,Nu), = 1, . . . , K, = (cid:109) (cid:108) Nu wembed (4) Each segment Sk is encoded with sentence encoders (Reimers & Gurevych, 2019) fembed into semantic vector representation ek Rd, which captures the conversational content of the segment. We then apply K-Means clustering C() to the embeddings {ek}K k=1, as illustrated in Figure 3a: C({ek}K k=1) {E1, . . . , EE}, (cid:91) e=1 Ee = {e1, . . . , eK}. (5) This procedure partitions into semantically coherent topical episodes (Raedt et al., 2024). Examples of clustering results and episode-level utterance samples are shown in Figure A3. For each cluster Ee, we can compute its centroid embedding: 1 Ee medoid = arg max SkEe ek, S(e) ce = (cid:88) SkEe cos(ek, ce). (6) We then identify the medoid segmenti.e., the conversation segment in each clusters whose embedding is most similar to the centroid in terms of semantic similarity. The medoid segment, as the representative of the cluster, contains multiple turns from both speakers and is used as the patched prompt in the subsequent block prefill eviction step. 1This strategy is infeasible in LongConvQA since (i) the future queries are unknown at compression time, and (ii) each new query requires new prefill and compression step (Kim et al., 2025). 4 (a) Conversation Clustering (b) Build Episodic KV Cache (c) Episodic KVs based Decoding Figure 3: EpiCache Overview. (a) offline segmentation and embedding of the conversation, fol- (b) Building episodic KV caches under fixed GPU lowed by clustering into topical episodes. memory usage based on representative segments of each cluster. (c) an incoming query is embedded, matched to the closest episode, and the corresponding cache is retrieved for answer generation. Stage 2. Episodic KV Cache Compression. As discussed in Section 2.3, patched prompts guide cache eviction toward retaining tokens relevant to the prompt content. Building on this insight, EPICACHE uses the medoid segment of each episode as the patched prompt, enabling it to collect episode-specific KV entries under the memory budget. For each episode {1, . . . , E}, we perform block prefill eviction over entire context, appending its patched prompt Pe after each block (Figure 3b). Attention scores are then computed with Pe, and the top tokens are retained to form an episode-specific cache (e) KV. Finally, all episodic caches are collected into = {C (1) KV } and stored offline for later retrieval. KV, . . . , (E) Stage 3. Query-KVs Matching and Decoding. At decoding time, each user query qi is embedded with the same encoder fembed used in clustering, ensuring that it lies in the same representation space as the episode centroids. The query is then matched to the closest centroid as follows: qi = fembed(qi), = arg max e[1,E] cos(qi, ce). (7) As illustrated in Figure 3c, the framework retrieves the corresponding episodic cache (e) KV from and conditions generation on it: fLM(qi (e) KV , ). This design enables query-specific retrieval from compressed episodic caches while keeping cache size bounded under memory budget . This query-to-episode matching process incurs overhead from embedding, centroid matching, and cache retrieval, which will be analyzed in Section 4.4."
        },
        {
            "title": "3.2 SENSITIVITY-AWARE LAYER-WISE KV BUDGET ALLOCATION",
            "content": "We further address the accuracy degradation of block prefill by proposing KV cache budget allocation strategy. The key idea is to measure how much each layers Key state representation deviates under block prefill and to distribute KV budget across layers in proportion to this deviation. Simulating Block Prefill via Custom Masking. To quantify the deviation caused by block prefill eviction, we introduce custom masking scheme. Each transformer layer is represented as simplified function that takes the previous layers output (ℓ1) RN d, where is the sequence length and the hidden dimension, and produces the lth layer output ℓ under given mask M: ℓ = (cid:0)X ℓ1, M(cid:1) . While is the causal mask, we replace it with custom mask that uses budget , while attending sink tokens and the most recent tokens (Xiao et al., 2024). This simulates block prefill eviction in single forward pass, enabling direct measurement of its effect on layer representations. 5 (a) Key similarity across layers (b) KL divergence with Full KV (c) LongConvQA accuracy Figure 4: Layer-wise Sensitivity Analysis and KV Budget Allocation. (a) Key state cosine similarity across normalized layer positions. (b) KL divergence is measured between block prefill (M =4K) and full KV answer predictions, with uniform allocation as the baseline. Per-sample divergence shifts are shown when applying three allocation strategiessensitivity-aware, PyramidKV, and retrieval head profiling-on the Realtalk benchmark with Qwen2.5-7B. (c) LongMemEval accuracy comparison across allocation methods using Qwen2.5-7B. Layer Sensitivity Guided KV Budget Allocation. We quantify per-layer impact by comparing Key states produced under the causal mask and the custom mask M.2 For each layer ℓ, the forward pass under each mask produces: ℓ full = (cid:0)X ℓ1 full , M(cid:1) ℓ K, ℓ block = (cid:0)X ℓ1 block, M(cid:1) ℓ K, (8) full and ℓ where ℓ block are the l-th layer Key states computed under and M, respectively. We then define layer sensitivity as the average cosine similarity between the two sets of Key vectors across heads and tokens: σℓ = 1 HN (cid:88) (cid:88) h=1 i=1 cos(cid:0)k(ℓ,h) full,i , k(ℓ,h) block,i (cid:1), sℓ = 1 σℓ (9) Empirically, σℓ exhibits large variation across layers yet remains consistent across different inputs in Figure 4a (the shadowed regions denote input variance), indicating that sensitivity is modeldependent rather than input-dependent. We define sℓ as the sensitivity score for layer ℓ. Based on this observation, we propose sensitivity-aware budget allocation strategy that assigns larger cache budgets to layers more sensitive to block prefill and smaller budgets to less sensitive ones. Specifically, we redistribute the global budget (M L) according to layer sensitivity scores sℓ, with α controlling how sharply the allocation emphasizes sensitive layers:"
        },
        {
            "title": "M alloc",
            "content": "ℓ = α ℓ j=1 α (cid:80)L (L ), (cid:88) ℓ="
        },
        {
            "title": "M alloc",
            "content": "ℓ = M, (10) We evaluate this approach by measuring how much budget allocation shifts the KL divergence between block prefill and full KV cache answer predictions, where negative values indicate closer alignment to full KV cache answer generation. As shown in Figure 4b, sensitivity-aware allocation shifts KL divergence by 0.80 relative to uniform allocation. In contrast, prior allocation strategies such as PyramidKV (Cai et al., 2025), which follows pyramid-shaped budgeting, and retrieval head profiling based allocation (Wu et al., 2025b) tend to increase KL divergence under block prefill. This gap is directly reflected in task performance: as shown in Figure 4c, sensitivity-aware allocation consistently improves LongConvQA accuracy and complements episodic cache management framework, while other allocation strategies severely degrade accuracy. 2Further details regarding the rationale for using Key states deviation are provided in Section C.1. Figure 5: LongConvQA Evaluation Results (Realtalk, LoCoMo, and LongMemEval) results with fixed KV cache budget size-M across four LLMs. The number of episodes (clusters) fixed to E=4 in all experiments. The average full KV lengths of the three benchmarks are 26K, 21K, and 21K."
        },
        {
            "title": "4.1 SETUP",
            "content": "Models and Benchmarks. We evaluate on four pretrained LLMs: LLaMA-3.2-3B, LLaMA-3.18B (Grattafiori et al., 2024), Qwen2.5-3B, and Qwen2.5-7B (Qwen et al., 2025). All experiments follow the LongConvQA setup in Equation (2), where models answer various queries grounded in long conversation histories with compressed KV cache. We use three benchmarks: Realtalk (Lee et al., 2025) and LoCoMo (Maharana et al., 2024), containing multi-day human-human dialogues, and LongMemEval (Wu et al., 2025a), consisting of multi-session user-LLM conversations. Further details of LongConvQA benchmarks are provided in Section A.1. Baselines. We compare against comprehensive KV cache compression methods adapted to block prefill. StreamingLLM (Xiao et al., 2024) applies static retention of sink and recent tokens, SnapKV, InfiniPot, and KVzip (Li et al., 2024; Kim et al., 2024; 2025) use patched prompt based attention scoring, and KeyDiff (Park et al., 2025) scores tokens based on similarity among Key states and retains those with distinctive representations. Detailed baseline setting can be found in Section A.2. EPICACHE Setup. EPICACHE clusters the conversation history into episodic segments and applies patched-prompt KV cache compression, with per-layer budgets allocated by sensitivity measurements. We use E=4 episodes and Qwen3-0.6B (Zhang et al., 2025) as the embedding model. Layer sensitivities are profiled once on BookSum (Kryscinski et al., 2022) sample for each LLM and reused across all experiments, with the sharpness hyper-parameter set to α=1.1 for the LLaMA series and α=1.3 for the Qwen series. The overall process is detailed in Algorithm 1, while further setup specifications are provided in Section A.3. 7 Figure 6: Memory Scalability up to 100K Context. Conversation histories between user and LLMs scaled to 100K tokens across four LLMs with LongMemEval. Comparison of InfiniPot and KVzip (M =6K) with EPICACHE (4 episodes, =6K24K)."
        },
        {
            "title": "4.2 MAIN EVALUATION RESULTS",
            "content": "LongConvQA Evaluation. Figure 5 shows results on the three LongConvQA benchmarksRealtalk, LoCoMo, and LongMemEvalunder block prefill eviction with varying cache budget . StreamingLLM consistently shows the lowest accuracy, while patched-prompt methods (SnapKV, InfiniPot, KVzip) and Key states similarity-based KeyDiff suffer significant accuracy degradation compared to full KV. EPICACHE achieves significantly higher scores across all models and benchmakrs, exceeding baselines by around 20 scores under tightly compressed budgets (M =24K), and approaching full KV performance at budgets (M =6-8K). These consistent improvements across the three benchmarks highlight the effectiveness of episodic KV cache compression, along with sensitivity-aware budget allocation, in preserving conversational context under fixed memory constraints. Detailed results including sub-task accuracy are provided in Section D. Memory Scalability Evaluation. Figure 6 evaluates LongConvQA under extended conversation lengths, scaling up to 100K tokens.3 Open-source LLMs exhibit declining QA performance as context length grows to 100K, as reported in Wu et al. (2025a), and the performance gap between full KV and cache compression methods (KVzip, InfiniPot) becomes increasingly pronounced. EPICACHE delivers higher accuracy than baselines at the same memory budget across all context lengths, and as the KV cache budget increases, its accuracy steadily approaches full KV, demonstrating the memory scalability of our approach."
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "We conduct ablation studies to examine the design choices of EPICACHE. The detailed setups and results are provided in Section B, and we summarize the design aspects here: Alternative design: replacing episodic caches with RAG-like approach that directly inputs clustered conversation segments with the query. EPICACHE ablation studies: effect of block size Mblock; window size wembed; encoder choice fembed; number of episodes E; number of medoids for patched prompts."
        },
        {
            "title": "4.4 EFFICIENCY ANALYSIS",
            "content": "Figure 7a reports decoding latency breakdown and peak GPU memory usage, comparing full KV with EPICACHE under cache budgets of 2K-8K. Latency is evaluated per turn, consisting of answer decoding4 and, for EPICACHE, query embedding, centroid matching, and KV cache retrieval. With fewer KV entries stored, EPICACHE reduces decoding latency by up to 2.4 and peak GPU memory by 3.5 compared to full KV. These results demonstrate that EPICACHE achieves both faster decoding and substantially lower memory usage, as block prefill bounds peak memory to the fixed cache budget throughout prefill and decoding. 3LongMemEval (Wu et al., 2025a) supports stacking conversation sessions with associated QA pairs, allowing conversation histories to be constructed at custom lengths. 4For fair comparison, decoding latency is measured by generating up to 10 tokens per turn. 8 (a) Decoding latency breakdown and peak memory usage (b) Number of episode switch in Realtalk Figure 7: Efficiency Analysis in Multi-Turn Conversation: (a) Per-turn decoding latency and peak GPU memory for full KV (100K) and EPICACHE (E=4) with LLaMA-3.2-3B. Query Embed and Match: query encoding and centroid matching, KVs Retrieve: loading episodic cache from CPU to GPU memory. (b) Cumulative episode switches in Realtalk with E=4, showcasing how often episodes change across turns. Episode switching analysis in Figure 7b shows that retrieval overhead in EPICACHE arises only when the matched episode changes between turns. We measure this by clustering each conversation, embedding subsequent turns, and tracking how often the assigned episode switches. The cumulative switch counts grow sublinearly compared to the Worst Case line (episode switched every turn), indicating that consecutive turns remain within the same episode. This conversational persistence amortizes retrieval cost, and when incorporated into the per-turn latency breakdown of Figure 7a, the effective overhead remains under 5% while maintaining robust LongConvQA accuracy. Detailed measurements information are provided in Section D.1."
        },
        {
            "title": "5 RELATED WORK",
            "content": "KV Cache Compression. To mitigate the growing memory cost of KV caches, prior work either quantizes states into lower precision (Hooper et al., 2024; Liu et al., 2024b) or evicts less important tokens based on attention scores (Zhang et al., 2023; Li et al., 2024; Cai et al., 2025; Kim et al., 2025). As discussed in Section 2.2, most cache eviction methods follow the post-prefill approach, which requires full context prefilling and leads to unbounded peak memory. Block prefill methods such as InfiniPot (Kim et al., 2024), FINCH (Corallo & Papotti, 2024), and KeyDiff (Park et al., 2025) bound memory but suffer sharp accuracy drops in multi-turn conversation scenario. Retrieval-based Attention Another line of work improves decoding efficiency by selectively retrieving only the most relevant parts of the KV cache for each query token, thereby reducing the cost of attention computation. Quest (Tang et al., 2024) retrieves KV entries at the granularity of pages, while SqueezedAttention (Hooper et al., 2025) and ClusterKV (Liu et al., 2024a) clusters Key states and loads the cluster most relevant to the query. A2ATS (He et al., 2025) apply vector quantization to construct codebooks and restore Key states to determine which parts of the KV cache to retrieve. These methods share two key limitations. First, they operate in the post-prefill regime, assuming unbounded memory usage when building the retrieval index during prefill. Second, their retrieval unitspages, clusters, or codebooksdo not align with the episodic structure of conversations, limiting their applicability to LongConvQA under strict memory budgets."
        },
        {
            "title": "6 CONCLUSION",
            "content": "EPICACHE is framework that combines block-wise prefill with episodic clustering and sensitivityaware budget allocation to preserve topic-relevant context under fixed memory. Across multiple LongConvQA benchmarks, EPICACHE substantially outperforms existing compression methods, showing that efficient multi-turn interaction is feasible even under strict resource constraints and marking practical step toward memory-efficient conversational AI."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298/. Anthropic. Introducing the next generation of claude. https://www.anthropic.com/ news/claude-3-family, 2024. David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 07, pp. 10271035, USA, 2007. Society for Industrial and Applied Mathematics. ISBN 9780898716245. Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, and Danqi Chen. Cache me if you can: How many kvs do you need for effective long-context lms?, 2025. URL https://arxiv. org/abs/2506.17121. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2025. URL https://arxiv.org/abs/2406.02069. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Giulio Corallo and Paolo Papotti. FINCH: Prompt-guided key-value cache compression for large language models. Transactions of the Association for Computational Linguistics, 12:15171532, 2024. doi: 10.1162/tacl 00716. URL https://aclanthology.org/2024.tacl-1. 83/. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 562569, Sapporo, Japan, July 2003. Association for Computational Linguistics. doi: 10.3115/1075096.1075167. URL https://aclanthology.org/ P03-1071/. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, and Qingan Li. A2ATS: Retrieval-based KV cache reduction via windowed rotary position embedding and query-aware vector quantization. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1245112463, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.644. URL https://aclanthology.org/2025.findings-acl.644/. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Coleman Richard Charles Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, Sebastian Zhao, June Paik, Michael W. Mahoney, Kurt Keutzer, and Amir Gholami. Squeezed attention: Accelerating long context length LLM inference. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3263132652, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1568. URL https://aclanthology. org/2025.acl-long.1568/. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. S. Joty, G. Carenini, and R. T. Ng. Topic segmentation and labeling in asynchronous conversations. Journal of Artificial Intelligence Research, 47:521573, July 2013. ISSN 1076-9757. doi: 10. 1613/jair.3940. URL http://dx.doi.org/10.1613/jair.3940. Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, and Hyun Oh Song. Kvzip: Query-agnostic kv cache compression with context reconstruction, 2025. URL https: //arxiv.org/abs/2505.23416. Minsoo Kim, Kyuhong Shim, Jungwook Choi, and Simyung Chang. InfiniPot: Infinite context processing on memory-constrained LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1604616060, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.897. URL https://aclanthology. org/2024.emnlp-main.897/. Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. Booksum: collection of datasets for long-form narrative summarization, 2022. URL https: //arxiv.org/abs/2105.08209. Dong-Ho Lee, Adyasha Maharana, Jay Pujara, Xiang Ren, and Francesco Barbieri. Realtalk: 21-day real-world dataset for long-term conversation. arXiv preprint arXiv:2502.13270, 2025. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=poE54GOq2l. Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, and Minyi Guo. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression, 2024a. URL https://arxiv. org/abs/2412.03213. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024b. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1385113870, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.747. URL https://aclanthology.org/2024.acl-long.747/. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence, 2025. Accessed: 2025-01-25. NVIDIA. Kv-cache compression leaderboard. https://huggingface.co/spaces/ nvidia/kvpress-leaderboard, 2025. Accessed: 2025-09-01. OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Junyoung Park, Dalton Jones, Matthew Morse, Raghavv Goel, Mingu Lee, and Chris Lott. Keydiff: Key similarity-based kv cache eviction for long-context llm inference in resource-constrained environments, 2025. URL https://arxiv.org/abs/2504.15364. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Maarten Raedt, Frederic Godin, Chris Develder, and Thomas Demeester. Revisiting clustering for efficient unsupervised dialogue structure induction. Applied Intelligence, 54:128, 04 2024. doi: 10.1007/s10489-024-05455-5. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: In Forty-first International Query-aware sparsity for efficient long-context LLM inference. Conference on Machine Learning, 2024. URL https://openreview.net/forum?id= KzACYw0MTV. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: In The Thirteenth InternaBenchmarking chat assistants on long-term interactive memory. tional Conference on Learning Representations, 2025a. URL https://openreview.net/ forum?id=pZiyCaVuti. Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=EytBpUGB1Z. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=RkRrPp7GKO. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1972419731, Mar. 2024. doi: 10.1609/aaai.v38i17.29946. URL https: //ojs.aaai.org/index.php/AAAI/article/view/29946."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 DATASET We evaluate EPICACHE on three LongConvQA benchmarks: Realtalk (Lee et al., 2025), LoCoMo (Maharana et al., 2024), and LongMemEval (Wu et al., 2025a). Three benchmarks follow the LongConvQA formulation in Section 2.1, where long conversational history is provided and the model is required to answer sequence of queries = q1, . . . , qNq grounded in dialogue history. This formulation is intended to assess the answer accuracy of LLMs in multi-turn conversation. Realtalk. Realtalk (Lee et al., 2025) is real-world dataset of 10 long-term conversations, where pairs of participants engaged in daily messaging for 16-21 days. Unlike LLM-simulated corpora such as LoCoMo, Realtalk captures natural dialogue including typos, abbreviations, asynchronous response gaps, and consecutive messages, while also reflecting diverse emotional expressions and shifts in persona consistency. For evaluation, the dataset provides annotated memory probing questions across three subtasksand commonsenserequiring models to recall and reamulti-hop, son over extended histories. Following the original setup, we adopt GPT-based scoring (gpt-4o-mini-2024-07-18) to assess open-ended generation. reasoning, temporal LoCoMo. LoCoMo (Maharana et al., 2024) is benchmark of long-term conversations, created through human-machine pipeline where LLM-based agents generate dialogues grounded in distinct personas and temporal event graphs, and human annotators refine them for long-range consistency. The dataset consists of 10 conversations, each spanning up to 35 sessions with around 300 turns. The QA benchmark is divided into five subtasks: (i) Single-hop, (ii) Multi-hop, (iii) Temporal reasoning, (iv) Open-domain knowledge, and (v) Adversarial. Evaluation is conducted with open-ended genetation and report F1 score. We exclude the adversarial subtask for the following reason. This task evaluates whether model can detect unanswerable questions by choosing between plausible but incorrect answer and no such information response. Under KV cache compression, however, models often over-predict the latter, which spuriously inflates adversarial scores. For instance, on LLaMA-3.2-3B the score is only 12.11 with full KV, but rises to 49.78 under 4K KVzip compression. Unlike the consistent degradation seen in other subtasks (e.g., temporal or multi-hop reasoning), this inflation does not indicate real gains. Since open-source models already struggle on this task (Maharana et al., 2024), including it would be misleading. We therefore omit adversarial results from our main evaluation and defer more careful study of unanswerability detection under compression to future work. LongMemEval. LongMemEval (Wu et al., 2025a) benchmarks long-term memory in userassistant interactions with five core abilitiesinformation extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstentionthrough seven question types (single-session user/assistant/preference, two-hop, multi-session synthesis, knowledge update, temporal reasoning, and abstention). key property is its length-configurable chat histories: the benchmark provides standardized settings with extremely long contexts (e.g., up to 1.5M tokens), designed as controlled stress tests of memory and retrieval mechanisms. We follow the open-ended generation setup and report F1 scores for this dataset. To align LongMemEval with the LongConvQA formulation in Section 2.1, we utilize the custom session stacking provided by LongMemEval5 to build coherent long conversations from user-LLM. Using this feature, we construct evaluation sets while preserving the original distribution of all question types. Specifically, we sample QA pairs according to the benchmarks task-type proportions, retrieve the corresponding evidence conversation sessions, and assemble them into chronologically consistent histories. We then evaluate models at context lengths of 20K, 40K, 60K, 80K, and 100K tokens. This design allows us to test KV cache compression under scalable memory budgets. 5https://github.com/xiaowu0162/LongMemEval A.2 KV CACHE COMPRESSION BASELINE SETUP We adapt existing KV cache compression methods to the block prefill setting for fair comparison with our approach. The baselines include both static retention and attention-based eviction strategies, as well as similarity-based selection. StreamingLLM. Following Xiao et al. (2024), we retain fixed number of sink and recent tokens throughout block prefill. Specifically, we fix the number of sink tokens to 128 for all models, while the remaining budget 128 is assigned to the most recent tokens. SnapKV. We adapt SnapKV (Li et al., 2024) to the block prefill setting, where future queries are not accessible in LongConvQA. Following the original design, we use the window tokensin our case the last 64 tokens of each blockas the patched prompt, and then apply the scoring function in Equation (3). Tokens with the highest attention relevance to this patched prompt are retained. KVzip. We adapt KVzip (Kim et al., 2025) to block prefill by treating the entire block of tokens as the patched prompt. At each block boundary, we append repetition instruction (e.g., Repeat the part of the previous context exactly) followed by the full block tokens, and then apply the patched-prompt scoring method. InfiniPot. We adopt the InfiniPot (Kim et al., 2024) by employing general-purpose patched prompt designed to highlight globally important content. Specifically, we append the instruction Summarize the previous context highlighting the most important parts. at the end of each block and compute scores according to Equation (3). This encourages selection of semantically informative tokens across the block. KeyDiff. KeyDiff (Park et al., 2025) is KV cache eviction method for block prefill. For each block, it constructs an anchor key by averaging the key states of all tokens, and computes the dissimilarity score of each token as the negative cosine similarity between its key state and the anchor. These scores are then used to guide eviction. Following the original implementation, we evaluate Mblock 128, 512, 1024, 2048, which includes the default setting of 128, and report results using the configuration that achieved the best performance. To ensure fairness, all attention-based methods (SnapKV, InfiniPot, KVzip, EPICACHE) use the same scoring formulation from Equation (3), and all eviction methods are combined with head-wise non-uniform token selection as suggested by Feng et al. (2024). In practice, we adopt the max aggregation for smax rather than averaging across heads, since empirical results consistently showed superior performance across all baselines. A.3 EPICACHE SETUP Overall Process. We provide the complete procedure of EPICACHE in Algorithm 1. The framework consists of two phases. In Phase A, the conversation history is clustered into topical episodes and compressed KV cache is prepared for each episode. In Phase B, online queries are answered by retrieving the most relevant episodic cache. In Phase A1, we segment the conversation history, embed each segment, and cluster them into episodes. This step can be performed offline, and the cost of segment encoding and K-means clustering is negligible (under minute). Each episode is represented by its centroid and medoid segment that serves as patched prompt. In Phase A2, we measure per-layer sensitivity by comparing Key states under full and block-prefill masks, and allocate layer-wise budgets proportionally using the sharpness hyper-parameter α. In Phase A3, we construct episodic KV caches by performing blockwise prefill with the patched prompt appended, and then compress the resulting caches according to the allocated budgets from Phase A2. Although prefill must be repeated for every episode, the peak memory remains flat during this process (see Figure 1c), making it practical for constrained-memory environments. In Phase B, when new query arrives, we embed the query and compute its similarity to the episode centroids. The query is then routed to the most relevant episodic cache, which is loaded for decoding. 15 Algorithm 1 EPICACHE with Layer-wise Budget Allocation Pseudo Code Require: (history, Nu turns), fembed, wembed, E, ; fLM (L layers, heads); masks {M, M}; sharpness α; calibration batch with = 1 e=1, and layer budgets {M alloc ℓ }L ℓ=1 KV, . . . , (E) KV }, centroids {ce}E Ensure: Episodic caches = {C (1) Phase A: Clustering and Prefill A1. Conversation Segment & Clustering (Offline) 1: Partition into = Nu/wembed segments {Sk}K 2: Run K-Means Clustering on {ek} to obtain {Ee}E e=1. 3: for = 1 to do ce 1 4: Ee Build patched prompt Pe by concatenating utterances of S(e) medoid arg maxSkEe cos(ek, ce) ek; S(e) SkEe (cid:80) medoid. 5: 6: end for A2. Measure layer sensitivity & allocate KV budgets. k=1 and encode ek = fembed(Sk). full(x) fLM(x, M)W ℓ (cid:80)H (cid:80)N ; ℓ i=1 cos(cid:0)k(ℓ,h) block(x) fLM(x, M)W ℓ block,i(x)(cid:1) full,i(x), k(ℓ,h) h=1 for ℓ=1:L ℓ σℓ(x) 1 HN 7: for each do 8: 9: 10: end for 11: σℓ 1 12: wℓ xB σℓ(x); sℓ 1σℓ (cid:80) α ℓ j=1 α A3. Build episodic KV caches. ; alloc (cid:80)L ℓ (LM ) wℓ 13: for = 1 to do Block-wise prefill over H, appending Pe to each block of Mblock tokens. 14: Compute scores w.r.t. Pe with Equation (3) and retain the top tokens. 15: (e) 16: 17: end for 18: {C (1) KV compressed cache for episode e. KV, . . . , (E) KV }. Phase B: Online decoding 19: For query qi: qi fembed(qi); 20: Retrieve (e) arg maxe cos(qi, ce) KV and generate with compressed cache: fLM(qi (e) KV ). If the same cache is selected as in the previous turn, no additional retrieval is required since the cache remains resident, further reducing overhead. Detailed Settings For segment construction, we set the embedding window size wembed to 4, selected from 2, 4, 8. We observed that performance differences among these values were minor, making wembed=4 reasonable default. To cluster segments into episodes, we apply the standard K-means algorithm with k-means++ initialization (Arthur & Vassilvitskii, 2007). This offline segmentation and clustering stage completes within one minute, incurring negligible overhead. For sensitivity-aware budget allocation, we estimate per-layer weights using single randomly sampled conversation from the BookSum (Kryscinski et al., 2022) dataset. By performing two forward passesone with the full causal mask and one with the block-prefill maskwe measure the layerwise deviations and compute allocation weights. Because only one sample is used, the overhead of this calibration step is negligible. In block prefill, the cache always maintains size : as conversation segments are added in blocks of Mblock, the cache can temporarily grow up to + Mblock entries, after which eviction is applied to reduce it back to . larger Mblock enables the model to cover the entire conversation more quickly but increases the temporary peak memory footprint, while smaller Mblock lowers peak memory at the cost of slower coverage. We set Mblock 128, 512, 1024, 2048 to balance this trade-off. For the patched prompt, we use the medoid segment of size 4, selected from 4, 8, 12. Ablation studies show that different segment sizes yield only marginal performance differences. After constructing episodic KV caches, we offload them to offline memory (e.g., CPU) to minimize GPU memory usage. During online decoding, episodic caches are retrieved from offline memory. This design enables constrained GPU memory usage while keeping retrieval overhead manageable. 16 Figure A1: Alternative Design Exploration: (a) Comparison with RAG-like baseline, which feeds clustered segments directly with query-based matching. (b) Sweep of block prefill size Mblock. (c) Effect of segmentation window size wemb. (d) Encoder choice for clustering and query embedding. (e) Effect of episode number in episodic clustering. (f) Number of medoid samples in patched prompts. All experiments use LLaMA3.1-8B on the Realtalk and LoCoMo benchmarks."
        },
        {
            "title": "B ALTERNATIVE DESIGN EXPLORATION",
            "content": "B.1 COMPARISON WITH RAG-LIKE APPROACH. representative alternative design is RAG-like approach, where conversation segments are clustered and the query is used to select the most relevant clusters, which are then directly fed into the LLM to form KV cache under fixed budget. The key difference from block prefill eviction is that block prefill spans the entire dialogue and selects tokens in context-holistic manner, whereas the RAG-like method isolates only the chosen clusters when constructing the KV cache. As shown in Figure A1 (a), this leads to substantially worse performancenot only below EPICACHE but also below block prefill-based compression methods such as KVzip. This result further suggests that clustering alone is insufficient for directly constructing episodic memory inputs: conversation histories segmented only by clustering remain under-contextualized. Developing more effective clustering or episode partitioning strategies tailored to retrieval thus remains an interesting direction for future work. B.2 EPICACHE ABLATION STUDY. Block Size. Figure A1 (b) varies the block size Mblock used during block prefill. Accuracy differences remain minor across settings. In practice, smaller block sizes reduce peak memory but require more iterations to span the full context, whereas larger blocks incur higher memory during prefill but process the history more efficiently. This reflects memory-throughput trade-off, and block size can be tuned according to deployment constraints. Clustering Design. Figures Figure A1 (c-f) examine clustering-related hyper-parameters. (i) Figure A1 (c) sweeps the segmentation window size wemb for utterance grouping, showing little sensitivity and indicating robustness to segmentation granularity. (ii) Figure A1 (d) varies the encoder for clustering and query embedding. The LLMs own embedding layer yields weaker performance, while lightweight dedicated encoders (MiniLM-L6-v2, 0.06B; Qwen3-0.6B) deliver strong gains 17 (a) Key states similarity (b) Value states similarity (c) Layer outputs L2 distance Figure A2: Layer-wise Sensitivity Analysis. layer-wise deviation results under the full and block masks using Qwen2.5-7B on LoCoMo conversation history. Key and Value states are measured by cosine similarity, while hidden states at layer outputs are measured by L2 distance; shaded regions indicate variance across input samples."
        },
        {
            "title": "Medoid segments examples",
            "content": "0. Video game A: Its game used to play lot . . . mostly 1. Movie 2. Literature 3. Weather play for fun . . . B: also have finished the first game . . . my favorite games of all time are ... A: love the art and think he is an incredible director . . . B: havent watched movies in while . . . A: Have you ever read book called . . . B: Im currently reading ... multi generational family ... A: love the weather today its gotten warmer B: just got home from work . . . its raining like crazy . . . parts of the city are flooded (b) Medoid samples by cluster. (a) t-SNE visualization Figure A3: Episodic clustering of conversation segments. (a) t-SNE visualization of conversation clustering. (Silhouette score=0.28) (b) Medoid segments illustrate coherent topics per cluster. over InfiniPot. Larger encoders (Qwen3-4B) add only marginal improvements, suggesting that small encoders suffice for EPICACHE. (iii) Figure A1 (e) sweeps the number of episodes E, where larger improves segmentation and accuracyespecially under tight budgetsat the cost of maintaining more episodic caches offline. (iv) Figure A1 (f) varies the number of medoid samples in patched prompts, with little effect, suggesting that only few representatives per cluster are sufficient."
        },
        {
            "title": "C FURTHER ANALYSIS",
            "content": "C.1 BLOCK-PREFILL SENSITIVITY ANALYSIS We analyze layer-wise deviations under block prefill by comparing multiple internal statesKey, Value, and layer outputsacross Transformer layers computed with the full causal mask (M) and the block mask (M). To this end, we forward LoCoMo (Maharana et al., 2024) conversation history samples under both masks and plot the resulting internal states differences across layers, as defined in Equations (8) and (9). Specifically, each plot reports cosine similarity of Key and Value states, and L2 distance of layer outputs, respectively. We find that Value states (Figure A2b) exhibit consistently low similarity across layers, offering little discriminative trend. Layer outputs, measured by L2 distance (Figure A2c), show monotonic error accumulation pattern rather than meaningful variation. In contrast, Key states (Figure A2a) provide 18 LLaMA3.2-3B Qwen2.5-3B"
        },
        {
            "title": "Method",
            "content": "M Decode (ms) Embed. (ms) Retr. (ms) Peak Mem. KVs (GB) (GB) Decode (ms) Embed. (ms) Retr. (ms) Peak Mem. KVs (GB) (GB)"
        },
        {
            "title": "Full KV",
            "content": "100K"
        },
        {
            "title": "EPICACHE",
            "content": "2K 4K 6K 8K 68.9 28.1 29.6 30.1 32.2 6.1 6.0 5.5 5.6 2.0 4.0 6.0 7.9 28.4 8.2 8.4 9.3 11.2 11.6 0.2 0.5 0.7 0.9 54. 36.7 37.7 38.9 42.3 5.2 5.5 5.4 5.6 1.2 2.2 2.9 3.7 20. 7.6 8.8 9.5 10.5 3.8 0.1 0.2 0.2 0.3 Table A1: Runtime and memory comparison under full KV and block prefill across LLaMA and Qwen. Columns: (memory budget), Decode (ms), Embed. (ms), Retrieve. (ms), Peak Memory (GB), and KVs Storage (GB). Latency is averaged per turn over 100 multi-turn queries on LongMemEval with 100K context length, reported across five runs. Measurements are conducted on an NVIDIA DGX A100 system. clear differentiation across layers. This observation motivates our use of Key state deviation as the sensitivity measure for budget allocation, as discussed in Section 3.2. Further analysis of why these trends differ across Key, Value, and layer-wise output representations is left for future work. C.2 CONVERSATION CLUSTERING ANALYSIS In this section, we provide qualitative examples of conversation clustering to illustrate how episodic structures emerge in practice. Conversation histories are divided into segments of wembed = 4 utterances, which are then embedded using Qwen3-0.6B (Zhang et al., 2025). Segment embeddings are clustered with K-Means, and the resulting clusters are visualized in two dimensions via t-SNE, as shown in Figure A3 (a). For each cluster, we further present representative medoid segments in Figure A3 (b). These examples demonstrate that the clustering procedure consistently groups segments into coherent topical episodes, such as games, movies, literature, or weather. The medoid samples highlight the interpretability of each episode and indicate how such episode-level partitioning can serve as the basis for episodic KV cache compression."
        },
        {
            "title": "D DETAILED EXPERIMENTAL RESULTS",
            "content": "D.1 EFFICIENCY RESULTS Measurement Setup. We use server equipped with 8 NVIDIA A100-40GB (PCIe) GPUs and dual Intel Xeon Platinum 8275CL CPUs, with host-device transfers over PCIe 4.0 x16. Latency is measured on LongMemEval (Wu et al., 2025a) with 100K context length, where each evaluation spans 100 multi-turn queries. We report the average per-turn latency across five runs for both LLaMA3.2-3B and Qwen2.5-3B. Results. Table A1 presents the runtime and memory breakdown. Full KV incurs high decoding latency and large peak GPU memory, with LLaMA reaching 68.9ms per turn and 28.4GB memory. Notably, the KV cache storage of LLaMA (11.6GB) even exceeds the models own parameter size, underscoring the unsustainable cost of KV caching. By contrast, Qwen2.5-3B exhibits lower KV storage (3.8GB) due to its smaller attention head configuration (Ainslie et al., 2023) (2 heads vs. 8 heads in LLaMA). With EPICACHE, both models achieve substantial efficiency gains. On LLaMA3.2-3B, decoding latency drops from 68.9ms to 28.1ms (2.4 faster) and peak memory from 28.4GB to 8.2GB (3.5 smaller). For Qwen2.5-3B, latency improves from 54.1ms to 36.7ms (1.5 faster) and memory from 20.7GB to 7.6GB (2.7 smaller). The additional cost from query embedding and cache retrieval accounts for only about 5% of per-turn latency on average, confirming that the overhead introduced by episodic caching is minimal compared to the overall efficiency benefits. 19 D.2 LONGCONVQA SUBTASK RESULTS We provide detailed subtask results corresponding to the main LongConvQA experiments highlighted in Figure 5. For Realtalk and LoCoMo, results are reported in Table A2 for LLaMA3.2-3B and LLaMA3.1-8B and in Table A3 for Qwen2.5-3B and 7B. For LongMemEval, results across all four models are reported in Table A4."
        },
        {
            "title": "E FUTURE WORK",
            "content": "EPICACHE demonstrates that memory-bounded episodic KV caching can substantially improve accuracy on LongConvQA while maintaining minimal latency overhead. By clustering dialogue into episodes and dynamically retrieving episode-specific caches, it provides practical solution that balances efficiency and effectiveness in resource-constrained environments. There remain several promising directions for future research. First, while we adopt standard embedding-based clustering for episode construction, more advanced clustering strategies specialized for conversational structure (Raedt et al., 2024) can be developed. Such methods, orthogonal to our framework, can yield more coherent episodic boundaries and further strengthen the effectiveness of episodic KV cache management. Second, EPICACHE currently operates with fixed number of episodes. Extending it to adaptively determine the optimal number of episodes from the conversation history could improve scalability across diverse dialogue lengths and domains. In addition, incorporating KV cache quantization (Hooper et al., 2024; Liu et al., 2024b) into episodic caches would reduce cache storage and transfer costs, alleviating the movement overhead during the retrieval stage. Exploring these extensions would further enhance the practicality of episodic KV caching for long conversational scenario. 20 LLaMA3.2-3B LoCoMo (Full KV length: 21.8K) Realtalk (Full KV length: 26.4K) Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE Multi-hop Temporal Open-domain Single-hop Avg Multi-hop Temporal Common Avg 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 36.0 17.3 23.1 27.3 31. 15.9 21.3 25.6 28.6 10.7 15.7 20.7 23.7 22.0 21.9 28.1 31.2 29.3 30.4 33.7 33.9 15.1 3.7 6.0 9.9 11. 7.2 12.0 17.0 20.5 3.7 8.3 11.9 15.7 4.5 13.3 16.5 21.8 15.9 19.7 22.2 23.9 13.2 17.3 11.5 10.6 11. 10.1 10.0 11.9 11.4 11.6 11.5 13.4 14.7 12.0 10.0 12.0 11.8 13.7 14.2 12.1 12.7 54.5 17.6 28.7 36.8 44. 15.0 23.7 31.3 36.9 11.2 17.9 23.8 30.2 17.3 24.5 34.2 41.5 33.1 42.3 46.4 48.2 40.3 14.3 21.9 27.8 33. 13.3 20.0 26.1 30.4 9.2 15.1 20.1 25.0 15.2 20.8 28.0 33.6 27.6 33.6 36.9 38.3 39.0 20.1 28.5 31.7 34. 22.1 29.2 32.8 35.6 10.4 16.8 20.6 24.8 18.7 25.0 26.1 30.7 34.7 37.4 39.9 40.0 30.8 10.5 14.7 18.2 20. 9.4 16.0 19.7 24.5 5.6 14.2 14.5 20.7 9.5 10.1 15.2 18.1 23.0 24.7 25.6 26.6 38.2 25.0 30.0 33.2 33. 24.3 31.9 32.4 36.9 19.5 18.2 24.8 29.7 27.9 29.4 36.3 34.5 41.3 41.0 44.2 42.9 35.3 16.6 22.7 26.0 28. 16.8 23.8 27.0 31.0 9.7 15.9 18.5 23.7 16.0 19.1 22.8 25.7 30.5 32.4 34.3 34.6 LLaMA3.1-8B LoCoMo (Full KV length: 21.8K) Realtalk (Full KV length: 26.4K) Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE Multi-hop Temporal Open-domain Single-hop Avg Multi-hop Temporal Common Avg 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 43.1 23.4 30.8 33.3 37. 15.2 23.0 29.8 33.1 18.7 23.3 27.5 33.0 24.4 28.8 32.8 37.2 33.2 37.7 40.3 40.9 22.7 6.6 10.9 13.8 18. 12.3 22.1 29.6 33.8 4.4 14.1 18.6 25.8 8.4 24.3 31.0 35.0 26.3 33.8 36.5 39.6 17.2 14.3 13.6 13.3 14. 10.2 13.9 13.9 13.8 17.7 13.8 14.5 16.4 22.8 15.3 13.3 13.1 14.2 15.9 17.8 16.9 67.4 25.5 37.3 46.8 55. 19.6 29.5 40.1 45.5 21.8 31.5 39.7 46.2 24.7 36.8 47.3 54.6 43.5 55.8 60.8 64.8 50.5 20.5 29.2 35.3 41. 16.7 25.8 34.5 38.8 17.3 25.3 31.5 37.7 21.2 31.4 39.1 44.8 36.3 45.4 49.3 52.2 49.2 20.2 28.7 37.3 41. 21.2 30.0 34.4 38.3 13.0 21.9 29.6 33.0 20.6 29.5 36.4 38.8 37.9 39.6 42.3 44.2 55.9 15.8 26.6 32.2 36. 14.9 25.8 33.3 38.2 8.2 16.5 25.3 29.9 16.6 26.4 33.4 41.3 35.2 44.9 50.5 50.9 48.4 34.3 44.4 42.3 45. 35.0 39.1 43.4 43.4 28.8 30.9 33.3 33.3 34.6 40.0 41.5 41.1 45.2 46.7 35.2 46.5 52.0 20.4 30.1 35.8 40. 20.5 29.5 35.3 39.0 13.2 20.9 28.2 31.7 20.9 29.7 35.8 40.3 37.8 43.0 46.5 47.5 Table A2: LongConvQA (LoCoMo and Realtalk) Evaluation: Comparison of different KV cache compression methods under block-prefill with LLaMA series models. Qwen2.5-3B Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE Qwen2.5-7B Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE LoCoMo (Full KV length: 21.9K) Realtalk (Full KV length: 26.6K) Multi-hop Temporal Open-domain Single-hop Avg Multi-hop Temporal Common Avg 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 33.2 14.1 17.9 21.3 23.2 12.4 18.2 20.0 23.9 10.4 14.0 19.6 21. 11.8 16.2 19.5 21.9 23.6 27.1 27.3 31.3 22.9 8.0 12.7 13.1 14.3 16.1 19.7 20.0 18.8 14.2 15.8 16.4 17. 6.1 10.3 12.7 14.3 7.6 10.7 13.3 17.0 12.3 11.5 11.5 13.7 12.1 12.6 10.2 13.9 13.2 13.6 12.0 9.2 7. 11.9 12.6 11.3 14.1 13.4 11.4 10.8 10.0 49.1 10.4 16.2 21.1 27.4 8.8 15.7 19.9 25.1 7.8 14.1 20.8 27. 11.4 14.7 19.1 24.3 23.8 29.7 37.2 41.7 38.4 10.6 15.5 19.0 22.9 11.2 16.7 19.6 22.8 10.0 14.3 18.9 23. 10.4 14.0 17.4 21.1 19.8 24.1 28.8 32.7 32.7 12.4 15.4 17.9 21.6 8.6 15.4 17.5 23.9 3.9 9.2 11.5 18. 10.6 13.3 15.0 18.8 25.6 30.0 32.6 33.9 28.0 5.6 9.0 9.3 12.6 6.1 8.9 12.0 11.4 9.0 6.0 10.0 15. 5.3 8.4 10.0 11.4 13.2 17.1 19.7 25.0 39.6 23.5 27.0 26.2 28.8 26.3 24.3 28.2 31.1 23.5 21.6 22.6 24. 18.2 21.7 27.8 28.4 37.7 36.9 36.4 41.1 31.6 11.1 14.3 15.3 18.7 10.1 13.9 16.7 19.5 8.9 9.6 12.5 17. 9.4 12.4 14.7 17.0 22.0 25.4 27.5 31.1 LoCoMo (Full KV length: 21.9K) Realtalk (Full KV length: 26.6K) Multi-hop Temporal Open-domain Single-hop Avg Multi-hop Temporal Common Avg 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 36.2 17.6 23.8 27.8 30.8 15.1 19.2 23.4 27.7 12.8 18.9 26.3 28.3 14.3 19.4 24.3 25.6 26.4 29.0 32.6 32. 19.2 5.9 8.0 9.1 13.2 14.2 20.0 23.7 14.0 16.7 22.0 25.2 23.7 13.7 16.2 19.5 23.8 15.4 20.9 24.6 28. 16.6 12.6 13.1 15.2 14.9 12.0 10.8 15.0 14.0 12.4 13.9 14.3 15.9 11.7 13.9 12.5 13.0 13.2 14.1 15.1 15. 59.3 14.7 24.2 30.2 39.4 12.4 19.9 26.0 32.7 13.6 21.8 29.8 36.4 12.9 20.6 27.2 34.8 29.3 38.5 46.5 52. 44.1 13.3 20.1 24.4 30.8 13.2 19.2 24.3 29.4 14.0 20.8 27.2 30.8 13.3 19.0 24.2 29.5 24.9 31.6 37.5 41. 38.7 9.8 15.8 20.6 24.4 10.4 17.1 21.6 26.3 9.8 12.0 15.7 20.3 12.2 17.7 22.5 26.8 24.1 31.1 33.0 33. 52.3 10.5 16.0 24.4 25.1 15.7 22.7 30.0 29.4 15.0 19.9 26.1 32.7 10.8 16.0 24.6 28.7 21.3 29.3 39.4 46. 43.4 14.2 24.8 29.2 33.5 15.2 23.7 25.5 31.5 18.4 26.3 21.1 28.0 23.9 29.8 32.6 37.7 36.6 37.5 40.6 43. 45.3 10.8 17.2 23.5 26.1 13.4 20.5 25.8 29.0 13.3 19.5 21.1 26.9 13.3 18.8 24.9 29.3 24.7 31.3 36.9 41. Table A3: LongConvQA (LoCoMo and Realtalk) Evaluation: Comparison of different KV cache compression methods under block-prefill with Qwen series models. 22 Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE Method Full KV SnapKV (Li et al., 2024) InfiniPot (Kim et al., 2024) KeyDiff (Park et al., 2025) KVzip (Kim et al., 2025) EPICACHE SH LLaMA3.2-3B TH MS TR-E TR-I 21K 84.6 10.0 26.9 40.0 54.5 67.1 46.2 48.5 60.0 76.0 35.3 54.2 55.8 56. 30.8 44.7 58.1 73.5 73.0 79.9 85.0 85.0 0.8 5.3 5.3 7.9 0.8 7.9 2.6 4.4 0.5 2.1 6.5 2.1 0.0 2.6 5.3 7. 10.5 12.6 10.0 10.0 12.5 1.8 2.4 15.8 12.4 10.8 12.3 12.4 13.3 4.1 2.4 7.4 6.6 1.8 6.5 12.4 12. 7.4 16.6 13.4 12.5 47.9 26.6 40.2 37.1 37.1 40.5 33.6 33.6 40.7 34.3 34.3 54.3 37.1 30.9 37.1 37.1 40. 40.5 41.4 40.7 40.7 27.1 17.4 20.4 23.0 27.1 19.1 18.4 25.9 25.0 17.7 15.4 11.4 25.9 15.2 15.0 21.3 26. 21.0 27.0 27.2 26.6 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K Qwen2.5-3B TH MS TR-E TR-I SH 21K 80.8 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 2K 4K 6K 8K 23.5 48.8 59.3 56.3 31.0 46.2 55.9 70.2 12.6 30.5 51.6 62. 18.4 39.4 51.9 68.4 52.6 74.4 77.3 77.3 14.0 1.2 6.5 3.9 11.4 1.2 5.3 4.8 6.1 14.0 11.4 15.3 22. 1.2 6.5 1.2 8.8 3.5 14.0 16.7 16.7 15.0 1.3 0.8 19.6 20.4 11.9 16.5 10.6 15.7 6.3 11.1 21.1 15. 1.3 5.0 20.0 15.0 10.0 11.0 15.4 15.0 50.2 11.9 11.9 32.4 29.8 22.6 33.3 32.4 35.9 44.1 34.5 43.7 30. 13.7 23.8 44.1 39.5 32.4 46.7 46.7 46.7 23.7 0.1 1.1 5.9 7.9 5.4 19.4 25.7 27.3 11.1 18.9 20.5 22. 10.9 13.8 17.6 20.7 22.9 17.6 22.0 22.8 KU 52.3 31.4 48.5 58.7 56.6 40.7 52.3 51.7 52. 5.7 34.4 32.2 37.7 30.3 37.8 50.8 59.1 50.8 53.9 55.1 56.6 KU 59.0 25.6 27.9 34.5 41. 17.5 31.5 40.1 47.3 27.6 33.0 35.4 47.5 22.5 25.3 34.3 40.9 46.7 55.9 55.3 55.9 IP Avg. 6. 6.1 6.5 6.3 7.1 8.9 7.6 6.7 7.6 2.6 6.8 9.0 7.7 7.6 7.4 6.2 6.9 6.0 9.1 8.3 6.2 39. 17.7 26.1 33.0 35.6 26.3 29.4 31.9 36.2 15.1 24.2 26.9 27.9 18.2 24.0 31.3 37.5 34.4 39.3 39.6 39.5 IP Avg. 9.1 3.8 4.6 5.7 8.1 4.9 6.8 6.1 7.7 3.3 5.7 9.6 7.8 4.2 6.4 5.7 10.0 6.7 9.1 10.5 10. 40.7 12.7 18.7 26.4 28.6 14.8 25.2 28.7 34.5 17.3 22.5 30.4 33.9 11.8 19.2 27.5 32.5 28.7 37.0 39.2 39. SH 87.2 35.6 63.3 65.1 74.3 39.8 62.4 81.3 90.3 26.0 60.0 69.2 70.7 33.8 56.6 61.3 73. 72.3 87.2 83.8 88.2 SH 88.6 19.9 42.9 50.4 67.4 29.8 44.7 66.1 78.6 18.1 43.1 57.5 69. 21.1 45.4 57.8 69.6 70.4 83.6 86.1 86.0 LLaMA3.1-8B TH MS TR-E TR-I 14.1 3.8 9.7 8.3 13. 1.8 7.9 11.0 9.1 6.8 14.7 11.2 11.2 7.5 9.7 11.8 14.4 16.7 14.1 13.8 13.5 17.6 3.3 3.9 9.0 13. 3.4 4.1 13.3 18.8 9.5 12.9 13.6 18.4 8.0 6.7 6.9 7.7 3.3 17.4 25.4 17.5 56.5 29.8 45.8 49.4 53. 29.1 53.9 54.9 54.0 28.8 41.4 48.4 46.1 36.0 35.3 42.7 48.6 46.5 56.5 55.7 56.5 28.5 23.3 24.4 30.4 25. 31.7 23.0 28.1 28.5 12.3 20.2 26.1 30.0 19.4 24.1 29.4 26.9 24.0 28.5 25.3 28.5 Qwen2.5-7B TH MS TR-E TR-I 39. 4.9 10.9 10.9 17.1 1.6 22.6 26.7 34.0 8.2 5.8 13.0 23.6 4.8 14.1 23.5 23.5 36.9 38.3 38.3 38.3 35. 8.3 22.0 17.0 24.4 2.0 25.3 32.9 33.6 9.4 9.9 13.9 32.9 2.4 22.0 22.8 23.4 31.1 29.8 33.1 33.1 32. 33.1 33.6 32.9 32.9 33.3 36.4 36.4 32.9 31.1 37.1 34.2 39.1 22.8 31.7 33.6 32.9 42.4 37.6 32.9 37.6 35. 21.5 28.8 28.4 31.6 23.1 36.7 40.5 41.4 14.3 20.5 26.8 25.6 19.8 23.3 31.9 35.0 35.9 49.6 39.9 40.5 KU 56.1 25.4 45.8 50.5 52.5 35.8 46.8 54.9 56.8 22.8 31.8 52.1 50.1 27.2 42.7 47.5 51.4 56.4 54.6 54.9 55. KU 47.9 31.1 38.4 42.6 42.9 32.2 41.6 48.1 50.1 31.5 32.7 38.0 50.2 27.1 29.9 34.4 40. 48.5 41.3 48.8 47.3 IP Avg. 6.3 8.8 10.8 11.8 10.9 7.6 9.2 5.6 9.4 5.5 6.1 8.5 5. 9.1 12.4 11.3 6.2 10.5 3.9 10.7 6.4 43.1 20.2 32.4 35.6 38.7 24.1 32.7 40.4 42.2 17.1 29.6 36.7 37. 21.5 29.8 33.6 37.1 37.1 42.6 42.9 43.0 IP Avg. 12.9 8.9 10.7 11.3 14.1 11.8 10.3 11.7 13. 11.7 10.7 9.9 11.9 8.9 11.5 11.2 10.6 12.4 12.8 12.7 12.9 46.9 19.3 29.3 30.7 36.7 20.5 34.0 41.7 45. 18.4 24.7 30.8 40.2 16.5 27.5 33.8 37.7 43.7 46.6 46.9 47.2 Table A4: LongConvQA (LongMemEval) Evaluation: Evaluation results with Qwen and LLaMA series models under block prefill. SH = Single Hop, TH = Two Hop, MS = Multi-Session, TR-E = Temporal Reasoning (explicit), TR-I = Temporal Reasoning (implicit), KU = Knowledge Update. IP = Implicit Preference."
        }
    ],
    "affiliations": [
        "Apple",
        "Hanyang University"
    ]
}