{
    "paper_title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking",
    "authors": [
        "Zhengwei Tao",
        "Haiyang Shen",
        "Baixuan Li",
        "Wenbiao Yin",
        "Jialong Wu",
        "Kuan Li",
        "Zhongwang Zhang",
        "Huifeng Yin",
        "Rui Ye",
        "Liwen Zhang",
        "Xinyu Wang",
        "Pengjun Xie",
        "Jingren Zhou",
        "Yong Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines."
        },
        {
            "title": "Start",
            "content": "2025-10-29 WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking Zhengwei Tao((cid:0)), Haiyang Shen, Baixuan Li, Wenbiao Yin((cid:0)), Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, Xinyu Wang, Pengjun Xie, Jingren Zhou, Yong Jiang((cid:0)) Tongyi Lab , Alibaba Group https://tongyi-agent.github.io/blog https://github.com/Alibaba-NLP/DeepResearch https://huggingface.co/datasets/Alibaba-NLP/WebLeaper"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM)-based agents have emerged as transformative approach for open-ended problem solving, with information seeking (IS) being core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as tree-structured reasoning problem, enabling substantially larger set of target entities to be embedded within constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasksBasic, Union, and Reverse-Unionto systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarksBrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines. 5 2 0 2 8 2 ] . [ 1 7 9 6 4 2 . 0 1 5 2 : r Figure 1: Results on comprehensive training setting. All WebLeaper scores are averaged over three runs. The metric of the first three figures are accuracy. SR denotes Success Rate on WideSearch. *Equal contribution. Correspondence to: tttzw@stu.pku.edu.cn, {yinwenbiao.ywb, yongjiang.jy}@alibaba-inc.com. (cid:0)"
        },
        {
            "title": "Introduction",
            "content": "The LLM-based agents mark paradigm shift in AI, delivering transformative solutions to challenges once deemed intractable across diverse domains (Guo et al., 2024; Ye et al., 2023). Among their core capabilities, information seeking (IS) plays crucial role in enabling the cognitive autonomy of these agents. This ability not only drives their adaptability in open-ended tasks but also underpins new generation of powerful commercial systems, including OpenAI Deep Research (OpenAI, 2025b), Googles Gemini (Gemini, 2025), and Perplexity AI (Perplexity, 2025), Kimi-Researcher (Team, 2025). While numerous studies have sought to enhance the IS capabilities of agents through complex questionanswering pipelines and advanced fine-tuning strategies (Wu et al., 2025a; Li et al., 2025c;b; Tao et al., 2025; Qiao et al., 2025; Lu et al., 2025), most existing approaches primarily concentrate on improving the search depth, giving comparatively little attention to search efficiency. Our preliminary experiments indicate that current LLM-based agents search inefficiently. As shown in Figure 2, the distribution of valid actions for competitive IS agent peaks around 0.04, meaning that in most cases, only small fraction of actions are effective (Wong et al., 2025; Xue et al., 2025). This low valid-action rate reflects suboptimal search behaviors, including redundant query reformulations, retrieval of irrelevant information, and unnecessarily long search chains. Such inefficiencies not only increase computational and time costs but also limit the agents overall IS performance. The design of synthetic training tasks incurs this inefficiency. In typical IS agent setups, the agent begins with set of known entities and incrementally gathers information to infer all target entities. However, prior work often constructs tasks in which the target entities are overly sparse (Wu et al., 2025a; Li et al., 2025c;b). Such sparsity limits the agents exposure to informative cues, reducing opportunities to learn to locate relevant information within constrained context window. As result, the agent spends more actions processing irrelevant content, weakening its search strategies, leading to lower performance. Furthermore, it can bias the measurement of search efficiency, which we prove in later section. This bias makes it difficult to obtain an accurate training signal, thereby obstructing the systematic learning of more efficient search behaviors. These limitations underscore the need to redesign training tasks, enabling optimized seeking efficiency and stronger IS capabilities. Figure 2: The distribution of valid actions of the agent based on the GPT model on our synthesized IS task. The valid actions are those seeking the correct target entities required by the question. To address these challenges, we propose WebLeaper, framework designed with two core objectives: (1) to construct new IS tasks containing substantially larger number of target entities; and (2) to generate solution trajectories that achieve both high accuracy and high efficiency. For the first objective, we model the IS process as tree-structured reasoning task, which compactly accommodates more target nodes within limited context. Based on this formulation, we systematically increase task complexity through three dataset variants. First, leveraging curated Wikipedia tables, we synthesize Basic version, which directly addresses the challenge of entity sparsity by creating high-density search space within single, structured source. To mirror more realistic scenarios that demand integrating information from multiple sources, our Union variant constructs tasks that require synthesizing facts across different sources, thereby increasing search ambiguity. Finally, to mitigate the risk of agents adopting simplistic, keyword-based shortcuts, the Reverse-Union variant reverses the logical flow, compelling the agent to first deduce intermediate entities from scattered clues before completing the main search task. For the second objective, we construct task-completion trajectories that are filtered according to Information-Seeking Rate (ISR) and Information-Seeking Efficiency (ISE), retaining only those that solve the task both accurately and efficiently. These metrics are then incorporated into the hybrid reward system during the following reinforcement learning stage. Models trained on this curated dataset after supervised-finetuning and reinforcement learning yield our final IS agent. We conduct extensive experiments on both basic and comprehensive settings to evaluate our approach across five benchmarks: BrowserComp (Wei et al., 2025), GAIA (Mialon et al., 2023), Seal-0 (Pham et al., 2025), WideSearch (Wong et al., 2025), and xbench-DeepSearch (Xbench-Team, 2025). Our method achieves consistent improvements on all benchmarks. Ablation studies on the dataset design further confirm the effectiveness of our proposed components. We summarize our contribution as follows: We design new information-seeking task formulation on tree-structured reasoning problem, leading to the inclusion of substantially larger set of target entities within constrained context. Based on this formulation, we construct the Basic, Union, and Reverse-Union datasets. We generate and filter task-solving trajectories using the proposed Information-Seeking Rate (ISR) and Information-Seeking Efficiency (ISE) metrics, retaining only those trajectories that solve tasks both accurately and efficiently. These metrics are also designed for our hybrid RL reward system. We conduct extensive experiments on five public IS benchmarks, BrowserComp, GAIA, XbenchDeepSearch, WideSearch, and Seal-0, achieving consistent improvements over strong baselines."
        },
        {
            "title": "2 Definitions",
            "content": "An Information-Seeking (IS) task challenges an agent to answer complex natural language question by navigating vast information space to assemble complete set of required entities. This process is inherently sequential, involving the progressive discovery of entities, understanding their properties (attributes), and leveraging relationships between them to uncover further entities. This section formally defines the components of such task and the metrics for evaluating an agents performance, emphasizing the importance of identifying both final and intermediate entities in the reasoning chain. 2.1 Information-Seeking Task An entity is the fundamental unit of information. An Information-Seeking (IS) task is the process of identifying and collecting specific set of target entities from , based on question. Formally, an IS task is tuple: = q, , where is the natural language question and is the set of the target entities that collectively satisfy the conditions posed by q. Critically, the required set includes not only the final, explicit answers but also all intermediate entities that are necessary stepping stones in the reasoning process. Consider the question: : Which player of team in the 200405 season, who was born in the 1990s? This team was founded in 1966 and is an East German football team. To solve this, an IS agent must seek for information online, and find the target entity set as answer: = {Robert Rudwaleit, Danny Kukulies, . . .}. (1) (2) 2. Information-Seeking Agent We focus on an Information-Seeking Agent that interacts with web environment to solve an IS task within the ReAct framework (Yao et al., 2023). The agents operation is sequential decision-making process occurring over discrete time steps = 1, . . . , T. At each step, the agent analyzes its current state (including the initial question and all previously gathered information), generates thought for planning its next move, executes tool-based action to seek new information, and receives an observation from the environment. This entire process is captured in the agent trajectory is defined as HT = (q, τ1, α1, o1, τ2, α2, o2, . . . , τT, αT, oT), (3) 3 where τi is the planning thought, αi is the seeking action, and oi is the resulting observation at step i. At the end of the process, the agent has obtained set of entities , which is the union of all unique entities discovered across all steps."
        },
        {
            "title": "2.3 Quantifying Information Collection and Efficiency",
            "content": "To guide an agent towards successfully solving IS tasks, its performance framework must value the entire reasoning process, not merely the final output. Our central thesis is that by explicitly quantifying the value of all required information discovered, we can create stronger signal for learning effective search strategies. To this end, we define principles to formalize the performance (the total information gain) and the efficiency (the gain per action) of the agents collection process. Information-Seeking Rate (ISR) Recall that denotes the set of target ground-truth entities for the task, with cardinality = R. is the set of entities actually obtained by the agent during its operation. The intersection therefore contains all required entities that were successfully retrieved. The information collection rate directly measures the fraction of required entities successfully obtained by the agent: ISR = O = O . (4) ISR [0, 1], and higher values indicate more thorough coverage of the required information. Information-Seeking Efficiency (ISE) While ISR measures completeness, the information collection efficiency reflects the average number of action steps to discover the target entity: ISE = , (5) where is the total number of steps of the solving trajectory. Higher ISE implies greater IS efficiency. The stability of measuring ISE is important for providing unbiased training signals. Proposition 1 (Variance of ISE). Let Xi denote the number of steps the agent takes to discover the i-th new entity in R. Therefore ISE = . Assume X1, . . . , Xn be i.i.d. random variables with finite mean µ > 0 and finite variance σ2, Xi > 0 almost surely, then: = i=1 Xi Var(ISE) = (cid:19) . (cid:18) 1 (6) This proposition shows that as the number of target entities grows, measuring ISE becomes more stable and reliable performance metric. The detailed proof is provided in Appendix A.2."
        },
        {
            "title": "3 Method",
            "content": "To enhance the information efficiency of the IS agent, our approach trains the model on calibrated task = q, together with the corresponding task-solving trajectory H. In prior IS agent training setups, the dataset typically contained only limited number of target entities (R). This design substantially restricts the potential improvement in information-seeking efficiency and, in turn, limits the agents overall capability. The limitation incurs two problems: With small volume of R, it is difficult to train the agent to retrieve information efficiently within limited context length. Our method relies on measuring the information-seeking efficiency ISE. As shown in Eq. (6), small set of target entities introduces measurement bias in the ISE metric. To overcome these shortcomings, we introduce WebLeaper, novel data synthesis framework specifically designed to boost information-seeking efficiency. Our method consists of two main components: (1) QA 4 Figure 3: An overview of WebLeaper. The reasoning structure is modeled as tree. root entity (question entity) connects to set of second-layer entities. (a) Version-I (Basic) constructs simple reasoning tree from single information source. (b) Version-II (Union) creates complex task by finding maximal union between two trees that share common set of relations within their subtrees (e.g., both have has_nationality). (c) Version-III (Reverse-Union) reverses the reasoning process. It provides fuzzed clues (third-layer entities) as question entities, forcing the agent to first deduce second-layer anchor entity (an entity from the second layer), then other relevant subtrees. synthesis pipeline for generating calibrated tasks, and (2) trajectory construction process for producing realistic task-solving sequences. We describe the QA synthesis pipeline and trajectory construction process in detail in the following subsections. For detailed walkthroughs of the examples for each synthesis version, please refer to Appendix A.6."
        },
        {
            "title": "3.1 Entity-Intensive Task Synthesis",
            "content": "3.1.1 Version-I: Basic In an information-seeking task, the reasoning structure matters. We use tree, denoted as Ti, to represent this structure, where nodes are entities and edges are relations between them. The IS agent must start with some known entities in the tree and reason along the edges to determine the target ones. To incorporate as many target entities as possible, we use this tree structure for its compact and hierarchical organization. Synthesizing such task = q, requires large volume of relevant entities, which is non-trivial. Following the one-entity-at-a-time collection strategy of prior work is prohibitively expensive. Therefore, we exploit the structured tables contained in Wikipedia articles, which encapsulate rich relational information. These tables naturally provide groups of entities connected by specific relationships, enabling us to efficiently construct the reasoning tree Ti. We crawled approximately 2 million tables from Wikipedia and applied multi-stage cleaning procedure, retaining only large, well-formed, and structurally homogeneous tables. The detailed data cleaning procedure and construction rationale are described in Appendix A.4. To construct the reasoning structure illustrated in Figure 3(a), we populate its layers using information from single table. The entities extracted from the table title form the root of the tree (i.e., the question entities). Next, we employ an LLM to select the most representative, non-redundant column of values from the tabletypically the primary keyas the second-layer entities (e.g., Czesław Miłosz). An 5 edge between the root entity and second-layer entity indicates that the table contains this entity. The third-layer entities are derived from the remaining columns of the table, with their values representing attributes of the corresponding second-layer entity (e.g., country: Poland, year: 1980). In this layer, an edge signifies that the second-layer entity possesses the given property defined by the third-layer entity. Each second-layer entity and its associated third-layer entities form subtree, which we denote as Si,j. These subtrees, each possessing set of relations Rel(Si,j) that connect its layers, represent cohesive units of information (e.g., specific laureate and all their details). The full reasoning tree Ti is thus composed of set of such subtrees {Si,j}. The question provides the root entities, while all entities in the subtrees (both second and third layers) constitute the final answer. The detailed construction process and the required reasoning path for the example task are explained in Appendix A.6.1. 3.1.2 Version-II: Union While effective, the reasoning structure of our basic tasks is derived from single sources, limiting their structural complexity and the scope of questions we can pose. To address this, we aim to construct tasks with more intricate reasoning structure that spans multiple information sources by uniting reasoning trees from our Basic version that share similar themes and structures. To generate more challenging questions, we propose uniting reasoning subtrees in Basic version that share similar themes and structures. naive approach, such as randomly combining subtrees, often results in semantically incoherent questions. To systematically discover the most substantial integration opportunities, our approach models this as Union operation, which identifies multiple reasoning trees whose respective subtrees share some common relations. The primary challenge is to systematically search the entire collection of trees to find all groups that are suitable for union. To avoid combinatorial explosion from enumerating all possible combinations, we develop an algorithm to efficiently discover only maximal unions. This problem is formally modeled as Maximal Biclique Enumeration (see Appendix A.5), which effectively identifies groups of reasoning subtrees and their shared subtree relations. As illustrated in Figure 3(b), the reasoning trees for Nobel Prize in Literature laureates and Booker Prize winners both contain subtrees where second-layer entities (authors) are connected to third-layer entities via relations like has_nationality and has_name. Our method identifies this shared subtree structure. Relations not shared across all sets of subtrees, such as has_gender (present only in the Nobel tree), are discarded during the union. Once maximal union is identified, we leverage an LLM to synthesize question based on the common features of the selected subtrees. For instance, the question Which authors have won both the Nobel Prize in Literature and the Booker Prize? requires identifying the two sets of laureates as intermediate Target Entities and then finding their intersection to produce the final Target Entities. The complete walkthrough is in Appendix A.6.2. 3.1.3 Version-III: Reverse-Union While the Union method generates complex, multi-source tasks, vulnerability remains: an agent could solve the query and use direct keyword searches on the constituent sources (e.g., search Nobel Prize winners, then Booker Prize winners). This approach circumvents the intended synthesis of information, reducing the cognitive load and failing to stimulate true reasoning capabilities similar to WebSailor (Li et al., 2025c). To address this, we introduce Reverse-Union, paradigm designed to enforce more robust cognitive workflow by reversing the standard reasoning flow. As illustrated in Figure 3(c), this method combines two stages to construct challenging task: Deductive Fuzz: This stage implements the fuzz by defining the Question Entities as set of 6 descriptive third-layer entities. Instead of being named directly, central anchor entity (an entity from the second layer) is described through its corresponding third-layer entities. In the example, the description the 1980s prize-winner that wrote novel about group of British boys stranded on an uninhabited island serves as clues in the form of Question Entities. An agent must first deduce from these clues to identify the anchor entity, William Golding. Union-based Search Construction: After fuzzing the anchor, this stage constructs the expansive search part of the task, ensuring the anchor serves only as bridge to the final answer. To achieve this, we first select specific third-layer entity from the anchors subtree (e.g., his country) to act as pivot. We then formulate the remainder of the question to compel an agent to use this pivot to launch new search across the unified trees. The final Target Entities are thus defined as the set of second-layer entities that share this pivot attribute (i.e., are also British) and satisfy the original intersection condition (i.e., winning both prizes). By structuring tasks this way, Reverse-Union prevents agents from succeeding with simple keyword searching and mandates more robust, multi-step reasoning process. The detailed process of question generation and the required reasoning path are explained in Appendix A.6.3. 3.2 Information-Guided Trajectory Construction After synthesizing the task, this section elaborates on the construction of task-solving trajectories. As shown in Eq.(3), our agent solves task within the ReAct framework (Yao et al., 2023). We equip the agent with the following tools: Search This action enables the agent to conduct Google search by several queries. The parameters of this tool are {queries, filter_year}, enabling temporal filtering of search results. This tool would return the top relevant URLs and their snippets as the observation. Visit This action enables the agent to visit multiple URLs. The parameters of this tool are {urls, goal}. This tool would return the summarized visited paragraphs as the observation. After generating large set of trajectories by executing our constructed tasks with an open-source model, we apply filtering procedure to select high-quality examples for training. Our goal is to retain trajectories that demonstrate both accuracy in collecting the required entities and efficiency in the use of actions, in accordance with the metrics defined in Section 2.3. Specifically, we impose the following selection criteria: Coverage Criterion. We require that the trajectory achieve sufficient completeness in information collection. Formally, we keep only those trajectories whose ISR satisfies ISR > α, where α is predefined coverage threshold. To compute ISR, we accumulate the obtained target entities in all actions. We compute ISR as Eq.( 4). Efficiency Criterion. We further require that the trajectory maintain high efficiency in discovering useful entities. This translates into selecting those trajectories whose ISE satisfies ISE > β, where β is predefined efficiency threshold. For ISE, we accumulate the obtained target entities in Visit actions. The reason for not including Search in ISE is that we observe entities found in Search are less precise and would be updated by the following Visit action. We compute ISR as Eq.(5). Through this filtering process, we ensure that the retained trajectories are both accurate in acquiring the target entities and efficient in their action usage, providing strong supervision signals for training agents to perform precise and effective information-seeking."
        },
        {
            "title": "3.3 Reinforcement Learning with Hybrid Reward Systems",
            "content": "Following supervised fine-tuning (SFT) on the trajectories generated via our information-guided method (Section 3.2), we further enhance the agents policy using reinforcement learning (RL). critical com7 ponent of RL is the reward function, which provides the training signal. However, standard reward mechanisms are fundamentally misaligned with the entity-intensive tasks synthesized by WebLeaper. The most common approach, simple binary reward (e.g., success/failure), suffers from extreme sparsity. This issue is dramatically exacerbated in our setting, where task may require dozens of entities; rewarding the agent only upon perfect completion of such large set makes positive feedback so rare that effective learning becomes nearly impossible. Furthermore, the very methods for implementing reward functioneven more granular onepresent their own intractable challenges. On one hand, conventional automated metrics like Exact Match or word-level F1 scores are too brittle. They cannot gracefully handle minor semantic variations (e.g., USA vs. United States) and would incorrectly penalize the agent, problem that compounds severely across large entity set. On the other hand, deploying more sophisticated LLM-as-a-Judge to evaluate correctness seems promising, but it struggles with scalability and reliability. Asking judge model to accurately verify long list of entities in single assessment imposes high cognitive load, leading to inconsistent scores, while running it for every single entity is prohibitively expensive for RL. This leaves us in predicament: simple methods are too inaccurate, and accurate methods are too impractical. To overcome these intertwined challenges, we design Hybrid Reward System. This system provides nuanced, accurate, and cost-effective training signal, specifically tailored to the unique demands of our entity-intensive tasks while maintaining compatibility with standard benchmarks. It is composed of two core components: granular, F-score-based reward for our synthesized tasks, and the retention of conventional reward functions for existing public benchmark data. Granular F-Score for Entity-Intensive Tasks. For the approximately 500 entity-intensive QA pairs reserved for RL, we develop fine-grained reward function based on the ISR metric. Recall that ISR = RO (Equation 4) measures the recall of the retrieved entities. Building upon ISR as our measure of recall, we designed more comprehensive reward signal that also accounts for precision. An agent could otherwise achieve high score by retrieving many irrelevant entities. Furthermore, practical reward function must gracefully handle minor semantic variations (e.g., USA vs. United States). Therefore, we define soft versions of precision and recall by introducing scoring function s(eo, er) [0, 1] that measures the semantic similarity between retrieved entity eo and ground-truth entity er R. Instead of monolithic judgment, we evaluate at the individual entity level. To balance accuracy and efficiency, we first categorize entities in the ground-truth set by their semantic type (e.g., person names, dates, organizations) and assign an appropriate evaluation modality to each category. For instance, person names might be evaluated using near-exact match to handle minor variations, while more abstract concepts might require targeted LLM-as-a-Judge assessment. Based on this semantic scoring function s, we define our soft recall Rc (a generalization of ISR) and soft precision P: Rc = = 1 1 erR eoO s(eo, er) max eoO s(eo, er) max erR (7) (8) This formulation credits the agent for finding entities that are semantically equivalent to the ground truth. We then aggregate and Rc using weighted F-score to compute the final reward RWebLeaper. This addresses potential biases in our synthesized ground-truth set R, which may be slightly overor under-complete. The reward RWebLeaper is defined as: RWebLeaper = (1 + ω2) Rc ω2P + Rc (9) where ω is hyperparameter that balances the importance of precision and recall. value of ω > 1 prioritizes recall (aligning more closely with the original goal of ISR), while ω < 1 emphasizes precision. 8 Table 1: Results on multiple benchmarks. All benchmarks except WideSearch report Pass@1. WideSearch reports Success Rate (SR), Row F1, and Item F1. Bold scores indicate the highest values among all opensource agents. and stand for base and comprehensive training setting. Model / Framework BrowseComp GAIA xbench-DS Seal-0 WideSearch SR Row F1 Item F1 Claude-4-Sonnet OpenAI-o3 OpenAI DeepResearch ASearcher-Web-32B DeepDive-32B DeepDiver-V2-38B MiroThinker-32B-DPO-v0.2 Kimi-K2-Instruct-1T WebExplorer-8B WebDancer-QwQ-32B WebSailor-32B WebShaper-QwQ-32B WebLeaper-Union WebLeaper-Reverse-Union WebLeaper-Reverse-Union Proprietary Agents 68.3 70.5 67.4 64.6 66.7 Open-Source Agents 52.8 64.1 57.7 50.0 51.5 53.2 53.3 69.9 67.0 73. 42.1 50.5 53.0 50.0 53.7 38.3 53.3 35.0 62.3 66.0 72.0 12.2 49.7 51.5 5.2 14.8 13.4 13.0 14.1 15.7 3.8 10.5 22.1 23.0 38.8 18.9 21.3 35.1 37.2 48.6 2.3 4.5 1.1 0.0 0.0 0.0 4.0 4.0 4.0 31.7 34.0 29.7 9.3 2.1 9.9 22.2 25.8 31.0 57.9 52.6 54.4 34.5 5.5 31.5 34.5 40.8 48.8 Hybrid Integration. For tasks originating from existing training QA, we retain their original, often binary, reward functions, which we denote as Rlegacy. Our final hybrid reward function, Rhybrid, is therefore conditional on the tasks origin, ensuring that the agent is evaluated appropriately for each data source: Rhybrid(HT, ) = RWebLeaper(O, R) Rlegacy(O, R) if is from WebLeaper otherwise (10) This hybrid reward signal provides rich, fine-grained feedback on our entity-intensive tasks while maintaining compatibility with established evaluation protocols. The agents policy is then optimized against this comprehensive reward using Group Relative Policy Optimization (GRPO) (Shao et al., 2024), enabling it to refine its information-seeking strategies effectively. Policy Optimization with Hybrid Reward. The agents policy, denoted πθ parameterized by θ, is optimized using GRPO. For each task in our RL dataset, we sample group of trajectories {H1, . . . , Hk} from the current policy πθ. Each trajectory Hi is assigned reward Ri = Rhybrid(Hi, ). Instead of using learned value function, GRPO estimates the advantage for each trajectory by standardizing its reward relative to the others in the group: ˆAi = Ri mean({Rj}k std({Rj}k j=1) j=1) + ϵstd (11) where ϵstd is small constant for numerical stability. This group-relative advantage ˆAi is applied to every timestep within the trajectory Hi. The policy is then updated by minimizing clipped surrogate objective, similar to PPO (Schulman et al., 2017), which is averaged over all trajectories in the group and all timesteps in each trajectory. The GRPO loss function is: LGRPO(θ) = {Hi}k i=1 πθ (cid:34) 1 i=1 1 Hi Hi t=1 (cid:32) (cid:18) min ri,t(θ) ˆAi, clip(cid:0)ri,t(θ), 1 ε, 1 + ε(cid:1) ˆAi (cid:19)(cid:33)(cid:35) (12) where ri,t(θ) = πθ (ai,tsi,t) πold(ai,tsi,t) is the importance sampling ratio at timestep of trajectory i, and ε is the clipping hyperparameter. By optimizing this loss, the policy πθ learns to favor actions that lead to 9 Table 2: Ablation study on training results across different data sources (for efficiency considerations, we use the WideSearch (English subset) and BrowseComp (200 subset), while the full sets are used for the other benchmarks). Numbers in parentheses denote the difference compared to training only with the WebSailor-V2-5k data. denotes mixed version that includes the WebSailor-V2-5k data. Data Source BrowseComp WideSearch 25.17 24.50 33.15 38. GAIA 67.69 66.02 Seal-0 xbench-DS 34.23 33.93 60.00 62. Avg. 44.05 45.21 20.67 (-4.50) 27.50 (+2.33) 27.67 (+2.50) 32.26 (-0.89) 41.70 (+8.55) 44.07 (+10.92) 40.78 (-26.91) 69.90 (+2.21) 66.99 (-0.70) 30.03 (-4.20) 35.14 (+0.82) 37.24 (+3.01) 58.33 (-1.67) 62.33 (+2.33) 66.00 (+6.00) 36.41 (-7.64) 47.31 (+3.26) 48.39 (+4.34) WebSailor-V2-5k WebSailor-V2-10k Basic-5k Union-5k Reverse-Union-10k trajectories with higher-than-average rewards within sampled group, effectively internalizing the complex preferences defined by our Rhybrid function."
        },
        {
            "title": "4.1 Setup",
            "content": "Benchmarks We conduct extensive evaluations of our method on five challenging QA benchmarks that demand complex information-seeking capabilities, namely BrowseComp (Wei et al., 2025), GAIA (Mialon et al., 2023), xbench-DeepSearch (xbench-DS) (Xbench-Team, 2025), Seal-0 (Pham et al., 2025), and WideSearch (Wong et al., 2025). For GAIA, we adopt the 103-sample text-only validation subset (Li et al., 2025d), while for all other benchmarks, we utilize their complete test sets. Baselines We select representative set of mainstream and competitive information-seeking agents as our baselines, including proprietary agents (Claude-4-Sonnet (Anthropic, 2025), OpenAI-o3 (OpenAI, 2025a), OpenAI DeepResearch (OpenAI, 2025b)) and open-source agents (ASearcher (Gao et al., 2025), DeepDive (Lu et al., 2025), DeepDiver-V2 (Team), MiroThinker (Team et al., 2025b), Kimi-K2 (Team et al., 2025a), WebExplorer (Liu et al., 2025), WebDancer (Wu et al., 2025a), WebSailor (Li et al., 2025c), WebShaper (Tao et al., 2025)). Training Configurations To maintain the basic deep search ability, we combine our data with 5,000 WebSailor-V2 (Li et al., 2025b) data to train the model. We separately merge 5,000 WebSailor-V2 data with Basic, Union, and Reverse-Union data of WebLeaper, which stimulates the IS ability to larger degree (with α in ISR set to 0.3 and β in ISE set to 0.1). We employ Qwen3-30B-A3B-Thinking-25071 as the base model, trained using the Megatron framework2. This is our default base setting in which most experiments are conducted. Comprehensive and Realistic Settings To more rigorously evaluate whether the training data of WebLeaper can remain effective under more comprehensive and realistic scenarios, we introduce the comprehensive setting. We mix WebLeaper data into the corpus of Tongyi-DeepResearch-30B-A3B, covering both the supervised fine-tuning and reinforcement learning stages, to examine its overall impact on performance. It is worth noting that this serves only as supplementary setting applied in certain experimental sections. Unless otherwise specified, we adopt the base WebLeaper experimental configuration by default. Evaluation Metrics and Inference Hyper-parameters The overall evaluation follows the settings specified by each benchmark. For BrowseComp, GAIA, xbench-DS, and Seal-0, we report the pass@1 scores obtained via LLM-as-a-judge evaluation as the final results. For WideSearch, we report the success 1https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507 2https://github.com/NVIDIA/Megatron-LM 10 Figure 4: Ablation study results on information-guided trajectory construction strategies. rate (SR) for fully retrieving all target results, along with two F1 scoresRow F1 and Item F1which are computed using combination of string matching and LLM-as-a-judge evaluation, in alignment with the official evaluation protocol. During LLM inference, we configure the sampling parameters (temperature and top-p) to 0.6 and 0.95, respectively."
        },
        {
            "title": "4.2 Overall Performance",
            "content": "Base Setting As shown in Table 1, WebLeaper achieves state-of-the-art performance compared to mainstream open-source agents on five challenging information-seeking QA benchmarks. Notably, on benchmarks other than BrowseComp and WideSearch, it even delivers performance comparable to, or surpassing, that of agents built on Claude-4-Sonnet and OpenAI-o3. Even on the highly challenging BrowseComp benchmark, WebLeaper significantly outperforms Kimi-K2-Instruct-1T, despite the latter having much larger parameter scale. It is also worth noting that the Reverse-Union data, which incorporates greater task complexity on top of the Union data, employs an fuzz strategy that further facilitates the models ability to integrate information-seeking with planning and reasoning, thereby enhancing its overall information-seeking QA capability. Overall, the observed performance improvements validate that our proposed approachesentityintensive task synthesis and information-guided trajectory constructionsignificantly enhance the agents information-seeking capabilities, even under modest parameter budget. Comprehensive Setting We also train our method on the comprehensive setting, and compare it to more competitive methods. The results are shown in Figure 1. WebLeaper reaches 73.2 on GAIA, 38.8 on BrowseComp, and 72.0 on xbench-DeepResearch. On the harder WideSearch benchmark, WebLeaper also attains the highest Success Rate and Item-F1, clearly outperforming all competitors. These results demonstrate that our approach generalizes well and remains effective even when evaluated under the comprehensive and realistic training setting."
        },
        {
            "title": "4.3 Capability Gains Induced by Entity-Intensive Task Synthesis",
            "content": "To investigate the effectiveness of our entity-intensive task synthesis method, we conduct comparative analysis against training solely on the WebSailor-V2 dataset (using 5,000 and 1,000 samples, respectively), synthetic corpus specifically designed to stimulate the agents deep search capability. As shown in Table 2, we investigate the impact of different entity-intensive task synthesis strategies through an ablation study on all these benchmarks. The Basic setting exhibits substantial drops across all three datasets compared to WebSailor-V2-5k. This poor performance can be attributed to the inherent limitations of the Basic data construction method: tasks generated under this setting tend to be overly simple, allowing the model to infer complete answers from only few information sources. Such shortcut patterns encourage the model to overfit to superficial cues rather than learning to integrate diverse information, ultimately impairing generalization. 11 Figure 5: Effectiveness and efficiency comparison between WebLeaper and WebSailor-V2. In contrast, the Union strategy consistently outperforms WebSailor-V2-5k, achieving an average improvement of +3.26. By combining heterogeneous information sources and increasing the complexity of task construction, Union mitigates the shortcut problem inherent in Basic, forcing the model to reason over dispersed and complementary evidence. This leads to more robust performance across datasets and demonstrates the effectiveness of the proposed data construction approach. Furthermore, compared to Union, Reverse-Union introduces certain degree of reasoning complexity into the information-seeking process, making it more challenging for the model to readily identify where to begin entity retrieval. This design particularly enhances the models planning and decision-making capabilities in information-seeking tasks. The improvement in these abilities is clearly reflected in performance, leading to substantial and widespread gains across all benchmarks. 4.4 Impact of Information-Guided Trajectory Construction We compare the proposed information-guided trajectory construction strategies across ISR-Only, ISE-Only, and ISR+ISE on three representative benchmarksGAIA, BrowseComp, and WideSearchto examine the independent and combined effects of ISE and ISR. On GAIA and BrowseComp, ISR+ISE achieves the best performance, suggesting that integrating precision and efficiency constraints produces trajectories that are both goal-directed and concise, thereby reducing redundant exploration. This indicates that in more complex browsing tasks, relevance and efficiency constraints complement each other to generate higher-quality trajectories. In contrast, on WideSearch, the three strategies deliver comparable results, with performance differences falling within the margin of variance. This suggests that for broad search tasks, the specific choice of trajectory filtering plays less critical rolelikely because training on entity-intensive synthesized data already provides strong broad search capabilities. 4."
        },
        {
            "title": "Joint Gains in Efficiency and Effectiveness",
            "content": "As illustrated in Figure 5, WebLeaper consistently outperforms the baseline in terms of both effectiveness and efficiency. In the WideSearch and BrowseComp benchmarks, our approach achieves markedly higher performance scores while requiring fewer average action rounds, indicating that the search process is not only more accurate but also more efficient. Similarly, in the GAIA and xbench-DS tasks, our method improves effectiveness while simultaneously reducing the operational cost. This demonstrates that our design enables more targeted search strategy, resulting in reduced interaction steps without sacrificingand in fact enhancingthe quality of the results. Overall, these results validate that our proposed method achieves superior joint optimization of informationseeking efficiency and task performance compared to the baseline. This reflects our key insight: an agent should not merely learn to search, but rather learn to search efficiently and wisely, thereby achieving better balance between efficiency and effectiveness. 12 Figure 6: Figure shows the training curve of the hybrid reward system, indicating that using the WebLeaper data leads to stable increase in reward. We terminated the experiment at 135 steps when web access resources were exhausted and evaluated the results at this point."
        },
        {
            "title": "4.6 Reinforcement Learning using WebLeaper",
            "content": "Table 3: RL Results on comprehensive setting. All benchmarks except WideSearch report Avg Pass@1 from 3 rollouts. WideSearch reports Success Rate (SR), Row F1, and Item F1. BrowseComp GAIA xbench-DS 37.80 69. 69.0 WideSearch SR 1.5 Row F1 Item 23.0 45.4 38.8 (+1.0) 73.2 (+3.3) 72.0 (+3.0) 4.0 (+2.5) 31.0 (+8.0) 48.5 (+3.1) SFT SFT+RL We further evaluate our approach through reinforcement learning, adopting the Additional Settings for More Comprehensive and Realistic Training (see Section 4.1) where WebLeaper data is mixed into larger training corpus for both SFT and subsequent RL stages. As demonstrated by the results in Table 3 and Figure 6, using WebLeaper data for RL yields consistent and significant improvements. The results table shows that after RL fine-tuning, the model comprehensively surpasses the SFT-only baseline across all benchmarks. This positive performance trend is echoed by the reward curve in Figure 6, which exhibits stable and continuous upward trajectory throughout the training process. This indicates that the model is effectively learning from the reward signals derived from the WebLeaper data, progressively refining its information-seeking strategy towards greater efficiency and accuracy. Even with the experiment concluding at 135 steps, the clear learning trend underscores the potential for further gains. The results strongly validate the effectiveness of the WebLeaper dataset. It not only serves as robust foundation for supervised fine-tuning but also provides high-quality signal for RL, successfully guiding the agent to master more sophisticated and optimal information-seeking behaviors."
        },
        {
            "title": "5 Related Work",
            "content": "5."
        },
        {
            "title": "Information Seeking Agent",
            "content": "LLM-powered information-seeking agents can be broadly categorized into 3 streams: (1) enhancing core models via supervised fine-tuning (Zeng et al., 2023; Wu et al., 2025a; Li et al., 2025c;b; Tao et al., 13 2025; Su et al., 2025; Fang et al., 2025a); (2) advancing agent architecture for improved planning and robustness (Qiao et al., 2025; Xu et al., 2025a; Li et al., 2025a); and (3) developing multi-agent systems for collaborative problem-solving (Wu et al., 2023; Hong et al., 2024). Our work aligns with the first category but addresses key limitation. Prior methods often train on tasks focused on correctness with single-fact answers, which is insufficient for large-scale information gathering. We posit that the number of entities in an answerits entity richnessis critical dimension for evaluating an agents completeness and efficiency. This paper aims to bridge this gap by creating and utilizing entity-rich QA data to enhance agent capabilities for comprehensive information acquisition."
        },
        {
            "title": "5.2 Agent Data Synthesis",
            "content": "Synthetic data generation is pivotal for agent training, with primary applications in tool use (Wu et al., 2025a; Tao et al., 2025; Shen et al., 2025; Fang et al., 2025b), code generation (Jimenez et al., 2024; SHEN et al., 2025; Xu et al., 2025c; Shao et al., 2025), and GUI automation (Xu et al., 2025b; Sun et al., 2025; Pahuja et al., 2025). These efforts primarily combat data scarcity. Within the information-seeking domain, existing data synthesis approaches increase task difficulty through multi-step reasoning (Wu et al., 2025b;a; Tao et al., 2025) or long-horizon planning (Qiao et al., 2025). We contend that such methods often overlook the semantic richness of the training data itself. In contrast, our approach centers on synthesizing QA data with high entity-level complexity. We hypothesize that this focus on data semantics is crucial and complementary path to improving agent reasoning and world knowledge alignment."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we addressed the critical challenge of low search efficiency in LLM-based informationseeking agents, bottleneck that constrains their overall performance. We argued that the sparsity of target entities in conventional training tasks is primary contributor to this inefficiency. To overcome this, we introduced WebLeaper, novel framework for constructing entity-intensive IS tasks and generating efficient solution trajectories. By formulating IS as tree-structured reasoning problem and systematically increasing task complexity through our Basic, Union, and Reverse-Union task synthesis variants, we created rich training environment. Furthermore, our information-guided trajectory curation, using ISR and ISE metrics, ensures that the agent learns from solutions that are both accurate and efficient. Our extensive experiments demonstrated that WebLeaperconsistently improves performance across five challenging benchmarks, validating that enhancing search efficiency is powerful lever for boosting the overall capabilities of IS agents."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Towards general agentic intelligence via environment scaling, 2025a. URL https: //arxiv.org/abs/2509.13311. Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, et al. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311, 2025b. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976, 2025. Gemini. Gemini deep research, 2025. URL https://gemini.google.com/app. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VT F8yNQM66. Baixuan Li, Yunlong Fan, Tianyi Ma, Miao Gao, Chuanqi Shi, and Zhiqiang Gao. Raspberry: Retrievalaugmented monte carlo tree self-play with reasoning consistency for multi-hop question answering. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1125811276, 2025a. Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor-v2: Bridging the chasm to proprietary agents via synthetic data and scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2509.13305. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent, 2025c. URL https://arxiv.org/abs/2507.02592. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025d. doi: 10.48550/ARXIV.2504.21776. URL https://doi.org/10.48550/a rXiv.2504.21776. Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501, 2025. 15 Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, and Yuxiao Dong. Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl. arXiv preprint arXiv:2509.10446, 2025. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. OpenAI. Introducing openai o3 and o4-mini, 2025a. URL https://openai.com/index/introducing-o 3-and-o4-mini/. OpenAI. Deep research system card, 2025b. URL https://cdn.openai.com/deep-research-system-c ard.pdf. Vardaan Pahuja, Michael Chang, Hong-Lak Lee, Igor Mordatch, and Sergey Levine. Explorer: Scaling exploration-driven web trajectory synthesis for multimodal web agents, 2025. Perplexity. Perplexity deep research, 2025. URL https://www.perplexity.ai/. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models. arXiv preprint arXiv:2506.01062, 2025. Zile Qiao, Shen Huang, Jialong Wu, Kuan Li, Wenbiao Yin, Xinyu Wang, Liwen Zhang, Baixuan Li, Zhengwei Tao, Weizhou Shen, Xixi Wu, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, and Jingren Zhou. WebResearcher: Unleashing unbounded reasoning capability in long-horizon agents, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zheng-Xin Shao, Yeyun Gong, Yelong Shen, Jian Jiao, Ruoss Jia, Yujiu Yang, Nan Duan, and Weizhu Chen. Case2Code: Scalable synthetic data for code generation. In Proceedings of the 31st International Conference on Computational Linguistics. Association for Computational Linguistics, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Haiyang SHEN, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, and Yun Ma. Shortcutsbench: large-scale real-world benchmark for API-based agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=kKILfPkhSz. Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, and Yun Ma. Ragsynth: Synthetic data for robust and faithful rag component optimization, 2025. URL https://arxiv.org/abs/2505.10989. Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Scaling agents via continual pre-training, 2025. URL https://arxiv.org/abs/2509.13310. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. OS-genesis: Automating GUI agent trajectory construction via reverse task synthesis. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 55555579, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.277. URL https://aclanthology.org/2025.acl-long.277/. 16 Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. WebShaper: Agentically data synthesizing via information-seeking formalization, 2025. Kimi Team. Kimi researcher tech report, 2025. URL https://moonshotai.github.io/Kimi-Researche r/. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. MiroMind AI Team et al. Mirothinker: An open-source agentic model series trained for deep research and complex, long-horizon problem solving, 2025b. OpenPangu Team. Openpangu deepdiver-v2: Multi-agent learning for deep information seeking, 2025b. URL https://ai. gitcode. com/ascend-tribe/openPangu-Embedded-7B-DeepDiver. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency, 2025a. URL https://arxiv.org/abs/2505.22648. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. WebWalker: Benchmarking LLMs in web traversal. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10290 10305, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.508. URL https://aclanthology.org/2025.acl-long.508/. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. Xbench-Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents, 2025a. URL https://arxiv.org/abs/2502.12110. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. AgentTrek: Agent trajectory synthesis via guiding replay with web tutorials. In International Conference on Learning Representations, 2025b. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. KodCode: diverse, challenging, and verifiable synthetic dataset for coding, 2025c. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: Endto-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. 17 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. AgentTuning: Enabling generalized agent abilities for LLMs. arXiv preprint arXiv:2310.12823, 2023. URL https: //arxiv.org/abs/2310.12823."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Declaration on the Use of LLMs We declare that the use of LLMs during the preparation of this manuscript was strictly limited to languagerelated assistance, such as sentence refinement and grammatical correction. All substantive content was independently authored by the authors and rigorously reviewed and verified following any LLM-assisted modifications. During the experiments, all usage of LLMs was solely for academic research purposes, with no inappropriate applications. Detailed experimental settings are provided in the Experiments section of this paper. No other reliance on LLMs is involved in this work. A.2 Proof of Proposition 1 This appendix provides the detailed mathematical derivation for Proposition 1, as presented in Section 2.3. The purpose of this proof is to formally establish that the variance of the Information-Seeking Efficiency (ISE) metric is inversely proportional to n, the number of required entities. This property, Var(ISE) = O(1/n), demonstrates that ISE becomes an increasingly stable and reliable performance measure as the complexity of the task (i.e., the size of n) grows. Proof. Let Xi be the number of steps the agent takes to discover the i-th new entity in the required set R. We assume {Xi}n i=1 are independent and identically distributed (i.i.d.) random variables with mean E[Xi] = µ and variance Var(Xi) = σ2. The total number of steps is = i=1 Xi = entity, defined as = 1 i=1 Xi. Let be the average number of steps to find one required . By definition, ISE = n/T = 1/X. From the properties of i.i.d. random variables, the mean and variance of are: E[X] = µ, σ2 Var(X) = . (13) (14) We are interested in the variance of ISE, which is function of the random variable X. Let this function be (X) = 1/X. We can approximate the variance of ISE using the Delta method, which states that for function with non-zero derivative at µ: Var( (X)) (cid:0) (E[X])(cid:1)2 Var(X). First, we compute the derivative of (x) = 1/x, which is (x) = x2. Evaluating this derivative at the mean µ: (µ) = µ2. Now, substituting this and the variance of from Equation (14) into the Delta method formula: Var(ISE) (cid:16) µ2(cid:17)2 σ2 = 1 µ4 σ2 = (cid:19) . (cid:18) 1 This completes the proof. A.3 Data Statistics Figure 7 illustrates the distribution of our training data. Figure 8 displays the entity count distribution of our training data. significant portion of our samples contain at least 100 entities, underscoring the inherent difficulty of our dataset. As formalized in Equation 6, this complexity is crucial for robustly measuring efficiency, which in turn leads to improved overall performance. 19 Figure 7: The distribution of our training data. A.4 Data Cleaning and Basic Task Construction This section elaborates on the data processing and construction methodology for the Basic version tasks introduced in Section 3.1.1. Rationale for Tree Structure In information-seeking tasks, the reasoning structure is paramount. We chose tree structure for our basic tasks because it offers compact and hierarchical organization of entities. This structure is highly efficient for representing large number of interconnected entities that stem from common query concept, mirroring many real-world information-gathering scenarios. reasoning tree is composed of root (question entity) and set of subtrees, where each subtree represents cohesive unit of information. Multi-Stage Table Cleaning To ensure the quality and suitability of the data used for task synthesis, we crawled approximately 2 million tables from Wikipedia and subjected them to rigorous multi-stage cleaning procedure. This was essential because raw web tables are often noisy and inconsistent. The stages were as follows: Size Filtering: We first discarded tables that were either too small (fewer than 10 rows or 3 columns) to capture meaningful relational information, or too large (more than 200 rows or 20 columns) to be processed efficiently and form coherent task. Semantic and Structural Filtering: We then removed semantically irrelevant columns that frequently appear in web tables, such as those containing serial numbers, notes, or references. Tables with significant formatting errors (e.g., numerous merged cells that disrupt the relational structure) were also excluded. Isomorphism and Homogeneity: Finally, we retained only groups of isomorphic tables (tables sharing the same column headers and structure). This step was crucial for ensuring structural homogeneity across our dataset, which is prerequisite for identifying common subtree structures needed for the Union operation described later. The resulting collection contains clean, well-structured tables with set of meaningful fields as columns and multiple rows, where each row can be transformed into subtree. Reasoning Tree Population To construct the three-layer reasoning tree from single table, we populate the layers as follows: First Layer (Question Entities): Entities mentioned in the tables title or caption are extracted to form the root of the tree. Second Layer (Roots of Subtrees): We employ an LLM to analyze the tables columns and select one that contains no duplicate entries. This column is treated as the key, and its values become the 20 Figure 8: Entity Count Distribution in Training Data. significant portion of our samples contains at least 100 entities, underscoring the inherent difficulty of our dataset. This complexity, as formalized in Equation equation 6, is crucial for robustly measuring efficiency, which in turn contributes to improved overall performance. second-layer entities of the tree. Each of these entities serves as the root of subtree. The LLM is effective at identifying columns like Name or Title that serve this unique identification purpose. Third Layer (Leaves of Subtrees): The values in the remaining columns of the table constitute the third layer, representing the leaf entities associated with each second-layer entity. A.5 Maximal Union Algorithm for Task Synthesis This section provides the formal definition and algorithmic implementation for discovering maximal union groups, as introduced in Section 3.1.2. The core of our approach is to reformulate the search for compatible reasoning trees as Maximal Biclique Enumeration (refer to 1 problem on bipartite graph. Problem Formulation Let Tbase = {T1, T2, . . . , TN} be our collection of basic reasoning trees. We first construct bipartite graph = (U, V, E), where = Tbase is the set of all trees, and is the set of all unique relation names found within the subtrees across all trees in Tbase. An edge (Ti, vj) exists if the relation vj is present in any subtree of tree Ti (i.e., vj Rel(Ti), where Rel(Ti) = (cid:83) Rel(Si,k)). In this construction, maximal union directly corresponds to maximal biclique (U , ), where is set of trees and is set of their common relations. Our goal is to find all such maximal bicliques that satisfy certain size and semantic constraints. Formally, we seek to find all maximal pairs (U , ) that satisfy: find maximal (U , ) subject to Ti , Rel(Ti), kmin, mmin. (15) Here, maximality means that no other tree can be added to and no other relation can be added to without violating the biclique property. Solving this by reformulating it as standard maximal biclique enumeration problem is computationally efficient compared to an exhaustive search. Algorithm and Implementation Details Input: collection of base reasoning trees Tbase; minimum number of trees for valid union, kmin; 21 minimum number of common relations, mmin. Goal: To find all maximal union groups, which are the solutions (U , ) to Eq. (15) that also satisfy the semantic matching criteria below. Subtree Relation Matching Criteria: To ensure the semantic coherence of unions, we impose strict matching criteria. For relations connecting the second and third layers, we require they share the same standardized name, data type, and domain. For the second-layer entities themselves (the roots of the subtrees), we relax this constraint, requiring only match in data type and domain. This flexibility allows for the union of trees with conceptually similar but differently named second-layer entities (e.g., fusing tree where entities are Authors with another where they are Writers). Output: set of maximal union groups , where each element is tuple U, that meets the specified criteria. The process is detailed in Algorithm 1. Algorithm 1: Maximal Union Identification Algorithm Input: collection of base reasoning trees Tbase, minimum trees kmin, minimum common relations mmin. Output: set of maximal union groups . 1 ; // 1. Construct the bipartite graph from trees and subtree relations 2 Let be the set of trees from Tbase and be the set of unique standardized relation names found within the subtrees of all trees in Tbase; 3 Construct the graph = (U, V, E) where an edge (u, v) exists if tree contains the relation in its subtrees (i.e., Rel(u)); // 2. Enumerate maximal bicliques from the graph 4 EnumerateMaximalBicliques(G); ; // 3. Filter and validate bicliques to form final union groups // Leverages standard algorithms like MICA or Eclat 5 for each maximal biclique (U, V) in do // Check size constraints from Eq. (1) if < kmin or < mmin then continue; // Validate semantic compatibility of second-layer entities Let Tid, Did be the type and domain of the second-layer entities of the first tree in U; is_compatible true; for each tree do if us second-layer entity type = Tid or domain = Did then is_compatible false; break; // If all checks pass, add to the set of valid union groups if is_compatible then {U, V}; 6 7 8 10 11 12 13 14 16 return ; A.6 Detailed Examples of Task Synthesis This section provides detailed explanations and reasoning walkthroughs for the examples of the three task synthesis versions presented in Section 3 and Figure 3. 22 A.6.1 Version-I: Basic The goal of the basic version is to create task with clear, hierarchical reasoning structure derived from single, self-contained set of entities. Example Question: Who were the Nobel Prize winners in Literature between 1980 and 1990? Please include their name, country, award year, and gender. Construction Process: The task is constructed from single Wikipedia table, forming reasoning tree. The layers shown in Figure 3(a) are populated as follows: First Layer (question entities): Derived from the tables title and specified constraint, forming the querys scope: Literature Nobel Prize, year 19801990. Second Layer (subtree roots): Populated from the tables key column (e.g., author names): Czesław Miłosz, William Golding, . . . . Third Layer (subtree leaves): Consists of values from the remaining columns, representing attributes for each second-layer entity. For example: man, Poland, 1980 for Czesław Miłosz. The edges connecting the second to the third layer represent relations like has_gender, has_country, has_award_year. Reasoning Path: An agent is expected to follow this hierarchical structure: Identify Scope: Recognize the Question Entities from the query: Nobel Prize in Literature, 19801990. Retrieve Second-Layer Entities: Retrieve the second-layer entities, which are the authors: Czesław Miłosz, William Golding, .... Gather Attributes: For each second-layer entity, follow the relations to retrieve their associated third-layer entities, such as Poland, 1980, man for Czesław Miłosz. A.6.2 Version-II: Union This version increases structural complexity by requiring the agent to perform relational operations across distinct reasoning trees. Example Question: Which authors have won both the Nobel Prize in Literature and the Booker Prize? For each, provide their name, nationality and the year they won the Nobel. Construction Process: Once maximal union is identified (e.g., between the reasoning trees for Nobel Prize laureates and Booker Prize winners, which share common relations like has_nationality within their subtrees), an LLM generates task requiring information integration. The LLM is prompted to find an interesting relationship, such as the intersection of the two sets of second-layer entities (authors), and then weave this logic into natural language question. Reasoning Path: The task is constructed from maximal union of two distinct reasoning trees. To solve this, an agent must: Retrieve First Entity Set: Identify the first concept, Nobel Prize in Literature, and retrieve the full set of corresponding second-layer entities from the first tree, RNobel (T1). Retrieve Second Entity Set: Identify the second concept, Booker Prize, and retrieve its full set of second-layer entities from the second tree, RBooker (T2). Find Intersection: Perform relational join to find the intersection of the two sets of second-layer entities based on name. The final Target Entities are the entities present in both sets, such as {William Golding, J.M. Coetzee, . . . }, along with their requested third-layer attributes. 23 Figure 9: Distribution of Search, Visit, and total tool call. A.6.3 Version-III: Reverse-Union This version introduces challenging cognitive workflow by intentionally obfuscating the querys entry points. Motivation and Design: The Union method, while creating multi-source tasks, has vulnerability: an agent could solve it with simple keyword searches for each source, bypassing deeper reasoning. Reverse-Union inverts the information flow, forcing an agent to first deduce core anchor entity (a second-layer entity) from descriptive clues and then use that entity as pivot to expand its search. Example Question: Who are the authors from the same country as the 1980s prize-winner that wrote novel about group of British boys stranded on an uninhabited island, and who have also won both this reward and the Booker Prize? For each of them, what is their name, country, and the respective years they won each award? Construction Process: The construction builds upon the unified space from Version-II with reverse logic: Source: We use the unified information space from the Nobel and Booker prize union. Select Anchor: An entity at the intersection of the second layers is chosen as the anchor, e.g., William Golding. Obfuscate Anchor: Instead of naming the anchor, unique descriptive clues based on its third-layer attributes are generated: the 1980s prize-winner and wrote novel about... British boys... These clues become the Question Entities. Create Union Trigger: third-layer attribute of the anchor, his nationality (British), is selected as the pivot for the next stage of the query. Required Reasoning Process: To solve this task, an agent must execute two-stage process: Deduction Stage: The agent must first resolve the descriptive clues (which are third-layer entities) to identify the second-layer anchor entity. The clues 1980s prize-winner and novel about stranded British boys uniquely point to William Golding. This inferential step is crucial. Union Stage: Having deduced William Golding, the agent identifies his nationality (a third-layer entity in his subtree): British. This becomes the pivot for the main query. The agent must then find all second-layer entities who (1) share this third-layer attribute (British) and (2) have won both the Nobel Prize and the Booker Prize. This requires filtering the unified entity space to find the final set of Target Entities, which includes authors like William Golding, Kazuo Ishiguro, and J.M. Coetzee."
        },
        {
            "title": "B Tool Call Analysis",
            "content": "As shown in Figure 9, our method involves significantly large number of actions, including Search, Visit, and total tool calls. The density distributions indicate that tool calls often exceed several dozen per instance, with many cases surpassing 50 actions. This high frequency of actions reflects the intensive interaction and comprehensive exploration carried out by our approach, ensuring that the method thoroughly leverages available tools to achieve optimal performance."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}