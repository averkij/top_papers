{
    "paper_title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
    "authors": [
        "Lijie Yang",
        "Zhihao Zhang",
        "Zhuofu Chen",
        "Zikun Li",
        "Zhihao Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 6 7 0 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TIDALDECODE: FAST AND ACCURATE LLM DECODING WITH POSITION PERSISTENT SPARSE ATTENTION Lijie Yang Carnegie Mellon University lijiey@andrew.cmu.edu Zhihao Zhang Carnegie Mellon University zhihaoz3@cs.cmu.edu Zhuofu Chen Carnegie Mellon University zhuofuc@cs.cmu.edu Zikun Li Carnegie Mellon University zikunl@cs.cmu.edu Zhihao Jia Carnegie Mellon University zhihao@cmu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.11."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance on various applications. As LLMs evolve, they are increasingly being adapted to manage tasks with long contexts, such as Chain-of-Thought reasoning (Wei et al., 2023), document summarization (Huang et al., 2021), and retrieval-augmented generation (Ram et al., 2023; Zhang et al., 2024b). However, quickly and efficiently serving long-context LLMs is challenging due to the inherent memory and compute bottlenecks in the Transformer architectures (Vaswani et al., 2023). LLM inference involves two separate stages: prefilling and decoding. The prefilling stage computes the activations for all input tokens and stores the keys and values for all tokens in the key-value (KV) cache, allowing the LLM to reuse these keys and values to compute attention for future tokens. In each decoding stage, the LLM decodes one new token using all input tokens and previously generated tokens. The KV cache size grows linearly in the sequence length (Kwon et al., 2023). For instance, with context length of 128K tokens, the KV cache of LLama2-7B with half-precision can easily Equal contribution 1The codebase to reproduce performance and efficiency results for TidalDecode included in this paper can be found at https://github.com/DerrickYLJ/TidalDecode"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The heatmap for one decoding step of Llama3-8B-Instruct (AI, 2024a), where columns and rows indicate different Transformer layers and tokens in the KV cache, respectively. For each layer, the 5 tokens (10% sparsity) with the highest attention scores of the first attention head are highlighted in yellow, which are the tokens used for sparse attention. We feed an input prompt Use only the provided search results to write high-quality, concise answer to the question.n<begin of text>n The magic number is: 15213. nnn Question: What is the magic number? Keep the response short and direct. Answer: , and the LLM outputs 15213. The results show strong spatial coherence of tokens chosen for sparse attention in the decoding step. reach 64 GB2, creating substantial memory pressure for LLM serving. In addition, the LLM decoding stage is memory-bounded since decoding one new token requires accessing all previous tokens in the KV cache, making KV cache access the primary bottleneck for long-context LLM decoding. This memory-bound nature severely limits the scalability and efficiency of LLM serving. To address this problem, recent work has introduced sparse attention, which approximates full attention using small portion of tokens with the highest attention scores. Compared to full attention, sparse attention reduces computation cost and memory access while preserving the LLMs generative performance (Ge et al., 2024; Zhang et al., 2023). Existing sparse attention techniques can be classified into two categories: evictionand selection-based methods. First, eviction-based sparse attention reduces memory usage for the KV cache by selectively discarding less relevant tokens from the KV cache, therefore reducing the number of tokens computed in attention mechanisms (Xiao et al., 2023; Zhang et al., 2023). While these methods decrease the size of the KV cache, they can be inadequate for tasks where critical information is carried by tokens that are prematurely evicted, such as the needle-in-the-haystack tasks (Peng et al., 2023). On the other hand, selection-based sparse attention maintains all tokens in the KV cache, estimates their attention scores, and selects small subset of tokens to participate in each LLM decoding step. This approach is prone to issues related to distribution shifts caused by appending sparsely attended, biased KV representations back into the cache. This paper presents TidalDecode, an algorithm and system for fast and precise LLM decoding, utilizing position persistent sparse attention (PPSA). key insight behind TidalDecode is the observation 2The KV cache size is computed as: LayersKV HeadsHead DimSeq LenFP16 Size2 (for K+V) = 32 32 128 128K 2 bytes 2 = 64 GB."
        },
        {
            "title": "Preprint",
            "content": "that tokens chosen for sparse attention based on their highest attention scores exhibit significant overlap across consecutive Transformer layers within each decoding phase. Figure 1 illustrates this overlap in single decoding step of LLaMA-3-8B instruct AI (2024a) with an input of 51 tokens. Each column in the figure corresponds to Transformer layer, and each row indicates one token in the KV cache. Selection-based sparse attention methods select the 5 tokens with the highest attention scores (highlighted in yellow) for attention computation in each head. As the figure depicts, there is recurring pattern where consecutive layers consistently focus on the same set of tokens, indicating spatial coherence in the selection of tokens for sparse attention. Instead of independently selecting tokens for sparse attention at each layer, TidalDecode introduces few token selection layers, which perform full attention to identify the tokens with the highest attention scores. All remaining layers implement position persistent sparse attention, where only the tokens selected by the token selection layers are retrieved from the KV cache for attention. Consequently, all other layers between two token selection layers operate on the same set of tokens, reducing the overhead of token selection. Experiments across diverse set of LLMs and datasets demonstrate that using just two token selection layers one at the beginning and one in the middle is sufficient to achieve high generative performance while minimizing computation and memory overheads. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Additionally, to address the KV cache distribution shift, TidalDecode introduces cache-correction mechanism that periodically refills the KV cache using full attention for all sparsely decoded tokens to mitigate bias in the KV representations. Comprehensive evaluation with the LongChat-7b-v1.5-32k, Llama-3-8B, Llama-3-70B, and Llama3.1-8B models on the Needle-in-the-Haystack, PG-19, and LongBench tasks demonstrates that TidalDecode can consistently achieve the best performance efficiency trade-off compared with the best existing sparse attention methods. We have implemented custom GPU kernels for PPSA and an end-to-end system for TidalDecode. Compared with existing full and sparse attention implementations, our system reduced the end-to-end inference latency by up to 2.1 and 1.2, respectively. In conclusion, our contributions are: We propose TidalDecode, streamlined and efficient algorithm and system for fast and high-quality LLM decoding, utilizing position persistent sparse attention. To address KV cache distribution shifts, we introduce cache-correction mechanism that periodically refills the KV cache with using full attention for sparsely decoded tokens. Empirically, we demonstrate the effectiveness and efficiency of TidalDecode through comprehensive evaluation, showing that TidalDecode significantly outperforms existing sparse attention methods."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Long-context model. Efficiently handling long-context inputs is essential for various LLM tasks in real-world applications such as document summarization, question answering, and dialogue systems (Wang et al., 2024). Recent advancements, including rotary positional encoding (RoPE) (Su et al., 2023), have enabled models to manage extended context lengths effectively. The LLaMA-3 model series supports up to 8K tokens, with enhanced versions such as Gradient-AI-Llama3 (AI, 2024a) and LLaMA 3.1 (AI, 2024b) extending this limit to 128K tokens. Additionally, proprietary LLMs such as GPT-4 Turbo and GPT-4o (OpenAI, 2024) support up to 128K tokens, and Claude 3.5 Sonnet allows up to 200K tokens (Anthropic, 2024). While recent work has introduced efficient attention kernel implementation (Dao et al., 2022; Dao, 2023), processing long-context inputs continues to be constrained by significant memory usage and computational costs from the extended KV cache. TidalDecode is designed to mitigate these challenges by reducing latency and memory overhead through an efficient strategy for selecting tokens with the highest attention scores and one-time intermediate re-calibration, ensuring both efficiency and high-quality output. To alleviate the intrinsic computational and memory bottleneck in long-context LLM inference, recent works on sparse attention have approached this problem from two main perspectives: evictionand selection-based methods."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An overview of the decoding step in TidalDecode, which performs full attention for the first two layers, full attention with token selection for the third layer and middle layer, and position persistent sparse attention for all other layers. Eviction-based sparse attention. Xiao et al. (2023); Zhang et al. (2024a) propose to reduce KV cache memory usage by evicting tokens that are considered less relevant during inference. These suffer from potential performance degradation, especially in tasks where every token may carry crucial information (e.g., needle-in-the-haystack tasks), since tokens with high importance for future decoding step can be mistakenly evicted as the generation proceeds, which makes selection-based methods more popular choices in latest sparse attention works. Selection-based sparse attention. Instead of evicting past tokens in the KV cache, Child et al. (2019); Kitaev et al. (2020); Choromanski et al. (2020); Tang et al. (2024); Ribar et al. (2023) preserve the full KV cache and only select important tokens to attend with the attention module on the fly. More specifically, Child et al. (2019) leverages fixed attention mask to select tokens while Tang et al. (2024); Ribar et al. (2023); Choromanski et al. (2020); Kitaev et al. (2020) aim to identify and retain the most relevant tokens at each layer by approximating attention scores. Although these methods are more selective, they operate independently at each layer and are not guaranteed to obtain the ground-truth tokens with the highest attention scores, failing to capture token relevance patterns that persist across layers. Moreover, attention score estimation algorithms sometimes introduce unnecessary complexity, diminishing the practical efficiency gains they are designed to achieve. Improving upon prior works, TidalDecode leverages shared pattern of most important tokens across consecutive layers to further reduce the computational overhead and memory access required for token selection."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section introduces TidalDecode, an efficient algorithm and system for fast LLM decoding using position persistent sparse attention and KV cache correction. Figure 2 shows an overview of TidalDecode. TidalDecode uses the same prefilling mechanism as existing systems and performs full attention to compute the key-value (KV) cache for all prompt tokens. In each decoding step, TidalDecode uses three types of attention layers: full attention, full attention with token selection, and position persistent sparse attention. First, TidalDecode performs full attention for the initial Transformer layers to avoid early performance degradation as identified by prior work (Tang et al., 2024). Second, the layer immediately after full attention and single middle layer (e.g., layer 2 and 13 in Figure 2) perform full attention with token selection, where TidalDecode stores the inner product3 between the current query and key vectors of all tokens in KV cache during full attention and then selects tokens contributing to the highest attention scores. Third, all other layers perform position persistent sparse attention, where only tokens selected from the previous token selection layer are loaded from the KV cache to perform attention computation. 3We dont store attention score as state-of-the-art attention kernels dont materialize the attention score. Since the softmax operation is ordering invariant, we store the inner product value instead."
        },
        {
            "title": "3.1 POSITION PERSISTENT SPARSE ATTENTION (PPSA)",
            "content": "Attention mechanisms have been widely used in todays LLMs. For each attention head, the output is computed via scaled multiplicative formulation as follows. Ai = QiKi/ d, Hi = softmax(Ai)Vi (1) where Qi, Ki, and Vi are the query, key, and value tensors for the i-th attention head. Ai is matrix representing the attention scores between tokens, and Hi is the output of the i-th attention head. Instead of attending to all input tokens, existing sparse attention methods approximate attention computation by attending the query Qi to subset of previous tokens the highest attention scores. Prior work generally performs token selection for individual attention heads and Transformer layers, introducing high runtime overhead. For example, selecting the tokens with highest attention scores using top-k can take longer than computing full attention (see Figure 7), thus diminishing the benefits of performing sparse attention. The key insight behind TidalDecodes position persistent sparse attention is an observation that tokens with highest attention scores for consecutive Transformer layers highly overlap. We use the LLaMA-3-8B model and the needle-in-the-haystack test on PG-19-mini dataset with context length of 100K tokens to quantify this observation. We randomly select 100 requests from the dataset, compute full attention, and analyze the top 256 tokens with the highest attention scores for each Transformer layer. Figure 3a shows the overlap ratios for all pairs of transformer layers, where an overlap ratio of 1 indicates that the tokens with highest attention scores are always identical in these layers, and an overlap ratio of 0 means the top tokens do not overlap in the two layers. Note that we select top 256 tokens from 100K tokens in the KV cache, so randomly selected tokens hardly overlap. In Figure 3b, we compute average recall rates of selected tokens by choosing different re-selection layers. We observe that without re-selection layers, where all layers possess low overlap ratio with Layer 3 shown in the purple box in Figure 3a, the average recall rates are less than 20%. When we choose Layer 13 to perform re-selection, the average recall rates boost to almost 40% due to higher overlap ratios between Layer 13 and its subsequent layers, shown by red boxes in Figure 3a. (a) Overlap of Tokens with the Highest Attention Scores between Layers (b) Recall Rate by Re-selection Layer Figure 3: By retrieving the top-256 tokens from 100K-context-length Needle-in-the-Haystack test conducted on PG-19-mini, 3a shows the overlap ratio of tokens with the highest attention scores across layers, showing that consecutive layers tend to share large number of critical tokens. 3b depicts the recall rates, indicating that different choices of re-selection layers have high impact on the recall rates there is clear peak, delineating the optimal layers for token re-selection. Based on this observation, we design position persistent sparse attention to maximally leverage the token overlaps between consecutive Transformer layers to reduce the computation cost for token selection while achieving high predictive performance. Algorithm 1 shows the TidalDecode algorithm for interleaving full attention and PPSA layers. After the initial full attention layers, TidalDecode uses token selection layer that computes full attention and selects tokens with the highest attention scores. To select tokens, TidalDecode stores the inner product Q, on the fly together with full attention"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 TidalDecode else if is Token Selection Layer then q, k, = (Wqkv, h) C.append(k, v) if is Full Attention Layer then = FullAttention(q, C[:]) 1: Input: Current embedding h, KV cache C, token budget 2: Output: Logits 3: Initialize: ρ = [] 4: for each decoder layer do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: logits = lm head(h) 18: return logits = FullAttention(q, C[:]) C.getKey, ρ := argTopK(q, K, m) = SparseAttention(q, C[ρ]) end if = FFN(o) else Initialize the token buffer to store selected tokens Dense attention with the full KVCache Dense attention with the full KVCache Update token buffer Sparse attention with the tokens in the token buffer calculation. TidalDecode then selects the top tokens with the highest inner product values to form token set . Note that using the inner product to select top-k is equivalent to the post-softmax attention score as the softmax operator is ordering invariant. All PPSA layers after token selection layer computes sparse attention by only loading the keys and values for tokens in , thus limiting the number of tokens participating in attention computations and reducing memory access. straightforward approach to designing TidalDecode is to select the tokens once after full attention and perform PPSA using the same set of tokens for all subsequent layers. However, our preliminary experimentation shows that using single token set for all Transformer layers reduces the LLMs predictive performance by large margin since distant Transformer layers are less correlated compared to consecutive layers, as shown in Figure 3a. To address this issue, TidalDecode performs token re-selection in middle layer, where TidalDecode recalibrates the selected tokens with the highest attention scores by applying full attention and re-selecting top-k token to update , ensuring that token selection remains optimal for the remaining layers. This re-selection mechanism significantly boosts the model performance and promotes accurate and efficient PPSA throughout the model. Extensive evaluation on both small and large models on wide range of datasets shows that using single middle layer for token re-selection is sufficient to preserve the LLMs generative performance, while introducing small runtime overhead. However, deciding which layer to perform token reselection is critical to model performance. As shown in Figure 3, choosing different layers for token re-selection results in different recall rates, where layer 11 and 13 achieve optimal performance. Introducing one-time token re-selection at an optimal layer ensures the selected tokens are recalibrated, effectively mitigating the drift in token importance and elevates accuracy from 15% (without re-selection) to almost 40%. 3.2 KV CACHE CORRECTION For tokens decoded by sparse attention methods, their key/value representations can deviate from the original representation of full attention decoded ones, which we refer to as polluted tokens. The problem can be further exacerbated as their KV pairs are added to the KV cache, resulting in the error accumulation or distribution shift of the KV cache. This can lead to model performance drop in scenarios where the generation length is fairly long. To this end, TidalDecode uses cache-correction mechanism as shown in Figure 4 to periodically correct the polluted tokens in the KV cache. For every decoding step performed by TidalDecode, there will be cache correction step through prefill over all polluted tokens to update their KV representations in the cache. The choice of can be at the level of thousands of decoding steps but also depend on different models and tasks. Notice that the cache correction step can be performed concurrently with the sparse decoding step. Nevertheless,"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Cache Correction we havent used cache correction in our evaluations to make it fair comparison against existing methods."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETTING In this section, we conduct extensive experiments to assess both the performance and efficiency of TidalDecode. Our evaluations are performed on widely used open-source models, including Llama-27B Touvron et al. (2023) and Llama-3-8/70B. Both models are pretrained decoder-only transformers, exhibiting similar yet distinct architectural features. For instance, Llama 3-8B incorporates group query attention (GQA), feature not present in Llama 2-7B. In Section 4.2, we evaluate TidalDecodes performance on various tasks, including needle-in-the-haystack, language modeling on PG-19, and LongBench. In Section 4.3, we write customized attention kernels and compare TidalDecodes kernel efficiency against existing state-of-the-art sparse attention methods. Finally, in Section 4.4, we conclude our evaluations with detailed sensitivity analysis on the choice of different token selection layers. We use TD+LX to denote TidalDecode with layer selected as the token re-selection layer throughout this section. 4.2 PERFORMANCE EVALUATION To evaluate the effectiveness of TidalDecode, we conduct two key downstream NLP experiments: the needle-in-the-haystack test and perplexity evaluation on the PG-19 dataset (Rae et al., 2019). These tasks provide robust benchmarks for measuring both sparse attention models ability to retrieve critical information in challenging scenarios and their performance on long-context language modeling tasks. 4.2.1 NEEDLE-IN-THE-HAYSTACK Table 1: Results of 10k-context-length Needle-in-the-Haystack test on LongChat-7b-v1.5-32k. TidalDecode achieves the same or better results than Quest and significantly better results than cache eviction algorithms such as H2O, TOVA, and StreamingLLM. TidalDecode achieves full accuracy with only 512 token budget. Method / Budget K=32 K=64 K=128 K=256 K=512 H2O TOVA StreamingLLM Quest TD+L7(Ours) 1% 1% 1% 3% 1% 0% 8% 1% 0% 1% 5% 1% 65% 99% 99% 99% 100% 99% 100% 73% 92% 98% 1% 3% 3% The Needle-in-the-Haystack test assesses LLMs ability to handle long-dependency tasks, which is particularly critical for sparse attention algorithms. Eviction-based methods Xiao et al. (2023); Zhang et al. (2023) may discard essential tokens, while selection-based approaches often fail to consistently identify the ground-truth tokens with the highest attention scores in long contexts. Since"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Comprehensive results of 10K-, 32K-, and 100K-context-length Needle-in-the-Haystack test on Llama-3-8B-Instruct-Gradient-1048k, Llama-3.1-8B-Instruct, and Llama-3-70B-InstructGradient-1048k with PG-19-mini dataset. Across all models, TidalDecode consistently outperforms Quest, showing that TidalDecode with only two token selection layers can effectively retain critical information. TidalDecode achieves full accuracy with 64, 64, and 128 tokens in 10K-, 32K-, and 100K-context-length tests, which is only 0.6%, 0.2%, and 0.1% of total input lengths, respectively. Model (context length) Method / Budget K=32 K=64 K=128 K=256 K=512 LLaMA-3-8B (10K) LLaMA-3-8B (100K) LLaMA-3.1-8B (10K) LLaMA-3.1-8B (32K) LLaMA-3-70B (10K) LLaMA-3-70B (32K) Quest TD+L13(Ours) TD+L15(Ours) Quest TD+L13(Ours) TD+L15(Ours) Quest TD+L13(Ours) TD+L14(Ours) Quest TD+L13(Ours) TD+L14(Ours) Quest TD+L14(Ours) TD+L31(Ours) Quest TD+L14(Ours) TD+L31(Ours) 74% 88% 92% 88% 98% 100% 84% 98% 100% 100% 100% 94% 100% 99% 94% 50% 38% 98% 65% 86% 92% 100% 100% 100% 98% 100% 92% 84% 90% 87% 86% 74% 98% 100% 100% 100% 100% 100% 98% 100% 100% 100% 100% 94% 100% 88% 92% 100% 100% 78% 98% 100% 100% 100% 100% 98% 100% 100% 100% 80% 98% 100% 72% 68% 93% 100% 100% 100% 87% 90% 97% 100% 100% 100% 90% 80% 50% 78% 88% 82% 98% 98% 100% 100% 98% 100% 92% 80% 92% 82% Quest is the current state-of-the-art approach on this task, we first run TidalDecode on the same test as Quest on the LongChat-7b-v1.5-32k model and obtained Table 1 with competitive performance. To demonstrate the effectiveness of TidalDecode on long-dependency tasks, we further evaluate TidalDecode on tasks with 10K-, 32K-, and 100K-context-window lengths with the LLaMA-3-70B, LLaMA-3-8B, LLaMA-3.1-8B model using the PG-19-mini dataset, shown in Table 2. To ensure fairness, both TidalDecode and Quest use dense attention in the first two layers. In each test, we inserted random password within the text and tested whether the specific method could retrieve the password correctly. From Table 2, TidalDecode consistently outperforms Quest and achieves full accuracy with an extremely low sparsity (about 0.5% across all context lengths and models). These results demonstrate TidalDecode can achieve state-of-the-art performance with only two token selection layers. While Quest relies on page-level importance estimation for token selection, TidalDecodes exact selection with token reuse approach proves more effective for this task. Also, note that TidalDecode can reduce the token budget by up to 8 when achieving 100% accuracy compared with Quest. This further demonstrates that TidalDecodes exact token selection layer can obtain more relevant tokens than Quest. 4.2.2 LANGUAGE MODELING Perplexity measures the negative likelihood of how well model predicts the next word in sequence, with lower values indicating better performance. We evaluate TidalDecode on Llama-3-8B-InstructGradient-1048k with the PG-19 dataset, which includes up to 100 books, providing comprehensive long-context benchmark. As shown in Figure 5, TidalDecode+L9/13/15 consistently achieves lower perplexity than Quest across all token budget options (2048, 4096). This indicates that TidalDecodes position persistent sparse attention mechanism can effectively retain critical information without significantly sacrificing"
        },
        {
            "title": "Preprint",
            "content": "(a) Token Budget = 2048 (b) Token Budget = 4096 Figure 5: Perplexity evaluation on the PG-19 dataset from 0 to 32K tokens. The results compare TidalDecode with different token re-selection layers (L9, L13, L15) to Quest across token budgets (2048 5a, 4096 5b). Lower perplexity indicates better model performance. Full refers to dense attention as baseline. model accuracy, even as the sequence length grows, demonstrating its robustness for long-context inputs. 4.2.3 LONGBENCH EXPERIMENT Table 3: Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3-8B-Instruct-Gradient-1048k. TidalDecode outperforms Quest at 4096 token budget and achieves an average score higher than full-weight attention. The maximum F1-score for each task is in bold. Method (K)/Task MFQA NrtQA Qasp 2Wiki HotQA QMSm TrQA PRe Avg Full (1024) Quest TD+L13 (1024) Quest (4096) TD+L13 (4096) 30.76 26.21 28.57 28.92 30.94 5.52 4.08 7.63 3.74 6. 14.56 13.32 12.19 11.11 13.63 13.85 12.61 13.56 12.83 14. 11.50 10.75 9.82 12.15 13.71 19.43 19.56 20.37 19.36 19. 86.56 77.00 32.33 83.47 79.78 85.91 86.30 63.84 75. 72.50 78.00 29.09 30.75 31.13 32.86 We also evaluate TidalDecode on LongBench, benchmark designed to test LLMs on long-context tasks across diverse NLP domains (Bai et al., 2023). We focus on eight tasks: MultiFieldQA (MFQA), NarrativeQA (NrtQA), Qasper (Qasp), 2WikiMQA (2Wiki), HotpotQA (HotQA), QMSum (QMSm), TriviaQA (TrQA), and Passage Retrieval (PRe), which collectively composite comprehensive evaluation benchmark in single/multi-document QA, summarization, and retrieval. We evaluate all methods with LLaMA-3-8B-Instruct-Gradient-1048k. TidalDecode is compared against full-weight attention and Quest at token budgets of 1024 and 4096. As shown in Table 3, TidalDecode consistently outperforms Quest on all tasks at = 4096 and on five tasks at = 1024. Surprisingly, TidalDecode, in most cases, matches or exceeds full attention baseline with notable sparsity: 14% on NrtQA, 50% on MFQA, 80% on Qasp, 50% on 2WikiMQA, 32% on HotQA, 29% on QMSm, 35% on TrQA, and 33% on PRe. We hypothesize this is because TidalDecodes token selection process can filter out irrelevant information, thus leading to higher performance. These results demonstrate TidalDecodes generic ability to select tokens with the highest attention scores, achieving competitive or superior performance while significantly reducing token usage, making it ideal for long-context scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: End-to-end latency results on LLaMA-2-7B model for Full attention baseline(Full), Quest, and TidalDecode(TD) when context length is 10K, 32K, and 100K, respectively."
        },
        {
            "title": "4.3 EFFICIENCY EVALUATION",
            "content": "To show the efficiency of TidalDecode, we write customized kernels for our approach and measure the end-to-end decoding latency. We conduct evaluation under the configuration of Llama-2-7B on one Nvidia A100 (80 GB HBM, SXM4) with CUDA 12.2. We compare TidalDecode with state-of-the-art full attention serving library FlashInfer (Ye et al., 2024) and also the Quest implementation. As shown in Figure 6, we can observe that TidalDecode can consistently outperform full attention baseline and Quest by large margin under all token budgets and context lengths. TidalDecode achieves this through token pattern reuse to minimize the token selection overhead. Notice that the latest LLaMA-3 model shares the same architecture as LLaMA-2, except it uses Group-Query-Attention instead of Multi-Head-Attention. However, this does not affect the relative efficiency comparison against Quest and full attention. (a) 32 Layers (b) 64 Layers Figure 7: Overall attention latency results for different methods on the LLaMA model with (a) 32 and (b) 64 layers. We use the full attention model as reference and show TidalDecode and Quests overall attention latency ratio. For each group of the bar plots, the left/middle/right bar denotes the full attention baseline, Quest, and TidalDecode, respectively. In Figure 7, we compare the overall attention latency between different methods on the LLaMA model with 32/64 layers. For the 32-layer LLaMA model, we have 2 full attention layers + 2 token selection layers + 28 sparse attention layers, while Quest has 2 full attention layers + 30 Quest attention layers. For the 64-layer LLaMA model, we have 2 full attention layers + 2 token selection layers + 60 sparse attention layers, while Quest has 2 full attention layers + 62 Quest attention layers. Thus, by completely removing the token estimation overhead in the sparse attention layers, for the 32-layer"
        },
        {
            "title": "Preprint",
            "content": "and 64-layer LLaMA model under all context lengths, TidalDecode can consistently achieve the lowest serving latency while bringing up to 5.56 speed-up ratio against the full attention baseline and 2.17 speed-up ratio against Quest. When the context length is 10K, Quest has higher latency due to the token selection overhead, which aligns with the end-to-end results in Figure 6. In contrast, TidalDecode still achieves significant speed-up by utilizing the position persistent sparse attention mechanism. Figure 8: The breakdown latency results for the full attention, token selection attention, sparse attention, and Quest attention kernels over 10K, 32K, and 100K context length. We use full attention latency as reference and report other kernels relative latency ratio. We use token budget of K=512 for TidalDecode and Quest across all evaluations. In Figure 8, we further break down the latency comparison for different attention modules to show why TidalDecode can bring significant speed-up consistently. We compare different attention modules, namely, the full attention layer, the token selection layer, TidalDecodes sparse attention layer, and the Quest attention layer over the 10K, 32K, and 100K context length. We can observe that, as TidalDecodes sparse attention kernel can directly reuse previous token patterns, it completely removes the important token estimation overhead in the Quest attention kernel, resulting in up to 3.36 speed-up compared with the Quest implementation. On the other hand, even though TidalDecodes token selection layer has slightly higher latency, we only have two token selection layers even in the 70B LLaMA model that has 64 layers in total. 4.4 SENSITIVITY ANALYSIS ON TOKEN RE-SELECTION LAYER In this section, we conduct sensitivity studies for different choices of the token re-selection layer. As TidalDecode only has one token re-selection layer in the middle, it is critical to choose the best-performed one. As shown in Figure 9, we have two interesting findings: (1). Different choices of token re-selection layers can significantly affect the accuracy of the results (2). For models within the same model family, the optimal token re-selection layer is consistent over different tasks. In our setup, the optimal token re-selection layer for the LLaMA-2-7B model is layer 7, while for the LLaMA-3-8B/LLaMA-3.1-8B model is layer 13. concurrent KV cache compression work also identifies that layer 13 is surprisingly important for their approach as well (Shi et al., 2024). For more detailed sensitivity results on the choice of different token re-selection layers, please refer to the appendix for more results."
        },
        {
            "title": "5 CONCLUSION",
            "content": "To conclude, we introduce TidalDecode, an efficient LLM decoding framework with sparse attention. On observing the correlation of the pattern of tokens with the highest attention scores across different consecutive layers, TidalDecode proposes only to select tokens twice: once at the beginning layers and once in the middle layer to serve as token re-selection layer. We find that using two token selection layers is necessary and sufficient to achieve high-generation quality. Additionally, by"
        },
        {
            "title": "Preprint",
            "content": "(a) LLaMA-2 Model (b) LLaMA-3 Model Figure 9: Sensitivity study on the choice of different token re-selection layer. We evaluate LLaMA2-7B-LongChat, LLaMA-2-7B-Yarn, LLaMA-3-8B, and LLaMA-3.1-8B with TidalDecode with token budget of 256. reusing the token patterns throughout the sparse attention layer, TidalDecode greatly reduces the token selection overhead, resulting in significant end-to-end speed-up ratio against existing methods. More interestingly, the optimal choice of the token re-selection layer is consistent across different tasks if the model is in the same model family."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This research is supported by NSF awards CNS-2147909, CNS-2211882, CNS-2239351, and research awards from Amazon, Cisco, Google, Meta, Oracle, Qualcomm, and Samsung."
        },
        {
            "title": "REFERENCES",
            "content": "Gradient AI. https://huggingface.co/ gradientai/Llama-3-8B-Instruct-Gradient-1048k, 2024a. Accessed: 2024-0926. Llama-3-8b-instruct-gradient-1048k. Meta AI. Llama 3.1: Advanced open-source language model. https://ai.meta.com/blog/ meta-llama-3-1/, 2024b. Accessed: 2024-09-26. Anthropic. 3.5 claude-3-5-sonnet, 2024. [Accessed 20-06-2024]. sonnet. Claude https://www.anthropic.com/news/ Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding, 2023. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022."
        },
        {
            "title": "Preprint",
            "content": "Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 14191436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112. Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. OpenAI. Introducing gpt-4o: our fastest and most affordable flagship model. https://platform. openai.com/docs/models, 2024. [Accessed 28-05-2024]. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https: //arxiv.org/abs/1911.05507. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models, 2023. URL https://arxiv. org/abs/2302.00083. Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference, 2023. Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty. Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction. arXiv preprint arXiv:2409.17422, 2024. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv. org/abs/2406.10774. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762. Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: survey of techniques to extend the context length in large language models, 2024. URL https://arxiv.org/abs/2402.02244. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023."
        },
        {
            "title": "Preprint",
            "content": "Zihao Ye, Ruihang Lai, Roy Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze. Cascade inference: Memory bandwidth efficient shared prefix batch decoding. https:// flashinfer.ai/2024/01/08/cascade-inference.html, Jan 2024. URL https: //flashinfer.ai/2024/01/08/cascade-inference.html. Accessed on 2024-0201. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024a. Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, and Zhihao Jia. Accelerating iterative retrieval-augmented language model serving with speculation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 6062660643. PMLR, 2127 Jul 2024b. URL https://proceedings.mlr.press/v235/zhang24cq.html."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LONGBENCH FOR LLAMA-3.1-8B-INSTRUCT Table 4: Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3.1-8B-Instruct. The maximum F1-score for each task is in bold. Method MFQA NrtQA Qasp 2Wiki HotQA QSm TrQA Pre Avg Full Quest (1024) TD+13 (1024) Quest (4096) TD+13 (4096) 27.02 22.35 23.70 26.34 25.89 25.59 14.89 23.25 21.17 26.29 13.05 12.44 11.14 11.99 12.65 16.64 14.24 13.53 15.61 16.86 16.86 14.12 13.72 16.26 15.94 23.88 23.86 22.69 23.61 23. 91.48 81.71 92.35 90.73 90.22 97.67 95.73 92.15 96.35 95.47 39.02 34.92 36.57 37.76 38.32 TidalDecode and full-weight attention share the maximum F1 scores for all tasks, achieving the best scores in three tasks (NrtQA, 2Wiki, and TrQA). TidalDecode significantly outperforms Quest in 4/8 tasks (NrtQA, Qasp, 2Wiki, and TrQA) and full-attention in 3/8 tasks (NrtQA, 2Wiki, and TrQA). For other tasks, we stay close to the full attention and also obtains higher average score than Quest. A.2 END-TO-END EFFICIENCY EVALUATION RESULTS Table 5: TidalDecode end-to-end efficiency results on LLaMA-2-7B Context Length Full Attention (ms) TidalDecode(ms) K=32 K=64 K=128 K=256 K=512 K=1024 K=2048 K=4096 10K 32K 100K 19.22 25.71 45.70 16.94 17.18 17.15 16.98 16.96 17.89 17.92 17.64 17.70 17.91 21.26 21.09 21.38 21.19 21. 17.32 17.97 21.38 17.19 18.48 21.65 17.63 18.98 22.34 Table 6: Quest end-to-end efficiency results on LLaMA-2-7B Context Length Full Attention (ms) TidalDecode(ms) K=32 K=64 K=128 K=256 K=512 K=1024 K=2048 K=4096 10K 32K 100K 19.22 25.71 45.70 20.39 19.86 19.44 19.35 20.18 20.47 20.85 20.73 21.06 20.62 24.93 25.18 24.77 24.90 24.84 19.91 20.94 25.10 20.23 21.35 25. 21.09 22.11 26."
        },
        {
            "title": "Preprint",
            "content": "A.3 FULL SENSITIVITY STUDIES ON DIFFERENT TOKEN RE-SELECTION LAYER Table 7: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.2-3B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold. RL/K TidalDecode+L2 TidalDecode+L3 TidalDecode+L4 TidalDecode+L5 TidalDecode+L6 TidalDecode+L7 TidalDecode+L8 TidalDecode+L9 TidalDecode+L10 TidalDecode+L11 TidalDecode+L12 TidalDecode+L13 TidalDecode+L14 TidalDecode+L15 TidalDecode+L16 TidalDecode+L17 TidalDecode+L18 TidalDecode+L19 TidalDecode+L20 TidalDecode+L21 TidalDecode+L22 TidalDecode+L23 TidalDecode+L24 TidalDecode+L25 TidalDecode+L26 TidalDecode+L27 Quest 64 6% 32 512 128 2% 12% 14% 16% 30% 8% 10% 24% 2% 6% 14% 16% 20% 28% 2% 10% 22% 26% 36% 18% 26% 26% 32% 46% 6% 10% 16% 18% 28% 20% 20% 46% 60% 84% 6% 12% 32% 58% 66% 44% 58% 50% 60% 64% 4% 12% 16% 22% 28% 50% 84% 96% 98% 98% 42% 80% 94% 98% 100% 28% 44% 54% 60% 72% 2% 8% 16% 22% 36% 4% 16% 12% 22% 34% 6% 16% 20% 32% 2% 2% 10% 12% 18% 28% 2% 6% 10% 18% 32% 6% 10% 12% 18% 24% 8% 10% 16% 26% 6% 6% 0% 12% 12% 26% 4% 14% 14% 18% 26% 2% 10% 16% 20% 28% 4% 8% 14% 16% 22% 2% 10% 10% 22% 26% 0% 10% 12% 22% 26% 46% 56% 72% 88% 96%"
        },
        {
            "title": "Preprint",
            "content": "Table 8: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.1-8B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 13 and Layer 14 are the best two re-selection layers for accuracy. RL/K TidalDecode+L2 TidalDecode+L3 TidalDecode+L4 TidalDecode+L5 TidalDecode+L6 TidalDecode+L7 TidalDecode+L8 TidalDecode+L9 TidalDecode+L10 TidalDecode+L11 TidalDecode+L12 TidalDecode+L13 TidalDecode+L14 TidalDecode+L15 TidalDecode+L16 TidalDecode+L17 TidalDecode+L18 TidalDecode+L19 TidalDecode+L20 TidalDecode+L21 TidalDecode+L22 TidalDecode+L23 TidalDecode+L24 TidalDecode+L25 TidalDecode+L26 TidalDecode+L27 TidalDecode+L28 TidalDecode+L29 TidalDecode+L30 TidalDecode+L31 32 36% 8% 0% 14% 8% 6% 34% 64% 56% 52% 8% 128 46% 14% 16% 52% 28% 10% 50% 82% 84% 82% 28% 64 38% 10% 10% 30% 12% 10% 44% 78% 74% 76% 10% 256 58% 34% 34% 52% 40% 18% 66% 90% 94% 86% 40% 100% 100% 100% 100% 98% 100% 100% 100% 96% 56% 72% 18% 98% 64% 84% 64% 68% 58% 76% 68% 62% 40% 56% 28% 64% 40% 66% 30% 66% 40% 64% 34% 70% 38% 58% 30% 68% 32% 70% 36% 56% 30% 88% 54% 86% 74% 60% 62% 48% 46% 52% 54% 50% 62% 54% 56% 56% 52% 42% 78% 46% 74% 70% 50% 60% 48% 38% 46% 46% 54% 48% 50% 40% 48% 48% 36%"
        },
        {
            "title": "Preprint",
            "content": "Table 9: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3-70B-Instruct-Gradient-1048k; we first run top = 512 and filter out those layers that do not achieve full accuracy with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 14 and Layer 31 are the best two Re-selection layers for accuracy. RL/K L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22 L23 L24 L25 L26 L27 L28 L29 L30 L31 L32 - - - 32 - - - - - - - - - - - - 64 - - - - - - - - - - - - 256 - - - - - - - - - - - - 128 - - - - - - - - - - - - 512 6% 37% 23% 63% 70% 90% 70% 30% 83% 70% 63% 50% 87% 93% 100% 100% 100% 97% 63% 87% 97% 100% 93% 87% 97% 100% 97% 53% 93% 97% 100% 100% 93% 100% 33% 60% 77% 80% - 87% - 93% 100% 50% 87% 93% 87% 100% 80% 83% 93% 97% - 90% 100% 33% 67% 80% 90% 97% 100% 100% 100% 97% 100% 27% 73% 80% - - - 50% 70% 83% - - 53% 80% 93% - RL/K L33 L34 L35 L36 L37 L38 L39 L40 L41 L42 L43 L44 L45 L46 L47 L48 L49 L50 L51 L52 L53 L54 L55 L56 L57 L58 L59 L60 L61 L62 L63 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 128 64 32 50% 87% 90% - 256 512 93% 100% 97% 70% 83% 100% 100% 100% 97% 100% 50% 83% 97% 90% - 80% 100% 37% 83% 83% 87% - 50% - 97% - 53% - 67% - 83% - 70% - 63% - 77% - 97% - 77% - 70% - 93% - 77% - 70% - 60% - 53% - 87% - 57% - 50% - 50% - 57% - 30% - 43% - 43% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "Preprint",
            "content": "Table 10: Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for Llama-3-8B-Instruct-Gradient-1048k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 9, Layer 13, and Layer 14 are the best three re-selection layers for accuracy. RL/K TidalDecode+L2 TidalDecode+L3 TidalDecode+L4 TidalDecode+L5 TidalDecode+L6 TidalDecode+L7 TidalDecode+L8 TidalDecode+L9 TidalDecode+L10 TidalDecode+L11 TidalDecode+L12 TidalDecode+L13 TidalDecode+L14 TidalDecode+L15 TidalDecode+L16 TidalDecode+L17 TidalDecode+L18 TidalDecode+L19 TidalDecode+L20 TidalDecode+L21 TidalDecode+L22 TidalDecode+L23 TidalDecode+L24 TidalDecode+L25 TidalDecode+L26 TidalDecode+L27 TidalDecode+L28 TidalDecode+L29 TidalDecode+L30 TidalDecode+L31 64 32 256 88% 28% 68% 72% 16% 14% 92% 512 16 128 98% 78% 84% 76% 94% 64% 0% 6% 10% 16% 84% 2% 10% 16% 28% 80% 10% 12% 32% 52% 24% 4% 6% 10% 14% 28% 2% 10% 10% 10% 26% 64% 80% 90% 96% 52% 90% 96% 100% 98% 100% 96% 100% 72% 76% 86% 94% 56% 74% 94% 100% 98% 98% 8% 14% 22% 44% 94% 66% 92% 92% 96% 100% 100% 100% 74% 68% 88% 98% 100% 100% 74% 94% 92% 88% 100% 100% 94% 44% 50% 72% 66% 82% 96% 42% 60% 74% 82% 96% 88% 60% 72% 74% 74% 98% 98% 96% 58% 74% 82% 84% 64% 74% 96% 78% 98% 90% 94% 90% 10% 38% 60% 66% 98% 82% 60% 70% 68% 72% 98% 88% 58% 78% 70% 86% 78% 62% 58% 76% 70% 92% 92% 100% 66% 86% 84% 82% 90% 54% 64% 66% 80% 94% 88% 100% 84% 80% 94% 96% 94% 94% 66% 66% 76% 84% 90% 72% 80% 88% 80% 96% 96% 100% 80% 90% 86% 88% 96% 90% 74% 90% 88% 84%"
        },
        {
            "title": "Preprint",
            "content": "Table 11: Sensitivity study of re-selection layer (RL) on 3k-context-length Needle-in-the-Haystack test for LongChat-7b-v1.5-32k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy. RL/K TidalDecode+L2 TidalDecode+L3 TidalDecode+L4 TidalDecode+L5 TidalDecode+L6 TidalDecode+L7 TidalDecode+L8 TidalDecode+L9 TidalDecode+L10 TidalDecode+L11 TidalDecode+L12 TidalDecode+L13 TidalDecode+L14 TidalDecode+L15 TidalDecode+L16 TidalDecode+L17 TidalDecode+L18 TidalDecode+L19 TidalDecode+L20 TidalDecode+L21 TidalDecode+L22 TidalDecode+L23 TidalDecode+L24 TidalDecode+L25 TidalDecode+L26 TidalDecode+L27 TidalDecode+L28 TidalDecode+L29 TidalDecode+L30 TidalDecode+L31 256 128 64 32 54% 2% 6% 2% 78% 10% 52% 67% 76% 4% 36% 65% 99% 17% 87% 94% 70% 96% 99% 99% 80% 98% 100% 100% 96% 58% 82% 96% 71% 7% 31% 59% 78% 16% 59% 71% 77% 34% 61% 68% 77% 17% 32% 53% 48% 5% 10% 28% 64% 24% 41% 57% 69% 37% 47% 62% 46% 16% 24% 28% 34% 10% 4% 4% 15% 8% 3% 2% 19% 7% 1% 0% 20% 6% 3% 0% 19% 10% 2% 0% 18% 4% 4% 0% 13% 5% 2% 0% 21% 6% 2% 0% 16% 7% 2% 0% 19% 7% 1% 0% 21% 4% 2% 0% 17% 10% 2% 1% 16% 7% 3% 0% 15% 9% 1% 1% 16% 5% 2% 1%"
        },
        {
            "title": "Preprint",
            "content": "Table 12: Sensitivity study of re-selection layer (RL) on 3k-context-length Needle-in-the-Haystack test for Yarn-Llama-2-7b-128k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy. RL/K TidalDecode+L2 TidalDecode+L3 TidalDecode+L4 TidalDecode+L5 TidalDecode+L6 TidalDecode+L7 TidalDecode+L8 TidalDecode+L9 TidalDecode+L10 TidalDecode+L11 TidalDecode+L12 TidalDecode+L13 TidalDecode+L14 TidalDecode+L15 TidalDecode+L16 TidalDecode+L17 TidalDecode+L18 TidalDecode+L19 TidalDecode+L20 TidalDecode+L21 TidalDecode+L22 TidalDecode+L23 TidalDecode+L24 TidalDecode+L25 TidalDecode+L26 TidalDecode+L27 TidalDecode+L28 TidalDecode+L29 TidalDecode+L30 TidalDecode+L31 64 0% 256 3% 512 128 32 0% 25% 0% 11% 26% 39% 65% 85% 5% 17% 34% 60% 92% 16% 42% 65% 87% 96% 73% 83% 89% 95% 100% 73% 95% 98% 98% 100% 87% 92% 97% 94% 99% 7% 21% 43% 60% 95% 12% 31% 58% 69% 93% 20% 21% 46% 68% 97% 2% 15% 28% 51% 92% 4% 5% 20% 34% 88% 16% 20% 49% 53% 91% 2% 25% 44% 56% 90% 10% 13% 21% 43% 86% 9% 16% 85% 3% 2% 15% 84% 0% 80% 7% 3% 0% 79% 7% 0% 0% 77% 5% 1% 0% 76% 8% 0% 0% 74% 7% 1% 0% 0% 0% 73% 9% 2% 10% 71% 0% 70% 5% 1% 0% 68% 8% 3% 0% 67% 9% 0% 0% 65% 8% 2% 0% 64% 8% 1% 0% 2% 10% 62% 0% 4% 1% 2% 2% 0% 0% 0% 2% 0% 1% 1% 1% 0% 1% 0%"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University"
    ]
}