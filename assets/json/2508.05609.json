{
    "paper_title": "Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity",
    "authors": [
        "Yuhan Zhang",
        "Long Zhuo",
        "Ziyang Chu",
        "Tong Wu",
        "Zhibing Li",
        "Liang Pan",
        "Dahua Lin",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 0 6 5 0 . 8 0 5 2 : r Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity Yuhan Zhang1, 2 Long Zhuo2 Ziyang Chu2, 3 Tong Wu4 Zhibing Li2, 5 Liang Pan2 Dahua Lin2, 5 Ziwei Liu6 1Fudan University 2Shanghai Artificial Intelligence Laboratory 3Tsinghua University 4Stanford University 5The Chinese University of Hong Kong 6S-Lab, Nanyang Technological University Figure 1: Overview of Hi3DEval, unified framework for 3D generation evaluation with three key components: 1) Hierarchical evaluation protocols that jointly assess object-level and partlevel quality, with extended material evaluation via reflectance cues. 2) large-scale benchmark featuring diverse set of 3D generative models and extensive human-aligned annotations generated via multi-agent, multi-modal LLMs pipeline. 3) hybrid automated scoring system that integrates video-based and naive 3D-based representations to enhance evaluators perceptions of 3D structure."
        },
        {
            "title": "Abstract",
            "content": "Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by reliable multi-agent annotation pipeline. We further propose 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and Preprint. material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing scalable alternative to manual evaluations. Project page is available at https://zyh482.github.io/Hi3DEval/."
        },
        {
            "title": "Introduction",
            "content": "Creating vivid and high-fidelity 3D assets remains fundamental yet challenging problem in computer vision and graphics, with wide-ranging applications in gaming, virtual and augmented reality, and robotics. In recent years, the field has witnessed significant breakthroughs, driven by the emergence of large-scale datasets [8, 7], expressive neural representations [37, 23], sophisticated optimization techniques [40, 45, 28, 29, 4], and powerful network architectures [63, 72, 74, 52]. As the visual realism of generated content continues to advance, establishing reliable and fine-grained metrics for evaluating and comparing different approaches has become critical objective. Existing 3D evaluation frameworks can be broadly categorized into two paradigms: training-free protocols and data-driven evaluators. The former often extend conventional 2D metrics to 3D domains through handcrafted heuristics [13, 46], while methods like GPTEval3D [61] leverage powerful Multimodal Language Models (MLLMs) such as GPT-4V 1 with multi-view renderings to enhance 3D reasoning. In pursuit of greater transparency and alignment with human judgment, recent efforts (e.g. 3DGen-Bench [73], Gen3DEval [35], T23DAQA [12]) explore training lightweight scoring models supervised by human annotations or pseudo-labels generated by GPT-4V. Despite these advances, current approaches remain coarse-grained and overlook material attributes when evaluating textures. Moreover, base on 2D renderings, these methods inherently struggle to capture the spatial continuity and structural complexity of 3D assets, limiting their reliability and robustness. To address these limitations, we present unified evaluation framework, Hi3DEval, that supports both imageand text-conditional 3D generation and advances 3D evaluation in the following dimensions: 1) Hierarchical evaluation scheme across varying granularities. We introduce hierarchical evaluation scheme, supporting both object-level and part-level assessment. Specifically, the objectlevel evaluation provides holistic evaluation of generated 3D assets, considering geometry, texture, and prompt alignment. While the part-level evaluation enables localized diagnosis of quality issues within semantic regions, enhancing interpretability and failure analysis. Together, they provide more comprehensive view of generation performance. 2) Physical material evaluation with reflectance cues. We propose material-subject evaluation protocol that goes beyond aesthetic-level judgments to explicitly assess core physical properties, such as albedo, saturation, and metallicness. In practice, we use reflectance cues under diverse illumination to simulate realistic perception scenarios, enabling robust assessment across diverse 3D representations, even when PBR are not explicitly disentangled during generation. 3) Large-scale dataset with human-aligned annotation pipeline. We construct large-scale comprehensive dataset involving all aforementioned dimensions, dubbed Hi3DBench. To trade off the subjectivity of purely manual labeling and the inconsistency of purely GPT-based labeling with human judgment, we introduce multi-agent, multi-modal annotation pipeline, producing more consistent and faithful assessments. 4) Hybrid 3D representation-based automated scoring system. To overcome the limitations of static image representation, we propose 3D-aware automated scoring system leveraging hybrid 3D representations. To be specific, we use video-based representations to enhance evaluators understanding of spatio-temporal consistency for object-level and material-subject evaluation, and we apply pretrained geometric embeddings for the part-level to achieve deep shape perception. Experimental results show that such hybrid 3D-aware signals yield more reliable and humanconsistent evaluation than conventional static image-based approaches. Specifically, the proposed video-based scoring pipeline, adopted for the object-level and material-subject evaluations, achieves superior pairwise alignment with humans across all assessed dimensions. Furthermore, we conduct qualitative studies on the part-level scoring model, demonstrating that it exhibits capability in localizing generation flaws, enabling fine-grained diagnostic analysis. 1 GPT-4V system card: https://openai.com/index/gpt-4v-system-card"
        },
        {
            "title": "2 Related works",
            "content": "3D object generation. Prior work in 3D content generation has explored variety of approaches, including leveraging 2D generative models, incorporating 3D geometric priors, and directly learning from large-scale 3D datasets. DreamFusion [40] leverage text-to-image diffusion model [17, 43] to optimize differentiable 3D representations through Score Distillation Sampling (SDS). Subsequent methods [29, 54, 57, 36, 28, 47, 51, 32] have refined optimization framework and SDS loss to enhance visual quality. Another line of research [33, 30, 45, 34] focuses on adapting 2D generative models for multi-view synthesis, providing strong 3D priors that enable reconstruction of 3D assets via sparse-view reconstruction techniques or Large Reconstruction Models [18, 50, 27, 66, 64, 53, 58]. More recently, native 3D generation approaches[72, 77, 63, 74, 52, 5] have achieved state-of-the-art performance by training directly on 3D data collections [8, 7, 62], significantly improving both geometric fidelity and computational efficiency. As generation techniques evolve, the need for an efficient, robust, and comprehensive 3D evaluation framework has become increasingly urgent. 3D generation evaluation. Early approaches to evaluating 3D content often relied on labor-intensive user studies or 2D-based metrics such as CLIP Score [15, 20] and Aesthetic Score [44], which are insufficient for capturing the holistic structure of 3D assets. To address these limitations, 3Bench [13] proposed aggregating multi-view image features via hand-crafted formulations. With the growing adoption of the \"LLM-as-a-Judge\" [76] paradigm, GPTEval3D [61] utilized GPT-4V to perform pairwise comparisons and established leaderboard using Elo scoring [10]. However, reliance on closed-source model raises concerns about reproducibility and transparency. Subsequent efforts [48, 73, 35] fine-tuned open-source models to produce pairwise comparison outcomes using pseudo-labels generated by GPT-4V. Despite progress, this pairwise-to-score pipeline presents scalability challenges as leaderboard sizes increase. As result, recent works [73, 68, 46, 12] have shifted toward absolute scoring paradigms. For example, GT23D-Bench [46] introduces training-free protocol in fine-grained dimensions using curated ground-truth sets and conventional metrics, while T23DAQA [12] enhances 3D understanding through video-based representations. Furthermore, driven by the rise of reward modeling (RM), DreamReward [68] and 3DGen-Score [73] learn scoring models from human preference data, achieving stronger alignment with human judgments. Despite these advancements, two main limitations remain: most methods rely heavily on 2D renderings derived from 3D assets, which fail to capture true 3D structure and spatial consistency, and evaluations are typically constrained to the object level, lacking finer-grained analysis. To address these gaps, our method introduces hybrid, 3D-aware scoring system and proposes part-level evaluation framework to enable more detailed and structure-aware assessment. Material generation evaluation. Generating high-quality textures conditioned on 3D geometry [3, 2, 70, 6, 11, 71, 9, 69, 21] has attracted growing interest, highlighting the need for evaluation metrics specifically tailored to this task. However, existing metrics are inadequate for capturing the physical plausibility or alignment of textures with underlying geometry. For example, Frechet Inception Distance (FID) [16] and Kernel Inception Distance (KID) [1] assess the distribution of multi-view renderings, but are not well-suited for object-level evaluation. Similarly, CLIP Score is primarily designed for measuring text-image alignment, while Aesthetic Score offers only coarse assessment of visual appeal. Crucially, these metrics overlook essential material properties such as albedo, metallic, and roughness, which are fundamental to physical plausibility and perceptual realism. In this work, we incorporate multi-lighting setups and perform fine-grained, physically aware assessments that more accurately reflect the quality and realism of generated textures."
        },
        {
            "title": "3 Hierarchical 3D scoring dataset",
            "content": "To enable systematic evaluation of generative 3D models, we construct Hi3DBench, large-scale benchmark tailored for multi-dimensional and hierarchical quality assessment. In contrast to prior benchmarks [61, 35, 68] that focus solely on object-level pairwise comparisons or limited textconditioned scenarios, Hi3DBench comprises over 15,000 procedurally generated assets with hierarchical annotations spanning object-, part-, and material-levels in absolute scoring format. Furthermore, to reduce labor-intensive manual annotations and mitigate the subjectivity of human ratings, we introduce Multi-agent Multi-modal Annotation Pipeline (M²AP), which harnesses diverse set of MLLM agents to collaboratively yield scalable, consistent, and reliable quality assessments. 3 Figure 2: Annotation of the Hi3DBench Dataset. The top left illustrates the annotation pipeline of M²AP, while the top right presents human alignment experiment comparing annotations from single agent and M²AP. M²AP integrates advanced multimodal agents, incorporates reflection to address hallucination issues, and utilizes an elaborate scoring prompt along with rendered rotation videos and multi-view images to produce the final evaluation. The bottom row displays three annotation examples at object, part, and material levels. We detail the dataset construction process in Section 3.1, including asset generation, part segmentation, and relighting; describe the annotation pipeline in Section 3.2 and evaluate its reliability in Section 3.3."
        },
        {
            "title": "3.1 Data curation",
            "content": "To support hierarchical and multi-dimensional evaluation of 3D generative models, we construct Hi3DBench by systematically curating large-scale collection of procedurally generated 3D assets. The data curation process is designed to ensure diversity, representativity, and compatibility with downstream annotation and evaluation. Specifically, it consists of three key steps: 1) diverse asset generation; 2) part-level segmentation for fine-grained analysis; 3) relighting for material realism. Diversified generation. Our benchmark includes 15,300 assets in total, generated from 30 distinct 3D generative methods (including 9 text-to-3D models and 21 image-to-3D models). For each method, we generated 510 objects with prompts borrowed from 3DGenBench [73], which spanning diverse semantic categories and difficulties. The complete list of models involved and details about this process can be found in Section A.2. Additionally, to facilitate visualization and subsequent evaluation, we render each 3D asset into 360-degree surround videos in three distinct formats: RGB, Normal, and Shading. Meanwhile, to reduce the potential errors introduced by visual quality, we use unified rendering pipeline and consistent settings to render all methods and assets. More details on the rendering implement can also be found in Section A.2. Part-level segmentation. Following the generation phase, 3D assets undergo structurally meaningful segmentation process, which is crucial for fine-grained analysis. Specifically, we adopt PartField [31] in semantics-free manner, which leverages rich local geometric features to perform effective unsupervised clustering. However, the number of part clusters is not automatically inferred by PartField [31] but manually specified instead. Given that structural complexity varies significantly depending on the input prompt, we argue that applying fixed number of clusters across all objects is suboptimal. For instance, \"a potted cactus\" can be segmented into three parts (pot, soil, and cactus), whereas \"a kitty cat\" exhibits richer structural details (e.g., head, torso, limbs and tail) that demands finer partition. To accommodate such variation, we utilize GPT to estimate an appropriate number of structurally meaningful parts for each prompt based on its semantic complexity, followed by manual validation to ensure its plausibility and consistency. More details on the selection of the part segmentation method and the impact of the cluster count can be found in Section A.2. Relighting. To ensure consistent and accurate material assessment across diverse assets, we apply standardized relighting protocol. Each object is rendered under both controlled point-light conditions and various High Dynamic Range Imaging (HDRI) environments. Specifically, we place point-light sources at two principal azimuthal angles (top and right) and render from complementary viewpoints to reveal object reflectance characteristics. Further, to simulate real-world scenarios, we adopt six 4 HDRI maps spanning indoor and outdoor environments with varying natural and artificial illumination. This dual-scheme relighting captures material fidelity under both idealized and realistic lighting, enabling comprehensive evaluation. More details about the relighting setting and visualizations of HDRI maps involved are provided in Section A.2."
        },
        {
            "title": "3.2 Annotation pipeline",
            "content": "To enable large-scale, reliable, and cost-efficient quality annotation, we introduce Multi-agent Multimodal Annotation Pipeline (M²AP). The pipeline (illustrated in the top-left panel of Figure 2) utilizes with an Elaborate Prompt, incorporating content like Stepped Instructions and Typical Examples to guide the annotation. In practice, M²AP uses Rotating Videos and Multi-view Images as inputs for comprehensive understanding of 3D assets, and processes rich visual data with advanced image-aware and video-aware Mutimodular LLMs. Additionally, we also employ most-recent advanced reasoning model (i.e., GPT 4.1, Claude 3.7, and Grok-3) and thinking model (i.e., Gemini 2.5 Pro and O3 / O4-mini), dubbed Reflection mechanism, to ensure consistency and alleviate hallucination by self-revision. Further information on the annotation pipeline, including the selected agents, prompt design, and annotation cost, are provided in Section A.3. In the following paragraphs, we demonstrate how it supports hierarchical annotations at object, part, and material levels, respectively. Object-level criteria. At the object level, M²AP provides holistic quality assessments of 3D content across five key dimensions, in line with established 3D evaluation protocols [61, 73]. Geometry Plausibility (GP), assessing the structural integrity and physical feasibility of the generated shape, with absence of distortions or floating parts; Geometry Details (GD), assessing the fidelity of fine-scale structures, distinguishing welldefined, intentional surface features from noise; Texture Quality (TQ), assessing the visual fidelity of surface textures in terms of resolution, realism, aesthetics, and consistency across views; Geometry-Texture Coherency (GTC), assessing the alignment between geometry and texture, ensuring textures naturally follow shape contours and consistently reflect geometric details; Prompt Alignment (PA), assessing the semantic and/or identity consistency between the input prompt and the generated 3D asset. Part-level criteria. The part-level assessment dives into the segmented components of the 3D object, enabling fine-grained analysis and precise localization of generation flaws. It focuses on Geometry Plausibility (GP) and Geometry Details (GD) for each part, complementing object-level evaluation by exposing localized artifacts. Material-subject criteria. Material-subject evaluation targets the intrinsic perceptual quality and physical properties of textures, filling critical gap in prior frameworks with four key dimensions: Details and Complexity (DC), assessing the textures visual richness and detail while ensuring balanced complexity that preserves aesthetic harmony; Colorfulness and Saturation (CS), assessing the textures color distribution and clarity, focusing on diversity, saturation, and suitability. Consistency and Artifacts (CA), assessing the textures consistency and realism under varying lighting conditions, focusing on the presence of visible seams and shading artifacts; Material Plausibility (MP), assessing whether its diffuse and specular effects realistically reflect the material properties described in the prompt."
        },
        {
            "title": "3.3 Alignment with human judgments",
            "content": "To assess the reliability of our automated annotation pipeline, we conduct human-agent alignment study. As shown in the top-right panel of Figure 2, our proposed M²AP outperforms single-agent baselines by clear margin in terms of L1 loss, highlighting the advantage of collaborative agent reasoning in producing more consistent and accurate annotations. In addition, we conduct ablation studies to validate the effectiveness of physical plausibility check and reflection. As shown, the proposed M²AP achieves lower L1 loss compared to the variants without hallucination mitigation, 5 Figure 3: Overview of video-based scoring pipeline. Left: Contrastive learning aligns the video encoder with the prompt encoder under diverse rendering conditions. Right: Quality heads are trained to regress scores. Specifically, we apply cosine similarity for prompt-aware dimensions. i.e., via reflection and physical plausibility check. It indicates that M²AP demonstrates strong human alignment. For example, M²AP provided score of 7.3 for teapot model, closer to the human score of 7.5 than the standalone GPT-4.1 (5.0). Detailed quantitative results are provided in Section A.3."
        },
        {
            "title": "4.1 Video-based scoring model",
            "content": "To robustly assess the quality of 3D assets, we adopt video-based evaluation paradigm that captures spatio-temporal cues from rendered turntable sequences. This could provide more comprehensive perception of 3D content compared to image-based representations. Model. We leverage the pretrained InternVideo2.5 [56] encoder, which demonstrates effective videotext alignment in embedding space, to extract rich spatiotemporal features from multi-modal rendered videos. Following common practices in video quality assessment [59], we design lightweight prediction head to process the high-dimensional features from the video encoder. The unit structure of the prediction head comprises two Conv3D layers with GeLU activations to model spatiotemporal dependencies across frames. Finally, Linear layer outputs the scalar quality score. This setup leverages the strong spatio-temporal representations of the video encoder while maintaining efficiency and generalizes well across object categories, materials, and lighting variations. Pipeline. While the video encoder demonstrates strong video-text alignment, we observe obvious domain gaps when processing 3D rendered videos. To address this, we implement two-stage training strategy, as illustrated in Figure 3. In the first stage, we curate diverse dataset including both scanned and generated 3D objects and render them under various visual conditions (e.g., albedo, normal, lighting). Through contrastive learning, we align the rendered videos with their corresponding prompts and conditioning descriptions, guided by CLIP [42] pretrained encoders as the target embedding space. In the second stage, we train the quality prediction head together with the final two MLP layers of the video encoder, using human-aligned quality scores. This enables domain-specific adaptation for scoring while preserving the general spatio-temporal representations. Criterion. Since annotations are collaboratively provided by multiple agents, the resulting scores lie on continuous scale. Therefore, we treat the scoring task as regression problem and utilize SmoothL1 loss as the primary objective, which combines the stability for small errors of L2 loss with the robustness for outliers of L1 loss. Furthermore, to capture relative quality judgments and enhance the models discriminative ability, we also incorporate ranking loss as an auxiliary objective. The final loss function is formulated as follows. Lreg = (cid:26)0.5(s ˆs)2/bata, if ˆs < bata, ˆs 0.5 bata, otherwise, Lrank = (cid:88) i,j max(0, (si sj)(ˆsi ˆsj)), Ltotal = Lreg + λLrank, 6 (1) (2) (3) Figure 4: Overview of the 3D-based scoring pipeline. Left: Visualizations of the raw mesh, face embeddings, and part masks used in the pipeline, from top to bottom, exhibiting the strong capability of PartField [31] in capturing local geometric features. Right: Illustration of the scoring pipeline. We first project the pretrained features into latent space tailored for the scoring task, then apply attention modules to enable information flow with the global context and within each part. where denotes the ground-truth score and ˆs denotes the predicted score, and the constant λ controls the strength of the ranking supervision. In this paper, we set the default value bata at 1.0. Object-level setup. In the first stage, we choose 6k scanned objects from Omniobjs [62] and 6k generated objects, which are rendered to 16-frame videos of albedo, normal, point-light, and HDRI types. The video encoder is fully fine-tuned on video-prompt pairs with contrastive loss. We use 4 NVIDIA A800-SXM4-80GB GPUs and train the encoder for approximately 4 hours. During the second stage, five prediction heads are trained in parallel using around 10k samples per dimension, with each head corresponding to the specific video types (normal, RGB). Material-subject setup. The first stage is identical to Object-level Setup. For the second stage, we independently train the four parallel prediction heads using approximately 10k samples per dimension. The trainable parameters are about 37M per dimension using an Adam optimizer with learning rate of 4e-4. The batch size is set to 16 per GPU with 16 frames per video. Each dimension completes training on 8 NVIDIA A800-SXM4-80GB GPUs in about 8 hours for 15 epochs. 4.2 3D-based scoring model Preliminary. PartField [31] is feedforward class-agnostic part-segmentation model that learns continuous 3D feature field for part-aware shape understanding. It is trained using weak part proposals derived from 2D segmentations and 3D part annotations, without enforcing consistent semantics or granularity. This flexible supervision enables cross-modality generalization (e.g., meshes, point clouds, and Gaussian splats) and robust representations under open-world. Pipeline. We build our part-level scoring pipeline on top of PartField [31], which provides strong local geometric features from unsupervised 3D segmentation. As shown in Figure 4, starting from these pretrained features, we project them into scoring-specific latent space via two-layer encoder. Additionally, to handle the varying number of mesh faces per part, we apply the adjacency-based k-NN pooling to standardize into fixed-dimensional representation. Operating on local structure solely, they lack broader contextual understanding. To address this, we introduce two complementary interaction modules: cross-attention module that allows incorporation of global contextual cues and self-attention module that captures intra-part dependencies. This design enables the model to capture both fine-grained geometric details and holistic structural context for precise assessment. Part-level setup. Our scoring model contains approximately 5 million trainable parameters and is trained on 22.7k samples per evaluation dimension. Prior to training, we normalize all groundtruth scores into the [0,1] range and apply weighted sampling strategy to balance the original score distribution in the training set. Moreover, to encourage the intra-object part-level information exchange, we adopt the object as the minimal input unit instead of isolated parts. Specifically, each training batch consists of 8 objects, averaging around 6 parts per object. The model is trained for total of 5000 steps using the Adam optimizer with learning rate of 3e-5. Training is performed on single NVIDIA A100-SXM4-80GB GPU and is completed in approximately 4 hours. 7 Figure 5: Radar visualization of Hi3DBench. Models are ranked top-down by their total score across all dimensions, each predicted by our automated scoring system. The two charts on the left show object-level results, while the two right show material-subject results. For clarity, only the top 6 models are shown in the charts. And the full leaderboards are provided in Section B.3. Table 1: Pairwise rating alignment at the object level. We report the average pairwise accuracy across different scoring models. Note that ImageReward [65] and GPTEval3D [61] are tailored specifically for text-to-3D generation, and thus entries for image-to-3D are left blank. Image-to-3D TQ Text-to-3D TQ Metrics GTC GTC GD GD GP GP PA PA CLIP Score [42] ViCLIP Score [55] Aesthetic Score [44] Image Reward [65] GPTEval3D [61] 0.556 0.557 0.657 0.568 0.690 0.580 0.591 0.634 0.598 0. 0.606 0.625 0.607 0.607 0.677 0.556 0.577 0.629 0.513 0.667 0.604 0.617 0.623 0.610 0.649 0.589 0.589 0.570 - - 0.588 0.570 0.613 - - 0.605 0.611 0.622 - - 0.636 0.640 0.675 - - 0.623 0.623 0.630 - -"
        },
        {
            "title": "Ours",
            "content": "0.774 0.725 0.755 0.749 0.726 0. 0.703 0.753 0.732 0."
        },
        {
            "title": "5 Evaluation",
            "content": "To systematically assess generative performance, we employ our automated scoring system to construct comprehensive leaderboards, as illustrated in Figure 5. These leaderboards offer an objective and reproducible benchmark of state-of-the-art methods across multiple evaluation dimensions. Beyond model ranking, we further evaluate the reliability and robustness of our scoring system through extensive qualitative and quantitative analyses. As detailed in Section 5.1 and Section 5.2, these evaluations are organized hierarchically, spanning from coarse-grained object-level assessment to finer-grained material-subject and part-level evaluations, thereby demonstrating the effectiveness of our system across varying levels of detail. Additionally, we conduct ablation studies to systematically validate the design choices and contributions of individual components within the scoring pipeline. Detailed experimental settings and results are presented in Section B.1 and Section C."
        },
        {
            "title": "5.1 Quantitative analysis",
            "content": "Baseline metrics. In our evaluation, we incorporate 5 representative baseline metrics, encompassing CLIPScore [15], ViCLIP [55], Aesthetic score [44], ImageReward [65] and GPTEval3D [61]. Each metric is computed on our test set and averaged on 40 views. For GPTEval3D [61], we replace the original GPT-4v model with the more advanced GPT-4o when calculating pairwise comparisons. Additionally, noting that GPTEval3D [61] and ImageReward [65] are designed exclusively for text-to-3D evaluation, therefore they are not applicable to image-to-3D models. Object-level evaluation. In Table 1, we show the pairwise rating alignment in object-level across 1000 test objects pairs sampled from human annotations [73], covering both text-to-3D and image-to3D settings. As shown, our model consistently outperforms other evaluation methods in significant margin and achieves the highest accuracy across all dimensions, highlighting the effectiveness of our scoring system. Additional comparison results with T3Bench [13] are provided in Section B.2. Material-subject evaluation. For material evaluation, we sample 1000 image-to-3D pairs and 300 text-to-3D pairs from the test set to comprehensively assess the performance of our scoring model. As reported in Table 2, our model demonstrates strong alignment with human judgments, comprehensively outperforming baseline metrics across all dimensions. Notably, our model excels in lighting-sensitive dimensions such as Consistency and Artifact, indicating its capability to capture subtle material properties affected by illumination and artifacts. 8 Table 2: Pairwise rating alignment at the material level across objects. We report the average pairwise accuracy across different scoring models. As GPTEval3D [61] does not explicitly account for material properties in its texture assessment, its scores are mapped only to albedo-related dimensions for fair comparison. Metrics CLIP Score [42] ViCLIP Score [55] Aesthetic Score [44] Image Reward [65] GPTEval3D [61]"
        },
        {
            "title": "Ours",
            "content": "DC 0.647 0.673 0.690 0.627 0.630 0.767 Text-to-3D CS 0.607 0.657 0.690 0.613 0. 0.773 CA 0.543 0.577 0.563 0.540 - 0.733 MP 0.640 0.620 0.633 0.613 - 0.745 DC 0.699 0.701 0.642 - - 0.723 Image-to-3D CA CS 0.678 0.673 0.633 - - 0.771 0.604 0.630 0.550 - - 0.737 MP 0.689 0.698 0.619 - - 0. Figure 6: Visual examples of the object-level scoring model. We present representative examples in both image-to-3D and text-to-3D settings. For each object, we compare the predicted scores from our model with human ground-truth annotations across all evaluation dimensions. These results demonstrate the models ability to produce accurate and consistent assessments at the object level."
        },
        {
            "title": "5.2 Qualitative analysis",
            "content": "Object-level evaluation. We illustrate the scoring performance at the object level in Figure 6. As shown, our scoring system is capable of producing accurate and consistent scores for both textto-3D and image-to-3D objects across all dimensions. This demonstrates that our scoring model could capture and analyze 3D-aware visual clues, such as geometric structure and texture fidelity, highlighting its capability to perform holistic assessments at the object level. Part-level evaluation. The part-level evaluation paradigm enables more nuanced understanding of geometric quality within 3D assets by decomposing the object into semantically coherent components. This granularity allows the model to isolate and assess localized imperfections, such as collapses, distortions, or multi-faces, that are often obscured in holistic object-level evaluations. As shown in Figure 7, for Geometry Plausibility, this fine-grained perspective facilitates the detection of subregions that critically undermine overall perceptual quality, such as body distortions in the frog or extraneous limbs in the cat. For Geometry Detail, it provides comprehensive view of the spatial distribution of fine-scale features, distinguishing meaningful details from noise. Material-level evaluation. Our material-level evaluation focuses on more detailed analysis of texture. As illustrated in Figure 8, our model is able to look for differences in both details and lighting reactions and examine whether the texture maintains suitable visual harmony. For Consistency and Artifacts, our model is capable of handling various conditions of lights and capturing critical defects when turning to side or back. For Material Plausibility, our model can understand the object without additional prompt information and judge the correctness of reflections."
        },
        {
            "title": "6 Conclusion",
            "content": "This work takes significant step toward systematic evaluation of conditional 3D generation by proposing unified framework that integrates hierarchical granularity, physical material realism, and automated annotation. By combining object-level and part-level assessment, our framework enables both holistic assessments and fine-grained quality diagnosis. The introduction of multi-agent annotation pipeline further enables scalable and human-aligned dataset construction, supporting robust model training and evaluation. Our hybrid 3D-aware scoring models, grounded in both 9 Figure 7: Visual examples of the part-level scoring model. We apply normalized colormap to visualize part-level scores within objects, where blue indicates high-quality regions and red denotes low-quality regions. Left: Our scoring can locate surface distortions and abnormal structures in terms of geometry plausibility. Right: Our scoring can reflect the spatial distribution of geometric details. Figure 8: Visual examples of the material-level scoring model. We compare our score rating results with baseline metrics and human annotations. Our model can accurately capture the texture representations for detail analysis, observe obvious lighting differences or shading artifacts for lighting conditions, and understand basic reflection rules for material plausibility. geometry and video-based representations, offer promising alternative to traditional image-based proxies. We believe this framework can facilitate broader efforts toward generalizable 3D quality assessment, and serve as viable alternative to human evaluation. Limitations and future work. Although our framework advances hierarchical evaluation for 3D object generation, it currently focuses on object-centric, static assets. Extending the evaluation paradigm to compositional scenes or dynamic content remains an open challenge for future work. Second, our part-level annotation framework requires high-quality mesh segmentation and assumes consistent part semantics across different object instances, which may be unreliable for highly deformable or abstract shapes. Future work will aim to extend the evaluation paradigm to more complex scenarios, including dynamic and scene-level compositions, and explore adaptive segmentation strategies and more sophisticated multi-modal integration methods, thereby improving generalization. Societal Impacts. While the automated evaluation pipeline enhances scalability and consistency, it may inadvertently reinforce biases present in training data or propagate subjective quality norms at scale. We encourage responsible use of our framework and ethical deployment in applications."
        },
        {
            "title": "References",
            "content": "[1] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. [2] T. Cao, K. Kreis, S. Fidler, N. Sharp, and K. Yin. Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41694181, 2023. [3] D. Z. Chen, Y. Siddiqui, H.-Y. Lee, S. Tulyakov, and M. Nießner. Text2tex: Text-driven texture synthesis via diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1855818568, 2023. [4] R. Chen, Y. Chen, N. Jiao, and K. Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2224622256, 2023. [5] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, et al. 3dtopia-xl: Scaling high-quality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. [6] W. Cheng, J. Mu, X. Zeng, X. Chen, A. Pang, C. Zhang, Z. Wang, B. Fu, G. Yu, Z. Liu, et al. Mvpaint: Synchronized multi-view diffusion for painting anything 3d. arXiv preprint arXiv:2411.02336, 2024. [7] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [8] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [9] K. Deng, T. Omernick, A. Weiss, D. Ramanan, J.-Y. Zhu, T. Zhou, and M. Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In European Conference on Computer Vision, pages 90107. Springer, 2024. [10] A. E. Elo. The proposed uscf rating system, its development, theory, and applications. Chess life, 22(8):242247, 1967. [11] Y. Fang, Z. Sun, T. Wu, J. Wang, Z. Liu, G. Wetzstein, and D. Lin. Make-it-real: Unleashing large multimodal model for painting 3d objects with realistic materials. arXiv preprint arXiv:2404.16829, 2024. [12] K. Fu, H. Duan, Z. Zhang, X. Liu, X. Min, J. Wang, and G. Zhai. Multi-dimensional quality assessment for text-to-3d assets: Dataset and model. arXiv preprint arXiv:2502.16915, 2025. [13] Y. He, Y. Bai, M. Lin, W. Zhao, Y. Hu, J. Sheng, R. Yi, J. Li, and Y.-J. Liu. T3 bench: Benchmarking current progress in text-to-3d generation. arXiv preprint arXiv:2310.02977, 2023. [14] Z. He and T. Wang. Openlrm: Open-source large reconstruction models. https://github.com/ 3DTopia/OpenLRM, 2023. [15] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. [16] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [17] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [18] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu, F. Liu, K. Sunkavalli, T. Bui, and H. Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [19] Z. Huang, M. Boss, A. Vasishta, J. M. Rehg, and V. Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. [20] A. Jain, B. Mildenhall, J. T. Barron, P. Abbeel, and B. Poole. Zero-shot text-guided object generation with dream fields. In CVPR, pages 857866. IEEE, 2022. 11 [21] D. Jiang, X. Yang, Z. Zhao, S. Zhang, J. Yu, Z. Lai, S. Yang, C. Guo, X. Zhou, and Z. Ke. Flexitex: Enhancing texture generation via visual guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 39673975, 2025. [22] H. Jun and A. Nichol. arXiv:2305.02463, 2023. Shap-e: Generating conditional 3d implicit functions. arXiv preprint [23] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [24] X. Kong, S. Liu, X. Lyu, M. Taher, X. Qi, and A. J. Davison. Eschernet: generative model for scalable view synthesis. arXiv preprint arXiv:2402.03908, 2024. [25] Z. Lai, Y. Zhao, H. Liu, Z. Zhao, Q. Lin, H. Shi, X. Yang, M. Yang, S. Yang, Y. Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. [26] Y. Lan, F. Hong, S. Yang, S. Zhou, X. Meng, B. Dai, X. Pan, and C. C. Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In European Conference on Computer Vision, pages 112130. Springer, 2024. [27] J. Li, H. Tan, K. Zhang, Z. Xu, F. Luan, Y. Xu, Y. Hong, K. Sunkavalli, G. Shakhnarovich, and S. Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. [28] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65176526, 2024. [29] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang, K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300309, 2023. [30] M. Liu, R. Shi, L. Chen, Z. Zhang, C. Xu, X. Wei, H. Chen, C. Zeng, J. Gu, and H. Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. [31] M. Liu, M. A. Uy, D. Xiang, H. Su, S. Fidler, N. Sharp, and J. Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. [32] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [33] Y. Liu, C. Lin, Z. Zeng, X. Long, L. Liu, T. Komura, and W. Wang. Syncdreamer: Generating multiviewconsistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [34] X. Long, Y.-C. Guo, C. Lin, Y. Liu, Z. Dou, L. Liu, Y. Ma, S.-H. Zhang, M. Habermann, C. Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008, 2023. [35] S. Maiti, L. Agapito, and F. Kokkinos. Gen3deval: Using vllms for automatic evaluation of generated 3d objects. arXiv preprint arXiv:2504.08125, 2025. [36] G. Metzer, E. Richardson, O. Patashnik, R. Giryes, and D. Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. [37] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [38] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [39] M. Oquab, T. Darcet, T. Moutakanni, H. Q. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. B. Huang, S.-W. Li, I. Misra, M. G. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jégou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. [40] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 12 [41] G. Qian, J. Mai, A. Hamdi, J. Ren, A. Siarohin, B. Li, H.-Y. Lee, I. Skorokhodov, P. Wonka, S. Tulyakov, and B. Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [42] A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Amanda, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. Cornell University - arXiv,Cornell University - arXiv, Feb 2021. [43] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [44] C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. [45] Y. Shi, P. Wang, J. Ye, M. Long, K. Li, and X. Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [46] S. Su, X. Cai, L. Gao, P. Zeng, and Q. Du. Gt23d-bench: comprehensive general text-to-3d generation benchmark. arXiv preprint arXiv:2412.09997, 2024. [47] J. Sun, B. Zhang, R. Shao, L. Wang, W. Liu, Z. Xie, and Y. Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. [48] Z. Sun, T. Wu, P. Zhang, Y. Zang, X. Dong, Y. Xiong, D. Lin, and J. Wang. Bootstrap3d: Improving 3d content creation with synthetic data. arXiv preprint arXiv:2406.00093, 2024. [49] G. Tang, W. Zhao, L. Ford, D. Benhaim, and P. Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv preprint arXiv:2408.13679, 2024. [50] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. [51] J. Tang, T. Wang, B. Zhang, T. Zhang, R. Yi, L. Ma, and D. Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2281922829, 2023. [52] D. Tochilkin, D. Pankratz, Z. Liu, Z. Huang, A. Letts, Y. Li, D. Liang, C. Laforte, V. Jampani, and Y.-P. Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. [53] V. Voleti, C. Yao, M. Boss, A. Letts, D. Pankratz, D. Tochilkin, C. Laforte, R. Rombach, and V. Jampani. SV3D: novel multi-view synthesis and 3d generation from single image using latent video diffusion. CoRR, abs/2403.12008, 2024. [54] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1261912629, 2023. [55] Y. Wang, Y. He, Y. Li, K. Li, J. Yu, X. Ma, X. Chen, Y. Wang, P. Luo, Z. Liu, Y. Wang, L. Wang, and Y. Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [56] Y. Wang, X. Li, Z. Yan, Y. He, J. Yu, X. Zeng, C. Wang, C. Ma, H. Huang, J. Gao, M. Dou, K. Chen, W. Wang, Y. Qiao, Y. Wang, and L. Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. [57] Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [58] Z. Wang, Y. Wang, Y. Chen, C. Xiang, S. Chen, D. Yu, C. Li, H. Su, and J. Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In European Conference on Computer Vision, pages 5774. Springer, 2024. [59] H. Wu, E. Zhang, L. Liao, C. Chen, J. H. Hou, A. Wang, W. S. Sun, Q. Yan, and W. Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023. 13 [60] K. Wu, F. Liu, Z. Cai, R. Yan, H. Wang, Y. Hu, Y. Duan, and K. Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [61] T. Wu, G. Yang, Z. Li, K. Zhang, Z. Liu, L. Guibas, D. Lin, and G. Wetzstein. Gpt-4v (ision) is humanaligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2222722238, 2024. [62] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. [63] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [64] J. Xu, W. Cheng, Y. Gao, X. Wang, S. Gao, and Y. Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [65] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [66] Y. Xu, Z. Shi, W. Yifan, S. Peng, C. Yang, Y. Shen, and W. Gordon. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. arxiv: 2403.14621, 2024. [67] Y. Yang, Y. Huang, Y.-C. Guo, L. Lu, X. Wu, E. Y. Lam, Y.-P. Cao, and X. Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. [68] J. Ye, F. Liu, Q. Li, Z. Wang, Y. Wang, X. Wang, Y. Duan, and J. Zhu. Dreamreward: Text-to-3d generation with human preference, 2024. [69] K. Youwang, T.-H. Oh, and G. Pons-Moll. Paint-it: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43474356, 2024. [70] X. Yu, Z. Yuan, Y.-C. Guo, Y.-T. Liu, J. Liu, Y. Li, Y.-P. Cao, D. Liang, and X. Qi. Texgen: generative diffusion model for mesh textures. ACM Transactions on Graphics (TOG), 43(6):114, 2024. [71] X. Zeng, X. Chen, Z. Qi, W. Liu, Z. Zhao, Z. Wang, B. Fu, Y. Liu, and G. Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42524262, 2024. [72] L. Zhang, Z. Wang, Q. Zhang, Q. Qiu, A. Pang, H. Jiang, W. Yang, L. Xu, and J. Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [73] Y. Zhang, M. Zhang, T. Wu, T. Wang, G. Wetzstein, D. Lin, and Z. Liu. 3dgen-bench: Comprehensive benchmark suite for 3d generative models. arXiv preprint arXiv:2503.21745, 2025. [74] Z. Zhao, Z. Lai, Q. Lin, Y. Zhao, H. Liu, S. Yang, Y. Feng, M. Yang, S. Zhang, X. Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. [75] C. Zheng and A. Vedaldi. Free3d: Consistent novel view synthesis without 3d representation. arXiv, 2023. [76] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [77] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and S.-H. Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147, 2023."
        },
        {
            "title": "A More details about scoring dataset",
            "content": "A.1 Overview In this paper, we conduct comprehensive 3D evaluation dataset with hierarchical annotations based on 3DGen-Bench [73]. To be specific, we extend the object-level annotations in 3DGen-Bench [73], and propose novel part-level and material-subject evaluation through customized preprocessing (e.g., part segmentation and relighting). In summary, we obtain 15.3k synthesized 3D assets, with 4k related object-level annotations, 23k part-level annotations, and 11k material-subject annotations. The complete dataset can be accessed and downloaded publicly from the Huggingface repo: https: //huggingface.co/datasets/anonymous-mY2nG5/H3DBench. A.2 Data curation 3D generative models involved. Our benchmark includes 30 3D generative models in total, including 9 text-to-3D models and 21 image-to-3D models. The full list of involved models is provided below. 9 Text-to-3D models: MVDream [45], LucidDreamer [28], GRM [66], Magic3D [29], Latent-NeRF [36], DreamFusion [40], SJC [54], Point-E [38], Shap-E [22]. 21 Image-to-3D models: Hunyuan3D 2.0 [74], Trellis [63], SPAR3D [19], TripoSR [52], Unique3D [60], CRM [58], LN3Diff [26], InstantMesh [64], Wonder3D [34], OpenLRM [14], Stable Zero123 [32], Zero-1-to-3 XL [32], Magic123 [41], LGM [50], GRM [66], SyncDreamer [33], Shap-E [22], Triplane-Gaussian [77], Point-E [38], EscherNet [24], Free3D [75]. Implementation details. We generated 510 assets for each method guided by 510 prompts proposed by 3DGen-Bench [73]. All experiments are conducted with the official public code and the default hyperparameters. Notably, Trellis [63] and Hunyuan3D 2.0 [74] only release their code and checkpoints for image-to-3D; thus, we havent conducted experiments for text prompts. The entire generation process consumes around 2 weeks, utilizing 4 NVIDIA A100-SXM4-80GB GPUs. Rendering details. Our rendering pipeline is implemented using both Blender and Kiui Python tools. Specifically, Blender is used for HDRI and point-light rendering due to its high-quality output, while Kiui is adopted for RGB and normal map rendering, benefiting from its computational efficiency and implementation simplicity. All videos are rendered at default resolution of 512512 pixels with 25 FPS, and we maintain consistent lighting and camera parameters across all methods. The average rendering cost per video is 17.47s using Blender. Part-level segmentation. To support part-level annotation and evaluation, we propose to perform part segmentation first. To handle open-vocabulary 3D assets and accommodate structural failures (e.g., mesh collapses or topological inconsistencies) commonly observed in generative outputs, we adopt semantic-free, category-agnostic partitioning strategy. We evaluate several state-of-the-art methods, such as SAMPart3D [67], SAMesh [49] and PartField [31]. However, SAMPart3D [67] relies on sample-specific optimization and requires approximately 30 minutes per mesh, making it impractical for large-scale preprocessing. Compared to SAMesh [49], lifting 2D segmentation into 3D, PartField [31] builds hierarchical segmentation tree based on learned feature fields, offering finer control over part granularity and more stable performance. Additionally, its learned features can be directly leveraged in our scoring pipeline. Thus, we adopt PartField [31] as the final codebase. Since the number of parts is not predicted by models, we prompt GPT to assign suitable target part counts for each prompt, as illustrated in Figure S1. Relighting. To accurately capture real-world texture quality and enhance fine-grained detail evaluation, we implement multi-lighting setup using controlled point lights and environmental HDRI maps. Four point light sources are employed to cover complete surfaces across all viewing angles, which are separately positioned at right, top, right-top, and right-bottom relative to the object. Besides, we select six diverse HDRI environment maps of both natural and artificial lighting conditions in indoor and outdoor scenarios, as shown in Figure S2. For each lighting configuration, we generate 40-frame 3Poly Haven website: https://polyhaven.com/hdris 15 Figure S1: Visualization about 3D part segmentation. Left: Due to varying complexity across prompts, assigning fixed number of parts to all objects is suboptimal. Right: We propose to estimate prompt-specific part counts with GPT4V to better reflect meaningful structural granularity. video sequence with the object rotating 360 degrees. This rigorous relighting protocol establishes robust and comprehensive representation of material characteristics that closely approximates real-world viewing conditions. Qualitative results are visualized in Figure S3. A.3 Annotation pipeline In this section, we provide detailed introduction to our automatic annotation system (M 2AP) and present related experiments concerning its design and validation. Selected agents. To automatically provide practical and human-aligned scores for each 3D asset, we employ advanced multi-modal large language models (MLLMs) as our labeling agents. For objective and fair evaluations, preliminary experiments led to the selection of two types of models as scoring experts: thinking models and reasoning models. Thinking models excel at deep reflection and analysis, while reasoning models leverage extensive knowledge bases to deliver more efficient and stable results. The selected models include GPT 4.1 ( https://openai.com/), GPT o3/o4 mini ( https://openai.com/), Gemini 2.5 Pro ( https://gemini.google.com/), Claude 3.7 ( https://www.anthropic.com/), and Grok-3 ( https://x.ai/). Gemini 2.5 Pro processes rotating videos of 3D objects as input, while other agents process multi-view images. Prompt design. To mitigate potential MLLM hallucination and ensure accurate, consistent 3D asset evaluations, we engineered an elaborate prompt. The prompt guides the MLLMs through systematic process, defines clear assessment criteria, incorporates physical realism checks, includes reflection phase, and uses comparative examples to align automated scoring with human perceptual judgments. Stepped instruction. The MLLM is first assigned the role of an expert evaluator, tasked with providing detailed initial description of the 3D asset. It then systematically analyzes and follow step-by-step instruction the asset from multiple views (Geometry, Normal Map, RGB) and perspectives, aiming for comprehensive understanding. The findings are structured into predefined JSON output, ensuring methodical evaluation flow. Criterion definition. For consistent and objective scoring, the prompt specifies set of key dimensions: Geometry Plausibility (GP), Geometry Details (GD), Texture Quality (TQ), Geometry-Texture Coherency (GTC), Prompt-Asset Alignment (PA), Details and Complexity (DC), Colorfulness and Saturation (CS), Consistency and Artifacts (CA), and Material Plausibility (MP). As illustrated in Figure S4 and Figure S5, each dimension is equipped with multi-level scoring rubric, supported by clearly defined qualitative descriptors and structured evaluation protocol. This enables MLLMs to systematically assess the 3D content by comparing it with standardized quality levels. Physical alignment. To ensure generated assets are realistic, the prompt emphasizes assessing physical plausibility. It involves checking the correct positioning of object parts and evaluating if material properties (e.g., smoothness of wood or metal) align with real-world expectations. Structural anomalies or incorrect physical characteristics are penalized. 16 Figure S2: Visualization about HDRI maps. We select six HDRI maps from Poly Haven3, including natural, artificial, indoor, outdoor, daylight and nightlight conditions. Figure S3: Visualization about relighting. We present the first frame of each object under varying illumination conditions. The leftmost metallic sphere serves as reference, reflecting the HDRI environment or point light source position. Additionally, for the right-top and right-bottom light configurations, we adjust the camera elevation to ensure full object coverage. 17 Table R1: Ablation study about the annotation pipeline. We conduct systematic comparison between our proposed annotation pipeline and baseline approaches with single LLM agents, employing the L1 loss between model outputs and human judgments as an evaluation metric. Through component-wise ablation studies, we further analyze each key element in our pipeline design."
        },
        {
            "title": "Method",
            "content": "GPT 4.1 Claude 3.7 Gemini 2.5 Grok 3 o3/o4 mini w/o Physical w/o Reflection Full Single Agent M²AP L1 Loss 0.838 1. 1.020 0.920 0.702 0.568 0.476 0. Table R2: API cost for annotation procedures."
        },
        {
            "title": "Annotation type",
            "content": "Object-level Part-level Material-subject Cost (USD) 2.5k 1.0k 0.6k Total 4.1k Reflection. self-correction phase is integrated to enhance reliability. The MLLM, acting as an autonomous auditor, re-evaluates its initial textual analysis and scores against the predefined criteria, ignoring its prior numerical assignments. If discrepancies are found, it provides revised scores, aiming to improve the accuracy and consistency of the final assessment. Typical examples. To align automated scores with human perception, the prompt includes examples comparing human annotations with typical MLLM evaluations. It guides the agents to understand human evaluators focus, calibrate its scoring to reflect human priorities and sensitivities to flaws or strengths, and reduce biases, ultimately capturing nuances important to human judgment. The structured prompt design aims to minimize ambiguity and ensure MLLMs adhere to rigorous, consistent, and human-aligned evaluation. Ablation study. Our experiments validate the effectiveness of the proposed 2AP annotation pipeline through systematic comparisons with individual MLLM agents and component-wise ablations, using L1 distance to human annotations as the evaluation metric. As shown in Table R1, the complete 2AP framework achieves superior performance (L1=0.257), significantly outperforming individual state-of-the-art models. Furthermore, ablations demonstrate the critical role of each component: omission of either the Physical Alignment check (L1=0.568) or Reflection mechanism (L1=0.476) substantially degrades performance, confirming their importance for annotation accuracy. Annotation Cost. Without parallelization, annotating single object typically takes around 20 to 60 seconds, depending mainly on the network latency. In terms of cost, completing one M²AP annotation-which involves calls to multiple VLM APIsfor single object incurs approximately 0.15 USD. Statistics of the cost for each setting are shown in Table R2. More details about video-based scoring model B.1 Ablation experiments Prompt encoder. Due to the different performance of CLIP [42] and DINOv2 [39] image encoders in 3D awareness, we investigate the effect of image encoders in training stage-1. As shown in Table R3, there exists clear decrease in scoring accuracy when DINOv2 is employed as the image prompt encoder. One potential explanation is that the CLIP text encoder is selected as the text prompt encoder, Table R3: Ablation study about the video-based scoring model. We evaluate the L1 loss of scores and the accuracy of pairwise rating in the DC dimension under several settings. Our final configuration selects CLIP as the prompt encoder, sets the dropout ratio to 0.5, and combines SmoothL1 loss and ranking loss as the objective function. Prompt Encoder DINOv Dropout Ratio 0."
        },
        {
            "title": "Objective Function\nMAE\nMSE",
            "content": "L1 Loss Pairwise Accuracy 0.550 0.621 0.426 0.708 0.332 0.758 0.397 0.757 ours 0.312 0.798 18 Table R4: Ablation study on video frames. We calculate L1 loss for each dimension under different frame number settings. The average inference time per object is listed below. Frames 4 8 32 16 (Ours) DC 0.338 0.335 0.311 0.312 CS 0.330 0.311 0.296 0. CA 0.341 0.337 0.294 0.288 MP 0.343 0.323 0.291 0. Inference Time (s / it) 0.211 0.250 0.497 0.320 Table R5: Pairwise rating alignment with 3Bnech at the object level. Image Reward, 3Bnech, and GPTEval3D could only calculate the pairwise scores among text-to-3D objects. Metrics GP GD Text-to-3D TQ CLIP Score [42] ViCLIP Score [55] Aesthetic Score [44] Image Reward [65] T3Bench [13] GPTEval3D [61] 0.556 0.557 0.657 0.568 0.661 0.690 0.580 0.591 0.634 0.598 0.647 0.689 0.606 0.625 0.607 0.607 0.628 0. GTC 0.556 0.577 0.629 0.513 0.673 0.667 PA GP Image-to-3D TQ GD 0.604 0.617 0.623 0.610 0.631 0.649 0.589 0.589 0.570 - - - 0.588 0.570 0.613 - - - 0.605 0.611 0.622 - - - GTC 0.636 0.640 0.675 - - - PA 0.623 0.623 0.630 - - -"
        },
        {
            "title": "Ours",
            "content": "0.774 0.725 0.755 0.749 0.726 0. 0.703 0.753 0.732 0.710 which provides better alignment for the CLIP image encoder in the latent space, leading to more effective training outcomes in stage-1. Dropout ratio. Our prediction head incorporates Dropout layers followed by two Conv3D layers to mitigate overfitting, as depicted in Figure 3. Through the ablation study in Table R3, we demonstrate that increasing the dropout ratio not only accelerates training convergence but also enhances inference accuracy. This suggests that aggressive dropout is particularly effective when processing largescale video features extracted by the encoder, likely due to its capacity to robustly regularize highdimensional spatio-temporal representations. Objective function. As described in 4.1, our final loss function is composed of Smooth L1 Loss and Rank Loss. To examine the effectiveness of our loss function, we conduct ablation experiments in which prediction heads are trained using different losses in DC dimension. Table R3 reveals that MAE falls short in penalty for large errors compared to MSE and ours. For pairwise rating accuracy, our ranking loss obviously contributes to the relative comparison capability of the prediction head, which demonstrates the effectiveness of our loss design. Frame count. We carry out experiments on different frame numbers of input videos. As shown in Table R4, with frames increasing from 4 to 16, the scoring accuracy also shows consistent upward trend. However, there is no significant difference between 16 frames and 32 frames in the aspect of accuracy, which indicates abundant information is included in 16 frames. Considering the trade-off between accuracy and inference efficiency, we adopt 16 frames as the final setting. B.2 Comparison with T3Bench To further validate the effectiveness of our scoring framework, we conduct supplementary experiment comparing pairwise rating alignment with 3Bench [13], benchmark designed for evaluating text-to-3D generation. Specifically, we follow the standard pairwise protocol used in Section 5.1. As reported in Table R5, our method achieves significantly higher alignment with human judgments compared to 3Bench [13], providing more reliable discrimination ability in pairwise comparisons. B.3 Leaderboard Object level. Table R6 presents the comprehensive leaderboard for object-level evaluation across 22 methods, including image-condition and text-condition methods. Hunyuan3D 2.5 achieves the highest overall performance (16.561), outperforming other approaches across most dimensions, particularly in Geometry Plausibility (6.46). Image-to-3D methods generally dominate the upper rankings, with Hunyuan3D, Trellis, and SPARD3D forming the top three. 19 Table R6: Full leaderboard at the object level. We accumulate the scores of five dimensions as the overall score and sort methods in the sequence of overall performance. Method Method Type GP GD TQ GTC PA Hunyuan3D 2.5 [25] Hunyuan3D 2.0 [74] Hunyuan3D 2.5 [25] Trellis [63] SPARD3D [19] TripoSR [52] InstantMesh [64] CRM [58] MVdream [45] Unique3D [60] OpenLRM [14] Wonder3D [34] Stable-Zero123 [32] Magic123 [41] GRM-Image [66] LGM [50] Lucid-Dreamer [28] GRM-Text [66] Latent-NeRF [36] Magic3D [29] SyncDreamer [33] Dreamfusion [40] Triplane-Gaussian [77] Image-to-3D Image-to-3D Text-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Text-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Text-to-3D Text-to-3D Text-to-3D Text-to-3D Image-to-3D Text-to-3D Image-to-3D 6.46 6.2919 6.42 5.8626 5.7791 5.2216 5.4242 4.745 4.4064 4.9288 3.7754 3.7879 3.6052 3.4617 3.2932 3.2148 2.9346 3.0096 2.7265 2.9015 2.9423 2.669 2. 2.86 2.7215 2.7 2.392 2.3031 2.4225 2.2252 2.2991 2.742 2.3233 2.2614 2.0092 1.6548 1.74 1.857 1.6733 1.5891 1.6627 1.7067 1.6239 1.5323 1.2525 1.1859 2.79 2.7644 2.45 2.4693 2.4749 2.3758 2.3063 2.3777 2.8116 1.9627 2.0922 1.9658 2.0293 2.0094 1.8885 1.8891 2.1069 1.7389 1.7065 1.5395 1.2134 1.183 1.2028 0.981 0.9876 0.947 0.9702 0.9601 0.9562 0.9587 0.9164 0.951 0.776 0.902 0.9255 0.8902 0.898 0.849 0.8118 0.8333 0.898 0.8412 0.9431 0.8529 0.9137 0.6647 3.47 3.4334 3.18 3.5048 3.4842 3.3643 3.363 3.219 2.5879 3.1989 2.2298 2.0874 2.2578 2.2171 2.0735 2.0304 2.0297 1.793 1.7688 1.5618 1.2776 1.3446 1.2908 Overall 16.561 16.1988 15.697 15.1989 15.0014 14.3404 14.2775 13.5572 13.4989 13.1897 11.2608 10.7758 10.4374 10.3262 9.9612 9.6193 9.4936 9.1023 8.7497 8.5698 7.8185 7.3627 6. Table R7: Full leaderboard at the material level. We accumulate the scores of four dimensions as the overall score and sort methods in the sequence of overall performance."
        },
        {
            "title": "Method Type",
            "content": "DC Hunyuan3D 2.0 [74] Trellis [63] SPAR3D [19] TripoSR [52] InstantMesh [64] Wonder3D [34] CRM [58] Mvdream [45] LN3Diff [26] GRM-text [66] Magic3d [29] OpenLRM [14] LucidDreamer [28] Dreamfusion [40] LGM [50] GRM-image [66] Stable Zero123 Zero-1-to-3 [32] 3DTopia-XL [5] SyncDreamer [33] Latent-Nerf [36] Triplane [77] SJC [54] Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Text-to-3D Image-to-3D Text-to-3D Text-to-3D Image-to-3D Text-to-3D Text-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Image-to-3D Text-to-3D Image-to-3D Text-to-3D 2.5332 2.4812 2.4742 2.3262 2.1675 1.9707 2.0305 2.0029 1.9392 1.7664 1.7630 1.9777 1.8763 1.5700 1.7470 1.6203 1.8004 1.7317 1.6359 1.2629 1.4644 1.1629 0.9066 CS 3.0001 3.0014 3.0561 2.7841 2.7699 2.5512 2.6174 2.5179 2.5072 2.4341 2.2465 2.4608 2.3311 2.2032 2.3838 2.3085 2.3785 2.3282 2.2525 2.0990 1.9590 1.9399 1. CA 3.0832 3.0138 2.6510 2.7001 2.6279 2.5148 2.3580 2.4175 2.3494 2.5874 2.6972 2.0886 2.3131 2.7198 2.2318 2.4136 2.1614 2.2199 1.9778 2.4309 2.0826 2.0471 2.0290 MP 2.9344 2.9036 2.6756 2.6832 2.5735 2.3972 2.3836 2.2525 2.3432 2.3500 2.0950 2.2672 2.1951 2.1211 2.1953 2.1629 2.0483 2.0657 1.9235 1.8567 1.7380 1.7167 1."
        },
        {
            "title": "Overall",
            "content": "11.5509 11.4000 10.8568 10.4936 10.1388 9.4339 9.3896 9.1907 9.1390 9.1378 8.8018 8.7943 8.7156 8.6141 8.5579 8.5052 8.3887 8.3454 7.7898 7.6494 7.2440 6.8665 5.3016 Material subject. Based on the object-level evaluation, we select 23 methods that demonstrate acceptable performance in texture and geometry. The overall leaderboard is shown in Table R7, suggesting great potentials for text-to-shape methods and space for improvement in aspects of texture detail and visual harmony. 20 Table R8: Ablation experiments of the 3D-based scoring model. We conduct ablation experiments for proposed attention modules and predict heads with the criterion of normalized L1 and L2 loss on the test set. The setting of our model is composed of two attention modules for global-part interaction and inner-part interaction, and the predict head is Linear layer. ours w/o global_attn w/o part_attn w/o attns 2-layer mlp 3-layer mlp L1 Loss L2 Loss 0.0850 0. 0.0866 0.0127 0.0935 0.0142 0.0913 0.0135 0.0903 0.0131 0.0877 0.0127 More details about 3D-based scoring model C.1 Ablation experiments We conduct ablation experiments for our proposed 3D-based scoring model on 493 test parts, including the two attention modules and the depth of MLP, as described in Section 4.2. Attention modules. As shown in Table R8, experimental results clearly demonstrate the complementary roles of the cross-attention and self-attention modules in part-level quality prediction. The cross-attention mechanism effectively integrates global contextual cues into each part representation, while the self-attention module plays crucial role in capturing intra-part dependencies and enforcing local coherence. Removing the self-attention module leads to significant drop in performance, indicating the essentiality of modeling inner-part interactions. Predict head. To investigate the impact of MLP depth in the prediction head, we conduct an ablation study varying the number of layers from 1 to 3. As summarized in Table R8, the predictor achieves the best performance with single-layer MLP. Interestingly, using 3-layer MLP yields slightly worse results, while the 2-layer variant performs the worst among all settings. We hypothesize that deeper MLP may introduce unnecessary complexity and overfitting risks. These findings suggest that simple 1-layer MLP strikes better balance between capacity and generalization for this task. 21 Figure S4: Scoring criterion of each dimension at the object level. 22 Figure S5: Scoring criterion of each dimension at the part level and material level."
        }
    ],
    "affiliations": [
        "Fudan University",
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}