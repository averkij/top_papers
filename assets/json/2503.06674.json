{
    "paper_title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
    "authors": [
        "Yihong Luo",
        "Tianyang Hu",
        "Jiacheng Sun",
        "Yujun Cai",
        "Jing Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-$\\alpha$, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-$\\alpha$ into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/"
        },
        {
            "title": "Start",
            "content": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching Yihong Luo 1* Tianyang Hu 2 Jiacheng Sun 2 Yujun Cai 3 Jing Tang 4,"
        },
        {
            "title": "4 HKUST (GZ)",
            "content": "5 2 0 2 2 1 ] . [ 2 4 7 6 6 0 . 3 0 5 2 : r Figure 1. User Study Time! Which one do you think is better? Some images are generated by Pixart-α (50 NFE). Some images are generated by TDM (4 NFE), distilling from Pixart-α in data-free way with merely 500 training iterations and 2 A800 hours. All images are generated from the same initial noise. We put the location of generated images by TDM in footnote."
        },
        {
            "title": "Abstract",
            "content": "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methodsbased on distribution matching [19, 41] and trajectory matching [27, 35]reduce sampling to as few as one step, they fall short on complex tasks like textto-image generation. Few-step generation offers better balance between speed and quality, but existing approaches face persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces datafree score distillation objective, aligning the students trajectory with the teachers at the distribution level. Further, we develop sampling-steps-aware objective that de- *Work was partly done during an internship at Huawei Noahs Ark Lab. Corresponding Author. TDM (left to right): bottom, bottom, top, bottom, top. couples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-α, delivering superior quality and signifIn particular, our method icantly reduced training costs. distills PixArt-α into 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours mere 0.01% of the teachers training cost. In addition, our proposed TDM can be extended to accelerate text-tovideo diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/. 1. Introduction Accelerating diffusion model sampling is essential for the efficient deployment of AIGC models. Thanks to various diffusion distillation techniques, the number of sampling 1 Figure 2. The comparison between Four-step generated images by TDM under different training iterations and pre-trained diffusion models with 25 steps and 5.5 CFG. It can be seen that the ultra-fast convergence of our method, without sacrificing the sample quality. steps has been drastically reduced to as few as one. Most notably, approaches based on distribution matching through score distillation, such as DMD [41], Diff-Instruct [19], and SiD [44], have demonstrated strong performance in this regime. However, even state-of-the-art one-step generation methods fall short in complex tasks, particularly in textIn such cases, few-step generation to-image generation. strikes better balance between speed and image quality. Yet, existing distribution matching methods are primarily designed for one-step generation, optimizing solely for performance at this extreme and lacking the flexibility to extract extra information with increased sampling steps. An alternative approach, commonly known as trajectory distillation [16, 27, 35], operates on the instance level, aiming to simulate the original diffusion generation trajectories in as few steps as possible. However, matching entire instancelevel trajectories with fewer steps poses substantial challenges, requiring high model capacity. Consequently, its performance in few-step generation remains suboptimal. To bridge this gap, we propose novel framework that non-trivially unifies trajectory distillation and distribution matching, termed Trajectory Distribution Matching (TDM). Specifically, our approach aligns the students trajectory with the teachers at the distribution level using novel, data-free score distillation objective. TDM supports deterministic sampling which inherently has better fewstep performance than stochastic sampling, enabling faster convergence [1, 14, 15, 32]. Our method combines the strengths of both distribution and trajectory matching. On the distribution matching side, the addition of trajectory information enables us to better inherit teachers knowledge in fine-grained fashion for multi-step generation. On the trajectory matching side, since we match the pre-trained diffusion distribution at different timesteps, the learning of the generator becomes easier, and better performance is expected from the same model capacity. To accommodate diverse use cases, an ideal generator should also offer users flexible control over the number of sampling steps. However, existing works in text-to-image diffusion distillation either only support less-adjustable steps based on deterministic sampling [11, 27, 39] or adjustable steps based on stochastic sampling, which sacrifices image quality for flexibility [17, 25]. To address these limitations, we propose novel sampling-steps-aware objective that decouples learning targets among different sampling steps, enabling flexible deterministic sampling. We name the model trained with this technique as TDM-unify. Furthermore, inspired by our new perspective built on the commonalities between TDM and CMs [35], we introduce surrogate training objective that employs the Pseudo-Huber metric, which has proven effective in iCT [33]. Our method achieves state-of-the-art (SOTA) performance across various text-to-image backbones with remarkable efficiency. On SD-v1.5 [26], our TDM-unify demonstrates superior performance in both 1-step and 4-step generation, substantially outperforming existing approaches. On distilling SDXL [24] for 4-step inference, our method surpasses SDXL-Lightning [11] by +2.17 in Human Preference Score (HPS) [37] and +1.05 in CLIP Score, outperforms Hyper-SD [25] by +0.74 in HPS and +1.81 in CLIP Score, and exceeds DMD2 [42] by +3.42 in HPS and +0.57 in CLIP Score. Notably, our approach demonstrates exceptional training efficiency, requiring only 2 A800 days for SDXL compared to LCMs [17] 32 A100 days and DMD2s 160 A100 days. On distilling PixArt-α [2] for 4step inference, our method surpasses the multi-step teacher in terms of HPS, AeS, and user preference. This is achieved by merely 500 training iterations and 2 A800 hours, only 0.01% of the teachers training cost. Additionally, our method can distill the video diffusion models to 4-step generator with better performance. In particular, we distill the CogVideoX-2B [40] to 4-step generator by TDM, improving the Vbench [8] total score from 80.91 to 81.65. 2. Background Diffusion Models (DMs). DMs [7, 31] define forward diffusion process that gradually injects Gaussian noises into the data in steps: q(xtx) (xt; x, σ2 I), where σt specifies the noise schedule. The diffused samples can be 2 Figure 3. Additional Samples by TDM with 4-step generation on SDXL backbone. directly obtained by xt = + σtϵ, where ϵ (0, I). Then the diffusion network fϕ is trained by denoising via: Et,x,ϵωtfϕ(xt, t) x2 2, where ωt controls the importance of the denoising at different timesteps. After training, we can estimate the score of diffused samples via: xt log pt(xt) sϕ(xt, t) = xtfϕ(xt,t) σ2 , (1) where pt(xt) = (cid:82) q(xtx)pd(x)dx. Sampling from DMs can be achieved by solving corresponding Probability Flow ODE (PF-ODE) via multiple steps (typically 25) [34]. Diffusion Distillation. Recently, there are two appealing ways to explicitly transfer knowledge from pre-trained DMs to few-step students: 1) Trajectory distillation [16, 16, 21, 27, 33, 35, 39], which typically aims to simulate teacher ODE trajectories on the instance level. These methods suffer from difficult instance-level matching and numerical errors when solving teachers PF-ODE; 2) Distribution matching via score distillation [19, 41, 44], which distills 3 from pre-trained DMs on distribution level via certain distribution divergence. These methods mostly ignore the intermediate steps of trajectory, making it hard to extract extra information with increased sampling steps. More discussion on related works deferred to Appendix due to limited space. 3. Methodology Despite the success of existing diffusion distillation methods, achieving industry-ready few-step performance remains challenging. The challenge might stem from trajectory distillation and distribution matching through score distillation both of which suffer from different limitations as we discussed above. To address these limitations, we propose Trajectory Distribution Matching (TDM), novel framework that unifies trajectory distillation and distribution matching, aligning the students trajectory with the teachers at the distribution level. First, we introduce distribution-level trajectory matching objective that enables efficient knowledge transfer while avoiding numerical errors in solving teachers PF-ODE (Sec. 3.1). Second, we propose sampling-steps-aware objective that supports flexible step adjustment (Sec. 3.2). Finally, we develop surrogate training objective inspired by consistency models to enhance optimization (Sec. 3.3). 3.1. Trajectory Distribution Matching The diffusion distillation via simulating the teacher trajectory typically minimizes the following objective [16, 27]: min θ d(ODESolver(xt, fϕ, t, s), DDIM(xt, fθ, t, s)), (2) where d(, ) is distance metric, xt is the diffused samples, fϕ is the teacher network, and fθ is the student network. The learning objective can be re-written as the following joint forward KL divergence: To address these issues, we propose to align the ODE trajectories of the distillation model with the teacher model at corresponding timesteps at the distribution level. Specifically, at each training iteration, we use an ODESolver (e.g., DPMSolver [13]) to sample from the K-step student model pθ and save the intermediate samples xti on the trajectory. Then motivated by the success of score distillation such as Diff-Instruct [19] and VSD [36], we seek to minimize the following marginal reverse KL divergence: L(θ) = K1 (cid:88) i=0 KL(pθ,ti (xti )pϕ,ti (xti )), (4) where pϕ is the pre-trained teacher DM, xti is the samples on the student trajectory at timestep ti, and pθ,ti(xti) denotes the implicit distribution of xti. By default, we let ti = i, where is the terminal timestep. The advantage of the objective is twofold: 1) we only need to sample from the student which is data-free and computationally efficient; 2) we can simulate the teacher trajectory at the distribution level without requiring sampling from the teacher. This also avoids numerical error in solving teacher ODE. It is crucial to note that while we need to solve the student ODE, we are essentially just training K-step generator that is parameterized by the discrete form of the ODE sampler. Following previous works [19, 41], we diffuse the distribution to align the training objective with the diffusion process for better transferring knowledge from pre-trained DMs. The final objective becomes: L(θ) = K1 (cid:88) ti+1 (cid:88) i=0 τ =ti λτ KL(pθ,τ ti (xτ )pϕ,τ (xτ )), (5) where pθ,τ ti(xτ ) (cid:82) q(xτ xti)pθ,ti(xti)dxti denotes marginal diffused distribution at timestep τ . min θ KL(pt(xt)pϕ(xsxt)pt(xt)pθ(xsxt)) (3)"
        },
        {
            "title": "The gradient of the proposed objective can be computed",
            "content": "where pt(xt) is the marginal distribution of diffused samples, pϕ(xsxt) is the backward diffusion process which can be sampled by ODESolver, and pθ(xsxt) is the desired one-step denoising distribution. The objective suffers from multiple issues: 1) This objective enforces point-to-point match on the trajectory which makes it difficult to achieve optimal performance due to limited model capacity; 2) This objective requires solving ODE via multi-step DMs which is not only computationally expensive but also potentially suffers form numerical error. We argue that simulating the teachers ODE trajectories can maximize the inheritance of teacher knowledge, thereby reducing learning difficulty, accelerating convergence, and improving distillation performance. However, existing trajectory distillation methods have suffered severe performance degradation due to the aforementioned issues. as follows: θL(θ) = K1 (cid:88) ti+1 (cid:88) i=0 τ =ti K1 (cid:88) ti+1 (cid:88) i= τ =ti λτ [xτ log pθ,τ ti (xτ ) sϕ(xτ , τ )] xti θ λτ [sψ(xτ , τ ) sϕ(xτ , τ )] xti θ . Since the score of student distribution is hard to access, we use another DM sψ for approximation. Following GANs tradition [4], we call sϕ as real score and sψ as fake score. We briefly summarize the learning of generator in Fig. 4. For ease of reference, we rewrite the objective for training student θ with the same gradient in Eq. (6) as follows: L(θ) = K1 (cid:88) ti+1 (cid:88) i=0 τ =ti λτ xti sg(xti )2 2, (6) 4 the disadvantage is that the pretrained diffusion model limits the performance ceiling. Specifically, the performance upper bound of our method is to perfectly sample from the pre-trained diffusion model. We found that the currently popular SD-v1.5 [26], due to the uneven quality of training data, its generation quality does not align well with human preferences. To address this issue, we suggest fine-tuning SD-v1.5 on compact high-quality dataset before distilling student models. This ensures more capable teacher model, ultimately leading to enhanced student performance. Although recent work [42] suggest utilizing real data by GANs in distillation, it is less effective and more computationally expensive compared to our proposed pipeline (Sec. 4.1). 3.2. Enabling Flexible Sampling Steps Once trained with our proposed objective (Equation 5), we can develop powerful K-step generator. While practitioners may need to adjust the number of sampling steps in real-world applications, using the K-step generator with fewer steps (M < K) yields suboptimal results. This limitation arises because our approach only ensures alignment between intermediate samples in the ODE trajectory and their corresponding diffused distributions. We observe that existing few-step methods supporting deterministic sampling [11, 27, 39] suffer from similar limitations, specifically: the distillation target at each step is fixed to samples at predetermined timesteps along the ODE trajectory. This constrains their flexibility in specifying the number of sampling steps and requires multi-stage training to gradually reduce the sampling steps [11]. To address this, we propose that the training objective should be sampling-steps-aware to optimize performance across different step counts. We therefore introduce the following objective: K1 (cid:88) tK i+1 (cid:88) EK i= τ =tK λτ KL(pθ,τ tK (xτ K)pϕ,τ (xτ )), (8) i )pθ(xtK = K)dxtK (xτ K) (cid:82) q(xτ xtK , where pθ,τ tK is uniformly sampled from list of desired sampling steps and we let tK i. Note that the is injected into both student and fake scores as condition for guidance. Why do we need sampling-steps-aware fake score? For simplicity, we directly consider the case without noise without loss of generality. Suppose the fake score is shared among pK1 (x) and pK2 (x), the optimal fake score becomes: pK1 (x)x log pK1 (x) + pK2 (x)x log pK2 (x) pK1 (x) + pK2 (x) (9) See Appendix for derivation. It can be seen that even given the optimal fake score, using shared fake scores still introduces bias into training. This is also the reason that we ensure the diffused intervals do not overlap in the Kstep learning objective in Eq. (5). recent work [3] shares Figure 4. Trajectory Distribution Matching. An illustration of training 2-step generator by TDM in data-free way. where xti = xti + λτ [sϕ(xτ , τ ) sψ(xτ , τ )], xti is the sample on the student trajectory, and sg denotes stop gradient. The xti can be regarded as revised samples obtained by performing one gradient descent step on the reverse KL divergence. Note that we deliberately ensure that the diffused intervals [ti, ti+1] do not overlap in Eqs. (5) and (6). The main reason for this is that trajectory samples at different timesteps have different distributions. Theoretically, we should use different fake scores for modeling. However, non-overlapping intervals allow us to use the same fake score for modeling. In this scenario, the timestep can naturally separate samples from different distributions. Moreover, when backpropagating through xt, we only consider one ODE step for saving GPU memory. Learning Fake Score sψ The fake score can be trained by denoising as follows: L(ψ) = K1 (cid:88) i= Epθ,ti (xti )Eq(xτ ˆxti )ωτ fψ(xτ , τ ) ˆxti 2 2, (7) where ˆxti denotes the clean sample corresponding to noisy samples xti . Since training the generator only requires computing the score of noisy samples diffused from xti, we introduce importance sampling to enhance the efficiency of learning fake score: L(ψ) = = K1 (cid:88) i=0 K1 (cid:88) i=0 Epθ,ti (xti )q(xτ ˆxti ) q(xτ xti ) q(xτ xti ) ωτ fψ(xτ , τ ) ˆxti 2 2 Epθ,ti (xti )q(xτ xti ) q(xτ ˆxti ) q(xτ xti ) ωτ fψ(xτ , τ ) ˆxti 2 2. The importance sampling strategy allows us to learn the score in the vicinity of xti with lower variance while maintaining an unbiased denoising score matching objective. Better Teacher Better Student The proposed objective is data-free, which is double-edged sword: the advantage lies in not needing to collect image data for training, while similar idea in sampling-steps aware training, however, their work is limited to point-to-point matching and its extension to text-to-image generation remains unexplored. 3.3. Surrogate Training Objective The Consistency Models (CMs) [35] minimize the following objectives for enforcing self-consistency: L(θ) = d(fθ(xtn , tn), sg(fθ(xtn1 , tn1))), (10) The fθ(xtn1, tn1) can be regarded as revised samples obtained by denoising from less noisy input. This reveals that both CMs and our proposed method minimize the distance between the generated samples and revised samples while the revised samples are obtained in different way. The distance metric is typically chosen as l2 [17, 35], however, iCT [33] has observed that Pseudo-Huber metric is more effective than l2 metric in [33]. Motivated by their success, and the similarity between CMs and ours in learning generator, we suggest using Pseudo-Huber Metric to form the surrogate training objective as follows: L(θ) = K1 (cid:88) ti+1 (cid:88) (cid:113) i= τ =ti xti sg(xti )2 2 + c2 c, (11) where is hyperparameter. Following iCT [33], we let = for data with dimensions. The corresponding 0.00054 updating gradient for training generator θ becomes: K1 (cid:88) ti+1 (cid:88) i=0 τ =ti [sψ(xτ , τ ) sϕ(xτ , τ )] (cid:112)sψ(xτ , τ ) sϕ(xτ , τ ) 2 + c2 xti θ . (12) The Huber metric performs normalization on the gradient, potentially leading to more stable training process. Remark We notice that recent DMD2 [42] and MMD [28] also develop few-step generators based on score distillation [19, 36, 41] or moment matching. Our work is more related to DMD2, since MMD applies moment matching for training both generator and fake score in distillation, which does not connect with certain distribution divergence, and is essentially different from score distillation related to distribution divergence. TDM build on score distillation is more flexible, since we can replace reverse KL by more expensive Fisher divergence [44] to achieve better performance (see Tab. 8 in the Appendix). Besides, DMD2 always aims to predict clean samples at each step which ignores the intermediate stage of diffusion ODE trajectory, while MMD uses DDPM sampler [7] from noisy real data for prediction in training which introduces large stochasticity. In contrast, our objective is aimed at performing deterministic trajectory distillation at the distribution level. Their designs increase the learning difficulty of both the generator and fake score, and are unnecessarily challenging and sub-optimal for few-step sampling. Moreover, their fake score are fully shared among different steps, this is flawed in theory and further increases the learning difficulty. As result, DMD2 has to train fake scores multiple times ( 5) at each iteration for better performance. Lastly, our work builds non-trivial connection between trajectory distillation and distribution matching, delivering new unified distillation paradigm. 4. Experiment Experiment Setting. The distillation is conducted on the JourneyDB dataset [23] solely with its prompts, since our method is image-free. Using the technique proposed in Sec. 3.2, we develop TDM-unify on SD-v1.5 that enables flexible sampling steps. We compared two variants of utilizing real data for TDM-unify: 1) fine-tuning SD-v1.5 before distillation, termed TDM-unify-SFT; 2) applying GANs during distillation, termed TDM-unify-GAN. We use LAION-AeS-6.5+ [30] as the compact high-quality dataset. More experiment details can be found in the Appendix D. Evaluation. We employ Aesthetic Score (AeS) [30] to evaluate image quality and adopt the Human Preference Score (HPS) v2.1 [37] to evaluate the image-text alignment and human preference. Additionally, we include CLIP score (CS) [5] to provide more comprehensive evaluation. We mainly compare our model against the open-source state-of-the-art (SOTA) models. 4.1. Main Results Quantitative Results. We evaluate our method against existing baselines, including fine-tuned models and prior distillation approaches, using the HPS benchmark. As demonstrated in Tab. 1, our approach consistently achieves superior performance across all architectural variants and evaluation metrics. While Hyper-SD shows competitive results, its performance relies on human feedback learning, which may artificially inflate machine metrics. In contrast, our method achieves better results without such techniques. Notably, our unified model provides SOTA performance on various sampling steps by deterministic sampling, highlighting the effectiveness of our sampling-steps aware objective. For comparison, PeRflow, the previous SOTA in supporting deterministic sampling, performs significantly worse in 1 or 2-step generation. Data Introduction Strategy. We Compare two approaches for incorporating high-quality data: (1) fine-tuning DMs before TDM distillation, and (2) using GANs during distillation. The TDM-unify-SFT clearly (31.31 HPS) outperforms TDM-unify-GAN in terms of 4-step generation, while being more stable and computationally efficient. We note that TDM-unify-GAN requires an extra 30% of the overall computational cost compared to TDM-unify-SFTs 3 A800 days (includes fine-tuning and distillation), or its 4-step performance degrades to 29.80 HPS. This indicates the effectiveness of our proposed strategy in utilizing real data. Integrating LoRA into Customized Models. We examine Table 1. Comparison of machine metrics on text-to-to-image generation across state-of-the-art methods. TDM-unify-SFT is initialized from fine-tuned SD-v1.5 and TDM-unify-GAN is initialized from original SD-v1.5. HFL denotes human feedback learning which might hack the machine metrics. We highlight the best among distillation methods. Model Backbone HFL Steps Animation Concept-Art Painting Photo Average HPS Aes CS Image-Free? Base Model (CFG = 3.5) Base Model + Fine-tuning (CFG = 3.5) InstaFlow [12] PeRFlow [39] PeRFlow [39] Hyper-SD [25] TDM-unify-GAN (Ours) TDM-unify-SFT (Ours) LCM-dreamshaper [18] PeRFlow [39] TCD [43] Hyper-SD [25] DMD2 [42] TDM-unify-GAN (Ours) TDM-unify-SFT (Ours) Base Model-1024 (CFG=7.5) TCD [43] LCM [18] SDXL-Turbo-512 [29] SDXL-Lighting [11] Hyper-SD [25] DMD2 [42] TDM (Ours) Base Model-1024 (CFG=3.5) YOSO-512 [20] LCM-1024 [17] TDM-1024 (Ours) SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1. SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SDXL SDXL SDXL SDXL SDXL SDXL SDXL SDXL PixArt-α PixArt-α PixArt-α PixArt-α No No No No No Yes No No No No No Yes No No No No No No No No Yes No No No No No No 25 25 1 1 2 1 1 1 4 4 4 4 4 4 4 25 4 4 4 4 4 4 4 25 4 4 4 26.29 31.10 23.17 12.37 19.75 28.65 29.80 29. 26.51 22.79 23.14 31.06 30.69 32.04 32.40 34.66 29.65 30.79 32.54 34.20 35.58 32.87 36.42 33.54 31.40 31.96 34.61 24.85 29.88 23.04 13.50 19.43 28.16 28.66 28.90 26.40 22.17 21.11 30.01 29.43 30.86 31.65 33.70 27.50 29.38 31.03 32.97 34.54 31.56 35. 32.35 31.18 30.60 33.54 24.87 29.53 22.73 13.64 19.41 28.41 28.82 29.22 25.96 21.28 21.08 30.47 29.75 31.06 31.35 33.43 27.98 29.60 31.04 33.15 34.54 31.01 35.51 32.00 31.26 30.70 33.45 26.01 28.94 22.97 11.53 18.40 26.90 26.80 27. 24.32 23.50 23.62 28.97 28.07 29.35 29.86 30.95 26.13 27.87 28.60 30.52 31.90 30.39 32.25 30.93 28.15 28.92 31.23 25.50 29.86 22.98 12.76 19.25 28.01 28.54 28.90 25.80 22.43 22.24 30.24 29.49 30.83 31.31 33.19 27.81 29.41 30.80 32.71 34.14 31.46 34. 32.21 30.60 30.55 33.21 5.49 5.85 5.27 4.47 4.91 5.64 5.97 6.02 5.94 5.35 5.43 5.78 5.91 6.07 6.08 6.17 5.88 5.84 5.81 6.23 6.18 5.88 6.28 6.23 6.23 6.17 6.42 33.03 33.68 30.04 15.49 25.83 30.87 31.89 32. 31.55 30.77 29.07 31.49 31.53 32.40 32.77 36.28 33.42 34.84 35.03 34.62 34.27 35.51 36.08 34.11 31.83 33.49 33.66 Table 2. Comparison of machine metrics on integrating LoRA into unseen customized models across state-of-the-art methods. HFL denotes human feedback learning which might hack the machine metrics. The FID is computed between teacher samples and student samples for measuring the style preservation. We highlight the best among distillation methods."
        },
        {
            "title": "Backbone HFL Steps",
            "content": "Animation Concept-Art"
        },
        {
            "title": "Photo Average",
            "content": "HPS Realistic LCM [18] PeRFlow [39] TCD [43] Hyper-SD [25] TDM (Ours) Dreamshaper LCM [18] PeRFlow [39] TCD [43] Hyper-SD [25] TDM (Ours) SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1.5 SD-v1."
        },
        {
            "title": "No\nNo\nNo\nNo\nYes\nNo",
            "content": "25 4 4 4 4 4 25 4 4 4 4 4 31.28 28.85 26.66 28.42 31.31 32.33 31.90 29.78 27.37 29.46 32.05 32.91 30.08 27.05 25.73 26.21 30.35 31.29 30.19 28.25 26.50 27.49 30.98 31. 29.73 28.08 25.83 26.85 30.90 31.49 30.26 29.11 26.66 28.26 31.37 32.18 29.02 26.91 24.54 26.33 28.86 29.78 29.28 27.23 25.16 26.42 28.87 29.95 30.03 27.72 25.69 26.95 30.36 31.22 30.41 28.59 26.42 27.91 30.82 31. Aes CS FID 5.88 5.79 5.59 5.82 5.97 6.04 6.02 5.98 5.74 6.01 6.13 6.22 34.41 30.95 31.93 31.28 32.19 32. 34.20 31.10 32.15 31.28 31.54 32.30 - 26.89 25.84 28.65 37.83 20.23 - 25.36 23.49 28.65 38.70 20.44 the efficacy of LoRA integration across unseen base models, evaluating both generation quality and style preservation. Style preservation is assessed by the FID [6] between the teacher samples and student samples. We train TDMLoRA for original SD-v1.5 in an image-free way. As shown in Tab. 2, our method significantly outperforms existing approaches. While Hyper-SD achieves competitive HPS and AeS scores, its high FID scores indicate poor style preservation relative to the original base models. Visual comparisons in Appendix further demonstrate our methods superior capability in maintaining both high-fidelity generation and faithful style preservation. Qualitative Comparison. We present the qualitative comparison in Fig. 5. It is clear that our method has better visual quality and text-image alignment compared to competing baselines and even the teacher diffusion with 25 steps. User Study. To further verify the effectiveness of our proposed method, we conduct an extensive user study across different backbones. The results in Fig. 6 shows that our method clearly outperforms the teacher diffusion and other most competing methods. Surpassing Teacher Diffusion Without Requiring Extra Data. We observe notable phenomenon: our proposed method outperforms both the original multi-step diffusion models and their fine-tuned variants across machine metrics and user study, without requiring extra data. The reason be7 Figure 5. Qualitative comparisons of TDM against most competing methods on SDXL. All images are generated by the same initial noise. Figure 6. The user study about the comparison between our method and the most competing methods. Table 3. Comparison on training cost across backbones and methods. TDM-unify-SFTs cost includes the fine-tuning stage. TDMunify is more costly as it requires training various sampling steps."
        },
        {
            "title": "Method",
            "content": "Backbone NFE HPS"
        },
        {
            "title": "Training Cost",
            "content": "DMD2 [42] TDM-unify-GAN TDM-unify-SFT SD-v1.5 SD-v1.5 SD-v1.5 LCM [18] DMD2 [42] TDM LCM [18] YOSO [20] TDM"
        },
        {
            "title": "SDXL\nSDXL\nSDXL",
            "content": "PixArt-α PixArt-α PixArt-α 4 4 4 4 4 4 4 4 4 31.53 32.40 32.77 29.41 31.46 34. 30.55 30.60 32.01 30+ A800 Days 4 A800 Days 3 A800 Days"
        },
        {
            "title": "14.5 A100 Days\n10 A800 Days\n2 A800 Hours",
            "content": "hind this phenomenon lies in our distillation target: the sequence of pre-trained diffused distributions pϕ,t(xt). While multi-step DMs generate samples by solving PF-ODE, they inevitably introduce numerical errors, making it challenging to sample perfectly from the distribution sequence. In contrast, our distillation objective does not require sampling from the teacher distribution, resulting in superior distilled performance despite using 10x fewer NFEs. Figure 7. The visualization of ODE trajectory with clean samples at different timesteps. It is clear that our method suffers less from the CFG artifact and has better visual quality. The prompt is dog reading book. See Appendix for more visualizations. TDM Learned ODE Trajectory at Distribution Level Well. We present the middle clean samples in the trajectory of Teacher DMs and our TDM in Fig. 7. It can be seen that TDM learns the trajectory well. Notably, we found that 8 Table 5. Comparison on HPS across variants based on SD-v1.5."
        },
        {
            "title": "Method",
            "content": "TDM-unify"
        },
        {
            "title": "1 Step",
            "content": "2steps"
        },
        {
            "title": "4 Steps",
            "content": "28.90 30.52 w/o Conditioned on Sampling Steps w/o Surrogate Training Objective w/o unify training (TDM-4step) 26.11 28.23 20.81 29.15 30.20 29.08 TDM-4Step w/o Importance Sampling / / / / 31. 29.39 30.85 31.35 31.35 29.27 Figure 8. 4 step generation from LCM and our method initialized by LCM. Our method can recover LCM from poor deterministic sampling via merely 100 training iterations. Table 4. Evaluation of text-to-video on Vbench."
        },
        {
            "title": "Method",
            "content": "Total Score Quality Score Semantic Score CogVideoX-2B TDM (4 NFE) 80.91 81.65 82.18 82.66 75.83 77. TDM suffers less from the CFG artifact and has better details at each step, finally forming better visual quality. Training Cost. We highlight that it is extremely efficient to train 4-step generator that surpasses the teacher model using our proposed TDM. This is due to: 1) We support deterministic sampling, which inherently possesses good fewstep capabilities; 2) Our distillation target is to simulate the teachers trajectory at the distribution level, each step only requires partial denoising and does not always need to predict clean image, which reduces the difficulty of learning. As shown in Tab. 3, our method substantially outperforms existing approaches while drastically reducing computational requirements. Notably, on SDXL and PixArt-α, our method requires only 1.25% and 0.57% of the training costs of DMD2 and YOSO, respectively, demonstrating exceptional training efficiency. Fixing LCMs from Poor Deterministic Sampling. While deterministic sampling is crucial for our methods efficiency and performance, popular models like LCM [17] are limited to stochastic sampling, potentially compromising their effectiveness. We investigated whether our approach could recover deterministic sampling capability in these models. Using LCM-dreamshaper as our baseline, we demonstrate that our method can successfully recover deterministic sampling functionality within just 100 training iterations, leading to superior 4-step performance  (Fig. 8)  . 4.2. Additional Results on Text-to-Video Acceration Our proposed TDM can be extended to text-to-video generation, benefiting from its quick convergence and data-free property. We distill our 4-step TDM from CogVideoX2B [40]. We evaluate our method on the widely used VBench [8]. Results shown in Tab. 4 show that TDM surpasses the teacher CogVideoX-2B by notable margin while using just 4 NFE. Figure 9. Comparison to DMD2 under LoRA fine-tuning. 4.3. Ablation Study We conduct comprehensive ablation studies on our proposed components, with results shown in Tab. 5. See Appendix for Setting details and additional ablation studies. Unify Training. Without the unify training, performance drops notably in fewer steps (20.81 vs. 28.90 HPS for 1step, 29.08 vs. 30.52 HPS for 2-steps). Importance Sampling. Adopting importance sampling for fake score learning significantly improves the 4-step generators performance (31.35 vs. 29.27 HPS). Without it, we observe degraded mode coverage and training stability. Sampling-Steps-Aware Training. Conditioning on sampling steps enables flexible step selection during inference. Without this conditioning, performance drops notably (26.11 vs. 28.90 for 1-step, 29.39 vs. 31.31 for 4-steps), likely due to distribution interference across different steps by sharing fake score as we analyzed in Sec. 3.2. Surrogate Training Objective. Our surrogate objective, inspired by Consistency Models, enhances the generators performance (31.31 vs. 30.85 HPS), validating the effectiveness of this design choice. Comparison to DMD2. It is important to distinguish our method from DMD2 [42]. While DMD2 employs score distillation [36] with direct image prediction at multiple timesteps, our approach focuses on distribution-level trajectory alignment which enables easier learning and faster convergence. To gain more intuitive insight into the superiority of our proposed TDM, we conducted experiments in distilling original SD v1.5 via LoRA. We present the FID between teacher samples and student samples at the training process in Fig. 9. We observed that the optimal FID of our method is 20% lower than that of DMD2, and it only requires 4% of the training time to reach the FID convergence of DMD2, which demonstrates the significant advantages of our TDM 9 in terms of training efficiency and performance. 5. Conclusion We propose TDM, new distillation paradigm that unifies trajectory distillation and distribution matching. Our method can surpass the teacher in data-free manner, achieving new SOTA performance with fewer steps. This is achieved with extremely low training costs. Specifically, we only need 2 A800 hours to distill 4-step generator with TDM that outperforms its teacher PixArt-α in both quantitative and human evaluations. This shows the efficiency and effectiveness of our method, paving the way for easier and more inclusive future research on diffusion distillation."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytican analytic estimate of the optimal reverse variarXiv preprint dpm: ance in diffusion probabilistic models. arXiv:2201.06503, 2022. 2 [2] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 2 [3] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter In The Abbeel. One step diffusion via shortcut models. Thirteenth International Conference on Learning Representations, 2025. 5, 2 [4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 4 [5] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. [6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 6 [8] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 9 and Taesung Park. Distilling Diffusion Models into Conditional GANs. In European Conference on Computer Vision (ECCV), 2024. 1 [10] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2024. 1 [11] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation, lightning: 2024. 2, 5, 7 [12] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality arXiv preprint diffusion-based text-to-image generation. arXiv:2309.06380, 2023. 7, 1 [13] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information Processing Systems, 2022. 4 [14] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 2 [15] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 2, [16] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 2, 3, 4 [17] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2, 6, 7, 9, 1 [18] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module, 2023. 7, 8 [19] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2023. 1, 2, 3, 4, 6 [20] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-toimage synthesis by self-cooperative diffusion gans, 2024. 7, 8, 2 [21] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1429714306, 2023. 3, [9] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, [22] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distilla10 [37] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2, 6 [38] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. ArXiv, abs/2311.09257, 2023. 1 [39] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. 2, 3, 5, 7, 1 [40] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, [41] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. 1, 2, 3, 4, 6 [42] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis, 2024. 2, 5, 6, 7, 8, 9, 1, 3 [43] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation, 2024. 7, 1 [44] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024. 2, 3, 6, 1 tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1 [23] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Journeydb: benchmark for generative image understanding, 2023. 6 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [25] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis, 2024. 2, 7, 1 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 5 [27] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 1, 2, 3, 4, 5 [28] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching, 2024. 6, 1, 2 [29] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023. 7, 1 [30] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [32] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 2, 3 [33] Yang Song and Prafulla Dhariwal. Improved techniques for In The Twelfth International training consistency models. Conference on Learning Representations, 2024. 2, 3, 6, 1 [34] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 3 [35] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 1, 2, 3, 6 [36] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 4, 6, 9, 1 Learning Few-Step Diffusion Models by Trajectory Distribution Matching"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Training Algorithm C. Additional Related Works We present the algorithm for distilling K-step TDM in Algorithm 1. Algorithm 1 Trajectory Distribution Matching. Require: learning rate η, desired sampling steps K, total iterations , real score fϕ. Ensure: optimized models fθ, fψ. 1: Initialize weights {θ, ψ} by ϕ; 2: for 1 to do 3: 4: Sample noise ϵ from standard normal distribution; Sample {xti}K1 i=0 with initialized noise ϵ from gen5: 6: 7: 8: 9: 10: 11: erator fθ by steps via ODESolver. Sample xtm from {xti }K1 i=0 . Sample Timesteps from tm to tm+1. Obtain noisy samples xt q(xtxtm ) # update fake score Compute Loss Lψ following Line 234. ψ ψ ηψLψ; # update Generator Compute Loss Lθ following Eq. (11). θ θ ηθLθ; 12: 13: 14: end for B. Derivations Derivations of Eq. (9). Given the case without noise, the learning objective of the fake score sψ by score matching is: L(ψ) =EpK1 (x) log pK1(x) sψ(x)2 2 + EpK2 (x) log pK2 (x) sψ(x)2 2. (13) Its gradient can be computed as follows: (cid:90) ψL(ψ) = [2pK1(x)( log pK1 (x) sψ(x)) +2pK2(x)( log pK2 (x) sψ(x))] (14) sψ(x) ψ dx The global minimum is achieved when ψL(ψ) = 0. It is clear that when sψ(x) = pK1 (x)x log pK1 (x) + pK2 (x)x log pK2 (x) pK1 (x) + pK2 (x) , (15) we have ψL(ψ) = 0. Hence the optimal fake score is given in Eq. (15). Recently, there mainly have been two lines in diffusion distillation: Trajectory Distillation and Distribution Matching. Trajectory distillation tries to distill few-step student model by simulating the generative process of DMs. These methods typically predict the multi-step solution of PF-ODE solver by one step [12, 21, 27, 39]. Consistency family [10, 17, 33, 35, 43] enforces self-consistency. These methods suffer from numerical errors when solving pretrained PF-ODE. Distribution Matching tries to distill student models via match at the distribution level. DiffusionGAN hybrid models [9, 29, 38] have been proposed for this aim, however, stabilizing GAN training requires real data, careful architecture design, and auxiliary regression losses. Another promising way for distribution matching is through score distillation [19, 22, 36, 41, 44]. These methods typically ignore the intermediate steps of the trajectory. Our method explores trajectory distillation at the distribution level, enjoying the best of two worlds. Although Hyper-SD [25] explores combining trajectory distillation and distribution matching, their works treat these two techniques as distinguished parts, requiring multiple training objectives and multiple training stages. In contrast, our proposed objective naturally unifies trajectory distillation and distribution matching, providing highly efficient and effective distillation method. Besides, motivated by the similarity between consistency models and our proposed method in learning generator, we propose surrogate training objective, introducing the Huber metric into training. This leads to better performance and potentially encourages the community to explore other types of distance metrics in distilling via distribution matching. Recently, DMD2 [42] also explored distilling few-step generator via distribution matching. However, our method is fundamentally different from their work. Their work tries to predict clean images at different timesteps, ignoring alignment with the teachers trajectory. This leads to harder learning and slower convergence. In particular, in distilling 4-step SDXL, we only require 1.25% of the training cost for DMD2, while achieving significantly better performance. Besides, recent work MMD [28] also developed multi-step generator based on similar style with score distillation. However, our work is essentially different from them, since MMD applies moment matching for diffusion distillation, while we propose new distillation paradigm that unifies trajectory distillation and distribution matching. Specifically, MMD employs moment matching to train both generator and fake score which is fundamentally different from score matching as 1 noted in their paper [28]; In contrast, we use reverse KL to train the generator and score matching to train the fake score. Moreover, MMD uses ancestral sampling (DDPM sampler [7]) from noisy real data in training, which introduces large stochasticity in intermediate samples, is suboptimal for few-step sampling. In contrast, we use deterministic sampling from noise which is image-free, more effective for fewer-step sampling, and builds non-trivial connection with trajectory distillation. Additionally, concurrent work [3] explored flexible deterministic sampling but was limited by point-to-point selfconsistency and required training the generator at arbitrary timesteps, challenging model capacity. Its extension to text-to-image generation also remains unclear. In contrast, our method trains the generator at only timesteps with distribution-level matching, enabling more effective flexible deterministic sampling. We further achieve state-of-the-art performance in text-to-image generation. D. Experiment details We use the AdamW optimizer for both the generator and fake score. By default, the β1 is set to be 0, the β2 is set to be 0.999. SD-v1.5 We adopt constant learning rate of 2e-6 for the generator and 2e-5 for the fake score. We apply gradient norm clipping with value of 1.0 for both the generator and fake score. We use batch size 256. We set the CFG as 3.5. Generally, the training is done within 20k iterations for unified training and within 3k iterations for specific 4-step training. SDXL We adopt constant learning rate of 1e-6 for the generator and 5e-6 for the fake score. We apply gradient norm clipping with value of 1.0 for both the generator and fake score. We use batch size 64. We set the CFG as 8. Since SDXL has 2.7B parameters, fine-tuning it at 1024 resolution is computationally expensive. We first fine-tune for 1k iterations at 512 resolution, then fine-tune for another 1k iterations at 1024 resolution. The fake score is initialized from the pre-trained SDXL in both stages. PixArt-α We adopt constant learning rate of 2e-6 for the generator and 2e-5 for the fake score. We set the CFG as 3.5. We apply gradient norm clipping with value of 1.0 for both the generator and fake score. We use batch size 32. The training can be done within 500 iterations. E. Additional Experiments Figure 10. Visual samples of varying the condition steps and sampling steps. The prompt is corgi with sunglasses, traveling in the sea that TDM-unify exhibits systematic behavioral variations across different combinations of conditional and sampling steps. Notably, the model demonstrates an intrinsic understanding of the underlying ODE trajectory, adaptively positioning itself at appropriate points along this path given the specified condition steps. F. Ablation Studies F.1. Ablation Details We use the same hyperparameters and training iterations for all variants, with differences only in the ablating components. The formulation of using GANs during distillation Following the previous work [11, 20], we use latent discriminators, with the backbone based on the UNet encoder from SD-v1.5. In particular, we perform the following loss for learning generator: K1 (cid:88) tK i+1 (cid:88) EK i=0 τ =tK {λτ KL(pθ,τ tK (xτ K)pϕ,τ (xτ ))+ (16) λτ Ep θ,τ tK (xτ K)ADVLoss(xτ , τ, K)}. The Effect of Varying Sampling Steps. We investigate the impact of different sampling and conditioning configurations, with results visualized in Fig. 10. The results show The ADVLoss is the adversarial loss. Note that for fair comparison, we inject the desired sampling steps into GANs discriminator too. 2 Table 6. Comparison on HPS across variants in 4-step generation based on SD-v1.5. Train-DDIM Train-DPMSolver Test-DDIM Test-DPMSolver HPS 31.04 31.35 30.86 31.30 Table 7. Comparison on HPS across variants in 4-step generation based on SD-v1.5."
        },
        {
            "title": "Method",
            "content": "TDM (Matching noisy samples xti) Matching clean samples ˆxti HPS 31.35 24.63 Table 8. The effect of using more expensive Fisher Divergence in 4-step generation."
        },
        {
            "title": "Method",
            "content": "TDM TDM w/ Fisher HPS 31.35 31.70 Details in implementing original loss We use normalization proposed in DMD [41], while we do not use the normalization in our proposed surrogate loss. Details in implementing DMD2 Following the original DMD2 paper [42], we update the fake score 10 times per iteration. Other hyper-parameters remain consistent with our configuration. F.2. Additional Ablation To gain more comprehensive understanding of our proposed methods, We conducted additional ablation studies in this section. Comparison on Mode Coverage. We found that using importance sampling for learning the fake score led to improved performance (Tab. 5) and better mode coverage. As shown in Fig. 11, our method demonstrates notably superior image quality and mode coverage. This improvement may be attributed to the fact that the fake score cannot accurately track the student distribution without using importance sampling. We additionally compare to the concurrent work DMD2 [42]. We found that DMD2s generated results also suffer from mode collapse, while its impact is somewhat less severe compared to the variant without importance sampling. This may be due to DMD2 trains the fake score multiple times at each iteration, resulting in more accurate fake score at the cost of slow training. (a) DMD2 (b) Ours w/o Importance Sampling (c) Ours Figure 11. Comparison on Mode Cover in 4-step generation based on SD-v1.5. It is clear that our method has better mode cover and image quality. The prompt is cute dinosaur, cartoon style (a) matching clean samples (b) Ours (matching noisy samples) Figure 12. Comparison on the compatibility with deterministic samplers in the 4-step generation on SD-v1.5. It is clear that our method (matching noisy samples) has better visual quality. Effect of Different ODE Solvers. In the experiments presented in the main body, we use DDIM [32] as the ODE solver during training and DPMsolver [15] during inference. Here we ablate the choice of ODE solver, with results shown in Tab. 6. We find that using DDIM versus DPM during training yields similar performance, which may be attributed to two reasons: 1) regardless of whether DDIM or DPM is used during training, we can utilize DPM sampling at test time to improve performance; 2) our training only backpropagates through one ODE step, preventing higherorder ODE solvers like DPMSolver from benefiting from higher-order information in the training. 3 Figure 13. An example of the evaluation question for our user study. Matching noisy samples xti v.s. clean samples ˆxti core design of our method is to align noisy samples xti predicted by the model with the target diffusion, rather than the clean samples ˆxti predicted by the model. This design makes the support of deterministic sampling possible. We conduct experiments on matching clean samples, the results are shown in Tab. 7 and Fig. 12. It is clear that our method has better performance, while matching clean samples deliver poor deterministic sampling with notable artifacts. Flexibility for using different distribution divergence The proposed TDM has the flexibility for using different distribution divergence instead of reverse KL divergence. In particular, the performance of TDM can be further improved by more expensive Fisher divergence (Tab. 8). G. User Study Details We conducted user research by presenting users with two anonymous images generated by different models and asking them to select the sample with higher image quality and better prompt alignment. We randomly selected 20 prompts for image generation. Each image was manually verified to ensure the absence of inappropriate or dangerous content. An example of an evaluation question is shown in Fig. 13. In total, we collected approximately 40 user responses. H. Additional Qualitative Results We present the additional visualization of ODE trajectory with clean samples at different timesteps in Fig. 16. We present the visual samples of interesting LoRA into unseen customized models in Figs. 14 and 15. It can be seen that compared to the competing baseline, our method shows better visual quality and better style preservation. 4 Figure 14. Additional Samples of integrating LoRA into unseen customized models - dreamshaper. 5 Figure 15. Additional Samples of integrating LoRA into unseen customized models - realisticvision. Figure 16. Additional visualization of ODE trajectory with clean samples at different timesteps. It is clear that our method suffers less from the CFG artifact and has better visual quality."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab"
    ]
}