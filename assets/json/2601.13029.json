{
    "paper_title": "Think3D: Thinking with Space for Spatial Reasoning",
    "authors": [
        "Zaibin Zhang",
        "Yuhan Wu",
        "Lianjie Jia",
        "Yifan Wang",
        "Zhongbo Zhang",
        "Yijiang Li",
        "Binghao Ran",
        "Fuxi Zhang",
        "Zhuohan Sun",
        "Zhenfei Yin",
        "Lijun Wang",
        "Huchuan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 9 2 0 3 1 . 1 0 6 2 : r Think3D: Thinking with Space for Spatial Reasoning Zaibin Zhang1, Yuhan Wu1, Lianjie Jia1, Yifan Wang1, Zhongbo Zhang1, Yijiang Li2, Binghao Ran1, Fuxi Zhang1, Zhuohan Sun1, Zhenfei Yin3, Lijun Wang1, Huchuan Lu1 1 Dalian University of Technology, 2 University of California San Diego, 3 University of Oxford, dlutzzb@gmail.com, {tracy1252684562,jialianjie}@mail.dlut.edu.cn, ljwang@dlut.edu.cn Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Understanding and reasoning about the physical world requires spatial intelligencethe ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/globalview switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing new dimension of multimodal intelligence. Code and weights are released at https://github.com/ zhangzaibin/spagent. 1. Introduction Understanding and interacting with the physical world has long been fundamental objective of vision language models (VLMs) [2, 11, 18]. Achieving this objective necessiFigure 1. Comparison between prior think with image [74] and our proposed think with space. While the former reasons over 2D content by manipulating images, our method operates directly within 3D point cloud space for spatial understanding. tates spatial intelligencethe ability to reason about geometry, viewpoint, and spatial relationships [15, 62, 67]. Despite remarkable progress in visual understanding, current VLMs remain powerful yet fundamentally 2D analyzers. Their performance drops sharply on tasks requiring spatial reasoningsuch as multi-view understanding and route planning. For instance, although recent models achieve near human-level performance on comprehensive benchmarks like MMMU [68], they still lag far behind humans in tasks that demand genuine 3D reasoning [62, 66]. Two main research directions have emerged to bridge this gap. The first seeks to internalize spatial knowledge by training on large-scale and spatially diverse datasets [5, 14, 19, 45, 76], which requires enormous computation and may sacrifice general reasoning ability. The second, known as think with image [39, 40, 71, 74, 77], enables models to call external tools (e.g., zoom, crop, depth estimation) to enhance perception. However, these 2.5D operations can only capture shallow spatial cuessuch as relative depth and object orderingand fail to support deeper reasoning across multiple views or 3D geometry [30, 58]. By comparison, humans intuitively build consistent 3D representations of their surroundings and leverage them for comprehensive Inspired by this cognitive process, we spatial reasoning. ask: Can VLMs think with 3D space as humans do? Recent advances in 3D reconstruction [20, 49, 53] make this possible. These models can estimate camera poses and reconstruct 3D point clouds from videos or multi-view images, providing geometric foundation for explicit spatial reasoning. Building on this foundation, we propose Think3D-a framework that enables VLMs to actively interact with reconstructed 3D point clouds and reason in spatial manner through thinking with 3D space. Understanding 3D space is difficult for VLMs as effective spatial reasoning requires consistent reference. When model manipulates point cloud, it needs an anchor to interpret rotations and directions consistently. Without such an anchor, spatial manipulations become ambiguous, and the model cannot determine how to move within 3D space in coherent manner. In Think3D, we use the estimated camera poses as anchors, providing stable and intuitive reference for spatial operations. With this design, the model can autonomously decide how to manipulate the 3D sceneselecting camera, choosing rotation, or determining where to explore next. During point cloud manipulation, it can also switch between global view, which captures the overall scene structure, and local view, which focuses on fine-grained object details. This flexibility allows the model to reason over both coarse and fine spatial cues. Crucially, the process is not one-shot but inherently iterative: the model repeatedly interacts with the reconstructed 3D scene, actively observes new views, and refines its understanding step by step. Through this iterative reasoning process, Think3D develops coherent spatial representation, mirroring the way humans explore in 3D space. Interestingly, we observe that the effectiveness of the above spatial exploration is strongly correlated with the intrinsic reasoning ability of VLMs. While large models such as GPT-4.1 and Gemini-2.5-Pro naturally generate diverse and semantically meaningful viewpoints, smaller models tend to drift toward redundant or even misleading camera poses, ultimately constraining their spatial understanding. To close this gap, we introduce reinforcement learning approach, Think3D-RL, that enables smaller models to autonomously discover effective exploration policies. Crucially, Think3D-RL relies solely on final task rewards, without any supervision over how the model should navigate or manipulate the 3D scene. During training, the model performs multi-round spatial exploration, and the reward reinforces trajectories that yield stronger downstream performance. Through this reward-driven learning process, the model progressively learns when and how to interact with the 3D environment, converging toward significantly more informative viewpoint manipulation strategies. As result, smaller models begin to exhibit increasingly consistent exploration behaviors that more closely resemble those of large VLMs, ultimately leading to substantial improvements across diverse spatial reasoning benchmarks. We evaluate Think3D on three challenging benchmarks (BLINK Multi-view [16], MindCube [66], and VSIBench [62]) and observe consistent improvements across all tasks: on BLINK Multi-view and MindCube, Think3D yields an average +7.8% gain when applied to GPT-4.1 and Gemini-2.5-Pro, and achieves an additional +4.7% improvement on VSI-Bench. Moreover, for smaller VLMs trained with our RL framework, the benefit of spatial exploration increases substantiallythe performance gain from tool usage rises from only +0.7% before RL to +6.8% after RLdemonstrating that learned exploration strategies significantly strengthen the models ability to extract informative 3D viewpoints and compensate for limited model capacity. Our main contributions can be summarized as follows: 1. new perspective on spatial reasoning. We introduce the concept of Think with Space, which redefines spatial reasoning as an active 3D exploration process, in contrast to conventional passive 2D perception. 2. framework for explicit 3D interaction. We design Think3D, allowing the VLM-based agent to manipulate point clouds through camera-based reference actions and iterative spatial reasoning chains. 3. Reinforcement learning for spatial exploration. We formulate the models acquisition of viewpoint and action selection as an RL process, enabling it to develop efficient 3D exploration strategies that enhance reasoning performance across spatial benchmarks. 2. Related Work 2.1. VLMs for Spatial Reasoning Recent advances in Vision Language Models (VLMs) have substantially improved spatial reasoning key capability for understanding and interacting with the physical worlddriven by increasingly capable models [9, 22, 25, 36, 47, 64] and by comprehensive benchmarks [4, 9, 10, 29, 58, 62]. Methods such as VLM-3R [14], SpatialRGPT [9], and SpatialVLM [5] incorporate 3D reconstruction, depth cues [33], and large-scale 3D spatial VQA data [3, 69] to enhance quantitative spatial reasoning. Recent works further strengthen the coupling between perception and reasoning via spatial prompting [22, 25, 30, 42, 69], mental simulation [8, 22], visual chain-of-thought or RL-based reasoning [13, 36, 52, 54], and explicit visual grounding [59]. In robotics, systems such as RoboBrain [19, 44], Gemini Robotics [1, 45], and RoboRefer [75] extend these capabilities to embodied interaction and precise 3D spatial grounding, and are evaluated on standardized spatial benchmarks such as VSI-Bench [62] and MindCube [10, 58, 66]. 2.2. VLM tool calling The efficacy of VLMs is further enhanced by tool calling, where the model leverages external tools via prompting or code generation, as in HuggingGPT [38] and related systems [41, 56, 64, 71]. For long-horizon or highcomplexity problems, agent-based systems have been applied to long-video understanding [6, 42, 65, 70], highresolution image analysis [21, 63, 78], and medical diagnosis [26, 28]. OpenThinkImage [39] provides unified platform for tool-augmented vision-language models, while others [17, 24, 27, 43, 48, 61, 78] train VLMs to use specific toolsets through fine-tuning. Reinforcement learning (RL) has become central paradigm for tool-use and reasoning policies [7, 12, 39, 60, 73, 77] learning. In particular, DeepEyes [74] promotes thinking with images, enabling models to leverage internal visual reasoning capabilities without external tools and directly inspiring our design. 2.3. 3D Reconstruction In the parallel field of computer vision, 3D reconstruction from 2D images has seen significant breakthroughs, largely driven by transformer-based architectures [34]. DUSt3R [51] introduces novel paradigm for multi-view 3D reconstruction that does not require predefined camera poses. Building on this, MASt3R [23] enhances the process by regressing dense local feature maps to produce metricscale reconstructions. VGGT [49], feed-forward neural network, is capable of directly inferring comprehensive set of 3D scene attributesincluding camera parameters, depth maps, and point tracksfrom multiple views in single forward pass. Methods like CUT3R [50], MapAnything [20], and Pi3 [53] further support continual reconstruction, multi-task metric 3D geometry, and permutationequivariant visual geometry, providing versatile backbones for our 3D spatial reasoning framework. 3. Think3D for Spatial Reasoning 3.1. Framework Overview As illustrated in Figure 2, Think3D equips VLM with the ability to explore and reason directly in 3D via multi-turn observe manipulate reflect loop. Given multi-view images (or short video) {It}T t=1 and query q, the VLM autonomously decide whether to invoke the 3D reconstruction tool to obtain 3D point cloud and camera poses. During the subsequent 3D interaction process, the VLM is able to iteratively manipulate the point cloud and observe the 3D environment from novel views. By progressively accumulating complementary geometric observations, the VLMs form an explicit 3D chain of thought, facilitating structured spatial exploration that cannot be achieved using static 2D inputs alone. The above 3D interaction process is powered by the following three key components of Think3D. We present the details in the subsequent sections. 3D Manipulation Toolkit integrates suite of callable 3D tools, providing the agent with flexible and expressive control for exploring the 3D environment. Spatial Reasoning Agent performs 3D interactions by calling 3D manipulation tools and reasoning over the geometric observations. Think3D-RL Reinforcement Learning Module optimizes multi-step 3D exploration policy through tool calling, trained with Group Relative Policy Optimization (GRPO) [37]. 3.2. 3D Manipulation Toolkit Under the Think3D framework, suite of callable 3D tools enables flexible agentic 3D manipulation and exploration, featuring three core functionalities: 3D reconstruction, 3D transformation, and novel-view rendering. 3D Reconstruction: Given multi-view images {It}T t=1, 3D point cloud and the corresponding camera poses can be estimated using Pi3 [53]. Each camera is represented as Ct = (Kt, Rt, tt), (1) where Kt R33 denotes the intrinsic matrix, Rt SO(3) denotes the rotation matrix, and tt R3 represents the camera center in world coordinates. Here, indexes the input views. Depth and confidence predictions are fused across views to obtain cleaned colored point cloud: = {(xn, cn)}N n=1, (2) where xn is the 3D location and cn is the RGB color. 3D Transformation: To enable flexible 3D exploration, the agent is able to manipulate the reconstructed 3D point cloud to select optimal viewpoints. At each step, it predicts: (i) discrete camera index {1, . . . , }, (ii) pair of rotation angles (α, β) specifying horizontal (azimuth) and vertical (elevation) rotations, and (iii) binary transformation mode {global, ego} indicating whether to use global overview or an ego-centric view. Given the selected input camera Ci = (Ki, Ri, ti) as spatial anchor, Figure 2. The Think3D pipeline. The VLM interacts with the 3D scene through iterative calls to the 3D Manipulation Toolkit, issuing viewpoint-manipulation actions that control camera pose and rendering parameters. Each rendered image is appended to the agents memory and informs the next reasoning step, forming repeated cycle of observe manipulate reflect. we construct virtual camera defined as parameters are formed as: Cnew = (Ki, R(α, β) Ri, ti), (3) ak = (nk, mk, αk, βk), (6) which maintains the camera center fixed at ti while updating its orientation according to the agent-predicted 3D rotation R(α, β), which is induced by the specified azimuth and elevation offsets. When = I, virtual camera Cnew coincides with the original viewpoint of Ci. In the global (gods-eye) mode, Novel View Rendering: all 3D points in are projected with Cnew to generate an overview of the entire 3D scene. In the ego-centric mode, the point set is further restricted to wide field-of-view cone aligned with the forward direction of Ci to projection, thereby emulating first-person perspective. lightweight, point-based renderer then produces the synthesized image ˆI as follows: ˆI = Render(cid:0)X , Cnew, m(cid:1). (4) 3.3. VLM-based Spatial Reasoning Agent As shown in Figure 2 (a), the VLM-based agent equipped with the above Manipulation tools can iteratively explore the 3D environments and build 3D-aware CoT for better spatial reasoning. In the k-th iteration, given the history Hk1, the VLM acts as multimodal policy: ok = πθ (cid:0)q, {It}, Hk1 (cid:1), (5) where and {It} denote the input query and original multiview images, respectively. The output ok is reasoning or conclusive responses and may optionally issues 3D manipulation tool calling with corresponding parameters (As shown in Figure 2 (b)). The 3D Reconstruction module is mostly called at the beginning of exploration with binary code rk {0, 1}. When rk = 1, the Pi3 [53] model will be invoked to reconstruct the 3D point cloud and estimate camera poses from the multi-view inputs. For 3D Transformation and Novel View Rendering Modules, the tool calling nk {1, . . . , } is the index of the selected anchor camera Cnk ; mk {global, ego} specifies the view mode (global overview vs. ego-centric); αk, βk denote the azimuth and elevation angles, respectively. Given the predicted parameter ak, the 3D manipulation toolkit (Sec. 3.2) will instantiate corresponding virtual camera following Eq. (3): C(k) new = (cid:0)Knk , R(αk, βk) Rnk , tnk (cid:1), (7) which keeps the camera center fixed at tnk and updates its orientation based on the predicted angles. The view rendering module will be further invoked to synthesize novel view ˆIk of the 3D environments from the virtual camera pose (k) new according to Eq. (4). The synthesized view will be incorporated into the cumulative observation history as Hk = Hk1 {(ˆIk, ak)}. (8) Thus, Think3D implements an iterative observe manipulate reflect loop in which the VLM maintains an explicit 3D-aware CoT over the rendered spatial views. The detailed prompts are provided in the supplementary material. 3.4. Think3D-RL for Multi-Step Exploration While the reasoning loop allows the model to explore 3D space, its effectiveness depends on learning which viewpoints provide informative observations and when such exploration should be conducted. We therefore optimize the exploration policy using reinforcement learning. Trajectory Formulation & Training-time Sampling. We represent an agentic reasoning episode as the following trajectory: τ = {(s1, o1), (s2, o2) . . . , ((sK, oK)), ˆy}, (9) Table 1. Results on BLINK (Multi-view) and MindCube Subset (%). Think3D denotes our spatial reasoning framework with maximum three exploration iterations. Qwen3-VL-4BRL refers to the model trained with our Think3D-RL, and Qwen3-VL-4BGRPO denotes the variant trained using the standard GRPO. All baselines and their corresponding variants are evaluated over three runs. Model BLINK (MV) MC (Rotation) MC (Among) MC (Around) Avg Proprietary models GLM-4.5V [46] Doubao-1.5 [35] Specialized Spatial Models RoBoBrain [19] Spatial-MLLM [57] VLM-3R [14] REVPT [77] GPT-4.1 [31] Think3D (GPT-4.1) Gemini-2.5-Pro [11] Think3D (Gemini-2.5-Pro) Qwen3-VL-4B [32] Think3D (Qwen3-VL-4B) Qwen3-VL-4BGRPO Qwen3-VL-4BRL Think3D (Qwen3-VL-4BRL) 39.85 50.93 55.64 56.06 41.35 51.89 45.00 72.50 32.50 32.50 25.00 30.00 45.00 40.00 57.50 47.50 47.50 55. 22.50 35.00 52.50 35.00 37.50 50.50 38.09 49.61 49.54 42.77 37.84 47.35 36.82 63.91 (+27.09) 44.86 52.88 (+8.02) 47.87 48.62 (+0.75) 52.38 45.11 53.64 (+8.53) 60.00 63.33 (+3.33) 85.00 86.67 (+1.67) 34.17 35.83 (+1.66) 35.00 34.17 39.17 (+5.00) 46.67 57.50 (+5.00) 49.17 54.17 (+5.00) 20.00 28.33 (+8.33) 21.67 28.33 35.00 (+6.67) 55.00 60.83 (+14.16) 58.33 60.83 (+2.50) 49.62 61.19 (+11.57) 59.34 63.34 (+4.00) 41.67 33.33 (-8.34) 28.33 32.50 39.17 (+6.67) 35.92 36.53 (+0.61) 34.34 35.03 41.74 (+6.71) where sk = (q, It, Hk1) represents an input to the VLM agent at the k-th iteration; ˆy denotes the final answer generated by the agent; and denotes the total number of exploration steps determined by the agent. To improve training efficiency, we discretize the space of camera poses into set of canonical viewpoints and use only their rendered images during optimization. The policy still learns both when to explore and which canonical view to select. These viewpoints include top, left, and right views; additional details are provided in the supplemental material. Continuous control over camera parameters remains available at inference. Trajectory-level reward. Rewards are assigned only at the end of each trajectory: R(τ ) = Rans(ˆy) + Rfmt(ˆy), (10) where Rans evaluates answer correctness and Rfmt applies small formatting bonus. This trajectory-level reward jointly reinforces all preceding viewpoint decisions, thereby promoting more efficient multi-step spatial exploration. Optimization. We train the policy using Group Relative Policy Optimization (GRPO), which provides stable, groupnormalized advantages for multi-turn reasoning. Following standard practice, we apply token-wise mask to exclude observation tokens (rendered images encoded as text) from gradient updates, so that only tokens corresponding to model-generated actions and answers are optimized. 4. Experiment 4.1. Experiment Setup Setting and Dataset Our reinforcement learning (RL) training framework is based on SWIFT [72]. We fine-tune the VLM using the GRPO training strategy with 8 rollouts per step to estimate advantages. The model is trained for one epoch on 8 H200 GPUs with batch size of 8 and gradient accumulation of 4, using cosine learning rate schedule with 5% warmup and base learning rate of 1 106. The maximum completion length is set to 1024 tokens. During training, the language model is fully fine-tuned while the vision encoder is frozen. The training set contains 977 samples randomly selected from the MindCube dataset, with no overlap with the test set. During inference, we deploy Pi3 tool on RTX 3090 GPU to perform inference. Benchmarks We evaluate our method on 3 challenging spatial reasoning benchmarks: BLINK (Multi-view) [16], MindCube [66], and the video-based VSI-Bench [62]. BLINK (Multi-view) uses all the multi-view data from the BLINK dataset and focuses on multi-view geometric understanding, particularly assessing models ability to infer relative camera motion across views. MindCube contains 3 canonical camera-motion typesrotation, around, and among. We sample 40 questions from each category, resulting in 120 questions in total for evaluation. VSI-Bench assesses visualspatial intelligence in dynamic egocentric videos across four tasks: route planning, object relative direction prediction, appearance order reasoning, and relative Table 2. Results on VSI-Bench-tiny (%). Think3D denotes our spatial reasoning framework with maximum of two exploration iterations when using proprietary baselines and three when using Qwen-VL-4B. Qwen3-VL-4BRL refers to the model trained with our Think3D-RL, and Qwen3-VL-4BGRPO denotes the variant trained using the standard GRPO. All baselines and their corresponding variants are evaluated over three runs. Model Route Plan Rel. Dir. Rel. Dist. App. Order Avg Proprietary models GLM-4.5V [46] Doubao-1.5 [35] Specialized Spatial Models RoBoBrain [19] Spatial-MLLM [57] VLM-3R [14] REVPT [77] GPT-4.1 [31] Think3D (GPT-4.1) Gemini-2.5-Pro [11] Think3D (Gemini-2.5-Pro) Qwen3-VL-4B [32] Think3D (Qwen3-VL-4B) Qwen3-VL-4BGRPO Qwen3-VL-4BRL Think3D (Qwen3-VL-4BRL) 34.69 42.86 28.57 38.30 46.94 28.57 41.03 18.00 36.00 44.00 64.27 40.00 40.00 40.00 16.00 40.00 38.00 40. 79.16 71.40 12.24 65.31 55.10 51.02 48.72 43.07 23.20 46.94 51.08 39.90 40.80 45.26 (+5.18) 45.58 46.93 (+1.35) 34.69 30.61 (-4.08) 28.57 26.53 39.46 (+12.93) 40.63 45.30 (+4.67) 28.67 37.30 (+8.63) 40.67 44.00 (+3.33) 38.00 44.00 44.00 (+0.00) 43.30 46.00 (+2.70) 50.67 54.00 (+3.33) 35.33 29.33 (-6.00) 36.00 40.00 42.67 (+2.67) 68.00 68.00 (+0.00) 55.73 68.24 (+12.51) 42.44 52.38 (+9.94) 30.61 46.94 59.86 (+12.92) 48.18 51.14 (+2.96) 45.16 51.61 (+6.45) 38.28 39.08 (+0.80) 33.30 39.37 46.33 (+6.96) distance. We adopt the VSI-Bench-tiny split and sample 7 frames from each video for evaluation. All models are evaluated on the same sample sets for fair comparison. Baseline Models For leading closed-source state-of-theart models, we evaluate GLM-4.5V [46], Doubao-1.5 [35], GPT-4.1 [31], and Gemini-2.5-Pro [11]. In addition, comparisons are made against specialized models fine-tuned on spatial reasoning datasets, including RoboBrain [19], Spatial-MLLM [57], VLM-3R [14], as well as REVPT [77], tool-augmented fine-tuning method. 4.2. Main Results The results on the multi-view reasoning benchmark show that Think3D substantially improves the performance of proprietary models such as GPT-4.1 and Gemini-2.5-Pro, yielding 11.57% and 4.00% relative gains, respectively, without any additional training. In contrast, when applied to smaller models such as Qwen3-VL-4B, the improvement is marginal (0.61%), suggesting that limited spatial reasoning capacity restricts the benefits of exploration. However, once Qwen3-VL-4B is fine-tuned using Think3D-RL (Qwen3-VL-4BRL), the model exhibits improvement of 6.71% with Think3D. This provides strong evidence that RL effectively strengthens viewpoint selection and spatial exploration. We further analyze how RLtrained models achieve these gains in Section 5.4. On VSI-Bench, the results further support the effectiveness of Think3D, yielding 2.96% improvement on GPT-4.1 and 6.45% improvement on Gemini-2.5-Pro. These gains indicate that Think3D also enhances performance on videobased spatial reasoning tasks. Moreover, our RL-fine-tuned model achieves larger improvements when equipped with Think3Drising from 0.8% to 6.96%highlighting that RL training enables the model to exploit 3D spatial exploration more effectively. We also provide qualitative example of the Think3D reasoning process in Figure 3. 5. Ablation Study 5.1. Ablation of Components As shown in Table 3, we conduct an ablation study on the key components of Think3D. Compared to the GPT-4.1 baseline (first raw) that never calls the 3D tool, we find that directly using the 3D reconstruction space without an appropriate anchor camera pose to guide point cloud manipulation leads to mild performance drop. This indicates that raw 3D input alone is insufficient, as the model must actively explore multiple viewpoints to arrive at the correct answer. Adding anchor camera selection and ego-view configuration greatly improves performance. These components enable the model to process 3D point clouds more efficiently and develop more comprehensive understanding of spatial relationships. 5.2. Ablation of Space Exploration Strategy As shown in Figure 4, we analyze the spatial extask ploration strategies of VLMs across multiple Figure 3. Spatial exploration behavior of Think3D. The agent autonomously selects viewpoints and switches between global and egocentric views; after RL training, it explores angles more systematically than the untuned baseline. Table 3. Ablation on different 3D reasoning components. All numbers are accuracy (%). All results are reported in accuracy (%). 3D Rec. denotes reasoning with reconstructed 3D geometry; Cam. Anchor indicates using the camera pose as the manipulation anchor; Cam. Cho. enables camera selection; and Ego-view specifies whether the model may request ego-centric views. We report results on BLINK (multi-view) and MindCube. 3D Rec. Cam. Anchor Cam Cho. Ego-view. BLINK MindCube 55.83% 42.80% 55.43% 42.31% 56.67% 57.57% 65.41% 60.00% 66.17% 61.67% typesincluding multi-view reasoning, planning, and object-orientation estimationand across models with different base capabilities. Visualizing GPT-4.1s exploration behavior reveals clear task-dependent patterns. in route planning and appearance-order For instance, route tasks, GPT-4.1 predominantly uses top-down viewpoints to capture global spatial structure. In contrast, for tasks such as MindCube and object-orientation estimation, the model relies more on rotational viewpoints that better support orientation inference. 5.3. Ablation of Reinforcement Learning Dynamics As shown in Figure 5, we visualize the training dynamics of the RL process by tracking the evolution of both the accuracy-based reward and the number of reasoning turns per trajectory. During the first 50 training steps, the model tends to reduce the number of turns in an attempt to increase the reward. However, this reduction leads to noticeable drop in accuracy: with fewer turns, the model invokes spatial tools less frequently and thus obtains fewer 3D viewpoints. After about 50 training steps, the model gradually increases its use of spatial tools to render 3D point-cloud images, which results in steady improvement in the overall reward. capture global spatial structure. This alignment indicates that RL effectively enhances the models ability to perform informed and purposeful 3D exploration. Figure 6. model-level spatial exploration patterns. Strong models concentrate on informative angles such as oblique and top-down views; after RL fine-tuning, Qwen3-VL-4B shifts its angle distribution toward similar pattern. 5.5. Ablation of Exploration Rounds We further analyze how the number of exploration iterations affects model performance. As shown in Figure 7, after RL training, Qwen3-VL-4B-RL begins to follow the same trend as the stronger models: its accuracy steadily increases as the number of exploration turns grows. This suggests that RL enables the model to develop deeper and more effective spatial exploration capabilities, thereby enabling significantly more efficient utilization of Think3D. Figure 7. Performance of different exploration rounds. After RL, the smaller model benefits more from additional steps and follows the same upward trend as the stronger models. Figure 4. Task level spatial exploration patterns. This figure shows the distribution of viewpoint selections made by GPT-4.1 across different tasks. The exploration patterns vary substantially: for instance, tasks such as route planning exhibit strong preference for top-down views (0,60), while others rely on more diverse or oblique perspectives. Figure 5. Reinforcement Learning Dynamics. As RL fine-tuning progresses, the model learns when extra 3D tool calls are worthwhile, shifting from shorter but less accurate trajectories to more informative explorations with higher reward. 5.4. Ablation on What the Model Learns through RL To better understand what the model learns from reinforcement learning, we analyze its spatial exploration behavior before and after RL fine-tuning. We first visualize the spatial exploration trajectories of strong modelssuch as GPT4.1 and Gemini-2.5-Pro, whose robust spatial exploration strategies are associated with substantial performance gains under Think3D. We then compare these strong models with smaller model, Qwen3-VL-4B, and its RL-enhanced variant, Qwen3-VL-4B-RL. Specifically, we examine the distribution of viewpoint selections across different angle combinations on the multi-view benchmarks. As shown in Figure 6, Qwen3-VL-4B-RL adopts viewpoint patterns that more closely match those of the stronger modelsfor example, selecting top-down perspectives more frequently to 6. Conclusion We introduce Think3D, framework that enables VLMbased agents to actively reason in 3D space rather than rely on passive 2D perception. By interacting with reconstructed point clouds via an augmented 3D manipulation toolkit and iteratively exploring the scene, Think3D yields much deeper and more consistent spatial understanding. Our RL-enhanced variant further learns efficient exploration strategies, allowing smaller VLMs to approach the behavior and performance of large proprietary systems. Experiments on BLINK, MindCube, and VSI-Bench confirm strong gains and cross-benchmark generalization. Overall, Think3D shows that explicit 3D interaction is promising path toward authentic spatial reasoning in VLMs."
        },
        {
            "title": "References",
            "content": "[1] Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. 3 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [3] Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, and Rahul Krishnan. Synthetic vision: Training vision-language models to understand physics. arXiv e-prints, pages arXiv2412, 2024. 2 [4] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 94909498. IEEE, 2025. 2 [5] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 1, 2 [6] Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, and Yali Wang. Lvagent: Long video understanding by multi-round dynamical collaboration of mllm agents. arXiv preprint arXiv:2503.10200, 2025. 3 [7] Yang Chen, Yufan Shen, Wenxuan Huang, Sheng Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Jiajun Bu, Botian Shi, and Yu Qiao. Learning only with images: Visual reinforcement learning with reasoning, rendering, and visual feedback. arXiv preprint arXiv:2507.20766, 2025. [8] Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, and Ruqi Huang. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint arXiv:2510.18632, 2025. 3 [9] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 2 [10] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. 2, 3 [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 5, 6 [12] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. 3 [13] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. 3 [14] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 1, 2, 5, 6 [15] Jie Feng, Jinwei Zeng, Qingyue Long, Hongyi Chen, Jie Zhao, Yanxin Xi, Zhilun Zhou, Yuan Yuan, Shengyuan Wang, Qingbin Zeng, et al. survey of large language model-powered spatial intelligence across scales: Advances in embodied agents, smart cities, and earth science. arXiv preprint arXiv:2504.09848, 2025. [16] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. 2, 5 [17] Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, and Shanghang Zhang. Tiger: Tool-integrated geometric reasoning in vision-language models for robotics. arXiv preprint arXiv:2510.07181, 2025. 3 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1 [19] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. 1, 3, 5, 6 [20] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv preprint arXiv:2509.13414, 2025. 2, 3 [21] Jaeseong Lee, Yeeun Choi, Heechan Choi, Hanjung Kim, and Seonjoo Kim. training-free, task-agnostic framework for enhancing mllm performance on high-resolution images. arXiv preprint arXiv:2507.10202, 2025. [22] Phillip Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. Perspectiveaware reasoning in vision-language models via mental imagery simulation. arXiv preprint arXiv:2504.17207, 2025. 2, 3 [23] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 3 [24] Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, and Philip Torr. Olympus: universal task In Proceedings of the router for computer vision tasks. Computer Vision and Pattern Recognition Conference, pages 1423514246, 2025. 3 [25] Benlin Liu, Yuhao Dong, Yiqin Wang, Zixian Ma, Yansong Tang, Luming Tang, Yongming Rao, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondences boost spatial-temporal In Proceedings reasoning in multimodal language model. of the Computer Vision and Pattern Recognition Conference, pages 37833792, 2025. 2, 3 [26] Jiale Liu, Huan Wang, Yue Zhang, Xiaoyu Luo, Jiaxiang Hu, Zhiliang Liu, and Min Xie. Insightx agent: An lmm-based agentic framework with integrated tools for reliable x-ray ndt analysis. arXiv preprint arXiv:2507.14899, 2025. 3 [27] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European conference on computer vision, pages 126142. Springer, 2024. [28] Xinheng Lyu, Yuci Liang, Wenting Chen, Meidan Ding, Jiaqi Yang, Guolin Huang, Daokun Zhang, Xiangjian He, and Linlin Shen. Wsi-agents: collaborative multi-agent system for multi-modal whole slide image analysis. arXiv preprint arXiv:2507.14680, 2025. 3 [29] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16488 16498, 2024. 2 [30] Damiano Marsili, Rohun Agrawal, Yisong Yue, and Georgia Gkioxari. Visual agentic ai for spatial reasoning with In Proceedings of the Computer Vision and dynamic api. Pattern Recognition Conference, pages 1944619455, 2025. 2, 3 [31] OpenAI. Introducing gpt-4.1 in the api, 2025. https:// openai.com/index/gpt-4-1. 5, 6 [32] QwenTeam. Qwen3-vl: Sharper vision, deeper thought, broader action, 2025. https://qwen.ai/blog?id= 99f0335c4ad9ff6153e517418d48535ab6d8afef& from = research . latest - advancements - list. 5, [33] Rajarshi Roy, Devleena Das, Ankesh Banerjee, Arjya Bhattacharjee, Kousik Dasgupta, and Subarna Tripathi. Bydeway: Boost your multimodal llm with depth prompting in In Proceedings of the IEEE/CVF Intertraining-free way. national Conference on Computer Vision, pages 60586064, 2025. 2 [34] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 3 [35] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. 5, 6 [36] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 2, 3 [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. 3 [39] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 2, 3 [40] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 2 [41] Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1188811898, 2023. 3 [42] Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, and Hiroyuki Sakai. Spatialprompting: Keyframe-driven zero-shot spatial reasoning with off-the-shelf multimodal large language models. arXiv preprint arXiv:2505.04911, 2025. 3 [43] Zitian Tang, Shijie Wang, Junho Cho, Jaewook Yoo, and Chen Sun. How can objects help video-language understanding? arXiv preprint arXiv:2504.07454, 2025. [44] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 3 [45] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: arXiv preprint Bringing ai arXiv:2503.20020, 2025. 1, 3 into the physical world. [46] Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, et al. Glm-4.5 and glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. URL https://arxiv. org/abs/2507.01006. 5, 6 [47] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Gpt-4v (ision) for Takamatsu, and Katsushi Ikeuchi. robotics: Multimodal task planning from human demonstration. IEEE Robotics and Automation Letters, 2024. 2 [48] Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao. Mllm-tool: multimodal large language model for tool agent learning. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 66786687. IEEE, 2025. 3 [49] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2, [50] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perIn Proceedings of the ception model with persistent state. Computer Vision and Pattern Recognition Conference, pages 1051010522, 2025. 3 [51] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [52] Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, Dacheng Tao, and Xipeng Qiu. Visuothink: Empowering lvlm reasoning with multimodal tree search. arXiv preprint arXiv:2504.09130, 2025. 3 [53] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. pi3: Scalable permutation-equivariant visual geometry learning. arXiv e-prints, pages arXiv2507, 2025. 2, 3, 4 [54] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. 3 [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [56] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3 [57] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. 5, 6 [58] Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, and Weidi Xie. Spatialscore: Towards unified evaluation for multimodal spatial understanding. arXiv preprint arXiv:2505.17012, 2025. 2, 3 [59] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 3 [60] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. [61] Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Philip Torr, and Jian Wu. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. In European Conference on Computer Vision, pages 164182. Springer, 2024. 3 [62] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 2, 3, 5 [63] Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. Visionthink: Smart and efficient vision lanarXiv preprint guage model via reinforcement learning. arXiv:2507.13348, 2025. 3 [64] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2, 3 [65] Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, and Chuang Gan. Vca: Video curious agent for long video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2016820179, 2025. 3 [66] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Structural Priors for Vision Workshop at ICCV25, 2025. 1, 2, 3, [67] Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, et al. How far are vlms from visual spatial intelligence? benchmark-driven perspective. arXiv preprint arXiv:2509.18905, 2025. 1 [68] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1 [69] Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei Wang, and Liqiang Nie. Spatial understanding from videos: Structured prompts meet simulation data. arXiv preprint arXiv:2506.03642, 2025. 2, 3 [70] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep video discovery: Agentic search with tool use for long-form video understanding. arXiv preprint arXiv:2505.18079, 2025. 3 [71] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Pyvision: arXiv preprint Qilong Wu, Kaipeng Zhang, and Chen Wei. Agentic vision with dynamic tooling. arXiv:2507.07998, 2025. 2, [72] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. 5 [73] Weicheng Zheng, Xiaofei Mao, Nanfei Ye, Pengxiang Li, Kun Zhan, Xianpeng Lang, and Hang Zhao. Driveagentr1: Advancing vlm-based autonomous driving with hybrid thinking and active perception. arXiv e-prints, pages arXiv 2507, 2025. 3 [74] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 1, 2, 3 [75] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. 3 [76] Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, et al. Robotracer: Mastering spatial trace with reasoning in vision-language models for robotics. arXiv preprint arXiv:2512.13660, 2025. 1 [77] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. 2, 3, 5, 6 [78] Muzhi Zhu, Yuzhuo Tian, Hao Chen, Chunluan Zhou, Qingpei Guo, Yang Liu, Ming Yang, and Chunhua Shen. Segagent: Exploring pixel understanding capabilities in mllms by imitating human annotator trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 36863696, 2025. A. Prompts and Implementation Details Training-free Workflow Prompt. The prompts used in Think3D are divided into three parts: system prompt (see Fig. 10), tool prompt (which includes the description of the 3D tools) (see Fig. 8), and continual prompt (see Fig. 9), which refers to the prompt update and context management performed at the beginning of each reasoning round. Figure 8. The Pi3 Tool Prompt. The prompt specifies the tools capabilities, key control parameters, and multi-angle query usage strategies to support comprehensive spatial understanding. Figure 9. Multi-step prompt for iterative 3D viewpoint exploration. Including angle selection, camera rotation controls, tool invocation rules to refine spatial reasoning. Figure 10. The system prompt. Instruction prompt detailing tool invocation rules and the multi-step workflow for iterative 3D viewpoint exploration, including tool-call format, recommended angles, and guidelines for reasoning with reconstructed camera poses. Table 4. Training and evaluation parameters used in both the RL optimization process and subsequent evaluation."
        },
        {
            "title": "Parameter\nFoundation model\nNumber of trained agents\nNumber of solution rounds\nNumber of evaluation rounds\nHorizon for discussion history\nToken limit for prompts\nToken limit for responses\nTraining temperature\nEvaluation temperature\nClipping epsilon\nWeight of KL penalty\nNumber of training epochs\nTraining batch size\nRollout batch size\nOptimizer name\nLearning rate\nWeight decay\nGradient norm\nGradient clipping\nGradient checkpoint\nFlash Attention\nMixed precision\nEnable vLLM\nEnable DeepSpeed",
            "content": "Setting Qwen3-4B-Instruct 1 3 3 1 180000 1024 0.6 1.0 0.2 0.05 1 32(8*4accu) 64 AdamW 1e-6 0.1 0.5 False True True True False True Table 5. Performance of GPT-4.1 and Think3D across different temperature settings on the BLINK (Multi-view) and MindCube benchmarks, including average performance across both datasets. Temperature Model BLINK (%) MindCube Avg (%) Avg (%) = 0.0 = 0. = 0.3 = 0.5 = 0.7 = 1.0 GPT-4.1 Think3D (GPT-4.1) GPT-4.1 Think3D (GPT-4.1) GPT-4.1 Think3D (GPT-4.1) GPT-4.1 Think3D (GPT-4.1) GPT-4.1 Think3D (GPT-4.1) GPT-4.1 Think3D (GPT-4.1) 32.33 72.93 37.59 66. 40.61 63.91 34.59 64.66 31.58 63.16 42.80 66.17 50.83 60.00 49.17 54. 51.67 56.67 53.33 58.33 53.33 55.83 55.83 61.67 46.21 63.23 46.27 57. 48.90 58.48 48.65 59.92 47.90 57.67 52.58 62.79 RL Training Prompt. During RL training, online tool invocation is extremely time-consuming, so we pregenerated three offline viewpointsleft view (-45,0), right view (45,0), and top view (0,60). In the RL prompt, we restrict the model to select only from these three viewpoints. Since smaller open-source models exhibit weaker instruction-following abilities, we further categorize the continual prompts based on the current iteration round. The corresponding prompts ares shown in Fig 11, Fig 12, and, Fig 13. Prompt for evaluation without tools. When no tool is available, we adopt standard chain-of-thought (CoT) [55] reasoning. The corresponding prompt is shown in Fig 14. B. Further Experiment Analysis Ego view analysis As shown in Fig 15, we visualize the proportion of ego-view versus global-view usage by GPT4.1 across different tasks. We find that tasks requiring fine-grained local understandingsuch as MindCube and Object Directionexhibit much higher reliance on egoview. In contrast, tasks like Route Planning, which demand broader global context, show minimal use of ego-view and favor global-view instead. Tool calling iteration analysis As shown in Fig 16, we also visualize the proportion of tool calls across different tasks. We find that for route planning, GPT-4.1 uses the tools much less frequently. For the other tasks, GPT-4.1 often performs multiple rounds of tool calls to obtain richer spatial information. C. Think3D-RL Training Parameters As shown in Tab 4, we provide the parameters used for both RL training and evaluation. D. Reproducibility Across Different Temperatures To verify reproducibility, we conduct experiments under different temperature settings, as shown in Tab 5 . E. Interaction Visualization We provide additional visualization examples, as illustrated in the figures below. Figure 11. The RL system prompt. Instruction prompt defining the constrained 3-view 3D analysis workflow, including tool-call format, angle selection rules (left, right, top), and iterative reasoning steps for viewpoint-guided spatial understanding. Figure 13. The RL continuation prompt used in the final turn. Final-turn instruction prompt specifying the no-tool phase, requiring explicit reasoning and final answer based solely on previously generated 3D views and the original image. Figure 12. The RL continuation prompt used during non-final turns. Iterative-step instruction prompt outlining allowed viewpoint choices (left/right/top), tool-call rules, and the decision process for progressing or concluding 3D spatial analysis. Figure 14. The prompt without tools. Base instruction prompt for direct image-question analysis, requiring explicit reasoning and final answer formatting without tool interactions. Figure 15. Ego view usage ratio across different tasks. Distribution of GPT-4.1s reliance on ego-view versus global-view across tasks. Fine-grained tasks emphasize ego-centric information, whereas tasks requiring broad context predominantly utilize global-view. Figure 16. Tool calling iteration ratio across different tasks. GPT-4.1 rarely uses tools for route planning, while conducting multiple rounds of tool calls for other tasks to acquire richer spatial information. Figure 17. The Mindcube example. Figure 18. The Mindcube example. Figure 19. The BLINK example. Figure 20. The BLINK example. Figure 21. The VSI-Bench example. Figure 22. The VSI-Bench example."
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "University of California San Diego",
        "University of Oxford"
    ]
}