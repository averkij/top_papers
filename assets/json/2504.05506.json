{
    "paper_title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering",
    "authors": [
        "Ahmed Masry",
        "Mohammed Saidul Islam",
        "Mahir Ahmed",
        "Aayush Bajaj",
        "Firoz Kabir",
        "Aaryaman Kartha",
        "Md Tahmid Rahman Laskar",
        "Mizanur Rahman",
        "Shadikur Rahman",
        "Mehrad Shahmohammadi",
        "Megh Thakkar",
        "Md Rizwan Parvez",
        "Enamul Hoque",
        "Shafiq Joty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro."
        },
        {
            "title": "Start",
            "content": "CHARTQAPRO : More Diverse and Challenging Benchmark for Chart Question Answering Ahmed Masry * , Mohammed Saidul Islam *, Mahir Ahmed , Aayush Bajaj Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty York University, Canada, Dialpad Inc., Canada, RBC, Canada MILA - Quebec AI Institute, Canada, Qatar Computing Research Institute (QCRI) Nanyang Technological University, Singapore, Salesforce Research, USA {masry20, saidulis, mrahmed, mdfkabir, aarykary}@yorku.ca {tahmid20, mizanurr, shadikur, msm97, enamulh}@yorku.ca {aayush.bajaj, megh.thakkar}@mila.quebec, mparvez@hbku.edu.qa, sjoty@salesforce.com 5 2 0 2 0 1 ] . [ 2 6 0 5 5 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce CHARTQAPRO, new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart typesincluding infographics and dashboardsand featuring 1,948 questions in various types, such as multiplechoice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show substantial performance drop for LVLMs on CHARTQAPRO; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on CHARTQAPRO, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release CHARTQAPRO at https://github.com/visnlp/ChartQAPro."
        },
        {
            "title": "Introduction",
            "content": "Data visualizations such as bar and line charts are very popular for analyzing data and making in- * Equal contribution. Corresponding author. Equal contribution. Figure 1: Performance gap between ChartQA (Masry et al., 2022) and CHARTQAPRO for various LVLMs. formed decisions across various domains such as finance, journalism, and science (Kim et al., 2020; Masry et al., 2024b; Hoque et al., 2022). However, answering complex questions about charts can pose significant challenges as the user needs to combine visual perception with cognitive reasoning. Chart Question Answering (CQA) systems aim to assist users by taking questions about charts as input and generating answers. Unlike traditional visual question answering involving natural images and scenes, CQA requires models to interpret structured data visually, reason over relationships among visual elements and text, and derive contextual insights. Due to its real-world relevance, CQA has become key task for evaluating recent LVLMs (Wang et al., 2024a; OpenAI et al., 2024; Georgiev et al., 2024; Grattafiori et al., 2024) . These LVLMs have obtained remarkable performance on multimodal tasks, including CQA. For instance, on ChartQA (Masry et al., 2022), Claude Sonnet 3.5 (Anthropic, 2024) achieves an accuracy of 90.5%, while GPT4 (OpenAI et al., 2024) and Gemini (Georgiev et al., 2024) reach 85.7% and 87.2%, respectively (Figure 1). OpenFigure 2: CHARTQAPRO covers more diverse range of questions compared to existing chart question answering datasets  (Table 1)  , providing an extensive evaluation of chart understanding abilities. source LVLMs also appear to be catching up, with Qwen2.5-VL (Wang et al., 2024a) reporting 89.5%. These striking results prompt two core questions: (i) Is chart understanding and reasoning already solved task? and (ii) Have open-source models truly matched their closed-source counterparts? to analyze multiple charts simultaneously. These types of questions and complex layouts are absent from current benchmarks, suggesting that existing evaluations do not fully capture the real-world challenges in chart understanding and create an overly optimistic perception of progress in this field. closer look at ChartQA reveals key limitations. First, its chart images lack visual diversity, coming from few online sources like Statista and Pew Research Center. It primarily includes only bar, line, and pie charts with numeric labels directly on visual elements, reducing the need for actual visual reasoning. Second, the benchmark focuses largely on factoid questions that require simple data extraction or basic arithmetic. Earlier datasets (Kahou et al., 2017; Chaudhry et al., 2020; Singh and Shekhar, 2020) suffer from similar issues, and are also curated from synthetic data or templated questions. Although recent work, CharXiv (Wang et al., 2024b), addresses some of these limitations, it relies on charts sourced exclusively from papers on arXiv, limiting visual and topical diversity, and also lacking numerous real-world question types. In contrast, real-world charts encompass diverse domains like economy, health, etc., and wide variety of question types, including hypothetical (e.g., future price prediction), multiple-choice (e.g., in educational exams), conversational (e.g., in decisionmaking meetings) and unanswerable (e.g. due to missing data). Additionally, multi-chart layouts and dashboards are often used in finance, business intelligence, and scientific reports, requiring users To address these limitations and rigorously evaluate LVLMs on chart understanding, we present CHARTQAPRO, comprehensive benchmark of 1341 charts sourced from 157 diverse online platforms. CHARTQAPRO includes 1948 humanwritten, human-verified question-answer pairs covering factoid, multiple-choice, conversational, hypothetical, multi-chart, and unanswerable queries, making it representative of real-world use cases (see Figure 2). Beyond bar, line, and pie charts, CHARTQAPRO features images with complex visualizations such as multi-chart layouts, infographics, and dashboards, introducing greater visual and analytical complexity. Inspired by conversational and multi-document QA in text such as CoQA (Reddy et al., 2019) and HotpotQA (Yang et al., 2018), some questions also require multi-turn interactions or referencing accompanying paragraphs, probing broader range of multimodal reasoning skills. Our evaluations reveal sharp performance drop for both closedand open-source models on CHARTQAPRO (Figure 1). For example, the SoTA Claude Sonnet 3.5s accuracy falls from 90.50% to 55.81%, demonstrating that CHARTQAPRO presents more challenging and realistic benchmark for chart understanding, and that there is"
        },
        {
            "title": "Dataset",
            "content": "Real vs. Synthetic # Chart Sources"
        },
        {
            "title": "Topic Diversity",
            "content": "Infographics & Dashboards"
        },
        {
            "title": "MCQ Conversational Hypothetical Unanswerable Fact Checking",
            "content": "PlotQA (Methani et al., 2020) Synthetic ChartQA (Masry et al., 2022) CharXiv (Wang et al., 2024b) CHARTQAPRO (Ours)"
        },
        {
            "title": "Real",
            "content": "1 4 1 157 Table 1: Comparison of CHARTQAPRO with existing chart-based QA benchmarks. Features are grouped into Chart Images (real vs. synthetic data, number of sources, topic diversity, infographics/dashboards, accompanying paragraph, multi-chart support) and Questions Types (MCQ, conversational, hypothetical, unanswerable). = Supported, = Not Supported, = Partially Supported. substantial room for improvement in LVLMs chart reasoning abilities. Moreover, while opensource models seemed to match closed-source ones on ChartQA, they still lag significantly on CHARTQAPRO with the best, Qwen2-VL-7B (Wang et al., 2024a), achieving only 37.17%. This suggests that prior benchmarks might have overstated progress due to their limited diversity. types (multiple-choice, conversational, hypothetical, etc.), offering more challenging benchmark. Our contributions include: (i) comprehensive benchmark that evaluates diverse and complex realworld chart understanding abilities; (ii) extensive evaluation of openand closed-source models, revealing significant performance declines compared to previous benchmarks; (iii) in-depth qualitative analyses and ablation studies, identifying key challenges and future directions for improving LVLMs chart reasoning abilities."
        },
        {
            "title": "2 Related Work",
            "content": "Chart Understanding Datasets Numerous tasks and benchmarks have been developed to evaluate LVLMs chart understanding abilities, such as question answering (Masry et al., 2022; Wang et al., 2024b), chart summarization (Kantharaj et al., 2022b), fact-checking (Akhtar et al., 2023a,b), and explanation generation (Kantharaj et al., 2022a). Among these, chart question answering is the most commonly used for evaluation. Early benchmarks like STL-CQA (Singh and Shekhar, 2020) and Leaf-QA (Chaudhry et al., 2020) relied on synthetically generated charts and templated questions. Later benchmarks, such as ChartQA (Masry et al., 2022), PlotQA (Methani et al., 2020), and CharXiv (Wang et al., 2024b), used real-world charts and more complex questions requiring advanced visual reasoning. However, these benchmarks extract charts from limited sources  (Table 1)  , cover few question types, and have reached performance saturation due to recent strong LVLMs (Figure 1). In contrast, CHARTQAPRO sources from 157 diverse online domains and includes human-written, verified questions across multiple Vision-Language Models for Charts Advances in vision-language models have significantly improved chart understanding and reasoning. These models can be categorized into: (i) closed-source, (ii) open-source general multimodal models, and (iii) chart-specific models. Closed-source models (OpenAI et al., 2024; Georgiev et al., 2024) achieve the highest performance on recent chart understanding benchmarks (Masry et al., 2022; Wang et al., 2024b). Open-source general multimodal models (Wang et al., 2024a; Li et al., 2024; Chen et al., 2025; Wu et al., 2024b; Abdin et al., 2024; Laurençon et al., 2024; Masry et al., 2025; Rodriguez et al., 2024) currently lag behind, but are rapidly closing the gap. Chart-specific models (Masry et al., 2024b,a; Zhang et al., 2024; Masry et al., 2023) demonstrate strong performance on standard benchmarks (Masry et al., 2022; Akhtar et al., 2023b; Kantharaj et al., 2022b; Masry and Hoque, 2021). However, their generalization to real-world chart understanding remains uncertain due to their reliance on instruction-tuning datasets with limited task diversity. CHARTQAPRO offers more comprehensive benchmark, ensuring that model improvements reflect real progress in chart understanding abilities of these models."
        },
        {
            "title": "3.1 Dataset Construction",
            "content": "Our dataset construction pipeline consists of three key stages (see Figure 3): (i) Chart Image Collection, (ii) Question-Answer Annotation, and (iii) Question-Answer Review. We detail each stage below: 1 - Chart Images Collection Stage and CHARTQAPRO prioritizes both visual topical diversity. We sourced chart images from diverse platforms featuring real-world visualizations, including multi-series line charts, stacked and Figure 3: CHARTQAPRO Dataset Construction Process grouped bar charts, dashboards, and infographics. Key sources include Pew Research (Pew, 2024), Tableau (Tableau, 2024), the Public Policy Institute of California (PPIC) (PPIC, 2024), and Our World in Data (OWID) (OWID, 2024) (see Figure 6 for more details). For Pew and Tableau, we randomly sampled charts from Islam et al. (2024) which are already diverse in visual styles, while for other sources, we manually selected charts with varied formats to enhance dataset diversity. Some charts were accompanied by textual descriptions that provided additional context, improving the interpretability of the corresponding chart images. To further expand coverage, we collected an additional 1041 charts from the web, building upon prior efforts from ChartInstruct (Masry et al., 2024a) to include dashboards and infographics. In total, CHARTQAPRO is compiled dataset of 1341 chart images from 157 online platforms, covering broad spectrum of chart types and styles. Additional details are provided in Appendix A.1. 2 - Question-Answer Annotation Stage CHARTQAPRO includes five types of questionanswer pairs: (i) Reasoning, (ii) Conversational, (iii) Multiple-Choice, (iv) Hypothetical, and (v) Fact-Checking. Nine team members collaboratively created these QA pairs, with five focusing on reasoning questions and the remaining four handling other categories. To ensure highquality annotations, we adopted human-VLM collaboration process for each QA type: Curating Seed QA pairs: Annotators crafted diverse set of seed QA pairs covering different question types that required complex reasoning. VLM-Assisted Expansion: Using GPT-4o, Gemini, and Claude, we expanded the seed set by generating additional QA pairs. We decided to employ multiple models to mitigate bias. Each model was prompted with seed QA pair and tasked with generating five new pairs per chart. In addition, annotators interactively prompted VLMs to generate additional QA pairs beyond those derived from the seed set, encouraging the models to produce diverse and novel questions. Human Refinement: Annotators manually reviewed the generated questions to filter the ones that are overly simple (e.g., direct data retrieval from charts) or revise the questions that are unclear or ambiguous. key feature of CHARTQAPRO is the inclusion of unanswerable questions. These questions were carefully curated by humans to be closely related to the charts topic while unanswerable based solely on the chart image. Also, CHARTQAPRO features questions on chart-text pairs, with some referring only to the chart, others only to the text, and some requiring integration of both, posing greater challenge for vision-language models. We present brief description of various question types below: Reasoning: Reasoning with charts is common real-world task involving visual perception, trend analysis, and mathematical reasoning. While such questions appear in benchmarks like ChartQA, we focus on more complex cases requiring compositional calculations and deeper pattern, trend, and outlier analysis (e.g., Figure 2a, b). Conversational: Conversational questions consist of multiple interrelated QA pairs for given visualization, where each question naturally builds upon the previous one. These questions help us assess how well VLMs handle contextual dependencies, such as coreference resolution and logical or arithmetic reasoning (e.g., Figure 2c). Multiple-Choice: Multiple-choice questions (MCQs) are widely used in assessments and educational materials. We focused on MCQs that require complex reasoning, including trend analysis, anomaly detection, extrapolation, and time series analysis (e.g., Figure 2d). Each question is presented with four answer choices, covering various formats such as dates, percentages, locations, and specific labels derived from the data. Hypothetical: Hypothetical questions introduce assumptions beyond observable chart data (e.g., Figure 2e). Answering these questions requires not only extracting information accurately but also making inferences, estimations, or approximations based on patterns and trends present in the visualization. These questions add an extra layer of complexity by requiring the model to reason beyond explicit data points. Fact-Checking: Fact-checking questions involve evaluating claim about chart by extracting and verifying relevant data (e.g., Figure 2f). Each claim is classified as either True (confirmed by data) or False (contradicted by data). These questions test the models ability to interpret chart information and assess the validity of claims, crucial skill for misinformation detection, incorrect prediction, fake news detection, etc. Stage 3 - Question-Answer Review After creating the QA pairs, we conducted quality assessment to ensure accuracy and clarity. Seven annotators, all co-authors with expertise in visualization, performed this review. Five focused on factoid questions, while the remaining two handled other categories. Each annotator reviewed questions from category they had not originally worked on, then cross-checked their responses with the categorys original creator. Any identified errors in the questions or answers were collaboratively revised until both parties reached an agreement. In rare instances, ambiguous questions were modified to resolve disagreements. For subjective questions (e.g., value estimations), minor discrepancies (<1%) were considered acceptable. Overall, the initial agreement rate between annotators was 66.17% before resolving all discrepancies."
        },
        {
            "title": "3.2.1 Visual Diversity",
            "content": "Unlike the ChartQA (Masry et al., 2022) dataset, which sources its charts from only four origins, our benchmark incorporates diverse range of sources. These include web charts collected from various websites and links across the internet, as well as charts from Tableau, Pew Research, PPIC, and OWID. As shown in Figure 4, the majority of charts (74%) were collected through web crawling, followed by charts from Tableau (14%), covering diverse range of topics, such as, Politics, Economy, Health, Environment, Technology, etc. The corpus also includes various chart types such Figure 4: Distribution of topics per source in CHARTQAPRO. The inner ring represents online sources, while the outer ring shows topic distribution for each source. as bars, lines, pies, scatter plots, dashboards, infographics, maps, etc. (see Table 2), with bar charts being the most common (31.8%), followed by line charts (26.5%). To further quantify the visual diversity of images compared to earlier benchour chart marksChartQA (Masry et al., 2022) and CharXiv (Wang et al., 2024b)we conducted an experiment where we first encoded all images from each benchmark into feature vectors using CLIP vision encoder (Radford et al., 2021) with sentencetransformers (Reimers and Gurevych, 2019). For each benchmark, we then computed the pairwise cosine distances among all images. In this context, higher average pairwise distance indicates that the images are less similar and therefore more visually diverse. Our CHARTQAPRO benchmark exhibits an average distance of 0.53, while ChartQA and CharXiv show averages of 0.26 and 0.27, respectively. Moreover, Figure 10 in A.3 shows that most pairwise distances in CHARTQAPRO exceed those in the other benchmarks. These results conclusively demonstrate that our CHARTQAPRO benchmark is significantly more diverse than the existing benchmarks, offering richer and more varied set of visual representations."
        },
        {
            "title": "3.2.2 Linguistic Diversity",
            "content": "We conducted detailed analysis of the linguistic features of our benchmark dataset (see Appendix A.3). Unlike existing chart-based bench-"
        },
        {
            "title": "Bar Line Pie Area Scatter Bubble Dashboard Infographic Other",
            "content": "Math & Visual Reasoning"
        },
        {
            "title": "Count",
            "content": "427 355 29 30 8 258 190 37 1081 311 214 98 Table 2: Distribution of chart and question types in CHARTQAPRO. marks that focus on short question-answer pairs, CHARTQAPRO provides more diverse and linguistically rich dataset. It features 6,638 unique tokens in questions and 1496 in answers, significantly surpassing CharXiv (4545) and ChartQA (2427). The questions in CHARTQAPRO are longer and more varied, averaging 106.05 characters and 18 tokens, compared to CharXiv (96.3 characters, and 17.2 tokens) and ChartQA (63.25 characters, and 11.5 tokens), while answers remain concise at 6.7 characters and 1.18 tokens. Additionally, CHARTQAPRO captures real-world variability with diverse syntactic structures, informal language, and typographical errors, making it comprehensive benchmark for evaluating complex question-answering models in the chart domain. We further analyze the linguistic diversity and richness of the text in chart images by extracting text using the Google OCR API1 and using lexical diversity and semantwo key metrics: tic diversity (Figure 11). Lexical diversity, measured via the type-token ratio (TTR), is highest for CHARTQAPRO (0.15), followed by ChartQA (0.13) and ChartXiv (0.11), indicating richer vocabulary in CHARTQAPRO. Semantic diversity, quantified as the average pairwise cosine distance between text embeddings computed using sentence transformers (Reimers and Gurevych, 2019), is also maximum for CHARTQAPRO (0.84) compared to ChartQA (0.75) and ChartXiv (0.78), suggesting broader semantic coverage. Overall, these findings collectively demonstrate that CHARTQAPRO exhibits greater linguistic diversity than previous benchmarks. More details are provided in A.3.1."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "paragraph pi which the task might use. The objective is for the multimodal LLM to take ci and qi as input (along with the prompt) and autoregressively generate the answer ai. We provide all our prompts in A.4 to ensure reproducibility and transparency."
        },
        {
            "title": "4.2 Models",
            "content": "To evaluate the current state-of-the-art in chart understanding, we benchmark diverse set of closedand open-source models. The closed-source models include: (i) GPT-4o (OpenAI et al., 2024), (ii) Gemini-Flash-1.5 and 2.0 (Georgiev et al., 2024), and (iii) Claude Sonnet 3.5 (Anthropic, 2024). For open-source models, we categorize them based on parameter size. Models with fewer than 7B parameters include: (i) Intern-VL2.5-1B (Chen et al., 2025), (ii) Janus-1.3B (Wu et al., 2024a) (iii) Qwen-VL2-2B (Wang et al., 2024a), (iv) InternVL2.5-2B (Chen et al., 2025), (v) SmolVLM-2.3B (SmolVLM, 2024), (vi) Ovis1.6-Llama3.2-3B (Lu et al., 2024), (vii) DeepSeek-VL2-3.4B (Wu et al., 2024b), and (viii) Phi 3.5-Vision-4B (Abdin et al., 2024). In the 7-12B parameter range, we evaluate: (i) Qwen-VL2-7B (Wang et al., 2024a), (ii) InternVL2.5-8B (Chen et al., 2025), (iii) Idefics-3-Llama3.1-8B (Laurençon et al., 2024), (iv) LLaVA-NextMistral-7B (Li et al., 2024), (v) Ovis1.6-Gemma29B(Lu et al., 2024), and (vi) Llama 3.2-Vision-11B (Grattafiori et al., 2024). In addition, we also evaluate chart-specific LVLMs: (i) ChartGemma (Masry et al., 2024b), (ii) ChartInstruct-LLama2 (Masry et al., 2024a), (iii) TinyChart (Zhang et al., 2024). All models are assessed with three prompting strategies: Direct prompting, Chain-of-Thought (CoT) (Wei et al., 2023), and Program-of-Thought (PoT) (Chen et al., 2023). All experiments were run on Google Cloud Platform (GCP) using A100 GPU. We formulate the CHARTQAPRO tasks as multimodal question-answering challenges. The dataset consists of examples, denoted as = {ci, qi, ai}N i=1, where each example includes chart image ci, question qi, and the corresponding ground truth answer ai. For certain charts, the formulation also includes corresponding context 1https://cloud.google.com/vision/docs/ ocr"
        },
        {
            "title": "4.3 Evaluation Metric",
            "content": "We enhance the relaxed accuracy metric commonly used for CQA (Masry et al., 2022; Methani et al., 2020) for all the question types. Specifically, for numeric answers, we maintain 5% error margin, but for answers in years we require an exact match to avoid bias from minimal differences (e.g., 2008 vs. 2009). For textual answers (e.g., labels or common words), we employ the ANLS score (Biten et al., Model Direct Chain-of-Thought (CoT) Program-of-Thought (PoT) Factoid MCQ Convers. FactChk. Hypoth. Overall Factoid MCQ Convers. FactChk. Hypoth. Overall Factoid MCQ Convers. FactChk. Hypoth. Overall Human Baseline 80.00 94.00 88.70 92.00 70. 85.02 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Closed-Source Models GPT4-o Gemini-Flash-2.0 Gemini-Flash-1.5 Claude Sonnet 3.5 Open-Source Models Intern-VL2.5-1B Janus-1.3B Qwen-VL2-2B Intern-VL2.5-2B SmolVLM-2.3B Ovis1.6-LLama3.2-3B DeepSeek-VL2-3.4B Phi 3.5-Vision-4B Qwen-VL2-7B Intern-VL2.5-8B Idefics-3-LLama-3.1-8B LLaVA-Next-Mistral-7B Ovis1.6-Gemma2-9B LLama 3.2-Vision-11B Chart-Specific Models ChartGemma-3B TinyChart-3B ChartInstruct-LLama2-7B 35.76 43.43 39.96 38.84 9.15 4.56 15.90 13.86 13.32 12.87 12.20 17.48 30.70 35.21 20.69 15.35 30.25 12. 6.86 8.52 7.09 46.72 60.28 57.00 51.40 7.00 1.86 27.57 10.74 16.82 0.46 7.47 30.37 44.85 25.70 2.29 35.98 4.67 2.33 0.0 7.00 0.0 34.75 40.25 39.70 44.53 6.20 6.74 24.26 14.02 17.71 4.18 19.40 28.54 35.68 32.26 31.96 21.09 28.93 0. 16.00 17.46 3.77 45.49 67.62 47.13 55.60 16.63 40.98 34.42 45.90 46.31 40.98 36.88 41.99 48.36 53.27 10.76 41.80 27.86 27.18 1.22 33.19 0.0 28.91 24.47 45.31 45.48 8.17 5.31 12.82 18.92 25.21 10.17 19.21 37.27 37.23 29.61 36.83 17.79 28.21 10. 6.53 16.06 6.91 37.67 46.85 42.96 43.58 9.33 9.21 20.68 17.81 19.14 13.50 16.28 24.73 35.59 35.67 20.03 21.97 26.83 11.09 6.84 13.25 4.88 37.40 51.51 42.37 53.61 5.45 3.54 16.62 9.42 13.03 14.43 9.63 10.55 32.95 29.53 20.06 9.43 18.09 19. 11.01 8.97 3.83 61.68 69.15 64.01 78.03 0.46 0.0 30.84 6.07 7.47 7.45 1.40 32.71 46.26 23.36 2.29 4.20 12.42 47.66 1.86 6.07 0.0 33.93 43.84 40.17 43.84 14.86 6.05 23.89 13.02 18.60 8.37 18.09 27.20 37.60 28.87 30.98 19.30 17.68 19. 15.21 11.05 4.43 57.37 67.62 56.14 65.16 21.17 29.91 38.52 36.06 36.88 35.27 38.11 8.19 50.40 56.14 11.14 38.93 25.05 44.45 2.45 28.27 0.40 30.83 39.89 39.42 46.11 17.08 6.97 13.00 19.23 22.15 16.60 23.25 8.16 29.65 27.73 35.36 21.71 20.49 13. 15.02 14.24 10.65 41.68 53.66 45.97 55.81 8.96 7.03 21.90 13.46 16.76 15.42 14.33 15.23 37.17 31.99 19.51 14.74 18.39 25.43 9.80 11.67 3.42 39.22 51.18 45.57 46.58 1.07 5.12 13.66 1.13 4.03 17.41 10.27 10.34 11.74 26.14 10.06 4.93 22.59 19. 12.69 5.64 0.09 42.99 57.00 35.51 54.20 0.0 1.86 23.83 6.07 12.61 5.60 3.27 32.71 44.85 18.69 5.41 2.33 20.56 39.25 0.0 0.0 0.31 38.62 46.34 40.98 46.17 0.64 6.61 15.22 2.51 11.22 5.86 15.94 16.62 20.42 11.43 19.41 3.72 17.33 19. 10.14 4.11 1.69 44.67 56.81 50.40 52.04 0.40 3.68 8.60 2.04 5.73 30.32 22.54 0.0 28.96 34.83 7.62 13.79 32.37 27.45 14.18 0.0 2.04 44.43 44.86 47.26 46.90 2.04 3.60 3.06 3.06 12.52 24.10 17.43 5.10 10.64 22.60 18.60 13.26 25.30 23. 21.61 15.92 0.0 40.48 51.44 44.42 48.05 0.85 4.74 13.86 2.10 6.76 16.22 12.30 12.24 18.86 23.88 11.16 5.98 22.89 22.95 11.52 4.59 0.61 Table 3: Accuracy (%) on CHARTQAPRO by Prompt Type (main headers) and Question Type (subEach Prompt Type block has five question types plus an Overall sub-column. Color codheaders). open-source models below 7B parameters , ing for comparison: closed-source models , human baseline , open-source models between 7-12B parameters , chart-specific models . We bold the best score within each model category. 2019). Finally, multiple-choice questions (e.g., a, b, c, d) and fact-checking tasks (true, false) are evaluated using an exact-match criterion. Additional details are provided in A.5."
        },
        {
            "title": "4.4 Main Results",
            "content": "Table 3 presents each models performance on the CHARTQAPRO dataset under three prompting strategies (Direct, Chain-of-Thought, and Program-of-Thought) and across five question types. Closed-source models consistently outperform open-source counterparts in all prompting setups, and they also benefit from more extensive reasoning strategies (CoT or PoT), which boost overall accuracy. Notably, Chain-of-Thought yields the highest scores, with Claude Sonnet 3.5 achieving the top accuracy of 55.81%, while GPT4o ranks lowest among the closed-source group. We also observe that conversational, hypothetical, and factoid queries pose the greatest challenge for these models, whereas fact-checking and multiple-choice questions yield relatively higher accuracylikely because the narrower range of possible answers increases the likelihood of correct response. In contrast, open-source models below 7B parameters (highlighted in blue) exhibit substantially lower performance across all prompt types, often falling below 20% overall accuracy. However, certain open-source models in the 712B range (shaded in orange) show more promise; for instance, Qwen2-VL-7B and InternVL-2.5-8B both exceed 30%. Surprisingly, these models often perform worse when asked to produce long-form reasoning (as in CoT or PoT), suggesting they may lack sufficient training or alignment with step-bystep answer styles. Finally, chart-specific models perform poorly under all setups, indicating that they may be heavily overfitted to particular visual and question types and thus generalize poorly to broader chart-based QA scenarios. Overall, these findings indicate that none of the models have achieved near-human-level chart understanding (See A.6), leaving considerable room for improvementa result that contrasts sharply with the previously reported high accuracies on previous datasets (Figure 1 and Appendix A.7)."
        },
        {
            "title": "4.5 Qualitative Analysis",
            "content": "We examined 150 random samples to find common failure patterns and discovered three major error categories. Figure 5 presents representative errors, while additional examples are provided in A.8. Visual Perception: common source of error is the failure to accurately recognize data values from chart images. This often occurs when charts are overcrowded with visual elements (e.g., bars, lines) or when data values are not explicitly shown, requiring inference based on geometric properties like height or area. While both open-source and closedsource models struggle with visual perception, it is the primary issue for closed-source models. Instruction Following: Open-source and chartspecific models struggle to generate proper chainof-thought (CoT) or program-of-thought (PoT) responses when explicitly prompted. Many generated programs even fail to execute due to runtime errors. Figure 5: Sample errors across three categories: Visual Perception, Instruction Following, and Math Reasoning. Model Chart Dashboard Infographic Normal Unanswerable No Para With Para Chart Type (A) Answer Type (B) Paragraph Presence (C) performance decline on such complex visuals. Closed-Source Models GPT4-o Gemini-Flash-2.0 Gemini-Flash-1.5 Claude Sonnet 3. Open-Source Models 39.63 52.34 43.93 54.63 21.20 Qwen-VL2-2B 18.88 SmolVLM-2.3B 26.15 Phi 3.5-Vision-4B 37.18 Qwen-VL2-7B InternVL2.5-8B 36.74 LLama-3.2-Vision-11B 23.96 Chart-Specific Models ChartGemma ChartInstruct-LLama2 TinyChart 7.01 5.97 13. 44.49 54.64 49.03 57.42 19.41 15.15 20.96 31.61 35.10 26.32 4.74 2.84 11.20 47.74 58.70 51.61 59.30 19.93 16.36 23.12 33.43 32.38 31.27 8.98 2.48 13. 39.71 51.44 47.22 57.63 21.02 19.99 28.72 37.13 31.41 29.09 8.38 6.03 16.28 50.13 63.14 40.65 47.98 19.24 8.49 7.66 28.99 53.92 9.75 0.27 0.0 0. 40.04 52.29 44.16 54.33 21.16 18.30 25.12 35.30 35.08 25.14 6.94 5.64 15.25 52.29 62.44 57.65 65.29 17.59 14.65 22.19 37.47 39.50 27.29 6.24 0.0 0. Table 4: Ablation results on CHARTQAPRO across (A) Chart Type , three independent dimensions. (B) Answer Type , (C) Paragraph Presence . Additionally, Llama 3.2 Vision-11B (Grattafiori et al., 2024) performs poorly in the direct-answer setup (11.09% accuracy), often ignoring the prompt and persistently generating CoT explanations, suggesting overfitting to CoT-style training. Math Reasoning: While all models struggle with complex mathematical operations in our benchmark, closed-source models mitigate this issue to some extent by effectively utilizing long reasoning traces, such as Chain-of-Thought (CoT) or Program-of-Thought (PoT), allowing them to break down problems into steps and leverage external tools (e.g., Python). In contrast, open-source models fail to utilize these prompting strategies. In the direct-answer setup, they particularly struggle to perform multiple mathematical operations and generate the final answer correctly."
        },
        {
            "title": "4.6 Ablation Studies",
            "content": "Table 4 shows ablation results on CHARTQAPRO on three independent dimensions: (A) Chart Type, (B) Answer Type, and (C) Paragraph Presence. Chart Type: Closed-source models demonstrate greater robustness to complex visual layouts, such as dashboards and infographics. In contrast, both open-source and chart-specific models exhibit Answer Type: Among closed-source models, GPT-4o and Gemini Flash 2.0 handle unanswerable questions relatively well, while Gemini Flash 1.5 and Claude Sonnet 3.5 show lower robustness. Similarly, open-source models generally perform worse on unanswerable questions. Chart-specific models, however, struggle significantly, with performance near zero, highlighting their limited ability to handle ambiguous or missing information. Paragraph Presence: Closed-source models can effectively utilize the additional context. Among open-source models, smaller models struggle with this added context, while larger models are more robust. Chart-specific models perform poorly with added context, likely due to overfitting, except for ChartGemma (Masry et al., 2024b). Overall, our analysis shows that while closedsource models generally lack in recognizing data values (visual perception), open-source and chartspecific models struggle with visual complexity, ambiguous information, and added context, highlighting the need for improvements to match closedsource models in chart understanding. We present exemplar details in A.9 and Figure 13."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced CHARTQAPRO, more diverse and challenging benchmark for chart question answering, designed to push the limits of current vision-language models (VLMs) in real-world chart reasoning. By incorporating 1341 charts from 157 sources and broad spectrum of question typesincluding factoid, multiple-choice, fact-checking, conversational, and hypothetical queriesour benchmark reveals significant performance gaps between existing models and humanlevel understanding. Our extensive evaluation shows that even the strongest closed-source models experience substantial performance drops, underscoring that chart reasoning remains an unsolved challenge. Through detailed error analysis and ablation studies, we identify key areas for improvement, paving the way for future advancements in multimodal reasoning. We hope CHARTQAPRO serves as catalyst for developing more robust and capable models for real-world chart comprehension. As future work, we plan on expanding the benchmark by introducing dynamic and interactive charts and dashboards, as current benchmarks only use screenshots of the charts which often does not happen in real-world scenarios. We also aim to curate large-scale training dataset in reasoning formats following recent advances in LLM training, hoping to develop significantly more proficient chart understanding and reasoning models."
        },
        {
            "title": "Limitations",
            "content": "While CHARTQAPRO is designed to comprehensively evaluate chart understanding, there are few limitations to consider. First, our benchmark primarily focuses on chart question answering (ChartQA) as the core evaluation task. While this task effectively measures models ability to extract, interpret, and reason over chart data, other chart-related taskssuch as chart-to-summary generation or chart-to-code translationare also valuable and remain unexplored in this work. Second, although we carefully tuned prompts to ensure fair and consistent evaluation across all models, performance may vary slightly by applying further prompt engineering techniques. While certain models might benefit from additional prompt engineering, we do not expect such adjustments to lead to substantial improvements or change the overall findings in our study. the in dashboards CHARTQAPRO are static screenshots rather than interactive elements. In real-world scenarios, most dashboards often allow users to hover, filter, or manipulate data dynamically, which can impact how insights are extracted. Since our benchmark does not incorporate interactivity, models are evaluated solely on the static visual and textual information presented in the images. included Third, Despite these limitations, CHARTQAPRO provides rigorous and diverse benchmark that highlights key challenges in chart reasoning and serves as valuable resource to advance multimodal research."
        },
        {
            "title": "Ethical Considerations",
            "content": "During the dataset collection process, we carefully considered several ethical aspects to ensure the integrity of our work. All collected images underwent thorough manual review by the authors to filter out any content that could be considered harmful or offensive. Additionally, our benchmark does not feature any proprietary data, as all charts were sourced from publicly available online platforms. We plan to release the dataset only for research purposes. The question-answer (QA) generation process was carried out exclusively by the authors, all of whom are researchers with expertise in chart understanding. While large vision-language models (LVLMs) were used as assistance tools in the QA expansion process, all questions and answers were manually reviewed and refined to ensure accuracy, coherence, and ethical neutrality. No external or paid annotators were involved in this study. Instead, all individuals who contributed to dataset annotation were granted co-authorship to recognize their contributions. All annotators were informed that their annotations would be included in the dataset released for research purposes. Finally, AI writing assistants were used to refine the writing and enhance the papers presentation."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to thank the anonymous reviewers for their helpful feedback. This research was supported by the Natural Sciences Engineering Research Council (NSERC) of Canada and Canada Foundation for Innovation (CFI). Additionally, it received support through Google Cloud Platform (GCP) credits award from Googles PaliGemma Academic Program."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Mubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2023a. Reading and reasoning over chart images for evidence-based automated fact-checking. In Findings of the Association for Computational Linguistics: EACL 2023, pages 399414, Dubrovnik, Croatia. Association for Computational Linguistics. Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sahar Tahmasebi, Oana Cocarascu, and Elena Simperl. 2023b. Chartcheck: An evidence-based factchecking dataset over real-world chart images. arXiv preprint arXiv:2311.07453. Anthropic. 2024. Introducing the next generation of claude. Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusiñol, Ernest Valveny, C. V. Jawahar, and Dimosthenis Karatzas. 2019. Scene text visual question answering. Preprint, arXiv:1905.13648. R. Chaudhry, S. Shekhar, U. Gupta, P. Maneriker, P. Bansal, and A. Joshi. 2020. Leaf-qa: Locate, encode attend for figure question answering. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 35013510. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Preprint, arXiv:2211.12588. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, and 23 others. 2025. Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling. Preprint, arXiv:2412.05271. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations. Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, Andrea Tacchetti, and 1117 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Enamul Hoque and Maneesh Agrawala. 2019. Searching the visual style and structure of d3 visualizations. In IEEE Transactions on Visualization and Computer Graphics (Proc IEEE InfoVis 2019), volume 26, pages 12361245. IEEE. Enamul Hoque, Parsa Kavehzadeh, and Ahmed Masry. 2022. Chart question answering: State of the art and future directions. Preprint, arXiv:2205.03966. Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024. DataNarrative: Automated data-driven storytelling with visualizations and texts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1925319286, Miami, Florida, USA. Association for Computational Linguistics. Samira Ebrahimi Kahou, Adam Atkinson, Vincent Michalski, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017. Figureqa: An annotated figure dataset for visual reasoning. CoRR, abs/1710.07300. Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty. 2022a. Opencqa: Open-ended question answering with charts. arXiv preprint arXiv:2210.06628. Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. 2022b. Chart-to-text: largescale benchmark for chart summarization. Preprint, arXiv:2203.06486. Dae Hyun Kim, Enamul Hoque, and Maneesh Agrawala. 2020. Answering questions about charts and generating visual explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 113. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. 2024. Building and better understanding vision-language models: insights and future directions. Preprint, arXiv:2408.12637. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural embedding alignment for multimodal large language model. Preprint, arXiv:2405.20797. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263 2279, Dublin, Ireland. Association for Computational Linguistics. Ahmed Masry and Enamul Hoque. 2021. Integrating image data extraction and table parsing methods for chart question answering. Chart Question Answering Workshop, in conjunction with the Conference on Computer Vision and Pattern Recognition (CVPR), pages 15. Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. Unichart: universal vision-language pretrained model for Preprint, chart comprehension and reasoning. arXiv:2305.14761. Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-André Noël, Sathwik Tejaswi Madhusudhan, Marco Pedersoli, Bang Liu, Nicolas Chapados, Yoshua Bengio, Enamul Hoque, Christopher Pal, Issam H. Laradji, David Vazquez, and 3 others. 2025. Alignvlm: Bridging vision and language latent spaces for multimodal understanding. Preprint, arXiv:2502.01341. Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024a. Chartinstruct: Instruction tuning for chart comprehension and reasoning. Preprint, arXiv:2403.09028. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. 2024b. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. Preprint, arXiv:2407.04172. Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, and 262 others. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. OWID. 2024. Our world in data. Pew. 2024. Pew research center. PPIC. 2024. Public policy institute of california (ppic). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Preprint, arXiv:2103.00020. Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. Coqa: conversational question answering challenge. Preprint, arXiv:1808.07042. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, François Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-André Noël, Mats Leon Richter, Saverio Vadacchino, Shubbam Agarwal, and 24 others. 2024. Bigdocs: An open and permissively-licensed dataset for training multimodal models on document and code tasks. Preprint, arXiv:2412.04626. Hrituraj Singh and Sumit Shekhar. 2020. STL-CQA: Structure-based transformers with localization and encoding for chart question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 32753284, Online. Association for Computational Linguistics. SmolVLM. 2024. Smolvlm - small yet mighty vision language model. Statista. 2024. Statista. Tableau. 2024. Tableau public. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. 2024b. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Preprint, arXiv:2406.18521. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. 2024a. Janus: Decoupling visual encoding for unified multimodal understanding and generation. Preprint, arXiv:2410.13848. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, and 8 others. 2024b. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. Preprint, arXiv:2412.10302."
        },
        {
            "title": "A Appendices",
            "content": "A.1 Dataset Construction Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600. Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. 2024. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning. Preprint, arXiv:2404.16635. In this section, we outline the sources from which we collected all the chart images. Pew. The Pew Research Center (Pew, 2024) publishes data reports on social issues, public opinion, and demographic trends, often using charts and text to tell clear data story. For our dataset, we collected subset of images from larger corpus compiled by (Islam et al., 2024). This corpus, which includes 22,760 figures (charts and other images) scraped from the Pew Research website up to March 14, 2024, provided our initial pool of images. From this pool, we selected subset and then further filtered it. We excluded simple statistical charts and basic visualizations like single bar or line charts, focusing instead on visually diverse charts covering range of topics. We further collected the paragraphs associated with these chart images. The associated paragraphs not only describe the visualized data but also offer additional context not explicitly mentioned in the charts, enhancing their interpretive value. Tableau. We used Tableau Public (Tableau, 2024) as source for our dataset. Tableau Public allows users to create and share interactive dashboards made up of data visualizations on variety of topics. We sourced the chart images for our dataset from larger corpus collected and curated by (Islam et al., 2024). Due to the complex nature of the dashboard representation, they manually curated the data, focusing on dashboards with stories presented in paginated format, where each page included both text and corresponding chart. The final Tableau corpus from (Islam et al., 2024) consists of 100 dashboards covering diverse range of topics and chart images. From this pool, we manually selected our own Tableau corpus based on specific criteria. We ensured that the selected dashboards included variety of chart images, accompanying paragraphs of reasonable length, and broad representation of topics. OWID. Our World in Data (OWID) (OWID, 2024) is non-profit online platform that provides research and data on wide range of global issues, including poverty, disease, hunger, climate change, and inequality. We sourced chart images from OWID focusing on including diverse range of complex charts, i.e., multi-series line charts and multi-column bar charts to enhance the dataset. PPIC. The Public Policy Institute of California (PPIC) (PPIC, 2024) is an independent research institute dedicated to informing public policy in California. Through data-driven research and analysis, PPIC examines wide range of policy areas, including the economy, education, environment, and governance. Similar to OWID corpus we sourced chart images that excluded simple statistical charts and basic visualizations like single bar or line charts, focusing instead on visually diverse charts covering range of topics to enhance the dataset. WebCharts. We built WebCharts corpus by leveraging prior work from efforts from ChartGemma (Masry et al., 2024b) and ChartInstruct (Masry et al., 2024a). Their chart image collection process began with seed list of 157 websites known to host charts (originally compiled by Hoque and Agrawala (2019)), then querying Google Images using terms like chart images, graphs, and visual data. This initial search yielded large number of images, which we then filtered using binary Vision Transformer (ViT) (Dosovitskiy et al., 2021) classifier to identify and isolate chart images. Any remaining non-chart images were manually removed to ensure accuracy. This process, starting with the seed list and refined through image search and classification, ultimately gave us pool of 41,000 chart images. From this larger set, we carefully selected 800 charts, prioritizing visual and topical diversity. Our final selection emphasizes high visual quality and representation across range of chart styles, formats, and subject matter. In addition, we manually curated 200 infographic charts, which serve to highlight data visualization trends aimed at storytelling and public engagement. The extensive coverage of our dataset stands in contrast to prior datasets, which often relied on limited number of sources, such as Statista (Statista, 2024) or Pew (Pew, 2024), and exhibited restricted stylistic variation. By incorporating significantly larger pool of sources, our dataset ensures broader domain coverage and richer stylistic representation, addressing critical limitations in existing chart corpora. In addition to collecting the chart images, we also gathered metadata associated with them, including the URL, alt text, and other relevant details. Finally, the careful curation process resulted in diverse collection of 1341 chart images spanning various types and styles. We provide samples from each source in Figure 6 and our different questions categories in Figure 7. A.2 Complex Visualizations Multi-chart images, infographics, and dashboards all vital data visualizations that serve different purposes. Multi-chart images combine multiple charts in single visual often for comparison or to present different aspects of dataset. Infographics integrate text, images, and charts to explain concepts or tell story, focusing on clarity and engagement rather than detailed data analysis. Dashboards organize charts, tables, and key metrics in structured layout, providing an overview of important data for quick interpretation and decision-making. Table 5 presents examples of each type for reference. A.3 Dataset Analysis A.3.1 Visual Diversity Figure 8 shows example charts from diverse topics in our CHARTQAPRO benchmark. A.3.2 Linguistic Diversity In our analysis, we first quantified the lexical diversity of each dataset by computing the Type-Token Ratio (TTR). Let denote the total number of tokens (i.e., words) extracted from dataset and the number of unique tokens. The TTR is given by TTR ="
        },
        {
            "title": "U\nT",
            "content": ". Higher TTR values indicate richer vocabulary and, consequently, greater lexical diversity. Our experiments revealed that the ChartQAPro dataset achieved TTR of 0.1516, compared to 0.1377 for ChartQA and 0.1189 for Chartxiv. To assess semantic diversity, we computed the average pairwise cosine distance between text embeddings. We obtained vector representations for each text using the Sentence-BERT model all-MiniLM-L6-v2. For given text sample i, let vi denote its embedding. The cosine distance between two embeddings vi and vj is calculated as d(vi, vj) = 1 vi vj vi vj . We then computed the overall semantic diversity as the average of these distances over all unique pairs, Davg ="
        },
        {
            "title": "2\nN (N − 1)",
            "content": "(cid:88) i<j d(vi, vj), Figure 6: Example of chart images collected from different sources and their corresponding QA pairs in CHARTQAPRO. where is the total number of text samples. higher value of Davg indicates that the texts are more semantically dispersed. ChartQAPro showed an average cosine distance of 0.8439, compared to 0.7558 for ChartQA and 0.7831 for Chartxiv. Overall, these metricslexical diversity (TTR) and semantic diversity (average pairwise cosine distance computed using Sentence-BERT all-MiniLM-L6-v2)demonstrate that the ChartQAPro dataset is linguistically more diverse than the previous benchmarks. Figure 11 illustrates these findings, showing that ChartQAPro outperforms ChartQA and Chartxiv with higher TTR and semantic diversity. A.4 Prompts for Models Evaluation To promote transparency and reproducibility, we provide the exact prompts used to evaluate our models. Table 6 presents the prompts for the Direct Question Answering setup, Table 7 details those for the Chain-of-Thought setup, and Table 8 outlines the prompts for the Program-of-Thought setup. A.5 Evaluation Metric We evaluate ChartQA model predictions using relaxed correctness metric that handles numeric, textual, and list-based responses through three cases: 1. MCQ & Fact Checking Answers: We use exact match to evaluate these two types of questions. 2. Numeric Answers: For numeric answers (excluding years), small relative error is allowed. Let and denote the target and predicted numbers, respectively. The relative error is defined as = t ."
        },
        {
            "title": "The prediction is deemed correct if",
            "content": "E ϵ, with ϵ = 0.05. 3. Year Answers: For answers representing years, an exact match is required to prevent false positives (e.g., 2009 and 2010 would otherwise yield an error rate below 0.05). A.7 Performance Comparison with Previous"
        },
        {
            "title": "Benchmarks",
            "content": "4. Textual Answers: For non-numeric textual answers, we use the Average Normalized Levenshtein Similarity (ANLS) metric (Biten et al., 2019) rather than strict matching. single targetprediction pair is evaluated by Table 10 compares the performance of Claude the top-performing model, on Sonnet 3.5, CHARTQAPRO against its results on two prior chart-reasoning benchmarks: ChartQA (Masry et al., 2022) and CharXiv (Wang et al., 2024b). the function C(t, p): A.8 Error Analysis Figure 12 presents sample model errors across three categories: visual perception failures, instructionfollowing issues (CoT, PoT, direct), and mathematical reasoning mistakes. A.9 Ablations Results Figure 13 presents sample errors from open-source modelsPhi 3.5 Vision 4B (Abdin et al., 2024), Llama 3.2 Vision 11B (Grattafiori et al., 2024), and TinyChart (Zhang et al., 2024)across three categories: complex visuals, unanswerable questions, and charts with accompanying paragraphs. C(t, p) = 1, 0, ExactM (p, t), ExactM (p, t), ANLS(p, t), if question is MCQ or Fact Checking, if and are years, if and are numeric and if and are numeric and t p t 0.05, > 0.05, otherwise. (1) List-based Answers: For responses provided as lists (encoded as strings), we first parse the lists and then compute the score for each corresponding targetprediction pair. Let = [t1, t2, . . . , tN ] and = [p1, p2, . . . , pN ]."
        },
        {
            "title": "The overall score for the list is",
            "content": "Clist(T, ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 C(ti, pi). (2) Overall Evaluation: The final accuracy is computed by averaging the scores over all examples: Accuracy ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j=1 Cj. This metric tolerates minor numeric errors, enforces exact matching for years to avoid misleading correctness from near-miss values, and uses the ANLS score (Biten et al., 2019) to assign partial credit for nearly correct textual answers (e.g., Female vs. Females). We will open-source the evaluation metric code to ensure reproducibility and facilitate further research. A.6 Human Baseline Setup To approximate an upper bound on model performance, we conducted human baseline experiment. An expert in-house graduate student answered 50 randomly sampled questions from each category (Factoid, Conversational, etc.) using the exact same prompts provided to the models to ensures consistency and fairness. The resulting accuracies are reported in Table 3 under the Direct prompting setup, as Chain-of-Thought and Program-of-Thought formats do not directly apply to human responses. Multi-Chart Image"
        },
        {
            "title": "Displays key metrics for quick interpretation",
            "content": "Table 5: Examples of Multi-Chart Images, Infographics, and Dashboards, with distinct background colors for clarity. Figure 7: More examples of different question types in CHARTQAPRO. Figure 8: Examples of different charts related to major topics, i.e., Politics, Environment, Economy, Health, Technology, International Affairs etc. in CHARTQAPRO. Figure 9: Examples of VLM-assisted question-and-answer pairs, where: (a) the VLM generates question along with correct answer, marked in Green text, (b) the VLM generates question, but the answer is incorrect, marked in Red text."
        },
        {
            "title": "Factoid",
            "content": "You are given factoid question that you need to answer based on the provided image."
        },
        {
            "title": "Multi Choice",
            "content": "Your answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple answers, put them in brackets using this format [Answer1, Answer2]. Remember to generate the final answer only without any additional text! Question: <question> You are given question along with different possible answers. You need to select the correct answer from them based on the provided image. Your answer should be one of the options letters only: a, b, or (just the letter itself without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple answers, put them in brackets using this format [Answer1, Answer2]. Remember to generate the final answer only without any additional text! Question: <question>"
        },
        {
            "title": "Hypothetical",
            "content": "You are given hypothetical question that you need to answer based on the provided image."
        },
        {
            "title": "Fact Checking",
            "content": "Your answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple answers, put them in brackets using this format [Answer1, Answer2]. Remember to generate the final answer only without any additional text! Question: <question> You are given fact statement that you need to assess based on the provided image. Your answer should be either true or false (without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple answers, put them in brackets using this format [Answer1, Answer2]. Remember to generate the final answer only without any additional text! Question: <question> Conversational You are given multi-turn conversation, and your job is to answer the final question based on the conversation history and the information in the provided image. Your answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple answers, put them in brackets using this format [Answer1, Answer2]. Remember to generate the final answer only without any additional text! Conversation: <conversation> Question: <question> Table 6: Prompt Templates for Each Question Category in the Direct setup."
        },
        {
            "title": "Factoid",
            "content": "You are given factoid question that you need to answer based on the provided image. You need to think step-by-step, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. . Remember to think step-by-step and format the final answer in separate sentence like \"The answer is X\" Question: <question>"
        },
        {
            "title": "Multi Choice",
            "content": "You are given question along with different possible answers. You need to select the correct answer from them based on the provided image. You need to think step-by-step, but your final answer should be one of the options letters only: a, b, or (just the letter itself without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. . Remember to think step-by-step and format the final answer in separate sentence like \"The answer is X\" Question: <question>"
        },
        {
            "title": "Hypothetical",
            "content": "You are given hypothetical question that you need to answer based on the provided image. You need to think step-by-step, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to think step-by-step and format the final answer in separate sentence like \"The answer is X\" Question: <question>"
        },
        {
            "title": "Fact Checking",
            "content": "You are given fact statement that you need to assess based on the information in the provided image. You need to think step-by-step, but your final answer should be either true or false (without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. . Remember to think step-by-step and format the final answer in separate sentence like \"The answer is X\" Question: <question> Conversational You are given multi-turn conversation, and your job is to answer the final question based on the conversation history and the information in the provided image. You need to think step-by-step, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. . Remember to think step-by-step and format the final answer in separate sentence like \"The answer is X\" Conversation: <conversation> Question: <question> Table 7: Prompt Templates for Each Question Category under the Chain of Thought Setup"
        },
        {
            "title": "Factoid",
            "content": "You are given factoid question that you need to answer based on the provided image. You need to write an executable python code that calculates and prints the final answer, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to return python code only without any additional text. Question: <question>"
        },
        {
            "title": "Multi Choice",
            "content": "You are given question along with different possible answers. You need to select the correct answer from them based on the provided image. You need to write an executable python code that calculates and prints the final answer, but your final answer should be one of the options letters only: a, b, or (just the letter itself without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to return python code only without any additional text. Question: <question>"
        },
        {
            "title": "Hypothetical",
            "content": "You are given hypothetical question that you need to answer based on the provided image. You need to write an executable python code that calculates and prints the final answer, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to return python code only without any additional text. Question: <question>"
        },
        {
            "title": "Fact Checking",
            "content": "You are given fact statement that you need to assess based on the information in the provided image. You need to write an executable python code that calculates and prints the final answer, but your final answer should be either true or false (without any additional text). If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to return python code only without any additional text. Question: <question> Conversational You are given multi-turn conversation, and your job is to answer the final question based on the conversation history and the information in the provided image. You need to write an executable python code that calculates and prints the final answer, but your final answer should be single word, number, or phrase. If the question is unanswerable based on the information in the provided image, your answer should be unanswerable. Do not generate units. But if numerical units such as million, m, billion, B, or are required, use the exact notation shown in the chart. If there are multiple final answers, put them in brackets using this format [Answer1, Answer2]. Remember to return python code only without any additional text. Conversation: <conversation> Question: <question> Table 8: Prompt Templates for Each Question Category in the Program-of-Thought setup."
        },
        {
            "title": "Prompt Template",
            "content": "Generate some of the most difficult Factoid Questions alongside the Corresponding Answers for the given image. The questions could be related to numerical or visual reasoning. And the Answers could be number, text label, or common phrase (Yes, No). You should respond in an Array of JSON objects format with the following keys: (i) Question, and (ii) Answer. Multiple-Choice will upload some charts, graphs, infographics or other data visualizations. Generate five multiplechoice questions."
        },
        {
            "title": "Hypothetical",
            "content": "Each question should contain four options and one correct answer. Questions should require some complex calculations such as trend analysis, anomaly detection, extrapolation, or time series analysis. For the correct answer, show your calculations as well. You are an AI that generates concise and specific hypothetical questions based on chart images. Your task is to analyze the chart and generate short, data-driven hypothetical question that explores future trends, impacts, or extrapolations based on the data. Avoid adding unnecessary explanations or context like Based on the chart data. . . or meaningful hypothetical question could be. . . . Keep the question focused and directly related to the chart. The question should make an assumption about future trends, impacts, or extrapolations based on the data. Fact-Checking ### Task Description: Given chart image in the input, your task is the following: 1. Analyze the given chart image and generate 3 to 5 pairs of claims and verdicts about its data. Half of the claims should be supported by the charts data, while the other half are refuted. 2. Avoid using terms like rows, columns, or elements from the data table; refer to chart or chart image instead. If the claim is supported, the verdict should be True. If the claim is refuted, the verdict should be False, followed by brief explanation. 3. The claims should cover comparisons of values or trends, basic statistical values (maximum, minimum, mean, median, mode) without using exact numbers from the chart. 4. Ensure diverse range of claims addressing various visual aspects of the chart, resulting in 3-5 turns of claims and verdicts. 5. Generate the claims in between <claim > tags, and the verdicts/answers in between <answer > tags, without any additional explanation."
        },
        {
            "title": "Conversational",
            "content": "Show me conversational question answering for analyzing the <chart type >. Make sure this looks like proper conversation that makes references to previous questions/answers. Make sure all the questions are such that the answer is concise and all questions require arithmetic and logical reasoning. Please make sure to ask mathematical and visual reasoning questions that require multiple complex operations (e..g, sum, min, max, diff, ratio, . . . etc). Table 9: Prompt Templates for generating questions using VLMs."
        },
        {
            "title": "Description",
            "content": "Accuracy (%) ChartQA (Masry et al., 2022) Standard benchmark for chart reasoning CharXiv (Wang et al., 2024b) CHARTQAPRO (Ours) Scientific charts from arXiv, limiting diversity Diverse in chart sources, topics, styles, and question types 90.50 60.20 55.81 Table 10: Performance of Claude Sonnet 3.5 across three chart-reasoning benchmarks. The lower accuracy on CHARTQAPRO (55.81%) illustrates its increased difficulty compared to ChartQA (90.50%), highlighting the need for more robust chart understanding capabilities. Figure 10: Box plot of pairwise cosine distances among chart images. CHARTQAPRO exhibits higher median and consistently larger distances, indicating significantly greater visual diversity. Figure 11: Linguistic Diversity Comparison Across Datasets. The figure shows lexical diversity (TTR) and semantic diversity (cosine distance) for ChartQA, Chartxiv, and ChartQAPro. Higher TTR and semantic diversity indicate richer vocabulary and broader semantic coverage. ChartQAPro exhibits the highest diversity. Figure 12: Sample errors across three categories: Visual Perception, Instruction Following (CoT, PoT, Direct), and Mathematical Reasoning. Figure 13: Sample errors from open-source models across different categories in CHARTQAPRO."
        }
    ],
    "affiliations": [
        "Dialpad Inc., Canada",
        "MILA - Quebec AI Institute, Canada",
        "Nanyang Technological University, Singapore",
        "Qatar Computing Research Institute (QCRI)",
        "RBC, Canada",
        "Salesforce Research, USA",
        "York University, Canada"
    ]
}