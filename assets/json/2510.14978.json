{
    "paper_title": "Learning an Image Editing Model without Image Editing Pairs",
    "authors": [
        "Nupur Kumari",
        "Sheng-Yu Wang",
        "Nanxuan Zhao",
        "Yotam Nitzan",
        "Yuheng Li",
        "Krishna Kumar Singh",
        "Richard Zhang",
        "Eli Shechtman",
        "Jun-Yan Zhu",
        "Xun Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO."
        },
        {
            "title": "Start",
            "content": "Nupur Kumari1 Yuheng Li2 Eli Shechtman2 Sheng-Yu Wang1 Nanxuan Zhao2 Yotam Nitzan2 Krishna Kumar Singh2 Jun-Yan Zhu1 Xun Huang Richard Zhang2 5 2 0 2 6 1 ] . [ 1 8 7 9 4 1 . 0 1 5 2 : r 1Carnegie Mellon University 2Adobe"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO. 1 INTRODUCTION Large-scale text-to-image models have achieved remarkable success, generating images of high fidelity that closely align with textual descriptions Ramesh et al. (2022); Peebles & Xie (2023); Kang et al. (2023). Despite these advances, text-only conditioning offers limited user control and falls short for many downstream applications (Meng et al., 2022; Gal et al., 2023). In practice, users often wish to start with an existing image to perform tasks like adjusting local attributes, changing the style, or placing an object in new context. These image editing operations require precise, image-guided control that text-only prompts cannot provide. While collecting large-scale textimage pairs is relatively straightforward (Schuhmann et al., 2021), constructing supervised datasets for editing tasks is far more challenging. As one requires pair of images, the input and its edited counterpart, along with the text instruction, and such data is rarely available online. Early methods addressed this by synthetically generating editing pairs (Brooks et al., 2023) from pretrained model, using zero-shot editing techniques (Hertz et al., 2023). However, synthetic datasets can quickly become outdated with new and improved base models, and they risk amplifying and propagating artifacts of the synthetic editing process. More recent approaches extract frames from videos and annotate their differences (Chen et al., 2025; Song et al., 2023b; Krojer et al., 2024). Although promising, the applicability of this strategy is constrained by the diversity of transformations present in natural video sequences, where obtaining pixel-aligned beforeandafter edited pairs is nearly impossible. final alternative is to manually create training pairs (Winter et al., 2024; Magar et al., 2025), but this can be quite laborious and does not scale as easily."
        },
        {
            "title": "Preprint",
            "content": "In this work, we explore the possibility of training an image editing model without any training pairs. Our key idea is to leverage supervision from Vision Language Models (VLMs) (Liu et al., 2023a), relying on their general image-understanding capabilities to check whether the generated images satisfy the editing instructions. Prior works have studied the use of specialized models or generalpurpose VLMs in improving generative models along dimensions such as text-alignment and aesthetic quality, primarily using reinforcement learning (Black et al., 2024; Liu et al., 2025a). In contrast, our method is the first to explore using gradient feedback from VLMs for general instruction-following, and we distill this feedback into lightweight generative model that can generalize to arbitrary images and edit instructions. Our final method combines the VLM-feedback with distribution matching loss to ensure that generated outputs remain in the realistic image domain while following the edit instructions. In summary, our contributions are threefold: 1. We propose NP-Edit (No-Pair Edit), framework for training image editing models using gradient feedback from VisionLanguage Model (VLM), requiring no paired supervision. 2. For efficient training and effective VLM feedback, our formulation combines it with distribution matching loss to learn few-step image editing model. The final model remains competitive with existing baselines trained on supervised data. 3. We conduct comprehensive empirical study analyzing the impact of (i) different VLM backbones, (ii) dataset scale and diversity, and (iii) VLM loss formulation. Our findings show that performance improves directly with more powerful VLMs and larger datasets, demonstrating its strong potential and scalability."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Diffusion-based image editing. Development of large-scale text-to-image models has enabled wide range of downstream applications, including local image editing (Hertz et al., 2023; Meng et al., 2022), stylization (Sohn et al., 2023; Hertz et al., 2024; Jones et al., 2024), personalization and customization (Gal et al., 2023; Ruiz et al., 2023). These can broadly be viewed as different forms of image-editing capabilities. Early approaches often relied on zero-shot inference-time methods (Hertz et al., 2023; Parmar et al., 2023; Cao et al., 2023; Avrahami et al., 2023; Kim et al., 2023) or flexible but slow optimization-based techniques (Gal et al., 2023; Ruiz et al., 2023; Kumari et al., 2023). To improve efficiency and robustness, subsequent works introduced training-based approaches (Brooks et al., 2023; Xiao et al., 2025; Chen et al., 2023; Fu et al., 2024; Sun et al., 2024). However, obtaining large datasets of image pairs remains challenging: synthetic curation (Brooks et al., 2023; Zhang et al., 2023; Zhao et al., 2024; Hui et al., 2024; Yang et al., 2024b; Tan et al., 2025; Cai et al., 2025; Kumari et al., 2025) risks becoming outdated as generative models improve, while human annotation (Winter et al., 2024; Magar et al., 2025; Ge et al., 2024; Sushko et al., 2025) is costly and labor-intensive. Recent efforts have explored constructing paired data from videos (Chen et al., 2025; Song et al., 2023b) or simulation environments (Yu et al., 2025), although these remain limited in either annotation diversity or visual realism. We also target similar image-editing capabilities but remove the need for paired data (Zhu et al., 2017), by using differentiable feedback from visionlanguage models instead of ground-truth edits. Post-training for image generation. Post-training methods typically align image generators with human preferences using either Direct Preference Optimization (DPO) (Wallace et al., 2024; Yang et al., 2024a) or Reinforcement Learning (RL) (Black et al., 2024). While early RL-based works use feedback from simple scalar reward model (Kirstain et al., 2023; Xu et al., 2023), the paradigm has recently been enhanced by employing sophisticated Vision-Language Models (VLMs) as judges to provide more generic and accurate reward signals (Ku et al., 2024). Although post-training has been successfully applied to text-to-image generation, its use for image editing models has been less explored. Concurrently to our work, EARL (Ahmadi et al., 2025) begins to address this by using VLM-as-a-judge framework to post-train an image-editing model with RL. However, RL-based approaches often depend heavily on good initialization, typically requiring Supervised Fine-Tuning (SFT) phase with paired editing data. In contrast, our method leverages differentiable feedback from the VLM model, thereby obviating the need for an initial SFT stage and enables the learning of image editing models without the use of synthetically generated data."
        },
        {
            "title": "Preprint",
            "content": "Related to our work, Luo et al. (2025) recently introduced method that incorporates gradient feedback from VLMs to satisfy various criteria, including the horizon line, style, and layout, in generated images. However, their framework operates in per-example optimization setting, requiring costly LoRA fine-tuning (Hu et al., 2022) for each criterion and prompt pair, and also does not consider image editing tasks. Few-step diffusion models. Standard diffusion (or flow-matching) models require many sampling steps to generate high-quality images. Many prior works reduce the number of denoising steps for faster sampling by predicting larger denoising steps, including consistency models (Kim et al., 2024; Geng et al., 2024; Song et al., 2023a; Yang et al., 2024c; Song & Dhariwal, 2024; Lu & Song, 2025; Heek et al., 2024), shortcut models (Frans et al., 2024), meanflow (Geng et al., 2025), and inductive moment matching (Zhou et al., 2025). Another line distills pre-trained multi-step teacher into few-step student by matching ODE trajectories (Song et al., 2023a; Salimans & Ho, 2022; Geng et al., 2023), using adversarial loss (Sauer et al., 2024b; Kang et al., 2024; Yin et al., 2024a; Sauer et al., 2024a; Xu et al., 2024), or applying score distillation (Luo et al., 2023; Yin et al., 2024b;a; Zhou et al., 2024). In our framework, we adopt DMD (Yin et al., 2024b) as distribution matching objective. This ensures that our few-step editing models output remains in the real-image manifold defined by the pre-trained text-to-image teacher, while using VLM-feedback to ensure the model follows the editing instructions."
        },
        {
            "title": "3 BACKGROUND",
            "content": "3.1 DIFFUSION MODELS Diffusion or flow-based models are class of generating models that learn the data distribution by denoising samples corrupted by different levels of Gaussian Noise (Ho et al., 2020; Song et al., 2021; Lipman et al., 2023). Given real sample x, forward diffusion process creates noisy samples xt = αtx + σtϵ over time (0, 1], where ϵ (0, I), and αt, σt define noise schedule such that xT (0, I) and x0 = x. The denoising model is parameterized to reverse the forward diffusion process by either predicting the noise ϵ (Ho et al., 2020) added to the sample or velocity towards the clean sample (Liu et al., 2023b; Salimans & Ho, 2022). In our work, we follow the flow-based formulation, with the forward denoising process being linear interpolation, i.e., αt = 1 and σt = t. The training objective for flow-based model, with parameters θ, can be simplified to the following: Ext,t,c,ϵN (0,I)wtv vθ(xt, t, c), (1) where = ϵ and wt is time dependent weighting factor. The denoising network can be conditioned on other inputs c, such as text prompt, reference image, or both, as in our case. 3.2 VISION LANGUAGE MODELS (VLMS) Vision Language Models (VLMs) trained from multimodal image-text data have shown exemplary visual understanding and reasoning capabilities and can serve as general-purpose visual model. common strategy for training such large-scale VLMs is via visual instruction tuning (Liu et al., 2023a), which aligns pre-trained Vision Encoder output with the input word embedding space of pretrained Large Language Model (LLM). More specifically, the image is encoded into set of tokens using the vision encoder, Xv = g(x). The input question regarding the image and its ground truth answer are tokenized in the LLM input embedding space as Xq and Xa, respectively. projector module, fϕ, projects the vision-encoded tokens into the LLM word embedding space and is trained via standard autoregressive loss to maximize the probability of predicting the correct answer: p(XaXv, Xq) = (cid:16) aifϕ(Xv), Xq, Xa<i (cid:17) , (cid:89) i= (2) where Xa = [a1 aL] is of token length L, and Xa<i denotes all the tokens before the current prediction token index. The final loss simplifies to cross-entropy over the total vocabulary length. In our experiments, we use LLaVa-OneVision-7B (Li et al., 2024) as the VLM that uses SigLIP (Zhai et al., 2023) vision encoder and Qwen-2 LLM (Qwen-Team, 2024), and is among the state-of-the-art VLMs of this scale."
        },
        {
            "title": "4 METHOD",
            "content": "Given pretrained text-to-image diffusion model Ginit and dataset = {(yi, ci, cy i=1 of reference image y, corresponding edit instruction c, and captions cy and cx that describe the reference and edited image respectively, we fine-tune Ginit into few-step image editing model Gθ without requiring ground truth edited image according to the edit instruction. Our approach, No-Pair (NP)-Edit, introduces VLM-based loss to evaluate edit success and combines it with distribution matching loss to ensure outputs remain within the natural image domain. Below, we first detail the construction of the dataset, then our training objective, and finally other implementation details. )}N , cx"
        },
        {
            "title": "4.1 EDIT INSTRUCTION DATASET",
            "content": "Each dataset sample consists of real image as reference and an associated edit instruction, c. Following prior works (Liu et al., 2025b; Ye et al., 2025), we focus on several categories of local editing operations, such as Add, Replace, Remove, Adjust shape, Action, Stylization, Text editing, Color, Material, and Background change, as well as more free-form editing tasks such as Customization or Personalization (Gal et al., 2023; Ruiz et al., 2023; Kumari et al., 2023). Candidate instructions for each type are generated using Qwen2.5-32B VLM (Qwen-Team, 2025). Given an image-instruction pair, we further query the VLM to assess its validity and to suggest the caption, cx, for the edited image. For the customization task, we restrict reference images to those showing prominent central object, either filtered from real image corpus or generated via the pretrained model (Tan et al., 2025; Kumari et al., 2025), and prompt the VLM to generate caption that places the object in novel background or context. In total, for local and free-form editing instructions, our dataset consists of 3M and 600K reference images, respectively. The input prompt to the Qwen2.5-32B VLM for each setup is shown in Appendix D. 4.2 TRAINING OBJECTIVE Training diffusion or flow-based model (Ho et al., 2020; Liu et al., 2023b) for image editing without pairs presents unique challenge. Standard diffusion training takes as input noised versions of ground-truth image. In our setting, no such ground-truth edited image exists; thus, we cannot construct these intermediate noisy inputs. On the other hand, directly mapping noise to the edited image in single step is naturally challenging and yields poor fidelity (see Appendix B). To address this, during training, we propose to unroll the backward diffusion trajectory starting from noise using two-step sampling procedure (Song et al., 2023a). Specifically, given the reference imageinstruction pair (y, c), the editing model Gθ first predicts provisional clean image ˆx0 θ from noise ϵ. Then, second step refines this estimate by feeding an interpolated noisy input back into the model: ˆx0 θ = ϵ ˆvθ, x0 θ = ˆxt θ tvθ, where vθ Gθ(ˆxt θ, t, c, y), θ + tϵ; ϵ (0, I), (0, 1). where ˆvθ Gθ(ϵ, = 1, c, y), ϵ (0, I), θ = (1 t)ˆx0 ˆxt (3) With the second step, the model is now trained on noisy intermediate states at timesteps determined by t, while being more efficient than full backward unroll. In our method, we focus on few-step generationspecifically four stepsand restrict [0.25, 0.5, 0.75] in the second step. Few-step generator provides better estimate of the denoised image, x0 θ, at intermediate steps, which in turn enables effective VLM-based feedback. VLMs tend to give unreliable judgments when inputs are noisy or blurry (see Appendix B). This also enables faster inference and lowers training costs. VLM-based editing loss. To evaluate whether an edit is successfully applied in x0 θ, we define set of template questions with corresponding ground truth answers, DQA = {(Xqj , Xaj )}j, tailored to each edit category. The VLM is instructed to answer with binary yes and no answer, i.e., Xaj {Yes, No}, and Xaj denotes the opposite response. The loss is then binary cross-entropy over the predicted logit difference between the tokens corresponding to the correct and opposite responses, respectively. LVLM = (cid:88) log p(aj), where p(aj) = σ(ℓ(j) aj ℓ(j) aj ) (4)"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Method. We fine-tune pretrained text-to-image model into few-step image-editing model using differentiable VLM-feedback regarding edit success. In addition, we use distribution matching loss (DMD (Yin et al., 2024a)) to ensure output images remain in the natural image manifold. where ℓ(j) aj is the logit corresponding to the token Xaj , σ is the sigmoid function, and p(aj) is the probability of correct answer, while restricting normalization to only the Yes and No tokens, which we observe to be more effective during training (Zhang et al., 2024). Computing this loss is relatively fast, as it only requires single forward call to the VLM per question, as opposed to autoregressive token prediction. For each edit instruction, we use two complementary questions to compute the editing loss: (1) Edit-verification question to assess whether the intended edit is applied, and (2) Identity-preservation question to ensure the image is not over-edited and is consistent with the reference image. Specifically, for the local image-editing instructions, we verify edit success with the following question The objective is to evaluate if the editing instruction has been executed in the second image. Editing instruction: {edit instruction}. Answer with Yes or No. except removal edit-type, where we directly evaluate if the intended object is removed by asking Answer with Yes or No if the image has {object name}. For the identity-preservation question, we ask the following: Answer with Yes or No if the second image is exactly the same as the first image. IGNORE the changes in the second image because of the edit: {edit instruction}. We provide the list of all questions along with their system and user prompts for all editing types, including free-form editing in Appendix E. Distribution matching with text-to-image teacher model. While VLM feedback evaluates the efficacy of instruction following, it does not enforce the generated outputs to remain in the real image domain. To ensure this and keep the output distribution of the generator aligned with the pre-trained model, we apply Distribution Matching Distillation (DMD) (Yin et al., 2024b;a) between the fine-tuned model, Gθ, and the pre-trained text-to-image (teacher) model, Ginit. DMD minimizes the KullbackLeibler (KL) divergence between the real image distribution, as estimated by the teacher model, and the output distribution of the fine-tuned model. The gradient of this KL-divergence loss with respect to the generator parameters can be simplified to: θDKL = ϵN (0,I),t(0,1),x0 θ (cid:104) (vreal(xt θ, t, cx) vgen(xt θ, t, cx)) (cid:105) , dG dθ (5) where cx is the text caption describing the noisy edited image xt θ and vreal, vgen represents the predicted velocity from the teacher and trainable auxiliary model, Aϕ respectively. The auxiliary model is trained along with Gθ to learn the current output distribution of Gθ using flow-based denoising objective. This loss ensures that the edited images not only satisfy the instruction but also remain faithful to the text conditioned distribution of real images modeled by the pretrained teacher. 4.3 TRAINING DETAILS The pretrained model Gθ is originally designed to generate an image, x, conditioned only on text c. To adapt it to our editing task, we extend its conditioning to include the reference image y. Following recent works (Xiao et al., 2025; Tan et al., 2025), we concatenate the VAE encoding of the reference image to the noisy target image encoding along the token sequence dimension, similar to text embedding, thereby enabling the model to attend to both text and visual conditions. To stabilize training, in the initial few iterations, we train the model with the objective of simply reconstructing the concatenated reference image. This encourages the network to propagate content"
        },
        {
            "title": "Preprint",
            "content": "from the reference input, aligning it toward producing realistic images under joint textimage conditioning. After this, we introduce our main training objective as explained in the previous section. The final loss for the generator is weighted combination of the VLM-based editing loss and DMD loss. The auxiliary network, Aϕ, is updated Naux times for every generator, Gθ, update (Yin et al., 2024a). Our pre-trained generative model is 2B parameter internal DiT-based (Peebles & Xie, 2023) latent space diffusion model. The overall training pipeline is illustrated in Figure 1 and is detailed more formally in Algorithm 1 below. Other training hyperparameters are detailed in Appendix E. Algorithm 1: NP-Edit: our training method Input: Pretrained VLM and text-to-image model Ginit, Dataset = {(yi, ci, cy Output: Few-step image-editing model Gθ , cx )}. 1 Gθ copyWeights(Ginit); Aϕ copyWeights(Ginit) 2 // Warmup with identity loss 3 for step = 1 to Nwarmup do (y, cy) , ϵ (0, I), (0, 1] xt (1 t)x + tϵ vθ Gθ(xt, t, y, cy) Lid vθ where = ϵ θG θG ηGθG Lid. 9 10 end for 11 12 // Main training loop 13 while train do 14 {(y, c, cx)} , ϵ (0, I), [0.25, 0.5, 0.75, 1] vθ Gθ(ϵ, = 1, y, c) x0 θ ϵ vθ if < 1 then θ tvθ θ (1 t)x0 θ, t, y, c); xt vθ Gθ(xt x0 θ xt end if Compute LVLM // Eqn. 4 Compute θDKL // Eqn. 5 θG θG ηGλvlmθG LVLM ηGλdmdθDKL for local step = 1 to Naux do 5 6 7 8 16 17 18 19 20 22 23 24 25 26 28 29 ϵ (0, I), {(y, c, cx)} x0 θ Gθ(ϵ, y, c) // edited image with backward unroll xt θ (1 t)x0 vϕ Aϕ(xt ϕA ϕA ηAθv vϕ where = ϵ x0 θ ϵ (0, I) (0, 1) θ, cx) θ + tϵ end for 30 31 end while θ + tϵ ; ϵ (0, I)"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we show the results of our method on local image editing as well as more free-form image editing tasks like customization, and compare them with the state-of-the-art baseline methods. 5.1 LOCAL IMAGE-EDITING Benchmark. For evaluation, following prior works, we use the English subset of GEditBenchmark Liu et al. (2025b), which captures real-world user interactions across different edit types. We also show results on the ImgEdit (Ye et al., 2025) benchmark in Appendix A. Evaluation metric. For quantitative evaluation, we follow prior works and use GPT4o-based VIEScore (Ku et al., 2024) metric. It scores each edit on: (1) Semantic Consistency (SC) score,"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Qualitative comparison on GEdit-Bench under the few-step sampling setting. For an upper-bound comparison, in the 1st column we show results of the best multi-step sampling method (as measured by the quantitative metrics in Table 1). Our method performs on par or better than baseline methods across different edit types in the few-step setting. We show more samples in the Appendix Figure"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Quantitative evaluation on GEdit-Bench. Our method performs on par or better than baselines under the few-step setting. For multi-step sampling, it still outperforms OmiGen and remains competitive with many of the larger-scale models like BAGEL and FLUX.1 Kontext. All numbers reported in 10 Method #Param #Step SC Score PQ Score Overall Omni-Gen (Xiao et al., 2025) BAGEL (Deng et al., 2025) FLUX.1-Kontext (Labs et al., 2025) Step1X-Edit (Liu et al., 2025b) v1.1 Qwen-Image-Edit (Wu et al., 2025) FLUX.1-Kontext (Labs et al., 2025) Step1X-Edit (Liu et al., 2025b) v1.1 Qwen-Image-Edit (Wu et al., 2025) Turbo-Edit (Deutch et al., 2024) NP-Edit (Ours) 4B 7B 12B 12B 20B 12B 12B 20B 1B 2B 50 50 28 28 50 4 4 4 4 4 5.52 7.02 6.29 7.30 7. 5.80 6.61 6.82 3.84 6.16 6.14 6.26 6.65 7.37 7.50 5.74 6.43 6.21 6.67 7.69 4.97 6.14 5.65 6.79 7.36 5.04 6.01 6.06 3.84 6.10 Table 2: Free-form editing task, Customization, evaluation on Dreambooth. We perform better than OminiControl, DSD, and SynCD, which are trained for this task on synthetic datasets. When compared to FLUX.1-Kontext and Qwen-Image-Edit, we still perform comparably in the few-step setting. All numbers are reported in Method #Param #Step SC Score PQ Score Overall DSD (Cai et al., 2025) SynCD (Kumari et al., 2025) FLUX.1-Kontext (Labs et al., 2025) Qwen-Image-Edit (Wu et al., 2025) OminiControl (Tan et al., 2025) DSD (Cai et al., 2025) SynCD (Kumari et al., 2025) FLUX.1-Kontext (Labs et al., 2025) Qwen-Image-Edit (Wu et al., 2025) NP-Edit (Ours) NP-Edit (Ours) 12B 12B 12B 20B 12B 12B 12B 12B 20B 2B 2B 28 30 28 50 8 8 8 8 8 8 4 6.71 7.66 8.19 8.53 6.33 6.37 7.71 7.99 8.08 7.68 7. 7.41 7.83 7.45 7.79 7.82 6.78 6.84 7.18 7.44 7.56 7.28 6.78 7.54 7.61 8.02 6.22 6.29 7.07 7.39 7.62 7.33 7.10 evaluating whether the edit instruction was followed, and (2) Perceptual Quality (PQ) score, assessing realism and absence of artifacts. Following VIEScore, for the Overall score, we take the geometric mean between SC and PQ for each image, and average across images in the evaluation benchmark. Baselines. We compare our method with leading baselines, including FLUX.1-Kontext (Labs et al., 2025), Step1X-Edit (Liu et al., 2025b), BAGEL (Deng et al., 2025), OmniGen (Xiao et al., 2025), and Qwen-Image-Edit (Wu et al., 2025). Since no prior work explicitly targets few-step editing, we simply evaluate the above baselines with few-step sampling as well as their original multi-step setting for an upper-bound comparison. We also include Turbo-Edit (Deutch et al., 2024), state-of-the-art zero-shot few-step method that requires no paired supervision (as zero-shot) and is thus closest to our setup. We use the open-source implementation of all baselines, with further details in the Appendix F. Results Table 1 shows the quantitative result. In the few-step setting, our method achieves the best Overall and Perceptual Quality (PQ) score compared to baseline methods. When compared to their original multi-step sampling, our few-step model still outperforms OmniGen and remains competitive with BAGEL, FLUX.1-Kontext, despite being 6 smaller parameter-wise. While Step1X-Edit and Qwen-Image-Edit perform better, they are substantially larger models. Figure 2 provides qualitative comparison. As we can see, our method can successfully follow different editing instructions while being consistent with the input reference image. For instance, in the 6th row (sheep color change), our approach produces more natural edit compared to baselines. It also performs comparably to the multi-step variant for edits like lighting the candle in 4rth row or making the person wave in 7th row. 5.2 FREE-FORM EDITING: CUSTOMIZATION Benchmark. We use the widely adopted DreamBooth (Ruiz et al., 2023) dataset for evaluation. It consists of 30 objects and 25 prompts per object category. The goal is to generate the same object as shown in the reference image, but in different context, as mentioned in the text prompt. Baselines. We compare against state-of-the-art unified image-editing baselines such as FLUX.1Kontext (Labs et al., 2025) and Qwen-Image-Edit Wu et al. (2025) as well as OminiControl (Tan"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Qualitative comparison on Customization task. Our method can generate the object in new contexts while having better fidelity under few-step sampling. We show more samples in the Appendix Figure 12. et al., 2025), DSD (Cai et al., 2025), and SynCD (Kumari et al., 2025), which are feed-forward models trained specifically for this task on synthetic datasets. Evaluation metric. Here as well, we use the VIEScore evaluation with similar Semantic Consistency (SC) score to evaluate identity and text alignment and the Perceptual Quality (PQ) score to measure realism, and the geometric mean of the two for the Overall score. We also report CLIPScore (Radford et al., 2021) and DINO (Oquab et al., 2023) similarity-based metrics in the Appendix. Results. As shown in Table 2, our method performs comparably to state-of-the-art methods. In the few-shot sampling setting, all the baseline methods fail to generate realistic samples at 4 steps; therefore, we compare with them at 8 sampling steps. Our method still results in higher fidelity samples as Figure 3 shows, while maintaining object identity with the reference image. Note that our method performs better than OminiControl, which is also few-step (8) model for this task. 5.3 ABLATION In this section, we perform several ablations to analyze the role of different components of our method, dataset scale, and stronger VLMs. All ablations are done on the local image-editing task. Training objective. We ablate our training objective across four settings: (1) using only distribution matching loss, (2) using only the VLM-based editing loss, (3) removing the identity-preservation loss DQA, and (4) replacing the binary-cross entropy loss (Eqn. 4) with standard cross-entropy over the full vocabulary. Results are shown below in Table 3. Training without the VLM-based loss and relying solely on distribution matching significantly degrades the models capabilities at following editing instructions. We observe that VLM-based loss is essential for maintaining consistency between input and edited images and for certain editing tasks like Removal (Figure 4 and Appendix Figure 5). However, only training with VLM-based loss leads to unrealistic outputs (Appendix Figure 6), and the training eventually diverges, as evidenced by the low overall score in Table 3, underscoring the need for DMD loss. In addition, using binary cross-entropy loss and having question to check consistency between input and edited images improves the overall performance. Dataset and VLM scale. To study the role of dataset scale, we vary the number of unique reference images in training. Our final dataset represents the maximum scale feasible under our computational resources. Table 4 shows the performance across different dataset sizes and VLM backbones. We observe consistent gains with larger datasets, suggesting that further scaling of data could yield"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Training objective ablation. We compare on the GEdit-Bench using the VIEScore metric. Ablating different components of our method leads to drop in performance, indicating its importance. Method SC Score PQ Score Overall Ours w/ only DMD w/ only VLM w/o VLM identity w/ standard CE loss 6.16 4.93 2.03 5.70 5. 7.69 7.51 3.48 7.67 7.64 6.10 4.93 1.93 5.76 5.89 Table 4: Dataset and VLM scale and comparison with Reinforcement Learning on the GEditBench. Increasing dataset scale and using stronger VLMs leads to increased performance. Our method also performs better than post-training an SFT model with RL (Liu et al., 2025a). Method SC Score PQ Score Overall 1 % Dataset 50 % Dataset 100 % Dataset InternVL-2B InternVL-14B LLava-0.5B LLava-7B SFT SFT + RL SFT + Ours 4.41 5.41 6.16 5.36 5.88 4.57 6.16 3.91 4.55 6. 7.10 7.73 7.69 7.67 7.74 7.50 7.69 5.70 5.47 7.83 4.66 5.52 6.10 5.45 5.89 4.59 6.10 3.64 4.19 6. Figure 4: Qualitative analysis of ablation experiments. Our method maintains better input and edited image alignment compared to only training with DMD loss, which also fails on tasks like removal. Compared to fine-tuning an SFT model with RL, our method results in better fidelity while following the edit instruction. Please zoom in for details. additional improvements. Similarly, larger parameter VLM-backbone leads to better performance, across different VLMs such as InternVL (Chen et al., 2024) and LLava-OneVision (Li et al., 2024), underscoring the promise that our method can improve as more powerful VLMs are developed. Our method vs. Reinforcement Learning (RL). RL is common post-training strategy for improving pre-trained models without paired supervision and can also leverage VLMs as the reward model, similar setup to ours. Thus, we benchmark our method against Flow-GRPO (Liu et al., 2025a), widely used RL method for text-to-image diffusion. However, since RL relies on reasonable initialization, we need to first train an image-editing model via Supervised Fine-Tuning (SFT) on paired dataset (Lin et al., 2025). We then fine-tune it with Flow-GRPO using the same LlavaOneVision reward model as in our approach. As shown in Table 4, SFT alone performs poorly, likely due to the limited quality of paired data. Our method surpasses both SFT and SFT+RL, despite requiring no paired supervision. Fine-tuning the model with some paired data before applying our approach can slightly improve the pixel-level consistency between the input reference and output edited image (as shown in Figure 4), although the quantitative numbers are similar. The Appendix provides additional results and more detailed discussion of the methods limitations."
        },
        {
            "title": "6 DISCUSSION AND LIMITATIONS",
            "content": "This paper introduces new paradigm for enabling image editing capabilities given pre-trained textto-image diffusion model, without paired before-and-after edit supervision. Our approach combines differentiable feedback from VLMs to ensure editing success with distribution matching objective to maintain visual realism. This method achieves competitive performance with recent state-of-the-art baselines trained on paired data while enabling efficient few-step generation. Despite these promising results, our method has limitations. Without pixel-level supervision, edits may deviate from the input image in fine-grained details or fail to fully preserve subject identity. We show in Appendix that adding perceptual similarity loss (e.g., LPIPS (Zhang et al., 2018)) between input and edited images alleviates this to some extent, though often at the cost of editing quality. Another constraint for our method is the need to keep the VLM in GPU memory, introducing VRAM overhead. We expect ongoing advances in stronger and more efficient VLMs can help address this issue. Overall, our framework scales effectively with large unpaired datasets and highlights path toward more flexible post-training of generative models for diverse downstream tasks."
        },
        {
            "title": "Preprint",
            "content": "Acknowledgment. We thank Gaurav Parmar, Maxwell Jones, and Ruihan Gao for their feedback and helpful discussions. This work was partly done while Nupur Kumari was interning at Adobe Research. The project was partly supported by Adobe Inc., the Packard Fellowship, the IITP grant funded by the Korean Government (MSIT) (No. RS-2024-00457882, National AI Research Lab Project), NSF IIS-2239076, and NSF ISS-2403303."
        },
        {
            "title": "REFERENCES",
            "content": "Kingma DP Ba Adam et al. method for stochastic optimization. arXiv preprint arXiv:1412.6980, 1412(6), 2014. Saba Ahmadi, Rabiul Awal, Ankur Sikarwar, Amirhossein Kazemnejad, Ge Ya Luo, Juan Rodriguez, Sai Rajeswar, Siva Reddy, Christopher Pal, Benno Krojer, et al. The promise of rl for autoregressive image editing. arXiv preprint arXiv:2508.01119, 2025. Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4):111, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In International Conference on Learning Representations (ICLR), 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Shengqu Cai, Eric Ryan Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon. Wetzstein. Diffusion self-distillation for zero-shot customized image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In IEEE International Conference on Computer Vision (ICCV), 2023. George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. In Conference on Neural Information Processing Systems (NeurIPS), 2023. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. In SIGGRAPH Asia Conference Proceedings, 2024."
        },
        {
            "title": "Preprint",
            "content": "Pierre Fernandez, Guillaume Couairon, Herve Jegou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In IEEE International Conference on Computer Vision (ICCV), 2023. Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. In International Conference on Learning Representations (ICLR), 2024. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), 2023. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. Zhengyang Geng, Ashwini Pokle, and Zico Kolter. One-step diffusion distillation via deep equilibrium models. In Conference on Neural Information Processing Systems (NeurIPS), 2023. Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control. In International Conference on Learning Representations (ICLR), 2023. Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, and Jun-Yan Zhu. Customizing text-toimage models with single image pair. In SIGGRAPH Asia Conference Proceedings, 2024. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In European Conference on Computer Vision (ECCV), 2024."
        },
        {
            "title": "Preprint",
            "content": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In International Conference on Learning Representations (ICLR), 2024. Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In IEEE International Conference on Computer Vision (ICCV), 2023. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Chris Pal, and Siva Reddy. Learning action and reasoning-centric image editing from videos and simulation. Advances in Neural Information Processing Systems, 37:3803538078, 2024. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Association for Computational Linguistics (ACL), 2024. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi. Generating multi-image synthetic data for text-to-image customization. In IEEE International Conference on Computer Vision (ICCV), 2025. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations (ICLR), 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Conference on Neural Information Processing Systems (NeurIPS), 2023a. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. In Conference on Neural Information Processing Systems (NeurIPS), 2025a. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations (ICLR), 2023b. Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. In International Conference on Learning Representations (ICLR), 2025."
        },
        {
            "title": "Preprint",
            "content": "Grace Luo, Jonathan Granskog, Aleksander Holynski, and Trevor Darrell. Dual-process image generation. In IEEE International Conference on Computer Vision (ICCV), 2025. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, and Yedid Hoshen. Lightlab: Controlling light sources in images with diffusion models. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 111, 2025. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations (ICLR), 2022. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In Transactions on Machine Learning Research (TMLR), 2023. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision (ICCV), 2023. Qwen-Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qwen-Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2. 5-vl/. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia Conference Proceedings, 2024a. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision (ECCV), 2024b. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. In Conference on Neural Information Processing Systems (NeurIPS), 2023."
        },
        {
            "title": "Preprint",
            "content": "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021. Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In International Conference on Learning Representations (ICLR), 2024. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, 2023a. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Object compositing with diffusion model. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023b. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, and Ranjay Krishna. Realedit: Reddit edits as large-scale empirical dataset for image transformations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1340313413, 2025. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In IEEE International Conference on Computer Vision (ICCV), 2025. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. In European Conference on Computer Vision (ECCV), 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/2508. 02324. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023."
        },
        {
            "title": "Preprint",
            "content": "Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024b. Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024c. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. In Conference on Neural Information Processing Systems (NeurIPS), 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024b. Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE International Conference on Computer Vision (ICCV), 2023. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017."
        },
        {
            "title": "Appendix",
            "content": "A Additional Comparison with Baseline Methods Ablation Study Limitation Dataset Construction Details Training Implementation Details E.1 Local-image editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Free-form editing (Customization) . . . . . . . . . . . . . . . . . . . . . . . . . . Other Baseline Details Societal Impact 17 17 19 20 23 24"
        },
        {
            "title": "A ADDITIONAL COMPARISON WITH BASELINE METHODS",
            "content": "Local image-editing. Here, we compare on another commonly adopted image-editing benchmark, ImgEdit (Ye et al., 2025) (Basic), which covers nine local editing types across diverse semantic categories with total of 734 samples. Quantitative results under their proposed GPT4o-based evaluation protocol are reported in Table 5, with VIEScore (Ku et al., 2024) results in Table 6. Consistent with the trend observed on GEdit-Bench, our method has better or on-par performance in the few-step setting and remains comparable to many of the multi-step baselines as well. Qualitative comparisons are shown in Figure 11, with additional examples on GEdit-Bench in Figure 10. Customization or free-form editing. Here, we report additional metrics commonly used for evaluation. Specifically, CLIPScore (Radford et al., 2021) and TIFA (Hu et al., 2023) to measure text alignment; and similarity in DINOv2 (Oquab et al., 2023) feature space after background masking to measure identity alignment, denoted as MDINOv2-I. We also report an overall Geometric score (Yan et al., 2023) by taking the geometric mean of TIFA and MDINOv2-I. The results are shown in Table 7. Consistent with the VIEScore evaluation reported in the main paper, our method performs comparably with other baselines in the few-step sampling regime. We show more sample comparisons in Figure 12. Table 5: ImgEdit-Bench comparison using their proposed GPT-4o based evaluation protocal and few-step sampling setting. Our method outperforms baseline methods on the Avg of all edit types. Method Extract Remove Compose Avg Style Adjust Replace Add #Param Action Bg Qwen-Image-Edit Flux.1-Kontext Step1X Edit NP-Edit (Ours) 20B 12B 12B 2B 3.14 3.51 3.66 4.44 2.83 2.97 2.60 4.13 3.70 3.89 3.46 4. 3.25 3.04 3.44 3.94 3.00 3.15 2.50 3.57 3.52 3.31 3.25 4.52 1.96 1.82 1.77 2.01 2.71 2.37 2.41 2.71 3.06 2.46 2.38 3. 3.02 2.95 2.83 3."
        },
        {
            "title": "B ABLATION STUDY",
            "content": "Training objective. Here, we provide more detailed analysis by examining performance across different editing sub-types. As reminder, we ablated our training objective under four settings: (1) using only the distribution matching loss, (2) using only the VLM-based editing loss, (3) removing the identity-preservation question from DQA, and (4) replacing the binary-cross entropy loss as explained in Eqn. 4 with standard cross-entropy over full vocabulary length. As shown in Figure 5, training with only the DMD loss yields comparable performance on certain sub-edit types such as Color change, since DMD matches the text-conditioned score between the fine-tuned and pre-trained models, thus improving overall text alignment. However, it fails on tasks like Removal, underscoring"
        },
        {
            "title": "Preprint",
            "content": "Table 6: VIEScore evaluation on ImgEdit-Bench. Our method performs on par or better than baselines under the few-step setting. For multi-step sampling, it still outperforms OmniGen and remains competitive with many of the larger-scale models like BAGEL and FLUX.1 Kontext. All numbers reported in 10 Method #Param #Step SC Score PQ Score Overall BAGEL FLUX.1-Kontext Step-1X Edit v1.1 QwenImage Edit QwenImage Edit FLUX.1-Kontext Step-1X Edit v1.1 NP-Edit (Ours) 7B 12B 12B 20B 20B 12B 12B 2B 50 28 28 50 4 4 4 4 7.55 6.94 7.26 8. 6.23 6.08 6.00 6.72 6.22 6.73 7.30 7.77 5.14 5.22 5.37 7.78 6.47 6.19 6.72 7.85 5.46 5.14 5.14 6.62 Table 7: Quantitative evaluation of free-form editing task, Customization, on DreamBooth dataset. All numbers reported in Method #Param #Step MDINO Score CLIP Score TIFA Geometric Score DSD SynCD FLUX.1-Kontext Qwen-Image-Edit OminiControl DSD SynCD FLUX.1-Kontext Qwen-Image-Edit NP-Edit (Ours) NP-Edit (Ours) 12B 12B 12B 20B 12B 12B 12B 12B 20B 2B 2B 28 30 28 50 8 8 8 8 8 8 4 6.55 7.34 7.72 7.47 6.16 5.88 7.11 7.50 7.29 6.82 7.03 3.08 3.09 3.07 3. 3.02 3.15 3.16 3.08 3.08 2.97 3.04 8.71 8.53 8.88 9.37 8.12 8.93 9.11 8.83 8.96 8.73 8.89 7.32 7.71 8.14 8.22 6.64 6.99 7.79 7.98 7.91 7.54 7.72 Figure 5: Performance for each sub edit-type. Training with only DMD loss fails to achieve certain tasks like removal and style changes. In addition, using binary cross-entropy loss and VLM identity-based questions helps improve the overall performance. Figure 6: Training with only VLM-editing loss leads to lower fidelity samples with the model only maximizing the edit success probability. Current generalpurpose VLMs are often not good at subjective tasks like evaluating image fidelity, highlighting the requirement of distribution matching loss in our framework. the importance of VLM-based editing loss and its generalizability across diverse editing instructions. In addition, VLM-based loss also helps in maintaining better consistency between input and edited images (first row of Figure 4 in the main paper). However, when training with only the VLM-based editing loss, theres gradual degradation in image quality, as Figure 6 shows, highlighting the complementary role of distribution matching losses such as DMD."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Unreliable VLM response on intermediate outputs of multi-step diffusion model. Here we show 28-step diffusion process, denoising predictions from early steps (e.g., = 4), which correspond to high noise levels, are blurry and semantically ambiguous. This can lead to unreliable responses from the VLM, as shown here. Therefore, we adopt few-step diffusion model that always generates sharp images. Figure 8: Our (4-step) vs single-step editing model. We compare our final 4-step model with single-step model, both trained via our approach. Editing an image in single step is still challenging and leads to lower-fidelity outputs. Figure 9: Limitation. Our method can struggle to maintain exact pixel consistency between the input and edited image. Having LPIPS (Zhang et al., 2018) loss between the input and output edited image can resolve it to an extent (top row) but at the cost of reduced editing success (bottom row). Sampling steps. For our method, we chose to train few-step image-editing model instead of multi-step diffusion model, as multi-step diffusion has noisy or blurry estimate of the final output in the early stages of diffusion. This can make it difficult to get reliable response from the VLM, as shown in Figure 7. On the other hand, predicting an edited image in one step is still challenging, as mentioned in the main paper, and shown here in Figure 8. Thus few-step provides nice balance between the two extremes of single and multi-step sampling for our purposes."
        },
        {
            "title": "C LIMITATION",
            "content": "A limitation of our framework is the lack of pixel-wise supervision to preserve regions that should remain unchanged under given edit instruction. Consequently, edited images can deviate from the input image in details or spatial alignment, as shown in Figure 9, 1st column. While our VLM-based editing loss includes question to check consistency between the input and edited images, it does not enforce pixel-level alignment. Empirically, we find that current VLMs struggle to detect subtle changes. To mitigate this, we experiment with an additional LPIPS (Zhang et al., 2018) loss between the input and output edited images. As shown in the last column of Figure 9, this improves consistency but also negatively impacts editing quality, particularly for edit-types like Removal. Future work could explore specialized VLMs that are more sensitive to fine-grained, pixel-level differences."
        },
        {
            "title": "D DATASET CONSTRUCTION DETAILS",
            "content": "Each tuple in our dataset = {(yi, ci, cy i=1 consists of real reference-image, y, corresponding edit instruction, c, and text prompt corresponding to the reference and edited image, cy and cx, respectively. We use text-image dataset corpus to select reference images. Given reference image, we prompt Qwen-2.5-32B VLM to suggest different possible editing instructions. The system and user prompt for it are as follows: )}N , cx Role: system, Content: You are helpful assistant and an expert in image editing. Role: user, Content: Task: As researcher in image editing, your task is to generate simple editing instructions based on the given image. The edit types you can use include: 1) local color change, 2) local texture, 3) adjust (shape change), 4) add, 5) remove, 6) replace, 7) bg, 8) style, 9) action, and 10) text manipulation **Important**: Ensure that you create balanced distribution of these edit types when generating the instructions. Each example should utilize different edit type, and the edit types should be evenly distributed across all examples. When using the add edit type, DO NOT USE vague placements like near, under, or beside, instead, specify the exact location where the object should be placed. For example, instead of add castle near the trees use add castle in the clearing between the trees. Ensure that each instruction is straightforward and points to single, clear edit change. Avoid complex or multi-step instructions. **Avoid Redundancy**: Make sure to introduce diversity in the edit instructions. Given the input image, could you generate simple edit instructions for different possible edit types by following the format of examples below and based on what you have seen in the image? Here are some examples showing the use of various edit types: Good example 1: {color change example} Good example 2: {texture change example} Good example 3: {adjust shape example} Good example 4: {add example} Good example 5: {remove example} Good example 6: {replace example} Good example 7: {bg example} Good example 8: {style example } Good example 9: {action example} Good example 10: {text manipulation example} Bad Examples: the edit instructions are hard/impossible to perform well, or mention vague terms that make the editing model struggle to perform well, and you should not follow. Bad example 1: - Instruction: make this dog look like its ready for formal evening out? - Type: add - Reasoning: This instruction is bad because it does not mention the exact changes that are needed to make the dog look like its ready for formal evening out. Bad example 2: - Instruction: remove the balloon [given an image of only balloons on white background] - Type: remove - Reasoning: This instruction is bad as it removes the only object in the image."
        },
        {
            "title": "Preprint",
            "content": "**Important Considerations**: 1. Avoid repetition of specific phrases: Do not reuse examples or themes from the above examples. Create entirely new and diverse themes and scenarios. 2. Logical Flow: Ensure that each instruction is logical and makes sense given the image. 3. Specificity in Insertions: When adding objects, use precise placement (e.g., in the sky or on the lake). Avoid vague terms like next to, around, or near. 4. Balanced use of edit types: Use variety of edit types such as [insertion], [replace], [local texture], [shape change], [style], [remove], [local color change], and [bg]. Ensure an even distribution of these edit types across your examples. 5. Diverse scenarios: Introduce variety in the scenarios, such as futuristic, historical, magical, surreal, or natural settings. Avoid overusing common tropes. 6. DO NOT suggest instructions that change very small/minute part of the image. Could you now generate 4 examples of **new, creative, and contextually relevant** edit instructions by following the format above? Avoid using the specific phrases, themes, or scenarios from the examples provided above. **Each example must use different edit type** from the ones listed above. Also, make sure to use each edit type equally across all generated examples. Finally, you should make the edit instructions as simple as possible so that the downstream editing model is able to work well. In the above user prompt, for the good examples, we randomly select an edit instruction for each editing type out of fixed set of manually defined edit instructions. Given edit instructions for each image, we again prompt the VLM to check the validity of the edit instruction and, if valid, to suggest possible caption for the edited image. The system and user prompt for this is: Role: system, Content: You are helpful assistant and an expert in image editing. Role: user, Content: Task: As researcher in image editing, given the input image, edit type, and the edit instruction, your task is to check if given edit instruction is valid and can be applied to the image. If it is valid, generate descriptive caption for what the image would look like after applying the edit instruction. If it is not valid, return invalid and explain why it is not valid, and output NA for the edited image caption. An edit instruction is invalid if it: 1. mentions to modify/remove/replace an object that is NOT PRESENT in the image. 2. is TOO HARD to make editing model to understand and perform well, e.g., remove any visible accessories. 3. DOES NOT change the image in any meaningful way, e.g., given the image of forest, change the background to dense forest. For the remove edit type: - DO NOT mention the object that is removed during the edit in the edited image caption. For example, given an image of cat in living room on sofa with the edit type remove and edit instruction: remove the cat Bad Example: cat is removed from the sofa in living room. Good Example: living room with sofa. Given the edit instruction and the original caption: Edit type: {edit type} Edit instruction: {simple edit instruction} Output format: Validity: ... Reasoning: ... Edited image Caption: ..."
        },
        {
            "title": "Preprint",
            "content": "Please provide concise but complete caption describing the edited image. Focus on the changes that would be made according to the edit instruction. Here are some more examples: Example 1: - Edit type: bg - Edit instruction: change the background to sunset view - Validity: valid - Reasoning: The edit instruction is valid because it adjusts the current blue sky to sunset view, which is meaningful change. - Edited image caption: park with sunset view. People are walking around in the park. Example 2: - Edit type: remove - Edit instruction: remove the wine glass - Validity: invalid - Reasoning: The edit instruction is invalid because it mentions removing wine glass that is not present in the image. - Edited image caption: NA **Important Considerations**: 1. DO NOT use instruction words like replaced, added, removed, modified, etc. in the caption. 2. Keep the caption general to explain any possible images resulting from the edit instruction. Only output the validity, reasoning, and edited image caption. Do not include any other text or explanations. After filtering the list of generated editing instructions using the above procedure, our final dataset consists of approximately 3M unique reference images with corresponding editing instructions spanning the 10 edit sub-types. Within the constraints of our available computational resources, this represents the largest dataset we were able to construct. For the customization task, we first instruct the VLM, to identify if the image has prominent object in the center. We provide an in-context sample image as well to the model. The exact system and user prompt for this is: Role: system, Content: You are helpful assistant and an expert in image personalization/customization. Role: user, Content: Task: You are assisting in research project on image personalization. Your goal is to evaluate whether the SECOND image contains **single, uniquely identifiable object** prominently positioned near the center of the frame. - The FIRST image (imagepath1) is an example of valid case. - The specific object category in the second image can be different focus only on **object uniqueness** and **image composition**. Good examples include object categories that can be personalized, have unique texture, and are not general objects: - Backpack, purse, toy, cat, dog, cup, bowl, water bottle, wearables, plushies, bike, car, clocks, etc. Bad examples include object categories that are general objects, and different instances of the category can not be distinguished: - Tree, building, door, flowers, food, vegetables, fruits, natural scenes, roads, etc. **Important Considerations:** 1. The object should be clearly recognizable and **visually distinct** from the background."
        },
        {
            "title": "Preprint",
            "content": "2. The object should be **near the center** of the image. 3. The **entire object** should be visible it should NOT be tight or zoomed-in crop. 4. The background can be natural but should not be overly cluttered or visually distracting. 5. The image should feature **single primary object**, not multiple equally prominent objects. Could you now judge the SECOND image and only provide the output, reasoning, and object name, in the following format: Output: True/False Reasoning: Brief explanation Object Name: The name of the object (e.g., backpack, cat, toy). If the VLM response predicts valid image, we then query it again to suggest new background context for the object category as follows: Role: system, Content: You are helpful assistant and an expert in image personalization/customization. Role: user, Content: Given an image of an object category, you have to suggest three DIVERSE background captions for the object. Provide detailed description of the background scene. Only suggest plausible backgrounds. DO NOT add the object name in the caption. DO NOT use emotional words in the caption. Be concise and factual but not too short. DO NOT mention the object name in the output captions. If the object is not thing, but scene, then output None. Example background captions for White plastic bottle are: 1. near the edge of marbled kitchen counter, surrounded by cutting board with chopped vegetables, salt shaker, and stainless steel sink in the background. 2. rests on tiled bathroom shelf, accompanied by toothbrush holder, mirror with foggy edges, and shower curtain partially drawn open. Example background captions for blue truck are: 1. parked beside graffiti-covered brick wall under cloudy sky, with city skyscrapers rising in the background. 2. resting in grassy field surrounded by wildflowers, with distant mountains and golden sunset in the background. Object: {object category name} Output: 1. 2. 3."
        },
        {
            "title": "E TRAINING IMPLEMENTATION DETAILS",
            "content": "E.1 LOCAL-IMAGE EDITING Training hyperparameters. We train on batch-size of 32 using Adam (Adam et al., 2014) optimizer with learning rate of 2 106, β1 as 0, and β2 as 0.9. We train for total of 10K iteratuions with auxiliary network, Aϕ, being updated 10 times for every generator, Gθ, update, following similar strategy adopted in DMD2 (Yin et al., 2024a). We train with the identity loss (Section 4.3) for 250 iterations. For faster convergence, the first 4K training iterations are trained with single step prediction (t = 1 in Line 3 of Algorithm 1), and then we start the 2-step unrolling of the diffusion trajectory. The final loss is weighted combination of VLM-based editing loss and distribution matching loss with λVLM = 0.01 and λDMD = 0.5. During training, we also add do nothing editing task with L2 loss between the input and edited image as regularization with 1%"
        },
        {
            "title": "Preprint",
            "content": "probability. This helps the model learn to maintain consistency between input and edited images. During training, we sample the editing instruction corresponding to each subtype uniformly, except removal, which is sampled with 25% probability. This is because, empirically, we observe that removal is more difficult than other edit-types like Color change. Template questions for VLM-based editing loss. As mentioned in the main paper, we evaluate VLM-based loss on two questions per edit type. Specifically for any edit type except removal, we use the following template: Role: user, Content: You are professional digital artist and an expert image editor. You will be provided with two images. The first being the original real image, and the second being an edited version of the first. The objective is to evaluate if the editing instruction has been executed in the second image. Editing instruction: {edit instruction} Answer with Yes or No. Note that sometimes the two images might look identical due to the failure of image editing. Answer No in that case. Role: user, Content: You are professional digital artist and an expert image editor. You will be provided with two images. Answer with Yes or No if the second image is exactly the same as the first image. IGNORE the changes in the second image because of the edit: {edit instruction}. Everything else should be the same. For the removal edit-type, we change the first question to explicitly ask about the presence of the target object to be removed, with the ground truth answer in this case being No. We find it to be more effective than generic template. Role: user, Content: You are professional digital artist and an expert image captioner. You will be provided with an image. Answer with Yes or No if the image has {object name}. E.2 FREE-FORM EDITING (CUSTOMIZATION) Training hyperparameters. We reduce the warmup iterations for which we train with the identity loss to 100 in this case, since customization often requires more drastic changes in the output image compared to the input reference image. Further, we increase λDMD = 2 instead of 0.5 as in the case of local image-editing. The rest of the hyperparameters remain the same. Both during training and inference, the input text prompt to the few-step generator, Gθ, is in the following template: Generate the main object shown in the first image in different setting and pose: { background scene description}. We train the 4-step model for 10K iterations. For the 8-step model, we fine-tune for 5K additional training steps starting from the 4-step model. Template questions for VLM-based editing loss. Here, we modify the questions to instead evaluate if the background context and pose are different in the generated image, i.e., editing success, and if the object identity is similar, i.e., image alignment and consistency between the input reference and edited image. The exact questions are as follows: Role: user, Content: You are professional digital artist and an expert in image editing. You will be provided with two images."
        },
        {
            "title": "Preprint",
            "content": "Answer with Yes or No if the {objectname} in the second image is in different pose and location than in the first image. Note that sometimes the second image might not have the same object because of the failure of image editing. Answer No in that case. Role: user, Content: You are professional digital artist and an expert in image editing. You will be provided with two images. Answer with Yes or No if the {objectname} in the second image is the exact same identity, with similar color, shape, and texture as in the first image. Note that sometimes the second image might not have the same object because of the failure of image editing. Answer No in that case."
        },
        {
            "title": "F OTHER BASELINE DETAILS",
            "content": "Flow-GRPO (Liu et al., 2025a). We follow the open-source implementation of Flow-GRPO and train with the same computational budget as our method, i.e., across 4 A100 GPUs and 2.5 days of training. The final model is fine-tuned from pre-trained image-editing model for 5K iterations. During training, we collect 16 images per-prompt with 12 denoising steps (28 during inference) for computing the mean and standard deviation in GRPO (Shao et al., 2024). Following their official implementation, we train with LoRA (Hu et al., 2022) of rank 32, α = 64, learning rate 1 104, and use the VLM to score edits on scale of 0 to 9, which is normalized between 0-1 to get the final reward. The exact prompt used to query the VLM is derived from VIEScore (Ku et al., 2024) and is shown below. Role: system, Content: You are helpful assistant and an expert in image editing. Role: user, Content: You are professional digital artist. You will have to evaluate the effectiveness of AI-generated edited image(s) based on given rules. You will have to give your output in this way (Keep your reasoning VERY CONCISE and SHORT): score : ..., reasoning : ... RULES: Two images will be provided: The first being the original real image and the second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to the failure of image edit. From scale 0 to 9: score from 0 to 9 will be given based on the success of the editing. (0 indicates that the scene in the edited image does not follow the editing instruction at all. 9 indicates that the scene in the edited image follows the editing instruction text perfectly.) Editing instruction: {edit instruction} Supervised Fine-Tuning. We train with the standard velocity prediction flow-objective for 30K iterations with batch-size of 32 and learning rate 2 106 with linear warmup of 2K iterations. To enable classifier-free guidance, we drop the image and text conditions 10% of the time. Sampling parameters for local image-editing baselines. We follow the open-source implementation to sample images from all the baseline models for the benchmark evaluations. The turbo-"
        },
        {
            "title": "Preprint",
            "content": "edit (Deutch et al., 2024) baseline requires caption corresponding to the edited image as well, and we use Qwen-2.5-32B-VLM to generate these captions for GEdit-Bench images. Sampling parameters for customization baselines. Here as well, we follow the open-source implementation to sample images from all the baseline models for the benchmark evaluations. In the case of DSD (Cai et al., 2025), it employs Gemini-1.5 to convert the input user-prompt into detailed prompt. However, we skipped this step for fair evaluation with other methods, which do not use any prompt rewriting tools. In the case of SynCD (Kumari et al., 2025), though it supports multiple reference images as input, we evaluated it with single reference image, to keep the setup similar to other baseline methods and Ours. For sampling images with OminiControl (Tan et al., 2025) and DSD (Cai et al., 2025), we follow their recommended prompt setting and replace the category name with this item."
        },
        {
            "title": "G SOCIETAL IMPACT",
            "content": "Our work introduces training framework for fine-tuning text-to-image models into few-step image-editing model without using paired supervision. By enabling few-step sampling, our method improves inference efficiency and reduces computational cost. Nonetheless, the broader risks of generative models, such as creating deepfakes and misleading content, also apply to our approach. Possible ways to mitigate this are watermarking generative content Fernandez et al. (2023) and reliable detection of generated images Wang et al. (2020); Corvi et al. (2023); Cazenavette et al. (2024)"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Qualitative comparison on GEdit-Bench. We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics  (Table 1)  , in the 1st column. Our method performs on par or better than baseline methods across different edit types in the few-step setting."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Qualitative comparison on ImgEdit-Bench. We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics  (Table 6)  , in the 1st column. Our method performs on par or better than baseline methods across different edit types in the few-step setting."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Qualitative comparison on DreamBooth. We show results of our and baseline methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics in the first column. Our method performs comparably with baseline methods on identity alignment while having better image fidelity across different concepts in the few-step setting."
        }
    ],
    "affiliations": [
        "Adobe",
        "Carnegie Mellon University"
    ]
}