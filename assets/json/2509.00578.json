{
    "paper_title": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection",
    "authors": [
        "Abdellah Zakaria Sellam",
        "Ilyes Benaissa",
        "Salah Eddine Bekhouche",
        "Abdenour Hadid",
        "Vito Renó",
        "Cosimo Distante"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains"
        },
        {
            "title": "Start",
            "content": "Sorbonne University Abu Dhabi, UAE"
        },
        {
            "title": "Graphical Abstract",
            "content": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection Abdellah Zakaria Sellam, Ilyes Benaissa, Salah Eddine Bekhouche, Abdenour Hadid, Vito Renó, Cosimo Distante 5 2 0 2 0 3 ] . [ 1 8 7 5 0 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Highlights",
            "content": "C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection Abdellah Zakaria Sellam, Ilyes Benaissa, Salah Eddine Bekhouche, Abdenour Hadid, Vito Renó, Cosimo Distante Introduces Context-Aware Fusion (CAF) integrating global scene context with local features Proposes dedicated Global Context Encoder (GCE) for comprehensive scene understanding Achieves SOTA results on CarDD benchmark Addresses fine-grained detection challenges difficult even for human experts Enhances Multi-Modal Fusion with global context embeddings for unified representation C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection Abdellah Zakaria Sellam Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy Ilyes Benaissa Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy"
        },
        {
            "title": "Salah Eddine Bekhouche",
            "content": "UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain"
        },
        {
            "title": "Abdenour Hadid",
            "content": "Sorbonne University Abu Dhabi, Al Reem Island, P.O. Box 34105, Abu Dhabi, 34105, Abu Dhabi, United Arab Emirates CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy Vito Renó"
        },
        {
            "title": "Cosimo Distante",
            "content": "Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy"
        },
        {
            "title": "Abstract",
            "content": "Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains, code and implementation details of the project are in this GitHub link. Keywords: Object Detection, Diffusion Model, Context-Aware Fusion, Global scene, Cardd 1. Introduction The paradigm for object detection has evolved from multi-stage, proposaldriven systems to unified, end-to-end architectures. This trajectory began with the R-CNN family, which established foundational pipeline of region proposal followed by classification and refinement [1, 2, 3]. Subsequently, single-stage detectors like YOLO and SSD achieved significant efficiency gains by unifying these steps into single network, direction later refined by RetinaNet, which demonstrated that specialized loss functions could close the accuracy gap with multi-stage methods [4, 5, 6, 7]. Further innovations, including anchor-free designs and keypoint estimation frameworks, diversified the architectural landscape for real-time detection [8, 9, 10]. More recently, architectures such as Sparse RCNN have recast detection as set prediction problem, enabling fully end-to-end training and leveraging global reasoning through transformers [11, 12]. Concurrently, generative modeling, particularly via denoising diffusion processes, has emerged as powerful technique for structured data synthesis [13, 14, 15]. This paradigm was successfully adapted to object detection by DiffusionDet, which formulates the task as generative process of transforming random noise into set of bounding boxes [16]. During training, ground-truth boxes are progressively noised, and the model learns to reverse this process. At inference, the model iteratively refines set of random boxes into precise detections conditioned on image features. This formulation offers inherent advantages, including dynamic output cardinality and progressive improvement of predictions. However, existing detectors face significant challenges in specialized domains such as automotive damage assessment, as highlighted by the CarDD dataset [17]. Many damage instances, such as scratches and cracks, are characterized by sub2 tle, low-contrast visual cues that are often confounded by specular reflections, complex lighting, and background clutter. While methods employing deformable convolutions (DCN/DCN+) have shown promise by adapting their receptive fields to local geometric deformations like dents [18], they often lack the global context needed to disambiguate faint or irregular patterns. Similarly, while DiffusionDet offers robust refinement mechanism, its reliance on local Region of Interest (RoI) features can be insufficient when global scene understanding is critical for accurate localization. To address these limitations, we propose C-DiffDet+, context-aware diffusion detector that builds upon the DiffusionDet architecture by integrating global scene representations into the iterative detection process. Our core hypothesis is that resolving local ambiguities in damage detection requires explicit conditioning on scene-level information. Specifically, C-DiffDet+ introduces three key components: Global Context Encoder (GCE) produces compact embedding of the entire scene, which is then injected into multi-scale backbone features by ContextAware Fusion (CAF) modules using adaptive attention. Finally, the diffusion head itself is context-conditioned, ensuring each box proposal is refined while attending to this global scene representation. This architecture synergistically combines the powerful iterative refinement capabilities inherent to its diffusion-based foundation with explicit, top-down contextual guidance. Comprehensive experiments on the CarDD benchmark validate our approach. C-DiffDet+ achieves state-of-the-art performance, yielding substantial improvements in average precision, particularly for challenging categories like scratches and cracks that are prone to local ambiguity. While deformable networks remain effective for geometrically distinct damages, our results demonstrate that global context conditioning provides complementary and significant advantage. By enriching both the feature hierarchy and the denoising process with holistic understanding of the scene, C-DiffDet+ overcomes key failure mode of existing detectors and establishes new baseline for robust object detection in complex visual domains. our Contributions are represented as follow: We identify and quantify the fundamental limitation of local feature conditioning in diffusion-based detectors for fine-grained visual tasks, demonstrating why global scene-level context is critical for disambiguation. We introduce Context-Aware Fusion (CAF) that integrates dedicated Global Context Encoder (GCE) with local proposal features through cross-attention mechanisms. 3 We propose Adaptive Channel Enhancement (ACE) blocks that enhance both backbone and FPN feature representations for improved detection performance. We enhance the existing Multi-Modal Fusion framework by integrating global context embeddings for unified temporal, positional, and contextual representations. We provide extensive experimental validation on CarDD benchmark. This paper is organized as follows. We review related work in Section 2. Section 3 describes our proposed method. In Section 4 we present the training setup and experimental results. Section 5 discusses the effectiveness of our contributions. Finally, Section 6 concludes the paper, describes limitations, and outlines directions for future work. 2. Related Work The evolution of deep object detection began with multi-stage paradigm, established by the R-CNN family. R-CNN demonstrated the power of CNN features for region proposals [1], Fast R-CNN made this process efficient by sharing convolutional computations [2], and Faster R-CNN introduced the Region Proposal Network, enabling end-to-end training [3]. subsequent trend favoured speed and simplicity, collapsing detection into single stage. Models like YOLO, which framed detection as direct regression problem [4], and SSD, which introduced multi-scale feature maps [5], defined this approach. Later, methods like RetinaNet addressed class imbalance with the focal loss [6], while anchor-free designs such as FCOS and CenterNet further simplified the pipeline by reframing detection as keypoint or center estimation [10, 9]. In parallel, architectural innovations enhanced network capabilities. Deformable Convolutional Networks (DCN) introduced learnable offsets to convolution kernels, allowing the sampling grid to adapt to object geometry, which is particularly effective for non-rigid shapes [18]. Other modules, like GCNet, were designed to capture long-range dependencies and aggregate global context, thereby improving feature discriminability [19]. While these operator-level improvements enhanced discriminative detectors, they remained fundamentally feed-forward mappings from features to predictions. paradigm shift occurred with the application of generative denoising diffusion models to computer vision. This led to DiffusionDet, which reformulated In this object detection as conditional noise-to-box generation process [16]. framework, set of random boxes is iteratively refined by learned denoising process conditioned on image features. This probabilistic approach offers attractive properties like progressive refinement and variable number of outputs, unifying proposal generation and box regression. However, the efficacy of these different approaches is challenged by finegrained detection tasks like vision-based car damage assessment. The CarDD benchmark, for example, highlights common failure modes on small, low-contrast, and visually ambiguous instances such as scratches and cracks [17]. Empirical studies show that while DCNs improve performance on geometric damage like dents, they still struggle when local cues are insufficient. Similarly, DiffusionDets iterative refinement is beneficial, but its performance is constrained by its conditioning mechanism, which relies solely on local Region of Interest (RoI) features. This limitation prevents it from effectively leveraging scene-level informationsuch as global illumination, material properties, and vehicle posethat is critical for disambiguating subtle damage. In this work, we argue that the next advance in fine-grained detection requires unifying the iterative refinement of diffusion models with explicit global scene reasoning. To this end, we propose the Context-Aware Diffusion Detector (CDiffDet+). Our key innovation is the integration of lightweight Global Context Encoder into the detectors backbone and fusion layers. This module produces comprehensive scene embedding that conditions the entire denoising diffusion head. By doing so, our model generates more robust proposals and final detections, particularly for categories like scratches and cracks where local evidence is ambiguous. This design enhances the detectors ability to perform scene-level reasoning while fully retaining the powerful generative properties and set prediction capabilities of the original DiffusionDet framework. The subsequent sections will detail the architecture, training protocol, and empirical validation of our approach. 3. Methodology Our proposed model builds upon the DiffusionDet architecture, extending it with context-aware mechanisms to better capture global scene information for object detection. As illustrated in Figure 1, the framework is composed of four key components: (1) Adaptive Channel Enhancement (ACE) blocks that refine backbone and FPN features, (2) Global Context Encoder (GCE) that captures holistic scene representations, (3) Context-Aware Fusion (CAF) module that integrates global context with local proposal features via cross-attention, and (4) Figure 1: Overview of the proposed Context-Aware DiffusionDet architecture. The framework consists of four key components: (1) Adaptive Channel Enhancement (ACE) blocks that enhance backbone and FPN features, (2) Global Context Encoder (GCE) for comprehensive scene understanding, (3) Context-Aware Fusion (CAF) that integrates global context with local features through cross-attention, and (4) enhanced Multi-Modal Fusion (MMF) with global context embeddings. This architecture addresses the local feature conditioning limitation in existing diffusionbased detectors by enabling comprehensive environmental context integration. 6 an enhanced Multi-Modal Fusion (MMF) module that incorporates global context embeddings. By modeling object detection as conditional denoising diffusion process, our framework inherits the generative strengths of DiffusionDet while addressing its key limitationthe reliance on local feature conditioning. The proposed CAF mechanism explicitly fuses global scene context with local features, enabling more comprehensive understanding of objectenvironment interactions. In the following sections, we provide the formal mathematical formulation of our model, along with the theoretical motivation behind each component. 3.1. Denoising Diffusion Framework Let x0 RN 4 be set of ground-truth boxes. The objective is to learn model that can reverse predefined noising process, starting from pure noise and iteratively generating these boxes. This generative formulation avoids the need for hand-designed anchors or proposal generation networks. 3.1.1. Forward Diffusion Process The forward process progressively injects Gaussian noise into the ground-truth boxes over timesteps. This process defines the training objective for our model. The distribution of noisy boxes xt at any timestep is given by closed-form solution, enabling efficient training: q(xtx0) = (xt; αtx0, (1 αt)I) (1) where αt = (cid:81)t s=1(1 βs) is derived from cosine variance schedule. This schedule is chosen for its smooth and gradual addition of noise, which has been shown to improve training stability and the quality of the learned reverse process [20]. 3.1.2. Reverse Denoising Process The reverse process is learned by network fθ that predicts the original clean boxes ˆx0 from noisy state xt, conditioned on the image and timestep t: ˆx0 = fθ(xt, t, I) (2) The design of this function fθ is the primary focus of our methodology, as its capacity to leverage contextual information through our proposed Context-Aware Fusion mechanism dictates the models performance. 7 3.2. Context-Aware Feature Extraction The efficacy of the denoising function hinges on its ability to extract and utilize both local and global visual information. As depicted in Figure 1, this is achieved through dual-stream backbone that processes the image in parallel to capture different levels of context. 3.2.1. Feature Generation From an input image I, Swin Transformer backbone generates hierarchy of feature maps at four stages, {C2, C3, C4, C5}, with strides of {4, 8, 16, 32}. This provides powerful, multi-scale representation of the image. 3.2.2. Adaptive Channel Enhancement (ACE) To improve feature quality, we apply Adaptive Channel Enhancement (ACE) blocks to the backbone features, particularly at Stage 4. The ACE mechanism implements channel-wise attention following squeeze-excitation approach: µc = 1 H32 W32 H32(cid:88) W32(cid:88) h=1 w=1 C(c) 5 [h, w] RBC ac = σ(W2 ReLU(W1 µc + b1) + b2) RBC Cenhanced 5 = C5 ac (3) (4) (5) where denotes element-wise multiplication, ac represents the learned channel attention weights, H32 = H/32 and W32 = W/32 are the spatial dimensions at stride 32 (Stage 4), W1 RCC/16, W2 RC/16C are learnable weight matrices, b1, b2 are bias terms, and σ represents the sigmoid activation function. The reduction ratio is set to 16 for computational efficiency. The channel-wise attention mechanism begins by compressing spatial information using global average pooling to obtain channel-wise statistics. It then learns adaptive channel weights through two-layer MLP with ReLU activation and re-weights the original features accordingly. This approach allows the model to focus on the most informative channels for object detection, improving feature discriminability and performance. The ACE block is applied only to the final stage (Stage 4) of the Swin Transformer backbone to enhance the C5 features before they are sent to the FPN. 8 3.2.3. Feature Pyramid Network (FPN) Feature Pyramid Network (FPN)[21] constructs rich multi-scale feature representation from the enhanced backbone features. The FPN follows topdown pathway with lateral connections, where higher-level, semantically stronger features are upsampled and combined with lower-level, spatially finer features: ) P5 = Conv33(Cenhanced 5 P4 = Conv33(C4 + Upsample(P5)) P3 = Conv33(C3 + Upsample(P4)) P2 = Conv33(C2 + Upsample(P3)) (6) (7) (8) (9) Each FPN level Pi has consistent channel dimension of 256, providing rich features for objects at various scales. The FPN enables the model to handle objects of different sizes effectively by providing appropriate feature resolution for each scale. 3.3. Multi-Modal Feature Integration The detection head, shown centrally in Figure 1, is designed to fuse the visual features with the current state of the diffusion proposals and other conditioning signals. 3.3.1. RoI Feature Extraction from Diffusion Proposals The noisy proposals xt are used to spatially sample the image features via RoIAlign. This is critical step where the abstract box coordinates from the diffusion process are connected to concrete visual evidence from the FPN feature maps {Pi}. For each proposal box, we first determine the appropriate FPN level based on the box size using logarithmic assignment strategy: level = log2( area/sbase) + 4 (10) where sbase = 224 is the base size and the level is clamped to the range [1, 5]. The RoIAlign operation then extracts fixed-size features from the assigned FPN level: froi = ΦRoIAlign({Pi}, xt, levels) RBN 77256 (11) These features are then aggregated using global average pooling to produce compact representation: froi = GlobalAvgPool(froi) RBN (12) 9 3.3.2. Context-Aware Fusion (CAF) The Context-Aware Fusion mechanism integrates global scene context with local proposal features through cross-attention. This mechanism requires Global Context Encoder (GCE) to generate comprehensive scene-level understanding. The GCE The global context extractor processes the entire image to capture holistic scene information, as described in Algorithm ??. It is composed of three convolutional layers with residual connections. To preserve spatial information, residual path downsamples the input image and projects it to the same feature dimension, ensuring compatibility with the main branch. The final global context vector is obtained by fusing the output features with the residual path and applying Global Average Pooling (GAP): = GAP(F3 + Ires) RBDf , (13) where GAP denotes Global Average Pooling and Df = 256 is the feature dimension. The CAF mechanism operates in two steps: 1. Inter-Proposal Self-Attention: This initial module allows the set of RoI features to communicate with each other. Each proposal feature in froi is linearly projected to Query, Key, and Value: Qs = froiWQs Ks = froiWKs Vs = froiWV (14) (15) (16) The output is refined set of features where each proposals representation has been updated based on its interaction with all other proposals: fself = LayerNorm(froi + MultiHeadAttn(Qs, Ks, Vs)) (17) 2. Global-Local Cross-Attention: In the second step, each self-attended proposal feature fself queries the global context vector g. This mechanism infuses top-down, scene-level information into each individual object hypothesis, allowing the model to disambiguate challenging cases: Qc = fselfWQc Kc, Vc = gWKc, gWV (18) (19) The resulting context-aware features are then computed: fcross = LayerNorm(fself + MultiHeadAttn(Qc, Kc, Vc)) (20) 10 3.3.3. Enhanced Multi-Modal Fusion The existing Multi-Modal Fusion framework is enhanced by integrating global context embeddings, creating unified representation that combines temporal, positional, and contextual information. This enhancement addresses the fundamental limitation of local feature conditioning by providing each proposal with access to comprehensive scene-level understanding. Three types of conditional embeddings are generated to provide non-visual context: 1. Time Embedding (Et): The time embedding captures the current diffusion timestep through sinusoidal encoding, enabling the model to adapt its predictions based on the denoising iteration. This embedding is generated as: Et = MLP(TimeEmbedding(t)) RB256 where the time embedding uses the standard sinusoidal positional encoding scaled to the diffusion timestep range. (21) 2. Positional Embedding (PE): Positional embeddings preserve the ordering and identity of proposals, generated from sequential indices [0, 1, 2, . . . , 1]: PE = MLP(PositionalEncoding([0, 1, . . . , 1])) RN 256 (22) These embeddings are trainable and learn to encode proposal-specific positional information. 3. Context Embedding (Cemb): Context embeddings integrate global scene understanding by processing the global context vector through an MLP: Cemb = MLP(g) RB256 (23) where is the global context from the Global Context Encoder. The enhanced Multi-Modal Fusion combines context-aware features with conditional embeddings through cross-attention mechanism: First, the conditional embeddings are prepared and broadcast to match the proposal dimensions: Ebroadcast Cbroadcast emb = Et 1T = Cemb 1T RBN 256 RBN 256 (24) (25) where 1N RN is vector of ones. The embeddings are concatenated along the feature dimension to form unified latent representation: 11 Llatent = concat(Ebroadcast , PE, Cbroadcast emb ) RBN 768 (26) The final fused representation is computed through cross-attention between contextaware features and latent embeddings: Qmmf = fcrossWQ,mmf RBN 256 Kmmf = LlatentWK,mmf RBN 256 Vmmf = LlatentWV,mmf RBN 256 The attention weights are computed as: Ammf = softmax (cid:19) (cid:18) QmmfKT 256 mmf RBN The attended output is: fattended = AmmfVmmf RBN 256 (27) (28) (29) (30) (31) The final fused representation combines the context-aware features with the attended multi-modal information: ffused = LayerNorm(fcross + fattended) (32) The enhanced Multi-Modal Fusion module is implemented as trainable component that learns optimal projection matrices WQ,mmf, WK,mmf, and WV,mmf to maximize the effectiveness of the cross-modal information integration. This design ensures that the model can adaptively learn which aspects of the multi-modal information are most relevant for each specific detection scenario. After the multi-modal fusion, final MLP projection is applied to refine the fused features and prepare them for the detection head: ffinal = MLPfinal(ffused) RBN 256 (33) where MLPfinal consists of two linear layers with ReLU activation and dropout: h1 = ReLU(ffusedW1 + b1) RBN 512 ffinal = Dropout(h1W2 + b2) RBN 256 (34) (35) 12 where W1 R256512, W2 R512256, b1 R512, and b2 R256 are learnable parameters. This final projection learns refined feature representation that optimally combines multimodal information through bottleneck architecture (256512256), while ensuring the output features have the proper dimensionality and statistical properties for the detection head. The bottleneck design captures the most essential multimodal information in compressed form, maintaining both expressiveness and computational efficiency. 3.4. Training and Inference The final, refined representation ffinal serves as the input to the prediction heads, which translate these rich features into concrete object detections. As shown on the right of Figure 1, the final representation is passed to three independent heads, each implemented as small MLP: 1. Classification Head: Predicts the class probabilities for each of the proposals, including \"no object\" class. 2. Box Regression Head: Predicts the refined coordinates of the clean box ˆx for each proposal. 3. Noise Predictor: Predicts the noise vector ˆϵθ that was added to the original clean box to produce xt. We employ set-based loss, which requires bipartite matching between the set of predictions and the set of ground-truth objects. The optimal matching ˆσ is found using the Hungarian algorithm to minimize pairwise matching cost: Cmatch = λclsLfocal(pi, cσ(i)) + λL1LL1(bi, bσ(i)) + λgiouLgiou(bi, bσ(i)) (36) where λcls, λL1, and λgiou are loss weighting coefficients, pi represents the predicted class probabilities, cσ(i) denotes the ground-truth class, and bi and bσ(i) represent the predicted and ground-truth bounding boxes, respectively. Once the matching is established, the total loss is computed as weighted sum of the same losses over the matched pairs: Ltotal = (cid:88) i= Lmatch(yi, ˆyˆσ(i)) (37) where Lmatch combines the classification, regression, and noise prediction losses for each matched proposal-ground-truth pair. 13 The inference process is generative loop that starts from set of boxes sampled from pure Gaussian noise, xT (0, I). The model then iteratively applies the reverse denoising step for small number of timesteps. We use the Denoising Diffusion Implicit Models (DDIM) formulation for efficient sampling: xt1 = αt1ˆx(t) 0 + (cid:112)1 αt1 ˆϵθ(xt, t, I) (38) where ˆx(t) ˆϵθ is conditioned on the current noisy state xt, timestep t, and input image I. 0 = fθ(xt, t, I) is the predicted clean box at step t, and the noise predictor The final predictions are obtained after the last denoising step, where the model outputs the refined bounding boxes ˆx0 along with their corresponding class probabilities and confidence scores. Non-maximum suppression (NMS) is then applied to remove duplicate detections and produce the final set of object predictions. 4. Experiments and Results 4.1. Experimental Setup 4.1.1. Dataset and Preprocessing For the training and evaluation of our model, we use the public CarDD (Car Damage Detection) dataset [22]. This dataset consists of 4,000 high-resolution images containing over 9,000 annotated damage instances. The annotations, provided as bounding boxes in COCO format, are categorized into six classes: dent, scratch, crack, glass shatter, tire flat, and lamp broken. The standard data split is 70.4% for training, 20.19% for validation, and 9.35% for testing. Table 1: Distribution of instances per category across Train, Validation, and Test datasets. Category dent scratch crack glass shatter lamp broken tire flat Total # Train Instances 1806 2560 651 475 494 225 6211 # Val Instances 501 728 177 135 141 62 1744 # Test Instances Total Instances 2543 3595 898 681 704 319 8740 236 307 70 71 69 32 785 During training, we employ standard data augmentation techniques, including random horizontal flipping and multi-scale resizing with cropping, to enhance the models generalization capabilities. The CarDD [22] dataset presents significant challenges for object detection models. primary difficulty is the fine-grained nature of the damage categories, which results in low inter-class variance; for instance, distinguishing between scratch and crack is non-trivial. The dataset also features wide diversity in the scale and shape of damages, from large-area deformations to small, localized defects. This is compounded by the high prevalence of small object instances, which requires model to effectively detect features at various resolutions. These characteristics make CarDD demanding benchmark for assessing model robustness in realistic damage detection scenarios. 4.1.2. Implementation Details and Hyperparameters Our model is implemented based on the Detectron2 framework. For optimization, we utilize the AdamW optimizer with base learning rate of 2.5 105 and weight decay of 0.0001. The model is trained for total of 20,000 iterations using batch size of 2. The learning rate schedule includes 1000-iteration linear warm-up, followed by learning rate decay steps at the 15,000 and 18,000 iteration marks. To ensure training stability, we apply gradient clipping with maximum norm of 1.0. The DiffusionDet head is configured to use 500 proposals during the iterative denoising process. 4.1.3. Evaluation Metrics To ensure fair comparison, we strictly follow the evaluation protocol of the CarDD benchmark [22], which builds on the COCO evaluation suite and provides comprehensive framework for measuring model performance. Our evaluation focuses on three aspects: detection accuracy, scale-dependent behavior, and computational efficiency. Detection Accuracy. Following CarDD, we report mean Average Precision (mAP) as the primary metric. The challenge metric, AP, corresponds to COCO dataset mAP@[.5:.95], averaged over IoU thresholds from 0.5 to 0.95. We also include AP50 and AP75, which evaluate detection performance at fixed thresholds, reflecting coarseand fine-grained localization accuracy see the detailed eqautions for the metrics in appendx 31. Scale-Specific Performance. To analyze robustness across object sizes, we adopt the COCO-defined metrics APS, APM, and APL, which measure accuracy on small (area < 322 px), medium (322962 px), and large (area > 962 px) instances. These results highlight the operational strengths and limitations of our method. 15 Computational Efficiency. Since our C-DiffDet+ introduces novel architecture, we extend the CarDD protocol with computational efficiency analysis. We report latency (ms), throughput (FPS), GFLOPs, and parameter count in the appendix ?? , all benchmarked on dual NVIDIA RTX 4500 Ada GPUs (24GB VRAM each). This provides balanced view of both accuracy and practical deployability. 4.2. Results 4.2.1. Quantitative Comparison with State-of-the-Art Methods Table 2 presents comprehensive comparison of bounding box detection performance across different evaluation metrics on the CarDD dataset. Our C-DiffDet+ model demonstrates superior performance across most metrics, establishing new state-of-the-art results in the majority of evaluation criteria and showcasing the effectiveness of our context-aware diffusion framework. In terms of overall performance, C-DiffDet+ achieves remarkable APbb of 64.8%, representing significant improvement of 1.4% over the previous best method, DiffDet. This substantial gain in mean Average Precision demonstrates that our context-aware approach effectively addresses the fundamental limitations of local feature conditioning in existing detection frameworks. The integration of global scene context through cross-attention mechanisms enables more accurate localization and classification of automotive damage across diverse scenarios. At higher IoU thresholds, our model shows particularly strong performance. C-DiffDet+ achieves an APbb 75 of 67.9%, outperforming DiffDet by 1.7%. This improvement at the stricter IoU threshold indicates that our model not only detects damage but also provides more precise bounding box localization. The enhanced precision can be attributed to the iterative refinement capabilities of our diffusion-based approach, where the denoising process progressively improves localization accuracy through multiple refinement steps. The most dramatic improvement is observed in small object detection (APbb ), where C-DiffDet+ achieves 45.5%, representing remarkable 6.8% increase over DiffDet. This exceptional performance in small damage detection validates our context-aware fusion mechanism, as small damage types like fine scratches or hairline cracks often require global scene understanding to distinguish from visual artifacts or natural surface variations. The Global Context Encoder provides comprehensive scene-level information that helps disambiguate these challenging cases. For medium-sized objects (APbb ), our model achieves 39.2%, which is 8.8% lower than DCN+. This performance gap suggests that while our context-aware approach excels at fine-grained and small damage detection, medium-sized damage patterns may benefit from different architectural considerations. Medium-sized damage often involves more complex geometric patterns that might require enhanced local feature processing or different attention mechanisms. In large object detection (APbb ), C-DiffDet+ achieves 66.0%, matching the performance of DCN+ and outperforming DiffDet by 1.8%. This parity in large object detection indicates that our model maintains competitive performance for substantial damage types while significantly improving detection of smaller, more challenging cases. Large damage patterns are typically more visible and may not require the same level of contextual reasoning that benefits smaller damage detection. The superior performance of C-DiffDet+ across most metrics can be attributed to several key architectural advantages. The Context-Aware Fusion (CAF) mechanism, which integrates global scene context with local proposal features, proves particularly effective for damage types that require comprehensive environmental understanding. The Adaptive Channel Enhancement (ACE) blocks in the backbone enable dynamic feature re-weighting based on learned importance, improving the models ability to focus on the most informative visual cues. Furthermore, the diffusion-based iterative refinement process provides progressive improvement in bounding box localization, as evidenced by the strong APbb 75 performance. The multi-modal fusion with global context embeddings ensures that each proposal benefits from comprehensive scene understanding, leading to more accurate damage classification and localization. These results validate our core architectural contributions and demonstrate that integrating global contextual information with diffusion-based detection significantly advances the state-of-theart in automotive damage assessment, particularly for challenging fine-grained and small-scale damage types. Table 2: Comparison of Bounding Box (%) on the CarDD dataset. Method Mask R-CNN [23] CM-R-CNN [24] GCNet [25] HTC [26] DCN [27] DCN+ [22] DiffDet [28] C-DiffDet+ (ours) APbb APbb 67.7 51.1 65.4 52.0 69.3 52.6 68.4 53.4 69.8 54.3 78.8 60.6 81.7 63.4 83.6 64.8 50 APbb 54.2 54.8 56.6 54.6 58.3 64.8 66.2 67.9 75 APbb 19.0 16.7 21.2 20.32 22.7 37.1 38.7 45. APbb 39.1 37.1 45.8 39.6 42.6 48.0 36.1 39.2 APbb 61.1 61.6 58.4 62.9 63.6 66.0 64.2 66.0 Table 3 presents per-category average precision (AP) comparison on the CarDD dataset, evaluating our C-DiffDet+ model against prior state-of-the-art 17 methods. C-DiffDet+ attains new state-of-the-art performance in three of the six damage categories, with the most pronounced improvements observed on finegrained and geometrically complex damage types. Specifically, C-DiffDet+ achieves leading AP of 42.2% on the crack category, substantial absolute improvement of 7.1% over the previous best result from DiffDet (35.1%). For lamp broken, our model also sets new record with 80.2% AP, surpassing DiffDet by 3.4%. The highest performance is seen in the glass shatter category, where C-DiffDet+ reaches 94.2% AP, exceeding the prior state-of-the-art. While not the top-performing model in all categories, C-DiffDet+ remains highly competitive. In the scratch category, it achieves strong 42.6% AP. For tire flat, it scores 92.0% AP, closely trailing the best result. The models performance on the dent category is 37.0%. The empirical improvements can be attributed to the Context-Aware Fusion (CAF) module, which conditions local proposal features on global scene representation through cross-attention. This approach enhances the ability to differentiate between subtle damage patterns and background textures, which is particularly effective for detecting low-contrast, fine-grained defects such as cracks. Additionally, the Adaptive Channel Enhancement (ACE) blocks in the backbone dynamically adjust channel responses, thereby enhancing the representation of structural cues that are crucial for identifying damage, such as broken lamps. In summary, the per-category analysis demonstrates that C-DiffDet+ advances the state of the art in three out of six damage types on CarDD, with significant improvements on the most challenging fine-grained categories. These results substantiate the effectiveness of integrating global contextual reasoning into diffusion-based detection framework for automotive damage assessment. 4.2.2. Visual Comparison visual comparison of detection results in Figure 2 reveals key insights about the performance of different methods. This analysis complements quantitative metrics by highlighting distinct failure modes and validating our fusion strategy. DCN+, while effective for high-contrast damage through its deformable convolutions, shows limitations in information-poor scenarios. It often misses subtle defects and fragments contiguous damage due to insufficient global reasoning. The DiffusionDet baseline demonstrates challenges with generative paradigms using only local features. While it occasionally improves recall, its predictions suffer from poor localizationproducing oversized boxes that dont conform to Table 3: Comparison between our C-DiffDet+ and the state-of-the-art on each category in CarDD in terms of mask and box AP. Bold text indicates the best result and underlined text indicates the second best. Method Mask R-CNN [23] CM-R-CNN [24] GCNet [25] HTC [26] DCN [27] DCN+ [22] DiffDet [28] C-DiffDet+ (ours) dent 29.4 28.5 29.6 30.4 33.0 42.2 38.9 37.0 scratch crack glass shatter lamp broken tire flat 25.6 24.5 26.5 26.8 29.7 42.3 44.8 42. 20.2 21.3 21.6 18.6 17.5 29.6 35.1 42.2 88.5 90.6 89.3 92.7 92.7 90.1 92.6 94.2 62.0 62.0 66.9 64.3 66.7 69.5 76.8 80.2 80.8 85.1 85.0 87.5 86.3 90.2 92.5 92.0 damage geometry, as the denoising process lacks global contextual priors. Our CDiffDet+ demonstrates transformative improvements through global-local fusion: Enhanced Precision and Localization: Bounding boxes generated by CDiffDet+ are consistently more precise and contour-adherent, particularly for challenging geometries like elongated scratches or irregular cracks. This is direct consequence of the ACE-block sharpening high-level feature maps and the CAF mechanism using global context to resolve boundary ambiguities. Enhanced Sensitivity to Challenging Instances: Our model demonstrates remarkable ability to detect faint, low-contrast damage that both DCN+ and DiffusionDet overlook. This is hallmark of effective information fusion, where the global context prior provides top-down signal that guides the localization of features which are statistically insignificant in isolation but meaningful within the broader scene context. Increased Prediction Confidence: Our model exhibits significantly higher and more consistent confidence scores in its predictions. This reduced epistemic uncertainty is direct result of the disambiguating power provided by the global context embedding, transforming ambiguous guesses into certain, informed decisions. These observations visually validate the quantitative gains in Tables 23, confirming that fusing local evidence with global context is essential for robust finegrained detection. 19 + D ) B ( f s Figure 2: This figure compares the performance of our model against the baseline. The first row contains the original images with the ground truth annotations. The second row shows the bounding boxes generated by DiffusionDet. The third row shows the more accurate bounding boxes produced by our enhanced model. 4.2.3. Heatmaps Visualization Figure 3 compares the predictions of the DiffDet model with those of our C-DiffDet+ model using representative samples from the CarDD dataset. While DiffDet can generally indicate damage, its responses tend to be diffuse and not well-aligned with the accurate contours of defects. The heatmaps often cover large undamaged areas, and subtle features such as hairline cracks or fine scratches are fragmented or completely overlooked. This limitation arises from its reliance on locally adapted features, which specular highlights, surface textures, and background clutter can easily disrupt. In contrast, the feature representations learned by C-DiffDet+ are significantly sharper and more defined. The activation maps are focused precisely on damaged regions, successfully capturing elongated scratches with continuous, well-defined responses and accurately tracing the jagged outlines of cracks. Low-contrast dents and punctures stand out better against their surroundings, enhancing their visibil20 ity even in challenging lighting conditions. Notably, the responses show much less overlap with undamaged areas, indicating that the model effectively suppresses noise while highlighting discriminative features. These improvements are substantial: they directly lead to better localization and more reliable detection. The feature maps provide the detector with richer boundary information, allowing for tighter bounding boxes around irregular damage, higher recall rates for subtle instances, and more stable confidence estimates. This qualitative evidence aligns with the quantitative gains reported in Tables 2 3, where C-DiffDet+ clearly outperforms DiffDet, especially in detecting finegrained categories like cracks. This demonstrates that the enhanced representational quality of our model underlies its superior performance in real-world automotive damage assessment. n r D s i u Figure 3: The images display visual comparison of two object detection models on Cardd dataset, DiffusionDet and our Model, for identifying car damage. The top row shows the original damaged car images. The middle row illustrates the heatmaps generated by the DiffusionDet model, which highlights areas it focuses on to detect damage. The bottom row presents the heatmaps from the Our Model combination, showing improved focus and accuracy in pinpointing the damaged regions. 21 5. Discussion 5.1. Ablation Studies 5.1.1. Quantitative Component Analysis Table 4 presents comprehensive ablation study that systematically evaluates the contribution of each architectural component in our C-DiffDet+ framework. The results demonstrate the synergistic effects of our proposed modules and provide quantitative evidence for the effectiveness of our context-aware approach. The baseline configuration (Baseline + CA) achieves an AP of 64.1%, representing 1.1% improvement over the pure baseline. This improvement validates the effectiveness of the Adaptive Channel Enhancement (ACE) mechanism, which dynamically re-weights channel features based on learned importance. The ACE blocks enable the model to focus on the most informative visual cues for damage detection, leading to enhanced feature representation quality. The improvement is particularly notable in small object detection (APS: 38.7% 41.5%, +7.2%) and medium object detection (APM : 36.1% 37.3%, +3.3%), demonstrating that channel attention is crucial for detecting fine-grained damage patterns. The addition of Context-Aware Fusion (CAF) without global embeddings (Baseline + CA + CAF) maintains the same overall AP (64.1%) but shows significant improvements in specific damage categories. Glass shatter detection improves dramatically from 92.6% to 96.7% (+4.4%), while lamp broken detection increases from 76.8% to 79.6% (+3.6%). These improvements can be attributed to the cross-attention mechanism that enables local proposals to attend to global scene context, proving particularly effective for damage types that require comprehensive environmental understanding. However, the absence of global embeddings limits the models ability to leverage scene-level information, resulting in suboptimal performance in small object detection (APS: 18.8%, -45.5% from baseline). The combination of ACE and CAF (Baseline + CA + CAF) achieves the second-best overall performance (AP: 64.3%) and demonstrates the complementary nature of these components. This configuration shows strong performance in large object detection (APL: 65.5%) and excels in glass shatter (97.2%) and lamp broken (82.5%) detection. The synergy between channel enhancement and context-aware fusion enables the model to leverage both local feature refinement and global context integration, leading to superior performance in damage types that benefit from both mechanisms. The integration of global embeddings (Baseline + Global_emb + CAF) without channel attention achieves competitive performance (AP: 64.0%) and shows particular strength in crack detection (41.2%, 22 +17.4% over baseline). This improvement demonstrates that global context information is crucial for disambiguating fine cracks from visual artifacts, as global embeddings provide comprehensive scene understanding that helps distinguish between actual damage and natural surface variations. However, the absence of channel attention limits the models ability to focus on the most informative features, resulting in suboptimal performance in other categories. The full CDiffDet+ configuration (Baseline + CA + Global_emb + CAF) achieves the best overall performance (AP: 64.8%) and demonstrates the optimal synergy between all components. This configuration shows superior performance across multiple metrics: best overall AP (64.8%), best small object detection (APS: 45.5%), and best crack detection (42.2%). The combination of channel attention, global context integration, and cross-attention fusion creates synergistic effect where each component addresses different aspects of the detection challenge. The ablation results reveal several key insights about our architectural design. First, the ACE mechanism provides consistent improvements across all configurations, demonstrating its fundamental importance for feature representation quality. Second, the CAF mechanism shows strong performance in specific damage categories but requires global embeddings to achieve optimal results, indicating that cross-attention alone is insufficient without rich global context. Third, the global embeddings significantly improve performance in fine-grained detection tasks, particularly for damage types that are easily confused with visual artifacts. Finally, the full integration of all components achieves the best performance, validating our hypothesis that the combination of local feature enhancement, global context understanding, and cross-modal fusion is essential for superior automotive damage detection. These results quantitatively demonstrate that each architectural component contributes meaningfully to the overall performance, with the full integration achieving optimal results through synergistic effects. The ablation study validates our design choices and provides evidence that the context-aware diffusion framework effectively addresses the fundamental limitations of existing detection methods. 5.1.2. Global context encoder Table 5 presents comprehensive ablation study that evaluates the effectiveness of different global context encoder architectures in our C-DiffDet+ framework. The results demonstrate the critical importance of encoder design choices and provide quantitative evidence for the superiority of our proposed residual CNN encoder approach. The Residual CNN Encoder achieves the highest overall performance, with 23 Baseline ACE Global_emb CAF AP APS APM APL dent 38.9 39.5 37.3 35.9 39.1 37.0 36.1 37.3 27.1 29.2 36.4 39.2 64.2 64.4 65.3 65.5 64.4 65.8 38.7 41.5 18.8 20.9 35.3 45. 63.4 64.1 64.1 64.3 64.0 64.8 scratch crack glass 92.6 93.2 96.7 97.2 95.3 94.4 44.8 44.2 43.2 42.8 40.7 42. 35.1 36.5 35.6 34.7 41.2 42.2 lamp 76.8 80.0 79.6 82.5 75.6 80.2 tire 92.5 90.4 91.9 92.7 91.9 92.0 Table 4: Ablation study results. indicates presence, absence. Best results are in bold, second best are underlined. an Average Precision (AP) of 64.8%, and shows excellent results across various damage categories, confirming our architectural design choice. This encoder architecture effectively combines the computational efficiency of convolutional operations with the representational strength of residual connections (see Figure .6). This combination allows for the effective capture of multi-scale spatial information while ensuring stable gradient flow. Using residual connections is especially beneficial for automotive damage detection, where identifying fine-grained features at different scales is essential for accurate classification and localization of damage. The Simple CNN Encoder shows competitive performance, achieving an average precision (AP) of 64.6%. It ranks second-best in several categories, including dent detection 40.1%, which is the best, scratch detection 42.9%, which is the best, and medium object detection, with an average precision of 39.3% (the highest score in that category). This performance indicates that basic convolutional encoder can effectively capture global scene context when integrated with context-aware fusion mechanism. The architectures success is due to its ability to learn hierarchical feature representations without the computational overhead associated with more complex designs. This makes it an ideal choice for scenarios where computational efficiency is important. The ViT Encoder exhibits significant decline in performance, achieving an average precision (AP) of 59.0%, which is decrease of 5.8% from the best result. This drop in performance is evident across all metrics, indicating that transformerbased architectures are not well-suited for capturing global context in automotive damage detection. Several factors contribute to this performance decline, including the absence of inductive biases for spatial relationships that are typically found in convolutional architectures, the quadratic computational complexity of the model that limits its effective receptive field, and the lack of hierarchical feature extraction, which is essential for detecting damage at multiple scales. Additionally, the ViTs patch-based approach may struggle to accurately capture the 24 fine-grained spatial relationships needed for thorough damage assessment. The Mamba Vision Encoder exhibits the lowest performance, with an Average Precision (AP) of 54.7%, which is 10.1% lower than the best-performing model. This indicates that state-space model architectures are ineffective for encoding global context in this domain. Although the Mamba architecture offers linear complexity, it struggles with limited spatial modeling capabilities. This shortcoming is particularly detrimental for automotive damage detection, where understanding spatial relationships and integrating multi-scale features are essential. The performance decline is especially significant in small object detection, achieving an AP of 31.8% (13.7% lower than the best model), and for fine-grained damage types like cracks, which have an AP of 24.4% (17.8% lower than the best). The ablation results reveal several key insights about global context encoder design for automotive damage detection. First, convolutional architectures consistently outperform transformer and state-space model approaches, demonstrating that inductive biases for spatial relationships are crucial for effective global context encoding. Second, residual connections provide significant benefits, as evidenced by the superior performance of the Residual CNN Encoder across most metrics. The residual connections enable deeper networks with stable training, leading to better feature representation quality and improved damage detection accuracy. Table 5: Ablation study results with the best value in each column bolded and the second best underlined. Global Context Encoder Simple CNN Encoder[29] Residual CNN Encoder ViT Encoder[30] Mamba Vision Encoder[31] AP APS APM APL dent 40.1 39.3 64.6 64.8 37.0 39.2 35.1 36.8 59.0 35.2 34.1 54.7 66.2 66.0 62.3 58.3 39.7 45.5 31.5 31.8 scratch crack glass 93.2 94.2 88.5 85.1 42.9 42.6 37.2 38.1 40.5 42.2 26.6 24. lamp 78.8 80.2 73.4 71.8 tire 91.9 92.0 88.2 84.2 Third, the performance gap between different encoder architectures is particularly pronounced in challenging detection scenarios. Small object detection shows the most significant performance variation (APS: 31.5% to 45.5%), indicating that global context encoding is critical for fine-grained damage types requiring comprehensive scene understanding. Similarly, crack detection performance varies dramatically (24.4% to 42.2%), demonstrating that the quality of global context representation directly impacts the models ability to disambiguate subtle damage patterns from visual artifacts. The superior performance of the Residual CNN Encoder unequivocally validates our architectural choice. This design strikes an optimal balance between 25 Table 6: Model performance on VEhide dataset with full class names. Model DiffDet (baseline) C-DiffDet+ Broken Light Missing Part Dent Tear Puncture Scratch Cracked Glass 21.4 23.3 28.0 31.27 52.0 50.5 33.5 31.9 62.0 63. 21.0 22.1 15.0 15.1 Model DiffDet (baseline) C-DiffDet+ APbb APbb 54.0 33.3 55.3 33.9 50 APbb 33.1 34.1 75 APS APM APL 38.5 38. 20.5 20.8 6.5 6.5 representational power and computational efficiency, proving highly effective for automotive damage detection. The encoders strength lies in its combination of convolutional operations and residual connections, which enable the learning of increasingly abstract representations without sacrificing the fine-grained spatial information crucial for identifying damage across varying scales and types. This hierarchical feature extraction process is visually substantiated by the feature map analysis presented in the Appendix (see Figure .6), which clearly illustrates how the network progressively isolates salient damage features from raw pixel input. The quantitative results from our ablation study further reinforce this, providing strong evidence that integrating well-designed Global Context Encoder with residual connections is not just beneficial, but essential for achieving robust and accurate performance in complex automotive damage assessment scenarios. To evaluate the generalization capability of our proposed C-DiffDet+ framework, we conducted additional experiments on the VeHIDE (Vehicle Hide Damage) dataset, challenging benchmark containing occluded and partially visible damage instances. As shown in Table A.2 (Appendix), our model demonstrates consistent performance improvements over both the baseline DiffusionDet and previous state-of-the-art methods. C-DiffDet+ achieves an overall AP of 33.9% on VeHIDE, representing 0.6% improvement over the baseline DiffusionDet (33.3%). This performance gain is particularly notable given the datasets emphasis on subtle, low-visibility damage patterns that require sophisticated contextual reasoning. The improvement is most pronounced at higher IoU thresholds, with C-DiffDet+ achieving 34.1% AP75 compared to DiffusionDets 33.1%, indicating superior bounding box localization precision. The contextual reasoning capabilities of our CAF mechanism prove especially valuable for the VeHIDE dataset, where many damage instances are partially oc26 Figure 4: Training loss curves comparing baseline DiffusionDet (blue) and C-DiffDet+ (orange) over 20,000 iterations. cluded or exhibit minimal visual contrast with surrounding vehicle surfaces. The global context encoding enables the model to infer damage presence based on contextual cues and spatial relationships, overcoming limitations of local feature analysis alone. This is evidenced by the models improved performance on small object detection (APs), where it maintains competitive performance despite the datasets additional challenges. These results demonstrate that the benefits of our context-aware diffusion framework extend beyond the CarDD benchmark, validating the generalizability of our approach across different automotive damage detection scenarios. The consistent performance improvement on both datasets confirms that integrating global context representation with diffusion-based detection provides robust solution for real-world vehicle inspection applications. The convergence behavior of the training loss, as depicted in Figure 4, provides critical insights into the efficiency and stability of our proposed C-DiffDet+ architecture compared to the baseline DiffusionDet model [28]. The \"Total Loss vs Iteration\" graph demonstrates several key advantages of our context-aware approach that directly correlate with the improved detection performance shown in our quantitative evaluations. 5.2. Training Loss Convergence Analysis Our C-DiffDet+ model exhibits significantly superior convergence characteristics. The model starts with substantially lower initial loss, indicating that our architectural enhancements provide better initialization and more stable gradient flow from the outset. This is attributable to the effective integration of the Global Context Encoder and Context-Aware Fusion mechanisms. 27 The convergence trajectory of our model shows both faster initial learning and superior final converged state, where it achieves demonstrably lower total loss than the baseline. While the baseline model undergoes erratic oscillations, our approach maintains smoother, more stable learning curve, which is crucial for diffusion-based architectures. The enhanced convergence can be mathematically explained through the improved gradient flow enabled by our architectural design. In the baseline DiffusionDet, predictions are generated solely from local region-of-interest (RoI) features, Froi. The gradient with respect to the model parameters θ is computed as: θLbase = θLmatch (yi, θ(Froi)) (39) In contrast, our C-DiffDet+ model first enriches these local proposal features with global context. The Context-Aware Fusion (CAF) mechanism integrates the local features Froi with global context extracted by the Global Context Encoder (GCE) from the input image I: Fcontext-aware = CAF (Froi, GCE(I)) (40) The final prediction is then generated from these context-aware features. Consequently, the gradient for our model is computed on predictions that are better informed: θLours = θLmatch (yi, θ(Fcontext-aware)) (41) The key difference highlighted by comparing Equation 39 and Equation 41 is that while the loss function Lmatch is the same, our models loss is computed on predictions derived from features enhanced with global context. This leads to more informative gradients during backpropagation, resulting in smoother and faster convergence. Our architecture allows the Global Context Encoder (GCE) to be optimized effectively. The gradient of the loss with respect to the GCE parameters, θGCE, is computed via the chain rule through the subsequent layers: θGCELours = θGCELmatch (yi, θ (CAF(Froi, GCE(I)))) (42) This dedicated gradient path allows the GCE to learn optimal global context representations specifically for the task of damage detection. Furthermore, the Adaptive Channel Enhancement (ACE) blocks contribute by dynamically reweighting channel features. The channel attention mechanism is expressed as: Fenhanced = Foriginal σ (MLP (GlobalAvgPool(Foriginal))) (43) 28 where denotes element-wise multiplication and σ is the sigmoid activation. This mechanism improves convergence by focusing gradient updates on the most discriminative channels, accelerating the learning process. The improved gradient flow can also be understood through the Lipschitz continuity of our loss function. The context-aware fusion mechanism helps create more Lipschitz-smooth loss landscape, which satisfies: θLours(θ1) θLours(θ2) θ1 θ2 (44) where is the Lipschitz constant. smaller Lipschitz constant leads to more stable gradient updates and faster convergence, which is what we observe in our training curves. The superior convergence characteristics of our model validate our design choices and demonstrate that integrating global context awareness with diffusionbased detection creates more stable and efficient learning process. This improved training dynamic directly contributes to the enhanced detection performance, making our approach not only more effective but also more practical for real-world deployment. 5.3. Component Analysis for Automotive Damage Detection 6. Conclusion and Future Work This work introduced C-DiffDet+, novel framework that successfully bridges the gap between generative denoising processes and global contextual reasoning for the challenging task of fine-grained object detection. Our core contribution lies in the seamless integration of dedicated Global Context Encoder (GCE) with diffusion-based detection head via principled Context-Aware Fusion (CAF) mechanism. This design explicitly addresses fundamental shortcoming in existing methods: the reliance on locally conditioned features that are often ambiguous in complex real-world scenes like automotive damage assessment. Extensive experimental validation on the CarDD benchmark demonstrates that our approach sets new state-of-the-art, achieving significant +3% AP improvement over previous best models. More importantly, the gains are most pronounced on the most challenging fine-grained categoriescracks and scratcheswhere contextual disambiguation is crucial. The superior performance is complemented by rigorous ablation study that quantitatively deconstructs the contribution of each component, confirming the synergistic effect of global context conditioning and adaptive feature enhancement. 29 6.1. Limitations and Dataset Biases Despite achieving state-of-the-art performance, the proposed C-DiffDet+ framework is subject to certain limitations, some of which are inherent to the challenge of fine-grained detection and the datasets available for training. primary limitation, observable in the qualitative results (Figure 5), is the models occasional failure to precisely localize highly irregular or diffuse damage boundaries. As shown in the second row of Figure 5, our model may generate bounding boxes that are either overly conservative, failing to capture the full extent of large scratch or dent, or imprecisely localized, slightly misaligning with the damages true orientation. These errors often occur in cases of low-contrast damage or complex textures where the visual cues are ambiguous even to human annotators. Furthermore, our analysis reveals that the evaluation process itself is sometimes hampered by inconsistencies in ground-truth annotations. The first row of Figure 5 showcases examples where the provided ground-truth boxes are either incomplete or do not tightly conform to the damages actual pixels. This annotation noise presents fundamental challenge for training and fairly evaluating high-precision detection models, as it introduces ceiling on achievable localization accuracy and can penalize models that predict more geometrically accurate boxes than the ground truth provides. These observations point to two important future directions: 1) the development of more robust loss functions that are less sensitive to annotation noise and outliers, and 2) the exploration of segmentation-based or polygon-based representation for damage detection, which would be better suited for capturing the amorphous shapes of many damage types than axis-aligned bounding boxes. Future Work The principles established here are broadly applicable. Our immediate future work will focus on two fronts: 1. Efficiency: Exploring knowledge distillation and conditional computation techniques to distill the global contextual knowledge into more lightweight architecture suitable for real-time deployment. 2. Generalization: Extending the context-aware paradigm to other vision tasks where global scene understanding is critical but underexplored in generative models, such as scene graph generation or 3D object detection from monocular images. We also plan to investigate self-supervised pre-training strategies on large-scale unlabeled data to learn even more robust and generalizable contextual representations. 30 Figure 5: Qualitative comparison between ground truth annotations in the CarDD dataset and our models predictions, illustrating both detection errors and inconsistencies in the dataset annotations that limit precise evaluation. Model DiffDet C-DiffDet+ MACs (G) Params (M) GFLOPs FPS (single GPU) 375.292 385.686 169.110 169.485 750.58 771.37 11.12 10.63 Appendix A. Example Appendix Section Appendix A.1. Computational Complexity Appendix B. Complete DiffusionDet Algorithm Appendix .1. Main Algorithm Appendix .2. Global Context Encoder (GCE) Algorithm Appendix .3. Multi-Modal Fusion (MMF) Algorithm 31 t ) i=1) i=1, img i=1 = Backbone(I) i=1 = FPN({h(i)} Initialize proposals: X0 = GT_Boxes RBK4 Initialize proposals: X0 = Random_Noise RBN 4 Algorithm 1 DiffusionDet: Context-Aware Diffusion-Based Object Detection Require: Input image RB3HW , number of proposals = 300, diffusion steps = 1000 Ensure: Detection results: bounding boxes Bout RBK4, class labels Cout RBKC 1: Stage 1: Feature Extraction 2: Extract backbone features: {h(i)}4 3: Apply ACE to Stage 4: h(4) = ACE(h(4)) {Adaptive Channel Enhancement} 4: Generate FPN features: {Pi}5 5: Extract global context: = GlobalContextEncoder(I) RB256 6: Stage 2: Proposal Initialization 7: if training mode then 8: 9: else 10: 11: end if 12: Normalize proposals: X0 = Normalize(X0, scale = 2.0) 13: Stage 3: Diffusion Process 14: for = 1 downto 0 do 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: end for 48: Stage 4: Post-Processing 49: Final denormalization: Bf inal = Denormalize(X0) 50: Apply NMS: Bout, Cout = NMS(Bf inal, pcls) 51: return Bout, Cout Stage 3.1: ROI Feature Extraction Denormalize proposals: img = Denormalize(Xt) Assign FPN levels: levels = AssignFPNLevel(X img Extract ROI features: Froi = ROIAlign({Pi}5 , levels) Aggregate features: Froi = GlobalAvgPool(Froi) RBN 256 Stage 3.2: Multi-Modal Attention Self-attention: Fself = MultiHead(Froi, Froi, Froi) Cross-attention: Fcross = MultiHead(Fself , g, g) {Context-Aware Fusion} Instance interaction: Fenhanced = MultiHead(Fcross, Froi) Stage 3.3: Multi-Modal Conditioning Time embedding: Et = TimeEmbedding(t) RB256 Positional encoding: = PositionalEncoding(N ) RN 256 Context embedding: Cemb = MLP(g) RB256 Fuse embeddings: Ff used = Fenhanced + Et + + Cemb Stage 3.4: Final MLP Projection Apply final MLP: Ff inal = MLPfinal(Ff used) RBN 256 Stage 3.5: Prediction Heads Classification: pcls = σ(MLPcls(Ff inal)) RBN Box regression: = MLPreg(Ff inal) RBN 4 Noise prediction: ˆϵθ = MLPnoise(Ff inal) RBN 4 Stage 3.6: DDIM Sampling Predict clean boxes: ˆx0 = Xt Update proposals: Xt1 = Stage 3.7: Box Renewal if > 0 and t%renewal_step == 0 then Filter high-confidence proposals: keep_mask = max(pcls) > 0.5 Count kept proposals: Kkept = (cid:80)(keep_mask) if Kkept < then Generate new proposals: Xnew = Random_Noise(N Kkept) Replace low-confidence proposals: Xt1[keep_mask] = Xnew 1 αt ˆϵθ 1 αt1 ˆϵθ αt1 ˆx0 + end if end if 32 Algorithm 2 Global Context Encoder for Scene-Level Understanding Require: Input image RB3HW , target dimension Cout = 256 Ensure: Global context vector RBCout 1: Step 1: Multi-Scale Feature Extraction 2: Conv1: F1 = ReLU(Conv2d(3 64, = 3, = 2, = 1)(I)) RB64H/2W/2 3: Conv2: F2 = ReLU(Conv2d(64 128, = 3, = 2, = 1)(F1)) RB128H/4W/ 4: Conv3: F3 = ReLU(Conv2d(128 Cout, = 3, = 2, = 1)(F2)) RBCoutH/8W/8 5: Step 2: Residual Connection 6: Downsample input: Ids = AvgPool2d(k = 3, = 2, = 1)(I) 7: Further downsample: Ids2 = AvgPool2d(k = 3, = 2, = 1)(Ids) 8: Final downsample: Ids3 = AvgPool2d(k = 3, = 2, = 1)(Ids2) 9: Project residual: Ires = Conv2d(3 Cout, = 1)(Ids3) RBCoutH/8W/8 10: Step 3: Feature Fusion 11: Add residual: Ff used = F3 + Ires RBCoutH/8W/8 12: Step 4: Global Context Aggregation 13: Global average pooling: gspatial = AdaptiveAvgPool2d(1, 1)(Ff used) RBCout11 14: Flatten: = view(gspatial, [B, 1]) RBCout 15: return Algorithm 3 Multi-Modal Feature Fusion and Conditioning Require: Enhanced ROI features Fenhanced RBN 256, global context RB256, timestep t, number of proposals , number of classes Ensure: Multi-modal fused features Ff used RBN 256 1: Step 1: Time Embedding Generation 2: Sinusoidal time encoding: Et = TimeEmbedding(t) RB256 3: Time projection: Et = MLPtime(Et) RB256 4: Broadcast time: Et = expand(Et, [B, N, 256]) RBN 256 5: Step 2: Positional Encoding 6: Generate position indices: pos = [0, 1, 2, . . . , 1] RN 7: Sinusoidal positional encoding: = PositionalEncoding(pos) RN 256 8: Broadcast positions: = expand(P E, [B, N, 256]) RBN 256 9: Step 3: Context Embedding 10: Context projection: Cemb = MLPcontext(g) RB256 11: Broadcast context: Cemb = expand(Cemb, [B, N, 256]) RBN 256 12: Step 4: Multi-Modal Feature Fusion 13: Additive fusion: Ff used = Fenhanced + Et + + Cemb 14: Layer normalization: Ff used = LayerNorm(Ff used) 15: Step 5: Adaptive Feature Modulation 16: Compute modulation weights: α = σ(MLPmod(g)) RB11 17: Context modulation: Fcontext = α Cemb 18: Final fusion: Ff used = Ff used + Fcontext 19: return Ff used 34 Appendix .4. Adaptive Channel Enhancement (ACE) Algorithm Algorithm 4 Adaptive Channel Enhancement (ACE) Require: Input features RBLC, reduction ratio = 16 Ensure: Enhanced features xout RBLC 1: Step 1: Spatial Aggregation (Squeeze) 2: Permute dimensions: = permute(x, [0, 2, 1]) RBCL 3: Global average pooling: µ = GlobalAvgPool(x) RBC 4: Step 2: Channel Learning (Excitation) 5: Dimension reduction: y1 = ReLU(FC1(µ)) RBC/r 6: Dimension restoration: y2 = σ(FC2(y1)) RBC 7: Reshape attention weights: α = y2 RBC1 8: Step 3: Feature Enhancement 9: Broadcast weights: α = permute(α, [0, 2, 1]) RB1C 10: Apply attention: xout = α 11: return xout Appendix .5. Context-Aware Fusion (CAF) Algorithm 35 Algorithm 5 Context-Aware Fusion through Cross-Attention Require: ROI features Froi RBN 256, global context RB256, number of heads = 8, head dimension dk = 32 Ensure: Context-aware features Fcross RBN 256 1: Step 1: Query, Key, Value Projections 2: Query projection: = Froi WQ RBN 256 3: Key projection: = WK RB1256 4: Value projection: = WV RB1256 5: Step 2: Multi-Head Attention 6: Split into heads: Qh, Kh, Vh = SplitHeads(Q, K, V, h, dk) 7: for = 1 to do 8: Compute attention scores: scoresi = Qi RBN 1 Apply softmax: weightsi = softmax(scoresi) RBN 1 RBN dk 9: 10: Weighted aggregation: headi = weightsi 11: end for 12: Step 3: Head Concatenation and Output 13: Concatenate heads: Fheads = Concat({headi}h 14: Output projection: Fcross = Fheads WO RBN 256 15: Step 4: Context Modulation 16: Compute modulation weight: β = σ(MLP(g)) RB11 17: Gated fusion: Fmodulated = β Fcross + (1 β) Froi 18: return Fmodulated h(Ki dk h)T i=1) RBN 256 36 Appendix .6. FPN Level Assignment Algorithm Algorithm 6 FPN Level Assignment for Multi-Scale ROI Processing Require: Proposal boxes RBN 4 (in pixel coordinates), base size sbase = 224 Extract box dimensions: = X[b, n, 2] X[b, n, 0] Extract box dimensions: = X[b, n, 3] X[b, n, 1] Calculate box area: area = Calculate box size: size = area Ensure: FPN level assignments levels RBN 1: Step 1: Box Size Calculation 2: for each batch and proposal do 3: 4: 5: 6: 7: end for 8: Step 2: Logarithmic Level Assignment 9: for each batch and proposal do 10: 11: 12: end for 13: Step 3: Level Mapping 14: Map levels to FPN features: 15: 1 (stride 2): levels == 1 {Very small objects} 16: 2 (stride 4): levels == 2 {Small objects} 17: 3 (stride 8): levels == 3 {Medium objects} 18: 4 (stride 16): levels == 4 {Large objects} 19: 5 (stride 32): levels == 5 {Very large objects} 20: return levels Compute level: level = log2(size[b, n]/sbase) + 4 Clamp to valid range: levels[b, n] = clamp(level, 1, 5) Analysis of Global Context Encoder Feature Maps Figure .6 provides comprehensive visualization of the feature representations learned at each processing stage within our Global Context Encoder. This analysis offers crucial insights into the hierarchical feature learning process that enables effective scene understanding. Low-level Feature Extraction (Conv1) The initial convolutional layer produces feature maps that capture the fundamental visual elements essential for damage detection. These include edge 37 and contour responses along damage boundaries, texture patterns that distinguish damaged from intact surfaces, and color gradient information critical for identifying scratches and paint damage. These basic shape patterns serve as the foundational building blocks for higher-level processing. Intermediate Feature Integration (Conv2) The second convolutional layer demonstrates progressive feature integration, combining low-level elements into more complex patterns. This stage shows emerging activation around potential damage regions and begins the initial suppression of irrelevant background information. Through this feature composition, the layer starts to form prototype damage representations. High-level Semantic Encoding (Conv3) The final convolutional layer produces semantically rich feature maps characterized by strong, focused activations that are precisely localized to damage areas. These maps provide clear differentiation between damage types like scratches, dents, and cracks. This layer also effectively suppresses visual distractions such as reflections and shadows while encoding the contextual relationships between the damage and surrounding vehicle parts. Residual Pathway Processing (A) The residual pathway is crucial for preserving spatial information. It maintains fine-grained details through progressive downsampling, providing spatial precision that is complementary to the main pathways semantic encoding. This process ensures the positional accuracy needed for final damage localization. Feature Fusion and Context Aggregation (B + Output) The final processing stage integrates information from both pathways. First, an element-wise fusion (Ff used = F3 + Ires) combines the high-level semantics with spatial precision. Next, global average pooling aggregates this spatial information into compact context vector g. The resulting final representation successfully captures both detailed damage information and the global scene context. This hierarchical feature progression demonstrates the encoders capability to transform raw pixel information into semantically meaningful representations that enable precise damage localization and effective resolution of ambiguous detection scenarios. The visualization confirms that our GCE successfully learns to focus on relevant damage patterns while incorporating broader contextual information essential for accurate automotive damage assessment. 38 Figure .6: Visualization of feature maps at each stage within the Global Context Encoder (GCE) Acknowledgement The authors thank Mr. Arturo Argentieri from CNR-ISASI Italy for his technical contribution to the multi-GPU computing facilities. This research was partially funded by the Italian Ministry of University and Research with the grant \"Future Artificial Intelligence ResearchFAIR\" CUP B53C22003630006 grant number PE0000013. References [1] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, in: Proc. IEEE CVPR, 2014. [2] R. Girshick, Fast r-cnn, in: Proc. IEEE ICCV, 2015. 39 [3] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, in: Proc. NeurIPS / arXiv, 2015. [4] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified, real-time object detection, in: Proc. IEEE CVPR, 2016. [5] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg, Ssd: Single shot multibox detector, in: Proc. ECCV, 2016. [6] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollár, Focal loss for dense object detection, in: Proc. IEEE ICCV, 2017. [7] L. Tan, T. Huangfu, L. Wu, W. Chen, Comparison of retinanet, ssd, and yolo v3 for real-time pill identification, BMC Medical Informatics and Decision Making 21 (1) (2021) 324. doi:10.1186/s12911-021-01691-8. URL https://doi.org/10.1186/s12911-021-01691-8 [8] H. Law, J. Deng, Cornernet: Detecting objects as paired keypoints, in: Proc. ECCV, 2018. [9] X. Zhou, D. Wang, P. Krähenbühl, Objects as points, in: Proc. arXiv / ICCV workshop, 2019. [10] Z. Tian, C. Shen, H. Chen, T. He, Fcos: Fully convolutional one-stage object detection, in: Proc. ICCV, 2019. [11] P. Sun, R. Zhang, Y. Cao, C. Shen, X. Jin, Z. Shen, H. Xiao, Sparse r-cnn: End-to-end object detection with learnable proposals, in: Proc. ECCV, 2021. [12] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko, End-to-end object detection with transformers, in: Proc. ECCV, 2020. [13] J. Ho, A. Jain, P. Abbeel, Denoising diffusion probabilistic models, NeurIPS (2020). [14] A. Nichol, P. Dhariwal, Improved denoising diffusion probabilistic models, arXiv preprint arXiv:2102.09672 (2021). [15] Y. Song, S. Ermon, Generative modeling by estimating gradients of the data distribution, Proc. NeurIPS / arXiv (2019). 40 [16] S. Chen, P. Sun, Y. Song, P. Luo, Diffusiondet: Diffusion model for object detection, arXiv preprint arXiv:2211.09788v2 (2023). [17] X. Wang, W. Li, Z. Wu, Car damage detection (cardd): large-scale dataset for vision-based car damage assessment, IEEE Transactions on Intelligent Transportation Systems 24 (9) (2023) 12341247. [18] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, Y. Wei, Deformable convolutional networks, in: Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 764773. [19] Y. Cao, J. Xu, S. Lin, F. Wei, H. Hu, GCNet: Non-local networks meet squeeze-excitation networks and beyond, in: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2019, pp. 19711980. [20] A. Nichol, P. Dhariwal, Improved denoising diffusion probabilistic models, arXiv preprint arXiv:2102.09672 (2021). [21] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, Feature pyramid networks for object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017, pp. 21172125. [22] X. Wang, W. Li, Z. Wu, Cardd: new dataset for vision-based car damage detection, IEEE Transactions on Intelligent Transportation Systems 24 (7) (2023) 72027214. doi:10.1109/TITS.2023.3258480. [23] K. He, G. Gkioxari, P. Dollár, R. Girshick, Mask r-cnn, in: 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 29802988. doi:10.1109/ICCV.2017.322. [24] Z. Cai, N. Vasconcelos, Cascade r-cnn: High quality object detection and instance segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (5) (2021) 14831498. doi:10.1109/TPAMI.2019. 2956516. [25] Y. Cao, J. Xu, S. Lin, F. Wei, H. Hu, Gcnet: Non-local networks meet squeeze-excitation networks and beyond, in: 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2019, pp. 19711980. doi:10.1109/ICCVW.2019.00246. 41 [26] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, C. C. Loy, D. Lin, Hybrid task cascade for instance segmentation, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 49694978. doi:10.1109/CVPR. 2019.00511. [27] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, Y. Wei, Deformable convolutional networks, in: 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 764773. doi:10.1109/ICCV.2017.89. [28] S. Chen, P. Gu, J. Zhang, X. He, J. He, Q. Tian, Y. Zhou, Y. Qiao, DiffusionDet: Diffusion model for object detection, in: Proceedings of the 39th International Conference on Machine Learning (ICML), Vol. 162 of Proceedings of Machine Learning Research, 2022, pp. 32313245. [29] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Advances in Neural Information Processing Systems (NeurIPS), Vol. 25, 2012, pp. 10971105. URL hash/c399862d3b9d6b76c8436e924a68c45b-Abstract. html https://proceedings.neurips.cc/paper/2012/ [30] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=YicbFdNTTy [31] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, X. Wang, Vision mamba: Efficient visual representation learning with bidirectional state space model, arXiv preprint arXiv:2401.09417 (2024). URL https://arxiv.org/abs/2401."
        }
    ],
    "affiliations": [
        "CNR-STIIMA, Institute of Intelligent Industrial Systems and Technologies for Advanced Manufacturing, c/o Campus Ecotekne, Via Monteroni, Lecce, 73100, LE, Italy",
        "Department of Innovation Engineering, University of Salento & Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
        "Department of Innovation Engineering, University of Salento, Via per Monteroni, Lecce, 73100, Lecce, Italy",
        "Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, Lecce, 73100, Lecce, Italy",
        "Sorbonne University Abu Dhabi, UAE",
        "UPV/EHU, University of the Basque Country, Sebastian, 20018, Sebastian, Spain"
    ]
}