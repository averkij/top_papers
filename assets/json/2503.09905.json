{
    "paper_title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
    "authors": [
        "Allison Andreyev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git"
        },
        {
            "title": "Start",
            "content": "Quantization for OpenAIs Whisper Models: Comparative Analysis 1st Allison Andreyev Independent Researcher Washington DC, United States allisonmandreyev@gmail.com 5 2 0 2 2 1 ] . [ 1 5 0 9 9 0 . 3 0 5 2 : r AbstractAutomated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for ofﬂine transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantiﬁes the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19% and model size by 45%, while preserving transcription accuracy. These ﬁndings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in Appendix Sec. A. Index TermsArtiﬁcial Intelligence, Large Language Models, Quantization, Automatic Speech Recognition I. INTRODUCTION The rise of large language models (LLMs) has enabled advancements across multiple communication modalities, including speech processing. Whisper is an automated speech recognition (ASR) system developed by OpenAI, designed for applications such as speech transcription, live translation, and captioning [18]. The model has been trained with 680,000 hours of audio data, far surpassing the magnitude of training data used for ASR models. Furthermore, as training data has been divvied into 97 total languages, Whisper is compatible to act as translation machine, frequently found to perform better than LLM-based ASR models [20]. This robust approach has made Whisper one of the leading ASR models in both research and practical applications, and allows Whisper to be amongst the top performers across many applications of speech processing, such as through translation, transcription, speech recognition, and zero-shot evaluation [17]. Additionally, due to Whispers open support for quantization e.g whispercpp lightweight model with built in quantization features, it led me to use Whisper as the focus of this study [2]. Currently, Whisper contains ﬁve versions: tiny, small, base, medium, and large. Due to its accuracy and easy of use, Whisper is becoming increasingly popular in both larger model variants exhibit research and commercial applications [12]. However, these models are not always accurate when transcribing speech, and some transcriptions have been found to contain hallucinations. increased latency Moreover, and computational demands, making deployment on resourceconstrained devices more challenging. [10] ﬁnd that roughly 1% of audio transcriptions by Whisper contained entire hallucinated phrases or sentences, while also ﬁnding 38% of hallucinations included harms such as violence, inaccuracies or false authority. While prior research has explored ﬁne-tuning strategies to enhance Whispers performance [6], [8], [11], [12], [14], [19], [21], the impact of quantization on model size reduction and latency optimization remains underexplored. A. Contributions and Organization This paper addresses this research gap with few contributions: Evaluating the capabilities of Whisper and 2 variants: Whisper_Streaming & whisper-timestamped, with an emphasis on their similarities and differences. Establishing and deﬁning quantization techniques and relevant hardware considerations applicable to Whisper models. qualitative review on usage between the Whisper and Whisper_Streaming & whisper-timestamped Examining model performance in terms of word error rate, processing speed, and latency, relative to model size, version, runtime, and quantization approach. Summarizing both qualitative and quantitative results from two experimental evaluations. describe current research results and limitations in Sec. II. Sec. III qualitatively compares Whisper and its two variants. In Sec. IV, conduct model performance experiment, followed by discussion of qualitative results in Sec. V. Sec. VI compares the Whisper base model size with its performance on the LibriSpeech audio dataset. Sec. VII introduces quantization and its applications for Whisper, while Sec. VIII compares the performance of whispercpp based on run type and quantization method. Sec. IX evaluates the impact of three quantization methods on whispercpps accuracy, latency, and model size. Finally, Sec. discusses the practical applications of this research, with conclusion in Sec. XI. B. Note on Naming Conventions In this work, explore several variants of the Whisper model. The naming conventions across these variants, developed by external organizations or individuals, are inconsistent, with some adopting different conventions: Whisper: Standard model developed by OpenAI whispercpp: C++ implementation of the Whisper model whisper-timestamped: Version with added timestamping functionality Whisper Streaming: Version adapted for streaming For consistency in this document, will refer to these variants using the most common or descriptive form of the name. Please note that the developers may use different naming conventions across various implementations. II. LITERATURE REVIEW So far, several studies have been conducted on quantizing LLMs and ASR speech transcription. [17] evaluate the Whisper model, describing it as an encoder-decoder transformer model, in which decoder predicts corresponding text caption for each audio segment. They also note that Whisper was trained on large and diverse dataset, which explains why its performance on speciﬁc dataset may not be as high as model trained on only one kind [17]. [20] evaluate Whisper model capabilities, and contrast it with LLM-based ASR models. [20] explain how LLM-based ASR models use speech encoder to process speech and generate embeddings which are passed into decoder-only LLM. In their experimentation, they ﬁnd that the performance of LLMbased ASR models correlates positively with the proﬁciency of the LLM in the language being recognized, posing limitation for LLM based ASR models. Additionally, [1] studies Whisper hallucinations in audio transcriptions, noting that Whisper has tendency to generate and produce incorrect repetitions of recognized text. Hallucinations refer to the generation of transcriptions that include fabricated or incorrect information that was not part of the original speech. Their experiment ﬁnds several offensive hallucinations produced in data transcription and notes that audio length signiﬁcantly affects the error rate and audio content had minimal relation on hallucination. [1]. Hallucinations pose challenge for Whisper, however this is issue that could be addressed with model quantization [22], method which has been previously found to decrease the WER, improving model accuracy [22]. [3] evaluate current quantization strategies, noting that moving from FP to INT quantization holds potential to reduce latency and memory footprint by factor of 16x. Several studies had conducted experiments quantizing ASR models. For example, [23] evaluates quantization methods effects on model latency, ﬁnding that quantization creates faster model inference and allows for model deployment on portable devices, on which stable network connection could be limited. However, the study mainly evaluates INT8 quantization, which is only one kind of integer quantization and may not speak for all methods such as INT4 or INT5, respectively. Meanwhile, [9] discusses quantizing ASR models: they ﬁnd that neural network architectures such as Whisper perform poorly on edge hardware due to computation requirements, and note that prior research on quantizing ASR models is limited. Notably, QAT requiring training and validation data during quantization may not always be available due to privacy or security issues, forming limitation for quantization models which require QAT. [22] evaluate quantization methods on Whisper accuracy and model size, relying on the P4Q quantization strategy, which utilizes block-wise N4 quantization applied to the models primary weights. The study evaluates their methods on 4 quantized Whisper models, demonstrating improvement in latency and accuracy, with 15.1% WER reduction for quantized Whisper. However, similar to [23], this study doesnt evaluate more than 1 type of quantization, which limits the experiment in terms of deducing pattern or relationship between quantization methods and model performance. Additionally, [23] proposes general quantizer which uses quantization scheme with ﬂoating point (FP) and backward-pass quantization aware training (BP-QAT). To evaluate this model, they use Whisper and the LibriSpeech dataset and benchmark accuracy using standard WER, ﬁnding it improves by up to 5.7% with their quantization methods. This method, however, is only evaluated on one model and version of Whisper, so its accuracy and results are not conﬁrmed for other sizes. Research on quantization has found it could bear beneﬁts to model accuracy, latency, and deployment opportunities [3], [9], [22], [23]. Quantizing the model could beneﬁt users who dont have stable internet access, or need to use the model on mobile device. The translation and transcription features of the model pose as key resources needed by the hard of hearing community along with language barriers. However, current research is limited in terms of quantization strategies applied, and latency categories studied. Furthermore, up until now, few studies have compared Whisper and its variants -whisper-timestamped and Whisper_Streaming and the implications of each model individually. By analyzing Whispers variants, greater insights can be analyzed about the model backend and differentiating factors. For example, Whisper_Streaming uses self-adaptive latency [15], possibly affecting how latency is impacted by quantization. Meanwhile, whisper-timestamped uses Dynamic Time Warping (DTW) approach (unique to whisper-timestamped) which allows improved timestamp accuracy, while also featuring conﬁdence scores for each word. Furthermore, whisper-timestamped is able to process longer ﬁles with little additional memory usage compared to the Whisper Base model [4], [13]. This difference in memory handing and individual word conﬁdence scores pose interesting new research directions about this model. In all, the Whisper model comparisons can clearly conclude on variant applications and limitations, which play key role in further research and development. for This paper aims to address these limitations by providing comprehensive evaluation of the 3 Whisper variations while running quantization experiment using 3 methods of integer quantization with comprehensive model accuracy and latency analysis. III. MODEL COMPARATIVE ANALYSIS A. Whisper Whisper, developed by OpenAI, is an ASR model capable of speech transcription, translation, and language identiﬁcation. When transcribing, the model detects voice activity and attenuates background noise or music. When setting up Whisper, its dependencies PyTorch & ffmpeg command line tool must also be installed. OpenAI has recently made an API version that can be imported into any Python ﬁle as module for personal modiﬁcation. However, it is noted Whisper is not designed for real-time transcription, and is only made to process audio with at least 1 full sentence, which is preferably less than 30 seconds in length. B. Whisper Timestamped whisper-timestamped is version of Whisper that creates word timestamps and more exact estimations on speech segments using Dynamic Time Warping approach [13]. That way, start and end time estimations for speech are more accurate and each word is processed individually, receiving its own conﬁdence score. By employing this approach, whisper-timestamped can process longer ﬁles with minimal additional memory overhead. OpenAI offers 9GB docker ﬁle and light installation of CPU, and whisper-timestamped can also be used as Python module. whisper-timestamped has several output formats: Outputs data in JSON format with: detailed timestamp data, language detection, conﬁdence score CSV, SRT, VTT, TSV ﬁle Into speciﬁed output directory, verbose mode Additionally, computation and conﬁdence scores can be enabled and disabled for each word, and the user can choose whether punctuation should be committed [5], [15], [17]. C. Whisper Streaming Whisper_Streaming is an optimized variant of Whisper designed for real-time speech transcription and translation. Typically, Whisper_Streaming can transcribe live speech with 3.3 second latency. On top of dependencies required for Whisper, Whisper_Streaming requires the Libra Sound File, sound processing library, and requires installing the Whisper back end and the OpenAI API. Whisper_Streaming comes with 4 simulation modes: Start at: Starts processing audio at time provided by user. Ofﬂine: Processes the full audio ﬁle once in ofﬂine mode, and then ﬁnds the lowest word error ratio. Comp unaware: Timer that measures processes/events; does not count compute time, meant to lower latency bounds and get true latency. Default usage Next, there were few key similarities and differences between Whisper_Streaming and the other 2 models. Text and debug variables are outputted as soon as that piece of the speech is processed, doing the transcription live rather than outputting ﬁnal data at the end. The model takes signiﬁcantly longer time to process longer audio; however, similar to whisper-timestamped, Whisper_Streaming offers several customization features in Python ﬁles and the command line, such as an ofﬂine mode, customizing buffer the language used, model, timing, when streaming starts, and minimum segment size (what size the buffer transcribes at time). Longer audio ﬁles need to be split into small pieces and then merged. In low latency streaming mode, words can be split in the middle. Unlike the other models, Whisper_Streaming does not do sentence segmentation: it instead makes word-level timestamps. The model processes the new audio segment twice before ﬁnalizing, updates the buffer to the timestamp with conﬁrmed audio segment. Limits processing buffer window & reprocesses the conﬁrmed sentence time stamps before moving on to the next speech piece, this is because the objective is to limit buffer size and increase efﬁciency for longer audio segments, while still ensuring accuracy. 1) Limitations: Due to lots of terminal output, it was difﬁcult to see the full text transcript. Noted, this is mitigated however, by the model storing the transcript in separate txt ﬁle. Based on this output, Whisper_Streaming is not ideal for pre-recorded audios due to its buffering method and accumulation of text data, making the terminal harder to sort. Additionally, for increased accuracy, each audio segment has one word processed at time, which can cause lag in the software. All can be used as Python module using its API. IV. MODEL PERFORMANCE EXPERIMENT To evaluate the accuracy and latency of the base Whisper models, this study utilizes 25 audio ﬁles from the LibriSpeech dataset, comprising both clean and challenging speech samples [16]. All models were executed in standardized virtual environment using Jupyter Notebook on an HP Envy CPU to ensure consistency. From this experiment, qualitative results were derived on the performance of all 3 models relative to each other. V. QUALITATIVE RESULTS A. Whisper Timestamped whisper-timestamped provided additional personalization compared to Whisper, such as specifying ﬁle output directory. function in the Python module of whisper-timestamped also takes several parameters that allow for further customization on the way the speech is transcribed. This model breaks off speech into segments, and then segments into words, sharing timestamps and conﬁdence scores for each individually. While Whisper provided timestamps for each sentence, whisper-timestamped provided timestamp for each word and conﬁdence score for each sentence, phrase, and word, separately. Conﬁdence is rated on scale from 0.00 to 1.0. Notably, the conﬁdence score and processing speed stayed the same for the longer and more complicated pieces of text. whisper-timestamped also has few unique traits: Features progress bar at the top of the output, with FPS and percentage processed. Output by default is in JSON format. Specifying language when translating would remove the output featuring language probability. Between model sizes, the largest model processes text at about 400 FPS and the smallest model processes almost 2000 FPS. Conﬁdence levels increased dramatically for each word between the tiny and large models. On average, the base model took about 10 seconds to transcribe speech. All models did not interpret intonation to structure sentences and capitalization properly. B. Whisper Using Whisper without the timestamped feature provided me with some timestamps with text that were particularly long. These timestamps were typically range of few seconds (for each sentence). By default, the model provided 5 output ﬁles: JSON, vtt, srt, txt, tsv All models would use commas, punctuation, and capitalization correctly, they were able to apply grammatical rules to sentence, which differentiated it from other speech transcription models. For example, sentence such as Bobs dogs were happily playing with Cat wouldve translated properly with Whisper [capitalizing in cat to make name], but may have translated to Bobs dogs were happily playing with cat on non-AI powered transcription devices. The time it took for the models to compute the language and text was the same for the more complex speech as the more clean speech. (About 10 seconds for the base model) Unlike whisper-timestamped, Whisper did not output: conﬁdence scores, timestamps for each word, and language probability. For both models, performance on the test and development set performance was similar. Throughout usage, there were some notable semantics utilized by the model, such as intonation to determine capitalization and sentence structure. The model goes through 2 layers of transcription, and it adjusts transcription as it goes, predictions of text may change in buffer as theyre being double-checked or the next part is being listened to. A. Whisper Streaming Whisper_Streaming demonstrated strong accuracy, with automatically generated timestamps deviating no more than 0.5 seconds from manually recorded ones. However, the software would frequently start each timestamp from 0.00s, even though the words started being spoken at later point in the audio recording. Additionally, it was frequent pattern to notice the software undercount seconds needed to pronounce phrase, being about 0.2s ahead of human-recorded timestamps most of the time. B. Whisper Timestamped the same level of seemed to have whisper-timestamped had more distinct timestamps, with each word getting its separate time frame. It accuracy as Whisper_Streaming when it came to comparing the generated timestamps to human-recorded ones. The beneﬁt of whisper-timestamped over Whisper_Streaming was the increased precision of the timestamps and more detail. whisper-timestamped also started off with more accurate timestamps than Whisper_Streaming, which usually started each segment at 0.00s. C. Observations"
        },
        {
            "title": "The following table displays qualiative usage observations",
            "content": "with Whisper based on model size. TABLE QUALITATIVE WHISPER USAGE EXPERIENCE ON LIBRISPEECH DATASETS BASED ON MODEL SIZE AND SPEECH DIFFICULTY Model Size Tiny Small Medium Large Clean Speech Quick output ( 10s), low GPU/CPU usage, inaccuracies with larger text or names, capitalization issues 10-20s output, best capitalization, good timestamp details 20-40s output, similar accuracy to large model Long download (2GB), slow processing (up to couple of minutes), punctuation and capitalization issues Challenging Speech Misses small background noises, e.g., They worshiped only worship heard Similar to Medium, but 2x faster N/A Modiﬁes structure to be correct grammatically while matching audio more closely D. Results VI. MODEL SIZE VS. PERFORMANCE EXPERIMENT Using 10 distinct recordings from the test-clear and test-other data sets from Librispeech, each audio segment was manually timestamped and compared with the timestamps provided by Whisper_Streaming and whisper-timestampeds base versions. Timestamps went up to the centiseconds (cs). Whisper_Streaming demonstrated strong accuracy, with automatically generated timestamps deviating no more than 0.5 seconds from manually recorded ones. The algorithm for both models was programmed to detect and check for trailing sounds like or correctly and performed well against human-labeled timestamps. whisper-timestamped prioritized precision and granularity, albeit at the cost of increased processing time compared to other models. VII. QUANTIZATION WITH WHISPER IX. QUANTIZATION FOR OPTIMIZATION EXPERIMENT Quantization involves converting neural network (NN) or large language model (LLM) to lower-precision format, reducing memory requirements for deployment on resourcelimited devices. Quantization maps continuous input values to discrete levels at the output. Whisper, being an audio LLM, has the ability to be quantized to be deployed and used on smaller devices with less computing power. Thus, the following experiment evaluates whether model accuracy and latency are affected positively through quantization techniques. A. Hardware Support for Quantization The deployment of quantized models beneﬁts from hardware accelerators that optimize the efﬁciency of quantization tasks. Key hardware platforms include: AMD and ARM CPUs: Support for mixed-precision operations and 8-bit integer quantization, with notable examples including AMD Zen 4 and ARM Neoverse V1/V2 architectures. Apple Silicon and NVIDIA GPUs: Apples chips (A17 Pro, M4) and NVIDIAs H100 GPU offer enhanced support for 8-bit integer quantization and tensor core optimization. Intel CPUs and Qualcomm GPUs: Intel Xeon processors feature support for 8-bit integer quantization via AMX, while Qualcomm Adreno GPUs provide optimization for mixed-precision tasks. VIII. QUANTIZED WHISPER CPU VS. GPU PERFORMANCE Using several quantization methods (Q4, Q5, and Q8), the base Whisper model was quantized to compare different implementation methods, experiences, accuracy, and observed differences. The following table describes the difference in total run time, divided into several components based on quantization and runtime type (CPU versus GPU). TABLE II WHISPER BASELINE VS. QUANTIZED COMPUTE TIMES BASED ON HARDWARE Time Load Mel Sample Encode Decode Batchd Prompt Total whispercpp (GPU) whispercpp (CPU) Standard 123.58 ms 43.29 ms 2.03 ms/run 4604.79 ms/run 226.40 ms/run 9.94 ms/run 0.00 ms/run 6786.58 ms Quantized 66.54 ms 51.26 ms 1.47 ms/run 5934.99 ms/run 9.75 ms/run 7.76 ms/run 0.00 ms/run 7414.24 ms Standard 162.27 ms 80.58 ms 1.84 ms/run 6468.15 ms/run 12.56 ms/run 7.94 ms/run 0.00 ms/run 8033.38 ms Quantized 94.51 ms 80.88 ms 1.87 ms/run 8612.49 ms/run 11.16 ms/run 9.10 ms/run 0.00 ms/run 10380.28 ms Next, the experiment evaluates how accuracy and latency are affected by the quantization of the Whisper models. Reference the Appendix for hardware speciﬁcations. For my audio data, used the open source Librispeech ASR [16] dataset, using the ﬁrst 10 audio ﬁles provided. These audio transcriptions then determine how quantization methods (Q4, Q5, Q8) affect model speed and accuracy. This study evaluates the WER and accuracy using [7], model which uses components of huggingface-evaluate and openai-whisper projects for WER calculation. The following table records key observations: TABLE III WER, MODEL SIZE, AND LATENCY BASED ON QUANTIZATION METHOD Metric Word Error Rate Accuracy Model Size Avg Latency Whisper CPP Base Model 0.0199 98.0% 141.11MB 10.64s INT5 0.0199 98.0% 52.75MB 11.11s INT4 0.0159 98.4% 44.33MB 10.55s INT8 0.0199 98.0% 77.99MB 9.02s X. APPLICATIONS The results of this study could have profound implications for resource-constrained environments, such as mobile devices, IoT, and embedded systems. By reducing the memory footprint and maintaining high accuracy, quantized Whisper models could provide real-time transcription services, enable lowlatency captioning, and improve accessibility for users with hearing impairments. The deployment of the Whisper models onto portable devices sets high benchmark for transcription, translation, and speech detection services available for commercial use. XI. CONCLUSION The study concludes that quantization is viable method for reducing model size and improving deployment efﬁciency without sacriﬁcing accuracy or latency. This experiment reduces model size by up to 45% while maintaining the same WER and decreasing latency by 19%. These results support the feasibility of Whisper on smaller devices, suggesting that Whisper can be effectively deployed on resource-limited devices such as smartphones and IoT systems, making real-time ASR more accessible and efﬁcient. Extending this research to other ASR models could enhance the scalability of audiobased AI applications. Future work could explore additional quantization techniques, optimize hardware deployment strategies, and investigate trade-offs in real-time performance. As ASR models scale, optimizing the trade-off between accuracy, efﬁciency, and real-time performance will be critical for nextgeneration AI deployment."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Baranski, J. Jasinski, J. Bartolewska, S. Kacprzak, M. Witkowski, and K. Kowalczyk. Investigation of whisper asr hallucinations induced by non-speech audio, 2025. [2] G. Gerganov. ggerganov/whisper.cpp: Port of openais whisper model in c/c++. [3] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer. survey of quantization methods for efﬁcient neural network inference, 2021. A. APPENDIX https://github.com/allisonandreyev/WhisperQuantization B. Cpuinfo Version: 9.0.0 Brand Raw: Intel(R) Xeon(R) CPU @ 2.20GHz Hz Advertised Friendly: 2.2000 GHz Hz Actual Friendly: 2.2000 GHz Hz Advertised: (2200000000, 0) Hz Actual: (2199998000, 0) Arch: X86 64 Bits: 64 Count: 2 Arch String Raw: x86 64 L1 Data Cache Size: 32768 L1 Instruction Cache Size: 32768 L2 Cache Size: 262144 L2 Cache Line Size: 256 L2 Cache Associativity: 6 L3 Cache Size: 57671680 [4] T. Giorgino. Computing and visualizing dynamic time warping alignments in r: The dtw package. Journal of Statistical Software, 31(7), 2009. doi: 10.18637/jss.v031.i07 [5] T. Giorgino. Computing and visualizing dynamic time warping alignments inr: ThedtwPackage. J. Stat. Softw., 31(7), 2009. [6] C. Graham and N. Roll. Evaluating openais whisper asr: Performance analysis across diverse accents and speaker traits. JASA Express Letters, 4(2):025206, 02 2024. doi: 10.1121/10. [7] ins8ai. Ins8ai/wer: Word error rate computation using components from huggingface-evaluate and openai-whisper projects, Oct 2023. [8] J. Johansson. Automatic Word Decoding Assessment Using Whisper and Machine Learning Techniques: An Automatic Speech Recognition Method to Assess the Early Reading Abilities of Young Children Reading Swedish. Dissertation, KTH Royal Institute of Technology, 2024. [9] S. Kim, A. Gholami, Z. Yao, N. Lee, P. Wang, A. Nrusimha, B. Zhai, Integer-only zero-shot T. Gao, M. W. Mahoney, and K. Keutzer. In ICASSP 2022 - 2022 quantization for efﬁcient speech recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 42884292, 2022. doi: 10.1109/ICASSP43922. 2022.9747552 [10] A. Koenecke, A. S. G. Choi, K. X. Mei, H. Schellmann, and M. Sloane. In ProceedCareless whisper: Speech-to-text hallucination harms. ings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, p. 16721681. Association for Computing Machinery, New York, NY, USA, 2024. doi: 10.1145/3630106.3658996 [11] Y. Liu and D. Qu. Parameter-efﬁcient ﬁne-tuning of whisper for lowIn 2024 5th International Seminar on resource speech recognition. Artiﬁcial Intelligence, Networking and Information Technology (AINIT), pp. 15221525, 2024. doi: 10.1109/AINIT61980.2024.10581773 [12] Y. Liu, X. Yang, and D. Qu. Exploration of whisper ﬁne-tuning strategies for low-resource asr. Journal of Audio, Speech, and Music Processing, 2024:29, 2024. doi: 10.1186/s13636-024-00349-3 [13] J. Louradour. whisper-timestamped. https://github.com/linto-ai/whisper-timestamped, 2023. [14] H. Ma, Z. Peng, M. Shao, J. Li, and J. Liu. Extending whisper with In ICASSP 2024 - 2024 IEEE prompt tuning to target-speaker asr. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1251612520, 2024. doi: 10.1109/ICASSP48485.2024. 10447492 [15] D. Machaˇcek, R. Dabre, and O. Bojar. Turning whisper into real-time In S. Saha and H. Sujaini, eds., Proceedings of transcription system. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Paciﬁc Chapter of the Association for Computational Linguistics: System Demonstrations, pp. 1724. Association for Computational Linguistics, Bali, Indonesia, Nov. 2023. doi: 10.18653/v1/2023.ijcnlp-demo.3 [16] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 52065210. IEEE, 2015. [17] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022. [18] N. Rao, S. ORiordan, and J. Coulis. tioning library audiovisual content with whisper. 0(0):03400352241310534, 2025. doi: 10.1177/03400352241310534 [19] S. Rijal, S. Adhikari, M. Dahal, M. Awale, and V. Ojha. Whisper Ai and labor: CapIFLA Journal, ﬁnetuning on nepali language, 2024. [20] Z. Song, Z. Ma, Y. Yang, J. Zhuo, and X. Chen. comparative study of llm-based asr and whisper in low resource and code switching scenario, 2024. [21] V. Timmel, C. Paonessa, R. Kakooee, M. Vogel, and D. Perruchoud. Fine-tuning whisper on low-resource languages for real-world applications, 2024. [22] Q. Zhao, G. Sun, C. Zhang, M. Xu, and T. F. Zheng. Speaker adaptation for quantised end-to-end asr models, 2024. [23] K. Zhen, M. Radfar, H. D. Nguyen, G. P. Strimel, N. Susanj, and A. Mouchtaris. Sub-8-bit quantization for on-device speech recognition: regularization-free approach, 2022."
        }
    ],
    "affiliations": []
}