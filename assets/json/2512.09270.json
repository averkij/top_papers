{
    "paper_title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "authors": [
        "Sangwoon Kwak",
        "Weeyoung Kwon",
        "Jun Young Jeong",
        "Geonho Kim",
        "Won-Sik Cheong",
        "Jihyong Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 7 2 9 0 . 2 1 5 2 : r MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification Sangwoon Kwak1*, Weeyoung Kwon2*, Won-Sik Cheong1, Jun Young Jeong1, Geonho Kim2, Jihyong Oh2 1Electronics and Telecommunications Research Institute, 2Chung-Ang University {s.kwak, jyj0120, wscheong}@etri.re.kr {weeyoungkwon, joelkimgh, jihyongoh}@cau.ac.kr https://cmlab-korea.github.io/MoRel/ Figure 1. Approaches for modeling long-range 4D Motion. (a) The all-at-once training experiences memory overflow and even suffers from limited representational capacity. (b) The chunk-based training mitigates the memory overflow but causes temporal flickering at chunk boundaries, substantially degrading visual quality. In contrast, (c) our Anchor Relay-based Bidirectional Blending (ARBB) approach successfully maintains both representation quality and temporal consistency by smoothly transiting the influence of each Key-frame Anchor (KfA). The rendered patches, frame-wise tOF [2], and temporal profile provide strong evidence for the effectiveness of our method."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where naıve extension of existing methods leads to severe memory explo- *Equal contribution. Corresponding author. sion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of longrange dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfAs while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our models capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCapLR. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free longrange 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations. The code and project page: https://cmlab-korea.github.io/MoRel/ 1. Introduction 3D Gaussian Splatting (3DGS) [10] has positioned itself as powerful paradigm for novel view synthesis (NVS), enabling real-time photorealistic rendering. Unlike Neural Radiance Fields (NeRF) [19], which rely on dense ray sampling and costly volume rendering, 3DGS uses explicit Gaussian primitives and GPU-parallel splatting pipeline to achieve high efficiency while preserving visual fidelity. This remarkable balance has naturally motivated its extension to dynamic and video-centric scenarios, giving rise to 4D Gaussian Splatting (4DGS) [23, 31, 34, 37]. However, existing 4DGS approaches encounter distinct challenges when modeling long-range videos, even those lasting only few minutes, which are typical of real-world content. As summarized in Fig. 2, these methods involve clear tradeoffs across different practical dimensions, highlighting the need for more scalable and versatile 4DGS solutions. The all-at-once training approach (Fig. 1(a), Fig. 2(a)) extends existing methods to long-range videos by either (i) augmenting canonical 3DGS with deformation field [5, 8, 14, 30, 33] or (ii) using 4D Gaussian primitives that jointly encode spatial-temporal characteristics [4, 31, 37]. While jointly optimizing all frames ensures global temporal consistency, it causes GPU memory explosion because modeling long-range dynamics demands an ever-growing number of high-dimensional Gaussians as shown in Fig. 1 top-right. Although all frames are accessible and allow the model to account for disoccluded regions, i.e., areas previously hidden but newly revealed over time, the reconstruction quality in such regions remains limited by restricted representational capacity [11, 23]. Furthermore, practical streaming scenarios require random temporal access to only the relevant portion of the video [1], yet all-at-once methods still mandate full model transmission, limiting scalability. The other candidate is the chunk-based approach (Fig. 1(b), Fig. 2(b)), which partitions long videos into short Figure 2. Conceptual comparison of existing 4DGS methods in modeling long-range 4D motion. (a) All-at-once approaches suffer from high memory usage, while (b) chunk-based methods inevitably fail to maintain temporal consistency. Even advanced variants struggle with system applicability such as random accessibility. Our ARBB framework resolves all these issues, achieving bounded memory and temporally coherent long-range modeling. temporal segments and trains an independent model for each chunk [13, 16, 29]. This divide-and-conquer scheme reduces memory overhead and naturally supports temporal random access. However, optimizing chunks in isolation disrupts temporal consistency, producing boundary artifacts and abrupt appearance shifts when segments are stitched together [13, 23, 32]. In addition, because each chunk model observes only limited temporal window, disoccluded regions that emerge in later chunks cannot be reconstructed, reducing overall scene completeness. Recent representations tailored for long-range videos still exhibit the trade-offs highlighted in Fig. 2. The slidingwindow strategy (Fig. 2(c)) [23] inherits the benefits of chunk-based methods by training an independent 4DGS model per window, but its overlapping-frame refinement is only local fix that cannot ensure global temporal consistency, and its reliance on external optical-flow increases system complexity. Another strategy, Temporal Gaussian Hierarchy (Fig. 2(d)) [34], builds multi-level structure to capture motions at different temporal scales, enabling nearly constant memory usage for long videos. However, maintaining this hierarchy requires continual Gaussian reallocation, per-timestamp segment selection, and CPUGPU streaming, leading substantially higher system complexity. To address the diverse challenges of modeling longrange dynamic content, we propose MoRel, novel 4DGS framework that overcomes the key limitations of existing methods. First, MoRel introduces the Anchor Relaybased Bidirectional Blending (ARBB) mechanism, which learns bidirectional deformations between Key-frame Anchors (KfA) and blends them through learnable temporal opacity control, effectively suppressing temporal discontinuities. Second, the Feature-variance-guided Hierarchical Densification (FHD) refines anchor representations based on local frequency characteristics, preventing redundant anchorpoint generation while preserving high-frequency detail. Third, MoRel maintains bounded GPU memory by dividing long sequences into anchor-based chunking with ondemand-loading of only the necessary KfA and deformation field. Fourth, the periodic placement of KfA provides natural temporal access points, enabling efficient random temporal access without loading the entire model. Finally, MoRel requires no external cues during training and uses simple rendering pipeline, avoiding unnecessary system complexity. Together, these components allow MoRel to achieve memory-efficient and flicker-free, and temporally consistent long-range 4D reconstruction suitable for realworld dynamic scenes. In line with these design benefits, quantitative results further show that MoRel outperforms recent 4DGS methods, obtaining the lowest tOF [3] strong reconstruction quality, and competitive GPU memory usage. 2. Related Works 2.1. All-at-once Approach for 4D NVS The first category, the all-at-once approach, jointly optimizes single canonical representation induced by Gaussian points across all sequence frames, modeling long-range dynamic scenes as unified space. In 4D Gaussian-based methods [4, 31, 36, 37] that explicitly introduce time index as an extra dimension, the temporal complexity inevitably scales with both the number of Gaussians and the sequence length. As the temporal span increases, these methods often suffer from memory overflow or excessively long training times. Additionally, deformation-based methods [5, 8, 14, 30, 32, 33] maintain canonical representation and learn deformation fields relative to it, achieving significant memory efficiency while retaining reconstruction quality. However, such capability is largely by-product of compactness rather than an intentional design for longrange modeling, and thus still encounters difficulties in handling complex long-range motion video. Moreover, they struggle to address specific challenges, such as newly appearing objects and rapid motions within short-temporal intervals [9, 12, 24]. 2.2. Chunk-based Approach for 4D NVS Recent Gaussian-based frameworks [13, 16, 29] adopt chunk-based or streamable learning strategies with improved efficiency. For example, GIFStream [13] first learns the canonical space and subsequently incorporates the deformation field before introducing compression modules for efficient 4D Gaussian transmission. While these works confirm the scalability potential of Gaussian frameworks, temporal flickering and appearance discontinuities often emerge at chunk boundaries because each segment is optimized independently without explicit inter-chunk consistency modeling. 3. Proposed Method 3.1. Overview of MoRel We adopt the anchor-pointbased representation [17], where sparse voxel grid of anchor-points defines canonical space parameterized by neural Gaussians. The preliminaries and all notations used throughout the paper are provided in Suppl. A. Building on this representation, our goal is to model long-range 4D motion with bounded memory usage while maintaining temporal coherence and motion fidelity, toward real-world system applicability, To this end, we employ the Anchor Relay-based Bidirectional Blending (ARBB) strategy (Sec. 1, Fig. 1), which consists of two sequential phases illustrated in Fig. 3 and is further organized into four training stages. This design enables scalable and efficient long-range 4D motion modeling under bounded memory conditions. In the Anchor Relay phase (Sec. 3.2), training begins with (i) the Global Canonical Anchor (GCA) training stage (Fig. 3(a)), where single GCA is trained from entire frames to provide globally consistent initialization. After training the GCA, variance-based levels are respectively assigned to anchor-points for later use in the Feature-variance-guided Hierarchical Densification (FHD) (Sec. 3.4). Subsequently, (ii) the Key-frame Anchor (KfA) training stage (Fig. 3(b)) optimizes each temporally periodic KfA at finer level respectively induced from the levelassigned GCA, forming local canonical spaces optimized for their respective chunks. Each KfA is further refined by FHD with the pre-assigned level to enhance spatial detail while suppressing redundant Gaussians, efficiently balancing memory overhead and reconstruction quality. In the Bidirectional Blending phase (Sec. 3.3), bidirectional deformation and temporal blending are learned through two consecutive stages. (i) In the Progressive Windowed Deformation (PWD) training stage (Fig. 3(c), each key anchor independently learns forward and backward deformation fields within sliding temporal windows, bounding memory usage and preventing inter-chunk interference. Next, (ii) the Intermediate Frame Blending (IFB) stage (Fig. 3-(d)) learns temporal fusion model through learned temporal opacity control, which adaptively transits KfA influence over time to achieve smooth and flicker-free motion continuity. By integrating these two phases, MoRel achieves high-fidelity, temporally coherent, and memory-efficient long-range 4D motion reconstruction. 3.2. Anchor Relay Phase (i) Global Canonical Anchor Training. In this stage, we skim through the entire video sequence to train AGlobal as shown in Fig. 3(a). It serves to ensure global consistency Figure 3. Overview of MoRel framework. To efficiently model long-range 4D motion with bounded memory and temporal consistency, MoRel adopts the Anchor Relay-based Bidirectional Blending (ARBB) strategy composed of four training stages which are organized into two phase. In the Anchor Relay phase (Sec. 3.2), GCA is first trained on entire frames with single point cloud. Next, each KfA is derived around its key-frame time index, while its spatial detail is enhanced through FHD (Sec. 3.4). In the Bidirectional Blending phase (Sec. 3.3), PWD training stage is executed to learn bidirectional deformation fields within local temporal windows to ensure robust motion modeling of each anchor. Finally, in IFB training stage, each pair of neighboring anchors are fused through learnable temporal opacity control, that smoothly transitions anchor influence over time, eliminating temporal flickering across chunks. across the entire sequences. In addition, we observed that previous works [13, 26, 30] require temporally-dense point clouds for modeling 4D motion, such as using all frames or sampling one every ten frames. However, for longrange 4D motion, such frequent point cloud requirements can significantly increase computational and memory overhead. To address this, we construct single point cloud for the initialization points for AGlobal detailed in Suppl. C.2. Such an efficient initialization not only ensures global coherence but also makes the framework be practically applicable to long-range 4D scenes. After training AGlobal, feature variance-based levels LaGlobal are assigned to its each anchor-point aGlobal by Eq. 2, resulting (cid:101)AGlobal. (ii) Key-frame Anchor Training. To effectively handle long-range 4D motion, we newly introduce periodically placed KfA, which are finely optimized around their corresponding time indices tn, enabling high-quality reconstruction of the assigned key moment. All AKey are initialized from the level-assigned global anchor (cid:101)AGlobal, rather than being trained from scratch, which ensures globally consistent appearance across the sequence. Each AKey also serves as local canonical space within its associated temporal range, [ max(0, tn GOP), min(tn +GOP, 1) ], where GOP (Group-of-Pictures) denotes the temporal spacing between adjacent KfAs. To enhance robustness, we n introduce temporal tolerance ϵ, allowing each AKey to capture variations within its local temporal ϵ-neighborhood, [ max(0, tn ϵ), min(tn + ϵ, 1) ]. In addition, to further refine spatial detail while suppressing redundant Gaussians, we employ FHD (Sec. 3.4), which adaptively grows and prunes anchor-points based on the frequency characteristics of each an . The periodic placement of KfAs, inspired by [1, 25], improves practical applicability in real-world systems by providing random access points and maintaining bounded memory usage, as detailed in Alg. 1, 2. using the pre-assigned LaGlobal AKey 3.3. Bidirectional Blending Phase (i) Progressive Windowed Deformation Training. As introduced in Fig. 1, we adopt bidirectional deformation scheme to better handle irregular motion and to achieve smooth transitions across adjacent KfAs. However, directly optimizing long-range 4D motion under this bidirectional design remains challenging as shown in Fig. 4. An all-atonce training (Fig. 4(a)) suffers from severe memory explosion. Alternatively, chunk-wise training (Fig. 4(b)) can alleviate this memory issue. Here, chunk refers to temporal frame range that can be rendered without reloading new KfA. However, this scheme can cause interchunk interference problem. Specifically, in the case of anism to smoothly blend the resulting KfAs for IF iterations within the corresponding chunkn. In here, as shown in Fig. 3(d), Fig. 4(c), only the blending weights are trained while the anchor attributes, deformation fields, and FHD remain frozen. For the blending weight, we build upon the opacity control mechanism adopted in previous works [4, 15, 30], where opacity exponentially decays with the temporal distance form central time which is tn for AKey in our case. However, in dynamic scenes with irregular motion such as occlusion, the spatio-temporal influence of each KfA can vary non-uniformly. To effectively model this characteristics, we newly introduce learnable temporal opacity control, assigning each anchor-point an its own n,k, and temporal decay speed ddir temporal offset odir n,k, where dir {Fw, Bw} (forward and backward). As shown in Fig. 3(d), the temporal opacity of n-th KfA is defined as wdir n,k = exp[λdecay ddir n,k τn odir n,k], (1) where λdecay denotes base decay coefficient controls the global decay speed. This learnable opacity control-based bidirectional blending enables not only smooth transitions between adjacent KfAs but also robust representation of irregular motions in the dynamic scene. The effectiveness of this design is validated through ablation studies (Tab. 3). Using the learned weight, the blending process is performed after reconstructing neural Gaussians [17] of each deformed anchor an tnt. The obtained opacity valk tnt and an+1 ues at an tn+1t are blended according to the learned weights rendered [10] to produce the final output, detailed in Suppl. E.3. 3.4. Feature-variance-guided Hierarchical Densification In FHD, we use anchor-points feature ˆfk variance as proxy of local frequency complexity and modulates levelwise densification. As shown in Fig. 5, the module comprises Variance-based Leveling (VL) and Level-wise Densification (LD). The objective is to suppress redundant anchor growth in unstable high-frequency regions during early training iterations and to enhance fine detail in later iterations of the KfA and PWD Training stages. Variance-based Leveling. After completing GCA training, in AGlobal is assigned hieach global anchor-point aGlobal erarchical level that reflects local frequency complexity, resulting in the level-assigned global anchor set (cid:101)AGlobal. The feature representation ˆfk of the each global anchor-point exhibits varying sensitivity depending on local frequency characteristics. High-frequency components are better fitted during later training stages and are particularly sensitive in the early phase [21]. Repeated accumulation of large gradient fluctuations increases the variance of ˆfk [20]. Therek = Var( ˆfk) serves fore, the variance of anchor features σ2 as reliable indicator of local frequency complexity. For Figure 4. Comparison of training strategies for modeling longrange 4D motion with bidirectional deformation. (a) All-atonce training suffers from memory overflow. (b) Chunk-wise training reduces memory cost but causes inter-chunk interference. (c) Our Bidirectional Blending (PWD + IFB) maintains bounded memory and prevents inter-chunk interference. n (Fig. 4(b), AKey is trained for the chunkn1, but later updated again for the chunkn including densification. Accordingly, the characteristics previously optimized for chunkn1 become corrupted. For example, new anchor-points may grow that were not trained for backward deformation toward chunkn1, or existing anchor-points crucial for representing chunkn1 may be pruned during the training for chunkn. We refer this phenomenon to backward contamination. To address this, we propose the PWD training strategy to as illustrated in Fig. 4(c). In our PWD training, each KfA is independently optimized within local Bidirectional Deformation Window (BDW), defined as temporal window modeled by single KfA via bidirectional deformation. Each AKey is dynamically loaded only when its BDWn is being optimized and unloaded afterward described in Alg. 1, resulting in efficient bounded memory usage, i.e., on-demanding loading. Each BDWn is trained for PWD iterations and the window then progressively slides to the next BDW with overlapping chunk. This progressive training not only bounds memory usage but also prevents inter-chunk interference, enabling stable and consistent long-range motion learning. Within the BDW, the deformation field Dn(, τn) of Akey performs both forward (+) and backward () deformation using the normalized relative time τn [1, 1] corresponding to [tn GOP, tn + GOP]. Each an queries its position to Dn(, τn) to obtain the deformation amount of its attribute, further detailed in Suppl. B.2. (ii) Intermediate Frame Blending Training. After the deformation fields are established through PWD, we proceed to the IFB training stage, where two adjacent KfAs, Akey n+1, are jointly loaded to train blending mechn and Akey wjS = (cid:40) 1, = 0, λL + (1 λL)ηt, 1, (3) S where ηt = jS denotes the normalized training progress ratio, and represents the total number of training iterations for each AKey in the corresponding stage. Here, λL controls the initial importance assigned to level L, enabling lower levels to retain stronger weights at early iterations while allowing higher levels to gain influence as ηt increases. This formulation ensures smooth temporal transition from low-frequencydominated densification at the beginning to high-frequencyoriented refinement toward the end. Neural gaussians that satisfy the growth criterion based on gjS are mapped onto the spatial grid and used as candidate positions for new anchors. 3.5. Summary of Training and Rendering Process The overall training procedure of our MoRel is summarized in Alg. 1. As described in Sec. 3.1, Fig. 3, four training stages are sequentially performed, and each training stage is iteratively optimized for predefined stage-specific number of iterations S, {GCA,KfA,PWD,IFB}. Note that, through the dynamic loading and unloading operations indicated in the algorithm, only one or two key anchors and their corresponding deformation fields are loaded at any given time, ensuring bounded memory usage during training. Similarly, the rendering process summarized in Alg. 2 utilizes the same bounded-memory principle. For given rendering time t, the required KfAs are first determined based on the GOP, and loaded only when necessary for rendering. This on-demand loading minimizes redundant memory usage, minimizing unnecessary load/unload operations. Through this design, MoRel achieves memoryefficient and temporally-consistent spatio-temporal novel view synthesis for long-range 4D motion. 4. Experiments 4.1. Experimental setup Implementation Detail. MoRel is built upon 4DGS [31] and Scaffold-GS [17], retaining most hyperparameters. Implementation details are provided in Suppl. B. SelfCapLR. To simulate real-world long-range 4D motion, we construct SelfCapLR dataset selected from an undistilled raw video dataset [30, 34]. It contains five representative and challenging dynamic sequences (Bike1, Bike2, Corgi, Yoga, and Dance) over 3500 frames. It has larger average motion magnitude and captured at wider spaces compared to other previous datasets as compared in Suppl. C.1. As result, it is an appropriate benchmark to evaluate challenging long-range 4D motion modeling. Evaluation Metrics. We employ PSNR, SSIM, and LPIPS [38]. To further evaluate temporal consistency, we Figure 5. Overview of Feature-variance-guided Hierarchical Densification. (a) Variance-based Leveling: After GCA training, we assign level to each anchor-point guided by the featurevariance. (b) Level-wise Densification: During the KfA and PWD trainings, gradients for KfA densification are modulated by levelspecific weights, enabling early low-frequency stabilization and late high-frequency refinement. each anchor-point aGlobal signed based on quantile thresholds {τ1, τ2} as follows: , hierarchical levels LaGlobal k are as-"
        },
        {
            "title": "LaGlobal",
            "content": "k = 0, σ2 < τ1 τ1 σ2 τ 1, 2, σ2 < τ2, (low-frequency), (high-frequency). (2) The LaGlobal is then used as control signal for LD, enabling balanced densification in terms of memory and highfrequency detail. Ablation and analysis on the number of levels along with visualization between anchor-point feature and image frequency are provided in Suppl. E.1, E.2. Level-wise Densification. LD makes growth decisions based on gradient statistics at each level. Following prior gradient-based densification schemes in anchor-based representations [17], we track the accumulated gradient for each neural gaussians instead of using single-step gradient. At training iteration jS , where {KfA, PWD} denotes the current training stage, gjS represents the magnitude of the gradient accumulated up to iteration jS . We apply level-specific weight wjS , forming the level-weighted statistic gjS = gjS , which serves as the criterion for level-wise densification. Here, wjS adjusts the relative importance of level at training iteration jS , placing greater emphasis on lower levels during early training to stabilize low-frequency structures, and gradually increasing the weight of higher levels in later stages to enhance highfrequency details. Specifically, wjS follows linear interpolation between the initial and final importance factors as wjS n 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 3: 4: 5: 6: 7: 8: Algorithm 1 Training process of MoRel  (Fig. 3)  1: procedure TRAIN MOREL 2: Input point cloud P, cameras = { cm m=0,...,M 1 t=0,...,T 1 }, GOP, where {GCA, KfA, PWD, IFB}, tolerance tol GCA Training Stage (Sec. 3.2(i), Fig. 3(a)) Initialize AGlobal by for jGCA in GCA do Pick random cm Train AGlobal with cm LaGlobal is pre-assigned to AGlobal for FHD (Sec. 3.4) KfA Training Stage (Sec. 3.2(ii), Fig. 3(b)) Initialize {AKey for [0, 1] do }n[0,N 1] from AGlobal, =T /GOP Load AKey KfA for jKfA Pick random {cm Train AKey do with cm t[ tntol, tn+tol ] , densified by FHD t[ 0, 1 ] }"
        },
        {
            "title": "Unload AKey\nn",
            "content": "PWD Training Stage (Sec. 3.3(i), Fig. 3(c)) for [0, 1] do"
        },
        {
            "title": "Load AKey\nfor jPWD",
            "content": "n , Dn(, τn) PWD do Pick random {cm Train AKey , Dn(, τn) with cm BDWn t[ tnGOP, tn+GOP ] t[ 0, 1 ] , densified by FHD }"
        },
        {
            "title": "Unload AKey",
            "content": "n , Dn(, τn) IFB Training Stage (Sec. 3.3(ii), Fig. 3(d)) for [0, 2] do , AKey do"
        },
        {
            "title": "Load AKey\nfor jIFB",
            "content": "n+1, Dn(, τn), Dn+1(, τn+1) Chunkn t[ tn, tn+GOP ] t[ 0, 1 ] } IFB Pick random {cm Train oFw , wFw with frozen AKey , AKey"
        },
        {
            "title": "Save AKey\nUnload AKey",
            "content": "t n+1, wBw n+1, , oBw , AKey n+1, Dn(, τn), Dn+1(, τn+1) , wdir n+1, Dn(, τn), Dn+1(, τn+1), odir , AKey n+1, Dn(, τn), Dn+1 Algorithm 2 Rendering process of MoRel 1: procedure RENDER MOREL(cren 2: , GOP) = t/GOP , AKey if AKey Load AKey n+1, Dn(, τn), Dn+1 are not loaded then , AKey n+1, Dn(, τn), Dn+1 Render cren by Bi-directional Blending , AKey n+1, Dn(, τn), Dn+1, odir with AKey if = (n + 1) GOP then , wdir n"
        },
        {
            "title": "Unload AKey",
            "content": "n , AKey n+1, Dn(, τn), Dn+1, odir , wdir adopt tOF [3] between consecutive frames. We additionally measure training and rendering memories which are crucial evaluation factors for long-range 4D motion modeling in terms of efficiency. 4.2. Comparisons Comparison methods. To validate our model, we conducted comparisons against state-of-the-art (SOTA) methods, which can be categorized into the (i) all-at-once training and (i) chunk-based training introduced in Sec. 1, 2. For the all-at-once training category, we included 4DGS [31], MoDec-GS [11], and LocalDyGS [32], while GIFStream [13] was used as representative chunk-based method. In addition, for more comprehensive evaluation, we adapted 4DGS to operate in chunk-wise manner and included it in our comparison experiments, named 4DGSchunk. Also, Demo videos and additional benchmark results are provided in Suppl. D. (i) All-at-once training. As shown in Tab. 1, all-at-once approaches exhibit generally lower quantitative results than ours, indicating degraded structural accuracy and visual fidelity. Our method shows particularly strong advantages on scenes with spatio-temporally large motion such as Corgi, Yoga and Dance, consistently outperforming the existing methods. These trends are further verified in the qualitative comparison shown in Fig. 6. For long-range 4D motion with 2k or more frame range, all-at-once approaches tend to lose motion expressiveness and degrade fine details due to their brute-force global optimization. In addition, these methods also suffer from the memory explosion problem, as shown in Tab. 2, which interferes the practical applicability to real-world systems. In contrast, our Morel delivers fine and robust reconstruction under even long-range motion with bounded memory usage thanks to the ARBB mechanism (Sec. 3.1, Fig. 3). (ii) Chunk-based training. Chunk-based approaches [13] similarly exhibit overall lower and relatively unstable quantitative performance compared to our method as shown in Tab. 2. In particular, our adapted 4DGS shows reasonable performance on relatively static scenes such as Bike1 and Bike2, with tendency to maintain SSIM; however, as shown in Tab. 2, it suffers from significantly poor tOF [2] scores. This indicates degraded temporal consistency caused by flickering at chunk boundaries. This is further supported by the temporal profile analysis provided in the Suppl. E.4. In contrast, our method benefits from bidirectional blending 3.3, achieving not only superior quantitative results 2 but also the best tOF score 2 among all compared approaches. 4.3. Ablation studies We conducted comprehensive ablation studies to evaluate the effectiveness of each component in MoRel, as summarized in Tab. 3. All results were computed on 300frame subset sampled from the SelfCapLR sequences. Since MoRel independently trains each BDW (Fig. 3, Sec. 3.3) in progressive manner, this subset evaluation reliably reflects the general trend of each components influence. The baseline, variant (a), is single-stage deformation model trained with single AGlobal. This setup maintains global consistency but fails to capture detailed local motion, leadGroup Method Bike1 Bike2 Corgi Yoga Dance Average 4DGS [CVPR24] [31] 21.62 / 0.662 / 0.375 22.61 / 0.700 / 0.346 19.72 / 0.566 / 0.489 13.72 / 0.715 / 0.402 17.09 / 0.597 / 0.399 18.95 / 0.648 / 0.402 MoDec-GS [CVPR25] [11] 21.55 / 0.659 / 0.355 22.13 / 0.666 / 0.348 17.38 / 0.539 / 0.492 17.51 / 0.732 / 0.396 17.48 / 0.621 / 0.363 19.61 / 0.643 / 0.391 LocalDyGS [ICCV25] [32] 22.25 / 0.664 / 0.341 22.66 / 0.675 / 0.340 19.37 / 0.551 / 0.462 21.38 / 0.755 / 0.333 17.56 / 0.615 / 0.377 20.64 / 0.652 / 0.371 GIFStream [CVPR25] [13] 18.43 / 0.658 / 0.345 20.11 / 0.655 / 0.345 19.83 / 0.566 / 0.467 22.02 / 0.771 / 0.335 14.73 / 0.614 / 0.533 19.02 / 0.653 / 0.405 22.26 / 0.695 / 0.344 22.52 / 0.699 / 0.340 19.90 / 0.570 / 0.492 14.62 / 0.712 / 0.389 17.23 / 0.603 / 0.379 19.31 / 0.656 / 0.389 22.32 / 0.668 / 0.321 22.57 / 0.670 / 0.319 19.93 / 0.575 / 0.461 22.18 / 0.780 / 0.297 17.99 / 0.628 / 0.377 21.00 / 0.664 / 0.355 4DGSchunk MoRel (Ours) (a) (b) (c) Table 1. Quantitative results comparison on our newly composed SelfCapLR. Group denotes (a) all-at-once training methods, (b) chunk-based approaches including our unidirectional deformation variant, and (c) our MoRel model. Red and blue denote the best and second-best performances, respectively. Each block element of 3-performance denotes (PSNR (dB) / SSIM / LPIPS). Group Method tOF Memory (MB) Training Rendering Variant PSNR SSIM LPIPS Memory (MB) Training Rendering 4DGS [CVPR24] [31] 0.222 18,000 (a) MoDec-GS [CVPR25] [11] 0.249 22,000 LocalDyGS [ICCV25] [32] 0.215 12,000 GIFStream [CVPR25] [13] 0.539 9, 4DGSchunk MoRel (Ours) 0.680 4,500 0.203 6,000 (b) (c) 154 122 93 65 126 Table 2. Metrics critical to long-range motion modeling. We highlight the key factors that determine models capability in long-range motion handling Figure 6. Qualitative comparison on SelfCapLR. Our MoRel demonstrates superior visual fidelity in long-range motion modeling compared to existing SOTA methods, thanks to its ARBB mechanism that effectively handles long-range 4D motion. ing to the lowest quantitative results. Variant (b) introduces KfAs that form local canonical spaces for each temporal segment, enhancing regional consistency and motion fidelity. This improvement of LPIPS support this claim. Moreover, this localized partitioning significantly reduces both training memory and rendering memory, thanks to the (a) 2-stage, GCA-only (+) uni-deformation 19.71 0.654 0.386 12, (b) 3-stage, (a) with KfA 19.90 0.647 (c) 3-stage, (b) with PWD and Linear Blend 20.66 0. (d) 4-stage, (b) with PWD and IFB (e) 4-stage, (d) with FHD 21.07 0.672 21.20 0. 0.364 0.358 0.342 0.348 4,500 6, 6,500 6,000 156 94 138 126 Table 3. Ablation studies on MoRel components. Each row evaluates the impact of specific design choice. Yellow-green cells highlight configurations with substantial gain. on-demand loading in Alg. 1-line 12 and 16. Next, variant (c) incorporates PWD and Linear Blending which linearly interpolates between adjacent KfAs based on temporal distance. Although the bidirectional deformation slightly increases the training memory but marginal, and we observe that even simple linear blending leads to noticeable quality improvements, by preventing inter-chunk interference. Replacing simple interpolation with IFB in variant (d) further improves the representation fidelity. This gain came from the learnable opacity control enables the model to effectively represent the scene even various forms of irregular motion. Finally, variant (e) applies FHD to balance between quality and memory usage. By efficiently control the anchor densification guided by feature-variance, we achieve nearly identical or slight improved performance while simultaneously reucing the rendering memory usage. Consequently, MoRel demonstrates that the integration of the proposed components enables effective long-range 4D motion representation while maintaining bounded memory usage. Further diverse analyses are provided in Suppl. E. 5. Conclusion We present MoRel, on-demand loading-based longrange 4D Gaussian Splatting framework that delivers flicker-free, memory-bounded, and faithful reconstructions of dynamic scenes. At its core, Anchor Relaybased Bidirectional Blending (ARBB) coordinates Key-frame Anchors (KfAs) and fuses their bidirectional deformations via learnable temporal opacity to maintain coherent motion. Feature-varianceguided Hierarchical Densification (FHD) further enriches anchor regions to recover high-frequency detail while avoiding overpopulation of anchor-points. Extensive experiments on SelfCapLR reveal that MoRel consistently attains sharper reconstructions, smoother temporal transitions and lower memory consumption, positioning it as compelling and scalable solution for real-world longrange 4D motion modeling."
        },
        {
            "title": "References",
            "content": "[1] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary Sullivan, and Jens-Rainer Ohm. Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology, 2021. 2, 4 [2] Mengyu Chu, You Xie, Laura Leal-Taixe, and Nils Thuerey. Temporally coherent gans for video super-resolution (tecogan). arXiv preprint arXiv:1811.09393, 1(2):3, 2018. 1, 7 [3] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via selfsupervision for gan-based video generation. ACM Transactions on Graphics, 2020. 3, 7 [4] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. In Proceedings of the ACM SIGGRAPH Conference, pages 111, 2024. 2, 3, 5 [5] Bardienus Duisterhof, Zhao Mandi, Yunchao Yao, JiaWei Liu, Jenny Seidenschwarz, Mike Zheng Shou, Ramanan Deva, Shuran Song, Stan Birchfield, Bowen Wen, and Jeffrey Ichnowski. Deformgs: Scene flow in highly deformable scenes for deformable object manipulation. World Symposium on the Algorithmic Foundations of Robotics, 2024. 2, 3 [6] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. pages 3376833780, 2022. 3, 5, 6 [7] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42204230, 2024. [8] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42204230, 2024. 2, 3 [9] Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, and Mathieu Salzmann. Temporally compressed 3d gaussian splatting for dynamic scenes. arXiv preprint arXiv:2412.05700, 2024. 3 [10] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. 2, 5 Global-to-local motion decomposition and temporal interval adjustment for compact dynamic 3d gaussian splatting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1133811348, 2025. 2, 7, 8, 5, 6 [12] Junoh Lee, ChangYeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic guassian splatting. In Proceedings of the Neural Information Processing Systems, pages 53845409, 2024. 3 [13] Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, and Yiyi Liao. Gifstream: 4d gaussian-based immersive video with feature stream. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2176121770, 2025. 2, 3, 4, 7, 8, [14] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 85088520, 2024. 2, 3 [15] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 5 [16] Zhenhuan Liu, Shuai Liu, Yidong Lu, Yirui Chen, Jie Yang, and Wei Liu. Efficient 4d gaussian stream with low rank adaptation. arXiv preprint arXiv:2502.16575, 2025. 2, 3 [17] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 3, 5, 6 [18] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 2024 International Conference on 3D Vision (3DV), pages 800809. IEEE, 2024. 3 [19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [20] Xin Qian and Diego Klabjan. The impact of the mini-batch size on the variance of gradients in stochastic gradient descent. arXiv preprint arXiv:2004.13146, 2020. 5 [21] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference on Machine Learning, pages 53015310, 2019. [22] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 3 [23] Richard Shaw, Michal Nazarczuk, Jifei Song, Arthur Moreau, Sibi Catley-Chandar, Helisa Dhamo, and Eduardo Perez-Pellitero. Swings: Sliding windows for dynamic 3d gaussian splatting. In Proceedings of the European Conference on Computer Vision, pages 3754, 2024. 2 [11] Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, WonSik Cheong, Jihyong Oh, and Munchurl Kim. Modec-gs: [24] Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, and Alois Knoll. Coda-4dgs: ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. 6 [36] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highIn Profidelity monocular dynamic scene reconstruction. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033120341, 2024. 3 [37] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In International Conference on Learning Representations, 2024. 2, [38] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [39] Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, and Hujun Bao. Lookcloser: Frequency-aware radiance field for tiny-detail scene. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1612216132, 2025. 8 Dynamic gaussian splatting with context and deformation In IEEE/CVF Internaawareness for autonomous driving. tional Conference on Computer Vision, 2025. 3 [25] Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on Circuits and Systems for Video Technology, 2012. 4 [26] Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic freeIn Proceedings of the IEEE/CVF Conviewpoint videos. ference on Computer Vision and Pattern Recognition, pages 2067520685, 2024. 4, 5 [27] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82488258, 2022. [28] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. 3 [29] Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, and Lan Xu. Vˆ 3: Viewing volumetric videos on mobiles via streamable 2d dynamic gaussians. ACM Transactions on Graphics, 2024. 2, 3 [30] Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, and Xiaowei Zhou. Freetimegs: Free gaussian primitives at anytime anywhere for dynamic scene reconstruction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2175021760, 2025. 2, 3, 4, 5, 6 [31] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. 2, 3, 6, 7, 8 [32] Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, and Ronggang Wang. Localdygs: Multi-view global dynamic scene modeling via adaptive local implicit feature deIn Proceedings of the IEEE/CVF conference on coupling. computer vision and pattern recognition, pages 95199529, 2025. 2, 3, 7, 8 [33] Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, Jinbo Yan, Kaiqiang Xiong, and Ronggang Wang. Swift4d: Adaptive divide-and-conquer gaussian splatting for compact In Internaand efficient reconstruction of dynamic scene. tional Conference on Machine Learning, 2025. 2, 3 [34] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. ACM Transactions on Graphics, 2024. 2, 6, [35] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highIn Profidelity monocular dynamic scene reconstruction. MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Notation In this paper, we use the following notations. An overview our processing pipeline, illustrating the relationships among the notations, is provided in Fig. 7. Anchor A: Anchor (canonical) space representing static scene. AKey: Key-frame anchor (KfA). AGlobal: Global Canonical Anchor (GCA). (cid:101)AGlobal: Level-assigned GCA by FHD. Ablended : Blended anchor by adjacent KfAs un IFB training stage. n: Index of KfA. : Total number of KfAs. e.g., AKey denotes the n-th KfA. Anchor-points a: Anchor-points that constitute an anchor space A, formed by discretizing into grid; each point has attributes defined in Sec.B. an: Anchor-points belonging to the n-th KfA. k: Index of anchor-points. Kn: Total number of anchor-points in the n-th KfA. e.g., AKey = 0, 1, , Kn 1 }. = { an Neural Gaussians: pera Gaussian set. i: Index of neural Gaussians assigned to an anchorpoint. I: Number of neural Gaussians per anchor-point. e.g., = 10 indicates each anchor-point has 10 neural Gaussians. Note that is not parameter dependent on or k; it is constant shared across all KfA and their anchor-points. Temporal indices t: Temporal index (frame). : Total number of frames in the video sequence. tn: Frame index corresponding to Akey . Deformation field Dn(, τ ): Deformation field of the n-th KfA that warps anchor-points according to the normalized relative time τ [1, 1]. single deformation field models both forward (+) and backward (-) temporally bidirectional directions in unified manner. τ : Normalized relative time between neighboring KfAs; τ = 1 and τ = 1 correspond to backward and forward endpoints, respectively. τn: Normalized relative time for Akey . e.g., Dn(an , τn) represents the temporally deformed amount for attributes of anchor-point an at τn, where τn [1, 1] is relative time corresponding to [tn1, tn+1]. Blending parameters odir ddir odir ddir wdir : set of temporal offsets assigned to Akey : set of temporal decay speeds assigned to Akey n,k: Temporal offset assigned to an , dir {Fw, Bw} n,k: Temporal decay speed assigned to an . n,k: Temporal blending weight assigned to an , derived by Eq. 1. Initial points and cameras P: Input point cloud for initializing AGlobal. C: set of input training camera frames. m: Index of camera. : Total number of cameras. cm e.g., = {cm : an individual frame of m-th camera at time t. = 0, , 1, = 0, , 1} Training index and stages jS : Training iteration index within one of the four jS training stages, {GCA, KfA, PWD, IFB}. : Training iteration index for n-th KfA, within the training stage S. e.g., jKfA training stage. denotes training index for AKfA in KfA : Total training iteration of the stage S. : Total training iteration of AKfA e.g., PWD = (cid:80)N 1 . Note that, in the GCA training stage, training is exefor the stage S. n=0 PWD cuted once to initialize AGlobal. Temporal units and windows GOP (Group of Pictures): Temporal spacing between adjacent KfAs. e.g., GOP = 100 means each KfM is located at every 100 frames. [1, 1], tn tn1 = GOP Chunk: Temporal range that can be rendered without reloading new KfA, equivalent to the temporal range covered by unidirectional deformation of KfA. Chunkn: n-th chunk that has temporal range of [ tn, tn + GOP ]. BDW (Bidirectional Deformation Window): Temporal range modeled by single KfA via bidirectional deformation. BDWn : n-th BDW that has temporal range of [ max(0, tn GOP), min(tn + GOP, 1) ]. ϵ: Temporal tolerance for robust training of KfA. Figure 7. Overview of processing pipeline including the relationship among notations. B. Preliminary B.1. Anchor-point-based Representation We adopt an anchor-point-based scene representation firstly introduced in Scaffold-GS [17] with sparse point cloud initialization [22]. We denote set of anchor-point positions by = {ak}K1 k=0 , where the k-th anchor-point is associated with learnable attribute θk = (pk, ˆfk, ℓk, Ok) as shown in Anchor-points stage of Fig. 7, where pk R3 is position of the anchor-point, ˆfk is an anchor-point feature that summarizes the local geometry and appearance around ak, ℓk R3 is scaling vector controlling the spatial extent of Gaussians associated with the anchor-point, and Ok RI3 is set of relative offsets that specify the canonical positions of the Gaussians attached to anchorpoint k. Let δk,cam denote the 3D displacement from the k,cam the viewing camera center to anchor-point ak, and direction. The attributes of the neural Gaussians associated with anchor-point are then defined as {attrk,i}I1 i=0 = Fattr( ˆfk, δk,cam, k,cam), (4) where each Gaussian attribute attrk,i contains color ck,i, quaternion orientation qk,i, scale sk,i and opacity αk,i as illustrated in Neural Gaussian Reconstruction stage in Fig. 7. Note that the position of each Gaussian is directly obtained by inner product of ℓk and Ok. To adapt anchor-points to the local complexity of scene, the anchor-points undergo densification, i.e., growing and pruning. The densification process is controlled by three parameters: (i) success threshold that measures how many views have observed the anchor-point, (ii) an opacity threshold that determines how visible an anchor-point is, and (iii) gradient threshold that identifies anchor-points requiring further adjustment (i.e., candidates for densification). Among these, the gradient criterion plays the most critical role. Within certain intervals, each anchor-point ak accumulates gradients gGk,i , and this value is compared against the preset gradient threshold to decide whether densification should occur. Hence, anchor-points with larger accumulated gradients are prioritized during densification. Our FHD (Sec. 3.4) further modulates this process by applying frequency-aware weighting to the accumulated gradients, enabling more controlled and adaptive densification. B.2. Hexplane Deformation For modeling temporal motion, we adopt hexplanebased deformation field that warps anchor-points according to normalized relative time variable, extending the deformation field introduced in [31] to operate bidirectionally. For each AKey , we define bidirectional deformation field Dn(an , τn), where τn [1, 1] denotes the bidirectional temporal coordinate that corresponds to [tn1, tn+1]. Given belonging to AKey an anchor-point an the temporally warped attributes of an shown in Bidirectional Deformation stage in Fig. 7. , τn) encodes at relative time τn as , Dn(an C. Dataset C.1. Dataset Statistics We construct the SelfCapLR dataset to enable rigorous evaluation of long-range motion, by reorganizing the original SelfCap [30, 34] sequences into long-duration captures with wide-baseline multi-view observations. To validate the statistical properties and difficulty of SelfCapLR, we compare it with established large-motion benchmarks, PanopticSports [18] and DyCheck-iPhone [6], under unified evaluation protocol, as summarized in Tab. 4. The table reports the number of images, FPS, number of cameras, resolution, average Optical Flow magnitude per second (OFps), and normalized camera distance. Specifically, to quantify motion magnitude, we compute OFps by estimating optical flow using RAFT [28] over 1-second temporal windows from fixed test camera. For each window, we retain only pixels whose flow magnitude exceeds small threshold to isolate dynamic regions, and then average the ℓ2 norm of the remaining flow vectors. Camera distance is defined as follows. We obtain camera centers from COLMAP [22] and, for each camera, compute the Euclidean distance to its nearest-neighbor camera center. To remove dataset-specific scale effects, we estimate the scene size from the COLMAP-reconstructed 3D point cloud. Specifically, we build an axis-aligned bounding box by taking the minimum and maximum coordinates along the x, y, and axes, and use the diagonal length of this box as representative scene length. The normalized camera distance is then given by the nearest-neighbor camera distance divided by this bounding-box diagonal. This normalization enables scale-invariant comparison of relative camera spacing and baselines across datasets, preventing absolute scene size from dominating the distance measure. In addition, Fig. 8 provides visual support for the quantitative results presented in Tab. 4. As shown in Fig. 8(a), the SelfCapLR exhibits significantly higher spatial resolution and much larger motion magnitude per unit time (1 second) compared to other datasets. Fig. 8(b) further visualizes comparison of the nearest camera views. Our SelfCapLR features strong camera parallax across wide spatial extent, whereas PanopticSports contains large parallax but from inward-facing cameras converging toward limited region, and DyCheck-iPhone consists of low-parallax monocular captures within much more limited space. These characteristics, combined with the long-range temporal range (around 3.5k frames at 60 FPS) of SelfCapLR, make motion modeling of this dataset particularly challenging. Dataset Scene #Images FPS #Cameras Training resolution Average OFps Normalized camera distance SelfCapLR PanopticSports Bike1 Bike2 Corgi Yoga Dance Basketball Boxes Football Juggle Softball Tennis Apple Block Paper-windmill DyCheck-iPhone Space-out Spin Teddy Wheel 3600 3600 3400 3600 3600 150 150 150 150 150 350 277 429 426 350 60 60 60 60 60 30 30 30 30 30 30 30 30 30 30 22 22 24 24 24 31 31 31 31 31 3 3 3 3 3 1080 1080 1080 1080 1920 1080 1920 1080 1920 1080 640 640 360 640 360 640 360 640 360 640 360 360 360 480 360 480 360 480 360 480 360 480 360 15.55 16.40 72.96 36.74 79.91 27. 26.17 25.92 24.55 23.91 21.53 3. 30.42 23.19 6.19 38.35 11.10 36. 0.109 0.109 0.079 0.155 0.112 0. 0.091 0.085 0.075 0.101 0.088 0. 0.001 0.001 0.001 0.001 0.001 0. Table 4. Dataset statistics. For each scene of the three datasets, we summarize the number of images, Frames-Per-Second (FPS), the number of cameras, the training image resolution, the average OFps, and the normalized camera distance used in our experiments. Figure 8. Dataset visualization. For the datasets analyzed in Tab. 4 and Sec. C.1, (a) we visualize frames sampled every 0.5 seconds over 1-second duration. The size of each frame is scaled according to its relative resolution, and OFps shown below each sequence name indicates the average optical flow magnitude per second. We also show (b) the distance between the closest pair of cameras. As can be seen from the visualizations, SelfCapLR exhibits high resolution, fast motion within unit time, and large camera parallax on wide spatial extent. C.2. Initial Point Cloud E. Further Analysis and Ablations Using temporally dense point clouds [13, 26, 30] imposes substantial burden on training for long-range video and becomes practical hurdle for real-world applications. Motivated by this, we use single set of point clouds to initialize MoRel. This point cloud is constructed by merging point clouds sampled thousands of frames apart along the temporal axis and repeatedly applying voxel-based decimation (voxel size = 0.01) until the total number of points is reduced to below 60k. The resulting sparse point cloud is used to initialize AGlobal in the GCA training stage 3.2. For fair comparison, we apply the same constructed point cloud set to all baseline models, except for [26]. Bike1 and Bike2: 2 point clouds sampled at 2000-frame intervals are merged. Corgi, Yoga, and Dance: 4 point clouds sampled at 1000frame intervals are merged. For [26], since it requires GOP-based point cloud initialization, we followed its original protocol on the SelfCapLR. D. Additional Results D.1. Generalization to DyCheck-iPhone Dataset We further evaluate the generalization ability of our proposed method on an additional dataset. Using the widely adopted Dycheck dataset [6], we compare our results with those reported in MoDec-GS [11], as summarized in Tab. 4. Notably, as shown in Tab. 4 and Fig. 8, the Dycheck dataset exhibits characteristics that are fundamentally different from those of SelfCapLR. Despite these differences, our method achieves solid reconstruction performance even without any dataset-specific tuning. We simply reuse the same core hyperparameterssuch as GOP size, densification parameters, and other KfA-related settingsthat were optimized for SelfCapLR, and adopt straightforward extension of the model. Although we report this naıve extension due to time limitations, further hyperparameter tuning tailored for Dycheck would likely yield additional performance gains. These results demonstrate that our method maintains strong reconstruction quality and reasonable model capacity, even on standard benchmarks that do not involve long-range motions. D.2. Demo Video The goal of our framework is to consistently and finely represent long-range 4D motion without underfitting or temporal flickering  (Fig. 1)  . Such properties cannot be fully validated by numerical metrics alone; they must also be examined through actual rendered video results. To this end, we provide demo video of novel view synthesis results on the SelfCapLR dataset, included in the supplementary material. We highly encourage readers to view it. E.1. Visualization on FHD As shown in Fig. 9, we analyze Feature-variance-guided Hierarchical Densification (FHD) in Sec. 3.4 through levelspecific renderings and their frequency responses. The top row (ac) presents renderings that selectively include only anchor-points assigned to Levels 02 by Variancebased Leveling (VL). Level 0 primarily reconstructs coarse, low-frequency scene structure, whereas higher levels increasingly focus on locally complex regions and finer details. The bottom row (df) reports the 2D FFT magnitudes of the corresponding level-specific renderings, where the spectra shift toward stronger high-frequency components from Level 0 to Level 2. These results indicate that the VL-assigned hierarchy consistently reflects local frequency complexity, and that FHD concentrates the representational capacity of higher levels on regions requiring high-frequency refinement. E.2. Ablation on the Number of Levels In this ablation, we use only 300 frames from SelfCapLR to evaluate the effect of hierarchy level. Tab. 6 analyzes the influence of level granularity in FHD by varying the number of hierarchy levels. As granularity increases, FHD consistently improves reconstruction quality while reducing storage. With the three level setting, the storage per AKey drops from 72.9 MB to 57.3 MB, corresponding to 21.4% reduction. The gain is more pronounced in challenging scenes with large OFps, uch as Corgi (OFps: 72.96) and Dance (OFps: 79.91), where accurate capacity allocation is critical. Fig. 10 provides qualitative breakdown of level contributions. The baseline trained without FHD in Fig. 10 (a) grows anchor-points uniformly and results in the largest anchor-point set. All subsequent visualizations (b-f) are obtained under the three-level FHD setting. Rendering only Level 0 in (b) recovers coarse low frequency structure but misses fine textures and dynamic regions. Level 1 in (c) captures intermediate complexity patterns and moderate motions that are not fully represented at Level 0. Level 2 in (d) focuses on high frequency and fast moving regions with small number of anchor-points, showing that FHD concentrates the highest level on the most demanding content. Combining Levels 0 and 1 in (e) reconstructs most static content with substantially fewer anchor-points than (a), while leaving out the remaining high frequency dynamics. Aggregating all levels in (f) adds the Level 2 details back and completes the dynamic and high frequency content. This progression explains why finer level assignment improves both fidelity and storage efficiency. The two level variant reduces storage but slightly lowers quality in several sequences. This behavior is expected because binary variance partition is too coarse to repreMethod Apple Block Paper-windmill Space-out SC-GS [7] Deformable 3DGS [35] 4DGS [31] MoDec-GS [11] MoRel (Ours) SC-GS [7] Deformable 3DGS [35] 4DGS [31] MoDec-GS [11] MoRel (Ours) mPSNR mSSIM Storage mPSNR mSSIM Storage mPSNR mSSIM Storage mPSNR mSSIM Storage 114.2 42.01 52.02 18.24 79.55 0.221 0.213 0.201 0.220 0.213 0.511 0.510 0.515 0.522 0.519 0.548 0.559 0.550 0.590 0. 446.3 160.2 123.9 17.08 63.92 115.7 118.9 63.52 13.65 52.81 13.98 14.87 13.89 15.57 15.12 173.3 87.71 61.52 23.78 38.37 0.692 0.696 0.691 0.699 0.702 14.79 14.59 14.29 14.65 15. 14.87 14.89 14.44 14.92 14.96 14.96 15.61 15.41 16.48 16.92 Spin Teddy Wheel Average mPSNR mSSIM Storage mPSNR mSSIM Storage mPSNR mSSIM Storage mPSNR mSSIM Storage 232.4 109.4 78.54 18.37 62.63 0.464 0.461 0.460 0.480 0.475 0.354 0.345 0.339 0.374 0.350 239.2 106.1 96.50 16.68 78.76 11.90 11.79 10.83 12.44 11.77 318.7 117.1 80.44 12.28 40. 219.1 133.9 71.80 26.84 68.56 0.407 0.392 0.413 0.433 0.448 0.516 0.508 0.509 0.521 0.522 12.51 11.20 12.31 12.56 13.14 13.90 13.72 13.72 14.60 14.70 14.32 13.10 14.89 15.53 15. Table 5. Quantitative results comparison on the DyCheck-iPhone datasets [6]. Each block element denotes (mPSNR(dB) / mSSIM / mLPIPS / Storage(MB)). Figure 9. FHD Visualization and Frequency Analysis. (ac) Level-specific renderings that retain only Gaussians associated with Level 02 anchor-points under FHD. (df) 2D FFT magnitudes of the corresponding renderings, showing increasingly dominant high-frequency components at higher levels. sent the continuous spectrum of scene complexity. Regions with intermediate texture or motion are forced into either the low level or high level group. Under the progressive densification schedule, this mis grouping delays refinement in those regions and can lead to mild under densification. Introducing third level separates low, mid, and high complexity anchor-points, enabling more accurate refinement placement and recovering the quality gains observed in the full FHD setting. E.3. Backward Contamination An illustration of the inter-chunk interference, referred to as backward contamination, described in Sec. 3.3 is shown in Fig. 11. When bidirectional deformation is trained in naıve chunk-wise manner, the result optimized for Chunkn1 during Jn1 iterations can be corrupted by the Level Bike1 Bike2 Corgi Yoga Dance Average 1 2 3 22.39 / 0.679 / 0.306 22.54 / 0.689 / 0.305 22.51 / 0.691 / 0.309 66 52.5 48.8 22.74 / 0.680 / 0.314 22.76 / 0.684 / 0.315 22.68 / 0.684 / 0.315 20.31 / 0.584 / 0.457 67 54.5 20.26 / 0.587 / 0.459 20.31 / 0.586 / 0.452 61.5 54 50.7 21.28 / 0.789 / 0.297 20.95 / 0.788 / 0.296 21.42 / 0.795 / 0.297 18.05 / 0.617 / 0.357 74.5 68.8 17.89 / 0.615 / 0.353 61.3 95.5 86.8 18.09 / 0.617 / 0.355 81.8 20.95 / 0.670 / 0.346 72.9 20.88 / 0.673 / 0.346 63.3 21.00 / 0.675 / 0.346 57.3 Table 6. Ablation on the number of hierarchy levels in FHD on the SelfCapLR dataset.Level indicates the number of anchor-point hierarchy levels, and we ablate this granularity by varying this count. Each block element of 4-performance denotes (PSNR(dB) / SSIM / LPIPS Storage(MB)). Figure 10. FHD level-wise rendering comparison.(a) Result trained without FHD, using all anchor-points and thus the largest anchorpoint set. (bd) Renderings that retain only Gaussians attached to Level 02 anchor-points, where higher levels progressively capture high-frequency and dynamic regions. (e) Combining Levels 0 and 1 reconstructs most static content with small number of anchor-points. (f) Combining all levels merges the dynamic regions from higher levels into the final reconstruction. subsequent updates of AKey during Jn iterations. The two rendered images correspond to the same time t0 [tn1, tn]; the left image (green box) shows the result after completing only Jn1 iterations of Chunkn1, while the right image (red box) shows the result after completing Jn. As observed in the patch regions, the rendering after Jn1 exhibits clean convergence in object areas. However, after training proceeds to Jn, newly grown anchor-points for optimizing Chunkn remain as ghost-like residues because they are never trained in the backward direction for Chunkn1. Meanwhile, anchor-points that were crucial for Chunkn1 may be pruned, degrading the clearness of the object. This phenomenon is highlighted by the orange dotted ellipses in the rendered patches. Consequently, naıve chunk-b ased training inherently suffers backward contamination. In contrast, our PWD training and IFB training strategies (Sec. 3.3) effectively eliminate such backward contamination. E.4. Temporal Profile Analysis Fig. 12 visualizes the temporal flickering issue that arises in unidirectional deformation, using temporal profile representation. The profile is constructed by accumulating 1D scanline at the center of the rendered frame over time, producing 2D image. Fig. 12(a) shows the result of chunk-wise trained unidirectional method, while Fig. 12(b) presents the result of our bidirectional deformation. As seen in (a), visibly distinguishable discrete boundaries appear at every chunk transition, whereas in (b) such boundaries disappear, exhibiting smooth temporal continuity. This temporal discontinuity manifests as temporal flicker in actual rendered videos, significantly degrading perceptual quality. F. Limitation and Future Works While our method provides strong scalability and temporal consistency for long-range videos, it remains challenging to handle scenes in which the spatial extent becomes significantly larger or the spatial characteristics change over time. The GCA, which is introduced to maintain global Figure 11. Backward contamination. Example visualization and rendered patches illustrating the backward contamination issue in naıve chunk-wise training of bidirectional deformation. temporal coherence, becomes less effective when the scene undergoes substantial spatial changes. In such cases, the GCA would need to be re-initialized (analogous to introducing new large chunk), and temporal inconsistencies may appear around this transition. To address spatial variation while preserving the temporal consistency of our approach, one promising direction is to incorporate spatial grids or frequency-aware representations, as explored in prior works [27, 39]. These components could potentially be integrated with our chunk-wise on-demand temporal loading strategy and the feature-variancebased frequency approximation used in MoRel. We envision extending this idea toward unified framework that handles spatio-temporally largescale motion more effectively, which we leave as an important avenue for future work. Figure 12. Temporal profile visualization. (a) Unidirectional deformation produces visibly distinguishable chunk boundaries, whereas (b) our bidirectional deformation yields smooth temporal continuity."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "Electronics and Telecommunications Research Institute"
    ]
}