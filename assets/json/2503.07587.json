{
    "paper_title": "Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru",
    "authors": [
        "Dunant Cusipuma",
        "David Ortega",
        "Victor Flores-Benites",
        "Arturo Deza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 8 5 7 0 . 3 0 5 2 : r Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru Dunant Cusipuma1, David Ortega1, Victor Flores-Benites1,2, Arturo Deza1,2 1Artificio 2Universidad de Ingeneria Tecnologia (UTEC) Lima, Peru {dunant.c, david.ortega, vfloresb, deza}@artificio.org"
        },
        {
            "title": "Abstract",
            "content": "As multimodal foundational models start being deployed experimentally in Self-Driving cars, reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, country with one of the worst (aggressive) drivers in the world, high traffic index, and high ratio of bizarre to nonbizarre street objects likely never seen in training. In particular, to preliminarly test at cognitive level how well Foundational Visual-Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting gap in their alignment. 1. Introduction After several decades of development, self-driving cars are finally driving in San Francisco, London and Shanghai [1]; and yet despite their initial success, accidents in the industry still occur due to miscalculation at different stages of the driving pipeline: be it at perception, navigation, decision making and/or control even in end-to-end trained systems [8]. And while it is generally accepted that most of these errors occur at the decision making stage, it is Figure 1. As multi-modal foundation models start being tested for Autonomous Driving applications, we inquire their cognitive alignment under Visual Question Answering scheme of multiple videos comparing the answers of VLMs to those of Humans with tools from systems neuroscience. For this example in particular, closer look reveals that the policeman is telling the driver to run through the red light. These sort of edge-case scenarios allow us to better probe cognitive alignment. currently unknown how well AI systems in Autonomous Vehicles (AV) are able generalize to current visual out-ofdistribution scenarios, and how the causal factor of crashes and accidents stem from such stages (perception, decision making, control). Indeed, at the perceptual stage, an active research questions is: how well will current AV systems perform when we push them to their limit? [21]. In industry more so than academia, current alternative for out-of-distribution testing relies on simulated data (GenAI) where prompt & graphic engineers render plethora of fake collisions or other OOD scenarios to train and test AV systems and while this is non-trivial, it is based mainly on anecdotal experience or biased creative input [30, 61]. We however find an interesting and realistic Figure 2. Overview of the VQA procedure on the Robusto-1 Dataset. set of 5 second clips are seen by ground truth anotators (authors) and Meta-Tags are extracted from 16 different categories. These are then passed per each video to Blind Oracle LLM that formulates set of 5 variable questions per clip. An addition 5 set of multiple choice questions that have Yes/No answers and or involved rating or counting, and 5 open counterfactual questions are added to the total pool of 15 questions per clip. We then ask group of VLMs and Humans these questions to collect their answers. alternative: we collect data from Peru, country with one of the worst (aggressive) drivers in the world, high traffic index, and high ratio of bizarre to non-bizarre street objects. Naturally, one must wonder how well will these AD systems generalize to these situations where reckless driving, tuk-tuks, street dogs and unpaved roads are the norm rather than the exception? Indeed, it has not been the first time that pushing the limits of classical datasets in unconventional ways has previously been useful in computer vision and artificial intelligence [2, 3, 6, 7, 22]. Recall that for classical object recognition, datasets such as Object-Net [4] and Stylized ImageNet [17] have complemented the success of ImageNet [53], teaching us about texture bias or cue-conflicting background information [14, 19, 23]. Similarly, from scope of domain generalization [67], we hope that the Robusto-1 dataset can be complementary to many other datasets in Autonomous Driving that focus on testing on clean driving conditions in USA, Canada, UK, Europe or China & Japan, where the social norm is to follow the rules, as is proper in first-world countries where self-driving cars are currently being deployed. However, if we can push the boundary in testing of what is generally not seen at training, then we can have better assessment of how well these systems will perform when facing the unexpected. Thus our paper presents three main contributions: First, we will take advantage of dataset that is gathered from Peru, that as argued previously is not only by nature out-ofdistribution, but it is also highly likely that our type of video data has never been seen in training [48]; Second, we push the limit to comparing AV perceptual systems not through what is standard in terms of object recognition or semantic segmentation, but at the visuocognitive level through Visual Question Answering (VQA). The importance of running these tests is critical given the explosion of multimodal foundational models that are generally openly available by big tech corporations and have started to be adapted for Autonomous Driving [44]; Third, we use Representational Similarity Analysis (RSA) [33, 35] framework, that is originally from Systems Neuroscience and now pervasive in the neonascent field of NeuroAI [25, 45, 59], to study the cognitive alignment between humans and VLMs in the context of (Autonomous) Driving. Indeed, benchmarking multimodal foundation models on VQA is also useful because we have seen early experiments of self-driving cars driving with VLMs [44, 51, 63], signaling that this may be the way of the future. In addition, we hope that as the field of Autonomous Driving moves into releasing their foundation models, that this dataset and evaluation framework may be useful to compare their performance to humans, where data from every country in the world especially emerging economies given their out-ofdistribution nature, and unique driving ecosystem, can help Self-Driving cars become reality worldwide. 2. Dataset Construction most of these questions have high degree of objectivity. total of 285 videos of varying length of at least 5 minutes each were recorded. From these videos, 200 five-second videos were sampled for the Robusto dataset. total of 7 held-out videos were used for the preliminary VQA analysis in this paper. The initial set of 285 videos were collected through 2 cameras placed as dashcams from two sedanlike cars/vehicles. These cameras were one DASHCAMAzdome-BN03 2k-1440p GPS and one GOPRO HERO 10 set to 1920 1080. These videos were collected from 3 cities in Peru: Lima, Cusco and Cajamara. Peru was chosen as first iteration for the Robusto dataset because it has the 2nd worst drivers in the world, the highest traffic index of Latin America, and the 4th highest rank in Global poorest quality of road infrastructure in 2023 according to studies by Compare The Market [20], & TomToms Traffic Index. Inspired by the Visual-Language Model evaluations of LingoQA [44], we then selected 200 5-second scenes from the database sampled at 10 Hz. These 50 frames per video/scene contained mixture of interesting events ranging from jaywalking, reckless driving in varying levels of clutter from which we then later formulate set of questions for VQA (see Ortega et al. [48] on why this is useful). 2.1. Question Formulation For each 50 frame clip we select total of 15 questions per clip to show to both humans and machines. These 15 questions are composed in 3 main groups: 1) Variable questions, that are exclusive to the image and that were prompted by an Oracle LLM that has access to the annotated Meta-data [5] these have an open-ended answer but are mostly factual and have more objective notion of ground truth (See Table 3); 2) Multiple choice questions that were formulated by the authors of the paper covering wide range of topics and that have Yes/No answer-like component or that will require rating/counting-like answer; 3) Counterfactual & hypothetical questions that are also formulated by the authors and will be shared for every single clip and will require advanced reasoning (what-if or happens?) from humans & VLMs, and have an open-ended answer with no objective ground truth."
        },
        {
            "title": "2.1.1 Block 1: Variable Questions",
            "content": "As mentioned earlier, the variable questions pertain to the specific video in question. These questions are based on metadata, which is why they are variable per video, as the environments depicted in the videos differ. The questions may include: What is the driving maneuver performed in this scene?, What is the traffic condition in this scene?, What is the weather condition?, Is there any pedestrian activity?, etc. The significance of these questions lies primarily in the responses provided by the Oracle LLM, and"
        },
        {
            "title": "2.1.2 Block 2: Multiple Choice Questions",
            "content": "Q6: Please rate the level of clutter from 1 to 10; 10 being the highest level of clutter and 1 being the lowest. This question is of high relevance because it will require spatio-temporal reasoning of the 5 second clip in addition to high level perceptual task such as clutter, which is somewhat ambigious despite having image-computable models that still shown some variation [13, 37, 47, 52, 70]. Q7: Is this recurrent driving scenario for you? This Yes/No question is quite interesting for humans because we can assess how often it is that they encounter certain out-of-distribution events given the nature of each clip. Moreover, this question is interesting for machines because it probes sense of meta-awareness as the VLM that has never really driven car [62, 65], yet forcing it to give yes/no answer may give us insights of the training data it has been fed with. Q8: How many pedestrians are there in the scene? Both humans and VLMs will likely have hard time counting pedestrians in moving scene, so this question is important especially in crowded conditions where it is likely both humans and VLMs will have hard time likely providing ball-park estimates. It is important to also notice that VLMs are generally not good counters [34, 42, 43, 62], so having an open question like where they must integrate identity spatio-temporally will be important even if prompted with an ordinal range of intervals (0,1,2-3,4-6,7-10,11-20,21+). Q9: Is this situation hazardous for the driver? Assessment of danger with Yes/No response is an important cue for driving in both humans and VLMs thus adding this constraint is highly important for alignment & morality research in the future of autonomous vehicles [10, 11, 39, 59] Q10: On scale of 1-10, how well do you think an autonomous vehicle would perform in this scene? This last rating question tackles two interesting sub-questions which are how dangerous the scene is, and theory-ofmind angle of transplanting the human or the VLM in an Autonomous Vehicle. This of course is interesting because the assumption is that humans will assess greater levels of danger in terms of assessing difficult situation compared to machine; while such VLMs may possibly underscore the difficulty of navigating scenario."
        },
        {
            "title": "2.1.3 Block 3: Counterfactual & hypothetical ques-",
            "content": "tions We finally also decided to add set of 5 constant counterfactual & hypothetical questions to each human and maFigure 3. figure that shows how to calculate the System Similarity Matrix through Model Gramians as done in Representational Similarity Analysis (RSA) [35]. A) We transform each answer into vector through an embedding to later calculate each systems Gramian. Upper triangular parts of the Gramians across two systems are then correlated (violet). This can be applied to both humans and VLMs. B) The system similarity matrix calculated over all humans and machines allows us to get an idea of how each system is similar to one another. cartoon with no real values is shown in this diagram. chine in our dataset. counterfactual is known as type of question similar to hypothetical what-if? scenario except that [18] it is based on the knowledge of the outcome. Mainly, hypotheticals are of the type: what would you do if? (focus on the future), while counterfactuals are of the type: what would you have done if? (conditioned on the knowledge of the real outcome). Both counterfactual and hypothetical research has been shown to test interesting cognitive biases of each system that will likely arise due to differences in training data, learning algorithm, architecture, inference procedure, and in the case of humans, potentially cultural biases [27, 38, 71] and only recently have they started to be explored with rigor in Autonomous Driving [12, 32, 64]. We list three counterfactual questions and two hypothetical questions that will be asked for every short 5 second clip to each human and machine. These are: Q11: What would have had to happen in this video for crash to have occured involving the driver? Q12: What would have had to happen in this video for an external crash to have occured not involving the driver? Q13: What would have happened in the scene if you had done the opposite action (brake vs accelerate, or accelerate vs brake) ? Q14: What would be the next action to perform Uturn in the next frames if the driver was driving an ambulance instead? Q15: What would be the next action to perform Uturn in next frames if the driver was driving motorcycle instead? 3. Comparing Humans and VLMs through"
        },
        {
            "title": "Systems Neuroscience",
            "content": "In systems neuroscience, representational similarity analysis (RSA) was created to properly study the degree of similarity between any two biological or artificial systems [31, 35, 68]. However, the main problem when trying to compare an artificial neural network to biological system such as monkeys brain, is that often times the dimensionality of the feature vector responses of both systems are not compatible with each other. Let us consider the following: suppose feature vector (fA()) for the hidden layer of neural network has dimensionality dA > 1000 (5th layer of AlexNet), while the biological sensor array (fB()) has dimensionality dB < 100 (for example the average spiking rate of neural array that can record from 80 neurons), how can we compute correlation or notion of similarity between both systems when the dimensionality of the space that we are sampling from is different, even though they are exposed to the same input stimuli like an image I? Given the previous limitation, we can not compute well-known error function such as Mean Square Error (MSE) across the two observations because the dimensions of such observations do not match, i.e. (fA(I)) (fB(I)) is noncomputable. The solution to this dimensionality incompatibility (dA = dB) as posed by RSA is that we feed both systems the same images and create an Gramian Matrix that normalizes the dimensionality by creating matrix of inner products of the feature responses to themselves across wide variety of such input images. In that sense we are implicitly studying the topography of each system and their convergence/divergence with respect to each other [45]. Luckily, we are in very fortunate situation where we do not have dimensionality incompatibility issue for our analysis since both Humans and VLMs will respond to the same questions, and, although there can be differences in terms of varying length of answers per question across all systems, these are normalized to the same dimensionality through sentence embedding [49, 57]. This implies that we can calculate other measures of answer variability such as the euclidean distance between any two answers or one answer and the median of all answers per question more meaningful metric as we will see later in the Experimental section of this paper. That being said, applying Representational Similarity Analysis when the dimensionality is preserved across all systems is just as applicable to us even more so because knowing ground truth is irrelevant in our experiments (!). Recall that to make such comparisons, even humans themselves will disagree with one another given the ambiguity of certain questions and the stimuli shown where in many cases there is no objective ground truth answer (e.g. the counterfactual and hypothetical questions). We will discuss more about why knowing ground truth is irrelevant in our experiments in the next sub-section (Sec 3.1) by expanding more on the methodology behind RSA, though more detailed review can be seen in Kriegeskorte et al. [35]. 3.1. Representational Similarity Analysis To calculate the Gramian Matrix of each System we must first transform each answer (a sentence) to vector through an embedding ϕ(), the characteristic Gramian Matrix of each system given the contexts of responses is: = ϕ1ϕ1 ϕ2ϕ1 ... ϕ1ϕ2 ϕ2ϕ2 ... ϕN ϕ1 ϕN ϕ2 ϕ1ϕN ϕ2ϕN ... . . . . . . . . . . . . ϕN ϕN Taking inspiration from Representational Similarity Analysis (RSA) in Computational Neuroscience [35], we create these System Gramians for each model that is representative of the answers each model has given to the set of = 105 questions (7 videos 15 questions). The goal of this approach is to then summarize the similarity across all systems by creating correlation matrix by pitting each Systems Gramian (S) against each other by only taking the upper triangular component () of such Gramians, and correlating them through the pearson correlation (ρ). This yields the following cross-systems summary matrix M: = 1 , ρ(S 1 ) ρ(S 2 , 1 ) ... , ρ(S 1 ρ(S 2 ) ρ(S 2 , 2 ) ... , 2 ) . . . . . . . . . . . . ρ(S ρ(S 1 , ) 2 , ) ... , ) ρ(S 1 ) ρ(S Notice that is also symmetric, of size (total number of humans and GenAI systems), and has all its diagonal elements set to 1 given the autocorrelation of each system to itself; furthermore all elements are bounded in the interval (cid:2) 1, 1(cid:3). 3.2. Metric-based Analysis An additional type of analysis we have the option to run given equality of dimensions of all embeddings, is distancebased from the embeddings ϕ() in their latest space. In particular to assess similarity of all systems to each systems, we will compute and visualize the L2 distance to the median of all responses per question per video for all systems. This allows us to find reference point (the median) from which we can compute distance excluding outliers to know how similar systems are depending on the distance they have to such reference point from uni-dimensional perpsective. The L2 distance of system to the median per question and video is determined by: L2(ϕA i,j(I), edian(ϕ i,h(I))) 3.3. Dimensionality Reduction We will then finally view the results in the simplest form of analysis by performing PCA to the first two main principal components divided of all the systems embedding vectors divided by block (1,2, and 3). This qualitative description should allow us to visualize the raw overlap of answers across all systems (humans and VLMs) before following-up with quantitative analysis such as the Distance to the Median and RSA. 4. Experiments The main experiment of this paper had 9 volunteer Human subjects who gave their digital consent for public use of their anonymized data, and 6 VLM models what were shown the same subset of 7 videos each with 15 questions per video. In what follows in the paper, we will analyze and draw interim conclusions of the convergence and divergence of each system both across and between each group (humans vs VLMs, humans vs humans and VLMs vs VLMs). While we did not set out to prove or disprove any initial hypothesis, we were inclined to predict that most VLMs would respond very differently to each other given the different ethos that each company has regarding compute and training algorithms; and that conversely Humans Figure 4. The first general result we find after applying Representational Similarity Analysis (RSA) to responses of both humans and VLMs, is that system convergence and divergence is modulated by the type of questions asked. Broadly speaking, we find that all VLMs respond very similar to each other independent of the types of questions asked with surprisingly high correlation for counterfactuals & hypotheticals. Humans on the other hand diverge heavily for counterfactual & hypotheticals and converge strongly for multiple-choice. would present high degree of similarity in their responses. To our surprise, we found that in some cases the results seemed to be the opposite, with exceptions of high partial overlap across all systems for the multiple choice questions. 4.1. Representational Similarity Analysis We first begin by analyzing the results from computing the similarity matrices through the Gramian matrices of each system as done in RSA (Section 3.1), and separate these analysis in 3 blocks. Block 1 consists of the first questions 5 questions (1 to 5) that are composed of variable videospecific questions; Block 2 consists of 5 multiple-choice questions (questions 6-10), and Block 3 consists of questions 5 counterfactual & hypothetical (questions 11-15). These are shown in Figure 4. Broadly speaking, we find that VLMs all share high rate of similarity in their answers independent of the type of questions asked. Humans on the other side, have greater degree of variance. With very high level of alignment in the multiple choice questions, then the variable ones, and finally almost no similarity in the counterfactual & hypothetical questions. When looking at Figure 4, Block 2 shows relativaly high degree of similarity across all systems (Humans and VLMS). This in way is expected because the degrees of freedom for the answers in the multiple choice questions is quite low as there are some questions that are Yes/No, and others that have 10 options (for example the 1-10 clutter rating question). Table 3 in the supplementary material summarizes these types of questions. 4.2. Inter-System Agreement To better understand at more fine-grained level how each systems answer geometrically steered the overall systems topology for the similarity matrices, we proceeded to perform an inter-system agreement analysis by computing the distances between each embedding to the median response per answer across all systems for every question and answer pair. This allows us to probe how aligned Humans and VLMs are to each other both between and across systems at per question basis (Humans vs Humans, VLMs vs Machines, and VLMs vs VLMs). The results from this analysis can be shown here in Figure 5. We find once again that there is difference in the distribution of the pattern of answers depending on the nature of the questions asked to both humans and VLMs. In some cases the answers are more similar (see for example Block 1) likely due to the natural of the questions although they are open they are less ambiguous. Similarly, block 2 despite Figure 5. In this figure we show the distance of each response per question across all systems to the median response. Responses placed here for the VLM was the average response per question rather than single response. We generally observe that the overlap across the answers for VLMs and Humans shifts depending on the nature of the questions asked with larger partial overlap for block 2 given the nature of multiplechoice questions and the smaller space that answers can space as they are prefixed. Variance for block 3 on the other hand is larger across humans and VLMs given the complexity of counterfactual & hypothetical questions. being multiple choice shows that the raw pattern of answers are very different across both humans and VLMs in the two bi-modal distributions have peaks placed on opposite ends of the spectrum highlighting greater biases, although there seems to be one mode of the bi-modal distribution that is shared. Finally block 3 shows the largest difference across Humans and VLMs, likely due to the complex nature of the questions (these are counter-factual and hypotheticals). Humans and VLMs answer these last group of questions very differently, and while we can not assume that the way they answer is different from the way they think (internal representations vs behavioural answers), it does shed light on such cognitive differences. 4.3. Dimensionality Reduction Finally, to have global assessment of how the general pattern of raw responses get mapped into 2D plane across all systems, we projected all answers depending on what block they belong to from each system into 2D plane with PCA, respectively yielding 22%, 52%, 29% explained variance. These plots show that there is partial overlap across both systems, but such results are misleading as verified by the explained variances , and because some plots (Block 3) seem to suggest high alignment between humans and machines, but this is not true until looking into our previous analysis from Sections 4.1,4.2, highlighting the importance of RSA. 5. Discussion In this paper we have barely scratched the surface through preliminary study with humans and VLMs to know if AIChatbots can drive, and more precisely if they would react and behave in similar way to us through Visual Question Answering (VQA). Of course this broader question requires more in-depth research not only at the autonomous driving level by doing trajectory prediction in simulated 3D or real world environment, but also at behavioral level through Q&A experiments [44], and by internally probing representations in Humans and Machines (for now VLMs, commonly known as AI-Chatbots with multi-modal capacity). And while it is difficult to answer this question, in this discussion section we hope to fill in the pieces of this puzzle through understanding the results of some of our experiments, allowing us to build scaffolding into what future experiments and research in this direction could look like. First it is important to decouple our results while interesting with the fact that for any two systems that have similar language responses that have been inferred from an experiment does not imply that the systems think similarly, although there could be hidden correlation. This is because, for two systems to be representationally aligned, system responses to multiple stimuli (be it images or text), they should share common response at the behavioural stage, but also throughout the internal feature space of system [45, 59]. In mathematical terms, we believe it is necFigure 6. collection of 2D projections of the answers from both Humans (violet) and VLMs (green) divided by Blocks. We notice that Humans and VLMs have partial overlap but they generally answer questions very differently, and these vary depending on the type of questions asked. These average embeddings per answer also highlight the variability of answers VLMs have to the same questions. essary, but not sufficient, because two systems could have articulated the same answer, independent of whether they are both representationally aligned or not. This has been recently discussed in the NeuroAI Turing Test [16]. It is of course also possible (though less likely), that two systems have very similar internal representations but their answers diverge due to later-stage processing differences in the internal model of each system. This is why benchmarking using human data in behavioral psychophysical setting has been good first stepping stone for research in the LLM space through platforms like LMArena or OpenLLM, but still requires testing the alignment of their internal representations. All of this is to say that perhaps the future of research towards building safer Autonomous Driving systems would imply comparing not only outputs from humans answering surveys (as we did in this paper and where most VQA research is headed), but also advanced behaviour & internal representations (e.g. comparing the eye-movements or brain activations of an experienced NYC Taxi Driver in MRI of MEG scanner with the attention maps and feature activation maps of VLM) and while these sound like futuristic experiments, current methods in computational neuroscience already make this possible [22, 54, 55, 58, 65]. Zooming out, we have found that there is some alignment of VLMs to Humans but this is quite mild compared to how aligned VLMs are aligned to each other, or how divergent Human responses are to each other given the contrived questions they were prompted with. Why do most VLMs behave so similarly as we have inferred given their answering patterns? We believe it has to do with the curse of training with the same large pool of data: The Internet and most models being based on variant of Transformer [66]. As VLM models scale up due to commercial hype, and the data is all you need hypothesis (aka The Bitter Lesson [60]), they still fail to align with humans in how they answer questions in critical case scenarios like driving1. Indeed, it seems that it does not matter where VLM is trained, what data was used, or what architecture was assigned, they were all highly similar. We would have liked to see an outlier VLM in our study that had stronger difference to the others, in addition to similarity to particular human or group of them. However, we also do see that even across humans, many of them diverge strongly when answering such questions shedding light on the complexity of the representational alignment problem [35]. Following the Anna Karenina principle of All happy families are alike; each unhappy family is unhappy in its own way., we observe something similar for VLMs and Humans in the driving context for this preliminary experiment where All VLMs are alike, each Human is different in their own way. And while this may not be negative outcome, it does shed light on the fact that most VLMs are not as different as we think they are despite continuous race towards building intelligent machines that may one day drive on their own [26, 29]. 1Of course, this is debatable goal because Humans are imperfect drivers and are likely not the long-term gold standard, but VLMs have not achieved super-human prowess [56] in general VQA [9, 15, 28, 36, 41, 46] nor driving VQA [44, 50], to allow us to make claim that it is an advantage that all VLMs respond the same way and different than humans. 6. Acknowledgements"
        },
        {
            "title": "References",
            "content": "Wed like to thank Tobias Gertenberg, Greta Tuckute, Robert Geirhos for insightful discussions over email, and Stanfords Wu Tsai Neuroscience Institute for feedback. This work was privately funded by Artificio and UTEC. All experimental surveys for data collections were approved by the Artificio Co-Founding team who are also authors of this paper. VLMs were ran locally, or through third party APIs, and through the GPU resources of the Google Cloud StartUp Program. All human subjects (Peruvian citizens between the ages of 18 and 30) digitally consented to the use, release and publication of this data with proper anonymization as preliminary study for research purposes in the field of Autonomous Driving and Artificial Intelligence. All Human and VLM raw data & code is open-sourced in the following HuggingFace repository: Robusto-1 All work was done in Lima, Peru. All authors contributed significantly in the design, construction & analysis of the experiments in addition to the writing of this paper. [1] Waymo One is now open to everyone in San Francisco waymo.com. https://waymo.com/blog/2024/06/ waymooneisnow- opentoeveryoneinsan-francisco/. [Accessed 20-11-2024]. [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 2 [3] Manel Baradad Jurjo, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio Torralba. Learning to see by looking at noise. Advances in Neural Information Processing Systems, 34:25562569, 2021. 2 [4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. 2 [5] William Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet Singh. Towards language models that can see: Computer vision through the lens of natural language. arXiv preprint arXiv:2306.16410, 2023. 3 [6] Arjun Chandrasekaran, Ashwin Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. We are humor beings: Understanding and predicting visual humor. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4603 4612, 2016. 2 [7] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. [8] Amit Chougule, Vinay Chamola, Aishwarya Sam, Fei Richard Yu, and Biplab Sikdar. comprehensive review on limitations of autonomous driving and its impact on accidents and collisions. IEEE Open Journal of Vehicular Technology, 2023. 1 [9] Ana Claudia Akemi Matsuki de Faria, Felype de Castro Bastos, Jose Victor Nogueira Alves da Silva, Vitor Lopes Fabris, Valeska de Sousa Uchoa, Decio Goncalves de Aguiar Neto, and Claudio Filipi Goncalves dos Santos. Visual question answering: survey on techniques and common trends in recent literature. arXiv preprint arXiv:2305.11033, 2023. 8 [10] Julian De Freitas and Mina Cikara. Deliberately prejudiced self-driving vehicles elicit the most outrage. Cognition, 208: 104555, 2021. 3 [11] Julian De Freitas, Sam Anthony, Andrea Censi, and George Alvarez. Doubting driverless dilemmas. Perspectives on psychological science, 15(5):12841288, 2020. 3 [12] Julian De Freitas, Andrea Censi, Bryant Walker Smith, Luigi Di Lillo, Sam Anthony, and Emilio Frazzoli. From driverless dilemmas to more practical commonsense tests for automated vehicles. Proceedings of the national academy of sciences, 118(11):e2010202118, 2021. 4 [13] Arturo Deza and Miguel Eckstein. Can peripheral representations improve clutter metrics on complex scenes? Advances in neural information processing systems, 29, 2016. [14] Samuel Dodge and Lina Karam. study and comparison of human and deep learning recognition performance under visual distortions. In 2017 26th international conference on computer communication and networks (ICCCN), pages 1 7. IEEE, 2017. 2 [15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 8 [16] Jenelle Feather, Meenakshi Khosla, Murty, and Aran Nayebi. Brain-model evaluations need the neuroai turing test. arXiv preprint arXiv:2502.16238, 2025. 8 [17] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing In Internashape bias improves accuracy and robustness. tional Conference on Learning Representations, 2018. 2 [18] Tobias Gerstenberg. Counterfactual simulation in causal cognition. Trends in Cognitive Sciences, 2024. 4 [19] Zahra Golpayegani, Patrick St-Amant, and Nizar Bouguila. Clarifying myths about the relationship between shape bias, In 2023 20th Conference on accuracy, and robustness. Robots and Vision (CRV), pages 281287. IEEE, 2023. 2 [20] Noemi Hadnagy. The best & worst drivers in the world, 2023. 3 [21] Isaac Han, Dong-Hyeok Park, and Kyung-Joong Kim. new open-source off-road environment for benchmark generalization of autonomous driving. IEEE Access, 9:136071 136082, 2021. 1 [22] Anne Harrington and Arturo Deza. Finding biological plausibility for adversarially robust features via metameric tasks. In International Conference on Learning Representations, 2022. 2, 8 [23] Katherine Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias in convolutional neural In Advances in Neural Information Processing networks. Systems, pages 1900019015. Curran Associates, Inc., 2020. 2 [24] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 4 [25] Eghbal Hosseini, Colton Casto, Noga Zaslavsky, Colin Conwell, Mark Richardson, and Evelina Fedorenko. Universality of representation in biological and artificial neural networks. bioRxiv, 2024. [26] Eghbal Hosseini, Colton Casto, Noga Zaslavsky, Colin Conwell, Mark Richardson, and Evelina Fedorenko. Universality of representation in biological and artificial neural networks. bioRxiv, pages 202412, 2024. 8 [27] Kai-Chieh Hsu, Karen Leung, Yuxiao Chen, Jaime Fisac, and Marco Pavone. Interpretable trajectory prediction for autonomous vehicles via counterfactual responsibility. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 59185925. IEEE, 2023. 4 [28] Dawei Huang, Chuan Yan, Qing Li, and Xiaojiang Peng. From large language models to large multimodal models: literature review. Applied Sciences, 14(12):5068, 2024. 8 [29] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. 8 [30] Applied Intuition. Traffic sign datasets. Applied Intuition Products. [31] Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte. Deep supervised, but not unsupervised, models may explain it cortical representation. PLoS computational biology, 10 (11):e1003915, 2014. 4 [32] Lara Kirfel, Robert MacCoun, Thomas Icard, and Tobias Gerstenberg. Anticipating the risks and benefits of counterfactual world simulation models. 2023. 4 [33] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network represenIn International conference on machine tations revisited. learning, pages 35193529. PMLR, 2019. 2 [34] Eliza Kosoy, Annya Dahmani, Andrew Lampinen, Iulia Comsa, Soojin Jeong, Ishita Dasgupta, and Kelsey Allen. Decoupling the components of geometric understanding in vision language models. arXiv preprint arXiv:2503.03840, 2025. 3 [35] Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2:249, 2008. 2, 4, 5, 8 [36] Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, and Wenhu Chen. Theoremexplainagent: Towards multimodal explanations for llm theorem understanding. arXiv preprint arXiv:2502.19400, 2025. [37] Cameron Kyle-Davidson, Elizabeth Yue Zhou, Dirk Walther, Adrian Bors, and Karla Evans. Characterising and dissecting human perception of scene complexity. Cognition, 231:105319, 2023. 3 [38] David Lagnado, Tobias Gerstenberg, and Roi Zultan. Causal responsibility and counterfactuals. Cognitive science, 37(6):10361073, 2013. 4 [39] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 3 [40] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 4 [41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 8 [42] Dezhi Luo, Haiyun Lyu, Qingying Gao, Haoran Sun, Yijiang Li, and Hokin Deng. Vision language models know law of conservation without understanding more-or-less. arXiv preprint arXiv:2410.00332, 2024. [43] Jingyuan Ma, Damai Dai, and Zhifang Sui. Large language models are unconscious of unreasonability in math problems. arXiv preprint arXiv:2403.19346, 2024. 3 [44] Ana-Maria Marcu, Long Chen, Jan Hunermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question answering for autonomous driving. 2, 3, 7, 8 [45] Patrick Mineault, Niccol`o Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew arXiv preprint Payne, et al. Neuroai for ai safety. arXiv:2411.18526, 2024. 2, 5, 7 [46] Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Faitheval: Can your language model stay faithful to context, even if the moon is made of marshmallows. arXiv preprint arXiv:2410.03727, 2024. 8 [47] Fintan Nagle and Nilli Lavie. Predicting human complexity perception of real-world scenes. Royal Society open science, 7(5):191487, 2020. 3 [48] David Ortega, Dunant Cusipuma, Victor Flores, and Arturo Deza. Why has autonomous driving failed? perspectives from peru and insights from neuroai. Artificio Blog Series, 2023. 2, [49] Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 5 [50] Kaavya Rekanar, Martin Hayes, Ganesh Sistu, and Ciaran Eising. Optimizing visual question answering models for driving: Bridging the gap between human and machine attention patterns. arXiv preprint arXiv:2406.09203, 2024. 8 Jan Hunermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, and Oleg Sinavski. Carllava: Vision language models for camera-only closed-loop driving. arXiv preprint arXiv:2406.10165, 2024. 2 [51] Katrin Renz, Long Chen, Ana-Maria Marcu, [52] Ruth Rosenholtz, Yuanzhen Li, and Lisa Nakano. Measuring visual clutter. Journal of vision, 7(2):1717, 2007. 3 [53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Imagenet large Aditya Khosla, Michael Bernstein, et al. scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 2 [54] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib Majaj, Rishi Rajalingham, Elias Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score: Which artificial neural network for object recognition is most brain-like? BioRxiv, page 407007, 2018. [55] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal Hosseini, Nancy Kanwisher, Joshua Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences, 118(45):e2105646118, 2021. 8 [56] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. 8 [57] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33:1685716867, 2020. 5 [58] Vighnesh Subramaniam, Colin Conwell, Christopher Wang, Gabriel Kreiman, Boris Katz, Ignacio Cases, and Andrei Barbu. Revealing vision-language integration in the brain with multimodal networks. ArXiv, pages arXiv2406, 2024. 8 [59] Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley Love, Erin Grant, Getting arXiv preprint aligned on representational alignment. arXiv:2310.13018, 2023. 2, 3, 7 Iris Groen, Jascha Achterberg, et al. [60] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. 8 [61] Machine Learning team at Parallel Domain. Parallel domain synthetic data improves cyclist detection. Parallel Domain Blog Series, 2022. 1 [62] Tristan Thrush, Jared Moore, Miguel Monares, Christopher Potts, and Douwe Kiela. am strange dataset: Metalinguistic tests for language models. arXiv preprint arXiv:2401.05300, 2024. 3 [63] Hanlin Tian, Kethan Reddy, Yuxiang Feng, Mohammed Quddus, Yiannis Demiris, and Panagiotis Angeloudis. Large (vision) language models for autonomous vehicles: Current trends and future directions. Authorea Preprints. 2 [64] Stratis Tsirtsis, Manuel Gomez Rodriguez, and Tobias Gerstenberg. Towards computational model of responsibility judgments in sequential human-ai collaboration. In Proceedings of the Annual Meeting of the Cognitive Science Society, 2024. [65] Greta Tuckute, Nancy Kanwisher, and Evelina Fedorenko. Language in brains, minds, and machines. Annual Review of Neuroscience, 47. 3, 8 [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 8 [67] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Yu Philip. Generalizing to unseen domains: survey on domain generalization. IEEE transactions on knowledge and data engineering, 35(8):80528072, 2022. 2 [68] Daniel LK Yamins, Ha Hong, Charles Cadieu, Ethan Solomon, Darren Seibert, and James DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the national academy of sciences, 111(23):86198624, 2014. 4 [69] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 4 [70] Chen-Ping Yu, Dimitris Samaras, and Gregory Zelinsky. Modeling visual clutter perception using proto-object segmentation. Journal of vision, 14(7):44, 2014. 3 [71] Alessandro Zanardi, Andrea Censi, Margherita Atzei, Luigi Di Lillo, and Emilio Frazzoli. counterfactual safety margin perspective on the scoring of autonomous vehicles riskiness. IEEE Transactions on Control Systems Technology, 2023. Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru"
        },
        {
            "title": "Supplementary Material",
            "content": "6.1. Human Protocol VQA 6.2. Multimodal Input Processing Pipeline total of nine humans participated in this small pilot experiment as volunteers. consent digital consent form was given to the volunteers where they were briefly told about the goals of the study. Participants were required to perform the task on computer or laptop and were not allowed to use their phones to ensure wider field of view, as watching video on phone may result in missing key elements in such short clips. Responses were recorded digitally and stored anonymously with encrypted participant IDs. Participants provided their digital consent by ticking on box in Google Forms spreadsheet to share their data in anonymized way for research and commercial purposes. The participant demographics consisted of nine individuals aged 18 to 35 from Peruvians living in Peru. Subjects were recruited as mixture of friends and colleagues of the authors through open advertising in different group chats. The participants had varying levels of driving experience and were fluent in English, as the questions were asked and answered in English. Participants also digitally confirmed their english fluency, and participants who did not have such were potentially going to be removed from the analysis. This was not the case and we analyzed all 9 subjects in the experiment. It is important to note that the VLMs were also tested using the same questions in English. All participants were Peruvian. We are aware that the small study group is interesting because Peruvians generally speak spanish (not english), and that VLMs have likely not seen dashcam driving data in Peru (a spanish speaking country). future study will include English-speaking participants (e.g. Americans) and show them mixture of data from both people driving in the United States and Peru to study the interaction of language fluency and dashcam data provenance to the study. Interestingly, some participants at the end of the experiment thought it was text-base labelling task (given what they have read in the news about manual bounding-box labeling being required to train AI models), as they were unfamiliar with Question-Answering (QA) research. Approximately half of the participants reported via email, Slack or WhatsApp that many questions seemed subjective however, this is not negative comment, as it verifies the intention of our experiment to push the boundary of human interpretability through OOD stimuli with questions of varying level of subjectiveness such as the hypotheticals & counterfactuals in Block 3. To systematically evaluate the ability of Vision-Language Models (VLMs) to analyze driving scenes, we implemented input processing protocol tailored to the specific requirements of each model. The objective was to ensure that all models received equivalent multimodal input while respecting their individual API constraints and format requirements. This protocol enabled us to compare their performance fairly across tasks involving video-based visual question answering (VQA). Each model was provided with series of frames extracted from driving videos alongside set of structured questions. Given the variability in how different VLMs process visual inputs, we employed prompt adaptation mechanism that converts video data into compatible format for each model. Below, we describe the input processing strategy for each VLM tested in our experiments. CogVLM. For CogVLM, video data were submitted as complete binary files. The input video was read from local file in binary mode and passed along with an adapted prompt via the Replicate API. The input dictionary included keys for the prompt, the binary video file (input video), and generation parameters (top set to 0.9, temperature set to 1, and max new tokens of 2000). Qwen 2. For Qwen2, videos were hosted remotely. Each video was downloaded using HTTP requests, converted into an in-memory file using Pythons BytesIO module, and then combined with the adapted prompt to form the input. These data were sent to the model via the Replicate API in similar structure as for CogVLM, enabling Qwen2 to process video inputs directly from remote sources. Pixtral. Pixtral Large model processes video content by analyzing individual frames rather than receiving complete video file as single input. For each video, frames were extracted at rate of 1 FPS and converted to Base64encoded JPEG strings. The resulting input was constructed as message comprising text component (the adapted prompt) followed by series of image components. Each image was represented by Base64 string prefixed with \"data:image/jpeg;base64,\". DeepSeekV3. DeepSeek V3 was evaluated by extracting video frames at 10 FPS and converting each frame into Base64-encoded string. The adapted prompt was combined with list of these image strings (each prefixed with \"data:image/jpeg;base64,\") into message structure, which was then submitted to the model via its API. Gemini. Gemini processes video inputs by first combining the system prompt with marker that denotes the start of the visual sequence. Each video frame is then converted into an image component via the APIs Part.from image method, and text component containing the user prompt is appended. Gemini 2.0 was deployed on Google Cloud Platform (GCP) through Vertex AI, utilizing the checkpoint gemini-2.0-flash-exp in accordance with the guidelines provided in the Vertex AI Generative Models documentation. The LLM generation parameters were set to maximum of 100 tokens, temperature of 1.0, and top-p of 0.9. Llama. For Llama-based models, our protocol transforms each video frame into Base64-encoded JPEG string that is then integrated with the system instructions and user prompt into single text block. This combined input is submitted to the model via its API. In our experiments, Llama 3.2 was deployed in GCP using Vertex AI, employing the checkpoint Llama-3.2-11B-Vision-Instruct-meta. The LLM generation parameters were set to maximum of 100 tokens, temperature of 1.0, and top-p of 0.9. 6.3. MetaData & Tags The distribution of driving scenarios suggested that we create pre-fixed list of 16 meta-tags from which we manually annotate certain properties from video clip. Sample metadata attributes are: 1. Vehicle Action, 2. Driving Action Reasoning, 3. Vehicle Motion Behavior, 4. Traffic Signs, 5.Traffic Lights, 6. Weather Conditions, 7. Road Surface Conditions, 8. Road Structures, 9. Static Objects, 10. Other Vehicle Behaviors, 11. Pedestrian Behaviour, 12. Unexpected Obstacles, 13. Emergency Situations, 14. Lighting Conditions, 15. Traffic Conditions, 16. Driving Environment. The full list of information of the labels derived from the meta-data attributes can be seen in the Table 4. These meta-tags are available for all 200 videos, and the 7 external ones used in the study of our paper, and were used as the basis for prompting the Oracle LLM the variable questions. 6.4. Question Generation Details with LLMs To assess the current gap in the ability of Language Models to understand driving scenes, we designed process for generating context-specific questions for each video. This process focuses on the first block of queries, termed Variable, one of three blocks used in our experiments (the other two being Multiple Choice and Counterfactual & Hypothetical). In this block, each video is associated with set of five targeted questions, along with concise answers, that are derived solely from the metadata manually curated for the corresponding driving scene. Initially, we compiled database containing key metadata for each video. This metadata includes general information such as the sample identifier, scene location, ego vehicle details (e.g., vehicle actions, motion behavior), and external factors (e.g., traffic signs, weather conditions, road surface conditions). For each video, the curated metadata is stored in JSON file that is subsequently processed using GPT-based models accessed through the ChatGPT platform (specifically, through https://chatgpt.com/gpts). Our approach leverages customizable GPTs, which are configured through two primary components: detailed system instructions and an initial conversation starter phrase. The system instructions explicitly guide the model to generate five relevant questions based exclusively on the provided metadata, while the starter phrase establishes the context for the conversation, ensuring consistency and clarity throughout the exchange. The instructions provided to the GPT are as follows: You are an AI assistant specialized in analyzing driving scenarios. You will receive list of JSON objects, each containing partial metadata about different driving scenes. Be aware that the provided data is incomplete, and important elements of the scenes may be missing. For each JSON sample, your task is to: 1. Read the JSON object. 2. Include the \"#\" and \"Name\" from the JSON object at the beginning to indicate which sample you are analyzing. 3. Generate **five** relevant and contextually appropriate questions based solely on the information available in the JSON object. 4. Provide short and direct answers to each question. Focus on what is observed in the scene according to the metadata, and consider that there might be elements not explicitly mentioned. Example format: Sample #: 1 Name: 2023_01_10_153834_044_clip_00_16_100 Q1: [Question 1] A1: [Answer 1] Q2: [Question 2] A2: [Answer 2] Q3: [Question 3] A3: [Answer 3] Q4: [Question 4] A4: [Answer 4] Q5: [Question 5] A5: [Answer 5] The conversation begins with the following starter prompt, which underscores the need to analyze each JSON sample individually: Below is list of JSON samples, each containing partial information about different driving scenes. Please analyze each sample individually. For each one: - Generate five relevant questions based on the metadata. - Provide short and direct answers to each question. Remember that the metadata may be incomplete, and consider the possibility that there are other elements not mentioned in the file. [Insert the list of JSON samples here] 6.5. Testing Frame Processing Capacity We conducted synthetic experiment to evaluate whether each LLM could correctly interpret the temporal sequence of frames and detect objects introduced at specific moments. series of frames was generated depicting red ball on white background moving diagonally from the bottom-left to the top-right corner. The objective was to verify whether the models could infer the balls direction by processing the frames in the correct temporal order. Additionally, we introduced green star in one frame at time to assess whether the models were capable of examining all frames throughout the sequence. In each iteration of the experiment, the green star was inserted into different frame. If model accurately recognized the presence of the green star, it suggested that the model had successfully processed that particular frame rather than skipping or averaging across the sequence. The questions posed to the models focused on identifying the direction of the movement of the red ball and specifying if other objects were present in the frames. The following prompt was used in each iteration: Task: Answer the following questions based solely on the sequence of images provided. The images represent frames from short video sequence. Questions: 1. In which direction is the red ball moving? 2. Do you see any other objects besides the red ball? If so, please describe the object(s) and their color(s). Instructions: - Carefully analyze each image frame by frame. - Base your answers only on what is visibly present in the images. - Do not assume any information that is not directly observable. - Provide concise answer, and explain your reasoning if necessary. By repeating this process for multiple iterations (placing the green star in different frames each time) and examining the models responses, we assessed whether they could track the trajectory of the red ball and the newly introduced object without overlooking any part of the video. Figure 7. standing. Images used to analyze the models temporal under-"
        },
        {
            "title": "6.5.1 Results",
            "content": "The results confirmed that Pixtral supports maximum of six frames per input, which means it did not successfully process the test at frame rate of 10 fps. However, when tested at 1 fps, it demonstrated accurate frame sequence recognition, including the detection of the green star in the final frame. On the other hand, Deepseek was tested at 10 fps and exhibited performance comparable to other models in terms of general response. However, key limitation was identified: Deepseek only supports OCR (Optical Character Recognition), meaning its analysis is restricted solely to textual content present in the images. Since the model does not process visual information beyond text, we infer that its performance was influenced by the filenames and image descriptions, which contained hints about the video content. In fact, when the file names were changed, the model completely lost its accuracy in responses, confirming that its performance relied on external textual information rather than genuine understanding of the visual content. We highlighted this limitation in the results  (Table 1)  , where Deepseek appears with dagger symbol (), indicating that while it accepts image inputs, it only processes them for OCR purposes rather than for true visual scene understanding. Additionally, we evaluated Qwen2 and CogVLM using the Replicate platform, setting the frame rate to 10 fps. According to available benchmarks, these models can process longer videos at higher frame rates. However, we standardized the input to 10 fps to ensure consistent comparison across models, providing each Vision-Language Model (VLM) with an equivalent amount of temporal information. While both models successfully passed the test, there is evidence of internal processing mechanisms that influence how frames are interpreted. Due to this additional processing, these models are marked with an asterisk (*) in the results table to indicate potential differences in frame handling compared to other models. Models Name DeepSeek V3 Pixtral *Qwen *CogVLM Gemini Llama deepseek-chat pixtral-large-latest Qwen2-VL-7B cogvlm2-video Gemini-2.0-flash-exp Llama-3.2-11B-Vision-Instruct Test Passed? 10fps 1fps 0.5fps - - - - - - - - - - - Table 1. Comparison of vision-language models, including test results. Models marked with * were run through the Replicate platform. Models marked with have pseudo multi-modal capabilities (see Section 6.5). 6.6. Running Visual-Language Models We conducted our experiments using six publicly available Vision-Language Models (VLMs): Deepseek, Pixtral, Qwen2, CogVLM, Gemini, and Llama. These models were developed by organizations from three different countries: the United States of America (Gemini and Llama), France (Pixtral), and China (DeepSeek, CogVLM, and Qwen). Below, we describe the key aspects of how each model was accessed, configured, and tested. [69] and CogVLM2. The Qwen2 Qwen2 and CogVLM2 [24] models were accessed through the Replicate platform, which offers straightforward interface for evaluating AI models. Despite their fee-based model usage, the cost per query proved minimal relative to other platforms and was justified given our limited set of video prompts. Setting up and running the models was straightforward process, as it did not require the installation of additional tools or the implementation of advanced configurations. However, the example Python script provided by Replicate per model was modified to enable its use through the API. The modifications were primarily aimed at ensuring that the input consisted of the trial dataset videos and the prompt which had already been processed as previously detailed. These queries were directly loaded into the system, allowing for the efficient generation of results in near-instantaneous manner. In terms of performance, the response time for each model was approximately 9-16 seconds, ensuring rapid turnaround for queries. Additionally, the estimated cost per query to CogVLM model was $0.000725 and to Qwen2 model was $0.000975 providing reference for computational efficiency and resource allocation. Both models demonstrated fast and consistent performance on basic visual and textual analysis tasks. However, certain limitations were observed when interpreting images repetitively, evidencing low variability in their responses, since they responded exactly the same to the same image and text input. Despite this limitation, the accessibility and ease of use of Replicate was valuable tool to run and test models without requiring significant computational resources. Pixtral. We evaluated the Pixtral Large model using its official API, which offers complimentary and direct access to its functionalities. Following the official documentation, we integrated the Pixtral model through JSON-based requests to transmit images and prompts. On average, Pixtral required 1.52.8 seconds per query when the input consisted of five images plus question. However, processing times increased for more complex images, such as those containing multiple overlapping objects or environments with variable lighting. In these cases, response times extended due to challenges in classifying secondary or out-ofdistribution (OOD) objects. In one specific test case, involving counterfactual & hypothetical question and an urban scene with traffic and various unidentified objects on the street, Pixtral required approximately 9 to 16 seconds to generate response, likely due to the complexity in the image. The experiment with the 7 videos ended with 99 % success rate in executing requests without errors (only one error was obtained during the experiment). Overall, Pixtral showed strong performance on tasks such as generating textual descriptions and variability in its responses without going out of context. In conclusion, the Pixtral API proved to be robust, user-friendly, and highly effective, making it valuable tool for the development and evaluation of VisionLanguage tasks. DeepSeek-V3. DeepSeek V3 [40] was evaluated through its official API to assess its capability in visual and textual analysis tasks. The integration was carried out through JSON-based requests, achieving an average response time of 0.9 seconds per query, highlighting its speed compared to other models tested. The experiment used frame rate of 10 images per second. For each query, 10 repetitions were performed to ensure consistency of the results. Regarding token management, DeepSeek models use tokens as basic units to process text and as basis for billing. token can represent character, word, number, or symbol. Approximately, the cost per query for us was 1200-1500 tokens. query consists of processed message/prompt and set of 50 images. The prompt contains approximately 913 characters, and the images are in HD, with resolution of 1920 1080 pixels. The exact number of tokens processed per query is determined based on the models response. publicly available tokenizer facilitated offline estimation of token usage, allowing for more efficient planning of model queries. DeepSeeks source code is available in its official GitHub repository, further enabling transparency and reproducibility. Gemini. Gemini 2.0 was deployed on Google Cloud Platform (GCP) via Vertex AI, utilizing the checkpoint gemini-2.0-flash-exp to ensure seamless integration into our experimental pipeline. Our implementation followed the guidelines provided in the Vertex AI Generative Models documentation available at https://cloud. google . com / vertex - ai / generative - ai / docs / reference / python / latest / vertexai . generative_models. We tested this model with videos recorded at 1920 1080 resolution and 10 frames per second, encoding each frame prior to submission through the Vertex AI API. For each question on every video, the experiment was repeated 20 times to capture the variability in the LLM responses. Llama. Llama 3.2 was deployed on Google Cloud Platform (GCP) via Vertex AI following the recommended guidelines for uploading pre-built models to the Model Registry and deploying them to Vertex AI Endpoint. In our experiments, we used the checkpoint Llama-3.211B-Vision-Instruct-meta. The model was deployed on an a2-highgpu-1g machine equipped with one NVIDIA Tesla A100 GPU. Video frames, provided in JPEG format, were used as inputs. Notably, this model exhibited limitation in its processing capacity, as it was able to process only up to three frames per video. To capture the variability in the responses, each question for every video was repeated 20 times. 6.7. Sentence Embedding textual data in high-dimensional vector To represent space, we used sentence embedding model that encoded semantic information while preserving contextual dependencies. The primary sentence embedding used for the plots presented in the main body of this paper was all-mpnet-base-v2, transformer-based architecture pre-trained on large-scale corpora and optimized for semantic similarity tasks available in https: //huggingface.co/sentence-transformers/ all - mpnet - base - v2. To generalize our results, we re-ran our analysis using two other sentence embeddings such as paraphrase-mpnet-base-v2 and e5-large-v2 to illustrate the effects of different emThese rebeddings on the final pattern of results. Both of sults for RSA can be seen in Figure 9. these sentence embeddings are available in https : //huggingface.co/sentence-transformers/ paraphrase - mpnet - base - v2 and https : / / huggingface.co/intfloat/e5largev2 respectively. 6.8. Data Curation and Additional Analysis There were certain cases for the multiple choice questions where the VLMs did not correctly answer one of the main responses, or answered with small variant. For example, in some cases there are answers that only had Yes/No, that were responded with similar but Yes, Option: no exact answers like Option: [No], Answer: No or [No], etc. These variants of Yes/No were cured to be the same as Yes or No respectively. Option: For other multiple-choice questions, there were examples such as those for the clutter rating where the VLM responded to some false interval that was not in the options. For example, Option: 2 to 4, Option: More than 10, Option: 1 to 5, Option: 10 or more or just Option: 9. To curate the data, the solution was to review and contrast the original intervals we proposed as multiple-choice responses  (Table 3)  and verify whether the answers fit within the provided 1 to 5, Option: ranges. For example, Option: More than 10, or Option: 2 to 4 did not fit into any of the established ranges. In such cases, the response was discarded and not considered for analysis. On the other hand, there were cases where the response did fit within one of the ranges, such as Option: 9 or Option: 11-15. In this data curation process, we were strict in ensuring that the responses matched correctly. results, we find total of 1734/5460 (31.75%) modifications in all Vision-Language Models (VLMs). On the other hand, responses that could not be included in the analysis were ignored and discarded. Ignored responses include, for example, those that did not fit within any of the predefined multiple-choice ranges. There were total of 79/5460 (1.44%) of ignored responses. Next, we will provide detailed breakdown of the modifications and ignored responses for each VLM. Processed Data:"
        },
        {
            "title": "As final",
            "content": "Llama-3.2 - Modifications: 350, Ignored: 2, Total responses: 1050 cogvlm2 - Modifications: 22, Ignored: 0, Total responses: 105 deepseek_v2 - Modifications: 327, Ignored: 44, Total responses: 1050 gemini-2.0 - Modifications: 667, Ignored: 33, Total responses: 2100 pixtralModifications: 350, Ignored: 0, Total responses: 1050 qwen2 - Modifications: 18, Ignored: 0, Total responses: 105 All results in the main body of this paper were done with the curated responses. However, we also re-did our analysis with the uncurated (raw) responses, and also using single answer instead of the average (pooled) answer per query per VLM. Indeed, as can be seen in our raw data repository: Robusto-1, there are cases where some VLMs produce highly varying responses to the same questions. To address this variablilty (given that the embedding of several Yess and Nos can be Maybe, and similarly for open response questions, we also re-did our analysis with single responses, and found no large variation to the same pattern of results as using the pooled answer per VLM. We have added these main results in the supplementary plots. Models DeepSeek V3 Pixtral Qwen2 CogVLM Gemini Llama Name deepseek-chat pixtral-large-latest Qwen2-VL-7B cogvlm2-video Gemini-2.0-flash-exp Llama-3.2-11B-Vision-Instruct"
        },
        {
            "title": "API Access\nDirect\nDirect\nReplicate\nReplicate\nDirect\nVertex AI",
            "content": "Input Modality Images & Text Images & Text Video & Text Video & Text Images & Text Images & Text Frame Rate (fps) 10 1 10 10 10 0.5 Table 2. Summary of parameters and input modalities for evaluated Vision-Language Models. API Access indicates the method through which each model is accessed: Direct access via dedicated API, or indirectly via external platforms such as Replicate or custom deployment on Vertex AI. Questions Question 1 Question 2 Question 3 Question 4 Question 5 Q6: Please rate the level of clutter from 1 to 10. Consider 10 as the highest level of clutter and 1 as the lowest. Q7: Is this recurrent driving scenario for you? Q8: Estimate how many pedestrians are there in the scene? Q9: Is this situation hazardous for the driver? Q10: On scale of 1-10, how well do you think an autonomous vehicle would drive in this scene? Consider 10 as perfect driving and 1 as terrible driving. Q11: What would have had to happen in this video for crash to have occured involving the driver? Q12: What would have had to happen in this video for an external crash to have occured not involving the driver? Q13: Imagine if you had taken the opposite action in this scene (for example, braking instead of accelerating, or accelerating instead of braking). What do you think would have happened? Q14: What would be the next action to perform U-turn in the next frames if the driver was driving an ambulance instead? Q15: What would be the next action to perform U-turn in the next frames if the driver was driving motorcycle instead? Open-ended text response Open-ended text response Open-ended text response Open-ended text response Open-ended text response 1yes/no 0,1, 2-3,4-6,7-10,11-20, 21+ yes/no 1-10 Open-ended text response Open-ended text response Open-ended text response Open-ended text response Open-ended text response Table 3. Overview of the questions and expected response formats, grouped into three categories: Variable (Questions 15), Multiple Choice (Questions 610), and Counterfactual & Hypothetical (Questions 1115), as administered to human participants and VisionLanguage Models. Vehicle Actions Single-Label Driving Action Reasoning Multi-Label & OpenEnded Vehicle Motion Behavior Multi-Label Traffic Signs Multi-Label Traffic Lights Single-Label Weather Conditions Multi-Label Road Surface Conditions Multi-Label Road Structures Multi-Label Static objects Other Vehicle Behaviors Multi-Label & OpenEnded Multi-Label Pedestrian Behavior Multi-Label Unexpected Obstacles Multi-Label & OpenEnded Emergency Situations Single-Label Lighting Conditions Single-Label Traffic Conditions Single-Label Driving Environment Single-Label Ego Vehicle Describes the physical actions performed by the vehicle, such as turns, acceleration, braking, lane changes, etc. Its purpose is to capture the observable behavior of the vehicle in the scene. Explains the reasoning behind the vehicles actions (e.g., stopping due to pedestrian or changing lanes to avoid an obstacle). Its purpose is to provide the necessary context to understand why the observed actions were taken. Describes the observable motion of the vehicle, such as steady driving, acceleration, or braking, based on the visual cues in the segment. Its purpose is to capture how the vehicle moves during the segment in qualitative way, without requiring precise numerical values. External Factors Identifies and categorizes the traffic signs visible in the scene (e.g., stop signs, yield signs, speed limits). Its purpose is to evaluate how traffic signs influence the decisions of the driver and the vehicle. Captures the state of the traffic light in the scene (red, green, yellow, off). Its purpose is to determine how the traffic light signals influence the vehicles behavior. Describes the weather conditions during the driving event (e.g., fog, rain, sunny). Its purpose is to evaluate how weather conditions affect driving decisions and visibility. Describes the physical condition of the road, including potholes, poor maintenance, slippery surfaces, and temporary roadworks or debris. Its purpose is to evaluate how the road surface affects vehicle control and driving safety. Describes the physical infrastructure elements present on or alongside the road, such as islands, tunnels, and pedestrian crossings. Its purpose is to capture how these structures influence the driving behavior of the vehicle. Identifies buildings, poles, trees, and other static objects in the environment. Its purpose is to describe the urban or rural context surrounding the road. Describes the interactions and maneuvers of external vehicles, including public transport, taxis, motorbikes, and private vehicles, and how they affect the driving decisions of the ego vehicle (e.g., lane invasion, sudden stops, overtaking). Its purpose is to capture the influence of other vehicles on the behavior of the ego vehicle. Observes the behavior of pedestrians in the scene (crossing, waiting on the sidewalk, walking on the road). Its purpose is to capture how pedestrians interact with the vehicle and how they influence driving decisions. Describes any unexpected object or situation on the road, such as improperly parked vehicles, street vendors, or animals. Its purpose is to identify uncommon events that may affect driving. Describes emergency situations or rare events that require rapid response (accidents, roadblocks, roadworks). Its purpose is to identify incidents that alter the normal flow of traffic and require immediate attention. Describes the lighting conditions in the scene, such as natural lighting, street lighting, poorly lit areas. Its purpose is to evaluate how visibility affects driving decisions. Describes the state of traffic on the road, such as free-flowing, congested, stopped. Its purpose is to evaluate how traffic density affects driving decisions. Describes the environment in which the driving takes place, including areas that may affect vehicle behavior, such as school zones, markets, construction sites, or rural areas. Its purpose is to capture how the driving environment influences driving decisions. Table 4. Driving scene attributes used as meta-data for LLM Q&A formulation. The table lists attributes grouped under Ego Vehicle and External Factors, indicating the label type (Single-Label or Multi-Label, with some requiring open-ended responses) and providing description of each attributes purpose in capturing different aspects of the driving scenario. Figure 8. collection of sample frames from 7 held-out videos used in our experiments form the Robusto-1 dataset. There is combination of rural and urban scenes that humans and VLMs view. This preliminary study focused only on showing humans and machines 7 videos, but the dataset is composed of 200 additional videos (See Supplement) that we are releasing to the public for further research and experiments. Figure 9. collection of all the RSA plots for the 3 different types of embeddings used in the paper (all-mpnet, paraphrase-mpnet, e5large). We observe that the pattern of results stays of our initial analysis stays the same with different levels of intensity. Figure 10. In this graph however we show how the RSA results would have looked like if we had just used one response rather than pooled (averaged) several observations per answer (also for all embeddings: all-mpnet, paraphrase-mpnet, e5-large). We find very similar trend to the pooled responses for the VLMs. Though it would appear that pooling answers shows greater level of convergence across VLMs. Figure 11. The Distance to the Median comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for all-mpnet. Figure 12. The Distance to the Median comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for paraphrase-mpnet. Figure 13. The Distance to the Median comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for e5-net. Figure 14. The PCA visualization of comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for all-mpnet. Figure 15. The PCA visualization of comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for paraphrase-mpnet. Figure 16. The PCA visualization of comparison of using pooled vs single embedding is used across all systems (in particular the VLM). The same pattern of results holds for e5-net."
        }
    ],
    "affiliations": [
        "Artificio",
        "Universidad de Ingeneria Tecnologia (UTEC)"
    ]
}