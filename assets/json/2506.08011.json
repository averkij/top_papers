{
    "paper_title": "Play to Generalize: Learning to Reason Through Game Play",
    "authors": [
        "Yunfei Xie",
        "Yinsong Ma",
        "Shiyi Lan",
        "Alan Yuille",
        "Junfei Xiao",
        "Chen Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs."
        },
        {
            "title": "Start",
            "content": "Play to Generalize: Learning to Reason Through Game Play Yunfei Xie1, Yinsong Ma2, Shiyi Lan3, Alan Yuille2, Junfei Xiao2*, Chen Wei1 1Rice University, Website Code & Model & Data 2Johns Hopkins University, 3NVIDIA https://yunfeixie233.github.io/ViGaL https://github.com/yunfeixie233/ViGaL 5 2 0 2 9 ] . [ 1 1 1 0 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose novel post-training paradigm, Visual Game Learning or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that after training 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g.. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base models performance on general visual benchmarks, challenge where specialist models often fall short. Our findings suggest new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs. Figure 1 Overview of ViGaL. Left: We propose novel post-training paradigm where MLLMs are tuned via RL to play arcade-style games such as Snake [32]. We demonstrate that gameplay post-training enables MLLMs to achieve out-of-domain generalization, enhancing their performance on downstream multimodal reasoning tasks requiring math, spatial and multi-discipline reasoning, without using in-domain math or multi-displine data during RL. Right: Our ViGaL (RL on game) achieves higher average accuracy increase than MM-Eureka [48] (RL on math) across three multimodal math benchmarks. This is notable because MM-Eureka uses RL on large-scale, curated math datasets, while ViGaL only uses game data. Details are in Tab 2. *Project Lead; Corresponding Author 1. Introduction Games, beyond their entertainment value, provide rich and diverse structured environments for developing and studying general reasoning and problem-solving abilities. Humans from early childhood acquire foundational cognitive skills through diverse game-like activities such as arranging objects, navigating spaces, and manipulating tools. These experiences foster essential building blocks of abstract thinking, including pattern recognition, spatial reasoning, and causal inference [8, 9]. In cognitive science, games are used as experimental platforms to reveal the inductive biases of the human mind [2, 3], such as planning depth in the game Four-in-a-Row [63], or the cognitive basis of tool use through the game Virtual Tools [4]. AI agents, too, benefited from games resembling aspects of human play. These environments encourage exploration, robustness to sparse rewards, and learning from multimodal inputs. For example, emergent tool use has been observed in agents trained via hide-and-seek [7], and Atari gameplay has been incorporated into training generalist agents [52]. By learning in these environments, AI systems develop robust and transferable reasoning capabilities. In this work, we specifically study the use of gameplay in the context of post-training multimodal large language models to effectively reason. Recent work has shown that posttraining with Reinforcement Learning (RL) can unlock reasoning behaviors from their base models [16, 49]. These RL-trained models are able to successfully think before they speak, generating internal chain-of-thought traces before outputting final answer. More importantly, growing evidence suggests that RL often generalizes more robustly to outof-distribution samples than supervised fine-tuning (SFT), another widely used post-training approach. For example, models trained with RL on CLEVR [31] generalize to more challenging Super-CLEVR benchmark [40], models trained on math problems extend reasoning to physics questions [48], and agents trained to navigate one environment successfully adapt to novel locations [14]. In each case, RL-trained models consistently outperform their SFT counterparts. While these results demonstrate promise of out-of-distribution generalization, they typically remain within single domain. The source and target tasks still belong to the same family, such as STEM questions [48] or spatial navigation [14]. In this work, we explore the potential of stronger form of out-of-domain generalization: transferring from one domain to an entirely different one, specifically, from gameplay to math questions. As illustrated in Fig. 1, we show that post-training 7B-parameter multimodal model, Qwen2.5-VL-7B [6], to play simple arcade-style games like Snake [32] (1) generalizes to nail outof-distribution unseen Atari games (Sec. 2.3), and (2) obtains enhanced out-of-domain capabilities on multimodal math benchmarks, e.g., MathVista [44], and multi-discipline question answering, e.g., MMMU [71]. Despite never seeing any worked solutions, equations, or diagrams during RL, our model outperforms not only large-scale industrial systems like GPT-4o [30], but also specialist models post-trained on in-domain datasets for reasoning (Tabs. 2 and 3). Moreover, our model obtains improvements on multimodal reasoning benchmarks without sacrificing its general visual capabilities, challenge for domain specialist models (Tab. 4). Interestingly, recent works challenge the necessity of ground-truth labels of in-domain questions for RL [54, 75], while our approach suggests that in-domain questions themselves may not be required. Why does it work? We hypothesize that gameplay encourages generalizable cognitive primitives or skills that are transferable to multimodal reasoning benchmarks such as spatial understanding and sequential planning. Unlike SFT or RL on math questions, which could reinforce memorization on training data [14, 73], gameplay training may incentivize more flexible representations and strategies. Supporting this view, our ablation studies reveal that both the prompt and reward designs play critical roles in enabling effective learning (Sec. 3.2). Figure 2 Post-training MLLMs to reason through RL with games. We propose post-training MLLMs via RL by playing visual games. We demonstrate this with two games: the classic arcade game Snake [32], and Rotation, self-designed task to investigate spatial reasoning. In each game, the model receives multimodal inputs and follows reasoning instructions, e.g., path planning in Snake, angle estimation in Rotation. It reflects to choose an action, outputs its chain-of-thoughts and decision, e.g., best/worst move or predicted angle, and receives reward. Through game playing, the model obtains reasoning abilities that transfer to downstream multimodal reasoning tasks such as math and multi-discipline question answering  (Fig. 3)  . We also find that different games emphasize distinct reasoning skills: Snake, 2D grid game where the player maneuvers the snake to avoid collisions and reach apples, promotes performance on multimodal math questions concerning 2D coordinates, while Rotation, puzzle to identify the rotating angle of 3D objects, performs better on angle and length related ones  (Fig. 5)  . Furthermore, training on both tasks together leads to consistently better performance on downstream multimodal reasoning benchmarks than training on either game alone, suggesting the scalable possibility of games (Tab. 2). These results suggest new post-training paradigm. Beyond collecting domain-specific data, we can also design scalable and controllable pre-text games that unlock desired reasoning behaviors transferable to downstream tasks. Synthetic game environments provide structured, rule-based reward signals with high controllability, enabling stable RL learning through difficulty scheduling. There are contemporary works studying the properties of reasoning models with game environments, taking advantage of its controllability [56], while we emphasize on its out-of-domain generalization capability. Scaling the data in these environments is also significantly easier than collecting human-annotated data. Altogether, these findings indicate promising paradigm of post-training with synthetic tasks such as games, reminiscent of the rise of self-supervised learning in vision and language [18, 27, 51], where pretraining on synthetic yet principled pre-text tasks leads to broad generalization. The following sections are organized as follows: In Sec. 2, we focus on game tasks, introducing how to post-train with RL on games and show improvements on unseen games to demonstrate out-of-distribution generalization. In Sec. 3, we focus on out-of-domain generalization evaluation, further showing training on visual games brings improvement to out-of-domain generalization on unseen visual reasoning tasks. In Sec. 4, we summarize recent developments of reinforcement learning and generalization in MLLMs and highlight how our ViGaL differs by leveraging simple games to achieve stronger generalization. 3 2. Reinforcement Learning on Visual Games In this section, we introduce ViGaL, novel post-training paradigm designed to enhance generalization capabilities. Sec. 2.1 describes the Snake and Rotation game environments used for training and evaluation. Sec. 2.2 outlines the reinforcement learning algorithm employed in our framework. Sec. 2.3 presents implementation details and provides comprehensive evaluation on both in-distribution and out-of-distribution games. 2.1. Game Environment As show in Fig. 2, under our ViGaL paradigm, the model is trained in game environment where it receives states from game environment, outputs next actions, and obtains rewards as feedback from the environment. Formally, each task, given an instruction 洧냪, can be formulated as partially observable Markov decision process (POMDP): (S, A, O, 洧녢, 洧녠, 풜), where is the set of possible environment states, is the set of observations available to the model, and represents actions model can do in this game environment. 洧녢 : is the state transition function, while 洧녠 is binary reward from the environment representing the correctness of action. Due to partial observability, the agent perceives only observations 洧녶 = 풜(洧). We design two distinct games, Snake and Rotation, to study the proposed paradigm  (Fig. 2)  , each targeting different MLLM capabilities. The Snake game is inspired by how competition can incentivize reasoning abilities in MLLMs [20]. It focuses on strategic decision-making by challenging the model to choose appropriate actions while competing with another snake. Meanwhile, the Rotation game draws inspiration from rotation-angle prediction as supervised pre-text task in self-supervised learning [26]. This game evaluates the MLLMs visual perception capabilities, particularly in understanding complex 3D spatial transformations. Through these complementary games, we can systematically explore and improve reasoning and perception, two distinct and fundamental aspects of MLLM abilities. Snake. We set up dual-snake game based on SnakeBench [32]. Each model controls one snake independently. The objective of each snake is to reach apples and score points and outcompete the other. At time 洧노, the environment state 洧멇롐 contains the coordinates of the snakes (洧논洧노 洧멇롐, 洧녽洧노 洧멇롐) for snake 洧녰 {1, 2}, the coordinates of the apple (洧논洧노 chosen by each snake. These elements are positioned on 10 10 game board. In each round 洧노, each snake selects its next move 洧냢洧노 洧녰 from {up, down, left, right}. snake dies if it collides with itself, the other snake, or the boundary of the board. If one snake dies, the other snake wins. If both snakes die simultaneously, the snake with the higher score wins. Unlike SnakeBench [32] which uses only text to represent the game state, we use both an image of the game board and text descriptions as observation 洧녶洧노 = 풜(洧멇롐) for enhanced representation. 洧녩), and the last moves 洧냢洧노 洧녩, 洧녽洧노 洧녰 Rotation game. We design rotation game to study the spatial reasoning capabilities of MLLMs. We present the model with two views of the same 3D object: an initial view 洧냪init and rotated view 洧냪rot. The rotated view is created by rotating the 3D object from its initial orientation by either 90 or 180 around the 洧녾-axis, which points toward the viewer. The task is to determine which degree, 90 or 180, is applied to transform the object from the initial orientation to the rotated orientation. To guide the models reasoning, we provide an in-context example consisting of another image pair with known rotation angle. Similar to Snake game, we provide observations with both images and text. 2.2. Rule-Based Reinforcement Learning We apply rule-based RL to directly post-train MLLMs for visual games, without relying on supervised learning as warm up. The algorithm is described as follows: Reward design. Instead of relying on outcomeor process-based reward models, following previous approaches [29, 76], we use simple rule-based reward function to avoid reward hacking [25] and help the model learn how to play the games effectively. This reward function has two components: an accuracy reward and format reward. The total reward 洧 is computed as the sum of an accuracy reward and format reward 洧 = 洧르ccuracy + 洧 format. The accuracy reward 洧르ccuracy is 1 if the answer is correct, and 0 otherwise. The format reward 洧 format = 0.1 if the response is correctly formatted, and 洧 洧 desired format is: format checks whether the response follows task-specific format: format = 0 otherwise. For Snake game, the <think>...</think><best_answer>...</best_answer><worst_answer>...</worst_answer> . As suggested by the format, we encourage the model to predict both positive move that moves toward the apple and negative move that leads to failure. This reward encourages contrastive decision-making, which not only improves the models gameplay abilities but also boosts downstream reasoning performance on visual math benchmarks. We ablate the effect in Tab. 5b. For the rotation task, the required format is simply <think>...</think><answer>...</answer>. Advantage estimation and policy update. We employ REINFORCE Leave-One-Out (RLOO) algorithm [1, 35] in our RL training phase. We do not incorporate KL divergence regularization in our implementation, following the technique proposed in Group Policy Gradient [15]. Without KL constraints that limit how much the policy can change, the model may explore the solution space more freely, potentially discovering better reasoning strategies. This design choice allows our model to adapt more flexibly during our RL phase. Text prompt design. While the model takes images as input to understand the current state of the game, we design structural text prompt framework to also provide game guidance. Our game prompts consist of two parts: (1) game settings and (2) reasoning instructions. (1) To help the model understand the game environment, we describe the background, current game state, rules, goals, action space, etc. in text besides the input image. (2) In the reasoning instruction part, we provide specific thinking guidance since games can be approached with various thinking chains. To encourage broader thinking, we implement different types of reasoning instructions to guide decision-making process. Specifically, we used GPT-4o [30] to synthesize mathematical thinking instructions for Snake, such as finding the nearest apple by calculating Manhattan distances, and spatial thinking instructions for Rotation, for example, identify major symmetry axes in the original image. As shown in Fig. 4a, these reasoning instructions help models to lengthen responses or chains of internal thinking traces. With reasoning instructions for games, the obtained reasoning abilities generalize to downstream evaluation on visual math questions (Tab. 5a). The details of text prompt design, including the reasoning instructions used, are in Appendix Sec. A.2. Controlling game difficulty. Thanks to using the synthetic game data engine, we can flexibly generate large-scale training data with precisely controlled difficulty levels. This completely eliminates the need for extensive data filtering strategies used in previous rule-based RL work training on domain-specific data like math [5, 48], where difficulty is hard to define and filtering can significantly reduce dataset size. In Snake, we define difficulty based on snake length, where longer snakes create more complex game situations and more constrained movement options, closely aligning with how humans perceive difficulty when playing Snake ourselves. In Rotation, difficulty is determined by the rotation angle between two images, where smaller angle differences present greater perceptual challenges. Based on empirical results, we established optimal difficulty parameters for RL training, which we ablate in Tab. 5c. This controlled progression of difficulty enables more effective learning trajectories. Wins (/10) Model Acc. (%) Game ViGaL Qwen2.5-VL-7B Model ViGaL vs. Qwen2.5-VL-7B Qwen2.5-VL-72B Llama-4-Maverick Gemini-2.5-Pro Claude-3.7-Sonnet GPT-4o o4-mini 9 7 7 8 6 8 6 ViGaL Qwen2.5-VL-7B Qwen2.5-VL-72B Llama-4-Maverick Gemini-2.5-Pro Claude-3.7-Sonnet GPT-4o o4-mini 71.9 47.4 52.1 66.2 51.0 65.6 61.5 70. Space Invaders Ms. Pacman Seaquest Alien Frogger Breakout Pong 280.0 1370.0 80.0 540.0 7.0 0.0 -26.0 85.0 670.0 60.0 450.0 5.0 9.0 -26.0 Cumulative Reward 2251.0 1253.0 (a) Snake game. (b) Rotation game. (c) Atari game. Table 1 Game Performance. (a) In Snake, ViGaL consistently achieves the highest win rate(6-9 wins out of 10 matches), further surpassing larger proprietary models. (b) In Rotation, ViGaL demonstrates overall best performance with the best accuracy compared to leading commercial language models. (c) In the Atari Games, ViGaL training on Snake and Rotation games shows remarkably impressive zero-shot generalization to unseen Atari games, achieving nearly double the cumulative reward compared to Qwen2.5-VL-7B. 2.3. Implementation and Evaluation on Games Implementation details. We employ Qwen2.5-VL-7B-Instruct [6] as our base model. We follow DeepSeek-R1 [16], using combination of rule-based format rewards and accuracy rewards, with RLOO [1, 35] as the core RL algorithm. We implement our training within multimodal input RL framework based on OpenRLHF [28]. For hyperparameters, we adopt the default settings from MM-Eureka [48], including global batch size of 128, rollout batch size of 128, rollout temperature of 1.0, and learning rate of 1洧6. Training uses 6 A100-80G GPUs. Game training data. We build custom game environments to collect training data for our experiments. For Snake game, we leverage SnakeBench [32] as our data engine. This environment allows us to input actions to control snake movements and generate gameplay trajectories. To create meaningful gameplay data, we implement policy network based on Proximal Policy Optimization (PPO) [53] with linear output layer. This network continuously generates actions for two snakes that attempt to collect apples while avoiding death, enabling automatic capture of diverse gameplay trajectories for RL training. For the Rotation game, we utilize Hunyuan3D [59], model that generates 3D meshes based on images or text instructions. We render each mesh into 2D images from different orientations, creating image pairs with associated rotation angles as ground truth labels for RL training data. Our comprehensive data generation pipeline enables producing training samples at any desired scale with fully customized settings. For our experiments, we synthesize 36K samples for Snake and 36K samples for Rotation, which have shown to be sufficient for convergence. Further details of data synthesis are in Appendix Sec. A.1. Competing with leading models on Snake and Rotation. To evaluate the game capabilities of ViGaL models, we initialize these environments in diverse states that were not seen during training. For Snake in Tab. 1a, we randomly initialize the game 10 times and have two models compete against each other to directly measure the win count of each model. For Rotation in Tab. 1b, we measure the rotation angle prediction accuracy on comprehensive validation set consisting of 3D object meshes unseen during training. Our 7B-parameter model consistently outperforms proprietary models in both Snake and Rotation games. These results confirm that RL effectively unlocks the ability of small 7B model to excel in visual games that require environmental understanding, reasoning, planning, and interactive decision-making. 6 (a) Out-of-distribution games. (b) Out-of-domain tasks. Figure 3 Samples from our generalization reasoning benchmarks. We evaluate the proposed ViGaL with two types of generalization: (a) out-of-distribution generalization, where models trained on our visual games are tested on unseen Atari games [66]; and (b) out-of-domain generalization, where models trained only on game tasks are evaluated on diverse multimodal reasoning tasks including mathematical reasoning, geometric problem-solving, 3D understanding on CLEVR+ and multi-discipline reasoning on MMMU series. Out-of-distribution generalization to Atari games. To evaluate out-of-distribution generalization, we test ViGaL on Atari-GPT [66], benchmark for evaluating MLLMs as decision-making agents in Atari video games such as in Fig. 6. The benchmark consists of seven different Atari games, with detailed settings in Appendix Sec. B.1. We follow most settings and prompts from Atari-GPT, with small modification providing explicit JSON output to ensure format correctness for all models. Following Atari-GPT [66], we report cumulative reward over 1K steps as the evaluation metric, where higher rewards indicate better performance. As shown in Tab. 1c, ViGaL demonstrates significant cumulative reward improvement on Atari games despite being trained only on Snake and Rotation games. This is particularly notable because Atari games differ substantially from our training games in both visual appearance and gameplay strategies. These results suggest that our rule-based RL training approach enables strong out-of-distribution generalization to entirely unseen game environments. 3. Visual Reasoning Generalization Evaluation collection. To obtain clearer picture of the various facets of MLLM performance, we follow prior studies [39, 61] and systematically and carefully divide existing benchmarks into two broad groups: (i) reasoning-oriented benchmarks, which require multi-step or mathematical reasoning to solve the problems, and (ii) general-purpose perception benchmarks, which primarily assess broad visual understanding and perception abilities. For reasoning-oriented benchmarks, we comprehensively evaluate the visual reasoning generalization capabilities of RL through gaming on diverse collection of tasks that specifically demand advanced visual reasoning skills, including math-focused tasks like Math and Geometry, and other comprehensive reasoning benchmarks beyond math, like CLEVR+ and Multi-Discipline. Fig. 3b illustrates specific examples from each benchmark. Math evaluates multimodal math reasoning with widely-used datasets: MathVista (testmini) [44], MathVerse (testmini) [74], and MathVision (test) [65]. MathVista offers diverse problems spanning VQA, logic, algebra, and geometry; MathVerse emphasizes algebraic and geometric image comprehension; MathVision tests abstract visual reasoning. Geometry evaluates structural interpretation skills across mathematical diagrams, medical images, charts, and architectural layouts. It uses datasets GeoMath (Geo170K [24], 7 Math360K [55]) and Geometry3K [45], featuring both choice and non-choice questions. Following Reason-RFT [57], we test with 820 GeoMath and 800 Geometry3K samples. CLEVR+ evaluates the integration of mathematical and spatial reasoning skills through challenging arithmetic problems in complex 3D block-based scenes, including sub-tasks on CLEVR-Math [41] and Super-CLEVR [40]. Following Reason-RFT [57], we use 1K test samples from each of CLEVR-Math and Super-CLEVR. Multi-Discipline evaluates college-level expert knowledge across six disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. We follow the evaluation setting of MMMU [71] val set (900 questions) and MMMU-Pro [72] overall score (average of standard 10-option and vision-only settings). For general-purpose perception benchmarks, we systematically evaluate comprehensive visual capabilities. Following previous work, these benchmarks are categorized into three distinct types: General, Vision-Centric, and OCR & chart. Specifically, for General, we evaluate MuirBench [64] for multi-image understanding and CRPE [33] for relation understanding. For Vision-Centric benchmarks, we assess MMVP [62], RealWorldQA [68], MMStar [12], MME [22], and BLINK [23] to thoroughly evaluate perception, real-world understanding, and multi-modal capabilities. For OCR & Chart understanding, we specifically use AI2D [34] for diagram understanding, SEED-Bench-2-Plus [37] for text-rich visual comprehension, DocVQA [47] for document understanding, and OCRBench [43] for comprehensive OCR evaluation. 3.1. Main Results Zero-shot generalization from gameplay to multimodal reasoning. Our approach consistently shows remarkable generalization capabilities on mathematical and other reasoning tasks, despite having no direct exposure to in-domain training data during RL post-training. As shown in Tab. 2, our method notably outperforms models specifically RL-trained on mathematical tasks. For instance, ViGaL Snake + Rotation achieves 0.5% higher accuracy than MM-Eureka-Qwen7B [48] on Math and 28.7% on Geometry, even though MM-Eureka-Qwen-7B was explicitly trained on high-quality mathematical and geometry datasets. This strong generalization extends beyond mathematics. Tab. 3 shows that ViGaL Snake + Rotation outperforms R1-OneVision-7B [70] by 5.4% on average across MMMU series benchmarks, which test multi-disciplinary reasoning. This is particularly notable since R1-OneVision-7B was trained on carefully curated comprehensive dataset spanning multiple subjects. These empirical results suggest that gameplay-based post-training develops fundamental reasoning capabilities that transfer more effectively than direct RL training on diverse taskspecific datasets. Moreover, the gameplay environment appears to encourage general problemsolving strategies that consistently generalize well to out-of-domain tasks. Blending multiple games enhances generalization. As shown in Tab. 2, post-training on Snake achieves the best performance on the CLEVR+ benchmark, while training on Rotation yields stronger results on geometry reasoning. Their comparative strengths are further illustrated in Fig. 5. Notably, training the model on both Snake and Rotation games together enables it to learn complementary skills from each environment, improving the overall benchmark average to 63.1%. These findings suggest that combining diverse game environments can drive meaningful performance gains. This demonstrates the potential of Visual Gaming Learning as promising training paradigm for enhancing generalizable reasoning, without requiring large-scale domain-specific data. Expanding the diversity of games during training consistently scales performance across wide range of visual reasoning tasks. 8 Model Avg. Avg. MathVista MathVerse MathVision Avg. GeoMath Geo3K Avg. Math Geometry Proprietary Model GPT-4o [30] Gemini-2.0-Flash [58] 47.5 48.1 55.4 56.4 61.4 73. 50.2 54.6 30.4 41.3 46.8 54.4 50.2 55.3 General Multimodal Language Model InternVL2.5-8B [13] Llava-OV-7B [36] Qwen2.5-VL-7B [6] 48.2 41.2 46.3 47.7 64.4 63.2 68.0 39.5 26.2 49. 19.7 26.0 55.2 60.7 44.8 Multimodal Reasoning Model Post-Trained on Qwen2.5-VL-7B 40.9 46.8 R1-Onevision-7B [70] 40.9 42.7 R1-VL-7B [11] MM-Eureka-Qwen-7B [48] 39.3 50.1 46.5 38.1 Reason-RFT-Zero-7B [57] 51.3 48.7 VLAA-Thinker-7B [10] 52.1 47.8 OpenVLThinker-7B [17] 51.6 49.4 ViGaL Snake 52.8 49.3 ViGaL Rotation 53.9 50.6 ViGaL Snake + Rotation 64.1 63.5 73.0 60.7 68.0 70.2 70.7 71.2 71.9 46.4 40.0 50.3 35.3 51.7 47.9 51.1 50.4 52. 29.9 24.7 26.9 18.3 26.4 25.3 26.5 26.3 27.5 35.0 39.0 28.4 54.9 53.9 56.4 55.0 57.9 57.1 63.0 77.6 44.0 45.4 42.0 53.1 55.0 51.1 49.2 49.9 51.7 51.0 43.5 53.5 47.3 43.7 45. 24.5 36.1 3.8 54.8 56.6 63.5 60.0 64.1 63.3 Table 2 Main results on multimodal mathematical benchmarks. We primarily compare with multimodal reasoning models post-trained on math data based on Qwen2.5-VL-7B [6]. Results from reasoning models post-trained with mathematical data are de-emphasized, while our ViGaL models are exclusively post-trained using visual games. Best scores of post-trained models in each Avg. column are highlighted in bold. Preserving general visual capabilities while reasoning enhancement. To comprehensively examine whether generalization on reasoning tasks leads to degradation in general visual capabilities, we evaluate ViGaL Snake + Rotation on broader set of MLLM benchmarks. As shown in Tab. 4, compared to Qwen2.5-VL-7B prior to RL tuning, our model maintains comparable general visual performance while achieving stronger math reasoning results. In contrast, other models that improve math performance through RL post-training often exhibit substantial drops in general visual capabilities. These results demonstrate that our gameplaybased approach enables math generalization without compromising other visual abilities. 3.2. Ablation Study We ablate key design choices in the Snake environment, evaluate each variant on downstream benchmarks, and report the results in Tab. 5 and Fig. 4. The corresponding ablation for the Rotation environment is provided in Appendix Sec. B.2. Reasoning instructions in the text prompt help. We use reasoning instructions, such as finding the nearest apple by calculating Manhattan distances, in the text prompts to guide the model thinking chains. The complete text prompts are in Appendix Sec. A.2. In Tab. 5a, we demonstrate that reasoning instructions brings significant improvement of 1.9%, from 59.5% to 61.4%, for Snake in average accuracy over the three out-of-domain benchmarks. Fig. 4a shows that integrating reasoning instructions during training significantly increases response length. These results highlight the effectiveness of adding reasoning instructions in the text prompt, helping RL training and generalization to downstream benchmarks. 9 Model CLEVR+ Multi-Discipline Avg. Avg. CLEVR-M S-CLEVR Avg. MMMUval MMMU-Prooverall Avg. Proprietary Model GPT-4o [30] Gemini-2.0-Flash [58] 55.9 51.2 46.3 68.1 64.9 34.3 27.6 60.5 General Multimodal Language Model InternVL2.5-8B [13] Llava-OV-7B [36] Qwen2.5-VL-7B [6] 54.8 64.4 42.9 49.4 50.3 54.9 93.5 69.7 74.6 35.3 29.1 35.2 45.2 36.5 45. 69.1 71.9 56.0 48.8 54.3 Multimodal Reasoning Model Post-Trained on Qwen2.5-VL-7B 53.7 65.1 R1-Onevision-7B [70] 53.9 68.0 R1-VL-7B [11] MM-Eureka-Qwen-7B [48] 62.8 79.3 58.6 76.2 Reason-RFT-Zero-7B [57] 61.7 83.4 VLAA-Thinker-7B [10] OpenVLThinker-7B [17] 60.4 82.4 64.4 82.6 ViGaL Snake 63.3 80.7 ViGaL Rotation 64.7 81.7 ViGaL Snake + Rotation 75.5 87.4 98.4 99.4 94.7 93.8 92.6 93.0 91.9 54.7 48.6 60.1 53.0 72.1 71.0 72.6 68.3 71. 42.3 39.7 46.4 40.9 40.1 38.5 46.2 45.9 47.7 51.9 50.0 55.8 51.2 48.2 54.8 55.8 54.1 58.0 51.9 34.3 24.1 37.0 32.6 29.4 36.9 30.6 31.9 22.1 36.6 37.7 37.4 Table 3 Main results on multimodal spatial and multi-discipline reasoning benchmarks. We extend our evaluation to non-mathematical reasoning tasks, comparing with multimodal reasoning models post-trained on domain-specific data based on Qwen2.5-VL-7B [6]. CLEVR-M denotes CLEVR-Math [41], and S-CLEVR stands for Super-CLEVR [40]. Results from reasoning models post-trained with corresponding in-domain data are de-emphasized, while our ViGaL models remain exclusively post-trained using visual games. Best scores of post-trained models in each Avg. column are highlighted in bold. Reward design of pre-text game matters for downstream tasks. We show that reward design of RL for games plays crucial role for the downstream tasks. As shown in Tab. 5b, we first ask the model to predict only the best next move, defined as the action that moves toward the closest apple while avoiding death. In our improved reward design, we task the model with simultaneously predicting both the best and worst next moves, where the worst move leads directly to losing the game. As shown in Fig. 4b, predicting both best and worse moves improves the reasoning length, implying better thinking abilities. More importantly, it leads to improvements across all downstream tasks, bringing an average increase of 1.8%. These results suggest that proper reward design in pre-text game can improve not only gameplay capabilities but also generalization to downstream tasks. Furthermore, inspired by several prior works that improve model performance without labeled rewards [75] or with random labels [54], we also provide random reward ablation, where we still ask the model to predict both best and worst moves but use random moves as the labels. We report the results in the last row in Tab. 5b. In our gameplay setting, RL with random labels reports 49.4% on averagne and does no provide significant gains over the base model, different from the conclusions in prior works [54]. Potential explanations lie in the difference in data domains and base models, where other works applied random labels to text-only mathematical data while our work applies random labels to visual game data. 10 Model Avg. Avg. Avg. General MuirBench CRPErel. Avg. MMVP Vision-Centric RealWorldQA MMStar BLINKval MMEp Avg. OCR & Chart SEEDBench-2+ DocVQA val OCRBench AI2D w. M. Proprietary Model GPT-4o [30] 74.8 72. 68.0 76.6 69.4 75.4 64. 68.0 1614 82.6 84.6 72.0 91.1 736 Qwen2.5-VL-7B [6] 72.4 68.0 59.6 76.4 65.8 74.3 68. 63.9 56.4 1698 83.3 83.9 70.4 95.7 Multimodal Reasoning Model Post-Trained on Qwen2.5-VL-7B General Multimodal Language Model 66.8 R1-Onevision-7B [70] R1-VL-7B [11] 67.4 63.3 MM-Eureka-Qwen-7B [48] 71.8 68.9 68.4 66.9 Reason-RFT-Zero-7B [57] 69.7 65.9 VLAA-Thinker-7B [10] 64.3 OpenVLThinker-7B [17] 72.2 68.6 ViGaL Snake + Rotation 46.3 54.1 61.1 58.5 57.1 52.8 60. 87.3 72.4 76.7 75.2 74.6 75.8 76.7 56.5 59.6 65.1 58.5 62.6 50.4 65.7 61.3 70.3 74.3 58.0 71.6 32.3 74.6 58.0 61.4 66.1 65.3 65.4 60.2 67.3 57.8 55.6 65.9 59.1 60.4 59.1 65.4 48.7 51.0 54.0 51.6 53.0 49.9 55. 1504 1657 79.2 81.7 1626 81.5 84.3 1653 79.8 83.3 1593 80.6 83.4 1513 1685 82.2 84.8 66.4 68.2 68.0 67.4 69.1 89.4 92.0 88.1 90.9 92.7 81.0 87.0 82.0 84.5 86. Table 4 Main results on multimodal language benchmarks targeting more general and comprehensive visual ability. We compare with models post-trained on Qwen2.5-VL-7B [6]. Best category averages are highlighted in bold. Note that MMEp is excluded from vision-centric category average accuracy due to scale differences. Controlling game difficulty to steadily improve reasoning. Gameplay for RL post-training offers the unique opportunity to easily control the difficulty of the task itself. We present an ablation study on the importance of difficulty control. We define difficulty based on snake length, where states with longer snakes are considered more difficult. For our controlled difficulty approach, we collect training data using states where snake length falls within moderate range between 1 and 5. As shown in Fig. 4c, models trained with the difficulty control strategy maintain relatively stable trends of increasingly longer responses throughout training. In contrast, models without difficulty control, which contain hard samples, experience struggle in gameplay. As shown in Tab. 5c, the approach with difficulty control achieves 61.4% overall accuracy compared to 60.6% without difficulty control. These findings suggest that our game engine can easily generate data with suitable difficulty to stabilize RL training and can help prevent model collapse during optimization. RL on games shows data scalability. Thanks to using game engine, we can generate data at any scale with high flexibility. To show data scalability on RL of visual games, we conduct experiments using 16k and 32k snake game samples, respectively. As in Tab. 5d, scaling data from 16k to 32k brings performance improvement of 1.3% on average across all domains. This suggests the potential of the proposed ViGaL paradigm to improve downstream performance by easily scaling training data, which contrasts with the data scaling challenges of domain-specific human annotated data, requiring extensive manual effort. Both text and vision contribute to better visual reasoning. To isolate the contributions of text and vision modalities, we conduct an ablation study with text-only setting. In this setup, we represent game statesincluding snake positions, apple locations, and boundary constraintsusing only textual descriptions during RL training. The model trained with textonly inputs on the Snake game demonstrates substantial improvements across all multimodal benchmarks, with average performance increasing from 49.1% to 59.6%. Incorporating visual inputs yields an additional 1.8% performance gain. These results demonstrate that multimodal RL enhances visual reasoning capabilities, with complementary contributions from both text and vision modalities. Fig. 4d shows that including the vision modality leads to increased response length during RL training, suggesting more detailed reasoning processes. RL generalizes better than SFT from games to math. To evaluate the out-of-domain generalization of ViGaL, we compare it with supervised fine-tuning (SFT) using identical visual game data. Tab. 5f shows that SFT with Snake game data degrades the base models performance on both mathematical reasoning and geometry tasks by notable 9.7% and 12.7%, respectively. (a) Text prompt design. (b) Reward design. prompt Avg. Math CLEVR+ Geo. reward Avg. Math CLEVR+ Geo. base model w/o reasoning instr. w/ reasoning instr. 49.1 59.5 62.3 47.7 48.0 49.4 54.9 80.4 82.6 44.8 50.1 55.0 base model best moves best & worst moves w/ random label 49.1 59.6 62.3 49.4 47.7 48.2 49.4 47.5 54.9 80.4 82.6 55.4 44.8 50.2 55.0 47.5 (c) Difficulty control. (d) Data scalability. difficulty control Avg. Math CLEVR+ Geo. training samples Avg. Math CLEVR+ Geo. base model w/o difficulty control w/ difficulty control 49.1 60.6 62. 47.7 48.8 49.4 54.9 81.4 82.6 44.8 51.8 55.0 base model 16K 36K 49.1 60.1 62.3 47.7 48.9 49. 54.9 81.2 82.6 44.8 50.3 55.0 (e) Input modality. (f) SFT vs. RL. input modality Avg. Math CLEVR+ Geo. post-training Avg. Math CLEVR+ Geo. base model text vision & text 49.1 59.6 62.3 47.7 48.5 49.4 54.9 80.1 82. 44.8 50.3 55.0 base model SFT RL 49.1 47.2 62.3 47.7 38.0 49.4 54.9 71.5 82.6 44.8 32.1 55. Table 5 Ablation study. We ablate different aspects of ViGaL with Snake environment and evaluate on downstream benchmarks. The similar evaluation with Rotation is in Sec. B.2 in the Appendix. Each benchmark consists of several subtasks (Tab. 2 and Tab. 3), and we report their averages. The base model is Qwen2.5-VL-7B, whose results are in gray. The default settings in Tab. 2 and Tab. 3 are highlighted in blue . While SFT produces modest improvements on CLEVR+, these gains are substantially smaller than those achieved by RL. Overall, RL improves performance by 12.3%, whereas SFT decreases performance by 1.9%. This stark contrast demonstrates that RL better preserves and extends the models reasoning capabilities to new domains. Different games benefit distinct math subfields. We hypothesize that gameplay fosters fundamental skills like spatial modeling and sequential planning that can transfer to visual math questions. Different games may enhance distinct reasoning abilities. To investigate this hypothesis, we analyze accuracy differences across MathVerse [74] subcategories between ViGaL models trained with Snake or Rotation, as shown in Fig.5. Training on the Snake game significantly improves performance on the Expressions and Coordinates subcategories. Both tasks involve algebraic functions and coordinate-level interpretations of graphical representations, closely aligning with the spatial reasoning in Snakes 2D-grid environment. In contrast, training on Rotation notably enhances performance on questions about angles and lengths, consistent with Rotations requirement to reason about rotational angles of 3D objects. These results suggest that different games obtains specialized reasoning skills that correspond to their unique gameplay mechanics. Furthermore, joint training on both games leads to improvements across all reasoning categories (see Appendix Sec.B.4). We also include qualitative analyses illustrating improvements in mathematical reasoning after RL in Appendix Sec. C. 4. Related Work Reinforcement Learning in MLLMs. Reinforcement Learning (RL) increasingly enhances reasoning in Large Language Models (LLMs) beyond Supervised Fine-Tuning (SFT). Text-only models like DeepSeek-R1 [16] show RLs efficacy, especially with rule-based rewards, for complex reasoning. This paradigm is now actively being extended to Multimodal LLMs (MLLMs). (a) Text prompt. (b) Reward design. (c) Difficulty control. (d) Input modality. Figure 4 Ablation study on the impact of design choices on response length. Solid orange lines represent the full configuration, while dashed grey lines indicate the ablated counterpart. The graphs demonstrate how (a) incorporating reasoning instructions, (b) designing rewards to consider both best and worst moves, (c) implementing difficulty control, and (d) utilizing multimodal inputs all contribute to increased response length as training progresses, implying better reasoning abilities. Figure 5 Snake vs. Rotation: Subfield differences on MathVerse. Positive values indicate better results from ViGaL Snake, and negative values measure how much ViGaL Rotation performs better. Interestingly, Snake enhances most on Expressions and Coordinates, tasks aligned with Snakes 2D grid. Rotation improves angle and length reasoning, reflecting its focus on 3D object rotations. Recent MLLM research explores RL for improved visual reasoning, drawing inspiration from LLM successes. For instance, various works [11, 29, 50] investigate multi-stage training, trace supervision, or rule-based RL for specific visual subdomains like geometry and counting. Others focus on different RL algorithms like Process Reward Models (PRMs) [46, 69], often moving beyond SFT-based Chain-of-Thought generation [19, 60]. Many efforts are moving towards simpler rule-based rewards [29, 76] over complex reward models prone to hacking [21]. Unlike approaches that train on costly, domain-specific reasoning datasets, our ViGaL paradigm contributes by extending rule-based RL to simple, synthetic visual games, demonstrating that these can serve as scalable, cost-effective pre-text tasks. Generalization in MLLMs. Achieving robust generalization to novel tasks, distributions, and domains is central goal in the development of MLLMs. RL has shown promise for better out-of-distribution (OOD) generalization compared to SFT [11, 48], and developing multi-step reasoning like CoT [67] is itself form of generalization. Generalization is often pursued by training on large, diverse instruction-following datasets [13, 38, 42] or by explicitly training general reasoning capabilities [29, 70]. While these methods advance OOD generalization, they typically operate within the same broad domain of complex visual reasoning as the training data. Our ViGaL paradigm, however, investigates stronger form of out-of-domain generalization. We show fundamental skills learned from simple synthetic games transfer zero-shot to enhance performance on entirely different, complex domains like visual mathematics and multi-displine questions, without any exposure to corresponding domain-specific data. 13 5. Conclusion We introduced Visual Game Learning (ViGaL), novel post-training paradigm where MLLMs learn transferable reasoning by playing simple arcade-style games. Our core finding is that RL on games like Snake and Rotation, without any in-domain math data, significantly boosts MLLM performance on mathematical and multi-discipline benchmarks, surpassing specialized models and even large proprietary systems. Ablations confirm the importance of game design, reward structure, and that RL outperforms SFT, while distinct games unlock different skills. We posit that games instill fundamental cognitive primitives, suggesting new avenue for using scalable, controllable synthetic games as powerful pre-text tasks to unlock generalizable reasoning. This work opens doors to exploring broader range of game-based learning for robust AI. Future directions include investigating the synergistic effects among diverse games and developing deeper understanding of the transfer mechanisms. Acknowledgment We thank Haoqin Tu and Yuxuan Cheng for their valuable feedback on this manuscript. We also thank the authors of the paper Cambrian-1 [61] for the template reference."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gall칠, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet 칖st칲n, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In ACL, 2024. [2] Fahad Alhasoun and Sarah Alneghiemish. Probabilistic programming bots in intuitive physics game play. In AAAI, 2021. [3] Kelsey Allen, Franziska Br칛ndle, Matthew Botvinick, Judith E. Fan, Samuel J. Gershman, Alison Gopnik, et al. Using games to understand the mind. Nature Human Behaviour, 2024. [4] Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. PNAS, 2020. [5] Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning-oriented reinforcement learning. arXiv:2504.03380, 2025. [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2023. [7] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In ICLR, 2019. [8] Lara Bertram. Digital learning games for mathematics and computer science education: The need for preregistered rcts, standardized methodology, and advanced technology. Frontiers in Psychology, 2020. [9] Franziska Br칛ndle, Kelsey Allen, Josh Tenenbaum, and Eric Schulz. Using games to understand intelligence. In CogSci, 2021. [10] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, et al. SFT or RL? an early investigation into training R1-Like reasoning large vision-language models. arXiv:2504.11468, 2025. 14 [11] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-V: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Dee p-Agent/R1-V, 2025. [12] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. [14] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, et al. SFT memorizes, RL generalizes: comparative study of foundation model post-training. arXiv:2501.17161, 2025. [15] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. GPG: simple and strong reinforcement learning baseline for model reasoning. arXiv:2504.02546, 2025. [16] DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. [17] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. OpenVLThinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv:2503.17352, 2025. [18] Carl Doersch, Abhinav Gupta, and Alexei Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. [19] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-V: Exploring long-chain visual reasoning with multimodal large language models. arXiv:2411.14432, 2024. [20] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multi-agent debate. In ICML, 2023. [21] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv:2312.09244, 2023. [22] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. [23] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [24] Jiahui Gao, Renjie Pi, Jiacheng Ye, Wanjun Zhong, Yufei Wang, et al. G-LLaVA: Solving geometric problem with multi-modal large language model. arXiv:2312.11370, 2023. Jipeng Zhang, [25] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. arXiv:2210.10760, 2022. 15 [26] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv:1803.07728, 2018. [27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. [28] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. OpenRLHF: An easy-to-use, scalable and high-performance RLHF framework. arXiv:2405.11143, 2024. [29] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, et al. Vision-R1: Incentivizing reasoning capability in multimodal large language models. arXiv:2503.06749, 2025. [30] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, et al. GPT-4o system card. arXiv:2410.21276, 2024. [31] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. [32] Greg Kamradt. Snake Bench: Competitive snake game simulation with LLMs. https: //github.com/gkamradt/SnakeBench, 2025. [33] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. [34] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. [35] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get baseline for free! In ICLR Workshop on Deep Reinforcement Learning Meets Structured Prediction, 2019. [36] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, et al. LLaVA-OneVision: Easy visual task transfer. arXiv:2408.03326, 2024. [37] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [38] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, et al. LLaVA-NeXT-Interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv:2407.07895, 2024. [39] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, et al. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024. [40] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L. Yuille. Super-CLEVR: virtual benchmark to diagnose domain robustness in visual reasoning. In CVPR, 2023. [41] Adam Dahlgren Lindstr칬m and Savitha Sam Abraham. Clevr-Math: dataset for compositional language, visual and mathematical reasoning. In IJCLR, 2022. [42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. https://llav a-vl.github.io/blog/2024-01-30-llava-next/, 2024. 16 [43] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: On the hidden mystery of ocr in large multimodal models. arXiv:2305.07895, 2023. [44] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, et al. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv:2310.02255, 2024. [45] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and SongChun Zhu. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv:2105.04165, 2021. [46] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, et al. URSA: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv:2501.04686, 2025. [47] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. [48] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, et al. MM-Eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv:2503.07365, 2025. [49] OpenAI. Introducing OpenAI o1. https://openai.com/o1/, 2024. [50] YingZhe Peng, Gongrui Zhang, Xin Geng, and Xu Yang. LMM-R1. https://github.c om/TideDra/lmm-r1, 2025. [51] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI, 2018. [52] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv:2205.06175, 2022. [53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. [54] Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in rlvr. https://rethink-rlvr.notion.site/Spurious-Rewards-Rethi nking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f, 2025. Notion Blog. [55] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-LLaVA: Bootstrapping mathematical reasoning for multimodal large language models. arXiv:2406.17294, 2024. [56] Parshin Shojaee*, Iman Mirzadeh*, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity, 2025. [57] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-RFT: Reinforcement fine-tuning for visual reasoning. arXiv:2503.20752, 2025. 17 [58] Gemini Team. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [59] Tencent Hunyuan3D Team. Hunyuan3D 2.0: Scaling diffusion models for high-resolution textured 3d assets generation. arXiv:2501.12202, 2025. [60] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, et al. LLaMAV-o1: Rethinking step-by-step visual reasoning in LLMs. arXiv:2501.06186, 2025. [61] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [62] Shengbang Tong, Zhuang Liu, Yuexiang Zhu, Xingjian Chen, Ruoyu Zhang, Bo Li, et al. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024. [63] Bas Van Opheusden, Ionatan Kuperwajs, Gianni Galbiati, Zahy Bnaya, Yunqi Li, and Wei Ji Ma. Expertise increases planning depth in human gameplay. Nature, 2023. [64] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. [65] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with Math-Vision dataset. In NeurIPS, 2024. [66] Nicholas R. Waytowich, Devin White, M.D. Sunbeam, and Vinicius G. Goecks. Atari-GPT: Investigating the capabilities of multimodal large language models as low-level policies for atari games. arXiv:2408.15950, 2024. [67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [68] X.AI. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v, 2024. [69] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, et al. AtomThink: slow thinking framework for multimodal mathematical reasoning. arXiv:2411.11930, 2024. [70] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, et al. R1-OneVision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv:2503.10615, 2025. [71] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [72] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 18 [73] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? arXiv:2504.13837, 2025. [74] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, et al. MathVerse: Does your multi-modal LLM truly see the diagrams in visual math problems? arXiv:2403.14624, 2024. [75] Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. [76] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-Zeros aha moment in visual reasoning on 2b non-sft model. arXiv:2503.05132, 2025."
        },
        {
            "title": "Appendix",
            "content": "Content A. Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.1. Training Data Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19 A.2. Training Prompt in Visual Game Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.1. Evaluation Detail of Atari Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2. Ablation On Rotation Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.3. Synergistic Effects of Mathematical Data Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 B.4. Synergistic Effects of Multi-Game Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.5. Reasoning Ability Boundary via Pass@洧녲 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C. Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 A. Data A.1. Training Data Synthesis This section provides additional implementation details for the game training data synthesis process, expanding on the methodology outlined in Section 2.3. For the Snake game, the environment consists of 10 10 grid game board with two snakes of 1-grid initial length. At each time step 洧노, each snake receives one action respectively to move, resulting in new game state 洧멇롐+1. To generate meaningful moves that accomplish the objective of collecting more apples while remaining alive, we implement policy network based on Proximal Policy Optimization (PPO) [53]. The observation space is represented as 10 10 grid with distinct values indicating empty cells (0), apples (1), the agents own body (2), and other agents bodies (3). These observations are stacked across 4 time steps to incorporate temporal information, resulting in an input tensor R10104. The policy network architecture consists of two convolutional layers with 3 3 kernels, followed by fully connected layers. The first convolution has 洧냤1 = 16 output channels while the second has 洧냤2 = 32 output channels, both followed by ReLU activation functions. After flattening, the features pass through fully connected layer with 256 units before outputting action logits for the four possible movements (right, left, up, down), which are then transformed into probability distribution 洧롄(洧녩洧) using softmax. The value function follows similar architecture but produces single scalar output 洧녤 (洧). To prevent the snake from easily dying, we incorporate action priors that discourage suicidal moves by masking logits for dangerous actions (e.g., moving into walls or other snake bodies). The model employs the standard PPO objective with entropy regularization coefficient 洧띻 = 0.01 to encourage exploration, along with value function coefficient 洧랝 = 0.5 and clipping parameter 洧 = 0.2. During training, we use buffer size of 2048 and minibatch size of 32 with Adam optimizer at learning rate 洧랙 = 103. Agents receive reward of 洧 = +1 for collecting apples and penalty of 洧 = 1 for dying. This reward structure, combined with the PPO algorithm, enables agents to learn complex behaviors such as obstacle avoidance, apple pursuit, and multi-step trajectory planning. With this policy network, we continuously collect data from the Snake game, generating diverse game scenarios that serve as training examples for downstream RL training. For the Rotation game, training data comprises synthetically generated visual puzzles focused on 3D spatial reasoning, specifically the understanding of object rotations. We utilized diverse collection of 540 unique 3D object meshes in total, with 408 meshes sourced from Hunyuan3D 2.0 [59] and an additional 132 meshes from Hunyuan3D 2.5. Hunyuan3D is large-scale 3D asset generation system capable of producing high-resolution textured objects, providing wide variety of shapes and textures for our game. Our custom data generation pipeline produced pairs of images (洧냪init, 洧냪rot) for each mesh, representing the object before and after defined rotation. The generation of each (洧냪init, 洧냪rot) pair followed precise sequence. First, to establish diverse initial viewpoint for 洧냪init, the 3D object was subjected to base orientation: 0 rotation around its x-axis, an angle selected from the set {0, 45, . . . , 315} around its y-axis, and 0 around its z-axis. To further enhance visual variety and prevent the learning of trivial transformations from canonical poses, an additional z-axis rotation, chosen from {0, 30, . . . , 330}, was subsequently applied. The rendering of the object after these compound initial transformations yielded the 洧냪init image. Subsequently, the 洧냪rot image was generated by applying the target rotation to the 21 object state depicted in 洧냪init. This target rotation was exclusively around the z-axis by an angle of either 90 or 180, which also served as the ground truth label for the sample. Our coordinate system is defined with the 洧논-axis pointing to the right, the 洧녽-axis pointing upward, and the 洧녾-axis pointing outward from the screen toward the viewer; thus, all target rotations occur in the plane of the image. All objects were rendered at 512 512 pixel resolution using consistent perspective camera providing frontal view, under standardized lighting conditions. Visualizations of coordinate axes were not included in the rendered images. This process resulted in approximately 32k unique (洧냪init, 洧냪rot) pairs derived from the pool of 537 meshes allocated for generating test instances. As detailed in Section 2.1, each training instance presented to the MLLM comprised four imagesan example pair (洧냪ex rot) and task pair (洧냪task rot ). The example pairs were generated using separate, dedicated set of 3 meshes, ensuring that the objects seen in in-context examples were distinct from those used in the test portion of any given prompt. Both example and test pairs were generated via the methodology described above. init , 洧냪task init, 洧냪ex 22 A.2. Training Prompt in Visual Game Learning Prompt for Snake Game Your role is to guide snake within Snake game featuring multiple apples. This game is played on board of size 10 by 10. The board uses standard Cartesian coordinate system, where (0,0) represents the bottom-left position and (9,9) is the top-rightmost coordinate. Apples at: {apple_position} Direction of Your Last Action: {last_action} Rules: 1) If you move onto an apple, you grow and gain 1 point. 2) If your head moves to position where its coordinates (x, y) are outside the board boundaries (meaning < 0, > 9, < 0, or > 9), or into space occupied by another snakes body, or into space occupied by your own body, you die. Thats the worst move. 3) The goal is to prioritize snake not die, then efficiently collecting apples. First avoid the worst move, then for each apple, find the nearest apple by calculating Manhattan distances. But only choose best next move to get closer the nearest apple if you can confirm best next move will not run outside the range of the listed coordinates, run into the position of another snake, or yourself. Otherwise it will be the worst move. Your snake with the ID {snake_id} in {snake_color} has its head now positioned at {snake_position}, and its body extends to {body_position} You should avoid your next move into your own snakes position. Enemy snakes in {enemy_color} positions: {enemy_position}. Decreasing your coordinate is to the LEFT, increasing your coordinate is to the RIGHT. Decreasing your coordinate is DOWN, increasing your coordinate is UP. Read out another snakes position and apple position. Try to predict another snakes next move and avoid colliding with it. Best answer is one of next move that is the closest to the apple and not lead to your death. Worst answer is all of next moves 1. makes your heads coordinates (x, y) are outside the board boundaries, meaning < 0, > 9, < 0, or > 9. 2. moves into position occupied by another snakes body. 3. moves into position occupied by body of yourself. Check all the next moves to list out all the worst moves in <worst_answer> tag. If no worst answer, return None for worst answer, e.g., \"<worst_answer>None</worst_answer>\" The best answer and the worst answer are mutually exclusive and different. You need first to give your reasoning process then to choose one of best next move and worst next move from [UP, DOWN, LEFT, RIGHT]. The reasoning process and answer are enclosed within <think> </think>, <best_answer> </best_answer> and <worst_answer> </worst_answer> tags, respectively, i.e., \"<think> reasoning process here </think><best_answer> one best move here </best_answer><worst_answer> all worst moves here </worst_answer>\" 23 Prompt for Rotation Game Im showing you 4 images. Images 1-2 are an example pair, and Images 3-4 are the test pair. In each pair, the first image shows the initial orientation, and the second shows the object after rotation. ### EXAMPLE OF ROTATION ### Example: Image 1 shows the initial view and Image 2 shows the object after 180 degree rotation. ### YOUR TASK ### Now, considering the transformation from Image 3 (initial) to Image 4 (rotated) . Determine the angle of rotation from Image 3 to Image 4 on the plane Analyze the rotation carefully using the example pair (Images 1-2) as reference. 1. Coordinate System Transformation: - Draw an x-y coordinate system on both original and rotated images with origin at center - Identify distinct feature point and note its coordinates in both images - Apply rotation matrix equations to verify the transformation Example: star icon at coordinates (3,1) in the original image appears at (-1,3) in the rotated image. Testing with the 90 clockwise rotation matrix [cos(90), sin(90); -sin(90), cos(90)] confirms the transformation from (3,1) to (-1,3), verifying 90 clockwise rotation. 2. Angular Displacement Measurement: - Mark the image center as the origin in both images - Draw straight line from center to distinctive feature in both images - Measure the angle between these two lines using counterclockwise as positive Example: line from center to red dot makes 30 angle with horizontal in the original image. In the rotated image, this line makes 210 angle with horizontal. The difference (180) indicates clockwise 180 rotation. 3. Symmetry Axis Tracking: - Identify major symmetry axes in the original image - Locate the same symmetry axes in the rotated image - Calculate the angular displacement between original and rotated axes Example: rectangular logo has vertical and horizontal symmetry axes. After rotation, the vertical axis now points right and horizontal points down. This 90 shift of both axes confirms clockwise 90 rotation. 4. Triangle Configuration Analysis: - Select three non-collinear distinct points forming triangle in both images - Compare the orientation of this triangle in both images using vector cross products - Determine rotation angle from the triangles orientation change Example: Three points form right triangle with vertices clockwise arranged. After rotation, the same triangle has its vertices arranged in counterclockwise order while maintaining the same shape. This inversion indicates clockwise 180 rotation. 5. Polar Coordinate Comparison: - Convert key points to polar coordinates (r,洧랚) relative to image center - Compare 洧랚 values of the same features in original and rotated images - Calculate consistent angular difference across multiple points Example: feature at polar angle 45 in the original image appears at 135 in the rotated image. Another feature shifts from 10 to 100. Both show +90 shift in polar angle, confirming clockwise 90 rotation. Choose the rotation angle from this list: [counter clockwise 90, 180] 24 The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"<think> reasoning process here </think><answer> answer here </answer>\" We provide additional details regarding the training prompts used in RL training with the Snake and Rotation games. Our game prompts consist of two main parts: game settings and reasoning instructions. For the Rotation game, to promote diverse problem-solving approaches, each training instance included one spatial reasoning strategy hint randomly selected from predefined set of five: 1. Coordinate System Transformation, 2. Angular Displacement Measurement, 3. Symmetry Axis Tracking, 4. Triangle Configuration Analysis, or 5. Polar Coordinate Comparison. Bold text indicates reasoning instructions synthesized by GPT-4o [30]. 25 B. Evaluation B.1. Evaluation Detail of Atari Game Figure 6 Goal and example response from model of Atari games used for evaluation. We implement 7 kinds of Atari games from Atari-GPT [66]. 26 To evaluate out-of-distribution generalization, we test ViGaL on Atari-GPT [66], benchmark for evaluating MLLMs as decision-making agents in Atari video games, as shown in Fig. 6. The benchmark consists of seven different Atari games: Alien, Frogger, Pong, Ms. Pacman, Seaquest, Space Invaders and Breakout. These games present diverse visual environments which is different from Snake game and Rotation game, and require different strategic approaches to finish the goal, making them an ideal test bed for ViGaL evaluating out-of-distribution generalization capabilities. For evaluation, we input game frames as pixel observations to our model, following the established protocol in Atari-GPT. Specifically, each game frame is resized from 210 160 3 to 512 512 3, then provided to our model along with game-specific action information. We maintain context buffer containing the two previous frames and responses together with the current frame to enable temporal reasoning. Following Atari-GPT, we implement frame skipping of 8 frames, which extends the standard 4-frame skipping in ALE to reduce computational intensity while preserving gameplay continuity. We evaluate our method through four independent rollouts of 1,000 timesteps each and report the average cumulative reward, with results presented in Tab. 1c. B.2. Ablation On Rotation Game Table 6 Ablation study. Similar to the evaluation in Tab. 5, we analyze how different aspects of our post-training strategy within the Rotation game affect downstream generalization benchmarks. The base model is Qwen2.5-VL-7B, with results shown in gray. The default settings from Tab. 2 and Tab. 3 are highlighted in blue . We observe the same improvement trends for each strategy as reported in Tab. 5. (a) Prompt design. prompt Avg. Math CLEVR+ Geo. base model w/o Reasoning Instruction w/ Reasoning Instruction 49.1 61.4 47.7 48.9 54.9 80.4 44.8 54.8 62. 49.3 80.7 57.9 (b) SFT vs. RL. post-training Avg. Math CLEVR+ Geo. base model SFT RL 49.1 55.6 62.6 47.7 44.0 49.3 54.9 75.4 80.7 44.8 47.5 57.9 (c) Difficulty control. difficulty control Avg. Math CLEVR+ Geo. base model w/o difficulty control w/ difficulty control 49.1 61.0 62.6 47.7 48.0 49.3 54.9 80.2 80. 44.8 54.8 57.9 As shown in Tab. 6, we conduct similar ablation study to Tab. 5, but replace the Snake game environment with the Rotation game. Our results demonstrate the same consistent improvement trends on downstream generalization benchmarks for each strategy employed. Specifically, we control the task difficulty by varying the rotation angles between two images. In the uncontrolled difficulty setting, the rotation angle between images can be clockwise 90, counter-clockwise 90, or 180. However, we found that explicitly requiring the model to distinguish between clockwise and counter-clockwise rotations leads to training difficulties. Therefore, we remove it and only retain option of clockwise 90 and 180 rotations. Unlike the Snake game, we cannot conduct the ablations shown in Tab. 5e because the Rotation game is inherently vision-dependent and requires visual input. Similarly, we cannot perform the ablations in Tab. 5b because the Rotation game provides only binary answer options, 27 making it impossible to meaningfully designate both \"best\" and \"worst\" answers simultaneously. B.3. Synergistic Integration with Mathematical Data Model Math Avg. MathVista MathVerse MathVision base model MM-Eureka-Qwen-7B ViGaL (w/o Math Data) ViGaL (w/ Math Data) 47.7 50.1 50.6 51.8 68.0 73.0 71.9 72. 49.0 50.3 52.4 54.5 26.0 26.9 27.5 27.7 Table 7 Ablation study on Math data. We conduct experiment of additionally training ViGaL on mathematical data MMK12 [48]. The base model is Qwen2.5-VL-7B, whose results are in gray. The setting with highest average accuracy is highlighted in blue . Although our work primarily demonstrates mathematical performance improvement without training on math data, we conducted additional experiments to explore the synergistic benefits of integrating mathematical data into our training pipeline. In our experimental setup, we implemented two-stage training process. In stage 1, we followed our original approach, training the model exclusively on Snake and Rotation games. For stage 2, we trained our model on MMK12 [48], multimodal mathematical reasoning dataset containing approximately 12k examples. We maintained identical data and training settings as MM-Eureka-Qwen-7B [48]. The only difference was our models additional stage 1 training on visual games. As shown in Tab. 7, the integration of mathematical data in stage 2 yielded continuous improvement of 0.9% on average across three mathematical benchmarks compared to using only stage 1 training. This demonstrates the synergistic relationship between our visual game learning approach and mathematical data fine-tuning. Moreover, ViGaL (w/ Math Data) significantly outperformed MM-Eureka-Qwen-7B by 1.4% on mathematical benchmarks on average, despite both models using same math data. These results suggest that visual game learning can serve as an effective foundation training stage that can be further enhanced with domain-specific data to improve performance on target tasks. Figure 7 Accuracy differences between ViGaL-Snake+Rotation and base model without RL training across mathematical subfields in Mathverse. The synergistic effects of jointly training on two games observed suggest that complementary games can enhance overall mathematical reasoning capabilities. 28 B.4. Synergistic Effects of Multi-Game Training As discussed in Sec. 3.2, our analysis reveals that each game develops distinct reasoning abilities in the model. To investigate potential combined benefits, we conducted experiments where models were trained simultaneously on both the Snake and Rotation games. Fig. 7 shows that joint training effectively combines the strengths of each individual game, improving performance across the mathematical areas where each game shows particular effectiveness, resulting in greater overall gains on Mathverse. These results suggest that strategically combining games with complementary strengths offers simple yet effective approach to enhance model generalization abilities. B.5. Reasoning Ability Boundary via Pass@洧녲 Evaluation Figure 8 Pass@洧녲 performance curves on MathVista comparing base models with their zero-RL counterparts trained on mathematical data and game data, respectively. We explore the reasoning ability boundary of models trained with different RL approaches by evaluating the pass@洧녲 metric. This metric measures the probability that at least one of 洧녲 independent model samples solves given problem, indicating the true scope or boundary of models reasoning capability - essentially what problems the model can potentially solve given enough sampling attempts. We evaluate the pass@洧녲 performance of three models: the Base Model without RL training, MM-Eureka-Qwen-7B-Instruct, and our ViGaL. As shown in Fig. 8, our ViGaL consistently demonstrates increasing pass@洧녲 scores on Mathverse as 洧녲 increases. This finding suggests that our approach can effectively solve complex problems when allowed multiple reasoning attempts, uncovering capabilities not apparent in single-sample evaluations. Moreover, compared to the other RL-trained model, MM-Eureka-Qwen-7B-Instruct, our model achieves steeper improvement in pass@洧녲 as 洧녲 increases. This indicates that ViGaL possesses broader reasoning boundary and stronger reasoning abilities, enabling it to solve wider range of problems when given sufficient opportunities to explore different solution paths. 29 Finally, our results demonstrate that as 洧녲 increases, base models without RL training eventually outperform RL-trained models. This aligns with the findings in [73] that highlight fundamental limitation of reinforcement learning with verifiable rewards (RLVR): while RL training significantly improves performance at small 洧녲 values (e.g., pass@1), base models possess wider coverage of solvable problems. This suggests trade-off where RL optimization focuses on solving high-probability problems at the expense of broader solution coverage. Future work should explore RLVR algorithms that can improve pass@洧녲 performance across all values of 洧녲, effectively extending the reasoning boundary beyond that of the base model. C. Case Study (a) case study from Mathverse. Base model misinterpreted the geometric configuration and rotation direction, while our model correctly identified the perpendicular relationship and calculated the proper angle. (b) case study from Mathverse. Base model misperceived critical visual information like symmetry and coordinates in graphs, while our model demonstrated accurate visual perception for mathematical elements. Figure 9 Comparison of base model and our model after rule-based RL training, showing improved visual-mathematical reasoning on geometric and coordinate problems. 30 We provide quantitative comparison examples below to demonstrate reasoning improvements on mathematical problems after RL training. In Fig. 9a, when solving geometric angle problem, the base model fails to correctly interpret the critical relationship between perpendicular lines and corresponding angles. It makes contradictory assumptions about angle measures, leading to an incorrect calculation of the required rotation. In contrast, our ViGaL precisely tracks the geometric constraints and properly calculates the angle difference between initial and target positions. In Fig. 9b, when analyzing function properties from graph, the base model incorrectly claims the function lacks symmetry despite clear visual evidence. It fails to recognize the fundamental y-axis symmetry of the parabola shown in the image. Our model immediately identifies this critical symmetrical pattern and correctly applies the appropriate mathematical definition of an even function, demonstrating enhanced visual perception of mathematical structures."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "NVIDIA",
        "Rice University"
    ]
}