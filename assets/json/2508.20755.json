{
    "paper_title": "Provable Benefits of In-Tool Learning for Large Language Models",
    "authors": [
        "Sam Houliston",
        "Ambroise Odonnat",
        "Charles Arnal",
        "Vivien Cabannes"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 5 7 0 2 . 8 0 5 2 : r Provable Benefits of In-Tool Learning for Large Language Models Sam Houliston1,, Ambroise Odonnat2,, Charles Arnal3,, Vivien Cabannes3, 1ETH Zürich, 2Inria, Univ. Rennes 2 and IRISA, 3FAIR at Meta Equal Contribution. Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable. Date: August 29, 2025 Correspondence: Vivien Cabannes at vivc@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have revolutionized artificial intelligence, redefining how machines understand and generate human language. Beyond this, LLMs are rapidly evolving from static predictors into dynamic, context-aware systems capable of reasoning, adapting, and acting over time. Coding assistants are accelerating software development and lowering the barrier to entry for programming, hinting at the emergence of highly automated, agentic workflows. This transformation is driven by advances in architecture and interaction design. Retrieval-augmented generation (RAG, Lewis et al., 2020b) enables models to access external knowledge in real time, grounding their responses in contextually relevant information. In parallel, memory augmentation, through the use of scratchpads or memory modules, empowers models to organize their reasoning, break down problems, iteratively refine outputs, and maintain coherence across extended sequences. Together, these capabilities mark shift away from purely in-weight learning, where all knowledge and reasoning must be encoded within the parameters of model, toward more modular and interpretable systems that can utilize tools, access external information, and leverage structured memory. Our study investigates the theoretical foundation for why such tool-augmented approaches outperform traditional monolithic models. This evolution raises fundamental question: What is the most efficient way for model to acquire and utilize knowledge: should facts be internalized through parameter updates, or should models be taught to access and manipulate external sources of truth? At the heart of this question lies core tradeoff between in-weight learning, where information is compressed into the models parameters during training, and tool-augmented learning (or in-tool learning), where the model learns to interact with external resources such as databases or APIs to retrieve information when needed. While the former is bounded by the models capacity and sensitive to forgetting, the latter offers the potential for open-ended knowledge access, generalization, and interpretability. In this work, we formalize this tradeoff and provide rigorous theoretical framework for understanding the benefits of tool use in large language models. Contributions. Our main contributions are as follows: 1. We derive theoretical lower bound showing that the number of distinct facts model can store purely in its weights is fundamentally limited by its number of parameters. This highlights structural bottleneck in relying solely on in-weight memorization. 2. We construct an explicit upper bound proving that tool-augmented models can, in principle, retrieve an unbounded number of facts by learning to interface with an external database. This result is supported by formal circuit construction demonstrating the feasibility and efficiency of such tool use. 3. We validate these theoretical results in controlled 1 Figure 1 Benefits of in-tool learning. Illustration of factual recall via in-weight learning (memorization) versus in-tool learning (external retrieval). While tool use may incur higher latency, it offloads storage, enabling scalable recall without increasing model size. It also better preserves the models original capabilities during finetuning. experimental setting, where models are trained from scratch either to memorize facts or to learn to use an external tool. Our findings empirically confirm the predicted scaling laws and memory limitations. 4. We demonstrate the practical implications of our findings for existing LLMs, showing that finetuning to learn new facts is significantly less effective than teaching models how to access structured tools or infer generalizable rules. This reinforces the argument that future LLM development should prioritize tool-use capabilities over rote memorization. Together, these results provide both conceptual and empirical foundation for understanding why models that use tools outperform those that attempt to internalize all the knowledge. Our study suggests shift in model design philosophy: from training ever-larger monolithic models toward building modular systems that learn to query, not just to store information. Our codebase is freely available for researchers to further explore memory load in large language models1. Related work. Recent work has increasingly explored tool-augmented language models. This trend is evident in the open-source literature, with models such as Toolformer (Schick et al., 2023), ReAct (Yao et al., 2023), and HuggingGPT (Shen et al., 2023), as well as in industrial systems that now support function calling, enabling modular LLM-agent architectures. External memory mechanisms such as RAG (Lewis et al., 2020c), RETRO (Borgeaud et al., 2022), and retrievalaugmented transformers (Shao et al., 2024) serve as tools in their own right, enhancing knowledge access. In similar spirit, scratchpads (Nye et al., 2021) and chainof-thought prompting (Wei et al., 2022) externalize reasoning and expand model capacity (Besta et al., 2025). This is perhaps unsurprising, as several studies have quantified the limitations of in-weight knowledge (Hahn, 1The code and reproducibility instructions are available at https://github.com/ambroiseodt/itl. 2020; Merrill & Sabharwal, 2024; Cabannes et al., 2025; Allen-Zhu & Li, 2025c), and highlighted interference during editing (Meng et al., 2022; Dai et al., 2022). Taken together, these lines of research delimit growing frontier that contrasts in-weight learning with increasingly sophisticated mechanisms for retrieval, reasoning, and tool use. This is the dichotomy we aim to investigate in our work, contributing to the broader effort to deepen the theoretical and empirical understanding of language models (see, e.g., Bietti et al., 2023; Zekri et al., 2025; Beirami et al., 2025). Additional related work can be found in Appendix for the interested reader."
        },
        {
            "title": "2 Setting",
            "content": "To highlight the differences between in-weight memorization and tool-augmented reasoning, we introduce family of factual recall tasks. Let be family of datasets, where each dataset is finite collection of facts. Each fact is triplet (n, a, v) of strings, consisting of: name (e.g., Thierry de Sienne), an attribute (e.g., birthplace), and value Va (e.g., Germany). Each dataset defines mapping VD : (n, a) (cid:55) v, which assigns to each nameattribute pair its corresponding value. The goal of the recall task is to produce the ground-truth value VD(n, a) when given query (n, a). Let be class of models, where each model implements recall rule R(f ) : (cid:83) aA Va. For instance, the model class may be the set of all transformers with given architecture, each model corresponding to specific choice of weights, and the recall rule R(f ) being defined by prompting the model with query asking for the value associated to pair (n, a) and parsing the text obtained by auto2 regressively sampling from the model until an end-ofsequence token is emitted."
        },
        {
            "title": "3 In-Weight Lower Bound",
            "content": "Definition 2.1 (Learnability). We say that the model class can learn the recall task associated to family of datasets if, for each dataset D, there exists model such that achieves perfect accuracy on the value recall task for D, i.e. R(f ) = VD. Note that this definition abstracts away the process of training and does not account for sample efficiency or optimization difficulty. Sentence format. Our study is concerned with factual recall performed by LLMs via question answering. We define dataset with strict grammar that specifies the format of all questionanswer pairs. In the in-weight setting, the model is trained to directly generate the answer from its parameters. The user produces query string using the format = φ1(a) φ2(n) φ3(a), and the assistant must then generate the answer string in the form = ψ1(n) ψ2(a) ψ3(v), where φi and ψi are deterministic string functions (i.e. templates) that encode the structure of the input and output, and denotes string concatenation. For example, given the previous fact example, we have: = Where was (cid:125) (cid:124) (cid:123)(cid:122) φ1(a) = Thierry de Sienne (cid:125) (cid:124) (cid:123)(cid:122) ψ1(n)=n Thierry de Sienne (cid:123)(cid:122) (cid:125) (cid:124) φ2(n)=n was born in (cid:125) (cid:123)(cid:122) (cid:124) ψ2(a) born? (cid:124) (cid:123)(cid:122) (cid:125) φ3(a) Germany (cid:125) (cid:123)(cid:122) (cid:124) ψ3(v)=v In the in-tool setting, the model learns to issue structured tool query that retrieves the value from an external database. The user query remains unchanged, but the assistant must first produce an intermediate tool query string = χ1(a) χ2(n), where the χi are deterministic string templates encoding the attribute and name in format expected by the tool interface. This query is sent to an external retrieval system (e.g., database or API), which returns the corresponding value ξ(f, a, n), where ξ(f, a, n) = if the assistant correctly formats the query, and an error message otherwise. The assistant then produces the final answer A, reusing the same output grammar as in the in-weight case. For example, using the same fact example, we have: = To answer this request, will make tool-call. <DB> FIND birthplace (cid:125) (cid:124) (cid:123)(cid:122) χ1(a) , FOR Thierry de Sienne </DB> (cid:123)(cid:122) (cid:125) (cid:124) χ2(n) This section illustrates the limitations of in-weight learning through theoretical lower bounds. The following lemma provides lower bound on the size of model class that can learn given family of datasets. Lemma 3.1 (Capacity lower bound.). Let be family of factual datasets defined over fixed set of names and attributes A. Let be model class that can learn D. Then, we have D. (1) Proof. This follows from simple enumeration argument, since the definition of learnability (Definition 2.1) is equivalent to the inclusion {VD D} {R(f ) M}. Considering the cardinality of those sets concludes the proof. Lemma 3.1 implies lower bound on the number of parameters required to define class of models that could memorize given set of facts. This allows us to provide an upper bound on the parameter requirement. Theorem 3.2 (Parameter lower bound). Let be class of models with parameters, each quantized to bits. Let be the family of all datasets defined over names , attributes A, and value sets Va. Then, if can learn without accessing external tools (in-weight learning), the number of parameters must satisfy: (cid:88) aA log2 Va = #Facts, where #Facts = A is the number of facts, and is the average of log2 Va/b over A. Proof. This is consequence of Lemma 3.1. The total number of distinct parameter configurations in the model family is at most 2bP (i.e., 2bP ). Moreover, defining any dataset from requires distinguishing between all possible assignments of values for the attributes of each name. For single name, there are (cid:81) aA Va possible value assignments; for all names, this yields = (cid:0)(cid:81) aA Va(cid:1)N . Applying Lemma 3.1 with these quantities and taking the base-2 logarithm of both sides of the inequality completes the proof. This formal grammar establishes controlled setting to characterize the information storage and retrieval capabilities of LLMs precisely. An example of queries formatted in JSON files can be found in Appendix C.1. Theorem 3.2 shows that the number of parameters needed to memorize arbitrary fact mappings grows linearly with the number of facts. It proves hard capacity ceiling on in-weight learning: Once the number of facts exceeds threshold determined by model size, memorization becomes impossible without architectural changes or external memory. Note that Theorem 3.2 could be refined in two directions. First, the capacity of model class does not necessarily grow exponentially with the number of quantization bits. In practice, empirical studies such as Allen-Zhu & Li (2025c) suggest that the effective representational capacity of language models is often much lower than the theoretical maximumtypically around 2 bits per parameter, meaning that could be replaced by 2 in the theorem. Second, in practice, facts are often highly structured and correlated. For example, knowing that someone is Japanese makes it more likely that they are older than 60 than if they were from Kenya, due to population-level age differences. These kinds of regularities mean that not all combinations of facts are equally likely. One can account for this by introducing probability distribution over datasets in D, and then asking: How many parameters are needed for model to correctly learn the most likely (1 ε) fraction of datasets? However, even with these statement refinements, the relationship between the number of parameters and the number of learnable effective facts still scales linearly."
        },
        {
            "title": "4 In-Tool Upper Bound",
            "content": "In this section, we show that tool use allows models, and in particular transformers (Vaswani et al., 2017), to circumvent the limitations of in-weight learning and accurately recall an unlimited number of facts without needing additional parameters. More specifically, we exhibit an upper bound on the number of parameters needed for transformer to implement the kind of database querying described in Section 2. Let be family of factual datasets defined over some names, attributes, and value sets , A, {Va}aA. We introduce the following notion of query-based learnability for model augmented with access to an external database. Definition 4.1 (Query-based Learnability). For factual dataset D, the associated retrieval system RD is recall rule which can be queried in the manner described in Section 2 to retrieve any fact of D. We say that tool-augmented model solves the recall task for family of datasets if, for any D, it achieves perfect accuracy on the value recall task for by querying RD. Note that Definition 4.1 is stronger than Definition 2.1, as single model (rather than family of models) is tasked with learning any dataset D, provided it has access to correct retrieval system. In the theorem below, we consider Llama3-like transformers (Grattafiori et al., 2024) with fixed token vocabulary; we make few mild technical assumptions, e.g. the queries and answers are shorter than the maximum model context length, which we detail in Appendix B, along with the proof of the statement. Theorem 4.2 (Parameter upper bound). Let be family of factual datasets defined over some names, attributes, and value sets , A, {Va}aA. Given RD = {RD}DD family of retrieval systems associated to D, there exists transformer with at most 8 transformer blocks, an embedding dimension of at most O(A) and total number of parameters at most O(A2) that can solve the value recall task for if equipped with the ability to access RD. Proof sketch. The proof consists of designing algorithmic mechanisms to auto-regressively generate the tool query and the answer in the string ξ A. At high level, the transformer needs to: (i) parse the attribute from the user query Q; (ii) emit the corresponding χ1(a); (iii) emit the suffix χ2(n) of by copying the name part from the question Q; and (iv) perform similar copying operation to emit the final answer after observing the value returned by the database. Parsing the attribute from Q. First, one could reserve the first transformer block to retrieve absolute position encoding (see, e.g. Golkar et al., 2024, Proposition 2.2). Then, one can reserve 2A attention heads in the second block, so that each head fires whenever the suffix of the input stream matches one of the fixed strings φ1(a) or φ3(a). The firing pattern can be written into onehot attribute register of dimension 2A that is carried forward in the residual stream. Emitting χ1(a). The attribute can be retrieved from the attribute register, which in turn enables the transformer to auto-regressively output χ1(a). Emitting χ2(n). The emission of χ2(n) can be done by copying φ2(n). This requires parsing φ2(n), which can be done using the attribute register to identify the position of the token that starts φ2(n), followed by using an induction head to copy the full name. mechanism such as counting the number of whitespace characters in and subtracting the number of whitespace characters in φ1(a) φ3(a) can be used to determine when to stop copying (thus avoiding the inclusion of φ3(a) when emitting χ2(n)). Emitting A. similar construction allows copying the value returned by the database, as well as the name n, to emit the final answer A. Overall, this construction uses embeddings of dimension O(A) with constant number of layers, leading to total number of parameters that scales as O(A2). 4 Figure 2 Scaling of parameter requirements with the number of facts. Minimum number of parameters required to achieve at least 95% recall as function of the dataset size. Results are averaged across 10 runs with standard deviation. In-weight models require increasingly more parameters as the number of facts grows, approximately following the linear trend = αx + β with (α, β) = (8.14, 5171), consistent with Theorem 3.2. In contrast, in-tool models exhibit sharp saturation: beyond critical point (dashed vertical line around 1K facts), the parameter requirement flattens, indicating transition to tool-based retrieval. Figure 3 Transition from memorization to retrieval in intool models. Accuracy on out-of-distribution databases (i.e., databases of facts unseen during training) as function of the dataset size. Below the transition point (the same dashed vertical line as in Figure 2), in-tool models memorize specific databases and generalize poorly, worse than the best random model that outputs the same value for all queries (red dashed line). Above this threshold, they learn to construct tool queries that generalize across datasets, resulting in stable accuracy even on new data. Note that the bounds do not depend on the number of names or possible values. While our theoretical results provide foundational insights, empirical validation remains crucial, since Theorem 3.2 describes worst-case scenario, and Theorem 4.2 is an existence result that does not account for optimization difficulty."
        },
        {
            "title": "5 Controlled Experiments",
            "content": "This first experimental section investigates the difference between in-weight and in-tool learning by pretraining models from scratch on synthetic factual datasets. Our results support the theoretical insights: tool-augmented recall outperforms in-weight memorization in terms of the number of facts learned per parameter. Additional experimental details and results are provided in Appendix C.2. Experimental setup. To validate our theoretical findings, we place ourselves in controlled setting. We construct synthetic biographical datasets from fixed list of names and four attributes A: birth place, birth date, current address, occupation; which can take 7, 16800, 213, and 100 values respectively. This yields 4N atomic facts per dataset instance. For each dataset size, we train family of small Llama3-style transformer models (Grattafiori et al., 2024) with 2 layers, 2 attention heads, vocabulary size of 260 (byte encoding with 4 special tokens), and context window of 257. The embedding dimension ranges from 4 to 128, resulting in models with between 2K and 0.6M parameters. Our models are trained for up to 100,000 steps using the AdamW optimizer (Loshchilov & Hutter, 2019), with batch size of 128 samples, decoupled weight decay coefficient of 0.1, and (β1, β2) = (0.9, 0.95). We use cosine learning rate scheduler with warmup phase of 50 steps and maximum learning rate of 0.001, and final learning rate ratio of 0. We compare two training regimes, illustrated in Figure 1: (i) in-weight, where the model must directly generate the answer from its parameters, and (ii) in-tool, where the model first emits structured lookup query to an external database, which returns the requested value that the model then formats into an answer. Theory validation. Our empirical results strongly support the theoretical predictions presented in Sections 3 and 4. As shown in Figure 2, the in-weight regime exhibits an unbounded increase in the number of required parameters as the number of facts grows. This behavior demonstrates the existence of lower bound on parameter requirements, consistent with the general claim of Theorem 3.2. Note that the empirical scaling is slightly sublinear for small dataset sizes, approximately following square-root trend. This suggests the presence of subtle phenomena not captured by the theorem, which we leave to future work. This confirms that parameter count imposes hard limit on memorization-based learning. In contrast, the in-tool regime displays clear transition: after critical dataset size (around 1,000 facts in our setting), the parameter requirement saturates and 5 To explore how structure affects memorization, we introduce controlled correlations between facts using parameter α [0, 1]. For each pair name-attribute (n, a), we draw Bernoulli variable Bern(α), and set VD(n, a) = V1(n, a) if = 1, or V2(n, a) otherwise, where, V1 assigns values based on the family name, while V2 assigns values independently at random. As such, α = 1 amounts to family members sharing birthplace, current address, and occupation, while α = 0 corresponds to the unstructured case used in earlier experiments. As shown in Figure 4, increasing the correlation reduces the number of parameters required for in-weight models to achieve high recall. This effect could be understood in light of Lemma 3.1: by introducing correlations, we constrain the set of possible datasets2, thereby lowering the lower bound on model capacity. Our results suggest that transformers are able to adapt to this structure, effectively compressing related facts through shared patterns and achieving high recall with fewer parameters. This aligns with previous findings on how structural assumptions about the data influence learning in transformers and neural networks more broadly (Valvoda et al., 2022; Dziri et al., 2023; Arnal et al., 2024; Wang et al., 2025; Mahajan et al., 2025)."
        },
        {
            "title": "6 Large-Scale Experiments",
            "content": "This section investigates whether the limitations of inweight learning persist in real-world settings involving pretrained multi-purpose language models. In particular, we show that while large models can store vast amounts of information in their parameters, introducing new knowledge through finetuning may interfere with existing capabilities due to finite capacity and training dynamics. In contrast, tool-augmented learning promises scalability without forgetting. We provide further related results in Appendix C.3. Experimental setup. We fine-tune instruction-tuned language models on the synthetic factual datasets from Section 5, ranging from 500 to 50k atomic facts. Models include SmolLM 2 Instruct (135M, 360M, 1.7B) (Allal et al., 2025) and Llama 3.1/3.2 Instruct (1B, 3B, 8B) (Grattafiori et al., 2024), each trained to reach at least 95% recall. We use the same training setup as in Section 5, with learning rates adapted to model size. For SmolLM models, we use the values reported in the original paper: 103 for 135M and 360M, and 3 104 for 1.7B. For Llama models, we scale learning rates η with model size P, setting η = 2 105 at 8B, and following the established inverse power law η 1/2 for stable updates, with the number of parameters (Kaplan et al., 2020; Gao et al., 2023). 2This constraint applies in expectation: with high probability, the dataset is sampled from smaller effective family, tightening the lower bound in expectation. Figure 4 Impact of the dependency between facts on in-weight memorization. Minimum parameter requirement to achieve at least 95% recall on datasets generated with correlation between facts of α. The introduction of such correlation breaks the independence between triplets, making the dataset more compressible thanks to common patterns between facts. Thus, as correlation increases, the number of effective units of facts decreases, requiring fewer parameters to store them. remains nearly constant. This plateau confirms that the model has shifted from memorizing facts to delegating recall to an external database, in line with the existence results in Theorem 4.2. From memorization to rule learning. This transition in the in-tool regime reflects more than capacity Initially, shift; it reveals learning phase transition. models faced with tool-augmented recall resort to memorizing factanswer pairs, similar to the in-weight regime. However, once exposed to sufficient diversity of facts, the model discovers the underlying logic for constructing tool queries. As shown in Figure 3, where the best OOD accuracy achieved by in-tool models is displayed, this results in dramatic improvement in generalization to out-of-distribution facts. This phenomenon mirrors the grokking effect observed in other settings: delayed but sharp shift from brute-force memorization to systematic rule learning (Power et al., 2022; Nanda et al., 2023). Once the model learns the correct format for querying, retrieval generalizes across databases, decoupling recall accuracy from the number of stored facts. What counts as fact? Our study so far assumes clean distinction between facts (e.g., Thierry de Sienne was born in Germany) and rules (e.g., birthplace queries can be answered by lookup function). In our attribution task, this boundary is sharp, but in broader domains such as mathematics or commonsense reasoning, the line between facts and rules is often ambiguous. Are theorems facts, or derived consequences of axioms? Ontologies may reveal intricate interdependencies that blur this distinction. 6 Figure 5 Impact of factual memorization on general language capabilities (HellaSwag). Hellaswag accuracy as function of the dataset size for pretrained models finetuned until reaching at least 95% recall on the database. Toollearning (dashed) preserves general capabilities almost perfectly across models and dataset sizes. In contrast, in-weight learning (solid) leads to noticeable degradation, especially for smaller models and larger factual loads. This supports our theoretical prediction that parameter-based memorization causes interference and loss of prior capabilities, due to finite capacity and update interference. Larger models are more robust to such forgetting, but still exhibit performance decay beyond 10k facts. Frequent checkpoints are saved to evaluate: (i) factual recall, (ii) HellaSwag accuracy, used as proxy for general language abilities; this benchmark presents four candidate sentence completions, and the model is evaluated on its ability to select the most plausible one (Zellers et al., 2019), and (iii) Total Variation (TV) distance from the base models output distribution, estimated over 100 natural language prompts by generating completions with the base model and computing the mean tokenlevel ℓ1 distance between base and fine-tuned output probabilities. Overloading threatens general capabilities. Motivated by our theoretical results, we now investigate whether the benefits of in-tool learning persist in large, pretrained language models. Figure 5 shows how HellaSwag accuracy evolves as models are trained to memorize increasing numbers of facts. In-tool learning (dashed lines) retains general performance across all scales, confirming that externalizing factual storage prevents interference. In contrast, in-weight learning (solid lines) shows clear degradation, especially in smaller architectures. This supports our theoretical intuition: parameterbased memorization consumes capacity and risks overwriting prior knowledge. We note that larger models degrade more slowly, suggesting they can absorb new information with less interference. These results unFigure 6 TV distance for different memorization loads. Similar setup to Figure 5, where we display the Total Variation (TV) distance. In-weight finetuned models deviate more from their base models when memorizing bigger datasets. derscore the scalability advantage of tool-augmented learning for preserving general capabilities. Token distribution change. The differences between in-weight and in-tool learning can be further understood by measuring how much each method alters the models output distribution. As shown in Figure 6, in-weight learning leads to significant driftespecially in smaller modelswith total variation (TV) distance growing sharply with the number of memorized facts. In contrast, tool-learning causes minimal divergence, even at large scales. This offers complementary perspective on our results: in-weight learning requires shifting the next-token distribution to encode new information, while in-tool learning preserves the models original behavior by delegating recall. Efficiency trade-offs. Another advantage of in-tool learning is its training efficiency: As shown in Figure 7, models acquire the tool-use function call pattern in fewer than 20 steps, as soon as the set is large enough to elicit tool-use. In contrast, in-weight learning requires substantially more training to memorize individual facts. However, this efficiency comes with trade-off: In-weight models support cheaper inference, producing answers directly without generating intermediate tool queries or relying on external calls. We also observe that tool-use can be slightly unstable to train; early stopping proved effective to prevent capability loss once the tool behavior was acquired."
        },
        {
            "title": "7 Discussion",
            "content": "This work provides unified theoretical and empirical account of the tradeoff between in-weight and in-tool 7 exploration of tool use is limited to factual recall via structured databases, excluding other important tools such as computer use, reasoning trace generation, or learnable memory modules integrated into the model architecture. Extending our analysis to these richer settings is promising direction for future work."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big datacentric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: part 3.1, knowledge storage and extraction. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum? id=FxNNiUgtfa. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id= oDbiL9CLoS. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. In The Thirteenth International Conference on Learning Representations, 2025c. URL https://openreview.net/forum? id=FxNNiUgtfa. Charles Arnal, Clement Berenfeld, Simon Rosenber, and Vivien Cabannes. Learning with hidden factorial structure, 2024. URL https://arxiv.org/abs/2411.01375. Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander Nicholas DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=u3U8qzFV7w. Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Zixuan Chen, Hubert Niewiadomski, and Torsten Hoefler. Reasoning language models: blueprint, 2025. URL https://arxiv.org/abs/ 2501.11223. Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Figure 7 Training steps required for memorization. Number of training steps as function of the dataset size for pretrained models finetuned until perfect recall. For the same optimization parameters, in-weight compute requirements scale with the memorization load. On the other hand, learning tool-use is faster and independent of dataset size in our setup. Llama models are better predisposed to learn tool-use from their pretraining compared to SmolLM models. learning. Theoretically, we establish that the number of facts model can store in its parameters is bounded by its size, implying that scaling knowledge capacity through model enlargement is inherently inefficient. In contrast, models that learn to interact with external tools can access unbounded factual knowledge without increasing their parameter count. Controlled experiments confirm this sharp divide: while in-weight models require everlarger architectures to memorize growing datasets, toolaugmented models exhibit phase transition, rapidly shifting to rule-based querying once sufficient diversity is observed. This decouples memory capacity from model size. Large-scale experiments extend these findings to pretrained models, showing that in-weight finetuning for factual recall degrades general capabilities, consequence of limited capacity forcing new information to overwrite prior knowledge. Tool-based approaches, by externalizing factual storage, preserve core skills, reduce training costs, and introduce minimal behavioral drift. These results underscore key design insight: language models scale more effectively not by internalizing ever more information, but by learning to access and orchestrate it, emerging less as static predictors and more as systems capable of modular interaction with structured external resources. Limitations. While our analysis provides conceptual clarity and theoretical insight, it is grounded in idealized, controlled settings. Although insightful, such synthetic datasets may not fully reflect the complexity of realworld knowledge. Moreover, the optimization dynamics are not considered in our theoretical bounds, and the 8 Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=3X2EbBLNsk. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 22062240. PMLR, 17 23 Jul 2022. URL https://proceedings.mlr.press/v162/ borgeaud22a.html. Vivien Cabannes, Berfin Şimşek, and Alberto Bietti. Learning associative memories with gradient descent. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024a. Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=Tzh6xAJSll. Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Yang, Francois Charton, and Julia Kempe. Iteration head: mechanistic study of chain-of-thought. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL https: //aclanthology.org/2022.acl-long.581/. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memoryefficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, pp. 933941. JMLR.org, 2017. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: limits of transformers on compositionality. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1083510866. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/gao23h.html. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning LLMs on new knowledge encourage hallucinations? In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 77657784, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.emnlp-main.444. URL https://aclanthology.org/ 2024.emnlp-main.444/. Siavash Golkar, Alberto Bietti, Mariel Pettee, Michael Eickenberg, Miles Cranmer, Keiya Hirashima, Geraud Krawezik, Nicholas Lourie, Michael McCabe, Rudy Morel, Ruben Ohana, Liam Holden Parker, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, and Shirley Ho. Contextual counting: mechanistic study of transformers on quantitative task, 2024. URL https://arxiv.org/abs/2406.02585. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, and Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407. 21783. Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, December 2020. ISSN 2307-387X. doi: 10.1162/tacl_a_00306. URL http: //dx.doi.org/10.1162/tacl_a_00306. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. URL https://arxiv.org/abs/1606. 08415. Sam Houliston, Alizée Pace, Alexander Immer, and Gunnar Rätsch. Uncertainty-penalized direct preference optimization, 2024. URL https://arxiv.org/abs/2410.20187. Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, and Kai Shu. Can knowledge editing really correct hallucinations? In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=hmDt068MoZ. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438, 2020. doi: 10.1162/tacl_a_00324. URL https://aclanthology.org/2020.tacl-1.28/. 9 Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. URL https://arxiv.org/abs/2207.05221. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/ abs/2001.08361. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, 2020a. ISBN 9781713829546. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020b. Curran Associates Inc. ISBN 9781713829546. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020c. Curran Associates Inc. ISBN 9781713829546. Danny D. Leybzon and Corentin Kervadec. Learning, Insights from tracking LLM forgetting, remembering: memorization during training. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen (eds.), Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 4357, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.blackboxnlp-1.4. URL https: //aclanthology.org/2024.blackboxnlp-1.4/. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=Bkg6RiCqY7. Divyat Mahajan, Mohammad Pezeshki, Charles Arnal, Ioannis Mitliagkas, Kartik Ahuja, and Pascal Vincent. Compositional risk minimization. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=axN83wi3NH. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=-h6WAS6eE4. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NjNGlPh8Wh. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=9XFSbDPmdW. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models, 2023. URL https://arxiv.org/ abs/2303.09014. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 24632473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/ D19-1250/. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022. URL https://arxiv. org/abs/2211.05102. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets, 2022. URL https://arxiv.org/abs/2201.02177. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool learning with large language models: survey. FronISSN tiers of Computer Science, 19(8), January 2025. 2095-2236. doi: 10.1007/s11704-024-40678-2. URL http: //dx.doi.org/10.1007/s11704-024-40678-2. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 54185426. Association for Computational Linguistics, November 2020. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main.437/. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=Yacmpz84TH. Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=iAkhPz7Qt3. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=yHdTscY6Ci. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Roformer: EnPan, Wen Bo, and Yunfeng Liu. hanced transformer with rotary position embedding. Neurocomputing, ISSN 09252024. 2312. https://doi.org/10.1016/j.neucom.2023. 127063. URL https://www.sciencedirect.com/science/ article/pii/S0925231223011864. 568:127063, doi: Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, and Ryan Cotterell. Benchmarking compositionality with formal languages. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 60076018, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.525/. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 11 Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier Teytaud, and David LopezPaz. Meta Lingua: minimal PyTorch LLM training library, 2024. URL https://github.com/facebookresearch/ lingua. Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. Grokking of implicit reasoning in transformers: mechanistic journey to the edge of generalization. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385. Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. Knowledge mechanisms in large lanIn Yaser guage models: survey and perspective. Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 70977135, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.416. URL https://aclanthology.org/2024.findings-emnlp.416/. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: survey. ACM Comput. Surv., 57(3), November 2024b. ISSN 0360-0300. doi: 10.1145/3698590. URL https://doi.org/10.1145/3698590. Xiaohan Wang, Dian Li, Yilin Zhao, Sinbadliu, and Hui Wang. Metatool: Facilitating large language models to master tools with meta-task augmentation, 2024c. URL https://arxiv.org/abs/2407.12871. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= WE_vluYUL-X. Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, and Ievgen Redko. Large language models as markov chains, 2025. URL https: //arxiv.org/abs/2410.02724. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/ 2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. comprehensive study of knowledge editing for large language models, 2024. URL https://arxiv.org/ abs/2401.01286. Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, and Jun Wang. How do large language models capture the ever-changing world knowledge? review of recent advances. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 8289 8311, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.516. URL https://aclanthology.org/2023.emnlp-main.516/."
        },
        {
            "title": "A Extended Related Work",
            "content": "Memorization capabilities of language models. The \"Physics of LLMs\" series (Parts 3.13.3) offers comprehensive and theoretically grounded account of various aspects of memorization with LLMs. Part 3.1 (Allen-Zhu & Li, 2024) covers knowledge storage, distinguishing between the storage of facts and whether they are extractable in practice. With controlled experiments on synthetic biographies, the authors showed that training on diverse or augmented forms of facts (such as paraphrasing or sentence re-ordering) is crucial for facts to be recoverable via prompting. This is done with the use of linear probing to diagnose where facts are encoded in hidden states. Part 3.2 Knowledge Manipulation (Allen-Zhu & Li, 2025b) extends this discussion beyond storage and extraction by evaluating whether LLMs can flexibly use stored facts for downstream reasoning. Four factual manipulation tasks are defined (retrieval, classification, comparison, and inverse search) to show that LLMs, including GPT-4, are competent only at direct retrieval. Lastly, Part 3.3 Knowledge Capacity Scaling Laws (Allen-Zhu & Li, 2025a) quantifies the knowledge storage limits of LLMs, demonstrating consistent capacity of 2 bits per parameter across various models. Their experimental design spans multiple architectures, quantization, and positional encoding methods. Links between memorization capacity and model size. Roberts et al. (2020) investigate the extent to which factual knowledge can be stored in model parameters by fine-tuning language models of varying sizes on closed-book QA tasks. They find that performance scales with the model size, with larger models being able to approach the accuracy of retrieval-based systems. Allen-Zhu & Li (2025a) present exhaustive experiments in controlled setting by fine-tuning models on synthetic biographical datasets under different conditions (training duration, model architectures, quantization, and data augmentations) and conclude that most models achieve knowledge capacity of 2 bits per parameter regardless of quantization. Wider works on factual memorization. Early empirical studies establish that pretrained LMs encode relational and factual knowledge. Petroni et al. (2019) shows that BERT can answer cloze-style queries competitively with supervised baselines, even without fine-tuning. Radford et al. (2019) further demonstrates that autoregressive models like GPT-2 acquire zero-shot QA abilities through scale alone, with performance improving log-linearly with model size. Subsequent work formalizes these behaviors. Wang et al. (2024a) propose taxonomy of memorization, comprehension, and application, highlighting the fragility of in-weight knowledge. Kadavath et al. (2022) show that, to some extent, LLMs can self-assess their factual correctness via self-evaluation metrics (True) and (I Know). Such estimations are found to improve with model size or with ensembles. These findings inform recent work on confidence modeling and selective generation. Leybzon & Kervadec (2024), the authors track training dynamics and find that factual memorization is highest early and late in training, with mid-training phases marked by forgetting. These observations have implications for curriculum learning and data curation strategies. Finally, Kandpal et al. (2023) links factual recall and the frequency of that information in pretraining corpora. The authors show that models underperform on rare facts, even at large scale, pointing to data coverage bottleneck and motivating retrieval-augmented approaches. Knowledge extraction from LLMs. Prompt design plays central role in retrieving stored knowledge. Jiang et al. (2020) shows that standard close prompts underestimate LLM knowledge and proposes automated prompt mining and paraphrasing to improve extraction accuracy on the LAMA benchmark. Similarly, Allen-Zhu & Li (2025c) demonstrates that without sufficient factual augmentation, stored facts may remain unrecoverable, highlighting gap between memorization and extractability. Both works stress that retrieval performance depends on prompt diversity and internal representations. Learning novel knowledge and avoiding forgetting. LLMs face challenges when integrating new knowledge post-pretraining. The authors of Gekhman et al. (2024) demonstrate that fine-tuning on novel facts is both inefficient and destabilizing: new knowledge is learned slowly. It increases hallucination rates, suggesting that fine-tuning may conflict with pre-trained knowledge. The study in Allen-Zhu & Li (2024) echoes this by showing that only sufficiently augmented knowledge is robustly encoded and extractable. The broader review in Zhang et al. (2023) surveys continual learning methods, emphasizing the need for non-destructive updates that preserve anterior capabilities. Collectively, these works highlight the fragility of in-weight representations when adapting to novel information. Knowledge editing. Another line of research considers the editing of stored knowledge in LLMs via direct mechanistic intervention on the weights, aiming to alleviate the computational burden of retraining. wide range of editing techniques is reviewed by Zhang et al. (2024). Wang et al. (2024b) frame editing techniques in terms of cognitive-inspired mechanisms (external injection, model merging, and intrinsic editing), and introduce unified benchmark (KnowEdit) alongside analyses of knowledge localization. Huang et al. (2025) tackle the core question of whether knowledge editing can correct hallucinations, introducing HalluEditBench and evaluating editing methods along multiple axes such as locality and generalization. Their results underscore both the promise and limitations of editing techniques in practice. Complementarily, Part 3.2 of the Physics of LLMs series (Allen-Zhu & Li, 2025b) highlights deeper structural limitation: even when facts are faithfully stored in the model, their downstream manipulation (e.g., comparison or inverse search) remains brittlesuggesting that editing alone may not grant models flexible reasoning over internalized knowledge. Mechanistic understanding of memorization. series of recent works uncovers the mechanistic basis of how induction heads, which transformers store and recall information, with particular focus on two mechanisms: enable in-context learning by attending to repeated structures within prompt, and associative memories, where weight matrices implicitly store token associations via outer products. Bietti et al. (2023) analyzes simplified training setup, showing that transformers first learn global bigram statistics before gradually developing induction heads that enable pattern completion from local context. They also interpret certain weight matrices as associative memories formed during training. Building on this, Cabannes et al. (2024a) provides theoretical analysis of gradient-based learning in associative memory modules, revealing oscillatory learning dynamics caused by imbalanced token frequencies and correlated embeddings. These oscillations affect how quickly and reliably memories are stored. Cabannes et al. (2024b) derive scaling laws for memory capacity and retrieval reliability, validating them empirically in small transformer models. Collectively, these works offer theoretical and empirical foundation for how memorization emerges through optimization dynamics in LLMs. Tool-use in large language models. Retrieval-augmented generation (RAG) (Lewis et al., 2020a) represents an early and influential line of work that treats external tools (e.g., retrievers) as memory extensions, enabling LLMs to access factual information without storing it parametrically. More broadly, the recent survey (Qu et al., 2025) systematically categorizes tool-use into planning, selection, calling, and response stages, and highlights its promise for improving factuality, compositional reasoning, and generalization. growing number of works explore how models acquire tool-use capabilities. Toolformer (Schick et al., 2023) introduces self-supervised setup where LLMs learn when and how to call APIs (e.g., calculators, search) by generating and filtering synthetic training data. MetaTool (Wang et al., 2024c) introduces benchmark to assess tool-use awareness and selection under realistic agent settings, showing that current LLMs still struggle with tool choice and reasoning under ambiguity. Houliston et al. (2024) offers principled offline preference-based RL algorithm tailored to LLM alignment under ambiguous context by leveraging reward or preference uncertainty estimates - such method could naturally extend to tool learning, where ambiguity in delegation decisions is central challenge. Combining tool use with multi-step reasoning, ART (Paranjape et al., 2023) proposes framework where LLMs write reasoning steps as executable programs that can call tools and integrate their outputs seamlessly. This work illustrates how tool use can serve as functional memory system to complement and augment the limited and potentially brittle parametric memory of LLMs."
        },
        {
            "title": "B Theoretical Details",
            "content": "In this section, we prove the bound from Theorem 4.2 on the number of parameters needed for transformer to learn when and how to query retrieval system. Using the same example and notations as in Section 2, given question of the form the model must output well-formatted string = Where was (cid:125) (cid:124) (cid:123)(cid:122) φ1(a) Thierry de Sienne (cid:124) (cid:123)(cid:122) (cid:125) φ2(n)=n born? , (cid:124) (cid:123)(cid:122) (cid:125) φ3(a) = To answer this request, will make tool-call. <DB> FIND birthplace (cid:125) (cid:124) (cid:123)(cid:122) χ1(a) FOR Thierry de Sienne </DB> , (cid:125) (cid:123)(cid:122) (cid:124) χ2(n) where <DB> and </DB> are special tokens which, when parsed, result in the query being provided to the retrieval system. Let us call this subtask the querying task. The retrieval system then returns string containing the answer to Q. Then, given the context Q, T, ξ(f, a, n), must return well-formatted answer ξ(f, a, n) = Germany, = Thierry de Sienne (cid:125) (cid:124) (cid:123)(cid:122) ψ1(n) was born in (cid:124) (cid:123)(cid:122) (cid:125) ψ2(a) Germany (cid:123)(cid:122) (cid:125) (cid:124) ψ3(v) . Let us call this second subtask the formatting task. We only prove that the querying task, i.e., outputting as function of Q, can be learnt by transformer for which the bounds from Theorem 4.2 apply: similar arguments are enough to prove that the formatting task can be learnt as well (and simultaneously) by such model. Furthermore, we make the following simplifying assumptions: 1. The map φ1 : (cid:55) φ1(a) is injective. 2. The only question mark in each question is at the very end of the string. 3. The string is provided to the models tokenizer in such way that each substring φ1(a), φ2(n), φ3(a) is separately tokenized, i.e., the tokenized version of is such that no token contains characters belonging to several substrings. 4. There does not exist a, such that = but φ1(a) is substring of φ1(a). 5. The number of tokens in the questions and answers does not exceed given constant. The first, second, and third assumptions are reasonable and could be relaxed at the cost of more complex proof. Some variant of the fourth assumption cannot be avoided without additional constraints on the names , as two distinct pairs (a, n), (a, n) could otherwise be mapped to the same question Q. The last condition forces the number of tokens in the names, attribute,s and values to be less than some large number, e.g., 10k. This weak constraint is essentially technicality linked to the limitations of positional embeddings, hence to the context length of any transformer, rather than true algorithmic constraint. Theorem B.1 (Parameter Upper Bound, querying task). Consider finite set of attributes and set of names. Then there exists transformer with at most 8 transformer blocks, an embedding dimension of at most O(A), and total number of parameters at most O(A2), which can achieve perfect accuracy on the querying task, i.e., that outputs as function of for any A, . = χ1(a) χ2(n) = φ1(a) φ2(n) φ3(a) Proof. We focus on the most informative elements of the proof and omit some tedious details. 15 Given transformer with layers and {1, . . . , L} and internal dimension D, we let ak RD denote the input of the feedforward network of the k-th transformer block (i.e. the renormalization of the sum of the output of the multi-head attention and the residual connection), and yk RD denote the output of the k-th transformer block (i.e. the renormalization of the sum of the output of the feedforward network and of the blocks second residual connection, see Figure 8 for an illustration of Transformer block). Figure 8 Architecture of Transformer block, Vaswani et al. (2017). In what follows, we mean by the vector ak above the i-th token of sequence the vector ak found within the k-th layer of the transformer and to which direct line of residual connections starting at the embedding of the i-th token leads (and same for yk). By an encoding of X, for some quantity or object that can take values in some set S, we mean the image of by some injection : Rn. Committing minor abuse of conventions, we say that vector Rm (e.g. ak) contains another vector Rn (e.g. an encoding of some object) if the coordinates are subset of the coordinates of v, i.e. = π(v) for some orthogonal projection along sub-family of the orthogonal basis. By convention, whenever we state that vector contains several vectors u1, . . . , ul, the dimensions in which those vectors are contained do not intersect, i.e. = {u1} . . . {ul} {w} for some vector and up to some permutation of its coordinates. When we say that vector ak or yk above given token contains flag corresponding to certain condition on the token and its predecessors, we mean that one of the coordinates of the vectors (without overlap with the coordinates used to store other information) is equal to 1 if this condition is fulfilled (the flag is raised), and to 0 otherwise. Remember that we must design transformer that outputs the sequence = To answer this request, will make tool-call. <DB> FIND birthplace (cid:125) (cid:124) (cid:123)(cid:122) χ1(a) FOR Thierry de Sienne </DB> , (cid:123)(cid:122) (cid:125) (cid:124) χ2(n) when provided with = Where was (cid:125) (cid:124) (cid:123)(cid:122) φ1(a) Thierry de Sienne (cid:123)(cid:122) (cid:125) (cid:124) φ2(n)=n . born? (cid:124) (cid:123)(cid:122) (cid:125) φ3(a) Concretely and due to the auto-regressive nature of transformers, the model will progressively concatenate tokens to until joint string has been produced. We are going to show that there exists transformer with 8 layers and embedding dimension CA for > 0 large enough such that for any input question = φ1(a) φ2(n) φ3(a) (for some A, ), the following facts hold simultaneously3. Fact: any yk above any token contains an encoding of the absolute position of the token within the entire sequence. The positional encodings, which contain the desired information, are added to the tokens learned embedding to form the input to the models first transformer block. For large enough embedding dimension and judicious choice of learned embeddings, and as long as the number of tokens does not exceed given limit, the absolute position can be recovered by the first transformer block (using the fact that two-layer neural networks are universal approximators and assuming again that the embedding dimension is large enough). This information can then be stored in some of the coordinates of y1, and be passed to all consecutive yk (e.g. using the residual connections and trivial attention heads). Note that the embedding dimension needed does not depend on or . Fact: y3 contains one flag per attribute A. The flags corresponding to = are never raised, and the flag corresponding to is only raised above the last token of φ1(a). At any step, let denote the last token of the currently processed sequence, and let be its absolute position. As stated above, y1 contains an encoding of p. It can also contain an encoding of (using similar arguments). 3In the proof, approximating functions, for instance needed to compute the absolute position, amounts to perfect approximation on finite subset using the fact that the set of tokens is discrete 16 The attention layer of the second transformer can then pass this information to its feedforward network. Using again the approximation properties of neural networks, for each there can be dedicated entry va in the networks output y2 such that: if > len(φ1(a)), then va = 2, if len(φ1(a)) and is not equal to the p-th token of φ1(a), then va = 2. if len(φ1(a)) and is equal to the p-th token of φ1(a), then va = 1/len(φ1(a)) and In the third attention layer, the coordinates va corresponding to each can be summed. If = a, this sum is strictly negative. If = a, then: if < len(φ1(a)), then the sum is positive but strictly smaller than 1, if = len(φ1(a)), the sum is exactly 1, and if > len(φ1(a)), the sum is strictly negative. Hence, all cases are differentiated. The following feedforward network can then output within y3 flag for all A, which is raised only if = and the current token is the last token of φ1(a). As the lengths of the substrings {φ1(a)} are bounded, this can be done with an embedding dimension in O(A). Fact: y1 contains flag that is only raised above the question mark at the end of φ3(a), and another flag that is raised above every token after the question mark. This is essentially trivial (using our second assumption). Fact: y4 contains vector that encodes the length in tokens of above every token after the question mark at the end of φ3(a). The facts above state that y3 contains an encoding of the absolute position of the current token, flag fa corresponding to that is only raised above the last token of φ1(a), flag f? that is only raised above the question mark at the end of φ3(a), and flag f? that is raised above every token after the question mark (these two flags are passed from y1 to y3). At each token, two non-trivial queries are made in the fourth attention layer if and only if f? is raised. The first non-trivial query matches only with key generated from fa and its output value is an encoding of the position of the last token of φ1(a) as well as an encoding of the value of A. The second non-trivial query matches only with key generated from f? and its output value is an encoding of the position of the question mark at the end of φ3(a). The fourth feedforward network then takes these two positional encodings and the identity of A, and outputs (in some subset of the coordinates of y4) the distance between the last token of φ1(a) and the question mark, minus the length in tokens of φ3(a). This is precisely the length in tokens of n. This can be done with an embedding dimension in O(A). Fact: the model successfully outputs χ1(a) FOR after Q. non-trivial query is made in the second attention layer if and only if the flag f? is raised. This query matches the flag f? and brings to y2 an encoding of the position of the question mark. y2 also contains an encoding of the position of the current token, and copy of the flag f?. This information is passed to y3. The fourth attention layer can match the flag above the last token of φ1(a) which describes the value of and pass this value to the fourth feedforward network, along with the encoding of the position of the question mark, the encoding of the position of the current token and the copy of the flag f?. If f? is raised, then the fourth feedforward network can compute the distance between the question mark and the current token, and output an encoding of the l-th token of χ1(a) FOR (or flag saying that no token should be output if is larger than the length of χ1(a) FOR). This can be done with an embedding dimension in O(A). This encoding can then be passed to the following layers, until the correct l-th token is output. Fact: y5 contains flag that is only raised above the last token of χ1(a) FOR. This follows from the same arguments as the previous fact. Fact: above every token after χ1(a) FOR, y6 contains an encoding of the distance between the current token and the last token of χ1(a) FOR. 17 The sixth attention layer can match with the flag from the previous fact to pass its position to the sixth feedforward layer, as well as the position of the current token. These can be processed by the feedforward layer to output the desired distance. Fact: above every token after the last token of χ1(a) FOR, y5 contains an encoding of the distance between the first token of φ2(n) = and the last token of χ1(a) FOR. This distance is equal to the length of plus the length of φ3(a) χ1(a) FOR. The length of is encoded in y4 above every token after the question mark at the end of φ3(a) (using an earlier fact). This quantity can then be passed to the fifth feedforward network. y3 contains flag corresponding to above the last token of φ1(a) (using an earlier fact again), which is raised nowhere else. This quantity can be passed to y4, queried by dedicated attention head in the fifth attention layer, and passed to the fifth feedforward network. As there are finitely many A, the fifth feedforward network can memorize every length φ3(a) χ1(a) FOR with an embedding dimension in O(A). It adds the length of φ3(a)χ1(a)FOR to that of n, and encodes it into y5. Fact: the model successfully outputs after χ1(a) FOR, and y7 contains flag that is only raised above the last token of (this copy of) n. For every token after χ1(a) FOR, the sixth layer can use the encoding of the distance from the previous fact and the encoding of the position of the current token to output an encoding of the position of the token that is entries before the current token. The seventh attention layer can then use this encoding to match the token at position (which is part of n) and get its value t. We know that y6 also contains an encoding of the distance between the current token and the last token of χ1(a) FOR. If is smaller than the length of n, the seventh feedforward network outputs copy of t, which lets the model output t. If is exactly the length of (i.e., the current token is the last token of the copy of n), the seventh feedforward network outputs flag within y7. If is greater than n, the flag is not raised. Hence the model does output after χ1(a)FOR. Fact: the model successfully outputs </DB> after n, and thus successfully outputs in response to Q. The eighth attention layer matches the flag in y7 that is raised above the last token of n. It passes the existence of this flag, the position of the last token of n, and the position of the current token to the eighth feedforward network, which computes the distance between the two positions and outputs an encoding of the d-th token of </DB>, which the model can then output. The combination of these facts is enough to complete the proof of Theorem B.1."
        },
        {
            "title": "C Experimental Details",
            "content": "In this section, we provide details on our experimental setup, specifically regarding the database construction, language modeling pipeline, and training runs. We also provide additional results in both controlled and large-scale settings. Seeds, when provided, are relative to distinct data generation and network initialization, and as such, results are presented with the standard deviation across runs (e.g., we conducted 10 different runs to obtain Figure 2). Reproducibility. The code and reproducibility instructions can be found at https://github.com/ambroiseodt/itl. We provide in the main paper and the appendix all the details needed to reproduce our results. Experiments were conducted using high-performance GPUs such as NVIDIA V100 and A100. Our implementation supports distributed training, but we have also ensured that it can be run on single device. C.1 Factual Recall Database In our experiments, we construct synthetic biographical datasets using fixed list of names (each name is pair {first name-last name}) and four attributes A: birth place, birth date, current address, occupation; which can take 7, 16800, 213, and 100 values respectively. This yields 4N atomic facts per dataset instance. We present in Figure 9 an example of such dataset, where we query the birth date of Kenny McRoy, comparing the in-weight and in-tool settings. We adopt dialog format, with chat template. We design our tokenizer to take care of the special tokens needed (see Appendix C.2). We can see that the in-weight setting leads to single-turn dialog for the LLM while the in-tool one requires multi-turn dialog. # In-weight query { \"dialog\": [ {\"source\": \"user\", \"content\": \"When was Kenny McRoy born?\"}, {\"source\": \"assistant\", \"content\": \"Kenny McRoy was born on 19/05/1998.\"}, ], \"people_id\": 0, \"answer\": \"19/05/1998\", } # In-tool query { \"dialog\": [ {\"source\": \"user\", \"content\": \"When was Kenny McRoy born?\"}, { \"source\": \"assistant\", \"content\": \"To answer this question will make request to the database:n```sqlnFIND birth_date FOR Kenny McRoyn```\", }, {\"source\": \"database\", \"content\": \"19/05/1998\"}, {\"source\": \"assistant\", \"content\": \"Kenny McRoy was born on 19/05/1998.\"}, ], \"people_id\": 0, \"answer\": \"19/05/1998\", } Figure 9 Examples of queries for the in-weight versus and in-tool settings. C.2 Controlled Experiments In our experiments of Section 5, we train small Llama3-style language models (Grattafiori et al., 2024) from scratch. To that end, we build language modeling pipeline following the literature standard (Videau et al., 2024). Below, we provide details on the tokenization, the architecture, and the training setup, alongside further results. Tokenizer. Transformer models (Vaswani et al., 2017), like most neural networks, operate on numerical data, and in particular, sequences of vectors. To perform language modeling, given sequence is mapped to sequence of numerical vectors by first splitting it into words or subwords and then encoding each word or subword into vector. This process is referred to as the tokenization process and is typically performed using tokenizers. Most state-of-the-art language models have an associated pre-trained tokenizer; it is also possible to encode text using their byte representation. Since we pretrain our model from scratch, we considered the latter byte tokenizer to avoid potential bias from pretrained tokenizers. It simply splits words into single-character tokens and maps each of them to their raw byte representation (taking values in [0, 256)). It results in vocabulary space of size 256 for the byte encoding (256 = 28), which can be augmented by adding special tokens. The vocabulary size reaches 260 with the addition of 4 special tokens: <user>, <assistant>, <database> and <eod>. Each plays specific role during the dialog between user and language model assistant: The <user> token marks the beginning of the dialogue by the user, The <assistant> token marks the language model assistant turn, which can either answer directly or make call the the tool, The <database> token marks the beginning of the database result to the request, The <eod> token indicates the end of the dialog. Chat templates. We model the factual recall task as dialog between user and language model assistant. The user asks questions about given person, for instance, their date of birth, birthplace, occupation, or current address. The assistant can then either answer directly using the information contained in its weights (in-weight learning) or make call to tool, implemented in our code as an SQL agent that has access to database of biographies of persons. To decode such interactions, we implement wrapper around the tokenizer to allow for dialog interactions, akin to the HuggingFace chat templates. Online generation of batches. Data is organized into subsets that each represent source of data. It follows the standard in the literature, which accounts for the fact that, in practice, pretraining data aggregates texts from different sources like Wikipedia or arXiv. To optimize data generation, batches are created on the fly by sampling with pre-defined proportions from the different subdirectories. Text is tokenized also on the fly by iterating through each line of the sampled JSONL files. Transformer architecture. Our models are small transformer decoders following the Llama3 implementation (Grattafiori et al., 2024) with Rotational Positional Embedding (Su et al., 2024), Flash Attention (Dao et al., 2022), efficient attention for prefilling and token generation, gated linear units (Dauphin et al., 2017; Shazeer, 2020) with SiLU activation (Hendrycks & Gimpel, 2023), RMSNorm layers (Zhang & Sennrich, 2019), and KV caching (Pope et al., 2022) for the inference. The feed-forward hidden dimension is fixed to 4 times the embedding dimension. For each dataset size, we train family of small Llama3-style transformer models (Grattafiori et al., 2024) with 2 layers, 2 attention heads, vocabulary size of 300, and context window of 257. The embedding dimension ranges from 4 to 128 (to ensure that ROPE constraints between the number of heads and the embedding dimension are met), resulting in models with between 2K and 0.6M parameters. Training. Our models are trained for up to 100,000 steps using the AdamW optimizer (Loshchilov & Hutter, 2019), with batch size of 128 samples, decoupled weight decay coefficient of 0.1, and (β1, β2) = (0.9, 0.95). We use cosine learning rate scheduler with warmup phase of 50 steps and maximum learning rate of 0.001, and final learning rate ratio of 0. Evaluation and inference. At inference time, we need to generate tokens sequentially, which is known to be slow. To optimize it, we make use of the standard practices in the literature, like KV caching. To evaluate the factual recall of our model, we feed it prompts to be completed. We implement additional attention modes, such as prefilling 20 and generation, to decode the dialog and allow interacting with the agent. These operations are parallelized by defining specific prompt loader during evaluation. It efficiently deals with queues of prefilled tokens to generate prompt completions. In our work, the tool is an SQL agent relying on sqlite3 to access database containing Tool implementation. biographies of people with their first name, last name, date of birth, place of birth, occupation, and current address. The language model assistant can learn to make queries to this database thanks to in-tool learning. The SQL agent can be used to create the database and insert new elements in it, and has agentic capabilities not only to execute query and return result but also to parse instructions from an LLM prompt, execute them, and answer in natural language to the language model assistant. In the experiments displayed in Section 5, the recall accuracy is fixed at 95%. To better Further results. understand how it can impact the parameter requirement, we display in Figure 10 the minimum parameter requirement as function of the factual recall for 4 different numbers of facts. We notice that the higher the recall, the bigger the models, and that this increase is steeper when the number of facts to retrieve increases. This can be explained from Theorem 3.2, which makes the dependency between the model size and the number of facts explicit. Given that tool use enables the retrieval of an unbounded number of facts without the need to increase the number of parameters, as shown in Figure 2, this implies that in-tool learning is all the more beneficial when one wants more effective model on bigger dataset. This follows the increasing trends of toll use in the literature for harder benchmarks such as mathematics. Figure 10 Scaling of parameter requirements with the level of factual recall. Minimum number of parameters required as function of the factual recall. The higher the number of facts retrieved, the bigger the models. When the total number of facts to retrieve increases, we observe that the parameter requirement increases too. This shows that in-tool learning is all the more beneficial when very effective models are needed on large databases. 21 C.3 Large Scale Experiments In this section, we provide additional results related to Section 6. Figure 11 HellaSwag performance (relative to base model) versus memorization load. Same setup as Figure 5; the dashed line represents the worst (lowest) performance among tool-models (Smol-135M). Figure 12 Total Variation versus memorization load. Same setup as Figure 6; the dashed line represents the worst (highest) TV distance attained among tool-models (Smol-135M). HellaSwag performance relative to base model. Figure 5 in the main section of the paper displays the absolute HellaSwag accuracy attained by different models finetuned on datasets of increasing size. To complement this, we examine the relative drop in HellaSwag performance from the base model as the memorization load increases, in Figure 11. This perspective offers clearer view of the dynamics by which larger models maintain their prior capabilities under heavier memorization loads. Tool-learning (dashed line representing the worst performance over all tool-models) preserves general capabilities almost perfectly across model and dataset sizes. In contrast, in-weight learning leads to noticeable degradation, especially for smaller models and larger factual loads. This supports our theoretical prediction that parameter-based memorization causes interference and loss of prior capabilities, due to finite capacity and update interference. Larger models are more robust to such forgetting, but still exhibit performance decay beyond 10K facts. simplified intuition helps interpret the consistent ordering across model sizes: larger models possess more \"capacity volume,\" so absorbing fixed amount of new information replaces smaller fraction of prior knowledge. While this view captures the trend, it likely omits key confounding factors: larger models are typically trained with more compute and data, and more complex architectures may also be more brittle to local updatessuggesting the optimization dynamics themselves differ in important ways. Total variation versus memorization load. The intuition laid out above is also observed in the TV distance suffered by models as the memorization load grows: larger models diverge less than smaller counterparts. Figure 12 depicts the Total Variation of finetuned models from their base-model, for different memorization loads. The figure shows the same results as Figure 6 for in-weight models but replaces in-tool data by the highest (worst) TV achieved (by SmolLM 135M), represented as the dashed baseline. We notice in-tool TV baseline is constant across dataset sizes, and is lower than in-weight learning deviations - this is due to tool-use mastery unlocking unbounded factual recall. Overall, bigger models deviate less from their base model for the same in-weight memorization load. The distance is dramatically greater for smaller models on bigger datasets close to 50k facts. Training dynamics. For databases of 500K and 50K facts, we display the recall accuracy, the HellaSwag performance, and the total variation attained by checkpoints throughout training until complete memorization in Figures 13 and 14. Across model scales, we observe that recall accuracy improves rapidly during early training, with most models nearing saturation within 3040 steps. However, substantial number of additional steps are spent refining the final few percent of recall, suggesting that later training primarily sharpens the output distribution 22 rather than acquiring new associations. Notably, most of the degradation in HellaSwag performance and increase in total variation occur early in training, indicating that the bulk of distributional shift and capability loss is incurred in the initial stage of training for memorization, while later steps have comparatively minor effects. Figure 13 Metrics throughout training on database of 500 facts. The dashed line represents the worst in-tool learning baseline, highlighting that tool use attains full recall while maintaining very high ( 98%) level of prior capabilities on HellaSwag, and by deviating less than 0.04 in TV from its base model. Figure 14 Metrics throughout training on dataset of 50K facts. The worst in-tool model (dashed) mastered the tool in very few training steps, conserving higher HellaSwag capabilities and deviating less from its base model."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "FAIR at Meta",
        "Inria, Univ. Rennes 2 and IRISA"
    ]
}