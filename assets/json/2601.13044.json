{
    "paper_title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
    "authors": [
        "Warit Sirichotedumrong",
        "Adisai Na-Thalang",
        "Potsawee Manakul",
        "Pittawat Taveekitworachai",
        "Sittipong Sripaisarnmongkol",
        "Kunat Pipatanakul"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 4 4 0 3 1 . 1 0 6 2 : r Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition Warit Sirichotedumrong, Adisai Na-Thalang, Potsawee Manakul, Pittawat Taveekitworachai, Sittipong Sripaisarnmongkol, Kunat Pipatanakul Typhoon, SCB 10X"
        },
        {
            "title": "Abstract",
            "content": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving critical gap in eﬀicient streaming solutions. We present Typhoon ASR Real-time, 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves 45 reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcriptionincluding context-dependent number verbalization and repetition markers (ๆ; mai yamok)creating consistent training targets. We further introduce two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community. Project Resources: Code: https://github.com/scb-10x/typhoon-asr Typhoon ASR Real-time: https://huggingface.co/typhoon-ai/typhoon-asr-realtime Website: https://opentyphoon.ai/model/typhoon-asr-realtime"
        },
        {
            "title": "Introduction",
            "content": "Automatic Speech Recognition (ASR) has experienced transformative progress in recent years, driven by advances in self-supervised learning and large-scale multilingual modeling. Models such as Wav2Vec2 (Baevski et al., 2020) and XLS-R (Babu et al., 2021) demonstrated the power of pre-training on massive unlabeled audio corpora, while OpenAIs Whisper (Radford et al., 2023) established new benchmarks for transcription accuracy across dozens of languages. However, despite these achievements, state-of-the-art ASR systems face critical limitations when deployed in production environments requiring real-time streaming, particularly for morphologically complex and low-resource languages like Thai. Production-Grade ASR Demands Beyond Accuracy. While encoder-decoder architectures like Whisper excel at offline transcription accuracy, they are fundamentally not well-suited for low-latency streaming applications. The autoregressive decoding process introduces unpredictable latency, and these models are prone to hallucinationsgenerating plausible but incorrect outputs, particularly for numbers, dates, and domain-specific terminology. For Thai, tonal language without explicit word boundaries, these issues are 1 Figure 1 Pareto eﬀiciency of Thai ASR models. The scatter plot illustrates the trade-off between computational cost (x-axis, GFLOPs per 30s audio segment on logarithmic scale) and average Character Error Rate (y-axis) across three benchmarks. Our proposed streaming models, Typhoon ASR Realtime and Typhoon Isan ASR Realtime, occupy the optimal lower-left region (highlighted), demonstrating comparable accuracy to the offline state-of-the-art Pathumma-Whisper Large-v3 baseline while achieving an approximate 45 reduction in computational complexity. compounded by unique linguistic challenges: ambiguous number representations, context-dependent repetition markers, and inconsistent handling of loanwords. These problems are not adequately addressed by model architecture alone; they require systematic data preparation. Thai ASR Challenge. Thai presents distinct diﬀiculties for ASR systems. As low-resource language relative to English or Mandarin, available training data is limited and often inconsistently annotated. The Thai writing system does not use spaces to delimit words, making tokenization non-trivial and rendering standard metrics like Word Error Rate inapplicable. Moreover, the same written form can correspond to multiple spoken realizations: the digits 10150 might be read as postal code (หน งหาศนย; nueng sun nueng ha sun) or as quantity (หน งรอยหาสบ; nueng muen nueng roi ha sip). Without careful งหม normalization, this inherent ambiguity introduces noise into the training process, leading to unreliable model behavior. งศนยหน นหน Our Approach: Data Quality and Consistency as First-Class Principle. We argue that for production-ready ASR, particularly in low-resource settings, data quality and consistency are as important as model architecture. While existing work focuses primarily on scaling model parameters or training data volume, we demonstrate that systematic data curationspecifically, comprehensive text normalizationis essential for building robust, reliable systems. In this work, we introduce Typhoon ASR Realtime1, FastConformer-Transducer model optimized for streaming Thai speech recognition. While Transformer-based encoder-decoder architectures (e.g., Whisper) currently dominate the Thai ASR landscape, we argue that the Conformer familydespite its proven eﬀiciency 1https://huggingface.co/typhoon-ai/typhoon-asr-realtime in streaming scenariosremains significantly under-explored and under-utilized for Thai. Our approach revisits this high-potential architecture, combining its superior real-time inference capabilities with rigorous data preparation pipeline to unlock performance that rivals massive offline models. We make the following contributions: Instead of Whisper-based model, we chose the FastConformer-Transducer, specifically engineered for low-latency streaming applications with consistent, hallucination-free outputs. scalable semi-supervised data pipeline that leverages consensus from multiple teacher models to curate 11,000 hours of training data with minimal human intervention, overcoming the scarcity of large-scale labeled Thai corpora. comprehensive text normalization pipeline that resolves systemic ambiguities in Thai transcription, creating canonical training target. multi-stage dialect adaptation strategy that extends the model to the Isan (north-eastern) dialect while mitigating catastrophic forgetting of Central Thai. Typhoon ASR Benchmark and TVSpeech that address the reproducibility crisis and metric instability in Thai ASR by introducing standardized evaluation protocols. Gigaspeech2-Typhoon provides rigorous, canonically normalized target to resolve the orthographic ambiguities prevalent in public datasets, ensuring metrics reflect phonetic accuracy rather than formatting luck. Complementing this, TVSpeech introduces challenging, manually curated dataset to bridge the gap between academic clean speech and the acoustically complex, real-world environments encountered in production."
        },
        {
            "title": "2 Data Curation and Normalization",
            "content": "A core principle of our models is that the quality and consistency of training data are paramount. We identified that raw transcripts from public corpora are often inadequate for high-fidelity ASR. To address this, we developed unified data strategy comprising strict normalization pipeline, consensus-based transcription system with human verification, and two distinct data mixtures for general and dialect-specific training. 2.1 Consensus-Based Transcription Pipeline for Pseudo labeling While high-quality data are essential, creating labeled data from scratch is challenging, as modern ASR models require transcription at scale. For example, Whisper was trained on more than 500k hours of speech (Radford et al., 2023), which is not feasible in our setting. Therefore, we must leverage pseudo-labeling to address this limitation. To ensure high-quality training data at scale, we developed consensus-based audio transcription and verification pipeline illustrated in Figure 2. The pipeline employs parallel labeling approach where raw audio files are simultaneously processed by three Thai Whisper-Large models: Pathumma-Whisper-Large (Tipaksorn et al., 2024), Biodatlab-Distill-Whisper-Large (Aung et al., 2024), and our Internal-Whisper-Large. The system implements majority voting strategy at the comparison stage. When at least two models produce identical transcriptions, that consensus output is selected and proceeds to verification. In cases where no majority agreement exists among the three models, the system defaults to the Pathumma-Whisper-Large transcription as the authoritative output due to its established high performance. All selected transcriptions undergo an automated complexity verification step that checks for Arabic numerals or special punctuation. Transcriptions flagged as complex are routed to human-in-the-loop review for manual correction, ensuring adherence to our strict normalization rules described in Section 2.2. Clean transcriptions bypass manual review and proceed directly to the final transcription database. This hybrid approach balances eﬀiciency with quality assurance, maintaining consistency across our training corpus while minimizing human annotation overhead. 3 Figure 2 The Consensus Audio Transcription and Verification Pipeline. Raw audio is processed in parallel by three Whisper-Large models. majority voting strategy selects consensus transcriptions, defaulting to PathummaWhisper-Large when no agreement exists. Complex transcriptions undergo human review, while clean outputs proceed directly to storage. 2.2 Text Normalization Pipeline Consistent orthography is prerequisite for robust ASR. We developed normalization pipeline that transforms text into canonical representation matching the spoken form. Our pipeline follows the Thai transcription guidelines proposed by Na-Thalang et al. (2025), ensuring linguistic consistency between training targets and evaluation benchmarks. The primary normalization rules and the ambiguities they resolve are detailed in Table 1. 2.3 General Training Data Our primary training dataset comprises approximately 11,000 hours of Thai audio. To achieve both broad acoustic coverage and precise output formatting, we constructed composite dataset that balances massivescale public corpora with targeted internal datasets. As detailed in Table 2, the data mixture is designed to address specific modeling requirements: Foundation (Scale & Diversity): We rely on Gigaspeech2 as the backbone of our training, providing over 10,000 hours of diverse acoustic environments to learn robust speech features. Robustness & Benchmarking: We incorporate an Internal Curated Media datasetsourced from publicly available content but processed internallyto enhance conversational robustness, alongside Common Voice 17.0 (Ardila et al., 2020) to ensure performance on standard open benchmarks. 4 Table 1 Examples of Text Normalization Rules and Ambiguities"
        },
        {
            "title": "Ambiguous Interpretation",
            "content": "Normalized Target (Canonical Form)"
        },
        {
            "title": "Number\nStandardization",
            "content": "10150 Spoken as quantity: muen... (ten thousand...) nueng งศนยหน งหาศนย หน (nueng sun nueng ha sun) ไขเปดเบอรศนยส ฟอง Written with digit: ไขเปดเบอร 04 ฟอง (mixed format) ไขเปดเบอรศนยส (khai pet boe sun si fong) ฟอง ตสามสบส นาท Written as ต 34 นาท (implying 3:14 AM) ตสามสบส นาท (ti sam sip si nathi) เกงๆ N/A (simple repetition) เกง เกง (keng keng) เปนอยางๆ Incorrect full-phrase repeat: pen yang pen yang เปน อยาง อยาง (pen yang yang) ContextAware Repetition (ๆ; mai yamok) Ambiguous Symbol 6-7 (range) Read as subtraction: hok lop chet 6-7 (minus) Read as range: hok thueng chet 6-7 (separator) Read as range: hok thueng chet Foreign Words website Inconsistent transliterations. หกถงเจด (hok thueng chet) หกลบเจด (hok lop chet) หกขดเจด (hok khit chet) เวบไซต (wep-sai) Normalization Enforcement: While large public datasets provide breadth, they often lack consistent text normalization. To address this, we integrated Internal TTS (Orpheus) data specifically generated with complex numeric sequences. This small but critical subset enforces strict adherence to our normalization standards, preventing common hallucinations in number and date transcription. Table 2 General Training Data Composition. The mixture balances large-scale public corpora with internal curated sets for robustness and precision. Data Source Specific Focus Hours Utterances Gigaspeech2 Internal Curated Media Conversational Robustness Common Voice 17.0 Internal TTS (Orpheus) Numeric Normalization Read Speech Benchmark Large-scale Acoustic Diversity Total 2.4 Isan Dialect Adaptation Data 10,329.5 631.0 35.4 3.2 9,843,999 93,879 31,312 2,697 10,999. 9,971,887 For the dialect adaptation stage, we curated specific dataset of approximately 303 hours. Unlike the large-scale general corpus, every utterance in this adaptation set is gold-standard, humanverified transcription. We designed this mixture not only to introduce Isan acoustic features but also to maintain robustness in Central Thai and enforce strict output formatting. As detailed in Table 3, the data is strategically split between Isan and General Thai sources: Isan Sources (Dialect Signal): We utilize 28 hours of our Internal Isan Dataset to capture broad acoustic variety in the dialect, combined with 28 hours from the Public Isan Dataset (SCB 10X) to 5 cover general and finance-specific domains. General Thai Sources (Regularization & Formatting): To prevent catastrophic forgetting and enforce normalization rules, we include 184 hours of Internal Curated Public Media (General Conversation) for acoustic breadth. We further augment this with 62 hours of Internal TTS focused on numeric sequences and targeted Public Subset for repetition markers (ๆ; mai yamok). These Central Thai anchors ensure the model retains its ability to handle complex formatting even as it adapts to the new dialect. Table 3 Isan Dialect Adaptation Data Mixture. gold-standard mixture of Isan data (for dialect learning) and General Thai data (for regularization and formatting consistency). Data Source Language Specific Focus Hours Utterances Internal Curated Media Central Thai General Conversation / Acoustic Variety Internal Isan Dataset Isan Public Dataset Internal TTS Public Subsets Isan Isan Central Thai Numeric Formatting Central Thai Repetition Markers (ๆ; mai yamok) Acoustic Variety Recorded General & Finance Domain Total"
        },
        {
            "title": "3 Model Architecture and Training Strategy",
            "content": "3.1 Model Architecture 184.25 28.13 27.80 62.48 0.33 25,650 31,707 9,987 23,752 106 302.99 91,202 To address the inherent latency and computational bottlenecks of autoregressive models like Whisper (Radford et al., 2023), we employ the FastConformer-Transducer architecture (Rekesh et al., 2023; NVIDIA, 2025). While Whisper requires processing padded 30-second chunks, our model utilizes streaming-optimized FastConformer encoder with an 8 depthwise convolutional subsampling layer (256 channels) and reduced kernel sizes. This aggressive downsampling results in an encoder approximately 2.4 faster than standard Conformer, enabling eﬀicient frame-synchronous streaming while local attention mechanisms ensure stability on long-form audio. 3.2 General Thai Model Training We fine-tuned all model parameters, initializing from pre-trained English FastConformer-Transducer (Large) model. Training was conducted for single epoch on the 11,000-hour General Training Data (Section 2.3) using 2NVIDIA H100 GPUs with the hyperparameters specified in Table 4. The entire fine-tuning process was completed in approximately 17 hours, demonstrating the eﬀiciency of the FastConformer architecture even on massive datasets. Table 4 Training Hyperparameters Hyperparameter Value Optimizer Learning Rate Schedule Cosine annealing (peak 0.001) 5,000 Warmup Steps 128 Effective Batch Size AdamW 3.3 Curriculum Learning for Dialect Adaptation To adapt the model to Isan dialect without catastrophic forgetting of Central Thai, we employ two-stage curriculum on the 303-hour Isan adaptation dataset (Section 2.4), as illustrated in Figure 3. 6 Stage 1 (Global Adaptation): Full-model fine-tuning for 10 epochs with conservative learning rate (η = 105). This gently adjusts the encoders acoustic filter banks to capture Isan-specific tonal variations without overwriting robust features learned during Central Thai pre-training. Stage 2 (Linguistic Adaptation): We freeze the encoder and fine-tune only the decoder and joint network for 15 epochs with higher learning rate (η = 103). This stage focuses on learning Isan lexical structures and dialect-specific particles (e.g., บ [bo; no], เฮด [het; do]) while relying on the stable acoustic representations from Stage 1. Figure 3 Two-stage curriculum learning for dialect adaptation. Stage 1 employs low learning rate (105) for gentle acoustic adaptation of the full model over 10 epochs. Stage 2 freezes the encoder (preserving acoustic stability) and employs high learning rate (103) for rapid linguistic specialization of the decoder and joint network over 15 epochs."
        },
        {
            "title": "4 General Thai Evaluation",
            "content": "To evaluate the models performance on standard Central Thai, we utilize the Typhoon ASR Benchmark. This assessment focuses on two critical dimensions: academic accuracy on clean speech and robustness in real-world environments. 4.1 Typhoon ASR Benchmark To establish reproducible standard for Thai speech recognition, we introduce Typhoon ASR Benchmark. This benchmark unifies evaluation across two distinct axes: academic strictness and real-world robustness, and both datasets are publicly released for community use. Crucially, to ensure deterministic evaluation, all ground truth transcripts in this benchmark have been renormalized to strictly follow the transcription guidelines proposed in (Na-Thalang et al., 2025). This resolves systemic ambiguities (e.g., floating-point pronunciations, repetition markers) that plague existing public leaderboards. Statistics are detailed in Table 5. Remark on Standardization and Fairness We acknowledge that applying strict normalization to test sets impacts the absolute scoring of off-the-shelf baselines, which may have been trained on different orthographic conventions (e.g., spelling out numbers vs. using digits). However, we argue that the current lack of standardization in Thai ASR presents greater threat to evaluation validity. Without canonical target, metrics like character error rate (CER) often penalize models for valid stylistic choices (e.g., 10 vs. สบ [sip]) rather than phonetic errors. By enforcing the standardized guideline from Na-Thalang et al. (2025), we shift the evaluation focus from formatting luck to phonetic accuracy. We encourage future research to adopt this canonical form to ensure consistent, comparable benchmarking across the community. 7 4.1.1 Standard Track: Gigaspeech2-Typhoon Standard public evaluation on Gigaspeech2 (Yang et al., 2024) often suffers from normalization variance. We define the Standard Track using fixed subset of 1,000 utterances (1.01 hours), released as Gigaspeech2Typhoon (Typhoon Team, SCB 10X, 2026b). This subset uses our canonical normalization to serve as stable baseline for clean speech accuracy, ensuring that evaluation focuses on phonetic accuracy rather than formatting inconsistencies. Table 5 The Typhoon ASR Benchmark. The benchmark comprises two tracks to evaluate both clean-read accuracy and real-world robustness."
        },
        {
            "title": "Dataset",
            "content": "Hours Utts Diﬀiculty Standard Robustness TVSpeech (Typhoon Team, SCB 10X, 2026a) Gigaspeech2-Typhoon (Typhoon Team, SCB 10X, 2026b) 1.01 3.75 1,000 Moderate"
        },
        {
            "title": "Hard",
            "content": "4.1.2 Robustness Track: TVSpeech To measure performance in in-the-wild conditions, we release the Robustness Track via TVSpeech (Thai Video Speech) (Typhoon Team, SCB 10X, 2026a). This dataset consists of 570 utterances (3.75 hours) curated from diverse public media channels on YouTube under the Creative Commons Attribution (CC-BY) license. Selection Criteria. Unlike random sampling, this test set was manually curated to maximize both acoustic and semantic complexity. Annotators prioritized segments with high lexical densityspecifically selecting clips containing domain-specific terminology, proper names, and technical jargon across categories such as Finance, Technology, and Variety Vlogs. This selection strategy ensures the model is evaluated not just on background noise robustness, but on its ability to accurately resolve low-frequency entities without relying on excessive language model hallucination. 4.2 Main Results Table 6 compares Typhoon ASR Realtime against offline baselines and foundation models. Competitive Accuracy with High Eﬀiciency: Typhoon ASR Realtime achieves 6.81% CER on Gigaspeech2-Typhoon, delivering accuracy comparable to the offline Pathumma-Whisper Large-v3 (5.84%) despite being 45 more computationally eﬀicient. On the Thai subset of the FLEURS benchmark (Conneau et al., 2023), our strict normalization strategy results in higher CER (13.87%) compared to baselines. This is an expected artifact of orthographic mismatch (e.g., our model outputs สบ [sip] while FLEURS expects 10), rather than phonetic error. Dialect Model Generalization: The Typhoon Isan2 model achieves the best performance across both general Thai datasets, demonstrating that our curriculum learning strategy improves general robustness. Foundation Model Variance: Gemini 3 Pro underperforms on our strictly normalized benchmarks (10.95% on TVSpeech, 12.50% on Gigaspeech2). This gap highlights divergence in objective: while general-purpose foundation models prioritize semantic coherence, they often fail to adhere to the rigid, verbatim transcription standards required for high-precision ASR evaluation. Validation of Data Pipeline Superiority To isolate the impact of our data curation strategy from architectural differences, we trained two offline baselines on our dataset: Typhoon Whisper Large-v3 (Typhoon Team, SCB 10X, 2026c) and Typhoon Whisper Turbo (Typhoon Team, SCB 10X, 2026d). direct comparison with Pathumma-Whisper Largev3which shares the exact same model architecture but utilizes different training corpusisolates the specific contribution of our data pipeline. 2https://huggingface.co/typhoon-ai/typhoon-isan-asr-realtime Table 6 Impact of Data Quality on Model Performance. Comparisons show two distinct advantages: 1) Architecture: Our Streaming model competes with offline baselines. 2) Data Pipeline: When training the exact same architecture (Whisper Large-v3), our data pipeline improves performance by over 4% absolute CER on noisy data (TVSpeech) compared to the state-of-the-art Pathumma baseline. For Fleurs, values in parentheses denote CER evaluated against references normalized via our guidelines, highlighting that high baselines often stem from formatting mismatches rather than phonetic errors. Type Model CER (%) TVSpeech Gigaspeech Fleurs Orig. (Norm.) Proprietary Foundation Models Offline Gemini 3 Pro Open-Source Offline Baselines Offline Offline Offline Biodatlab Whisper Large Biodatlab Distil-Whisper Large Pathumma-Whisper Large-v3 Ours (Typhoon Data Pipeline) Streaming Streaming Typhoon ASR Realtime Typhoon Isan ASR Realtime Offline Offline Typhoon Whisper Turbo Typhoon Whisper Large-v3 10.95 18.96 13.82 10.36 9.99 9. 6.85 6.32 12.50 13.22 8.24 5.84 6.81 6.93 4.79 4.69 11.35 (6.91) 16.50 (15.26) 6.77 (8.63) 6.29 (7.88) 13.87 (9.68) 14.55 (10.15) 10.52 (7.08) 9.98 (5.69) As shown in Table 6, our data pipeline yields substantial improvements across both benchmark tracks: Standard Track (Gigaspeech2): We reduce CER from 5.84% to 4.69%, representing relative error reduction of 20% on the general academic baseline. Robustness Track (TVSpeech): The advantage is most pronounced in challenging, in-the-wild environments, where we reduce CER from 10.36% to 6.32%. This 4.04% absolute reduction confirms that our consensus-based labeling and strict normalization (Section 2) significantly enhance model robustness against background noise and overlapping speech, independent of the underlying neural architecture. The Impact of Normalization on FLEURS: On the FLEURS benchmark, our models initially show higher CER (e.g., 9.98% for Typhoon Whisper Large-v3) compared to baselines. However, this is an artifact of orthographic mismatchour models output spoken forms (e.g., สบ [sip]) while the benchmark expects digits (10). When we evaluate against references normalized to our canonical guideline (shown in parentheses in Table 6), our Typhoon Whisper Large-v3 achieves 5.69% of CER, effectively outperforming both the open-source baselines and the proprietary Gemini 3 Pro (6.91%). This confirms that the perceived gap is stylistic, not phonetic. 4.3 Architectural Efficiency and Streaming Capability critical advantage of Typhoon ASR Realtime is its structural suitability for live applications. While offline models maximize accuracy via bidirectional context, our system prioritizes latency and throughput. FastConformer-Transducer. We adopt the FastConformer-Transducer architecture (Rekesh et al., 2023), an industry standard for eﬀicient streaming ASR. By utilizing aggressive downsampling in the encoder frontend, this architecture significantly reduces the sequence length passed to attention layers, lowering computational cost (FLOPs) compared to standard Conformers (Gulati et al., 2020). Furthermore, our use of Transducer (RNN-T) decoder (Graves, 2012) enables true streaming inference, avoiding the latency bottlenecks inherent in the autoregressive decoding of Whisper-based models. Parameter Eﬀiciency. Typhoon ASR Realtime contains only 115 million parameters. In contrast, the 9 state-of-the-art offline baseline, Pathumma-Whisper Large-v3, utilizes approximately 1.55 billion parameters. Our model is roughly 13 smaller, offering massive reduction in memory footprint suitable for resource-constrained deployment."
        },
        {
            "title": "5 Isan Dialect Evaluation",
            "content": "To rigorously assess the models capabilities on the Isan dialect, we conducted both quantitative benchmarking (CER) and qualitative human evaluation (A/B testing). 5.1 Quantitative Results (Isan Benchmark) We evaluated performance on held-out Isan test set derived from the SCB 10X Thai Dialect Isan Dataset (Typhoon Team, SCB 10X, 2025a). Table 7 compares Typhoon models against external baselines and Gemini 2.5 Pro. To ensure fair comparison against available open resources, we included Whisper-Medium-Dialect, baseline we trained specifically on the combined public dialect corpora from SLSCU (Suwanbandit et al., 2023) and LOTUS-TRD (Thatphithakkul et al., 2024). Table 7 Isan Dialect Character Error Rate (CER). Offline models generally outperform streaming architectures. The ablation shows that Stage 2 (Linguistic Specialization) is critical, yielding 5.57% improvement over Stage 1. Model Type Model Name Foundation Gemini 2.5 Pro Offline Whisper-Medium-Dialect SLSCU Korat Model (Suwanbandit et al., 2023) Typhoon-Whisper-Medium-Isan (Typhoon Team, SCB 10X, 2025b) Streaming Typhoon Isan ASR Realtime (Stage 1: Acoustic) Typhoon Isan ASR Realtime (Stage 2: Final) CER (%) 10.20 17.72 70.08 8.85 16.22 10.65 Results Analysis: Offline vs. Streaming Gap: The offline Typhoon-Whisper-Medium-Isan achieves the lowest CER (8.85%), benefiting from the full-context attention mechanism. Competitive Streaming Performance: Our Typhoon Isan ASR Realtime achieves 10.65% CER. While slightly higher than the offline counterpart, it significantly outperforms the Whisper-Medium-Dialect baseline (17.72%)which was trained on the standard public datasets (Suwanbandit et al., 2023; Thatphithakkul et al., 2024)demonstrating the superiority of our curriculum learning strategy. Ablation Analysis: Impact of Curriculum Learning The step-wise improvement in Table 7 validates our two-stage adaptation strategy (Section 3.3). Stage 1 (Acoustic Adaptation): Adapting the encoder alone yields CER of 16.22%. While this captures the tonal distinctiveness of Isan, the model lacks the dialect-specific vocabulary required for accurate transcription. Stage 2 (Linguistic Specialization): Freezing the encoder and aggressively fine-tuning the decoder/ joint network provides massive 5.57% absolute reduction in CER (16.22% 10.65%). This confirms that for dialect adaptation, adjusting the language modeling components is as critical as acoustic alignment. 5.2 Qualitative Human Evaluation Given the high morphological variance in Isan, CER does not always correlate perfectly with usability. We further validated these results via human A/B testing. 10 5.2.1 Protocol and Agreement We compared the same systems against Gemini 2.5 Pro (reference) on 500 Isan audio samples. Two native Isan speakers performed blind pairwise comparisons (2,000 total judgments) with moderate inter-annotator agreement (Cohens Kappa = 0.56). 5.2.2 Comparative Results Figure 4 A/B testing results showing Win-Tie-Loss counts for Gemini 2.5 Pro against various competitor systems (N=1000 comparisons). The dashed red line marks the 500-count (50%) threshold. Results (Figure 4) show: Gemini Dominance: Gemini consistently crosses the 50% win threshold against all competitors, likely due to massive pre-training allowing it to infer semantics even when acoustic signals are ambiguous. Typhoon Superiority among ASR: Among dedicated ASR systems, our Typhoon Isan models secure the highest combined number of wins and ties. Baseline Failure: The external SLSCU baseline registers negligible wins, aligning with its high CER (70.08%) observed in Table 7. 5.3 Analysis: Divergence Between Metrics and Human Preference Our results highlight divergence between automated metrics and human preference. While Gemini 2.5 Pro and Typhoon Isan Realtime have nearly identical CER (10.20% vs 10.65%), Gemini wins more often in human preference. This suggests that Foundation Models optimize for semantic coherence (readability), whereas ASR models optimize for phonetic fidelity. However, for 115M parameter streaming model to perform within 0.45% CER of massive Foundation Model demonstrates the effectiveness of our dialect adaptation curriculum."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "While Typhoon ASR Realtime demonstrates that compact, data-centric models can outperform large-scale offline baselines, we acknowledge specific limitations inherent to our design choices and scope. 11 6.1 Limitations Orthographic Rigidity vs. Readability. Our strict normalization pipeline (Section 2.2) prioritizes phonetic fidelity over stylistic readability. For example, the model is trained to transcribe English terms into Thai transliteration (e.g., เวบไซต [wep-sai] instead of website) and numbers into spoken words (e.g., หน ง ศนย [nueng sun] instead of 10). While this eliminates ambiguity for the acoustic model, the raw output may not be immediately suitable for end-user display without post-processing. Limited Code-Switching Support. The current model is optimized for Thai-dominant speech. In scenarios involving heavy Thai-English code-switchingcommon in technical and corporate environmentsthe model forces phonetic mapping to Thai characters rather than switching to Latin script. This limits its immediate utility in bilingual domains where mixed-language orthography is preferred. The Semantic Gap. As observed in the Isan evaluation (Section 5.3), our 115M-parameter model relies primarily on acoustic cues. Unlike massive foundation models (e.g., Gemini), it lacks the deep world knowledge required to resolve complex semantic ambiguities or context-dependent homophones (คาพองเสยง; kham phong siang) when the acoustic signal is unclear. 6.2 Future Directions Inverse Text Normalization (ITN). To bridge the gap between phonetic transcription and user-facing applications, future work should develop robust ITN systems that convert spoken-form Thai (e.g., หน งศนย งหาศนย [nueng sun nueng ha sun]) back into written formats (e.g., postal codes, dates, currency) based on หน contextual cues. Contextual Biasing for Domain Adaptation. Real-world applications often require recognition of domain-specific vocabulary (e.g., proper names, technical terms, organizational entities) that are underrepresented in training data. Exploring shallow fusion or contextual biasing techniques could enable runtime adaptation without full model retraining. Handling Multi-Speaker Scenarios. Production deployments require integration with speaker diarization systems to attribute transcriptions in multi-party conversations. Additionally, addressing overlapping speech remains an open challenge for the Thai ASR community. Thai Dialect Coverage. The success of our Isan adaptation curriculum (Section 5) suggests scalable path for extending coverage to other regional variants, including Northern (Kam Mueang) and Southern (Pak Tai) dialects. Investigating unified models capable of zero-shot dialect identification and adaptation would democratize access to voice technology across Thailand. On-Device Deployment. Given our models parameter eﬀiciency (115M), exploring quantization (INT8/ INT4) and on-device optimization (ONNX, CoreML) could enable privacy-preserving, offline Thai ASR on mobile and IoT devices."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced Typhoon ASR Realtime, streaming ASR system that challenges the trend of parameter scaling by prioritizing data quality and architectural eﬀiciency. With only 115M parameters, our model outperforms offline baselines 13 its size on standard benchmarks, validating our hypothesis that rigorous normalization is essential for low-resource languages. Our investigation into Isan dialect adaptation revealed critical divergence between automated metrics and human preference: while Foundation Models like Gemini excel at semantic coherence (high human preference), they struggle with the verbatim precision required for low-CER transcription. Typhoon ASR bridges this gap, offering the phonetic fidelity required for technical ASR tasks with the speed necessary for real-time deployment. We release the Typhoon ASR Benchmark, gold-standard human-labeled dataset with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community."
        },
        {
            "title": "Acknowledgments",
            "content": "Beyond the primary authors, we gratefully acknowledge the Typhoon Team members at SCB 10X whose contributions made this project possible: Chanakan Wittayasakpan, Kritsadha Phatcharoen, Sirinya Chaiophat, Surapon Nonesung, Natapong Nitarach, Tanawin Samutsin, Shah Faisal Wani, Krisanapong Jirayoot, Oravee Smithiphol, Kasima Tharnpipitchai, and Kaweewut Temphuwapat. We also extend our appreciation to the SCBx R&D Team for their support, resources, and valuable insights. Lastly, we are grateful to the global and local AI communities for open-sourcing resources and sharing knowledge."
        },
        {
            "title": "References",
            "content": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 42184222, 2020. Zaw Htet Aung, Thanachot Thavornmongkol, Atirut Boribalburephan, Vittavas Tangsriworakan, Knot Pipatsrisawat, and Titipat Achakulvisut. Thonburian whisper: Robust fine-tuned and distilled whisper for Thai. In Mourad Abbas and Abed Alhakim Freihat, editors, Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024), pages 149156, Trento, October 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.icnlsp-1.17. Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. Xls-r: Self-supervised cross-lingual speech representation learning at scale, 2021. URL https://arxiv.org/abs/2111.09296. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12449 12460. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf. Alexis Conneau, Min Ma, Simran Khan, Yu Zhang, Ankur Bapna, Chung-Cheng Boito, and . Fleurs: Few-shot learning evaluation of universal representations of speech. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18, 2023. Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Yulin Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. In Interspeech 2020, 2020. Adisai Na-Thalang, Chanakan Wittayasakpan, Kritsadha Phatcharoen, and Supakit Buakaw. Developing an open conversational speech corpus for the isan language, 2025. URL https://arxiv.org/abs/2511.21229. NVIDIA. Asr models - nvidia nemo toolkit documentation, 2025. URL https://docs.nvidia.com/deeplearning/ nemo/user-guide/docs/en/main/asr/models.html#fastconformer. Accessed: September 29, 2025. URL: https: //docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fastconformer. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. Robust speech In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara recognition via large-scale weak supervision. Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/radford23a.html. D. Rekesh, O. Kuchaiev, S. Hauth, G. E. Henter, and B. Ginsburg. Fast conformer with linearly scalable attention for eﬀicient speech recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18, 2023. doi: 10.1109/ASRU57964.2023.10389701. Artit Suwanbandit, Burin Naowarat, Orathai Sangpetch, and Ekapol Chuangsuwanich. Thai Dialect Corpus and In Proc. INTERTransfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition. SPEECH 2023, pages 40694073, 2023. doi: 10.21437/Interspeech.2023-1828. 13 Sumonmas Thatphithakkul, Kwanchiva Thangthai, and Vataya Chunwijitra. The Development of LOTUS-TRD: Thai Regional Dialect Speech Corpus. In 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), pages 16, 2024. doi: 10.1109/O-COCOSDA64382.2024.10800335. Pattara Tipaksorn, Wayupuk Sommuang, Oatsada Chatthong, and Kwanchiva Thangthai. Pathumma Whisper Large V3 (TH), 2024. URL https://huggingface.co/nectec/Pathumma-whisper-th-large-v3. Typhoon Team, SCB 10X. Thai dialect-isan dataset. https://huggingface.co/datasets/typhoon-ai/ thai-dialect-isan-dataset, 2025a. Typhoon Team, SCB 10X. Typhoon-whisper-medium-isan: thai isan dialect asr model. https://huggingface.co/ typhoon-ai/typhoon-isan-asr-whisper, 2025b. Typhoon Team, SCB 10X. TVSpeech: Robust Thai ASR Test Set. https://huggingface.co/datasets/ typhoon-ai/TVSpeech, 2026a. Typhoon Team, SCB 10X. Gigaspeech2-Typhoon: Standardized Thai ASR Benchmark. https://huggingface. co/datasets/typhoon-ai/gigaspeech2-typhoon, 2026b. Typhoon Team, SCB 10X. Typhoon-Whisper-Large-v3: Robust Thai Automatic Speech Recognition. https:// huggingface.co/typhoon-ai/typhoon-whisper-large-v3, 2026c. Typhoon Team, SCB 10X. Typhoon-Whisper-Turbo: Eﬀicient Thai Automatic Speech Recognition. https: //huggingface.co/typhoon-ai/typhoon-whisper-turbo, 2026d. Yifan Yang, Zuoquan Lin, Shiyu Zhou, Liwei Guo, Yixuan Hu, Rui Zhang, Jinpeng Li, Pakey Ma, Zimalanjiang Yiming, Ziang Zheng, Feng Lin, Yingying Gao, Chenlin Li, Song Wen, Shuo Sun, Liming Song, Gandong Xie, Yu Hu, Xie Chen, Xunying Liu, Dan Su, Wenchao Wang, Binbin Zhang, Di Wu, Xiong Qin, Xingchen Song, Zhendong Peng, Zhuoyuan Yao, Yinfeng Yu, Zejun Ma, and Lei Xie. Gigaspeech 2: An evolving, large-scale, multi-domain, multilingual speech recognition corpus, 2024."
        }
    ],
    "affiliations": [
        "SCB 10X",
        "Typhoon"
    ]
}