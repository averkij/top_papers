{
    "paper_title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "authors": [
        "Minghao Wu",
        "Weixuan Wang",
        "Sinuo Liu",
        "Huifeng Yin",
        "Xintong Wang",
        "Yu Zhao",
        "Chenyang Lyu",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications."
        },
        {
            "title": "Start",
            "content": "2025-4-23 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Minghao Wu1,2, Weixuan Wang3, Sinuo Liu1,3, Huifeng Yin1,4, Xintong Wang1,5, Yu Zhao1, Chenyang Lyu1, Longyue Wang1, Weihua Luo1, Kaifu Zhang1 1 Alibaba International Digital Commerce 2 Monash University 3 The University of Edinburgh 4 Tsinghua University 5 Universit√§t Hamburg As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications. 5 2 0 2 2 ] . [ 1 1 2 5 5 1 . 4 0 5 2 : r Figure 1 The overview of this work. We examine over 2,000 multilingual (non-English) benchmarks, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. 2025 Alibaba International Digital Commerce. All rights reserved The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks 1. Introduction The remarkable capabilities of large language models (LLMs) have transformed natural language processing (NLP), with applications spanning diverse domains and languages worldwide [Ouyang et al., 2022, Sanh et al., 2022, OpenAI, 2023, Touvron et al., 2023, Anil et al., 2023, Mesnard et al., 2024, Yang et al., 2024, DeepSeek-AI et al., 2025]. As these technologies increasingly serve users across linguistic boundaries, robust multilingual evaluation becomes not merely academic but essential [Zhu et al., 2024, Qin et al., 2025]. Despite advances in multilingual pre-training, significant disparities persist in how models perform across different languages, writing systems, and cultural contexts. Current evaluation frameworks often fail to capture this diversity, with benchmarks predominantly focused on high-resource languages and evaluation methodologies that inadequately represent realworld use cases in global contexts [Hada et al., 2024]. This paper presents comprehensive analysis of multilingual benchmarking practices to understand past approaches, assess present correlations with human judgments, and chart future directions for more equitable, representative, and effective multilingual evaluation of language technologies. To provide comprehensive analysis, we first establish robust dataset of multilingual benchmarks by collecting and annotating papers from the arXiv cs.CL category (20212024) (Section 3). After filtering 370,000 papers through both automated LLM-based screening and expert review, we identify 2,024 relevant studies containing multilingual benchmarks from 148 countries. Our analysis follows temporal framework organized around three key questions: 1. PAST: What benchmarks do we currently have? (Section 4) We document historical trends, revealing significant disparities in language representation, the evolution of task types, translation methods, and more. Our analysis reveals several key patterns: despite deliberate exclusion of English-only benchmarks, English remains the most represented language, with high-resource languages dominating while low-resource languages are underrepresented. Most benchmarks (61.4%) use original language content rather than translations, with human translations accounting for just 13.2%. Discriminative tasks (66.5%) significantly outnumber generative ones (23.5%), with text classification remaining dominant while question answering shows dramatic growth following the emergence of LLMs in 2023. Dataset sizes have consistently expanded across all categories over the years, with an estimated cost of $11 million USD. Most content is sourced from publicly accessible domains, such as news (17%) and social media (13.3%), while high-value domains like healthcare and law remain underrepresented. evelopment efforts are concentrated in China, India, Germany, the UK, and the USA, with Europe emphasizing academic research and China and the USA demonstrating stronger academia-industry collaborations. 2. PRESENT: What is the current status of multilingual evaluation? (Section 5) We present two key insights about multilingual LLM evaluation. First, users across different languages (English, Chinese, French, German, Spanish, and Russian) share remarkably similar interests, with writing tasks dominating (30-45%), followed by commonsense reasoning and programming tasks, suggesting universal priorities despite linguistic differences. Second, when comparing benchmark performance with human judgments, STEM-related tasks (ARC and MGSM) show consistently stronger correlations across languages (0.70-0.85), while other tasks like XQuAD correlate poorly (0.11-0.30). Importantly, simply translating English benchmarks proves insufficient for robust evaluation, localized benchmarks (like CMMLU for Chinese) show substantially higher correlation with human judgments (0.68) than translated equivalents (0.47 and 0.49), highlighting the critical need for culturally and linguistically authentic evaluation resources. 3. FUTURE: What do we need, and what should we do next? (Section 6) Based on our analysis, we outline key principles for effective multilingual benchmarks, emphasizing the need The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks for accurate, contamination-free, challenging, practically relevant, linguistically diverse, and culturally authentic evaluations. It identifies critical research directions, including addressing the imbalance in natural language generation (NLG) tasks, improving representation for lowresource languages, creating localized benchmarks that reflect cultural nuances, leveraging LLMs as multilingual judges while addressing biases, and developing efficient benchmarking methods to manage growing complexity. Our analysis highlights the existing gaps in multilingual evaluation, including fragmented efforts, limited language coverage, and mismatch between academic benchmarks and real-world applications. Therefore, we propose call to action for the NLP community to prioritize the development of multilingual benchmarks that are not only well-aligned with human judgments but also culturally relevant and practically applicable (Section 7). We emphasize the importance of collaboration among researchers, practitioners, and communities to ensure that language technologies serve all users equitably, regardless of their linguistic background. Our contributions through this position paper are multifaceted. First, we provide large-scale and comprehensive analysis to date of multilingual benchmarking trends, documenting historical patterns and identifying critical gaps in language coverage. Second, we quantitatively evaluate how well current benchmarks align with human judgments across multiple languages, offering insights into which evaluation approaches best reflect real-world perceptions of model quality. Third, we propose concrete strategies and call to action for developing the next generation of multilingual benchmarks, balancing practical constraints with the need for greater linguistic diversity and cultural authenticity. By critically examining existing practices and charting clear directions forward, we aim to catalyze more equitable, representative, and meaningful evaluation methodologies that can better guide the development of truly multilingual language technologies serving the global community. 2. Related Work Multilingual Large Language Models Large language models (LLMs) have revolutionized the landscape of natural language processing (NLP) and artificial intelligence (AI) [Brown et al., 2020, Ouyang et al., 2022, Bai et al., 2022, OpenAI, 2023, Anil et al., 2023, Rivi√®re et al., 2024]. Multilingual large language models (MLLMs) also have been developed to address multilingual downstream tasks, extending the capabilities of LLMs to multiple languages [Scao et al., 2022, Wu et al., 2024, Wang et al., 2024b]. Early LLMs are typically pre-trained on large-scale English datasets [Touvron et al., 2023, Jiang et al., 2023]. However, recent advancements have made multilingual capabilities critical feature for modern LLMs. For instance, while Llama-1 models primarily focus on English [Touvron et al., 2023], the Llama-3.1 series models now support 8 languages [Dubey et al., 2024]. Similarly, Qwen series models initially focus on Chinese and English [Bai et al., 2023], but the Qwen 2.5 series models have expanded to support more than 29 languages [Yang et al., 2024]. To facilitate the development of MLLMs, significant efforts have been made to collect multilingual corpora. number of multilingual corpora are curated for pre-training [Ortiz Su√°rez et al., 2020, Lauren√ßon et al., 2022, Nguyen et al., 2024]. Furthermore, recent works also attempt to collect and synthesize datasets for supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [Li et al., 2023, Lai et al., 2023, Singh et al., 2024b]. Overall, Zhu et al. [2024] present systematic survey on the recent progress of MLLMs. Multilingual Evaluation As large language models (LLMs) continue to advance, there is growing need for comprehensive and fair evaluation of their capabilities across diverse languages and cultures [Guo et al., 2023, Chang et al., 2024, Wang et al., 2024a]. Such evaluations are crucial for 3 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Aspect Description Year and Month The publication year and month of the paper. Languages Task Category The languages covered by the dataset. The task types discussed in the paper (e.g., discriminative, generative, or both). The specific tasks covered (e.g., sentiment analysis, question answering, summarization, etc.). The approximate size of the dataset, categorized as: <100, 1001K, 1K10K, 10K100K, or >100K. The Affiliation type of the creator of the dataset (e.g. academic, industry, or both). The affiliations that create the dataset. The countries of the affiliations that create the dataset. The method used for dataset translation (e.g., not translated, human translation, Google Translate, etc.). The domains of the dataset (e.g., news, social media, etc.). Tasks Dataset Size Affiliation Type Affiliation Country Translation Domain Table 1 Annotation scheme for the collected paper. understanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y√ºksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, vision-language benchmark spanning 28 languages, with focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with total of 10K questions. In this position paper, we conduct comprehensive investigation of over 2,000 studies on Ours multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research. 3. Scope, Collection, and Annotation In this section, we outline our approach to determining the scope of datasets included in our study, detail our collection process from arXiv submissions, and describe our annotation methodology. These The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Figure 2 Distribution of the top 50 languages in our multilingual benchmark collection. Although English is deliberately excluded from the collection, it still appears as the most frequent language in the collection. This distribution illustrates the current imbalance in multilingual evaluation benchmarks. steps are critical for ensuring that our benchmark provides representative and well-documented foundation for evaluating multilingual NLP capabilities. Scope Our work follows the approach of Yu et al. [2022], focusing exclusively on labeled datasets in which system is tasked with generating an output label ùë¶ from an input text ùë•. It is important to note that the output label is not limited to single categorical value but may also consist of generated text, allowing for the production of more complex outputs. To maintain this focus on clear input-output relationships and ensure that the generated labels remain meaningful and contextually relevant, we deliberately exclude training datasets, unlabeled datasets, machine translation datasets, language identification datasets, and multi-modal datasets from our study. Furthermore, we also exclude the programming languages from our study. Collection In this work, we collect papers under the cs.CL category of arXiv from January 1, 2021, to December 31, 2024 using the arXiv API. The arXiv API provides programmatic access to metadata and abstracts of papers, enabling efficient data collection. From this process, we initially retrieved total of 370K papers. To refine the dataset, we utilize n 2 . 5 - 7 - t ruc to analyze the abstracts of each paper and filter out those irrelevant to our study. Following this automated step, we conduct manual review to ensure the suitability of each paper for inclusion in our study. This rigorous process resulted in final dataset of 2,024 papers. Annotation Besides utilizing metadata from the arXiv API, three authors manually annotate the collected papers following the annotation scheme presented in Table 1. These authors, each with at least one year of experience in NLP research and proficiency in multiple languages, bring valuable expertise to the annotation process. The additional annotations not only provide new insights into the data but also serve as valuable resource for future research. https://info.arxiv.org/help/api/index.html 5 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks 4. PAST: What Benchmarks Do We Have? In this section, we present comprehensive analysis of the current landscape of multilingual benchmarks based on our paper collection. We examine the distribution of languages, the evolution of task types, translation methods, and more across benchmarks collected from 2021 to 2024. Understanding the existing benchmark ecosystem is crucial for identifying gaps in language coverage, tracking shifts in evaluation focus, and recognizing opportunities for more inclusive and representative benchmark development. Languages Figure 2 illustrates the distribution of the top 50 languages across our collected benchmarks. Notably, even though we deliberately exclude English benchmarks during the collection process, English still tops the chart, peaking near 1000 occurrences. Similarly, other high-resource languages (HRLs) such as Chinese (zh), Spanish (es), French (fr), and German (de) occupy the leading positions. In contrast, low-resource languages (LRLs) appear much less frequently. This distribution underscores the dominance of high-resource languages within our benchmark collection, while highlighting the challenges in achieving broader linguistic representation. Translations Figure 3 illustrates the distribution of translation methods used in benchmark creation. Notably, the majority (61.4%) of benchmarks are not translated, suggesting they are created in their original languages. Human translations account for 13.2% of the benchmarks, representing the highest quality but most resource-intensive approach. Among machine translation tools, Google Translate leads with 8.8%, followed by GPT series models [OpenAI, 2023] (5.0%) and DeepL (1.9%). Specialized translation systems like NLLB series [Costa-juss√† et al., 2022] (1.8%), IndicTrans (0.9%), Opus-MT [Tiedemann and Thottingal, 2020] (0.6%), and Microsoft Translator (0.4%) account for smaller portions. This distribution highlights both the prevalence of nativelanguage benchmark development and the growing role of various machine translation technologies in multilingual benchmark creation. Figure 3 Distribution of translation methods used in benchmark creation. In our collected benchmarks, 66.5% of the papers focus on discriminative tasks, 23.5% on Tasks generative tasks, and 10.0% on both. Figure 4(a) shows the percentage distribution of the top 5 tasks from 2021 to 2024. Text classification has consistently been the dominant task. Question answering and Machine reading comprehension have shown dramatic growth. Conversely, named entity recognition has exhibited declining trend. Sentiment analysis has maintained relatively stable representation. The emergence of large language models (LLMs) in 2023 has significantly reshaped the landscape of multilingual research, with notable shift towards more complex tasks like question answering. 6 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks (a) Tasks (b) Dataset Sizes Figure 4 (a) Percentage distribution of top 5 tasks in multilingual benchmarks from 2021 to 2024. TC: Text Classification, NER: Named Entity Recognition, SA: Sentiment Analysis, QA: Question Answering, MRC: Machine Reading Comprehension. (b) Distribution of multilingual benchmarks by dataset size from 2021 to 2024. Categories include very large (>100K examples), large (10K-100K), medium (1K-10K), small (100-1K), and very small (<100) datasets. Dataset Sizes Figure 4(b) illustrates the evolution of dataset sizes in multilingual benchmarks from 2021 to 2024. We observe consistent upward trend across all size categories, with particularly significant growth in larger datasets. Very large datasets (>100K examples) have nearly tripled from 104 in 2021 to 304 in 2024, while medium (1K-10K) and small datasets (100-1K) have grown from 72 to 228 and 39 to 102 examples, respectively. This trend reflects the growing emphasis on large-scale evaluation resources in the era of foundation models. Assuming each example costs $0.10 USD on average, these benchmarks have cost more than $11 million USD from 2021 to 2024. Domains Figure 5 presents the domain distribution across our multilingual benchmark collection. News content constitutes significant portion at 17.0% of the benchmarks, followed by social media at 13.3% and Wikipediaderived content at 9.6%. This concentration highlights clear trend: multilingual benchmarks predominantly leverage publicly accessible sources rather than specialized, high-value domains. Educational and literary domains each represent 6.5% of the benchmarks, with technology (6.5%) and politics (6.4%) following closely behind. More specialized domains like health (4.5%), entertainment (4.2%), and law (3.8%) appear less frequently despite their practical importance. This distribution reflects both the practical convenience of using readily available public data sources and the challenges in obtaining high-quality data from specialized domains, suggesting opportunities for greater representation in these high-value areas. Figure 5 Distribution of domains across multilingual benchmarks in our collection. The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks (a) Country Distributions (b) Affiliation Type Distributions of Top 50 countries Figure 6 (a) Top 5 countries in multilingual benchmark creation from 2021 to 2024. (b) Affiliation type distributions of the top 50 countries in multilingual benchmark creation. We merge the countries in European Union (EU) into one category for better visualization. Countries and Institutions Figure 6(a) highlights the countries leading multilingual benchmark development from 2021 to 2024. In this paper, we introduce the term G5 countries to refer to the China, India, Germany, UK, and USA, which together account for at least 40% of multilingual benchmark development. Among them, only China shows steady growth from 2021 (10.4%) to 2024 (16.4%). Figure 6(b) illustrates the institutional distribution across the top 50 countries: Europe leads with strong academic focus, while China and the USA feature more balanced academiaindustry collaborations. The predominantly academic-driven nature of these benchmarks points to gap between research and real-world application, suggesting opportunities for greater industry engagement in multilingual benchmark creation. 5. PRESENT: What is the Current Status of Multilingual Evaluation? Understanding the current landscape of multilingual evaluation is essential for developing effective benchmarks and assessment methodologies. In this section, we examine the present state of multilingual evaluation from two critical perspectives: the actual interests and needs of multilingual users (Section 5.1), and the alignment between existing multilingual benchmarks and human judgments (Section 5.2). By investigating these dimensions, we aim to identify gaps between user expectations and evaluation practices, and to establish foundation for more representative and meaningful multilingual benchmarks. 5.1. What Are the Multilingual Users Interested in? Setup To understand the interests of multilingual users, we analyze the distribution of user instructions in Chatbot Arena [Chiang et al., 2024] and WildChat [Zhao et al., 2024b]. We analyze six We use publicly available geographical data for visualization purposes only. The map representation does not imply any political stance or territorial claims. Data Source: https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ 8 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks languages, including English, Chinese, French, German, Spanish, and Russian, with 10K instructions for each language. We employ n 2 . 5 - to categorize the instructions. We provide 5 seed categories: Greeting, Writing, Translation, Math, and Programming, but allow the model to introduce new categories. Users from different countries share common interests. We present the distribution of user instructions in Figure 7. Our analysis reveals striking similarities in user interests across different languages. Writing tasks dominate user interactions across all six languages, comprising 30-45% of all instructions. This is followed by commonsense reasoning and programming tasks, which consistently appear among the top three categories in almost all languages. Interestingly, while translation tasks are present in non-English languages (ranging from 5-6% of instructions), they are understandably absent in English. Mathematical tasks appear consistently across all languages but at lower frequencies (3-7%). These patterns suggest that despite linguistic and cultural differences, users across different languages primarily use LLMs for similar purposes, with content creation and practical problem-solving being universal priorities. The minor variations observed, such as the higher proportion of programming queries in Russian (17%) compared to German (6%), likely reflect differences in professional demographics or educational emphases rather than fundamental divergences in interests. Furthermore, it is important to note that the user instructions are collected from Chatbot Arena and WildChat, which are primarily used for research purposes. Therefore, the distribution of user instructions may not accurately reflect the general populations interests. Figure 7 Distribution of user instruction categories across six languages. We discard the Greetings\" category, as it is not task-oriented instruction. 5.2. Do These Benchmarks Correlate Well with Human Judgments? Setup To investigate the correlation between multilingual benchmarks and human judgments, we compare the performance of large language models (LLMs) on multilingual benchmarks with human evaluations collected from Chatbot Arena. Specifically, we evaluate 30 popular LLMs on 8 diverse and widely used multilingual benchmarks: XNLI [Conneau et al., 2018], ARC [Clark et al., 2018], HellaSwag [Zellers et al., 2019], TruthfulQA [Lin et al., 2022], MMLU [Hendrycks et al., 2021], GlobalMMLU [Singh et al., 2024a], XQuAD [Artetxe et al., 2020], and MGSM [Shi et al., 2023]. For ARC, HellaSwag, TruthfulQA, and MMLU, translations are provided by Lai et al. [2023] using Google Translate. To assess the alignment between LLM performance and human preferences, we compare the rankings of these models on the benchmarks with their Elo rankings from Chatbot Arena. The Elo rankings, which reflect human judgments, are considered as of the cutoff date of December 30, 2024. The comparison is conducted across five languages: Chinese, French, German, Spanish, and Russian. Spearmans ùúå is used as the correlation metric to quantify the relationship between the two ranking systems. We present the list of LLMs used in our evaluation in Appendix A. The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Type Chinese French German Spanish Russian Discriminative XNLI ARC HellaSwag TruthfulQA MMLU GlobalMMLU Generative XQuAD MGSM Natural Language Inference STEM Question Answering Commonsense Reasoning Question Answering Understanding Understanding 0.233 0.818 0.547 0.473 0.487 0.235 0.735 0.684 0.613 0.398 0. 0.410 0.767 0.745 0.614 0.371 0.395 Question Answering Mathematics 0.110 0.855 0.814 0.301 0.848 0.483 0.801 0.772 0.624 0.345 0. 0.225 0.798 0.588 0.803 0.811 0.773 0.303 0.331 0.154 0.711 Table 2 The Spearmans ùúå for various benchmarks across 5 languages. The highest correlation for each language is highlighted in bold. Type indicates the capability type that the benchmark is testing. STEM-related tasks are more aligned with human judgments. As shown in Table 2, benchmarks that focus on STEM-related capabilities consistently exhibit stronger correlations with human judgments across all languages. Particularly, ARC and MGSM stand out with remarkably high correlation coefficients. ARC, which tests commonsense and scientific reasoning, shows strong correlations ranging from 0.735 in French to 0.818 in Chinese. Similarly, MGSM, which evaluates mathematical problem-solving abilities, demonstrates exceptionally high correlations across all languages (0.7110.848). In contrast, natural language inference (XNLI) and question-answering tasks (XQuAD) generally show lower correlations with human preferences. We believe this discrepancy is due to the fact that reasoning capabilities are language agnostic and less affected by translation quality. In contrast, tasks that require understanding of specific linguistic nuances or cultural contexts may be more sensitive to translation errors, leading to weaker correlations with human judgments. Referring to our analysis in Section 4, these findings highlight concerning trend: while significant financial resources are invested in developing multilingual benchmarks, many of these may fail to align well with human judgments. Translation is NOT all you need. The correlation data in Table 2 clearly demonstrates that simply translating English benchmarks into other languages is insufficient for robust multilingual evaluation. We observe substantial variation in correlation strengths across different languages for the same benchmark. For instance, XNLI shows weak correlation in Chinese and French (0.233 and 0.235) but moderate correlation in Russian (0.588). Similarly, XQuAD exhibits particularly poor correlation in Chinese (0.110) compared to German (0.301). Furthermore, we observe that GlobalMMLU, which contains questions translated by human translators instead of machine translation, shows higher correlations with human judgments compared to the machine-translated MMLU. This finding highlights the importance of high-quality translations. In addition to the results presented in Table 2, we also evaluate Localized benchmarks are crucial. these LLMs on CMMLU [Li et al., 2024], which includes authentic exam questions from various Chinese exams. CMMLU demonstrates correlation of 0.682 with human judgments in Chinese, significantly higher than the correlation of translated MMLU in Chinese (0.473 and 0.487). This finding suggests that translated benchmarks often fail to capture language-specific nuances, cultural contexts, and linguistic features that influence human judgments. It underscores the importance of localized benchmarks specifically designed to capture these nuances and contexts. 10 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Figure 8 conceptual framework illustrating the essential characteristics of effective multilingual benchmarks and future research directions for advancing multilingual evaluation. 6. FUTURE: What We Need and What We Should Do Next? This section explores two complementary perspectives, as shown in Figure 8. Firstly, we identify essential characteristics of effective multilingual benchmarks (Section 6.1). Secondly, we propose concrete directions for future research efforts that address persistent gaps in evaluating language models across diverse languages, contexts, and applications (Section 6.2). By considering both the ideal benchmark properties and actionable research directions, we aim to chart path toward more equitable and comprehensive multilingual evaluation. 6.1. What We Need for Effective Multilingual Benchmarks? Effective multilingual benchmarks require several key characteristics to meaningfully evaluate LLM capabilities across diverse languages. The development of such benchmarks presents unique challenges beyond those faced in monolingual evaluation settings. Drawing inspiration from Reiter [2025], we propose the following key characteristics for good multilingual benchmarks: Accurate: Benchmark accuracy is foundational to meaningful evaluation. All benchmarks must contain reliable ground truth annotations, properly verified by domain experts. Recent research reveals that even widely adopted benchmarks like MMLU contain numerous errors [Gema et al., 2025], undermining evaluation validity. Contamination-free: Benchmark contamination occurs when evaluation data appears in models training corpus, leading to inflated performance metrics that misrepresent models true capabilities. Recent research even demonstrate that the data contamination in one language can be transferred to another language [Yao et al., 2024]. Challenging enough: The performance of recent state-of-the-art models has quickly saturated on widely used benchmarks, with scores approaching or exceeding human performance. As shown in l - by - M, the average lifespan of popular benchmark is only 2.6 years before it is not sufficiently challenging. Therefore, multilingual benchmarks must maintain an appropriate difficulty level that can differentiate between models. Practically relevant: As shown in Figure 5 and Figure 6(b), about 70% of the benchmarks are released by the academic community and these benchmarks are created from the public https://r0bk.github.io/killedbyllm/ 11 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks sources, which may not always reflect real-world applications. Truly useful benchmarks must evaluate capabilities that align with how language technologies are actually used in practice. Without this practical grounding, benchmarks risk optimizing for capabilities that have limited real-world impact, creating disconnect between research advancements and actual user needs. Linguistically diverse: Effective multilingual benchmarks must include diverse set of languages representing different language families, writing systems, and linguistic typologies. Our analysis of language distribution in existing benchmarks (as detailed in Figure 2) reveals significant imbalance: English remains the most frequently evaluated language, followed by Chinese, Spanish, and French, despite our intentional exclusion of English-only benchmarks from this study. Culturally authentic: Multilingual benchmarks must reflect the cultural diversity of the worlds languages, ensuring that evaluation tasks are culturally appropriate and authentic. Recent research has highlighted the importance of cultural considerations in benchmark design [Son et al., 2024, Zhao et al., 2024a, Chiu et al., 2024]. Our results also demonstrate that CMMLU [Li et al., 2024] aligns better with Chinese users judgments, compared with the translated MMLU [Hendrycks et al., 2021, Singh et al., 2024a]. Advancing toward more comprehensive multilingual benchmarks following these principles is essential for ensuring language technologies serve global populations equitably and perform reliably across wide range of languages. 6.2. What We Should Do Next? Building on our analysis of necessary characteristics for effective multilingual benchmarks, we now outline five critical research directions. Natural Language Generation While most existing multilingual benchmarks focus on discriminative tasks like classification, question answering, and multiple-choice problems, natural language generation (NLG) capabilities remain significantly underassessed across diverse languages. As discussed in Section 4, about 66% of the benchmarks are focused on discriminative tasks, while only 23% of the benchmarks are focused on NLG tasks. This imbalance is particularly concerning as generative applications are increasingly prevalent in real-world deployments of language technologies. Low-Resource Languages As shown in Figure 2, low-resource languages, which lack substantial amounts of digital text data, remain significantly underrepresented in current multilingual benchmarks. This underrepresentation creates problematic cycle: models perform poorly on these languages, leading researchers to focus on higher-resource languages where improvements are more easily demonstrable, further widening the capability gap. Breaking this cycle requires deliberate effort to develop specialized benchmarks that focus on low-resource languages and their unique linguistic characteristics. Localized Benchmarking Current evaluation approaches often rely on translated content from English or other high-resource languages. The machine-translated evaluation data often preserves source language syntactic structures and cultural references while failing to capture idioms and region-specific knowledge essential to natural language use. As shown in Section 5.2, the localized benchmarks can achieve better alignment with the target language and culture. Recent work has 12 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks begun addressing these issues by incorporating more diverse cultural perspectives [Li et al., 2024, Son et al., 2024, Zhao et al., 2024a, Chiu et al., 2024], but there remains significant room for benchmarks that assess models on their ability to handle locally meaningful applications. LLM-as-a-Judge Recent research has demonstrated the potential of using LLMs themselves as evaluation tools for assessing the quality of model-generated text in English [Zheng et al., 2023, Dubois et al., 2024]. This approach offers promising opportunities for multilingual evaluation by extending these techniques across diverse languages and tasks. However, deploying LLMs as judges in multilingual contexts also introduces unique challenges, including potential evaluation biases that mirror the language disparities in the judge models themselves and questions about their cross-lingual evaluation capabilities for low-resource languages. These strengths and challenges highlight the need for further research to develop multilingual LLM judges with more balanced cross-lingual capabilities. Efficient Benchmarking Current benchmarks often include numerous languages and tasks to thoroughly assess model capabilities. The size of benchmarks grows linearly with the number of languages and combinatorially with the number of tasks and evaluation dimensions. As shown in Figure 4(b), the size of the benchmarks is growing rapidly over the years. Future research should aim to develop methods for efficient LLM evaluation, such as identifying representative language-task subsets, employing statistical sampling techniques, or using adaptive testing approaches that maintain evaluation quality while reducing computational requirements. 7. Call to Action As we conclude our analysis of multilingual benchmarking, we recognize that advancing this field requires not merely technical solutions but fundamental shift in how we collaborate as global research community. The challenges identified throughout this paperfrom linguistic disparities to cultural authenticitycannot be addressed by isolated efforts. Therefore, we issue this call to action for researchers, practitioners, and organizations invested in the equitable development of language technologies. Global Collaboration for Inclusive Benchmarking The creation of truly representative multilingual benchmarks demands collaboration across linguistic, organizational, national, and cultural boundaries. The fragmentation of efforts we observe today leads to significant resource inefficiencies, with multiple teams solving similar problems in isolation. Individual contributions, while valuable, often duplicate work and waste financial and computational resources that could be better allocated through coordinated efforts. We advocate for international research consortia specifically focused on multilingual benchmark development, where expertise across different languages and cultures can be pooled to create more comprehensive evaluation frameworks. Such collaboration would enable more efficient resource allocation while ensuring broader language coverage and deeper linguistic insights than any single organization could achieve alone. Human-Aligned Evaluation Our analysis reveals critical gap between benchmark performance and meaningful human assessment across different linguistic contexts. Future benchmarking efforts must prioritize alignment with diverse human judgments, recognizing that language proficiency is ultimately about human communication and understanding. This requires engaging native speakers and linguistic experts from varied backgrounds in benchmark design, validation, and interpretation. 13 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Beyond mere translation of existing benchmarks, we need evaluation paradigms that capture languagespecific nuances, cultural references, and pragmatic understanding that reflect how humans actually judge language quality in different cultural contexts. The disconnect between benchmark scores and human perception undermines the practical utility of our evaluation methods and must be addressed through systematic incorporation of human feedback. Application-Oriented Benchmarking The academic nature of many existing benchmarks creates concerning divide between what we measure and what matters in real-world deployments. Future multilingual benchmarks should be designed to reflect practical applications across diverse contexts, such as education, healthcare, commerce, and more. This requires engaging stakeholders beyond academic research, including industry practitioners, public service providers, and community organizations across different regions. By grounding benchmark tasks in concrete use cases that vary by linguistic and cultural context, we can ensure our evaluation methods drive improvements in language technologies that truly serve diverse global communities. Such application-oriented benchmarks would help bridge the gap between research advancements and meaningful impact in multilingual settings. The path forward for multilingual benchmarking requires commitment from all stakeholders in the language technology ecosystem. By fostering collaboration across boundaries, centering human judgment in our evaluation methods, and focusing on real-world applications, we can develop benchmarking approaches that drive equitable progress in language technologies. The global nature of communication demands nothing less than global effort to ensure language models serve all communities fairly and effectively. We invite researchers, practitioners, and organizations to join this collaborative effort toward more inclusive, representative, and meaningful multilingual evaluation. 8. Conclusion In this position paper, we present comprehensive analysis of multilingual benchmarking practices by systematically examining over 2,000 studies. Our findings uncover persistent disparities in language representation, evolving task types, dataset sizes, and other critical factors. Through presentfocused investigation, we identify user interests across different languages and highlight significant gaps between benchmark scores and actual human preferences, particularly in translation-based evaluations. Our analysis underscores six key limitations in current multilingual evaluation practices and proposes guiding principles for effective multilingual benchmarking. Additionally, we outline five critical research directions to advance the field. Finally, we advocate for global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications and address the diverse needs of multilingual users."
        },
        {
            "title": "References",
            "content": "R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka, B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez, M. Khalman, J. Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv.2312.11805. 14 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks M. Artetxe, S. Ruder, and D. Yogatama. On the cross-lingual transferability of monolingual representations. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46234637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.acl-main.421/. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023. doi: 10.48550/ARXIV.2309.16609. URL https://doi.org/10.48550/arXiv.2309.16609. Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. TranJohnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosiute, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073, 2022. doi: 10.48550/ARXIV.2212.08073. URL https://doi.org/10.48550/arXiv.2212.08073. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie. survey on evaluation of large language models. ACM Trans. Intell. Syst. Technol., 15(3):39:139:45, 2024. doi: 10.1145/3641289. URL https://doi.org/10.1145/3641289. W. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, B. Zhu, H. Zhang, M. I. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=3MW8GKNyzI. Y. Y. Chiu, L. Jiang, B. Y. Lin, C. Y. Park, S. S. Li, S. Ravi, M. Bhatia, M. Antoniak, Y. Tsvetkov, V. Shwartz, and Y. Choi. Culturalbench: robust, diverse and challenging benchmark on measuring the (lack of) cultural knowledge of llms. CoRR, abs/2410.02677, 2024. doi: 10.48550/ARXIV.2410.02677. URL https://doi.org/10.48550/arXiv.2410.02677. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. A. Conneau, R. Rinott, G. Lample, A. Williams, S. Bowman, H. Schwenk, and V. Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In E. Riloff, D. Chiang, J. Hockenmaier, and 15 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269/. M. R. Costa-juss√†, J. Cross, O. √áelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Y. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzm√°n, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URL https://doi.org/10.48550/arXiv.2207.04672. DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Rozi√®re, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. M. Kloumann, I. Misra, I. Evtimov, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. CoRR, abs/2404.04475, 2024. doi: 10.48550/ARXIV.2404.04475. URL https://doi.org/10.48550/arXiv.2404.04475. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu?, 2025. URL https://arxiv.org/abs/2406.04127. Z. Guo, R. Jin, C. Liu, Y. Huang, D. Shi, Supryadi, L. Yu, Y. Liu, J. Li, B. Xiong, and D. Xiong. Evaluating large language models: comprehensive survey. CoRR, abs/2310.19736, 2023. doi: 10.48550/ARXIV.2310.19736. URL https://doi.org/10.48550/arXiv.2310.19736. R. Hada, V. Gumma, A. Wynter, H. Diddee, M. Ahmed, M. Choudhury, K. Bali, and S. Sitaram. Are large language model-based evaluators the solution to scaling up multilingual evaluation? In 16 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Y. Graham and M. Purver, editors, Findings of the Association for Computational Linguistics: EACL 2024, pages 10511070, St. Julians, Malta, Mar. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-eacl.71/. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=d7KBjmI3GmQ. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310. 06825. URL https://doi.org/10.48550/arXiv.2310.06825. F. Koto, N. Aisyah, H. Li, and T. Baldwin. Large language models only pass primary school exams in Indonesia: comprehensive test on IndoMMLU. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1235912374, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.760. URL https://aclanthology.org/2023.emnlp-main.760/. V. Lai, C. Nguyen, N. Ngo, T. Nguyen, F. Dernoncourt, R. Rossi, and T. Nguyen. Okapi: Instructiontuned large language models in multiple languages with reinforcement learning from human feedback. In Y. Feng and E. Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 318327, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.28. URL https://aclanthology.org/2023.emnlp-demo.28/. H. Lauren√ßon, L. Saulnier, T. Wang, C. Akiki, A. V. del Moral, T. L. Scao, L. von Werra, C. Mou, E. G. Ponferrada, H. Nguyen, J. Frohberg, M. Sasko, Q. Lhoest, A. McMillan-Major, G. Dupont, S. Biderman, A. Rogers, L. B. Allal, F. D. Toni, G. Pistilli, O. Nguyen, S. Nikpoor, M. Masoud, P. Colombo, J. de la Rosa, P. Villegas, T. Thrush, S. Longpre, S. Nagel, L. Weber, M. Mu√±oz, J. Zhu, D. van Strien, Z. Alyafeai, K. Almubarak, M. C. Vu, I. Gonzalez-Dios, A. Soroa, K. Lo, M. Dey, P. O. Suarez, A. Gokaslan, S. Bose, D. I. Adelani, L. Phan, H. Tran, I. Yu, S. Pai, J. Chim, V. Lepercq, S. Ilic, M. Mitchell, A. S. Luccioni, and Y. Jernite. The bigscience ROOTS corpus: In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, 1.6tb composite multilingual dataset. K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ ce9e92e3de2372a4b93353eb7f3dc0bd-Abstract-Datasets_and_Benchmarks.html. H. Li, F. Koto, M. Wu, A. F. Aji, and T. Baldwin. Bactrian-x : multilingual replicable instructionfollowing model with low-rank adaptation. CoRR, abs/2305.15011, 2023. doi: 10.48550/ARXIV. 2305.15011. URL https://doi.org/10.48550/arXiv.2305.15011. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1126011285, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.671. URL https://aclanthology.org/2024.findings-acl.671/. S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of 17 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229/. T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi√®re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. H√©liou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A. ChoquetteChoo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, and et al. Gemma: Open models based on gemini research and technology. CoRR, abs/2403.08295, 2024. doi: 10.48550/ARXIV.2403.08295. URL https://doi.org/10.48550/arXiv.2403.08295. Y. Mohamed, R. Li, I. S. Ahmad, K. Haydarov, P. Torr, K. Church, and M. Elhoseiny. No culture In Y. Alleft behind: ArtELingo-28, benchmark of WikiArt with captions in 28 languages. Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2093920962, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1165. URL https: //aclanthology.org/2024.emnlp-main.1165/. T. Nguyen, C. V. Nguyen, V. D. Lai, H. Man, N. T. Ngo, F. Dernoncourt, R. A. Rossi, and T. H. Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 42264237, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.377/. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. P. J. Ortiz Su√°rez, L. Romary, and B. Sagot. monolingual approach to contextualized word embeddings for mid-resource languages. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 17031714, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.156. URL https://aclanthology.org/2020.acl-main.156/. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. L. Qin, Q. Chen, Y. Zhou, Z. Chen, Y. Li, L. Liao, M. Li, W. Che, and P. S. Yu. survey of multilingual large language models. Patterns, 6(1):101118, 2025. doi: 10.1016/J.PATTER.2024.101118. URL https://doi.org/10.1016/j.patter.2024.101118. E. Reiter. We need better llm benchmarks, Jan 2025. URL https://ehudreiter.com/2025/01/03/ we-need-better-llm-benchmarks/. 18 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks M. Rivi√®re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram√©, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J. Grill, B. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. KlimczakPlucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. Ji, K. Mohamed, K. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sj√∂sund, L. Usui, L. Sifre, L. Heuermann, L. Lago, and L. McNealus. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. doi: 10.48550/ARXIV.2408.00118. URL https://doi.org/10.48550/arXiv.2408.00118. D. Romero, C. Lyu, H. A. Wibowo, S. G√≥ngora, A. Mandal, S. Purkayastha, J. Ortiz-Barajas, E. VillaCueva, J. Baek, S. Jeong, I. Hamed, Z. X. Yong, Z. W. Lim, P. M. Silva, J. Dunstan, M. Jouitteau, D. L. Meur, J. Nwatu, G. Batnasan, M. Otgonbold, M. Gochoo, G. Ivetta, L. Benotti, L. A. Alemany, H. Maina, J. Geng, T. T. Torrent, F. Belcavello, M. Viridiano, J. C. B. Cruz, D. J. Velasco, O. Ignat, Z. Burzo, C. Whitehouse, A. Abzaliev, T. Clifford, G. Caulfield, T. Lynn, C. S. Palacios, V. Araujo, Y. Kementchedjhieva, M. Mihaylov, I. A. Azime, H. B. Ademtew, B. F. Balcha, N. A. Etori, D. I. Adelani, R. Mihalcea, A. L. Tonja, M. C. B. Cabrera, G. Vallejo, H. Lovenia, R. Zhang, M. Estecha-Garitagoitia, M. Rodr√≠guez-Cantelar, T. Ehsan, R. Chevi, M. F. Adilazuarda, R. Diandaru, S. Cahyawijaya, F. Koto, T. Kuribayashi, H. Song, A. Khandavally, T. Jayakumar, R. Dabre, M. F. M. Imam, K. R. Y. Nagasinghe, A. Dragonetti, L. F. DHaro, O. Niyomugisha, J. Gala, P. A. Chitale, F. Farooqui, T. Solorio, and A. F. Aji. CVQA: culturally-diverse multilingual visual question answering benchmark. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 1568882ba1a50316e87852542523739c-Abstract-Datasets_and_Benchmarks_Track.html. V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. F√©vry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= 9Vrb9D0WI4. T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagn√©, A. S. Luccioni, F. Yvon, M. Gall√©, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Lauren√ßon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al. BLOOM: 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100. F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=f R3wGCk-IXp. S. Singh, A. Romanou, C. Fourrier, D. I. Adelani, J. G. Ngui, D. Vila-Suero, P. Limkonchotiwat, K. Marchisio, W. Q. Leong, Y. Susanto, R. Ng, S. Longpre, W. Ko, M. Smith, A. Bosselut, A. Oh, A. F. T. Martins, L. Choshen, D. Ippolito, E. Ferrante, M. Fadaee, B. Ermis, and S. Hooker. Global MMLU: understanding and addressing cultural and linguistic biases in multilingual evaluation. CoRR, abs/2412.03304, 2024a. doi: 10.48550/ARXIV.2412.03304. URL https://doi.org/10.48550/ arXiv.2412.03304. S. Singh, F. Vargus, D. Dsouza, B. Karlsson, A. Mahendiran, W.-Y. Ko, H. Shandilya, J. Patel, D. Mataciunas, L. OMahony, M. Zhang, R. Hettiarachchi, J. Wilson, M. Machado, L. Moura, D. Krzemi≈Ñski, H. Fadaei, I. Ergun, I. Okoh, A. Alaagib, O. Mudannayake, Z. Alyafeai, V. Chien, S. Ruder, S. Guthikonda, E. Alghamdi, S. Gehrmann, N. Muennighoff, M. Bartolo, J. Kreutzer, A. √úst√ºn, M. Fadaee, and S. Hooker. Aya dataset: An open-access collection for multilingual instruction tuning. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1152111567, Bangkok, Thailand, Aug. 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.620. URL https://aclanthology.org/2024.acl-long.620/. G. Son, H. Lee, S. Kim, S. Kim, N. Muennighoff, T. Choi, C. Park, K. M. Yoo, and S. Biderman. KMMLU: measuring massive multitask language understanding in korean. CoRR, abs/2402.11548, 2024. doi: 10.48550/ARXIV.2402.11548. URL https://doi.org/10.48550/arXiv.2402.11548. J. Tiedemann and S. Thottingal. OPUS-MT building open translation services for the world. In A. Martins, H. Moniz, S. Fumega, B. Martins, F. Batista, L. Coheur, C. Parra, I. Trancoso, M. Turchi, A. Bisazza, J. Moorkens, A. Guerberof, M. Nurminen, L. Marg, and M. L. Forcada, editors, Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479 480, Lisboa, Portugal, Nov. 2020. European Association for Machine Translation. URL https: //aclanthology.org/2020.eamt-1.61/. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https: //doi.org/10.48550/arXiv.2302.13971. W. Wang, B. Haddow, and A. Birch. Retrieval-augmented multilingual knowledge editing. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 335354, Bangkok, Thailand, Aug. 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.21. URL https://aclanthology.org/2024.acl-long.21/. W. Wang, M. Wu, B. Haddow, and A. Birch. Bridging the language gaps in large language models with inference-time cross-lingual intervention. CoRR, abs/2410.12462, 2024b. doi: 10.48550/ ARXIV.2410.12462. URL https://doi.org/10.48550/arXiv.2410.12462. M. Wu, Y. Yuan, G. Haffari, and L. Wang. (perhaps) beyond human translation: Harnessing multiagent collaboration for translating ultra-long literary texts. CoRR, abs/2405.11804, 2024. doi: 10.48550/ARXIV.2405.11804. URL https://doi.org/10.48550/arXiv.2405.11804. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. F. Yao, Y. Zhuang, Z. Sun, S. Xu, A. Kumar, and J. Shang. Data contamination can cross language barriers. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1786417875, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.990. URL https://aclanthology.org/2024.emnlp-main.990/. X. Yu, T. Chatterjee, A. Asai, J. Hu, and E. Choi. Beyond counting datasets: survey of multilingual dataset construction and necessary resources. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 37253743, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.findings-emnlp.273. URL https://aclanthology.org/2022.findings-emnlp.273/. A. Y√ºksel, A. K√∂ksal, L. K. Senel, A. Korhonen, and H. Schuetze. TurkishMMLU: Measuring massive multitask language understanding in Turkish. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 70357055, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.413. URL https://aclanthology.org/2024.findings-emnlp.413/. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can machine really finish In A. Korhonen, D. Traum, and L. M√†rquez, editors, Proceedings of the 57th your sentence? Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https: //aclanthology.org/P19-1472/. W. Zhao, D. Mondal, N. Tandon, D. Dillion, K. Gray, and Y. Gu. WorldValuesBench: large-scale benchmark dataset for multi-cultural value awareness of language models. In N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1769617706, Torino, Italia, May 2024a. ELRA and ICCL. URL https://aclanthology.org/ 2024.lrec-main.1539/. W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng. Wildchat: 1m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024b. URL https://openreview.net/forum?id=Bl8u7ZRlbM. L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. S. Zhu, Supryadi, S. Xu, H. Sun, L. Pan, M. Cui, J. Du, R. Jin, A. Branco, and D. Xiong. Multilingual large language models: systematic survey. CoRR, abs/2411.11072, 2024. doi: 10.48550/ARXIV. 2411.11072. URL https://doi.org/10.48550/arXiv.2411.11072. 21 The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks Model Chinese French German Spanish Russian google/gemma-1.1-7b-it CohereForAI/aya-expanse-32b google/gemma-7b-it meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-2-7b-chat-hf microsoft/Phi-3-small-8k-instruct Qwen/Qwen2.5-Coder-32B-Instruct meta-llama/Meta-Llama-3-8B-Instruct ibm-granite/granite-3.0-8b-instruct microsoft/Phi-3-medium-4k-instruct google/gemma-2-27b-it google/gemma-1.1-2b-it microsoft/Phi-3-mini-128k-instruct meta-llama/Llama-3.2-1B-Instruct CohereForAI/aya-expanse-8b meta-llama/Llama-2-13b-chat-hf meta-llama/Llama-3.2-3B-Instruct google/gemma-2-2b-it HuggingFaceH4/zephyr-7b-beta microsoft/Phi-3-mini-4k-instruct google/gemma-2b-it mistralai/Mistral-7B-Instruct-v0.2 HuggingFaceTB/SmolLM2-1.7B-Instruct Qwen/Qwen1.5-14B-Chat google/gemma-2-9b-it Qwen/Qwen1.5-4B-Chat mistralai/Ministral-8B-Instruct-2410 ibm-granite/granite-3.0-2b-instruct Qwen/Qwen1.5-7B-Chat allenai/OLMo-7B-Instruct 1118.6 1267.1 1095.5 1211.2 1031.8 1122.9 1277.2 1135.2 1130.5 1165.1 1278.8 1076.4 1076.3 1023.0 1241.2 1055.3 1084.7 1190.3 1017.9 1081.8 1049.3 1068.4 1106.5 1202.6 1243.3 1083.6 1256.8 1130.3 1196.1 1071. 1018.5 1199.8 979.7 1142.8 925.1 1091.9 1182.5 1113.1 1027.3 1070.4 1190.1 963.9 994.2 1021.4 1166.1 992.3 1031.0 1129.3 989.5 1033.2 852.2 983.9 1001.9 1068.4 1142.9 929.8 1133.4 1003.1 1017.3 879.6 1049.9 1196.6 978.9 1138.8 956.4 1075.4 1192.8 1101.6 983.2 1100.7 1206.0 947.8 1009.0 1010.9 1180.3 998.2 1053.6 1105.1 975.5 1038.5 909.9 979.2 948.5 1042.3 1180.2 904.9 1128.0 988.4 1022.4 885.3 1052.1 1199.7 983.3 1176.5 989.9 1110.6 1219.7 1174.5 1034.0 1098.7 1223.9 991.4 1056.1 1030.2 1161.4 1076.9 1095.7 1144.6 1040.4 1094.3 985.9 1025.7 941.4 1079.2 1199.7 1013.6 1131.1 1037.0 1012.4 975.9 1076.6 1249.8 1014.7 1187.2 1015.4 1138.4 1250.3 1138.8 1102.5 1169.8 1255.9 1020.1 1039.0 972.9 1228.4 1075.9 984.0 1142.1 1067.1 1056.5 964.9 1045.8 1033.0 1073.3 1220.1 977.5 1222.9 1081.4 1035.8 970.9 Table 3 LLMs used for evaluation and their Elo scores on 5 languages up to December 30, 2024. A. LLMs in Evaluations In this work, we evaluate 30 LLMs on 8 multilingual benchmarks across 5 languages. We present all the LLMs used in this work in Table 3."
        }
    ],
    "affiliations": [
        "Alibaba International Digital Commerce",
        "Monash University",
        "The University of Edinburgh",
        "Tsinghua University",
        "Universit√§t Hamburg"
    ]
}