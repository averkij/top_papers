{
    "paper_title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
    "authors": [
        "Sreyan Ghosh",
        "Sonal Kumar",
        "Zhifeng Kong",
        "Rafael Valle",
        "Bryan Catanzaro",
        "Dinesh Manocha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet."
        },
        {
            "title": "Start",
            "content": "Preprint Under Review. SYNTHIO: AUGMENTING SMALL-SCALE AUDIO CLASSIFICATION DATASETS WITH SYNTHETIC DATA Sonal Kumar Zhifeng Kong Rafael Valle Bryan Catanzaro Sreyan Ghosh Dinesh Manocha NVIDIA, CA, USA University of Maryland, College Park, USA 4 2 0 2 ] . e [ 1 6 5 0 2 0 . 0 1 4 2 : r ABSTRACT We present Synthio, novel approach for augmenting small-scale audio1 classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using T2A model trained only on weakly-captioned AudioSet. Code will be available here."
        },
        {
            "title": "INTRODUCTION",
            "content": "Audio classification is the foundational audio processing task of understanding the input audio and assigning it to one or multiple predefined labels. However, training audio classification models requires lot of high-quality labeled data, which is not always readily available (Ghosh et al., 2022). Manually collecting and annotating large-scale audio datasets is an expensive, time-consuming, and noisy process (Nguyen et al., 2017; MartÄ±n-Morato & Mesaros, 2021), and recent concerns about data privacy and usage rights further hinder this process (Ren et al., 2023). Data augmentation, which involves expanding original small-scale datasets with additional data, is promising solution to address data scarcity. Traditional augmentation techniques attempt to diversify audio samples by applying randomly parameterized artificial transformations to existing audio. These methods include spectral masking (Park et al., 2019), temporal jittering (Nanni et al., 2020), cropping (Niizumi et al., 2021), mixing (Seth et al., 2023; Ghosh et al., 2023b; Niizumi et al., 2021) and other techniques (Saeed et al., 2021; Al-Tahan & Mohsenzadeh, 2021; Manocha et al., 2021). While these approaches have shown success, they operate at the level of observed data rather than reflecting the underlying data-generating process that occurs in real-world scenarios. As result, they statistically modify the data without directly influencing the causal mechanisms that produced it, leading to high correlations between augmented samples and limited control over diversity. Generating synthetic data from pre-trained text-to-audio (T2A) models addresses the limitations of standard data augmentation techniques while retaining their strengths of universality, controllability, and performance (Trabucco et al., 2024). The recent success of generative models makes this approach particularly appealing (Long et al., 2024; Evans et al., 2024b). However, generating synthetic audio presents unique challenges due to the complexity of waveforms and temporal 1We use audio to refer to acoustic events comprising non-verbal speech, non-speech sounds, and music. 1 Preprint Under Review. dependencies (Ghosh et al., 2024b). We highlight the 3 main challenges in generating effective synthetic data for audio classification: i) Consistency with the original data: Synthetic audio that does not align acoustically with the original dataset can hinder effective augmentation and may cause catastrophic forgetting (Geiping et al., 2022). This misalignment includes spectral, harmonic, and other inherent acoustic characteristics not easily controlled through prompts. Maintaining consistency with T2A models trained on internet-scale data remains challenge, and standard fine-tuning can often lead to overfitting (Weili et al., 2024). ii) Diversity of generated data: Ensuring compositional diversity in the generated synthetic data (e.g., sound events, temporal relationships, background elements, etc.) is critical for effective augmentation. Additionally, lack of diversity can lead to poor generalization and learning of spurious correlations, impacting performance. Simple, hand-crafted prompts (e.g., Sound of metro) often result in repetitive patterns, and creating diverse, meaningful prompts is labor-intensive. Complex prompts can generate audios that do not preserve the original label. iii) Limitations of current T2A models: T2A models often struggle to generate diverse audios and follow details in prompts. This is largely due to the lack of large-scale, open-source datasets for training, as well as the inherent complexity of non-speech audio domains (Ghosal et al., 2023). These limitations highlight the need for more advanced approaches for synthetic data generation in audio. Our Contributions. To address these challenges, we propose Synthio, novel, controllable and scalable approach for augmenting small-scale audio classification datasets with synthetic data. Our proposed approach has 2 main steps: i) Aligning the Text-to-Audio Models with Preference Optimization: To generate synthetic audios with acoustic characteristics consistent with the small-scale dataset, we introduce the concept of aligning teaching with learning preferences. Specifically, we align the generations of the T2A model (acting as the teacher) with the target characteristics of the small-scale dataset using preference optimization. This approach ensures that the synthetic audios reflect the acoustic properties of (or sound similar to) the downstream dataset, enabling the classification model (the student) to perform well on test data with similar characteristics. To achieve this, we train diffusion-based T2A model with preference optimization, where audios generated from Gaussian noise are treated as losers and audios from the downstream dataset are treated as winners. ii) Generating Diverse Synthetic Augmentations: To generate diverse audios for augmentation, we introduce the concept of language-guided audio imagination and imagine novel acoustic scenes with language guidance. Specifically, we generate diverse audio captions that are then used to prompt T2A models to generate audios with varied compositions. To achieve this, we propose MixCap, where we prompt LLMs iteratively to generate captions combining existing and new acoustic components. Additionally, we employ self-reflection module that filters generated captions and prompts the LLM to revise those that do not align with the intended label. To summarize, our main contributions are: 1. We introduce Synthio, novel data augmentation approach for audio classification that expands small-scale datasets with synthetic data. Synthio uses novel methods to tackle the inherent challenges of producing consistent and diverse synthetic data from T2A models. 2. We evaluate Synthio across 10 datasets in 4 simulated low-resource settings, demonstrating that, even with T2A model trained on weakly captioned AudioSet, Synthio outperforms all baselines by 0.1%-39%. Figure 1: Performance comparison of Synthio with other augmentation methods on down-sampled ESC50 (100 samples). Traditional augmentation, such as SpecAug, degrades performance on small-scale datasets. Naive synthetic augmentation outperforms traditional methods significantly but plateaus with higher sample counts. Synthio further enhances performance by generating consistent and diverse synthetic data. 3. We conduct an in-depth analysis of the generated augmentations, highlighting Synthios ability to produce diverse and consistent data, its scalability, and its strong performance on complex tasks such as audio captioning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Data Augmentation for Audio and Beyond. Expanding or augmenting small-scale datasets with additional data has been widely studied in the literature. Traditional augmentation methods, which 2 Preprint Under Review. apply randomly parameterized artificial transformations to data during training, remain the most common approach across language Wei & Zou (2019); Karimi et al. (2021), vision (Shorten & Khoshgoftaar, 2019; Wang et al., 2017; Yun et al., 2019), and audio (Park et al., 2019; Spijkervet, 2021). For audio, specific techniques include SpecAugment, adding background noise, reverberation, and random spectrogram transformations. With the emergence of generative models, synthetic data augmentation has been increasingly adopted for language (Ghosh et al., 2023a; 2024c; Chen et al., 2021) and vision (Trabucco et al., 2024; Zhao et al., 2024; He et al., 2023), proving to be more effective than traditional methods. These approaches generally incorporate explicit steps to ensure the consistency and diversity of generated augmentations. While some progress has been made in speech, synthetic data for audio remains relatively underexplored. Recently, parallel work has investigated simple prompting of T2A models to improve T2A generation (Kong et al., 2024) and environmental scene classification (Ronchini et al., 2024). Text-to-Audio Generation. In recent years, there has been significant surge in research on textto-audio (T2A) models. The most popular architectures include auto-regressive models based on codecs (Kreuk et al., 2023; Copet et al., 2024) and diffusion models Liu et al. (2023); Ghosal et al. (2023); Evans et al. (2024a). Clotho (Drossos et al., 2020) and AudioCaps (Kim et al., 2019) remain the largest human-annotated datasets for training these models. However, large-scale datasets for T2A model training are still scarce. Recently, Yuan et al. (2024) synthetically captioned AudioSet (Gemmeke et al., 2017), demonstrating its effectiveness for training T2A models. For downstream adaptation, earlier works have primarily relied on Empirical Risk Minimization (ERM). Majumder et al. (2024) introduced preference optimization for T2A models, creating synthetic preference dataset based on scores provided by CLAP model (Elizalde et al., 2023)."
        },
        {
            "title": "3 BACKGROUND",
            "content": "Diffusion Models. Diffusion models consist of two main processes: forward process and reverse process. Given data point x0 with probability distribution p(x0), the forward diffusion process gradually adds Gaussian noise to x0 according to pre-set variance schedule Î²1, , Î²T and degrades the structure of the data. At the time step t, the latent variable xt is only determined by the xt1 due to its discrete-time Markov process nature. At inference time, the diffusion model iteratively executes the reverse process times starting from randomly sampled Gaussian Noise (Ïµ (0, I)). For more details on diffusion models, we request our readers to refer to Appendix A.1. We now explain Direct Preference Optimization (DPO), simpler alternative to RLHF (Christiano et al., 2017). Abusing notation, we will also x0 as random variables for language. Reward Modeling. Estimating human preferences for particular generation x0, given the context c, is challenging because we do not have direct access to reward model r(c, x0). In our scenario, we assume only ranked pairs of samples are available, where one sample is considered winner (xw 0 ) and the other loser (xl 0) under the same conditioning c. Based on the Bradley-Terry (BT) model, human preferences can be modeled as: 0c) = Ï(r(c, xw (1) where Ï represents the sigmoid function. The reward model r(c, x0) is parameterized by neural network Ï and trained through maximum likelihood estimation for binary classification: 0 ) r(c, xl pBT(xw 0 xl 0)) c,xw 0 ,xl LBT(Ï) = 0 , xl 0 [log Ï(rÏ(c, xw 0 ) rÏ(c, xl 0) are drawn from dataset labeled with human preferences. Here, prompt and data pairs (xw RLHF : The goal of RLHF is to optimize conditional distribution pÎ¸(x0c), where Dc, such that the latent reward model r(c, x0) is maximized. This is done while regularizing the distribution through the Kullback-Leibler (KL) divergence from reference distribution pref, resulting in the following objective: 0))] (2) max pÎ¸ EcDc,x0pÎ¸(x0c)[r(c, x0)] Î²DKL[pÎ¸(x0c)pref(x0c)] (3) Here, the hyperparameter Î² controls the strength of regularization. DPO : DPO directly optimizes the conditional distribution pÎ¸(x0c) to align data generation with the preferences observed in (any form of) feedback. The goal is to optimize the distribution of generated data such that it maximizes alignment with human preference rankings while maintaining consistency with the underlying reference distribution pref(x0c). 3 (6) (7) Preprint Under Review."
        },
        {
            "title": "The optimal solution p",
            "content": "Î¸ (x0c) for the DPO objective can be expressed as: Î¸ (x0c) = pref(x0c) exp(r(c, x0)/Î²) Z(c) where Z(c) is the partition function, defined as: Z(c) = pref(x0c) exp(r(c, x0)/Î²) (4) (5) This term ensures proper normalization of the distribution, and Î² controls the regularization, balancing between adherence to the reference distribution and preference maximization. The reward function r(c, x0) is then reparameterized as: r(c, x0) = Î² log + Î² log Z(c) Î¸ (x0c) pref(x0c) Using this reparameterization, the reward objective can be formulated as: LDPO(Î¸) = c,xw 0 ,xl 0 [log Ï (Î² log pÎ¸(xw pref(xw 0 c) 0 c) Î² log pÎ¸(xl pref(xl 0c) 0c) )] By optimizing this objective, DPO enables direct preference learning, optimizing the conditional distribution pÎ¸(x0c) in such way that it better reflects human preferences, as opposed to traditional approaches that optimize the reward function first and then perform reinforcement learning. DPO for Diffusion Models: Very recently, Wallace et al. propose formulation for optimizing diffusion models with DPO. The primary issue with optimizing diffusion with DPO is that the distribution pÎ¸(x0c) is not tractable due to the need to consider all possible diffusion paths leading to x0. To address this, Wallace et al. propose to leverage the evidence lower bound (ELBO) to incorporate latents x1 , which represent the diffusion path. The reward R(c, x0 ) accounts for the entire sequence, leading to the reward function: r(c, x0) = EpÎ¸(x1T x0,c)[R(c, x0 )] (8) Instead of directly minimizing the KL-divergence as typically done, they propose to utlize the upper bound of the joint KL-divergence DKL[pÎ¸(x0 c)pref(x0 c)]. This is integrated into the optimization objective, enhancing the practicality of training diffusion models with preferences. The new objective, aiming to maximize the reward and match the distribution of the reverse process of pÎ¸ to the reference model pref, is given by: max pÎ¸ Ec,x0pÎ¸(x0T c)[r(c, x0)] Î²DKL[pÎ¸(x0 c)pref(x0 c)] (9) Training efficiency is improved by approximating the intractable reverse process using forward approximation q(x1 x0). The DPO then integrates this into the loss function, which involves comparing the log likelihood ratio of the probabilities under pÎ¸ and pref for winning and losing paths: LDPO-Diffusion(Î¸) = (c,xw 0 ,xl 0)Dpref [log Ï (Î²T log pÎ¸(xw pref(xw 1 xw 0 ) xw 0 ) Î²T log pÎ¸(xl pref(xl 1 1 xl 0) xl 0) )] (10) After applying Jensens inequality to take advantage of the convexity of log Ï, we push the expectation outside, allowing us to simplify the objective. By approximating the denoising process with the forward process, the final form of the loss for DPO in diffusion models, in terms of the L2 noise estimation losses, becomes: LDPO-Diffusion(Î¸) = (11) 0)Dpref,t,Ïµw where is the L2 weighted noise estimation losses between the preferred (winner) and less preferred (loser) samples. [log Ï (Î²T Ï(Î»t)L)] (c,xw 0 ,xl ,Ïµl"
        },
        {
            "title": "4 METHODOLOGY\nLet Dsmall = {(ai, li), 1 â¤ i â¤ n} be a high-quality, small-scale human-annotated audio classification\ndataset with n audio-label pairs. Let Da-c be a potentially noisy, large-scale weakly-captioned dataset\nof audio-caption pairs with zero intersection with Dsmall. Our goal is to train a T2A model T Î¸ using\nDa-c, then use it to generate a synthetic dataset Dsyn and then finally add it to Dsmall (now attributed as\nDtrain) to improve audio classification performance. This is accomplished through two key steps: first,\naligning the generations from T Î¸ with the acoustic characteristics of Dsmall, and second, generating\nnew captions to prompt T Î¸ for creating synthetic audio data. We elaborate on these next.",
            "content": "4 Preprint Under Review."
        },
        {
            "title": "4.1 ALIGNING THE TEXT-TO-AUDIO MODEL USING PREFERENCE OPTIMIZATION",
            "content": "T2A models trained on internet-scale data often generate audio that diverges from the characteristics of small-scale datasets, resulting in distribution shifts. These mismatches can include variations in spectral (e.g., frequency content), perceptual (e.g., pitch, loudness), harmonic, or other acoustic characteristics 2. This misalignment arises from the non-deterministic nature of T2A generation and it is impractical to provide detailed attributes (like loud or high-pitched) in prompts, as (i) there are no scalable methods for extracting specific attributes for each label, and (ii) T2A models struggle with accurately following finegrained prompt details (Wang et al., 2024). Figure 2: We propose to align the T2A model Î¸ with the small-scale dataset Dsmall using DPO. This helps us generate audios with acoustic characteristics aligned to that of Dsmall. To address these issues, we propose the concept of aligning teaching with learning preferences. Our approach assumes that the classification model (viewed as the student) performs better when trained on synthetic audio that closely matches the inherent acoustic properties of our high-quality and human-labeled Dsmall. Thus, we align the generations of the T2A model (viewed as the teacher) to Dsmall, ensuring that the generated augmentations align with the desired characteristics and sound similar, ultimately enhancing the student models ability to generalize to similarly characterized test data. As shown in Fig. 2, we achieve this using preference optimization (DPO in our case) and align generations of Î¸ with Dsmall. Unlike standard fine-tuning, which can lead to less diverse outputs and overfitting due to narrow focus on minimizing loss, preference optimization encourages greater exploration in the models output space, preventing mode collapse and fostering more diverse augmentations. Additionally, DPO leverages pairwise learning, offering richer training signals compared to the independent outputs used in standard fine-tuning, further mitigating overfitting risks. We detail our two-step approach for DPO optimization below: Step 1: Construction of the Preference Dataset. To create our preference dataset Dpref = {(aw j)}, we first generate template-based captions for each instance in Dsmall in the form: Sound of label, where label is the category associated with the audio. For each instance, we prompt the T2A model times, with all generations starting from randomly initialized Gaussian noise (generation configuration is detailed in Section 5). Each generated audio is then paired with the corresponding ground-truth audio from the gold dataset. This resulting Dpref dataset has instances, where the generated audio is treated as the loser and the ground-truth audio as the winner. This simple approach has proven highly effective in aligning generations by generative models by prior work (Majumder et al., 2024; Tian et al., 2024). 1), , (aw , al 1 , al Step 2: Preference Optimization Using DPO. After constructing Dpref, we train our T2A model on this dataset with DPO using the approach outlined in Section 3. The resulting aligned model is referred to as Î¸ aln. Details of the hyper-parameters used for training are provided in Section 5. 4.2 GENERATING DIVERSE SYNTHETIC AUGMENTATIONS It is not well-studied in the literature on how to leverage synthetic audio generation for downstream tasks. The only existing work relied on manually crafted prompt templates (e.g., Sound of {label}) (Ronchini et al., 2024). It has significant limitation: there is no precise control over the specific components in the generated audio for given caption. This can result in repetitive or completely inconsistent patterns, particularly with weaker T2A models 3. These could bias the model to learn spurious correlations, known issue in synthetic data augmentation (Ghosh et al., 2024c). 2When prompted with sound of bus for the category bus in the TUT-Urban dataset, the generated audio may not reflect the typical bus sounds in European cities (where TUT was recorded), as bus sounds can vary by region, with some featuring loud engines and dense crowds while others have quieter engines and sparse crowds. 3For example, when prompted with Sound of park, we observed that 9 out of 10 times, the model generated the sound of children playing as part of the generated audio. On the other hand, when prompted with Sound of airport, the model generates audios with background announcements, which could vary by regions. Preprint Under Review. Figure 3: Overview of our proposed Language-Guided Audio Imagination for generating diverse synthetic augmentations. Starting with the small-scale dataset, we first generate audio captions and use an LLM to extract acoustic components (Prompt 1). Using these components and audio labels, we prompt the LLM to generate new and diverse captions (Prompt 2), which are then used to prompt the aligned T2A model for audio generation. The generated audios are filtered for label consistency using CLAP, with accepted audios added to the final synthetic dataset. Rejected audios undergo caption revision (Prompt 3) through self-reflection process, and the revised captions are used to regenerate audios, iterating this process times. Example captions are in Table 6. While the alignment stage helps the T2A model generate audio with acoustic characteristics similar to the small-scale dataset (e.g., spectral, harmonic, etc.), it does not fully account for the compositional diversity of the generated audios (e.g., sound events, their temporal relationships, background elements). To tackle this, we propose the concept of language-guided audio imagination, where we propose to imagine novel audios guided by language. Specifically, we leverage the reasoning abilities of LLMs to generate diverse and meaningful captions for category label in controlled yet scalable manner. These captions are then used to prompt our aligned T2A model for generating novel audios. 4.2.1 GENERATING DIVERSE PROMPTS WITH MIXCAP We propose MixCap, prompt generation method that creates diverse and effective captions in three steps: First, we employ GAMA (Ghosh et al., 2024a) to caption all audio files in Dsmall. Next, we prompt an LLM to extract phrases describing the acoustic components of the audio. These components correspond to the acoustic elements such as backgrounds and foreground events, and their attributes and relations, etc (see prompt in Appendix A.2). Finally, for each training instance in Dsmall, we prompt the LLM with the ground-truth label and the extracted components from all instances to generate diverse audio captions that blend existing and new components. This approach prevents overemphasis on events already present in Dsmall and encourages diversity in the generated audio. These captions are then used to prompt the aligned T2A model to generate synthetic data Dsyn. 4.2.2 FILTERING & SELF-REFLECTION syn and the rejected ones as Drej Filtering. After generating captions and their corresponding audio, we filter the audio for label consistency. While LLMs can generate diverse captions, the audio produced must remain aligned with the ground-truth label. To ensure this, we use CLAP to evaluate the generated audio, accepting those that meet similarity threshold of p% and rejecting the rest. We denote the accepted audios as Dacc syn. The CLAP model is pre-trained on Da-c and fine-tuned on Dsmall to adapt to the target dataset. Example captions are in Table 6, and prompts are in Appendix A.2. Self-Reflection. For the rejected audios in Drej syn, we prompt the LLM to reflect on its generated captions and revise them to better align with the target label. Precisely, we feed the LLM with the original caption of each rejected audio along with extracted components from all accepted captions in Dacc syn and task it to rewrite the rejected captions. The revised captions are then used to generate new audio, which is again filtered using CLAP. Audios that meet the threshold are added to Dacc syn; 6 Preprint Under Review. those that do not are returned to Drej illustrate examples of generated and revised captions in Table 6. syn. This process repeats for iterations or until Drej syn is empty. We Fine-tuning for Audio Classification. After the self-reflection stage, the final set of accepted synthetic audios is denoted as Dsyn, containing audio-label pairs, where represents the augmentation factor (e.g., with 100 gold samples, we generate 100 synthetic samples). This set is then combined with Dsmall to form the final training dataset Dtrain, which is then used to train the audio classification model."
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "Models and Hyper-Parameters. For our T2A model, we choose the Stable Audio architecture (Evans et al., 2024b). Stable Audio is based on the Diffusion Transformer architecture, and we refer our readers to the original paper for more details. We train the model from scratch on SoundVECaps (Yuan et al., 2024) (with 1.5 million weakly captioned audio-caption pairs) to avoid any data leakage. For training, we employ batch size of 64, an AdamW optimizer, learning rate of 5e-4, and weight decay of 1e-3 for 40 epochs. For DPO-based alignment tuning, we fine-tune with batch size of 32 and learning rate of 5e-4 for 12 epochs. For our audio classification model, we employ the Audio Spectrogram Transformer (AST) (Gong et al., 2021) (pre-trained on the AudioSet dataset) and fine-tune it with batch size of 24 and learning rate of 1e-4 for 50 epochs. In each experiment, we adjust the number of generated augmentations (ranging from 1 to 5) based on performance on the validation set. All hyper-parameters are fixed across splits, and results are averaged across 3 runs. Datasets. We create small-scale datasets by downsampling commonly used audio classification datasets to samples. Our selected datasets include mix of music, everyday sounds, and acoustic scenes. For multi-class classification, we use NSynth Instruments, TUT Urban, ESC50 (Piczak), USD8K (Salamon et al., 2014), GTZAN (Tzanetakis et al., 2001), Medley-solos-DB (Lostanlen & Cella, 2017), MUSDB18 (Rafii et al., 2017), DCASE Task 4 (Mesaros et al., 2017), and Vocal Sounds (VS) (Mesaros et al., 2017), evaluating them for accuracy. For multi-label classification, we use the FSD50K (Fonseca et al., 2022) dataset and evaluate it using the Fmacro metric. We exclude AudioSet from evaluation as Sound-VECaps is derived from it. To ensure downsampled dataset that has label distribution similar to that of the of the original dataset, we employ stratified sampling based on categories. Our experiments are conducted with = {50, 100, 200, 500} samples, and we downsample the validation sets for training while evaluating all models on the original test splits. 1 Baselines. Our baselines include: (i) Gold-only (No Aug.): We employ only the small-scale dataset for training and do not perform any augmentations. (ii) Traditional augmentation baselines: SpecAugment, Noise Augmentation (we either add random Gaussian noise or background noise from AudioSet and present averaged results), Pitch and Time Shift and Audiomentations (Jordal, 2021) combination of the AddGaussianNoise, TimeStretch, PitchShift, Shift, SpecFrequencyMask, TimeMask and TimeStretch combination with the highest average score on 4 datasets and splits and was selected after grid search over all possible combinations). (iii) Generative baselines: Vanilla Synthetic Augmentation (Vanilla Syn. Aug.) we prompt TÎ¸ with template captions), Vanilla Syn. Aug. + LLM Caps we prompt TÎ¸ with random captions generated with LLMs. (iv) Finally, inspired by Burg et al. (2023), we also employ retrieval baseline where instead of generating augmentations from our T2A model trained on Da-c, we just retrieve the top-n instances (w.r.t. CLAP similarity) from the AudioSet for each instance in Dsmall as our augmentations. Ablations. We ablate Synthio with: (i) w/o Self-Reflection: We remove the repetitive self-reflection module and iterate and filter only once; (ii) w/o DPO: We skip the tuning step and prompt the un-alined Î¸ for augmentations; (iii) w/ ERM: We replace DPO tuning with standard Empirical Risk Minimization(ERM)-based fine-tuning with diffusion loss; (iv) w/ Template Captions: We remove MixCap and self-reflection modules and prompt Î¸ aln with template captions; (v) w/o MixCap: Similar to our Random Captions baseline, but we retain all other modules of Synthio. 6 RESULTS AND DISCUSSION In this section, we present our main benchmark results, followed by detailed analyses to highlight Synthios strengths and explore its versatility. Key areas of investigation include Synthios consistency and diversity in augmentations, performance on synthetic-only datasets, scalability, impact on longtailed categories, and its potential extension to complex audio understanding tasks like captioning. 7 Preprint Under Review. Table 1: Result comparison of Synthio with baselines on 10 datasets and 4 small-scale settings. refers to the number of samples in the small-scale dataset augmented with synthetic data. Synthio outperforms our baselines by 0.1% - 39%. We also highlight the relative improvements by Synthio compared to the Gold-only. Method ESC-50 USD8K GTZAN Medley Gold-only (No Aug.) 22.25 55.09 47.05 47.23 TUT 37.60 NSynth VS MSDB DCASE FSD50K 33.32 77.49 56.85 12.09 8.69 Random Noise Pitch Shifting SpecAugment Audiomentations Retrieval Vanilla Syn. Aug. + LLM Caps. Synthio (ours) 50 w/ Template Captions w/ ERM w/o Self-Reflection w/o MixCap w/o DPO 57.42 59.32 58.36 60.13 37.14 63.54 65.84 35.86 37.22 36.73 38.24 35.80 41.50 40.90 45.20 46.80 46.00 47.25 42.55 55.35 63. 18.50 46.55 20.55 48.17 19.50 47.18 20.35 48.30 19.20 43.65 40.75 47.23 36.80 55.36 49.50+122% 76.12+38% 68.20+44% 60.58+28% 43.84+17% 40.83+22% 80.67+4% 54.52 41.25 56.60 41.30 58.00 45.25 52.18 42.70 52.55 36.55 32.42 34.34 27.32 28.15 31.27 33.17 38.17 76.41 78.17 77.27 79.12 71.42 78.37 78.77 66.11 69.80 72.57 64.72 68.12 78.57 79.75 78.56 78.70 79.03 64.40 61.70 64.55 54.65 56. 37.52 38.62 39.50 36.13 40.31 41.37 42.00 42.81 41.93 41.39 52.55 54.50 53.25 54.51 51.35 54.10 57.05 60.15+5% 59.60 57.75 57.25 58.80 57.55 9.15 10.04 9.22 11.44 7.28 11.76 11.84 13.21 12.93 12.81 13.28 10.53 15.89 13.07 17.23+42% 15.41+77% 14.15 13.28 15.63 14.82 14.53 13.76 14.17 14.02 13.61 11. Gold-only (No Aug.) 56.75 72.89 64.15 57.81 47. 39.11 84.32 65.60 12.50 10.70 Random Noise Pitch Shifting SpecAugment Audiomentations Retrieval Vanilla Syn. Aug. + LLM Caps. Synthio (ours) 100 w/ Template Captions w/ ERM w/o Self-Reflection w/o MixCap w/o DPO 71.54 73.52 72.43 73.82 68.24 77.31 79.73 65.50 66.75 69.75 71.05 61.55 68.25 67.90 46.21 47.50 50.07 51.14 45.39 49.96 48. 58.50 56.98 59.55 58.46 47.50 58.06 59.32 48.50 52.45 54.83 77.25 63.58 67.05 65.79 83.35+47% 85.00+17% 71.20+11% 71.23+23% 52.42+11% 44.92+15% 86.70+3% 64.20 78.00 66.57 73.20 68.52 77.65 66.52 73.50 60.81 66.75 38.20 39.53 41.96 42.15 37.84 42.31 41.83 83.33 85.07 85.14 85.24 83.27 84.78 84.83 80.32 81.81 82.38 78.30 75.46 85.11 84.73 82.53 83.52 84.67 42.76 43.74 44.38 42.27 40. 68.15 67.25 69.55 68.50 66.15 49.95 51.11 51.75 50.63 48.78 66.15 68.25 66.40 68.40 58.55 63.55 65.95 68.80+5% 66.05 68.00 66.20 66.35 67.85 14.95 15.02 15.72 14.29 10.05 13.78 14.12 13.35 12.19 14.17 16.93 10.93 15.73 16.32 19.38+55% 17.33+62% 16.32 17.21 15.89 16.77 14.83 14.32 15.19 13.96 14.93 13. Gold-only (No Aug.) 84.75 74.80 77.00 67.41 55. 48.77 87.38 68.80 23.15 14.82 Random Noise Pitch Shifting SpecAugment Audiomentations Retrieval Vanilla Syn. Aug. + LLM Caps. Synthio (ours) 200 w/ Template Captions w/ ERM w/o Self-Reflection w/o MixCap w/o DPO 83.55 84.90 85.10 85.25 82.55 85.40 85.80 86.10+2% 85.95 85.35 84.85 84.95 84.80 75.50 78.55 76.25 77.30 73.65 77.10 79.55 75.15 74.48 76.46 75.80 71.20 77.96 78.37 82.81+11% 82.05+7% 80.84 79.82 81.97 81.27 76. 79.25 80.20 78.25 79.55 75.30 54.42 55.44 55.72 55.21 53.25 55.51 54.73 66.71 67.74 65.70 67.00 65.80 78.97 74.14 79.40+18% 56.83+3% 77.56 74.43 75.53 73.50 73.13 55.99 55.76 56.39 55.27 55.99 17.77 18.73 18.91 19.17 15.36 19.04 19.34 24.82 23.11 27.36 26.29 19.51 28.55 28. 86.45 87.47 87.42 86.08 86.28 86.49 87.02 47.83 65.45 48.12 69.80 54.80 69.25 53.15 70.50 47.63 63.55 55.20 72.95 56.21 73.16 57.10+17% 87.52+0.2% 80.40+17% 32.81+42% 21.60+46% 74.55 56.33 74.40 56.15 75.55 56.76 78.55 55.54 73.15 52.73 87.25 86.92 86.22 85.78 86.52 20.19 19.26 18.45 20.58 19.58 29.12 29.81 31.13 28.35 26.79 Gold-only (No Aug.) 90.75 87.88 79.25 75.65 65.72 63. 89.33 72.05 36.68 20.19 Random Noise Pitch Shifting SpecAugment Audiomentations Retrieval Vanilla Syn. Aug. + LLM Caps. Synthio (ours) 500 w/ Template Captions w/ ERM w/o Self-Reflection w/o MixCap w/o DPO 89.55 88.50 89.50 89.95 85.50 91.50 89.90 92.10+2% 91.70 91.20 91.85 91.70 90.15 88.25 88.83 89.01 88.75 84.86 88.18 86.91 89.18+2% 88.93 88.25 88.72 87.93 88.21 78.90 79.75 80.25 81.25 77.25 79.35 79.55 82.25+4% 80.40 79.15 80.15 80.95 79.45 76.01 75.61 76.68 77.66 73.62 77.97 77.91 78.62+4% 76.64 77.38 78.57 76.61 76. 65.10 64.93 66.74 66.92 62.73 65.93 65.95 67.81+3% 66.47 65.80 66.21 65.91 66.01 64.15 64.59 64.43 65.21 61.44 64.52 64.39 65.40+3% 64.71 64.27 63.89 64.23 63.61 90.15 89.87 90.38 91.34 87.33 90.31 90.09 91.42+2% 90.97 88.74 90.17 90.23 89.83 73.25 72.15 72.95 73.65 70.20 73.25 73.05 74.70+3% 73.35 74.20 72.15 73.40 72.65 37.21 36.54 38.33 38.75 30.17 37.26 38.74 39.24+6% 38.28 38.03 37.97 39.11 37.04 21.57 22.17 22.52 23.11 15.28 24.39 23.83 24.93+23% 23.65 23.72 23.62 22.83 21. Main Results. Table 1 showcases the performance comparison between Synthio and the baseline methods. Synthio consistently outperforms all baselines by 0.1%-39%, achieving notable improvements in overall classification accuracy compared to Gold-only. The highest gains are observed on USD8K, while the least is on Vocal Sound, likely due to the T2A datasets heavy representation of music compared to the more sparse vocal sounds. Performance gains tend to decrease as the number of gold samples in Dsmall grows, aligning with observed trends in prior studies. Although Vanilla Synthetic Augmentations emerge as the strongest baseline, they lag behind Synthio by 3.5%. Ablations. The most significant performance drop in Synthio is observed w/o DPO, resulting in an average decline of 4.5%, highlighting the crucial role of consistency in generating effective augmentations. Second to w/o DPO, the the highest drop is seen in w/ Template Captions, with average decline of 2.7%, thus highlighting the importance of MixCap. 8 Preprint Under Review. Figure 4: Comparison of spectral and pitch features between generated audios in Dsyn and real audios in Dsmall (for = 100). Synthio-generated audios closely replicate the features of real data, demonstrating its ability to produce augmentations that maintain consistency with the original dataset (also see FAD scores in Sec. A.4.3). 6.1 HOW CONSISTENT AND DIVERSE ARE AUGMENTATIONS GENERATED BY SYNTHIO? Table 2: CLAP similarity score between real audios and generated data. Lower scores show higher compositional diversity among generated augs. Fig. 4 compares the distributions of pitch and various spectral features between generated audios in Dsyn and real audios in Dsmall across different methods on the USD8K and NSynth datasets. The features analyzed include Pitch Salience (clarity of the main pitch) (Ricard, 2004), Spectral Flatness (tonal vs. noise-like quality) (Peeters, 2004), Flux (rate of spectral change) (Tzanetakis & Cook, 1999), and Complexity (level of sound detail) (Laurier et al., 2010). Notably, Synthio-generated audios closely replicate the spectral features of the original audios, showing the best alignment among all methods and demonstrating Synthios ability to generate consistent augmentations. Table 2 presents CLAP similarity scores between ground-truth audios and their generated augmentations, averaged across all dataset instances. Audios generated with Synthio achieve the highest compositional diversity for generated audios among all baselines. Table 8 shows that audios generated using Synthio have the highest similarity with the ground-truth category label. w/ Template Captions w/ ERM w/ Template Captions w/ ERM Vanilla Syn. Aug. Synthio (ours) Vanilla Syn. Aug. Synthio (ours) USD8K() NSynth() 47.22 34.58 46.84 52.54 45.17 35.09 46.82 50.01 31.76 22.97 33.00 42.33 33.81 23.03 37.16 43.98 Method 200 # 6.2 HOW GOOD ARE SYNTHETIC AUDIOS GENERATED BY SYNTHIO? Consistent with prior findings in vision (He et al., 2023), we observe that synthetic data alone performs sub-optimally compared to human-annotated data. However, our results show that enhancing the consistency and diversity of synthetic data aided by smallscale version of the target dataset significantly improves model performance. Table 3 compares models trained exclusively on synthetic data with our baselines (i.e., only Dsyn is used for training AST). Synthio outperforms all baselines by 0.1%-26.25%, with DPO-based alignment driving the improvements. Table 3: Performance comparison of Synthio with baselines on synthetic-only audio classification. Method GTZAN VS TUT MSDB Gold-only (No Aug.) Vanilla Syn. Aug. Synthio (ours) 100 w/ Template Captions w/ ERM w/o DPO Gold-only (No Aug.) Vanilla Syn. Aug. Synthio (ours) 200 w/ Template Captions w/ ERM w/o DPO 64.15 29.05 33.10 24.50 25.65 17.60 77.00 32.35 35.15 29.90 28.10 19.85 84.32 47. 34.13 39.20 30.99 32.76 21.57 21.69 24.51 21.73 24.40 20.39 87.38 55.32 41.96 48.14 35.53 36.29 26.85 24.23 27.00 23.61 25.71 21. 65.60 35.60 56.45 40.40 42.85 30.20 68.80 39.25 61.45 41.20 46.70 36.75 6.3 CAN SYNTHIO BE EXTENDED TO THE MORE COMPLEX AUDIO CAPTIONING TASK? Audio captioning, unlike classification, involves describing the content of an audio sample using natural language, making it more complex task. To demonstrate Synthios effectiveness for audio captioning, we evaluated it on down-sampled versions of AudioCaps. For this task, we adapted Synthio by extracting components directly from the available captions and retrained our T2A model on modified version of Sound-VECaps, excluding any audio from AudioSet. 9 Preprint Under Review. Table 4: Performance comparison of Synthio with baselines on audio captioning. Training and evaluation were conducted using the EnCLAP framework (Kim et al., 2024), and the dataset was enriched with 4 synthetic samples. As shown in Table 4, Synthio significantly outperforms baseline settings, with improvements largely due to better alignment w/ DPO. However, manual inspection revealed that generated audios occasionally do not match their captions compositionally, reflecting limitations of the current T2A model. While this issue does not affect classification, it poses challenges for captioning. We will explore more advanced methods as part of future work. Gold-only (No Aug.) Vanilla Syn. Aug. VECaps Retrieval Synthio (ours) Gold-only (No Aug.) Vanilla Syn. Aug. VECaps Retrieval Synthio (ours) METEOR () CIDEr () 0.0754 0.0741 0.0550 0.104 0.112 0.140 0.100 0.202 0.127 0.135 0.088 0.185 0.067 0.092 0.068 0.119 0.112 0.136 0.082 0. 0.125 0.128 0.108 0.169 0.157 0.166 0.097 0.256 0.148 0.157 0.094 0.227 SPIDEr () SPICE () Method 1000 500 Table 5: Performance comparison of Synthio with other baselines on different values of ."
        },
        {
            "title": "6.4 HOW WELL DOES SYNTHIO SCALE?\nTable 5 compares the performance of Synthio,\nSpecAugment, and Vanilla Synthetic Augmentations\nacross different scaling factors N = {1, 2, 3, 4, 5},\nwhere N represents the number of synthetic sam-\nples generated per original sample in the small-scale\ndataset. As observed, SpecAugment, a traditional\naugmentation method, cannot scale with increasing\nN , and the performance of Vanilla Synthetic Aug-\nmentations plateaus at higher N . A similar saturation\noccurs with Synthio when MixCap is not used. Even\nwithout DPO, Synthio maintains better scalability,\nthough with reduced overall performance. These re-\nsults highlight that MixCapâs ability to generate diverse captions is crucial for Synthioâs scalability.",
            "content": "SpecAugment Vanilla Syn. Aug. Synthio (ours) w/o MixCap w/o DPO SpecAugment Vanilla Syn. Aug. Synthio (ours) w/o MixCap w/o DPO 47.50 71.25 83.35 73.50 66.75 47.50 75.60 83.15 72.85 66.60 47.50 76.75 82.55 71.55 65.95 47.50 77.25 81.75 68.45 64. 47.50 67.90 77.45 64.30 61.55 41.96 38.27 44.81 42.15 39.82 41.96 41.54 44.92 42.27 40.31 41.96 42.31 43.56 41.95 40.17 41.96 35.28 36.37 41.08 39.42 41.96 33.13 35.28 40.41 39. Dataset Method Scaling Factor NSynth ESC50 3x 4x 1x 2x 5x 6.5 DOES SYNTHIO HELP LONG-TAILED CATEGORIES? Figure 5 shows the classification accuracy on four underrepresented categories in the ESC-50 and NSynth datasets, comparing performance before and after applying Synthio augmentations. We selected categories with the lowest frequency in the datasets, such as flute and guitar, which appear only once in the down-sampled sets. Synthio significantly boosts accuracy, with improvements up to 48%. Notably, categorie labels like flute and guitar, which originally had 0% accuracy, show substantial gains with Synthio augmentation. This demonstrates Synthios effectiveness in enhancing performance on long-tail labels, common challenge in realworld datasets (Zhang et al., 2023). Figure 5: Category-wise improvement in performance with Synthio augmentations for long-tailed categories."
        },
        {
            "title": "7 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "content": "We introduced Synthio, novel approach for augmenting small-scale audio classification datasets with synthetic data. Synthio incorporates several innovative components to generate augmentations that are both consistent with and diverse from the small-scale dataset. Our extensive experiments demonstrate that even when using T2A model trained on weakly-captioned AudioSet, Synthio significantly outperforms multiple baselines. However, Synthio has some limitations: (i) Its performance is influenced by the capabilities of the T2A model and the quality of its training data. As T2A models continue to improve, we expect Synthios performance to benefit accordingly. (ii) The process of generating audio captions using LLMs may introduce biases inherent in the LLMs into the training process. (iii) Synthio is computationally more intensive than traditional augmentation methods due to the need for prompting LLMs and T2A models. We anticipate that ongoing advancements in model efficiency will help mitigate these computational challenges. 10 Preprint Under Review."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "All codes will be open-sourced upon paper acceptance, including all T2A checkpoints. All experimental details, including training parameters and hyper-parameters, are provided in Section 5."
        },
        {
            "title": "REFERENCES",
            "content": "Haider Al-Tahan and Yalda Mohsenzadeh. Clar: Contrastive learning of auditory representations. In International Conference on Artificial Intelligence and Statistics, pp. 25302538. PMLR, 2021. Max Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. Image retrieval outperforms diffusion models on data augmentation. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/ forum?id=xflYdGZMpv. Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang. An empirical survey of data augmentation for limited data learning in nlp. Transactions of the Association for Computational Linguistics, 11:191211, 2021. URL https://api.semanticscholar.org/CorpusID: 235422524. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 736740. IEEE, 2020. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024a. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024b. Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra. FSD50K: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2022. Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Goldstein, and Andrew Gordon Wilson. How much data are augmentations worth? an investigation into scaling laws, invariance, and implicit regularization. arXiv preprint arXiv:2210.06441, 2022. Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776780. IEEE, 2017. Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023. Sreyan Ghosh, Ashish Seth, and Umesh. Decorrelating feature spaces for learning general-purpose audio representations. IEEE Journal of Selected Topics in Signal Processing, 16(6):14021414, 2022. doi: 10.1109/JSTSP.2022.3202093. 11 Preprint Under Review. Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran, Sakshi, Utkarsh Tyagi, and Dinesh Manocha. DALE: Generative data augmentation for low-resource legal NLP. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 85118565, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.528. URL https://aclanthology.org/2023.emnlp-main.528. Sreyan Ghosh, Ashish Seth, Srinivasan Umesh, and Dinesh Manocha. Mast: Multiscale audio spectrogram transformers. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023b. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. Gama: large audio-language model with advanced audio understanding and complex reasoning abilities, 2024a. URL https:// arxiv.org/abs/2406.11768. Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Reddy Evuru, Ramaneswaran S, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. Compa: Addressing the gap in compositional reasoning in audio-language models. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/ forum?id=86NGO8qeWs. Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, Chandra Kiran Reddy Evuru, , Ramaneswaran S, Sakshi, and Dinesh Manocha. ABEX: Data augmentation for low-resource NLU via expanding abstract descriptions. In The 62nd Annual Meeting of the Association for Computational Linguistics, 2024c. Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778, 2021. Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI. IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION? In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=nUmCcZ5RKF. Jordal. audiomentations, 2021. URL https://zenodo.org/record/13639627. Akbar Karimi, Leonardo Rossi, and Andrea Prati. AEDA: An easier data augmentation technique for text classification. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wentau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 27482754, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.234. URL https://aclanthology. org/2021.findings-emnlp.234. Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Jaeyeon Kim, Jaeyoon Jung, Jinjoo Lee, and Sang Hoon Woo. Enclap: Combining neural audio codec and audio-text joint embedding for automated audio captioning. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 67356739, 2024. doi: 10.1109/ICASSP48485.2024.10446672. Zhifeng Kong, Sang-gil Lee, Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Rafael Valle, Soujanya Poria, and Bryan Catanzaro. Improving text-to-audio models with synthetic captions. arXiv preprint arXiv:2406.15487, 2024. 12 Preprint Under Review. Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=CYK7RfcOzQ4. Cyril Laurier, Owen Meyers, Joan Serra, Martin Blech, Perfecto Herrera, and Xavier Serra. Indexing music by mood: design and integration of an automatic content-based annotator. Multimedia Tools and Applications, 48:161184, 2010. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On llms-driven synthetic data generation, curation, and evaluation: survey. arXiv preprint arXiv:2406.15126, 2024. Vincent Lostanlen and Carmine-Emanuele Cella. Deep convolutional networks on the pitch spiral for musical instrument recognition, 2017. URL https://arxiv.org/abs/1605.06644. Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization. arXiv preprint arXiv:2404.09956, 2024. Pranay Manocha, Zeyu Jin, Richard Zhang, and Adam Finkelstein. Cdpam: Contrastive learning for perceptual audio similarity. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 196200. IEEE, 2021. Irene MartÄ±n-Morato and Annamaria Mesaros. What is the ground truth? reliability of multi-annotator data for audio tagging. In 2021 29th European Signal Processing Conference (EUSIPCO), pp. 7680. IEEE, 2021. Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, Emmanuel Vincent, Bhiksha Raj, and Tuomas Virtanen. Dcase 2017 challenge setup: Tasks, datasets and baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events, 2017. Loris Nanni, Gianluca Maguolo, and Michelangelo Paci. Data augmentation approaches for improving animal audio classification. Ecological Informatics, 57:101084, 2020. An Thanh Nguyen, Byron Wallace, Junyi Jessy Li, Ani Nenkova, and Matthew Lease. Aggregating and predicting sequence labels from crowd annotations. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 299309, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1028. URL https://aclanthology.org/P17-1028. Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. Byol for audio: Self-supervised learning for general-purpose audio representation. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 18. IEEE, 2021. Daniel Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin Cubuk, and Quoc Le. Specaugment: simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Geoffroy Peeters. large set of audio features for sound description (similarity and classification) in the cuidado project. CUIDADO Ist Project Report, 54(0):125, 2004. Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pp. 10151018. ACM Press. ISBN 978-1-4503-34594. doi: 10.1145/2733373.2806390. URL http://dl.acm.org/citation.cfm?doid= 2733373.2806390. 13 Preprint Under Review. Zafar Rafii, Antoine Liutkus, Fabian-Robert Stoter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The MUSDB18 corpus for music separation, December 2017. URL https://doi.org/10. 5281/zenodo.1117372. Zhao Ren, Kun Qian, Tanja Schultz, and Bjorn W. Schuller. An overview of the icassp special session on ai security and privacy in speech and audio processing. In Proceedings of the 5th ACM International Conference on Multimedia in Asia Workshops, MMAsia 23 Workshops, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400703263. doi: 10.1145/3611380.3628563. URL https://doi.org/10.1145/3611380.3628563. Julien Ricard. Towards computational morphological description of sound. DEA pre-thesis research work, Universitat Pompeu Fabra, Barcelona, 2004. Francesca Ronchini, Luca Comanducci, and Fabio Antonacci. Synthesizing soundscapes: Leveraging text-to-audio models for environmental sound classification. arXiv preprint arXiv:2403.17864, 2024. Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive learning of general-purpose audio representations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 38753879. IEEE, 2021. J. Salamon, C. Jacoby, and J. P. Bello. dataset and taxonomy for urban sound research. In 22nd ACM International Conference on Multimedia (ACM-MM14), pp. 10411044, Orlando, FL, USA, Nov. 2014. Ashish Seth, Sreyan Ghosh, Srinivasan Umesh, and Dinesh Manocha. Slicer: Learning universal audio representations using low-resource self-supervised pre-training. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Connor Shorten and Taghi Khoshgoftaar. survey on image data augmentation for deep learning. Journal of big data, 6(1):148, 2019. Janne Spijkervet. Spijkervet/torchaudio-augmentations, 2021. URL https://zenodo.org/ record/4748582. Jinchuan Tian, Chunlei Zhang, Jiatong Shi, Hao Zhang, Jianwei Yu, Shinji Watanabe, and Dong Yu. Preference alignment improves language model-based tts. arXiv preprint arXiv:2409.12403, 2024. Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data In The Twelfth International Conference on Learning augmentation with diffusion models. Representations, 2024. URL https://openreview.net/forum?id=ZWzUA9zeAg. George Tzanetakis and Perry Cook. Multifeature audio segmentation for browsing and annotation. In Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA99 (Cat. No. 99TH8452), pp. 103106. IEEE, 1999. George Tzanetakis, Georg Essl, and Perry Cook. Automatic musical genre classification of audio signals, 2001. URL http://ismir2001.ismir.net/pdf/tzanetakis.pdf. Jason Wang, Luis Perez, et al. The effectiveness of data augmentation in image classification using deep learning. Convolutional Neural Networks Vis. Recognit, 11(2017):18, 2017. Yuanyuan Wang, Hangting Chen, Dongchao Yang, Zhiyong Wu, Helen Meng, and Xixin Wu. Audiocomposer: Towards fine-grained audio generation with natural language descriptions. arXiv preprint arXiv:2409.12560, 2024. Jason Wei and Kai Zou. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6382 6388, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1670. URL https://aclanthology.org/D19-1670. 14 Preprint Under Review. Zeng Weili, Yichao Yan, Qi Zhu, Zhuo Chen, Pengzhi Chu, Weiming Zhao, and Xiaokang Yang. Infusion: Preventing customized text-to-image diffusion from overfitting. In ACM Multimedia 2024, 2024. Yi Yuan, Dongya Jia, Xiaobin Zhuang, Yuanzhe Chen, Zhengxi Liu, Zhuo Chen, Yuping Wang, Improving audio generation with visual Yuxuan Wang, Xubo Liu, Mark Plumbley, et al. enhanced caption. arXiv preprint arXiv:2407.04416, 2024. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 60236032, 2019. Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):1079510816, 2023. Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, and Jun Liu. Ltgc: Long-tail recognition via leveraging llms-driven generated content. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1951019520, June 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "Table of Contents: A.1 Background on Diffusion Models A.2 Prompts A.3 Examples A.4 Extra Results A.5 Dataset Details A.1 DIFFUSION MODELS Diffusion models consist of two main processes: forward process and reverse process. Given data point x0 with probability distribution p(x0), the forward diffusion process gradually adds Gaussian noise to x0 according to pre-set variance schedule Î²1, , Î²T and degrades the structure of the data. At the time step t, the latent variable xt is only determined by the xt1 due to its discrete-time Markov process nature, and can be expressed as: 1 Î²txt1, Î²tI), p(xt xt1) = (xt; (12) As increases over several diffusion steps, p(xT ) approaches unit spherical Gaussian distribution. The marginal distribution of xt at any given step can be expressed analytically as: p(xt x0) = (xt; (13) where Î±t = s=1(1 Î²s). The reverse process aims to reconstruct the original data from the noise-corrupted version by learning series of conditional distributions. The transition from xt to xt1 is modeled as: Î±tx0, (1 Î±t)I), , Ït Î¸ ), (14) pÎ¸(xt1 xt) = (xt1; Âµt1 Âµt1 (xt Î¸ = Î¸ Î²t 1 Î±t 1 Î±t Ït12 ÏµÎ¸ (xt, t)) , 1 Î±t1 1 Î±t i=1 Î±i, Î¸ represents the learnable parameters, Âµt1 is the standard deviation estimate, and ÏµÎ¸(xt, t) is the noise estimated by the neural network. where Î±t = 1 Î²t, Î±t = Ït12 The reverse process estimates the data distribution p(x0) by integrating over all possible paths: t=1 pÎ¸(xt1 xt) dx1 pÎ¸(x0) = pÎ¸(xT ) is the mean estimate, Î²t, (16) (17) = Î¸ Î¸ Î¸ (15) 15 Preprint Under Review. where pÎ¸(xT ) = (xT ; 0, I). At inference time, the diffusion model iteratively executes the reverse process (Eq. 17) times starting from randomly sampled Gaussian Noise (Ïµ (0, I)). A.2 PROMPTS Fig. 6, 7, 8 and 9 illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7. Figure 6: LLM prompt (Prompt 1) for extracting components from audio captions. Figure 7: LLM prompt (Prompt 2) for generating new audio captions given elements from existing captions. A.3 EXAMPLES Table 6 presents examples of captions generated by the Synthio framework, along with their revised versions for captions that were initially rejected. 16 Preprint Under Review. Figure 8: LLM prompt for generating random captions for Random Captions baselines in Table 1. Figure 9: LLM prompt (Prompt 3) for rewriting captions of rejected audios. A.4 EXTRA RESULTS A.4.1 RESULTS ON THE FULL TRAINING SPLITS Table 7 presents the performance comparison of Synthio on the full original dataset splits (where the entire training set is used without any downsampling). While Synthio outperforms all baselines, traditional augmentation methods prove to be much more competitive in this scenario. This contrasts 17 Preprint Under Review. Dataset Label USD8k children playing USD8k children playing USD8k street music USD8k street music TUT TUT TUT TUT airport airport bus bus NSynth keyboard NSynth keyboard NSynth organ NSynth organ Medley Violin Medley Violin Medley Flute Medley Flute AudioCaps - AudioCaps - Generated Caption Revised Caption Children playing in bustling city park with distant traffic noise Children playing in schoolyard during recess with teachers whistle Street music playing near busy intersection filled with honking cars and pedestrians. Street music from bustling market as people chatter and vendors shout airport with people talking and walking around in an empty hallway In the airport, people are talking with the sound of crowd of people in the background, as announcements play. Bus passing by on road while people are chatting at nearby cafe. bus passing by on road as it continues to blow into the microphone keyboard accompaniment to live band performance at bustling cafe. man typing on keyboard at office serene church service with an organ playing melody and soft brass are playing. An organ plays as guitars are playing together in the background. violin being played during classical symphony orchestra performance violin performing lively jig at bustling street fair flute playing in tranquil forest during the early morning Flute performance in bustling city park during sunny afternoon. dog barks repeatedly in the background while car engine starts In the distance, faint thunder rumble is audible, accompanied by the gentle rustling of leaves in the wind. NA Children playing in neighborhood alley with sound of distant construction NA Street music echoing through an alleyway during lively street festival. NA airport ambiance with people talking and children running around NA bus idling on road with birds chirping nearby NA keyboard rhythms echoing in an empty auditorium during rehearsal break NA An organ plays during lively music festival with various instruments. NA Violin solo during quiet candlelight dinner in fancy restaurant. NA Flute music echoing in an ancient stone cathedral. - Soft rain falls on metal roof, creating rhythmic tapping sound. Table 6: Examples of generated and revised captions from the Synthio methodology. Table 7: Comparison of Synthio and other baselines on the full original dataset splits (using all samples from the original training set as Dsmall). Method USD8K GTZAN Medley VS MSDB Gold-only Random Noise Pitch Shift Spec. Aug. Audiomentations Retrieval Vanilla Syn. Aug. Synthio (ours) 88.23 86.17 87.58 87.92 88.01 78.27 89.57 89.57 82.00 82.35 83.02 82.50 82.75 69.25 82.85 82.85 80.99 79.72 79.63 79.14 81.26 73.24 81.79 81.79 92.73 92.94 92.17 92.42 92.47 80.43 93.15 93. 73.9 74.55 74.6 74.5 75.05 69.95 75.85 74.24 with the results in Table 1 where traditional augmentations showed minimal improvements in performance. 18 Preprint Under Review. Table 8: CLAP score between generated audios and the label. Method USD8K NSynth Real Vanilla Syn. Aug. Synthio 100 w/ Template Captions w/ ERM Real Vanilla Syn. Aug. Synthio 200 w/ Template Captions w/ ERM 12.67 14.34 31.26 29.31 24.15 10.13 12.55 21.87 20.31 17.14 14.46 17.54 27.32 26.62 21.54 9.4 12.91 16.16 15.82 13. Table 9: Comparison of our trained Stable Diffusion model on AudioCaps test set Model FAD PANN () FAD VGG () IS PANN () CLAP LAION () AudioLDM2-large Tango-Full0FT-AC Tango 2 Make-an-Audio 2 Stable Audio VECaps (ours) Stable Audio VECaps + AudioCaps-FT (ours) 32.50 18.47 17.19 11.75 15.12 14.93 1.89 2.19 2.54 1.80 2.21 2.19 8.55 8.80 11.04 - 15.07 15.42 0.45 0.57 0.52 0.60 0.57 0.56 A.4.2 AUDIO GENERATION RESULTS FOR OUR TRAINED STABLE DIFFUSION Table 9 presents comparison of audio generation results across several evaluation metrics. We evaluate our trained Stable Diffusion model (used in our experiments, including version further fine-tuned on AudioCaps) against other available models and baselines from the literature. Notably, our model performs competitively with other fully open-source models across most metrics. A.4.3 FAD SCORES FOR GENERATED AUGMENTATIONS To offer an alternative perspective on the distributional consistency between the generated augmentations and the ground-truth small-scale dataset, we compare the Frechet Audio Distance (FAD) scores (Kilgour et al., 2018). For this experiment, we use Synthio with Template Captions. Table 10 presents comparison of FAD scores between Synthio and other baselines. Synthio achieves the highest FAD score, indicating that it produces the most consistent audio augmentations. Table 10: Comparison of FAD score of Vaniall Syn. Aug. and Stable Audio VECaps (ours). Dataset Model FAD VGG ()"
        },
        {
            "title": "100 NSynth",
            "content": "Vanilla Syn. Aug. Stable Stable Audio VECaps (ours) 200 TUT Vanilla Syn. Aug. Stable Audio VECaps (ours) 1.83 1.42 1.71 1.45 A.5 DATASET DETAILS NSynth Instruments: NSynth is large-scale dataset consisting of musical notes played by variety of instruments. It includes rich set of acoustic features from instruments like guitars, flutes, and more, providing diverse sound textures for classification tasks. TUT Urban: The TUT Urban dataset captures everyday sounds from urban environments, including It is commonly used for acoustic scene noises like traffic, human activities, and construction. classification and environmental sound recognition. ESC-50: ESC-50 is well-known dataset for environmental sound classification, containing 50 categories of everyday sounds such as animal noises, natural elements, and human activities, making it suitable for multi-class classification challenges. 19 Preprint Under Review. UrbanSound8K (USD8K): USD8K is curated collection of urban sounds divided into ten classes, including sirens, street music, and car horns. It is used widely for evaluating models on sound event detection in real-world scenarios. GTZAN: GTZAN is music genre classification dataset that includes ten music genres such as pop, rock, and jazz. It is standard benchmark for evaluating music classification models, although it has known data quality issues. Medley-solos-DB: This dataset consists of solo recordings of different musical instruments, making it valuable for studying isolated instrument sounds and training models for music instrument recognition. MUSDB18: MUSDB18 is used primarily for music source separation tasks. It contains full-track recordings of different music styles, providing mix of vocals, drums, bass, and other instruments, useful for multi-class classification. DCASE Task 4: Part of the DCASE challenge, this dataset focuses on domestic sound scene and event classification. It includes various audio clips recorded in home environments, often used for anomaly detection and sound event classification. Vocal Sounds (VS): This dataset includes various vocal sounds such as singing, speech, and vocal effects, providing rich data for studying voice classification and enhancing models for vocal audio recognition tasks."
        }
    ],
    "affiliations": [
        "NVIDIA, CA, USA",
        "University of Maryland, College Park, USA"
    ]
}