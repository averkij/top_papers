{
    "paper_title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding",
    "authors": [
        "Hao Lu",
        "Jiahao Wang",
        "Yaolun Zhang",
        "Ruohui Wang",
        "Xuanyu Zheng",
        "Yepeng Tang",
        "Dahua Lin",
        "Lewei Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio."
        },
        {
            "title": "Start",
            "content": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding Hao Lu*, Jiahao Wang*, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin and Lewei Lu SenseTime Research 5 2 0 2 2 ] . [ 2 6 9 4 1 2 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "achieved remarkable progress Video multimodal large language models (Video-MLLMs) have in video underthey remain vulnerable to hallucinastanding. However, tionproducing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above the first benchmark issues, we introduce ELV-Halluc, dedicated to long-video hallucination, enabling systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the models ability to distinguish semantics within and across events. To support this, we curate dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including substantial 27.7% reduction in SAH ratio. The dataset and evaluation code can be found at https://github.com/hlsv02/ELV-Halluc. Introduction Video multimodal large models have demonstrated strong capabilities in visual understanding(Bai et al. 2025; Comanici et al. 2025; Zhu et al. 2025). However, serious challenge still remainsthe hallucination, where models generate content that is inconsistent with or even fabricated beyond the video content, thereby impacting the reliability *These authors contributed equally. Project Lead Corresponding author: luotto@sensetime.com Figure 1: Illustration of how increasing semantic complexity in long-video scenarios amplifies Semantic Aggregation Hallucination (SAH). Red arrows indicate erroneous aggregation into internal semantic groups. of the models in practical applications. Many works(Rawal et al. 2025; Zhang et al. 2024a; Li, Im, and Fazli 2025; Kong et al. 2025; Wang et al. 2024c) have attempted to measure hallucination in video MLLMs, but they primarily focus on short videos ranging from few seconds to tens of seconds, leaving hallucination issues in long-video contexts largely unexplored. They attribute hallucinations in VideoMLLMs to factors such as visionlanguage misalignment, poor frame quality, or suboptimal frame sampling strategies, which cause the model to rely on incomplete or inaccurate visual evidence. Alternatively, model may correctly perceive visual semantics but over-rely on strong language priors, disregarding visual input and producing incorrect content. While above causes indeed covers large proportion of hallucinations, another cause has been overlooked in prior short video hallucination benchmarks: cases where the model correctly perceives and outputs accurate frame-level semantics but still produces incorrect content by misattributing semantics across event. For example, in Figure 1, the model attributes Starbucks to the first event, where the host is holding some paper while explaining the news. However, the mention of Starbucks actually corresponds to later event in the video. In this case, while the perception of frame-level visual semantics is correct, the error arises from misaggregating information across temporal segmentsincorrectly linking visual cues from one event to concepts from another. We refer to this phenomenon as Semantic Aggregation Hallucination (SAH). In short-video scenarios, the impact of SAH is limited because frame-level semantics usually map directly to single, self-contained event. As result, logically consistent event semanticsespecially those aligned with language priorsrelatively rare (e.g.,person, horse, riding; it is unlikely for the model to hallucinate horse riding person,). In contrast, long videos often contain multiple temporally extended, yet semantically coherent events, increasing the risk of misattributing concepts across events. As illustrated in Figure 1, this richer temporal structure amplifies the likelihood of SAH, where the model confuses when an event occurs, even if all the visual elements are correctly perceived. To address the aforementioned limitations, we introduce ELV-Halluc, the first long video hallucination benchmark. As SAH is particularly prominent and challenging in long videos yet remains underexplored, ELV-Halluc is designed for studying SAH by quantifying semantic complexity through event-based videos and categorizing hallucination aspects based on semantic granularity, including visual details, action, object, and declarative content. To facilitate focused investigation, it adopts an adversarial triplet question pair design: (1) Ground Truth Question paired with InVideo Hallucinated Question, and (2) Ground Truth Question paired with Out-of-Video Hallucinated Question. We use the accuracy gap between in-video and out-of-video hallucinated question pairs to quantify models sensitivity to semantic misalignment across eventsa key aspect of SAH. Furthermore, we define the SAH Ratio as the proportion of such cases among all hallucinations, enabling systematic and interpretable analysis. We conducted extensive experiments on ELV-Halluc, covering 14 open-source MLLMs and 2 closed-source models. Our findings confirm the existence of SAH and reveal that it does not necessarily correlate with overall hallucination rates. Notably, SAH becomes more severe as semantic complexity increasessuch as with more events or denser frame samplingand is more likely to occur in fine-grained, rapidly changing aspects (e.g., visual details rather than declarative content). Since SAH arises from incorrect aggregation of frame-level semantics across events, we show that strengthening the mapping between frames and eventssuch as through improved positional encodingscan help reduce its occurrence. We further adopt DPO(Rafailov et al. 2023) strategy that explicitly discourages the models preference for hallucinated semantics. Our contributions are summarized as follows: We introduce ELV-Halluc, the first long-video benchmark designed specifically to evaluate SAH. We conduct extensive experiments demonstrating that SAH positively correlates with semantic complexity and semantic variation rate(e.g., more SAH on visual details than declarative content). This relationship causes SAH to sometimes exhibit trends opposite to overall hallucination levels (e.g., when more frames are sampled). We validate the effectiveness of multimodal positional encoding in mitigating SAH and further adopt DPO strategy to reduce SAH. By curating 8K QA pairs with and without hallucinations, we achieve maximum 27.7% reduction in SAH ratio while also improving overall performance (+0.9% on VideoMME). Related Works Video Understanding Benchmarks Video Understanding benchmarks such as Video-MME(Fu et al. 2025) and MVBench(Li et al. 2024c) aim to provide comprehensive evaluation of video understanding capabilities, covering multiple video lengths and diverse aspects of comprehension. Some benchmarks, however, focus on specific abilities of video models; for example, ETBench(Liu et al. 2024) emphasizes temporal localization and timeawareness, while Video-Holmes(Cheng et al. 2025) evaluates complex reasoning capabilities through QA pairs requiring strong reasoning skills. In long video contexts, LVBench(Wang et al. 2024b) evaluates model comprehension for ultra-long videos exceeding one hour, while MLVU(Zhou et al. 2024) designs tasks with different requirementssuch as holistic understanding, single-detail comprehension, and multidetail reasoningto assess long-video capabilities. Similarly, EgoSchema(Mangalam, Akshulakov, and Malik 2023) emphasizes evaluating model performance in egocentric video scenarios. However, hallucinationan important and relatively independent aspect of model reliabilityremains largely underexplored in these general-purpose video understanding benchmarks. Hallucination Evaluation in Video-MLLMs Several prior efforts have aimed to construct hallucinationspecific benchmarks. VideoHallucer(Wang et al. 2024c) categorizes hallucinations into two types: intrinsic, where the model outputs content inconsistent with the original video, and extrinsic, where correctness cannot be determined solely from the video. EventHallusion(Zhang et al. 2024a) further identifies two main sources of hallucination in VideoMLLMs: language priors and vision-language bias, and investigates these through QA designs involving rare events and misleading contexts. VidHalluc(Li, Im, and Fazli 2025) focuses on evaluating hallucinations in dynamic video segments and argues that the inductive bias inherent in visual encoders makes hallucinations more likely when processing semantically similar videos. ARGUS(Rawal et al. 2025), on the other hand, emphasizes hallucination evaluation in openended video captioning tasks. Nevertheless, these existing benchmarks share two major limitations: (1) They primarily target short videos with relatively simple semantics. (2) They lack explicit discussion of Semantic Aggregation Hallucination (SAH), critical challenge in long-video understanding. ELV-Halluc To address above issues, we propose ELV-Halluc, Event based Long Video Hallucination benchmark and conduct inFigure 2: Overview of the data construction pipeline. Scratch captions are first generated using Gemini 2.5 Flash, followed by manual verification to obtain ground-truth captions. Different colors indicate semantic content requiring further modification for hallucination. depth analysis on SAH. Table 1 shows statistical comparison with other video hallucination benchmarks. Videos QAs per Video Avg Video Length (s) Videohallucer EventHallusion VidHalluc ARGUS Ours 948 397 5002 500 200 1.89 1.77 1.86 19 24 85.6 11.2 24.7 26.3 672.4 Table 1: Statistical comparison with previous video hallucination benchmarks Event by event Video collection Our benchmark is composed of Event-by-Event Videos. We define event-by-event Videos as videos that consist of multiple clearly separated events sharing the same overall topic. (e.g., news broadcast with multiple news items). Event-by-event Videos offer several advantages for establishing long-video hallucination benchmark: Videos with clearly separated events can reduce captioning difficulty by isolating semantic units. Increase the likelihood of inducing SAH, as the semantics of event by event videos can be reorganized into multiple plausible yet incorrect descriptions. In event by event videos, the number of events can serve as an intuitive indicator of semantic complexity. Finally, we manually collected 500 videos from YouTube. We removed overlapping samples with datasets such as YouCook2 to prevent potential data leakage. Semi-automated Caption Pipeline As shown in Figure 2, we adopt three stage semiautomated caption pipeline to reduce human effort while ensuring annotation quality. Video Quality Recheck To reduce annotator disagreement on the Event-by-Event concept, we conducted quality recheck. Annotators retained videos with 210 clearly distinguishable events and provided keyword summarizing each videos core event (e.g., scoring moments in basketball, news coverage in broadcasts). total of 348 videos were retained, each reviewed by at least two annotators. Automated Caption Generation with Gemini We used Gemini-2.5 Flash to generate initial captions. Gemini was prompted to segment videos based on annotated keywords, exclude transitional or non-essential parts, and produce detailed captions for each identified event. Human Verification and Refinement Annotators are asked to revise the gemini generated captions through the following steps: 1. Correcting inaccurate time ranges; 2. Fixing factual errors in the captions; 3. Removing redundant segments (e.g., introductions, summaries, transition parts); 4. Adding missing events and manually annotating captions. Following this semi-automated process, we obtained 348 high-quality Event-by-Event videos with human-refined ground truth captions, ensuring accuracy while substantially reducing manual annotation costs. Hallucinate QAs curation We design adversarial question pairs for better hallucination evaluating. model should be able to both chose correct Figure 3: Dataset statistics: duration distribution (left), topic distribution (middle), and number of events per video (right). caption and reject hallucinated captions. Based on this principle, we use GPT-4o to modify ground truth captions by introducing hallucination elements. Each modified caption is paired with its ground truth to form Question pair, and the pair is considered correct only if model answers both questions correctly. Our modifications specifically target the semantics of four aspects: Visual details: attributes such as color, shape, size, patterns, spatial relationships, or on-screen text (OCR). Action: denotes the key activity or motion being performed. Object: refers to humans or physical objects mentioned in the caption. Declarative content: descriptive or propositional statements summarizing situation, asserting an outcome, or conveying belief or result, rather than concrete actions or events. (e.g., Team is leading, The match seems intense,) hallucinated caption, SAH wont serve as possible cause, since the hallucinated content does not exist in the video. Therefore, subtracting the out-video mislead rate from the in-video mislead rate approximates the contribution of Semantic Aggregation Hallucination.As shown in Figure 4, We use an object as an example to demonstrate the In-video and Out-video modifications. After applying these modifications, we obtain 20072 hallucinated captions across 348 videos. Hallucinated Caption Quality Check We used GPT-4o to automatically recheck all modified captions, ensuring that in-video captions introduce the desired aspect change present in other events ground truths, while out-video captions introduce changes absent from all ground truths. Captions meeting above criteria were retained, yielding 348 Event-by-Event videos and 8,630 hallucinated caption pairs. Figure 4: Showcase of in-video and out-video modifications. GPT-4o is instructed to modify captions by altering one of these aspects. To further investigate SAH, we design two modification strategies: In-video modification: GPT replaces an object in the ground truth caption with an object drawn from another event within the same video. Out-video modification: GPT replaces an object in the ground truth caption with fabricated object that does not appear in any captions from the video. Captions after modification must remain plausible and reasonable, such that correctness cannot be judged without watching the video. If model is misled by an in-video hallucinated caption, all hallucination types could be responsible causes. In contrast, if the model is misled by an out-video Figure 5: Examples of ELV-Halluc questionanswer pairs across different semantic aspects. The left part illustrates the complete QA pair format in ELV-Halluc. Final Benchmark and Evaluation Metrics We select 200 videos from the original set of 348, leaving the remaining 148 videos as the training set for DPO. For each selected video, we choose 24 captions to construct binary QA pairs by appending the question prefix: Is the following caption totally correct? Reply with Yes or No only. These QA pairs cover the aforementioned four aspects: visual details, objects, actions, and declarative content. Each aspect includes 6 questions, formed from 2 triplets drawn from different events within the same video. Each triplet consists of three captions: ground truth, in-video hallucinated, and out-of-video hallucinated. Examples of the final QA pairs are shown in Figure 5. We form adversarial QA pairs by combining groundtruth question with hallucinated question, resulting in two pairs per triplet: (GT, In-Video Hallucination) (GT, Out-ofVideo Hallucination) pair is considered correct only if the model predicts Yes for the ground-truth question and No for the hallucinated question. Overall, the benchmark contains 4,800 binary QA pairs, which can be further grouped into 3,200 adversarial QA pairs. Figure 3 presents detailed statistics of ELV-Halluc, illustrating its diversity in video length, topics, and number of events. Accuracy We use Accuracy to evaluate the overall hallucination level of models in long-video scenarios. Specifically, we report the following metrics: In-Video Accuracy: Accuracy on QA pairs containing in-video hallucinations. OutVideo Accuracy: Accuracy on QA pairs containing out-ofvideo hallucinations. SAH Ratio We further propose the SAH Ratio to quantify the proportion of Semantic Aggregation Hallucination (SAH) among all hallucination errors. If model achieves high accuracy on out-of-video hallucinations but significantly lower accuracy on in-video ones, it indicates difficulty in resolving semantic misalignment across eventsthe hallmark of SAH. Therefore, the accuracy gap reflects how prone the model is to confusing correct frame-level content with incorrect event-level attribution, making it suitable proxy for SAH severity. Therefore, we use the SAH Ratio instead of the absolute difference between Out-Video and In-Video accuracy. This approach enables more precise measurement of the relative severity of SAH while minimizing the influence of the models absolute performance level. Consequently, it facilitates targeted solutions specifically addressing SAH. The metric is computed as follows: SAH Ratio = OutAcc 1 InAcc InAcc where OutAcc and InAcc denote the accuracy on out-ofvideo and in-video hallucination pairs, respectively. Experiments and discussions We evaluate 14 open-source models ranging from 1B to 78B parameters, along with two closed-source MLLMs on ELVHalluc. As shown in Table 2, the results demonstrate that current SOTA LLMs continue to face significant challenges with hallucination in long-video understanding(Note: The results of Gemini-2.5 Flash should not be directly compared with other models, as it was used to generate the initial captions, which may introduce bias). The results also validate the existance of SAH, as most MLLMs exhibit substantially lower accuracy on in-video hallucinated captions than on out-of-video hallucinated captions. Among the open-source models, Qwen2.5-VL-32B achieves the lowest SAH Ratio, at only 0.2%. In FigSAH increase as the semantic complexity rises. ure 6, we observe that Qwen2.5-VL-3B, Qwen2.5-VL-72B, InternVL3-8B, and Gemini2.5-Flash (representing different model sizes and families) exhibit positive correlation between the SAH ratio and the number of video events, as larger number of events typically introduces more complex semantics. Meanwhile, we find that the SAH ratio does not show any consistent relationship with video length, since in our event-by-event dataset, video length does not necessarily correspond to higher number of events. (a) SAH Ratio vs. Event Count (b) SAH Ratio vs. Video Length Figure 6: Relationship between SAH and video properties: (a) SAH Ratio correlation with the number of events; (b) SAH Ratio correlation with video duration. SAH Occurs More Frequently at Rapidly Changing Semantic We found that model tend to exhibit SAH more frequently on semantics that change rapidly over time. During modification, we define four aspectsvisual details, action, object, and declarative contenteach representing different level of semantic granularity. Among them, visual details vary the fastest, followed by actions, as objects often engage in multiple actions over time. Objects themselves change less frequently, while declarative content represents higher-level semantics that evolve the slowest. Based on the results of 14 open-source models, we plot the SAH ratio for these four aspects (see Figure 7). Results reveal that Video-MLLMs exhibit the highest SAH ratio on visual details, followed by actions and objects, and the lowest on declarative content, suggesting that models are more prone to SAH on semantics with higher temporal variability. Hallucination under Varying Frame Numbers and Model Sizes. As shown in Figure 8, We use the Qwen2.5VL and InternVL3 series models to investigate the relationship between the number of video frames sampled and the occurrence of hallucinations. For Qwen2.5-VL, we disable the dynamic resolution mechanism to ensure that the number of frames serves as the sole varying factor. For accuracy, we observe that for most models, increasing the number of frames generally leads to higher overall hallucination accuracy. For the SAH ratio, most models tend to exhibit higher values as the number of frames increases. We attribute above Models LLM size Visual Details In. Out. Diff. In. Object Out. Diff. Action Out. Diff. Declarative Content Diff. Out. In. In. Avg Acc Avg Diff. SAH Ratio InternVL3-1B InternVL3-2B SmolVLM-2.2B Qwen2.5VL-3B LLaVA-Video-7B Video-chatgpt-7B LLaVA-OV-7b Qwen2.5VL-7b InternVL3-8B InternVL3-14B Qwen2.5VL-32B InternVL3-38B Qwen2.5VL-72B InternVL3-78B GPT-4o Gemini2.5-Flash 0.5B 1.5B 1.7B 3B 7B 7B 7B 7B 7B 14B 32B 32B 72B 72B / / 8 7 0 2.2 3.7 2 8 10.2 12.5 17.5 16.5 25.3 24 25 11 15.5 0 10.5 3.7 2.5 13.2 26 19.5 24.5 24.5 29 35.5 31.2 3 8.5 0 8.3 0 0.5 5.2 15.8 7.0 7.0 8.0 3.7 11.5 6. 7.7 47 8.3 58 0.6 11 8.7 8.7 3 7.7 4.5 2.5 9.5 17.5 14.5 22.8 21.7 24.2 35.7 32 8 56.5 Open Source Models 2.3 11 8.5 17.2 2 5 6.1 13.8 -2 2.5 -0.7 1.7 4.2 13.7 13.2 30.7 5.0 19.5 1.7 24.5 2.8 24.5 3.8 28 5.8 41.5 4.5 36. 8.7 7.2 0 5 3.7 1.2 8.7 13 13.5 16.3 17.2 24 27.8 28.5 12.5 10.5 0 8 3.2 1.2 10.7 20.7 20.5 17.7 15.0 30 32.3 31.2 Closed Source Models 0.7 2.3 8.7 50.5 8.7 58.8 10.2 53. 3.8 3.3 0 3 -0.5 0 2 7.7 7.0 1.4 -2.2 6 4.5 2.7 1.5 2.7 11.3 10 0 6 4 2.2 7.7 16.8 12.8 15.2 15.2 24.5 32.3 24.2 8.5 48.7 8.3 13 0 6 4 3.2 7.5 10.5 17.7 15.5 7.2 24.2 27 26.5 9.5 -3 3 0 0 0 1 -0.2 -6.3 4.9 0.3 -8 -0.3 -5.3 2.3 1 3.3 9.9 11.1 1 7.4 3.6 2.0 9.9 18.1 16.3 19.2 17.7 26.1 32.0 29.3 8.7 53.1 1.5 5.8 0.5 4.3 -0.6 0.2 2.8 7.6 5.9 2.6 0.1 3.3 4.1 3.9 0.9 4. 1.6 6.3 0.5 4.5 -0.6 0.1 3.0 8.8 6.8 3.1 0.2 4.3 5.8 5.4 1.0 9.8 Table 2: Main results on ELV-Halluc. Diff. denotes the gap between in-video and out-video accuracy. Note that the semiautomatic annotation pipeline uses Gemini for initial captioning, which may introduce bias; therefore, metrics for Gemini-2.5 Flash should not be directly compared with other models. All accuracies are shown as percentages. Figure 7: SAH Ratio across different semantic aspects. VD, Ac, Ob, and DC represent Visual Details, Action, Object, and Declarative Content, respectively. findings to the fact that an increased number of frames provides the model with richer and more complex semantic information. On one hand, this additional information reduces uncertainty about the overall video content, thereby improving the models robustness against overall hallucinations. On the other hand, the added complexity increases the likelihood of semantic mismatches, leading to more SAH. Interestingly, the Qwen2.5-VL-32B model exhibits different trend. We hypothesize that this may be due to the reinforcement learning-based post-training of Qwen2.5-VL32B-Instruct, which likely enhances its ability to aggregate visual semantics effectively. Furthermore, we observe that larger language models generally achieve higher overall hallucination accuracy, suggesting improved robustness against global hallucinations. However, we do not observe consistent correlation between model size and the SAH ratio, indicating that increasing model capacity does not necessarily mitigate semantic (a) Qwen2.5-VL Acc vs Frame (b) Qwen2.5-VL SAH vs Frame (c) InternVL3 Acc vs Frame (d) InternVL3 SAH vs Frame Figure 8: Effect of frame nums and model size on hallucination performance. Each row corresponds to model family (Qwen2.5-VL and InternVL3), showing (left) overall hallucination accuracy and (right) SAH ratio across varying frame numbers for different model sizes. aggregation hallucinations. Strategies for Mitigating SAH Position Encoding Decease SAH As SAH occur due to errors introduced during the semantic grouping process. We argue that stronger RoPE mechanism could enhance the models ability to bind semantic relationships, thereby reducing errors that occur during grouping. Our experiments validate this hypothesis, as shown in Table 3. Specifically, we evaluate checkpoints based on Qwen2-VL, applying different positional encoding strategies: vanilla RoPE(Su et al. 2024), TAD-RoPE(Gao et al. 2024), m-RoPE(Wang et al. 2024a), and VideoRoPE(Wei et al. 2025). The results demonstrate that VideoRoPE achieves the lowest SAH ratio. However, the findings also indicate that stronger RoPE variants do not necessarily mitigate oveall hallucinations. Method vanillarope tad rope mrope videorope In. Out. 0.94 2.75 2.62 0.44 2.06 1.12 2.06 1.19 SAH Ratio 1.82 2.18 0.95 0.88 Table 3: Different RoPE Strategies on ELV-Halluc. Bold values indicate the best performance. In. and Out. denote the average in-video and out-video accuracy, respectively. Mitigate SAH with DPO Considering that SAH mainly arise from incorrect grouping of correct video semantics, applying method to suppress the models attention to hallucinatory semantics should reduce SAH possibilities. Therefore, we adopt the Direct Preference Optimization(Rafailov et al. 2023) approach to further optimize the model. We leverage the remaining 148 videos ground-truth and hallucinated caption of all events to construct the positive and negative response pairs required for DPO. Specifically, we use the following template to generate data pairs: Please provide detailed caption for this segment during mm:ss - mm:ss. Chosen: Ground truth caption Rejected:Hallucinated caption Finally, we create two separate datasets of 4k positivenegative pairs each: one using in-video hallucinated captions and the other using out-of-video hallucinated captions. We use Qwen2.5-VL-7B as the base model and conduct three training settings: 1. In-video pairs, 2. Out-video pairs, 3. Combining above 2 types of pairs. Model Qwen2.5vl-7B + invideo-4k + outvideo-4k + together-8k ELV-Halluc Avg Acc 15.9 16.2 16.0 16.4 SAH Ratio 8.3 6.0 (-27.7%) 8.6 (+3.6%) 8.4 (+1.2%) VideoMME Short Medium Long Avg 61.9 72.7 62.3 72.4 73.8 62.8 62.4 73.4 51.3 51.9 52.6 51.8 61.7 62.6 62.0 62. Table 4: Performance comparison of base model and DPO variants on ELV-Halluc and VideoMME benchmarks. As shown in Table 4, applying DPO with in-video pairs yields the most notable improvement, reducing the SAH ratio from 8.3 to 6.0. This indicates that aligning the models preference toward correct event semantics within the same video is highly effective for mitigating semantic aggregation hallucinations. Additionally, the overall average accuracy on ELV-Halluc improves by 0.3 points, while general video understanding performance on VideoMME also improves slightly (+0.4), demonstrating that mitigating hallucination does not compromise general capability. In contrast, DPO with out-of-video pairs reduces overall hallucination slightly but unexpectedly increases the SAH ratio. This suggests that optimizing the model to reject content entirely irrelevant to the video does not effectively improve SAH and may even bias the model toward overreliance on language priors. When combining in-video and out-of-video pairs (8k samples together), the model achieves balanced trade-off: it retains most of the benefits of in-video optimization, while maintaining robustness against out-of-video hallucinations. However, the combined setting does not surpass in-video DPO in reducing SAH. Figure 9: Attention Gap Heatmap after DPO with In-video pairs, BLUE means lower attention after DPO. As illustrated in Figure 9, attention visualization reveals that after DPO with in-video pairs, the model significantly reduces its focus on incorrect yet semantically plausible regions, shown in blue areas indicating decreased attention weights. This suggests that DPO effectively reshapes the models internal preference distribution, promoting stronger grounding of responses in relevant visual semantics. Such alignment at the attention level provides mechanistic explanation for the observed improvement in SAH mitigation. Conclusion and Limitations In this work, we addressed the underexplored challenge of hallucination in long-video understanding by introducing ELV-Halluc, the first benchmark tailored to evaluate Semantic Aggregation Hallucination (SAH). We identified SAH as distinct and increasingly prominent error type in semantically complex, multi-event videos. To enable comprehensive evaluation, we benchmarked 14 open-source Video-MLLMs (1B78B) and two closed-source models (GPT-4o and Gemini 2.5 Flash). Our experiments revealed key SAH patterns and overall hallucination trends. To mitigate SAH, we proposed DPO-based approach and curated an 8K-pair adversarial dataset, achieving 27.7% reduction in SAH and 0.9% gain on VideoMME. Despite its contributions, our work has several limitations. First, although our semi-automated pipeline reduces manual effort, Gemini-generated captions may introduce bias, potentially inflating Gemini 2.5 Flashs performance. Second, while event-based video construction improves control and diversity, it still differs from real-world long videos. Third, the dataset scale is limited due to the high annotation cost. Nonetheless, we believe our benchmark, analysis, and mitigation strategies lay solid foundation for advancing reliable long-video understanding. References Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin, J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: Versatile VisionLanguage Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Cheng, J.; Ge, Y.; Wang, T.; Ge, Y.; Liao, J.; and Shan, Y. 2025. Video-Holmes: Can MLLM Think Like arXiv preprint Holmes for Complex Video Reasoning? arXiv:2505.21374. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Duan, H.; Yang, J.; Qiao, Y.; Fang, X.; Chen, L.; Liu, Y.; Dong, X.; Zang, Y.; Zhang, P.; Wang, J.; et al. 2024. Vlmevalkit: An open-source toolkit for evaluating large In Proceedings of the 32nd ACM multi-modality models. International Conference on Multimedia, 1119811201. Fu, C.; Dai, Y.; Luo, Y.; Li, L.; Ren, S.; Zhang, R.; Wang, Z.; Zhou, C.; Shen, Y.; Zhang, M.; et al. 2025. Videomme: The first-ever comprehensive evaluation benchmark In Proceedings of of multi-modal llms in video analysis. the Computer Vision and Pattern Recognition Conference, 2410824118. Gao, H.; Qu, J.; Tang, J.; Bi, B.; Liu, Y.; Chen, H.; Liang, L.; Su, L.; and Huang, Q. 2025. Exploring hallucination of large multimodal models in video understanding: Benchmark, analysis and mitigation. arXiv preprint arXiv:2503.19622. Gao, M.; Liu, J.; Li, M.; Xie, J.; Liu, Q.; Zhao, B.; Chen, X.; and Xiong, H. 2024. Tc-llava: Rethinking the transfer from image to video understanding with temporal considerations. arXiv preprint arXiv:2409.03206. Hu, K.; Wu, P.; Pu, F.; Xiao, W.; Zhang, Y.; Yue, X.; Li, B.; and Liu, Z. 2025. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Kong, M.; Zeng, X.; Chen, L.; Li, Y.; Yan, B.; and Zhu, Q. 2025. MHBench: Demystifying Motion Hallucination in VideoLLMs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 44014409. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Li, B.; Zhang, Y.; Guo, D.; Zhang, R.; Li, F.; Zhang, H.; Zhang, K.; Zhang, P.; Li, Y.; Liu, Z.; et al. 2024a. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Li, C.; Im, E. W.; and Fazli, P. 2025. Vidhalluc: Evaluating temporal hallucinations in multimodal large language models for video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1372313733. Li, J.; Lu, W.; Fei, H.; Luo, M.; Dai, M.; Xia, M.; Jin, Y.; Gan, Z.; Qi, D.; Fu, C.; et al. 2024b. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632. Li, K.; Wang, Y.; He, Y.; Li, Y.; Wang, Y.; Liu, Y.; Wang, Z.; Xu, J.; Chen, G.; Luo, P.; et al. 2024c. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2219522206. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual instruction tuning. Advances in neural information processing systems, 36: 3489234916. Liu, Y.; Ma, Z.; Qi, Z.; Wu, Y.; Shan, Y.; and Chen, C. W. 2024. Et bench: Towards open-ended event-level videolanguage understanding. Advances in Neural Information Processing Systems, 37: 3207632110. Liu, Z.; Guo, L.; Tang, Y.; Yue, T.; Cai, J.; Ma, K.; Liu, Q.; Chen, X.; and Liu, J. 2025. Vrope: Rotary position embedding for video large language models. arXiv preprint arXiv:2502.11664. Maaz, M.; Rasheed, H.; Khan, S.; and Khan, F. S. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424. Mangalam, K.; Akshulakov, R.; and Malik, J. 2023. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36: 4621246244. Marafioti, A.; Zohar, O.; Farre, M.; Noyan, M.; Bakouch, E.; Cuenca, P.; Zakka, C.; Allal, L. B.; Lozhkov, A.; Tazi, N.; Srivastav, V.; Lochner, J.; Larcher, H.; Morlon, M.; Tunstall, L.; von Werra, L.; and Wolf, T. 2025. SmolVLM: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299. Peng, S.; Yang, S.; Jiang, L.; and Tian, Z. 2025. Mitigating Object Hallucinations via Sentence-Level Early Intervention. arXiv preprint arXiv:2507.12455. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741. Rawal, R.; Shirkavand, R.; Huang, H.; Somepalli, G.; ARGUS: Hallucination and and Goldstein, T. 2025. arXiv preprint Omission Evaluation in Video-LLMs. arXiv:2506.07371. Su, J.; Ahmed, M.; Lu, Y.; Pan, S.; Bo, W.; and Liu, Y. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568: 127063. Team, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Soricut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.; et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Fan, Y.; Dang, K.; Du, M.; Ren, X.; Men, R.; Liu, D.; Zhou, C.; Zhou, J.; and Lin, J. 2024a. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191. Wang, W.; He, Z.; Hong, W.; Cheng, Y.; Zhang, X.; Qi, J.; Gu, X.; Huang, S.; Xu, B.; Dong, Y.; et al. 2024b. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035. Wang, Y.; Wang, Y.; Zhao, D.; Xie, C.; and Zheng, Z. 2024c. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338. Wei, X.; Liu, X.; Zang, Y.; Dong, X.; Zhang, P.; Cao, Y.; Tong, J.; Duan, H.; Guo, Q.; Wang, J.; et al. 2025. VideoRoPE: What Makes for Good Video Rotary Position Embedding? arXiv preprint arXiv:2502.05173. Zhang, J.; Jiao, Y.; Chen, S.; Zhao, N.; Tan, Z.; Li, H.; and Chen, J. 2024a. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597. Zhang, Y.; Wu, J.; Li, W.; Li, B.; Ma, Z.; Liu, Z.; and Li, C. 2024b. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713. Zhao, Y.; Huang, J.; Hu, J.; Wang, X.; Mao, Y.; Zhang, D.; Jiang, Z.; Wu, Z.; Ai, B.; Wang, A.; Zhou, W.; and Chen, Y. 2024. SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning. arXiv:2408.05517. Zheng, Y.; Zhang, R.; Zhang, J.; Ye, Y.; Luo, Z.; Feng, Z.; and Ma, Y. 2024. LlamaFactory: Unified Efficient FineIn Proceedings of the Tuning of 100+ Language Models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). Bangkok, Thailand: Association for Computational Linguistics. Zhou, J.; Shu, Y.; Zhao, B.; Wu, B.; Xiao, S.; Yang, X.; Xiong, Y.; Zhang, B.; Huang, T.; and Liu, Z. 2024. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv e-prints, arXiv2406. Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Tian, H.; Duan, Y.; Su, W.; Shao, J.; et al. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479."
        }
    ],
    "affiliations": [
        "SenseTime Research"
    ]
}