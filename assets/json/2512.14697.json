{
    "paper_title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "authors": [
        "Yue Zhao",
        "Hanwen Jiang",
        "Zhenlin Xu",
        "Chutong Yang",
        "Ehsan Adeli",
        "Philipp Krähenbühl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 9 6 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
            "content": "Yue Zhao1,2 Hanwen Jiang1,3 Zhenlin Xu4, Chutong Yang1 Ehsan Adeli2,* Philipp Krahenbuhl1,* 1 UT Austin 2 Stanford University 3 Adobe Research 4 Mistral AI http://cs.stanford.edu/yzz/npq/ Non-parametric quantization [Sec. 3.1] Lattice code Vector quantization Spherical lattices [26] Learned VQ [77, 85] FSQ [56] LFQ [86] BSQ [91] Random projection [14] Spherical Leech lattice [Sec. 3.4] Densest sphere packing lattices [Sec. 3.3] c3 c1 c4 c2 c1 c6 c3 c4 BSQ (d = 2) simple square lattice A2 (Densest packing at = 2) hexagonal lattice Figure 1. Upper left: Venn Diagram that contains all definitions and quantization methods covered in this paper. We provide unified formulation of various non-parametric quantization methods [56, 86, 91] from lattice-coding perspective in Section 3.1. The geometric interpretation of the entropy penalties in Section 3.2 then leads to family of densest hypersphere packing lattices (Section 3.3). Based on the spherical Leech lattice, 24-d case of the densest hypersphere packing lattices, we instantiate Spherical Leech Quantization (Λ24-SQ) in Section 3.4 and apply it to modern discrete auto-encoders (middle) and visual autoregressive models (right). Lower left: An illustrative 2D comparison between BSQ and spherical densest-packing lattice quantization (A2). Middle: An auto-encoder with Λ24-SQ outperforms BSQ in image reconstruction and compression (Qualitative results on the top). Right: Qualitative and quantitative results of visual autoregressive generation model with Λ24-SQ on ImageNet-1k. For the first time, we train discrete visual autoregressive generation model with codebook of 196, 560 without bells and whistles and achieve an oracle-like performance."
        },
        {
            "title": "Abstract",
            "content": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to large codebook. In this paper, we present unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As step forward, we explore few possible candidates, including random lattices, gen- *Equal advising. Work done before joining Mistral AI. 1 eralized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (Λ24-SQ), leads to both simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to stateof-the-art auto-regressive image generation frameworks. 1. Introduction tion oracle (1.78 FID) on ImageNet-1k. Learning discrete visual tokenization is fundamental to visual compression [19, 29], generation [10, 27, 75], and understanding [5]. Although discrete token-based visual modeling [4, 75] follows recipe similar to that of language modeling, we observe paradox: Visual information carries much more data than language in quantity and diversity1; However, the visual vocabulary size of vision models lags far behind that of Large Language Models (LLM)2. To bridge this gap, non-parametric quantization (NPQ) methods [56, 86, 91] have recently been proposed, demonstrating codebook scalability and parameter efficiency compared to vector quantization [33, 77]. However, existing NPQ methods have their own flaws and require ad hoc tweaks (e.g., regularization terms), which eventually boil down to the fact that most methods are heuristic and lack principled design. In this paper, we propose simple and effective quantization method, called Spherical Leech Quantization (Λ24SQ), which scales to codebook of 200K while keeping the training of both visual tokenizers and visual autoregressive models as simple as possible. Λ24-SQ is theoretically grounded in the intersection of vector quantization and lattice codes. We first provide unified formulation of existing non-parametric quantization methods from the perspective of lattice coding and reinterpret entropy penalties as lattice relocation problem. This motivates family of densest hypersphere packing lattices, among which the Leech lattice in the first shell [51] instantiates the codebook of Λ24-SQ. Spherical Leech Quantization features the following advantages. (i) Simplicity: Thanks to the densest sphere packing principle, Λ24-SQ enables the autoencoder to train with the simplest loss trio (i.e. ℓ1, GAN, and LPIPS) without any regularization terms such as commitment loss and entropy penalties. (ii) Efficiency: Because of the fixed lattice vectors, Λ24-SQ is excluded from gradient updates, being both memory and runtime efficient. (iii) Effectiveness. Λ24-SQ effectively pushes the rate-distortion tradeoff frontier. Specifically, Λ24-SQ-based autoencoder improves rFID from 1.14 to 0.83 compared to BSQ with slightly smaller effective bitrate (d = 17.58 vs. = 18). See Figure 1. Based on Λ24-SQ, we introduce improved techniques in training autoregressive visual generation models with very large codebook. For the first time, we train discrete visual autoregressive generation model with codebook of 200K, comparable to frontier language models, without bells and whistles (index subgrouping [86, 91], multihead prediction [36], bit flipping/self-correction [80], etc.) and achieve generation FID of 1.82 FID, close to the valida1Human language conveys information at several tens of bits/s [61, 69] while the brain receives visual input at 106 bits/s [47]. 2The tokenizer has 199,997 elements in GPT-4o [41] while 129,280 in Deepseek-R1 [34]. Meanwhile, typical visual codebook is around 1, 024 16, 384. 2. Preliminaries 2.1. Visual tokenization and quantization Visual tokenization. Visual tokenization transforms visual input into set of discrete representations using an auto-encoder architecture and bottleneck module based on vector quantization (VQ) [33, 77]. In this paper, we consider single images as input for simplicity. Given an image RHW 3 and an encoder (decoder) denoted by (G), we have E() encoder R( p )d QVQ () quantizer ˆZ G() decoder ˆI, (1) where is the spatial downsample factor and the quantizer assigns each to the closest entry in learnable codebook = [c1, , cK] RKd, i.e. ck2. The entire model (E, G, ˆz = ck = arg minck and Q) is end-to-end trainable using approximation methods such as straight-through estimator (STE) [8], Gumbel Softmax [42], or more recent tricks like Rotation Trick [28]. One of the key challenges is to learn the codebook effectively, especially when the codebook size increases. Implicit codebooks. Yu et al. introduced fixed implicit codebook CLFQ = {1}d [86], where its best quantizer is binary quantization QLFQ (z) = sign(z). Binary Spherical Quantization (BSQ) [91] further projects the hypercubeshaped codebook onto unit sphere, i.e. CBSQ = { 1 }d. Finite Scalar Quantization (FSQ) [56] extends the codebook to multiple values per dimension i.e. (cid:81)d 2 }. We refer to these implicit codebook-based methods as nonparametric quantization (NPQ). Both LFQ and BSQ require an entropy regularization term [43] to encourage high code utilization: i=1{0, , Li Lentropy = [H[q(z)]] γH [E [q(z)]] , (2) where q(z) ˆq(cz) = (cid:80) quantization approximation [1]. exp(τ (cz)2) exp(τ (cz)2) is soft cCLFQ Each of the NPQ variants has its own pros and cons. (1) LFQ is easiest to implement, but the computational cost for entropy increases exponentially. (2) BSQ provides an efficient approximation with guaranteed bound, but still suffers from codebook collapse without proper entropy regularization. (3) FSQ does not need such complex regularization, but the way to obtain the number of levels per channel (L1, , Ld) is somewhat heuristic [56]. Although the geometric landscape of all these quantization methods varies (Figure 2 in [91]), we show in the upcoming chapter that they can be interpreted from the same lattice coding perspective. This unified formulation further motivates us to develop novel non-parametric quantization variant that is both theoretically sound and implementation-wise easy. Table 1. Comparisons of different non-parametric quantization methods. we overload (cid:81) for both scalar product and Cartesian product. Method LFQ [86] Input range Code vectors Codebook size (C) Minimum distance (dmin) Rd 2d {1}d (cid:81)d FSQ [56] i=1( Li (cid:81)d i=1{ Li (cid:81)d 2 , Li 2 ) 2 , , Li i=1 Li 1 2 } BSQ [91] Fd(N )-SQ Λ24-SQ (this paper) Sd1 Sd1 S23 }d { 1 2d 2 3.3 and δmin(N ) (cid:83) 1 32 {2,3,4} Λ24(2)s [18] 196,560 3 2 2.2. Lattice-based codes (iii) Binary Spherical Quantization [91]: Lattice. d-dimensional lattice is defined by discrete set of points in Rd that constitutes group. In particular, the set is translated such that it includes the origin. Mathematically, an d-dimensional lattice Λn is represented by Λd = {λ Rdλ = Gb}, = g1 g2 , gd (3) where Rdd is called the generator matrix, comprising of generator vectors gi in columns, and Zd. Lattice-based codes. Λd in Eq. (3) has infinite elements by definition. In practice, we include additional constraints so that the new set is enumerable: Λd = {λ Rdλ = Gb, (λ) = c1, h(λ) c2}. Particularly, spherical lattice codes [26] refers to family of lattice-based codes with constant squared norm, i.e. Λd,m = {λ Rdλ = Gb, λ2 = m}. We will see more examples in Section 3.3. Besides, we define the quantizer associated with lattice Λ by QΛ(z) = arg mintΛ t, which offers bridge to vector quantization (Section 2.1). (4) 3. Method 3.1. Non-parametric quantization as lattice coding From the perspective of the lattice-based codes defined in Eq. (4), we can describe all variants of non-parametric quantization methods [56, 86, 91] in the same language, despite the varying geometric landscapes. (i) Vanilla Lookup-Free Quantization [86]: = e1 e2 Id, ed f1(λ) = λ0 = d, f2(λ) = λ1 = d. (5) (6) Here, ei is the standard basis vector, taking the value of 1 at the i-th index and 0 elsewhere. The constraints in Eq. (6) are equivalent to saying that λi = 1 for all i. (ii) Finite Scalar Quantization [56]: For simplicity, we consider the special case where any Li equals L. = Id, h(λ) = λ 2 . (7) = 1 Id, f1(λ) = λ0 = d, f2(λ) = λ2 = 1. (8) Although it appears that Eq. (8) is simply scaled version of Eq. (6), it is worth noting that the input range varies: Sd1 in BSQ while Rd in LFQ. (iv) Random-projection Quantization (RPQ) [14]. RPQ initializes the codebook {p1, , pN } using standard normal distribution, followed by an ℓ2 normalization. Due to the codebook existence, strictly speaking, RPQ does not belong to lookup-free quantization by definition. Nevertheless, we can still include it in the same picture, where the generator matrix and contraints look like the following: = p1 p2 , (λ) = λ2 = 1, (9) pN where RdN slightly abuses the definition, pi follows projected normal distribution pi PN d(0, I). 3.2. Entropy regularization as lattice relocation We review the entropy regularization term from the perspective of lattice coding. We give geometric interpretation of the entropy regularization and show that the two subterms correspond to pushing the input point towards the lattice points and finding an optimal configuration of the lattice. Re-interpretating entropy regularization. The first term in Eq. (2) minimizes the entropy of the distribution that is assigned to one of the codes. This means that every input should be close to one of the centroids instead of the decision boundary3. This becomes less of an issue since the codebook of interest is huge, exemplified by γ being greater than 1 as reported in [43, 86]. Ablation studies in BSQ [91] also reveal that we can omit [H[q(z)]] but not H[E[q(z)]] while achieving similar performance. The second term maximizes the entropy of the assignment probabilities averaged over the data, which favors class balance [48]. Assuming that has uniform distribution over its input range, we have E[q(z)] = (q(z) = ck) = (cid:82) dz = Vk, where Vk is the Voronoi region for the codeword ck. H[E[q(z)]] is maximized when all Vk have equal volumes. Vk 3This principle is also known as the cluster assumption [11, 32]. Table 2. Best known results for dense packing. The content is adapted from Table 1.1 in [18]. Dimension 1 2 3 5 6 7 8 12 16 densest packing A2 A3 D4 D5 E6 E7 E8 K12 Λ16 Λ24 Entropy maximization as hypersphere packing. An alternative way is to assume equal radii for all hyperspheres and find the densest sphere packing [18]. The best known results in dimensions 1 to 8, 12, 16, and 24 are summarized in Table 2, where to E8 and Λ24 have been proved optimal among all lattices [16, 18]. Given these basics, we now propose few candidates. (i) Random projection lattice follows RPQ in Section 3.1 where the projected normal distribution initializes points. We use it as baseline to compare the dispersiveness of different candidate lattice codes in Figure 2, which turns out to be surprisingly strong in higher dimensions. (ii) Fibonacci lattice constructs points that are evenly distributed with each of them representing almost the same area [30] in unit square [0, 1)2. We can map this point distribution to unit-length sphere S2 using cylindrical equal-area projection. From Figure 2a, δmin(N ) achieved by this 3D spherical Fibonacci lattice is close to the known densest packing [71] and much better than random projection. We explore its high-dimensional generalization with the hyperspherical coordinate system inspired by [67], denoted by Fd(N ). Construction details are left in Section A. (iii) Densest sphere packing lattice has been introduced and summarized in Table 2. We pay particular attention to the Leech lattice Λ24 [51]. Λ24 can be constructed in many ways [17] and we use the most convient way to calculate. The vectors in the first shell have minimal norm 32 and fall into three types; we summarize their shapes and numbers in Table 3 and provide more details in Section B. Normalizing these 196, 560 vectors in the first shell to unit length results in the Spherical Leech Quantization (Λ24-SQ) codes. We can easily get δmin( 1 Λ24(2)s) = 1 32 FSQ implicitly maximizes entropy. The interpretation explains why FSQ does not suffer from codebook collapse even without entropy penalties. Given an input Rd, FSQ first applies bounding function , and then rounds to the nearest integers, f () bound = 2 tanh(z) QFSQ () quantize ˆz = round( z). (10) 2 , ˆzi + 1 i=1[ˆzi 1 Therefore, the input range is (L/2, L/2) and all integer points within this range are valid lattice points4. The codebook size is Ld, often in the range of 28 216. The Voronoi cell for each lattice point ˆz is unit-length hypercube (cid:81)d 2 ), implicitly complying with the entropy maximization principle. LFQ requires explicit entropy maximization. Although the Voronoi cell for each point in LFQ is also identical, the range of input and quantized output is unbounded. This breaks the uniform distribution assumption, thus requiring explicit regularization. Whats left? BSQ is missing so far. Since its input lies on hypersphere, BSQ has to be treated separately. We will discuss the lattice relocation problem for spherical lattice codes in Section 3.3, where BSQ is one such code. 3.3. Spherical lattices and hypersphere packing Spherical lattices. The overall pipeline is written as follows: Rd norm() normalize = QΛ() quantize ˆz = QΛ( z), (11) where norm() is another way of bounding, and the Voronoi regions now take arbitrary shapes on the hyperspherical shell. For simplicity, we study surrogate problem that approximates the Voronoi regions by placing d-dimensional balls with varied radii5, where is the cardinality of the lattice in which we are interested. Entropy maximization as dispersiveness pursuit. The entropy maximization term corresponds to finding the most dispersive configuration to relocate the balls. Sloane et al. formally state this problem of placing points on ddimensional sphere to maximize the minimum distance (or angle) between any pair of points in [71]. This problem generalizes the Tammes problem [73] in dimensions greater than 3. Mathematically, we write this max-min problem as maxc1, ,cN Sd1 min , where we dedistance(cj, ck) 1j<kN (cid:124) (cid:123)(cid:122) δmin(N ) (cid:125) note δmin(N ) for future empirical analysis. 4When = 2, = 3, this is the well-known simple cubic or primitive cubic lattice in crystallography; We will see this again in Section 3.3. 5This will leave some holes, but we believe that the total volume of holes is negligible compared to the balls. (a) = 3. (b) = 24. Figure 2. δmin(C) w.r.t. across different lattices discussed in Section 3.4 in low dimensions (d = 3) and high dimensions (d = 24). The advantage of the densest sphere packing lattices over other candidates is more visible in higher dimensions. 4 Table 3. Vectors in the first shells of the Leech lattice. Λ24(n)i indicates the Leech vectors of norm 2n (or type and shape i; the signs are suppressed for simplicity. The table is extracted from Table 4.13 in [18]. Class Shape Number Class Shape Number Λ24(0) (024) 1 Λ24(2)3 Λ24(2)2 (28016) 27 759 Λ24(2)4 (31123) 212 24 (cid:19) (cid:18)24 (42022) 22 Table 4. Comparison between the proposed Spherical Leech Quantization (Λ24-SQ) and BSQ [91]. Method BSQ [91] Λ24-SQ (this paper) Input range Code vectors Codebook size δmin(C) }18 S17 { 1 18 218 = 262, 144 2 0.471 18 S23 See Table 1 196, 560 217.58 3 2 0.866 AR output Self-correct (1) 262, 144-way logits (1) 196, 560-way logits (2) 18 binary logits bitwise flip (2) 24 nonary logits 9-itwise toggle (cid:83) {2,3,4} Λ24(2)s) = 3 for = 2, 3, 4 and δmin( 1 2 . 32 From Figure 2b, δmin(N ) is much larger than all other candidates. BSQ are not the densest packing lattice codes. Before we conclude this chapter, we compare Λ24-SQ with BSQ, the prior art, in Table 4. Since the codebook size takes log2(196, 560) 17.58 bits, we use BSQ with = 18. Λ24-SQ increases δmin(C) by more than 80% (0.471 0.866), indicating its superiority. Empirical results in Section 5 also support that Λ24-SQ enables simple loss design such that the entropy regularization term can be omitted. Comparing Figures 2a and 2b, we also conclude that the improvement in δmin(C) of BSQ over the random lattice baseline decreases when increases. 3.4. Spherical Leech Quantization in practice 32 (cid:83) 1 Instantiation. Λ24-SQ follows the pipeline of Eq. (11) with {2,3,4} Λ24(2)s. Despite the huge the lattice being codebook size, because the lattice vectors are fixed, we can use tiling and JIT-compiling techniques to reduce both memory and runtime costs compared to vanilla VQ. Accomodating smaller codebooks. In some cases with less data, codebook size of 196, 560 may be too large. We can also take one type of Λ24(2)s or its subset so that the codebook size range 1, 104 98, 304. 3.5. Integration with other quantization techniques Multi-scale residual quantization. Since Λ24-SQ is an inplace replacement of VQ, we can use it in combination with other techniques, such as multiscale quantization [45] and 5 In this paper, we integrate residual quantization [6, 50]. Λ24-SQ into the VAR tokenizer [75] for image generation, allowing for direct comparison with quantization methods such as VQ in VAR [75] and BSQ in Infinity [36]. Aligning with vision foundation models. Better reconstruction does not always lead to better generation quality [35, 56, 63, 86]. VAVAE [77] proposes to address this reconstruction-generation dilemma by aligning latent embeddings with vision foundation models. We use the VF loss [84] between the latent embedding before quantization and the feature extracted from pretrained DINOv2 [58]. 4. Autoregression with Very Large Codebook 4.1. Representing the codebook mapping Preliminaries. As NPQ scales up the effective size of the codebook, effectively representing the codebook mapping for the autoregressive models becomes big issue. The most straightforward way is to represent each code by unique index. It uses an embedding matrix RV to map each index to vector and an unembedding matrix RDV to get the final logits of dimension for simple classification problem. Memory cost and training stability are the two biggest challenges. There are two more solutions: (1) index subgroup [86] and (2) bitwise operation [36, 80]. The former is compatible with the autoregression framework, but the resulting sequence length grows linearly w.r.t. the number of groups. Han et al. model bitwise tokens with multiple BCE losses in parallel in [36]. However, it only applies to LFQ/BSQ and relies on bitwise self-correction to mitigate the train-test discrepancy. In the following, we show our improvements to accommodate the family of spherical lattice codes. Simple classification with memory optimization. We adopt the cut cross entropy (CCE) [81] to address memory consumption. Since the visual auto-regressive models are trained from scratch, we use Kahan summation [46] to maintain numerical stability, as suggested by [81]. We leave the training techniques in Section 4.2. Factorized d-itwise prediction. Densest sphere-packing lattices like E8 and Λ24 take integer values, but all possible values go beyond binary [91]. We generalize the concept of bitwise prediction and propose factorized d-it6 prediction. Assuming independence across channels, the joint log-probability of one lattice code c(1:d) is approximated by the sum of the log-probabilities of each dimension, i.e. log p(c(1:d)) (cid:88) log p(c(i)), (12) where p(c(i)) denotes the probability of the i-th element of 6Short for log base-d unit, analogous to bit for log2() and nat for ln(). Figure 3. Training curve for 16-layer -CC model. The Dion optimizer addresses the problem of exploding gradient norm. Z-loss effectively regularizes log Z2 and smoothens the loss and gradient curve, leading to lower training loss. Distributed Orthonormalized Updates. We use Dion optimizer [2] for all weight tensors greater than 1D and Lion [13] for 1D weight tensors and the [un]embedding layers. The unembedding layer is updated with learning rate scaled by 1/ din, where din is its input dimension. From Figure 3, both techniques lead to smoother training dynamics, with lower variance and fewer spikes, and achieve lower final loss value. 4.3. Sampling The sampling follows convention [75]. We apply classifierfree guidance (CFG), first proposed for diffusion models [38] and later adopted in AR-based models [56, 72]. At inference, each tokens logit zg is formed by zg = zu + s(zc zu), where zc is the conditional logit, zu is the unconditional logit, and is the CFG scale. We use layerwise linearly scaling CFG, first introduced in Infinity [36]. We observe that linearly scaling top-K is also helpful. For the factorized d-itwise configuration, we apply CFG on the normalized probability, i.e. pg = pu + s(pc pu), where = exp according to Eq. (12). Nucleus sampling (top p) [39] is also used. (cid:17) log p(c(i)) (cid:16)(cid:80)d 5. Experiments 5.1. Experimental setup Architectures. We train the image tokenizer with different quantization methods on ImageNet-1K [66]. The experiments cover two network architectures: (1) Vision Transformers (ViT) [24], which runs at high throughput and yields high reconstruction fidelity, and (2) ConvNets, which are more commonly seen in image generation. We compare our method with VQ-based [65, 75] and BSQ-based methods [36]. The training details are specified in the Appendix. Training objectives. We use weighted average of three losses, the mean absolute error (MAE, ℓ1), GAN, and perceptual loss, without any other regularization terms. The MAE, GAN, and perceptual loss optimize the PSNR, FID, and LPIPS score according to their respective definitions. Therefore, this trio can no longer be simplified. Evaluation. We evaluate image compression in the Kodak Lossless True Color Image Suite. It includes 24 24bit lossless color PNG images. We report PSNR and MSFigure 4. Codebook usage histogram. The imbalance in huge codebook calls for dedicated training tricks in 4.2. Usage is computed on IN-1k val50k over 10 VAR levels. y-axis in log scale. : 4,096 VQ codebook indices and density are normalized for illustrative purposes. the d-dim lattice codes. For Λ24-SQ, we use 24 9-way classification heads to include all possible values {4, , 4}. d-itwise self-correction is also possible by toggling any element with certain probability, though we do not explore it in this paper. 4.2. Training We train 16-layer Infinity model but observe consistent increase in gradient norms and explosion of loss (the blue curve in Figure 37). natural hypothesis for this is about the large codebook8: We plot the codebook usage on ImageNet-val, sorted by density, in Figure 4. For the standard VQ codebook, the largest frequency and smallest frequency are within the same order of magnitude ( 7.69e4 1.37e4 5.6). For Λ24-SQs large codebook, the ratio between the most frequent index and the least frequent one surges to 8.90e5 2.41e6 37. The imbalance is more visible after the VF alignment (Sec. 3.5). We further hypothesize that it prevents prior visual auto-regressive models from utilizing large visual codebook for generation, although the low utilization problem during reconstruction appears to be fixed [70, 93]. Despite the difficulty, we recognize that this is not unique issue in visual generation. An unbalanced, large codebook is common when training large language models [15, 57, 82]. Therefore, we borrow two simple and effective techniques, namely Z-loss [15] and improved optimization with orthonormalized matrix updates [44, 52]. Z-loss [15] prevents the final output logits from explod2 ing. Namely, LZ = α log Z2 = α where we set α to be 104 as in [57]. exp(zi) (cid:12) (cid:12) (cid:12)log (cid:16)(cid:80)V (cid:17)(cid:12) (cid:12) (cid:12) , 7The loss explosion occurs earlier when the model goes to 1B. 8Also, we drop the entropy regularization that promotes class balance. Table 5. Image reconstruction results on COCO2017 and ImageNet-1k (256 256). COCO2017 val ImageNet-1k val Method Arch. Quant. Param. #bits TP PSNR SSIM LPIPS rFID PSNR SSIM LPIPS rFID DALL-E dVAE [64] MaskGIT [10] SD-VAE 1.x [65] ViT-VQGAN [85] BSQ-ViT [91] Λ24-SQ-ViT C 98M 54M 68M 182M 13 7.5 13 34.0 25.153.49 .7497.1124 .3014.1221 55.07 25.463.93 .7385.1343 .3127.1480 36.84 VQ 10 37.6 17.522.75 .4194.1619 .2057.0473 2.23 VQ 14 22.4 22.543.55 .6470.1409 .0905.0323 1.23 VQ 1.55 VQ - BSQ 174M 18 45.1 25.083.57 .7662.0993 .0744.0295 1.14 Λ24-SQ 174M 18 45.1 26.003.67 .8008.0879 .0632.0262 0.83 8.90 17.932.93 .4223.1827 .2018.0543 6.07 22.823.97 .6354.1644 .0912.0390 - 5.81 25.364.02 .7578.1163 .0761.0358 5.15 26.374.15 .7934.1011 .0622.0317 - - - - - Method BPP PSNR MS-SSIM JPEG2000 0.2986 29.192 0.2963 29.151 WebP MAGVIT2 0.2812 23.467 0.2812 27.785 BSQViT Λ24-SQ 0.2747 29. .9304 .9396 .8452 .9481 .9637 Image Table 6. compression on Kodak. Λ24-SQ use only ℓ1 loss and does not use arithmetic coding. SSIM [79] at different levels of bits per pixel (BPP). We assess image reconstruction and generation on the ImageNet1k validation set. Reconstruction is measured by FID, PSNR, SSIM, and LPIPS [90]. Generation is measured by FID, Inception Score (IS) [68], and improved precision and recall (IPR) [49], calculated by the ADM Tensorflow Evaluation Suite [23]. We use rFID/gFID to disambiguate. 5.2. Main results: Comparison to state-of-the-art State-of-the-art image reconstruction. Table 5 compares the image reconstruction results on COCO 2017 and ImageNet-1k. ViT-based auto-encoder with Λ24-SQ reduces rFID by 1020% and improves all other metrics. State-of-the-art image compression. We show the compression results on Kodak in Table 6. Since the resolution is 768512 or 512768, we encode/decode them in 256256 tiles without overlapping or padding. We compare our method with traditional codecs, including JPEG2000 [22] and WebP [31], and tokenizer-based approaches, including MAGVITv2 [86] and BSQViT [91]. Λ24-SQ-ViT achieves higher PSNR and MS-SSIM scores while using slightly smaller BPP. Note that the rate-distortion tradeoff can be further improved (25% less bitrate) by training an unconditional AR model for arithmetic coding [21, 91], which is not the primary focus of this paper. State-of-the-art image generation. We select the classFigure 5. VAR Tokenizer. VAR+Λ24-SQ achieves an rFID of 0.84; VAR+Λ24-SQ (+VF) achieves an rFID of 0.92. More metrics are given in Table 12. 7 Table 7. Image generation on ImagetNet. (re) refers to rejection sampling. Method gFID IS Prec Rec # Params Steps VQGAN(re) [27] 5.20 280.3 1.4B - VIM(re) [85] 1.7B - 3.04 227.4 RQ-TF(re) [50] 3.80 323.7 3.8B - 3.05 222.3 0.80 0.58 3.1B LlamaGen [72] - - - VAR-d24 [75] -CC+Λ24-SQ 2.09 312.9 0.82 0.59 1.0B 2.18 332.3 0.78 0.63 1.0B (0.3B) VAR-d30 [75] -CC+Λ24-SQ (Val data) 1.92 323.1 0.82 0.59 2.0B 1.82 333.4 0.78 0.64 2.8B (0.4B) 1.78 236.9 0.75 0.67 - 256 1024 68 256 10 7 10 7 - conditioned Infinity [36] as baseline. Infinity-CC employs 7-level next-scale prediction backbone, saving 25% tokens and running 30% faster than VAR (10 levels) [75]. (i) VAR tokenizer. We train VAR tokenizer with the standard schedule (100 epochs) suggested in Infinity [36]. We use Λ24-SQ as the bottleneck with two codebook sizes: (1) complete codebook, whose bitrate is similar to BSQ (d = 18), and (2) subset of 16,384 codes, whose bitrate is equivalent to BSQ (d = 14). Figure 5 clearly demonstrates the superiority of our method. (ii) VAR generation. Table 7 shows the generation results on ImageNet-1k. We also provide the oracle result computed from the validation set in the bottom row. InfinityCC+Λ24-SQ works comparably with VAR-d24 in terms of model size (1B) while being 30% more efficient. Note that our results achieve higher recall and push the precisionrecall tradeoff closer to the validation oracle. We attribute this to the larger codebook, which better captures visual diversity. When the parameters increase to 2.8B, -CC+Λ24SQ achieves an FID of 1.82, which is comparable to both VAR-d30 and the oracle result. 5.3. Scientific investigations and ablative studies Dispersiveness leads to better rate-distortion tradeoff. We start from the comparison in Figure 2 and ask if dispersiveness, quantified by higher δmin(N ), leads to better rate-distortion tradeoff for visual tokenization. We train plain ViT-small encoder-decoder on ImageNet-128 Table 8. Quantizer with higher δmin lead to better reconstruction. VQ (PN -): projected normal distribution initialization. Note that we gray out the last two rows to indicate that the learnable configurations are not used elsewhere. Method VQ (RP, ) VQ (RP, ) BSQ Λ24-SQ 214 214 214 214 Utility rFID LPIPS SSIM PSNR 95.09% 13.08 0.1080 0.7086 23.018 100.0% 12.00 0.1021 0.7240 23.354 97.21% 12.98 0.1048 0.7058 23.171 100.0% 11.16 0.1007 0.7258 23.390 (Fixed) 217 218 BSQ BSQ 57.04% 12.46 0.0963 0.7296 23.742 70.00% 10.96 0.0914 0.7351 23.752 VQ (RP, ) 196, 560 94.87% 9.10 0.0829 0.7624 24.216 196, 560 94.84% 8.98 0.0811 0.7647 24.282 Λ24-SQ (Learn) VQ (PN -init.) 196, 560 95.45% 9.13 0.0832 0.7623 24.226 196, 560 94.78% 8.78 0.0820 0.7644 24.274 Λ24-SQ Pred. head gFID IS Prec Rec BCE CE 10.7 219.6 0.85 0.21 BSQ [36] 10.3 187.3 0.85 0.27 BSQ [36] Λ24-SQ 9-way CE 11.7 155.8 0.82 0.29 Λ24-SQ 215.4 0.85 0.30 8.7 CE Figure 6. VF alignment improves convergence and final generation results, especially recall. The model has 12 layers (240M). Table 9. -CC with different prediction heads. CFG = 2, = 0.95, and varies. Grid search in Figure 8. while varying only the quantization bottleneck. We also test two vocabulary sizes, medium (214) and large (196, 560). With the codebook fixed, we find Λ24-SQ achieves the best rFID, LPIPS, SSIM, and PSNR. The random projection VQ and BSQ follow behind. We also test learnable codebooks given these as initialization. The conclusion still holds, and learnable codebook does not greatly affect the final results. VF alignment helps discrete tokens, too. Figure 5 shows worse reconstruction quality after aligning with the DINOv2 feature [58]. However, Figure 6 demonstrates that the VAR generation using VF-aligned tokenizer converges faster and achieves better final results in gFID, IS, and especially recall. This extends the findings in VAVAE [84] about VF alignment from continuous latents to discrete ones. VAR generation with different heads. We test various prediction heads, namely 18 BCE vs. 262, 144-way CE for -CC+BSQ, 24 9-way CE vs. 196, 560-way CE for -CC+Λ24-SQ in Table 9, showing that Λ24-SQ +CE achieves great results despite simplicity. The factorized ditwise prediction yields worse gFID and lower recall in both cases, implying that factorized approximation sacrifices diversity. We also find that the optimal sampling hyperparameters vary and conduct small-scale grid search in Figure 8. Scaling the codebook size does matter. The last but not least critical question is whether increasing the codebook size benefits the generation results. To answer this, we used the two VAR tokenizers with = 196, 560 vs. 16, 384) with reconstruction results in Figure 5, and trained VAR models with varied sizes on top while keeping all the rest settings the same. To report the gFID, we search the sampling hyFigure 7. Scaling effect of the codebook size. Left: Increasing the codebook size improves gFID when the model is large (0.49B). Right: Increasing the codebook size pushes the Precision-Recall Pareto frontier towards the oracle precision-recall derived from the validation set (see the zoom-in at the bottom left). perparameters to find an optimal value. From Figure 7, we conclude that increasing the codebook size improves gFID when the model is large, e.g., 12-layer (0.24B) to 16-layer (0.49B). This echoes the finding in LLMs that larger models deserve larger vocabularies [74]. We look at the improved precision and recall metric in the right subplot of Figure 7. We use top-p = 0.95, CFG of lin(1, 0.33), and vary top-k to obtain the data points. We find that when the codebook size increases, the precision-recall Pareto frontier moves towards the oracle precision-recall derived from the val set. 6. Related work Vector quantization (VQ) [33, 77] lays the foundations of learning discrete visual tokens. However, VQ is notoriously difficult to train, which is attributed to the misalignment between the embedding distribution of the model and the codebook [40]. Optimization tricks are then introduced, e.g. Gumbel Softmax [3, 42], Rotation trick [28], and Index 8 Backpropagation [70]. The line of lattice-based quantization methods covered in this paper [56, 86, 91] addresses this misalignment issue by keeping the codebook fixed. Our Λ24-SQ is an intuitive extension in this direction. Scaling visual tokenizers has recently attracted attention and covers many directions, including increasing parameters [83], training data [37], unifying generation and understanding [55, 92], and encoding multiple modalities [54, 78]. Our paper focuses on scaling the vocabulary size. Despite several advances in reconstruction [70, 93], none have reported that an expanded vocabulary benefits generation yet. Our paper shows that visual autoregressive model scales to very large codebook (200K) without tricks such as index subgrouping [86], etc. Autoregressive visual generation applies an autoregressive model to visual generation [12, 25, 76] similar to the LLM paradigm [7, 9, 62]. Modern AR model employs visual tokenizer for efficiency [27]. Subsequent work explores what to auto-regress [75, 87] and auto-regressive order [59, 88]. Most AR models are based on medium-sized visual vocabulary (1K10K), limiting their potential [86]. Lattice coding has wide applications in digital communication [89] and cryptography [60]. In this paper, we borrow this concept to describe different non-parametric quantization methods and others in the same language. This further inspires us to devise new quantization methods based on the principle of densest hypersphere packing. The hyperspherical prior is also loosely related to some recent work about learning on spherical manifold [20, 53]. 7. Conclusion We have introduced spherical Leech quantization (Λ24-SQ), novel quantization method that scales the visual codebook to 200K, and demonstrated its applications in visual compression, reconstruction, and generation on ImageNet. In the future, we are interested in verifying its effectiveness in larger-scale settings, e.g., text-conditioned visual generation. Acknowledgments. This material is based upon work in part supported by the National Science Foundation under IIS-1845485. The authors acknowledge the Grant No. IFML Center for Generative AI and the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing computational resources that have contributed to the research results reported within this paper. This work was supported by Hoffman-Yee Research Grant from the Stanford Institute for Human-Centered Artificial Intelligence (HAI). This research was also supported in part by Lambda, Inc. YZ would like to thank Jeffrey OuyangZhang and Mi Luo for their help in setting up environments on TACC; Yi Jiang and Bin Yan for their clarification on VAR and Infinity baselines."
        },
        {
            "title": "References",
            "content": "[1] Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. NeurIPS, 2017. 2 [2] Kwangjun Ahn, Byron Xu, Natalie Abreu, Ying Fan, Gagik Magakyan, Pratyusha Sharma, Zheng Zhan, and John Langford. Dion: Distributed orthonormalized updates. arXiv preprint arXiv:2504.05295, 2025. 6 [3] Alexei Baevski, Steffen Schneider, and Michael Auli. vqwav2vec: Self-supervised learning of discrete speech representations. In ICLR, 2020. 8 [4] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In CVPR, 2024. 2 [5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022. 2 [6] Christopher Barnes, Syed Rizvi, and Nasser Nasrabadi. Advances in residual vector quantization: review. TIP, 1996. 5 [7] Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. neural probabilistic language model. JMLR, 2003. [8] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic arXiv preprint neurons for conditional computation. arXiv:1308.3432, 2013. 2 [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 9 [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 2, 7 [11] Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In AISTATS, 2005. 3 [12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. 9 [13] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. NeurIPS, 2023. [14] Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning with randomprojection quantizer for speech recognition. In ICML, 2022. 1, 3 [15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. JMLR, 24(240):1113, 2023. 6 [16] Henry Cohn, Abhinav Kumar, Stephen Miller, Danylo Radchenko, and Maryna Viazovska. The sphere packing problem in dimension 24. Annals of mathematics, 2017. 4 9 [17] John Horton Conway and Neil JA Sloane. Twenty-three constructions for the leech lattice. Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences, 381 (1781):275283, 1982. [18] John Horton Conway and Neil James Alexander Sloane. Sphere packings, lattices and groups. Springer Science & Business Media, 2013. 3, 4, 5, 2 [19] Thomas Daede, Nathan Egge, Jean-Marc Valin, Guillaume Martres, and Timothy Terriberry. Daala: arXiv perceptually-driven next generation video codec. preprint arXiv:1603.03129, 2016. 2 [20] Tim Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub Tomczak. Hyperspherical variational autoencoders. In UAI, 2018. 9 [21] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi GrauMoya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. In ICLR, 2024. 7 [22] Antonin Descampe, David Tschumperle, Gilles Burel, Charles Deledalle, Pierre A. Carre, and Benoit Macq. Openjpeg - an open-source jpeg 2000 codec written in C. In PCS, 2006. 7 [23] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. 7 [24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 6 [25] Alexei Efros and Thomas Leung. Texture synthesis by non-parametric sampling. In ICCV, 1999. 9 [26] Thomas Ericson and Victor Zinoviev. Codes on Euclidean spheres. Elsevier, 2001. 1, 3 [27] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 2, 7, [28] Christopher Fifty, Ronald Junkins, Dennis Duan, Aniketh Iyengar, Jerry Liu, Ehsan Amid, Sebastian Thrun, and Christopher Re. Restructuring vector quantization with the rotation trick. In ICLR, 2025. 2, 8 [29] Allen Gersho and Robert Gray. Vector quantization and [30] signal compression. Springer, 2012. 2 Alvaro Gonzalez. Measurement of areas on sphere using fibonacci and latitudelongitude lattices. Mathematical geosciences, 42(1):4964, 2010. 4, 1 [31] Google Developers. WebP Compression Techniques. https : / / developers . google . com / speed / webp/docs/compression, 2025. Accessed: 2025-1010. 7 [32] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. NeurIPS, 2004. 3 [33] Robert Gray. Vector quantization. IEEE ASSP Magazine, 1 (2):429, 1984. 2, 8 [34] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633 638, 2025. 2 [35] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. [36] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In CVPR, 2025. 2, 5, 6, 7, 8, 3 [37] Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. In ICML, 2025. 9 [38] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS workshop, 2022. 6 [39] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020. 6 [40] Minyoung Huh, Brian Cheung, Pulkit Agrawal, and Phillip Isola. Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks. In ICML, 2023. [41] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 [42] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017. 2, 8 [43] Aren Jansen, Daniel PW Ellis, Shawn Hershey, Channing Moore, Manoj Plakal, Ashok Popat, and Rif Saurous. Coincidence, categorization, and consolidation: Learning to In ICASSP, recognize sounds with minimal supervision. 2020. 2, 3 [44] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. 6 [45] Biing-Hwang Juang and Gray. Multiple stage vector quantization for speech coding. In ICASSP, 1982. [46] William Kahan. Pracniques: further remarks on reducing truncation errors. Communications of the ACM, 8(1):40, 1965. 5 [47] Kristin Koch, Judith McLean, Ronen Segev, Michael Freed, Michael Berry, Vijay Balasubramanian, and Peter Sterling. How much the eye tells the brain. Current biology, 16(14):14281434, 2006. 2 [48] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized information maximization. NeurIPS, 2010. 3 [49] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 2019. 7 [50] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. 5, 7 10 [51] John Leech. Notes on sphere packings. Canadian Journal of Mathematics, 19:251267, 1967. 2, 4 [52] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. 6 [53] Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg. ngpt: Normalized transformer with representation learning on the hypersphere. In ICLR, 2025. 9 [54] Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, and Yinfei Yang. Atoken: unified tokenizer for vision. arXiv preprint arXiv:2509.14476, 2025. 9 [55] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. NeurIPS, 2025. 9 [56] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. In ICLR, 2024. 1, 2, 3, 5, 6, [57] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. In CoLM, 2025. 6 [58] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 5, 8 [59] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. 9 [60] Chris Peikert et al. decade of lattice cryptography. Foundations and trends in theoretical computer science, 10(4): 283424, 2016. 9 [61] Steven Piantadosi, Harry Tily, and Edward Gibson. Word lengths are optimized for efficient communication. PNAS, 108(9):35263529, 2011. 2 [62] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018. [63] Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, and Ali Farhadi. When worse is better: Navigating the compression-generation tradeoff in visual tokenization. arXiv preprint arXiv:2412.16326, 2024. 5 [64] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 7 [65] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 6, 7 [66] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Imagenet large Aditya Khosla, Michael Bernstein, et al. IJCV, 115(3):211252, scale visual recognition challenge. 2015. 6 [67] K. Sadri. Can the fibonacci lattice be extended to dimensions higher than 3? Mathematics Stack Exchange, 2019. URL:https://math.stackexchange.com/q/3297830 (version: 2024-05-03). 4, 1 [68] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. NeurIPS, 2016. 7 [69] Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):5064, 1951. 2 [70] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization. In ICCV, 2025. 6, 9 [71] NJA Sloane, RH Hardin, WD Smith, et al. Spherical codes. URL: http://neilsloane.com/packings/, 2000. 4 [72] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 6, 7 [73] Pieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the surface of pollen-grains. Recueil des travaux botaniques neerlandais, 27(1):184, 1930. 4 [74] Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies. NeurIPS, 2024. 8 [75] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 2, 5, 6, 7, 9 [76] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 2016. [77] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 1, 2, 5, 8 [78] Junke Wang, Yi Jiang, Zehuan Yuan, Bingyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint imagevideo tokenizer for visual generation. NeurIPS, 2024. 9 [79] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In ACSSC, 2003. 7 [80] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. TMLR, 2024. 2, 5 [81] Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, and Philipp Krahenbuhl. Cut your losses in largevocabulary language models. In ICLR, 2025. 5 [82] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies In ICLR, for large-scale transformer training instabilities. 2024. 6 [83] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion In ICCV, parameters for autoregressive image generation. 2025. 9 [84] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. 5, 8 [85] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. 1, 7 [86] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 1, 2, 3, 5, 7, 9 [87] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024. 9 [88] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. In ICCV, 2025. 9 [89] Ram Zamir. Lattice coding for signals and networks: structured coding approach to quantization, modulation, and multiuser information theory. Cambridge University Press, 2014. [90] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7 [91] Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. In ICLR, 2025. 1, 2, 3, 5, 7, 9 [92] Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krahenbuhl, and De-An Huang. Qlip: Text-aligned visual tokenization unifies autoregressive multimodal understanding and generation. arXiv preprint arXiv:2502.05178, 2025. 9 [93] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. NeurIPS, 2024. 6, 9 12 A. Constructing Fibonacci Lattices Fibonacci lattice constructs points that are evenly distributed with each of them representing almost the same area [30] in unit square [0, 1)2 using the formula: (xi, yi) = (i ψ , ) for 0 < n, (13) (cid:17) (cid:16) Fn+1 Fn where ψ = limn . We can map this point distribution to unit-length sphere S2 using cylindrical equal-area projection. = 1+ 2 (cid:19) (cid:18)xi yi (cid:18)θi = 2πxi ϕi = arccos(1 2yi) (cid:19) = cos θi sin ϕi = sin θi sin ϕi = cos ϕi . The absolute value of the Jacobian determinant for the change of variables (u1, , ud+1) (cid:55) (r, φ1, , φd) is (cid:12) (cid:12) (cid:12) (cid:12) (u1, , ud+1) (r, φ1, , φd) (cid:12) (cid:12) (cid:12) (cid:12) = rd (cid:89) k=2 sink1 φk. (18) Therefore, Equation 16 reduces to p(Φ1:d) (cid:89) k=2 sink1 φk. (19) With Equation 17, we have p(Φk) sink1 φk. Then we get the normalization constants Zk = (cid:82) π 0 sink1 φdφ = Γ((k+1)/2) for = 2, , and Z1 = (cid:82) 2π 0 dφ1 = 2π. Finally, we arrive at πΓ(k/2) (14) (Φk = φk) = (cid:40) 1 2π , 1 π Γ( k+1 2 ) Γ( 2 ) = 1 sink1 φk, = 2, , (20) A.1. Generalizing the Spherical Fibonacci Lattice to higher dimensions We provide the details to generalize the 3D spherical Fibonacci lattice [30] to higher dimensions [67]. Given d-dimensional vector = (u1, u2, , ud+1) Sd Rd+1, we can also represent it in the hyperspherical coordinate system (r, φ1, φ2, , φd), where φ1 [0, 2π] , φ2, , φd [0, π], and specifically, = 1 for = 1. The conversion to Cartesian coordinates is given as follows: ud+1 = cos(φd) ud = sin(φd) cos(φd1) ... u2 = sin(φd) sin(φd1) sin(φ2) cos(φ1) u1 = sin(φd) sin(φd1) sin(φ2) sin(φ1). We examine the distribution over the angular coordinates Φ1, ,d1,d [0, 2π] [0, π]d1. Denote the cumulative distribution function with another variable (Y = y) = FΦ(φ) = (cid:90) φk 0 p(Φ = u)du (21) . (22) (cid:40) 1 = 2π y, = 1 ... The Fibonacci-like spiral (n) = generated by the following formula: (cid:16) (n) 1 , , (n) (cid:17) is (n) = + 1 d1 = {na1}, , (n) ... (n) 1 = {nad1}, (23) (24) (25) (26) (27) p(Φ1:d) = p(Φ1, Φ2, , Φd) (15) = ρ(Φ1)ρ(Φ2Φ1) ρ(ΦdΦ1, , Φ1:d1). (16) where {x} refers to xs decimal part, i.e. {x} = x. a1:d satisfies ai aj / Q, = j. The angles are given by taking the inverse: The key observation is that the angles are independently distributed. To see this, then (φk+1, , φd) parameterizes subsphere isoporphic to Sdk with rescaled radius = sin(φ1) sin(φk). In other words, p(Φk+1:dΦ1:k) = p(Φk+1:d) for any [d]. As such, Equation 16 can be simplified to: if we fix (φ1, , φk), = 1(Y (n) φ(n) ) φ[t + 1] = φ[t] (φ[t]) (φ[t]) (28) (29) p(Φ1:d) = (cid:89) α=1 B. Details of the Leech Lattice pα(Φα). (17) Generator matrix. The generator matrix for the unconstrained Λ24 is given in Table 10. 1 Table 10. Generator matrix for the Leech lattice Λ24. The table is adapted from Table 4.12 in [18]. 1 8 4 4 4 4 4 4 2 4 4 4 2 4 2 2 4 2 2 2 0 0 0 3 0 0 0 4 0 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 2 2 0 0 2 2 2 0 0 0 0 0 0 1 1 1 0 0 0 0 4 0 0 2 0 0 0 0 0 2 2 0 2 2 2 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 4 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 4 0 0 2 0 2 2 0 2 2 2 2 2 2 1 0 0 0 0 0 0 0 0 0 4 0 2 0 2 0 0 2 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 4 2 0 0 2 0 0 2 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 4 2 2 0 0 0 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 2 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 C. Experimental Details C.1. Architectures ViT tokenizer. The architecture used to get the main results follows [91]. To conduct ablative studies in Table 8, we use smaller architecture (ViT-small) for fast iteration. Both the hidden dimension and the number of layers are halved. CNN-based VAR tokenizer. The architecture follows Infinity [36]9. We use 7 scales: [12, 22, 42, 62, 82, 122, 162], which amounts to 521 tokens in total. Infinity-CC. The Infinity model used in the paper mostly follows the original Infinity paper [36]. We make two key changes: (1) The text condition is converted to class condition, which is representation by single <SOS> token. The vocabulary size is augmented to + + 1, where = 1000. The additional 1 refers to the no-class index, which is used to randomly replace the original class index with probability of 0.1, to enable classifier-free guidance (CFG) at the sampling phase. (2) We revert the shared AdaLN to an unshared version following VAR [75]. Ta9https://github.com/FoundationVision/BitVAE ble 11 summarizes the model configurations and parameter size in this paper. C.2. Training specifications ViT tokenizer. We train the image tokenizer with batch size of 32 per GPU. We use AdamW optimizer with (β1, β2) = (0.9, 0.99) with 1 104 weight decay. The base learning rate is 4 107 (or total learning rate of 1104) and follows half-period cosine annealing schedule. The model is trained for 1M steps, which amounts to 200 epochs over the entire ImageNet-1k training set. We use an ℓ1 loss weight of 1, perceptual loss weight of 0.1, and an adversarial loss weight of 0.1 throughout the experiments. VAR tokenizer. We train the VAR tokenizer with batch size of 8 per GPU. Two schedules are used: (1) the fast schedule trains the model for 500k iterations with 8 GPUs, which approximately sees the training data 25 epochs; (2) the standard schedule trains the model for 500k iterations with 32 GPUs, which is approximately 100 epochs. 2 lin(x0, s) deTable 13. Advanced sampling techniques. notes the linear scaling strategy which starts from x0 and increment/decrements by per scale. . Tokenizer rFID CFG top Λ24-SQ (25 ep) 1.08 Λ24-SQ (100 ep) 0.84 Λ24-SQ (100 ep) 0.84 Λ24-SQ (100 ep) 0.84 Λ24-SQ (100 ep) 0.84 5 103 2 5 103 2 lin(1, 0.33) 5 103 lin(1, 0.25) 5 103 lin(1, 0.33) lin(104, 103) gFID 8.78 7.46 6.81 7.33 6.68 Λ24-SQ (vf) Λ24-SQ (vf) Λ24-SQ (vf) 1.18 1.18 1. lin(1, 0.33) 5 103 5.79 lin(1, 0.33) 2, 500 5.41 lin(1, 0.33) lin(2000, 100) 5.30 It is also worth noting that, according to the bottom half of Table 13, the optimal decreases when the tokenizer is trained with the VF loss. This is most likely because the probability density is more skewed, as is illustrated in Figure 4. Qualitative Results. Figure 9 shows more generation results sampled by Infinity-CC + Λ24-SQ (2B). We cherrypick the images and emphasize the quality and diversity. Table 11. Model configurations for Infinity-CC. layer embed dim # heads # params (head) # epochs 12 16 24 32 768 1152 1536 8 12 16 16 242M (151M) 394M (226M) 1B (300M) 2.8B (402M) 50 200 350 400 Table 12. VAR Tokenizer. rFID LPIPS SSIM PSNR . (fast schedule) 16,384 BSQ Λ24-SQ 16,384 262,144 BSQ Λ24-SQ 196,560 Λ24-SQ (vf) 196,560 (standard schedule) 262,144 BSQ Λ24-SQ 196,560 Λ24-SQ (vf) 196,560 1.82 1.36 1.29 1.08 1.18 1.07 0.84 0.92 0.1268 0.5626 19.989 0.1170 0.5957 20.639 0.1106 0.6006 20.683 0.1005 0.6280 21.315 0.1088 0.6006 20.734 0.1064 0.6035 20.430 0.0954 0.6333 21.535 0.1041 0.6118 21. D. More Results D.1. VAR tokenization First, we retrain VAR tokenizer with fast schedule (25 epochs). We use Λ24-SQ as the bottleneck with two codebook sizes: (1) the full codebook, whose bitrate is similar to BSQ (d = 18), and (2) subset of 16,384 codes, whose bitrate is equivalent to BSQ (d = 14). From the upper half of Table 12, Λ24-SQ outperforms BSQ in all metrics in both cases. Next, we train VAR tokenizer with the standard schedule (100 epochs) suggested in Infinity [36]. The full numbers are reported in the bottom half of Table 12, supplementing Figure 5. D.2. VAR generation Grid search of sampling parameters. We run smallscale grid search of sampling hyperparameters for InfinityCC with different prediction heads. We compare the gFID score on IN-1k by generating 10 samples per class (10k generated samples in total). From Figure 8, we conclude that the optimal top varies significantly across different prediction head settings. Advanced sampling techniques. In Section 4.3, we introduced advanced sampling techniques, including layerwise linearly scaling CFG and linearly scaling top-k. We show related ablation studies in Table 13. We use lin(x0, s) to denote the linear scaling strategy, which starts from x0 and changes by per scale. We can see that both layerwise linear scaling CFG and top-k bring noticeable improvement. (a) BSQ + BCE (b) BSQ + CE (c) Λ24-SQ + d-way CE (d) Λ24-SQ + CE Figure 8. Hyperparameter grid search supplementing Table 9. We use fixed temperature τ = 1 in (a-d) and top = 0.95 in (b-d). Figure 9. More sampled generation results of Infinity-CC + Λ24-SQ (2B). Classes are 323: monarch butterfly; 107: jellyfish; 417: balloon; 279: arctic fox."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Mistral AI",
        "Stanford University",
        "UT Austin"
    ]
}