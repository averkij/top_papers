{
    "paper_title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing",
    "authors": [
        "Xin Guo",
        "Zhiheng Xi",
        "Yiwen Ding",
        "Yitao Zhai",
        "Xiaowei Shi",
        "Xunliang Cai",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew effect\"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average."
        },
        {
            "title": "Start",
            "content": "2025-10-31 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Xin Guo1, Zhiheng Xi1, Yiwen Ding1, Yitao Zhai2, Xiaowei Shi2, Xunliang Cai2, Tao Gui1,3, Qi Zhang1, Xuanjing Huang1 1Fudan University 2Meituan 3Shanghai Innovation Institute {tgui,xjhuang}@fudan.edu.cn Self-improvement has emerged as mainstream paradigm for advancing the reasoning capabilities of large visionlanguage models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounceda dynamic we term the Matthew effectawhich ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7BInstruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average. aMatthew effect is sociological concept originally proposed by Robert K. Merton (Merton, 1968), which can be summarized as the rich get richer and the poor get poorer. 1. Introduction Large vision language models (LVLMs) have demonstrated impressive reasoning capabilities across complex multimodal tasks (Bai et al., 2025; Zhu et al., 2025). While supervised fine-tuning (SFT) can further improve model performance, its effectiveness is limited by the current lack of sufficient large-scale, highquality annotated datasets (Peng et al., 2025; Zhang et al., 2025b). In response, self-improvement has emerged as promising alternative, enabling LVLMs to iteratively explore and learn from successful trajectories (Deng et al., 2024; Huang et al., 2023; Wang et al., 2025). This paradigm not only elimFigure 1 Matthew effect in self-improvement of LVLMs over iterations and our re-balanced solution. Dark areas illustrate the imbalanced distribution in vanilla selfimprovement, where dominant head and narrow tail become more severe over iterations. Light areas depict re-balanced self-improvementour methods for counteracting Matthew effect by reducing the head and augmenting the tail. *Equal contribution. Corresponding authors. 5 2 0 O 0 3 ] . [ 1 4 7 4 6 2 . 0 1 5 2 : r Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing inates reliance on manual annotations but also promotes better distribution alignment between selfgenerated and real-world data (Jiang et al., 2025). However, our preliminary experiments reveal that self-improvement with varying sampling numbers ùêæ encounters performance bottleneckand even declinein visual reasoning scenarios (3.2). We delve into this process (3.3) and uncover significant imbalance in the distribution of selfgenerated successful trajectories (Ding et al., 2025; Dohmatob et al., 2024). Specifically, we observe that simple samples overwhelmingly dominate the head, while challenging samples in the narrow tail remain underexplored. As illustrated in Figure 1, this imbalance becomes increasingly severe as iterations progress: the dominant head expands further, whereas the narrow tail becomes more marginalized, which we term the Matthew effect (Merton, 1968). Further comparative analysis reveals the Matthew effect of self-generated data manifests in distinct ways; namely, worsening imbalance across difficulty levels (Tong et al., 2024; Xue et al., 2025) and trend toward increasingly shorter average response lengths (Wang et al., 2023), ultimately leading to performance degradation. To this end, we propose four effective strategies for head-tail re-balancing (4) inspired by Kong et al. (2025) and Xi et al. (2024). Intuitively, we first focus on distribution-reshaping to better utilize existing sampled data. For head reduction, we introduce threshold clipping, which randomly truncates responses beyond predefined threshold ùêø to limit successful trajectories per query. For tail augmentation, we propose repeat-based padding, which ensures equal frequency of all queries through repetition. Further, we augment tail data through trajectory-resampling to boost data diversity, introducing adaptive-weighted resampling, which dynamically adjusts resampling weights based on fail rate (Tong et al., 2024). However, this resampling strategy suffers from low efficiency. Therefore, we develop guided resampling, which efficiently explores tail data by initializing model reasoning from varying intermediate steps. We conduct experiments on visual reasoning tasks across Qwen2-VL-7B-Instruct (Wang et al., 2024a) and InternVL2.5-4B (Zhu et al., 2025) models under the settings of sampling number ùêæ = 8 and ùêæ = 16, respectively (5). The results demonstrate that our proposed methods effectively mitigate Matthew effect in visual self-improvement through head-tail re-balancing, achieving significant performance enhancements. Our contributions are summarized as follows: We delve into the visual self-improvement process, revealing that the key challenge of performance bottlenecks is the imbalanced head-tail distribution and Matthew effect over iterations. To address this, we propose methodological framework for head-tail re-balancing, which integrates four strategies from distribution-reshaping and trajectory-resampling perspectives. We conduct comprehensive experiments across different models and visual reasoning tasks, demonstrating the effectiveness of head-tail re-balancing in counteracting the Matthew effect within visual self-improvement. 2. Related Work 2.1. Self-improvement in visual reasoning. LVLMs have demonstrated remarkable performance across various visual reasoning scenarios (Bai et al., 2025; Zhu et al., 2025), where self-improvement approaches have been widely employed (Deng et al., 2024; Wang et al., 2025; Yang et al., 2023). Among these, self-critic (Wang et al., 2025) and self-correction (Cheng et al., 2025; Ding and Zhang, 2025; Wu et al., 2025a) emerge as prevalent optimization strategies. Prior work typically relies on separate critic models for error 2 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing detection and correction, which requires substantial additional resources (Sun et al., 2025; Xiong et al., 2025; Zhang et al., 2025a). In contrast, Wang et al. (2025) introduce unified model that simultaneously generates responses and performs self-critic to refine. Similarly motivated, Ding and Zhang (2025) propose Sherlock to selectively revise erroneous segments in reasoning trajectories. Moreover, to enhance data efficiency, many studies employ direct preference optimization (DPO) in self-improvement (Deng et al., 2024; Tanji and Yamasaki, 2025; Wang et al., 2025). However, these works lack thorough exploration of the iterative process. In this paper, we focus on the essence of vanilla self-improvement and propose targeted and effective strategies to enhance its performance. 2.2. Distribution bias in self-generated data. Previous work has identified biases sampling and head-tail imbalance in self-improvement sampling process (Dohmatob et al., 2024; Shumailov et al., 2024) and addressed them by increasing the proportion of difficult queries through adaptive-weighted (Kong et al., 2025; Tong et al., 2024; Xue et al., 2025) and guided (Ding et al., 2025) sampling methods. However, these studies primarily focus on text-based reasoning while lacking thorough investigation into visual scenarios. While SynthRL (Wu et al., 2025b) enhances the distribution balance by rewriting queries on visual reasoning tasks, it does not delve into the iterative process. Instead, we conduct an in-depth investigation into the observable performance and underlying properties of LVLMs during self-improvement, revealing similar distribution bias. To this end, we introduce four re-balancing strategies, effectively mitigating the Matthew effect and achieving significant performance improvements. 3. Matthew Effect in Self-improvement While existing research has explored self-generated data, the self-improvement process in visual reasoning scenarios remains underexplored. Therefore, we delve into this process to uncover its intrinsic properties. 3.1. Formulating Self-improvement In this paper, we formulate the vanilla self-improvement as follows. Given model Mbase, training dataset Dbase = {(ùëûùëñ, ùëéùëñ)}ùëÅ ùëñ=1 where ùëûùëñ represents the query, ùëéùëñ denotes the ground-truth answer, and ùëÅ is the dataset size, we define the initial model as M0 = Mbase and the number of iterations as ùëá. At each iteration ùë° [1, ùëá], the self-improvement sequentially performs three key stages: exploration, filtering, and learning. Exploration. At iteration ùë°, for each query ùëûùëñ Dbase, the model Mùë°1 generates ùêæ different responses {ÀÜùëü (ùë°) ùëñ,ùëò Mùë°1(ùëûùëñ). Therefore, the sampled dataset is represented as , where ÀÜùëü (ùë°) ùëñ,ùëò }ùêæ ùëò=1 (ùë°) sample = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ,ùëò ) 1 ùëñ ùëÅ, 1 ùëò ùêæ}. Filtering. To obtain high-quality training data, we define binary reward function as ùëü ùëì (ùëûùëñ, ùëéùëñ, ÀÜùëé(ùë°) ùëñ,ùëò ) = (cid:40)0, 1, if ÀÜùëé(ùë°) if ÀÜùëé(ùë°) ùëñ,ùëò ùëéùëñ ùëñ,ùëò = ùëéùëñ , 3 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing (a) Performance bottlenecks. (b) Distribution of difficulty level. (c) Distribution of response length. Figure 2 Performance bottlenecks and distribution characteristics in self-improvement. (a) Phenomenon of performance bottlenecks under different sampling numbers ùêæ. (b) Comparison of difficulty level distributions between original and self-generated data, ranging from level 1 (easiest) to level 5 (most difficult). (c) Differences in response length distributions between original and selfgenerated data, with dashed lines indicating mean values. where ÀÜùëé(ùë°) ùëñ,ùëò is the answer extracted from response ÀÜùëü (ùë°) ùëñ,ùëò . Using it, we obtain the filtered dataset: (ùë°) filter = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ,ùëò ) (ùë°) sample ùëü ùëì (ùëûùëñ, ùëéùëñ, ÀÜùëé(ùë°) ùëñ,ùëò ) = 1}. Learning. At iteration ùë°, the model Mùë° is trained by fine-tuning the base model Mbase on the filtered dataset (ùë°) . The objective is to minimize the negative log-likelihood: train = (ùë°) filter (ùë°) SFT = ùîº(ùëû,ùëü)D (ùë°) train ùëü ùëó= log ùëÉ(ùëü ùëóùëû, ùëü< ùëó; Mùë°). Through ùëá iterations of this process, the model continuously generates higher-quality responses, which in turn enhances its performance in the next iterations, thereby achieving self-improvement. 3.2. Plateaus in Varying Sampling Numbers First, to reveal the impact of sampling number ùêæ, we conduct experiments over five self-improvement iterations with varying value of ùêæ. Figure 2a uncovers two key observations: (1) Different ùêæ exhibits similar performance trends during self-improvement: notable improvements in early iterations, rapid convergence to performance bottlenecks, and even declining trends (e.g., ùêæ = 16) in later iterations; (2) Sampling number plays dominant role in the first iteration, where higher sampling number leads to better performance; while in later iterations, sampling number has virtually no impact on performance. For instance, at iteration 1, the ùêæ = 16 model outperforms the ùêæ = 4 model by 3.89 points on the test set. While at iteration 5, the ùêæ = 16 model even performs 0.39 points worse than the ùêæ = 4 model. 3.3. Imbalance in Self-improvement To figure out the bottlenecks of self-improvement, we then analyze the distribution of self-generated data (Ding et al., 2025; Tong et al., 2024). Differences between original and self-generated data. To characterize the properties of selfgenerated data, we first compare it with the original data from two dimensions: difficulty and length. 4 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing (a) Data with different accuracy. (b) Length with different ùêæ. (c) Length with different level. Figure 3 Matthew effect over iterations. (a) Matthew effect in the distribution of data in Dfilter with different accuracy under the setting of ùêæ = 4. (b) Trends in average response length (i.e., number of tokens) across iterations under different sampling numbers ùêæ. (c) Matthew effect in average response length for data of different difficulty levels across iterations under the setting of ùêæ = 8, where difficulty increases progressively from level 1 to level 5. Imbalanced difficulty distribution. As shown in Figure 2b, the difficulty distribution (see Appendix for difficulty level categorization) differs markedly between the original and self-generated data, revealing dominant head and narrow tail pattern. While original data exhibits balanced difficulty distribution, self-generated data suffers from severe imbalance, with easy samples (level 1) comprising 51.1% of the total and difficult samples (level 5) nearly absent. Shorter average response length. As revealed in Figure 2c, self-generated responses are significantly shorter than original ones (averaging 277 tokens vs. 395 tokens), with some extremely short responses (<10 tokens) lacking CoT reasoning. This suggests that self-generated data is prone to producing abbreviated reasoning processes, including instances that deliver conclusions directly even under CoT prompting. Matthew effect over iterations. Further, we investigate distribution shifts during the iterative process. To begin with, we analyze how queries with different accuracy are distributed in self-generated data. Results in Figure 3a indicate that throughout iterations, well-mastered data (with 100% accuracy) occupies an increasing proportion, while poorly-performed data (with 25% accuracy) is gradually squeezed out of the training dataset Dtraina phenomenon we term the Matthew effect. Given that high-accuracy queries account for more samples, the diminishing proportion of difficult tail becomes even more severe, limiting the self-improvement performance ceilings. Next, we analyze response length changes during the iterative process. As shown in Figure 3b, average response length consistently declines across iterations, while higher sampling number mitigates this degradation. To delve deeper, we compare average response length across various difficulty levels over iterations. Figure 3c reveals three key observations: (1) Simple data (level 1) maintains relatively stable response length during iterations, showing minimal degradation. (2) Difficult data (level 5) suffers the most severe length degradation, with dramatic reduction of 56.5%. (3) At iteration 1, the higher the difficulty level, the longer the response length; however, this trend completely reverses by the fifth iteration, with difficult data generating the shortest responses of merely 136 tokens. These findings suggest that Matthew effect manifests in response length as wellsimple data requiring shorter responses maintains appropriate length, whereas difficult data requiring longer responses undergoes significant degradation. Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing 4. Methodology To alleviate the phenomenon of dominant head and narrow tail, we propose four effective re-balanced strategies from two perspectives: distribution-reshaping and trajectory-resampling. Regarding distribution-reshaping, motivated by under-sampling and over-sampling in machine learning (Mohammed et al., 2020), we propose two intuitive strategies. On one hand, we reduce the quantity of head to increase the proportion of tail samples, introducing the Threshold Clipping (TC) strategy. On the other hand, we directly augment the number of tail through repetition, proposing the Repeated-based Padding (RP) strategy. From trajectory-resampling perspective, we also propose two strategies. The first is Adaptiveweighted Resampling (AR), which dynamically adjusts resampling weights based on fail rate. The second is Guided Resampling (GR), which initializes model exploration from varying intermediate reasoning steps (Ding et al., 2025; Xi et al., 2024). 4.1. Distribution Reshaping Threshold clipping. Adopting the philosophy of less is more, we propose threshold clipping, which increases the proportion of tail by reducing the number of head. Specifically, threshold clipping sets threshold ùêø and randomly truncates responses to limit each query to at most ùêø correct responses. Formally, at iteration ùë°, instead of using the filtered dataset directly, we train the base model Mbase on the following dataset: (ùë°) train-TC = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ,ùëò ) (ùë°) filter ùëò ùêø}. Repeat-based padding. Moreover, we consider increasing the quantity of tail directly and introduce repeat-based padding to enforce balanced data distribution. Specifically, repeat-based padding ensures that all queries appear with equal frequency ùêæ in the next training dataset. To achieve this, queries with insufficient correct samples are padded through repetition. Formally, at iteration ùë°, the training dataset is expressed as follows: ùëñ,ùëò mod ùëòùëñ where ùëòùëñ is the number of ÀÜùëüùëñ, that is, the number of correct responses out of ùêæ sampling for ùëûùëñ. (ùë°) train-RP = {(ùëûùëñ, ÀÜùëü (ùë°) ) Dfilter 1 ùëò ùêæ}, 4.2. Trajectory Resampling Adaptive-weighted resampling. However, merely reshaping data distribution yields limited benefits. As excessive duplication may reduce data diversity and trigger overfitting, we additionally employ resampling strategies to enhance the proportion of tail. Drawing inspiration from the pass rate (Team et al., 2025) and fail rate (Tong et al., 2024) metrics, we propose adaptive-weighted resampling, which dynamically adjusts the resampling weights for each query based on its fail rate. Specifically, for query with ùëòùëñ successful trajectories out of ùêæ samples, we perform ùêæ ùëòùëñ additional resampling operations. This hierarchical resampling assigns more weight to tail data, thereby increasing their proportion. Formally, at iteration ùë°, model Mùë°1 generates ùêæ ùëòùëñ new responses {Àáùëü (ùë°) ùëñ,ùëò Mùë°1(ùëûùëñ). Then we obtain the resampled and refiltered datasets: for each query ùëûùëñ Dbase, where Àáùëü (ùë°) ùëñ,ùëò }ùêæùëòùëñ ùëò=1 (ùë°) resample-AR = {(ùëûùëñ, Àáùëü (ùë°) ùëñ,ùëò ) 1 ùëñ ùëÅ, 1 ùëò ùêæ ùëòùëñ} 6 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing refilter-AR = {(ùëûùëñ, Àáùëü (ùë°) and finally merge into the training dataset: (ùë°) ùëñ,ùëò ) (ùë°) resample-AR ùëü ùëì (ùëûùëñ, ùëéùëñ, Àáùëé(ùë°) ùëñ,ùëò ) = 1}, (ùë°) train-AR = (ùë°) filter (ùë°) refilter-AR . Guided resampling. Nevertheless, adaptive-weighted resampling is essentially brute-force sampling with limited efficiency improvements. To this end, we propose guided resamplinga novel resampling strategy that initializes model exploration from various intermediate reasoning steps. Specifically, the model exploits guided signals to achieve efficient resampling, starting its reasoning process from different intermediate reasoning steps. This strategy enables the model to navigate toward promising trajectories within vast exploration space, while facilitating progressive learning of complex reasoning. Formally, for each successful trajectory ÀÜùëüùëñ (ùë°) , we decompose it into ùëÜ filter steps ÀÜùëüùëñ(1) , , ÀÜùëüùëñ(ùëÜ) . Generated by Àáùëüùëñ(ùë†) Mùë°1(ùëûùëñ, ÀÜùëüùëñ(<ùë†) ), the resample dataset is expressed as: (ùë°) resample-GR = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ(<ùë†),ùëò , Àáùëü (ùë°) ùëñ(ùë†),ùëò) ùëòùëñ < ùêø, 1 ùë† ùëÜ}. Similar to adaptive-weighted resampling, we obtain the refiltered and training dataset as (ùë°) and (ùë°) . refilter-GR train-GR 5. Experiments 5.1. Experimental Setups Datasets. We adopt MMPR (Wang et al., 2024b)a multimodal reasoning dataset derived from multiple sourcesas our primary dataset. From it, we randomly extract 7,980 mathematical reasoning samples (Cao and Xiao, 2022; Lu et al., 2021; Seo et al., 2015) to construct curated subset, MMPRmini, with details presented in Appendix A. For out-of-domain (OOD) evaluation, we further utilize MathVerse (Zhang et al., 2024) and We-Math (Qiao et al., 2024) datasets. Models. We employ two widely-used LVLMs as our base models: Qwen2-VL-7B-Instruct (Wang et al., 2024a) and InternVL2.5-4B (Zhu et al., 2025). All analytical experiments in Section 3 are conducted on Qwen2-VL-7B-Instruct. Following the paradigm of Zelikman et al. (2024), we initialize the from the base model instead of further SFT model, and restart training from this base model at each iteration. Implementation details. All experiments are conducted on eight A100-80GB GPUs using SWIFT (Zhao et al., 2025) framework for training and vLLM (Kwon et al., 2023) framework for sampling and testing. We set the number of self-improvement iterations ùëá = 5 and focus on the sampling number ùêæ = 8 and ùêæ = 16. For training, we use learning rate of 3 105 and train for 1 epoch to avoid overfitting. For sampling, we set the temperature to 0.7 with maximum 4096 new tokens. While for testing, the temperature is set to 0. Additionally, we configure method-specific parameters, including threshold ùêø = 4 for TC and number of steps ùëÜ = 4 for AR. 5.2. Main Results The main results are presented in Table 1, including the final and optimal performance. Overall, our findings are as follows: 7 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing"
        },
        {
            "title": "Models",
            "content": "Sampling Num. Method In-domain Out-of-domain Avg. MMPR-mini opt. final MathVerse opt. final We-Math final opt. final opt. w/o SI 23.71 22.46 47.36 31.18 Qwen2-VL-7B InternVL2.5-4B ùêæ = 8 ùêæ ="
        },
        {
            "title": "Vanilla SI\nTC\nRP\nAR\nGR\nVanilla SI\nTC\nRP\nAR\nGR",
            "content": "43.04 44.67 28.93 29.06 41.31 40.98 52.93 52.36 27.79 42.79 52.76 41.78 41.78 44.67 52.13 28.55 52.59 42.78 42.78 45.67 46.42 30.08 30.08 52.59 42.47 42.47 52.82 28.93 52.82 28.93 45.67 45.67 42.33 28.30 53.91 53.91 42.33 27.28 45.80 45.80 41.36 41.36 52.76 28.30 28.30 43.66 43.04 42.43 42.43 28.68 45.92 45.92 52.70 28.68 42.28 28.30 30.33 51.49 47.05 47.05 42.56 41.96 41.96 53.91 28.93 28.55 43.41 43.41 47.68 47.68 29.95 29.95 54.20 54.20 43.94 43.94 52.76 52.76 52.70 53.91 w/o SI 47. 29.31 48.10 41.66 ùêæ = 8 ùêæ ="
        },
        {
            "title": "Vanilla SI\nTC\nRP\nAR\nGR\nVanilla SI\nTC\nRP\nAR\nGR",
            "content": "48.32 49.36 49.99 49.11 52.59 53.28 52.87 52.41 29.95 33.12 50.40 32.36 33.12 51.72 50.86 31.98 50.75 31.85 48.80 64.62 64.62 49.77 63.99 63.99 50.54 67.13 68.13 31.98 66.25 49.11 30.33 66.25 67.50 67.50 33.12 33.12 51.90 54.20 50.84 50.84 50.57 66.75 64.74 48.99 71.64 71.64 32.61 34.14 51.32 53.33 51.86 51.86 49.46 64.99 67.63 50.03 33.25 51.67 51.78 52.70 51.38 31.98 33.63 33. 64.99 67.63 50.46 48.74 49.60 47.94 49.08 50.03 30.58 31.09 31.60 30. 52.30 50.57 66.75 64.74 Table 1 Main results. The best result for each setting is in bold, while the second-best is marked with underline. Num. refers to number. SI, TC, RP, AR, and GR denote self-improvement, threshold clipping, repeat-based padding, adaptive-weighted resampling, and guided resampling, respectively. Final indicates the final performance, and opt. indicates the optimal performance across all iterations. Scaling the sampling number shows poor cost-efficiency in self-improvement. While vanilla self-improvement yields substantial gains over the base modelfor instance, improving the average performance of Qwen2-VL-7B-Instruct by 10.13 points at sampling number of ùêæ = 8further increasing the sampling number proves ineffective. Compared to ùêæ = 8, the optimal average performance at ùêæ = 16 improves by merely 0.05 points. Given the doubled computational cost, such marginal gain is not cost-effective. These findings indicate that blindly scaling the sampling number through brute-force methods fails to provide critical breakthrough in self-improvement. Head-tail re-balancing improves the performance of self-improvement across various models and datasets. For more efficient enhancement, re-balancing the distribution of head and tail data throughout self-improvement achieves significant performance improvements across varying models and datasets, particularly with RP and GR strategies. For example, with Qwen2-VL-7B-Instruct model at ùêæ = 16, RP outperforms vanilla self-improvement by 3.39 points on the in-domain test set and 1.20 points on average, while GR achieves improvements of 4.02 and 2.58 points, respectively. Moreover, comparison between final and optimal performance shows that vanilla self-improvement frequently exhibits suboptimal results in the final iteration relative to its peak performance, reflecting performance bottlenecks during training. In contrast, our re-balancing strategies, especially GR, consistently reach optimal performance in the final iteration, demonstrating greater stability and potential for further improvement. 8 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Head-tail re-balancing mitigates the Matthew effect in self-improvement. As shown in Figure 4, our proposed strategies effectively alleviate the imbalanced difficulty distribution in successful trajectories. Among them, distribution-reshaping methods exhibit superior mitigation on Qwen2VL-7B-Instruct at ùêæ = 16, with RP reducing head data proportion from 51.1% to 24.8% and boosting tail data from 1.5% In contrast, AR shows limited to 6.6%. tail data augmentation due to inefficient sampling in vast solution spaces, leading to modest performance gains. Conversely, GR focuses more on enhancing tail sample coverage and proportion. Despite slightly less mitigation than TC and RP, GR provides step-by-step guidance for tail samples, enables better mastery of complex reasoning trajectories, and yields substantial performance improvements from 41.36 to 43.94. Additional results of Matthew effect mitigation on InternVL2.5-4B and other sampling settings are provided in Appendix D. Figure 4 Data distribution of difficulty levels (1=easiest, 5=most difficult) in successful trajectories under different strategies with Qwen2-VL-7B-Instruct at ùêæ = 16. 6. Discussion 6.1. Ablation Study Avg. MMPR MathVerse We-Math Variants of reshaping strategies. We evaluate various re-balancing variants, with results presented in Table 2 (upper). First, experiments with TC reveal trade-off in the choice of ùêø: large ùêø inadequately alleviates data imbalance, whereas small ùêø fails to ensure sufficient diversity. We also explore head clipping (HC), which removes fully-correct queries (i.e., ùëòùëñ = ùêæ). Despite promising generalization capability, HC suffers from poor in-domain performance. Additionally, we test repeat-based inverting (RI), method that retains ùêæ ùëòùëñ samples for each query ùëûùëñ Dfilter and supplements the deficit through repetition. Compared to RP, RI yields marginally lower performance, emphasizing the importance of data diversity. Table 2 Final performance of re-balancing variants on Qwen2-VL-7B-Instruct with ùêæ = 16. Method TC (ùêø = 2) TC (ùë≥ = 4) TC (ùêø = 8) HC RI RP GR (ùëÜ = 2) GR (ùë∫ = 4) GR (ùëÜ = 8) 41.56 42.43 41.86 41.27 41.71 42.28 42.31 43.94 41.80 53.10 52.70 52.47 53.10 51.26 51.49 27.66 28.68 29.44 28.93 28.81 28.30 43.91 45.92 43.66 41.78 45.04 47.05 45.80 47.68 44.67 27.79 29.95 26. 53.33 54.20 54.08 Variants of resampling strategies. Table 2 (lower) illustrates the performance of GR under different values of intermediate reasoning steps ùëÜ. Too small ùëÜ (ùëÜ = 2) results in inadequate guidance effectiveness, while too large ùëÜ not only increases computational cost but also limits diversity, hindering further performance gains. This highlights the importance of selecting an appropriate value for ùëÜ. Furthermore, we compare our resampling strategies against brute-force resampling. As shown in Table 1, AR and GR with ùêæ = 8 achieve comparable or even superior performance to brute-force 9 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing resampling (i.e., vanilla SI with ùêæ = 16). Regarding efficiency, brute-force resampling requires ùêæ resampling, while AR reduces this to ùêæ ùëòùëñ operations ( 50% cost reduction), and GR requires only 1 resampling on minimal tail data. Focusing on data distribution, our strategies mitigate the Matthew effect in self-improvement, delivering superior performance with enhanced efficiency. 6.2. Self-improvement as Efficient Sampling Avg. 59.10 52.18 33."
        },
        {
            "title": "Method",
            "content": "batch sampling MMPR MathVerse We-Math iterative sampling + TC + RP Driven by data distribution shifts during self-improvement, we hypothesize that inter-iteration sampling exhibits greater variance than intra-iteration sampling. To leverage this, we introduce iterative sampling, which combines ùêæ = 8 samples over 5 iterations (40 samples total) to enhance data diversity. For comparison, we also implement batch sampling, which draws all 40 samples at once. Results in Table 3 demonstrate that iterative sampling outperforms batch sampling, supporting our view of self-improvement as an efficient sampling method. Furthermore, applying distribution-reshaping strategies to iterative sampling validates their effectiveness. Notably, RP outperforms both batch sampling and vanilla iterative sampling, with improvements of 3.34 and 1.79 points respectively. Table 3 Performance comparison between batch sampling and iterative sampling on InternVL2.5-4B model. 31.47 32.23 33. 49.68 50.65 51.47 64.99 66.62 69.89 52.59 53.10 50.63 48.13 6.3. Self-correction Benefits Self-improvement Self-correction has emerged as promising learning paradigm (Cheng et al., 2025; Ding and Zhang, 2025; Wu et al., 2025a), encouraging deeper reasoning and generating longer chains of thought. Therefore, we investigate its effectiveness for head-tail re-balancing in visual self-improvement (see Appendix for implementation details). In contrast to the resampling strategies discussed earlier, self-correction refines existing incorrect samples through ùêæ ùëòùëñ operations per query, achieving efficiency comparable to AR and fully leveraging the potential of incorrect instances. Results in Figure 5 demonstrate that self-correction effectively counteracts the Matthew effect, yielding substantial performance improvements. Figure 5 Average performance comparison between vanilla and self-correction in visual self-improvement. Notably, in addition to verifying final results, we also filter out data without CoT reasoning process. The results in the table 4 indicate that CoT length filtering improved the quality of tail data, demonstrating superior overall performance."
        },
        {
            "title": "Method",
            "content": "MMPR MathVerse We-Math Avg. w/o CoT filtering w/ CoT filtering 44.04 46.67 29.44 30.96 55.11 54. 42.87 44.19 Table 4 Performance comparison between with and without CoT filtering on Qwen2-VL-7B-Instruct model. 10 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing 6.4. The Power of Seeing To validate whether the final model truly enhances reasoning capabilities, we evaluate its performance on tail data under two settings: with and without image inputs. Figure 6 indicates that the base model exhibits negligible performance difference between these settings, indicating poor capability to leverage visual information for solving challenging problems. In contrast, vanilla self-improvement leads to substantial gain in real reasoning performance. Our re-balanced strategies further amplify this increment, with RP strategy demonstrating an 18.8-point advantage when seeing images. Figure 6 Comparison of tail data performance with and without images on Qwen2-VL-7B-Instruct at ùêæ = 8. Additionally, for error type analysis and case study, please refer to Appendix and Appendix C. 7. Conclusion In this work, we identify critical challenge behind performance bottlenecks in visual self-improvement: the Matthew effect, where simple samples in the head progressively dominate successful trajectories, while difficult data in the tail becomes increasingly narrowing. To counteract it, we introduce four effective re-balanced strategies from distribution-reshaping and trajectory-resampling perspectives: threshold clipping, repeat-based padding, adaptive-weighted resampling, and guided resampling. Experimental results demonstrate that these strategies successfully reduce head dominance and increase tail proportion, thereby improving the performance ceilings. Future work will explore counteracting the Matthew effect on larger models and broader datasets, alongside developing more efficient re-balancing strategies."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. URL https://doi.org/10. 48550/arXiv.2502.13923. Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 15111520. International Committee on Computational Linguistics, 2022. URL https://aclanthology.org/2022.coling-1.130. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Vision-language 11 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing models can self-improve reasoning via reflection. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 88768892. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LONG.447. URL https://doi.org/10.18653/v1/2025.naacl-long.447. Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Y. Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/ed45d6a03de84cc650cae0655f699356-Abstract-Conference.html. Yi Ding and Ruqi Zhang. Sherlock: Self-correcting reasoning in vision-language models. CoRR, doi: 10.48550/ARXIV.2505.22651. URL https://doi.org/10. abs/2505.22651, 2025. 48550/arXiv.2505.22651. Yiwen Ding, Zhiheng Xi, Wei He, Lizhuoyuan Lizhuoyuan, Yitao Zhai, Shi Xiaowei, Xunliang Cai, Tao Gui, Qi Zhang, and Xuanjing Huang. Mitigating tail narrowing in LLM self-improvement via socratic-guided sampling. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 1062710646. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LONG.533. URL https://doi.org/10. 18653/v1/2025.naacl-long.533. Elvis Dohmatob, Yunzhen Feng, Pu Yang, Fran√ßois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=KVvku47shW. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 10511068. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.67. URL https://doi.org/10.18653/v1/ 2023.emnlp-main.67. Chunyang Jiang, Chi-Min Chan, Wei Xue, Qifeng Liu, and Yike Guo. Importance weighting can help large language models self-improve. In Toby Walsh, Julie Shah, and Zico Kolter, editors, AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2425724265. AAAI Press, 2025. doi: 10.1609/AAAI.V39I23. 34602. URL https://doi.org/10.1609/aaai.v39i23.34602. Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, and Wei Ye. Rethinking the sampling criteria in reinforcement learning for LLM reasoning: competencedifficulty alignment perspective. CoRR, abs/2505.17652, 2025. doi: 10.48550/ARXIV.2505. 17652. URL https://doi.org/10.48550/arXiv.2505.17652. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, 12 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Cosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=CjwERcAU7w. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611626. ACM, 2023. doi: 10.1145/ 3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 67746786. Association for Computational Linguistics, 2021. doi: 10.18653/ V1/2021.ACL-LONG.528. URL https://doi.org/10.18653/v1/2021.acl-long.528. Robert Merton. The matthew effect in science: The reward and communication systems of science are considered. Science, 159(3810):5663, 1968. Roweida Mohammed, Jumanah Rawashdeh, and Malak Abdullah. Machine learning with oversampling and undersampling techniques: Overview study and experimental results. 2020. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. Yi Peng, Chris, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, Rongxian Zhuang, Xuchen Song, Yang Liu, and Yahui Zhou. Skywork R1V: pioneering multimodal reasoning with chain-of-thought. CoRR, abs/2504.05599, 2025. doi: 10.48550/ARXIV.2504.05599. URL https://doi.org/10.48550/arXiv.2504.05599. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. doi: 10.48550/ ARXIV.2407.01284. URL https://doi.org/10.48550/arXiv.2407.01284. Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Llu√≠s M√†rquez, Chris CallisonBurch, Jian Su, Daniele Pighin, and Yuval Marton, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 14661476. The Association for Computational Linguistics, 2015. doi: 10.18653/V1/D15-1171. URL https://doi.org/10.18653/v1/d15-1171. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross J. Anderson, and Yarin Gal. AI models collapse when trained on recursively generated data. Nat., 631(8022):755759, 2024. doi: 10.1038/S41586-024-07566-Y. URL https://doi.org/10.1038/s41586-024-07566-y. Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, and Wentao Zhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification. 13 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing CoRR, abs/2502.13383, 2025. doi: 10.48550/ARXIV.2502.13383. URL https://doi.org/10. 48550/arXiv.2502.13383. Naoto Tanji and Toshihiko Yamasaki. Iterative self-improvement of vision language models for image scoring and self-explanation. CoRR, abs/2506.02708, 2025. doi: 10.48550/ARXIV.2506.02708. URL https://doi.org/10.48550/arXiv.2506.02708. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. doi: 10.48550/ARXIV.2501. 12599. URL https://doi.org/10.48550/arXiv.2501.12599. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficultyaware rejection tuning for mathematical problem-solving. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 0ef1afa0daa888d695dcd5e9513bafa3-Abstract-Conference.html. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024a. doi: 10.48550/ARXIV. 2409.12191. URL https://doi.org/10.48550/arXiv.2409.12191. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. CoRR, abs/2411.10442, 2024b. doi: 10. 48550/ARXIV.2411.10442. URL https://doi.org/10.48550/arXiv.2411.10442. Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Taha A. Kass-Hout, Furong Huang, and Cao Xiao. Enhancing visual-language modality alignment in large vision language models via self-improvement. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 268282. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.FINDINGS-NAACL.15. URL https://doi.org/10.18653/v1/2025.findings-naacl.15. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1348413508. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.754. URL https://doi.org/10.18653/v1/2023. acl-long.754. Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, and Nanyun Peng. Visco: Benchmarking fine-grained critique and correction towards self-improvement in visual reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 95279537, June 2025a. Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, and Michael Qizhe Shieh. Synthrl: Scaling visual reasoning with verifiable data synthesis. CoRR, abs/2506.02096, 2025b. doi: 10.48550/ ARXIV.2506.02096. URL https://doi.org/10.48550/arXiv.2506.02096. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing Huang. Training large language models for reasoning through reverse curriculum reinforcement learning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=t82Y3fmRtk. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 1361813628, June 2025. Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, and Kam-Fai Wong. DAST: difficulty-aware self-training on large language models. CoRR, abs/2503.09029, 2025. doi: 10.48550/ARXIV.2503.09029. URL https://doi. org/10.48550/arXiv.2503.09029. Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and Wang. generation. arXiv preprint arXiv:2310.08541, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Self-taught reasoner bootstrapping reasoning with reasoning. In Proc. the 36th International Conference on Neural Information Processing Systems, volume 1126, 2024. Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, and Dongzhan Zhou. Critic-v: Vlm critics help catch vlm errors in multimodal reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 90509061, June 2025a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, and Hongsheng Li. MATHVERSE: does your multimodal LLM truly see the diagrams in visual math problems? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and G√ºl Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, volume 15066 of Lecture Notes in Computer Science, pages 169186. Springer, 2024. doi: 10. 1007/978-3-031-73242-3_10. URL https://doi.org/10.1007/978-3-031-73242-3_10. 15 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Shanghang Zhang, Peng Gao, and Hongsheng Li. MAVIS: mathematical visual instruction tuning with an automatic data engine. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025b. URL https://openreview.net/forum?id=MnJzJ2gvuf. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. SWIFT: scalable lightweight infrastructure for fine-tuning. In Toby Walsh, Julie Shah, and Zico Kolter, editors, AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2973329735. AAAI Press, 2025. doi: 10.1609/AAAI.V39I28.35383. URL https://doi.org/10.1609/aaai.v39i28.35383. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. CoRR, abs/2504.10479, 2025. doi: 10.48550/ARXIV.2504.10479. URL https://doi.org/10.48550/arXiv.2504.10479. Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing"
        },
        {
            "title": "Appendix",
            "content": "A. Experimental Details A.1. Dataset Details We adopt MMPR (Wang et al., 2024b) as our primary dataset. From it, we randomly extract 7,980 mathematical reasoning samples (Cao and Xiao, 2022; Lu et al., 2021; Seo et al., 2015) to construct MMPR-mini, with 7,183 for training and 797 for in-domain testing. Specifically, we select queries from Geometry3K (Lu et al., 2021), GeoQA+ (Cao and Xiao, 2022), and GEOS (Seo et al., 2015) respectively, randomly sampling 10% of data from each dataset to construct the test set. MMPR-mini primarily focuses on mathematical visual reasoning problems, including multiple-choice questions, open-ended questions, and other formats, containing only queries with their corresponding ground truth without CoT reasoning trajectories. Additionally, for out-of-domain (OOD) evaluation, we select two widely-adopted mathematical visual reasoning datasets: MathVerse (Zhang et al., 2024) and We-Math (Qiao et al., 2024), comprising 788 and 1,740 queries respectively. All datasets we utilized are open-source. A.2. Difficulty Level Categorization Using Qwen2-VL-7B-Instruct, we perform 64-shot sampling on each query, and categorize the queries into 5 difficulty levels based on their pass@64 performance. This classification prioritizes balanced data distribution across all levels, with data difficulty ascending from level 1 to level 5. A.3. Prompt Details In this work, we use the unified prompt template (see Figure 7) in the phase of training, sampling and testing across varying datasets. Figure 7 Prompt for training, sampling and testing. B. Error Type Analysis First, we analyze the incorrect data and categorize it into the following error types: No Reasoning Process (NP): Models provide answers directly without demonstrating any stepby-step reasoning or explanation process. Comprehension Error (CE): Models exhibit misunderstanding of either the question content or the visual information presented in the image. 17 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Knowledge Error (KE): Models employ incorrect formulas, theorems, or other factual information. Logic Error (LE): Models generate flawed reasoning steps, perform incorrect calculations, or establish faulty cause-and-effect relationships. Format Error (FE): Models produce responses whose format does not meet the specified requirements. Then we utilize GPT-4.1 (OpenAI, 2023) for extensive error type classification, setting temperature=0.1 and permitting up to 3 outputs. The prompt template is illustrated in Figure 8, with classification outcomes detailed in Table 5. Figure 8 Prompt for determining error type. The results reveal several key findings: (1) The overall distribution of error types varies significantly across different models. For instance, with the Qwen2-VL-7B-Instruct model, the number of samples directly providing final answers increases substantially, particularly under the TC strategy, which reduces the quantity of data and further amplifies the proportion of direct-answer samples, making No Reasoning Process the most frequent error type in TCs incorrect data. In contrast, the InternVL2.5-4B model exhibits few \"No Reasoning Process\" errors. (2) Even without self-improvement, format errors occur with extremely low frequency, indicating that the selfimprovement process truly enhances models reasoning capabilities rather than simple instructionfollowing abilities for format. (3) Self-improvement achieves significant improvements mainly in comprehension errors and logic errors, substantially reducing the occurrence of both error types. (4) Compared to vanilla self-improvement, our re-balanced strategies enhance models capabilities 18 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing"
        },
        {
            "title": "Method",
            "content": "NP CE KE LE FE Qwen2-VL InternVL2.5 w/o SI Vanilla SI TC RP AR GR w/o SI Vanilla SI TC RP AR GR 286 4 36 189 182 137 152 34 77 185 153 128 0 1 0 14 7 0 227 132 150 123 137 24 21 16 10 13 8 13 12 12 9 12 14 286 216 91 222 170 124 176 135 123 116 113 117 2 1 0 0 2 2 0 2 0 0 0 Table 5 Error type analysis. NP, CE, KE, LE and FE denote No Reasoning Process, Comprehension Error, Knowledge Error, Logic Error, and Format Error, respectively. SI, TC, RP, AR, and GR denote self-improvement, threshold clipping, repeat-based padding, adaptive-weighted resampling, and guided resampling, respectively. in question and image comprehension as well as reasoning logic. In rare instances, the model consistently returns T; these cases are omitted from the table. Manual examination of these cases reveals that the responses are accurate but were misclassified as incorrect due to our exact-match rules, with an illustrative example provided in Figure 9. C. Case Study Our main results show that repeat-based padding and guided resampling achieve superior performance among our proposed strategies. Therefore, we showcase examples of these two strategies in Figure 10 and 11 respectively. Both cases reveal that vanilla self-improvement exhibits limited capabilities in visual comprehension and geometric element understanding. For instance, in Figure 10, vanilla self-improvement incorrectly determines the relationship between AOC and BDC, while in Figure 11 demonstrates the confusion of height ‚Ñé with radius ùëü, resulting in final errors. Both repeat-based padding and guided resampling successfully address these problems. D. Matthew Effect Mitigation In Section 5, we analyzed the effectiveness of our proposed rebalanced strategies in mitigating the Matthew effect and their performance on visual reasoning tasks using Qwen2-VL-7B-Instruct at ùêæ = 16. Here, we present three additional configurationsQwen2-VL-7B-Instruct at ùêæ = 8, InternVL2.54B at ùêæ = 8, and InternVL2.5-4B at ùêæ = 16in Figure 12, Figure 13 and Figure 14. Results demonstrate that our proposed re-balancing strategies effectively mitigate the Matthew effect in self-improvement across different experimental settings. Overall, the repeat-based padding (RP) method exhibits the best mitigation performance. Among the two trajectory-resampling strategies, guided resampling (GR) generally outperforms adaptive-weighted resampling (AR), which aligns with the performance patterns of these strategies on the test sets. 19 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 9 case misclassified as incorrect due to exact-match rules, where pi and ùúã appear in different formats. E. Batch Sampling and Iterative Sampling In Section 6, we used InternVL2.5-4B at ùêæ = 8 as an example to discuss that self-improvement can be viewed as an efficient sampling approach. Additionally, we conducted similar experiments on Qwen2-VL-7B-Instruct at ùêæ = 16 and observed similar phenomena. Specifically, we combine ùêæ = 16 samples across 5 iterations (totaling 80 samples) as iterative sampling, while batch sampling involves directly sampling 80 times from the base model. As shown in Table 6, iterative sampling delivers improvements of 5.65 points on the in-domain test set and 3.17 points on average. Nevertheless, our distribution-reshaping strategies do not consistently perform well under the iterative sampling perspective. This might be because for ùêæ = 16, TC with ùêø = 4 significantly reduces the data quantity, while RP leads to excessive repetition, both of which may hinder advantages in efficient sampling scenarios. 20 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 10 An example of repeat-based padding with Qwen2-VL-7B-Instruct model on MMPR-mini test set. Compared to vanilla self-improvement, repeat-based padding successfully finds the relationship between AOC and ABC, reaching the right answer."
        },
        {
            "title": "Method",
            "content": "MMPR MathVerse We-Math Avg. batch sampling 38.52 iterative sampling + TC + RP 44.17 41.78 42. 26.52 28.68 27.41 26.52 49.37 38.14 51.15 52.24 48.45 41.33 40.48 39. Table 6 Performance comparison between batch sampling and iterative sampling on Qwen2-VL7B-Instruct model. F. Applying Self-correction to Self-improvement Given self-correction capabilities shown in Figure 15, we explore applying self-correction to selfimprovement for head-tail re-balancing, incorporating the refined outputs into subsequent training iterations. Formally, we first define the set of incorrect samples as: (ùë°) discard = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ,ùëò ) (ùë°) sample ùëü ùëì (ùëûùëñ, ùëéùëñ, ÀÜùëé(ùë°) ùëñ,ùëò ) = 0}. For ÀÜùëüùëñ (ùë°) discard , we design the self-correction prompt ùëù to resample ùëüùëñ Mùë°1(ùëûùëñ, ÀÜùëüùëñ, ùëù) and form 21 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing the resample dataset: (ùë°) self-correction = {(ùëûùëñ, ÀÜùëü (ùë°) ùëñ,ùëò , ùëù, ùëü (ùë°) ùëñ,ùëò ), (ùëûùëñ, ùëü (ùë°) ùëñ,ùëò ) ùëòùëñ < ùêæ}, where we filter out samples with failed corrections and insufficient CoT reasoning length. Finally, we obtain the training dataset as following: (ùë°) train-BP = (ùë°) train (ùë°) self-correction . The prompt ùëù we used (Kumar et al., 2025) is presented in Figure 16. Results in Section 6.3 demonstrate that applying self-correction to self-improvement yields substantial performance gains. We provide an illustrative example in Figure 17, where iterative selfcorrection enables the model to engage in deeper reasoning through generating longer thinking chains, ultimately correcting errors in vanilla self-improvement. Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 11 An example of guided resampling with InternVL2.5-4B model on MathVerse test set. In this case, vanilla self-improvement demonstrates the confusion of height ‚Ñé with radius ùëü, while guided resampling addresses this problem successfully. 23 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 12 Data distribution of difficulty levels in successful trajectories under different strategies with Qwen2-VL-7B-Instruct at ùêæ = 8. Figure 13 Data distribution of difficulty levels in successful trajectories under different strategies with InternVL2.4-4B at ùêæ = 8. Figure 14 Data distribution of difficulty levels in successful trajectories under different strategies with InternVL2.4-4B at ùêæ = 16. 24 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 15 An example of self-correction capabilities of Qwen2-VL-7B-Instruct model. 25 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 16 Prompt for self-correction. 26 Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing Figure 17 An example of self-correction with InternVL2.5-4B model on MMPR-mini test set. In this case, the model of vanilla self-improvement made identification errors during tangent calculations, indicating deficient visual comprehension capabilities. In contrast, the self-correction method successfully addresses this problem and performs detailed computations to reach the correct answer."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Meituan",
        "Shanghai Innovation Institute"
    ]
}