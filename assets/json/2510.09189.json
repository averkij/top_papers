{
    "paper_title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
    "authors": [
        "Changjiang Gao",
        "Zixian Huang",
        "Jingyang Gong",
        "Shujian Huang",
        "Lei Li",
        "Fei Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available."
        },
        {
            "title": "Start",
            "content": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning Changjiang Gao*, Zixian Huang, Jingyang Gong, Shujian Huang, Lei Li, Fei Yuan National Key Laboratory for Novel Software Technology, Nanjing University The University of Hong Kong, Carnegie Mellon University, Shanghai Artificial Intelligent Laboratory gaocj@smail.nju.edu.cn, huangsj@nju.edu.cn, leili@cs.cmu.edu jygong@hku.hk, {huangzixian, yuanfei}@pjlab.org.cn 5 2 0 2 0 1 ] . [ 1 9 8 1 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both highand lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for wider range of languages. The code 1 and model 2 are publicly available."
        },
        {
            "title": "Introduction",
            "content": "however, Large Language Models (LLMs; OpenAI, 2025; Anthropic, 2025; Comanici et al., 2025; DeepSeekAI et al., 2025a; Kimi Team et al., 2025; ByteDance, 2025; Yang et al., 2025) excel in reasoning; translation-enhanced LLMs (Alves et al., 2024a; Rei et al., 2025; Sun et al., 2024; Lu et al., 2024) face challenges in this area. As shown in Figure 1, Tower-Plus-9B (Rei et al., 2025), translation-enhanced model, significantly improves multilingual instruction-following capabilities, yet underperforms on reasoning, such as LiveCodeBench-V5 (Jain et al., 2024) and AIME2025 (AIME, 2025). *Work done during internship at Shanghai Artificial Intelligence Laboratory 1https://github.com/CONE-MT/LLaMAX2.0 2https://huggingface.co/collections/LLaMAX/ llamax20-68ad1c154fcf2623b75a068c 1 Figure 1: Comparison of translation-enhanced models on reasoning tasks. Tower-Plus-9B and LLaMAX-3Alpaca struggle with LiveCodeBench-V5 (LCB_V5) and AIME2025, whereas Qwen3-XPlus-8B effectively addresses these challenges. The root of this dilemma lies in the limitations of the current training approach (Rei et al., 2025; Lu et al., 2024; Dou et al., 2025; Zheng et al., 2025), which typically begins with base model and then trains on large multilingual datasets. The preference for starting with base model instead of an instruct model stems from the fact that full fine-tuning (FFT) can result in catastrophic forgetting (Li et al., 2024a; Alexandrov et al., 2024). Unlike previous work, Qwen3-XPlus are built on Qwen3 instruct models rather than base models. Since fundamental reasoning skills like math and coding are universal, there is no need to learn basic concepts in multiple languages (Huang et al., 2025; Gao et al., 2025). Meanwhile, to mitigate catastrophic forgetting, we apply layer-selective tuning which effectively balances translation quality and reasoning capabilities without the need for extra parameters. It employs two-phase tuning process, training the four layers closest to the embedding layer and the fifteen layers further away, which consistently yields significant improvements across various datasets and model backbones. As result, Qwen3-XPlus significantly reduce the reliance on Figure 2: Average translation performance from English to 16 languages (enx). Unlike previous methods that train from base model, Qwen3-XPlus begins with an instruct model and, using limited parallel data, achieves significant improvements in translation. large amounts of high-quality data. Particularly, small amount of parallel data is enough for translation-enhancment. As depicted in Figure 2 3, Tower-Plus-9B has an impressive 32 billion tokens, LLaMAX (Lu et al., 2024) features 60 billion tokens, Sailor2-8B-Chat (Dou et al., 2025) needs 500 billion tokens, and Hunyuan-MT (Zheng et al., 2025) takes it even further with 1311 billion tokens. Not to mention, higher-quality data are needed for the supervised fine-tuning Chung et al., 2024; Ouyang et al., 2022), which poses significant challenge for low-resource languages. Howerver, Qwen3-XPlus utilize only 0.8 billion tokens from with our careful pre-processing. We standardize the original data from NLLB (NLLB Team et al., 2022; Schwenk et al., 2021) and OPUS-100 (Tiedemann, 2012) to format-unified, clean, deduplicated, language-consistent, qualitycontrolled, and instructed-formatted dataset. Qwen3-XPlus significantly improves translation performance, achieving over 15+ spBLEU points increase and 40+ xComet points in lowresource (sw), with notable enhancements in highresource translations. Utilizing small parallel data alone, it also demonstrates an average improvement of over 1 point across 7 multilingual tasks, including xIFEval (Huang et al., 2025), MGSM (Shi et al., 2022), XGPQA (Rein et al., 2024; Huang et al., 2025), and so on. Furthermore, comprehensive testing on 15 popular reasoning datasets, such as BBEH (Kazemi et al., 2025), Livecodebench (Jain et al., 2024), Olymmath (He et al., 2024) and so 3x includes Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese on, shows that it surpasses existing translationenhanced models and performs on par with Qwen3 instruct models. Our main contributions can be summarized as follows: We propose new training recipe: using small amount of parallel data for layer-selective tuning on an instruct model, which significantly reduces complexity and makes it more accessible for wider range of languages. We introduce 2 open-sourced translationenhanced Qwen3-XPlus-8B and Qwen3-XPlus14B, which maintaining reasoning capabilities. Extensive experiments on Qwen3-XPlus and comprehensive benchmark evaluations demonstrate that we can achieve balance between translation quality and reasoning capabilities."
        },
        {
            "title": "Large Language Models",
            "content": "Massively multilingual translation refers to building single machine translation model to handle many language directions (Zhang et al., 2020). Due to the multilingual and instruction-following nature of LLMs, they already show high translation performance in many directions without training (Bawden and Yvon, 2023; Zhu et al., 2024) or with minimal training (Li et al., 2024b; Cui et al., 2025; Huang et al., 2024). Based on this, specialized translation LLMs have been developed to do massive multilingual translation. For example, LLaMAX (Lu et al., 2024) and Tower (Alves et al., 2024b) apply continued pretraining and instruction tuning on the LLaMA-2 model (Touvron et al., 2 Figure 3: Overview of Qwen3-XPlus training recipe. After the data construction process, an instruct model is trained using layer-selective tuning strategy with instruction-format parallel data. 2023) with massive parallel and monolingual data, achieving comparable performance to specialized translation models. MT-R1-Zero (Feng et al., 2025) adapts the R1-Zero reinforcement learning framework to the translation task and resulting reasoning translation model. However, the models general instruction-following and reasoning capabilities drop after the training, which weakens the advantage of using LLMs for translation. This work finds simple, parameter-efficient but effective approach to train translation LLM while keeping (and even improving) its general ability."
        },
        {
            "title": "2.2 Parameter-Efficient Finetuning",
            "content": "There are many parameter-efficient finetuning (PEFT) techniques for tuning LLMs with less resource and reduced catastrophic forgetting. According to Han et al.s (2024) survey, there are mainly four types of PEFT: (1) Additive, including adapters (Houlsby et al., 2019; Pfeiffer et al., 2021; He et al., 2021) and soft prompts (Li and Liang, 2021; Liu et al., 2022); (2) Selective (Guo et al., 2021; Sung et al., 2021; Liao et al., 2023); (3) Reparameterized, mainly the LoRA family (Hu et al., 2021; Dettmers et al., 2023; Liu et al., 2024; Owodunni and Kumar, 2025); (4) Hybrid (He et al., 2021; Hu et al., 2023). Our proposed method belongs to selective PEFT. Figure 4: Translation performance of models that are single-layer tuned on parallel data. 2012), with 6 processing steps: 1. Transform the data into unified JSONL format, containing src, trg, src_line, tgt_line; 2. Clean invalid characters and punctuations to avoid encoding and tokenization issues; 3. Deduplicate the data in each language pair via SimHash (Manku et al., 2007) based on languagespecific tokenization and source-target length match. We first split the source and target texts into words or characters based on their languages, and then filter out the samples where either the source or the target text is too short (fewer than 2 tokens) or length mismatch (ones length is less than 0.3 of the others). Then, we concatenate the source and target of each sample, and calculate the SimHash conflicts between samples, deleting those with more than 2 conflicts; 4. Filter out the samples with incorrect language labels identified by fasttext 4; 5. Evaluate the translation quality of each sample with the conditional loss of small translation model (NLLB-200-Distilled-600M 5), ruling out the samples with higher loss than 90% of the FLORES-101 development samples. 6. Convert the data into instruction format by adding clear and diverse task instructions."
        },
        {
            "title": "3.2 Layer-Selective Tuning",
            "content": "In this section, we outline our new translation enhancement recipe (Figure 3). We train an instruct model with instruction-formatted parallel data ( 3.1) using layer-selective tuning ( 3.2)."
        },
        {
            "title": "3.1 Training Data Construction",
            "content": "Behavioral importance of the middle layers. To investigate how training different layers affects model behavior to guide the choice of target layers, we conduct experiments where each layer is independently trained. The results, illustrated in Figure 4, highlight distinct behaviors across layers, The parallel data used in our training process comes from two public accessible datasets, NLLB (NLLB Team et al., 2022) and OPUS-100 (Tiedemann, 4https://huggingface.co/facebook/ fasttext-language-identification 5https://huggingface.co/facebook/ nllb-200-distilled-600M 3 Algorithm 1: Qwen3-XPlus Two-Stage Training. Input: Epoch number L, learning rate η. Training data for Stage 1 and Stage 2 is both Dmulti, (cid:83) Den. Given an where Dmulti = Den instruct model, which pretrained parameter θ0 = {θie, θbottom_k, , θtop_m, θoe}, θie and θoe are the embedding parameters, and θbottom_k, , θtop_m are parameters of transformer layers.The parameters of Stage 1 are initialized as θ1 = θ0 with only θbottom_k being trainable, where θbottom_k = {θlayer_1, , θlayer_k}. Note, the parameters used for Stage 2 are initialized as θ2 = θ1, with only θtop_m being trainable, where θtop_m = {θlayer_nm, , θlayer_n}. // Stage 1, only θbottom_k being trainable. for epoch = 1 to do Shuffle Dmulti to obtain new training sequence. for each batch D1 Dmulti do l1 = (cid:80) θbottom_k θbottom_k η θbottom_k l1 logPθ1 (yx) x,yD end end // Stage 2, only θtop_m being trainable. for epoch = 1 to do Shuffle Dmulti to obtain new training sequence. for each batch D2 Dmulti do l2 = (cid:80) θtop_m θtop_m η θtop_m l2 logPθ2 (yx) x,yD end end parameters of the middle layers are frozen."
        },
        {
            "title": "4.1 Setting",
            "content": "Models and Baselines We compare Qwen3XPlus with range of baselines across four categories: (1) General instruction models, including the Gemma series (Gemma Team et al., 2024, 2025), Llama3 series (Touvron et al., 2023; Dubey et al., 2024), and Qwen series (Qwen et al., 2025; Yang et al., 2025); (2) Different tuning strategies on instruction models using our parallel data, including full fine-tuning and LoRA tuning. (3) Multilingual-enhanced models, including the Tower series (Alves et al., 2024c), Aya series (Dang et al., 2024; Üstün et al., 2024), Sailor2 (Dou et al., 2025), and LLaMAX3-8B-Alpaca (Lu et al., 2024); (4) Domain-specialized LLMs, such as the Qwen2.5-Math (Yang et al., 2024), and the Qwen2.5-Coder (Hui et al., 2024). Evaluation datasets and Metrics We evaluate Qwen3-XPlus on FLORES-101 (Goyal et al., 2022a) using the BenchMAX (Huang et al., 2025) 4 Figure 5: Layerwise nuclear norm of Qwen3-8B on the en-zh split of the Flores-101 dev dataset. About the 20th layers show the highest sensitivity in Q, and . revealing that training on intermediate layers, especially layer 20, results in significant decline in translation performance. This finding highlights the significance of middle layers (Skean et al., 2025, 2024), which have been demonstrated to capture more general and transferable representations. Gradient-based sensitivity results guide layer selection. In addition to single-layer training, we analyze the nuclear norm, which measures the magnitude of the gradient (Li et al., 2025), reflecting the sensitivity of model parameters to changes in input, thus revealing the stability and robustness of layers during training. Figure 5 illustrates the layer-wise nuclear norm (the parameter sensitivity of Q, K, and matrices) for en-zh data in the FLORES-101 development dataset, with similar trends observed in other translation directions. Internal encoder-decoder hypothesis inspires two-stage training approach. In decoder-only models, bottom layers (close to the input embedding layer) primarily focus on encoding information, while top layers (far away from the input embedding layer) emphasize the decoding process. By leveraging this insight, as discussed in previous studies (Chen et al., 2024; Lin et al., 2025), we enhance training strategies to optimize the performance of decoder-only models."
        },
        {
            "title": "3.3 Training Algorithm",
            "content": "As shown in Algorithm 1, we fine-tune an instruct model using the same instruction-formatted parallel data in two stages. In Stage 1, we focus on tuning the bottom layers of the model. Then, in Stage 2, we focus on adjusting the top layers of the model that have already undergone training in the first stage. Throughout the tuning process, the (a) Qwen3-XPlus-8B (b) Qwen3-XPlus-14B Figure 6: Comparison of xComet scores of Qwen3-XPlus with Qwen3, and between Full Fine-Tuning (FFT) and LoRA on the FLORES-101 test set covering 17 languages, with results for 7 representative languages shown in the figure. The results demonstrate that Layer-selective Tuning consistently enhances the translation performance of Qwen3 compared with both LoRA and FFT. In this figure, denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction. TowerInstruct-7B-v0.1 Hunyuan-MT-7B Sailor2-8B-Chat LLaMAX3-8B-Alpaca Tower-Plus-9B Aya-Expanse-8B Aya-Expanse-32B Qwen3-8B Qwen3-XPlus-8B Qwen3-14B Qwen3-XPlus-14B TowerInstruct-7B-v0.1 Hunyuan-MT-7B Sailor2-8B-Chat LLaMAX3-8B-Alpaca Tower-Plus-9B Aya-Expanse-8B Aya-Expanse-32B Qwen3-8B Qwen3-XPlus-8B Qwen3-14B Qwen3-XPlus-14B en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet 29.26 21.20 0.52 35.96 40.12 33.13 39. 35.24 38.02 36.92 39.01 72.56 67.68 17.81 89.98 91.74 79.28 88.63 89.89 91.35 91.98 92.86 0.71 5.55 0.67 10.00 2.45 1.49 2. 3.49 18.60 5.87 20.02 37.34 32.95 17.09 53.15 20.80 8.91 16.53 12.52 50.99 15.33 57.66 0.48 17.70 24.12 23.62 18.71 6.42 15. 29.85 27.84 32.40 32.03 53.90 56.92 60.84 72.43 53.76 19.81 40.65 75.47 73.17 80.12 80.47 0.25 8.92 2.42 12.04 2.47 4.94 11. 14.14 19.39 17.50 21.63 59.70 47.17 20.67 66.76 58.16 25.08 53.76 59.91 70.67 68.09 77.43 17.02 18.35 16.69 21.08 30.37 23.53 27. 26.88 26.95 28.71 28.96 62.16 73.67 60.53 77.72 82.96 70.67 80.70 81.65 82.15 83.95 84.90 0.48 13.70 5.60 17.57 9.66 23.77 28. 21.73 24.00 24.01 26.31 58.62 54.37 31.41 72.17 48.73 70.21 81.70 75.18 77.50 79.75 82.19 13.4 10.32 4.46 11.90 22.36 17.71 21. 16.20 18.08 18.77 20.31 62.28 58.28 37.03 76.11 85.53 70.53 82.79 76.48 80.54 82.19 84.68 en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet 18.26 28.43 16.03 26.62 28.83 25.75 30.21 28.86 32.82 31.78 35.97 67.80 87.96 54.11 83.22 79.85 68.36 78.30 80.27 85.52 84.54 89. 2.67 14.12 1.76 20.23 18.38 7.90 16.72 13.61 21.41 18.40 23.19 17.33 39.86 14.08 59.06 46.19 16.43 38.82 32.32 55.06 43.75 57. 3.82 7.53 4.30 16.03 19.02 11.39 18.25 19.68 22.68 22.35 24.91 29.22 36.37 33.86 74.88 70.29 40.78 64.11 72.64 77.36 78.09 83. 1.78 4.60 5.72 17.20 18.64 11.29 19.37 19.38 22.47 22.47 24.93 21.56 28.71 30.13 68.49 62.92 36.77 62.04 66.51 71.75 72.32 77. 10.89 20.37 3.83 16.51 19.39 17.85 21.26 20.82 23.31 23.02 25.41 69.26 83.94 28.77 80.33 75.55 65.45 75.21 78.90 83.13 83.04 87. 7.95 15.72 7.29 18.37 21.72 20.21 24.71 22.85 25.46 25.28 28.18 39.09 55.10 34.26 73.77 69.28 60.86 71.16 72.27 75.63 76.98 81. 11.26 14.31 7.18 17.39 20.68 18.41 22.07 20.23 22.73 22.90 25.53 70.69 51.93 36.11 72.23 71.93 60.74 70.84 71.79 75.77 77.07 82. Table 1: Comparison translation performance of Qwen3-XPlus with Qwen3, and between Full Fine-Tuning (FFT) and LoRA on the FLORES-101 test set covering 17 languages, with results for 7 representative languages shown in the table. Qwen3-XPlus-14B delivers the best performance on 21 of the 28 reported metrics. In this table, denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction. In this table, denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction. evaluation suitcase 6. For translation evaluation, we adopt two metrics: spBLEU (Goyal et al., 2022a) and xComet (Guerreiro et al., 2024). The spBLEU metric measures translations based on text surface, while xComet focuses on the semantic similarity between the source sentence and the translation. By using both metrics, we avoid inflated xComet scores that can arise from directly copying the source sentence, while also accounting for different valid translation possibilities. For our multilingual tasks, we evaluate seven (Conneau et al., 2018), benchmarksXNLI MGSM (Shi et al., 2022; Huang et al., 2025), 6https://huggingface.co/datasets/LLaMAX/ BenchMAX_General_Translation xIFEval (Huang et al., 2025), XStoryCloze (Lin et al., 2022), XCOPA (Ponti et al., 2020), XGPQA (Huang et al., 2025; Rein et al., 2024), and XWinograd (Muennighoff et al., 2023)using the BenchMax suite and the lm-evaluationharness (Gao et al., 2024) suite to assess the accuracy metrics for each task. reasoning tasks, we evaluate For popular including BBEH (Kazemi 15 benchmarks, et al., 2025), AIME2024 (AIME, 2025), AIME2025 (AIME, 2025), OlympiadBench (He et al., 2024), LiveMathBench (Liu et al., 2025), OlymMath (Sun et al., 2025), Math (Austin et al., 2021), LiveCodeBench-V5, LiveCodeBenchV6 (Jain et al., 2024), BigCodeBench-Hard (Zhuo 5 Figure 7: Comparison of Qwen3-XPlus, its start model Qwen3,Qwen2.5-Math-7B, Qwen2.5-Coder-7B and the leading multilingual model Tower-Plus-9B on 15 reasoning datasets. The results show that Qwen3-XPlus achieves overall performance comparable to Qwen3 and significantly surpasses Tower-Plus-9B."
        },
        {
            "title": "Models",
            "content": "AVG. XNLI MGSM xIFEval XStoryCloze XCOPA XGPQA XWinograd Qwen3-8B Qwen3-XPlus-8B Qwen3-14B Qwen3-XPlus-14B 55.30 56.93 57.64 58.61 42.44 44. 43.05 44.77 44.87 50.36 52.18 50.22 81.68 80.49 85.64 85.55 58.08 59. 58.73 61.14 60.40 61.44 61.87 63.60 35.85 34.26 41.43 40.10 63.79 67. 60.58 64.87 Table 2: Comparison of Qwen3-XPlus and Qwen3 on 7 multilingual tasks. Using only general parallel corpora and no task-specific multilingual data, Qwen3-XPlus wins 5 out of 7 tasks against Qwen3. et al., 2024), and HumanEval (Chen et al., 2021). (Contributors, We utilize the OpenCompass 2023) suite, employing accuracy as the metric for all for LiveCodeBench-V5, LiveCodeBench-V6, BigCodeBench-Hard, and HumanEval, which use pass@1 as the metric. tasks except"
        },
        {
            "title": "4.2.1 Effectiveness of layer-selective tuning",
            "content": "In Figure 6, we compared the performance of Qwen3-XPlus with its start model, Qwen3, as well as two alternative fine-tuning strategies: full finetuning (FFT) and LoRA. Since Qwen3-8B and Qwen3-14B are instruction-tuned models, enhancing their capabilities with limited data is non-trivial, and FFT on these models often causes catastrophic forgetting. For example, on Qwen3-8B, FFT leads to degraded translation performance across most languages. In comparison, LoRA helps mitigate catastrophic forgetting, but even after LoRA training, the models translation performance in most languages still falls short of its start model, Qwen38B or Qwen3-14B. In contrast, layer-selective tuning effectively improves the translation performance of Qwen3. Across 28 experimental settings, Qwen3-XPlus achieved higher xComet scores than Qwen3 in 27 cases. The improvement is especially pronounced for weaker languages like sw. Complete results are shown in Table 6."
        },
        {
            "title": "4.2.2 Performance Comparison",
            "content": "We evaluate Qwen3-XPlus against range of advanced LLMs on translation, multilingual, and general tasks and finds that Qwen3-XPlus exhibits the following strengths: Superior Translation Capability As shown in Table 1, Qwen3-XPlus demonstrates leading translation performance among current top-performing multilingual LLMs. In both the many-to-one and one-to-many settings, Qwen3-XPlus achieves the highest xComet scores in 6 of the 7 reported languages. Notably, Qwen3-XPlus outperforms even larger models. Despite using less than half the parameters, Qwen3-XPlus-14B achieves higher xComet scores than Aya-Expanse-32B across all evaluated languages, and Qwen3-XPlus-8B, with only one quarter of the parameters, surpasses AyaExpanse-32B in 12 of the 14 translation directions. Moreover, the advantage of Qwen3-XPlus is particularly pronounced for low-resource languages, where Qwen3-XPlus-14B outperforms the secondbest model, LLaMAX3-8B-Alpaca, by 4.51, 8.04,"
        },
        {
            "title": "Layer",
            "content": "enx xen Qwen3-8B 28.91 35."
        },
        {
            "title": "Top",
            "content": "Bottom 4 + Top 4 8 15 4 8 15 4 8 15 36.53 33.55 27.93 25.98 27.89 28. 32.28 33.12 32.82 29.64 22.00 17.79 33.79 35.25 36.39 36.40 36.59 38.02 Table 3: Ablation study on layer selection. and 10.67 xComet scores on sw, th, and bn, respectively. Complete results are shown in Table 7. Improved Multilingual Capability In Table 2, we evaluate Qwen3-XPlus on 7 multilingual datasets. Despite being trained solely on general parallel corpora without any task-specific multilingual data, Qwen3-XPlus demonstrates improved multilingual capabilities compared to its start models. Specifically, Qwen3-XPlus-8B outperforms Qwen3-8B on 6 out of 7 datasets, while Qwen3XPlus-14B outperforms Qwen3-14B on 5 out of 7 datasets. Furthermore, compared to other multilingual LLMs, Qwen3-XPlus consistently ranks among the best across all evaluated datasets. Its performance is particularly strong on xIFEval and XGPQA, where it exceeds the scores of existing top-performing multilingual models. Complete results are shown in Table 8. Sustained General Reasoning Capability Training instruction-tuned models on single task often leads to forgetting of general capabilities. However, as shown in Figure 7, Qwen3-XPlus maintains consistently stable general capabilities. Across reasoning tasks, including mathematics and code, Qwen3-XPlus consistently performs on par with its start model. Notably, compared to the current leading multilingual model Tower-Plus-9B, Qwen3-XPlus demonstrates clear advantage."
        },
        {
            "title": "4.2.3 Core Findings",
            "content": "We investigate three key factors underlying the strong performance of Qwen3-XPlus. The Start Model Qwen3-XPlus aligns into the multilingual space starting from an instruct model rather than base model, thereby leveraging the stronger capabilities of the Instruct variant. Tra7 ar bn cs de es fr hu ja ko ru sr sw te th vi zh Qwen3-8B One-Stage Two-Stage xen enx xen enx xen enx 38.28 32.15 40.48 44.73 32.63 46.00 35.59 29.04 31.44 36.56 41.10 23.08 33.51 31.17 37.34 31.10 29.38 19.02 31.36 40.44 30.21 49.44 24.09 24.79 21.13 35.19 25.23 6.53 15.83 36.53 38.34 34.99 42.09 34.63 42.20 46.76 34.14 47.52 37.82 31.22 32.94 38.39 43.57 32.03 36.18 33.04 39.80 33.35 31.98 24.85 32.44 40.21 31.12 50.32 25.41 25.45 22.48 35.11 32.00 23.32 27.54 32.91 40.30 35. 42.17 34.43 42.88 46.65 33.98 47.60 38.03 31.33 32.72 38.18 44.19 33.55 36.63 32.99 39.36 33.65 33.03 25.42 32.99 39.99 31.43 50.73 27.39 26.11 23.30 36.23 32.50 26.53 30.13 33.76 40.80 34."
        },
        {
            "title": "Avg",
            "content": "35.26 28.91 37.86 31.95 38.02 32. Table 4: Effectiveness of two-stage tuning in layerselective tuning. Experimental results show that the two-stage tuning strategy offers advantages over the one-stage approach and brings pronounced benefits for low-resource languages such as sw and te. ditional multilingual models usually rely on base models for continued pre-training, assuming better transferability, but this overlooks the substantial abilities already embedded in Instruct models, which are trained on extensive high-quality instruction-tuning corpora, much of it non-public. For instance, Sailor2-8B-Chat, built on Qwen2.57B-Base, shows weaker translation performance than the Instruct version Qwen2.5-7B and even lags behind the domain-specialized LLM Qwen2.5Coder-7B, as can be observed in Table 7. In contrast, by employing layer-selective tuning, Qwen3XPlus achieves smooth training process from instruct models, inheriting their strengths while further extending multilingual capability. The Data Requirements By starting from Instruct models, Qwen3-XPlus demonstrates strong performance across diverse tasks and aligns multilingual capability using only small amount of data, without relying on massive data for capability enhancement. Specifically, whereas Hunyuan-MT uses 1.3T tokens, Sailor2 uses 500B tokens, and Tower Plus uses 32B tokens, Qwen3-XPlus attains the most competitive multilingual and general-task performance with only 0.8B tokens. In particular, although Qwen3-XPlus is trained solely in general parallel corpora, it achieves highly competitive performance on specialized tasks such as code and math (Figure 7). However, for knowledge-intensive Figure 8: Translation performance on unseen languages. Qwen3-XPlus also delivers gains on languages that were unseen during the layer-selective tuning stage. Figure 9: Experiments on different backbones. layerselective tuning also brings improvements on the Llama3.1-8B instruction model. multilingual tasks like XGPQA  (Table 2)  , Qwen3XPlus does not surpass its start model, indicating that task-specific domain knowledge is essential for such tasks and cannot be fully compensated by general parallel corpora alone. The Training Process Qwen3-XPlus is trained using an efficient training procedure. By relying solely on the SFT stage, it achieves the best multilingual capability among models of comparable scale, without requiring the more demanding CPT or RL phases. This demonstrates the effectiveness of our layer-selective tuning approach and indicates the potential for further improvements through the incorporation of denser training stages."
        },
        {
            "title": "5.2 Effect of Two-Stage Training",
            "content": "In layer-selective tuning, the training process is designed in two stages to effectively adapt different layers of the model. To evaluate the necessity of this two-stage design, we compared it with single-stage approach in Table 4. The results show that sequentially fine-tuning the lower layers followed by the higher layers provides advantages over training both simultaneously. This improvement is likely due to the smoother adaptation process afforded by the two-stage design. Notably, even single-stage training significantly outperforms the start model Qwen3-8B, highlighting the importance of carefully selecting layers for fine-tuning in layer-selective tuning."
        },
        {
            "title": "5.3 Generalization to Unseen Languages",
            "content": "Furthermore, Table 3 investigates the impact of different layer selection strategies in layer-selective tuning on model performance. We first evaluate training exclusively on the lower layers and exclusively on the higher layers, and then examine combinations of both. Training few of the lower layers already surpasses the baseline, with the bottom four layers achieving the best results. For the higher layers, training the top fifteen achieves the largest improvement, likely because they capture more complex semantic features. Notably, layer 20 (the 16th from the top) negatively impacts performance and is skipped in the current experiments (Figure 4). Finally, when combining lower and higher layers, we observe that training the bottom four together with the top fifteen layers delivers the best translation performance. Consequently, this configuration is adopted in our main experiments. Qwen3-XPlus is trained on parallel corpora covering 17 languages. To evaluate its generalization to unseen languages, we test on 12 representative ones (Figure 8). The results show that Qwen3XPlus-14B consistently outperforms Qwen3-8B, demonstrating robust cross-lingual generalization and confirming the methods effectiveness in extending multilingual ability beyond the training set."
        },
        {
            "title": "5.4 Adaptability to Different Backbones",
            "content": "To verify the generality of layer-selective tuning across different models, we apply it to Llama3.18B using the same training setup. As shown in Figure 9, layer-selective tuning substantially improves performance across multiple languages, with particularly notable gains on low-resource languages. These results demonstrate the broad applicability and potential of our approach."
        },
        {
            "title": "Benchmark",
            "content": "Qwen3-8B"
        },
        {
            "title": "FFT",
            "content": "LT LT HumanEval LiveCodeBench-V5 LiveCodeBench-V6 BigCodeBench-Hard 92.68 55.69 48.57 25.00 81.71 23.95 23.43 18.92 82.93 25.15 26.29 20. 93.29 53.29 46.86 22.30 93.90 56.89 50.86 25.68 Table 5: Comparison of Full Fine-Tuning (FFT) and our layer-selective tuning (LT) on Code Generation Tasks. In most cases, FFT leads to performance degradation in instruction models, whereas LT enhances their code generation capability."
        },
        {
            "title": "5.5 Adaptability to Different Tasks",
            "content": "In our main experiments, we apply layer-selective tuning on translation training set. To further evaluate its applicability beyond translation, we conduct experiments on code generation tasks and present the results in Table 5. Specifically, we finetune Qwen3-8B on two datasets: (1) python-related samples selected from OpenThoughts (Guha et al., 2025), and (2) web-oriented samples synthesized to construct the WebSyn dataset. We then compare the performance of Full Fine-Tuning (FFT) with our proposed layer-selective tuning. Experimental results show that layer-selective tuning consistently outperforms FFT on both OpenThoughts and WebSyn. On OpenThoughts, layer-selective tuning achieves 1.22%2.86% higher accuracy, and on WebSyn it yields 0.61% 4.00% improvement. Notably, Qwen3-8B finetuned with layer-selective tuning gains 0.68% 2.29% across four WebSyn evaluation sets, while FFT decreases performance on three. These results indicate that layer-selective tuning remains effective even for code generation tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "Our approach using Qwen3-XPlus models demonstrates significant enhancement in translation performance across diverse languages, particularly in low-resource contexts. By leveraging instruct models trained on limited parallel datasets, we achieve substantial gains in multilingual tasks while maintaining competitive reasoning capabilities. This research not only addresses the challenges faced by translation-enhanced models but also sets the stage for future developments in multilingual."
        },
        {
            "title": "References",
            "content": "index.php/AIME_Problems_and_Solutions. Anton Alexandrov, Veselin Raychev, Mark Niklas Müller, Ce Zhang, Martin Vechev, and Kristina Toutanova. 2024. Mitigating catastrophic forgetting In Findin language transfer via model merging. ings of the Association for Computational Linguistics: EMNLP 2024, pages 1716717186, Miami, Florida, USA. Association for Computational Linguistics. Duarte Alves, José Pombal, Nuno Guerreiro, Pedro Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, and 1 others. 2024a. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint arXiv:2402.17733. Duarte Miguel Alves, José Pombal, Nuno M. Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024b. Tower: An Open Multilingual Large Language Model for Translation-Related Tasks. In First Conference on Language Modeling. Duarte Miguel Alves, José Pombal, Nuno M. Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024c. Tower: An Open Multilingual Large Language Model for Translation-Related Tasks. In First Conference on Language Modeling. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based forIn Proceedings of the 2019 Conference malisms. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23572367, Minneapolis, Minnesota. Association for Computational Linguistics. Anthropic. 2025. Introducing Claude 4. https://www. anthropic.com/news/claude-4. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models. Preprint, arXiv:2108.07732. Rachel Bawden and François Yvon. 2023. Investigating the translation performance of large multilingual language model: the case of BLOOM. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 157170, Tampere, Finland. European Association for Machine Translation. ByteDance. 2025. ByteDance Seed. https://seed. bytedance.com/en/seed1_6. AIME. 2025. AIME problems and solutions. https://artofproblemsolving.com/wiki/ Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi 9 Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, and 81 others. 2024. InternLM2 Technical Report. Preprint, arXiv:2403.17297. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. An image is worth 1/2 tokens after layer 2: Plug-andplay inference acceleration for large vision-language models. Preprint, arXiv:2403.06764. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating Large Language Models Trained on Code. Preprint, arXiv:2107.03374. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, and 16 others. 2024. Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):153. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, and 3290 others. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. Preprint, arXiv:2507.06261. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya Expanse: Combining Research Breakthroughs for New Multilingual Frontier. Preprint, arXiv:2412.04261. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. DeepSeek-V3 Technical Report. Preprint, arXiv:2412.19437. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, and 21 others. 2024. DeepSeek-Coder-V2: Breaking the Barrier of ClosedSource Models in Code Intelligence. Preprint, arXiv:2406.11931. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. In Thirty-Seventh Conference on Neural Information Processing Systems. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium. Association for Computational Linguistics. Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong Feng, Xin Mao, Man Tsung Yeung, Kunat Pipatanakul, Fajri Koto, Min Si Thu, and 22 others. 2025. Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs. Preprint, arXiv:2502.12982. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. Multilingual machine translation with open large language models at practical scale: An empirical study. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 54205443, Albuquerque, New Mexico. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 514 others. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, and Zuozhu Liu. 2025. Mt-r1-zero: Advancing llm-based machine translation via r1-zero-like reinforcement learning. Preprint, arXiv:2504.10160. 10 Changjiang Gao, Xu Huang, Wenhao Zhu, Shujian Huang, Lei Li, and Fei Yuan. 2025. Could thinking multilingually empower llm reasoning? Preprint, arXiv:2504.11833. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 Technical Report. Preprint, arXiv:2503.19786. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, and 179 others. 2024. Gemma 2: Improving Open Language Models at Practical Size. Preprint, arXiv:2408.00118. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022a. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022b. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Etash Kumar Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, and 31 others. 2025. Openthoughts: Data recipes for reasoning models. CoRR, abs/2506.04178. Demi Guo, Alexander Rush, and Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 48844896, Online. Association for Computational Linguistics. Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. 2024. Parameter-Efficient FineTuning for Large Models: Comprehensive Survey. Transactions on Machine Learning Research. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand. Association for Computational Linguistics. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards Unified View of Parameter-Efficient Transfer Learning. In International Conference on Learning Representations. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, pages 27902799. PMLR. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, EePeng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. 2023. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 52545276, Singapore. Association for Computational Linguistics. Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, and Fei Yuan. 2025. Benchmax: comprehensive multilingual evaluation suite for large language models. CoRR, abs/2502.07346. Zixian Huang, Wenhao Zhu, Gong Cheng, Lei Li, and Fei Yuan. 2024. Mindmerger: Efficiently boosting In AdLLM reasoning in non-english languages. vances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. 11 Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, and 5 others. 2024. Qwen2.5-Coder Technical Report. Preprint, arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. In The Thirteenth International Conference on Learning Representations. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc Le, and Orhan Firat. 2025. BIG-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2647326501, Vienna, Austria. Association for Computational Linguistics. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, and 150 others. 2025. Kimi K2: Open Agentic Intelligence. Preprint, arXiv:2507.20534. Hongyu Li, Liang Ding, Meng Fang, and Dacheng Tao. 2024a. Revisiting catastrophic forgetting in large language model tuning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 42974308, Miami, Florida, USA. Association for Computational Linguistics. Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. 2024b. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. Transactions of the Association for Computational Linguistics, 12:576 592. Ming Li, Yanhong Li, Ziyue Li, and Tianyi Zhou. 2025. How Instruction and Reasoning Data shape PostTraining: Data Quality through the Lens of Layerwise Gradients. Preprint, arXiv:2504.10766. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. Preprint, arXiv:2101.00190. Baohao Liao, Yan Meng, and Christof Monz. 2023. Parameter-efficient fine-tuning without introducing new latency. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 42424260, Toronto, Canada. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Preprint, 2023. arXiv:2305.20050. Lets Verify Step by Step. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, and 2 others. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 90199052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. 2025. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. Preprint, arXiv:2405.05803. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, pages 2155821572, Red Hook, NY, USA. Curran Associates Inc. Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. 2025. Are your LLMs capable of stable reasoning? In Findings of the Association for Computational Linguistics: ACL 2025, pages 1759417632, Vienna, Austria. Association for Computational Linguistics. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024. DoRA: WeightIn ProceedDecomposed Low-Rank Adaptation. ings of the 41st International Conference on Machine Learning, pages 3210032121. PMLR. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6168, Dublin, Ireland. Association for Computational Linguistics. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. 2024. LLaMAX: Scaling linguistic horizons of LLM by enhancing translation capabilities beyond 100 languages. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1074810772, Miami, Florida, USA. Association for Computational Linguistics. 12 Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. 2007. Detecting near-duplicates for web crawling. In Proceedings of the 16th International Conference on World Wide Web, WWW 07, page 141150, New York, NY, USA. Association for Computing Machinery. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599116111, Toronto, Canada. Association for Computational Linguistics. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, and 20 others. 2022. No language left behind: Scaling human-centered machine translation. Preprint, arXiv:2207.04672. OpenAI. 2025. Introducing GPT-5. https://openai. com/index/introducing-gpt-5/. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Abraham Toluwase Owodunni and Sachin Kumar. 2025. Continually adding new languages to multilingual language models. arXiv preprint arXiv:2509.11414. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487503, Online. Association for Computational Linguistics. Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. 2020. XCOPA: multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 23622376, Online. Association for Computational Linguistics. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and 24 others. 2025. Qwen2.5 Technical Report. Preprint, arXiv:2412.15115. Ricardo Rei, Nuno M. Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, and André F. T. Martins. 2025. Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs. Preprint, arXiv:2506.17080. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, and 7 others. 2024. Code Llama: Open Foundation Models for Code. Preprint, arXiv:2308.12950. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 64906500, Online. Association for Computational Linguistics. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations. Oscar Skean, Md Rifat Arefin, Yann LeCun, and Ravid Shwartz-Ziv. 2024. Does representation matter? exploring intermediate layers in large language models. CoRR, abs/2412.09563. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. 2025. Layer by layer: Uncovering hidden representations in language models. CoRR, abs/2502.02013. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2025. Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models. Preprint, arXiv:2503.21380. Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, 13 large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 27652781, Mexico City, Mexico. Association for Computational Linguistics. Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, and 14 others. 2024. BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. In The Thirteenth International Conference on Learning Representations. Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, and 89 others. 2024. Hunyuan-Large: An OpenSource MoE Model with 52 Billion Activated Parameters by Tencent. Preprint, arXiv:2411.02265. Yi-Lin Sung, Varun Nair, and Colin Raffel. 2021. Training Neural Networks with Fixed Sparse Masks. In Advances in Neural Information Processing Systems. Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12), Istanbul, Turkey. European Language Resources Association (ELRA). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, WeiYin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, and 1 others. 2024. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 Technical Report. Preprint, arXiv:2505.09388. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. Preprint, arXiv:2409.12122. Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628 1639, Online. Association for Computational Linguistics. Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, and Di Wang. 2025. Hunyuan-MT Technical Report. Preprint, arXiv:2509.05209. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024. Multilingual machine translation with 14 low-rank LoRA achieved significantly better performance than high-rank configurations, which might be attributed to its stronger ability to mitigate catastrophic forgetting. Based on these observations, we adopted the LoRA tuning with rank = 8 in our main experiments."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Models Information of the models evaluated in our study are listed in Table 9. A.2 Training Data Our training data mainly sources from NLLB and OPUS-100. Here we briefly introduce these two datasets. NLLB. Provided in CCMatrix (Schwenk et al., 2021), this dataset was created based on metadata for mined parallel corpus released by Meta AI (NLLB Team et al., 2022). It contains parallel text for 148 English-centric and 1465 nonEnglish-centric language pairs, with complete size of 450GB. OPUS-100. OPUS-100 is an English-centric multilingual corpus covering 100 languages. The languages were selected based on the volume of parallel data available in OPUS (https://opus.nlpl. eu). OPUS-100 contains approximately 55M sentence pairs. Of the 99 language pairs, 44 have 1M sentence pairs of training data, 73 have at least 100k, and 95 have at least 10k. A.3 Evaluation Benchmarks Table 10 lists the benchmarks used in our evaluation. A.4 Hyperparameter Settings We train the model for 1 epoch with learning rate of 1e-5, scheduled by cosine scheduler with minimum learning rate of 2e-6 and warmup ratio of 0.03. Mixed precision training (bf16) is used to improve efficiency. All experiments are performed on 8 NVIDIA H800 GPUs with per-device training batch size of 1 and gradient accumulation over 2 steps7. A.5 Evaluation Benchmarks A.6 Analysis of LoRA Variants and"
        },
        {
            "title": "Hyperparameters",
            "content": "In Table 11, we compared the translation performance of Qwen3 on the FLORES-101 dataset under various LoRA variants and hyperparameter settings. We observed that the vanilla LoRA showed clear advantage over other variants. Notably, 7More training details can be found in the conhttps://huggingface.co/LLaMAX/ figuration Qwen3-XPlus-17langs-14B/blob/main/training.yaml file: 15 en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet Qwen3-8B Qwen3-8B-FFT Qwen3-8B-LoRA Qwen3-XPlus-8B Qwen3-14B Qwen3-14B-FFT Qwen3-14B-LoRA Qwen3-XPlus-14B 35.24 9.73 35.63 38.02 36.92 40.37 37.19 39.01 89.89 34.21 84.64 91.35 91.98 90.99 86.57 92. 3.49 2.19 1.99 18.60 5.87 13.04 6.13 20.02 12.52 20.31 17.66 50.99 15.33 53.69 18.90 57.66 29.85 2.36 22.39 27.84 32.40 19.34 26.10 32. 75.47 22.56 67.04 73.17 80.12 63.68 67.32 80.47 14.14 5.02 10.62 19.39 17.50 17.13 16.12 21.63 59.91 24.63 47.49 70.67 68.09 67.70 60.78 77. 26.88 9.10 22.89 26.95 28.71 24.98 24.40 28.96 81.65 32.25 75.64 82.15 83.95 77.93 75.81 84.90 21.73 8.16 15.04 24.00 24.01 21.37 20.88 26. 75.18 37.12 66.77 77.50 79.75 73.84 69.40 82.19 16.20 2.19 11.82 18.08 18.77 14.76 17.08 20.31 76.48 21.57 68.17 80.54 82.19 74.66 77.12 84. en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet Qwen3-8B Qwen3-8B-FFT Qwen3-8B-LoRA Qwen3-XPlus-8B Qwen3-14B Qwen3-14B-FFT Qwen3-14B-LoRA Qwen3-XPlus-14B 28.86 10.73 26.57 32.82 31.78 34.35 28.26 35.97 80.27 35.25 73.86 85. 84.54 83.58 75.52 89.51 13.61 6.81 10.4 21.41 18.40 21.26 15.79 23.19 32.32 24.55 29.33 55.06 43.75 57.83 40.48 57.23 19.68 4.34 17.27 22. 22.35 16.62 20.39 24.91 72.64 23.75 65.55 77.36 78.09 71.63 70.27 83.40 19.38 5.88 16.78 22.47 22.47 20.07 18.78 24.93 66.51 25.34 58.28 71. 72.32 68.88 61.55 77.15 20.82 6.25 17.93 23.31 23.02 21.17 18.36 25.41 78.90 30.00 71.24 83.13 83.04 80.57 69.12 87.64 22.85 7.13 19.77 25. 25.28 23.42 22.61 28.18 72.27 28.00 64.89 75.63 76.98 73.69 68.99 81.90 20.23 5.69 17.37 22.73 22.90 20.38 21.34 25.53 71.79 25.41 63.33 75. 77.07 71.66 71.84 82.16 4 Table 6: Comparison of Qwen3-XPlus with Qwen3, and between Full Fine-Tuning (FFT) and LoRA on 17 languages from the FLORES-101 test set. In this table, denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction. 16 en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet 1.85 33.59 40.47 41. 36.82 0.38 30.96 31.07 25.89 36.2 21.67 21.94 29.61 20.65 2.29 28.52 30.16 31.80 29.26 21.20 0.52 35.96 40.12 33.13 39.72 35.24 9.73 35.63 38.02 36.92 40.37 37.19 39.01 14.73 79.36 95.16 94. 86.44 14.98 79.59 78.37 64.86 87.64 64.71 71.32 79.00 63.32 17.48 73.67 75.40 78.01 72.56 67.68 17.81 89.98 91.74 79.28 88.63 89.89 34.21 84.64 91.35 91.98 90.99 86.57 92.86 0.57 16.60 22.38 16. 0.63 2.38 5.83 1.70 1.58 5.41 0.29 0.11 1.15 0.52 0.09 0.19 0.67 0.85 0.71 5.55 0.67 10.00 2.45 1.49 2.60 3.49 2.19 1.99 18.60 5.87 13.04 6.13 20.02 13.98 47.84 67.43 41. 13.46 17.27 32.84 9.62 13.00 15.70 24.34 23.71 11.12 24.76 14.04 12.23 13.52 14.78 37.34 32.95 17.09 53.15 20.80 8.91 16.53 12.52 20.31 17.66 50.99 15.33 53.69 18.9 57.66 Super-Large Models 5.01 24.37 36.81 35.81 2.00 1.45 16.55 20.46 8.68 27.44 28.10 60.62 87.62 87.51 General LLMs 19.13 18.1 50.92 55.45 37.33 69.19 0.33 16.5 23.70 24. 0.90 6.46 13.80 6.11 6.50 13.48 Domain-Specialized LLMs 0.19 3.26 14.06 0.34 0.08 12.14 7.95 20.37 0.48 17.70 24.12 23.62 18.71 6.42 15.16 29.85 2.36 22.39 27.84 32.40 19.34 26.10 32. 28.25 25.90 34.95 34.47 20.61 42.03 36.35 51.62 0.08 0.98 5.04 0.08 0.02 1.32 1.06 8.66 Multilingual LLMs 53.90 56.92 60.84 72.43 53.76 19.81 40.65 Qwen3-XPlus 75.47 22.56 67.04 73. 80.12 63.68 67.32 80.47 0.25 8.92 2.42 12.04 2.47 4.94 11.93 14.14 5.02 10.62 19.39 17.50 17.13 16.12 21.63 14.92 60.33 87.52 82.90 18.61 33.12 59.67 28.07 31.96 55. 21.61 22.08 27.76 25.97 20.2 20.60 24.91 38.07 59.70 47.17 20.67 66.76 58.16 25.08 53.76 59.91 24.63 47.49 70.67 68.09 67.70 60.78 77.43 19.84 29.55 31.86 31.39 22.36 1.79 17.35 23.13 22.75 27. 4.75 12.14 21.90 0.63 0.32 20.80 21.13 25.04 17.02 18.35 16.69 21.08 30.37 23.53 27.93 26.88 9.1 22.89 26.95 28.71 24.98 24.40 28.96 67.62 86.96 88.44 87.98 69.97 21.52 64.60 73.58 69.12 82. 45.37 59.94 70.26 47.69 19.81 66.86 73.30 76.24 62.16 73.67 60.53 77.72 82.96 70.67 80.70 81.65 32.25 75.64 82.15 83.95 77.93 75.81 84.90 1.53 9.27 28.09 27.92 1.20 2.78 9.42 12.94 7.98 17. 1.19 0.78 11.82 0.07 0.03 9.31 5.65 13.48 0.48 13.70 5.60 17.57 9.66 23.77 28.63 21.73 8.16 15.04 24.00 24.01 21.37 20.88 26.31 16.87 40.02 87.96 87.11 17.63 18.07 40.48 50.75 33.72 61. 24.13 22.60 43.43 24.45 22.46 41.37 30.52 49.67 58.62 54.37 31.41 72.17 48.73 70.21 81.70 75.18 37.12 66.77 77.50 79.75 73.84 69.40 82.19 1.35 14.40 22.57 21.59 7.41 0.13 11.02 8.94 6.64 15. 0.24 2.35 9.66 0.40 0.13 7.14 3.63 11.03 13.4 10.32 4.46 11.90 22.36 17.71 21.71 16.20 2.19 11.82 18.08 18.77 14.76 17.08 20.31 16.68 59.19 89.80 88.85 39.49 11.11 60.44 53.13 38.73 70. 19.35 37.64 49.13 32.94 20.23 45.93 34.00 56.90 62.28 58.28 37.03 76.11 85.53 70.53 82.79 76.48 21.57 68.17 80.54 82.19 74.66 77.12 84.68 en spBLEU xComet sw spBLEU xComet th spBLEU xComet bn spBLEU xComet zh spBLEU xComet ar spBLEU xComet ko spBLEU xComet 2.91 30.83 38.78 36. 7.23 18.77 26.97 19.39 6.90 27.58 4.81 7.26 12.97 21.82 0.44 17.53 19.19 20.94 18.26 28.43 16.03 26.62 28.83 25.75 30.21 28.86 10.73 26.57 32.82 31.78 34.35 28.26 35.97 20.74 74.27 93.86 91. 44.02 57.18 81.09 59.90 31.92 74.29 71.10 40.40 53.45 63.34 38.89 59.59 61.28 62.04 67.80 87.96 54.11 83.22 79.85 68.36 78.30 80.27 35.25 73.86 85.52 84.54 83.58 75.52 89.51 1.18 15.84 29.92 27. 4.42 0.07 8.96 5.03 4.26 14.34 0.15 1.13 0.79 5.82 0.03 3.96 2.78 8.00 2.67 14.12 1.76 20.23 18.38 7.90 16.72 13.61 6.81 10.40 21.41 18.40 21.26 15.79 23.19 14.28 43.38 72.68 66. 20.07 11.41 29.03 15.02 19.95 35.24 11.70 13.76 15.72 17.29 14.01 13.81 15.87 23.64 17.33 39.86 14.08 59.06 46.19 16.43 38.82 32.32 24.55 29.33 55.06 43.75 57.83 40.48 57.23 Super-Large Models 1.90 20.20 26.37 26.54 7.06 2.38 15.06 14.20 10.34 18.94 18.81 64.89 88.32 85.61 General LLMs 30.65 22.78 65.86 54.89 41.12 66.27 1.72 21.47 27.93 27. 6.86 0.45 16.62 10.74 8.55 17.94 Domain-Specialized LLMs 0.70 1.64 3.36 10.66 0.16 10.13 6.86 11.47 3.82 7.53 4.30 16.03 19.02 11.39 18.25 19.68 4.34 17.27 22.68 22.35 16.62 20.39 24. 15.41 21.55 38.27 45.73 14.33 43.65 36.47 47.78 0.21 0.83 2.49 9.95 0.04 7.33 6.65 11.20 Multilingual LLMs 29.22 36.37 33.86 74.88 70.29 40.78 64.11 Qwen3-XPlus 72.64 23.75 65.55 77. 78.09 71.63 70.27 83.40 1.78 4.60 5.72 17.20 18.64 11.29 19.37 19.38 5.88 16.78 22.47 22.47 20.07 18.78 24.93 18.12 65.26 83.90 81.47 31.32 15.49 59.50 41.27 38.32 59. 12.45 19.17 30.42 36.05 14.80 28.57 31.96 42.60 21.56 28.71 30.13 68.49 62.92 36.77 62.04 66.51 25.34 58.28 71.75 72.32 68.88 61.55 77.15 10.85 25.52 27.07 26.42 8.99 1.42 10.74 12.91 10.66 17. 1.82 4.25 6.17 13.66 0.36 11.26 13.13 15.61 10.89 20.37 3.83 16.51 19.39 17.85 21.26 20.82 6.25 17.93 23.31 23.02 21.17 18.36 25.41 48.22 84.73 91.84 88.48 39.85 19.48 51.34 55.85 43.37 64. 47.68 49.02 55.35 58.76 28.90 56.78 59.08 61.88 69.26 83.94 28.77 80.33 75.55 65.45 75.21 78.90 30.0 71.24 83.13 83.04 80.57 69.12 87.64 1.92 24.31 30.38 29.68 9.60 0.38 10.61 14.37 10.04 21. 1.04 2.71 3.78 13.57 0.11 10.70 8.66 13.46 7.95 15.72 7.29 18.37 21.72 20.21 24.71 22.85 7.13 19.77 25.46 25.28 23.42 22.61 28.18 15.68 66.01 87.33 84.62 34.06 12.24 42.32 48.59 36.82 65. 17.83 39.27 31.81 48.63 11.74 41.13 39.29 47.14 39.09 55.10 34.26 73.77 69.28 60.86 71.16 72.27 28.00 64.89 75.63 76.98 73.69 68.99 81.90 2.40 23.44 27.45 26.80 9.33 1.87 16.73 12.75 7.78 19. 1.61 2.48 5.52 12.48 0.19 10.73 7.30 13.19 11.26 14.31 7.18 17.39 20.68 18.41 22.07 20.23 5.69 17.37 22.73 22.90 20.38 21.34 25.53 20.09 72.22 87.26 84.10 36.39 21.38 63.44 47.20 33.41 63. 33.68 35.73 50.14 47.68 12.55 44.55 38.95 49.97 70.69 51.93 36.11 72.23 71.93 60.74 70.84 71.79 25.41 63.33 75.77 77.07 71.66 71.84 82.16 DeepSeek-R1-0528 DeepSeek-V3-0324 Kimi-K2 Qwen3-235B-A22B Gemma-3-12B-it LLaMA3-8B LLaMA3.1-8B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B InternLM2-Math-7B Deepseek-Math-7B DeepSeek-Coder-V2-Lite CodeLlama-7b Qwen2.5-Math-7B Qwen2.5-Coder-7B Qwen2.5-Coder-14B Qwen2.5-Coder-32B TowerInstruct-7B-v0.1 Hunyuan-MT-7B Sailor2-8B-Chat LLaMAX3-8B-Alpaca Tower-Plus-9B Aya-Expanse-8B Aya-Expanse-32B Qwen3-8B Qwen3-8B-FT Qwen3-8B-LoRA Qwen3-XPlus-8B Qwen3-14B Qwen3-14B-FFT Qwen3-14B-LoRA Qwen3-XPlus-14B DeepSeek-R1-0528 DeepSeek-V3-0324 Kimi-K2 Qwen3-235B-A22B Gemma-3-12B-it LLaMA3-8B LLaMA3.1-8B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B CodeLlama-7B InternLM2-Math-7B DeepSeek-Math-7B DeepSeek-Coder-V2-Lite Qwen2.5-Math-7B Qwen2.5-Coder-7B Qwen2.5-Coder-14B Qwen2.5-Coder-32B TowerInstruct-7B-v0.1 Hunyuan-MT-7B Sailor2-8B-Chat LLaMAX3-8B-Alpaca Tower-Plus-9B Aya-Expanse-8B Aya-Expanse-32B Qwen3-8B Qwen3-8B-FFT Qwen3-8B-LoRA Qwen3-XPlus-8B Qwen3-14B Qwen3-14B-FFT Qwen3-14B-LoRA Qwen3-XPlus-14B Table 7: Translation results on 17 languages from FLORES-101 test set. In this table, denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction."
        },
        {
            "title": "Models",
            "content": "HumanEval+ XNLI MGSM xIFEval XStoryCloze MathQA XCOPA XGPQA XWinograd Aya-Expanse-8B CodeLlama-7B DeepSeek-Coder-V2-Lite LLaMAX3-8B-Alpaca Llama-3-8B Llama-3.1-8B Qwen2.5-14B Qwen2.5-32B Qwen2.5-7B Qwen2.5-Coder-7B Sailor2-8B-Chat Tower-Plus-9B TowerInstruct-7B-v0.1 DeepSeek-Math-7B Gemma-3-12B-IT InternLM2-Math-7B Qwen3-8B Qwen3-XPlus-8B Qwen3-14B Qwen3-XPlus-14B 40.24 31.71 74.39 24.39 54.27 61.59 73.17 84.15 75.00 85.37 39.63 0.00 19.51 51.83 0.00 28.05 76.83 76. 85.37 85.98 45.53 40.69 42.30 45.33 42.14 44.83 39.64 40.16 39.84 42.95 38.30 42.54 40.45 42.02 33.33 38.60 42.44 44.79 43.05 44.77 14.51 1.78 30.62 9.78 55.38 28.36 39.78 74.95 34.80 52.25 34.84 64.00 3.75 26.95 0.04 39.24 44.87 50. 52.18 50.22 40.46 30.51 43.41 36.74 65.32 45.46 58.35 82.67 57.64 61.01 43.01 75.52 30.19 33.33 25.19 35.63 81.68 80.49 85.64 85.55 64.80 56.45 62.69 61.84 59.50 64.47 66.72 65.91 63.25 58.58 63.56 61.65 59.11 58.56 52.81 55.68 58.08 59. 58.73 61.14 37.69 28.71 45.16 34.17 27.24 39.20 48.48 38.22 40.27 37.15 40.64 33.13 29.25 37.96 20.57 26.20 32.80 32.93 34.71 35.88 56.36 54.58 59.27 63.85 60.89 60.89 65.33 65.98 62.84 58.96 24.11 60.62 56.98 56.67 50.00 55.45 60.40 61. 61.87 63.60 20.85 14.40 22.86 21.78 27.25 19.80 32.41 38.12 26.19 23.83 24.11 27.36 15.77 20.25 0.00 18.12 35.85 34.26 41.43 40.10 74.67 76.42 80.24 74.80 71.09 81.70 82.72 71.66 80.58 70.60 81.64 72.02 78.40 76.26 51.63 61.50 63.79 67. 60.58 64.87 Table 8: Comparison of Qwen3-XPlus and existing LLMs on reasoning and multilingual tasks."
        },
        {
            "title": "General\nInstruct",
            "content": "Gemma3-IT (Gemma Team et al., 2025) LLaMA3-Instruct (Dubey et al., 2024) LLaMA3.1-Instruct (Dubey et al., 2024) Qwen2.5-Instruct (Qwen et al., 2025) Qwen3 (Yang et al., 2025) DomainSpecialized CodeLLaMA (Rozière et al., 2024) InternLM2-Math (Cai et al., 2024) DeepSeek-Coder-V2-Lite (DeepSeek-AI et al., 2024) Qwen2.5-Math (Yang et al., 2024) Qwen2.5-Coder (Hui et al., 2024) 12B 8B 8B 8B, 14B, 32B 8B, 14B 7B 7B 16B 7B 7B, 14B, 32B SOTA multimodal open model from Google Popular, classic open LLM from Meta Updated version of LLaMA3-Instruct Popular, classic open LLM from Alibaba SOTA open LLM with mixed thinking mode from Alibaba Open code LLM based on LLaMA2 Open math LLM based on InternLM2 Open code LLM based on DeepSeek-V2 Open math LLM based on Qwen-2.5 Open code LLM based on Qwen-2.5 MultilingualEnhanced SuperLarge"
        },
        {
            "title": "Ours",
            "content": "TowerInstruct-v0.1 (Alves et al., 2024b) Hunyuan-MT (Zheng et al., 2025) Aya-Expanse (Dang et al., 2024) Sailor2-Chat (Dou et al., 2025) LLaMAX3-Alpaca (Lu et al., 2024) Tower-Plus (Rei et al., 2025) 7B 7B 8B 8B 8B 9B Multilingual translation LLM based on LLaMA2 Multilingual translation model based on Hunyuan Advanced multilingual LLM based on Command South-East Asia languages focused LLM based on Qwen2 Multilingual translaation LLM based on LLaMA3 Multilingual translation and general LLM based on Gemma2 Qwen3 (Yang et al., 2025) DeepSeek-V3 (DeepSeek-AI et al., 2025b) DeepSeek-R1 (DeepSeek-AI et al., 2025a) Kimi-K2 (Kimi Team et al., 2025) 235B (22B Active) 671B (37B Active) 671B (37B Active) 1T (32B Active) SOTA open LLM with mixed thinking mode from Alibaba Popular open LLM from DeepSeek Popular, classic open reasoning LLM from DeepSeek SOTA reasoning LLM from Moonshot Qwen3-FFT Qwen3-LoRA Qwen3-XPlus 8B, 14B 8B, 14B 8B, 14B Qwen3 with fully finetuning on our multilingual data Qwen3 with LoRA finetuning on our multilingual data Qwen3 with layer-selective tuning on our multilingual data Table 9: Information of models used in our study."
        },
        {
            "title": "Translation",
            "content": "FLORES-101 (Goyal et al., 2022b) spBLEU, xCOMET Parallel sentences for 101 languages extracted from English Wikipedia"
        },
        {
            "title": "General",
            "content": "XNLI (Conneau et al., 2018) MGSM (Shi et al., 2022) xIFEval (Huang et al., 2025) XStoryCloze (Lin et al., 2022) XCOPA (Ponti et al., 2020) XGPQA (Huang et al., 2025; Rein et al., 2024) XWinograd (Muennighoff et al., 2023) MathQA (Amini et al., 2019) BBEH (Kazemi et al., 2025) AIME 2024, 2025 (AIME, 2025) OlympiadBench (He et al., 2024) LiveMathBench (Liu et al., 2025) OlymMath (Sun et al., 2025) Math (Lightman et al., 2023) MBPP (Austin et al., 2021) LiveCodeBench-V5, V6 (Jain et al., 2024) BigCodeBench-Hard (Zhuo et al., 2024) HumanEval+ (Liu et al., 2023)"
        },
        {
            "title": "Accuracy",
            "content": "Pass@1 Subset of MNLI translated into 14 languages, about textual entailment Subset of GSM translated into 10 languages, about grad-school math IFEval translated into 17 languages, about instruction following English StoryCloze translated into 10 languages, about story continuation COPA translated into 11 languages, about commonsense reasoning translated into 17 languages, about challenging scientific questions Winograd enriched to 6 languages, about coreference resolution Math word problems adapted from AQuA-RAT Extra hard version of Big-Bench with newer and harder tasks Problems from the American Invittional Mathematics Examination Olympiad-level bilingual multimodal math and physics promblems Challenging latest questions from mathematical competitions Olympiad-level math problems in parallel English and Chinese Challenging competition math problems with full step-by-step solutions Crowd-sourced entry level Python programming problems New problems from coding contests Practical and challenging programming problems Formatted programming problems and its improved version Table 10: Information on benchmarks used in our study. setting rank spBLEU xComet spBLEU xComet setting rank spBLEU xComet spBLEU xComet languagex xlanguage languagex xlanguage Qwen3-8B Qwen3-14B"
        },
        {
            "title": "LoRA",
            "content": "rsLoRA"
        },
        {
            "title": "DLoRA",
            "content": "8 16 32 64 128 8 16 32 64 128 8 16 32 64 128 18.01 16.83 15.87 15.31 13.95 15.58 13.64 12.51 14.60 13.17 17.49 16.62 15.53 15.52 14. 60.93 57.76 56.05 54.90 51.42 55.44 51.81 48.02 59.01 52.54 60.16 57.77 55.49 55.93 52.06 17.83 16.46 16.05 14.28 11.93 16.35 11.32 10.41 13.18 11.26 17.20 16.28 15.78 14.82 12. 61.49 57.67 56.35 54.03 47.05 57.56 47.82 41.80 56.66 49.56 61.06 58.14 56.29 55.69 48."
        },
        {
            "title": "LoRA",
            "content": "rsLoRA"
        },
        {
            "title": "DLoRA",
            "content": "8 16 32 64 128 8 16 32 64 128 8 16 32 64 128 20.79 13.45 10.91 6.75 7.55 8.29 5.52 8.66 9.96 9.59 12.90 9.68 8.32 5.62 8. 65.40 47.44 40.80 32.50 37.87 35.53 30.83 44.68 48.65 43.24 47.87 39.09 35.26 30.57 43.40 21.13 14.64 13.01 8.15 8.04 10.64 6.55 9.86 10.59 9.51 15.85 11.75 10.45 6.85 10. 65.13 49.51 44.22 35.30 40.55 39.48 33.21 46.39 49.81 44.66 53.31 42.75 38.73 32.50 44.50 Table 11: Translation performance of Qwen3 under different LoRA variants and hyperparameter settings."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "Shanghai Artificial Intelligent Laboratory",
        "The University of Hong Kong"
    ]
}