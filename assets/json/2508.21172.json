{
    "paper_title": "Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks",
    "authors": [
        "Matteo Pinna",
        "Andrea Ceni",
        "Claudio Gallicchio"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC."
        },
        {
            "title": "Start",
            "content": "Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks Matteo Pinna, Andrea Ceni, Claudio Gallicchio 1 5 2 0 2 8 ] . [ 1 2 7 1 1 2 . 8 0 5 2 : r AbstractEcho State Networks (ESNs) are particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC1. Index Termsreservoir computing, echo state networks, recurrent neural networks, deep learning I. INTRODUCTION EEP Neural Networks (DNNs) have driven major advances in fields such as computer vision and natural language processing, thanks to their capacity to learn hierarchical data representations through the composition of multiple non-linear layers [1][3]. Recurrent Neural Networks (RNNs) similarly benefit from depth, both in their architectural design and through their temporal unfolding, which effectively creates deep computational graphs across time steps. This temporal depth makes RNNs powerful for modeling sequential data but also introduces challenges analogous to those encountered in deep feedforward networks, such as vanishing and exploding gradients [4], [5]. These issues, compounded by increased computational demands, hinder the training of very deep recurrent models. Randomized untrained models offer an alternative to gradient-based training. In particular, the Reservoir Computing (RC) framework [6] uses fixed recurrent dynamics and trains only readout layer, reducing training complexity. Deep Echo State Networks (DeepESNs) [7] extend this idea by stacking multiple untrained reservoirs to build temporal hierarchies. However, while RC models bypass backpropagation, they are still susceptible to signal degradation or amplification in the forward pass, both in the temporal dimension and architectural dimension. These effects can impair the stability and expressiveness of the model, limiting their effectiveness on complex temporal tasks. To address analogous issues in Preprint. Under review. The authors are with the Department of Computer Science, University of Pisa, 56127 Pisa, Italy (e-mail: matteo.pinna@di.unipi.it; andrea.ceni@unipi.it; claudio.gallicchio@unipi.it). 1Code will be publicly available at github.com/nennomp/deepresesn. Fig. 1. Architectural organization of the proposed DeepResESN. (a) Structure of generic l-th reservoir layer in DeepResESN. The reservoir structure (shown in blue) consists of an input weight matrix W(l) , recurrent weight matrix W(l) , and non-linear activation function ϕ. The temporal residual connection (shown in purple) is modulated by an orthogonal matrix O. The temporal residual and non-linear paths are scaled by positive coefficients α(l) and β(l), respectively. (b) Complete illustration of DeepResESN architecture with NL reservoir layers. The first layer acts as residual reservoir in traditional shallow architecture and is fed the external input x(1). Subsequent layers receive as input the output of the previous reservoir, h(l1). The readout may be fed either the final layer states or the concatenation of states from all layers. See Section III for details. feedforward architectures, residual connections have proven highly effective. Introduced in the context of convolutional networks, Residual Networks (ResNets) [8] enhance information flow by incorporating identity-based skip connections, thereby enabling the training of very deep models. Although originally proposed for fully-trainable networks, the core principle, facilitating signal propagation through additional paths, is broadly applicable and conceptually appealing for mitigating dynamical degradation in untrained settings as well. Although residual pathways are promising solution, they remain underexplored in recurrent architectures. Some prior work has introduced temporal skip connections in trainable RNNs [9], [10], while Residual Echo State Networks (ResESNs) [11] applied temporal residual dynamics to shallow RC models. However, their extension to deep untrained RNNs has not yet been explored. In this paper, we introduce Deep Residual Echo State Networks (DeepResESNs), novel class of deep untrained RNNs that unify the hierarchical representation capabilities of DeepESNs with the enhanced temporal signal propagation of ResESNs, thereby providing principled generalization of both architectures. The architecture is graphically highlighted in Fig. 1. Specifically, (i) we propose deep RC model in which each untrained recurrent layer is augmented with temporal residual connection, governed by configurable mappings such as random orthogonal, cyclic, or identity transformations; (ii) we explore the effect of each transformation on the networks dynamics, leveraging spectral frequency analysis tools; (iii) we provide comprehensive mathematical analysis of DeepResESN dynamics, deriving necessary and sufficient conditions for stability and contractivity, and extending the Echo State Property (ESP) to the deep residual case; (iv) we evaluate DeepResESNs on time series tasks spanning memory, forecasting, and classification. The results demonstrate consistent improvements over both shallow and deep RC baselines, especially in settings requiring long-term temporal modeling. The remainder of this paper is organized as follows. Section II introduces background on RC. Section III presents the DeepResESN architecture and the spectral frequency analysis. Section IV presents our theoretical analysis. Section details empirical evaluations, and Section VI concludes the paper. The paper also includes an Appendix A, which is dedicated to mathematical proofs. II. RESERVOIR COMPUTING RC-based neural networks consist of large, untrained recurrent layer, called the reservoir, and trainable linear readout layer. The reservoir is randomly initialized subject to the Echo State Property (ESP) [12], stability condition useful to guide initialization in ESNs, and then left untrained. The readout is the only trainable component and processes the reservoirs output. The underlying intuition is to exploit sufficiently large non-linear reservoir to perform basis expansion of the input into high-dimensional latent space, enabling the downstream task to be more easily solved by the readout. This design circumvents the computational burden of traditional gradient descent optimization, and inherently avoids problems related to V/E gradients. fundamental baseline model in RC is the Leaky Echo State Network (LeakyESN), which exploits leaky-integrator neurons to tune the reservoir dynamics to the specific time scale of the input [13]. The state transition function of shallow LeakyESN is given by: h(t) = (1 τ )h(t 1) + τ ϕ (cid:16) (cid:17) , Whh(t 1) + Wxx(t) + (1) where h(t) RNh and x(t) RNx are, respectively, the state and the external input at time step t. The recurrent weight matrix is denoted as Wh RNhNh , the input weight matrix is denoted as Wx RNhNx , RNh denotes the bias vector, ϕ denotes an element-wise applied non-linearity, and τ (0, 1] denotes the leaky rate hyperparameter. The entries of Wx are sampled randomly from uniform distribution over (ωx, ωx), where ωx is the input scaling hyperparameter. Similarly, entries in are sampled randomly from uniform distribution over (ωb, ωb), where ωb represents the bias scaling hyperparameter. The entries in Wh are initially sampled from uniform distribution over (1, 1) and subsequently rescaled to have desired spectral radius ρ2, crucial hyperparameter governing the reservoirs dynamics and ESP. In practical applications, the spectral radius is generally constrained to be smaller than 1. The readout can be formulated as y(t) = Woh(t), where y(t) RNo denotes the network output at time step and Wo RNoNh denotes the readout weight matrix. The readout is typically trained using lightweight closed-form solutions, such as ridge regression or least squares methods. In the literature, it is possible to find models that lay their foundation at the intersection of the RC and DL frameworks. In particular, Deep Echo State Networks (DeepESNs) [7] represent class of deep RNN systems where multiple untrained reservoirs are stacked on top of each other. This, together with the architectural bias introduced by increasing the feedforward depth of the model, has shown promising advantages relative to more traditional, shallow RC approaches. Moreover, notice that DeepESNs provide an elementary modular organization for the dynamics of reservoir system, and generalize the concept of shallow LeakyESNs towards deep architectural constructions. More recently, Residual Echo State Networks (ResESNs) [11] introduced class of RC-based models that leverage residual connections along the temporal dimensions. Its state transition function is defined as follows: h(t) = αOh(t 1) + βϕ (cid:16) (cid:17) , Whh(t 1) + Wxx(t) + (2) where RNhNh is random orthogonal matrix, while α and β are positive hyperparameters that influence the quality of reservoir dynamics (see [11] for details). Interestingly, ResESN can be seen as generalization of the (shallow) LeakyESN model, where on the one hand the linear branch transformation is not necessarily constrained to the identity matrix, and each branch has independent scaling coefficients (i.e., α and β) that do not necessarily realize convex combination, making the model more flexible. III. DEEP RESIDUAL ECHO STATE NETWORKS We introduce Deep Residual Echo State Networks (DeepResESNs), novel class of deep untrained RNNs that merges the architectural principles of DeepESNs with the temporal residual connections of ResESNs. The proposed model consists of hierarchy of untrained recurrent layers, where each layer benefits from residual connection that propagates its previous state through simple mapping, creating shortcut along the temporal dimension. This design extends the advantages of residual connections to deep, hierarchical context. DeepResESN is characterized by hierarchy of untrained recurrent layers based on temporal residual connections. Fig. 1 2The spectral radius of matrix A, sometimes denoted as ρ(A), is defined as the largest among the lengths of its eigenvalues. graphically illustrates the proposed architecture. The first layer is driven by the external input, while subsequent layers are driven by the reservoir dynamics developed in the previous layer. To develop our mathematical description, let us assume hierarchical construction of the reservoir with number of NL layers, and let us use the superscript ()(l) to generally refer to the networks weights and hyperparameters at layer l, for = 1, . . . , NL. The state transition function computed by the l-th layer of DeepResESN is given by: (l) h(l)(t) = α(l)Oh(l)(t1)+β(l)ϕ(W h(l)(t1)+W (l) x(l)(t)+b(l)), (1) RNhNx for = 1, and (3) where the input is given by the external signal for = 1, i.e., x(1)(t) = x(t), and by the reservoir state of the previous layer for > 1, i.e., x(l)(t) = h(l1)(t). Assuming, for simplicity, that all reservoir layers have the same hidden size Nh, we (l) RNhNh for > 1. have Here, RNhNh is an orthogonal matrix, and α(l) [0, 1] and β(l) (0, 1] are layer-specific scaling coefficients that generalize the leaky rate mechanism from (1). In addition, each layer may have its own set of hyperparameters ρ(l), (l) (l) , all employed in the same way as previously , and ω ω described for LeakyESN. In this work we consider the hyperbolic tangent as the element-wise applied non-linearity (i.e., ϕ = tanh). The readout may be fed either the states from the last reservoir layer h(NL) or the concatenation of states from all reservoir layers [h(1), h(2), . . . , h(NL)], as graphically shown in Fig. 1 (b). We consider three types of orthogonal matrices for the temporal residual connections in DeepResESN, one randomly generated and two with fixed structure (see Fig. 2 for graphical representation). The first, denoted by R, is obtained via QR decomposition of random Nh Nh matrix with realvalued i.i.d entries in [1, 1). The second one, denoted by C, is the cyclic orthogonal matrix [14], [15], zero matrix with ones on the sub-diagonal and in the top-right corner. The third one is the identity matrix, denoted by I. The choice of these three cases is motivated by their distinct impact on short-term memory capacity, covering broad spectrum of dynamical behaviors [16]. In particular, the identity matrix provides relatively low memory capacity while the two other orthogonal matrices maximize it [17]. Additionally, the random orthogonal matrix offers stochastic scenario due to its random generation, and the cyclic orthogonal matrix provides deterministic alternative due to its fixed structure. These configurations thus provide representative set of dynamical regimes for analyzing the impact of different residual connec3 tivity structures in DeepResESN. It is worth noticing that the proposed DeepResESN approach to reservoir construction represents generalization of both ResESN and DeepESN. The former, whose dynamics are described by (2), corresponds to DeepResESN with only one reservoir layer, i.e., NL = 1. The latter, whose dynamics in each layer are ruled by leaky reservoir as in (1), corresponds to DeepResESN that employs the identity matrix in the temporal residual connections, and where the two coefficients α and β are related by 1 α = β = τ. Fig. 3. Spectral frequencies of (a) DeepResESNR, (b) DeepResESNC, and (c) DeepResESNI, in progressively deeper layers (columns). In each layer, and for all configurations, we consider Nh = 100 recurrent neurons, ρ = 1, α = 0.9, β = 0.1, ωx = 1 and ωb = 0. Results are averaged over 10 trials. Magnitudes have been normalized to ease visualization. Red arrows highlight the trend in spectral magnitudes. A. Spectral Frequency Analysis in the Here, analysis encoded leveraging how signal investigate input frequency temporal i=1 sin(φit), where φi spectral the is layers of DeepResESN. Consider tools, representation we progressively of series time deeper s(t) = (cid:80)12 is the i-th frequency in φ = [0.2, 0.331, 0.42, 0.51, 0.63, 0.74, 0.85, 0.97, 1.08, 1.19, 1.27, 1.32]. Frequency values are the same as in [18], [19]. The experiment can be summarized in three steps: (i) generate to time series s(t) of length = 1000; (ii) feed it DeepResESN and collect the output hidden states at each layer; and (iii) apply Fast Fourier Transform (FTT) [20] to extract the frequency components over time. Fig. 3 presents the frequency components at progressively deeper layers for each DeepResESN configuration. Fig. 2. Structure of the three orthogonal matrices (10 10) used in the temporal residual connections. In Fig. 3 (c), we observe that the identity configuration tends to filter out higher frequencies, with the filtering effect the ranbecoming stronger in deeper layers. In contrast, dom orthogonal configuration (Fig. 3 (a)) tends to filter out lower frequencies, while the cyclic orthogonal configuration (Fig. 3 (b)) appears to maintain frequencies relatively unchanged regardless of layer depth. These diverse behaviors highlight the importance of exploring different configurations for the temporal residual connections, as the temporal representation of the input can change depending on the specific configuration, particularly in deeper layers. IV. STABILITY ANALYSIS The dynamics of randomized, non-linear dynamical systems are crucial for assessing their effectiveness in learning and capturing relevant information. Thus, the study of such dynamics through appropriate mathematical tools may provide valuable insights into the systems behavior and operating regimes. In this section, we extend the ESP to DeepResESN and study its dynamics through the lens of linear stability analysis. k=1 Due to its hierarchical structure, the dynamics of layer in DeepResESN depend on the transformations of all previous layers and can therefore be expressed in terms of their collective states. Let us denote with {h(k)(t 1)}l = {h(1)(t 1), . . . , h(l)(t 1)} the collection of the states of all layers up to layer (inclusive) at time step 1. For the first layer (l = 1), state h(1)(t) depends on the external input x(t) and on h(1)(t 1). For layers after the first (l > 1), state h(l)(t) depends on h(l1)(t), which can be expressed as function of x(t) and {h(k)(t 1)}l1 k=1, and on h(l)(t 1). Therefore, in general, state h(l)(t) depends on the states of all layers up to (inclusive) at time step 1. We can capture these dependencies by defining function (l), the state transition function computed by the l-th layer, that encapsulates the networks dynamics up to layer l: (l) : RNx RNh . . . RNh (cid:125) (cid:124) (cid:123)(cid:122) RNh h(l)(t) = (l)(x(t), {h(k)(t 1)}l k=1) (4) Then, denoting with = (F (1), . . . , (NL)) the function that expresses the global dynamics of DeepResESN, the evolution of the system reads: : RNx RNh . . . RNh (cid:125) (cid:124) (cid:123)(cid:122) NL RNh . . . RNh (cid:125) (cid:124) (cid:123)(cid:122) NL h(t) = (cid:0)x(t), h(t 1)(cid:1). (5) Before delving deeper, let us define function ˆF to denote the iterated version of the global state transition function defined in (5). Assume an input sequence of length , denoted as sT (RNx ) and global state RNLNh . Then, ˆF (sT , h) is the global state of DeepResESN driven by external input sT with initial state h. More formally: ˆF : (RNx ) RNh . . . RNh (cid:125) (cid:124) (cid:123)(cid:122) NL ˆF (sT , h) = h, (cid:0)xT , ˆF ({x(t)}T t=1 , h)(cid:1), RNh . . . RNh (cid:125) (cid:124) (cid:123)(cid:122) NL if sT = [ ] (null seq.), if sT = (x(1), . . . , x(T )). (6) For the ESP of DeepResESN we use the definition introduced in [7] for hierarchical ESNs. In particular, we require that for any input sequence, the effect of initial conditions vanishes asymptotically. Assume DeepResESN whose global dynamics are governed by function defined in (5). Then, the ESP holds if for any input sequence of length , sT = [x(1), . . . , x(T )], and for all pairs of initial states h, RNLNh it holds that: lim ˆF (sT , h) ˆF (sT , h) = 0. (7) A. Stability of DeepResESN dynamics Here, we provide necessary condition for the ESP of DeepResESN. Following standard practice in the RC literature, we consider linearized version of the system expressed in (5). We obtain this linearization via first-order Taylor approximation evaluated around point h0 RNLNh . The linearized system reads: h(t) = JF,h(x(t), h0)(h(t 1) h0) + (x(t), h0), (8) where JF,h(x(t), h0) is the Jacobian of DeepResESN, evaluated at h0 and with external input x(t). Given the hierarchical structure of the model, JF,h(x(t), h0) can be expressed in terms of the Jacobians of each layer, which we refer to as inter-layer Jacobians. Notably, this results in the Jacobian being block matrix. For every i, = 1, . . . , NL, the inter-layer Jacobian of (i), with respect to state h(j)(t 1) and evaluated at h0, is given by: (i)(x(t),{h(k) 0 (t1)}i k=1) h(j) 0 (t1) , if j, JF (i),h(j) (x(t), h0) = 0, otherwise (9) Note that the partial derivative in (9) is zero matrix whenever < j, since the state transition function (i) at layer depends only on the states of previous layers and is independent of the states in subsequent ones. For details on its structure, see Appendix A-A and (20) in particular. The Jacobian of DeepResESN can now be expressed as follows: JF,h(x(t), h0) = = JF (1),h(1) (x(t), h0) ... JF (NL),h(1) (x(t), h0) JF (1),h(1) (x(t), h0) ... JF (NL),h(1) (x(t), h0) ... ... JF (1),h(NL) (x(t), h0) ... JF (NL),h(NL) (x(t), h0) 0 JF (NL),h(NL) (x(t), h0) , (10) where the step marked by follows from (9) and the fact that state transition function (i) at layer depends only on the states of previous layers. The linearized system in (8) is (asymptotically) stable if and only if, in the case of zero input, the spectral radius of the Jacobian, denoted by ρ(J), is strictly less than one. The spectral radius of the Jacobian plays crucial role in determining the regime under which linear dynamical system operates. More specifically, the system (without input) is stable when the magnitude of all the eigenvalues of are smaller than 1, as the system will asymptotically approach an equilibrium point. Conversely, if any eigenvalue has magnitude greater than 1, the system may exhibit instability. Theorem 1 (Necessary condition for the ESP of DeepResESN). Assume DeepResESN whose inter-layer and global dynamics are defined in (4) and (5), respectively. Furthermore, assume zero input and zero initial state. The global spectral radius of the system is expressed as: ρ(JF,h(0x, 0)) = max l=1,...,NL (l) ρ(α(l)O + β(l)W ). (11) where 0x RNx and 0 RNLNh are the zero input and the zero state vectors, respectively. Then, necessary condition for the ESP of DeepResESN is given by: ρ(JF,h(0x, 0)) < 1. (12) The proof is given in Appendix A-A. B. Contractivity of dynamics We provide sufficient condition for the ESP of DeepResESN. In RC systems, this is generally established by studying the conditions under which reservoir dynamics act as contraction mapping. This implies that, for any input sequence, the distance between two hidden states decreases at each time step by factor of 0 < < 1, where is the coefficient of contraction at each time step3. We adopt the contractivity framework of [21], including the Euclidean distance (or L2 norm) to express the distance between individual layers states, the maximum product metric for the distance between global states, and layer-wise and global contractivity definitions. Lemma 1 (Sufficient condition for contractivity of layers dynamics). Consider DeepResESN whose dynamics at layer are expressed by the state transition function (l), defined in (4). Then, sufficient condition for the contractivity of the dynamics of the reservoir at layer is given by: (l) C(l) = α(l) + β(l)(W + C(l1)W (l) ) < 1, (13) assuming (l1) to be contractive with coefficient C(l1) < 1. Furthermore, assuming C(0) = 0, for the first layer (l = 1), the input weight matrix contribution cancels out and (13) (1) simplifies to C(1) = α(1) + β(1)W < 1. The proof is given in Appendix A-B. Theorem 2 (Sufficient condition for the ESP of DeepResESN). Consider DeepResESN whose layer dynamics and global dynamics are expressed by state transition function (i) in (4) and in (5), respectively. Assume that for every = 1, . . . , NL the dynamics at layer are contractive with 3For generic state transition function , any input x, and states h, h: (x, h) (x, h) Ch h, with (0, 1). 5 coefficient of contraction C(l). Furthermore, assume the DeepResESN to have global dynamics that implement contraction with coefficient C, where: = max l=1,...,NL C(l) < 1. (14) Then, assuming the reservoir global state space is bounded, the DeepResESN satisfies the ESP for all inputs. The proof is given in Appendix A-C. C. Eigenspectrum Analysis The eigenspectrum4 of the Jacobian can reveal fundamental properties about the networks underlying behavior and dynamics. In particular, eigenvalues with magnitudes greater than one indicate potentially chaotic regime, while eigenvalues with magnitudes smaller than one suggest asymptotic stability. In this section, we analyze the eigenspectrum of DeepResESNR for progressively higher values of the spectral radius ρ (rows) and across progressively deeper layers (columns). This analysis is illustrated in Fig. 4. We observe diverse eigenspectra across different depths (Figures 4(a) and (b)), suggesting that deeper networks exhibit richer dynamics due to different layers being characterized by distinct the eigenvalues tend to concentrate eigenspectra. Notably, more around the origin as the network deepens. Additionally, Fig. 4(c) reveals stabilization effect in deeper layers: while eigenvalues fall outside the unit circle in the first layer, they are fully contained within the unit circle in the second and fifth layers. 4The eigenspectrum of matrix refers to the set of all its eigenvalues. Fig. 4. Eigenvalues of the Jacobian of DeepResESNR for spectral radii (a) ρ = 0.5, (b) ρ = 1, and (c) ρ = 2, for progressively deeper layers (columns). In each layer, we consider Nh = 100 recurrent neurons, ρ as specified in each subplot, α = 0.5, β = 1, ωx = 1, and ωb = 0. Model dynamics are driven by random input vector and random hidden state, both uniformly distributed in (1, 1). In orange the unitary circle. V. EXPERIMENTS In this section, the proposed approach is validated across memory-based, forecasting, and classification tasks for time series. Experimental setting: Model selection is carried out via random search, exploring up to 1000 configurations for each model with maximum runtime of 24h. Results are averaged across 10 random initializations, and the best configuration is chosen based on the performance achieved on the validation set. During model selection, all models employ reservoirs consisting Nh = 100 recurrent neurons. Explored hyperparameters are highlighted in Table I. Specifically, hyperparameters common to all models include the input scaling ωx, the bias scaling ωb, and the spectral radius ρ used to rescale the recurrent weight matrix Wh. For leaky variants, we explore the leaky rate τ, while for residual variants, we explore the coefficients α and β. For deep models, we explore the number of layers NL and an additional set of hyperparameters for layers beyond the first one. These will be referred to as inter hyperparameters. We also consider hyperparameter concat [False, True], which determines whether the readout is fed the states of the last layer or the concatenation of the states across all layers, respectively (see Fig. 1(b)). To ensure consistent number of trainable parameters when states are concatenated, the hidden size (i.e., the total number of reservoir units) is evenly split across layers. If an even split is not possible, the remaining neurons are allocated to the first layer. Experiments are carried out assuming all reservoirs are fully-connected. The readout is trained via ridge regression with Singular Value Decomposition (SVD) solver. The regularization coefficient λ of the readout is fixed at 0 for memory-based and forecasting tasks, while for classification tasks, we explore values in [0, 0.01, 0.1, 1, 10, 100]. Overall performance analysis: Fig. 5 provides an overview of DeepResESNs competitiveness relative to traditional shallow and deep RC, presenting the average performance achieved by each model relative to LeakyESN baseline (left) and critical difference plot ranking models based on their overall performance (right). In the left panel, we observe that DeepResESN delivers substantial improvements over traditional LeakyESN, achieving performance gains of TABLE MODEL SELECTION HYPERPARAMETERS. Hyperparameters Values concat NL ρ and inter-ρ ωx and inter-ωx ωb and inter-ωb (Leaky variants) τ and inter-τ (Residual variants) α and inter-α β and inter-β [False, True] [2, 3, 4, 5] [0.9, 1, 1.1] [0.01, 0.1, 1, 10] [0, 0.01, 0.1, 1, 10] [0.0001, 0.1, 0.5, 0.9, 0.99, 1]* [0, 0.0001, 0.1, 0.5, 0.9, 0.99, 1]* [0.0001, 0.1, 0.5, 0.9, 0.99, 1]* * Values 0.0001 and 0.99 have been explored only for memory-based tasks. 6 approximately +65.1%, +14.4%, and +17.5% for memory-based, forecasting, and classification tasks, respectively. These improvements also exceed those provided by ResESNs and DeepESNs, particularly in memory-based and forecasting tasks. The right panel further highlights DeepResESNs advantages. Our model ranks first in terms of overall performance. Notably, this advantage is statistically significant, as evidenced by the absence of any clique connecting to DeepResESN. See Section V-A, V-B, and V-C for further details on memorybased, forecasting and classification results, respectively. A. Memory-based The memory-based tasks considered are ctXOR, variation of the XOR task introduced in [23], and SinMem, inspired by the function approximation task introduced in [24]. Methodology: We use the Normalized Root Mean Squared Error (NRMSE) as the target metric. We generate time series of length = 6000, with the first 4000 time steps used for training, the next 1000 for validation, and the final 1000 for testing. Training and inference phases employ 200 time step washout to warm up the reservoir. The readout is fed the states at each time step. ctXOR: The ctXOR task evaluates the models trade-off between memorization and non-linear processing capabilities. Consider one-dimensional input time series x(t) uniformly distributed in (0.8, 0.8) and assume r(td) = x(td1)x(td). The task is to output the time series y(t) = r(td)p sign(r(td)), where is the delay and determines the strength of the nonlinearity. We consider delay = 5 (ctXOR5) and delay = 10 (ctXOR10). The non-linearity strength is set to = 2. SinMem: The SinMem task tests the models ability to reconstruct non-linear transformation of past version of the input. Given one-dimensional time series x(t) uniformly distributed in (0.8, 0.8), the task is to output the time series y(t) = sin(πx(t d)). For SinMem, we consider delays = 10 (SinMem10) and = 20 (SinMem20). input Discussion: Table II presents the results for memory-based tasks. Each DeepResESN configuration outperforms its shallow counterpart across the majority of tasks. In particular, the performance gap increases when greater long-term memorization capabilities are required due to bigger delays. Finally, performance is heavily influenced by the specific configuration employed in the temporal residual connections. Specifically, those employing either random orthogonal or cyclic orthogonal matrix considerably outperform those employing the identity matrix, leading to lower error by approximately one order of magnitude in SinMem10 and Sinmem20 tasks. B. Time Series Forecasting The time series forecasting tasks considered are Lorenz96 [25], Mackey-Glass [26] and NARMA. Methodology: Similarly to memory-based experiments, we employ the NRMSE as our target metric. Training, validation, and test splits are described individually for each dataset. At training and inference time, we employ 200 time step washout to warm up the reservoir. The readout is fed the states at each time step. Fig. 5. (left) Performance gain of each model class relative to LeakyESN, broken down by task and averaged across all task-specific datasets. For ResESNs and DeepResESNs, we consider the best-performing configuration for each dataset. (right) Critical difference plot computed via Wilcoxon test [22], summarizing the average rank (lower is better) of each model class across all tasks and datasets. Cliques, represented as solid lines, connect models with no statistically significant difference in performance. TABLE II PERFORMANCE ACHIEVED ON THE TEST SET OF MEMORY-BASED TASKS. REPORTED RESULTS REPRESENT MEAN AND STANDARD DEVIATION OVER 10 DIFFERENT RANDOM INITIALIZATIONS. WE HIGHLIGHT BASELINES AND OUR MODELS DIFFERENTLY. THE BEST RESULT IS IN BOLD. Memory-based LeakyESN ResESNR ResESNC ResESNI DeepESN DeepResESNR DeepResESNC DeepResESNI CTXOR5 (101) CTXOR10 (101) SINMEM10 (102) SINMEM20 (102) 3.60.1 8.70.3 35.90.2 37.60.1 3.60.1 7.30.9 0.20.0 12.71.4 3.60.1 6.61.0 0.20.0 11.31.3 3.60.1 8.50.4 36.30.2 37.60.2 3.70.1 6.51.0 14.81.7 24.03.8 3.20.2 4.10.2 0.50.1 1.90. 3.60.0 4.80.5 0.10.0 1.21.8 3.60.0 6.20.7 0.80.1 24.72.5 TABLE III PERFORMANCE ACHIEVED ON THE TEST SET OF TIME SERIES FORECASTING TASKS. REPORTED RESULTS REPRESENT MEAN AND STANDARD DEVIATION OVER 10 DIFFERENT RANDOM INITIALIZATIONS. WE HIGHLIGHT BASELINES AND OUR MODELS DIFFERENTLY. THE BEST RESULT IS IN BOLD. Forecasting LeakyESN ResESNR ResESNC ResESNI DeepESN DeepResESNR DeepResESNC DeepResESNI LZ25 (102) LZ50 (102) MG (104) MG84 (102) N30 (102) N60 (102) 13.90.4 33.70.6 3.30.3 8.40.7 10.71.9 16.80.6 12.20.5 33.90.6 3.10.3 17.72.0 10.20.1 17.10.5 11.60.4 34.40.5 3.20.3 17.21.0 10.20.1 15.00.1 12.70.2 33.00.3 3.20.3 8.40.7 10.30.0 17.90.5 14.70.6 34.61.2 3.10.4 5.31.1 10.81.9 16.91. 12.20.2 32.20.5 3.00.2 17.11.4 10.30.1 16.10.6 12.20.5 32.00.6 3.20.3 15.51.1 10.30.1 15.10.3 12.60.4 33.50.6 3.20.2 5.31.1 10.30.1 13.51.4 Lorenz96: The Lorenz96 (Lz) task is to predict the next state of the time series x(t), expressed as the following 5dimensional chaotic system: NARMA: Given one-dimensional input time series x(t) uniformly distributed in [0, 0.5], the NARMA task is to predict the next state of the following time series: x(t) = fi(t) = fi1(t)(fi+1(t) fi2(t)) fi(t) + 8, (15) y(t) = 0.3y(t1)+0.01y(t1) t=d (cid:88) y(ti)+1.5x(td)x(t1)+0.1. for = 1, . . . , 5. In our experiments, we focus on predicting the 25-th (Lz25) and 50-th (Lz50) future state of the time series. Thus, the task involves predicting y(t) = x(t + 25) and y(t) = x(t + 50), respectively. In order to assess the models performance with limited number of data points, we generate time series of length = 1200. The first 400 time steps are used for training, the next 400 for validation, and the final 400 for testing. Mackey-Glass: The Mackey-Glass (MG) task is to predict the next state of the following time series: x(t) = (t) = 0.2f (t 17) 1 + (t 17)10 0.1f (t) . (16) In our experiments, we focus on predicting the 1-st and 84th future state of the time series. Thus, the task involves predicting y(t) = x(t + 1) (MG) and y(t) = x(t + 84) (MG84), respectively. We generate time series of length = 10000, with the first 5000 time steps used for training, the next 2500 for validation, and the final 2500 for testing. i=1 (17) We will consider the NARMA30 (N30) and NARMA60 (N60), with (look-ahead) delay of = 30 and = 60, respectively. We generate time series of length = 10000, with the first 5000 time steps used for training, the next 2500 for validation, and the final 2500 for testing. Discussion: Table III presents the results for the forecasting tasks. In some cases, deeper architectures provide no positive, or negligible advantage. In particular, this is true for tasks with relatively low (look-ahead) delay such as Lz25, MG, and N30. However, DeepResESN shows superior performance in those tasks that require predicting states further into the future, such as Lz50, MG84, and N60. We hypothesize that tasks with lower delays may not offer sufficiently challenging forecasting problem for additional layers to be relevant. Finally, unlike what observed in memory-based tasks, there are no substantial differences between configurations. The only exception is MG84, where the identity configuration considerably outperforms the others."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "8 C. Time Series Classification The time series classification tasks considered are selection of classification problems from the UEA & UCR repository [27], [28], plus the MNIST dataset [29]. To process MNIST as time series, the images are flattened into onedimensional vectors, creating the so-called sequential MNIST (sMNIST) task. We also consider the permuted sequential MNIST (psMNIST) task, slight variation of sMNIST where random permutation is applied to pixels. Methodology: The validation set is obtained via 95 5 stratified split for sMNIST and psMNIST, and via 70 30 stratified split for the other datasets. After model selection, models are retrained on the entire original training set and evaluated on the test set. The readout is fed the states at the last time step. Discussion: Table IV presents the results for time series classification tasks. Deeper models generally outperform their shallower counterparts, with notable improvements in Adiac, Blink, FordA, FordB, and Kepler. Among the different configurations, the identity configuration often outperforms both random orthogonal and cyclic orthogonal variants. Specifically, DeepResESNR and DeepResESNC show minimal improvements over their shallow counterparts and occasionally yield worse results. In contrast, DeepResESNI achieves consistent improvements across nearly all datasets. We hypothesize that this superiority stems from the identity matrixs inherent property of preserving exact input information as it passes through multiple nonlinear reservoirs, whereas orthogonal matrices preserve only the inputs norm. Additionally, this difference may arise from how each configuration encodes temporal representations of the input, as discussed in Section III-A and graphically illustrated in Fig. 3. In particular, the (strong) filtering effect applied by the identity configuration may simplify the downstream task by providing the readout with already much diverse output frequencies. In this paper, we introduced Deep Residual Echo State Networks (DeepResESNs), novel class of deep untrained RNNs based on temporal residual connections. We empirically demonstrated how extending temporal residual connections to hierarchical architectures within the RC framework considerably enhances both memory capacity and long-term temporal modeling. We explored various configurations for the temporal residual connections and analyzed their effects in progressively deeper layers. Through the lens of linear stability analysis, we formalized necessary and sufficient conditions for obtaining stable and contractive dynamics in DeepResESN. Our model consistently outperforms traditional shallow and deep RC approaches across memory-based, forecasting, and classification tasks on time series. Future work will focus on investigating alternative configurations for the temporal residual connections, and integrating the presented framework with residual mappings along the spatial dimension. This work has been supported by NEURONE, project funded by the European Union - Next Generation EU, M4C1 CUP I53D23003600006, under program PRIN 2022 (prj. code 20229JRTZA), and by EU-EIC EMERGE (Grant No. 101070918). Computational resources were provided by Computing@Unipi, computing service of the University of Pisa."
        },
        {
            "title": "APPENDIX A\nPROOFS",
            "content": "A. Proof of Theorem 1 First, let us prove (11). Recall the global Jacobian of DeepResESN being lower triangular block matrix, as defined in (10). Due to such specific structure its eigenvalues correspond to the union of the eigenvalues of the matrices along its diagonal. More formally: eig(cid:0)JF,h(x(t), h(t1))(cid:1) = NL (cid:91) l=1 eig(cid:0)JF (l),h(l) (x(t), h(t1))(cid:1), (18) where eig() denotes the eigenvalues of its argument. Thus, the spectral radius of the Jacobian corresponds to the maximum spectral radius among those of the matrices along the diagonal: ρ(cid:0)JF (l),h(l) (x(t), h(t 1))(cid:1). ρ(cid:0)JF,h(x(t), h(t 1))(cid:1) = max (19) Consider (9), the partial derivative of the inter-layer Jacobian for layer when = = can be rewritten as: l=1,...,NL JF (l),h(l) (x(t), h(t 1)) = h(l)(t 1) (cid:2)α(l)Oh(l)(t 1) (l) +β(l) tanh (cid:0)W h(l)(t 1) (l) (l1)(x(t), {h(k)(t 1)}l1 k=1 ) + + b(l)(cid:1)(cid:3) 1 (ˆh (l) 1 (t))2 0 ... 0 1 (ˆh (l) Nh (t))2 (l) , (20) (l) (t) denotes the j-th element of the post-activation where ˆh vector ˆh(l)(t), defined as: (l) ˆh(l)(t) = tanh (cid:0)W h(l)(t 1) (l) (l1)(x(t), {h(k)(t 1)}l1 k=1 + (21) )(cid:1). Assuming zero input and zero state, (20) allows us to derive the following: (l) ρ(JF (l),h(l) (0x, 0)) = ρ(α(l)O + β(l)W ). Plugging (22) into (19) we have: ρ(JF,h(0x, 0)) = max l=1,...,NL (l) ρ(α(l)O + β(l)W ). (22) (23) Finally, following the necessary condition for the stability of linearized systems around the zero state from [21], which VI. CONCLUSIONS = α(l)O + β(l) 9 TABLE IV PERFORMANCE ACHIEVED ON THE TEST SET OF TIME SERIES CLASSIFICATION TASKS. REPORTED RESULTS REPRESENT MEAN AND STANDARD DEVIATION OVER 10 DIFFERENT RANDOM INITIALIZATIONS. WE HIGHLIGHT BASELINES AND OUR MODELS DIFFERENTLY. THE BEST RESULT IS IN BOLD. Classification Random LeakyESN ResESNR ResESNC ResESNI DeepESN DeepResESNR DeepResESNC DeepResESNI ADIAC BLINK FORDA FORDB KEPLER LIBRAS MALLAT SMNIST PSMNIST 2.70 50.0 50.0 50.0 14.3 6.67 12.5 50.0 50.0 56.03.0 50.52.7 69.21.3 60.80.9 67.03.0 76.41.1 78.73.6 70.74.0 68.32.8 59.42.4 61.04.3 64.01.4 56.71.4 55.41.9 78.41.8 87.05.8 81.61.0 83.80.3 57.31.5 61.23.8 63.91.6 57.21.0 52.82.2 75.81.5 81.83.7 77.25.7 84.00.3 61.51.5 74.74.8 69.21.3 60.80.9 67.03.0 76.41.1 78.73.6 78.01.9 79.11.4 58.42.1 71.72.3 82.31.6 66.71.2 71.32.6 74.41.9 83.71.5 80.12.0 78.70. 62.33.1 57.44.4 65.11.2 55.90.5 53.92.5 77.01.7 82.57.0 79.52.1 83.40.5 57.23.1 66.32.2 67.44.4 56.01.3 54.73.4 75.43.6 81.65.6 81.91.5 84.00.2 64.92.9 80.25.6 82.31.6 66.71.2 71.32.6 74.41.9 85.03.3 79.31.7 78.70.6 requires the global spectral radius to be smaller than 1, we establish that the necessary condition for stability in DeepResESN around the zero state is: α(l)h(l) (l) (l) + β(l)W +W (l)) (h(l) (l) [F (l1)(x, {h(k)}l1 k=1 (k)}l1 (l1)(x, {h k=1 ) )] ρ(JF,h(0x, 0)) = max l=1,...,NL (l) ρ(α(l)O + β(l)W ) < 1. (24) α(l)h(l) (l) (l) + β(l)(W h(l) Following reasoning similar to that presented in [21], [30], we observe that if the zero state is not stable fixed point for the 0 in the state transition function in (5), then there exists state neighborhood of the zero state 0 such that, given the same null input sequence, the network starting from 0 will not converge to 0. Instead, it will follow different trajectory compared to system starting from 0. Thus, the ESP condition in (7) would be violated, and (12) provides necessary condition for ensuring the ESP of DeepResESN. B. Proof of Lemma 1 The proof can be organized in two cases. case (i): shallow ResESN. We have that RNx , h(1), for = 1, DeepResESN becomes standard (1) RNh : (1)(x, h(1)) (1)(x, (1)) = α(1)O(h(1) +β(1)[ tanh(W (1)) (1) tanh(W h(1) + (1) (1) + (1) + b(1)) (1) + b(1))] α(1)h(1) (1) = (α(1) + β(1)W (1) (1) + β(1)W (1). )h(1) h(1) (1) (1) It follows that C(1) = α(1) +β(1)W for (1), which realizes contraction mapping if C(1) < 1. is Lipschitz constant case (ii): for > 1, the hierarchical structure must be taken into account. Assuming (l1) to be contraction with Lipschitz constant C(l1) < 1, we have that RNx , {h(k)}l RNh: k=1 (l)(x, {h(k)}l (k)}l , {h k=1) (l)(x, {h k=1 (k)}l k=1) = α(l)O(h(l) +β(l)[ tanh(W tanh(W (l)) (l) h(l) + (l) (l) + + b(l))] (l) (l1)(x, {h(k)}l1 k=1 (l) (k)}l1 (l1)(x, {h k=1 ) ) + b(l)) +C(l1)W (l) (l) {h(k)}l1 k=1 (k)}l1 {h k=1 (k)}l α(l){h(k)}l k=1 k=1 {h (l) {h(k)}l +β(l)(W k=1 {h (l) {h(k)}l + C(l1)W (k)}l k=1 k=1 {h (k)}l k=1) (l) = [α(l) + β(l)(W + C(l1)W (l) )] {h(k)}l k=1 (k)}l {h k=1. ) (26) (l) (l) It follows that C(l) = α(l) + β(l)(W ) is Lipschitz constant for (l). Thus, (l) realizes contraction mapping for C(l) < 1. + C(l1)W C. Proof of Theorem First, let us prove that (14) ensures global contractive dynamics. We have that RNx , {h(k)}NL k=1 RNh : (k)}NL k=1 , {h (25) (x, {h(k)}NL k=1 = [F (1)(x, {h(k)}1 [F (1)(x, {h ) ) (x, {h (k)}NL k=1 k=1), . . . , (NL)(x, {h(k)}NL )] k=1 (k)}NL (k)}1 k=1), . . . , (NL)(x, {h k=1 )] = max l=1,...,NL max l=1,...,NL max l=1,...,NL [F (l)(x, {h(k)}l k=1) (x, {h (k)}l k=1)] (C(l){h(k)}l k=1 {h (k)}l k=1) (C(l)){h(k)}NL k=1 {h (k)}NL k=1 . (27) Thus = maxl=1,...,NL and is contraction mapping if < 1. (C(l)) is Lipschitz constant for , Now, consider the iterated version of the state transition function ˆF defined in (6). Assume any input sequence of 10 [16] H. Jaeger, Short term memory in echo state networks, 2001. [17] A. Ceni and C. Gallicchio, Edge of stability echo state network, IEEE Transactions on Neural Networks and Learning Systems, 2024. [18] D. Koryakin, J. Lohmann, and M. V. Butz, Balanced echo state networks, Neural Networks, vol. 36, pp. 3545, 2012. [19] S. Otte, M. V. Butz, D. Koryakin, F. Becker, M. Liwicki, and A. Zell, Optimizing recurrent reservoirs with neuro-evolution, Neurocomputing, vol. 192, pp. 128138, 2016. [20] M. Frigo and S. Johnson, An adaptive software architecture for the fft, in Proc. ICASSP, vol. 98, 1998, p. 1381. [21] C. Gallicchio and A. Micheli, Echo state property of deep reservoir computing networks, Cognitive Computation, vol. 9, pp. 337350, 2017. [22] J. Demˇsar, Statistical comparisons of classifiers over multiple data sets, Journal of Machine learning research, vol. 7, no. Jan, pp. 130, 2006. [23] D. Verstraeten, J. Dambre, X. Dutoit, and B. Schrauwen, Memory versus non-linearity in reservoirs, in The 2010 international joint conference on neural networks (IJCNN). IEEE, 2010, pp. 18. [24] M. Inubushi and K. Yoshimura, Reservoir computing beyond memorynonlinearity trade-off, Scientific reports, vol. 7, no. 1, p. 10199, 2017. [25] E. N. Lorenz, Predictability: problem partly solved, in Proc. Seminar on predictability, vol. 1, no. 1. Reading, 1996. [26] H. Jaeger and H. Haas, Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication, science, vol. 304, no. 5667, pp. 7880, 2004. [27] A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam, and E. Keogh, The uea multivariate time series classification archive, 2018, arXiv preprint arXiv:1811.00075, 2018. [28] H. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A. Ratanamahatana, and E. Keogh, The ucr time series archive, IEEE/CAA Journal of Automatica Sinica, vol. 6, no. 6, pp. 12931305, 2019. [29] Y. LeCun, The mnist database of handwritten digits, http://yann. lecun. com/exdb/mnist/, 1998. [30] H. Jaeger, The echo state approach to analysing and training recurrent neural networks-with an erratum note, Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, vol. 148, no. 34, p. 13, 2001."
        },
        {
            "title": "BIOGRAPHY",
            "content": "Matteo Pinna received the M.Sc. degree in Computer Science from the University of Pisa, Italy, in 2024. Currently, he is Research Fellow at the Department of Computer Science, University of Pisa. Andrea Ceni received the Ph.D. degree in Computer Science from the University of Exeter, UK, in 2021. He has been Postdoctoral Research Associate at CEMPS, University of Exter. Currently, he is Assistant Professor at the Department of Computer Science, University of Pisa, Italy. Claudio Gallicchio received the Ph.D. degree in Computer Science from the University of Pisa, Italy, in 2011. Currently, he is Associate Professor at the Department of Computer Science, University of Pisa. His research interests include the fusion of concepts from deep learning, recurrent neural networks, and randomized neural systems. length , denoted as sT = {xk}T (k)}NL {h(k)}NL RNLNh : k=1 k=1 , {h k=1. Then, we have that , h)] [xT , ˆF ({xk}T 1 k=1 , )] ) , h)] [xT 1, ˆF ({xk}T 2 k=1 ) , , )] ) , k=1, ) k=1, h) ˆF ({xk}T ˆF (sT , h) ˆF (sT , = ˆF ({xk}T = [xT , ˆF ({xk}T 1 k=1 , h) ˆF ({xk}T 1 ˆF ({xk}T 1 k=1 k=1 = CF [xT 1, ˆF ({xk}T 2 k=1 C2 ˆF ({xk}T 2 k=1 . . . CT 1 ˆF (x1, h) ˆF (x1, = CT 1F (cid:0)x1, ˆF ([ ], h)(cid:1) (cid:0)x1, ˆF ([ ], = CT 1F (x1, h) (x1, CT = CT max , h) ˆF ({xk}T 2 k=1 (h(l) (l)) ) ) l=1,...,NL )(cid:1) CT D. (28) where is diameter that bounds the reservoir global state space. Observe that limN CT = 0, and that ˆF (sT , h) ) is upper-bounded by CT due to the last inequality ˆF (sT , in (28). Recalling (7), the ESP is satisfied for all inputs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, nature, vol. 521, no. 7553, pp. 436444, 2015. [2] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556, 2014. [3] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [4] Y. Bengio, P. Simard, and P. Frasconi, Learning long-term dependencies with gradient descent is difficult, IEEE transactions on neural networks, vol. 5, no. 2, pp. 157166, 1994. [5] X. Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010, pp. 249256. [6] D. Verstraeten, B. Schrauwen, M. dHaene, and D. Stroobandt, An experimental unification of reservoir computing methods, Neural networks, vol. 20, no. 3, pp. 391403, 2007. [7] C. Gallicchio, A. Micheli, and L. Pedrelli, Deep reservoir computing: critical experimental analysis, Neurocomputing, vol. 268, pp. 8799, 2017. [8] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [9] S. Chang, Y. Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui, M. Witbrock, M. A. Hasegawa-Johnson, and T. S. Huang, Dilated recurrent neural networks, Advances in neural information processing systems, vol. 30, 2017. [10] A. Ceni, Random orthogonal additive filters: solution to the vanishing/exploding gradient of deep neural networks, IEEE Transactions on Neural Networks and Learning Systems, 2025. [11] A. Ceni and C. Gallicchio, Residual echo state networks: Residual learning, recurrent neural networks with stable dynamics and fast Neurocomputing, p. 127966, 2024. [12] I. B. Yildiz, H. Jaeger, and S. J. Kiebel, Re-visiting the echo state property, Neural networks, vol. 35, pp. 19, 2012. [13] H. Jaeger, M. Lukoˇseviˇcius, D. Popovici, and U. Siewert, Optimization and applications of echo state networks with leaky-integrator neurons, Neural networks, vol. 20, no. 3, pp. 335352, 2007. [14] A. Rodan and P. Tino, Minimum complexity echo state network, IEEE transactions on neural networks, vol. 22, no. 1, pp. 131144, 2010. [15] P. Tino, Dynamical systems as temporal feature spaces, Journal of Machine Learning Research, vol. 21, no. 44, pp. 142, 2020."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Pisa, 56127 Pisa, Italy"
    ]
}