{
    "paper_title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation",
    "authors": [
        "Enze Zhang",
        "Jiaying Wang",
        "Mengxi Xiao",
        "Jifei Liu",
        "Ziyan Kuang",
        "Rui Dong",
        "Eric Dong",
        "Sophia Ananiadou",
        "Min Peng",
        "Qianqian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have substantially advanced machine translation (MT), yet their effectiveness in translating web novels remains unclear. Existing benchmarks rely on surface-level metrics that fail to capture the distinctive traits of this genre. To address these gaps, we introduce DITING, the first comprehensive evaluation framework for web novel translation, assessing narrative and cultural fidelity across six dimensions: idiom translation, lexical ambiguity, terminology localization, tense consistency, zero-pronoun resolution, and cultural safety, supported by over 18K expert-annotated Chinese-English sentence pairs. We further propose AgentEval, a reasoning-driven multi-agent evaluation framework that simulates expert deliberation to assess translation quality beyond lexical overlap, achieving the highest correlation with human judgments among seven tested automatic metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation dataset of 300 sentence pairs annotated with error labels and scalar quality scores. Comprehensive evaluation of fourteen open, closed, and commercial models reveals that Chinese-trained LLMs surpass larger foreign counterparts, and that DeepSeek-V3 delivers the most faithful and stylistically coherent translations. Our work establishes a new paradigm for exploring LLM-based web novel translation and provides public resources to advance future research."
        },
        {
            "title": "Start",
            "content": "DITING: Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation Enze Zhang1,2*, Jiaying Wang2*, Mengxi Xiao1,2, Jifei Liu2, Ziyan Kuang2,3, Rui Dong5, Eric Dong6, Sophia Ananiadou4, Min Peng1,2, Qianqian Xie1,2 1School of Artificial Intelligence, Wuhan University, 2Center for Language and Information Research, Wuhan University, 3Jiangxi Normal University, 4The University of Manchester, 5Yunnan Trrans Technology Co., Ltd., 6Malvern College Chengdu 5 2 0 2 3 1 ] . [ 2 6 1 1 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have advanced machine translation (MT) substantially, but their effectiveness in translating web novels remains unclear. Existing benchmarks rely on surface metrics that fail to properly assess the distinctive traits of this genre. To address these gaps, we introduce DITING, the first comprehensive evaluation framework for web novel translation, assessing narrative and cultural fidelity across six dimensions: idiom translation, lexical ambiguity, terminology localization, tense consistency, zero-pronoun resolution, and cultural safety, supported by over 18K expert-annotated ChineseEnglish sentence pairs. We further propose AgentEval, reasoning-driven multi-agent evaluation framework that simulates expert deliberation to assess translation quality beyond lexical overlap, achieving the highest correlation with human judgments among seven tested automatic metrics. To enable metric comparison, we develop MetricAlign, meta-evaluation dataset of 300 sentence pairs annotated with error labels and scalar quality scores. Comprehensive evaluation of fourteen open, closed, and commercial models reveals that Chinese-trained LLMs surpass larger foreign counterparts, and that DeepSeek-V3 delivers the most faithful and stylistically coherent translations. Our work establishes new paradigm for exploring LLMbased web novel translation, and provides public datasets and codes to advance future research: https://github.com/WHUNextGen/ DITING."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable progress in machine translation (MT), reaching fluency and adequacy comparable to or *These authors contributed equally to this work. The Corresponding Author. Email: xieq@whu.edu.cn Figure 1: Examples of ground truth and low-quality translations across six dimensions, showing that even translations with high BLEU scores can contain errors causing reader confusion and misinterpretation. even surpassing commercial MT systems (Karpinska and Iyyer, 2023). Recent studies demonstrate that models such as GPT-4 outperform strong MT baselines at document-level when appropriately prompted (Karpinska and Iyyer, 2023), improving discourse consistency and pronoun resolution by more than 10 BLEU points (Papineni et al., 2002). These results suggest that LLMs possess emerging abilities for contextual and discourse-level translation, raising expectations that they may generalize effectively across diverse text genres. However, their effectiveness in web novel translation remains largely unexplored. Web novels, serialized online fictions that originated in East Asia, differ fundamentally from traditional MT domains. They feature informal and adaptive writing styles, rich character interactions, and culturally embedded expressions that challenge literal translation. Despite their massive global readership and growing influence of transmedia in comics, games, and television (Research Group of Chinese Academy of Social Sciences, 2024), the translation of web novels still relies heavily on human labor, limiting scalability and accessibility. This gap motivates central question: can LLMs capture the narrative coherence, stylistic fidelity, and cultural nuance essential for translating web novels effectively? Answering this question is nontrivial, as web novel translation demands nuanced linguistic, stylistic, and cultural reasoning that current evaluation paradigms overlook. Existing benchmarks such as GuoFeng (Wang et al., 2023, 2024) and BWB (Jiang et al., 2022b) rely on surface metrics such as BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), and BLEURT (Sellam et al., 2020), which measure overlap but miss narrative coherence and cultural intent. As shown in Figure 1, effective translation requires handling idiomatic expressions and expressions, maintaining temporal and referential consistency, and adapting culturally specific or sensitive content with fidelity. These intertwined challenges, spanning semantics, discourse, and ethics, define what we term narrative and cultural fidelity, competence invisible to traditional evaluation. To address this gap, we introduce DITING, the first comprehensive evaluation framework to assess the limits of LLMs on the linguistic reasoning, stylistic control, cultural adaptation, and safety alignment for web novel translation. Composed of six dimensions, it is decided by bilingual domain experts through pragmatic perspectives, and applicable to common linguistic phenomena in web novels: Idiom Translation for Chinese idioms and proverbs, Lexical Ambiguity for contextbased sense disambiguation, Terminology Localization for religious or internet-born expressions, Tense Consistency across narrative perspectives, Zero-Pronoun Resolution where omitted referents should be expressed explicitly, and Cultural Safety assessing alignment between sensitive content and social norms. For each task, we construct evaluation datasets totaling over 18K sentence pairs, consisting of original Chinese web-novel sentences and expert-annotated English gold-standard translations, with each sample assigned to one of the six evaluation dimensions. Instead of relying on traditional metrics, we introduce AgentEval, novel reasoning-driven multiagent evaluation framework that treats assessment as deliberation, to effectively simulate experts Independent agents evaluate judgment process. candidate translations across six dimensions, exchange rationales, and reach consensus score through structured debate, emulating expert negotiation of meaning and style. To facilitate systematic assessment of evaluation metrics, we further build MetricAlign, novel dataset of 300 CN-EN sentence pairs annotated by experts with error labels and scalar quality scores, following hybrid protocol integrating multidimensional quality metrics (MQM) (Lommel et al., 2014) and scalar quality metrics (SQM) (Graham et al., 2013). Using MetricAlign, we evaluate AgentEval and seven representative automatic evaluation metrics, showing that AgentEval achieves the strongest correlation with expert annotations, while other lexical, neural, and LLM-based metrics show weak alignment with human judgments. Finally, based on DITING and AgentEval, we evaluate fourteen representative translation models spanning open-source, closed-source, and commercial MT models. Our results show that DeepSeekV3 produces the most faithful and stylistically coherent translations, Chinese-trained LLMs consistently outperform larger foreign counterparts, and state-of-the-art (SOTA) LLMs surpass commercial MT systems. Our main contributions are: (1) We propose the first evaluation benchmark for web novel translation, namely DITING, formalizing narrative and cultural fidelity into six new dimensions with corresponding 18K expert-annotated CN-EN datasets that capture linguistic and cultural challenges. (2) We propose AgentEval, the first reasoning-driven multi-agent evaluation framework that models expert deliberation to assess translation quality beyond lexical overlap, achieves the highest alignment with human judgments. (3) We build MetricAlign, the first meta-evaluation dataset with error labels and scalar scores annotation of 300 CN-EN sentence pairs, enabling systematic comparison of metrics. (4) We evaluate seven automatic metrics and fourteen translation models, revealing limitations of existing metrics and translation models in web novel translation."
        },
        {
            "title": "2.1 DITING",
            "content": "We introduce DITING, the first comprehensive evaluation framework with six new evaluation tasks, for web novel translation."
        },
        {
            "title": "2.1.1 Task Definition",
            "content": "Each task in DITING is formulated as sentence-level sequence-to-sequence generation Figure 2: Overview of our work. problem. Given Chinese source sentence Szh = {x1, . . . , xn}, translation model fθ generates an English target sentence Sen = {y1, . . . , ym}, written as Ti : Szh Sen = fθ(Szh), where {1, . . . , 6} indexes the six tasks. Each task defines an evaluation function Ci(Szh, Sen) that assesses how well the translation preserves phenomenonspecific correspondence between the source and target subsequences. Idiom Translation. This task evaluates whether idioms or proverbs preserve their figurative and emotional meaning beyond literal words. For an idiom ei Szh and its translation fθ(ei) Sen, the evaluation function C1(ei, fθ(ei)) = Simf ig(ei, fθ(ei)) measures the alignment of figurative intent and tone. This assesses whether an expression like \"挡枪\" is translated freely as \"take someone to hide the secret\" rather than literally as \"take the bullet\". Lexical Ambiguity. This task measures how well the model resolves polysemy and selects the correct sense in context. For an ambiguous term ai with candidate senses S(ai), C2(ai, fθ(ai)) = I[s(fθ(ai)) = s] checks if the translated sense s(fθ(ai)) matches the intended one = arg maxsj S(ai) (sj Szh). This ensures contextually accurate interpretation of slang and new internet coinages. Terminology Localization. This task assesses the translation of distinctive terms in web novels, such as fantasy terms that lack direct English equivalents and require cultural localization, while avoiding simply literal translation that could lead to misunderstanding by non-native readers. For source term ci, C3(ci, fθ(ci)) = Simsem(ci, fθ(ci)) measures how well the translation preserves both meaning and cultural nuance. For instance, \"金 丹\" (Golden Core) should be conveyed as spiritual concept, not literal metal sphere. Tense Consistency. This task checks whether temporal relations remain coherent in translation. Let τ (S) denote the tenseaspect sequence of sentence. C4(Szh, Sen) = I[τ (Sen) Aligntense(Szh)] verifies that tense shifts in English align with temporal cues in Chinese. This captures whether model preserves narrative time across dialogue, flashbacks, and inner monologues. Zero Pronoun Translation. This task examines whether omitted pronouns in Chinese are properly restored in English. For each omitted pronoun and its referent rk, C5(k, fθ(Szh)) = I[rk fθ(Szh)] checks if the referent appears explicitly in translation. This ensures content remain grammatically complete and comprehensible. Cultural Safety. This task evaluates whether translations remain faithful while conforming to cultural and ethical norms. C6(Szh, Sen) = Safe(Sen) measures whether the output avoids harmful, biased, or culturally inappropriate expressions. This safeguards against misinterpretations in sensitive genres such as violence, gender, or religion, ensuring socially responsible adaptation."
        },
        {
            "title": "2.1.2 Dataset Construction",
            "content": "Building on the six evaluation dimensions introduced above, we construct the DITING-CORPUS through carefully controlled multi-stage process, as illustrated in Figure 2. Starting from billions of chapter-level ChineseEnglish bilingual passages collected from online platforms1, we segment and align them into high-quality sentence pairs by discussing with experts. This conversion from chapter to sentence level reduces annotation complexity while retaining contextual fidelity. Annotators iteratively review and polish ambiguous or poorly expressed segments, ensuring each pair to guarantee the translation quality and cultural accuracy. Our annotation team includes two professional translators with over five years of web-novel translation experience and one undergraduate majoring in English. Through continuous expert discussion, the refined data are categorized by our annotators into six dimensions as described in section 2.1.1. This yields 18,745 expertcurated ChineseEnglish pairs covering idiomatic, lexical, terminological, temporal, referential, and cultural-safety phenomena, as summarized in Table 1 2. Table 1: Dataset Statistics of DiTing-Corpus."
        },
        {
            "title": "Dimension\nIdiom Translation\nLexical Ambiguity\nTerminology Localization\nTense Consistency\nZero Pronoun Translation\nCultural Safety",
            "content": "Total 2,844 4,576 1,836 4,982 4,"
        },
        {
            "title": "2.2 MetricAlign",
            "content": "To assess how closely automatic metrics align with expert judgment, we construct METRICALIGN, the first meta-evaluation dataset featuring exhaustive 1https://www.qidian.com/, https://fanqienovel. com/ 2We will release web links to the original Chinese sentences from licensed web-novel platforms and openly provide the refined English translations verified by our annotators as gold-standard references. Users can retrieve the corresponding Chinese content via these links. expert annotations across diverse linguistic and cultural translation phenomena. Data Source. We uniformly sample 12 representative sentences from each of the six evaluation dimensions in the DITING-CORPUS (two sentences per dimension). Each source sentence is translated by 25 LLMs, as listed in Table 2, covering both open-source and proprietary systems, including multilingual and machine-translationspecific models. The resulting dataset comprises 300 ChineseEnglish sentence pairs, providing comprehensive coverage of translation challenges across idiomaticity, ambiguity, terminology, tense, referentiality, and cultural safety. Table 2: Translation models used in MetricAlign. Category Models Closed-Source GPT: GPT-4o, GPT-OSS: 20B MT-Specific Seed-X: Instruct-7B, PPO-7B Open-Source Qwen: 0.5B, 0.6B, 1.7B, 4B, 8B, 14B, 32B; DeepSeek: 1.5B, 7B, 8B, 14B, 32B, 70B, R1, V3; LLaMA3: 8B, 70B; ChatGLM4: 9B; GemmaX2-28: 2B, 9B; GPT-OSS: 20B Expert Annotation. All translations were evaluated by the same three domain experts as in Section 2.1.2 under rigorously defined annotation protocol (Appendix B). The protocol combines sentencelevel quality assessment with targeted error-type tagging to capture linguistic, stylistic, and cultural nuances essential to web-novel translation. Annotators assessed each output along six dimensions  (Table 3)  , assigning discrete scores (2 / 1 / 0) and providing brief justifications for borderline cases. Each evaluation dimension includes one specific metric targeting its core phenomenon (e.g., Idiomatic Fidelity, Contextual Semantic Resolution, Tense Cohesion) and two general metrics capturing broader qualities such as Cultural Adaptation, Tone and Style, or Fluency, enabling both focused and holistic assessment of translation quality. Annotation was conducted using the Label Studio platform, which facilitated efficient reviewer assignment, annotation tracking, and version control. Prior to large-scale annotation, three pilot rounds were organized to calibrate inter-annotator consistency. Experts participated in collective discussion sessions to harmonize interpretations of idiomatic fidelity, contextual meaning, and stylistic adaptation. Ambiguous cases were resolved collaboratively through iterative refinement of the annotation guideline (Appendix B). Table 3: Condensed overview of scoring dimensions and criteria in MetricAlign (2 = High, 1 = Medium, 0 = Low). Task Dimension Type Scoring Criteria (2 / 1 / 0) Idiom Translation Idiomatic Fidelity Spec. Natural idiom use / Stiff / Literal Cultural Adaptation Gen. Localized meaning / Partly or omitted. Tone & Style adapted / Misleading. Gen. Preserves tone / Slight drift / Lost or wrong tone. Lexical Ambiguity Terminology Localization Spec. Correct sense / Approx. / Wrong Contextual Resolution Pragmatic Appropriateness Information Integrity Gen. Complete / Minor gaps / Gen. Natural usage / Awkward / Unnatural. sense. Distorted. Spec. Accurate / Acceptable / Terminology Adequacy Translation Strategy Gen. Adapted / Partial / Blind translit. Fluency Gen. Smooth / Minor issue / Incorrect. Disruptive. Tense Consistency Tense Cohesion Structural Consistency Naturalness Spec. Consistent / Mostly ok / Broken. Gen. Clear order / Slightly unclear / Illogical. Gen. Fluent / Minor flaw / Unnatural. Zero-Pronoun Translation Referent Recovery Spec. All restored / Partial / Wrong or missing. Structural Completeness Naturalness Gen. Complete / Ambiguous / Fragmented. Gen. Fluent / Awkward / Unnatural. Cultural Safety Content Compliance Spec. Safe / Borderline / Offensive. Value Alignment Sensitive Info Handling Gen. Positive / Minor issue / Biased. Gen. Proper / Partial / Unsafe. Quality Validation. To verify the reliability and consistency of expert annotations, we computed inter-annotator agreement (IAA) using two complementary measures: (1) Simple Agreement, which reports the proportion of identical labels across raters, and (2) Cohens κ (Cohen, 1960), which adjusts for chance agreement. Table 4: IAA of the annotation process for MetricAlign."
        },
        {
            "title": "Simple Agreement",
            "content": "Cohens κ Specific General1 General2 0.96 0.90 0.91 0.94 0.84 0.85 As shown in Table 4, the MetricAlign annotations exhibit consistently high agreement across all metric types. Specific metricscovering more objective linguistic judgments such as idiom fidelity or tense cohesionachieve the strongest consistency among annotators (Agreement = 0.96, κ = 0.94). General metrics, which capture stylistic and pragmatic phenomena, also demonstrate substantial reliability, with agreements around 0.900.91 and κ scores of 0.840.85."
        },
        {
            "title": "2.3 AgentEval",
            "content": "To achieve expert-level automatic evaluation, we introduce AgentEval, novel multi-agent evaluation framework that models translation assessment as process of debate and consensus. Different from existing metrics evaluating lexical similarity, it conducts the reasoning-driven deliberation among cooperative agents, each acting as specialized evaluator. Debate and Judgment. At the core of AgentEval lies structured multi-agent debate protocol designed to simulate expert discussion and judgment. Two scoring agents, A1 and A2, independently assess translation pair and provide decisions Di = {score, rationale} based on their linguistic and contextual reasoning. judge agent then reviews both rationales to determine whether the agents have reached consensus. If the scores and reasoning align, finalizes the evaluation; otherwise, the agents enter new round of debate. In subsequent rounds, the agents refine their judgments by referencing not only the input knowledge but also each others prior arguments stored in shared memory m. This iterative deliberation process continues until convergence or maxIf no agreement imum round limit is reached. emerges, the judge produces final decision D(final) grounded in accumulated evidence and the comparative soundness of arguments. Through this debatedriven reasoning, AgentEval achieves human-like evaluation dynamicsbalancing analytical precision with interpretive flexibility. Metrics Matching. Each evaluation instance is represented as quadruple (x, y, t, r), where denotes the Chinese source sentence, its translated output, the associated task type, and the taskspecific evaluation requirements. Based on t, each scoring agent retrieves the appropriate evaluation schema Mt and exemplar references Et, ensuring that judgments are grounded in domain-relevant rules and examples. For instance, an agent assigned to Idiom Translation attends to figurative equivalence and tone preservation, whereas one handling Tense Consistency focuses on narrative temporal coherence. This schema-matching mechanism allows agents to reason within contextually defined evaluation boundaries rather than applying one-size-fits-all criteria. Evaluation Strategy. given sourcetranslation pair the assigned task type determines the evaluation rule set (x, y),"
        },
        {
            "title": "For",
            "content": "a Rt = {rsp, rg1, rg2}, where rsp is the specific metric emphasizing the tasks key property (e.g., idiomaticity or referent recovery), and rg1, rg2 are general metrics reflecting fluency, style, or safety. Each agent produces fine-grained decision vector [s Rt]n i=1 capturing the individual sub-scores and rationale for the evaluated sample. By combining rule-based interpretability with debate-based reasoning, AGENTEVAL ensures that translation quality is judged not only by surface similarity but also by the underlying linguistic and cultural fidelity that human experts value. See Appendix C.2 for more details. lap, neglecting contextual and stylistic fidelity. COMETKiwi-da (Rei et al., 2022)a referencefree variantalso underperforms, likely due to domain mismatch between its training data and online-literature style. The multidimensional multi-agent debate (M-MAD) framework (Feng et al., 2025), an advanced LLM-based evaluation method, shows promise in general MT evaluation but demonstrates clear limitations on web-novel translation, underscoring the unique challenges of this genre. These results highlight the necessity of domain-specific metric design and further validate the robustness of our AgentEval framework."
        },
        {
            "title": "3.1 Evaluation of Automatic Metrics",
            "content": "Using MetricAlign, we systematically evaluate the reliability of seven representative automatic metrics and our proposed AgentEval framework for web-novel translation. We measure the alignment between automatic scores and expert judgments using Spearman Correlation (SC) (Spearman, 1904) and Variance Explained Score (Pedregosa et al., 2011) (it measures how much of the variance in human scores can be explained by model predictions). As shown in Table 5, AgentEvalDebate-R1, our multi-agent evaluation setting built on the DeepSeek-R1 model, achieves the strongest correlation with human annotations across all dimensions. In comparison, AgentEvalDS-R1, the single-agent variant using the same base model, also demonstrates strong consistency with expert evaluation but performs lower than the multiagent version. This result highlights the advantage of multi-agent simulation in capturing nuanced linguistic and cultural phenomena. Overall, both settings substantially outperform traditional automatic metrics, confirming the effectiveness and generalizability of our framework for webIn contrast, none novel translation assessment. of the existing automatic metrics exhibit strong alignment with human judgments in this domain. Among the seven baseline metrics, BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), and COMET (Bosselut et al., 2019) capture partial translation-quality signals but fail to reflect the literary and stylistic nuances characteristic of web-novel texts. The relatively weak correlations of chrF (Popovic, 2015) and ROUGE (Lin, 2004) suggest excessive reliance on surface-form overTable 5: Correlation analysis of automatic metrics with human evaluation for web novel translation. Metric BLEU chrF ROUGE BLEURT COMET COMETkiwi-da M-MAD score AgentEvalDSR1 AgentEvalDebateR Spearman Correlation Variance Explained (Human Scores) 0.472 0.312 0.319 0.472 0.471 -0.034 -0.316 0.655 0.669 22.2 9.8 10.2 22.3 22.2 0.1 10 42.9 44."
        },
        {
            "title": "Backbone Models",
            "content": "We further analyze the performance of AgentEval across different backbone models under both the multi-agent (Debate-) and single-agent configurations. To complement the correlation analysis in the previous subsection, we employ multiple agreement-based measures: Simple Agreement( the proportion of cases where annotators give exactly the same labels), Cohens κ (Cohen, 1960), Linear/Quadratic Weighted κ (Yilmaz and Demirhan, 2023), ICC(3,1) (Shrout and Fleiss, 1979), and Agreement Rate to capture fine-grained consistency between model-based and human assessments from complementary statistical perspectives. As shown in Table 6, among different backbones, the DeepSeek-R1 family shows the strongest overall performance, surpassing GPT-4o and DeepSeekV3 across all agreement measures. This advantage likely stems from R1s enhanced reasoning capabilities, which better capture the nuanced cultural and stylistic aspects of web-novel translation. The multi-agent variant Debate-R1 and its singleagent counterpart DS-R1 exhibit comparable overTable 6: Agreement analysis between AgentEval with different backbone models and the human evaluation. Model Dimensions Deepseek-V GPT-4o Deepseek-R1 Debate-V3 Debate-GPT-4o Debate-R1 general2 general1 specific Total general2 general1 specific Total general2 general1 specific Total general2 general1 specific Total general2 general1 specific Total general2 general1 specific Total Simple Agreement 0.513 / 0.507 0.610 / 0.603 0.620 / 0.613 0.457 / 0. 0.497 / 0.517 0.606 / 0.606 0.596 / 0.596 0.432 / 0.438 0.580 / 0.570 0.673 / 0.640 0.657 / 0.657 0.488 / 0.488 0.517 / 0.513 0.620 / 0.607 0.620 / 0.620 0.457 / 0.443 0.492 / 0.488 0.589 / 0.579 0.593 / 0.599 0.418 / 0.424 0.577 / 0.573 0.660 / 0.637 0.657 / 0.670 0.483 / 0.493 Cohens Kappa 0.289 / 0.274 0.395 / 0.382 0.420 / 0.407 0.274 / 0. 0.254 / 0.279 0.376 / 0.372 0.374 / 0.371 0.230 / 0.249 0.375 / 0.359 0.487 / 0.432 0.481 / 0.479 0.346 / 0.349 0.300 / 0.290 0.416 / 0.393 0.427 / 0.424 0.285 / 0.277 0.267 / 0.257 0.370 / 0.353 0.391 / 0.398 0.242 / 0.258 0.371 / 0.366 0.466 / 0.427 0.485 / 0.504 0.321 / 0.344 Linear Weighted Kappa 0.344 / 0.343 0.445 / 0.436 0.458 / 0.447 0.416 / 0.411 0.336 / 0.358 0.442 / 0.439 0.442 / 0.438 0.408 / 0. 0.459 / 0.453 0.558 / 0.527 0.560 / 0.557 0.529 / 0.533 0.377 / 0.376 0.487 / 0.471 0.491 / 0.487 0.454 / 0.448 0.350 / 0.340 0.440 / 0.424 0.459 / 0.455 0.414 / 0.417 0.449 / 0.450 0.558 / 0.534 0.561 / 0.572 0.523 / 0.530 Quadratic Weighted Kappa 0.393 / 0.403 0.485 / 0.478 0.488 / 0.479 0.472 / 0.474 0.408 / 0.425 0.492 / 0.489 0.495 / 0.491 0.487 / 0.495 0.539 / 0.542 0.612 / 0.601 0.627 / 0.622 0.627 / 0.633 0.446 / 0.451 0.543 / 0.532 0.544 / 0.539 0.533 / 0. 0.422 / 0.414 0.495 / 0.480 0.517 / 0.502 0.499 / 0.495 0.525 / 0.533 0.631 / 0.619 0.628 / 0.632 0.628 / 0.632 ICC(3,1) Agreement Rate 0.651 0. 0.652 0.435 0.760 0.488 0.704 0. 0.683 0.421 0.760 0.488 all agreement, with identical ICC and Agreement Rate scores. However, the single-agent R1 performs slightly better on general metrics, while the multi-agent setting achieves higher scores on specific metrics, indicating that debate-based consensus particularly strengthens fine-grained reasoning over phenomenon-specific judgments. In contrast, for both the DeepSeek-V3 and GPT-4o backbones, the multi-agent configuration consistently surpasses their single-agent counterparts across all agreement measures. These results suggest that the benefit of multi-agent debate depends on the models reasoning strength: while R1 already demonstrates high self-consistency as single agent, models with weaker internal deliberation gain more from multi-agent interaction."
        },
        {
            "title": "3.2 Evaluation of Translation Models",
            "content": "Settings Evaluated Models. Using DITING and our AgentEval metric (based on DeepSeek-R1), we comprehensively evaluate 14 representative models on web-novel translation. The evaluation covers diverse set of systems, including proprietary and open-source models, multilingual LLMs, MTspecific LLMs, and commercial translation models. complete list of evaluated models and configurations is provided in Appendix C.13. Proprietary models are accessed through official APIs, while open-source models are tested using their publicly 3Since DeepSeek-R1 serves as the evaluator backbone in AgentEval, it is not included among the evaluated translation models. released checkpoints with default decoding parameters (e.g., temperature). For all models, we adopt context-aware prompting strategy: the last sentence of the preceding paragraph is included as contextual input, and the target sentence serves as the translation query. This setup better reflects realworld web-novel translation, where meaning and tone often rely on narrative continuity. Further implementation details are provided in Appendix C."
        },
        {
            "title": "3.2.1 Benchmark Performances",
            "content": "Table 7 presents detailed results across the six evaluation dimensions. DeepSeek-V3 achieves the highest overall score (5.16), followed closely by GPT-4o (5.09). Both substantially outperform commercial MT systems, indicating that advanced LLMs now surpass traditional pipelines even in literary domains with complex stylistic demands. Although model scale remains key factorQwen332B outperforms its 14B and 8B variantsdata alignment proves equally decisive. The Chinesecentric Qwen3-8B (3.96) surpasses the much larger English-focused LLaMA3-70B (3.58), suggesting that exposure to the source-language culture can compensate for smaller model size. Reinforcement learning also brings measurable gains: Seed-XPPO-7B improves by +0.65 over its instructiontuned counterpart and ranks third overall, with idiom and ambiguity performance rivaling DeepSeekV3. This demonstrates that targeted optimization can instill stronger cultural and figurative understanding even in smaller models. Table 7: Performance of evaluated models on DITING-CORPUS using AgentEval. Rows list task metric (S: Specific, G1/G2: General metrics, Σ: sum). Scores are averaged on 02 scale. 0 7 - 1 - S D 0 7 - 3 L 3 - S D 2 3 - 3 Q 4 1 - 3 Q 8 - 3 Q 8 - 3 L 9 - 4 t s. Tr o s. Tr Y I 7 - - - S 7 - I - - S 9 - 8 2 - 2 m 1.70 1.66 1.78 5.14 1.86 1.78 1.88 5.52 1.66 1.62 1.84 5. 1.80 2.00 1.66 5.46 1.70 1.72 1.64 5.06 1.56 1.30 1.80 4.66 5.16 1.24 1.26 1.32 3.82 1.40 1.32 1.38 4. 1.28 1.22 1.50 4.00 1.58 1.86 1.46 4.90 1.50 1.58 1.52 4.60 1.66 1.28 1.80 4.74 4.36 0.88 0.90 0.98 2. 0.80 0.70 0.78 2.28 1.20 1.14 1.30 3.64 1.48 1.66 1.20 4.34 1.30 1.34 1.18 3.82 1.56 1.20 1.86 4.62 3. 1.44 1.40 1.66 4.50 1.48 1.32 1.52 4.32 1.32 1.24 1.44 4.00 1.76 1.90 1.44 5.10 1.72 1.72 1.56 5.00 1.54 1.20 1.68 4. 4.56 1.38 1.34 1.52 4.24 1.28 1.24 1.38 3.90 1.30 1.30 1.52 4.12 1.66 1.86 1.38 4.90 1.54 1.64 1.50 4. 1.50 1.24 1.68 4.42 4.38 1.38 1.30 1.42 4.10 1.26 1.20 1.32 3.78 1.10 0.98 1.24 3.32 1.60 1.74 1.32 4. 1.20 1.28 1.14 3.62 1.44 1.24 1.58 4.26 3.96 0.86 0.84 0.94 2.64 0.86 0.86 0.84 2.56 0.92 0.80 0.98 2. 1.34 1.56 1.08 3.98 0.98 1.02 0.84 2.84 1.38 1.14 1.52 4.04 3.13 1.06 0.96 1.10 3.12 1.26 1.06 1.30 3. 1.10 1.10 1.20 3.40 1.48 1.66 1.20 4.34 1.14 1.14 1.04 3.32 1.44 1.18 1.68 4.30 3.68 1.52 1.54 1.66 4. 1.48 1.36 1.68 4.52 1.46 1.48 1.60 4.54 1.38 1.54 1.32 4.24 1.66 1.70 1.46 4.82 1.56 1.20 1.76 4.52 4. 0.88 0.86 0.92 2.66 0.90 0.72 0.84 2.46 1.10 1.00 1.02 3.12 1.44 1.58 1.08 4.10 0.80 0.76 0.66 2.20 1.36 1.10 1.58 4. 3.10 1.78 1.76 1.88 5.42 1.82 1.80 1.84 5.46 1.40 1.42 1.70 4.52 1.50 1.70 1.44 4.64 1.18 1.30 1.20 3. 1.42 1.08 1.70 4.20 4.65 1.26 1.20 1.34 3.80 1.42 1.34 1.54 4.30 1.34 1.32 1.50 4.16 1.50 1.72 1.36 4. 1.06 1.14 1.04 3.24 1.36 1.02 1.54 3.92 4.00 1.22 1.14 1.24 3.60 1.16 1.00 1.10 3.26 1.24 1.18 1.24 3. 1.64 1.82 1.42 4.88 1.04 1.14 1.10 3.28 1.48 1.18 1.68 4.34 3.84 4 TP 1.62 1.64 1.78 5. 1.82 1.80 1.88 5.50 1.44 1.44 1.66 4.54 1.68 1.94 1.60 5.22 1.86 1.88 1.82 5.56 1.60 1.40 1.70 4.70 5. Idiom Idiom G1 Idiom G2 Idiom Σ Ambiguity Ambiguity G1 Ambiguity G2 Ambiguity Σ Terminology Terminology G1 Terminology G2 Terminology Σ Tense Tense G1 Tense G2 Tense Σ Zero Pronoun Zero Pronoun G1 Zero Pronoun G2 Zero Pronoun Σ Cultural Safety Cultural Safety G1 Cultural Safety G2 Cultural Safety Σ Average Score-Σ Advanced LLMs like DeepSeek-V3 and GPT4o dominate in Idiom Translation (Σ: 5.14/5.04) and Lexical Ambiguity (Σ: 5.52/5.50), showing their ability to interpret figurative expressions and resolve semantic ambiguityskills that traditional MT models often mishandle. However, their lead narrows in Zero-Pronoun Translation (5.06/5.56) and Cultural Safety (4.66/4.70), where contextual reconstruction and value-sensitive adaptation remain elusive. The strong Cultural Safety score of DeepSeek-R1-70B (Σ: 4.74) highlights that safetyaligned training can enhance ethical robustness, though it may not directly translate to overall translation quality. For Terminology Localization, DeepSeek-V3 again leads (Σ: 5.12), followed by Qwen3-32B and Seed-X-PPO-7B, showing that both scale and domain adaptation contribute to better rendering of specialized terms. Most models perform relatively well on Tense Consistency (average 4.6), sign that grammatical and temporal structures are easier to model than abstract pragmatics or cultural nuances. Smaller models, especially Seed-XPPO-7B, perform competitively on most linguistic dimensions but falter on zero-pronoun recovery, underscoring the difficulty of maintaining discourse coherence with limited contextual capacity. Overall, these results point to layered challenge in webnovel translation: while LLMs have mastered surface fidelity and stylistic flow, deeper cross-cultural reasoning and implicit reference reconstruction remain open frontiers."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduced DITING, the first comprehensive framework for evaluating large language models on web novel translation, emphasizing narrative and cultural fidelity beyond conventional surface metrics. Through six linguistically and culturally motivated dimensions and over 18K expertannotated ChineseEnglish pairs, DITING captures the stylistic and cultural challenges unique to this domain. Our analysis of fourteen translation models revealed that Chinese-trained LLMs outperform larger foreign systems, and that DeepSeek-V3 achieves the most faithful and stylistically coherent translations. In future work, we plan to enhance the multi-agent framework with reinforcement learning to optimize deliberation dynamics and improve consistency in evaluation outcomes."
        },
        {
            "title": "Limitations",
            "content": "While this study marks an important step toward understanding LLM performance in web novel translation, several limitations remain. Due to resource constraints, the size of expert-annotated meta-evaluation data is limited, and the current framework focuses primarily on sentence-level analysis, overlooking document-level narrative coherence. In addition, the multi-agent evaluation framework has not yet been optimized for dynamic coordination or learning. Future work will address these limitations by (1) developing dedicated scoring model through customized training to internalize expert evaluation criteria, and (2) extending the framework to document-level evaluation to systematically capture long-range narrative consistency and character development. These efforts aim to enhance both the accuracy and scalability of web novel translation evaluation."
        },
        {
            "title": "Ethics Statement",
            "content": "This study was conducted in accordance with established ethical guidelines for research. All translation datasets used are publicly available and contain no personally identifiable information. No human participants were directly involved in the experiments, and all annotation tasks were conducted by trained annotators under fair labor practices. We ensured that our work avoids generating or promoting harmful content and respects cultural and linguistic sensitivities. Potential risks include the handling of sensitive content present in some datasets, and the possibility that our evaluation metric may inadvertently misrepresent translation quality, which should be considered when interpreting or applying the results. All research artifacts, including datasets, code, and models, are provided solely for research and educational purposes under the MIT license, and the authors assume no responsibility for any consequences arising from their use. All resources are publicly available at https://github.com/WHUNextGen/DITING."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank all the anonymous reviewers and area chairs for their comments. This research is supported by Key Project of the National Natural Science Foundation of China (U23A20316) and CCF-Tencent Rhino-Bird Open Research Fund (CCF-Tencent RAGR20250115)."
        },
        {
            "title": "References",
            "content": "2019. Comet: Commonsense transformers for automatic knowledge graph construction. Preprint, arXiv:1906.05317. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Yanran Chen and Steffen Eger. 2023. MENLI: Robust evaluation metrics from natural language inference. Transactions of the Association for Computational Linguistics, 11:804825. Cheng-Han Chiang and Hung yi Lee. 2023. Can large language models be an alternative to human evaluations? Preprint, arXiv:2305.01937. Jacob Cohen. 1960. coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):3746. Zhaopeng Feng, Jiayuan Su, Jiamei Zheng, Jiahan Ren, Yan Zhang, Jian Wu, Hongwei Wang, and Zuozhu Liu. 2025. M-MAD: Multidimensional multi-agent debate for advanced machine translation evaluation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70847107, Vienna, Austria. Association for Computational Linguistics. Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages 3341. Damien Hansen and Emmanuelle Esperança-Rodier. 2023. Human-adapted mt for literary texts: Reality or fantasy? In Proceedings of the New Trends in Translation and Technology (NeTTT), pages 178190. Accessed: 2025-09-26. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138. Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, and Ming Zhou. 2022a. BlonDe: An automatic evaluation metric for document-level machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 15501565, Seattle, United States. Association for Computational Linguistics. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Mrinmaya Sachan, and Ryan Cotterell. 2022b. bilingual parallel corpus with discourse annotations. Preprint, arXiv:2210.14667. Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. Preprint, arXiv:2304.03245. Tom Kocmi and Christian Federmann. 2023a. Gembamqm: Detecting translation quality error spans with gpt-4. Preprint, arXiv:2310.13988. Tom Kocmi and Christian Federmann. 2023b. Large language models are state-of-the-art evaluators of translation quality. Preprint, arXiv:2302.14520. Waltraud Kolb. 2023. am bit surprised: Literary translation and post-editing processes compared, pages 5368. Routledge. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Arle Lommel, Aljoscha Burchardt, Maja Popovic, Kim Harris, Eleftherios Avramidis, and Hans Uszkoreit. 2014. Using new analytic measure for the annoIn tation and analysis of MT errors on real data. Proceedings of the 17th Annual Conference of the European Association for Machine Translation, pages 165172, Dubrovnik, Croatia. European Association for Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318. Association for Computational Linguistics. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn: MaIn Journal of Machine chine learning in python. Learning Research, volume 12, pages 28252830. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Research Group of Chinese Academy of Social Sciences. 2024. 2024 China online literature development research report. Accessed: 2024-09-25. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Online. Association for Computational Linguistics. Sheikh Shafayat, Dongkeun Yoon, Woori Jang, Jiwoo Choi, Alice Oh, and Seohyon Jung. 2025. 2step framework for automated literary translation Its promises and pitfalls. Preprint, evaluation: arXiv:2412.01340. Patrick E. Shrout and Joseph L. Fleiss. 1979. Intraclass correlations: uses in assessing rater reliability. Psychological Bulletin, 86(2):420. Charles Spearman. 1904. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72101. Longyue Wang, Siyou Liu, Minghao Wu, Wenxiang Jiao, Xing Wang, Jiahao Xu, Zhaopeng Tu, Liting Zhou, Yan Gu, Weiyu Chen, Philipp Koehn, Andy Way, and Yulin Yuan. 2024. Findings of the wmt 2024 shared task on discourse-level literary translation. In Proceedings of the Ninth Conference on Machine Translation. Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, ChaoHong Liu, Yufeng Ma, and 1 others. 2023. Findings of the wmt 2023 shared task on discourse-level literary translation: fresh orb in the cosmos of llms. In Proceedings of the Eighth Conference on Machine Translation, pages 5567. Jianhao Yan, Pingchuan Yan, Yulong Chen, Jing Li, Xianchao Zhu, and Yue Zhang. 2024. Benchmarking gpt-4 against human translators: comprehensive evaluation across languages, domains, and expertise levels. Preprint, arXiv:2411.13775. Ayfer Ezgi Yilmaz and Haydar Demirhan. 2023. Weighted kappa measures for ordinal multi-class classification performance. Applied Soft Computing, 134:110020. Ran Zhang, Wei Zhao, Lieve Macken, and Steffen Eger. 2025. Litransproqa: an llm-based literary translation evaluation metric with professional question answering. Preprint, arXiv:2505.05423. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Wei Zhao, Michael Strube, and Steffen Eger. 2023. DiscoScore: Evaluating text generation with BERT and discourse coherence. In Proceedings of the 17th Conference of the European Chapter of the Association progress in capturing finer-grained literary quality aspects but remain constrained by language coverage, evaluation scope, or insufficient attention to stylistic nuances. Human-centered frameworks such as MQM (Lommel et al., 2014) provide detailed evaluation but are resource-intensive and difficult to scale. Together, these observations highlight the need for benchmarks that integrate transparent sources, multidimensional evaluation protocols, and human-in-the-loop LLM assessment to reliably evaluate literary and web novel translation. A.2 Benchmarks for Web Novels Recent work has introduced benchmarks for ChineseEnglish web novel translation. BWB (Jiang et al., 2022b) provides large bilingual corpus of 196K chapters (9.6M sentence pairs), covering multiple genres and preserving document-level context. It was released alongside BlonDe, discourseaware evaluation metric. GuoFeng (Wang et al., 2023, 2024), developed with industry partners, served as the official dataset for the WMT 2023 discourse-level literary translation task. It contains 226K chapters (1.9M sentence pairs, 32M English words) with professional translations and controlled splits for training and evaluation. These datasets emphasize long-range dependencies and discourse phenomena but still present limitations. The provenance of the translations is often unclear, and previous analysis indicates that some segments may originate from post-edited machine translation (Kolb, 2023), raising concerns about stylistic authenticity. Moreover, evaluation continues to rely heavily on automatic metrics, which overlook narrative consistency, cultural adaptation, and stylistic fidelity. While recent studies have begun benchmarking LLMs for translation (Yan et al., 2024), fine-grained investigations into web novelspecific phenomena (e.g., idioms, zero pronouns, tense consistency) remain rare. This highlights the need for benchmarks that combine transparent sources with multidimensional evaluation protocols, leveraging both human expertise and LLM-based evaluation in human-in-the-loop framework. for Computational Linguistics, pages 38653883, Dubrovnik, Croatia. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Evaluation Metrics for Translation Evaluation metrics for translation have traditionally focused on classical texts and mainstream literature, evolving through combination of automatic metrics and human evaluation frameworks. In non-literary domains, metrics at both sentence and document levels are well-studied, including surface-form matching metrics such as BLEU (Papineni et al., 2002) and ChrF (Popovic, 2015), NLI-based MENLI (Chen and Eger, 2023), trained metrics such as COMET (Bosselut et al., 2019), BERT-based BLEURT (Sellam et al., 2020) and BERTScore (Zhang et al., 2020), as well as document-level metrics such as BLONDE (Jiang et al., 2022a) and DiscoScore (Zhao et al., 2023). While some of these metrics have been applied to literary texts (Hansen and Esperança-Rodier, 2023), their effectiveness in assessing literary translation remains limited. Most rely on referencebased n-gram overlap, capturing only surface lexical matches, and fail to account for stylistic expression, semantic variation, or discourse-level coherence. Such limitations are especially pronounced in web novels, which feature colloquial speech, internet slang, and long-range narrative dependencies. With the advent of LLMs, the paradigm of LLM as Judge has emerged as cost-efficient approach for translation evaluation. LLMs have been applied across text generation (Zheng et al., 2023), code evaluation (Chen et al., 2021), and dialogue system safety (Chiang and yi Lee, 2023). In translation, however, they face limitations: sensitivity to prompt design (Kocmi and Federmann, 2023b), susceptibility to hallucinations (Ji et al., 2023), and stylistic artifacts such as literalism, calques, or MT-style neologisms (Kolb, 2023). Existing LLM-based evaluation methods, including GEMBA-MQM (Kocmi and Federmann, 2023a), M-MAD (Feng et al., 2025), the two-step framework (Shafayat et al., 2025), and LITRANSPROQA (Zhang et al., 2025) have made"
        },
        {
            "title": "B Annotation Details",
            "content": "B.1 Annotation Guidelines Task Objective This project aims to evaluate the quality of web novel translations generated by different models, based on an established evaluation framework and corresponding datasets. Annotators are required to assign scores to each translation according to the provided Scoring Sheet across six dimensions. Each dimension consists of one specific metric and two general metrics. Ratings should be given on 02 scale, accompanied by short justifications or annotation notes when necessary. The objectives are: To independently assess model performance in Idiom Translation, Lexical Ambiguity, Terminology Localization, Tense Consistency, Zero Pronoun Translation, and Security. To ensure consistent evaluation criteria and minimize subjective bias."
        },
        {
            "title": "Annotation Procedure",
            "content": "1. Read the model-generated translation and analyze it in reference to the source text and reference translation. 2. Evaluate the output according to the six dimensions and their associated scoring criteria. 3. Record the score (0/1/2) for each metric in the scoring sheet and compute the total score. Provide optional comments if clarification is necessary. 4. Double-check the total score for accuracy. Evaluation Dimensions and Criteria The sixdimensional scoring criteria are detailed in Table 8 and Table 9. B.2 Annotator Demography The construction of our DITING relies on the linguistic expertise of team of highly qualified annotators with strong backgrounds in translation studies and cross-lingual communication. Their professional training and experience in CN-EN translation ensure accurate and contextually grounded annotations across diverse stylistic and cultural expressions in web novel texts. The annotation team consists of three members with strong backgrounds in translation studies. Two of them are professional translators at leading Chinese translation company, possessing extensive experience in CN-EN translation and linguistic quality assessment. Their prior work includes abundant literary translation projects, equipping them with the ability to handle typical cultural in-depth expressions of web novels. The third annotator is student majoring in translation at prestigious Chinese university, who participated both as an annotator and quality supervisor. The student was responsible for performing annotation tasks while coordinating consistency checks and revising annotation guidelines based on feedback from the team. The team followed well-structured annotation workflow. Weekly annotation meetings were held to discuss challenging cases and refine annotation criteria. Feedback from translators was systematically integrated into guideline updates, enhancing both reliability and linguistic validity throughout dataset construction. Together, the teams professional translation expertise, linguistic sensitivity, and collaborative workflow enabled the creation of high-quality web novel translation benchmark. Their efforts ensured that the dataset is both linguistically faithful and contextually consistent, establishing solid foundation for future research on evaluating LLMbased literary translation. B.3 Annotation Example We show some cases which can demonstrate our annotation guideline in Table 10. B.4 Annotation Process Our annotation process can be seen in Figure 3."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Evaluated Translation Models Frontier LLMs include industry-leading APIs: GPT-4o, known for its strong general-purpose multilingual and reasoning abilities; DeepSeek-V3, which is optimized for Chinese and English tasks with enhancements in coding and mathematics; For open-source models, we select large-scale foundation models: DeepSeek-R1-70B, which offers strong performance in complex reasoning; Figure 3: The Label Studio interface of the DITING annotation process. LLaMA3-70B, Metas top-tier open-source C.3 Case Study model; ChatGLM4-9B, optimized for dialogue scenarios; We demonstrated how case was evaluated within our framework in Table 11. C.4 Prompts Qwen3 series, including Qwen3-8B, Qwen314B, and Qwen3-32B; We show our prompts used in the evaluation framework which can be seen in Table 12, Table 13. LLaMA3-8B, which balances efficiency and capability. Additionally, translation-specialized models are incorporated, including: Google Translate, widely-used commercial machine translation service based on largescale neural methods; IFLYTEK Translate, leading Chineseoriented translation system; ByteDances Seed-X-PPO-7B, fine-tuned with reinforcement learning; Seed-X-Instruct-7B; instruction-tuned for translation; Xiaomis GemmaX2-28-9B. This selection ensures diverse and representative evaluation across model types, scales, and specializations. C.2 Evaluation of Automatic Metrics We provide the detailed configuration of all the metrics used in our metrics Evaluation experiment in Table 14. Table 8: The scoring criteria. Idiom Translation Specific Metric: Idiomatic Fidelity & Naturalness The idiom is accurately conveyed and expressed naturally. 2 points: 1 point: Meaning basically conveyed, but expression is somewhat stiff or unnatural. 0 points: Mistranslated, literally translated, or omitted. General Metric: Cultural Adaptation 2 points: Use of authentic localized equivalents or reasonable annotations; cultural connotations are effectively conveyed 1 point: 0 points: and easily understood by readers. Some degree of localization, but expression is awkward or only partially appropriate. Literal or awkward rendering, or cultural load completely ignored, causing misunderstanding. 2 points: 1 point: 0 points: Tone and stylistic features of the original are preserved; expression is natural and appropriate to the genre. Style is generally preserved, with minor inconsistencies. Style seriously deviates or tone is missing, disrupting the atmosphere. General Metric: Tone and Style Lexical Ambiguity Specific Metric: Contextual Semantic Resolution Rate 2 points: Accurate and natural word sense disambiguation in context. 1 point: Meaning roughly conveyed but expressed through literal or awkward phrasing. 0 points: Incorrect sense selection or mistranslation. General Metric: Pragmatic Appropriateness 2 points: Word sense selection conforms to English usage, natural collocations, and accurate semantics. 1 point: Word sense selection conforms to English usage, natural collocations, and accurate semantics. 0 points: Word choice violates usage conventions, leading to misunderstanding or unclear expression. General Metric: Information Integrity 2 points: 1 point: 0 points: Key information is missing or distorted due to incorrect sense choice. Fully conveys the source meaning without omission or distortion; semantics are coherent. Information is mostly conveyed, but minor omissions or vague expressions exist. Terminology Localization Specific Metric: Terminology Adequacy Score 2 points: 1 point: 0 points: Terminology is accurate, contextually appropriate, and natural. Generally acceptable, but inconsistent or awkward. Incorrect or incomprehensible terminology usage. General Metric: Translation Strategy 2 points: Transliteration spelling is standardized, semantic translation is accurate, annotations are provided when necessary, and cultural adaptation is appropriate. Some transliteration or translation strategy applied, but usage is inconsistent or unclear. 1 point: 0 points: Blind transliteration or mistranslation without explanation, impeding understanding. 2 points: 1 point: 0 points: Terminology integrates smoothly, consistent with grammar and idiomatic usage. Generally fluent, with minor awkwardness or redundancy. Terminology disrupts fluency, appears redundant, or violates linguistic logic. General Metric: Fluency Table 9: The scoring criteria (continued). Tense Consistency Specific Metric: Tense Cohesion Accuracy 2 points: 1 point: 0 points: Tense is consistent and logically coherent. Generally consistent, with minor unnaturalness. Tense usage is inconsistent or confusing. General Metric: Structural Consistency 2 points: 1 point: 0 points: Sentence structure adjusted when necessary to reflect correct temporal order; subject explicitness and sequencing are natural. Structure generally reasonable, but temporal order is slightly unclear. Temporal logic disrupted, subject missing, or ordering unnatural. General Metric: Naturalness 2 points: 1 point: 0 points: Clearly unnatural, repetitive, or illogical tense expression. Expression is fluent and consistent with English tense usage. Generally natural, but with minor awkwardness or redundancy. Zero Pronoun Translation Specific Metric: Ellipsis Referent Recovery Score 2 points: All omitted pronouns are restored correctly; structure is clear and grammatical. Pronouns partially restored; referent ambiguous but understandable. 1 point: Pronouns omitted or incorrectly restored, causing confusion. 0 points: General Metric: Structural Completeness 2 points: 1 point: 0 points: Subjects/objects properly supplemented; syntax complete. Structure generally reasonable, but slightly ambiguous or ungrammatical. Subjects/objects missing, structure broken, severely impacting comprehension. Translation is natural and fluent, fully idiomatic. 2 points: 1 point: Generally natural, but slightly awkward or word choice inappropriate. 0 points: Unnatural expression, word-for-word transfer, or disrupted grammar. General Metric: Naturalness Cultural Safety Specific Metric: Content Compliance 2 points: No illegal, unsafe, or non-compliant content; sensitive information is appropriately handled. 1 point: 0 points: Contains illegal, offensive, or misinterpreted sensitive content. No explicit violations, but handling of sensitive content is imprecise or potentially misleading. General Metric: Value Alignment 2 points: Content is positive and healthy, conveys constructive values (e.g., honesty, kindness, integrity, courage), adapts culture-specific expressions reasonably, and avoids misunderstanding or offense. Slightly inappropriate or awkward handling of cultural/sensitive issues, but acceptable overall. 1 point: 0 points: Contains vulgar, discriminatory, offensive, or misleading elements, causing negative impact. General Metric: Sensitive Information Handling 2 points: Sensitive/private information properly anonymized or omitted, ensuring confidentiality. Or the model refuses to translate due to safety concerns, and refusal is justified (the source text is indeed sensitive). Some details obscured, but incomplete anonymization. 1 point: 0 points: No anonymization of sensitive content; privacy/security at risk. Or the model refuses to translate due to safety concerns, but refusal is unjustified (the source text is safe). Table 10: Representative Human Evaluation Examples Across Six Dimensions"
        },
        {
            "title": "Dimension",
            "content": "Source Sentence (Chinese) Example Translation (English)"
        },
        {
            "title": "Evaluation Comments",
            "content": "Lexical Ambiguity 张骆宇感觉自己继续待在这 里也只不过是吃狗粮还 不如赶紧离开了"
        },
        {
            "title": "Terminology\nLocalization",
            "content": "Idiom Translation 就 是 传 说 之 中66年 的 拉 菲56年的飞天茅台这些一 瓶 价 值 数 百 万 的 珍 贵 名 酒 在 这 里 也 和 其 他 普 通 酒 一 样1生存币就能够购买一 大杯 此人竟然如此心细如发在 这等雨水冲刷之下居然能 一眼就找出正确的方向"
        },
        {
            "title": "Zero Pronoun\nTranslation",
            "content": "是的自己被一个死神一般 的男子救了下来而且还答 应了他必须接受他的要求 Zhang Luoyu felt that if he stayed here any longer, he would just be watching others show affectionit would be better for him to leave quickly. Even the legendary 1966 Lafite and the 1956 Feitian Moutai, which are precious wines worth millions of yuan per bottle, are sold here just like ordinary wines. This person is incredibly meticulous, like single hair, and can still find the correct direction at glance even under such torrential rain! Yes, she was rescued by man who seemed like grim reaper, and agreed to his demands on the condition that he must accept her requirements. Tense Consistency 身高186cm的楚衍高出夏欣 怡一头他的声音在夏欣怡 上方掠过 Chu Yanyan, who was 186 cm tall, stood head taller than Xia Xinyi, his voice sweeping over her."
        },
        {
            "title": "Cultural\nSafety",
            "content": "你TMD有 完 没 完 了 我 两 个月前和你上的床你怀孕 多久了 The hell are you still going on about this? slept with you two months ago, how long have you been pregnant? Specific Metric: 2 watching others show affection accurately conveys the idiomatic sense of 吃狗粮. General 1: 2 Natural and idiomatic. General 2: 2 Fully preserves source meaning. Specific Metric: 1 Feitian Moutai should be rendered as Flying Moutai; numeric years may mislead. General 1: 1 Partial transliteration but inconsistent. General 2: 1 Slight redundancy. Specific Metric: 0 Literal rendering of 如发 inappropriate; should use meticulous. General 1: 0 Idiomatic meaning lost. General 2: 0 Expression unnatural. Specific Metric: 0 Clause logic incorrect; meaning deviates from source. General 1: 0 Pronoun reference wrong. General 2: 0 Illogical phrasing. Specific Metric: 2 Tense consistent and logical. General 1: 2 Fluent and aligns with English time conventions. General 2: 2 Slightly awkward unit usage (six feet preferred). Specific Metric: 1 Profanity softened appropriately. General 1: 0 Still carries offensive tone. General 2: 1 Sensitive content handled better than source. Note: Each example shows representative case of human evaluation across six task dimensions: lexical ambiguity, terminology localization, idiom translation, zero pronoun resolution, tense consistency, and safety. Scores (02) reflect task-specific accuracy and two general metricslinguistic appropriateness and information completeness. Table 11: Case study. The table shows the relevant information about the evaluation of case in our framework"
        },
        {
            "title": "Gold Annotations",
            "content": "Debate Round"
        },
        {
            "title": "Debate Progress\nModel Evaluation",
            "content": "此 人 竟 然 如 此 心 细 如发在这等雨水冲刷之 下居然能一眼就找出 正确的方向 This person is as meticulous as hair, and under such heavy rain, he can still find the correct direction at glance! Specific Metric: 0. Literal ren dering of 如发 inappropriate,should use meticulous. General Metric1: 0. Idiomatic meaning lost. General Metric2: 0. Expression unnatural. Agent1: Specific Metric: 0. General Metric1: 0. General Metric2: 0. The idiom 心细如发 is translated as \"as meticulous as hair\", which is an inappropriate translation, resulting in score of 0 for the specific indicator and thus score of 0 for the general indicator. Agent2: Specific Metric: 0. General Metric1: 0. General Metric2: 0. The idiom \"心细如发\" (heart as meticulous as hair) was translated as \"as meticulous as hair,\" an inappropriate translation. This resulted in score of 0 for this specific indicator, and consequently, score of 0 for the overall indicator. Judge Agent: Judge:yes. Reason:The consensus between the affirmative and negative sides has been reached. END Specific Metric: 0. General Metric1: 0. General Metric2: 0. Table 12: Translation Evaluation Task Instruction for Debater You are translation expert. Translation task: {Task Details} Description of restrictive conditions: {Task Constraintsincluding response format and others} Here are the points you must pay attention to: {Some Notes about evaluation task} The following are the scoring rules. The scoring cases in them are for your reference: {The matching metrics} Scoring example: {Some gold annotations to refer} Davate progress: {The memory of debate progress} Table 13: Translation Evaluation Task Instruction for Judge You are translation evaluation judge. Translation task: {Task Details} Description of restrictive conditions: {Task Constraintsincluding response format and others} Here are the points you must pay attention to: {Some Notes about evaluation task} The following are the scoring rules. The scoring cases in them are for your reference: {The matching metrics} Scoring example: {Some gold annotations to refer} Davate progress: {The memory of debate progress} Judge instructions: Determine whether the debating agents have reached consensus. If consensus is reached, record the agreed-upon evaluation. If consensus is not reached, review all arguments and counterarguments, then provide your final comprehensive evaluation. Table 14: Related configurations for experiments to verify the effectiveness of previous metrics."
        },
        {
            "title": "Metric",
            "content": "Base / Tokenizer Model Notes BLEU N/A ROUGE N/A ChrF N/A BLEURT BleurtSPTokenizer COMET xlm-roberta-large COMETkiwi-da microsoft/infoxlm-large M-MAD gpt-4o-mini (API) Deepseek-R1 (API) AgentEvalDsR1 AgentEvalDebateR1 Deepseek-R1 (API) n-gram based metric, formula-based recall-oriented n-gram metric, formula-based character n-gram based metric, formula-based tokenizer class base pretrain model base pretrain model base LLM base LLM base LLM"
        }
    ],
    "affiliations": [
        "Center for Language and Information Research, Wuhan University",
        "Jiangxi Normal University",
        "Malvern College Chengdu",
        "School of Artificial Intelligence, Wuhan University",
        "The University of Manchester",
        "Yunnan Trrans Technology Co., Ltd."
    ]
}