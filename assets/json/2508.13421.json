{
    "paper_title": "Virtuous Machines: Towards Artificial General Science",
    "authors": [
        "Gabrielle Wehr",
        "Reuben Rideaux",
        "Amaya J. Fox",
        "David R. Lightfoot",
        "Jason Tangen",
        "Jason B. Mattingley",
        "Shane E. Ehrhardt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit."
        },
        {
            "title": "Start",
            "content": "Virtuous Machines: Towards Artificial General Science Gabrielle Wehr1, Reuben Rideaux1,2,3, Amaya J. Fox4, David R. Lightfoot1, Jason Tangen4, Jason B. Mattingley3,4,5, Shane E. Ehrhardt1* 1*Explore Science, Brisbane, Australia. 2School of Psychology, The University of Sydney, Sydney, Australia. 3Queensland Brain Institute, The University of Queensland, Brisbane, Australia. 4School of Psychology, The University of Queensland, Brisbane, Australia. 5Canadian Institute of Advanced Research (CIFAR), Toronto, Canada. *Corresponding author(s). E-mail(s): shane.ehrhardt@explorescience.ai; Keywords: artificial intelligence, scientific method, autonomous knowledge generation, artificial general science Funding: This work was funded by Explore Science. The workflows and algorithms are proprietary to Explore Science. Competing Interests: Authors affiliated with Explore Science are employees of the company. 5 2 0 2 9 1 ] . [ 1 1 2 4 3 1 . 8 0 5 2 : r Virtuous Machines: Towards Artificial General Science"
        },
        {
            "title": "Summary",
            "content": "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that domain-agnostic, agentic AI system can independently navigate the scientific workflow from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit. Virtuous Machines: Towards Artificial General Science Scientific discovery constitutes an ongoing development of explanatory theories that advance understanding and prediction of reality 1,2; contributing to technological and social progress. Throughout history, our capacity to formulate and test theories about the universe has evolved from early philosophical inquiry (Plato 3 and Aristotle 4) into modern scientific investigation. Fundamentally, conjecture and criticism form the basis for scientific advancement, where theories face rigorous testing against empirical evidence and competing explanations 5,6. Scientists have achieved remarkable breakthroughs through this approach; however, the scientific endeavour operates within inherent human cognitive constraints. Developing better explanatory theories 1,2 requires researchers to synthesise existing knowledge, formulate testable hypotheses, design rigorous experiments, and interpret results within broader theoretical frameworks. As research output and complexity grow exponentially 7,8 (>2.8 million/yr since 2022; 5.6% growth 9), researchers face increasing cognitive and practical limitations in their ability to synthesise existing knowledge and discover novel insights 10. Individual researchers typically report reading 200 300 articles per year, yet many fields now produce thousands annually, suggesting that complete coverage and awareness of disciplinary literature may no longer be practically achievable 11. Hypothesis generation increasingly occurs within narrowed conceptual spaces constrained by specialised training, while experimental design can suffer from limited crossdisciplinary methodological exposure. Such limitations may affect scientists capacity to develop unifying explanatory frameworks that transcend disciplinary boundaries and address fundamental questions the better explanations that can drive scientific progress 1,12. Computational approaches offer the potential to augment human research capabilities, while alleviating some of these limitations within the traditional scientific process. Early theoretical work in chaos theory 13 and computational principles 14 laid essential foundations for algorithmic processing of complex scientific problems. Today, AI applications are transforming scientific discovery across multiple domains from chemistry 15, synthetic biology 16,17, materials science 1820, mathematics 21,22, and algorithm development 23; accelerating research productivity and discovery in these areas. For example, AlphaFold has enabled accurate structure prediction for many proteins within hours 17, substantially accelerating downstream research 24. However, these specialised AI systems operate within narrow scientific domains, requiring significant human expertise and/or explicit programming to solve predefined problems. Modern transformer architectures 25, combined with large text corpora and scaling of computational infrastructure 26, have enabled the development of large language models (LLMs) which demonstrate broad scope versatility, general reasoning, and fluency in scientific discourse across multiple domains 2730. Agentic frameworks leverage these capabilities by integrating LLMs within autonomous architectures capable of goaldirected planning, tool use, and environmental feedback 31. These systems differ fundamentally from previous AI approaches through their capacity to navigate key phases of scientific inquiry within unified architecture, including hypothesis generation 3234, experimental design and execution 3537, manuscript writing 38, and paper evaluation 39. The first complete agentic pipeline for in silico computational research 38 autonomously generated machine learning research from concept to manuscript, with one such article passing peer review for inclusion at scientific workshop 40. Current end-to-end agentic scientific discovery frameworks continue to operate primarily in simulated or abstract digital environments 4045, typically producing outcomes with predictive power but lacking explanation of underlying causal mechanisms. While these predictions can inform subsequent theories, the absence of direct mechanistic insight constrains hypothesis development and generalisable principles 1,46. Systems that both generate predictions and transparently articulate causal reasoning would significantly advance autonomous scientific discovery by contributing to explanatory frameworks. Embodied AI offers pathway to address the limitations of previous (purely computational) approaches, as it integrates perception, action, and reasoning into agents within environments that provide direct feedback 4749. As result, the system can test hypotheses via physical experimentation (utilising the foundation provided by online platforms that interact with humans, through to manipulation of robotic appendages 35) and collect results 1,46 to support or refute proposed explanatory models. Systems have previously developed such potential in chemical discovery 3 Virtuous Machines: Towards Artificial General Science context by automating certain chemical experimentation tasks (e.g., physically mixing reagents and observing results) 50,51, but these systems lacked the capacity for autonomous hypothesis formulation or refinement of understanding based on experimental results. Here we explore this potential with the goal of developing an autonomous end-to-end scientific discovery pipeline capable of conducting real-world experiments. To this end, we implemented an agentic system incorporating hypothesis generation through experimental design, physical experimental implementation, data analysis, result interpretation, theory refinement, visualisation, and reporting. Completion of full scientific study required on average 17 hours processing $114 USD per research project (not including the time and averaged total marginal cost of $4,500 USD for the current experiment). We selected cognitive human participant payments of science as the validation domain based on our expertise and the fields established frameworks for remote experimentation. Given pre-validated cognitive paradigms testing visual working memory (VWM) 52, mental rotation 53, and imagery vividness 54; we tasked the system with completing three lines of research inquiry. Study 1 conducted controlled intervention, collecting new data; Studies 23 analysed the same 288-participant dataset from Study 1 with distinct hypotheses and analyses. Study 1 examined whether VWM precision and mental rotation performance share representational constraints, finding no correlation between individual performance patterns despite both tasks showing expected difficulty effects; attributed to the established reliability paradox 55. Study 2 explored whether imagery vividness influences how previous stimuli bias current perception and memory, finding that individuals with stronger imagery showed no greater carryover effects between trials, challenging theories that imagery and perception rely on common processing mechanisms. Study 3 investigated whether the precision of VWM predicts broader spatial reasoning abilities, finding negligible relationships and suggesting that apparent connections between visual-spatial tasks reflect general cognitive factors rather than specific shared processes. Together, these results demonstrate the feasibility of AI-driven empirical research, and steps toward an embodied AI framework that navigates all the key components of experimental scientific workflows."
        },
        {
            "title": "Methods",
            "content": "The end-to-end pipeline includes: (1) hypothesis formulation engine that identifies potential research questions and testable predictions by searching and validating novelty, breakthrough potential, and feasibility; (2) an experimental protocol engine that designs methodologies, presented as pre-registration report following Open Science Framework guidelines 56, and includes preliminary power analyses as required; (3) an implementation engine, currently interfaced with platforms for cognitive science; (4) data analysis engine that designs and executes transparent processing pipeline, covering raw data cleaning, outlier analysis, statistical testing, and interpretation of outcomes; (5) scientific decision-making, specifically synthesising and analysing experimental outcomes through inference frameworks to determine follow-up experiments and/or studies; (6) visualisation engine which designs and constructs set of figures and tables to illustrate results collated across experiments; (7) drafting of complete manuscript incorporating visualisations and validated citations; (8) peer-style evaluation; and (9) construction of final formatted manuscript. It achieves key goal by bridging discovery from in silico computational domains, to the real world, enabling the system to conduct empirical testing of hypotheses with experimental interventions on human participants, and to perform detailed analyses of complex, noisy real-world data. While demonstrated here through cognitive psychology experiments, the architecture employs domain-general principles designed to be applicable across diverse scientific fields and achieves fundamental goal of the emerging self-driving-laboratory paradigm 57,58. Multi-Agent System Architecture The system leverages hierarchical multi-agent architecture 59 to autonomously produce scientific research through the coordinated and collaborative efforts of task force of specialised AI agents (Figure 1). Unlike deterministic pipelines, each agent functions as an autonomous entity capable of receiving inputs, applying domain-specific reasoning, and producing outputs 4 Virtuous Machines: Towards Artificial General Science that advance the investigation. Within the dynamically layered network of agent interactions, consisting of orchestrators (agents with the ability to coordinate and create further sub-agents for themselves) and specialists (agents excelling in honed skillsets, such as coding, troubleshooting, or review), single top-level orchestrator (the master agent) coordinates the entire scientific workflow from beginning to end. This structure results in an emergent cascade of expertise and directed attentional flows, key to successfully navigating the unpredictable course of real-world experimentation 59,60. Figure 1: Simplified network architecture of the autonomous scientific discovery system. Directed graph illustrating the information flow and functional relationships between agents. The master agent (purple) coordinates the core scientific workflow agents (green) including method, data analysis, and visuals. Expansion to subagent modules (shades of brown) provides domain-specific capabilities, and interactions with specialist agent pathways (blue) handle specialised tasks including coding, review, troubleshooting, and inspection processes. The stacked panel agent boxes indicate agents completing tasks in parallel, while the dotted connections to blank boxes represent further agent lineages in the system not shown here for clarity. Arrows indicate bidirectional data flow between modules and across hierarchical levels. The distributed architecture enables offloading of complex research tasks while maintaining coherent experimental narratives through the centralised master coordinator. The system operates as an integrated modular workflow, where the master agent coordinates second-tier orchestrators (e.g., method agent and data analysis agent), each responsible for specific module of the research project, including idea generation, methodological design, realworld implementation, data analysis, experimental re-evaluation, visuals creation, manuscript preparation, peer review, and document construction. Each modular agent functions both autonomously and as coordinated part of the whole, maintaining independent reasoning threads while remaining responsive to the orchestrated scientific workflow. The orchestrators manage bidirectional information flows with other agents, validating the outputs from each stage to inform subsequent processes with the intention of maintaining methodological consistency, information integrity, and scientific rigour throughout the investigation. 5 Virtuous Machines: Towards Artificial General Science The framework accommodates multiple operation modes along continuum from autonomous investigation to human-in-the-loop machine learning collaboration models 61. When running autonomously, the system progresses independently from initial hypothesis to completed manuscript, with each agent making decisions guided by disciplinary standards and outputs from preceding stages. Collaborative modes, on the other hand, allow researchers to provide strategic input at specific intervention points, such as suggesting publications for consideration, bounding decision spaces according to their domain expertise or research priorities, specifying methodological constraints, or outlining visualisation preferences. Researchers can also review and refine all outputs produced by the agents at any stage, providing means of external quality control throughout the process. The system facilitates these collaborative capabilities without sacrificing its end-toend functionality, ensuring adaptability across varied research contexts and collaboration models. This flexible architecture enables researchers to deploy the system according to their specific needs from delegating discrete tasks to commissioning complete investigations with minimal supervision. Human-Inspired Cognitive Operators LLMs exhibit broad capabilities 62 yet typically struggle with planning over extended durations and self-verification 63,64. To address these limitations, we established foundational cognitive control framework for the system comprising four operators derived from psychological science abstraction, metacognition, decomposition, and autonomy (Figure 2). These operators coordinate planning, tool use, monitoring, evaluation, and refinement across research workflows, serving as computational analogues of the human executive functions that facilitate complex multi-stage inquiries. Each operator draws upon and extends established techniques, combined within the multi-agent system to support empirical investigation with minimal human oversight. Abstraction. The process of focusing on general patterns rather than instance-specific details 65,66 was operationalised as knowledge induction by enabling agents to develop their own heuristics and instructions rather than constraining them with predetermined directives. Concretely, this involved initial elicitation of latent background premises 67, self-driven exploration of problem scope (conceptually related to the Self-Ask method 68), and automated instruction generation (as validated previously 69). By beginning with universal principles, the system maintains broader conceptual search space for potential scientific insights. This implementation mirrors how human scientists maintain conceptual flexibility when developing novel theories 70, allowing exploration across disciplinary boundaries that might otherwise be constrained by specialised training. Metacognition. Awareness and regulation of ones own thinking processes 71,72 was operationalised at two levels, individual and collective, to assess and refine agent reasoning. While frontier LLMs inherently employ forms of internal test-time compute that dynamically scale with task complexity, the system implements explicit self-evaluation protocols that assess evidence quality, logical coherence, and rigour. At the individual level, this was implemented through self-reflective chains of thought 73, enabling each agent to interrogate its underlying assumptions prior to reaching conclusions. At the collective level, agent groups developed awareness of their joint thinking through reflective process operating on all agents reasoning traces, similar to Tree of Thoughts inference 74, but across several different agents and utilising an external Agentas-a-Judge 75 to assess, refine, and arbitrate those traces to align the group on decided path. These structured self-reflection mechanisms enhance accuracy in complex reasoning tasks 73,76 and facilitate transparent documentation of the evaluation processes. Decomposition. The breaking down of complex problems into more manageable components 77,78 was operationalised in the framework as explicit structuring of the solution search space. This decomposition enhances the systems capacity to manage the intricacy of multi-stage scientific workflows while maintaining precision at each step. Specifically, parameterisation of logical reasoning steps, conceptually aligned with least-to-most prompting 79, identifies constituent task components. This improves the tractability and transparency of multi-stage scientific workflows and enables verification and refinement of each component to maintain step-level precision. In 6 Virtuous Machines: Towards Artificial General Science addition, the systems recursive divide-and-conquer agentic architecture facilitates on-demand subdivision of effort as required, providing flexibility to adapt to challenging tasks 80, thereby improving reliability in the production of the required scientific deliverables. Autonomy. Self-directed goal pursuit 81,82 was implemented in orchestrator agents as local decisionmaking on tasks, constrained by explicit system objectives. Each orchestrator independently works to complete assigned goals by iterating through propose-validate-refine process akin to the Self-Refine algorithm 83. Iteration was governed by three policies: initiation, replanning, and termination. Initiation of sub-agents was invoked upon request of the agent to assist with task completion. Replanning was triggered when validation failed or marginal improvement on acceptance tests fell below patience threshold. Termination occurred either when validation and quality acceptance checks were all satisfied, or when pre-defined recursion limits were exhausted. Within each iteration, validation of the agents proposition utilised appropriate external tools where possible to assist with self-correction (shown previously to improve performance 84). These task feedback signals were then synthesised using framework analogous to Reflexion 85 and incorporated by the agent to inform subsequent propositions. The iterative self-editing continued until the stopping rules were met, at which point the agent returned its work to its orchestrator. Cognitive Offloading and Dynamic Memory Humans navigate complex tasks utilising sophisticated memory systems that can: i) hold and manipulate information in working memory 86,87, ii) selectively filter relevant details for the task at hand 88,89, and iii) offload information to external resources when internal capacity is exceeded 90. For the multi-agent system to maintain coherence over long periods, we emulated these capabilities in the system through the implementation of dynamic Retrieval-Augmented Generation (dRAG) system (comparable to DRAGIN 91) and construction of retrievable artifacts. Extending existing RAG architectures 92, the d-RAG provides dynamic memory which augments each agent with the cognitive flexibility, prior knowledge, and specific information necessary to carry out its task. Instead of using the same reference material regardless of context, the d-RAG creates and evolves specialised knowledge repositories for each research direction traversed. It functions analogously to how researchers develop domain-specific expertise through targeted literature engagement, and access knowledge during scientific inquiry, combining working memory with cognitive offloading to external resources. The d-RAG forms the core of multi-tier search engine tool accessible to the specialist archivist agent, which assists orchestrators requiring real-world information, reducing reliance solely on trained LLM knowledge that may be prone to factual inconsistencies 93. The search engine facilitates three depths of inquiry: (1) broad academic database searches via the APIs for Semantic Scholar 94, OpenAlex 95, and PubMed, (2) within-text multi-article d-RAG queries (akin to PaperQA-2 96), and (3) paper-specific question-answering. The system progressively builds its knowledge base by processing retrieved academic papers in response to agents queries, discarding irrelevant retrievals to maintain focused knowledge representations tailored to the specific research question. In addition, by allowing the system to offload complexity to specialised components and file artifacts, concise and compact representation of the overall research state can be maintained at all points. 7 Virtuous Machines: Towards Artificial General Science Hierarchical framework of cognitive agency levels. Concentric layers repFigure 2: resent ascending levels of agent sophistication, from basic retrieval mechanisms to advanced metacognitive capabilities. Each level encompasses the functionalities of those beneath it while introducing emergent properties. Retrieval forms the foundational layer, providing access to external information and internal memory stores. Abstraction enables pattern recognition and generalisation beyond specific instances. Metacognition introduces self-monitoring and strategic control of cognitive processes. Decomposition allows complex problems to be systematically partitioned into manageable components. Autonomy confers goal-directed behaviour independent of external guidance. Collaboration represents the highest level, enabling coordinated multi-agent interaction and collective problem-solving. Mixture of Agents To increase robustness of the system across the various scientific tasks and unique challenges posed by each, we employed Mixture of Agents (MoA) approach leveraging complementary strengths of different frontier LLMs 97. Though MoA may become less necessary as model intelligence advances, current training data choices and reinforcement learning by human feedback processes predispose models to inherent biases that can limit models scientific utility. As such, the best performing models for specific aspects of the system were selected to operate conjointly, enabling them to collaboratively accomplish complex tasks that proved highly challenging for any individual model to complete alone. The frontier models utilised include Anthropics Claude 4 Sonnet, OpenAIs o3-mini & o1, xAIs Grok-3, Mistrals Pixtral Large, and Googles Gemini 2.5 Pro. Core Functional System Components Idea Generation Frontier LLMs encode high-dimensional representations of knowledge through training on vast text corpora 98, enabling interpolation across distant scientific domains 62 and disparate concepts such as quantum entanglement and photosynthetic energy transfer to produce inquiries that may lie in unexplored interstices of existing research. Recent evidence of emergent reasoning in LLMs 99 and their ability to blend conceptual domains 100 supports this notion, suggesting that latent spaces encode abstractions conducive to creative synthesis. The vector-space abstraction enabling transfer in LLMs may be functionally analogous though not necessarily architecturally identical to the neural abstraction supporting human task generalisation 101. Seminal work in psychology suggests transfer between learning contexts requires shared structural elements 102,103 and exemplified the notion of abstraction in cognition 104,105, demonstrating how mental schemas facilitate knowledge transfer across disparate domains. Similarly, LLMs may interpolate between abstracted representations of their training data to produce novel information in new contexts 8 Virtuous Machines: Towards Artificial General Science other than the original domains 106,107. Although LLM-generated ideas are novel in that they are yet to be explored in human documentation, they fundamentally represent an amalgam of current human knowledge and are thus constrained by the models training data 108. The system here overcomes this limitation through empirical hypothesis testing, as each completed research idea serves as basis from which the system can ideate further scientific enquiry (Figure 3). By establishing recursive learning lineage that continually extends its knowledge beyond the original LLM training corpus, it facilitates the potential to yield truly novel hypotheses. Figure 3: Iterative experimentation cycles allow the system to extend beyond trained-on knowledge. Schematic representation of the self-directed research process executed by the autonomous discovery pipeline. Each circle represents complete experimental iteration, with transitions between cycles driven by hypothesis refinement and knowledge accumulation. The Internal/External Seed initiates the research trajectory, establishing initial hypotheses or responding to external queries. Progressive cycles demonstrate the capacity for autonomous experimental design, execution, and interpretation, with each iteration informed by previous outcomes. The expanding experimental space explored through successive iterations is characteristic of open-ended scientific discovery. To perform ideation, the master orchestrator of the system delegates to an idea agent responsible for the formulation of research hypotheses. For the results presented here, we provided seed guidance to the idea agent by specifying the available cognitive tasks and questionnaires, constrained by ethical approval requirements and our domain expertise to ensure validation of each research stage. Such guidance is optional, and if not provided, the agent instead independently targets research fields it considers relevant as the basis for subsequent ideation. The idea agent coordinates several further agents including review agent which evaluates ideas, novelty agent which communicates with an archivist agent to perform literature searches via academic databases, and feasibility agent that assesses methodological constraints and implementation requirements. This multi-agent collaboration aims to ensure that the ideas are scientifically sound, contribute new knowledge, and can be implemented successfully, with ideas passing all checks being constructed into multifaceted idea framework, which extends that used in previous work 38. The finalised ideas are ranked through multi-stage, multi-model tournament process, ultimately identifying single best idea recommended for further investigation. The full ideation process is summarised in Figure 4. 9 Virtuous Machines: Towards Artificial General Science Figure 4: Three-phase ideation process for hypothesis generation. Schematic representation of the iterative scientific ideation workflow implemented to develop novel research questions. Phase 1 (Generation) produces unique, ranked idea suggestions by brainstorming initial concepts, removing redundancies, filtering impractical proposals, ranking by scientific merit, and re-ranking to refine prioritisation. Phase 2 (Formulation) develops the suggestions into detailed research ideas through expansion of conceptual scope, formulation of testable hypotheses, review to interrogate validity, and iterative improvement. Phase 3 (Validation) yields final vetted research ideas via literature review for context, novelty checking against existing work, feasibility assessment for methodological adaptability, and finalisation of structured research proposal Methodological Design Upon receiving validated research idea, the master orchestrator delegates it to method agent responsible for developing an experimental protocol. Through multiple cycles of evaluation with review agent, final consolidated plan is optimised for scientific validity, reliability, and robustness. For studies identified as requiring priori statistical sample size determination, the method agent engages power analysis agent instead of relying on arbitrary conventions. Specifically, the power analysis agent independently conducts the necessary calculations to determine sample size for the research which satisfies scientific standards while meeting practical constraints. To do so, it coordinates the efforts of several specialists: an archivist agent that queries published literature through the search engine tool to establish the essential parameters for power calculations (including anticipated effect sizes, variance estimates, and appropriate statistical tests); coding agent that writes and executes power analysis code scripts; and troubleshooting agent that inspects the validity of the outputs and results to provide feedback. Upon completion, the method agent engages pre-registration agent, which develops pre-registration report compliant with Open Science Framework (OSF) standards 56, detailing hypotheses, independent and dependent variables, sampling procedures, exclusion criteria, analytical approaches, and anticipated outcomes. review agent then evaluates the proposed research plan, corrects any issues found, and revises the power analysis as necessary. This methodological design framework culminates in finalised experimental protocol that is designed to meet established methodological standards in the field and provide transparency from the earliest stages of the scientific process. Real-world Implementation For experiments requiring physical-world validation, the master orchestrator engages an implementation agent to manage the execution of the experiment within the confines of the specific tools for which ethics approval has been obtained. For the initial experiment, we focussed on online cognitive psychology paradigms, supporting previously validated Visual Working Memory Task 52, Mental Rotation Task 53 (MRT), and the Vividness of Visual Imagery Questionnaire-2 54 (VVIQ2). These were delivered to the agent as single HTML and JavaScript experiment hosted 10 Virtuous Machines: Towards Artificial General Science on Pavlovia (GitLab) with integrated participant information, consent, and SurveyMonkey questionnaire. The system was also interfaced with Prolific high-quality, well-established participant recruitment platform for online experiments 109111 which is compatible with web-based Pavlovia experiments. Based on the pre-registration specifications and in alignment with ethical approval requirements, the implementation agent defines and validates the participant recruitment parameters for the Prolific platform including sample size, age, and vision requirements. In collaboration with coding agent and troubleshooting agent, code is then written for Prolifics API and executed, which instantiates complete draft of the study on Prolific (linking to the existing hosted experiment). While the implementation also allows for autonomous live deployment of the draft study via single call to Prolifics API, for the current work we incorporated manual verification step and manual publication of the experiment. This additional step, though not technically necessary, enabled careful checking of all parameters before participant recruitment, to ensure human oversight and ethical compliance. Ethics approval for the autonomous studies presented here was obtained from Bellberry Human Research Ethics Committee (HREC; EC00455), after which the draft Prolific experiment for the first study (Study 1) was manually launched for 288 participants (sample size determined by Study 1s power analysis). Two experimenters were present throughout data collection to monitor participant communications. Studies 2 and 3 subsequently developed their own independent hypotheses and analysis plans, calculating optimal sample sizes of 120 and 566 participants respectively. However, all three studies analysed the same 288-participant dataset from Study 1, of which one participant failed to produce any data, and 10 participants failed to complete the study, leaving final sample of 277 participants with meaningful datasets. Upon study activation, eligible participants accessed the experimental tasks remotely online through the Pavlovia URL, with participation management data stored on Prolific and experimental response data captured in the GitLab repository. Importantly, all participant data are completely de-identified by the online recruitment platforms to ensure participant privacy; the system never has access to any personal identifying information. Following data collection, the implementation agent inspects the data, analyses its structure, identifies potential quality issues, and prepares documentation of data characteristics for subsequent analysis. This implementation framework provides generalisable approach to physical-world experimentation that facilitates extension beyond cognitive psychology to other domains requiring human participants or physical interaction. Data Analysis The master orchestrator passes experimental data onto dedicated data analysis agent that transforms raw observations into interpretable scientific evidence, prioritising reproducibility, statistical rigour, and explanatory clarity. It manages multi-stage analytical workflow, collaborating initially with an exclusions agent and archivist agent to establish theoretically informed, literature-backed data cleaning protocols, as well as validation agent to ensure alignment of the plan with pre-registration commitments. Each analytical stage is subsequently delegated to coding agent which uses custom code editing framework based on the file editing functionality of the Aider coding assistant 112, to develop the codebase for the step in structured environment. Each code execution attempt is evaluated by team of troubleshooting agents, who offer the coding agent diverse analytical perspectives and feedback on implementation challenges. Progress is documented, providing transparent analytical decision paths. Code refinement continues until the troubleshooting agents and independent verification agents reach consensus that the outputs are satisfactory, or when the predefined iteration limit is reached. The data analysis agent subsequently either proceeds to the next analytical steps until the entire data analysis pipeline has been completed, or requests guidance from the researcher if no further progress is being made. The resulting analytical workflow documents detailed interpretations of the results and the full analytical decision process. Experimental Re-evaluation 11 Virtuous Machines: Towards Artificial General Science Following completion of an experiment, the re-evaluation agent is tasked with determining the next most appropriate step for the research following structured decision architecture which evaluates all the findings collected up to that point. The decision tree incorporates both Bayesian and frequentist statistical frameworks, where conventional statistical thresholds (Bayes Factor > 10; < α with sufficient power) function as practical heuristics for the agent to interpret experimental outcomes regardless of the analytical methods employed. Contradictory evidence triggers novel theory generation pathways; theoretically consistent but imprecise results prompt precision enhancement strategies; strong null evidence initiates either theory revision (when conflicting with established frameworks) or alternative hypothesis testing (when consistent with current understanding); and inconclusive evidence prompts study enhancement recommendations. This structured approach formalises scientific judgment, which typically relies on researcher experience and intuition, guiding the progression from initial findings to theoretical understanding, and helping the re-evaluation agent determine whether the study in its current state is complete, or if further experiments are needed for the purposes of theory revision, precision enhancement, or parameter space mapping. The final context-specific recommendation is returned to the master orchestrator to act upon accordingly either returning process flow back to the method agent if additional follow-on experimentation is required, or noting follow-up study ideas for later use by the idea agent to further traverse the field of research. Visual Representation Data analysis outcomes for all the completed experiments are handed off by the master orchestrator to the visuals agent, to coordinate the creation of multiple figures and tables essential for clear scientific communication of the findings. The agent formulates visualisation strategy based on data characteristics and theoretical significance, delegating implementation to two further orchestrators in parallel figures agent and tables agent each responsible for their respective visual elements. The figures agent instantiates dedicated panel agent orchestrator for each component of multi-panel figures, establishing deeply layered reporting structure that maintains cohesion while enabling specialised attention to all individual elements of the visual hierarchy. Each panel agent operates in conjunction with multiple specialists: coding agent which develops and executes the visualisation code script; troubleshooting agent that resolves implementation issues; an inspection agent with vision capabilities that evaluates aesthetic clarity; and caption agent which crafts detailed figure caption highlighting findings and data representations. The tables agent coordinates an analogous specialist ensemble to transform analytical results into structured tabular formats with accompanying captions, generating tables in both LaTeX and Microsoft Word document formats to accommodate researcher and journal preferences. Through iterative refinement documented transparently at each stage these agents collaboratively produce multi-faceted visualisations conforming to disciplinary conventions while attempting to maximise data interpretability and clarity. For methodological visualisations, the visuals agent also modifies provided template scalable vector graphics (SVG) files to represent the specific experimental implementations in the study, employing caption agent to write figure captions that contextualise the representations within the experimental framework. This approach to visual documentation translates the experimental data and methodology into graphics intended to enhance the accessibility and impact of the research. Manuscript Development The master orchestrator delegates the process of writing full research report to manuscript agent, which leverages the skills of specialists to produce coherent and high-quality writeup. By engaging an archivist agent, the results are situated within the broader scientific literature, establishing connections with existing theoretical frameworks, and constructing contextual foundation that enhances the explanatory value of the findings. The manuscript agent also develops logical scaffold for the scientific narrative, delegating aspects to several specialist writing agents who synthesise all the research components hypotheses, methodological details, results, visualisations, and theoretical implications into report sections in collaboration with an archivist agent that provides additional relevant literature references as needed. To address the pervasive tendency of LLMs to generate fictitious citations 113115, the writing agents utilise 12 Virtuous Machines: Towards Artificial General Science validation tool built on doi.org to verify the authenticity of each reference, enabling correction of fabricated or inaccurate references prior to incorporation in the full manuscript. review agent provides evaluation of the initial writeup version, guided by domain-specific standards, identifying potential improvements in clarity, logical structure, methodological reporting, and theoretical integration. Based on the feedback, the writing agent incorporates targeted refinements, developing final manuscript to communicate findings. Peer Review Review of the completed research manuscript is conducted by specialist review agents, following the evaluation protocol used in our domain-agnostic publicly available AI pre-peer review tool Paper Wizard (https://paper-wizard.com). Emulating the process of human peer review, the evaluation examines multiple dimensions of the scientific work, including theoretical foundation, methodological rigour, statistical appropriateness and writing quality. Through detailed assessment, the system identifies both major and minor issues requiring attention in areas where quality could be improved prior to submission for publication. The relevant sections of the manuscript are flagged, and specific, actionable frameworks for addressing each issue are provided, representing quality assurance mechanism within the systems workflow. Document Construction The master orchestrator delegates final manuscript assembly to document agent responsible for incorporating all text, figures, tables, and captions into single publication-ready file. multi-format implementation allows researchers to specify preferred format between LaTeX or Microsoft Word. The LaTeX agent constructs its document in process that builds upon prior work 38, with modifications to the structure, layout, formatting, and construction of the document, in addition to building out compatibility, user flexibility and robustness. The Microsoft Word agent utilises similar process to construct its document, with both formats ultimately also being saved to final PDF format. Several time-consuming technical aspects are handled by the document agents, including placement of figures and tables, section and caption numbering, encoding of mathematical notation and special characters, reference formatting and typographical standardisation, to produce submission-ready manuscripts that adhere to scientific publishing standards. Importance of Feedback Central to the system is frequent evaluation, which functions as key regulatory mechanism integrated throughout the entire research pipeline. The architecture underlying the review process was derived from that of Paper Wizard the peer review tool which we tailored to create specialised evaluation protocols for each stage of the scientific process. Feedback allows the system to iteratively refine and adapt its efforts 116,117 until the high standards required for robust scientific inquiry in the field are met. As explicit evaluation and refinement cycles develop higherquality outputs than single-pass generation 73,83, such intraand inter-agent feedback mechanisms are fundamental to ensure and maintain methodological rigour and theoretical validity in AIdriven research 118. However, it is possible that as the intelligence of LLMs continues to advance, the need for extensive review may eventually diminish. Safety Given the potential for unintended system behaviours when operating with minimal human oversight, the framework incorporates multiple safety measure layers and operational safeguards to ensure system stability, prevent resource overconsumption, and mitigate potential security vulnerabilities. Autonomous code execution is constrained to timeout dynamically managed by the coding agent, but only up to ceiling hard limit, preventing excessive runtime while maintaining sufficient flexibility for computationally intensive processes such as statistical modelling. Storage consumption of the code is also continuously monitored and bounded by hard limits to prevent excessive memory use by the system. Package management follows verification protocol that evaluates each requested library against multiple security criteria: blacklist/whitelist status, typosquatting (slight misspellings of legitimate names) detection, popularity metrics, 13 Virtuous Machines: Towards Artificial General Science publisher verification, description analysis, and release history examination. This verification applies recursively to all dependencies, with installation confined to isolated virtual environments to minimise potential global environment contamination. Ubiquitous across all agents, every LLM provider response also undergoes safety evaluation including detection of language pattern shifts, semantic consistency checks, entropy analysis, code syntax validation, and screening for unexpected elements such as arbitrary code blocks, external URLs, or blacklisted keywords unrelated to the scientific task. Additional safeguards include API rate limiting to prevent service overload, logging of all system activities for auditability, and regular checkpointing to enable recovery from potential failures. These nested security measures improve the robustness of the operational framework and enable autonomous scientific discovery while maintaining appropriate boundaries on system capabilities and resource utilisation."
        },
        {
            "title": "Results",
            "content": "The aim of this research was to build an end-to-end system capable of producing meaningful scientific knowledge beyond the training data. We deployed hierarchical framework of AI agents in cognitive science to assess performance on complex tasks and the capacity to coordinate autonomously over extended periods to execute complete scientific workflows. Tasked with independently developing three different studies, the system successfully pursued new lines of scientific inquiry, from conception through analysis and interpretation, leading to empirical findings on human cognition. Beyond manual launch and monitoring of the experiment, downstream stages (e.g., analysis, visualisation, and manuscript drafting) ran autonomously; cosmetic selection of final figure variants for inclusion was performed post hoc. These efforts culminated in complete manuscripts for each study (Figure 5; also provided in Appendices 1 3), representing the primary result of the work presented here. There are (currently) no objective tests with which to evaluate the quality of scientific manuscripts; thus, here we employed structured expert review (mirroring established peer review processes) to assess the AI-generated manuscripts. 14 Virtuous Machines: Towards Artificial General Science Figure 5: Manuscript generated by the pipeline. The full 31-page manuscript produced in Study 1 spans hypothesis formulation to final formatting and follows standard scientific manuscript structure with embedded figures (visible in pages 6, 7, 11, 13, 15, 17, 28, 30), tables (pages 12 & 27), statistical outputs, and references. All content was autonomously generated and typeset in LaTeX. This represents one of the three independently generated manuscripts. System Performance 17 hours runtime The system executed full studies (from initial conception to manuscript) in on average per study, excluding data collection. Comparatively, typical human-led workflows can require weeks to months of human expert time depending on experimental complexity. More than 50 agents contributed at various stages, coordinating to execute distinct components of the scientific workflow. Computational usage per study varied with research complexity and difficulty, averaging 32.5 million tokens (distribution detailed in Table 1). In terms of system capabilities, 1000 3000 scientific publications were examined during the literature review process per study. The data analysis agents successfully implemented mixed-effects models and multi-level modelling; handled 279 heterogeneous raw data CSV files; and created 14 23 derived CSV files for statistical analysis (all available on GitHub). These agents also showed temporal persistence without human intervention (mean runtime: 8h 32m; SD = 3h 22m), exhibiting goaldirected behaviour to successfully generate functional debugged statistical code. The generated code totalled mean of 7696 lines (SD = 2426) per study. Each data analysis pipeline involved navigating 72 action-observation cycles on average (SD = 17), including error-recovery and deliberate code-optimisation to meet the specification. Visualisation outputs comprised 10 20 figure panels per manuscript, while each manuscript totalled 7000 8000 words and incorporated 40 50 verified references. 15 Virtuous Machines: Towards Artificial General Science Table 1: costs and tokens for all LLMs utilised are shown. Values reported as mean Token usage and duration for each module of the framework. Cumulative SD. Human Expert Evaluation Expert evaluation by human scientists of the three AI-generated manuscripts identified both strengths and limitations across multiple dimensions of scientific quality. Specifically, while the AI system demonstrated competent use of advanced methods and statistics, as well as comprehensive literature integration; it also exhibited occasional issues with theoretical distinctions, statistical reporting, and interpretation. This mixed competency profile provides insight into the current capabilities and constraints of AI-assisted scientific research, which do not map cleanly onto traditional metrics of research expertise."
        },
        {
            "title": "Positive Aspects",
            "content": "Clarity and fluency in scientific writing. All three manuscripts demonstrated clear, professional scientific writing that adhered to disciplinary conventions and maintained coherence across sections. The writing exhibited appropriate use of technical terminology and followed established formatting standards for scientific publication. Study 3s Alternative Mechanisms and Neural Efficiency section was particularly well-developed, providing an innovative and theoretically grounded interpretation of the results. Creative and theoretically motivated research questions. The system displayed originality in framing research questions, often exploring relationships not commonly addressed with the given cognitive paradigms. Study 1 explored task order effects and tested multiple model fits to describe the data, demonstrating sophisticated analytical thinking beyond the most obvious research directions. Study 2 investigated the link between mental imagery vividness and serial dependence in VWM and MRT tasks. Study 3 proposed an alternative explanation for null results grounded in individual differences in neural efficiency. Rigorous data screening, reliability checks, and control analyses. Data screening and reliability checks were comprehensive across studies. Study 1 applied split-half reliability analyses, Study 3 checked internal consistency across VVIQ2 subscales, and all studies employed sensible data cleaning protocols with participantand trial-level screening based on multiple behavioural indicators. These procedures supported data quality and systematically addressed potential confounds. Comprehensive and relevant literature engagement. Literature engagement was thorough and well-integrated into theoretical framing. Study 1 reviewed the historical development of VWM 16 Virtuous Machines: Towards Artificial General Science and mental rotation theory, providing detailed context for the research questions. Study 2 noted the reliability paradox in individual differences research 55, demonstrating awareness of contemporary methodological discussions. Study 3 effectively contextualised findings in relation to recent failed replication of foundational results 119. Advanced statistical methods and proper error control. Statistical methods were sophisticated and appropriate across all studies. Mixed-effects models were implemented with systematic comparisons between linear and non-linear fits where relevant. Effect sizes were consistently reported alongside significance tests. Family-wise error correction was properly applied, including Benjamini-Hochberg false discovery rate correction in Study 1."
        },
        {
            "title": "Negative Aspects",
            "content": "Theoretical misrepresentations and overstatement. Theoretical models were occasionally misrepresented, including inappropriate extension of VWM framework 120 to unrelated visuospatial tasks in Study 3, and Study 2s false claim about working memory resource allocation debates. Study 3 claimed novel imagery-precision link without acknowledging related accuracy research; the statement is misleading given existing literature showing relationship between imagery strength and VWM accuracy 121. Studies 2 and 3 also made interpretative claims unsupported by the data. Methodological claims and inconsistencies. Study 1 claimed the VWM cue procedure isolates memory for the target item without supporting evidence and described theoretically unlimited resolution despite digital constraints. Study 2 extensively discussed measure-reliability but did not explicitly test it (unlike Study 1), leading to unjustifiably confident theoretical interpretations. Study 3 also claimed task engagement metrics validated precision parameter estimates without adequate support for this inference. Statistical omissions. Study 1 did not discuss the incidental significant relationship found between overall VWM error and VVIQ2, evident in Table 1 and Figure 5. Study 2 treated perfectly correlated zero-crossing and width parameters as independent variables, with missing statistics for the width parameter due to this collinearity. Study 3 performed unnecessary multiple correlations across rotation angles, inflating comparisons without justification for expecting effects at specific angles. Presentation issues. Technical problems with visualisations included minor layout issues across Figures, and missing axis labels in Study 3. The figures and tables also often required human selection to identify the most suitable version for inclusion in the manuscript. Other minor problems included inconsistent terminology across Study 1 (intertrial interval vs inter-trial interval), missing spaces between degree symbols and following words, and awkward table formatting requiring manual adjustments. Study 3 reported Negligible instead of actual effect sizes in tables and incorrectly used sample size terminology when referring to trial-level data. Internal contradictions. Study 1 contained contradictory statements about methodological advances, advocating for larger samples and more sophisticated statistical approaches while subsequently stating that methodological innovation is needed beyond increased sample size or statistical power. Study 2 made unjustifiably strong claims about multiple comparison corrections addressing problems that have plagued previous individual differences research without appropriate qualification. Study 2 also cited different participant numbers in different sections, and Study 3 contained confusing statements about pre-registered thresholds (70%) versus implemented criteria (65%)."
        },
        {
            "title": "Overall Assessment",
            "content": "The required use of two cognitive tasks (Visual Working Memory Precision Task and Mental Rotation Task) and questionnaire on mental imagery (Vividness of Visual Imagery Questionnaire2), inherently constrained the range of research questions available to the system. Arguably, the 17 Virtuous Machines: Towards Artificial General Science most direct and obvious question whether individual differences in subjective imagery relate to performance on VWM or MRT was successfully identified and explored in Study 1. This demonstrates the systems capacity to recognise theoretically grounded, field-relevant hypotheses. However, it also ventured into less obvious but still interesting territory, such as serial dependence effects, inter-task associations, and exploratory modelling of parameter distributions. The system particularly excelled in rigour and comprehensiveness. Across the manuscripts, there was consistent emphasis on transparency, robustness checks, and consideration of alternative explanations. Scientists routinely face interpretative challenges when evaluating statistical outputs, particularly given that sufficiently large sample sizes can yield statistical significance for trivially small effects 122,123 where the meaningful magnitude of difference between data distributions is slight. When faced with statistically significant findings but small effect sizes in Study 3, the system demonstrated notable objectivity and sophistication by prioritising practical significance 124 over statistical significance alone commendable departure from the pervasive p-value fixation that has long plagued human scientific research 125,126, though it potentially risks undervaluing theoretically meaningful discoveries with modest but reliable effects. Discussion of prior literature was typically thorough and well-integrated into theoretical framing. However, this strength in systematic reasoning was not always matched by conceptual nuance. The system occasionally struggled to navigate abstract, multi-conditional ideas particularly those involving subtle theoretical distinctions or long-standing debates within the literature. Interestingly, while the manuscripts often demonstrated cautious and well-qualified interpretations, these were at times juxtaposed with overly broad or confident statements, creating tension between rigour and over-reach. These weaknesses including misrepresentation of theoretical frameworks and internal contradictions are also commonly encountered in human-produced manuscripts, suggesting these limitations may reflect broader challenges in scientific practice rather than being unique to AI-discovery frameworks. In sum, while AI systems can emulate the structure of scientific reasoning, more development is needed for fine-grained judgments that come from deep conceptual familiarity and years of experience navigating complex academic discourse in the field."
        },
        {
            "title": "Discussion",
            "content": "This study presents an end-to-end AI scientific discovery system that integrates analogues for abstraction, decomposition, metacognition, autonomy, and dynamic memory; and that navigates the complete scientific workflow from hypothesis formulation to manuscript preparation. While domain-specific AI systems have achieved high performance in narrow tasks, such as AlphaFolds protein structure prediction 17, as far as we are aware, this is the first demonstration of an autonomously conducted, end-to-end online experiment with human participants. The results, contained within the three scientific manuscripts that accompany this article, show that AI systems can conduct scientific inquiries with minimal human intervention. While prior systems have primarily operated within computational domains (simulations and modelling), the systems ability to collect and interpret real-world experimental data from human participants marks key step toward embodied scientific AI that is capable of testing hypotheses in real-world settings. These findings bridge the gap between in silico AI capabilities and practical scientific application, with important implications for both the future of AI-augmented scientific discovery and fundamental questions about the nature of knowledge creation. System Performance and Capabilities Efficiency. The system demonstrated efficiency gains compared with traditional research timelines, executing complete research projects in hours rather than the weeks or months typically required $114 USD average marginal cost per complete study (not including human by research teams. At recruitment costs), the system also offers cost reduction compared to even modest empirical studies, which often require substantial investments in researcher salaries, infrastructure, and overhead. The resulting productivity improvements suggest potential for transformative changes 18 Virtuous Machines: Towards Artificial General Science in how scientific research is conducted, and the pace at which scientific progress can be made. This cost efficiency could particularly benefit resource-constrained institutions and developing nations, potentially redistributing global scientific capacity. Rigour. Across the three studies, the system tended toward conservative methodological choices, appropriate statistical power, and transparent reporting of limitations indicating impartiality and cognitive flexibility which can at times challenge human researchers. Importantly, the systems documentation of all analytical decisions and availability of raw data enables reproducibility, which goes some way toward addressing reproducibility concerns in scientific literature where 70% of researchers have failed to reproduce another scientists experiments 127129. The ability to replicate experimental conditions and analyses at negligible marginal cost could alter how we validate scientific findings, and aligns with the growing movement toward open science practices 130. Sophistication. The system demonstrated the capacity to autonomously construct and execute multi-step data analysis pipelines, employing valid statistical techniques, evaluation of underlying assumptions, and measured interpretation of results. The system here sustained coherent coding and statistical reasoning for longer than eight hours for context, 50 55-minute 50% task-completion time horizon has been reported for Claude 3.7 Sonnet on research-engineering tasks 131. Adaptability. When encountering unexpected results, the systems problem-solving and decisionmaking capabilities showed real-time adaptation to outcomes and implementation issues beyond fixed plans. It dynamically modified its approach when confronted with unanticipated outcomes or implementation challenges (e.g., finding appropriate solutions to statistical models not converging), without fixating on the original approach. Coupled with continuous documentation of all decision processes, the resulting audit trail was both comprehensive and transparent. Grounding. By integrating LLMs with the d-RAG to contextualise findings, the system also exhibited quality scientific writing and contextual framing. It often situated findings within broader theoretical contexts, identified appropriate connections with relevant literature, and articulated limitations with clarity. The ability to engage with conceptual and communicative dimensions of scientific discourse suggests effective leveraging of its broad knowledge of relevant published literature. Limitations and Challenges Despite its achievements, the system exhibits several limitations that highlight areas for future development. Experimental implementation poses fundamental bottleneck in the verification of ideas of autonomous science systems 132. While the framework is general by design, currently its physical capabilities are domain-constrained to online experiments for which the necessary tools and interfaces are available. Representing an engineering challenge rather than fundamental limitation, the extension of the system to other scientific fields requires the development and incorporation of new and existing domain-specific toolsets, which are readily accommodated by the modular and generalist core system architecture. Furthermore, while the system was capable of end-to-end research, it was not infallible and can thus benefit from human refinement and oversight. In particular, data visualisations occasionally include graphical imperfections which persist despite inspection and verification mechanisms such as overlapping axis labels, misaligned label positioning, or suboptimal axis bounds. Though predominantly aesthetic rather than substantively inaccurate, these minor discrepancies highlight the intrinsic challenges in automated computer vision tasks that human visual perception resolves effortlessly 133,134. Such artifacts remain difficult for the system to detect autonomously, yet can be quickly and easily rectified with minimal human intervention. In addition, rigorous scientific inquiry involves lengthy, complex, multi-stage processes, requiring the system to handle extremely long chains of thought thousands of reasoning steps and 19 Virtuous Machines: Towards Artificial General Science conversational exchanges extending over 12 hours. The dynamic memory system and cognitive offloading mechanisms were central to maintaining conceptual continuity across research stages, effectively combining working memory with strategic external resource utilisation. The systems ability to selectively filter relevant information while preserving focused knowledge representations of the necessary context enabled it to navigate the entire scientific workflow without conceptual drift. Interestingly, many reasoning models generally degrade in their performance 135,136 over extremely long chains, losing focus and coherence across iterations. Sensitivity to early-stage accuracy also emerged as challenge. Poor question formulation or any conceptual errors introduced during hypothesis generation and methodological design propagate downstream persisting through multiple verification cycles at detriment to research outcomes. This potential single-point failure mode likely reflects the anchoring bias characterised in LLMs 137, whereby the first logical claims encountered by the model are weighted as ground truth and only weakly revised later, disproportionately shaping subsequent judgements. Consequently, once misconception enters the chain of thought, later processes tend to build upon it rather than correcting it 138. Explicit error-checking steps only seldom succeed in reversing the trajectory once false premise has been internalised. Interestingly, humans also exhibit this cognitive bias 139141, where initial information acts as reference point that substantially affects how subsequent information is assessed, particularly under uncertainty. This phenomenon emphasises the importance of robust verification protocols during the earliest conceptual stages of automated scientific investigation and highlights crucial advantage of human-AI collaborative scientific workflows where human expertise can intervene most effectively at points of highest conceptual leverage. Future Directions Our work points toward several high-impact future developments for autonomous scientific systems. The modular, domain-agnostic architecture we implemented here provides foundation for general-purpose scientific AI that can operate across research domains and accommodate various methodological techniques and constraints. Expanding beyond cognitive psychology represents an immediate opportunity. Applying the system to fields ranging from medicine to environmental science would require only domain-specific implementation interfaces and minimal changes to the fundamental scientific workflow engine. promising direction involves integration with physical laboratory automation and robotics, enabling direct manipulation of physical systems across chemistry, biology, and materials science. Enhancing the systems capacity for autonomous theory refinement also represents key direction for development. Incorporating intrinsic mechanisms for theoretical updating based on empirical results could potentially enable the system to produce more creative scientific contributions and breakthroughs of greater novelty. In addition, continued work on improving cognitive reasoning frameworks is important for strengthening the robustness and quality of research produced advancing toward system capable of generating truly novel experimental methodologies and theoretical insights. Models with real-world experimental capabilities represent potential pathway toward more advanced AI. The framework we have developed embodies the fundamental cycle of knowledge creation: executing movements to explore the world, reflection on outcomes, understanding relationships with prior knowledge, generating insights from these interactions, and developing foresight for future predictions 142. This process mirrors cognitive development in children, who learn primarily through physical manipulation of objects and progressive refinement of their understanding 105,143. While current LLMs excel at pattern recognition within training data, they remain limited by their inability to autonomously expand beyond those boundaries. Here the system coupled internal representations the exploration of latent connections between concepts with external measurement through experimentation. This creates virtuous cycle where hypotheses conceived in the models rich latent space can be validated against reality, with results feeding back to refine its conceptual framework. This suggests fundamentally different approach to advancing AI capabilities one rooted in the power of scientific investigation to build increasingly accurate models of the world. Toward this end, we use the intermediary milestone 20 Virtuous Machines: Towards Artificial General Science of Artificial General Science (AGS) to denote autonomous systems capable of independently driving scientific inquiry across all domains generating hypotheses, orchestrating experiments, and iteratively refining knowledge through empirical evidence. In recursive loop of discovery and learning, the scope of understanding and knowledge of the system could be continually expanded beyond trained-on human knowledge. Societal and Ethical Implications Autonomous scientific systems capable of designing and executing rigorous methodological plans will likely reshape the role of human scientists 144. While capable of independent operation with minimal human intervention, the balance between autonomy and human collaboration offers distinct advantages depending on the research goal. Currently, humans provide most value to these systems in creative problem formulation, conceptual innovation, and ethical oversight though the focal points for human contribution may shift as the technology advances. These systems can also address limitations of contemporary research, particularly the increasingly narrow specialisation that often constrains scientists to their areas of expertise 145, impeding progress on complex interdisciplinary problems. By bridging disciplinary boundaries, domainagnostic frameworks facilitate integrative research, empowering scientists to explore theoretically adjacent areas where they may lack training but possess valuable conceptual insights. Researchers can delegate any aspect(s) of the scientific workflow to the system as needed, enhancing both efficiency and innovation through collaboration. The accelerated pace of research enabled by autonomous scientific systems presents both opportunities and challenges. While rapid knowledge generation could expedite solutions to pressing global issues from climate change to disease, inadequate consideration of downstream effects risks unintended consequences. Of particular concern is the potential for producing research outputs at volume that overwhelms human researchers, necessitating innovative approaches to distil and communicate findings effectively within human cognitive capacity constraints. Notably, autonomous scientific systems are likely to generate high proportion of null findings, which, while typically remaining unpublished in traditional research 146, offer under-utilised value. By documenting non-significant outcomes, these results may mitigate publication bias 146, as well as characterise the null space of research fields highlighting where relationships are absent and conversely where they may exist. This dual benefit reduces wasted resources on redundant experiments and can guide researchers toward better-informed hypotheses for promising investigations. The environmental impact of running multiple LLMs over sustained periods also warrants consideration, as the material energy and carbon footprint of such systems is non-trivial, and yet to be quantified relative to the net sustainability of human labour. Balancing the dynamics of speed, volume, environmental impact, and meaningful human oversight will be important as autonomous discovery systems become more prevalent in scientific workflows. The ethical dimensions of these systems are multifaceted, with their potential to democratise highquality research capabilities representing promising long-term benefit. By reducing resource barriers, these systems can broaden participation in the scientific enterprise, enabling individuals, institutions and regions with limited infrastructure to contribute to progress on major challenges. However, as their research capabilities expand, safeguarding these systems becomes critical vulnerabilities to hacking or LLM prompt injection attacks 147 could lead to misuse with potentially severe consequences. Adequate safety mechanisms and optional human verification at each stage of pipelines may help mitigate these risks, yet scalable, comprehensive frameworks will be integral to balancing autonomy with responsible oversight. Beyond safety, ethical considerations include attribution of scientific credit, responsibility for research outcomes, and governance of AI-driven scientific systems. As these systems increasingly contribute to knowledge production, accountability frameworks must adapt to ensure transparency, trust, and fair credit allocation. This is of particular importance in scientific contexts where knowledge claims carry significant societal implications 148,149, whether beneficial or harmful. Delineating responsibility between human researchers and autonomous systems is complex, requiring clear ethical guidelines and governance structures to maintain public trust. The capability to generate thousands of studies 21 Virtuous Machines: Towards Artificial General Science rapidly also raises concerns about potential misuse, including automated p-hacking, deliberate generation of misleading findings, and high-volume/low-quality outputs that could strain peer review systems. Establishing quality control mechanisms, transparency standards, and detection systems for AI-generated research will be essential as these technologies proliferate. Such measures will enable human scientists to replicate autonomous studies, particularly those with highly impactful findings, to independently validate results, ensure ethical and safety compliance, and facilitate integration into existing societal infrastructure (e.g. patent filing). These ethical implications highlight the need for ongoing dialogue to align AI-driven scientific discovery with principles of safety, equity, and responsibility. On philosophical level, the empirical results of this study raises intriguing questions about the nature of knowledge, particularly the role of understanding in knowledge generation. While traditional epistemological frameworks posit human comprehension as an intrinsic component of knowledge creation 150, the system shown here demonstrates that structured scientific inquiry can produce valid empirical knowledge without requiring human-like understanding. Consequently, knowledge may be derived from mechanistic processes without necessitating conscious insight 151,152 distinction that invites consideration of how we conceptualise scientific knowledge and the processes through which it emerges. In sum, here we have presented an AI scientific discovery system which demonstrates that frontier LLMs augmented with human-inspired cognitive operators and physical interaction capabilities can independently execute end-to-end research. In 17 hours of system runtime (excluding data collection), with minimal human oversight, the system conceived, ran, analysed, and produced complete manuscripts for an online psychology experiment with 288 human participants. We replicated this capability across three distinct studies. As far as we are aware, this is the first demonstration of autonomous, end-to-end experimental research with human participants. Working in collaboration with human researchers, we foresee systems that can accelerate and elevate the rigour of all components of individuals scientific workflow in the pursuit of high-quality science. Although work remains in enhancing creative theory development and expanding experimental scope, the performance of this system marks step forward in AI actively participating in empirical investigations of the natural world, through structured scientific inquiry. The epistemological implications of knowledge generation by artificial systems invite reconsideration of traditional frameworks for scientific understanding, and as their capabilities develop further, thoughtful consideration of scientific integrity, safety, and inclusion. 22 Virtuous Machines: Towards Artificial General Science"
        },
        {
            "title": "References",
            "content": "[1] Deutsch, D. The beginning of infinity: explanations that transform the world (Viking, New York, 2011). OCLC: ocn682892569. [2] Popper, K. R. Conjectures and refutations: the growth of scientific knowledge Repr edn. Routledge Classics (Routledge, London, 1963). [3] Plato. Republic. Books 1-5 Vol. 1 of Loeb Classical Library (Harvard University Press, Cambridge, MA, 2013). Original work composed c. 380 BCE. [4] Aristotle. Posterior Analytics; Topica No. 391 in Loeb Classical Library (Harvard University Press, Cambridge, MA, 1960). Original work composed c. 350 BCE. [5] Deutsch, D. The fabric of reality: the science of parallel universes and its implications (Allen Lane, New York, 1997). [6] Popper, K. R. & Weiss, G. The Logic of Scientific Discovery. Physics Today 12, 5354 (1959). URL https://pubs.aip.org/physicstoday/article/12/11/53/914254/ The-Logic-of-Scientific-Discovery. [7] Bornmann, L. & Mutz, R. Growth rates of modern science: bibliometric analysis based on the number of publications and cited references. Journal of the Association for Information Science and Technology 66, 22152222 (2015). URL https://asistdl.onlinelibrary.wiley. com/doi/10.1002/asi.23329. [8] Landhuis, E. Scientific literature: Information overload. Nature 535, 457458 (2016). URL https://www.nature.com/articles/nj7612-457a. [9] Hanson, M. A., Barreiro, P. G., Crosetto, P. & Brockington, D. The strain on scientific publishing. Quantitative Science Studies 5, 823843 (2024). URL https://direct.mit.edu/ qss/article/5/4/823/124269/The-strain-on-scientific-publishing. [10] Tenopir, C., King, D. W., Christian, L. & Volentine, R. Scholarly article seeking, reading, and use: continuing evolution from print to electronic in the sciences and social sciences. Learned Publishing 28, 93105 (2015). URL https://onlinelibrary.wiley.com/doi/10.1087/ 20150203. [11] Tenopir, C., Volentine, R. & King, D. W. Scholarly Reading and the Value of Academic Library Collections: results of study in six UK universities: Based on study carried out for JISC Collections and presented by Carol Tenopir at the 35th UKSG Conference, Insights: the UKSG journal 25, 130149 (2012). URL http: Glasgow, March 2012. //insights.uksg.org/articles/10.1629/2048-7754.25.2.130. [12] Kuhn, T. S. The structure of scientific revolutions [2d ed., enl edn. International encyclopedia of unified science. Foundations of the unity of science, v. 2, no. 2 (University of Chicago Press, Chicago, 1962). [13] Lorenz, E. N. Deterministic Nonperiodic Flow. Journal of the Atmospheric Sciences 20, 130141 (1963). URL http://journals.ametsoc.org/doi/10.1175/1520-0469(1963)020 0130: DNF 2.0.CO;2. [14] von Neumann, J. in Probabilistic Logics and the Synthesis of Reliable Organisms From Unreliable Components (eds Shannon, C. E. & McCarthy, J.) Automata Studies. (AM-34) 4398 (Princeton University Press, 1956). URL https://www.degruyter.com/document/ doi/10.1515/9781400882618-003/html. 23 Virtuous Machines: Towards Artificial General Science [15] Buchanan, B. G. & Feigenbaum, E. A. in Dendral and Meta-Dendral 313322 (Elsevier, 1981). URL https://linkinghub.elsevier.com/retrieve/pii/B978093461303350026X. [16] Hayes, T. et al. Simulating 500 million years of evolution with language model. Science 387, 850858 (2025). URL https://www.science.org/doi/10.1126/science.ads0018. [17] Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583589 (2021). URL https://www.nature.com/articles/s41586-021-03819-2. [18] Merchant, A. et al. Scaling deep learning for materials discovery. Nature 624, 8085 (2023). URL https://www.nature.com/articles/s41586-023-06735-9. [19] Pyzer-Knapp, E. O. et al. Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials 8, 84 (2022). URL https://www.nature.com/articles/s41524-022-00765-z. [20] Szymanski, N. J. et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nature 624, 8691 (2023). URL https://www.nature.com/articles/ s41586-023-06734-w. [21] Lenat, D. B. On automated scientific theory formation: case study using the AM program. Machine intelligence 9 (1977). [22] Romera-Paredes, B. et al. Mathematical discoveries from program search with large language models. Nature 625, 468475 (2024). URL https://www.nature.com/articles/ s41586-023-06924-6. [23] Fawzi, A. et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature 610, 4753 (2022). URL https://www.nature.com/articles/ s41586-022-05172-4. [24] Tunyasuvunakool, K. et al. Highly accurate protein structure prediction for the human proteome. Nature 596, 590596 (2021). URL https://www.nature.com/articles/ s41586-021-03828-1. [25] Vaswani, A. et al. Attention Is All You Need (2017). URL https://arxiv.org/abs/1706.03762. Version Number: 7. [26] OpenAI et al. GPT-4 Technical Report (2023). URL https://arxiv.org/abs/2303.08774. Version Number: 6. [27] OpenAI. OpenAI o3 and o4-mini System Card. System Card (2025). URL https://cdn. openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf. [28] DeepSeek-AI et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025). URL https://arxiv.org/abs/2501.12948. Version Number: 1. [29] Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. System Card (2025). URL https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. [30] Google. Gemini 2.5 Pro Model Card. System Card (2025). URL https://storage.googleapis. com/model-cards/documents/gemini-2.5-pro.pdf. [31] Wang, L. et al. survey on large language model based autonomous agents. Frontiers of Computer Science 18, 186345 (2024). URL https://link.springer.com/10.1007/ 24 Virtuous Machines: Towards Artificial General Science s11704-024-40231-1. [32] Borrego, A. et al. Research hypothesis generation over scientific knowledge graphs. Knowledge-Based Systems 315, 113280 (2025). URL https://linkinghub.elsevier.com/ retrieve/pii/S0950705125003272. [33] Ghafarollahi, A. & Buehler, M. J. SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning. Advanced Materials 37, 2413523 (2025). URL https://advanced.onlinelibrary.wiley.com/doi/10.1002/adma.202413523. [34] Gottweis, J. et al. Towards an AI co-scientist (2025). URL https://arxiv.org/abs/2502.18864. Version Number: 1. [35] Burger, B. et al. mobile robotic chemist. Nature 583, 237241 (2020). URL https: //www.nature.com/articles/s41586-020-2442-2. [36] Ghafarollahi, A. & Buehler, M. J. AtomAgents: Alloy design and discovery through physicsaware multi-modal multi-agent artificial intelligence (2024). URL https://arxiv.org/abs/ 2407.10022. Version Number: 1. [37] M. Bran, A. et al. Augmenting large language models with chemistry tools. Nature Machine Intelligence 6, 525535 (2024). URL https://www.nature.com/articles/s42256-024-00832-8. [38] Lu, C. et al. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (2024). URL https://arxiv.org/abs/2408.06292. Version Number: 3. [39] Huang, S. et al. PaperEval: universal, quantitative, and explainable paper evaluation method powered by multi-agent system. Information Processing & Management 62, 104225 (2025). URL https://linkinghub.elsevier.com/retrieve/pii/S0306457325001669. [40] Yamada, Y. et al. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search (2025). URL https://arxiv.org/abs/2504.08066. Version Number: 1. [41] Intology. Zochi Technical Report (2025). URL https://www.intology.ai/blog/ zochi-tech-report. [42] Schmidgall, S. et al. Agent Laboratory: Using LLM Agents as Research Assistants (2025). URL https://arxiv.org/abs/2501.04227. Version Number: 2. [43] Swanson, K., Wu, W., Bulaong, N. L., Pak, J. E. & Zou, J. The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies. Nature (2025). URL https://www.nature.com/ articles/s41586-025-09442-9. [44] Weng, Y. et al. CycleResearcher: Improving Automated Research via Automated Review (2024). URL https://arxiv.org/abs/2411.00816. Version Number: 3. [45] Ifargan, T., Hafner, L., Kern, M., Alcalay, O. & Kishony, R. Autonomous LLM-Driven Research from Data to Human-Verifiable Research Papers. NEJM AI 2 (2025). URL https://ai.nejm.org/doi/10.1056/AIoa2400555. [46] Pearl, J. Causality: Models, Reasoning, and Inference 2 edn (Cambridge University Press, 2009). URL https://www.cambridge.org/core/product/identifier/9780511803161/type/ book. [47] Brooks, R. A. Intelligence without representation. Artificial Intelligence 47, 139 (1991). URL https://linkinghub.elsevier.com/retrieve/pii/000437029190053M. 25 Virtuous Machines: Towards Artificial General Science [48] Clark, A. Being There: Putting Brain, Body, and World Together Again https://direct.mit.edu/books/book/3917/ URL (The MIT Press, being-thereputting-brain-body-and-world-together. 1996). [49] Pfeifer, R., Bongard, J. C. & Brooks, R. How the body shapes the way we think: new view of intelligence Bradford book (The MIT Press, Cambridge, Massachusetts London, 2007). [50] Pagel, S., Jirasek, M. & Cronin, L. Validation of the Scientific Literature via Chemputation Augmented by Large Language Models (2024). URL https://arxiv.org/abs/2410.06384. Version Number: 1. [51] Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous chemical research with large language models. Nature 624, 570578 (2023). URL https://www.nature.com/ articles/s41586-023-06792-0. [52] Zhang, W. & Luck, S. J. Discrete fixed-resolution representations in visual working memory. Nature 453, 233235 (2008). URL https://www.nature.com/articles/nature06860. [53] Shepard, R. N. & Metzler, J. Mental rotation of three-dimensional objects. Science 171, 701703 (1971). Place: US Publisher: American Assn for the Advancement of Science. [54] Marks, D. F. New directions for mental imagery research. Journal of Mental Imagery 19, 153167 (1995). Place: US Publisher: Brandon House. [55] Hedge, C., Powell, G. & Sumner, P. The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. Behavior Research Methods 50, 11661186 (2018). URL http://link.springer.com/10.3758/s13428-017-0935-1. [56] Foster, E. D. & Deardorff, A. Open Science Framework (OSF). Journal of the Medical Library Association 105 (2017). URL http://jmla.pitt.edu/ojs/jmla/article/view/88. [57] Bayley, O., Savino, E., Slattery, A. & Noel, T. Autonomous chemistry: Navigating self-driving labs in chemical and material sciences. Matter 7, 23822398 (2024). URL https://linkinghub.elsevier.com/retrieve/pii/S2590238524003229. [58] Lo, S. et al. Review of low-cost self-driving laboratories in chemistry and materials science: the frugal twin concept. Digital Discovery 3, 842868 (2024). URL https: //xlink.rsc.org/?DOI=D3DD00223C. [59] Zhang, W. et al. AgentOrchestra: Hierarchical Multi-Agent Framework for GeneralPurpose Task Solving (2025). URL https://arxiv.org/abs/2506.12508. Version Number: 2. [60] Fourney, A. et al. Magentic-One: Generalist Multi-Agent System for Solving Complex Tasks (2024). URL https://arxiv.org/abs/2411.04468. Version Number: 1. [61] Mosqueira-Rey, E., Hernandez-Pereira, E., Alonso-Rıos, D., Bobes-Bascaran, J. & Fernandez-Leal, A. Human-in-the-loop machine learning: state of the art. Artificial Intelligence Review 56, 30053054 (2023). URL https://link.springer.com/10.1007/ s10462-022-10246-w. [62] Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4 (2023). URL http://arxiv.org/abs/2303.12712. ArXiv:2303.12712 [cs]. [63] Stechly, K., Valmeekam, K. & Kambhampati, S. On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks (2024). URL https://arxiv.org/ abs/2402.08115. Version Number: 2. 26 Virtuous Machines: Towards Artificial General Science [64] Valmeekam, K., Stechly, K. & Kambhampati, S. LLMs Still Cant Plan; Can LRMs? Preliminary Evaluation of OpenAIs o1 on PlanBench (2024). URL http://arxiv.org/abs/ 2409.13373. ArXiv:2409.13373 [cs]. [65] Gentner, D. & Hoyos, C. Analogy and Abstraction. Topics in Cognitive Science 9, 672693 (2017). URL https://onlinelibrary.wiley.com/doi/10.1111/tops.12278. [66] Goldstone, R. L. & Son, J. Y. The Transfer of Scientific Principles Using Concrete and Idealized Simulations. Journal of the Learning Sciences 14, 69110 (2005). URL http://www.tandfonline.com/doi/abs/10.1207/s15327809jls1401 4. [67] Liu, J. et al. Generated Knowledge Prompting for Commonsense Reasoning (2021). URL https://arxiv.org/abs/2110.08387. Version Number: 3. [68] Press, O. et al. Measuring and Narrowing the Compositionality Gap in Language Models (2022). URL https://arxiv.org/abs/2210.03350. Version Number: 3. [69] Zhou, Y. et al. Large Language Models Are Human-Level Prompt Engineers (2022). URL https://arxiv.org/abs/2211.01910. Version Number: 2. [70] Dunbar, K. N. & Klahr, D. in Scientific thinking and reasoning Oxford library of psychology, 701718 (Oxford University Press, New York, NY, US, 2012). [71] Fleming, S. M. & Daw, N. D. Self-evaluation of decision-making: general Bayesian framework for metacognitive computation. Psychological Review 124, 91114 (2017). URL https://doi.apa.org/doi/10.1037/rev0000045. [72] Shea, N. et al. Supra-personal cognitive control and metacognition. Trends in Cognitive Sciences 18, 186193 (2014). URL https://linkinghub.elsevier.com/retrieve/pii/ S1364661314000230. [73] Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022). URL https://arxiv.org/abs/2201.11903. Version Number: 6. [74] Yao, S. et al. Tree of Thoughts: Deliberate Problem Solving with Large Language Models (2023). URL https://arxiv.org/abs/2305.10601. Version Number: 2. [75] Zhuge, M. et al. Agent-as-a-Judge: Evaluate Agents with Agents (2024). URL https: //arxiv.org/abs/2410.10934. Version Number: 2. [76] Renze, M. & Guven, E. Self-Reflection in LLM Agents: Effects on Problem-Solving Performance (2024). URL https://arxiv.org/abs/2405.06682. Publisher: arXiv Version Number: 3. [77] Anderson, J. R. The architecture of cognition The architecture of cognition (Lawrence Erlbaum Associates, Inc, Hillsdale, NJ, US, 1983). Pages: xi, 345. [78] Newell, A. & Simon, H. A. Human problem solving Human problem solving (Prentice-Hall, Oxford, England, 1972). Pages: xiv, 920. [79] Zhou, D. et al. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (2022). URL https://arxiv.org/abs/2205.10625. Version Number: 3. [80] Prasad, A. et al. ADaPT: As-Needed Decomposition and Planning with Language Models, 42264252 (Association for Computational Linguistics, Mexico City, Mexico, 2024). URL https://aclanthology.org/2024.findings-naacl.264. 27 Virtuous Machines: Towards Artificial General Science [81] Bandura, A. Social cognitive theory: An agentic perspective. Annual Review of Psychology 52, 126 (2001). Place: US Publisher: Annual Reviews. [82] Ryan, R. M. & Deci, E. L. Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist 55, 6878 (2000). Place: US Publisher: American Psychological Association. [83] Madaan, A. et al. Self-Refine: Iterative Refinement with Self-Feedback (2023). URL https://arxiv.org/abs/2303.17651. Version Number: 2. [84] Gou, Z. et al. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing (2023). URL https://arxiv.org/abs/2305.11738. Version Number: 4. [85] Shinn, N. et al. Reflexion: Language Agents with Verbal Reinforcement Learning (2023). URL https://arxiv.org/abs/2303.11366. Version Number: 4. [86] Baddeley, A. D. & Hitch, G. in Working Memory , Vol. 8 4789 (Elsevier, 1974). URL https://linkinghub.elsevier.com/retrieve/pii/S0079742108604521. [87] DEsposito, M. & Postle, B. R. The Cognitive Neuroscience of Working Memory. Annual Review of Psychology 66, 115142 (2015). URL https://www.annualreviews.org/doi/10. 1146/annurev-psych-010814-015031. [88] Desimone, R. & Duncan, J. Neural Mechanisms of Selective Visual Attention. Annual Review of Neuroscience 18, 193222 (1995). URL https://www.annualreviews.org/doi/10. 1146/annurev.ne.18.030195.001205. [89] Corbetta, M. & Shulman, G. L. Control of goal-directed and stimulus-driven attention in the brain. Nature Reviews Neuroscience 3, 201215 (2002). URL https://www.nature. com/articles/nrn755. [90] Risko, E. F. & Gilbert, S. J. Cognitive Offloading. Trends in Cognitive Sciences 20, 676688 (2016). URL https://linkinghub.elsevier.com/retrieve/pii/S1364661316300985. [91] Su, W., Tang, Y., Ai, Q., Wu, Z. & Liu, Y. DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models (2024). URL https://arxiv.org/abs/2403.10081. Version Number: 3. [92] Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020). URL https://arxiv.org/abs/2005.11401. Version Number: 4. [93] Huang, L. et al. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems 43, 155 (2025). URL http://arxiv.org/abs/2311.05232. ArXiv:2311.05232 [cs]. [94] Kinney, R. et al. The Semantic Scholar Open Data Platform (2023). URL http://arxiv. org/abs/2301.10140. ArXiv:2301.10140 [cs]. [95] Priem, J., Piwowar, H. & Orr, R. OpenAlex: fully-open index of scholarly works, authors, venues, institutions, and concepts (2022). URL https://arxiv.org/abs/2205.01833. Version Number: 2. [96] Skarlinski, M. D. et al. Language agents achieve superhuman synthesis of scientific knowledge (2024). URL https://arxiv.org/abs/2409.13740. Version Number: 2. [97] Wang, J., Wang, J., Athiwaratkun, B., Zhang, C. & Zou, J. Mixture-of-Agents Enhances Large Language Model Capabilities (2024). URL http://arxiv.org/abs/2406.04692. 28 Virtuous Machines: Towards Artificial General Science ArXiv:2406.04692 [cs]. [98] Bommasani, R. et al. On the Opportunities and Risks of Foundation Models (2022). URL http://arxiv.org/abs/2108.07258. ArXiv:2108.07258 [cs]. [99] Wei, J. et al. Emergent Abilities of Large Language Models (2022). URL https://arxiv. org/abs/2206.07682. Version Number: 2. [100] Fauconnier, G. & Turner, M. The way we think: Conceptual blending and the minds hidden complexities The way we think: Conceptual blending and the minds hidden complexities (Basic Books, New York, NY, US, 2002). Pages: xvii, 440. [101] Garner, K. G. & Dux, P. E. Knowledge generalization and the costs of multitasking. Nature Reviews Neuroscience 24, 98112 (2023). Place: United Kingdom Publisher: Nature Publishing Group. [102] Thorndike, E. L. Educational psychology. (Lemcke & Buechner, New York, 1903). URL https://content.apa.org/books/10528-000. [103] Woodworth, R. S. & Thorndike, E. L. The influence of improvement in one mental function upon the efficiency of other functions. (I). Psychological Review 8, 247261 (1901). Place: US Publisher: The Macmillan Company. [104] Bartlett, F. C. Remembering: study in experimental and social psychology Remembering: study in experimental and social psychology (Cambridge University Press, New York, NY, US, 1932). Pages: xix, 317. [105] Piaget, J. The origins of intelligence in children The origins of intelligence in children (W Norton & Co, New York, NY, US, 1952). Pages: 419. [106] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large Language Models are Zero-Shot Reasoners (2022). URL https://arxiv.org/abs/2205.11916. Version Number: 4. [107] Zhou, Y., Liu, H., Srivastava, T., Mei, H. & Tan, C. Hypothesis Generation with Large Language Models (2024). URL https://arxiv.org/abs/2404.04326. Publisher: arXiv Version Number: 3. [108] Felin, T. & Holweg, M. Theory Is All You Need: AI, Human Cognition, and Causal Reasoning. Strategy Science 9, 346371 (2024). URL https://pubsonline.informs.org/doi/ 10.1287/stsc.2024.0189. [109] Palan, S. & Schitter, C. Prolific.acA subject pool for online experiments. Journal of Behavioral and Experimental Finance 17, 2227 (2018). URL https://linkinghub.elsevier. com/retrieve/pii/S2214635017300989. [110] Peer, E., Brandimarte, L., Samat, S. & Acquisti, A. Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology 70, 153163 (2017). Place: Netherlands Publisher: Elsevier Science. [111] Peer, E., Rothschild, D., Gordon, A., Evernden, Z. & Damer, E. Data quality of platforms and panels for online behavioral research. Behavior Research Methods 54, 16431662 (2021). URL https://link.springer.com/10.3758/s13428-021-01694-3. [112] Gauthier, P. aider (2025). URL https://github.com/Aider-AI/aider. Original-date: 2023-05-09T18:57:49Z. 29 Virtuous Machines: Towards Artificial General Science [113] Bhattacharyya, M., Miller, V. M., Bhattacharyya, D. & Miller, L. E. Rates of Medical Content. 158289-high-rates-of-fabricated-and-inaccurate-references-in-chatgpt-generated-medical-content. Inaccurate References (2023). High in ChatGPT-Generated URL https://www.cureus.com/articles/ and Cureus Fabricated [114] Walters, W. H. & Wilder, E. I. Fabrication and errors in the bibliographic citations generated by ChatGPT. Scientific Reports 13, 14045 (2023). URL https://www.nature. com/articles/s41598-023-41032-5. [115] Wu, K. et al. An automated framework for assessing how well LLMs cite relevant medical references. Nature Communications 16, 3615 (2025). URL https://www.nature.com/ articles/s41467-025-58551-6. [116] Ashby, W. R. An introduction to cybernetics An introduction to cybernetics (John Wiley and Sons, Oxford, England, 1956). Pages: ix, 295. [117] Wiener, N. Cybernetics; or control and communication in the animal and the machine Cybernetics; or control and communication in the animal and the machine (John Wiley, Oxford, England, 1948). Pages: 194. [118] Kon, P. T. J. et al. Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents (2025). URL https://arxiv.org/abs/2502.16069. Version Number: 2. [119] Ebert, W. M., Jost, L., Jansen, P., Stevanovski, B. & Voyer, D. Visual working memory as the substrate for mental rotation: replication. Psychonomic Bulletin & Review 32, 12041216 (2025). URL https://link.springer.com/10.3758/s13423-024-02602-4. [120] Bays, P. M., Catalao, R. F. G. & Husain, M. The precision of visual working memory is set by allocation of shared resource. Journal of Vision 9, 77 (2009). URL http: //jov.arvojournals.org/Article.aspx?doi=10.1167/9.10.7. [121] Jacobs, C., Schwarzkopf, D. S. & Silvanto, J. Visual working memory performance in aphantasia. Cortex 105, 6173 (2018). URL https://linkinghub.elsevier.com/retrieve/pii/ S001094521730360X. [122] Wasserstein, R. L., Schirm, A. L. & Lazar, N. A. Moving to World Beyond < 0.05. The American Statistician 73, 119 (2019). URL https://www.tandfonline.com/doi/full/ 10.1080/00031305.2019.1583913. [123] Wasserstein, R. L. & Lazar, N. A. The ASA Statement on -Values: Context, Process, and Purpose. The American Statistician 70, 129133 (2016). URL https://www.tandfonline. com/doi/full/10.1080/00031305.2016.1154108. [124] Kirk, R. E. Practical Significance: Concept Whose Time Has Come. Educational and Psychological Measurement 56, 746759 (1996). URL https://journals.sagepub.com/doi/ 10.1177/0013164496056005002. [125] Cohen, J. The earth is round (p < .05). American Psychologist 49, 9971003 (1994). URL https://doi.apa.org/doi/10.1037/0003-066X.49.12.997. [126] Sullivan, G. M. & Feinn, R. Using Effect Sizeor Why the Value Is Not Enough. Journal of Graduate Medical Education 4, 279282 (2012). URL https://meridian.allenpress.com/ jgme/article/4/3/279/200435/Using-Effect-Sizeor-Why-the-P-Value-Is-Not-Enough. [127] Baker, M. 1,500 scientists lift the lid on reproducibility. Nature 533, 452454 (2016). URL https://www.nature.com/articles/533452a. 30 Virtuous Machines: Towards Artificial General Science [128] Camerer, C. F. et al. Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour 2, 637644 (2018). URL https://www.nature.com/articles/s41562-018-0399-z. [129] Open Science Collaboration. Estimating the reproducibility of psychological science. Science 349, aac4716 (2015). URL https://www.science.org/doi/10.1126/science.aac4716. [130] Gong, K. Open science: The science paradigm of the new era. Cultures of Science 5, 39 (2022). URL https://journals.sagepub.com/doi/10.1177/20966083221091867. [131] Kwa, T. et al. Measuring AI Ability to Complete Long Tasks (2025). URL https: //arxiv.org/abs/2503.14499. Version Number: 2. [132] Zhu, M. et al. AI Scientists Fail Without Strong Implementation Capability (2025). URL https://arxiv.org/abs/2506.01372. Version Number: 2. [133] DiCarlo, J., Zoccolan, D. & Rust, N. How Does the Brain Solve Visual Object Recognition? Neuron 73, 415434 (2012). URL https://linkinghub.elsevier.com/retrieve/pii/ S089662731200092X. [134] Geirhos, R. et al. Shortcut learning in deep neural networks. Nature Machine Intelligence 2, 665673 (2020). URL https://www.nature.com/articles/s42256-020-00257-z. [135] Ballon, M., Algaba, A. & Ginis, V. The Relationship Between Reasoning and Performance in Large Language Models o3 (mini) Thinks Harder, Not Longer (2025). URL https: //arxiv.org/abs/2502.15631. Version Number: 1. [136] Lin, B. Y. et al. ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning (2025). URL https://arxiv.org/abs/2502.01100. Version Number: 2. [137] Lou, J. & Sun, Y. Anchoring Bias in Large Language Models: An Experimental Study (2024). URL http://arxiv.org/abs/2412.06593. ArXiv:2412.06593 [cs]. [138] Li, Y. et al. Beyond Single-Turn: Survey on Multi-Turn Interactions with Large Language Models (2025). URL http://arxiv.org/abs/2504.04717. ArXiv:2504.04717 [cs]. [139] Furnham, A. & Boo, H. C. literature review of the anchoring effect. The Journal of Socio-Economics 40, 3542 (2011). URL https://linkinghub.elsevier.com/retrieve/pii/ S1053535710001411. [140] Strack, F. & Mussweiler, T. Explaining the enigmatic anchoring effect: Mechanisms of selective accessibility. Journal of Personality and Social Psychology 73, 437446 (1997). URL https://doi.apa.org/doi/10.1037/0022-3514.73.3.437. [141] Tversky, A. & Kahneman, D. Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science 185, 11241131 (1974). URL https://www.science.org/doi/10.1126/science.185.4157.1124. [142] Kolb, D. A. Experiential learning: experience as the source of learning and development (Prentice-Hall, Englewood Cliffs, N.J, 1984). [143] Gopnik, A. et al. Theory of Causal Learning in Children: Causal Maps and Bayes Nets. Psychological Review 111, 332 (2004). URL https://doi.apa.org/doi/10.1037/0033-295X. 111.1.3. [144] King, R. D. et al. The Automation of Science. Science 324, 8589 (2009). URL https: //www.science.org/doi/10.1126/science.1165620. 31 Virtuous Machines: Towards Artificial General Science [145] Jones, B. F. The Burden of Knowledge and the Death of the Renaissance Man: Is Innovation Getting Harder? Review of Economic Studies 76, 283317 (2009). URL https://academic.oup.com/restud/article-lookup/doi/10.1111/j.1467-937X.2008.00531.x. [146] Rosenthal, R. The file drawer problem and tolerance for null results. Psychological Bulletin 86, 638641 (1979). URL https://doi.apa.org/doi/10.1037/0033-2909.86.3.638. [147] Liu, Y. et al. Prompt Injection attack against LLM-integrated Applications (2023). URL https://arxiv.org/abs/2306.05499. Version Number: 2. [148] Beck, U. Risk society: towards new modernity repr edn. Theory, culture and society (Sage, London, 1992). [149] Jasanoff, S. (ed.) States of knowledge: the co-production of science and social order transferred to digital print edn. International library of sociology (Routledge, London, 2010). [150] Laudan, L. Science and Values: The Aims of Science and Their Role in Scientific Debate No. 3 in Pittsburgh Series in Philosophy and History of Science (University of California Press, Berkeley, CA, 1986). [151] Humphreys, P. The philosophical novelty of computer simulation methods. Synthese 169, 615626 (2009). URL http://link.springer.com/10.1007/s11229-008-9435-2. [152] Leonelli, S. Data-centric biology: philosophical study (The University of Chicago Press, Chicago London, 2016). 32 Virtuous Machines: Towards Artificial General Science"
        },
        {
            "title": "Table of Contents",
            "content": "1. Study 1 Autonomously Generated Manuscript Independence of visual working memory precision and mental rotation performance: theoretical and methodological implications 2. Study 2 Autonomously Generated Manuscript Imagery vividness fails to predict serial dependence in visual working memory and mental rotation 3. Study 3 Autonomously Generated Manuscript Visual memory precision shows negligible spatial task links Appendix 1 Study 1 Autonomously Generated Manuscript Appendix 2 Study 2 Autonomously Generated Manuscript Appendix 3 Study 3 Autonomously Generated Manuscript"
        }
    ],
    "affiliations": [
        "Canadian Institute of Advanced Research (CIFAR), Toronto, Canada",
        "Explore Science, Brisbane, Australia",
        "Queensland Brain Institute, The University of Queensland, Brisbane, Australia",
        "School of Psychology, The University of Queensland, Brisbane, Australia",
        "School of Psychology, The University of Sydney, Sydney, Australia"
    ]
}