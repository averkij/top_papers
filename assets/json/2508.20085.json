{
    "paper_title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation",
    "authors": [
        "Zhecheng Yuan",
        "Tianming Wei",
        "Langzhe Gu",
        "Pu Hua",
        "Tianhai Liang",
        "Yuanpei Chen",
        "Huazhe Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/."
        },
        {
            "title": "Start",
            "content": "HERMES: Human-to-Robot Embodied Learning from Multi-SouRce"
        },
        {
            "title": "Motion Data for MobilE DexterouS Manipulation",
            "content": "Zhecheng Yuan1,2, Tianming Wei1,2, Langzhe Gu1,2, Pu Hua1,2, Tianhai Liang1,2, Yuanpei Chen3, Huazhe Xu1,2 1 5 2 0 2 8 2 ] . [ 2 5 8 0 0 2 . 8 0 5 2 : r AbstractLeveraging human motion data to impart robots with versatile manipulation skills has emerged as promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, highdimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to realworld scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/. Index TermsBimanual dexterous manipulation, Mobile manipulation, Sim2real, Reinforcement learning, Learning from human motion. I. INTRODUCTION CHIEVING human-level dexterity for robots has long been central challenge in robotic research. The prospect of bimanual robotic systems with dexterous hands that mirror human sensorimotor dexterity holds the promise of seamlessly integrating robots into daily human activities and environments. Despite notable progress, how to capitalize on the abundance of available human data and develop algorithms suited to intricate and high-precision dexterous manipulation remains underexplored. Humans continuously generate diverse bimanual manipulation data, inherently serving as natural guidance for robots to emulate human-like behaviors. Several previous studies [1] [5] have attempted to extract trajectories of human hands and manipulated objects from video data, subsequently applying them to robotic manipulation tasks. Nevertheless, these methods have predominantly targeted robots equipped with simple parallel gripper end effectors, failing to generalize indicates equal contribution. 1 Tsinghua University 2 Shanghai Qi Zhi Institute 3 Peking University Corresponding to huazhe xu@mail.tsinghua.edu.cn. effectively to dexterous hands due to the vastly greater complexity of action space. Despite recent advances that utilize kinematic retargeting approaches to produce humanlike robotic motions [6][10], these approaches still fall short in achieving physically-aware pose retargeting and bridging the embodiment gap to derive feasible robot actions capable of successfully accomplishing the intended tasks. critical limitation lies in omitting the modeling of interactions between robotic hands and manipulated objects, fundamental component of manipulation tasks. Consequently, neglecting these interactions undermines the robots ability to fully understand and adapt to the dynamics of manipulation scenarios. Therefore, in an attempt to address the aforementioned challenges, recent approaches have begun leveraging Reinforcement Learning (RL) paradigms [11][13], allowing robots to autonomously explore feasible motion strategies under the guidance of kinematic reference trajectories. These methods commonly design general reward functions encompassing object tracking, hand configurations, and collision dynamics. Maximizing such rewards drives the robot toward successful execution of complex manipulation tasks. Nonetheless, existing works [11][13] typically draw on limited human motion data sources and some have not transferred the trained robot behaviors to the physical world. Such limitations not only hinder the evaluation of whether the learned policies exhibit behaviorally plausible performance in the real world, but also preclude the integration of sim2real methodologies necessary for robust policy control and for deployment across various environmental conditions. Furthermore, current methods for sim2real transfer of bimanual dexterous manipulation predominantly rely on explicit extraction of object and robot state information [11], [14][16], thus failing to achieve end-toend visual learning. This limitation inherently confines learned policies to specific fixed setups, significantly hindering their adaptability to diverse scenarios. Motivated by these challenges, we propose HERMES, versatile human-to-robot embodied learning framework tailored for mobile bimanual dexterous hand manipulation. HERMES offers the following three advantages: Diverse sources of human motion: Our framework supports several human motion sources, including teleoperated simulation data, motion capture (mocap) data, and raw human videos. We also provide corresponding approaches for data acquisition, enabling HERMES to efficiently transform varied human motion data into robot-feasible behaviors through RL. Furthermore, these tasks share uniform set of reward terms, obviating the necessity of designing intricate and task-specific reward 2 Fig. 1. HERMES exhibits rich spectrum of mobile bimanual dexterous manipulation skills. The robot is able to navigate over extended distances in both indoor and outdoor environments, and effectively execute variety of complex manipulation tasks in unstructured, real-world scenarios, drawing upon behaviors learned from only one-shot human motion. functions. In contrast to the methods that depend on collecting large amount of demonstrations, we can achieve generalizable policy by augmenting single reference human motion trajectory coupling with RL training. End-to-end vision-based sim2real transfer: HERMES facilitates robust vision-based sim2real transfer by employing DAgger distillation, which converts state-based expert policies into vision-based student policies. Moreover, we introduce generalized, object-centric depth image augmentation and hybrid control approach, effectively bridging the perception and dynamic sim2real gap. Mobile manipulation capability: Our method endows robots with mobile manipulation skills. Building upon ViNT [17], we develop RGB-D based module for precise localization wherein the task is modeled as Perspective-n-Point (PnP) problem and addressed through an iterative process. This ensures seamless integration with subsequent manipulation tasks and unlock the policys capacity to operate autonomously across broad spectrum of real-world environments. With the integration of these capabilities, HERMES is empowered to execute wide range of complex or longhorizon mobile bimanual dexterous manipulation tasks across varied and unstructured real-world environments. We conduct extensive experiments in both simulation and real-world settings. The experimental results demonstrate that HERMES achieves superior performance with high task success rates and high sample efficiency. The learned policies can not only successfully transfer to the physical world but also exhibit generalization capabilities. Furthermore, the navigation component of HERMES demonstrates precise localization ability, facilitating effective deployment of trained manipulation policies in diverse in-the-wild scenarios. II. RELATED WORK A. Dexterous Manipulation from Human Demonstrations By harnessing human motion as the fuel of robot data, robots can acquire natural and versatile behaviors [10], [18] [24]. In contrast to the typically scarce and costly teleoperation datasets, human motion data offers more abundant and economically accessible resource for training robots. Recent advancements [1][5] have seen numerous studies leveraging human videos to extract features that assist in downstream policy learning. However, these efforts predominantly focus on transferring knowledge to robots with parallel-jaw endeffectors, often overlooking the necessity of bridging the embodiment gap and considering the complexity of high action space. In contrast, dexterous hands present greater challenges in modeling grasping postures and interactions with objects. Some recent works [6], [25], [26] have employed customized, high-precision equipment to collect egocentric human videos the for training robot policies, yet complexity of hand-object interactions. Additionally, several recent studies [11], [12], [27] utilize reinforcement learning to translate human hand motions into robotic behaviors, but these approaches are typically limited to simple, single-object tasks or lack closed-loop sim2real implementations. With they neglect to model suite of innovative designs, HERMES overcomes these shortcomings and equips the robot to perform various challenging, highdegree-of-freedom bimanual dexterous manipulation tasks. Moreover, we effectively transform diverse types of human motion into deployable robot policies. B. Vision-based Sim2real Manipulation Sim2real has achieved notable advancements in locomotion, with policies trained in simulation successfully deployed on quadruped or humanoid robots to perform agile motions [28] [33]. In terms of manipulation, recent research has increasingly focused on leveraging simulation to generate or augment training data to obtain deployable visuomotor policies [34] [40]. However, manipulation tasks, particularly those involving bimanual dexterous manipulation, present greater challenges for sim2real transfer due to the presence of high-frequency, fine-grained visual information. Several approaches [14][16], [41] have utilized depth cameras to extract object poses, shapes, and other relevant information, combining these with proprioceptive state data to facilitate sim2real transfer. While these methods can accurately obtain robot and object state information, the resulting policies often lack the ability to visually perceive environmental and object changes, which in turn forces manual specification of numerous state variables. Another line of works [36], [42], [43] employ extensive domain randomization to train generalizable visual policies. However, achieving diversity in such randomization schemes typically involves labor-intensive engineering efforts. Recent studies [44], [45] have explored the use of Gaussian splatting [46] to achieve photorealistic rendering, but this approach requires manual scanning of entire scenes and training tailored to specific robot embodiments and environments, which limits scalability. Depth images inherently preserve object shape and spatial structure, enabling efficient sim2real transfer without the need to overcome variations in textures [47], [48]. For instance, DextrAH-G [48] utilizes depth maps for sim2real transfer but necessitates manually designing finely tuned and specific noise. Here, we also leverage depth images to facilitate egocentric sim2real transfer in an end-to-end fashion. In contrast to DextrAH-G, our approach adopts more generalizable depth image augmentation strategy, which eliminates the need for camera-specific noise modeling. By applying our depth image processing method, we not only achieve strong semantic alignment between simulated and real-world observations but also obtain robust and generalizable visuomotor policies capable of handling complex manipulation tasks. C. Mobile Manipulation Mobile manipulation tasks further compound the challenges of robotic manipulation [49][54]. The robot should typically complete navigation phase before proceeding with the manipulation task. Many recent studies [52], [53], [55], [56] have adopted modular mobile manipulation framework and utilized pre-built maps for robot mobile manipulation. OKRobot [52] uses 3D map of the room built with iPhone and leverages an open-vocabulary object detector to perform 4 the RealSense L515 to capture RGBD observations and the RERVISION Fisheye camera for navigation. B. Simulation Design We employ both the MuJoCo [62] and MJX simulation platforms [63] to construct training environments tailored for our mobile bimanual robotic system. The kinematic structure of the robot is carefully modeled, and relevant dynamic parameters are configured to ensure stable behavior in the simulation. is configured to match The actuation range of each joint that of the physical robot. For the dexterous hands, whose fingers contain passive joints not directly actuated by motors, we leverage MuJoCos capabilities for modeling closed chain mechanisms to faithfully simulate these passive DoFs, which achieves high-fidelity reproduction of the hands physical interactions. In contrast to the conventional approaches of simulating inter-link motion relationships through mimic joints or tendon-based mechanisms, we utilize the equality constraint feature in MuJoCo to directly construct linkage structures as defined in the original CAD models. This formulation affords more precise representation of the motion dependencies among passive DoFs, leading to more faithful simulation of constrained multi-link dynamics. Furthermore, to enhance the stability of interactions between the robot and manipulated objects, we restructure the collision modeling scheme. Rather than performing collision detection based directly on the original mesh representations of the objects and the hand, we approximate their geometries using primitive shapes. This abstraction facilitates more granular and stable collision computation. The Inverse Kinematics (IK) control of the robotic system is implemented using the Mink library [64]. IV. REINFORCEMENT LEARNING METHOD A. Task Formulation learning. Our goal"
        },
        {
            "title": "We define each task under",
            "content": "the framework of goalis to enable conditioned reinforcement the robot the desired to acquire the capability to convert kinematic motion trajectory into physically executable robot behaviors. The tasks are formulated as the Markov Decision Process (MDP) = (S, A, , R, γ, G) , where is the state space, is the action space, is the transition function, is the reward, γ is the discount factor, is the reference trajectory. For goal-conditioned reinforcement learning, the state includes the proprioception information sp and the goal state sg from G, and the reward function at timestep is defined as rt = R(sp ). Under such formulation, the policy is able to acquire complex and fine-grained behaviors from diverse reference trajectories without relying on laborintensive reward engineering. , sg B. Collect One-shot Human Motion To validate the effectiveness and robustness of HERMES, we employ three distinct sources of human motion: teleoperation in simulation, motion capture data obtained from public datasets, and hand-object poses extracted from raw videos. Fig. 2. System Design. We construct unified setup of mobile bimanual robots equipped with dexterous hands in both simulation and the real world. Through high-fidelity simulation, this robotic platform is capable of enabling sim2real transfer across wide range of complex manipulation tasks. open-vocabulary object navigation with the map. It also adopts AnyGrasp [55] combined with LangSam [56] to perform openvocabulary grasping in the real world. COME-robot [53] utilizes global object map built by the robot for globallevel perception. It leverages GPT-4V [57] to perform target object perception and task planning. However, these mapbased methods are fundamentally constrained to small indoor spaces, as they become computationally intractable for largescale environments like outdoor areas. They also struggle in feature-deprived settings or mixed indoor-outdoor navigation scenarios, where reliable localization becomes particularly challenging. Another research explores end-to-end training paradigms that unify mobile navigation and manipulation into single framework [58][61]. However, such approaches not only demand larger number of demonstrations or taskspecific training, but also fail to leverage prior knowledge from navigation systems, thus constraining them to short-horizon tasks. To equip visuomotor policies with in-the-wild navigation capabilities, we incorporate the image-goal navigation foundation model that relies exclusively on RGB inputs and enables generalizable navigation, while being seamlessly integrated with the trained manipulation policy. line of III. SYSTEM DESIGN We build mobile bimanual robot equipped with two dexterous hands. To enable large-scale training in simulation, we also construct corresponding high-fidelity simulation models that accurately replicate the physical characteristics and kinematic structures of the real-world system. A. Hardware Design As shown in Figure 2, our robotic system is constructed by integrating an X1 mobile base, two 6-DoF Galaxea A1 arms, and two OYMotion 6-DoF dexterous hands. The torso part is assembled using lightweight aluminum alloy tubing to ensure structural rigidity. Additionally, we use laptop with an NVIDIA RTX 4090 GPU for supporting ROS control and network inference. Regarding the camera, we adopt 5 Fig. 3. The main pipeline of HERMES. HERMES comprises four-stage pipeline for achieving mobile bimanual dexterous manipulation through sim2real transfer. First, we acquire one-shot human demonstration drawn from diverse sources. Then, in stage 2, we train state-based RL teacher policy, then apply DAgger to distill it into vision-based student policy. Following this, HERMES execute long-horizon navigation using ViNT, followed by closed-loop PnP to finely adjust the robots pose and achieve precise alignment in stage 3. Once localization is achieved, the student policy is deployed in zero-shot fashion directly in the real world. Moreover, by leveraging merely single human reference trajectory in conjunction with RL training, we are able to derive the generalizable robot policy without the need for collecting extensive demonstrations. Teleoperation in simulation: We provide access to the preconfigured simulation that enables direct teleoperation of the robot for collecting demonstrations. The Apple Vision Pro is utilized to extract hand poses and arm movements, with data captured at frequency of 75 Hz. Mocap data: In contrast to direct teleoperation in simulation, retargeting mocap data to robotic hands presents significant challenges due to the embodiment gap between human and robotic hand structures. This discrepancy renders the retargeted trajectories from mocap data unsuitable for direct replay in simulation. Consequently, RL is often employed to enable robots to learn the desired behaviors from reference trajectories. In our study, we acuqire human motion capture data from the OakInk2 [65] dataset. Fig. 4. Pose extraction from videos. We utilize FoundationPose to extract the pose trajectories of multiple objects and employ WiLoR to capture the poses of both hands along with the positions of their finger joints. Fig. 5. The visualization of hand motion trajectory. We utilize WiLoR along with PnP algorithm to precisely transform the estimated hand poses into the robots frame. Extracted arm and hand poses from videos: Leveraging video data holds considerable promise for unlocking vast quantities of information to facilitate robot learning. To this end, we also provide pipeline for extracting human hand trajectories directly from raw video. To poses and object acquire the hand poses, we first employ WiLoR [66] to detect the hands in each video frame and extract 2D hand keypoints along with their corresponding 3D counterparts. We then select relatively stable subset of keypoints for the subsequent estimation, specifically those located at the wrist and the metacarpophalangeal joints. The spatial translation of the wrist in the camera coordinate system is estimated by solving Perspective-n-Point (PnP) problem [67] based on the 2D-3D correspondences, while the palms orientation is derived by fitting plane to the selected 3D keypoints. The extraction results are shown in Figure 5. Regarding the manipulated objects, we employ FoundationPose [68] to estimate the object poses directly from video frames, and utilize ARCode [69] scanning to reconstruct the object mesh. By leveraging the aforementioned procedures, we can align the hand and object poses extracted from the video with the robots frame to facilitate the subsequent learning process. Synthesize multiple trajectories: To obtain more generalizable policy, we perform the trajectory augmentation for the one-shot human motion reference by randomizing positions and orientations of the objects in predefined range. The hand and object poses across the augmented trajectories are transformed as follows: ˆApose [τk] = Ttrans Apose [τk] . For any given frame in the trajectory τ , we apply transformation matrix Ttrans to alter its pose, where Apose may represent either the object pose or the hand pose. By editing the reference trajectory, we enable spatial generalization from single human motion demonstration, obviating the need to manually collect large numbers of teleoped demonstrations. (1) Upon obtaining synthesized object and hand trajectories from various data sources, we initially employ DexPilot [70], popular retargeting method to map the captured human hand poses onto corresponding robot hand configurations. Subsequently, reinforcement learning is leveraged to refine and adapt the initialized robot behaviors. C. Generalizable Reward Design for Manipulation Standard reinforcement learning typically relies on handcrafted reward functions tailored to each specific task. However, designing such complicated reward structures often impedes scalability and usability, particularly for the dexterous hand. To alleviate this issue, we leverage one-shot human demonstration combined with generalizable reward formulation so that one unified reward function can be reused across tasks and simplify the specific design of challenging, longhorizon manipulation tasks. Recent advancements in locomotion [31], [71][73] have predominantly adopted such design paradigms for training humanoid or quadruped robots. However, manipulation tasks introduce additional complexities, as robots must not only perceive their own proprioception states but also effectively interact with external objects. Therefore, we propose generalizable reward framework tailored for manipulation tasks, capable of being applied across diverse tasks. Specifically, we design the following three reward terms: Object-centric distance chain: Capturing the dynamic spatial relationships between the human hands and the object 6 Fig. 6. Object-centric distance chain. This reward term is computed by tracking the temporal variations of vectors formed between the object center and each fingertip as well as the palm of both hands. stands as pivotal factor in enabling the policy to acquire fine-grained hand-object interaction skills. As illustrated in Figure 6, we designate the coordinates of the fingertips and palm of the hand, along with the center of the objects collision mesh, as keypoints. By modeling the temporal variation of vectors between these keypoints, we formulate the following reward function: rchain = (cid:40) exp (cid:110) 1 (cid:80)n i=1 (cid:13) (cid:13)r(i) (cid:13) ref r(i)(cid:13) (cid:111) (cid:13) (cid:13) , 0, if Ncontact Nnum otherwise, (2) where r(i) is the vector from object center to the fingertip or palm. Furthermore, we incorporate contact information into this reward term. Specifically, during the computation of the distance chain, we also evaluate the number of contact points between the fingertips and palms of both hand mesh Chand and the objects collision mesh Cobj. This reward component is activated only when the number of contact points Ncontact exceeds predefined threshold Nnum, ensuring that the policy attends to physically meaningful hand-object interactions. Ncontact = 1 2 (cid:88) (cid:88) i 1(Ci hand, Cj obj), (3) where 1(, ) is the indicator function that evaluates whether collision occurs between the hand and the object. In addition, we also encourage the robot to favor joint positions in the retargeted trajectories during exploration. Object trajectory tracking: For manipulation tasks which adopt human motion, critical indicator of policy success lies in its ability to track and follow the desired object trajectory. To this end, we introduce an additional reward component that explicitly aligns the policys behavior with the target objects trajectory: (cid:16) robj = exp k1 pobj pref2 k2 (dquat(qobj, ref ))2(cid:17) (4) where k1, k2 are the coefficients corresponding to the position and orientation terms in the robj. pobj and qobj represent the current position and orientation of the object, while pref and qref denote the position and orientation along the reference trajectory. The term dquat measures the distance between two quaternions. Power penalty: In order to enhance the smoothness of policy execution, we design power penalty reward to alleviate the jittering actions: rpenalty = λ (cid:88) fj qj , (5) jJhand where λ is the coefficient of rpenalty. Jhand denotes the set of all hand joints. fj is the actuation force applied at joint j, and qj is the velocity of joint j. By integrating all these reward components, the policy is endowed with the capacity to tackle wide spectrum of challenging and diverse manipulation tasks. D. Residual Action Learning While human trajectories provide coarse guidance on arm and hand movements throughout task execution, they often lack the precision required for accurate object interactions. To address this, we design distinct learning strategies for the arm and hand. For the arm, we decompose actions into coarse and fine components. At each timestep t, the coarse action ag is directly derived from the human trajectory, offering general movement direction. The fine component af is predicted by learned network af = π(s), refining the motion to ensure precise control. Hence, the final form of arm action aarm is aarm = af + ag. Regarding the hand part, due to inaccuracies in retargeting human demonstrations, we entirely adopt the network output ahand to model the interaction behaviors with objects. This hybrid approach allows the policy to capture the overarching structure of human motion while learning the nuanced adjustments necessary for effective and precise manipulation for robots. In addition, given the high-dimensional action space and inherent complexity of bimanual dexterous hand tasks, we incorporate the early termination strategy to curtail inefficient exploration [72]. Meanwhile, to enable the acquisition of an initial viable behavior, we disable object collisions during the early stages of training, allowing the policy to first focus on learning the approximate motion trajectory. E. Reinforcement Learning Algorithm We implement two distinct reinforcement learning algorithms. DrM [74], an off-policy method, leverages dormant ratio mechanism [75] to enhance exploration capabilities and demonstrates high sample efficiency. We adapt the network architecture and take the state observation as inputs for statebased RL training. DrM training for all tasks is conducted in the MuJoCo simulation environment. Concurrently, we also instantiate the tasks in the MJX platform, which supports GPUaccelerated parallel training, and employ the PPO algorithm to significantly reduce the overall wall-clock training time. More details can be found in Appendix A. V. SIM-TO-REAL TRANSFER The training of state-based RL policies typically relies on privileged information which is not accessible in realworld deployment scenarios. Consequently, it is imperative to distill the state-based policy into visual policy for achieving sim2real transfer. A. Leveraging Depth Image as Visual Input Prior work [28][30], [48] has explored the use of depth images for vision-based sim2real transfer. However, they often necessitate intricate and highly customized augmentation strategies to bridge the gap. In this work, we introduce more versatile, manipulation-tailored egocentric depth-image augmentation method. Specifically, we clip depth values beyond threshold distance (set per task). For real depth images, missing depth values resulting from edge capture failures are filled in with the maximum depth. To emulate real-world edge noise and blur in simulation, we augment simulated depth images by adding Gaussian noise and Gaussian blur during training. Additionally, to mimic missing depth values, we randomly set 0.5% of pixel values in simulation-rendered images to the maximum depth. To enrich the diversity of depth-noise distributions, we further employ the NYU Depth Dataset [76] and adopt mixup strategy that linearly blends the simulation-rendered depth image osim with certain data depth map odataset: ˆo = αosim +(1α)odataset, where α is the coefficient. As illustrated in Figure 7, our augmentation not only semantically aligns simulated renderings with realworld depth images, but also preserves crucial depth disparity cues essential for accurate visuomotor control. Furthermore, we visualize the distribution of depth values under similar frames in both simulation and real-world settings. As illustrated in Figure 8, our processing approach leads to close alignment in value distributions between real-world and simulated depth images, indicating reduced sim2real gap. B. DAgger Distillation Training In DAgger training, the state-based expert policy acts as the teacher to guide the learning of visual student policy. In contrast to prior approaches that distill to object masks or segmented images, HERMES directly distills the state into raw visual observations of entire visual scenarios. This design obviates the need for explicit camera calibration and facilitates the acquisition of the robots in-the-wild generalization capabilities. Furthermore, we introduce series of auxiliary design choices aimed at enhancing both the asymptotic performance of DAgger training. Model architecture: The input observations are rendered at resolution of 140 140 pixels and subsequently stacked into sequences of 3 consecutive frames before being passed to the image encoder. Consistent with the design of Yuan et al. [36], [77], we utilize the first two layers of ResNet-18 encoder [78] to more effectively capture fine-grained visual details. Moreover, to ensure distributional consistency between training and evaluation phases, we replace all BatchNorm layers in the encoder with GroupNorm [79]. Trajectory rollout scheduler: At the beginning of DAgger training, we adopt the expert policy to roll out trajectories for supervising the student. As training progresses, the reliance on expert rollouts is gradually reduced by annealing the probability p, while proportionally increasing the student policys participation in rollouts. The pseudocode for the DAgger training procedure is presented in Algorithm 1, where Nepochs 8 Fig. 7. Depth image visualization. We present visual comparison between simulated and real-world depth maps across two different tasks. Notably, after applying our preprocessing pipeline, the depth representations of the hand and object exhibit strong semantic correspondence, highlighting the efficacy of HERMES in bridging the sim2real gap. Algorithm 1 DAgger Distillation Training trajectory rollout scheduler Scheduler(p0) for 0 to Nepochs do for 0 to do Rollout Trajectory astudent πstudent(ostudent) aexpert πexpert(oexpert) S.RANDOM CHOOSE (astudent, aexpert, pt) {(ostudent, aexpert)} ostudent,t+1, oexpert,t+1 ENV.STEP(a) pt+1 S.STEP() end for πstudent UPDATE(πstudent, D) end for Policy Training noise into the proprioception states during training. More details and experimental results can be found in Appendix and Appendix H. C. Hybrid Sim2real Control Given the quasi-static nature of our tasks, we adopt hybrid control strategy to mitigate the gap between simulation and real-world dynamics: real-world visual observations are used to infer the actual action, which is then applied to the simulation environment to perform forward step. The updated joint positions of the simulated robot are subsequently transferred to the real robot for execution. Following this, the camera captures the image of the updated physical status Fig. 8. Depth intensity distribution. The horizontal axis represents the depth values, while the vertical axis indicates their corresponding proportions. This figure illustrates that the depth distributions derived from simulation and realworld images exhibit notable resemblance in value patterns. denotes the total number of training epochs, represents the trajectory rollout intervals, and refers to the replay buffer. At each interaction step with the environment, the action is determined by the trajectory rollout scheduler, which probabilistically selects between the student actions astudent and expert actions aexpert based on dynamically adjusted probability p. Here, we employ an exponential decay schedule to reduce the probability. We jointly optimize the student policy using combination of L1 and L2 action loss terms. Additionally, to mitigate the overfitting to the low-dimensional states, we inject uniform 9 B. Closed-loop PnP Localization For our mobile manipulation tasks, moderate discrepancies between the robots final pose and the target pose can lead to the manipulation policy failing to finish the task. However, ViNT does not guarantee termination within sufficiently tight error bound. To address this, we introduce local refinement step after ViNT completes navigation: closed-loop Perspective-n-Point (PnP) localization algorithm is employed to adjust the robot pose, ensuring closer alignment with the target pose. As shown in Figure 10, we first utilize the neural feature matching module Efficient LoFTR [82] to detect the correspondence between the current robot captured image Ic and the goal image Ig. Next, the detected features are lifted to 3D space with respect to the robots current coordinate frame by leveraging the camera intrinsic matrix and the depth map Dc. This yields Xa, the 3D coordinates of the matched features in the current camera coordinate frame. According to Equation 6, we then leverage the RANSAC PnP [83] and refine PnP algorithm [84], [85] to compute the relative rotation R33 and translation R3 between the robots current viewpoint and the goal pose that can minimize the reprojection : (cid:13) (cid:13) 2 Pg Pg (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:21) (cid:20) Pg 1 = K[R t] (cid:20) Xa (cid:21) , (6) the goal where Pg denotes the pixel positions of the matched image, Pg represents the reprojected features at positions. Adopting Efficient LoFTR can yield an inference rate of 16.7 Hz. By leveraging real-time feedback from PnP as the robot incrementally converges toward the target pose, we are able to iteratively refine the pose estimation, thus attaining more accurate visual correspondence. After getting the target pose calculated by our closed-loop PnP localization algorithm, we utilize Proportional-IntegralDerivative (PID) controller [86] to adjust the pose of our robot. The input of the controller is the instantaneous position and orientation error between the robots desired state and its actual state. Since the mobile base exhibits limited control accuracy, we define separate PID controllers for reducing errors individually along the and directions, as well as for the yaw angle. Based on this multi-dimensional error input, the PID controller computes and outputs corresponding planar velocity commands designed to minimize the error. These commands consist of velocities along the corresponding three directions. The velocity is computed as follows: vj = Kp,jej + Ki,j (cid:90) 0 ej(τ )dτ + Kd,j dej dt (7) where denotes specific axis, vj is the velocity, ej represents the error term, and Kp,j, Ki,j, Kd,j correspond to the proportional, integral, and derivative gain coefficients, respectively. To address the characteristics of our omnidirectional chassis, which incurs additional displacement during wheel reorientation, we implement sequential adjustment strategy, which Algorithm 2 Hybrid Sim2real Control error = Fig. 9. Hybrid Sim2real Control. We leverage real-world observations to infer actions and utilize simulation to compute the corresponding joint values, which are subsequently mapped onto the real robots. This hybrid strategy effectively mitigates the sim2real gap. of the real robot and incorporates the proprioception states as the input observation for the next inference cycle. The pseudocode of our control is provided in Algorithm 2. By sharing the same Inverse Kinematics (IK) method and dynamic parameters across simulation and the real world, this approach not only enables the policy to adapt its behavior based on realworld environmental variations but also effectively narrows the sim2real discrepancy. The pipeline is shown in Figure 9. for 0 to do Real-world Policy Execution at πpolicy(oreal obs,t) osim,t+1 ENV.STEP(at) Jall joints GET ALL CURRENT JOINT QPOS IN SIM() oreal obs,t+1 SET ALL QPOS IN REAL(Jall joints) end for VI. NAVIGATION METHODOLOGY This section elaborates on how to equip the visuomotor control policy with the capacity to perform mobile manipulation tasks. A. ViNT Navigation Foundation Model To endow the trained visuomotor policy with navigation capabilities, HERMES integrates an image-goal navigation foundation model [17], [80], [81] that operates solely on RGB inputs and supports long-horizon, in-the-wild navigation. This framework allows for seamless and low-cost fusion of manipulation and navigation modules, without necessitating additional fine-tuning of either component. We choose ViNT [17] for achieving image-goal robotic navigation. ViNT searches for the goal observations in the constructed topological map and computes sequence of relative waypoints based on the current and goal observations, which are then translated into actions to control the low-level mobile controller. We deploy ViNT on our customized robotic system, operating at frequency of 7.6 Hz. ViNT not only enables long-range, inthe-wild navigation but also demonstrates effective zero-shot generalization capability without necessitating model finetuning. 10 Fig. 10. The pipeline of closed-loop PnP localization. We first employ the Efficient LoFTR to extract dense visual correspondence, followed by estimating the transformation between the current frame and the goal location via solving the PnP problem. Subsequently, we use PID controller to execute the action. This entire process is executed in closed-loop manner and continues iteratively until the spatial discrepancy between the robots current pose and the goal falls below predefined threshold. prioritizes error correction in the order of x-direction, ydirection, yaw orientation. This staged compensation mitigates the coupling effects introduced by wheel realignment. VII. EXPERIMENTS In this section, we perform an extensive series of experiments aimed at evaluating the capabilities of HERMES across various aspects, including navigation and manipulation. Specifically, our primary experiments are designed to: (1) verify the efficacy of HERMES in efficiently and robustly transforming diverse human motion data into robot-plausible behaviors; (2) exhibit the effectiveness of our method in sim2real transfer; (3) quantify the accuracy and reliability of our navigation localization approach; (4) demonstrate the effectiveness of HERMES in mobile manipulation. Putoff Burner, where interactions involve only single object, ObjDex is able to complete the tasks; however, HERMES can achieve higher sample efficiency during training. Furthermore, in more intricate tasks involving multi-object interactions, ObjDex consistently fails, irrespective of the type of human motion data provided. Owing to our object-centric distance chain, HERMES is capable of robustly acquiring diverse manipulation skills even in long-horizon, multi-object environments. Moreover, HERMES demonstrates high sample efficiency and successfully learns policies in 3M training steps. B. Comparison with Non-learning Approach In this section, we highlight the central role of reinforcement learning in equipping robots with adaptive policies and delineate the advantages it affords over non-learning approaches. A. Sample Efficiency of HERMES We evaluate the training sample efficiency of HERMES across seven tasks. The visualizations of simulation tasks are shown in Figure 13. For each task, the source of the oneshot human motion demonstration is indicated in the title of each sub-figure in Figure 11. The vertical axis in the figure represents the proportion of the trajectory length successfully executed by the current policy relative to the total length of the trajectory. As demonstrated in Figure 11, regardless of the origin of the human motion data, HERMES reliably succeeds in converting human hand and arm actions into generalizable robot-executable behaviors. Additionally, we compare training performances with ObjDex [11]. ObjDex defines its reward based on the tracking of the objects joint movement, translations, and orientations. We re-implement this reward formulation within our own algorithmic framework. Figure 11 indicates that HERMES exhibits superior performance relative to ObjDex across all tasks. In tasks such as Bottle Handover, Flower Vase, and TABLE COMPARISON OF HERMES AND KINEMATIC RETARGETING. Tasks Method"
        },
        {
            "title": "HERMES",
            "content": "kinematic retargeting"
        },
        {
            "title": "Video tasks\nMocap tasks",
            "content": "78.115.0 88.96.7 0 0 For human motion trajectories derived from mocap data and videos, we compare HERMES with kinematic retargeting methods. Table indicates that while kinematic retargeting it can map human hand motions to robotic counterparts, fails to capture essential aspects such as object interactions and contact information. Moreover, the retargeting approaches cannot guarantee optimality. The snapshots of two approaches are visualized in II. Hence, RL emerges as crucial approach for refining robotic behaviors. It shapes the robot policies toward human-like motions and establishes physically plausible, context-appropriate object interactions. Regarding teleoperation data, we compare HERMES with direct replay of edited trajectories. We evaluate both methods 11 Fig. 11. The training curve of HERMES. The horizontal axis denotes the training steps, while the vertical axis represents the normalized task length successfully accomplished by the policy. Teleop refers to one-shot human motion teleoperation in simulation, Human video denotes trajectories extracted from video data, and Mocap corresponds to motion derived from mocap datasets. HERMES not only demonstrates the capability to accomplish diverse manipulation tasks originating from various forms of human motion, but also exhibits superior sample efficiency throughout training. All results are evaluated across 3 seeds. adaptively adjust movements and enhance execution success rates. These results also highlight that HERMES leveraging RL can effectively mitigate dynamic inconsistencies. C. Training wall-time We also leverage the MJX GPU parallel simulation [63] to instantiate the tasks and adopt the PPO algorithm [87] for policy training. We adopt reward terms in line with DrM. As illustrated in Figure 14, HERMES benefits from reduced wallclock training time, and the reward formulation demonstrates cross-algorithm generality. Meanwhile, compared to the baseline method, HERMES also attains higher sample efficiency and stronger asymptotic performance under PPO training. D. Real-world Manipulation Evaluation After conducting DAgger training, we subsequently transfer the trained visual student policy to the real world in zero-shot manner for most tasks. It should be noted that for the tasks pour teapot and putoff burner, the presence of substantial noise in the trajectory or transparent objects leads to excessively jittering motions, along with discrepancies between simulated and real-world object shapes. Consequently, we additionally fine-tune the policy using 5 extra real-world trajectories collected via policy rollouts. Table III presents the generalization performance of the policy evaluated across different object placements and poses, with each task assessed over 15 trials. As the baseline, we replace our depth-processing with raw depth inputs in the sim2real pipeline. HERMES not only successfully achieves zero-shot transfer for diverse long-horizon or contact-rich bimanual dexterous manipulation tasks, but also surpasses the baseline by +54.5% in success rate. These experimental Fig. 12. The comparison of kinematic retargeting and HERMES. The raw trajectories extracted from human videos and mocap data are insufficient to complete the task through mere kinematic retargeting. On the other hand, HERMES not only learns to follow these reference trajectories but also masters the nuances of object interaction. TABLE II COMPARISON OF HERMES AND REPLAY EDITED TRAJECTORIES. Tasks Method"
        },
        {
            "title": "HERMES",
            "content": "replay"
        },
        {
            "title": "Bottle Handover\nPlace Drawer",
            "content": "91.91.9 72.25.1 52.25.1 49.95.8 on two distinct tasks: long-horizon task involving articulated object manipulation (placedrawer) and contact-rich task (handover). For each task, the objects pose is randomized on the table, and each method is evaluated over 30 episodes. As demonstrated in Table II, due to the robots dynamic configuration, directly executing edited actions does not guarantee task success when object positions change. In contrast, HERMES, through RL training, learns residual actions that can 12 Fig. 13. Simulation training visualization. We visualize the majority of the training tasks. Leveraging single reference trajectory in conjunction with general reward design, HERMES can convert diverse human motion sources into robot feasible behaviors via RL training. TABLE III REAL-WORLD MANIPULATION EVALUATION RESULTS. ACROSS 6 REAL-WORLD BIMANUAL DEXTEROUS MANIPULATION TASKS, HERMES OBTAINS +54.5% PERFORMANCE GAINS ON AVERAGE. Method Tasks Bottle Handover Clean Table Scan Bottle Putoff Burner Clean Plate Pour Teapot Average 67.85.0 13.315."
        },
        {
            "title": "HERMES\nraw depth",
            "content": "73.3 0.0 60.0 0.0 66.7 13.3 66.7 6.7 73.3 40.0 66.7 20. experiments in two indoor and one outdoor scenarios, including two long-horizon navigation tasks. Table IV reports the computed localization errors in both translational distance and orientation. These errors are derived by solving the PnP problem and subsequently calculating the relative pose differences between the terminal pose and the target pose within shared reference frame. As shown in Table IV, incorporating our proposed approach significantly reduces localization errors, whereas ViNT suffers from substantial instability in localization accuracy. Additionally, we visualize the RGB images and corresponding point clouds of the stopping positions for both baselines and HERMES. Figure 15 demonstrates that HERMES not only achieves semantic alignment of RGB images but also accurately matches the point clouds with the target position due to improved localization precision. The experimental results presented in Figure 15 and Table IV highlight that current general navigation models, despite their ability to broadly match the goal image, exhibit large localization discrepancies, rendering them unsuitable for downstream visuomotor manipulation tasks. In contrast, through employing our simple yet robust closed-loop PnP localization algorithm, HERMES not only successfully addresses the low precision of the mobile robot base, but also achieves small localization errors. The wall-time training efficiency. HERMES also enjoys high Fig. 14. wall-time efficiency under parallel training. All results are evaluated across 3 seeds. results substantiate HERMESs capability to effectively bridge both visual and dynamic gaps, enabling successful sim2real transfer and demonstrating intricate manipulation skills. Moreover, for the two tasks involving fine-tuning with real-world rollout trajectories, owing to the reduced visual discrepancy achieved by HERMES, the trained policy exhibits enhanced generalization capabilities compared to the raw depth baseline. E. The Effectiveness of Closed-loop PnP In terms of the navigation experiments, we first evaluate the localization errors of the ViNT model augmented with our proposed closed-loop PnP localization algorithm. We conduct Fig. 15. The visualization of navigation results. The left two columns depict comparison between the target image and the terminal image achieved by our method. The right two columns present the point clouds captured at the end of navigation by ViNT and HERMES, compared against the point cloud of the target position. This figure illustrates that ViNT exhibits noticeable mismatch between the captured and target point clouds at the end of navigation, whereas HERMES achieves close alignment, which demonstrates the high localization accuracy of our approach. TABLE IV THE RESULTS OF NAVIGATION LOCALIZATION ERROR. Method Tasks indoor scene1 indoor scene2 outdoor scene1 HERMES dist error (cm) ori error () ViNT dist error (cm) 2.4 0.5 1.3 0.5 3.2 1.8 1.79 1.12 0.57 0.57 1.67 1.84 1811.3 7.33.1 12.97. ori error () 2.571.98 3.661.91 1.63 1.31 F. The Localization Ability of Closed-loop PnP in the Textureless Scenario Fig. 16. The localization ability of HERMES in textureless scenarios. Even in environments with sparse visual features, HERMES remains capable of executing fine-grained positional adjustments and achieving precise localization through the closed-loop PnP mechanism. We also compare the localization performance of RTABMAP [88], popular RGBD-based visual SLAM approach, against HERMES in the textureless scenario. Previous studies have shown that RGBD-based SLAM methods struggle to accurately localize in textureless environments since it is hard to track visual features in these scenes [88][90]. As shown in Table V, RTAB-MAP fails to accomplish the task while our approach consistently achieves precise localization even under such challenging conditions. The point clouds of iterative refinement processes of closed-loop PnP are shown in Figure 16. TABLE COMPARISON OF HERMES AND RTAB-MAP IN THE TEXTURELESS SCENARIO. Method Error dist error (cm) ori error () RTAB-MAP HERMES - 1.261.25 - 2.061. G. Mobile Manipulation Evaluation To evaluate the mobile manipulation ability of HERMES, we integrate the entire pipeline across all tasks. Each trained policy is tested over 10 runs. As illustrated in Figure 17, HERMES demonstrates strong real-world navigation, precise localization, and dexterous manipulation capabilities. We also apply the identical manipulation policy equipped with ViNT as baseline. Figure 17 reveals that, without closed-loop PnP localization, the policy cannot generalize or successfully complete tasks when faced with significant positional and rotational shifts. Conversely, HERMES achieves notable +54.0% improvement in manipulation success rate compared to pure ViNT. These findings underscore that closed-loop 14 mitigate these discrepancies, they nevertheless reduce overall success rates. In future work, we will attempt to deploy our algorithms on more robust and precisely engineered hardware systems and to lower the cost of simulation setup, improving reliability and reducing manual effort. X. ACKNOWLEDGMENT We gratefully acknowledge OYMotion for providing the dexterous hand hardware that supported this work, and we thank Sizhe Yang, Zixuan Liu and Zhengmao He for their insightful comments on the manuscript. This work is also supported by dushi program."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Zhou, R. Wang, Y. Tai, Y. Deng, G. Liu, and K. Jia, You only teach once: Learn one-shot bimanual robotic manipulation from video demonstrations, arXiv preprint arXiv:2501.14208, 2025. [2] H. Kim, J. Kang, H. Kang, M. Cho, S. J. Kim, and Y. Lee, Uniskill: Imitating human videos via cross-embodiment skill representations, arXiv preprint arXiv:2505.08787, 2025. [3] T. G. W. Lum, O. Y. Lee, C. K. Liu, and J. Bohg, Crossing the human-robot embodiment gap with sim-to-real rl using one human demonstration, arXiv preprint arXiv:2504.12609, 2025. [4] P. Dan, K. Kedia, A. Chao, E. W. Duan, M. A. Pace, W.-C. Ma, and S. Choudhury, X-sim: Cross-embodiment learning via real-to-sim-toreal, arXiv preprint arXiv:2505.07096, 2025. [5] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar, Mimicplay: Long-horizon imitation learning by watching human play, in 7th Annual Conference on Robot Learning, 2023. [6] R.-Z. Qiu, S. Yang, X. Cheng, C. Chawla, J. Li, T. He, G. Yan, D. J. Yoon, R. Hoque, L. Paulsen et al., Humanoid policy human policy, arXiv preprint arXiv:2503.13441, 2025. [7] Y. Qin, W. Yang, B. Huang, K. Van Wyk, H. Su, X. Wang, Y.-W. Chao, and D. Fox, Anyteleop: general vision-based dexterous robot armhand teleoperation system, arXiv preprint arXiv:2307.04577, 2023. [8] R. Yang, Q. Yu, Y. Wu, R. Yan, B. Li, A.-C. Cheng, X. Zou, Y. Fang, H. Yin, S. Liu et al., Egovla: Learning vision-language-action models from egocentric human videos, arXiv preprint arXiv:2507.12440, 2025. [9] K. Shaw, S. Bahl, A. Sivakumar, A. Kannan, and D. Pathak, Learning dexterity from human hand motion in internet videos, The International Journal of Robotics Research, vol. 43, no. 4, pp. 513532, 2024. [10] K. Shaw, S. Bahl, and D. Pathak, Videodex: Learning dexterity from internet videos, in Conference on Robot Learning. PMLR, 2023, pp. 654665. [11] Y. Chen, C. Wang, Y. Yang, and K. Liu, Object-centric dexterous manipulation from human motion data, in 8th Annual Conference on Robot Learning. PMLR, 2024. [12] K. Li, P. Li, T. Liu, Y. Li, and S. Huang, Maniptrans: Efficient dexterous bimanual manipulation transfer via residual learning, arXiv preprint arXiv:2503.21860, 2025. [13] Z. Mandi, Y. Hou, D. Fox, Y. Narang, A. Mandlekar, and S. Song, Dexmachina: Functional retargeting for bimanual dexterous manipulation, arXiv preprint arXiv:2505.24853, 2025. [14] T. Lin, Y. Zhang, Q. Li, H. Qi, B. Yi, S. Levine, and J. Malik, Learning visuotactile skills with two multifingered hands, arXiv preprint arXiv:2404.16823, 2024. [15] Y. Chen, C. Wang, L. Fei-Fei, and K. Liu, Sequential dexterity: Chaining dexterous policies for long-horizon manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 38093829. [16] T. Lin, Z.-H. Yin, H. Qi, P. Abbeel, and J. Malik, Twisting lids off with two hands, arXiv preprint arXiv:2403.02338, 2024. [17] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine, Vint: foundation model for visual navigation, in Conference on Robot Learning. PMLR, 2023, pp. 711733. [18] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, survey of robot learning from demonstration, Robotics and autonomous systems, vol. 57, no. 5, pp. 469483, 2009. [19] P. Mandikal and K. Grauman, Dexvip: Learning dexterous grasping with human hand pose priors from video, in Conference on Robot Learning. PMLR, 2022, pp. 651661. Real-world mobile manipulation results. Dark-colored bars Fig. 17. correspond to HERMES, whereas the light-colored bars correspond to only using ViNT. HERMES is capable of performing wide array of complex mobile bimanual dexterous manipulation tasks. In contrast, when relying solely on ViNT for localization, the trained manipulation policy fails to complete the tasks. PnP localization is the essential bridge linking navigation and manipulation, enabling both modules to synergize for enhanced performance. VIII. CONCLUSION In this work, we introduce HERMES, novel framework addressing critical challenges in bimanual dexterous robotic manipulation by effectively leveraging diverse human motion data sources and robust sim2real methodologies. Through the integration of hybrid control scheme alongside generalized DAgger-based distillation framework, HERMES facilitates effective sim2real transfer with high success rates. Additionally, we present navigation localization method employing closedloop PnP refinement, crucial for bridging the gap between navigation accuracy and manipulation precision. The comprehensive experimental results validate that HERMES not only achieves superior performance in complex manipulation tasks but also demonstrates exceptional adaptability to diverse realworld scenarios, thus providing solid foundation for future advancements in mobile robotic manipulation. IX. LIMITATIONS AND FUTURE WORK Although we have demonstrated the effectiveness of HERMES, several limitations remain. First, our tasks are quasistatic, for which the proposed hybrid sim2real control scheme is well suited; however, for highly dynamic, velocitydependent tasks, complicated system identification is still required for sim2real transfer. Second, to obtain favorable robot behaviors, we still manually tune physics collision parameters and approximate objects with primitive geometric shapes. Additionally, assembly and calibration mismatches between simulation and hardware persist; while closed-loop control can [20] J. Ye, J. Wang, B. Huang, Y. Qin, and X. Wang, Learning continuous grasping function with dexterous hand from human demonstrations, IEEE Robotics and Automation Letters, vol. 8, no. 5, pp. 28822889, 2023. [21] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu, and X. Wang, Dexmv: Imitation learning for dexterous manipulation from human videos, in European Conference on Computer Vision. Springer, 2022, pp. 570587. [22] Z. Luo, J. Cao, S. Christen, A. Winkler, K. Kitani, and W. Xu, Omnigrasp: Grasping diverse objects with simulated humanoids, Advances in Neural Information Processing Systems, vol. 37, pp. 21612184, 2024. [23] Y. Ze, Z. Chen, J. P. Ara Aˇsjo, Z.-a. Cao, X. B. Peng, J. Wu, and C. K. Liu, Twist: Teleoperated whole-body imitation system, arXiv preprint arXiv:2505.02833, 2025. [24] Z. He, K. Lei, Y. Ze, K. Sreenath, Z. Li, and H. Xu, Learning visual quadrupedal loco-manipulation from demonstrations, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 91029109. [25] S. Kareer, D. Patel, R. Punamiya, P. Mathur, S. Cheng, C. Wang, J. Hoffman, and D. Xu, Egomimic: Scaling imitation learning via egocentric video, arXiv preprint arXiv:2410.24221, 2024. [26] R. Hoque, P. Huang, D. J. Yoon, M. Sivapurapu, and J. Zhang, Egodex: Learning dexterous manipulation from large-scale egocentric video, arXiv preprint arXiv:2505.11709, 2025. [27] X. Liu, J. Adalibieke, Q. Han, Y. Qin, and L. Yi, Dextrack: Towards generalizable neural tracking control for dexterous manipulation from human references, arXiv preprint arXiv:2502.09614, 2025. [28] Z. Zhuang, Z. Fu, J. Wang, C. G. Atkeson, S. Schwertfeger, C. Finn, and H. Zhao, Robot parkour learning, in Conference on Robot Learning. PMLR, 2023, pp. 7392. [29] Z. Zhuang, S. Yao, and H. Zhao, Humanoid parkour learning, in 8th Annual Conference on Robot Learning. PMLR, 2024. [30] N. Rudin, J. He, J. Aurand, and M. Hutter, Parkour in the wild: Learning general and extensible agile locomotion policy using multi-expert distillation and rl fine-tuning, arXiv preprint arXiv:2505.11164, 2025. [31] A. Allshire, H. Choi, J. Zhang, D. McAllister, A. Zhang, C. M. Kim, T. Darrell, P. Abbeel, J. Malik, and A. Kanazawa, Visual imitation enables contextual humanoid control, arXiv preprint arXiv:2505.03729, 2025. [32] K. LEI, Z. He, C. Lu, K. Hu, Y. Gao, and H. Xu, Unio4: Unifying online and offline deep reinforcement learning with multi-step on-policy optimization, in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=tbFBh3LMKi [33] H. Qi, A. Kumar, R. Calandra, Y. Ma, and J. Malik, In-hand object rotation via rapid motor adaptation, in Conference on Robot Learning. PMLR, 2023, pp. 17221732. [34] J. Wang, Y. Qin, K. Kuang, Y. Korkmaz, A. Gurumoorthy, H. Su, and X. Wang, Cyberdemo: Augmenting simulated human demonstration for real-world dexterous manipulation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17 95217 963. [35] A. Maddukuri, Z. Jiang, L. Y. Chen, S. Nasiriany, Y. Xie, Y. Fang, W. Huang, Z. Wang, Z. Xu, N. Chernyadev et al., Sim-and-real cotraining: simple recipe for vision-based robotic manipulation, arXiv preprint arXiv:2503.24361, 2025. [36] Z. Yuan, T. Wei, S. Cheng, G. Zhang, Y. Chen, and H. Xu, Learning to manipulate anywhere: visual generalizable framework for reinforcement learning, in 8th Annual Conference on Robot Learning, 2024. [37] P. Hua, M. Liu, A. Macaluso, Y. Lin, W. Zhang, H. Xu, and L. Wang, Gensim2: Scaling robot data generation with multi-modal and reasoning llms, in 8th Annual Conference on Robot Learning, 2024. [38] L. Wang, Y. Ling, Z. Yuan, M. Shridhar, C. Bao, Y. Qin, B. Wang, H. Xu, and X. Wang, Gensim: Generating robotic simulation tasks via large language models, in The Twelfth International Conference on Learning Representations, 2023. [39] L. Xu, Z. Liu, Z. Gui, J. Guo, Z. Jiang, Z. Xu, C. Gao, and L. Shao, Dexsingrasp: Learning unified policy for dexterous object singulation and grasping in cluttered environments, 2025. [Online]. Available: https://arxiv.org/abs/2504. [40] H. Zhang, Z. Wu, L. Huang, S. Christen, and J. Song, Robustdexgrasp: Robust dexterous grasping of general objects, arXiv preprint arXiv:2504.05287, 2025. [41] T. Lin, K. Sachdev, L. Fan, J. Malik, and Y. Zhu, Sim-to-real reinforcement learning for vision-based dexterous manipulation on humanoids, arXiv preprint arXiv:2502.20396, 2025. 15 [42] J. Bjorck, F. Castaneda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang et al., Gr00t n1: An open foundation model for generalist humanoid robots, arXiv preprint arXiv:2503.14734, 2025. [43] R. Singh, A. Allshire, A. Handa, N. Ratliff, and K. Van Wyk, Dextrahrgb: Visuomotor policies to grasp anything with dexterous hands, arXiv preprint arXiv:2412.01791, 2024. [44] M. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor, and A. Silwal, Splatsim: Zero-shot sim2real transfer of rgb manipulation policies using gaussian splatting, arXiv preprint arXiv:2409.10161, 2024. [45] X. Li, J. Li, Z. Zhang, R. Zhang, F. Jia, T. Wang, H. Fan, K.-K. Tseng, and R. Wang, Robogsim: real2sim2real robotic gaussian splatting simulator, arXiv preprint arXiv:2411.11839, 2024. [46] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., vol. 42, no. 4, pp. 1391, 2023. [47] M. Dalal, M. Liu, W. Talbott, C. Chen, D. Pathak, J. Zhang, and R. Salakhutdinov, Local policies enable zero-shot long-horizon manipulation, arXiv preprint arXiv:2410.22332, 2024. [48] T. G. W. Lum, M. Matak, V. Makoviychuk, A. Handa, A. Allshire, T. Hermans, N. D. Ratliff, and K. Van Wyk, Dextrah-g: Pixels-to-action dexterous arm-hand grasping with geometric fabrics, in 8th Annual Conference on Robot Learning, 2024. [49] S. Yenamandra, A. Ramachandran, K. Yadav, A. S. Wang, M. Khanna, T. Gervet, T.-Y. Yang, V. Jain, A. Clegg, J. M. Turner et al., Homerobot: Open-vocabulary mobile manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 19752011. [50] Z. Fu, T. Z. Zhao, and C. Finn, Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation, arXiv preprint arXiv:2401.02117, 2024. [51] X. Huang, D. Batra, A. Rai, and A. Szot, Skill transformer: monolithic policy for mobile manipulation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 85210 862. [52] P. Liu, Y. Orru, J. Vakil, C. Paxton, N. Shafiullah, and L. Pinto, Demonstrating ok-robot: What really matters in integrating openknowledge models for robotics, in Robotics: Science and Systems XX, ser. RSS2024. Robotics: Science and Systems Foundation, Jul. 2024. [Online]. Available: http://dx.doi.org/10.15607/RSS.2024.XX.091 [53] P. Zhi, Z. Zhang, Y. Zhao, M. Han, Z. Zhang, Z. Li, Z. Jiao, B. Jia, and S. Huang, Closed-loop open-vocabulary mobile manipulation with gpt-4v, 2025. [Online]. Available: https://arxiv.org/abs/2404.10220 [54] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser, Tidybot: Personalized robot assistance with large language models, Autonomous Robots, vol. 47, no. 8, pp. 10871102, 2023. [55] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE Transactions on Robotics (T-RO), 2023. https://github.com/ [56] L. Medeiros, anything, segment Lang luca-medeiros/lang-segment-anything, 2023. [57] OpenAI, Gpt-4v(ision) system card, https://cdn.openai.com/papers/ GPTV System Card.pdf, 2023. [58] Y. Jiang, R. Zhang, J. Wong, C. Wang, Y. Ze, H. Yin, C. Gokmen, S. Song, J. Wu, and L. Fei-Fei, Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities, arXiv preprint arXiv:2503.05652, 2025. [59] H. Xiong, R. Mendonca, K. Shaw, and D. Pathak, Adaptive mobile manipulation for articulated objects in the open world, arXiv preprint arXiv:2401.14403, 2024. [60] X. Meng, X. Yang, S. Jung, F. Ramos, S. S. Jujjavarapu, S. Paul, and D. Fox, Aim my robot: Precision local navigation to any object, IEEE Robotics and Automation Letters, 2025. [61] R. Yang, Y. Kim, R. Hendrix, A. Kembhavi, X. Wang, and K. Ehsani, Harmonic mobile manipulation, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 36583665. [62] E. Todorov, T. Erez, and Y. Tassa, Mujoco: physics engine for model-based control, in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 50265033. [63] google, MuJoCo XLA (MJX), 2023. //mujoco.readthedocs.io/en/stable/mjx.html [Online]. Available: https: [64] K. Zakka, Mink: Python inverse kinematics based on MuJoCo, May 2025. [Online]. Available: https://github.com/kevinzakka/mink [65] X. Zhan, L. Yang, Y. Zhao, K. Mao, H. Xu, Z. Lin, K. Li, and C. Lu, Oakink2: dataset of bimanual hands-object manipulation in complex task completion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 445456. 16 [89] C. Campos, R. Elvira, J. J. G. Rodriguez, J. M. M. Montiel, and J. D. Tardos, Orb-slam3: An accurate open-source library for visual, visualinertial, and multimap slam, IEEE Transactions on Robotics, vol. 37, no. 6, p. 18741890, Dec. 2021. [Online]. Available: http://dx.doi.org/10.1109/TRO.2021.3075644 [90] Z. Shan, R. Li, and S. Schwertfeger, Rgbd-inertial trajectory estimation and mapping for ground robots, Sensors, vol. 19, no. 10, 2019. [Online]. Available: https://www.mdpi.com/1424-8220/19/10/2251 [91] K. Zakka, B. Tabanpour, Q. Liao, M. Haiderbhai, S. Holt, J. Y. Luo, A. Allshire, E. Frey, K. Sreenath, L. A. Kahrs, C. Sferrazza, Y. Tassa, and P. Abbeel, Mujoco playground: An open-source framework for gpu-accelerated robot learning and sim-to-real transfer. 2025. [Online]. Available: https://github.com/google-deepmind/mujoco playground [92] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, Learning to walk in minutes using massively parallel deep reinforcement learning, in Proceedings of the 5th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, vol. 164. PMLR, 2022, pp. 91100. [Online]. Available: https://proceedings.mlr.press/v164/rudin22a.html [93] W. Wan, H. Geng, Y. Liu, Z. Shan, Y. Yang, L. Yi, and H. Wang, Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 38913902. [94] G. Bradski, A. Kaehler et al., Opencv, Dr. Dobbs journal of software tools, vol. 3, no. 2, 2000. [95] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 10 37110 381. [96] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, Advances in Neural Information Processing Systems, vol. 37, pp. 21 87521 911, 2024. [66] R. A. Potamias, J. Zhang, J. Deng, and S. Zafeiriou, Wilor: End-to-end 3d hand localization and reconstruction in-the-wild, 2024. [67] S. Li, C. Xu, and M. Xie, robust (n) solution to the perspectiven-point problem, IEEE transactions on pattern analysis and machine intelligence, vol. 34, no. 7, pp. 14441450, 2012. [68] B. Wen, W. Yang, J. Kautz, and S. Birchfield, Foundationpose: Unified 6d pose estimation and tracking of novel objects, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17 86817 879. [69] AR Code, Ar code, 2022, accessed: 2024-09-28. [Online]. Available: https://ar-code.com/ [70] A. Handa, K. Van Wyk, W. Yang, J. Liang, Y.-W. Chao, Q. Wan, S. Birchfield, N. Ratliff, and D. Fox, Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system, in 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 91649170. [71] Z. Luo, J. Cao, K. Kitani, W. Xu et al., Perpetual humanoid control for real-time simulated avatars, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 89510 904. [72] X. B. Peng, P. Abbeel, S. Levine, and M. Van de Panne, Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Transactions On Graphics (TOG), vol. 37, no. 4, pp. 114, 2018. [73] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He, N. Sobanbab, C. Pan et al., Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills, arXiv preprint arXiv:2502.01143, 2025. [74] G. Xu, R. Zheng, Y. Liang, X. Wang, Z. Yuan, T. Ji, Y. Luo, X. Liu, J. Yuan, P. Hua et al., Drm: Mastering visual reinforcement learning through dormant ratio minimization, in The Twelfth International Conference on Learning Representations, 2024. [75] G. Sokar, R. Agarwal, P. S. Castro, and U. Evci, The dormant neuron phenomenon in deep reinforcement learning, in International Conference on Machine Learning. PMLR, 2023, pp. 32 14532 168. [76] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor segmentation and support inference from rgbd images, in Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12. Springer, 2012, pp. 746 760. [77] Z. Yuan, Z. Xue, B. Yuan, X. Wang, Y. Wu, Y. Gao, and H. Xu, Pre-trained image encoder for generalizable visual reinforcement learning, Advances in Neural Information Processing Systems, vol. 35, pp. 13 02213 037, 2022. [78] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [79] Y. Wu and K. He, Group normalization, in Proceedings of European conference on computer vision (ECCV), 2018, pp. 319. [80] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine, Gnm: general navigation model to drive any robot, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 72267233. the [81] A. Sridhar, D. Shah, C. Glossop, and S. Levine, Nomad: Goal masked diffusion policies for navigation and exploration, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 6370. [82] Y. Wang, X. He, S. Peng, D. Tan, and X. Zhou, Efficient loftr: Semidense local feature matching with sparse-like speed, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 21 66621 675. [83] M. Zuliani, Ransac for dummies, Vision Research Lab, University of California, Santa Barbara, vol. 1, 2009. [84] K. Madsen, H. B. Nielsen, and O. Tingleff, Methods for non-linear least squares problems, Informatics and Mathematical Modelling Technical University of Denmark, vol. 1, 2004. [85] E. Eade, Gauss-newton/levenberg-marquardt optimization, Tech. Rep., 2013. [86] M. J. Willis, Proportional-integral-derivative control, Dept. of Chemical and Process Engineering University of Newcastle, vol. 6, 1999. [87] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [88] M. Labbe and F. Michaud, Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for largescale and long-term online operation, Journal of Field Robotics, [Online]. Available: vol. 36, no. 2, p. 416446, Oct. 2018. http://dx.doi.org/10.1002/rob."
        },
        {
            "title": "APPENDIX",
            "content": "B. Human Motion Data Pre-processing 17 A. State-based RL Expert Training Details Model Architecture: Since DrM [74] is inherently designed as visual reinforcement learning algorithm, we replace its visual encoder with three-layer multilayer perceptron (MLP), employing ELU as activation functions. Meanwhile, the network architectures and hyperparameters of the Actor, Critic, and V-network remain unchanged. Training Hyperprameters: We employ the identical set of reinforcement learning hyperparameters as those in the original visual RL implementation, without conducting any further hyperparameter tuning specifically for DrM in our experiments. For PPO, we employ the implementation available in MuJoCo Playground [91], which is built upon the RSL-RL framework [92]. Reward Hyperparameters: The reward hyperparameters are listed in Table VI. In addition to the proposed reward information is incorporated during training terms, contact on teleoped-motion tasks to tune the distance-chain reward coefficients. TABLE VI REWARD HYPERPARAMETERS Description contact predefined threshold Nnum object position reward coefficient k1 object orientation reward coefficient k2 coefficient of penalty Value 2 1 1 1e 3 Observations of RL training: The dimensions of the observation for training expert state-based policy are provided in Table VII. TABLE VII OBSERVATION DESCRIPTIONS Description arm joint position hand joint position arm joint velocity hand joint velocity hand position hand quaternion object-centric distance chain contact information actuator value time distance with the reference hand position in reference hand quaternion in reference object position in reference object quaternion in reference Dimension 12 36 12 36 6 8 72 24 24 1 10 6 8 3 4 Action space: Actions are parameterized as sixdimensional end-effector pose: the first three are translation, and the last three are Euler angles. For residual actions, the translation range is [0.01, 0.01] and the orientation range is [0.04, 0.04] rad. Regarding the actions in trajectories, teleoperated motions are scaled to [0.2, 0.2] and [0.4, 0.4] rad, whereas motions derived from video and mocap are scaled to [1, 1] and [1, 1] rad. We apply downsampling to the collected human motion data in order to reduce episode length and redundant steps, thereby facilitating more efficient policy learning of task skills. For human video data, particularly when involving symmetric objects, we observe that the FoundationPose [68] estimator may introduce rotational ambiguities during pose prediction. To address this, we preprocess the object pose trajectories to mitigate pose discrepancies induced by object symmetry. C. Task Description We design suite of tasks specifically tailored for bimanual dexterous hand manipulation. Below, we provide detailed description of each individual task: Bottle Handover: The robot is required to execute hand-tohand transfer, passing bottle from the right hand to the left, followed by accurately placing it into designated container. Clean Table: The task entails sequentially picking up both bottle and cup from the tabletop and placing them into box. Scan Bottle: The robot must first grasp bottle from the table, subsequently pick up scanning device, and then complete bottle-scanning action. Place Drawer: This task requires the robot to open drawer, place multiple objects from the tabletop into the drawer sequentially, and finally close the drawer. Pour Teapot: The robot is required to perform pouring action by transferring liquid from container into teapot. Clean Plate: The robot must grasp both plate and dishcloth, and then execute wiping motion over the plates surface. Putoff Burner: This task involves picking up burner cap and performing two-step extinguishing sequence to put out an alcohol burner Flower Vase: The robot is required to insert flower into vase. D. DAgger Training Details"
        },
        {
            "title": "We list",
            "content": "the hyperparameters of DAgger training in Table VIII. Moreover, we randomize the camera viewpoint to account for discrepancies between the simulated and realworld camera poses. The action head is three-layer MLPs with two hidden layers using ReLU activations; its outputs are subsequently squashed with tangent activate function to enforce bounded actions in the target action space. The DAgger training action objective is: (cid:104) Lact(θ) = E(ot,a )D ata 2 (cid:105) +E(ot,a )D (cid:104) ata 1 (cid:105) , (8) where at = πθ(ot), and is the expert action. E. Navigation Details We adopt the same navigation configuration as employed in ViNT. For localization, we first capture RGB-D images of the target position. Then, during navigation, the images recorded along the reference trajectories serve as goal observations for TABLE VIII DAGGER TRAINING HYPERPARAMETERS"
        },
        {
            "title": "Description\nBuffer size\nLearning rate\nOptimizer\nScheduler adjusted probability p\nOptimizer eps\nBatch size\nAction repeat\nFrame stack\nDepth threshold distance d",
            "content": "Value 1e5 1e4 Adam 0.93 1e8 256 1 3 0.9 to 1.1 Fig. 18. Object generalization visualization. To help the policy adapt to different object shapes, we randomize each objects geometry during training to prompt the policy to adjust its actions accordingly. inferring the subsequent actions. In addition, we configure the number of waypoints to 4 and set the radius to 5, tailoring these parameters to the operational characteristics of our mobile platform. F. Instance Generalization We also conduct experiments to assess the object-level generalization capability of our approach. For the handover task, we select several bottles from the UniDexGrasp++ dataset [93] and introduce randomized variations in bottle shapes during each environment reset. The corresponding visualizations are presented in Figure 18. The trained policy, distilled via the DAgger framework, demonstrates zero-shot generalization to bottles of varying shapes in real-world settings. G. Qualitative Analysis of Closed-loop PnP the Figure 19 illustrates that across various scenarios, closed-loop PnP localization with iterative refinements progressively aligns the captured point clouds with the target positions, underscoring the localization precision achieved by HERMES. H. DAgger Training Regarding DAgger training, we compare our implementation of DAgger with both pure expert training (i.e., Behavior Cloning) and pure student training. Figure 20 reports each methods performance on two representative tasks. Clean Table is long-horizon task in which the robot must first place two objects into box and then move the box to the center of the table. As shown in Figure 20, pure imitation learning struggles to cover long-horizon and out-of-distribution states. In Clean Plate, where the human motion is extracted from raw videos, the expert policys actions contain non-negligible 18 noise; consequently, during early interactive training, action errors can be amplified by querying this noisy expert, leading to poor sample efficiency. In contrast, HERMES warms up the policy with expert data and then gradually reduces reliance on the expert action distribution, achieving high sample efficiency without sacrificing asymptotic performance in both tasks. I. Hybrid Control for Real-world Evaluation It is worth noting that, in our hybrid sim2real control framework, the simulation environment does not contain any object models. We rely on the simulated robot to carry out all kinematic and dynamic computations and to maintain alignment with the real system. Furthermore, to showcase the benefits of our hybrid sim2real control strategy, we visualize the temporal trajectory of the right-hand index-finger joint angle during policy execution. Figure 21 shows pronounced, nonlinear discrepancy between the joint target position inferred by the policy and the actual reached joint position. Such sizable mismatch is difficult for the policy to compensate for. In contrast, the hybrid sim2real control strategy forces the simulated and real robots to share an identical dynamics model. As demonstrated in Figure 21, the blue and green traces remain consistently overlapped across the entire horizon, indicating that hybrid control substantially reduces the sim2real gap. J. The Sequential Adjustment Strategy of Closed-loop PnP During the closed-loop PnP stage, the system takes goal RGBD image Ig and current robot-captured RGBD image Ic as inputs. At each timestep t, the robot updates Ic with its current view. The PnP algorithm which is made up of RANSAC PnP [83] and refine PnP [84], [85], [94] computes the relative rotation R33 and translation R3 between the current and goal poses. The pose errors in direction, direction and yaw orientation are derived as ex, ey, eyaw from the relative rotation and translation. These errors drive the PID controller [86] to output vx, vy, vyaw, which move the robot base to reduce the errors. The adjustment loop terminates when all errors satisfy ej < εj for x, y, yaw. Algorithm 3 Closed-loop PnP Adjustment Require: Goal RGBD image Ig, thresholds εx, εy, εyaw Ensure: Adjusted robot pose Initialize PID controllers (cid:16) ex , eyaw while max εyaw εx (cid:17) , ey > 1 do εy Capture current RGBD image Ic R, PnP(Ig, Ic) for {x, y, yaw} do Using RANSAC + Refine PnP ej ExtractPoseErrors(R, T) vj PIDController(ej) end for Send velocity command (vx, vy, vyaw) to robot end while K. Feature Matcher Visualization Efficient LoFTR [82] consistently extracts dense feature correspondences at high frequency across various environ19 Fig. 19. Closed-loop PnP visualization. Across diverse scenarios, the closed-loop PnP algorithm iteratively refines the robots pose and ultimately aligns it with the desired target pose with high precision. Fig. 20. DAgger training efficiency. HERMES attains high sample efficiency and asymptotic performance across different types of tasks. ments. This capability provides the rich feature correspondence required for the subsequent PnP pose estimation. The visualizations are shown in Figure 22. L. Comparison with the depth image from Depth Anything Depth-Anything [95], [96], widely adopted foundation model for depth estimation, demonstrates remarkable robustness and fine-grained detail perception, even under diverse and unstructured real-world conditions. Building on these capabilities, we also leverage Depth-Anything to bridge the perceptual gap between simulation and the real world. Consistent with HERMES, we also apply the mixup augmentation strategy with the disparity map to enhance the robustness. We first utilize the NYU Depth Dataset [76] and transform the depth maps into disparity maps ˆI to better align with the distribution of Fig. 21. The comparison of sent target joint position and the actual reached joint position in both simulation and real-world. The red, blue, and green curves exhibit similar overall trends. Compared with the blue and green curves, the red curve shows deviation in joint position at the same time step, while the blue and green traces overlaps throughout. This discrepancy points to the gap between the simulated and real-world dynamics, and adopting the hybrid control keeping the dynamics consistency between the simulation and the real robot. DA generated images: ˆI(x, y) ="
        },
        {
            "title": "1\nD(x, y)",
            "content": ", (9) where (x, y) is the pixel position. We then blend the DA input oDA with certain disparity map odisparity: ˆo = α oDA + (1 α) odisparity, where 20 Fig. 22. Feature matcher visualization of Efficient LoFTR. Efficient LoFTR is capable of establishing correspondences between the target RGB image and the current RGB frame at high frequency. crepancy in the underlying pixel-wise depth estimates across the two domains. This numerical gap ultimately suppresses overall success rates, making the method less effective than our direct preprocessing of raw depth images. TABLE IX COMPARISON OF HERMES AND DEPTH-ANYTHING Tasks Method"
        },
        {
            "title": "HERMES",
            "content": "Depth-Anything"
        },
        {
            "title": "Bottle Handover\nScan Bottle",
            "content": "66.7 73.3 40.0 33.3 Comparison of Depth-anything generated images between Fig. 23. simulation and real-world. Depth-Anything can produce depth images that are semantically aligned between simulation and real-world. Fig. 24. Depth intensity distribution of Depth-anything generated images. The red curve shows the depth-value distribution extracted from real-world RGB images, whereas the blue curve corresponds to that obtained from simulated images. Despite their semantic alignment, the depth maps estimated by Depth-Anything reveal pronounced quantitative gap between the simulated and real-world domains. α is the coefficient. ˆo is then used as the final visual input to the network. We compare our depth-augmentation scheme with the depth maps synthesized by Depth-anything-v2. As reported in Table IX, HERMES achieves better performance and success rates. Although Figure 23 shows that the synthetic depth maps can achieve semantic coherence between simulation and realworld depth estimation, Figure 24 reveals distributional dis-"
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai Qi Zhi Institute",
        "Tsinghua University"
    ]
}