{
    "paper_title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "authors": [
        "Taewoong Kang",
        "Kinam Kim",
        "Dohyeon Kim",
        "Minho Park",
        "Junha Hyung",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos."
        },
        {
            "title": "Start",
            "content": "EgoX: Egocentric Video Generation from Single Exocentric Video Taewoong Kang1, Kinam Kim1, Dohyeon Kim2, Minho Park1, Junha Hyung1, Jaegul Choo1 1KAIST AI, 2 Seoul National University {keh0t0, kinamplify}@kaist.ac.kr, kdh8156@snu.ac.kr {m.park, sharpeeee, jchoo}@kaist.ac.kr 5 2 0 2 9 ] . [ 1 9 6 2 8 0 . 2 1 5 2 : r Figure 1. Given single exocentric video, EgoX generates what the scene would look like from the actors eyes. Shown with an in-the-wild clip from The Dark Knight, our approach achieves realistic and generalizable egocentric generation."
        },
        {
            "title": "Abstract",
            "content": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in geometrically consistent manner. To achieve this, we present EgoX, novel framework for generating egocentric videos from single exocentric input. EgoX leverages the pretrained spatiotemporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces unified conditioning strategy that combines exocentric and egocentric priors via widthand channel-wise concatenation. Additionally, geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos. 1. Introduction Dont you wish you could experience iconic scenes from films like The Dark Knight as if you were the Joker yourself? Exocentric-to-egocentric video generation makes this possible by converting third-person scene into realistic first-person perspective. This capability opens up new possibilities in the film industry, where viewers are no longer limited to passively watching scene but can step into it and become the main character. They can become superhero themselves or experience what it is like to play on the field as an MLB player. Beyond entertainment, egocentric per- * indicates equal contributions. spectives are crucial in fields such as robotics and AR/VR, where understanding how the world appears from the actors point of view enables better imitation, reasoning, and interaction [15, 21]. This stems from the fact that humans perceive and interact with the world through first-person, egocentric viewpoint. However, generating such first-person perspectives is challenging, since the model must maintain scene consistency across views by reconstructing visible areas and realistically synthesizing unseen regions. straightforward way to achieve this is to use camera control model. Recent advances in camera control video generation models [18, 36, 50] have shown impressive performance in generating consistent views under moderate pose variations. However, these methods primarily focus on modest viewpoint changes, whereas exocentric-to-egocentric video generation requires extreme camera pose translation that drastically alters the visible field of view. This difference introduces two major challenges. First, extreme viewpoint shifts result in large unseen regions that must be plausibly synthesized based on scene understanding rather than direct observation. Second, only small portion of the exocentric view corresponds to the egocentric perspective, making it crucial for the model to distinguish between view-related information that should be used as conditioning and unrelated content that should be suppressed. As illustrated in Fig. 2, effective generation therefore requires selectively attending to meaningful regions while discarding irrelevant background areas and plausibly synthesizing uninformed regions in geometrically consistent manner. Therefore existing camera control models do not account for these challenges and thus often fail in exocentric-to-egocentric video generation. Due to the inherent difficulty of this task, previous approaches often avoid generating the egocentric view from scratch or require additional inputs to simplify the problem. EgoExo-Gen [44] takes both an exocentric video and the first egocentric frame as inputs to generate only the subsequent sequence. Exo2Ego-V [26] utilizes four simultaneous exocentric camera views to capture richer spatial context and reduce the uninformed regions. To address the limitations of previous approaches, we propose EgoX, novel framework that generates egocentric video from single exocentric video, achieving practical and generalizable egocentric generation from single exocentric input. Our method leverages the pretrained spatiotemporal knowledge of large-scale video diffusion models with minimal modification, enabling the model to plausibly synthesize unseen regions in geometrically consistent manner. Specifically, we design unified conditioning strategy that combines exocentric views and egocentric priors through width-wise and channel-wise integration with clean latent representations, requiring only lightweight LoRA-based adaptation. Furthermore, geometry-guided Figure 2. Exo-to-Ego view generation example. The model has to preserve view-related content from the exocentric input, generate uninformed regions realistically, and ignore unrelated areas for consistent egocentric synthesis. self-attention allows the model to focus on spatially relevant regions while suppressing unrelated areas, leading to coherent and high-fidelity egocentric video generation. By effectively leveraging pretrained weights, our approach produces high-quality egocentric videos and demonstrates strong generalization across diverse environments, including challenging in-the-wild scenarios, as illustrated in Fig. 1. To summarize, the major contributions of our paper are as follows: We propose novel framework EgoX for synthesizing high-fidelity egocentric video from single exocentric video by effectively exploiting the pretrained video diffusion models. We design unified conditioning strategy that jointly combines exocentric video and egocentric priors through width-wise and channel-wise integration, achieving robust geometric consistency and high-quality generation. We introduce geometry-guided self-attention and clean latent representations that selectively focuses on viewrelevant regions and enhances accurate reconstruction, leading to more coherent egocentric synthesis. Extensive qualitative and quantitative experiments demonstrate that EgoX outperforms previous approaches by large margin, achieving state-of-the-art performance on diverse and challenging exo-to-ego video generation benchmarks. 2. Related Work 2.1. Exo-to-Ego View Generation Prior works on exo-to-ego view generation have explored various conditioning mechanisms and task formulations to bridge the significant viewpoint gap. Some approaches [26, 28, 32] incorporate exocentric features by concatenating them channel-wise with the egocentric representation. However, this method struggles with the fundamental lack of pixel-wise correspondence between the two viewpoints. This spatial misalignment makes it difficult for the model to effectively leverage the conditioning information, often 2 leading to poor understanding of the scene geometry, which can result in overfitting or degradation in output quality. Other works, such as 4Diff [10], employ crossattention mechanisms to condition the generation on exocentric views. This approach, however, prevents the utilization of powerful pretrained diffusion weights, limiting its generalizability and resulting in lower-quality synthesis. To address these limitations, other methods utilize reference frames or multi-view conditions. For instance, EgoExo-Gen [44] require the first egocentric frame to generate the rest of the sequence. Exo2Ego-V [26] performs full video translation but relies on four exocentric video inputs and separately trained spatial and temporal modules, which limits its generalization and fails to fully exploit spatiotemporal priors. In contrast, our model generalizes effectively using pretrained video diffusion weights while requiring only single exocentric input. 2.2. Video Diffusion Models Recent advancements in video diffusion models [1, 5, 6, 14, 40, 46] have led to significant improvements in generative quality, producing highly realistic and coherent video sequences. This has spurred wide range of research exploring how to utilize these powerful generative capabilities in various applications [9, 19, 20, 33, 52]. key area of this research focuses on conditional video generation, where the synthesis process is guided by specific inputs. Many works [7, 19, 22, 45, 52] have demonstrated successful control using conditions such as depth maps or static images. Building on this, several methods have been proposed for camera-controlled video generation [4, 29, 50]. These approaches can be broadly categorized into two main groups. The first group [3, 4, 30, 43, 47] conditions the diffusion model directly on camera extrinsic parameters, often represented as raw matrices or Plucker coordinates. The second group [18, 25, 36, 42, 50] first lifts the input video into an intermediate 3D representation, such as point cloud. This 3D scene is then rendered from new, user-specified camera pose, and the resulting image is used as strong spatial condition to guide the final video generation. However, existing methods for camera control are primarily designed for modest changes in viewpoint. They struggle to handle the extreme camera pose differences, challenge that becomes particularly significant in exocentric-to-egocentric video generation. Our work addresses this critical gap by proposing model capable of generating coherent egocentric videos from significantly different exocentric perspective. 3. Method Given an exocentric video sequence = {Xi}F centric camera pose ϕ = {ϕi}F i=0 and egoi=0, the goal is to generate corresponding egocentric video sequence = {Yi}F i=0 that depicts the same scene from first-person viewpoint. The key challenge is to preserve the visible content in the exocentric view while synthesizing unseen regions in geometrically consistent and realistic manner. To this end, the exocentric sequence is first lifted into 3D representation and rendered from the target egocentric viewpoint (Sec. 3.1), which becomes an egocentric prior video . Both and the original exocentric video are then provided as inputs to video diffusion model (Sec. 3.2). In addition, geometry-guided self-attention (Sec. 3.3) is proposed to adaptively focus on view-consistent regions and enhance feature coherence across perspectives. 3.1. Egocentric Point Cloud Rendering For this stage, we render an egocentric prior video RF 3HW via point cloud rendering from the exocentric view. This prior provides both explicit pixel-wise RGB information and implicit camera trajectory cues that guide viewpoint alignment. Specifically, we first estimate monocular depth map Dm RF HW for each frame using single-image depth estimator [41], and video-based depth map Dv RF HW using temporal depth estimator [8]. Because Dm is estimated independently per frame, depth values often exhibit slight inconsistencies across time. In contrast, Dv produces temporally smooth yet affineinvariant depth estimate. To combine the advantages of both, we temporally align Dv with Dm. Following [16], we optimize affine transformation parameters α, β using momentum-based update strategy, yielding ˆα = {ˆαf }F and ˆβ = { ˆβf }F transformations. The final aligned depth is computed as: =0 =0, which represent the per-frame affine Df = 1 ˆα/Dv + ˆβ , (1) where Df denotes the final aligned depth map. Dynamic objects are masked out so that only static background regions are used during both alignment and rendering. For further details, please refer to [16]. After obtaining the aligned depth map Df , we convert it into 3D point cloud representation using the corresponding camera intrinsics. We then render the egocentric prior frames using point cloud renderer [34]: = render(X, Df , ϕ), (2) where RF 3HW is the exocentric RGB video and ϕ is egocentric camera poses. 3.2. Exo-to-Ego View Generation with VDM As illustrated in Fig. 3, the model takes an exocentric video RF 3HW and the egocentric prior video RF 3HW as conditioning inputs. Both inputs are 3 Figure 3. Overall pipeline. Given an exocentric video input, we first lift it into 3D point cloud and render the scene from the egocentric viewpoint to obtain the egocentric prior video. The clean exocentric video latent and the egocentric prior latent are combined via widthwise and channel-wise concatenation in the latent space, and then fed into pretrained video diffusion model equipped with the proposed geometry-guided self-attention. encoded by frozen VAE encoder, producing latent features x0 Rf chw and p0 Rf chw , respectively. These latents are then concatenated with the noisy latent zt Rf chw to form the input of the diffusion model. The egocentric prior latent p0 shares the same viewpoint as the target egocentric video and therefore preserves pixelwise correspondence. We concatenate p0 with zt along the channel dimension, providing viewpoint-aligned and temporally coherent guidance during generation. Although p0 offers explicit geometric cues for the regions visible in the rendered ego view, it remains noisy and lacks substantial portions of the scene. To complement the missing information in the rendered egocentric view, we further use the exocentric video latent x0 to provide broader scene context. Since the viewpoint of x0 differs from that of the noisy egocentric latent zt, their features are not pixel-wise aligned. Therefore, we concatenate x0 with zt along the width dimension, encouraging the model to infer cross-view correspondences and perform spatial warping implicitly. Unlike [17], which utilizes SDEdit [31] by concatenating noisy conditioning latent with noisy target latent for conditional generation, our method concatenates the clean latent x0 with the noisy zt throughout all denoising timesteps, while only zt is updated and x0 remains fixed. This design encourages the model to consistently reference fine-grained details from x0, enabling more accurate and reliable spatial warping. The overall relation between inputs and outputs is defined as: zt1 = fθ(x0, ztx0, p0m1, m0), (3) as: where fθ denotes single-step denoising function of the VDM, x0 is the exocentric video latent, p0 is the egocentric prior latent, and is the binary mask specifying whether each spatial region is used for conditioning or for synthesis. 4 Once the sampling is complete, we remove the exocentric part of the latent and decode only the egocentric part to obtain the final result. 3.3. Geometry-Guided Self-Attention As mentioned in Sec. 1, the exocentric video condition includes irrelevant regions that can distract the model during exo-to-ego view generation. To address this, we introduce Geometry-Guided Self-Attention (GGA) that adaptively emphasizes spatially corresponding regions between exocentric and egocentric representations. When egocentric query tokens qego Rlc attend to exocentric key tokens kexo Rlc, the attention should jointly account for semantic similarity (i.e., appearance) and 3D spatial alignment. Ideally, tokens that are both semantically similar and geometrically aligned with the egocentric viewpoint should receive higher attention weights, while unrelated or misaligned regions are suppressed to ensure geometric consistency and realism in the generated views. To achieve this, we leverage self-attention augmentation with 3D geometric cues. Using the 3D point cloud obtained in Sec. 3.1, we compute 3D direction vectors from the ego i=0, ci R3 in world space to camera centers = {ci}F each query and key token position, q, R3. The unit direction vectors are defined as ˆq = qci ,. qci2 We then compute the cosine similarity between the two direction vectors and incorporate it into the attention computation as multiplicative geometric prior. kci kci2 , ˆk = Specifically, the modified attention logits are formulated m,n = sm,n + log(cid:0)g(ˆqm, ˆkn) λg (cid:1), g(ˆa, ˆb) = cos sim(ˆa, ˆb) + 1, (4) (5) where sm,n = mkn denotes the standard attention logFigure 4. Geometry-Guided Self-Attention Overview. 3D direction similarities between egocentric queries and exocentric keys are used as an additive bias in the attention map, guiding the model to focus on geometrically aligned regions. Although the orange and red directions are the same key tokens, their directions differ due to different camera centers. The bluered pairs have similar directions and thus receive higher scores, whereas the greenorange pairs have opposite directions and obtain lower scores. its [39] and λg is hyperparameter that balances this geometry bias term defined in Eq. (5). We add one to the cosine similarity term to ensure positive values before taking the logarithm. Finally, given an egocentric query qm and an exocentric key kn, the attention weight am,n is computed as: am,n = = exp(s m,n) j=1 exp(s m,j) exp(sm,n) g(ˆqm, ˆkn) λg j=1 exp(sm,j) g(ˆqm, ˆkj) λg (cid:80)l (cid:80)l (6) (7) . This formulation allows the attention mechanism to be explicitly guided by geometric alignment between query and key directions, improving spatial consistency and visual coherence across views. In image generation, spatial relationships can be encoded by multiplying rotation matrices to each query and key before attention, as done in [10, 23, 24, 38]. However, in video generation, the camera center of qego changes at every frame, making it necessary to compute key directions relative to each query separately. This implies that the geometry bias term should be recomputed for every querykey pair within each frames attention operation. As illustrated in Fig. 4, even kexo located at the same position (e.g. red) may have entirely different direction vectors (e.g. red and orange) depending on the camera pose. To handle this, we compute all pairwise direction similarities between kexo and qego and use this term as an additive bias attention mask, allowing us to reuse optimized attention kernels. This formulation provides precise geometry-guided self-attention that effectively aligns exocentric and egocentric representations. 5 4. Experiments In the following sections, we aim to answer the following research questions that guide our experimental evaluation: How does our method outperform existing baselines in both qualitative and quantitative evaluations? (Sec. 4.2, Sec. 4.3) How accurately does the model reconstruct regions visible in the exocentric view? (Sec. 4.1, Sec. 4.3) How well does the model generalize to unseen scenes and challenging in-the-wild videos? (Sec. 4.2, Sec. 4.3) How does each proposed component contribute to overall performance and generation quality? (Sec. 4.4) 4.1. Experimental Setup Implementation Details. To support channel-wise concatenation of noisy latent and ego prior latent, we adopt the inpainting variant of Wan 2.1 (14B) Image-to-Video model [40] as our base model. We fine-tuned the model using LoRA (rank = 256) with batch size of 1, and single day on 8 H200 (140 GB) GPUs. For the dataset, we curated 4,000 clips from Ego-Exo4D [12] covering diverse scenes and actions, using 3,600 clips for training and 400 for testing. Additionally, we collected 100 unseen clips that are not included in the training set to evaluate generalization performance. More detailed information can be found in Sec. F. Baselines. Among exocentric-to-egocentric existing video generation approaches, Exo2Ego-V [26] and EgoExo-Gen [44] serve as representative baselines. We adopt Exo2Ego-V as our primary baseline, as EgoExo-Gen does not provide publicly available implementation. With the rapid progress in conditional video generation and recent methods have camera control models, several demonstrated performance comparable to or even surpassing Exo2Ego-V. Therefore, we additionally included Figure 5. Qualitative comparison. Each example shows the exocentric input views and the corresponding generated egocentric views. While other methods fail to reconstruct realistic and coherent videos, our approach produces geometrically accurate and high-quality egocentric generations. N/A indicates that the result is unavailable either due to missing ground truth or the need for additional input views. Trajectory Crafter [50], state-of-the-art camera control model, as well as Wan Fun Control [2] and Wan VACE [19], which offer distinct conditioning approach. Wan Fun Control applies channel-wise concatenation for conditioning, and Wan VACE employs an auxiliary conditioning network, providing diverse points of comparison for our method. For the fair comparison, we finetuned these baselines using the same training dataset as ours. Evaluation Metrics. To evaluate the quality of generated videos, we employed three types of criteria. Image Criteria. We measured PSNR, SSIM, LPIPS, and CLIP-I to assess how closely each generated frame matches the ground-truth distribution. Object Criteria. Following the object-level evaluation protocol of Ego-Exo4D [13], we assessed object-level consistency between the generated egocentric video and the ground truth. We used SAM2 [35] to segment and track objects and DINOv3 [37] to establish correspondences. For each matched object, we evaluated centerlocation error, Intersection-Over-Union(IoU), and Contour Accuracy to measure spatial alignment and boundary fidelity. 6 Image Criteria Object Criteria Video Criteria Scenarios Method PSNR SSIM LIPIS CLIP-I Seen Scenes Unseen Scenes Exo2Ego-V TrajectoryCrafter Wan Fun Control Wan VACE EgoX (Ours) Exo2Ego-V TrajectoryCrafter Wan Fun Control Wan VACE EgoX (Ours) 14.53 13.05 12.25 12.95 16.05 12.70 12.24 13.59 12.17 14.38 0.384 0.375 0.463 0.413 0.556 0.439 0.297 0.439 0.345 0.457 0.569 0.606 0.617 0.626 0.498 0.597 0.619 0.604 0.638 0. 0.774 0.780 0.810 0.829 0.896 0.679 0.778 0.799 0.820 0.877 Location Error 156.66 100.74 112.57 109.62 61.81 214.32 192.16 191.40 191.97 149. IoU 0.074 0.128 0.076 0.114 0.363 0.003 0.039 0.042 0.038 0.092 Contour Accuracy 0.364 0.427 0.417 0.376 0. 0.296 0.301 0.329 0.314 0.481 FVD 622.47 546.09 595.07 508.69 184.47 1283.50 821.71 968.78 1045.45 440.64 Temporal Flickering Motion Smoothness Dynamic Degree 0.960 0.960 0.968 0.989 0.977 0.971 0.966 0.971 0.995 0. 0.966 0.980 0.980 0.994 0.990 0.976 0.984 0.985 0.996 0.992 0.985 0.947 0.901 0.673 0.974 0.978 0.944 0.944 0.427 0.989 Table 1. Quantitative Results. Comparison on image, object, and video metrics. Our method achieves the best overall performance, with Wan VACE showing higher video scores due to static outputs. Best results are highlighted in bold, and second-best results are underlined. Image Criteria Object Criteria Video Criteria Method PSNR SSIM LIPIS CLIP-I EgoX (Ours) w/o GGA w/o Ego prior w/o clean latent 16.05 14.77 13.67 15.07 0.556 0.539 0.479 0.528 0.498 0.530 0.573 0.540 0.896 0.897 0.864 0.861 Location Error 61.81 64.30 90.70 70.17 IoU 0.363 0.326 0.417 0.376 Contour Accuracy 0.546 0.538 0.464 0. FVD 184.47 254.08 211.50 343.33 Temporal Flickering Motion Smoothness Dynamic Degree 0.977 0.969 0.974 0.963 0.989 0.987 0.990 0.986 0.974 0.877 0.802 0.864 Table 2. Ablation Study Results. Performance comparison by removing each core component of our framework. The full model achieves the best results, while excluding geometry-guided self-attention, ego prior, or clean latent conditioning causes performance degradation. Best results are highlighted in bold, and second-best results are underlined. Video Criteria. We measured FVD [11] to evaluate how closely the generated video aligns with the groundtruth distribution. In addition, we assessed VBench [51]- Temporal Flickering, Motion Smoothness, and Dynamic Degree to quantify temporal stability and motion quality. 4.2. Qualitative Results Fig. 5 visualizes the qualitative comparisons between our method and the baselines. Note that in the in-the-wild scenario, ground-truth egocentric videos are unavailable, and Exo2Ego-V is also not applicable since only single exocentric video is provided, which does not meet its fourview input requirement. Exo2Ego-V fails to generate highfidelity frames even when using four exocentric inputs, whereas our model achieves superior visual quality and generalizes well to unseen scenes from only single exocentric view. Trajectory Crafter struggles with large camera translations, producing spatial distortions and temporal inconsistencies. Both Wan VACE and Wan Fun Control fail to effectively utilize the exocentric conditioning input, resulting in mismatched geometry, degraded realism, and the inclusion of irrelevant exocentric content in the egocentric view. Overall, these results demonstrate that our model effectively leverages pretrained video diffusion knowledge to generate geometrically accurate, visually coherent, and highly realistic egocentric videos, maintaining strong performance even under challenging in-the-wild conditions. More qualitative results, including temporally aligned visualizations, can be found in Sec. H. 4.3. Quantitative Results As shown in Tab. 1, our method achieves the best overall performance across both image and object criteria. In particular, we observe significant performance gap in the objectbased criteria, indicating that our approach preserves scene geometry and object consistency more effectively than other baselines. While image-level scores may appear slightly lower due to the inherent challenge of synthesizing unseen regions that differ from the ground-truth egocentric view, our method still achieves the best results across all image metrics. For video-based metrics, Wan VACE records the highest temporal smoothness and flicker scores. However, this is largely attributed to its generation of overly static videos with limited motion, resulting in low dynamic degree. In contrast, our model produces temporally coherent and visually dynamic sequences, demonstrating better balance between spatial fidelity and motion realism. 4.4. Ablation Study We conducted ablation studies to evaluate the contribution of each core component in our framework, including the geometry-guided self-attention (GGA), the egocentric prior conditioning, and the clean latent representation. For each ablation variant, one component was removed while keeping all other settings identical. Quantitative evaluations were performed on the seen scene subset to ensure con7 Figure 6. Ablation qualitative comparison. Visual results when removing each core component. Removing any single component, GGA, the egocentric prior, or the clean latent representation, results in degraded generation quality and geometric consistency. ing visually implausible frames. Without the clean latent, the exocentric latent is concatenated in noisy state, which blurs fine-grained details. As result, the target latent fails to preserve these details, leading to missing or degraded object structures. In the last row of Fig. 6, for instance, the model does not generate the spoon or the small circular ingredients on the cutting board that appear in the groundtruth egocentric view. To further demonstrate the effectiveness of the geometryguided self-attention, we visualize the attention maps queried by egocentric tokens. As shown in Fig. 7, without GGA, the model attends to broad irrelevant regions, while with GGA, it sharply focuses on view-relevant areas, reinforcing geometric coherence and stabilizing feature alignment. Additional ablation studies are provided in Sec. G. 5. Conclusion We introduce EgoX, the first framework capable of generating egocentric videos from single exocentric input while achieving strong generalization across diverse scenes. Our method introduces unified conditioning strategy that combines exocentric and egocentric priors via widthand channel-wise concatenation for effective global context and viewpoint alignment, while leveraging lightweight LoRAbased adaptation to preserve the pretrained video diffusion models spatio-temporal reasoning ability. Furthermore, clean latent representations and geometry-guided self-attention enable the model to selectively focus on spatially relevant regions and maintain geometric consistency, resulting in coherent and high-fidelity egocentric generation. Despite its effectiveness, our current framework reFigure 7. Attention map visualization. Visualization of the attention weights when querying the center token of the egocentric view. Without GGA, the model attends to unrelated regions, whereas with GGA, attention is concentrated on related regions, highlighting improved spatial alignment. trolled comparison. As shown in Fig. 6 and Tab. 2, removing any of these components results in noticeable performance drop, both qualitatively and quantitatively. Without GGA, the model fails to maintain geometric alignment, attending to broad and unrelated regions, which leads to spatial inconsistency. Without the egocentric prior, the model lacks explicit pixel-wise and camera trajectory information, thus struggling to follow the correct viewpoint and produc8 quires an egocentric camera pose as input. Although this information can be provided interactively by users, incorporating an automatic head-pose estimation module would be valuable future direction."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 3 [2] aigc-apps. VideoX-Fun: flexible framework for video generation. https://github.com/aigcapps/ VideoX-Fun, 2024. Accessed: YYYY-MM-DD. 6 [3] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. Proc. CVPR, 2025. 3 [4] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled genarXiv preprint erative rendering from single video. arXiv:2503.11647, 2025. 3 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [6] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. 3 [7] Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, and Zhiyong Wu. Humo: Human-centric video generation via collaborative multi-modal conditioning, 2025. [8] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2283122840, 2025. 3 [9] Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, and Ziwei Liu. 4dnex: arXiv Feed-forward 4d generative modeling made easy. preprint arXiv:2508.13154, 2025. 3 [10] Feng Cheng, Mi Luo, Huiyu Wang, Alex Dimakis, Lorenzo Torresani, Gedas Bertasius, and Kristen Grauman. 4diff: 3d-aware diffusion model for third-to-first viewpoint translation. In European Conference on Computer Vision, pages 409427. Springer, 2024. 3, 5 [11] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, JunYan Zhu, and Jia-Bin Huang. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 7 [12] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 5 [13] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 6, 1, [14] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. 3 [15] Yuhang Hu, Boyuan Chen, and Hod Lipson. Egocentric visual self-modeling for autonomous robot dynamics prediction and adaptation. npj Robotics, 3(1):14, 2025. 2 [16] Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025. 3, 2 [17] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. 4 [18] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, et al. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. 2, 3 [19] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 3, 6 [20] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction, 2025. 3 [21] Daekyum Kim, Brian Byunghyun Kang, Kyu Bum Kim, Hyungmin Choi, Jeesoo Ha, Kyu-Jin Cho, and Sungho Jo. Eyes are faster than hands: soft wearable robot learns user intention from the egocentric view. Science Robotics, 4(26): eaav2949, 2019. [22] Kinam Kim, Junha Hyung, and Jaegul Choo. Temporal incontext fine-tuning for versatile control of video diffusion models. arXiv preprint arXiv:2506.00996, 2025. 3 [23] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generaIn Proceedings of tive model for scalable view synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95039513, 2024. 5 [24] Ruilong Li, Brent Yi, Junchen Liu, Hang Gao, Yi Ma, and Angjoo Kanazawa. Cameras as relative positional encoding. arXiv preprint arXiv:2507.10496, 2025. 5 9 [25] Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, and Xi Li. Realcam-i2v: Realworld image-to-video generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. 3 [26] Jia-Wei Liu, Weijia Mao, Zhongcong Xu, Jussi Keppo, and Mike Zheng Shou. Exocentric-to-egocentric video generation. Advances in Neural Information Processing Systems, 37:136149136172, 2024. 2, 3, [27] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, 2015. 1 [28] Mi Luo, Zihui Xue, Alex Dimakis, and Kristen Grauman. Put myself in your shoes: Lifting the egocentric perspective from In European Conference on Computer exocentric videos. Vision, pages 407425. Springer, 2024. 2 [29] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling reference-based camera control for video generation, 2025. 3 [30] Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, and Kaipeng Zhang. Yume: An interactive world generation model. arXiv preprint arXiv:2507.17744, 2025. 3 [31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 4 [32] Junho Park, Andrew Sangwoo Ye, and Taein Kwon. Egoworld: Translating exocentric view to egocentric view arXiv preprint using rich exocentric observations. arXiv:2506.17896, 2025. [33] Minho Park, Taewoong Kang, Jooyeol Yun, Sungwon Hwang, and Jaegul Choo. Spherediff: Tuning-free omnidirectional panoramic image and video generation via spherical latent representation. arXiv preprint arXiv:2504.14396, 2025. 3 [34] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Daniel Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Pytorch3d: An open-source library for 3d deep learning. In CVPR Workshops, 2020. 3 [35] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 6, 1 [36] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 2, 3 [37] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. 6, 1 [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 5 [40] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 5, 2 [41] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details. arXiv preprint arXiv:2507.02546, 2025. 3, 2 [42] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, and Mohit Bansal. EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video. arXiv preprint arXiv:2505.21876, 2025. 3 [43] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: LongarXiv term consistent world simulation with memory. preprint arXiv:2504.12369, 2025. 3 [44] Jilan Xu, Yifei Huang, Baoqi Pei, Junlin Hou, Qingqiu Li, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. Egoexogen: Ego-centric video prediction by watching exo-centric videos. arXiv preprint arXiv:2504.11732, 2025. 2, 3, 5 [45] Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, and Chen Li. Stand-in: lightweight and plug-and-play identity control for video generation. arXiv preprint arXiv:2508.07901, 2025. [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3 [47] Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, et al. Yan: Foundational interactive video generation. arXiv preprint arXiv:2508.08601, 2025. 3 [48] Brent Yi, Chung Min Kim, Justin Kerr, Gina Wu, Rebecca Feng, Anthony Zhang, Jonas Kulhanek, Hongsuk Choi, Yi Ma, Matthew Tancik, and Angjoo Kanazawa. Viser: Imperative, web-based 3d visualization in python, 2025. 1 [49] Guobing Yin. head-pose-estimation: Realtime human head pose estimation with onnxruntime and opencv. https://github.com/yinguobing/head-poseestimation, 2018. 1 [50] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 2, 3, 6 [51] Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. arXiv preprint arXiv:2412.09645, 2024. 7 [52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3 11 EgoX: Egocentric Video Generation from Single Exocentric Video"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. In-the-wild Ego camera. The ego camera for the in-thewild example was obtained by interactively determining its extrinsic parameters using Viser [48]. F. Implementation Detail F.1. GGA Implementation Detail Applying Geometry-Guided Self-Attention (GGA) directly in pixel space is not feasible because the diffusion model operates in the latent space. Therefore, we compute 3D direction vectors at the pixel level and downsample them by averaging over each 41616 patch, matching the VAE downsampling factor include temporal dimension. The resulting patch-level direction vectors are used as geometric cues in the latent-space attention. These geometric terms are precomputed once before the model inference to avoid runtime overhead. Additionally, applying the geometry-guided bias to all attention layers simultaneously would significantly increase memory usage and computational cost. To address this, we separately apply attention kernels for ego-to-exo and exo-to-ego attention, enabling efficient integration of geometric bias without exceeding memory constraints. F.2. Ego Camera Pose for In-the-wild Example Unlike the EgoExo4D [13] dataset, where ground-truth egocentric camera poses are provided, our in-the-wild examples do not include any ego camera pose annotations. To obtain the required egocentric poses for rendering, we manually determined the camera extrinsics using the 3D visualization toolkit Viser [48]. Specifically, we lifted the exocentric video into 3D point cloud and interactively selected the camera pose that best matches the expected egocentric viewpoint, as illustrated in Fig. 8. As mentioned in Sec. 5, incorporating an automatic head-pose estimation module would be valuable future extension. Potential options inFigure 9. Depth align comparison. The above egocentric view is rendered from 3D point clouds across all frames. Without depth alignment, the inconsistent depth values between frames lead to unstable and unexpected camera movements. clude video-based head-pose trackers [49] or SMPL [27]- based pose estimators, which could eliminate manual intervention and enable fully automatic exocentric-to-egocentric generation. F.3. Evaluation Detail In this section, we detail the evaluation procedure used to compute the Object Criteria, leveraging SAM2 [35] for object segmentation and DINOv3 [37] for appearance-based object matching. Object Criteria For each video, we perform object segmentation using SAM2 to obtain all valid object regions. For every detected object, we extract its bounding box and corresponding contour mask. Each object region is then cropped according to its bounding box and encoded into feature vector Rd using pretrained DINOv3 model. To establish correspondences between the ground-truth egocentric video and the generated output, we compute cosine similarities for all possible pairs of object embeddings: si,j = GT (cid:13) (cid:13) (cid:13)f GT (cid:13)2 model (cid:13) (cid:13)f model . (cid:13) (cid:13)2 (8) pair (i, j) is considered valid correspondence only if it satisfies high-confidence appearance threshold si,j τsim, where we set τsim = 0.9. These high-confidence 1 Image Criteria Object Criteria Video Criteria Method PSNR SSIM LIPIS CLIP-I EgoX (Ours) w/o GGA w/o Ego prior w/o Clean latent 14.38 13.27 13.01 14. 0.457 0.432 0.401 0.426 0.552 0.587 0.581 0.571 0.877 0.880 0.855 0.828 Location Error 149.93 154.27 171.95 169. IoU 0.092 0.089 0.059 0.063 Contour Accuracy 0.481 0.400 0.351 0.328 FVD 440.64 522.67 523.00 695.01 Temporal Flickering Motion Smoothness Dynamic Degree 0.9813 0.9812 0.9742 0.9811 0.9923 0.9921 0.9908 0.9917 0.989 0.955 0.843 0.876 Table 3. Ablation Study Results on Unseen Scenes. The performance trends on unseen scenes are consistent with those observed on seen scenes. Best results are highlighted in bold. matched object pairs form the basis for all downstream object-level metrics. Location Error. For valid matched pair, spatial alignment is measured using the Euclidean distance between the centers of the two bounding boxes. Let cGT and cmodel dej note their centers. The location error is computed as: i,j = (cid:13) loc (cid:13)cGT cmodel (cid:13) (cid:13) . (9) Lower values indicate better spatial consistency. Figure 10. GGA benefits example. Without GGA, events occurring outside the visible region are attended to, leading to the generation of unwanted events in the ego view. With GGA, the model effectively focuses only on the visible region, thereby preventing the generation of these unwanted events. Bounding Box IoU. To measure coarse geometric consistency, we compute the Intersection over Union (IoU) between the two bounding boxes: G. In-depth Ablation Study G.1. Ablation on Unseen Scene IoUi,j = Area(BGT Area(BGT Bmodel Bmodel ) ) . (10) Higher IoU indicates closer agreement in object position and scale. Contour Accuracy. To evaluate fine-grained geometric consistency, we measure contour-level similarity using the object contours extracted by SAM2. For each matched object pair, SAM2 produces contour mask, which we denote as GT for the ground-truth and generated frames, respectively. The contour IoU is then computed as: and model IoUcontour i,j = (cid:12) (cid:12)C GT (cid:12) (cid:12)C GT model model (cid:12) (cid:12) (cid:12) (cid:12) . (11) This metric captures whether the object shape is preserved beyond the coarse bounding-box alignment. F.4. Text Prompts Since our method builds on the pretrained diffusion model [40], text prompts are required to condition the model. We generate these text prompts using visionlanguage model (GPT-4o). The system prompt used for generating these descriptions is provided in Tab. 6, and examples of the resulting text prompts can be found in Fig. 18. 2 To further evaluate the generalization capability of each component, we additionally conduct ablation experiments on unseen scenes. As shown in Tab. 3, the overall trends closely follow those observed in the seen-scene setting: removing any single component leads to noticeable degradation in geometric consistency, fidelity, or temporal coherence. These results confirm that all three components, geometry-guided attention, the egocentric prior, and the clean latent strategy, are essential for achieving coherent, high-fidelity egocentric video generation, even in challenging unseen environments. G.2. Point cloud rendering To construct accurate egocentric prior frames, we employ monocular depth estimation [41] combined with depth alignment from ViPE [16]. To validate the importance of depth alignment, we compare point cloud rendering with and without the alignment module. As shown in Fig. 9, without depth alignment, monocular depth predictions exhibit frame-wise scale inconsistencies, causing even static background regions to shift across frames. Although the ego camera remains fixed, misaligned depth introduces artificial camera motion, which can confuse the generative model and degrade viewpoint consistency. In contrast, applying depth alignment corrects these temporal inconsistencies by ensuring that the depth scale is coherent across frames. As result, the rendered point clouds remain stable over time, Image Criteria Object Criteria Video Criteria Method PSNR SSIM LIPIS CLIP-I EgoX (Ours) w/o GGA Prior width, Exo Channel Prior width, Exo width GGA only for inference 16.05 14.77 13.83 14.85 15. 0.556 0.539 0.471 0.499 0.540 0.498 0.530 0.594 0.545 0. 0.896 0.897 0.736 0.876 0.895 Location Error 61.81 64. 83.08 71.93 64.34 IoU 0.363 0.326 0.213 0.261 0. Contour Accuracy 0.546 0.538 0.501 0.501 0.540 FVD 184.47 254.08 274.14 242.83 193.82 Temporal Flickering Motion Smoothness Dynamic Degree 0.977 0.969 0.964 0.953 0. 0.989 0.987 0.986 0.982 0.985 0.974 0.877 0.915 0.910 0. Table 4. Additional Ablation Results. The results from the conditioning strategy ablation and the GGA Training ablation are shown. These comparisons confirm that our integrated approach achieves the highest performance across all evaluated metrics. Best results are highlighted in bold. providing reliable egocentric prior for downstream video generation. G.3. Conditioning Strategy Ablation We evaluate how different conditioning strategies affect model performance by altering how the exocentric latent and the egocentric prior latent are combined. Conceptually, the exocentric view, whose spatial alignment with the egocentric target is not pixel-consistent and requires implicit warping, should be conditioned in way that preserves its global spatial structure, making width-wise concatenation natural choice. Conversely, the egocentric prior provides pixel-aligned viewpoint information, so channel-wise concatenation is better suited for injecting this fine-grained correspondence into the model. To validate this intuition, we experiment with alternative fusion layouts. One variant reverses the two conditioning directions, applying channel-wise concatenation to the exocentric latent and width-wise concatenation to the egocentric prior. Another variant concatenates both inputs widthwise. We do not test the setting where both inputs are concatenated channel-wise, as this requires adding extra network modules such as zero-convs. When both inputs are concatenated width-wise, their combined latent becomes too large to fit in memory. Therefore, we resize the fused tensor to match the original exocentric latent shape. Additionally, when the exocentric view is concatenated channelwise, geometry-guided attention cannot operate because the spatial structure needed for warping is lost, so this variant is evaluated without GGA. As shown in Tab. 4 and Fig. 13, across all comparisons, our proposed conditioning strategy consistently delivers the strongest results. When the exocentric latent is fused channel-wise, the model fails to learn the necessary warping behavior and cannot properly utilize the exocentric conditioning. Similarly, width-wise concatenation of both latents diminishes the influence of the pixel-aligned prior and leads to quality degradation caused by confusion between global and local information. In contrast, our design, width-wise concatenation for exocentric latents and channel, wise fusion for egocentric priors, achieves the best ge-"
        },
        {
            "title": "Ours",
            "content": "-GGA -Ego Prior -Clean Latent Runtime 10.5 min 6.5 min 6.5 min 6.5 min Table 5. Comparison of runtime for each component. Runtime for each component was measured on an NVIDIA H200 GPU ometric alignment, the most reliable conditioning behavior, and the highest visual quality. H. Additional Results H.1. GGA Training Ablation To understand the role of geometry-guided attention (GGA) during learning, we compare two settings: applying GGA only at inference time versus applying it during both training and inference. Because GGA serves as guidance mechanism rather than learnable module, one might expect it to be sufficient as an inference-only operation. However, when GGA is introduced solely at inference time, the model encounters an attention distribution it has never been trained to interpret. As shown in Tab. 4 and Fig. 13, this mismatch leads to noticeable drop in visual fidelity and weaker geometric alignment. In contrast, enabling GGA during training allows the model to learn attention patterns that naturally incorporate geometric bias. As result, the model produces sharper details, more stable reconstructions, and significantly more accurate geometry during egocentric generation. H.2. Runtime We measure runtime based on the denoising stage, which is the most computationally intensive component of our pipeline. Generating the egocentric prior takes less than 10 seconds and represents only small fraction of the total processing time. To quantify the overhead of each component, we evaluate variants of our system that disable individual modules. GGA introduces moderate overhead due to the additional attention bias computation required at every attention layer. However, this cost is deemed highly reasonable 3 and necessary for the significant overall performance improvements observed both qualitatively and quantitatively, particularly in areas like geometry and appearance. Crucially, the GGA provides essential guidance to the model. As illustrated in Fig. 10, when generating the ego-view, the model without GGA may inadvertently attend to events occurring outside the visible region. This leads to the generation of unwanted events within the final ego-view. In contrast, GGA effectively guides the attention mechanism not to attend to these irrelevant regions, thereby preventing the generation of these undesirable events. This critical ability to ensure clean, accurate, and relevant ego-view generation makes the additional computational cost of GGA worthwhile and necessary investment. Using the egocentric prior incurs similar cost to the difference between an imageto-video and text-to-video diffusion model, as it increases the input conditioning dimensionality without modifying the model architecture. The clean latent strategy, however, adds no computational overhead, since it only modifies the noise scheduling during denoising without adding extra operations. H.3. User Study To further evaluate the generalization capability of our method, we conducted user study involving 20 unseenscene videos and 10 in-the-wild videos. total of 19 participants were asked to choose the best video among the five methods, our method and four baselines, for each of the following criteria: Reconstruction Accuracy Which result best preserves the content visible in the exocentric video? Motion/Camera Consistency Which result best follows the motion and camera trajectory observed in the exocentric view? Overall Quality Which result provides the highest overall egocentric video quality? As shown in Fig. 11, our method received the highest number of selections across all questions, significantly outperforming all baselines. These results demonstrate that our approach not only reconstructs view-relevant content more faithfully but also generalizes effectively to challenging unseen and in-the-wild scenarios. H.4. Additional qualitative results We include time-axis visualizations in Figs. 14, 15 and 17, which allow clearer examination of temporal dynamics and overall video consistency. Consistent with the quantitative metrics, our method produces natural, high-fidelity egocentric videos with accurate geometry and stable motion. In contrast, Wan VACE often generates overly static videos with minimal dynamics, while other baselines either fail to properly incorporate the exocentric conditioning or exhibit noticeable artifacts and distortions. We also include Figure 11. User study results. Our method received the highest number of selections across all questions, significantly outperforming all baselines. Figure 12. Failure case due to task ambiguity. The models action misinterpretation occurs when it focuses on small, subtle cue. This is not strictly model failure, but rather limitation imposed by the tasks high ambiguity, where even human observer might struggle to correctly infer the action based on such sparse visual evidence. additional in-the-wild examples for time-axis visualizations in Fig. 16. H.5. Failure example Although our method performs robustly across diverse scenes, challenging real-world scenarios from datasets such as EgoExo4D [13] can still lead to occasional failure cases. These scenes often involve subjects facing away from 4 Figure 13. Additional ablation qualitative comparison. The qualitative results from the conditioning strategy ablation and the GGA Training ablation are shown. Model variants show limitations in geometric fidelity and detail reproduction, whereas our model consistently demonstrates the highest quality output. the camera, rapid or complex body movements, or lowresolution details, making accurate cross-view reasoning extremely difficult. As illustrated in Fig. 12, when an exocentric frame contains ambiguous actions, such as person bending one arm while the other arm is partially occluded, the model may misinterpret the configuration and generate an egocentric view with both arms extended. Such failure cases arise from inherent ambiguities in the exocentric input and the extreme viewpoint transformation required by the task. Figure 14. Qualitative results for time sequence. Our model accurately and seamlessly generates the entire time sequence. Figure 15. Qualitative results for time sequence. Our model accurately and seamlessly generates the entire time sequence. 6 Figure 16. Qualitative comparison with in-the-wild example. Our model generates the entire time sequence accurately and seamlessly, even on challenging in-the-wild examples. Baselines struggle to maintain visual quality and accurate camera movements across all frames. 7 Figure 17. Qualitative comparison for time sequence. Our model accurately and seamlessly generates the entire time sequence. In contrast, other baselines struggle with maintaining high visual quality and generating accurate camera movements across all frames. 8 System Prompt to obtain exo and egocentric video prompt You are hyper-realistic scene reconstruction AI. Your task is to analyze sequence of video frames provided in chronological order and produce comprehensive, two-part analysis: static scene overview followed by dynamic, frame-by-frame action breakdown. Your guiding principle is strict objectivity. MISSION PROTOCOL Phase 1: Scene Establishment First, analyze all provided frames to establish detailed, static description of the physical environment. Detail the surfaces (walls, floors), furniture, and all unmoving background items. This is your establishing shot. Phase 2: Action Transition Analysis After establishing the scene, provide detailed description of the action progression and transitions observed across the sequence. Focus on how actions evolve, change, and flow from one moment to the next, maintaining awareness of the overall context established in Phase 1. CRITICAL DIRECTIVES 1. Exhaustive Object Inventory: THIS IS YOUR MOST IMPORTANT TASK. You must meticulously identify and catalog EVERY visible item. - NO GENERIC TERMS: Do not use vague words like tool, box, utensil, or device. - BE SPECIFIC: Use precise names (e.g., smartphone, coffee mug, wooden spoon, cutting board, refrigerator, laptop computer, ceramic bowl, stainless steel knife). - DESCRIBE PROPERTIES: Include colors, materials, textures, and positions (e.g., blue ceramic mug on granite countertop). 2. Focus on Hand-Object Interaction: THE ACTIONS CORE. - For the [Exo view], your primary narrative focus MUST be the persons hands.** Describe their precise posture, movement, and interaction with objects (e.g., the persons right hand grasps the knife handle, the left hands fingertips stabilize the tomato). - Every action description should revolve around what the hands are doing. 3. Strict Objectivity: DESCRIBE, DO NOT INTERPRET. - AVOID JUDGMENT: Do not use subjective or abstract adjectives (e.g., AVOID modern, beautiful, cluttered, welllit). Describe only physical, measurable attributes. 4. Transition-Focused Analysis - Analyze the sequence as continuous flow of actions - Describe how movements and interactions transition and evolve - Focus on the progression and changes rather than individual frame descriptions - Maintain narrative continuity throughout the sequence OUTPUT STRUCTURE You MUST follow this exact two-block format: [Exo view] Scene Overview: Detailed description of the static background environment from the third-person perspective. List all background objects. Action Analysis: Describe the progression of actions and transitions observed throughout the sequence. Focus on how movements evolve, interactions change, and the flow of activities from beginning to end. Describe the continuous narrative of what is happening. [Ego view] Scene Overview: Detailed description of the static background environment from the first-person perspective. List all background objects. Action Analysis: Describe the progression of actions and transitions observed throughout the sequence from the first-person perspective. Focus on how movements evolve, interactions change, and the flow of activities from beginning to end. Describe the continuous narrative of what is happening from the ego viewpoint. {image} Table 6. System Prompt for VLM. This is the system prompt used to generate the input text prompt for our model. Since the exocentric views were width-wise concatenated, the prompt describes both the exocentric and egocentric views. 9 Figure 18. Used Prompt Example. This is the input text prompt for our model. Since the exocentric views were width-wise concatenated, the prompt describes both the exocentric and egocentric views."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "Seoul National University"
    ]
}