{
    "paper_title": "Frac-Connections: Fractional Extension of Hyper-Connections",
    "authors": [
        "Defa Zhu",
        "Hongzhi Huang",
        "Jundong Zhou",
        "Zihao Huang",
        "Yutao Zeng",
        "Banggu Wu",
        "Qiyang Min",
        "Xun Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections."
        },
        {
            "title": "Start",
            "content": "Frac-Connections: Fractional Extension of Hyper-Connections Defa Zhu1,, Hongzhi Huang1, Jundong Zhou1, Zihao Huang1, Yutao Zeng1, Banggu Wu1, Qiyang Min1,, Xun Zhou1 1ByteDance Seed Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections. Date: March 19, 2025 Correspondence: Defa Zhu at zhudefa@bytedance.com, Qiyang Min at qiyangming@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "5 2 0 2 8 1 ] . [ 1 5 2 1 4 1 . 3 0 5 2 : r Figure 1 Comparison of Frac-Connections and Hyper-Connections based on their expansion rates. Frac-Connections correspond to 1, while Hyper-Connections are defined by 1. The two connection types become identical when the expansion rate is = 1. 1 Residual connections [7] have revolutionized deep learning by facilitating the effective training of very deep networks. These connections mitigate gradient vanishing and are fundamental to architectures such as transformers and convolutional neural networks (CNNs). However, residual connections suffer from tradeoff between gradient vanishing and representation collapse, where the features of adjacent layers become excessively similar, particularly in very deep models [14, 25, 28]. Zhu et al. [28] introduce Hyper-Connections, an expansion of the dimension of hidden state and learnable depth and width connections, to address this issue. While effective, Hyper-Connections increase memory access by expanding the hidden states width. This raises the question: Can we enjoy the benefits of Hyper-Connections without increasing memory access? ) 1 + 0 , i0 ( Layer Index Figure 2 Cosine similarity between the input of the current and the previous layers for the OLMoE-7B models. The curve represents the median of similarity, while the shaded area indicates the range between the 5th and 95th percentiles. To this end, we propose Frac-Connections (FC), novel method that partitions the hidden states into multiple fractions rather than duplicating them and increasing their width. This approach extends the expansion rate of Hyper-Connections (HC) to the fractional domain. In particular, when = 1, Frac-Connections and Hyper-Connections are equivalent, as illustrated in Fig. 1. This reduces memory usage while preserving the ability to model multiple connection strengths. As shown in Fig.2, the similarity between adjacent hidden states in FC lies between that of HC and baseline (Pre-Norm), indicating that their representational capacity follows the order: HC > FC > Pre-Norm. To further validate the effectiveness of Frac-Connections, we conduct extensive experiments on large language models (LLMs), including both dense and Mixture-of-Experts (MoE) [21] architectures. Our results demonstrate that Frac-Connections significantly improve training stability and enhance downstream task performance across wide range of natural language processing benchmarks. We believe that the simplicity, scalability, and efficiency of Frac-Connections will enable their widespread adoption across various domains in machine learning, providing robust foundation for building the next generation of dense and sparse deep learning models."
        },
        {
            "title": "2 Related Work",
            "content": "Transformers [6, 9, 10, 23, 24] have revolutionized deep learning, particularly in natural language processing and computer vision. They rely on self-attention mechanisms to capture long-range dependencies and have become the foundation of large-scale models such as BERT [5] and GPT [2]. key component of Transformers is residual connections [7], which aid training but may also limit model expressiveness [28]. Our work focuses on replacing these residual connections to further enhance Transformer performance. Residual Connections and Their Limitations. Residual connections [7] have been key component in modern deep networks, enabling the training of very deep architectures by mitigating the gradient vanishing problem. 2 Figure 3 Figure 2. Frac-connections (FC) with an expansion rate of = 1/2. (a) Residual connections. (b) Hyper-connections: β1, β2, α0,0, α0,1, α1,0, α1,1, α2,1, and α2,2 are learnable scalars or scalars predicted by the network, depending on the specific HC version. (c) Frac-connections: Frac-connections split the hidden representations into smaller fractions and process each fraction independently. The scalars γ1,2, γ2,1, and γ2,2 are either learnable or predicted by the network, similar to hyper-connections. These fractions are concatenated (denoted as Cat) after processing, followed by integration into the main network pipeline. They are widely used in networks such as CNNs[12] and Transformers [23]. However, despite their effectiveness, residual connections introduce fundamental trade-off between gradient propagation and representation collapse [28], which can degrade performance in extremely deep models. ResiDual [25] addresses this issue by adopting dual-stream design with parallel PreNorm and PostNorm structures, while Hyper-Connections use weighted multi-stream design to significantly improve performance. While this improves performance, the multi-stream approach increases memory consumption. Our Frac-Connections build upon this design by reducing the hidden size of each stream, retaining the benefits of Hyper-Connections without increasing memory usage. Fractal Design. FractalNet [13] proposes partitioning the hidden states into multiple segments, each processed by networks of varying depths, enabling the training of extremely deep neural networks. Frac-Connections share similar design principle; however, instead of assigning each partition to different depth, we associate them with different connection weights."
        },
        {
            "title": "3 Preliminaries",
            "content": "Hyper-Connections (HC) enhance the representation of hidden states in neural networks by introducing hyper hidden matrix. Given the initial input h0 Rd, it is replicated times to construct the initial hyper hidden matrix: H0 = (cid:0)h0 h0 . . . h0(cid:1) Rnd, (1) where is the expansion rate. At the k-th layer, the input is the hyper hidden matrix from the previous layer, denoted as Hk1: Hk1 = (cid:0)hk1 1 hk1 2 . . . hk1 (cid:1) Rnd. (2) The final hidden vectors are aggregated using sum pooling, which reduces the hyper hidden matrix back to single vector. The hyper-connections are modeled by matrix HC R(n+1)(n+1), which defines the connection weights across different components: HCk = (cid:18)011 Bk Ar Am (cid:19) , (3) k, and Ar where Bk, Am For given network layer k, which integrates components such as self-attention and feed-forward networks, the output Hk+1 of the hyper-connections can be expressed as: are submatrices that define the connections within and between layers. where hk 0 is computed as the weighted sum of the hyper hidden matrix using Am k: Hk = Bk (T k(hk1 0 )) + Ar Hk1, hk1 0 = Am Hk1. (4) (5) These matrices capture the relationships across both the depth and width dimensions of the network and are visualized in Fig. 3. To further improve flexibility of the connections, Dynamic Hyper-Connections (DHC) extend this framework by making the weights input-dependent. Instead of using fixed parameters, the connection weights are dynamically predicted based on the input hidden vector Hk. This adaptive mechanism improves its ability to represent complex relationships. The advantages of DHC are particularly evident in tasks such as language modeling."
        },
        {
            "title": "4.1 Overview of Frac-Connections",
            "content": "The purpose of introducing frac-connections is to address the seesaw problem in residual connections while retaining the flexibility of constructing connection strengths, without incurring the additional memory overhead of splitting hidden states into parts as in hyper-connections. This is achieved by generalizing the expansion rate to fractional values. When = 1, frac-connections are equivalent to hyper-connections. For 0 < < 1, frac-connections can be viewed as fractional variant of hyper-connections that divides the hidden states into = 1/n parts instead of replicating them times, where (referred to as the frac-rate) represents the number of partitions. Let Rd represent the hidden state of layer. Instead of replicating into copies as in hyper-connections, frac-connections split into = 1/n parts: = (cid:0)h1 h2 . . . hm (cid:1) = Reshape(h, (m, d/m)), (6) where hi Rd/m for = 1, 2, . . . , m. The Frac-Connections (FC) can be represented by matrix FC, where each element defines the connection weight. The matrix is structured as follows: 4 FC = (cid:18)01m Y (cid:19) R(m+1)(2m) = 0 γ1,1 γ2,1 ... γm, 0 β1 γ1,m α1,1 γ2,m α2,1 ... ... . . . γm,m αm,1 βm α1,m α2,m ... . . . αm,m . (7) Consider the k-th network layer k, it integrates self-attention layers or feed-forward networks within transformers. The output of the FC, denoted by Hk, can be simply formulated as follows: Hk = FCk(T k, Hk1) = Bk k(cid:0)Yk Hk1(cid:1) + Ak Hk1. (8)"
        },
        {
            "title": "4.2 Dynamic and Static Frac-Connections",
            "content": "Since Frac-Connections is the fractional variant of Hyper-Connections, Frac-Connections can be implemented in two forms likewise: 1. Static Frac-Connections: The weights are learnable, but static during testing. 2. Dynamic Frac-Connections: The weights are dynamically computed based on the input, allowing greater flexibility. The matrix representation of dynamic frac-connections (DFC) is defined as follows: FC(H) = (cid:18)01m B(H) Y(H) A(H) (cid:19) Similarly, given layer and input H, we obtain the output of the DFC as follows: ˆH = FC(H)(T , H). (9) (10) In practice, we follow that of DHC [28], combining the dynamic and static matrices to achieve DFC. The dynamic parameters are obtained through linear transformation. To stabilize the training process, we introduce normalization before the linear transformation and apply the tanh activation function after it, scaling it by small initial learnable factor. The following equations detail how these dynamic parameters are computed: = norm(H) B(H) = sβ tanh(HWβ) + R1m Y(H) = sα tanh(HWγ) + Rmm A(H) = sα tanh(HWα) + Rmm (11) (12) (13) (14)"
        },
        {
            "title": "4.3 Initialization and Implementation",
            "content": "In order to make the initialization of the frac-connections equivalent to the Pre-Norm residual connections, we adopt the following initialization strategy. The dynamic parameters Wβ, Wγ, and Wα in Eqs. 12, 13, and 14 are initialized to 0, while the static matrices are initialized as follows: (cid:19) (cid:18)011 = (cid:18) 011 11m emm emm (cid:19) . 5 (15) The static components B, Y, and in Eqs. 7, 14, 12, 13 do not utilize weight decay, whereas the dynamic component does. Frac-Connections for transformer is illuminated in Algorithm 1 and Pytorch-style pseudocode is shown in Algorithm 2, 3. Algorithm 1 Frac-Connections for Transformers Require: Initial hidden vector h0 Rd Require: Fraction rate Ensure: Final output Initialize: h0 Hk Bk k1 Reshape(Yk 1: 2: H0 Reshape(cid:0)h0, (m, d/m)(cid:1) 3: for = 1 to do 4: 5: 6: end for 7: hL Reshape(cid:0)HL, (m, d/m)(cid:1) 8: Umembedding(cid:0)Norm(hL)(cid:1) 9: return Reshape(cid:0)T k(h0 Rm(d/m) Hk1, (d, )) k1), (m, d/m)(cid:1) + Ak Hk"
        },
        {
            "title": "4.4 Parameters and Computation",
            "content": "Static Frac-Connections. All learnable parameters are included in the frac-connection matrix FC in Eq. 7. The number of parameters in one FC is given by: θSHC = θB + θY + θA = + + = (2m + 1). Thus, the number of extra parameters is: Pextra = θSHC 2 L, (16) (17) where is the number of layers. For example, in OLMo-1B-7B-SFC4, Pextra = 1152. Dynamic Frac-Connections. The parameters of DHC are defined in Eqs. 11, 12, 13, and 14, and the number of parameters is given by: θDFC = θnorm + sβ + θWβ + θB + sα + θWγ + θY + θWα + θA = θnorm + dmodel/m (2m + 1) + (2m + 1) + 2, (18) (19) where dmodel is the dimension of the hidden states in the transformer, and θnorm depends on the type of normalization module. For RMSNorm [27], θnorm = dmodel/m. Similar to the static hyper-connections, the number of extra parameters is: (20) Pextra = θDFC 2 L, For example, for OLMo-1B-7B-DFC4, Pextra == 165, 056. The number of parameters for DFC used in the experiments is detailed in Table 1. Computational Analysis. The primary computational cost of both SFC and DFC occurs in line 5 of Algorithm 1, with complexity of O(dmodel 4m). For comparison, the computational cost of the Feed-Forward Network (FFN) is O(2 dmodel dffn), while the projection component of attention requires O(4 dmodel dmodel) operations. Since O(dmodel 4m) O(4 dmodel dmodel) < O(2 dmodel dffn), the computational overhead of FC implementations is negligible compared to the costs of both the FFN and attention projection operations. Here, dffn represents the inner dimension of the FFN. Our analysis confirms that regardless of whether SFC or DFC is implemented, both the additional parameters and computational overhead introduced remain minimal and can be considered negligible in the overall system performance. Detailed computational cost statistics of DFC are presented in Table 2. 6 Table 1 Comparison of number of parameters. Method FC Params(B) Total Params(B) Total Params rate (%) OLMo-1B2 OLMo-1B2-DFC4 OLMoE-1B-7B OLMoE-1B-7B-DFC - 0.000165 - 0.000165 1.17676442 1.17715846 6.91909427 6.91948832 - +0.014% - +0.0024% Table 2 FLOPs per token in forward pass. Method FC FLOPs (G) Total FLOPs (G) Total FLOPs rate (%) OLMo-1B OLMo-1B-DFC OLMoE-1B-7B OLMoE-1B-7B-DFC4 - 0.0013 - 0.0013 2.5587 2.5598 2.3580 2.3629 - +0.044% - +0.056%"
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate Frac-Connections on the pre-training of large language models, including sparse and dense models. Specifically, for sparse models we study Sparse Mixture-of-Experts (MoE) models [21] and follow the experimental setup described by OLMoE [16], conducting ablation studies on OLMoE-1.3B, which has 1.3B total parameters with 260M activated parameters. We further validate the effectiveness of our approach on larger sparse model, OLMoE-7B, which has 7B total parameters with 1.3B activated parameters. For dense models, we follow the OLMo2 [17] training setup to pre-train 1B2 parameter model. Importantly, all experiments were conducted without hyperparameter tuning, and the training hyperparameters were strictly aligned across comparative baselines. Through these experiments across different model scales and architectures, we aim to comprehensively demonstrate the applicability and benefits of our proposed Frac-Connections approach."
        },
        {
            "title": "5.1 Ablation Study",
            "content": "Figure 4 Training loss (0.999 EMA smoothed) loss for OLMoE-1.3B models. We conduct extensive ablation studies on the OLMoE-1.3B model to evaluate different configurations of Frac-Connections, as shown in Figure 4. Effect of different frac-rates. The leftmost of Figure 4 compares the baseline model against versions with Dynamic Frac-Connections (DFC) at different frac-rates (DFC2 and DFC4). The results show that DFC2 demonstrates significant improvement over the baseline, while DFC4 offers only marginal additional gains compared to DFC2. The OLMoE-1.3B-DFC4 model exhibits training loss reduction of approximately 0.014 compared to the baseline. 7 Static Frac-Connections (SFC) v.s. Dynamic Frac-Connections (DFC) . In the middle of Figure 4, both -SFC4 and -DFC4 outperform the baseline. Additionally, -DFC4 achieves better results than -SFC4, suggesting that the dynamic parameter prediction mechanism provides additional modeling capacity. Ablation study on the components of DFC. The rightmost of Figure 4 evaluates the impact of normalization, tanh activation, and rescaling by measuring their loss differences relative to the -DFC2 baseline. From the training loss perspective, removing rescaling (purple line, without sβ and sα in Eq. 12, 13, 14) causes the most severe performance degradation, followed by the removal of tanh activation (green line), while the absence of normalization (blue line) results in the least detrimental effect, though still negatively impacting performance. These findings demonstrate the hierarchical importance of each component in the DFC implementation, with rescaling being particularly crucial for maintaining optimal training dynamics. Given that the original DHC design components exhibit either substantial or modest improvements when implemented in DFC, we opt to preserve the complete original DHC architecture to maintain optimal performance characteristics. These findings underscore the effectiveness of Frac-Connections as lightweight yet impactful enhancement for transformer-based models, offering improved performance with minimal parameter overhead."
        },
        {
            "title": "5.2 MoE Models",
            "content": "Figure 5 Training and evaluation performance of OLMoE-7B models. The plots show the training loss, C4-en loss, and accuracy on HellaSwag, SciQ, Commonsense QA, Social IQA, and WinoGrande over the course of training. The results are EMA-smoothed for clarity. The OLMoE-7B-DFCx4 variant demonstrates improved loss reduction and higher accuracy across multiple benchmarks compared to the baseline OLMoE-7B model, indicating enhanced optimization efficiency and generalization. Converge curves. As shown in Figure 5, from the training loss and C4-en loss curves, we observe that OLMoE-7B-DFC4 achieves faster convergence, with reduction of 0.012 in training loss compared to the baseline. Furthermore, we observe that Hyper-Connections (OLMoE-7B-DHC4) converge significantly faster than Frac-Connections (OLMoE-7B-DFC4), suggesting that when applying HC or FC, trade-off between memory consumption and performance needs to be considered. Downstream performance. Throughout training, the OLMoE-7B-DFC4 variant maintains consistent advantage on most benchmarks, including Commonsense QA and WinoGrande QA. For HellaSwag, the OLMoE-7B-DFC4 variant maintains an early advantage over the baseline; however, as training progresses, the gap narrows, and the baseline model nearly catches up toward the end. Table 3 shows the performance of models trained with 3T tokens, and the OLMoE-7B-DFC4 variant demonstrates higher accuracy across most benchmarks. Specifically, it outperforms the baseline by +0.95% on WinoGrande (67.64% 68.59%), +0.50% 8 Table 3 Downstream evaluations for OLMoE-7B models with training 3T tokens. MMLU Var is modified version of MMLU that includes varying few-shot examples, providing stable feedback during early training. Method OLMoE-7B OLMoE-7B-DFC4 HellaSwag 74.28 74.48 BoolQ 72.87 72.11 WinoGrande MMLU Var PIQA SciQ Commonsense QA 67.64 68.59 41.83 42. 78.73 79.16 93.60 94.10 49.14 49.80 AVG 68.30 68.65 on MMLU Var (41.83% 42.33%), and +0.66% on Commonsense QA (49.14% 49.80%), indicating that Frac-Connections enhance knowledge retention and generalization. These results indicate that Frac-Connections not only improve training efficiency but also lead to better model generalization across diverse NLP tasks."
        },
        {
            "title": "5.3 Dense Models",
            "content": "Figure 6 Training and evaluation performance of OLMo2-1B2 models. The plots show the training loss, C4-en loss, and accuracy on HellaSwag and SciQ over the course of training. The results are EMA-smoothed for clarity. The OLMo2-1B2-DFCx4 variant demonstrates improved loss reduction and higher accuracy compared to the baseline OLMo2-1B2 model. We evaluate Frac-Connections through experiments on the OLMo2-1B2 model, as illustrated in Figure 6 and Table 4. OLMo2-1B2-DFC4 variant exhibits consistently lower training loss and C4-en loss compared to the baseline OLMo2-1B2 model. Furthermore, the OLMo2-1B2-DFC4 variant consistently outperforms the baseline on HellaSwag and SciQ throughout training. This suggests that Frac-Connections facilitate more efficient optimization and improving parameter utilization. Table 4 Downstream evaluations for OLMo2 models with training 2T tokens. MMLU Var is modified version of MMLU that includes varying few-shot examples, providing stable feedback during early training. Methods OLMo2-1B2 OLMo2-1B2-DFC4 HellaSwag 64.7 65.4 BoolQ 63.0 65.1 WinoGrande MMLU Var PIQA SciQ Commonsense QA 61.6 62.7 36.4 37.1 75.6 75.2 91.8 92.2 44.6 44. AVG 62.5 63.2 The downstream evaluation results in Table 4 demonstrate that OLMo2-1B2-DFC4 achieves superior performance across multiple tasks, particularly on BoolQ (+2.1%), WinoGrande (+1.1%), and SciQ (+0.4%), while maintaining comparable performance on PIQA. The average accuracy improvement of +0.7% confirms that Frac-Connections enhance generalization across diverse benchmarks. Notably, the improvements on reasoning-intensive tasks such as BoolQ and WinoGrande highlight the ability of Frac-Connections to enhance model expressiveness without increasing computational overhead."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Frac-Connections, an efficient alternative to Hyper-Connections that divides the hidden states into fractions rather than expanding their width. Frac-Connections address the seesaw effect between gradient vanishing and representation collapse while reducing memory usage and computational costs. Our experimental results demonstrate that Frac-Connections are practical and scalable solution for large language models."
        },
        {
            "title": "References",
            "content": "[1] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 2020. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. [8] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [9] Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, and Xun Zhou. Over-tokenized transformer: Vocabulary is generally worth scaling. arXiv preprint arXiv:2501.16975, 2025. [10] Zihao Huang, Qiyang Min, Hongzhi Huang, Defa Zhu, Yutao Zeng, Ran Guo, and Xun Zhou. Ultra-sparse memory network. arXiv preprint arXiv:2411.12364, 2024. [11] Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. 2017. [12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [13] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. [14] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020. [15] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. [16] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409.02060. 10 [17] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2024. URL https://arxiv.org/abs/2501.00656. [18] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI spring symposium series, 2011. [19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9), 2021. [20] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [21] Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean. The sparsely-gated mixture-of-experts layer. Outrageously large neural networks, 2017. [22] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, 2017. [24] Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, and Xiaoqing Li. Scale-distribution decoupling: Enabling stable and effective training of large language models. arXiv preprint arXiv:2502.15499, 2025. [25] Shufang Xie, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany Hassan Awadalla, Arul Menezes, Tao Qin, and Rui Yan. Residual: Transformer with dual residual connections. arXiv preprint arXiv:2304.14802, 2023. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [27] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [28] Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, and Xun Zhou. Hyper-connections. arXiv preprint arXiv:2409.19606, 2024."
        },
        {
            "title": "Appendix",
            "content": "A PyTorch Implementation of Frac-connections Algorithm 2 Pseudocode of frac-connections in PyTorch-like style. # h: hidden vector (BxLxD) class FracConnection(nn.Module): def __init__(self, dim, rate, config, dynamic_alpha, dynamic_beta, device=None): super(FracConnection, self).__init__() self.rate = rate self.dynamic_alpha = dynamic_alpha self.dynamic_beta = dynamic_beta self.use_tanh = config.use_tanh self.use_hc_norm = config.use_hc_norm self.use_scale = config.use_scale self.static_beta = nn.Parameter(torch.ones((rate,), device=device)) self.static_alpha = nn.Parameter(torch.cat([torch.eye((rate), device=device), torch.eye((rate), device=device)], dim=1) ) if self.dynamic_alpha: self.dynamic_alpha_fn = nn.Parameter(torch.zeros((dim // self.rate, rate*2), device=device)) if self.dynamic_beta: self.dynamic_beta_fn = nn.Parameter(torch.zeros((dim // self.rate, ), device=device)) if self.use_scale: self.dynamic_alpha_scale = nn.Parameter(torch.ones(1, device=device) * 0.01) self.dynamic_beta_scale = nn.Parameter(torch.ones(1, device=device) * 0.01) if self.use_hc_norm: self.layer_norm = LayerNorm(dim // self.rate) def width_connection(self, h): # get alpha and beta h_shape = h.shape h_reshape = h.reshape(h_shape[:-1] + (self.rate, h_shape[-1] // self.rate) ) if self.use_hc_norm: norm_h = self.layer_norm(h_reshape) else: norm_h = h_reshape if self.use_tanh: dynamic_alpha = F.tanh(norm_h @ self.dynamic_alpha_fn) else: dynamic_alpha = norm_h @ self.dynamic_alpha_fn if self.use_scale: dynamic_alpha = dynamic_alpha * self.dynamic_alpha_scale alpha = dynamic_alpha + self.static_alpha[None, None, ...] if self.use_tanh: dynamic_beta = F.tanh(norm_h @ self.dynamic_beta_fn) else: dynamic_beta = norm_h @ self.dynamic_beta_fn if self.use_scale: dynamic_beta = dynamic_beta * self.dynamic_beta_scale beta = dynamic_beta + self.static_beta[None, None, ...] mix_h = (alpha.transpose(-1, -2).contiguous().float() @ h_reshape.float()).bfloat16() return mix_h, beta def depth_connection(self, mix_h, h_o, beta): h_o_shape = h_o.shape = beta[..., None] * h_o.reshape(h_o_shape[:-1] + (self.rate, h_o_shape[-1]//self.rate)) + mix_h[..., self.rate:, :] h_shape = h.shape return h.reshape(h_shape[:-2] + (h_shape[-2] * h_shape[-1], )) 12 Algorithm 3 Pseudocode of transformer with frac-connections in PyTorch-like style. # h: hidden vector (BxLxD) # atten_frac_connection, ffn_frac_connection: frac-connection modules # attn_norm, ffn_norm: normalization modules # Attention Block mix_h, beta = atten_frac_connection.width_connection(h) mix_h_shape = mix_h.shape = mix_h[...,:self.rate,:].reshape(mix_h_shape[:-2] + (mix_h_shape[-2] // 2 * mix_h_shape[-1], )) = attn_norm(h) = self_attention(h) = atten_frac_connection.depth_connection(mix_h, dropout(h), beta) # FFN Block mix_h, beta = ffn_frac_connection.width_connection(h) mix_h_shape = mix_h.shape = mix_h[...,:self.rate,:].reshape(mix_h_shape[:-2] + (mix_h_shape[-2] // 2 * mix_h_shape[-1], )) = ffn_norm(h) = ffn(h) = ffn_frac_connection.depth_connection(mix_h, dropout(h), beta) 13 OLMo2 Model Results Figure 7 Loss and accuracy curves for OLMo2-1B2 and OLMo2-1B2-DFC4 models. 14 OLMoE-7B Model Results Figure 8 Loss and accuracy curves for OLMoE-7B and OLMoE-7B-DFC4 models."
        },
        {
            "title": "D Downstream Benchmarks",
            "content": "Table 5 Downstream Benchmarks. Downstream Benchmarks piqa [1] hellaswag [26] winogrande [19] openbook_qa [15] sciq [11] arc_easy [4] arc_challenage [4] copa [18] boolq [3] commonsense_qa [22] social_iqa [20] mmlu [8]"
        }
    ],
    "affiliations": [
        "ByteDance Seed"
    ]
}