{
    "paper_title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
    "authors": [
        "Anish Pahilajani",
        "Devasha Trivedi",
        "Jincen Shuai",
        "Khin S. Yone",
        "Samyak Rajesh Jain",
        "Namyong Park",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed",
        "Franck Dernoncourt",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 3 9 6 3 0 0 . 1 1 4 2 : r GRS-QA - Graph Reasoning-Structured Question Answering Dataset Anish Pahilajani1 *, Devasha Trivedi1 *, Jincen Shuai1 *, Khin S. Yone1 *, Samyak Rajesh Jain1 *, Namyong Park, Ryan A. Rossi2, Nesreen K. Ahmed3, Franck Dernoncourt2, Yu Wang4 1University of California Santa Cruz, 2Adobe Research, 3Cisco Outshift, 4University of Oregon {apahilaj, detrived, jshuai, kyone, srajeshj}@ucsc.edu park.namyong@gmail.com ryrossi@adobe.com nesahmed@cisco.com dernonco@adobe.com yuwang@uoregon.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics."
        },
        {
            "title": "Introduction",
            "content": "Reasoning in natural language is the fundamental aspect of intelligence (Huang and Chang, 2023), and QA tasks provide quantifiable way to test the reasoning capabilities of intelligent systems(Yang et al., 2018). Recently, the emergence of LLMs has demonstrated unprecedented reasoning capacity in answering questions (Wei et al., 2022; Wang et al., 2024). However, real-world applications often demand even more complex reasoning capability, such as multi-hop reasoning (Atif et al., 2023), where systems must integrate information from multiple sources and perform multiple steps of thinking in certain order to arrive at the final answer and conclusion. * Equal contribution To evaluate the multi-hop reasoning capabilities of LLMs, researchers have developed several multi-hop question-answering (M-QA) datasets, including HotpotQA, 2WikiMultiHopQA, and MuSiQue (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022). MuSiQue constructs genuine multi-step QA pairs by composing connected single-hop questions through bottom-up approach and mitigating existing common shortcuts. 2WikiMultiHopQA integrates structured and unstructured data to provide comprehensive and evidencebased reasoning paths, which ensures authentic multi-hop reasoning. HotpotQA is large-scale and crowd-sourced dataset comprising 113,000 Wikipedia-based QA pairs, offering sentence-level supporting facts for explainable predictions and introducing challenging comparison questions. Despite their contributions to benchmarking LLMs multi-hop reasoning capabilities, these datasets have notable limitations. First, most current M-QA datasets lack explicit reasoning structures for each QA pair, preventing LLMs from leveraging predefined reasoning pathways and forcing them to rely solely on their internal knowledge. Second, they mix questions with varying reasoning complexities without categorization, making it challenging to investigate LLMs QA capability at fine-grained structure level of the question. To address these limitations, we introduce GRSQA, novel Graph Reasoning-Structured Question Answering Dataset, that integrates explicit reasoning structure in the format of the graph (i.e., reasoning graph) to enable multi-hop reasoning analysis of LLM-based MQA. By incorporating reasoning graphs that represent multi-hop inference pathways, GRS-QA offers several advantages. First, these graphs provide transparent way to understand the logical steps LLMs should follow to reach the answer, making the reasoning process explicit and allowing researchers to pinpoint where model may struggle at fine-grained level. Second, the reasoning graphs are categorized based on their structural complexity. Furthermore, each reasoning graph is accompanied by metadata (e.g., the number of reasoning steps and types), which facilitates analysis to identify patterns in question difficulty, reasoning complexity, and LLM performance. Thirdly, the structured nature of the reasoning graphs enables the development of new evaluation metrics, such as reasoning recall/precision, that go beyond answer correctness and allow the assessment of LLMs to replicate the reasoning pathway. To construct the reasoning graph, we treat each sentence as node and add edges based on their original logical relations. In addition, to further investigate the importance of the reasoning structure in correctly answering the question compared to the content, we also generate structural negative samples by adding noise to their graph structure, such as adding extra sentence nodes and rewiring the reasoning graphs. Investigating the QA performance when paired with these additionally introduced negative samples helps demystify the importance of the structure in QA. Our contribution can be summarized as follows: First QA Dataset with Reasoning Graphs: GRS-QA introduces the first question-answering dataset that pairs each instance with explicit reasoning graphs. These reasoning graphs provide transparent pathway to understand the logical steps that LLMs should follow to reach the correct answer, allowing researchers to pinpoint specific areas where model may struggle. Comprehensive Analysis and Categorization of Reasoning Graphs: Each QA pair in GRSQA is accompanied by comprehensive metadata, such as reasoning types and reasoning complexity. This enables performance analysis according to the reasoning structure and offers new insights into how LLMs perform on questions with different levels of reasoning complexity. Negative Reasoning Graphs: In addition to the ground-truth reasoning graph, each QA pair also includes corresponding negative reasoning graphs where the content remains the same, but the structure is slightly modified. This enables an exclusive analysis of the structural impact on reasoning and QA performance."
        },
        {
            "title": "2 GRS-QA Dataset Construction",
            "content": "As motivated before, our work introduces new dataset, GRS-QA, which contains reasoning graphs that trace the logical paths from questions to answers. Specifically, we construct these graphs using QA pairs in the training sets of three existing multi-hop QA datasets: HotpotQA, MuSiQue, and 2WikiMultiHopQA, which provide diverse and comprehensive reasoning structures. These datasets have uniform fields, including question, answer, and supporting facts that serve as the golden contexts for answering the question. To build the reasoning graph, we treat each sentence as node and add edges denoting their connections based on the local relations derived from the original dataset. Figure 1 shows an example of building the positive and negative reasoning graphs for one QA instance from HotpotQA. Next, we will introduce the details of building GRS-QA."
        },
        {
            "title": "2.1 Positive Reasoning Graphs",
            "content": "Positive reasoning graphs illustrate the golden reasoning steps from question to its answer. In these graphs, nodes represent sentences of the golden context that address portions of the question. Edges between nodes define the logical flow that the LLM should follow to arrive at the correct answer. In the HotpotQA dataset, questions are categorized as either \"bridge\" or \"comparison.\" Each question is paired with two sentences from the golden context, which serve as nodes in reasoning graph. For \"comparison\" questions, no edges are established between nodes. For \"bridge\" questions, we use the \"keyword\" field to identify the second sentence as the tail node, designate the other sentence as the head node, and create an edge from head In the MuSiQue dataset, we utilize the to tail. provided question category (e.g., \"4hop2\") to determine the graph structure. The supplied sentence IDs are used to set up the nodes accordingly. For 2WikiMultiHopQA, the triplets in the \"evidences\" field provides sufficient information of establishing the edges for our graph. We extract entities from the initial sentence to locate their corresponding sentences. Additionally, the MuSiQue dataset includes an answerable field that indicates whether the provided context contains the necessary sentences to address parts of the question. If the \"answerable\" field is marked as False, we exclude that data point from GRS-QA, as we cannot generate reasoning graph with incomplete context. 2 Figure 1: Reasoning graphs constructed based on one QA instance from HotpotQA dataset (Yang et al., 2018) that maps out the logical steps required to arrive at the answer. The left-hand side illustrates the positive reasoning graph, which is constructed from the supporting paragraphs provided in the original dataset. This graph represents the gold reasoning path needed to answer the question. On the right-hand side, two types of negative reasoning graphs are derived from the original positive reasoning graphs by either perturbing the edges (e.g., inversing the edge direction in this case) or adding additional nodes with irrelevant sentences."
        },
        {
            "title": "2.3 Dataset Distribution",
            "content": "In addition to the golden reasoning graphs that map out the ground-truth logical steps to answer each question, we also create negative reasoning graphs by perturbing the structure of the positive reasoning graphs. Our goal here is to exclusively investigate the impact of the structure of the logic flow rather than the content of the sentences. Unlike conventional negative samples, where the context differs semantically from the golden one (e.g., the negative samples could be the ones with totally/slightly different sentences from the ground truth), we focus on structural differences. Specifically, we consider two types of structural perturbations: edge perturbation and node perturbation. For edge perturbation, we keep the original nodes but alter the edges by addition, removal, and rewriting, thereby changing the reasoning flow. For node perturbation, we randomly remove some nodes, add new nodes, or swap out nodes with others. The nodes we swap out or add are sourced from the global sentence pool except the ones of the positive graphs. Taking the positive reasoning graph in Figure 1(a) as an example, we reverse the edge from sentence s1 to s2 and create the negative reasoning graph 1 with the incorrect logic flow. Meanwhile, we add another irrelevant sentence s3 as node 3 and add an edge between it and node s1 to create the negative reasoning graph 2. The constructed reasoning graphs are categorized into four main types based on their logical structure: comparison, bridge, compositional, and bridgecomparison, resulting in 12 unique graph structures. Comparison graphs lack edge connections, while bridge graphs feature sequential edges. Bridgecomparison graphs consist of two disconnected components, each resembling bridge. Compositional graphs are tree-structured, where some branches may form bridges. Table 1 outlines the profiles of the QA pairs and their corresponding reasoning graphs. Additionally, the statistics of the GRS-QA dataset are illustrated in Figure 2, showing the distribution of question types (a), the number of hops (b), and the average token count (c). The majority of the dataset comprises bridge and comparison questions, aligning with their frequency in the underlying datasets and their common use in everyday reasoning. Figure 2 shows the average number of nodes and edges for questions with 2 to 5 hops. There is clear proportionality between edge counts and node numbers for 2-hop to 4-hop questions, but 5-hop questions deviate due to their unique graph structures. Lastly, Figure 2 displays the average token count for each graph type, revealing that graphs with simpler structures tend to have longer sentences, leading to higher token count. 3 Graph Type Question Decomposition Comparison_2_1 (C-2-1) Between Athlete and Fun, which band has more members? Athlete 1. How many members are in Athlete? Four members 2. How many members are in Fun? Three members Bridge_2_1 (B-2-1) Who beat the player that won the 2017 Australian mens open tennis single title in the US open? Novak Djokovic 1. Who wins the 2017 australian mens open tennis single title? Roger Federer 2. Who beat Roger Federer in the us open? Novak Djokovic Comparison_3_1 (C-3-1) In which country is the administrative territorial entity for the city where Charlie Harper was born? United Kingdom 1. Where was Charlie Harper born? Hackney 2. In which administrative territorial entity is Hackney located? Middlesex 3. Which country is Middlesex located in? United Kingdom Bridge_3_1 (B-3-1) In which country is the administrative territorial entity for the city where Charlie Harper was born? United Kingdom 1. Where was Charlie Harper born? Hackney 2. In which administrative territorial entity is Hackney located? Middlesex 3. Which country is Middlesex located in? United Kingdom Compositional_3_2 (CO3-2) In which country is Midway, in the same county as McRae in the same state as KAGH-FM? U.S. 1. What state is KAGH-FM located? Arkansas 2. In which administrative territorial entity is McRae located? White County 3. Which country is Midway (near Pleasant Plains), White County, Arkansas located in? U.S. Comparison_4_1 (C-4-1) Did Albrecht Alt and Asli Hassan Abade have the same occupation? no Bridge_4_1 (B-4-1) When did Ukraine gain independence from the first Allied nation to reach the German city where the director of The Man from Morocco was born? 1917 1. [\"Asli Hassan Abade\", \"occupation\", \"pilot\"] 2. [\"Asli Hassan Abade\", \"occupation\", \"military figure\"], 3. [\"Asli Hassan Abade\", \"occupation\", \"civil activist\"] 4. [\"Albrecht Alt\", \"occupation\", \"theologian\"] 5. [\"Albrecht Alt\", \"occupation\", \"lecturer\"] 6. [\"Albrecht Alt\", \"occupation\", \"professor\"] \"supporting_facts\": [[\"Asli Hassan Abade\", 0], [\"Albrecht Alt\", 0],[\"Albrecht Alt\", 2], [\"Albrecht Alt\", 6]] 1. Who is the director of The Man from Morocco? Mutz Greenbaum 2. What is the place of birth of Mutz Greenbaum? Berlin 3. What allied nation was the first to reach the german capitol of Berlin? Soviet Union 4. When did Ukraine gain independence from Soviet Union? 1917 Compositional_4_2 (CO4-2) Compositional_4_3 (CO4-3) Where is the place of death of the man who became leader of the largest country in Europe in square miles after the collapse of the nation Germany agreed to sign non-aggression pact with in 1939? Moscow In what country is Tuolumne, which is within county that borders the county containing Jamestown, and is located within the state where Some Like It Hot was filmed? United States 1. What is the largest country in europe by square miles? Russia 2. In 1939 Germany agreed to sign non-aggression pact with which country? the Soviet Union 3. Who became leader of Russia after the collapse of the Soviet Union? Boris Yeltsin 4. Where did Boris Yeltsin die? Moscow 1. In which administrative territorial entity is Jamestown located? Tuolumne County 2. Which entities share border with Tuolumne County? Stanislaus County 3. Where did they film some like it hot? in California 4. Which country is Tuolumne, Stanislaus County, in California located in?? United States Bridge_Comparison_4_1 (BC-4-1) Are both directors of films The Blue Bird (1940 Film) and Bharya Biddalu from the same country? no 1. [The Blue Bird (1940 film), director, Walter Lang] 2. [Bharya Biddalu, director, Tatineni Rama Rao] 3. [Walter Lang, country of citizenship, American] 4. [Tatineni Rama Rao, country of citizenship, India] Comparison_5_1 (CO-51) Which film has more directors, Red Cow (Film) or Chillerama? Chillerama 1. [\"Red Cow (film)\", \"director\", \"Tsivia Barkai Yacov\"] 2. [\"Chillerama\", \"director\", \"Adam Rifkin\"] 3. [\"Chillerama\", \"director\", \"Tim Sullivan\"] 4. [\"Chillerama\", \"director\", \"Adam Green\"] 5. [\"Chillerama\", \"director\", \"Joe Lynch\"] Bridge_Comparison_5_1 (BC-5-1) \"Do both films The Falcon (Film) and Valentin The Good have the directors from the same country? no 1. [\"The Falcon (film)\", \"director\", \"Vatroslav Mimica\"] 2. [\"Valentin the Good\", \"director\", \"Martin Fri010d\"] 3. [\"Vatroslav Mimica\", \"country of citizenship\", \"Croatian\"] 4. [\"Vatroslav Mimica\", \"country of citizenship\", \"Yugoslavia\"] 5. [\"Martin Fri010d\", \"country of citizenship\", \"Czech\"] Table 1: This table shows the Reasoning graphs of GRS-QA. The reasoning graphs demonstrate the decomposition of the larger question and the reasoning paths to approach the answer. Each of these is constructed using the context and relevant entities for each question. The decomposition is shown with varying formats in the right-most column of the graph, including more questions derived from the original question as well as triples that represent the relations between entities and, in turn, provide subsets of the context. This is consistent with the multiple datasets that each of the question types are extracted from. 4 (a) Number of Questions by Graph types in all dataset splits (b) Average number of nodes and edges in each question type Positive Graphs (c) Average number of tokens in each question types Positive Graphs Figure 2: Statistical Analysis of the Distribution of GRS-QA."
        },
        {
            "title": "3 Experiments",
            "content": "To assess how effectively GRS-QA challenges state-of-the-art models, we benchmark their performance from three key perspectives. First, we evaluate how well retrievers can fetch the correct sentences that form the positive reasoning graphs from pool of candidate sentences. Second, we assess LLM performance on directly answering questions with different reasoning structures without providing any supporting context. Finally, we measure the combined retrieval and generation performance, where LLMs generate answers using various forms of retrieved evidence. To ensure consistency and comparability, we use the same question set across all experiments. To balance statistical significance with budget constraints, we capped the number of questions per type at 100. For question types with fewer than 100 entries in the test set, we used all available questions. In total, we tested 660 questions across 12 different question types. The experiments are all conducted on shared GPU server equipped with six NVIDIA RTX 3090 devices with total of 10 GPU Hours."
        },
        {
            "title": "3.1 Retrieval Performance Benchmark",
            "content": "To evaluate retrieval performance on our GRS-QA dataset, we employ three methods: Best Matching 25 (BM25) (Robertson et al., 2009), Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), and Term Frequency-Inverse Document Frequency (TF-IDF). The objective is to determine whether retrieving relevant evidence sentences becomes more challenging with increasingly complex reasoning structures. For BM25, we use BM25Okapi from the built-in Python library. For DPR, we utilize the pretrained model from Facebook. For TF-IDF, we use the Vectorizer provided by scikit-learn. The retrieval evaluation process is as follows. First, we pull different question types from the test split of GRS-QA and, for each question, compare it against bag of all unique evidence sentences in the test set. Using one of the three retrieval methods (BM25, DPR, or TF-IDF), we retrieve the top 20 evidence sentences. We then calculate F1, precision, and recall by comparing the retrieved sentences with the ground truth sentences. Our results show general trend: BM25 performance drops as the questions become more complex (e.g., multi-hop questions), while DPR and TFIDF display inconsistent behavior across different question types. In some cases, their retrieval metrics are nearly zero. Despite these outliers, BM25 outperforms both DPR and TF-IDF on average, as shown in Figure 6, Figure 7, and Figure 8. We conclude that BM25 is our best retriever model, and the poor performance of DPR on average can be attributed to the fact that we do not train DPR model using the train split of our data. The comprehensive average performance of each retriever on all question types is presented in Table 5 in the appendix."
        },
        {
            "title": "3.2 LLM QA Performance Benchmark",
            "content": "To evaluate the LLM QA performance, we select three LLMs: Llama3(8B Instruct) (et al., 2024), GPT-3.5 (OpenAI, 2024a), and GPT4o-mini (OpenAI, 2024b). Their performance is measured using three metrics: exact match, F1 score, and LLM-asJudge (Zheng et al., 2023), with GPT4o-mini used as the judge. For all experiments, we use temperature of 0.2 for the LLMs and employ float16 precision for the Llama3 8B model. Our results indicate that GPT-3.5 achieves the highest performance, while Llama3 shows comparatively lower performance. Figure 9, Figure 10 and Figure 11 display the performance of all three LLMs across questions of different reasoning graph complexity. On average, as question complexity increases, LLM performance decreases, though there are exceptions like 5 (a) Recall Across Question of Different Reasoning Graphs (b) Weighted Recall Across Questions of Different Hops Figure 3: Comparison of BM25, TFIDF, and DPR Recall and Weighted Recall Across Question Types the comparison_5 question type. This is likely due to only one comparison_5 question in that specific set. Additionally, we had only around 41 5-hop questions compared to approximately 200 each of 2-hop, 3-hop, and 4-hop questions. This scarcity affects the statistical significance of the observed performance. Despite this, the observed trend indeed highlights that the underlying reasoning structure closely affects the M-QA performance."
        },
        {
            "title": "3.3 LLM Performance Benchmark",
            "content": "For this final benchmark, we explore four different retrieval configurations, best retriever, unstructured golden evidence, positive reasoning graphs, and negative reasoning graphs. First, we use our best retriever (BM25) and input the questions from the question set along with the top 20 retrieved edges. Second, we input the questions into the LLMs with unstructured gold evidence. Third, we provide the questions with structured gold evidence by using DOT language to replicate our positive reasoning graphs. Lastly, we prompt the LLMs with the questions and our negative reasoning graphs. For the performance of all three LLMs across these prompt types, refer to Figure 4 and Figure 5."
        },
        {
            "title": "3.3.1 RAG with Best Retriever",
            "content": "For this experiment, we provide LLMs with the top 20 evidence sentences retrieved using BM25, which is identified as the best-performing retriever compared with TF-IDF and DPR. As shown in Table 6 in the Appendix, GPT-3.5 generally outperforms both Llama3 and GPT-4o-mini across most question types, particularly in simpler question types like bridge_2_1 and comparison_2_1, where GPT-3.5 achieves an F1 score of 0.70 and 0.78, respectively. However, all models struggle with more complex question types, such as compositional_3_2 and bridge_4_1, where the metrics were significantly lower. This experiment demonstrates that even with retrieval assistance, LLM performance drops as the question complexity increases. Figure 12, Figure 13, and Figure 14 show the results for each model on each question type with these added retrieval results."
        },
        {
            "title": "3.3.2 RAG with Unstructured Golden Evidence",
            "content": "We then provide LLMs with unstructured golden evidence to investigate if removing irrelevant evidence improves performance. As expected, results improve significantly for all models across almost all question types. As seen in Table 7, GPT-3.5 continues to lead with F1 scores as high as 1.00 for comparison_4_1. Even for more complex question types like bridge_comparison_5_1, it achieves strong F1 score of 0.74, indicating that access to relevant evidence boosts LLM performance. This improvement can be attributed to the reduction of noise from irrelevant evidence in the prompt. Figure 18, Figure 19, and Figure 20 show the performance of each LLM for this set of experiments."
        },
        {
            "title": "3.3.3 RAG with Positive Reasoning Graph",
            "content": "Building on the unstructured evidence prompting, we now prompt the LLMs with our positive reasoning graphs, mapping out the logical steps that guide us from question to answer. Similarly, we see an improvement in LLM performance, suggesting that reasoning graphs help models establish clearer path from evidence to answer, improving both exact match and F1 scores across most question types.. Figure 15, Figure 16 and Figure 17 show the results for each LLM, while Table 8 contains the raw metric scores."
        },
        {
            "title": "3.3.4 RAG with Negative Reasoning Graph",
            "content": "Lastly, we also evaluate the LLMs performance when prompted with negative reasoning graph alongside the question. Figure 21, Figure 22, and 6 (a) GPT-3.5 as LLM-Judge (b) GPT-4o-mini as LLM-Judge (c) Llama3 as LLM-Judge Figure 4: LLM Judge Scores by Question Type for Different LLMs (a) GPT-3.5 as LLM Judge (b) GPT-4o-mini as LLM Judge (c) Llama3 as LLM Judge Figure 5: LLM Judge Scores by Hop Type for Different LLMs Figure 23 present the results for each LLM, while Table 9 provides the raw metric scores. In some cases, such as GPT-3.5 on the compositional_4_2 question type, we observe performance drops, along with other specific question types. This suggests that introducing negative reasoning graphs can sometimes lead to confusion or decreased performance, highlighting the sensitivity of LLMs to misleading or incorrect reasoning paths. This finding underscores the importance of ensuring that the reasoning graphs provided to LLMs are both accurate and relevant to avoid detrimental effects on their performance. and unstructured data from Wikipedia and Wikidata, emphasizing reasoning paths using evidence triples across four question types: inference, comparison, compositional, and bridge comparison. Building on these, our dataset challenges models with complex reasoning tasks, incorporating graph structures to prevent reasoning shortcuts. However, GRS-QA is proposing something that the above datasets lack: structural representation of the reasoning path that is needed to arrive at an answer. This structural representation, although presented were not implemented in the datasets as we have done for GRS-QA."
        },
        {
            "title": "4 Related Work",
            "content": "In developing GRS-QA, we leverage insights from three influential multi-hop question-answering datasets: HotpotQA, MuSiQue, and 2WikiMultiHopQA. HotpotQA (Yang et al., 2018) introduced multi-hop reasoning, requiring models to draw from multiple documents to answer complex questions, with \"supporting facts\" marking essential context. However, models often bypass true multihop reasoning by exploiting shortcuts. MuSiQue (Trivedi et al., 2022) addressed this by designing questions that enforce \"connected reasoning,\" ensuring models engage with all intermediate steps. 2WikiMultiHopQA (Ho et al., 2020) further advanced multi-hop tasks by integrating structured Complementary advancements in retrieval and reasoning also shaped GRS-QA. SURE generates summaries from retrieved passages and verifies answers via zero-shot prompting (Kim et al., 2024). (Karpukhin et al., 2020) uses dense retrieval with dual-encoder for question and passage embeddings, while ORQAretrieves evidence using question-answer pairs for supervision (Lee et al., 2019). (Izacard and Grave, 2021) combines generative models with passage retrieval. In KGQA, (Wu et al., 2023) transforms KG knowledge into text using Rewriter module, and MRPQA enhances answer prediction with minimal labeled data (Wang et al., 2022). (Chen et al., 2024) offers data synthesis framework with minimal human annotation, and PATHFID improves benchmarks by modeling 7 reasoning paths (Yavuz et al., 2022). BeamQA uses beam search in the embedding space to enhance KGQA (Atif et al., 2023). Recent LLM innovations include the LLM-Modulo framework for better critique reformulation in travel planning (Gundawar et al., 2024), LLM-Planner for sample efficiency in visual environments (Song et al., 2023), and GLoRe, which boosts reasoning in LLaMA-2 with stepwise reward models (Havrilla et al., 2024). The LLM Dynamic Planner excels in dynamic tasks (Dagan et al., 2023), while LLM-Assist advances autonomous driving planning (Sharan et al., 2023). Although these methods are improving with each release, GRS-QA poses an additional field that can be used to test models for their robustness when it comes to dealing with graph structures."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce Graph ReasoningStructured Question Answering Dataset (GRSQA), that captures the different types of reasoning pathways necessary for multi-hop questions. Building on existing multi-hop QA datasets, such as HotpotQA, MuSiQue, and 2WikiMultiHopQA, we map out the logical steps required to reach each answer and construct reasoning graphs for every question. This structured representation provides new lens for analyzing how LLMs handle complex multi-hop reasoning and offers fine-grained evaluation of the reasoning steps. Through our analysis of the reasoning structures in GRS-QA, we found that while current QA models perform well on many general tasks, they often struggle with questions that exhibit extremely complex reasoning structures, especially those requiring multi-step inference. This highlights the need for advancements in models ability to process and reason through intricate logical pathways."
        },
        {
            "title": "6 Limitations",
            "content": "One key limitation of GRS-QA is the imbalanced distribution of graph types, as illustrated in Table1. Certain reasoning structures, such as bridge_2_1 (60.54%) and comparison_2_1 (24.88%), are heavily more, while more complex types like bridge_comparison_5_1 and compositional_4_2 are significantly less. This imbalance may bias models toward performing better on more frequent graph types while struggling to generalize to less common but more complex reasoning patterns. In addition, the dataset spans multiple domains and does not adhere to fixed subject area. While this diversity helps increase the generalization capabilities of models trained on the dataset, it introduces challenges in domain-specific reasoning. Questions may vary significantly in their topics, from historical facts to scientific inquiries, without clear demarcation. Lastly, while the inclusion of multi-hop reasoning graphs is strength, it also presents limitation. The complex nature of these graphs requires models to engage in intricate, multi-step reasoning, which is not yet fully optimized in current LLMs."
        },
        {
            "title": "6.1 Future Work",
            "content": "As mentioned above, one of the limitations of this dataset is the imbalance of data types based on the complexity of the reasoning graph. In order to combat this, synthetic data could be generated to duplicate the complex structures that are being underrepresented in the current data. Given that GRSQA spans multiple domains, promising direction for future work would be to segment the dataset by domain (e.g., historical, scientific, general knowledge) and develop domain-adapted models or assist in creating models that are more robust to varying domains. This could help improve the performance of QA models in domain-specific tasks, allowing for more accurate reasoning in specialized contexts. Furthermore, integrating domain-specific knowledge bases could enhance multi-hop reasoning, enabling models to better handle the diversity of content. Furthermore, expanding the variety and quantity of these negative graphs could offer deeper insights into how different graph structures influence or impede the performance of LLMs in completing tasks. By increasing the diversity of negative graph structures, we can better understand how these models navigate and respond to complex reasoning challenges, potentially uncovering patterns that enhance or hinder their reasoning capabilities. Finally, we plan to expand our benchmarking efforts by testing wider variety of model architectures on the dataset. Comparing the performance of different approaches, such as graph neural networks (GNNs) or retrieval-augmented models. It could provide insights into which types of architectures are best suited to handle graph structure"
        },
        {
            "title": "References",
            "content": "Farah Atif, Ola El Khatib, and Djellel Difallah. 2023. Beamqa: Multi-hop knowledge graph question answering with sequence-to-sequence prediction and beam search. Mingda Chen, Xilun Chen, and Wen-tau Yih. 2024. Few-shot data synthesis for open domain multi-hop question answering. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 190208, St. Julians, Malta. Association for Computational Linguistics. Gautier Dagan, Frank Keller, and Alex Lascarides. 2023. Dynamic planning with llm. Preprint, arXiv:2308.06391. Abhimanyu Dubey et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, and Subbarao Kambhampati. 2024. Robust planning with llm-modulo framework: Case study in travel planning. Preprint, arXiv:2405.20625. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Railneau. 2024. Glore: When, where, and how to improve llm reasoning via global and local refinements. Preprint, arXiv:2402.10963. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060. Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. Preprint, arXiv:2212.10403. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. Preprint, arXiv:2007.01282. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. Preprint, arXiv:2004.04906. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. 2024. Sure: Summarizing retrievals using answer candidates for open-domain qa of llms. Preprint, arXiv:2404.13081. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised Preprint, 2019. open domain question answering. arXiv:1906.00300. OpenAI. 2024a. Gpt-3.5 turbo. OpenAI. 2024b. Gpt-4o-mini: Advancing cost-efficient intelligence. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Sharan, Francesco Pittaluga, Vijay Kumar G, and Manmohan Chandraker. 2023. Llm-assist: Enhancing closed-loop planning with language-based reasoning. Preprint, arXiv:2401.00125. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 29983009. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Yu Wang, Nedim Lipka, Ryan Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1920619214. Yu Wang, V.srinivasan@samsung.com V.srinivasan@samsung.com, and Hongxia Jin. 2022. new concept of knowledge based question answering (KBQA) system for multi-hop reasoning. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 40074017, Seattle, United States. Association for Computational Linguistics. the 2022 Conference of Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, and Wei Song. 2023. Retrieve-rewriteanswer: kg-to-text enhanced llms framework for knowledge graph question answering. Preprint, arXiv:2309.11206. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600. Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, and Caiming Xiong. 2022. Modeling multi-hop question answering as single sequence prediction. Preprint, arXiv:2205.09226. 9 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A Dataset Processing",
            "content": "A.1 HotPotQA Processing For each data item, nodes and edges are constructed, with nodes generated from positive paragraphs and characterized by attributes such as evidence and type. In cases where the data item is classified as \"bridge\" type, specific handling is applied to determine the presence of \"bridge\" in the positive paragraphs, influencing edge connections. This results in primary graph structure containing nodes and edges. The process also generates 5 sets of negative examples, where each set consists of two graphs: one with an additional node and another with altered edge connections, creating challenging negative samples. Additionally, another five sets of negative examples are created by randomly selecting nodes from negative paragraphs and constructing corresponding edges. The final output comprises list of processed data items, including the original question, concatenated positive evidence, correct answers, the primary positive graph, and sets of negative graphs. A.2 MuSiQue Processing For each data item, nodes and edges are constructed, with nodes generated from the answers of the question decomposition and edges created by using attributes such as the type. The process also generates five sets of negative examples, where each set consists of two graphs: one with an additional node and another with altered edge connections, creating challenging negative samples. Additionally, another five sets of negative examples are created by randomly selecting nodes from negative paragraphs and constructing corresponding edges. The final output comprises list of processed data items, including the original question, concatenated positive evidence, correct answers, the primary positive graph, and sets of hard and easy negative graphs. A.3 2WikiMultiHopQA Processing To construct 2WikiMultiHopQA reasoning graphs, we extracted four primary graph typesInference, Comparison, Bridge Comparison, and Compositionalfrom the raw dataset, varying node counts 10 to generate additional structures. Gold graphs were built using supporting facts and evidences, with nodes representing supporting facts and edges created based on relations between entities shown in the evidences. Five sets of negative graphs were created by using nodes from the context to add an extra node. Another five sets of negative graphs were created by randomly selecting and constructing the corresponding edges. The final output comprises list of processed data items, including the original question, concatenated positive evidence, correct answers, the primary positive graph, and sets of negative graphs. Due to entity string inconsistencies in the data for 2WikiMultiHopQA train set, there were 4507 instances of the data that were discarded since they were unable to be used to create positive graphs. A.4 Licensing Our use of the following datasets is consistent with their licenses, specifically: 2WikiMultiHopQA: Apache-2.0 https://github.com/AlabLicense NII/2wikimultihop?tab=Apache-2.0-1ov-file HotpotQA: CC BY-SA 4.0 License https://hotpotqa.github.io/ MuSiQue: CC BY 4."
        },
        {
            "title": "License",
            "content": "https://github.com/stonybrooknlp/musique Our dataset will be released under the Creative Commons Attribution 4.0 International License (CC BY 4.0). Statistical Analysis Breakdown of each dataset"
        },
        {
            "title": "Test",
            "content": "Bridge_2_1 Comparison_2_1 58384 13964 7298 1745 7298 1747 total 9043 9045 Table 2: Breakdown of Question Types and Unique Question Count for HotpotQA"
        },
        {
            "title": "Val",
            "content": "Bridge_2_1 Comparison_2_1 Comparison_3_1 Comparison_4_1 Comparison_5_1 Compositional_3_2 Bridge_Comparison_4_1 Bridge_Comparison_5_1 61209 41324 234 10 - 3 27266 308 7651 5165 29 1 - -"
        },
        {
            "title": "Test",
            "content": "7652 5167 30 2 1 1 3409 29 total 130354 16292 16301 Table 3: Breakdown of Question Types and Unique Question Count for 2WikiMultiHopQA Question Type Bridge_2_1 Bridge_3_1 Compositional_3_2 Bridge_4_1 Compositional_4_2 Compositional_4_3 total Train 11478 2987 519 516 101 319 15920 Val 1434 373 64 64 12 39 1986 Test 1436 374 66 65 14 41 1996 Table 4: Breakdown of Question Types and Unique Question Count for MuSiQue Method Recall 0.4921 BM25 0.1619 TF-IDF 0.1037 DPR F1 0.1182 0.0447 0.0285 Precision 0.0680 0.0261 0.0166 Table 5: Average Retrieval Metrics for BM25, TF-IDF, and DPR Figure 7: DPR Retrieval Across Question Types Figure 8: TFIDF Retrieval Across Question Types Figure 6: BM25 Retrieval Across Question Types Figure 9: GPT-3.5 Metrics - No Context Provided 11 Graph Type bridge_2_1 comparison_2_1 bridge_3_1 comparison_3_1 compositional_3_2 bridge_4_1 comparison_4_1 compositional_4_2 compositional_4_3 bridge_comparison_4_1 comparison_5_1 bridge_comparison_5_1 Graph Type bridge_2_1 comparison_2_1 bridge_3_1 comparison_3_1 compositional_3_2 bridge_4_1 comparison_4_1 compositional_4_2 compositional_4_3 bridge_comparison_4_1 comparison_5_1 bridge_comparison_5_1 Graph Type bridge_2_1 comparison_2_1 bridge_3_1 comparison_3_1 compositional_3_2 bridge_4_1 comparison_4_1 compositional_4_2 compositional_4_3 bridge_comparison_4_1 comparison_5_1 bridge_comparison_5_1 Graph Type bridge_2_1 comparison_2_1 bridge_3_1 comparison_3_1 compositional_3_2 bridge_4_1 comparison_4_1 compositional_4_2 compositional_4_3 bridge_comparison_4_1 comparison_5_1 bridge_comparison_5_ Graph Type bridge_2_1 comparison_2_1 bridge_3_1 comparison_3_1 compositional_3_2 bridge_4_1 comparison_4_1 compositional_4_2 compositional_4_3 bridge_comparison_4_1 comparison_5_1 bridge_comparison_5_1 EM (Llama3-8B) F1 (Llama3-8B) LLM (Llama3-8B) EM (GPT-4o-mini) F1 (GPT-4o-mini) LLM (GPT-4o-mini) EM (GPT-3.5) F1 (GPT-3.5) LLM (GPT-3.5) 0.49 0.43 0.04 0.47 0.04 0.03 0.50 0.00 0.15 0.30 0.00 0.50 0.60 0.53 0.07 0.50 0.06 0.06 0.57 0.04 0.16 0.40 0.08 0.52 0.66 0.75 0.09 0.63 0.10 0.15 1.00 0.07 0.22 0.66 0.00 0.72 0.50 0.46 0.10 0.07 0.07 0.06 0.00 0.07 0.22 0.20 0.00 0. 0.67 0.58 0.23 0.25 0.15 0.19 0.17 0.09 0.28 0.37 0.00 0.26 0.73 0.78 0.28 0.70 0.31 0.34 0.50 0.14 0.46 0.74 0.00 0.78 0.58 0.70 0.11 0.57 0.06 0.08 0.50 0.07 0.22 0.63 0.00 0.70 0.70 0.78 0.20 0.58 0.10 0.17 0.50 0.07 0.24 0.66 0.00 0.71 0.72 0.85 0.20 0.63 0.21 0.18 0.50 0.07 0.37 0.67 0.00 0.75 Table 6: Metrics for Prompt Type 0: Question w/ Top-20 Retrieved Evidence Using BM EM (Llama3-8B) F1 (Llama3-8B) LLM (Llama3-8B) EM (GPT-4o-mini) F1 (GPT-4o-mini) LLM (GPT-4o-mini) EM (GPT-3.5) F1 (GPT-3.5) LLM (GPT-3.5) 0.59 0.32 0.31 0.43 0.21 0.18 0.50 0.21 0.32 0.09 0.00 0.05 0.77 0.48 0.44 0.49 0.31 0.35 0.61 0.35 0.38 0.33 0.06 0.18 0.86 0.82 0.62 0.70 0.54 0.63 1.00 0.64 0.61 0.89 0.00 0.82 0.69 0.58 0.45 0.57 0.39 0.42 1.00 0.64 0.39 0.66 1.00 0.38 0.85 0.68 0.63 0.63 0.52 0.62 1.00 0.71 0.44 0.76 1.00 0. 0.90 0.82 0.75 0.73 0.70 0.71 1.00 0.86 0.68 0.93 1.00 0.80 0.65 0.54 0.36 0.60 0.30 0.32 1.00 0.50 0.37 0.77 0.00 0.70 0.80 0.67 0.50 0.64 0.40 0.54 1.00 0.58 0.41 0.80 0.40 0.74 0.90 0.86 0.68 0.67 0.61 0.68 1.00 0.64 0.66 0.86 1.00 0.80 Table 7: Metrics for Prompt Type 1: Question w/ Unstructured Ground Truth Evidence EM (Llama3-8B) F1 (Llama3-8B) LLM (Llama3-8B) EM (GPT-4o-mini) F1 (GPT-4o-mini) LLM (GPT-4o-mini) EM (GPT-3.5) F1 (GPT-3.5) LLM (GPT-3.5) 0.66 0.34 0.38 0.43 0.22 0.20 1.00 0.29 0.32 0.07 0.00 0.03 0.83 0.51 0.52 0.49 0.32 0.40 1.00 0.41 0.37 0.33 0.17 0.20 0.88 0.84 0.66 0.67 0.61 0.68 1.00 0.57 0.63 0.90 1.00 0.78 0.69 0.50 0.45 0.37 0.39 0.42 0.50 0.71 0.49 0.41 1.00 0.07 0.85 0.64 0.64 0.47 0.51 0.63 0.70 0.75 0.55 0.58 1.00 0.31 0.90 0.86 0.71 0.70 0.66 0.68 1.00 0.79 0.78 0.93 1.00 0. 0.68 0.52 0.38 0.60 0.27 0.38 1.00 0.64 0.34 0.70 0.00 0.65 0.82 0.68 0.52 0.63 0.37 0.62 1.00 0.68 0.44 0.78 0.40 0.71 0.88 0.90 0.66 0.67 0.55 0.72 1.00 0.79 0.71 0.89 1.00 0.82 Table 8: Metrics for Prompt Type 2: Question Positive Graph of Ground Truth Evidence EM (Llama3-8B) F1 (Llama3-8B) LLM (Llama3-8B) EM (GPT-4o-mini) F1 (GPT-4o-mini) LLM (GPT-4o-mini) EM (GPT-3.5) F1 (GPT-3.5) LLM (GPT-3.5) 0.61 0.35 0.40 0.47 0.27 0.25 1.00 0.29 0.29 0.08 0.00 0. 0.80 0.52 0.54 0.51 0.36 0.42 1.00 0.43 0.37 0.32 0.18 0.18 0.87 0.83 0.66 0.63 0.63 0.65 1.00 0.71 0.63 0.91 1.00 0.78 0.69 0.50 0.46 0.33 0.43 0.45 0.50 0.71 0.49 0.39 1.00 0.05 0.85 0.64 0.63 0.45 0.54 0.64 0.70 0.75 0.54 0.56 1.00 0.29 0.89 0.86 0.73 0.70 0.69 0.68 1.00 0.79 0.78 0.93 1.00 0.80 0.68 0.52 0.41 0.53 0.24 0.37 1.00 0.64 0.34 0.68 0.00 0. 0.83 0.68 0.54 0.57 0.36 0.58 1.00 0.66 0.43 0.76 0.40 0.70 0.91 0.89 0.65 0.63 0.55 0.69 1.00 0.71 0.71 0.90 1.00 0.78 Table 9: Metrics for Prompt Type 3: Question w/ Negative Graph of Ground Truth Evidence EM (Llama3-8B) F1 (Llama3-8B) LLM (Llama3-8B) EM (GPT-4o-mini) F1 (GPT-4o-mini) LLM (GPT-4o-mini) EM (GPT-3.5) F1 (GPT-3.5) LLM (GPT-3.5) 0.17 0.25 0.04 0.40 0.01 0.00 0.50 0.00 0.05 0.02 0.00 0.00 0.27 0.38 0.10 0.47 0.04 0.05 0.50 0.06 0.07 0.23 0.22 0. 0.33 0.64 0.12 0.63 0.09 0.12 0.50 0.14 0.10 0.56 1.00 0.68 0.31 0.49 0.11 0.17 0.09 0.02 0.00 0.00 0.10 0.40 1.00 0.23 0.40 0.57 0.21 0.32 0.16 0.11 0.12 0.14 0.14 0.52 1.00 0.40 0.39 0.73 0.21 0.63 0.22 0.20 0.50 0.14 0.20 0.75 1.00 0.75 0.27 0.52 0.09 0.47 0.03 0.02 0.00 0.07 0.02 0.14 1.00 0.15 0.38 0.62 0.18 0.52 0.09 0.07 0.00 0.07 0.08 0.21 1.00 0. 0.46 0.71 0.13 0.63 0.13 0.11 0.00 0.07 0.12 0.32 1.00 0.28 Table 10: Metrics for Prompt Type 4: Question w/ No Evidence 12 Figure 10: GPT4o-mini Metrics - No Context Provided Figure 13: GPT4o-mini Metrics - Best Retriever Figure 11: Llama3 Metrics - No Context Provided Figure 14: Llama3 Metrics - Best Retriever Figure 12: GPT-3.5 Metrics - Best Retriever Figure 15: GPT-3.5 Metrics - Positive Graph of Ground Truth Evidence 13 Figure 16: GPT4o-mini Metrics - Positive Graph of Ground Truth Evidence Figure 19: GPT4o-mini Metrics - Unstructured Ground Truth Evidence Figure 17: Llama3 Metrics - Positive Graph of Ground Truth Evidence Figure 20: Llama3 Metrics - Unstructured Ground Truth Evidence Figure 18: GPT-3.5 Metrics - Unstructured Ground Truth Evidence Figure 21: GPT-3.5 Metrics - Negative Graph of Ground Truth Evidence 14 Figure 22: GPT4o-mini Metrics - Negative Graph of Ground Truth Evidence Figure 25: BM25, TFIDF, and DPR Weighted Precision Across Question Types Figure 23: Llama3 Metrics - Negative Graph of Ground Truth Evidence Figure 24: BM25, TFIDF, and DPR Weighted F1 Across Question Types"
        }
    ],
    "affiliations": [
        "University of California Santa Cruz",
        "Adobe Research",
        "Cisco Outshift",
        "University of Oregon"
    ]
}