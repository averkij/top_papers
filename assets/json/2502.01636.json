{
    "paper_title": "Lifelong Sequential Knowledge Editing without Model Degradation",
    "authors": [
        "Akshat Gupta",
        "Phudish Prateepamornkul",
        "Maochuan Lu",
        "Ahmed Alaa",
        "Thomas Hartvigsen",
        "Gopala Anumanchipalli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this \"importance hacking\", the edited layers provide a much larger contributions to the model's output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B."
        },
        {
            "title": "Start",
            "content": "Akshat Gupta 1 Phudish Prateepamornkul 1 2 Maochuan Lu 1 Ahmed Alaa 1 Thomas Hartvigsen 3 Gopala Anumanchipalli"
        },
        {
            "title": "Abstract",
            "content": "Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide crucial insight into the inner workings of locatethen-edit methods. We show that norm-growth is hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this importance hacking, the edited layers provide much larger contributions to the models output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B. 5 2 0 2 3 ] . [ 1 6 3 6 1 0 . 2 0 5 2 : r 1. Introduction Knowledge editing is the task of editing specific facts that model learns during pretraining in data and computing efficient manner (De Cao et al., 2021; Yao et al., 2023; Hartvigsen et al., 2024). In this paper, we focus on category of parameter-modifying knowledge editing methods 1University of California, Berkeley 2 SCB DataX 3University Correspondence to: Akshat Gupta <akof Virginia. shat.gupta@berkeley.edu>. can be Code scalable-model-editing/encore found here - https://github.com/ (a) MEMIT (b) AlphaEdit Figure 1. The continuous growth of norm of edited MLP matrices in LLama3-8B during sequential knowledge editing, as function of number edits. called locate-then-edit methods, which modify only small subset of the model parameters to add or update knowledge. These methods have been the focus of many recent works (Gu et al., 2024; Gupta et al., 2024a;c; Ma et al., 2024; Kolbeinsson et al., 2024; Fang et al., 2024). While prior work showed knowledge editing methods like ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b) lead to catastophic loss of downstream performance within few hundred sequential edits (Gu et al., 2024; Gupta et al., 2024b), recently introduced method called AlphaEdit (Fang et al., 2024) has been able to push this to 3000 edits. In this paper, we aim to enable large-scale sequential knowledge editing without causing model degradation. Towards this goal, we push sequential knowledge editing to what we propose as the next frontier - performing 10,000 sequential knowledge edits without loss of downstream performance. Our work is based on two main observations. Firstly, we show that existing locate-then-edit knowledge editing methods are prone to overfitting on the edited fact, where the output probability of an edited fact is unusually higher when compared to the confidence with which an unedited model predicts facts. Secondly, we show that sequential knowledge updates made to model consistently lead to an increase in the norm of the weight matrix being edited, as shown in Figure 1. We show that this norm-growth is secret trick used by locate-then-edit methods, which we call importance hacking. The increasing norm of the edited matrix also increases the norm of the activation vectors produced by the edited layers on average. This allows the outputs produced from the edited layers to have larger influence 1 Lifelong Sequential Knowledge Editing without Model Degradation on the final output of the model. With this, the edited layers are able to override the information coming from other parts of the model, leading to successful knowledge edits but inadvertently causing loss of general ability, which might require information coming from other parts of the model. Based on these observations, we present targeted interventions to overcome the limitations of overfitting on edited facts and disproportionate norm growth. We first formalize locate-then-edit methods as two-step finetuning process, where the first step uses gradient descent for optimization and the second step directly optimizes the loss using least-square objective. This way of understanding knowledge editing enable us to apply appropriate interventions depending on the optimization scenario. To mitigate overfitting during the gradient descent process, we propose Most-Probable Early Stopping (MPES) - where we halt gradient descent when edited facts become most probable across all the different contexts used to calculate the loss. MPES reduces overfitting on edited facts and consequently improves editing performance while reducing loss of downstream performance. To counteract the growth of the norm of the edited matrix during sequential knowledge editing, we add frobenius-norm constraint within the knowledge editing objective of locate-then-edit methods (Meng et al., 2022b). This constraint still allows for closed-form solution and gives us the benefits of computationally efficient knowledge edits while allowing us to control the increasing norm of edited weight matrices, thus enabling longer and more robust sequential knowledge editing. Towards large-scale sequential knowledge editing, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE combines MPES with Frobenius-norm constrained objective and enables performing 10,000 sequential knowledge edits while maintaining the downstream performance of the original unedited model. ENCORE significantly outperforms prior locatethen-edit methods like ROME, MEMIT, and AlphaEdit, as verified on GPT2-XL, Llama-2-7B, and Llama-3-8B, and is significantly faster than its predecessors. Specifically, ENCORE is 61% faster than MEMIT and 64% faster than AlphaEdit when editing Llama3-8B. 2. Background and Related Work In this section, we provide brief introduction to locate-thenedit knowledge editing methods and present them as twostep fine-tuning process. For more detailed introduction to these methods, we refer the reader to prior works (Meng et al., 2022a;b; Gupta et al., 2024c). Locate-then-edit family of methods like ROME (Meng et al., 2022a) , MEMIT (Meng et al., 2022b) and AlphaEdit (Fang et al., 2024) are used to update facts that can be represented (a) Gradient descent step which finds the target activations for the MLP matrix. (b) Target activations are used to update the second MLP matrix (in red). Figure 2. Presenting locate-then-edit knowledge editing methods as two-step fine-tuning process. in the form of triplets of the form (subject, relation, object) or (s, r, o). Instead of updating all the weights of model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall (Meng et al., 2022a). The location of an edit within model is described by two-dimensional address - (i) an intermediate layer to be edited and (ii) token from the list of input tokens used to create the target representation. The exact layer to be edited is found using causal tracing (Meng et al., 2022a) or an empirical sweep over all decoder layers of the model (Hase et al., 2024; Yoon et al., 2024). Additionally, updating the second MLP layer in the FFN module of decoder layers has shown optimal knowledge editing performance (Geva et al., 2020; Meng et al., 2022a). This provides the first part of the editing address. Meng et al. (2022a) also showed that using the output representation of the subject token of sentence produces the best editing results. This provides the second part of the editing address, where the edit is made using the position index of the last token of the subject. We explain this with an example below. Given fact to be edited, for example - The capital of Malaysia is Singapore, the query phrase for the editing process is The capital of Malaysia is and the target phrase is Singapore. The first part of the editing address, the exact layer whose second MLP matrix gets edited, is decided before the editing begins. The second part of the editing address is the token index of the last subject token, which in this case would be the last subword-token in Malasiya. The intermediate hidden representation of this last subject token is used to make the edit. 2 Lifelong Sequential Knowledge Editing without Model Degradation Once the editing address has been decided, instead of updating the chosen MLP weight matrix directly using gradient descent, the process of locate-then-edit knowledge editing proceeds in two steps - 1. In the first step (Figure 2(a)), gradient descent is used to find the appropriate activation vector that acts as target for the weight matrix to be edited. This target activation is found such that the edited fact is generated by the model. In the example, this activation will cause the model to generate Singapore in response to the question. Note that in this step, no weights are updated and just an intermediate activation vector is found. This new activation vector now serves as the target for the MLP weight matrix chosen for editing. 2. The weight update happens in the second step of editing (Figure 2(b)), where the MLP matrix is updated with the target activation vector found in the previous step, using least squares loss function. This loss function tries to preserve the outputs of the MLP matrix for unrelated contexts while generating the target activation when the input corresponds to the query phrase. Specifically, let W0 be the initial weights of the second MLP matrix, which is being edited to ˆW . k0 is used to indicate an input to the MLP matrix representing activation vectors for information we want to preserve from the original model, and ke is input activation vectors representing facts we want to insert into the model. Let ve be the desired target activation vector for the edited MLP matrix found in step 1 of editing using gradient descent. Then the loss function used to update the MLP weight matrix is formulated using least-squares in the form of preservation-memorization objective (Gupta et al., 2024c): L( ˆW ) where argmin ˆW L( ˆW ) = λ (cid:124) (cid:88) i=1 (cid:13) ˆW ki (cid:13) (cid:13) 0 W0ki 0 (cid:123)(cid:122) preservation + (cid:13) 2 (cid:13) (cid:13) 2 (cid:125) (cid:88) j=1 (cid:124) (cid:13) ˆW kj (cid:13) (cid:13) vj (cid:13) 2 (cid:13) (cid:13) 2 (cid:123)(cid:122) memorization (cid:125) (1) Since the above objective is linear in the argument, we do not need to use gradient descent for optimization. Thus, locate-then-edit methods can be seen as unique type of finetuning method. Instead of updating the MLP matrix directly using gradient descent on the desired data, the weight update happens in two steps using two different types of objective functions for each step. The first step uses gradient descent whereas the second step uses closed-form solution. 3. Methods, Models, Datasets and Evaluation In this paper, we focus on three locate-then-edit methods - ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b) and AlphaEdit (Fang et al., 2024). As discussed in section 2, each of these algorithms use gradient descent to find intermediate target representations followed by second optimization step which is linear in the argument. ROME uses an equality-constraint to enforce memorization in place of the least square constraint as shown in equation 1, whereas equation 1 represents the objective function used in MEMIT. AlphaEdit adds null-space projection term in the MEMIT objective. Prior work has shown model degradation during sequential editing with ROME and MEMIT (Gu et al., 2024; Gupta et al., 2024b), whereas AlphaEdit is recent method that is able to perform sequential editing for up to 3,000 facts. In this paper, we adopt the experimental setting of AlphaEdit where they perform sequential edits in batches of 100 facts. This means that 100 facts are edited into the model with each weight update, and multiple such updates are performed sequentially. Since ROME only allows for one edit to be made at time to the model, we used the batched generalization of ROME, called EMMET (Gupta et al., 2024c), which uses the same equality-constraint objective as ROME but generalizes it to the batched editing case. We evaluate all algorithms on three representative models - GPT2-XL (Radford et al., 2019), Llama2-7B (Touvron et al., 2023) and Llama3-8B (Yoon et al., 2024). All experiments are performed on the CounterFact (Meng et al., 2022a) and zsRE (Mitchell et al., 2021) datasets, which are standard knowledge editing datasets. We present the results for Llama2-7B and Llama3-8B on CounterFact dataset in the main paper and present the remaining results in the appendix due to space constraints. In this paper, we evaluate the editing algorithms along two dimensions - editing performance and downstream performance. The editing performance evaluates the success of the knowledge editing algorithm in making successful edits, while downstream performance evaluates the extent of model degradation following prior work (Fang et al., 2024; Gupta et al., 2024b; Gu et al., 2024). Knowledge Editing Metrics: To evaluate editing performance, we use five standard knowledge editing metrics (Meng et al., 2022a). (i) Efficacy Score (ES), which measures if an edit was successfully made, (ii) Paraphrase Score (PS), which measures if the model is able to recall edited facts in different scenarios, (iii) Neighborhood Score (NS), which measures if edited facts disturbs unrelated knowledge, (iv) Overall Score (S), which is the harmonic mean of ES, PS and NS, and (v) Generation Entropy (GE), which measures the fluency of model. detailed explanation of these 3 Lifelong Sequential Knowledge Editing without Model Degradation Table 1. Comparison between prediction probabilities of edited facts versus facts that model knows through pretraining. MPES results in more natural prediction probabilities while being 39% - 76% faster than other methods. METHOD MODEL ORIGINAL FACT PROB EDITED FACT PROB W/O MPES EDITED FACT PROB W/ MPES TIME PER EDIT W/O MPES (S) TIME PER EDIT W/ MPES (S) EMMET MEMIT ALPHAEDIT GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B 0.39 0.52 0.49 0.39 0.52 0.49 0.39 0.52 0.49 0.98 0.92 0.99 0.65 0.75 0.77 0.98 0.84 0. 0.47 0.59 0.61 0.14 0.41 0.41 0.39 0.37 0.33 1.26 6.61 7.65 1.60 4.84 8.71 1.49 5.89 9. 0.63 ( 50%) 1.86 ( 71%) 1.86 ( 76%) 0.97 ( 39%) 2.79 ( 42%) 3.31 ( 61%) 0.83 ( 44%) 2.69 ( 54%) 3.43 ( 63%) metrics is given in Appendix A. The editing metrics for each model are calculated after making all 10,000 sequential edits (Fang et al., 2024). This approach ensures that the metrics capture both the success of the edits of the latest batch of facts as well as facts that were edited in the past. Downstream Performance Metrics: Following prior work (Fang et al., 2024; Gupta et al., 2024b), we measure downstream performance during knowledge editing using 6 tasks - massive multitask language understanding (MMLU) (Hendrycks et al., 2020), natural language inference (NLI, RTE) (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007), sentiment analysis (SST2) (Socher et al., 2013), paraphrase detection (MRPC) (Dolan & Brockett, 2005), and linguistic acceptability classification (CoLA) (Warstadt et al., 2019). We measure the downstream performance of models every 1000 edits, following the experimental settings of Fang et al. (2024). More details about experimental details can be found in Appendix J. 4. Overfitting during Knowledge Editing As discussed in section 2, locate-then-edit methods can be seen as two-step fine tuning process. The first step uses gradient descent to create intermediate target representations. The gradient descent step minimizes average cross-entropy loss for predicting the target fact over the query phrase augmented by random prefixes. The random prefixes are supposed to represent different contexts in which the edited fact can be recalled, thus aiming to create more general query representation. The average cross-entropy loss over all contexts is optimized as shown: L(θ) ="
        },
        {
            "title": "1\nN",
            "content": "j=N (cid:88) j=1 logPθ[oxj + p] (2) 4 Here represents the query phrase input to the model, represents the target fact to be edited into the model and xj represent random prefixes added to the query phrase. We observe that when the gradient descent process stops, the intermediate target representations are overoptimized on the edited facts and make the probability of the target fact unusually high. This can be seen in Table 1, where the edited facts are predicted with probability much closer to 1 compared to the probability with which an unedited model recalls fact. This phenomenon was also observed by Zhang et al. (2024). More details on the experimental settings can be found in Appendix B. As seen in Table 1, the probability with which the unedited GPT2-XL, Llama2-7B, and Llama3-8B models recall fact are 39%, 52% and 49% respectively. However, facts that get edited into the model with algorithms like Alphaedit or EMMET are predicted with average probabilities of 98-99%. The vocabulary size of these models is in the tens of thousands and placing such high probability mass over every edited fact is result of clear overfitting. We hypothesize that this overfitting of edited facts is one of the reasons why edited models lose their ability to perform downstream tasks when continuously edited using knowledge editing methods. 4.1. MPES and Knowledge Editing To overcome this overfitting over small subset of edited facts, we propose variant of early stopping called mostprobable early stopping (MPES). Conventionally, early stopping is used during training while monitoring validation loss, where training is halted when the validation loss stops improving. In MPES, we stop the gradient descent process in knowledge editing when the target fact becomes the most probable token for all query phrases used for optimization. While the gradient descent loss continues to go down and the target probability continues to increase, the rank of Lifelong Sequential Knowledge Editing without Model Degradation (a) MEMIT (b) AlphaEdit (c) EMMET Figure 3. Average downstream performance measured over 6 tasks (sec 2) for MEMIT, AlphaEdit and EMMET. We see that MPES is able to delay loss of downstream performance for both Llama3-8B and Llama2-7B with additional gains in efficiencey. Table 2. Sequential knowledge editing performance after 10,000 edits for different algorithms. We see that using MPES improves editing metrics across all algorithms and models. METHOD MODEL EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE OVERALL SCORE GENERATION ENTROPY MEMIT MEMIT + MPES ALPHAEDIT ALPHAEDIT + MPES EMMET EMMET + MPES LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B 81.04 49.68 88.43 65.78 61.1 72.67 84.15 88.43 94.98 85.03 96.8 92.96 64.67 49.29 70.83 57. 55.86 63.44 74.94 82.08 84.05 75.06 88.4 86.12 60.95 51.31 65.86 50.25 53.75 52.9 62.87 56.5 55.76 49.08 58.41 55.74 67.859 50.078 73.873 57. 56.74 61.948 72.933 72.832 74.331 65.995 77.393 74.424 442.59 373.48 542.1 560.78 540.92 465.81 583.4 565.36 569.24 567.34 584.28 564.82 the predicted word does not change further. With MPES, the rank of the predicted word performs the same function as validation loss in conventional early stopping. Apart from preventing the model from becoming overly optimized towards specific target fact, using MPES for halting gradient descent also has two other advantages. Firstly, it simplifies monitoring of the gradient descent process and provides principled approach to stopping gradient descent which is directly tied to the knowledge editing objective, that is, accurately recalling edited facts in variety of scenarios without overfitting. Secondly, by optimally stopping the gradient descent process, we also improve the efficiency of locate-then-edit algorithms. MPES leads to 39-76% reduction in editing time per edit depending on the editing algorithm and model used  (Table 1)  . The average downstream performance of AlphaEdit, MEMIT, and EMMET for Llama2-7B and Llama3-8B over 10,000 sequential edits is shown in Figure 31. Comparing 1For task-wise downstream performance, please refer to Appendix baseline methods with MPES, we see that MPES shows significant improvements in downstream performance for both AlphaEdit and MEMIT. For EMMET, MPES performs on par with the baseline but is much faster, as shown in Table 1. This shows that by halting gradient descent at critical point where the edited fact is recognized but not overly dominant, MPES helps maintain the models general abilities for much longer during sequential editing. The editing performance using MPES can be seen in Table 2. We see that MPES not only maintains the models general abilities for longer but also improves editing performance for all algorithms. This is because as the model over-optimizes for edited facts, it not only loses its general abilities but also the ability to recall facts edited in previous iterations, resulting in lower editing scores. This also verifies our hypothesis that overfitting over edited facts is one of the reasons behind the loss of models general ability. To summarize, with MPES we present principled way of stopping the gradient descent step during knowledge editing which results in improved editing performance, de5 Lifelong Sequential Knowledge Editing without Model Degradation lays the loss of downstream performance, and makes current knowledge editing methods much faster. LLMs proceed as follows: = LN1(hl1) al = Att(f l) gl = LN2(hl1 + al) ml = hl = hl1 + al + ml projσ(W cgl + bl c) + bproj (3) (4) (5) (6) (7) (a) Alphaedit (Llama3-8B) (b) MEMIT (Llama3-8B) Figure 4. Comparison between norm of edited MLP matrices and norm of unedited matrices after 5,000 and 10,000 sequential edits. 5. Norm Growth during Sequential Knowledge"
        },
        {
            "title": "Editing",
            "content": "Past work has shown that sequential knowledge editing can lead to an increase in the norm of the edited matrix (Gupta et al., 2024b). This disproportionate growth of norm can be seen in Figure 1 and 4 when using AlphaEdit and MEMIT on Llama3-8B. Both AlphaEdit and MEMIT modify the second MLP matrix of layers 4 to 8. To put the norm growth in perspective with the rest of the model, we plot the norms of the edited matrices after 5k and 10k edits along with the norm of other MLP matrices of the model. We clearly see the disproportionate growth in the norm for the edited layers in Figure 4, where the norm grows to more than 10 times its original value when using MEMIT, and twice the original norm when using AlphaEdit, while the norm of the matrices before and after the edited layers is not changed. The growth of the norm of the edited matrix is continuous process during knowledge editing. As shown in Figure 1, for not even one edit does the norm of the edited matrix remain constant or decrease during editing. The norm always increases! But is this growing norm harmful to the model? We answer this question by analyzing the residual stream and reveal very important property about the inner workings of knowledge editing methods. 5.1. The Secret Mechanics of Knowledge Editing Due to residual connections in an LLM, the final output of model can be written as summation of the outputs of the individual sub-modules. Let hl represent the intermediate hidden state vectors between each decoder layer. Then, the computations within layer of general decoder-only 6 The intermediate hidden vector between each layer, hl, is also sometimes referred to as the residual stream. LN1 represents the first LayerNorm (or equivalently RMSNorm for Llama models) that acts just before the attention module and LN2 represents the second LayerNorm just before the MLP module. Att represents the self-attention module in an LLM whereas the action of traditional MLP module is written in terms of the individual weight matrices (Wf c, Wproj). As the vectors computed in the attention and MLP modules get added back to the residual stream at each layer, the residual stream represents summation of an increasing number of vectors as we go deeper into the model. non-recursive formula for the output of the transformer just before unembedding is shown below: hL = h0 + i=L (cid:88) i=1 ai + i=L (cid:88) i= mi (8) Here, represents the total number of layers in model and hL represents the residual stream after the final layer. Note that hL is the activation just before the application of the final layernorm and unembedding matrices. Thus, the output vector at the final layer is summation of the outputs of individual self-attention and MLP sub-modules. Now, if the norm of the Wproj matrix grows as disproportionately as shown in Figures 1 and 4, the norm of the vectors that are produced from those edited MLP sub-modules will also grow. As the norm of few vectors in the summation in equation 8 grows, these vectors will begin to dominate the sum. Proof for this is shown in Appendix C.1, where we show that if the norm of vector in summation grows, the overall sum effectively tends towards that vector. We also show this effect empirically. The growing norm of activation vectors produced by edited layers before and after editing for Llama3-8B can be seen in Figure 5. Figure 5(a) shows the percentage norm for activation vectors produced from each sub-module of the unedited Llama3-8B model, in comparison to the sum of their norms2. After editing using AlphaEdit, which edits layers 4-8 in the Llama model for 10,000 sequential edits (edited layers are shown in red color on x-axis), we can see drastic increase in the percentage 2This is averaged over 30,000 generated tokens (Appendix C). Lifelong Sequential Knowledge Editing without Model Degradation (a) Average Norm Proportion For Unedited Llama3-8B (b) Average Norm Proportion for Llama3-8B using Alphaedit (c) Average Norm Proportion for Llama3-8B using MEMIT (d) Average Norm Proportion for Llama3-8B using ENCORE Figure 5. The figure shows the proportion of contribution of activations vectors produced from each sub-module to the residual stream. The edited layers are shown in red. We see that the influence of the output of the edited layers grows dramatically after contiuous editing. norms for vectors produced by the edited layers (Figure 5(b)). For example, the norm of vectors produced by layer 8, which accounted for only 1% of the total norm, after editing using AlphaEdit accounts for 7% of the norm. This effect is even more drastic for MEMIT as seen in Figure 5(c), where the activation vectors produced by layer 8 account for almost 40% of the total norm, and the vectors produced by all edited layers account for 85% of the total. This gives us crucial insight into the inner workings of locate-then-edit methods. Apart from optimizing for the preservation-memorization objective in equation 1, these methods also implicitly increase the norms of the edited matrices, and as consequence, the norm of the activation vectors produced from those matrices also increases. This makes the representation after the final layer (hL) be dominated by the output of edited layers, thus giving the edited layers larger authority over the final representations. This allows edited layers to override the information produced from other parts of the model, consequently leading to the success of these knowledge editing methods. This finding is central to explaining the inner workings of locate-then-edit methods, and it also explains their successes and failures. This kind of importance hacking makes the output of the edited layers more dominant in the final output and is shortcut taken by these methods to achieve edit success. However, as the norm of the edited matrix continues to grow when we edit it sequentially, suddenly the final representations begin to largely be composed of the 7 outputs of only the edited layers. This makes it impossible for the model to account for the information processed from other sub-modules which might be important to perform other downstream or unrelated tasks. We hypothesize that this is another cause of the loss of downstream performance. 5.2. Introducing ENCORE In the above discussion, we show how growing norm of edited matrices is detrimental to the functioning of edited models and hypothesize that it is the cause of loss of downstream performance. To test our hypothesis, we propose to add an additional term to the preservation-memorization objective to control this norm growth. Thus, we augment the MEMIT objective with norm-constraint: L( ˆW ) = λp (cid:124) (cid:88) i=1 (cid:13) ˆW ki (cid:13) (cid:13) 0 W0ki 0 (cid:123)(cid:122) preservation + (cid:13) 2 (cid:13) (cid:13) 2 (cid:125) (cid:88) j=1 (cid:124) (cid:13) ˆW kj (cid:13) (cid:13) vj (cid:123)(cid:122) memorization (cid:13) 2 (cid:13) (cid:13) 2 (cid:125) + λn (cid:124) (cid:13) ˆW W0 (cid:13) (cid:13) (cid:123)(cid:122) norm-constraint (cid:13) 2 (cid:13) (cid:13) (cid:125) (9) The original weight matrix is represented by W0 and ˆW represents the edited matrix. The above objective has Lifelong Sequential Knowledge Editing without Model Degradation Table 3. Editing performance of ENCORE when compared to baseline MEMIT and modifications using MPES and Norm Constraint. METHOD MODEL EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE OVERALL SCORE GENERATION ENTROPY MEMIT MEMIT + MPES LLAMA2-7B LLAMA3-8B LLAMA2-7B LLAMA3-8B MEMIT + NORM-CONST LLAMA2-7B LLAMA3-8B ENCORE LLAMA2-7B LLAMA3-8B 81.04 49.68 88.43 65.78 90.94 85. 92.57 88.77 64.67 49.29 70.83 57.58 81.31 77.08 82.64 78.19 60.95 51. 65.86 50.25 59.73 58.45 60.43 60.07 67.859 50.078 73.873 57.17 74.931 71. 76.043 73.707 442.59 373.48 542.1 560.78 539.58 367.46 560.16 523.61 (a) Llama2-7B (b) Llama3-8B Figure 6. Average downstream performance for during sequential editing with ENCORE compared to baseline of MEMIT and addition of MPES and Norm-Constraint (NC). closed-form solution as shown below (proof, Appendix D): = (V1 W0K1)K ˆW = W0 + where 1 (λpK0K 0 + K1K 1 + λnI)1 (10) Editing performance for 10,000 sequential edits using normconstraint can be seen in Table 3. Since we add the normconstraint to the MEMIT objective 3, we compare the performance of MEMIT and MEMIT+MPES. We see significant improvement in all editing metrics for both Llama2-7B and Llama3-8B. The downstream performance can be seen in Figure 6, where norm-constraint sustains downstream performance for much longer compared to MEMIT and MPES. To achieve our goal of performing 10,000 sequential edits without loss of downstream performance, we combine MPES with the norm-constraint objective in equation 9 and present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. With ENCORE, we bring back the classical knowledge editing objective of MEMIT in stronger, faster, and more robust way. The editing perfor3While adding norm-constraint works well with the MEMIT objective, using norm constraint with EMMET and AlphaEdit does not lead to such extensive improvements. We discuss this further in Appendix E. (a) Comparison unedited layers. with (b) Norm growth as function of edits. Figure 7. Growth in norm of edited matrices for ENCORE for Llama3-8B. mance for ENCORE can be seen in Table 3 and Figure 6. For Llama2-7B, we see that maximum improvement in downstream performance (Figure 6(a)) comes from the norm constraint and ENCORE performs equally well. But ENCORE is not only much faster, but also has better overall editing performance  (Table 3)  . For Llama3-8B (Figure 6(b)), we see that the combination of MPES and normconstraint is crucial for sustained downstream performance. ENCORE also controls the growing norm of edited matrices, as shown in Figure 7. As consequence, the activation norms for ENCORE have very similar distribution to the original model, which is shown in Figure 5(d). Thus, ENCORE is able to perform large-scale knowledge editing without taking the shortcut of importance hacking, which also prevents the loss of downstream performance. With ENCORE, we combine the insights presented in this paper about overfitting and implicit importance hacking in locate-then-edit methods and enable long-term sequential knowledge editing. ENCORE is able to consistently sustain downstream evaluation metrics of the original model while going through 10,000 sequential edits. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit for Llama3-8B4. 4ENCORE takes 1.01 seconds per edit for GPT2-XL, 3.33 seconds for Llama2-7B and 3.35 seconds for Llama3-8B. Lifelong Sequential Knowledge Editing without Model Degradation 6. Conclusion In this paper we present two major drawbacks of existing knowledge editing methods - (i) overfitting on edited facts and (ii) continuous and disproportionate norm-growth of edited matrix. We also present insights into the inner workings of locate-then-edit methods and show that they achieve edit success using the short-cut of norm-increase, which increases the importance of the output of edited matrices. We then present ENCORE, faster and stronger knowledge editing method compared to its predecessors. ENCORE is able to perform 10,000 sequential knowledge edits without noticeable loss of downstream performance in about 40% of the time taken by other algorithms for Llama3-8B. While many recent works have shown limitations of knowledge editing methods at scale, we show that with better understanding and appropriate regularizations, these methods can indeed be scaled. We hope this brings further research and excitement into the field of knowledge editing."
        },
        {
            "title": "Impact Statement",
            "content": "This paper advances the field of knowledge editing in LLMs by enabling scalable, updates to models stored knowledge without significant model degradation. Such techniques could be beneficial for multiple practical and societal uses, including quick updating of time-sensitive facts such as changing models knowledge following natural disasters or newly published research. However, as with any technology, there are potential risks with this method. The ability to efficiently overwrite and retain specific facts raises concerns about malicious editing or the injection of wrong information. Further research on robust governance and the use of these frameworks is still needed. Overall, we believe this research can lead to more responsive, up-to-date, and ethically aligned LLMs provided that careful attention is paid to responsible use."
        },
        {
            "title": "References",
            "content": "Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D. The fifth pascal recognizing textual entailment challenge. TAC, 7:8, 2009. Dagan, I., Glickman, O., and Magnini, B. The pascal recognising textual entailment challenge. In Machine learning challenges workshop, pp. 177190. Springer, 2005. De Cao, N., Aziz, W., and Titov, Editing factual knowledge in language models. arXiv preprint arXiv:2104.08164, 2021. I. Dolan, B. and Brockett, C. Automatically constructing corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005. 9 Fang, J., Jiang, H., Wang, K., Ma, Y., Wang, X., He, X., and Chua, T.-s. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355, 2024. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer arXiv feed-forward layers are key-value memories. preprint arXiv:2012.14913, 2020. Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, W. B. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 19, 2007. Gu, J.-C., Xu, H.-X., Ma, J.-Y., Lu, P., Ling, Z.-H., Chang, K.-W., and Peng, N. Model editing can hurt general abilities of large language models. arXiv preprint arXiv:2401.04700, 2024. Gupta, A., Baskaran, S., and Anumanchipalli, G. Rebuilding rome: Resolving model collapse during sequential model editing. arXiv preprint arXiv:2403.07175, 2024a. Gupta, A., Rao, A., and Anumanchipalli, G. Model editing at scale leads to gradual and catastrophic forgetting. arXiv preprint arXiv:2401.07453, 2024b. Gupta, A., Sajnani, D., and Anumanchipalli, G. unified framework for model editing. arXiv preprint arXiv:2403.14236, 2024c. Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., and Szpektor, I. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, volume 7, pp. 785794, 2006. Hartvigsen, T., Sankaranarayanan, S., Palangi, H., Kim, Y., and Ghassemi, M. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 36, 2024. Hase, P., Bansal, M., Kim, B., and Ghandeharioun, A. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. Advances in Neural Information Processing Systems, 36, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Kolbeinsson, A., OBrien, K., Huang, T., Gao, S., Liu, S., Schwarz, J. R., Vaidya, A., Mahmood, F., Zitnik, M., Chen, T., et al. Composable interventions for language models. arXiv preprint arXiv:2407.06483, 2024. Lifelong Sequential Knowledge Editing without Model Degradation Ma, J.-Y., Wang, H., Xu, H.-X., Ling, Z.-H., and Gu, J.-C. Perturbation-restrained sequential model editing. arXiv preprint arXiv:2405.16821, 2024. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022a. Meng, K., Sharma, A. S., Andonian, A., Belinkov, Y., and Bau, D. Mass-editing memory in transformer. arXiv preprint arXiv:2210.07229, 2022b. Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642, 2013. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv. org/abs/2307.09288, 2023. Warstadt, A., Singh, A., and Bowman, S. R. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625641, 2019. Yao, Y., Wang, P., Tian, B., Cheng, S., Li, Z., Deng, S., Chen, H., and Zhang, N. Editing large language models: Problems, methods, and opportunities. arXiv preprint arXiv:2305.13172, 2023. Yoon, J., Gupta, A., and Anumanchipalli, G. Is bigger edit batch size always better?an empirical study on model editing with llama-3. arXiv preprint arXiv:2405.00664, 2024. Zhang, M., Ye, X., Liu, Q., Ren, P., Wu, S., and Chen, Z. Uncovering overfitting in large language model editing. arXiv preprint arXiv:2410.07819, 2024. A. Knowledge Editing Metrics more detailed explanation of the knowledge editing metrics used in this paper is below: 1. Efficacy Score (ES): assesses whether an edit has been successful. It is calculated as the percentage of edits where (new fact) > (old fact) when evaluated on paraphrases of the query prompt. 2. Paraphrase Score (PS): measures the models ability to generalize after an edit. Specifically, it is the percentage of edits where (new fact) > (old fact) for paraphrased versions of the query prompt. 3. Neighborhood Score (NS): evaluates the locality of model edit by determining whether editing one fact affects other facts stored in the model. It is the percentage of unaffected facts in the neighborhood of the edited fact. 4. Generation Entropy (GE): measures the fluency of the models text generation post-edit. GE is computed as the weighted average of bi-gram and tri-gram entropies in the text generated by the edited model. lower GE indicates repetitive text generation, common failure mode (Meng et al., 2022a; Gupta et al., 2024b). 5. Score (S): introduced by (Meng et al., 2022a), this composite metric combines edit success, generalization, and locality into single score. It is calculated as the harmonic mean of the Efficacy Score (ES), Paraphrase Score (PS), and Neighborhood Score (NS). B. Experimental Detail on Overfitting During"
        },
        {
            "title": "Knowledge Editing",
            "content": "For each method and model, we conducted three different experiments: 1. Unedited fact recall probability - In this case, we calculate the average probability with which fact is recalled by the unedited/original model. These are the facts that the model learnt through its pre-training. The model is asked questions from the CounterFact dataset, and we average the probability with which the model predicts the fact correctly. 2. Edited fact probability without MPES - In this case, we evaluate the probability with which model recalls fact that is edited into the model. This is the standard baseline case without MPES. 3. Edited fact probability WITH MPES - Here we also evaluate the average probability by which model 10 Lifelong Sequential Knowledge Editing without Model Degradation Table 4. Knowledge editing performance for GPT2-XL on the CounterFact dataset for different algorithms in combination with MPES. MODEL METHOD EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE OVERALL SCORE GENERATION ENTROPY GPT2-XL EMMET EMMET+MPES ALPHAEDIT ALPHAEDIT + MPES MEMIT MEMIT + MPES NORM-CONSTRAINT ENCORE 79.27 95.08 88.58 95.52 94.04 91.43 93.89 93.21 67.45 79.47 70.33 82.08 79.91 73.68 80.9 78. 52.39 56.33 56.04 60.03 57.9 61.71 58.0 59.95 64.48 73.43 69.2 76.32 74.22 73.68 74.53 74.58 570.24 555.39 580.27 565.44 517.37 532.47 504.68 524.34 recalls an edited fact. In this case, MPES is used during editing the fact. C.1. Theoretical Analaysis of Growth of Vector Norm in Summation In each of the experiments we performed, we passed average numbers over 1000 edited facts with batch size of 1. We use the CounterFact dataset (Meng et al., 2022a) for all of these experiments. We show the result in table 1. As we can see from the table, the unedited fact token probability is pretty low but once we run the edited fact the probability increases to almost 1 for some cases. MPES brings the probability of fact recall for edited facts down to more natural value, which prevents the overfitting problem that we present in this paper. C. Experimental Details on Activation Norm"
        },
        {
            "title": "Growth",
            "content": "To assess the impact of the activation vectors generated by the edited layers before and after editing, we conducted an experiment where we edited our model on total of 10,000 facts using batch size of 100. Once the model was edited, we evaluated the norm of the activations produced by each layer by passing it through Wikipedia dataset containing 30,000 examples. For each example, the model performed one-word prediction task given an input context, and we measured the norm of the activation vectors produced from each layer in the model. We repeated the same process for the unedited model to compare the differences. For each of the 30,000 examples, we calculated the proportion of the activation norm at each layer. We then plotted the mean and standard deviation of these proportions for both the edited and unedited models in figures 8 - 13. As shown in our results, the proportion of activation norms for the layers that were edited is significantly higher than their neighboring layers. In fact, some of the edited layers show the highest proportions overall. The edited layers are highlighted with red color on the x-axis. We want to understand the effect of excessive increase in the norm of vector in sum of vectors. First, lets start with an easy example where we have summation of two vectors, = + and then there is excessive increase in the norm of the first vector, that is = ka + where is some positive scalar. To evaluate the effect of this increase, we analyses the tendencies of the sum as increases. We first want to understand the norm of s. We have the following : s2 = ka + b2 = k2a2 + 2ka + b2 From this we can clearly see that as increases the first term quadratic in will dominate. This means that as , s2 k2a2, or ka, which is the norm of the new vector. Thus, as the norm of one of the vectors in the summation increases, the norm of the summation tends to the norm of that vector with increasing norm. Next, we look at the tendencies of the orientation of the summation as the norm of one vector increases. Let θ be the angle between and ka. Then, the cosine of the angle between the summation and the new vector ka is as follows (note that angle between and ka is same as the angle between and a): cos θ = sa = (ka + b) ka + ba In the limit of , ka as shown above. Thus, the cosine expression in the limit can be written as: 11 Lifelong Sequential Knowledge Editing without Model Degradation cos θ = (ka + b) ka2 = ka ka2 + = 1 + ka2 ka2 Thus, as , the cosine of angle between the sum and the vector tends to 1, or the angle between the summation and the vector tends to zero. This shows that as the norm of vector in the summation continues to increase, the both the norm and the orientation of the summation tends towards the vector with increasing norm. Finally, the same proof can be generalied to summation of multiple vectors, where represents the sum of the remaining vectors. (a) Average Norm Proportion For Unedited GPT2-XL (b) Average Norm Proportion for GPT2-XL using Alphaedit (c) Average Norm Proportion for GPT2-XL using EMMET (d) Average Norm Proportion for GPT2-XL using MEMIT Figure 8. Activation norm growth for GPT2-XL. 12 Lifelong Sequential Knowledge Editing without Model Degradation (a) Average Norm Proportion For Unedited Llama2-7B (a) Average Norm Proportion For Unedited Llama3-8B (b) Average Norm Proportion for Llama2-7B using Alphaedit (b) Average Norm Proportion for Llama3-8B using Alphaedit (c) Average Norm Proportion for Llama2-7B using EMMET (c) Average Norm Proportion for Llama3-8B using EMMET (d) Average Norm Proportion for Llama2-7B using MEMIT (d) Average Norm Proportion for Llama3-8B using MEMIT Figure 9. Activation norm growth for Llama2-7B. Figure 10. Activation norm growth for Llama3-8B. 13 Lifelong Sequential Knowledge Editing without Model Degradation (a) Average Norm Proportion For GPT2-XL using MEMIT (a) Average Norm Proportion For Llama2-7B using MEMIT (b) Average Norm Proportion for GPT2-XL using Norm Constraint (b) Average Norm Proportion for Llama2-7B using Norm Constraint (c) Average Norm Proportion for GPT2-XL using ENCORE (c) Average Norm Proportion for Llama2-7B using ENCORE Figure 11. Activation norm growth for GPT2-XL using Norm Constraint and ENCORE. Figure 12. Activation norm growth for Llama2-7B using Norm Constraint and ENCORE. 14 Lifelong Sequential Knowledge Editing without Model Degradation (a) Average Norm Proportion For Llama3-8B using MEMIT (b) Average Norm Proportion for Llama3-8B using Norm Constraint (c) Average Norm Proportion for Llama3-8B using ENCORE Figure 13. Activation norm growth for Llama3-8B using Norm Constraint and ENCORE. 15 Lifelong Sequential Knowledge Editing without Model Degradation becomes free to increase the norm of edited matrices, which causes anomalous behaviors as discussed in the main paper. Thus, the norm constraint always comes at the trade-off of editability. For the MEMIT objective, the norm-constraint helps both editing and downstream performance and the algorithm is best able to make use of this constraint. However, this is not the case for other algorithms. The theoretical reasons why this happens are beyond the scope of this work. But we provide evidence for this trade-off empirically for EMMET below. Table 5 shows the editing performance of the EMMET baseline without any norm-constraint compared against two cases where norm-constraint is added to its objective. In one case, we present the best editing score that can be achieved when non-zero norm-constrain hyperparameter is added to the EMMET objective (λn = 20). In this case, we clearly see that even the best editing score with norm constraint is worse than the baseline EMMET. Figure 14(b) shows the downstream performance for this case, which is even worse than the baseline (Figure 14(a)). Secondly, in Table 5 we also show the editing score for the hyperparameter that achieves the best downstream performance with non-zero norm-constrain hyperparameter in the EMMET objective (λn = 100). We clearly see significant drop from baseline editing performance across all metrics. The downstream performance of this case is shown in Figure 14(c), which is not much different from the baseline (Figure 14(a)). Thus, we show that the norm-constraint does not work well with the EMMET objective. D. Proof for ENCORE Objective First we have that we can write the equation 9 in term of matrix form where we can stack the ki 0. Specifically, we define K0 = [k1 kB 0 k2 0 kP ] vB v2 and V1 = [v1 ] 0 ], K1 = [k1 column wise and instead the L2 norm will become the frobenius norm and we have that λp (cid:13) ˆW K0 W0K0 (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) ˆW K1 V1 (cid:13) (cid:13) + (cid:13) 2 (cid:13) (cid:13) +λn (cid:13) ˆW W0 (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) We can differentiate this expression with respect to ˆW and let it equal to 0, we get the following λp ˆW K0K 0 λpW0K0K V1K 0 + ˆW K1K 1 + λn ˆW λnW0 = 0 λp ˆW K0K = λpW0K0K 0 + ˆW K1K 0 + V1K 1 + λn ˆW 1 + λnW0 Since ˆW = W0 + we have the following λp(W0 + )K0K 0 + (W0 + )K1K = λpW0K0K 1 + λn(W0 + ) 1 + λnW0 0 + V1K λpW0K0K 0 + λpK0K +λn(W0 + ) = λpW0K0K 0 + W0K1K 0 + V1K 1 + K1K 1 1 + λnW0 (λpK0K 0 + K1K = (V1 W0K1)K 1 + λnI) = (V1 W0K1)K 1 0 + K1K 1 (λpK0K 1 + λnI)1 E. Norm-Constraint with Other Objectives While theoretically norm-constraint can be added to both EMMET and AlphaEdit objectives, it doesnt work well with these objectives. AlphaEdit already has norm constraint in its objective which is still not able to alleviate norm growth as shown in the main paper. Moreover, when norm-constraint is used with the EMMET objective, we see no appreciable improvements in downstream performance but incur penalty in editing scores. We believe that this is the nature of how norm-constraint works. When the norm-constraint is added, it penalizes change in weights from the original weight values. One way to prevent loss of downstream performance is to set the hyperparameter for norm-constraint very high, in which case we will never lose downstream performance. But in this scenario, we will also not be able to edit the model. In the other case, if we dont restrict norm growth, the editor 16 Lifelong Sequential Knowledge Editing without Model Degradation Table 5. Editing performance for EMMET with Llama2-7B with norm constraint. METHOD MODEL EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE OVERALL SCORE GENERATION ENTROPY LLAMA2-7B EMMET BASELINE LLAMA2-7B EMMET BEST-EDITING EMMET BEST-DOWNSTREAM LLAMA2-7B 93.85 94.24 90.35 87.32 87.2 85.97 58.07 54.36 53.65 76.281 74.12 72.57 579.79 566.08 541.16 (a) EMMET baseline (b) Downstream performance for best editing score with norm constraint. (c) Downstream peformance for best the scenario where we get best downstream performance with norm constraint. Figure 14. Comparison between the effect of adding norm-constrain the EMMET objective. We see no appreciable improvement in downstream performace but incur loss in editing performance. 17 Lifelong Sequential Knowledge Editing without Model Degradation F. Editing Hyperparameters and Hardware"
        },
        {
            "title": "Details",
            "content": "Here we present the hyperparameters that we find and used for the MPES, Norm constraint, and ENCORE. Tables 6, 7 and 8 show the hyperparameters for CounterFact dataset and tables 9, 10 and 11 show the hyperparameters for the zsRE dataset. Additionally, there is one more hyperparameter that requires tuning, as stopping immediately when the target token becomes the most probable may not always be optimal. We define this hyperparameter as the probability cutoff, which determines how many additional steps we take before stopping. Specifically, cutoff of +n means that we continue for more steps after the target token first becomes the most probable. All experiments in this paper are done on NVIDIA A6000, including experiments where editing speeds of different methods are timed. 18 Lifelong Sequential Knowledge Editing without Model Degradation Table 6. Hyperparameters for different algorithms with MPES on CouterFact dataset METHOD MODEL EMMET + MPES ALPHAEDIT + MPES MEMIT + MPES GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B λp 10,000 15,000 15,000 20,000 15,000 15,000 20,000 15,000 15,000 PROBABILITY CUT OFF +1 +0 +0 +1 +0 +0 +2 +1 +2 Table 7. Hyperparameters for Norm Constraint on CouterFact dataset METHOD MODEL λp NORM CONSTRAINT GPT2-XL LLAMA2-7B LLAMA3-8B 20,000 15,000 15,000 λn 10 10 Table 8. Hyperparameters for ENCORE on CouterFact dataset METHOD MODEL λp ENCORE GPT2-XL LLAMA2-7B LLAMA3-8B 20,000 15,000 15,000 λn 10 10 20 PROBABILITY CUT OFF +3 +2 +1 Table 9. Hyperparameters for different algorithms with MPES on zsRE dataset METHOD MODEL EMMET + MPES ALPHAEDIT + MPES MEMIT + MPES GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B GPT2-XL LLAMA2-7B LLAMA3-8B λp 10,000 15,000 15,000 20,000 15,000 15,000 20,000 15,000 15, PROBABILITY CUT OFF +1 +1 +2 +1 +1 +3 +5 +1 +4 Table 10. Hyperparameters for Norm Constraint on zsRE dataset METHOD MODEL λp NORM CONSTRAINT GPT2-XL LLAMA2-7B LLAMA3-8B 20,000 15,000 15,000 λn 40 10 20 19 Lifelong Sequential Knowledge Editing without Model Degradation Table 11. Hyperparameters for ENCORE on zsRE dataset METHOD MODEL λp ENCORE GPT2-XL LLAMA2-7B LLAMA3-8B 20,000 15,000 15,000 λn 40 10 PROBABILITY CUT OFF +4 +2 +3 20 Lifelong Sequential Knowledge Editing without Model Degradation G. Norm growth Result In this section we present the observation of norm growth during editing for different methods (including Norm Constraint and ENCORE) with different models. Figures 15 and 16 show the result for different methods for GPT2-XL. Figures 17 and 18 show the result for Llama2-7B. Lastly figures 19 and 20 show the result for Llama3-8B. 21 Lifelong Sequential Knowledge Editing without Model Degradation (a) Norm growth of GPT2-XL using Alphaedit (b) Norm growth of GPT2-XL using EMMET (c) Norm growth of GPT2-XL using MEMIT Figure 15. Norm growth of GPT2-XL across different methods (a) Norm growth of GPT2-XL using MEMIT (b) Norm growth of GPT2-XL using Norm Constraint (c) Norm growth of GPT2-XL using ENCORE Figure 16. Norm growth of GPT2-XL using Norm Constraint and ENCORE (a) Norm growth of Llama2-7B using Alphaedit (b) Norm growth of Llama2-7B using EMMET (c) Norm growth of Llama2-7B using MEMIT Figure 17. Norm growth of Llama2-7B across different methods 22 Lifelong Sequential Knowledge Editing without Model Degradation (a) Norm growth of Llama2-7B using MEMIT (b) Norm growth of Llama2-7B using Norm Constraint (c) Norm growth of Llama2-7B using ENCORE Figure 18. Norm growth of Llama2-7B using Norm Constraint and ENCORE (a) Norm growth of Llama3-8B using Alphaedit (b) Norm growth of Llama3-8B using EMMET (c) Norm growth of Llama3-8B using MEMIT Figure 19. Norm growth of Llama3-8B across different methods (a) Norm growth of Llama3-8B using MEMIT (b) Norm growth of Llama3-8B using Norm Constraint (c) Norm growth of Llama3-8B using ENCORE Figure 20. Norm growth of Llama3-8B using Norm Constraint and ENCORE 23 Lifelong Sequential Knowledge Editing without Model Degradation H. Downstream Performance In this section, we show the downstream performance for each method and also for different algorithms. Note that the reason why -axis shows the number of edits to be from 0 to 100 is because it shows number of gradient updates (or batch index) instead of number of facts edited. Since editing is done in batches of 100, to achieve 10,000 edits we perform total of 100 batched updates to the models sequentially. Thus, to get the total number of edits finished until certain point, the x-axis needs to be multiplied by 100, which is the batch size. Figures 21 - 26 show the result for the downstream performance for GPT2-XL on both CounterFact and zsRE datasets. Figures 27 - 32 show the result for Llama2-7B on both datasets. Lastly, figures 33 - 38 show the result for Llama3-8B on both datasets. 24 Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for GPT2-XL using AlphaEdit (b) Downstream Performance for GPT2-XL using AlphaEdit with MPES Figure 21. Downstream Performance for GPT2-XL using AlphaEdit and MPES with CounterFact dataset (a) Downstream Performance for GPT2-XL using EMMET (b) Downstream Performance for GPT2-XL using EMMET with MPES Figure 22. Downstream Performance for GPT2-XL using EMMET and MPES with CounterFact dataset (a) Downstream Performance for GPT2-XL using MEMIT (b) Downstream Performance for GPT2-XL using MEMIT with MPES (c) Downstream Performance for GPT2-XL using MEMIT with Norm Constraint (d) Downstream Performance for GPT2-XL using MEMIT with ENCORE Figure 23. Downstream Performance for GPT2-XL using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset 25 Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for GPT2-XL using AlphaEdit (b) Downstream Performance for GPT2-XL using AlphaEdit with MPES Figure 24. Downstream Performance for GPT2-XL using AlphaEdit and MPES with zsRE dataset (a) Downstream Performance for GPT2-XL using EMMET (b) Downstream Performance for GPT2-XL using EMMET with MPES Figure 25. Downstream Performance for GPT2-XL using EMMET and MPES with zsRE dataset (a) Downstream Performance for GPT2-XL using MEMIT (b) Downstream Performance for GPT2-XL using MEMIT with MPES (c) Downstream Performance for GPT2-XL using MEMIT with Norm Constraint (d) Downstream Performance for GPT2-XL using MEMIT with ENCORE Figure 26. Downstream Performance for GPT2-XL using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for Llama27B using AlphaEdit (b) Downstream Performance for Llama27B using AlphaEdit with MPES Figure 27. Downstream Performance for Llama2-7B using AlphaEdit and MPES with CounterFact dataset (a) Downstream Performance for Llama27B using EMMET (b) Downstream Performance for Llama27B using EMMET with MPES Figure 28. Downstream Performance for Llama2-7B using EMMET and MPES with CounterFact dataset (a) Downstream Performance for Llama2-7B using MEMIT (b) Downstream Performance for Llama2-7B using MEMIT with MPES (c) Downstream Performance for Llama2-7B using MEMIT with Norm Constraint (d) Downstream Performance for Llama2-7B using MEMIT with ENCORE Figure 29. Downstream Performance for Llama2-7B using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset 27 Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for Llama27B using AlphaEdit (b) Downstream Performance for Llama27B using AlphaEdit with MPES Figure 30. Downstream Performance for Llama2-7B using AlphaEdit and MPES with zsRE dataset (a) Downstream Performance for Llama27B using EMMET (b) Downstream Performance for Llama27B using EMMET with MPES Figure 31. Downstream Performance for Llama2-7B using EMMET and MPES with zsRE dataset (a) Downstream Performance for Llama2-7B using MEMIT (b) Downstream Performance for Llama2-7B using MEMIT with MPES (c) Downstream Performance for Llama2-7B using MEMIT with Norm Constraint (d) Downstream Performance for Llama2-7B using MEMIT with ENCORE Figure 32. Downstream Performance for Llama2-7B using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset 28 Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for Llama38B using AlphaEdit (b) Downstream Performance for Llama38B using AlphaEdit with MPES Figure 33. Downstream Performance for Llama3-8B using AlphaEdit and MPES with CounterFact dataset (a) Downstream Performance for Llama38B using EMMET (b) Downstream Performance for Llama38B using EMMET with MPES Figure 34. Downstream Performance for Llama3-8B using EMMET and MPES with CounterFact dataset (a) Downstream Performance for Llama3-8B using MEMIT (b) Downstream Performance for Llama3-8B using MEMIT with MPES (c) Downstream Performance for Llama3-8B using MEMIT with Norm Constraint (d) Downstream Performance for Llama3-8B using MEMIT with ENCORE Figure 35. Downstream Performance for Llama3-8B using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset 29 Lifelong Sequential Knowledge Editing without Model Degradation (a) Downstream Performance for Llama38B using AlphaEdit (b) Downstream Performance for Llama38B using AlphaEdit with MPES Figure 36. Downstream Performance for Llama3-8B using AlphaEdit and MPES with zsRE dataset (a) Downstream Performance for Llama38B using EMMET (b) Downstream Performance for Llama38B using EMMET with MPES Figure 37. Downstream Performance for Llama3-8B using EMMET and MPES with zsRE dataset (a) Downstream Performance for Llama3-8B using MEMIT (b) Downstream Performance for Llama3-8B using MEMIT with MPES (c) Downstream Performance for Llama3-8B using MEMIT with Norm Constraint (d) Downstream Performance for Llama3-8B using MEMIT with ENCORE Figure 38. Downstream Performance for Llama3-8B using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset 30 Lifelong Sequential Knowledge Editing without Model Degradation I. Editing Performance on zsRE dataset Tables 12 - 14 show the editing scores for the sequential editing experiments on zsRE. 31 Lifelong Sequential Knowledge Editing without Model Degradation Table 12. Editing performance for GPT2-XL on zsre dataset MODEL METHOD EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE GPT2-XL EMMET EMMET+MPES ALPHAEDIT ALPHAEDIT + MPES MEMIT MEMIT + MPES NORM-CONSTRAINT ENCORE 39.71 53.55 42.1 54.99 74.6 75.09 74.51 74. 27.17 39.42 33.61 43.18 61.77 61.58 61.9 61.79 9.78 16.81 14.61 18.4 22.4 23.37 23.39 23.41 Table 13. Editing performance for Llama2-7B on zsre dataset MODEL METHOD EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE LLAMA2-7B EMMET EMMET+MPES ALPHAEDIT ALPHAEDIT + MPES MEMIT MEMIT + MPES NORM-CONSTRAINT ENCORE 75.42 84.07 83.77 83.8 79.49 83.01 88.73 89.1 69.69 76.9 77.12 77.64 74.29 77.45 84.05 84. 33.89 41.88 41.96 41.97 41.8 44.64 47.98 48.51 Table 14. Editing performance for Llama3-8B on zsre dataset MODEL METHOD EDIT SCORE PARAPHRASE SCORE NEIGHBORHOOD SCORE LLAMA3-8B EMMET EMMET+MPES ALPHAEDIT ALPHAEDIT + MPES MEMIT MEMIT + MPES NORM-CONSTRAINT ENCORE 96.97 96.75 89.27 93.54 96.45 96.85 90.4 93.15 90.96 91.31 82.19 85.93 90.3 90.76 84.58 86.19 45.26 46.44 45.23 47.32 48.91 47.34 49.09 49. 32 Lifelong Sequential Knowledge Editing without Model Degradation J. Evaluation of Downstream Performance In this paper, we assess model degradation by measuring downstream performance at regular intervals of edits. Our evaluation suite is wide-ranging and consists of the following 6 tasks sentiment analysis (SST2) (Socher et al., 2013), paraphrase detection (MRPC) (Dolan & Brockett, 2005), natural language inference (NLI, RTE) (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), linguistic acceptability classification (CoLA) (Warstadt et al., 2019), and massive multitask language understanding (MMLU) (Hendrycks et al., 2020). For each task, we created subset of 100 examples balanced across all multiple-choice options. The models were evaluated on the tasks above, and the accuracy score was measured every 1000 edits. In order to improve the models initial performance and achieve meaningful signals, we provided few-shot examples. The few-shot prompt templates used for each task are shown in Figures 39-44. Lifelong Sequential Knowledge Editing without Model Degradation Review : an exhilarating futuristic thriller-noir , minority report twists the best of technology around gripping story , delivering riveting , pulse intensifying escapist adventure of the first order Sentiment : positive try as may , ca nt think of single good reason to see this movie , even Review : though everyone in my group extemporaneously shouted , thank you ! Sentiment : negative Review : the film performances are thrilling . Sentiment : positive Review : vera technical prowess ends up selling his film short ; he smoothes over hard truths even as he uncovers them . Sentiment : negative Review : [input] Sentiment : Figure 39. Few shot prompt template used for SST-2 34 Lifelong Sequential Knowledge Editing without Model Degradation Question: Which expression is equivalent to 4 9? (A) (4x 4) + (4x5) (B) (4+4) (4+5) (C) (4+4)+(4+5) (D) (4x 4) (4x5) Answer: Question: marketing researcher is conducting survey in large selling area by contacting small group of people that is representative of all people in that area. The small, representative group is known as the (A) population (B) sample (C) stratification (D) universe Answer: Question: research participant eats half bowl of M&M candies, and then stops eating. How would motivation researcher using drive reduction theory explain this participants behavior? (A) Humans are instinctively driven to eat sugar and fat when presented to them. (B) The Yerkes-Dodson law explains that people will eat food when presented to them, but usually in moderate amounts in order to avoid being perceived as selfish. (C) The primary drive of hunger motivated the person to eat, and then stop when she/he regained homeostasis. (D) The research participant was satisfying the second step on the hierarchy of needs: Food needs. Answer: Question: In deductively valid argument (A) If all the premises are true, the conclusion must be true (B) The conclusion has already been stated in its premises (C) If all the premises are true, the conclusion may still be false (D) Both and Answer: Question: [input] Answer: Figure 40. Few shot prompt template used for MMLU 35 Lifelong Sequential Knowledge Editing without Model Degradation Are the sentences paraphrases of each other. Sentence 1: Federal regulators have turned from sour to sweet on proposed $2.8 billion merger of ice cream giants Nestle Holdings Inc. and Dreyer Grand Ice Cream Inc . Sentence 2: Federal regulators have changed their minds on proposed $2.8 billion merger of ice cream giants Nestle Holdings and Dreyer Grand Ice Cream . Answer: Yes Are the sentences paraphrases of each other. Sentence 1: In the year-ago quarter , the steelmaker recorded profit of $16.2 million , or 15 cents per share , on sales of $1.14 billion . Sentence 2: In the second quarter last year , AK Steel reported profit of $16.2 million , or 15 cents share . Answer: No Are the sentences paraphrases of each other. Sentence 1: He added : ve never heard of more reprehensiblebehaviour by doctor . Sentence 2: The Harrisons lawyer Paul LiCalsi said : ve never heard of more reprehensible behaviour by doctor . Answer: Yes Are the sentences paraphrases of each other. Sentence 1: While dioxin levels in the environment were up last year , they have dropped by 75 percent since the 1970s , said Caswell . Sentence 2: The Institute said dioxin levels in the environment have fallen by as much as 76 percent since the 1970s . Answer: No Are the sentences paraphrases of each other. Sentence 1: [input 1] Sentence 2: [input 2] Answer: Figure 41. Few shot prompt template used for MRPC Is this sentence linguistically acceptable? Sentence: The evidence assembled by the prosecution convinced the jury. Answer: Yes Is this sentence linguistically acceptable? Sentence: live at the place where Route 150 crosses the Hudson River and my dad lives at it too. Answer: No Is this sentence linguistically acceptable? Sentence: The governments imposition of fine. Answer: Yes Is this sentence linguistically acceptable? Sentence: Sam gave the ball out of the basket. Answer: No Is this sentence linguistically acceptable? Sentence: [input] Answer: Figure 42. Few shot prompt template used for RTE 36 Lifelong Sequential Knowledge Editing without Model Degradation The town is also home to the Dalai Lama and to more than 10,000 Tibetans living in exile. Question: The Dalai Lama has been living in exile since 10,000. True or False? Answer: True P. Prayong, who like Kevala belongs to the Theravada sect of Buddhism, chose India over other Buddhist majority nations as it is the birthplace of Gautama Buddha. Question: P. Prayong is member of Theravada. True or False? Answer: False is The medical student accused of murdering an erotic masseuse he met on Craigslist drowning in more than $100,000 in student loan debt and is so broke he cant afford to pay an attorney, according to court papers. Philip Markoff, 23-year-old suspended Boston University medical school student, owes $130,000 in student loans and does not get money from his parents, leaving him to lean on taxpayer-funded attorney for his defense, according to court document in Boston Municipal Court that labels him indigent. Markoff graduated from the State University of New York-Albany and was second-year medical student at BU. Question: The medical student Philip Markoff was engaged. True or False? Answer: True Traditionally, the Brahui of the Raisani tribe are in charge of the law and order situation through the Pass area. This tribe is still living in present day Balochistan in Pakistan. Question: The Raisani tribe resides in Pakistan. True or False? Answer: False The latest attacks targeted the U-S embassy and top prosecutors office in the Uzbek capital. Question: [input]. True or False? Answer: Figure 43. Few shot prompt template used for CoLA 37 Lifelong Sequential Knowledge Editing without Model Degradation Turkey is unlikely to become involved in, or allow U.S. forces to use Turkish territory in Middle East war that does not threaten her territory directly. entails the U.S. to use Turkish military bases. True or False? Answer: False Brooklyn Borough Hall featured Whos Who in New Yorks literary community during the second annual Brooklyn Book Festival. According to Brooklyn Borough President Marty Markowitz, the boroughs zip code 11215 boasts more authors than anywhere else in the country. It appeared to be the case on Sunday. More than 100 authors were featured at the day-long event, including The Basketball Diaries writer Jim Carroll, former M*A*S*H star Mike Farrell, author and illustrator Mo Willems, Jack Kerouacs sometime lover and National Book Critics Circle Award recipient Joyce Johnson and PEN American Center President Francine Prose. entails the The Brooklyn Book Festival is held in Brooklyn Borough every year. True or False? Answer: True NASAs Saturn exploration spacecraft, Cassini , has discovered an atmosphere about the moon Enceladus . This is the first such discovery by Cassini, other than Titan , of the presence of an atmosphere around Saturn moon. entails the Titan is the fifteenth of Saturns known satellites. True or False? Answer: False Dr. Eric Goosby, pioneer in the fight against AIDS, is President Obamas choice to run the American effort to combat the disease globally, the White House announced Monday. The Presidents Emergency Plan For AIDS Relief, known as Pepfar, was championed by President George W. Bush. It is expected to spend $48 billion over the next five years and is credited with markedly reducing the diseases death rate. Its prevention policy has been controversial because of its emphasis on socially conservative methods. With new administration and Democratic majority in the House, organizations seeking prevention choices beyond abstinence and fidelity including renewed commitment to distributing condoms are eager to try to rewrite the guidelines. entails the Pepfar is committed to fighting AIDS. True or False? Answer: True [input] True or False? Answer: Figure 44. Few shot prompt template used for NLI"
        }
    ],
    "affiliations": [
        "SCB DataX",
        "University of California, Berkeley",
        "University of Virginia"
    ]
}