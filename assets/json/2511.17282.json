{
    "paper_title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
    "authors": [
        "Chuancheng Shi",
        "Shangze Li",
        "Shiming Guo",
        "Simiao Xie",
        "Wenhua Wu",
        "Jingtong Dou",
        "Chao Wu",
        "Canran Xiao",
        "Cong Wang",
        "Zifeng Cheng",
        "Fei Shen",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 8 2 7 1 . 1 1 5 2 : r Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation Chuancheng Shi1* Shangze Li2* Shiming Guo1* Simiao Xie1 Wenhua Wu1 Jingtong Dou1 Chao Wu2 Canran Xiao3 Cong Wang4 Zifeng Cheng4 Fei Shen5 Tat-Seng Chua5 1The University of Sydney 2Nanjing University of Science and Technology 3Central South University 4Nanjing University 5National University of Singapore"
        },
        {
            "title": "Abstract",
            "content": "text-to-image (T2I) models have advanced Multilingual rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culturerelated representations. We propose probing method that localizes culture-sensitive signals to small set of neurons in few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inferencetime cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity. 1. Introduction Ensuring cultural fairness and representation in generative AI [5, 10, 13, 17, 20, 23, 36, 40, 42] is essential for global accessibility and cultural diversity, aligning with the United Nations principles of inclusiveness and universality. Yet, when prompted in different languages, many state-of-theart methods [1, 7, 22, 34, 48] frequently produce culturally neutral or English-biased images, which weakens crosslingual cultural correspondence. Here, we use cultural consistency to denote the extent to which generated images ex- *Equal first authors. Equal second authors. Corresponding author. Figure 1. Cultural alignment in local languages. (a) LLMs/recommenders keep cultural consistency, but T2I models falter with noun-only prompts. (b) Adding culture-style modifier + noun restores consistency. hibit visual elements that are statistically associated with the target languages cultural context, beyond mere semantic correctness. To avoid conflating culturally typical elements with stereotypes, we require that cultural cues be both contextually appropriate and roughly consistent with real-world cultural statistics. Despite strong gains in semantic and visual fidelity, diffusion-based T2I models [35, 37] lag in cultural generalization. Prior work largely targets cross-lingual encoder alignment [27, 48] while overlooking cross-cultural grounding, defined as generating visuals that reflect each languages socio-cultural context. As shown in Figure 1(a), models often capture only literal meanings, for example, traditional building in Portuguese or Turkish, while missing culture-specific cues. In contrast, language-based systems such as LLMs [1, 30, 41] and recommender engines [16, 32] produce localized responses, revealing cross-modal gap in cultural grounding. We argue that the issue is insufficient activation rather Large-scale training corpora than missing knowledge. Figure 2. Overview of the CultureBench pipeline. First, manually collect and rigorously quality-control datasets from 15 linguistic regions; annotate culture-style modifier noun captions using GPT5-Nano [30] and, through human annotation, noun-only captions; convert annotated content into local languages via translation tools, supplemented by manual review. already contain diverse cultural attributes, and explicit prompts can elicit them. As shown in Figure 1(b), adding culture-style modifiers, for example, people wearing Chinese clothes or an Italian architecture, yields images with clear country-specific characteristics. The inconsistency happens because noun-only prompts dont strongly trigger cultural knowledge, so the model gives literal but not cultural interpretations. To validate this hypothesis and mitigate cultural inconsistency in T2I models, we first examine two representative systems [27, 48] and show that the failure arises from insufficient activation of culture-related knowledge rather than its absence, and that the effect is observed across two architecturally different diffusion methods. We then introduce two-stage probing method. We begin by comparing attention distributions between culture-style modifiers and nouns to localize culture-sensitive layers. Next, we use the Top-K SAE [4] to quantify activation differences between prompts with explicit cultural cues and noun-only prompts, revealing that culture-relevant representations cluster in few fixed layers and small set of neurons. Guided by these findings, we propose two complementary strategies: (1) zero-training activation scheme that amplifies the responses of the identified neurons at inference time, and (2) layerspecific fine-tuned scheme that updates only culturally relevant layers to improve consistency. Finally, experiments on our CultureBench show consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity. We highlight the following contributions: We empirically show that multilingual T2I models often produce culturally neutral or subtly English-biased outputs under multilingual prompts, thereby hindering crosslingual cultural consistency. We develop probing framework that localizes culturesensitive signals to few fixed layers and neurons by contrasting attention patterns and Top-K SAE activations, indicating that failures stem from insufficient activation rather than missing knowledge. We propose two alignment strategies: zero-training inference-time activation scheme and layer-specific fine-tuning scheme that updates only culturally relevant layers, improving cultural consistency while preserving fidelity and diversity. We curate and release CultureBench, 15-country benchmark with multilingual prompts and images, enabling evaluation of cross-cultural consistency and culturally adaptive training. 2. Related Work Cultural Text-to-Image Generation. Prior work has examined cultural bias, fairness, and stereotyping in T2I systems [9, 18, 26]. For example, SCoFT [25] expands cultural coverage through CCUB to mitigate stereotyping, and ViSAGe [19] quantifies visual stereotypes. However, existing studies remain largely focused on fairness and debiasing, with strong English-centric orientation and limited coverage of low-resource languages and diverse cultural contexts [6, 39]. Clear definitions and systematic evaluation of cross-lingual cultural consistency are still lacking. Noun-only or short prompts often collapse to culturally neutral or implicitly English-biased generations in Figure 3. Data distribution of the proposed CultureBench dataset across 15 languages. The dataset is divided into train, test, and neuron-detection subsets with ratio of 7:2:1. multilingual settings, yet unified diagnostic framework is absent [24]. At the representational level, prior work provides little insight into where culture-sensitive features reside within the model or how to control them at the layer or neuron level [21, 31]. Methodologically, there is still lack of lightweight, plug-and-play interventions and corresponding benchmarks that do not require large-scale retraining. Neuron Interpretability. Neuron-level interpretability links model activations to learned concepts in vision and language [2, 15, 29, 49]. Recent work further distinguishes shared and language-specific semantics at the neuron or direction level, clarifying how abstract meanings distribute across layers [14, 41, 45]. In multimodal settings, causal probing shows that individual neurons can be driven by visual concepts via textual proxies and directly manipulated to control semantics [11, 33, 38]. For concreteness, FEMN [33] locates sparse neuron groups mediating specific concepts (e.g., smile, striped) in CLIP-style model and demonstrates that directly patching or amplifying those units causally increases or suppresses the target concept in zero-shot classification and retrieval, establishing neuronlevel semantic control. However, prior studies still focus mainly on objector attribute-level notions and provide limited diagnostics for cultural representations, especially those that vary under multilingual prompts [8, 28, 43]. 3. CultureBench Dataset Data Collection. As shown in Figure 2, we collect culturally representative images across 15 language and region groups via geo-constrained web search using native and translated keywords. Each image is manually verified for clarity, authenticity, and representativeness to ensure Figure 4. Verify the hypothesis. Within the CultureBench test subset, performances under culture-style modifier + noun and noun-only prompt conditions are compared. Quantitative evaluation is conducted using CultureVQA. faithful depiction of its target culture. Data Annotations. CultureBench provides multidimensional annotations for assessing cultural awareness, including cultural categories, geographic regions, languages, and image content. For each sample, we provide two textual descriptions: culture-style modifier + noun caption generated by GPT5-Nano [30] and human-written noun-only caption. To reduce subjectivity in defining and labeling cultural attributes, we invited domain experts to review the taxonomy and subset of samples. Annotators kept only culturally appropriate, statistically grounded cues and treated mismatched ones as stereotypes. Data Distribution. As shown in Figure 3, the dataset contains 7,932 samples split into train, test, and neurondetection subsets at 7:2:1 ratio: the train subset is used for model and adapter training, the test subset for quantitative evaluation, and the neuron-detection subset for layerand neuron-level probing. Training is strictly limited to the train subset. Neither the test subset nor the neuron-detection subset is accessed during training, hyperparameter tuning, or model selection. Cultural Evaluation. We propose CultureVQA, singlechoice VQA task built on CultureBench. For each image, Qwen3-VL [47] and Gemini-2.5-Flash [3] must choose one label from 15 language/region categories or an unrecognisable option. We report accuracy as the proportion of samples whose predicted label exactly matches the ground truth. In pilot study, we found that the cultural labels predicted by these models are highly consistent with human annotations, supporting their use as automatic evaluators. This setup tests models ability to perform cultural attribution from visual cues alone, without textual prompts. 4. Cultural Probing 4.1. Cultural Gap To test the hypothesis that the issue lies in insufficient activation rather than knowledge absence, we conducted uniFigure 5. Methods for Neuronal Detection. (a) By comparing attention allocation between cultural-style modifiers and nouns across text-encoder layers, the layer with the largest divergence is designated as the culturally sensitive layer. (b) At this layer, features from the culture-style modifier + noun and noun-only prompts are fed into an SAE [4] to obtain sparse features, revealing neurons with heightened sensitivity to cultural cues. fied controlled experiment. Specifically, we used prompts composed of culture-style modifier + noun as inputs to AltDiffusion [48] and PEA-Diffusion [27], generated corresponding images, and evaluated them using the CultureVQA accuracy metric. As control, we repeated the same process with noun-only prompts. As shown in Figure 4, prompts combining culture-style modifier + nouns achieved the best CultureVQA performance, 44.39 for AltDiffusion and 35.62 for PEA-Diffusion, both substantially higher than the noun-only prompts. This consistent trend across two structurally different diffusion models suggests that the phenomenon is not model-specific and provides empirical support for our hypothesis. 4.2. Culture Layer Detection As illustrated in Figure 5(a), we compare the attention directed towards the target noun under paired culturestyle modifier + noun and noun-only prompts, averaging across heads to obtain hierarchical cultural relevance scores. Culture-style modifier + noun prompts consistently yield significantly higher scores than noun-only prompts, indicating that they are encoded with cultural semantics. We define paired prompts for each target concept: culture-style modifier + noun prompt Pcult and nounonly prompt Pnoun. The former augments the noun with culture-style modifier, while the latter removes it. We create such pairs covering diverse cultural elements to ensure generality. For each prompt, we annotate two token categories, the culture modifiers Tcult and the target nouns Tnoun. And exFigure 6. PEA-Diffusion cultural sensitivity. CA peaks layer 16. AltDiffusion results are provided in the appendix. tract layerwise attention. Let A(l) RBHSS be the multi-head attention at layer l, where is the batch size, is the number of attention heads, and is the sequence length. We average heads for robustness: A(l) = 1 (cid:88) h=1 Ah(l), (1) and retain only the entries from Tcult to Tnoun. Subsequently, we derive the subset of keyword pairs from A(l) denoted as Akey(l) RBTcultTnoun. If layer encodes culture semantics, the attention from culture modifiers to target nouns under cultural prompt should exceed that under its noun-only prompt. To quantify this, let tcult Tcult and tnoun Tnoun denote cultural modifier and target noun. For prompt , the attention from cultural modifiers to target nouns in layer is: Table 1. Validating Culture-Sensitive Neuron Detection in PEA-Diffusion. Neuronal accuracy on the test subset with cultural style modifier + noun prompts. Method PEA-Diffusion [27] + Masked Top-K Neurons + Masked Random Neurons CultureVQA 35.62 7.65 (-27.97) 33.04 (-2.58) Here denotes the ith sample in Ncult, I() is the indicator function.Using the same method, we obtained fnoun(m). For each neuron, we define the mean activation magnitude µcult(m) by calculating the magnitude of its activation in the cultural attention feature and adding it to the magnitude of its activation in the noun attention feature token: µcult(m) = (cid:80)Ncult i=1 (Zcult[i, m] (Zcult[i, m] > ϵ)) (cid:80)Ncult i=1 (Zcult[i, m] > ϵ) + β . (5) Here β denotes small positive constant added for numerical stability, ensuring that the denominator never becomes zero and preventing degenerate cases when the activation frequency is extremely low. Using the same method, we obtained µnoun(m). µnoun(m) denotes the average activation magnitude of the mth neuron on the noun subset Fnoun. Ultimately, to better capture neurons with both high firing rates and strong responses, we combine the activation frequency of cultural modifiers with their average activation magnitude to obtain weighted frequency score Scult: Scult(m) = fcult(m) µcult(m). (6) Using the same method, we obtained Snoun(m). Snoun(m) is the weighted frequency score for noun subsets. After computing Scult and Snoun, we rank neurons by Scult and select the top-K candidates. Neurons with substantial noun-side salience are removed, and the remaining ones are designated as culturally sensitive neurons. According to the aforementioned algorithm, as illustrated in Figure 7, six distinctly different cultural backgrounds are presented; in each instance, one or more pronounced peaks emerge, each bearing different index value. This indicates that the locations of corresponding cultural neurons do not overlap across cultures. 4.4. Cultural Validation To verify whether our detector accurately localizes culturesensitive neurons, we designed three controlled settings  (Table 5)  : (1) Baseline (no masking), (2) Masked Top-K Neurons (masking the Top-K culture-sensitive neurons identified by our method), and (3) Random Mask (masking the same number of neurons at random). Masking the identified Top-K neurons reduces the mean CultureVQA score Figure 7. Neuronal detection result. The weighted frequency scores show only few salient peaks per culture, indicating culture-specific neurons. We define the Top-K set as the peak neurons, with adapting to the number of salient peaks. Akey(l) tculttnoun (cid:80) (cid:80) tcult CA(P, l) = . (2) tnoun Tcult Tnoun For all culture/noun pairs (Pcult, Pnoun), we compute CA(l) = 1 (cid:88) i=1 [CA(Pcult,i, l) CA(Pnoun,i, l)]. (3) Larger CA(l) indicates that layer more effectively separates culture-style modifier from noun semantics. We compute CA(l) across prompt pairs and seeds, and mark layer as culture-sensitive if its CA notably exceeds the mean of its two neighboring layers. Based on this workflow, Figure 6 shows the detection results from PEADiffusion, revealing clear global peaks at specific layer. The results indicate that culturally relevant semantics are not uniformly distributed throughout the network, but rather concentrated in key layer. 4.3. Culture Neuron Detection We localize culturally sensitive neurons in the key layer. As shown in Figure. 5(b), we apply Top-K SAE [4] to obtain decomposition of attention features and select culturespecific neurons via comparative analysis. From the key layer, we construct cultural features Fcult RNcultDatt and noun features Fnoun RNnounDatt , where Ncult and Nnoun are the numbers of token pairs for the cultural and noun prompts, respectively. The attention feature dimension is Datt = Tcult Tnoun. We use weighted frequency score combining activation frequency and amplitude to quantify neuronal responses to cultural semantics. Specifically, let {1, 2, . . . , Datt} indexes specific neuron in the attention feature space. The activation frequency fcult(m) is defined as follows: for all Ncult lexemes with cultural elements, compute the proportion of samples in which the activation Zcult[i, m] of the mth neuro exceeds ϵ: fcult(m) = 1 Ncult Ncult(cid:88) i=1 (Zcult[i, m] > ϵ) . (4) from 35.62 to 7.65, whereas random masking yields comparable score (33.04) to the baseline. The sharp, targeted degradation absent under random masking indicates that the localized neurons are highly related to cultural semantics, thereby providing empirical support for the accuracy and interpretability of our neuron localization method. 5. Methods 5.1. Zero-Training Neuron Amplifier Upon identifying neurons sensitive to cultural information, we amplify underutilised cultural representations by intervening on targeted neuronal subset Mcult within the key layer, thereby modulating attention-related features and enhancing cultural attributes in the generated images. We define the original attention correlation features that need to be interfered Fraw RBSpairDatt where is the batch size, Spair is the number of prompt words in single batch. The latent vector Zraw RBSpairMcult after inputting them into the SAE encoder is given by: Zraw = SAE.encode(Fraw), (7) where the SAE, through sparse coding, decomposes the complex internal representations into more independent and semantically coherent neurons. At the same time, Zraw[b, p, m] denotes the initial activation value of the m-th neuron for the p-th prompt word in the b-th batch. We enhance cultural representation by modulating culturally specific neurons with manually defined λ. The formula is: Zenh[b, p, m] = (cid:40) (1 + λ)Zraw[b, p, m] Zraw[b, p, m] if Mcult otherwise . (8) Here Zenh RBSpairM is the modulated latent vector , is the collection of all neurons. λ is the feature fusion coefficient. We then map Zenh back to the attention space: Frec enh = SAE.decode(Zenh), (9) where SAE.decode() represents the SAE decoder and Frec enh RBSpairDatt is the attention association feature reconstructed after modulation. This step preserves the original semantic structure while enhancing culturally specific attention patterns. 5.2. Fine-Tuned Layer Enhancer To enable adaptive cultural representation without the need for manual adjustment of modulation strength, we propose layer-specific, fine-tuned scheme that updates only culturally relevant layers to improve consistency. We first analyze the text encoder layer by layer to identify the layer most sensitive to cultural cues, denoted as lc. small trainable module is inserted only into this layer, while all other parameters remain frozen. Let denote the hidden representation of the text encoder at layer lc. The enhancer produces an enhanced representation via residual transformation: = + g(cid:0)W2 σ(W1h)(cid:1), (10) where is the culture-enhanced hidden state, σ() is nonlinear activation, g() denotes normalization layer used to stabilize the residual transformation, and W1, W2 are small trainable matrices. During training, only the enhancer parameters are updated while keeping the backbone fixed. Given noun-only prompt p, the text encoder with enhancer fθ,ϕ and generator produce an image: ˆx = G(cid:0)fθ,ϕ(p)(cid:1). (11) The generated image is compared with ground-truth cultural image x(p), which is directly taken from the CultureBench dataset as the human-curated cultural reference corresponding to the noun-only prompt p, using pixellevel mean squared error (MSE) loss: LMSE = 1 (cid:88) i=1 (cid:13) (cid:13)ˆxi (p)(cid:13) 2 2, (cid:13) (12) where is the number of pixels. Only the enhancer parameters are optimized: ϕ = arg min ϕ LMSE. (13) During inference, the trained layer enhancer modulates the hidden representation according to the input prompt, thereby enhancing cultural consistency in the generated images while maintaining the original semantic structure. 6. Experiments And Analysis 6.1. Implementation Details Metrics. Following PEA-Diffusion [27], we employ CLIPScore [12] and ImageReward [46] to evaluate text-image alignment and perceptual preference. Concurrently, we employ LPIPS [50] to measure perceptual similarity and reconstruction fidelity, while introducing CultureVQA as novel metric for cultural recognition. Hyperparameters. In Fine-Tuned Layer Enhancer method, we train for 2,000 steps under mixed precision using AdamW (learning rate 5 105) with batch size of 1. For the zero-training variant, we set λ = 6. All experiments are conducted on single NVIDIA A6000 GPU. 6.2. Compare with SOTA Methods We conduct quantitative comparison between our zerotraining and fine-tuned built on PEA-Diffusion, and several state-of-the-art (SOTA) methods, including AltDiffusion [48], PEA-Diffusion [27], StableDiffusion XL [34], Table 2. Quantitative comparisons with SOTA methods. Using the noun-only prompts on the test subset. The best performance is marked in bold, and the second-best is underlined. Method StableDiffusion XL [34] FLUX.1-dev [22] Show-o2 [44] PEA-Diffusion [27] AltDiffusion [48] StableDiffusion 3.5 [7] Ours (Zero-Training) Ours (Fine-Tuned) CultureVQA CLIPScore 9.36 14.83 16.43 21.65 23.05 25.13 33.91 (+12.32) 36.63 (+14.98) 0.211 0.224 0.234 0.253 0.282 0.242 0.291 (+0.038) 0.290 (+0.037) ImageReward -1.82 -0.88 -0.91 -0.65 -0.11 -1.01 0.33 (+0.98) 0.31 (+0.42) LPIPS 0.756 0.692 0.691 0.673 0.688 0.715 0.654 (-0.019) 0.661 (-0.012) Figure 8. Qualitative comparison of generation results. Our approach generates images that are more culturally appropriate. StableDiffusion 3.5 [7], FLUX.1-dev [22], and Showo2 [44]. This evaluation measures semantic alignment and visual fidelity, providing comprehensive benchmark for cross-cultural image generation. our method achieves simultaneous gains in cultural understanding, text-image consistency, and perceptual quality, verifying that interpretable cultural alignment can be realized without sacrificing visual fidelity. Quantitative Results. To evaluate whether our cultural alignment strategies enhance cultural understanding without compromising semantic or visual quality, we designed quantitative comparison experiment  (Table 2)  . The results show that our fine-tuned model achieves CultureVQA score of 36.63, significantly outperforming AltDiffusion (23.05) and PEA-Diffusion (21.65). The zero-training variant attains the highest CLIPScore (0.291) and ImageReward (0.33), while maintaining competitive LPIPS. Therefore, Qualitative Results. To test whether our method can recover cultural features under noun-only prompts without losing semantic consistency, we conducted qualitative comparisons with several mainstream models (Figure 8). The results show that our approach consistently generates images that reflect the target regions cultural characteristics while preserving semantic accuracy, whereas other models often regress to culture-neutral generic prototypes. For example, under prompts such as Box made of metal or Figure 9. User Study. Evaluated using MCC, SCC, and CSR metrics, where higher scores indicate greater perceived realism and user preference. Figure 11. Hyperparameter results. Performance variations of CultureVQA under different λ values. Table 3. Ablation studies on CultureBench. On the CultureBench test subset, we conducted ablation analyses of two methods under both zero-training and fine-tuned settings. The best performance is marked in bold. Figure 10. Hyperparameter results. The effect of cultural enrichment varies under different λ values. AltDiffusion [48] Model Silver milk jug, our method produces culturally grounded depictions, while PEA-Diffusion and AltDiffusion generate correct objects but lack regional style; StableDiffusion XL/3.5 and FLUX.1-dev tend toward neutral aesthetics, and Show-o2 exhibits both cultural inconsistency and semantic drift. Overall, our method balances cultural and semantic coherence while maintaining high image. User Study. Building on the quantitative and qualitative results, we conducted human-centered user study on the CultureBench platform with 50 experts in cultural studies. Participants evaluated cultural perception using three metrics: MCC (Multi-Choice Culture), SCC (SingleChoice Culture), and CSR (Cultural Semantic Relevance, 15 scale). Higher scores indicate stronger cultural alignment and preference. As shown in Figure 9, our method outperforms all baselines across all three metrics. Notably, the human-rated CSR score reaches 77.6, significantly higher than the second-best score of 60.4, highlighting clear advantage in cultural semantic fidelity. This human-evaluated CSR metric further strengthens our evaluation pipeline. Overall, these results confirm that our method can generate culturally aligned content accurately and consistently. 6.3. Hyperparameter Sensitivity Analysis To investigate the impact of hyperparameter λ on the cultural attributes of generated images in zero-shot training, we compared results under different λ values while keeping model parameters and the noun-only prompt constant (see Figure 10 and Figure 11). When λ = 0, the output perfectly matched the original image. As λ increases, the images progressively align with the prototypical features of the target culture, accompanied by corresponding rise in CultureVQA scores. peak is reached at λ = 7 (35.92), followed by slight decline at λ = 8 (32.61). This indicates that λ effectively modulates the intensity of cultural consisMethod w/o Ours w/ Random (Zero-Training) w/ Ours (Zero-Training) w/ Random (Fine-Tuned) w/ Ours (Fine-Tuned) w/o Ours w/ Random (Zero-Training) w/ Ours (Zero-Training) w/ Random (Fine-Tuned) w/ Ours (Fine-Tuned) CultureVQA 23.05 20.38 (-2.67) 30.06 (+7.01) 21.04 (-2.01) 32.66 (+9.61) 21.65 21.04 (-0.61) 33.91 (+12.26) 22.34 (+0.69) 36.63 (+14.98) PEA-Diffusion [27] tency, though excessively high values may induce overfitting and marginally degrade metric performance. Therefore, we select λ = 7. 6.4. Ablation Study To evaluate the effectiveness of our proposed components and ensure that the performance gains are not caused by random enhancement, we conducted ablation studies on the CultureBench  (Table 3)  . Both the zero-training and finetuned variants exhibit clear improvements: under the zerotraining setting, CultureVQA increases by 7.01 on AltDiffusion and 12.26 on PEA-Diffusion; after fine-tuning, the gains further rise to 9.61 and 14.98, respectively. Random activation or random fine-tuning yields only minimal or even negative improvements. These results confirm that the performance gains stem from the targeted activation of culturally sensitive neurons. Furthermore, both AltDiffusion and PEA-Diffusion exhibit consistent performance improvements, further validating the universality and crossarchitecture effectiveness of our neuron detection and enhancement framework. 7. Conclusion In this work, we reveal that multilingual T2I models often yield culturally neutral or English-biased images not due to missing knowledge, but due to weak activation of culture-sensitive representations. By localizing culture signals to small neuron set in few layers, we introduce two lightweight remedies: inference-time cultural activation and layer-targeted cultural enhancement. On CultureBench, both strategies consistently improve cross-lingual cultural consistency while preserving fidelity and diversity. This points toward practical, controllable cultural alignment for inclusive generative models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Lihu Chen, Adam Dejl, and Francesca Toni. Identifying query-relevant neurons in large language models for longform texts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2359523604, 2025. 3 [3] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3 [4] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. 2, 4, 5 [5] Junyuan Deng, Xinyi Wu, Yongxing Yang, Congchao Zhu, Song Wang, and Zhenyao Wu. Acquire and then adapt: Squeezing out text-to-image model for image restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2319523206, 2025. [6] Moreno DInc`a, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, and Nicu Sebe. Openbias: Open-set bias detection In Proceedings of the in text-to-image generative models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1222512235, 2024. 2 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 7, 14 [8] Karen Fort, Laura Alonso Alemany, Luciana Benotti, Julien Bezancon, Claudia Borg, Marthese Borg, Yongjian Chen, Fanny Ducel, Yoann Dupont, Guido Ivetta, et al. Your stereotypical mileage may vary: Practical challenges of evaluating biases in multiple languages and cultural contexts. In The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1776417769, 2024. 3 [9] Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Instructing text-toFair diffusion: Kristian Kersting. arXiv preprint image generation models on fairness. arXiv:2302.10893, 2023. 2 [10] Haoyang He, Jiangning Zhang, Hongxu Chen, Xuhai Chen, Zhishan Li, Xu Chen, Yabiao Wang, Chengjie Wang, and Lei Xie. diffusion-based framework for multi-class anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, pages 84728480, 2024. 1 [11] Qinqin He, Jiaqi Weng, Jialing Tao, and Hui Xue. single neuron works: Precise concept erasure in text-to-image diffusion models. arXiv preprint arXiv:2509.21008, 2025. 3 [12] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [13] Teng Hu, Jiangning Zhang, Ran Yi, Yuzhen Du, Xu Chen, Liang Liu, Yabiao Wang, and Chengjie Wang. Anomalydiffusion: Few-shot anomaly image generation with diffusion model. In Proceedings of the AAAI conference on artificial intelligence, pages 85268534, 2024. 1 [14] Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, and Xiaodong Shi. From neurons to semantics: Evaluating crosslinguistic alignment capabilities of large language models via neurons alignment. arXiv preprint arXiv:2507.14900, 2025. 3 [15] Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, and Xuming Hu. Mmneuron: Discovering neuron-level domain-specific interpretation in multimodal large language model. arXiv preprint arXiv:2406.11193, 2024. 3 [16] Andreea Iana, Goran Glavaˇs, and Heiko Paulheim. Mind your language: multilingual dataset for cross-lingual news recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 553563, 2024. 1 [17] Jinhyeok Jang, Chan-Hyun Youn, Minsu Jeon, and Changha Lee. Rethinking peculiar images by diffusion models: Revealing local minimas role. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 24542461, 2024. 1 [18] Suchae Jeong, Inseong Choi, Youngsik Yun, and Jihie Kim. Culture-trip: Culturally-aware text-to-image genarXiv preprint eration with iterative prompt refinement. arXiv:2502.16902, 2025. 2 [19] Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi Dave, Rida Qadri, Chandan Reddy, and Sunipa Dev. Visage: global-scale analysis of visual arXiv preprint stereotypes in text-to-image generation. arXiv:2401.06310, 2024. [20] Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Mengmeng Wang, and Jingdong Wang. Ssmg: Spatial-semantic map guided diffusion model for In Proceedings of free-form layout-to-image generation. the AAAI Conference on Artificial Intelligence, pages 2480 2488, 2024. 1 [21] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2269122702, 2023. 3 [22] Black Forest Labs. black-forest-labs/flux github page, 2024. 1, 7, 14 [23] Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, KwanYee Lin, and Wayne Wu. Cosmicman: text-to-image foundation model for humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69556965, 2024. 1 [24] Yaoyiran Li, Ching Yun Chang, Stephen Rawls, Ivan Vulic, Translation-enhanced multilingual and Anna Korhonen. In Proceedings of the 61st Antext-to-image generation. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 91749193, 2023. [25] Zhixuan Liu, Peter Schaldenbrand, Beverley-Claire Okogwu, Wenxuan Peng, Youngsik Yun, Andrew Hundt, Jihie Kim, and Jean Oh. Scoft: Self-contrastive fine-tuning In Proceedings of the for equitable image generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1082210832, 2024. 2 [26] Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Evaluating societal representations in diffusion models. Advances in Neural Information Processing Systems, 36:5633856351, 2023. 2 [27] Jian Ma, Chen Chen, Qingsong Xie, and Haonan Lu. Peadiffusion: Parameter-efficient adapter with knowledge distillation in non-english text-to-image generation. In European Conference on Computer Vision, pages 89105. Springer, 2024. 1, 2, 4, 5, 6, 7, 8, 12, 14 [28] Margaret Mitchell, Giuseppe Attanasio, Ioana Baldini, Miruna Clinciu, Jordan Clive, Pieter Delobelle, Manan Dey, Sil Hamilton, Timm Dill, Jad Doughman, et al. Shades: Towards multilingual assessment of stereotypes in large lanIn Proceedings of the 2025 Conference of guage models. the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1199512041, 2025. 3 [29] Jesse Mu and Jacob Andreas. Compositional explanations of neurons. Advances in Neural Information Processing Systems, 33:1715317163, 2020. 3 [30] OpenAI. Chatgpt (gpt-5 model). https : / / chat . openai.com/, 2025. Large language model developed by OpenAI, accessed on 2025-11-08. 1, 2, 3 [31] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 70537061, 2023. 3 [32] Makbule Gulcin Ozsoy. Multilingual prompts in llmbased recommenders: Performance across languages. arXiv preprint arXiv:2409.07604, 2024. 1 [33] Haowen Pan, Yixin Cao, Xiaozhi Wang, Xun Yang, and Meng Wang. Finding and editing multi-modal neurons in pre-trained transformers. arXiv preprint arXiv:2311.07470, 2023. 3 [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 6, 7, 14 [35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1 [36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 1 [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [38] Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in prethe trained text-only transformers. IEEE/CVF International Conference on Computer Vision, pages 28622867, 2023. 3 In Proceedings of [39] Preethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 63676384, 2024. [40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85438552, 2024. 1 [41] Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. Language-specific neurons: The key to multilingual capabilities in large language models. arXiv preprint arXiv:2402.16438, 2024. 1, 3 [42] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19211930, 2023. 1 [43] Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, AiTi Aw, and Nancy Chen. Seaeval for multilingual foundation models: From cross-lingual alignment to cultural In Proceedings of the 2024 Conference of the reasoning. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 370390, 2024. 3 [44] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv Improved native unified multimodal models. o2: preprint arXiv:2506.15564, 2025. 7, 14 [45] Wanying Xie, Yang Feng, Shuhao Gu, and Dong Yu. Importance-based neuron allocation for multilingual neural machine translation. arXiv preprint arXiv:2107.06569, 2021. 3 [46] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 6 [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3 [48] Fulong Ye, Guang Liu, Xinya Wu, and Ledell Wu. Altdiffusion: multilingual text-to-image diffusion model. In Proceedings of the AAAI conference on artificial intelligence, pages 66486656, 2024. 1, 2, 4, 6, 7, 8, 12, [49] Zeping Yu and Sophia Ananiadou. Interpreting arithmetic mechanism in large language models through comparative neuron analysis. arXiv preprint arXiv:2409.14144, 2024. 3 [50] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 12. Examples of the geographical and cultural composition of the CultureBench dataset. The appendices provide additional details that support and extend the main paper. Appendix describes the datasets geographic composition, evaluation protocol, and explains the selection of specific countries. Appendix further validates the conjecture presented in the main text. Appendix presents further experimental results and ablation studies. Appendix details the user studys design and conduct. Appendix provides additional visualization results for PEA-Diffusion [27] and AltDiffusion [48] under both methods. Appendix addresses common issues. Appendix covers the limitations of our work. Appendix reflects on the ethical considerations of this research. A. Details of CultureBench Dataset This study presents the datasets geographical composition within broad cultural classification framework. It encompasses major cultural spheres such as the Arab world, East Asia, continental Europe, Latin America, and parts of Africa. This approach reflects the macro-sociological context of the data sources. It does not represent an essentialist or homogenized understanding of cultures. Cultural spheres are delineated by principal nations and languages, including Arabic, Chinese, Japanese, Korean, Thai, German, Russian, Italian, Dutch, Polish, Turkish, Ukrainian, Spanish, Portuguese, and French in parts of Africa. Figure 12 illustrates an exemplar composition of this studys dataset. Details of Data Collection. During data collection, we used Google Image Search 1 and complementary web resources. These included Wikipedia 2 and other public search platforms. We obtained publicly available images, setting keywords based on various cultural contexts to cover diverse scenarios and object types. After automated ex1https://images.google.com 2https://www.wikipedia.org traction, images undergo preliminary screening. This eliminates low-quality, semantically irrelevant, or copyrightinfringing samples. cross-cultural expert team then manually reviews images, prioritizing the exclusion of those conveying stereotypes or biases. In addition, these experts define and validate expert-curated cultural subdivisions within each broad cultural group. These include regional styles, ethnic or indigenous traditions, and locally distinctive visual elements. The team uses these subdivisions to guide manual filtering and annotation. This ensures an accurate representation of the intended cultural context without incorporating visual elements from other cultural backgrounds. As result, CultureBench captures both macro-level cultural identity and finer-grained intracultural diversity. Through this process, we have enhanced the cultural accuracy and fairness of our cross-cultural visual data while ensuring its diversity. B. Further Evidence on the Culture Gap To test whether the cultural gap arises from under-activation rather than knowledge absence, we conduct unified controlled experiment (Figure. 13). Using culture-neutral English prompt and fixing all stochastic factors (e.g., seed and sampler), we generate images while selectively activating different culture-neuron sets identified in previous analyses. Despite identical prompts, activating China, Japan, Italy, Germany, or Ukraine neuron sets yields clearly distinct cultural styles, whereas the baseline remains culturally neutral. This controlled intervention demonstrates that the model already encodes rich culture-specific representations, and that the failure stems primarily from insufficient activation during generation rather than missing cultural knowledge. Figure 13. Further evidence on the culture gap. For fixed English prompt and identical sampling settings, activating different cultureneuron sets steers both PEA-Diffusion (top) and AltDiffusion (bottom) toward distinct cultural styles. Table 4. Comparison of accuracy between CultureVQA and human experts on the CultureBench dataset. Model / Group Human Experts (Avg.) CultureVQA (Ours) Accuracy 94.18% 91.57% C. More Experiments C.1. Reliability of CultureVQA To assess the consistency of CultureVQA with human subjective cognition, we invited 30 domain experts to participate in comparative experiments using the CultureBench dataset. Each question contained four real-world images, and only one matched the specified cultural element. CultureVQA selected one image per question, and the experts made their choices under the same conditions. We then calculated and compared the accuracy rates for both CultureVQA and the experts. As shown in Table 4, CultureVQA achieved an accuracy of 91.57%, while human experts reached an average accuracy of 94.18%, resulting in gap of only 2.61 percentage points. This relatively small difference indicates that CultureVQAs performance closely approaches that of human experts, demonstrating high consistency with human cultural recognition in this task. C.2. Cultural Probing Universal Type For clarity in the main text, only the detection results from PEA-Diffusion are presented. This section additionally demonstrates the culture-sensitive layers and neurons associated with AltDiffusion for comparison purposes. Figure 14. AltDiffusion cultural sensitivity. CA peaks layer 14. Therefore, layer 14 is culturally sensitive. Figure 15. AltDiffusion neuronal detection result. The weighted frequency scores show only few salient peaks per culture, indicating culture-specific neurons. Culture Layer Detection. To test the generality of our detection method beyond PEA-Diffusion, we also analyze AltDiffusion. Following the main papers probing procedure, we create paired prompts (culture-style modifier + noun and noun-only), annotate modifier and noun token groups, and extract cross-attention maps from all layers. We then compare aggregated attention from cultural modifiers to their paired nouns, yielding layerwise cultural-attention Table 5. Validating Culture-Sensitive Neuron Detection in AltDiffusion. Neuronal accuracy on the test subset with cultural style modifier + noun prompts. Table 6. Quantitative analysis across domains. Generate using 100 captions from outside the domain and calculate the performance of CultureVQA. Bold rows mark the highest performance within each baseline group. Method AltDiffusion [48] + Masked Top-K Neurons + Masked Random Neurons CultureVQA 44.54 12.04 (-32.50) 42.45 (-2.09) Figure 16. Selection of Threshold K. The red dashed vertical line indicates the chosen threshold K. Neurons to the left of the line correspond to the selected Top-K culture-sensitive neurons, while the scores to the right fall below the cutoff and are discarded. contrast curves for AltDiffusion (Figure 14). Culture Neuron Detection. We employ the same methodology as in the main text to localize culturally sensitive neurons within the AltDiffusion culture layer. As illustrated in Figure 15, we present culturally sensitive neurons across six distinct cultural contexts. This demonstrates our approachs robust capability to detect culturally sensitive neurons across varied architectural frameworks. C.3. Cultural Validation in AltDiffusion To analyze the accuracy of culture neurons in AltDiffusion, we validate these neurons for each cultural group. For all 15 cultural groups, we report CultureVQA scores in three controlled settings: (1) Baseline (no masking), (2) Masked TopK Neurons (masking the Top-K identified culture-sensitive neurons), and (3) Random Mask (masking the same number of neurons at random). Table 5 shows that masking the TopK neurons causes substantial drop in CultureVQA performance. The mean score falls by 32.50 points compared to the AltDiffusion baseline. In contrast, randomly masking the same number of neurons reduces the mean score by just 2.09 points. This negligible drop stays close to the baseline. These results indicate that the culture-sensitive neurons we identify in AltDiffusion concentrate cultural information and capture cultural representations. C.4. Selection of Threshold To clarify how we choose the threshold K, we visualize the weighted frequency scores (WFS) of candidate neurons and inspect their response patterns. The main factor guiding the choice of is the presence of small subset of neurons that exhibit prominently higher responses than the rest. As shown in Figure 16, we plot the WFS curves for Method StableDiffusion XL [34] FLUX.1-dev [22] Show-o2 [44] PEA-Diffusion [27] AltDiffusion [48] StableDiffusion 3.5 [7] Baseline: PEA-Diffusion Ours (Zero-Training) Ours (Fine-Tuned) Baseline: AltDiffusion Ours (Zero-Training) Ours (Fine-Tuned) CultureVQA 10.00 17.00 15.00 15.00 15.00 17.00 22.00 (+7.00) 35.00 (+20.00) 20.00 (+5.00) 32.00 (+17.00) three different cultural groups; in each case, few leading neurons form clear peaks, followed by rapid decay. We set at the elbow before this sharp drop, so that neurons with salient high responses are retained, while the long tail of weakly responsive neurons is discarded. C.5. Details of SAE we set the sparsity coefficient to α = 1 32 . We adopt TopK SAE with hidden layer dimension of 4096. The model is optimized with AdamW using learning rate of 0.0004 and constant learning-rate schedule without warmup. We use an MSE reconstruction loss to encourage faithful feature reconstruction. C.6. Cross-Domain Generalization Results We evaluated our methods generalization using quantitative and qualitative tests on cue words not present in the CultureBench dataset. We used 100 out-of-domain captions to ensure the evaluation reflects true and reliable out-ofdistribution performance. Quantitative Results. Table 6 shows that our method consistently improves cross-domain cultural accuracy for both PEA-Diffusion and AltDiffusion. Under the zero-training setting, our approach yields gains of +7.00 and +5.00 CultureVQA points over PEA-Diffusion and AltDiffusion, respectively, outperforming all existing off-the-shelf models. When fine-tuned, the improvements increase to +20.00 and +17.00 points for PEA-Diffusion and AltDiffusion, respectively, with our method achieving the highest accuracy within each baseline group. These results show that the culture-sensitive neurons identified by our method provide substantial, transferable cultural prior, enabling robust generalization to captions outside the training distribution. Qualitative Results. As illustrated in Figure 17, when prompted with the Chinese instruction wood carving Figure 17. Cross-domain qualitative experiments. Our approach generates images that are more culturally appropriate. exhibit, most models exhibited semantic comprehension errors. Their outputs were inconsistent with the prompts meaning. Our approach, however, maintained both semantic and cultural coherence. In contrast to Arabic instructions like beautiful silk headscarf or French prompts such as An elegant dress, our approach offers greater precision. It captures key textual details regarding fabric texture, style, and aesthetic quality. It also adeptly integrates regional cultural symbols and aesthetic preferences in color coordination, pattern design, and figure representation. This enables the generation of images better suited to local cultural contexts. Taken together, these qualitative findings indicate that our approach is not only more robust and expressive but also more culturally sensitive across languages and regions. It substantially improves cross-linguistic and cross-cultural text-image alignment and image generation, thereby further validating its effectiveness and broad applicability in realworld, culturally diverse scenarios. D. Detail of User Study As shown in Figure 18, we conducted controlled user study to evaluate the perceptual effectiveness of this method. Fifty participants, aged 25-47 (23 female), completed three sequential perceptual tasks. Each task assessed the subjective performance of generated images, focusing on cultural alignment and preference. Multi-Choice Culture. During the Multi-Choice Culture (MCC) mission, we first used the noun-only prompt within CultureBench and fed it to multiple image generation models, obtaining numerous candidate images. Next, participants were asked to select all images they deemed representative of given target culture. This selection process then measured each models capacity to generate culturally coherent images without textual cultural cues. Single-Choice Culture. Single-Choice Culture (SCC) defines cultural perception as forced-choice task of cultural identification. For each noun-only prompt, we generate an image using specific model. Participants view the modelgenerated image and are asked to infer its most probable cultural category. We provide fixed set of 16 options: 15 predefined cultural categories plus an additional Uncertain option. Participants must select single option based precisely on the images visual content. Cultural Semantic Relevance. Cultural Semantic Relevance (CSR) is employed to evaluate the subjective cultural semantic alignment of images within given target culture. For each CSR item, we present an image generated by specific model, explicitly informing participants of its stylistic origin. Participants are required to rate the images alignment with the target culture within the context of that models style, on scale of 1 to 5. E. More Results To demonstrate the effectiveness of our approach more intuitively, we generated both an unenhanced baseline image and an enhanced image using the same prompt, while keeping the models seed and other generative parameters fixed. The comparison between the two is shown in Figure 19. The results show that the enhanced images more faithfully reflect the cultural context, including scene elements, character depictions, and fine textures. This leads to more accurate visual expressions of the target culture and further highlights the effectiveness of our approach in achieving cultural consistency. F. More Discussion Q1. What do we mean by cultural consistency in this work? We define cultural consistency as the degree to which an image from multilingual prompt shows statistically grounded, contextually appropriate cultural cues tied to the target languages socio-cultural context, rather than Figure 18. Example interface of the user study comprising three task types. MCC (top), SCC (bottom-left), and CSR (bottom-right). Figure 19. More Results. Further examples of results generated by AltDiffusion and PEA-Diffusion under different methods. merely literal semantic correctness. Our approach uses (i) fixed languageregion mapping, (ii) observable, moderate visual grounding (such as architecture, clothing, artifacts), and (iii) expert screening to remove stereotypical or inappropriate cues. These elements turn vague idea into an operational objective that can be measured and optimized. Q2. Why is CultureBench designed as medium-scale diagnostic benchmark rather than larger dataset? CultureBench is deliberately scaled to moderate size, as it serves as controlled diagnostic benchmark for assessing the cultural behaviour of multilingual text-to-image models rather than functioning as pre-training corpus: Approximately 7.9k images, meticulously annotated across 15 language-culture regions, suffice to reveal systemic cultural omissions under noun-only prompts and support neural-level analysis, whilst ensuring the feasibility of highquality, de-stereotyped annotation. We thus regard it as an extensible starting point rather than comprehensive catalogue covering all cultures. Q3. Why do we rely on CultureVQA, and how reliable is it as an evaluation metric? We adopt CultureVQA because it directly addresses the challenge of evaluating whether images accurately reflect the nuanced cultural attributes encoded in multilingual prompts. Unlike existing automatic metrics (e.g., FID, CLIPScore), which focus on pixel-level similarity or general correspondence, CultureVQA frames evaluation as semantic recognition problem, allowing assessment of cultural correctness. By using VQA-style cultural identification task, CultureVQA enables consistent, scalable, and multi-class evaluation across 15 cultural regions. Its reliability is supported by humanmodel consistency study (Appendix C.1). Q4. What evidence supports the hypothesis that cultural knowledge exists but is under-activated? Our claim is posed as hypothesis and supported by converging empirical evidence rather than formal proof. First, explicit culture-style modifiers (e.g., Italian architecture, person in traditional Chinese clothing) consistently trigger culturally grounded generations. In contrast, noun-only prompts collapse to neutral, culture-agnostic prototypes, indicating that the cultural capability is present but insufficiently activated. Second, masking the Top-K neurons identified by our probing method results in sharp drop in CultureVQA performance, whereas masking the same number of random neurons has minimal effect, suggesting causal relationship between these neurons and cultural semantics. Third, attention-based probing reveals stable culture-sensitive layer in which culture-style modifier + noun and noun-only prompts yield distinct activation patterns. Together, these findings show that the model has cultural representations, but they are under-activated without explicit cues. Q5. Why are our neuronand layer-level interventions reasonable, and do they over-claim causality? Our neuronand layer-level interventions are deliberately minimal and localized: both the zero-training neuron amplifier and the fine-tuned layer enhancer operate only within the culture-sensitive layer and within small subspace identified by our probing method, without modifying the diffusion backbone. Adjusting these neurons reliably improves CultureVQA scores and human-perceived cultural fidelity, while leaving CLIPScore, ImageReward, LPIPS, and visual diversity essentially unchanged. This indicates that the intervention is targeted rather than disruptive. Importantly, we do not claim to establish definitive causal attributions for individual neurons. Instead, we frame our approach as lightweight and interpretable control mechanism that leverages empirically responsive subspaces. It offers practical gains in cultural consistency while serving as useful starting point for future, more formal causal analyses. Q6. Is testing only on CultureBench lacking external validation? We agree that relying solely on CultureBench would limit external validation, which is why we include cross-domain experiment using 100 out-of-distribution captions not appearing in CultureBench (Appendix C.6). Our method still yields consistent gains in CultureVQA under both zero-training and fine-tuning in this out-of-domain setting, showing that the improvement is not tied to CultureBenchs specific prompts. Q7. This paper uses ChatGPT data to refine cultural products, but could bias and errors be introduced? The use of ChatGPT-generated text may introduce biases; hence, we have implemented multiple dedicated measures in our data construction process to minimise these risks. Firstly, the ChatGPT content we utilise is strictly confined to culture-style modifier + noun. All corresponding images are sourced exclusively from publicly available real-world reference materials and undergo multi-stage human expert review to eliminate stereotypes, semantic errors, and culturally inaccurate cues. Furthermore, the ChatGPTgenerated modifiers themselves undergo expert filtering to ensure they do not contain stereotypical or inappropriate cultural descriptions. Q8. When switching generative models, is it necessary to re-execute the Cultural Sensitivity Layer and Cultural Neuron Detection step within our methodology? Whether re-detection is required depends primarily on whether the text encoder of the new model has changed. If the new generative model continues to use the same text encoder as in our experiments, re-detection is generally unnecessary; if the text encoder has been altered, the detection steps must be re-executed. Q9. Mean square error (MSE) cannot assess cultural consistency, so is it appropriate to use it? MSE is not in itself an effective metric for measuring cultural consistency, and we therefore did not employ it as an evaluation criterion. Within our framework, MSE serves as the training signal for specific layer enhancers, rather than the objective we use to gauge cultural consistency. Q10. What are the roles of ϵ and β? ϵ is set to 0 by default, since activations greater than zero are treated as neuron firing. β is small stability constant added to the denominator to prevent it from becoming zero when neuron has zero or very few activations. G. Limitation CultureBench currently covers 15 cultural regions, but this still reflects only limited portion of global cultural diversity. Because the benchmark is built from publicly available image resources, some cultures, especially those from lowresource or marginalized communities, remain underrepresented. We emphasize that the currently included cultural regions are carefully curated and remain valid, but they do not yet exhaust the diversity within each region or across countries. In future iterations, we intend to identify underrepresented cultural groups by partnering with relevant organizations, actively seeking additional image sources, and introducing finer intra-cultural subdivisions, thereby expanding the benchmark to more countries and territories for more comprehensive and inclusive evaluation suite. H. Ethics Statement In this research, we acknowledge the potential misuse of image synthesis techniques, such as ours, for generating deceptive content and spreading disinformation, serious concern we address explicitly. However, we also note the substantial progress made in detection and prevention mechanisms in this domain. Our framework supports critical research initiatives and encourages third-party oversight, aiming to strike balance between technological advancement and security considerations. This balanced approach promotes responsible deployment while preserving the innovation potential."
        }
    ],
    "affiliations": [
        "Central South University",
        "Nanjing University",
        "Nanjing University of Science and Technology",
        "National University of Singapore",
        "The University of Sydney"
    ]
}