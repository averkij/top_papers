{
    "paper_title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories",
    "authors": [
        "Huiyang Shao",
        "Xin Xia",
        "Yuhong Yang",
        "Yuxi Ren",
        "Xing Wang",
        "Xuefeng Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 9 6 7 0 . 3 0 5 2 : r RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories Huiyang Shao Xin Xia Yuhong Yang Yuxi Ren Xing Wang Xuefeng Xiao ByteDance Inc."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlows superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques. 1. Introduction Diffusion models have revolutionized generative AI [10, 4143], achieving impressive results in various domains, from text [15, 47] and images [40? ? ? ] to 3D models [4, 22, 46], audio [9, 19], and restoration [1, 2]. However, their slow generation speed, often requiring dozens of steps per sample, remains significant limitation. Various distillation techniques attempt to accelerate this process, including normal [26, 56], adversarial [38, 45], progressive [36], and variational score distillation [29, 37, 46, 49, 53, 54]. Current distillation methods, despite their objectives, are hindered by challenges including substantial computational overhead, complex training regimens, and limitations in terms of generation speed, sample quality, and effective guidance [12, 28, 30, 31, 48, 55, 58]. This underscores the imperative for developing more generalized approaches to fully exploit the capabilities of diffusion models. Equal Contribution Project Leader The traditional diffusion process, as shown in Fig.1a, forms the foundation for most acceleration and distillation methods. However, its clear from the figure that this process suffers from several issues: (1) Varying Expectations. The expectations in the backward process differ across timesteps. Achieving highquality samples necessitates greater number of sampling steps, making it impossible for accelerated sampling methods to avoid quality degradation. (2) Overlapping Diffusion Paths. Since all sample eventually converges to the same standard Gaussian distribution, the diffusion probability paths will overlap. E[ϵtxt] may represent the intersection of multiple probability paths, leading to significant randomness in sampling outcomes and potentially substantial quality loss. (3) Sampling Instability. Even with closely positioned sampling points, the final generated results can differ significantly, introducing substantial instability. Several studies try to address these limitations: [23] introduced rectified flow (RF) using linear ordinary differential equation (ODE) for straight-path sampling (Fig.1b topleft), while [44] extended RF to first-order curved paths rectified diffusion (RD) for denoising diffusion probabilistic models (DDPM)-like models (Fig.1b top-right). These sample-noise matching approaches have also been applied to diffusion distillation by [25, 51]. (1*) Path Inconsistency: However, existing sample-noise matching algorithms brings new drawbacks while attempting to address above challenges: the gap between sample-noise matching and actual ODE sampling paths is too large, potentially leading to training difficulties and poor generalization; (2*) Limited Diversity: sampling probability paths are severely constrained, significantly reducing model generation diversity; (3*) Theoretical Gap: the method is highly intuitive but lacks fundamental theoretical derivation to prove its optimality for sampling stability. To address all the three challenges in traditional diffusion and the drawbacks introduced by existing sample-noise matching methods, we propose RayFlow (shown in Fig.1b bottom). Consistent Expectations and Path: For challenge (1) and (1*), we leverage pre-trained models to cal1 (a) The forward and backward process of traditional diffusion. (b) The forward and backward process of our diffusion method. Figure 1. The comparision of different diffusion process. culate unified noise expectation ϵµ = Et[E[ϵt]] across all timesteps, enabling efficient step compression without quality degradation. Individual Path Design: For challenge (2) and (2*), instead of converging to common Gaussian, each sample follows unique diffusion path towards its specific target mean with reduced variance, minimizing path overlaps and sampling randomness. Theoretical Guarantee: For challenge (3) and (3*), we prove that our method maximizes the path probability between the starting point, target mean, and origin, ensuring optimal sampling stability and reliable reconstruction of the original data point. Moreover, to improve training efficiency, we develop Time Sampler, an advanced importance sampling method that identifies crucial timesteps during training. By combining Stochastic Stein Discrepancies (SSD) with neural networks, Time Sampler approximates the optimal sampling timestep distribution to minimize variance of training loss estimator, thereby reducing computational redundancy and enhancing efficiency. Our key contributions can be summarized as follows: RayFlow Framework: We introduce an innovative diffusion framework with instance-independent target means. This approach offers enhanced control over the generative process, allowing for more efficient and precise sampling. Time Sampler: We develop an timestep importance sampling technique utilizing SSD. This method effectively identifies crucial timesteps during training, reducing computational redundancy and enhancing efficiency. Efficient Algorithms: We present practical algorithms for training and sampling with RayFlow, including fast one-step sampling variant for quicker generation. Theoretical Analysis: We provide thorough theoretical examination of RayFlow, detailing the derivation of path probabilities and the optimization of parameters to ensure maximal sampling stability. Through extensive experiments, we demonstrate the effectiveness of RayFlow in generating high-quality images with improved efficiency and controllability compared to existing acceleration algorithm. Our work opens up new approach for exploring and controlling diffusion processes. 2. Preliminaries 2.1. Denoising Diffusion Probabilistic Models [10] Consider dataset of real samples x0 Rd from distribution p(x0). DDPMs learn this distribution through iterative noising and denoising over discrete time steps, indexed by {1, . . . , }. Let (µ, Σ) denote the Gaussian distribution with mean µ and covariance Σ. Forward Process. The forward diffusion process gradually adds Gaussian noise through Markov chain: p(xtxt1) = (αtxt1, βtI) (1) where αt [0, 1] is the draft term and βt [0, 1] is noise variance. After steps, p(xT ) = (0, I). With setting + β2 α2 = 1, the process simplifies to: p(xtx0) = (αtx0, 1 αtI) (2) where αt = (cid:81)t factors up to step t. s=1 αs is the cumulative product of scaling Backward Process. The reverse process reconstructs x0 from xT by iteratively denoising: p(xt1xt, x0) = ( µ(xt, x0), ˆβtI) (3) where µ are parameters which need to learn, ˆβt = (1 αt1)/(1 αt)βt. 2 2.2. Simulation-Free Flow Matching Flow matching (FM) [8, 18, 20, 23] consider generative models that learn mapping between noise distribution p(xT ) and data distribution p(x0) via an ODE: dxt = vθ(xt, t)dt, (4) where vθ(yt, t) represents velocity field parameterized by neural network with weights θ. Let u(xt) be the marginal velocity field that generates the marginal probability p(xt). ψt(ϵ) = atx0 + btϵ, u(xtϵ) = ψ (cid:0)ψ1 (xtϵ)ϵ(cid:1) , where at, bt are coefficient. The FM objective, which aims to directly regress marginal velocity field (u(xt) has the same gradient with u(xtϵ) [18]): LCF = Et,p(xtϵ),p(ϵ) (cid:2)vθ(xt, t) u(xtϵ)2 2 (cid:3) . (5) According to [8]s setting, we have ut(xtϵ) = xt at /b2 tϵ, where the signal-to-noise ratio λt = log(a2 ), t/b). Denote ϵθ(xt, t) = 2 (vθ λ tbt t/αt xt), then we can rewrite Eq.(5) to: = 2(α bt 2 λ with λ at L(x0) = 1 2 Et,ϵ (cid:20) 1 2 b2 λ ϵθ(xt, t) ϵ2 (cid:21) (6) Eq.(6) shows the connection between diffusion and FM, demonstrating that their training processes differ only by different weighting factor. 2.3. Flow Trajectories This section examines several trajectory variants. + β2 Variance Preserving (VP) [10]. As described in Section 2.1, sets α2 = 1 and defines βt linearly between β0 and βT . VP uses square-root interpolation of αt, with forward process ψt(ϵ) = xt = 1 αtϵ and marginal velocity field u(xtϵ) = α 1α2 (αtx0 ϵ) αtx0 + Rectified Flow [23]. RF defines linear forward process: ψt(ϵ) = xt = (1 t)x0 + tϵ, and the marginal velocity field u(xtϵ) = ϵx0 1t 2.4. Importance Sampling (cid:80)n In variational inference, we frequently seek to estimate expected values of the form µ = Exp[ξ(x)], where is probability distribution over Rd and ξ : is an integrable function. The classical Monte Carlo estimate of µ is ˆµn = 1 i=1 ξ(xi), with xi being i.i.d. samples. However, when ξ is non-zero primarily in regions where is small, standard Monte Carlo sampling becomes inefImportance Sampling addresses this by sampling ficient. from an alternative distribution q, chosen to reduce the variance of the estimator. 3 Definition of Importance Sampling. Let : be probability density function such that q(x) > 0 for all where p(x)ξ(x) = 0. We can express the expected value µ as: µ = (cid:90) ξ(x) p(x) q(x) q(x)dx = Exq (cid:20) ξ(x) (cid:21) . p(x) q(x) (7) Importance Sampling Estimator. Given i.i.d. samples i=1 from the importance distribution q, the importance {xi}n sampling estimator ˆµq is defined as: ˆµq = 1 (cid:88) i= ξ(xi) p(xi) q(xi) . (8) This estimator is unbiased, meaning that its expectation is equal to µ. Moreover, we can get variance of Importance Sampling Estimator. Then the variance of the importance sampling estimator is given by Vq[ ˆµq] = 1 , where σ2 (cid:34) = Exq σ2 ξ2(x) (cid:19)2(cid:35) (cid:18) p(x) q(x) µ2. (9) 3. Proposed Method This section introduces our proposed method for RayFlow, which involves novel approach to diffusion processes in generative modeling. We present the theoretical foundations, the forward and backward processes, and the path probability. We also provide the optimal parameters for maximizing the path probability and outline the training and sampling algorithms. 3.1. RayFlow We introduce RayFlow, novel framework that transforms data through precise trajectory. The core of our method is markov chain that construct flow that sample x0 between target distribution p(xT ) = (ϵµ, σ2I), where ϵµ represents pretrained mean vector and σ is the standard deviation. This process is shown in following proposition (proof in Appendix.A.1). Proposition 1. Given data x0, pretrained mean ϵµ Rd and variance σ R>0, and target diffusion p(xT ) = (ϵµ, σ2I), we can describe the diffusion process with the following Markov chain. Flow Trajectories. Define probability flow path. ψt(ϵ) = αtx0 + (1 αt)ϵµ + 1 αtϵ (10) Forward Process. Add noise to image data. p(xtxt1, ϵµ) = (cid:0)αtxt1 + (1 αt)ϵµ, β σ2I(cid:1) Figure 2. RayFlow and importance time sampling. Time Sampler can find the key timesteps (five coordinates) of flow matching. Backward Process. Denoise from image data. (cid:18) 1 αt p(xt1xt, ϵµ) = 1 αt αt xt ϵµ, βtσ2I βt = (cid:18) (1 α2 )(1 αt1) 1 αt (cid:19) (cid:19) (11) (12) Path Probability and Optimization. key theoretical result of our framework is the characterization of optimal parameters that maximize the path probability - the likelihood of successfully transforming an image through the forward and backward processes. The optimal parameters are defined by following proposition (proof in Appendix.A.2): Proposition 2. For any general diffusion defined in Prop.1, the path probability (the probability of starting from ˆx0, forward to ˆϵµ, and backward to ˆx0) is given by: Theorem 1. Let Sϵ = {ϵt}T t=1 be the noise added in the forward process, ϵµ, σ be the parameters of the target distribution (ϵµ, σ2I). For sample ˆx0, we can obtain the optimal parameters that maximize the path probability. arg max Sϵ,ˆϵµ,ϵµ,σ p(x0 = ˆx0xT = ˆϵµ) (cid:89) t=1 p(ϵt = E[ϵt]) (16) where the optimal parameters are defined by: ϵ = {(1 ˆϵ µ = t=1, σ 0 αt)ϵµ}T αT )ϵµ, ϵ αT ˆx0 + (1 µ = Et[E[ ϵt]] (17) This theorem provides us with way to find the corresponding parameters of the target distribution that leads to optimal probability path, which ensures that the diffusion process is efficient and informative. p( ˆx0 ˆϵµ ˆx0) = 3.2. Timestep Sampling p(xT = ˆϵµx0 = ˆx0) (cid:124) (cid:125) (cid:123)(cid:122) Forward Path Prob. p(x0 = ˆx0xT = ˆϵµ) (cid:124) (cid:125) (cid:123)(cid:122) Backward Path Prob. (13) where the backward path probability is defined by: (cid:32) p(x0xT = ˆϵµ) = ˆϵµ (cid:88) αT + s= (e)s + (c)s (cid:112)αs1/αt1 (cid:33) (14) (cid:88) , s=1 βs αt αs σ2I and (e)s and (c)s are defined by: (e)s = E[ϵs] (15) αs1(1 α2 s) αs (1 αs) (c)s = 1 αs αs + αs αs1 1 αs αs1 + αsαs ϵµ E[ϵs] denotes the noise mean during forward at timestep s. By maximizing the probabilistic path, we can minimize the instability of sampling. But how to choose the parameters that make the probabilistic path maximized is important issue, so we propose the following theorem (proof in Appendix.A.3). Training diffusion models involves estimating the expectation in Eq.(5), which averages the loss over all timesteps t. While uniform sampling of is inefficient due to high variance and redundant computations. To address this, we introduce Time Sampler, novel importance sampling technique for efficient timestep selection during training. Ideally, we want to sample timesteps from distribution that minimizes the variance of Eq.(5). This optimal sampling distribution is shown in following proposition (proof in Appendix.B.3). Proposition 3. The optimal sampling distribution for Eq.(5) with minimal variance is: q(tx0, ϵµ) ξt(x0, ϵµ)p(t), (18) where ξt(x0, ϵµ) = ϵθ( which means for any probability distribution p, we have αt ˆx0 + (1 αt)ϵµ) ϵµ2 2. Vtq(t),(x0,ϵµ)[ξt(x0, ϵµ)] Vtp(t),(x0,ϵµ)[ξt(x0, ϵµ)] However, Estimating q(tx0, ϵµ) presents two key challenges: 1) limited samples: At each iteration, we can only access small batch of samples, making it difficult to accurately estimate the full distribution. 2) data dependency: 4 The sampling distribution varies for different x0 and ϵµ, requiring flexible approach to capture this dependency. To overcome these challenges, Time Sampler employs combination of SSD (please refer to Sec.B.2 for details) and neural networks. SSD provides powerful framework for fitting distributions with limited samples. It maintains set of particles that are iteratively updated to approximate the target distribution; To capture the dependency of q(tx0, ϵµ) on the input data, we use neural network Tϑ to parameterize the particle locations. Denoting the empirical distribution St = {titi [0, ]}n i=1, to approximate q(tx0, ϵµ). Hence, we desin the Time Sampler Tϑ : (x0, ϵµ) (cid:55) St. The particles in Time Sampler are updated iteratively using gradient-based approach. The update direction is determined by minimizing the KL divergence between the empirical distribution of particles and the target distribution. Following [21], let the updated distribution as q[εϕ], and we can use the direction of the fastest change in KL divergence as the update amount, recorded as ϕ. ϕ = arg min ϕB (cid:26) dϵ (cid:12) (cid:12) KL(q[ϵϕ]q) (cid:12) (cid:12)ϵ= (cid:27) (19) Algorithm 1: RayFlow Distillation Training Input: Epochs E, Timesteps ,Images = Prompt datasets Sc = {c(i)}N i=1, Time Sampler Tϑ(x0, ϵµ, t) Tunable parameter: Network Parameters θ and ϑ Output: Denoiser ϵθ(ϵ, c, t)"
        },
        {
            "title": "Construct Distillation Data",
            "content": "1 2 for = 1 to do 3 0 , ˆϵ(i) Sampling ˆϵ(i) from (0, I) ( ˆx(i) µ ) = Ψ(ϵθ, ˆϵ(i), c(i), K) ˆEkTϑ( ˆx(i) minϑ = {( ˆx0, ˆϵ(i) 0 , ˆϵ(i))[k] ξt( ˆx(i) µ )} Add data pair to dataset 0 , ˆϵ(i))2 2 6 7 end 8 9 for = 1 to do for = 1 to do"
        },
        {
            "title": "RayFlow Training",
            "content": "µ , t)}T 0 , ˆϵ(i) {ftft = Tϑ( ˆx(i) p(t ˆx(i) 0 , ˆϵµ) = ft ˆEt[ft] Sampling p(t ˆx(i) minθ ϵθ( minϑ ϕ(Tϑ( ˆx(i) ˆαt ˆx(i) 0 , ˆϵ(i) 0 +(1 µ )[t])2 0 , ˆϵµ) t=1 Time weight Proposition. ˆαt ˆϵ(i) µ )) ˆϵ(i) µ 2 2 4 5 12 13 14 15 where = {ϕ : ϕH 1}. It can be proven that the direction of the fastest gradient is the vector function = β/βH (coms from property of ssd), that is, end 16 17 end 18 return Network Parameters θ and ϑ dε (cid:12) (cid:12) KL(q[εϕ]q) (cid:12) (cid:12)ε=0 = Etq[AT ϕ(t)] (20) Here AT dient direction is ϕ(t) = [t ln q(t)]T ϕ(t) + ϕ(t). The graϕ() β() = Etq[Aq k(t, )] = Etq[t ln q(t)k(t, ) + tk(t, )] (21) The first term in the parentheses represents the driving term, which makes the particle tend to the target distribution, and the second term represents the diffusion term, which prevents the particles from getting too close. ln q(t) = ln (cid:32) 1 n (cid:88) i=1 (cid:33) ξt(x0, ϵµ)K(ti, t) (22) Then we can update Time Sampler Tϑ min ϑ = 1 (cid:88) (Tϑ(x0, ϵµ)[i] (ti + εϕ(ti)))2 (23) i=1 (cid:80)n Its notable that the above loss function is equavilent to 1 Its easy to understand we need to minin mize the update distance until convergence. i=1 ϕ(ti)2. Algorithm 2: RayFlow Distillation Sampling Input: Sampling steps K, Prompt Output: Sampling result ˆx0 1 Sampling ˆxK from (0, I) 2 for = to 1 do ˆϵk = ϵθ( ˆxk, c, k) ˆxk1 = 1 αt ˆxk 1αt αt 4 5 end 6 return ˆx0 noise prediction ˆϵk + βtϵ ϵ (0, I) Algorithm 3: RayFlow One-step Sampling Input: Prompt Output: Sampling result ˆx0 1 Sampling ˆxT from (0, I) 2 ˆϵT = ϵθ( ˆxT , c, ) αT ˆxT 1 3 ˆx0 = 1 αT αT 4 return ˆx noise prediction ϵ (0, I) ˆϵT + βT ϵ 3.3. Algorithms We present the key algorithms that implement our RayFlow framework, consisting of the training procedure (Algo.1) 5 and two sampling approaches (Algo.2 and Algo.3). The training algorithm consists of two main phases: data construction and model training. In the data construction phase, we generate synthetic training pairs by sampling noise vectors. The Time Sampler Tϑ learns to predict important timesteps for denoising. In the training phase, we optimize the denoising network ϵθ to predict and remove noise at each timestep, guided by the learned time weights. Ψ(ϵθ, ˆϵ(i), c(i), K) represents ODE solvers which sampling image from noise ˆϵ(i) with condition c(i) in steps, ˆϵ(i) represents sampling result at timestep k. ], where ˆϵ(i) µ = ˆEk[ˆϵ(i) we provide two sampling algorithms. The standard sampling Algo.2 iteratively denoises the input over steps using the trained denoiser. Starting from pure noise ˆxK, it progressively refines the output through repeated noise prediction. For efficiency, we also present one-step sampling variant Algo.3 that generates the final output directly using single denoising step at timestep . 4. Experiment Figure 3. Sensitivity analysis on COCO-5K dataset where Aesthetis score for with respect to different values of σ. COCO-5k ImageNet Cifar-100 Cifar-10 Module Clip Aes FID Clip Aes FID Clip Aes FID Clip Aes FID w/ Tϑ 34.3 5.9 4.0 36.0 5.6 1.9 28.9 4.9 1.6 29.1 4.9 1.7 w/o Tϑ 31.9 5.7 4.8 33.9 5.0 2.8 26.4 4.7 3.5 28.0 4.7 3.9 We present series of experiments evaluating quality, scalability and robustness of RayFLow. Our training costs around 2.5 8 * A100 GPU days. We train LoRA instead of UNet for convenience. We employ AdamW optimizer with learning rate 1e 6, 16 batch size, and 200 epochs. We adopt gaussian kernel K(x, xi) = exp for Time Sampler, where is bandwidth, and set it 0.25. For all of competitors, we follow their papers setting. (cid:16) xxi2 2h2 (cid:17) 4.1. Implementation Details Dataset. Our experiments utilize carefully curated subset of the LAION [39] and COYO datasets [3], following the data selection approach outlined in previous methods [16, 34]. The evaluation is performed on COCO-5k [17], ImageNet [7], Cifar 10 [14], Cifar 100 [14] datasets. Evaluation Metrics. We employ multiple complementary metrics to assess the quality and performance. The aesthetic predictor, pre-trained on the LAION dataset, evaluates visual appeal, while CLIP score (ViT-B/32) measures text-to-image alignment. Moreover, we incorporate recent metrics including Image Reward [50] and Pick Score [13]. Base Models. Our experimental framework is built upon three existing popular models: stable-diffusion-v15 (SD15) [35] with UNet architecture, stable-diffusion-xlv1.0-base (SDXL) [35] with UNet architecture, and PixArt [5] implementing the DiT architecture. These models serve as benchmarks for comparing various acceleration schemes, with detailed performance comparisons presented in Tab.1. Figure 4. Visualization of the importance distribution of timestep. Performance of Time Sampler (SDXL) on different datasets. 4.2. Main Results Timestep Importance Sampling. We conduct some experiments to verify the performance of our proposed Time Sampler, shown in Tab.4. Model with Time Sampler outperforms its counterpart without the module across all metrics and datasets. We provide visualization of the timestep importance distribution, which supports the effectiveness of the module. This visualization demonstrates how the module learns to adaptively focus on different timesteps during the diffusion process. This comprehensive evaluation across diverse datasets and metrics validates that the proposed Time Sampler module is indeed effective in enhancing the models performance in image generation tasks. Sensitivity Analysis. For the variance coefficient σ in RayFlow, we conduct experiment with different coefficients. We distill the SDXL model on the COCO-5k dataset using various σ. Fig.3 presents the sensitivity analysis results. We can observe that the model performs better when σ is set to 0.3. When σ is set to either 0.5 or 0.1, the results 6 Method Type Model Information Params Flow Distill Cost FID-Score 1-Step 2-Step 4-Step 8-Step 1-Step Aesthetics-Score 4-Step 2-Step SD15-Base [35] SD15-PeRFlow [51] SD15-LCM [27] SD15-TCD [57] Hyper-SD15 [34] SD15-Ray (ours) UNet VP 0.98B LoRA 67.5M RF LoRA 67.5M VP LoRA 67.5M VP LoRA 67.5M VP LoRA 67.5M Ray VP 3.5B UNet SDXL-Base [35] VP 3.5B UNet SDXL-Turbo [38] Unet RF 3.5B SDXL-PeRFlow [51] LoRA 197M VP SDXL-LCM [27] SDXL-TCD [57] LoRA 197M VP SDXL-Lightning [16] LoRA 197M VP LoRA 197M VP Hyper-SDXL [34] LoRA 197M VP SDXL-DMD2 [52] LoRA 197M Ray SDXL-Ray (ours) PixArt-α [5] PixArt-Σ [6] PixArt-δ [6] PixArt-Ray (ours) DiT DiT DiT DiT 610M VP 610M VP 610M VP 610M Ray Stable Diffusion V1.5 Comparision - 6.4 Day 4.5 Day 5.5 Day 12 Day 2.5 Day 19.8.03 5.37.06 5.29.05 5.56.07 5.41.04 5.10.03 12.1.06 5.33.03 5.05.07 5.10.04 5.05.06 4.86.05 11.6.04 5.21.08 5.22.04 5.23.09 5.21.05 4.78.04 Stable Diffusion XL Comparision - Day 6.4 Day 4.5 Day 5.5 Day Day 12 Day 12 Day 2.5 Day - - 1 Day 1 Day 15.7.03 4.32.07 4.20.05 4.27.07 4.50.08 4.35.06 4.27.07 4.19.02 4.15.03 10.5.02 4.16.05 4.22.07 4.22.04 4.10.03 4.18.09 4.22.04 4.06.03 4.02.02 9.52.01 4.00.04 4.05.06 4.17.05 4.20.07 4.08.03 4.17.05 3.98.02 3.90.02 PixArt DiT Comparision 13.6.02 12.9.02 4.92.07 4.78.02 11.4.03 11.3.02 4.76.05 4.59. 10.14.01 10.04.03 4.47.09 4.12.03 9.02.02 5.15.07 5.03.09 5.19.06 5.02.04 4.69.04 8.42.04 3.80.06 3.78.09 3.81.09 3.90.04 3.84.07 3.81.09 3.71.05 3.67.04 9.21.04 9.19.01 4.10.04 3.96.01 3.66.03 5.69.04 5.71.06 5.45.05 5.62.03 5.92.02 3.98.02 5.75.08 5.68.03 5.60.03 5.55.06 5.75.04 5.60.03 5.90.03 5.96. 3.78.02 3.84.04 5.54.06 5.84.04 3.89.05 5.78.09 5.83.07 5.81.08 5.85.04 6.03.03 4.14.05 5.85.04 5.65.08 5.49.08 5.43.10 5.48.05 5.49.08 5.99.06 6.03.02 4.12.01 4.24.02 5.70.08 5.92.02 4.11.04 5.59.07 5.86.05 5.63.03 5.64.05 6.13.04 4.26.03 5.90.09 5.85.04 5.97.06 5.89.05 5.95.09 5.97.06 6.07.04 6.15. 4.42.03 4.56.01 5.85.03 6.10.03 8-Step 4.54.01 5.96.03 5.96.04 5.98.06 6.00.05 6.25.02 4.61.02 5.70.05 5.60.06 5.74.10 5.65.03 5.68.07 5.74.10 6.14.01 6.24.01 4.51.04 4.70.02 5.95.09 6.17.03 Table 1. Quantitative comparison of state-of-the-arts models across various architectures and steps for FID and Aesthetic scores on the COCO-5k datasets. The best result is highlighted in bold. Distill Cost means 8 A100 days. are relatively poor. This phenomenon might be explained by the fact that larger variance values can cover more samples and enhance diversity, while smaller variance values can reduce sampling instability. With moderate variance values, all of these beneficial characteristics are combined very well. Quantitative Comparison Between Acceleration Algorithms. We conduct extensive experiments to validate the effectiveness of our proposed RayFlow framework, with results shown in Tab.1.Our experiments demonstrate consistent performance advantages across different model architectures. For SD15-based models, our SD15Ray achieves the best performance across 1-8 sampling steps, outperforming both the baseline and other acceleration methods like LCM and TCD. In SDXL variants, while SDXL-PeRFlow shows superior single-step performance, our SDXL-Ray demonstrates better overall efficiency in multi-step scenarios, achieving FID scores for 2-8 steps. For PixArt architectures, PixArt-Ray maintains the leading position with the best FID and aesthetic scores. Notably, all our variants consistently show better performance, indicating that our acceleration approach maintains image quality while reducing computational requirements. Quantitative Comparison Between Different Noise Matching Methods. We evaluate various noise matching methods derived from ReFlow [23] across SD15 and SDXL architectures, where shown in Tab.2. On both architectures, our method consistently outperforms existing approaches (ReFlow [23], PeRFlow [51], RDiff [24], and InstaFlow [25]) across multiple metrics. For SD15, RayFlow achieves the best score of most metrics, while for SDXL, Model Type FID CLIP Score Aes Score Image Reward Pick Score SD15-Base (25 step) UNet 5.08 31.16 5.85 SD15-ReFlow [23] UNet 5.92 26.63 5.47 SD15-PeRFlow [57] UNet 5.56 28.77 5.61 SD15-ReDiff [44] UNet 5.69 30.31 5.41 SD15-InstaFlow [25] LoRA 6.10 29.45 5.76 SD15-Ray (ours) LoRA 5.10 31.94 5.92 SDXL-Base (25 step) UNet 4.28 35.15 5.86 SDXL-ReFlow [23] UNet 4.95 31.95 5.84 SDXL-PeRFlow [57] UNet 4.99 30.21 5.62 SDXL-ReDiff [44] UNet 4.51 34.02 5.24 SDXL-InstaFlow [25] LoRA 4.69 30.81 5.57 SDXL-Ray (ours) LoRA 4.15 34.39 5.96 0.193 0.175 0.137 0.184 0.128 0. 0.905 0.624 0.630 0.785 0.442 1.021 0.215 0.225 0.225 0.204 0.223 0.228 0.222 0.235 0.233 0.220 0.240 0.251 Table 2. Quantitative comparisons on SD15, SDXL architectures between different noisy pair matching methods (two step). it attains state-of-the-art performance. Notably, our LoRAbased implementation demonstrates superior performance in image reward, indicating better preservation of image quality during the distillation process compared to UNetbased approaches. Qualitative Comparison. Fig.5 presents visual comparison between RayFlow and several state-of-the-art textincluding SDXL (50 NFEs), SDXLto-image models, Turbo, SDXL-Lightning, Hyper-SDXL, and DMD2. The comparison spans across diverse scenarios - portrait photography, urban landscapes, dynamic pet captures, and detailed animal close-ups. RayFlow demonstrates remarkable image generation capabilities, particularly in preserving fine details and maintaining visual coherence. Even 7 Figure 5. Qualitative comparison of RayFlow against other few-step text-to-image models. Please zoom in to check details, lighting, and aesthetic performances. All methods that do not have an NFE in place default to 4. Module Tϑ ϵµ Clip Aes COCO-5k Img.R FID Clip Aes Img.R FID ImageNet Stable Diffusion V1.5 Ablation 31.94 5.92 29.75 5.62 30.96 5.66 0.205 0.197 0.198 5.10 34.24 5.47 5.79 31.14 5.30 5.89 31.96 5.00 Stable Diffusion XL Ablation 34.39 5.96 32.67 5.48 31.97 5.72 1.021 0.980 0.937 4.05 36.06 5.65 4.77 32.77 5.31 4.85 33.90 5.09 PixArt DiT Ablation 33.18 5.84 31.23 5.61 30.23 5.43 0.258 0.230 0. 4.78 36.24 5.71 5.46 34.43 5.25 5.50 33.29 5.42 0.214 0.197 0.196 0.985 0.897 0.963 0.814 0.792 0.764 2.42 3.18 3.29 1.98 2.92 2. 2.12 2.93 3.04 Table 3. Performance (two steps) of ablation studies under different settings. ϵµ represents the RayFlow. in its single-NFE configuration, RayFlow produces images with comparable or superior quality to models requiring multiple steps. The results align with our quantitative findings in Tab.1, where RayFlow achieves exceptional aesthetic scores. These comprehensive evaluations establish RayFlow as leading solution in high-efficiency text-toimage generation, effectively balancing quality with computational efficiency. Additional comparative results can be found in our supplementary materials. Ablation Studies. We evaluate the effectiveness of two key components: the Time Sampler (Tϑ) and the RayFlow 8 trajectory (ϵµ). Experiments across SD15, SDXL, and PixArt architectures show that using ϵµ alone yields limited improvements, while Tϑ alone achieves better but suboptimal results. The combination of both components consistently achieves the best performance, with significant improvements in evaluation metrics, demonstrating their complementary effects in enhancing generation quality. 5. Conclusion In this work, we introduce RayFlow, novel diffusion framework designed to address the inherent limitations of traditional diffusion models and existing acceleration techniques. By guiding samples along unique paths towards pre-computed target means, RayFlow ensures consistent expectations, minimizes path overlap, and improves sampling stability, all theoretically grounded in path probability maximization. Our Time Sampler, utilizing Stochastic Stein Discrepancies for importance sampling, further optimizes training efficiency by identifying crucial timesteps. Extensive experiments demonstrate RayFlows superior performance in terms of sample quality, generation speed, and computational efficiency compared to existing acceleration methods. Future work includes extending RayFlow to other data modalities and exploring advanced strategies for target mean determination."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, and Adam et al. Letts. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. 1 [3] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. 6 [4] Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, et al. Digital life project: Autonomous In Proceedings of 3d characters with social intelligence. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 582592, 2024. 1 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 6, 7 [6] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-delta: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. 7 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, ImageNet: large-scale hierarchical imand Li Fei-Fei. age database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255. IEEE, 2009. 6 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, and Frederic et al. Boesel. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [9] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024. 1 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 68406851, 2020. 1, 2, 3, 12 [11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben ImPoole, Mohammad Norouzi, and David et al. Fleet. agen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [12] Kang et al. Distilling diffusion models into conditional gans. ECCV, 2024. 1 [13] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 6 [14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6 [15] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. 1 [16] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 6, 7 [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [19] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 1 [20] Qiang Liu. Rectified flow: marginal preserving apURL https://arxiv. transport, 2022. proach to optimal org/abs/2209.14577, 2022. 3 [21] Qiang Liu and Dilin Wang. Stein variational gradient descent: general purpose bayesian inference algorithm. Advances in neural information processing systems, 29, 2016. 5 [22] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 1 [23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, 3, 7 [24] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 7 [25] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 1, 7 [26] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 1 [27] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 7 [28] Weijian Luo, Zemin Huang, Zhengyang Geng, J. Zico One-step diffusion distillaarXiv preprint Kolter, and Guo-jun Qi. tion through score implicit matching. arXiv:2410.16794, 2023. 1 9 [29] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [30] Mei et al. Codi: Conditional diffusion distillation for higher-fidelity and faster image generation. CVPR, 2024. 1 [31] Nguyen et al. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. CVPR, 2024. 1 [32] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. [34] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 6, 7 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 6, 7 [36] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [37] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. 1 [38] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 1, 7 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 6 [40] Huiyang Shao, Qianqian Xu, Peisong Wen, Peifeng Gao, Zhiyong Yang, and Qingming Huang. Building bridge across the time: Disruption and restoration of murals in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2025920269, 2023. 1 [41] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Confernonequilibrium thermodynamics. ence on Machine Learning, pages 22562265. PMLR, 2015. 1 [42] Yang Song and Stefano Ermon. Generative modeling by In Advances estimating gradients of the data distribution. in Neural Information Processing Systems, pages 11895 11907, 2019. [43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn International Conference on Learning Representions. tations, 2021. 1 [44] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 1, 7 [45] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. 1 [46] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. 1 [47] Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. 1 [48] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. arXiv preprint arXiv:2405.16852, 2024. [49] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Patrick Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. EM distillation for one-step diffusion models. arXiv preprint arXiv:2405.16852, 2024. 1 [50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 6 [51] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. 1, 7 [52] Tianwei Yin and et al Gharbi. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 7 [53] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 1 [54] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 1 [55] Linfeng Zhang and Kaisheng Ma. Accelerating diffusion arXiv models with one-to-many knowledge distillation. preprint arXiv:2410.04191, 2024. 1 [56] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of difIn International confusion models via operator learning. ference on machine learning, pages 4239042402. PMLR, 2023. 1 10 [57] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 7 [58] Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen, and Siwei Lyu. Simple and fast distillation of diffusion models. arXiv preprint arXiv:2409.19681, 2024."
        },
        {
            "title": "List of Appendix",
            "content": "1. Introduction 2. Preliminaries 2.1. Denoising Diffusion Probabilistic Models [10] 2.2. Simulation-Free Flow Matching . . 2.3. Flow Trajectories . . . 2.4. Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3. Proposed Method 3.1. RayFlow . . . 3.2. Timestep Sampling . . 3.3. Algorithms . . . . . . . . . . . . . 4. Experiment 4.1. Implementation Details . . 4.2. Main Results . . . . . . . . . . 5. Conclusion A. RayFlow . . A.1. Flow Trajectory . A.2. Probability Path . . A.3. Optimal Probability Path . . A.4. Optimal Denoiser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Timestep Importance Sampling . . B.1. Stein Identity . . B.2. Stein Discrepancy . . . . . . . . . . . . . . . . . . . . . . . B.2.1 Basic Definition . . B.2.2 Stein Discrepancy Measure . B.2.3 RKHS Framework . . B.2.4 Kernelized Stein Discrepancy (KSD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3. Optimal sampling Distribution . . . . 1 2 2 3 3 3 3 3 4 6 6 6 8 . . . . . . . . . 13 . 13 . 16 . 18 . 21 . 21 . 21 . 21 . 22 . 22 . 22 . 22 . . . . . . . . . . . . . . . . . . . . 12 A. RayFlow A.1. Flow Trajectory Proposition 1. Given data x0, pretrained mean ϵµ Rd and variance σ R>0, and target diffusion p(xT ) = (ϵµ, σ2I), we can describe the diffusion process with the following Markov chain. Flow Trajectories. Define probability flow path. ψt(ϵ) = αtx0 + (1 αt)ϵµ + 1 αtϵ Forward Process. Add noise to image data. p(xtxt1, ϵµ) = (cid:0)αtxt1 + (1 αt)ϵµ, β σ2I(cid:1) Backward Process. Denoise from image data. p(xt1xt, ϵµ) = (cid:18) 1 αt xt 1 αt αt (cid:19) ϵµ, βtσ2I βt = (cid:18) (1 α2 )(1 αt1) 1 αt (cid:19) Proof. We split the proof process into two parts: Forward Process and Backward Process. We set α2 + β2 = 1 and αt = (cid:81)t i=1 α2 Forward Process , According to the forward process, we have: (24) (25) (26) xt = αtxt1 + (1 αt)ϵµ + βtϵt = αt(αt1xt2 + (1 αt1)ϵµ + βt1ϵt1) + (1 αt)ϵµ + βtϵt = αtαt1 α1x0 + αtαt1 α2(1 α1)ϵµ + + αtαt1(1 αt2)ϵµ + αt(1 αt1)ϵµ + (1 αt)ϵµ + αtαt1 α2β1ϵ1 + αtαt1 α3β2ϵ2 + + αtβt1ϵt + βtϵt (27) = αtαt1 α1x0 + (1 αtαt1 α1)ϵµ + αtαt1 α2β1ϵ1 + αtαt1 α3β2ϵ2 + + αtβt1ϵt + βtϵt where αtαt1 α2β1ϵ1 (0, α2 αtαt1 α3β2ϵ2 (0, α2 α2 α t1 α2 t1 α2 2β2 3β2 1 σ2I) 2 σ2I) we set α2 + β = 1, then we have: αtαt1 α2β1ϵ1 (0, α2 αtαt1 α3β2ϵ2 (0, α2 α2 α2 t1 α2 t1 α2 2(1 α2 3(1 α2 1)σ2I) 2)σ2I) based on the properties of Gaussian distributions: (µ1, σ2 1I), (µ2, σ2 2I), + (µ1 + µ2, (σ 1 + σ2 2)I), then α2 α2 α2 = α2 α2 = α2 t1 α2 t1 α2 t1 α2 2(1 α2 2σ2 α2 3σ2 α 1)σ2 + α2 α2 α2 α2 t1 α2 t1 α2 t1 α2 1σ2 + α2 1σ2. 3(1 α2 α2 2)σ2 t1 α2 3σ2 α α2 t1 α2 2σ2 13 (28) (29) (30) (31) (32) αtαt1 α2β1ϵ1 + αtαt1 α3β2ϵ2 t1 α2 t1 α2 (0, (α2 3 α2 α α2 1)σ2I). Lets expand it to t-dimension: αtαt1 α2β1ϵ1 + αtαt1 α3β2ϵ2 + βtϵt = ϵt ϵt (cid:0)0, (1 α2 1)σ2I(cid:1) , t1 α α2 we have xt = αtαt1 α1x0 + (1 αtαt1 α1)ϵµ + (cid:113) 1 α2 α t1 α2 1 ϵt. Since, αt = α2 α2 t1 α2 1, then we can get: p(xtx0, ϵµ) = (cid:0) αtx0 + (1 αt)ϵµ, (1 αt)σ2I(cid:1) Finally, we get: (Noise Process) (Noise Injection) (Noise Process) (Noise Injection) p(xtxt1, ϵµ) = (cid:0)αtxt1 + (1 αt)ϵµ, β2 p(xtx0, ϵµ) = (cid:0) σ2I(cid:1) αt)ϵµ, (1 αt)σ2I(cid:1) αtx0 + (1 xt = αtxt1 + ϵt, ϵt ((1 αt)ϵµ, β2 αtx0 + ϵt, ϵt ((1 xt = σ2I) αt)ϵµ, (1 αt)σ2I) where 0 < αt < 1 and 0 < βt < 1, σ R>0. Backward Process During the backward process, we are focus on the reverse probability distribution p(xt1xt, ϵµ). Hence, we have: p(xt1xt, ϵµ) = p(xtxt1, ϵµ)p(xt1x0, ϵµ) p(xtx0, ϵµ) (cid:0)αtxt1 + (1 αt)ϵµ, (1 α2 = )σ2I(cid:1) (cid:0) ( αtx0 + (1 αt1x0 + (1 αt)ϵµ, (1 αt)σ2I) αt1)ϵµ, (1 αt1)σ2I(cid:1) to the Due 1 (2π)k det(Σ) fact exp (cid:0) 1 that 2 (x µ)T Σ1(x µ)(cid:1), where Rk and (µ, Σ) the multi-varite gaussian distribution has probability density function p(x) p(xt1xt, ϵµ) exp (cid:26) 1 2σ2 (cid:20) (xt αtxt1 (1 αt)ϵµ)2 (1 α2 ) αtxt (1 (1 αt) (xt αt)ϵµ)2 (cid:21)(cid:27) (xt1 + αt1x0 (1 (1 αt1) αt1)ϵµ) (33) (34) (35) (36) (37) (38) (39) = (40) Since p(xt1xt, ϵµ) is w.r.t. xt1, we can convert this pdf to simplified formulation (by rearanging terms which has no realtionship with xt1 to C(xt, x0, ϵµ)): p(xt1xt, ϵµ) exp (cid:40) (cid:34) 1 2σ2 x2 + α2 t1 + (1 α)2ϵ2 µ 2αtxtxt1 2(1 αt)xtϵµ + 2αt(1 αt)xt1ϵµ x2 t1 + αt1x2 0 + (1 αt1)2ϵ2 µ 2 + 1 α2 αt1x0xt1 (2 2 1 αt αt1)xt1ϵµ + 2( αt1 + αt1)x0ϵµ (cid:26) = exp 1 2σ2 +C(xt, x0, ϵµ)]} (cid:20) α2 x2 t1 2αtxtxt1 + 2αt(1 αt)xt1ϵµ 1 α2 +C(xt, x0, ϵµ)]} 14 + x2 t1 αt1x0xt1 (2 2 αt1)xt1ϵµ 1 αt1 (41) Then we can rerange the remaining terms to construct gaussian distribution formulation which associated with xt1. exp 1 2σ2 (cid:18) α2 1 α2 (cid:19) + 1 1 αt1 x2 t1 2 (cid:18) αtxt αt(1 αt)ϵµ 1 α2 (cid:124) + αt1x0 + (1 1 αt1 αt1)ϵµ xt (cid:19) (cid:125) p(xt1xt, ϵµ) = exp (cid:26) = exp = exp (cid:26) = exp 1 2σ2 +C(xt, x0, ϵµ)]} (cid:20)(cid:18) α2 α2 (1 α2 αt1 + 1 α2 )(1 αt1) (cid:19) 1 2σ2 (cid:18) (cid:124) 1 αt )(1 αt1) (1 α2 (cid:123)(cid:122) (a) x2 (cid:19) (cid:125) x2 t1 2(b)xt1 + C(xt, x0, ϵµ) t1 2(b)xt1 + C(xt, x0, ϵµ) (cid:123)(cid:122) (b) (cid:21)(cid:27) (cid:20) x2 t1 2 (b) (a) xt1 + C(xt, x0, ϵµ) (cid:21)(cid:27) 1 2σ2/(a) (cid:16) xt1 (b) (a) 2σ2(a) (cid:17)2 + C(xt, x0, ϵµ) (µp(xt, x0, ϵµ), Σp(t)I) , µp(xt, x0, ϵµ) = (b) (a) , Σp(t) = σ2/(a) We can expand the µp(xt, x0, ϵµ) (42) µp(xt, x0, ϵµ) (1 α2 )(1 αt1) 1 αt αt(1 αt1)xt + αt(1 αt1)xt + αt(1 αt1)xt + (αtxt αt(1 αt)ϵµ)(1 αt1) + ( αt1x0 + (1 αt1)ϵµ)(1 α2 ) (αtxt αt(1 αt)ϵµ)(1 αt1) + ( (1 α2 αt1x0 + (1 )(1 αt1) αt1)ϵµ)(1 α2 ) αt1(1 α2 αt α αt1 + αt αt1 + (1 α2 1 αt )x0 + [α2 αt1 + αt1α2 )]ϵµ αt1(1 α2 )x0 + [1 αt α2 αt1 + αt αt1 1 αt αt1 + αt1α2 ]ϵµ αt1(1 α )x0 1 αt + 1 αt 1 αt α2 αt1 + αt αt1 αt1 + αt1α2 (cid:124) 1 αt (cid:123)(cid:122) (c) ϵµ (cid:125) (43) αt1(1 α2 ) 1 αt αt1(1 α2 ) 1 αt αt1(1 α2 ) 1 αt αt1(1 α2 ) 1 αt x0 + αt(1 αt1) 1 αt xt + (c) xt E[ϵt] αt + αt(1 αt1) 1 αt xt αt xt αt + + αt(1 αt1) 1 αt αt(1 αt1) 1 αt αt)ϵµ, (1 αt)σ2I) xt + (c), ϵt ((1 xt xt (cid:124) αt1(1 α2 ) αt (1 αt) αt1(1 α2 )(1 (1 αt) αt (cid:123)(cid:122) (d) E[ϵt] + (c) αt) ϵµ +(c) (cid:125) By reranging the terms, we can get: 15 = = = = = = = = = µp(xt, x0, ϵµ) = = = = = (cid:20) + αt1(1 α2 ) (1 αt) αt (cid:20) 1 α2 (1 αt)αt (1 α2 αt (1 αt1) 1 αt (cid:21) αt (1 αt1) 1 αt (1 αt1) + (cid:21) xt + (d) + (c) xt + (d) + (c) ) + α2 (1 αt) αt + α2 αt (1 αt) αt 1 α2 1 αt xt + (d) + (c) xt + (d) + (c) xt + (d) + (c) (44) (1 αt α2 αt1 + αt αt1 αt1 + αt αt1(1 α )(1 αt) ϵµ αt αt1α2 ) αt1(1 α2 )(1 αt) ϵµ αt (1 αt) αt1 + αt αt1 + αt1α2 ) ϵµ (c) + (d) = = = = = = = = = αt1α2 ) αt (1 αt) αtαt αt1 αtαt αtαt αtα2 αt1 + αtα2 αt1 + ( ( ( αt αt αt αtα2 αt1 + (αt αt1 (1 αt) αtα2 αt αtαt αt1 (1 αt) αtαt αt1 αt αt1) ϵµ αt1) ϵµ αt1 + (1 αt) αtαt αt1 αt αt1) ϵµ (1 αt)( (1 αt) αtαt αt1 αt αt αt/αt αt (1 αt)( (1 αt) (1 αt)(αt 1) (1 αt)αt ϵµ αt 1 αt ϵµ αt/αt) ϵµ Then we can get: Finally, we have µp(xt, x0, ϵµ) = 1 αt xt 1 αt αt ϵµ (Noise Process) p(xt1xt, ϵµ) = (cid:18) 1 αt xt 1 αt αt ϵµ, (cid:18) (1 α2 )(1 αt1) 1 αt (cid:19) (cid:19) σ2I (Noise Process) xt1 = 1 αt xt ϵt, ϵt (cid:18) 1 αt αt ϵµ, (cid:18) (1 α2 )(1 αt1) 1 αt (cid:19) (cid:19) σ2I (45) (46) (47) (48) A.2. Probability Path Proposition 2. For any general diffusion which defined in Prop.1, we have the path probability (the probability of starting from ˆx0 forward to ˆϵµ and backward to ˆx0) p( ˆx0 ˆϵµ ˆx0) = p(xT = ˆϵµx0 = ˆx0)p(x0 = ˆx0xT = ˆϵµ) (49) 16 where and p(x0xT = ˆϵµ) = (cid:32) ˆϵµ αT + (cid:88) s=1 (e)s + (c)s (cid:112)αs1/αt , (cid:88) s=1 βs αt αs (cid:33) σ2I (e)s = αs1(1 α2 s) αs (1 αs) E[ϵs] (c)s = 1 αs α2 αs1 + αt αs1 1 αs αs1 + αs1α2 ϵµ Proof. According to Eq.(43), we have µp(xt, x0, ϵµ) = = αt1(1 α2 ) 1 αt αt1(1 α2 ) αt (1 αt) + xt E[ϵt] αt αt(1 αt1) 1 αt xt + αt(1 αt1) 1 αt xt (cid:124) )E[ϵt] αt1(1 α2 αt (1 αt) (cid:123)(cid:122) (e)t (cid:125) +(c)t xt + (c)t, ϵt ((1 αt)ϵµ, (1 αt)σ2I) = 1 αt xt + (e)t + (c)t, Then we can write the backward process probability: p(xt1xt, xT = ˆϵµ) = xt + (e)t + (c)t, βtσ2I (cid:19) (cid:18) 1 αt For the convenience of subsequent analysis, we start from timestep p(xT 1xT = ˆϵµ) = (cid:18) 1 αT ˆϵµ + (e)T + (c)T , βT σ2I (cid:19) (50) (51) (52) (53) (54) (55) Then we can obtain the marginal distribution p(xT 2xT = ˆϵµ). p(xT 2xT = ˆϵµ) = = (cid:90) Rn (cid:90) Rn p(xT 2xT 1)p(xT 1xT = ˆϵµ)dxT 1 (cid:18) xT 1 αT 1 + (e)T 1 + (c)T 1, βT 1σ2I (cid:19) (cid:18) 1 αT ˆϵµ + (e)T + (c)T , βT σ2I (56) (cid:19) dxT 1 Its hard to calculate p(xT 2xT = ˆϵµ) directly. However, we can ensure that p(xT 2xT = ˆϵµ) is gaussian distribution. Hence, we just need to calculate the paramter of gaussian distribution (mean and variance). ExT 2p(xT 2xT =ˆϵµ)[xT 2xT = ˆϵµ] = ExT 2p(xT 2xT =ˆϵµ)[ExT 1p(xT 1xT =ˆϵµ)[xT 2xT 1]xT = ˆϵµ] (cid:20) xT 1 αT 1 + (e)T 1 + (c)T 1xT 1 (cid:21) (cid:21) xT = ˆϵµ (57) ˆϵµ + (e)T + (c)T (cid:19) (cid:21) + (e)T 1 + (c)T = ExT 2p(xT 2xT =ˆϵµ) = ExT 2p(xT 2xT =ˆϵµ) (cid:20) ExT 1p(xT 1xT =ˆϵµ) (cid:20) 1 αT 1 (cid:18) 1 αT = ˆϵµ αT αT + (e)T + (c)T αT 1 + (e)T 1 + (c)T 1 17 Due to the fact that V[Y ] = E[V[Y X]] + V[E[Y X]], hence, we can caculate the variance by: VxT 2p(xT 2xT =ˆϵµ)[xT 2xT = ˆϵµ] = ExT 1p(xT 1xT =ˆϵµ) + VxT 1p(xT 1xT =ˆϵµ) (cid:2)VxT 2p(xT 2xT 1)[xT 2xT 1]xT = ˆϵµ (cid:3) (cid:2)ExT 2p(xT 2xT 1)[xT 2xT 1]xT = ˆϵµ (cid:3) = ExT 1p(xT 1xT =ˆϵµ) (cid:104) βT 1σ2IxT = ˆϵµ (cid:105) + VxT 1p(xT 1xT =ˆϵµ) (cid:20) 1 αT 1 xT 1 1 αT 1 αT (cid:21) ϵµxT = ˆϵµ = βT 1σ2I + (cid:32) = βT 1 + (cid:18) 1 (cid:19)2 αT 1 (cid:33) σ2I βT α2 βT σ2I where βt = (cid:16) (1α2 )(1 αt1) 1 αt (cid:17) , After obtaining the paramter of p(xT 2xT = ˆϵµ), we can get p(xT 2xT = ˆϵµ) = (cid:32) ˆϵµ (cid:112)αT /αT 2 + (e)T + (c)T αT 1 + (e)T 1 + (c)T 1, βT 1 + (cid:32) (cid:33) (cid:33) σ2I βT α 1 Its easy to get the general term p(xt1xT = ˆϵµ) by mathematical induction. p(xt1xT = ˆϵµ) = (cid:32) ˆϵµ (cid:112)αT /αt1 + (cid:88) s=t (e)s + (c)s (cid:112)αs1/αt1 , (cid:88) s=t βs αt αs (cid:33) σ2I Finally, we can inference the probability density distribution at the timestep 0. p(x0xT = ˆϵµ) = (cid:32) ˆϵµ αT + (cid:88) s=1 (e)s + (c)s (cid:112)αs1/αt1 , (cid:88) s=1 βsα1 αs (cid:33) σ2I (58) (59) (60) (61) A.3. Optimal Probability Path Theorem 1. Let Sϵ = {ϵt}T (ϵµ, σ2I). For sample ˆx0, we can obtain the optimal parameters that maximize the path probability. t=1 be the noise added in the forward process, ϵµ, σ be the parameters of the target distribution arg max Sϵ,ˆϵµ,ϵµ,σ p(x0 = ˆx0xT = ˆϵµ) (cid:89) t=1 p(ϵt = E[ϵt]) where the optimal parameters are defined by: ϵ = {(1 ˆϵ µ = t=1, σ 0 αt)ϵµ}T αT )ϵµ, ϵ αT ˆx0 + (1 µ = Et[E[ ϵt]] Proof. According to Eq.(45), we can rewrite the backward process probability distribution. p(xt1xt, xT = ˆϵ µ) = (cid:18) 1 αt xt 1 αT αT (cid:19) µ, βt(σ)2I ϵ For the convenience of subsequent analysis, we start from timestep p(xT 1xT = ˆϵ µ) = (cid:18) 1 αT ˆϵ µ 1 αT αT (cid:19) µ, βT (σ)2I ϵ 18 (62) (63) (64) (65) Then we can obtain the marginal distribution p(xT 2xT = ˆϵ µ). p(xT 2xT = ˆϵ µ) = = (cid:90) Rn (cid:90) Rn p(xT 2xT 1)p(xT 1xT = ˆϵ µ)dxT 1 (cid:18) xT 1 αT 1 1 αT 1 αT 1 µ, βT 1(σ)a2I ϵ (cid:19) (cid:18) 1 αT ˆϵµ 1 αT αT µ, βT (σ)2I ϵ (cid:19) dxT 1 (66) Similar to Prop.2. Its hard to calculate p(xT 2xT = ˆϵ gaussian distribution. Hence, we just need to calculate the paramter of gaussian distribution (mean and variance). µ) directly. However, we can ensure that p(xT 2xT = ˆϵ µ) is ExT 2p(xT 2xT =ˆϵ = ExT 2p(xT 2xT =ˆϵ µ)[xT 2xT = ˆϵ µ] µ)[ExT 1p(xT 1xT =ˆϵ = ExT 2p(xT 2xT =ˆϵ µ) = ExT 2p(xT 2xT =ˆϵ µ) (cid:20) ExT 1p(xT 1xT =ˆϵ µ) (cid:20) 1 αT 1 (cid:18) 1 αT ˆϵ µ µ)[xT 2xT 1]xT = ˆϵ µ] 1 αT 1 αT 1 1 αT 1 αT 1 (cid:20) xT 1 αT 1 (cid:19) 1 αT αT ϵ µ ϵ µxT 1 (cid:21) (cid:21) xT = ˆϵ µ ϵ µxT (cid:21) (67) = = ˆϵ µ αT αT 1 ˆϵ µ (cid:112)αT /αT 2 and for the variance, we have ϵ µ 1 αT αT 1 αT αT 1 1 (cid:112)αT /αT 2 (cid:112)αT /αT 2 ϵ µ µ)[xT 2xT = ˆϵ µ] VxT 2p(xT 2xT =ˆϵ = ExT 1p(xT 1xT =ˆϵ µ) + VxT 1p(xT 1xT =ˆϵ µ) (cid:2)VxT 2p(xT 2xT 1)[xT 2xT 1]xT = ˆϵ (cid:3) (cid:2)ExT 2p(xT 2xT 1)[xT 2xT 1]xT = ˆϵ µ µ (cid:3) = ExT 1p(xT 1xT =ˆϵ µ) (cid:104) βT 1σ2IxT = ˆϵ µ (cid:105) + VxT 1p(xT 1xT =ˆϵ µ) (cid:20) 1 αT 1 xT 1 1 αT 1 αT 1 (cid:21) ϵ µxT = ˆϵ µ = βT 1(σ)2I + (cid:32) = βT 1 + βT α2 1 (cid:18) (cid:19)2 αT 1 (cid:33) (σ)2I βT (σ)2I After obtaining the paramter of p(xT 2xT = ˆϵ µ), we can get p(xT 2xT = ˆϵµ) = (cid:32) ˆϵµ (cid:112)αT /αT 2 1 (cid:112)αT /αT 2 (cid:112)αT /αT 2 (cid:32) ϵµ, βT 1 + (cid:33) (cid:33) σ2I βT α 1 Its easy to get the general term p(xt1xT = ˆϵµ) by mathematical induction. p(xt1xT = ˆϵµ) = (cid:32) ˆϵµ (cid:112)αT /αt1 1 (cid:112)αT /αt1 (cid:112)αT /αt1 ϵµ, (cid:88) s=t βs αt αs (cid:33) σ2I 19 (68) (69) (70) Finally, we can inference the probability density distribution at the timestep 0. (cid:33) (σ)2I βs αt αs p(x0xT = ˆϵ µ) = (cid:32) ˆϵ µ αT 1 αT αT (cid:32) = αT ˆx0 + (1 αT (cid:88) ϵ µ, s=t αT )ϵ µ (cid:33) (cid:32) = ˆx0, (cid:88) s=t βs αt αs (σ)2I = (0, I) 1 αT αT ϵ µ, (cid:88) s=t βs αt αs (cid:33) (σ)2I Hence, we can induce p(x0 = ˆx0xT = ˆϵ µ) = 1 p(x0 = ˆx0xT = ˆϵµ) p(x0 = ˆx0xT = ˆϵ µ) is the optimal probability path, and ϵ , ˆϵ µ, ϵ µ, σ are optimal parameters. (71) (72) A.4. Optimal Denoiser Let us assume that out training set consists of finite number of samples {x(1) {ϵ(1) µ }. This implies p(x0) is representsed by mixture of Dirac delta distributions: µ , , ϵ(K) 0 , , x(K) 0 } and target mean set p(x0) = 1 K (cid:88) i=1 δ(x0 x(i) 0 ) (73) Let us now consider the denoising loss. By expanding the expectations, we can rewrite the formula as an integral over the noisy samples 0 p(x0)[Extp(xtx0)[ϵθ(xt) ϵ(i) µ 2 2]] x(i) (cid:20)(cid:90) ( αtx(i) 0 + (1 αt)ϵ(i) µ , (1 αt)I)ϵθ(xt) ϵ(i) µ 2 2 dxt = x(i) 0 p(x0) 1 (cid:90) = = (cid:88) (cid:90) Rn (cid:88) i=1 i=1 1 (cid:124) Rn Rn ( αtx(i) 0 + (1 ( αtx(i) 0 + (1 αt)ϵ(i) µ , (1 αt)I)ϵθ(xt) ϵ(i) µ 2 2 dxt αt)ϵ(i) µ , (1 αt)I)ϵθ(xt) ϵ(i) µ 2 dxt (cid:123)(cid:122) L(ϵθ) (cid:125) ϵ θ = arg min ϵθ L(ϵθ) (cid:21) (74) (75) We can minimize L(ϵθ) independently for each xt: This is convex optimization problem; its solution is uniquely identified by setting the gradient w.r.t. ϵθ to zero: ϵθ L(ϵθ) (cid:34) = ϵθ (cid:34) (cid:34) (cid:34) 1 1 1 = = = 1 ( αtx(i) 0 + (1 (cid:88) i=1 αt)ϵ(i) µ , (1 αt)I)ϵθ(xt) ϵ(i) µ 2 2 (cid:35) ( αtx(i) 0 + (1 ( αtx(i) 0 + (1 ( αtx(i) 0 + (1 (cid:88) i=1 (cid:88) i=1 (cid:88) i= (cid:35) αt)ϵ(i) µ , (1 αt)I) ϵθ ϵθ(xt) ϵ(i) µ 2 2 (cid:35) αt)ϵ(i) µ , (1 αt)I) (2ϵθ(xt) 2ϵ(i) µ ) (cid:35) αt)ϵ(i) µ , (1 αt)I) ϵθ(xt) (cid:34) 1 (cid:88) i=1 ( αtx(i) 0 + (1 (cid:35) µ , (1 αt)I) αt)ϵ(i) ϵ(i) µ (76) Then we can solve the optimal denoiser by letting ϵθ L(ϵθ) = 0 ϵ θ(xt) = (cid:80)K 1 i=1 ( (cid:80)K i=1 ( αtx(i) 0 + (1 αtx(i) 0 + (1 αt)ϵ(i) µ , (1 αt)I)ϵ(i) µ µ , (1 αt)I) αt)ϵ(i) 1 B. Timestep Importance Sampling (77) B.1. Stein Identity The KSD is thus given by the norm of β in Hd: Lemma 1. For any bounded and smooth function (x), we have Ep[sq(x)f (x) + xf (x)] = 0, where sq(x) = ln q(x) is the score function of distribution q. Proof. Expanding the expectation: (cid:90) sq(x)f (x)p(x)dx + (cid:90) xf (x)p(x)dx = 0 Lets focus on the first term using integration by parts: (cid:90) sq(x)f (x)p(x)dx = (cid:90) ln q(x)f (x)p(x)dx Let = (x) and dv = ln q(x)p(x)dx. Then du = xf (x)dx and = (cid:82) ln q(x)p(x)dx. Applying integration by parts: (78) (79) (cid:90) sq(x)f (x)p(x)dx = (x) (cid:20) (cid:90) ln q(x)p(x)dx (cid:21) (cid:90) (cid:18)(cid:90) xf (x) ln q(x)p(x)dx dx (80) (cid:19) Let = (cid:82) ln q(x)p(x)dx. Substituting back into Eq.(78): (cid:90) sq(x)f (x)p(x)dx + (cid:90) xf (x)p(x)dx = [f (x)A] (cid:90) xf (x)(p(x) A)dx (81) Now, lets examine the two terms on the right-hand side: 1. [f (x)A] : This term vanishes because (x) is bounded and q(x) approaches zero at infinity, implying also approaches zero at infinity. 2. (cid:82) xf (x)(p(x) A)dx: This term equals zero if and only if p(x) = for all x. Since is constant, we have: This implies: xp(x) = xA = 0 xp(x) p(x) = ln q(x) Which is true if and only if p(x) = q(x), as ln p(x) = xp(x) p(x) Therefore, when p(x) = q(x), both terms on the right-hand side are zero, proving the theorem. . B.2. Stein Discrepancy Lets develop the concept of Stein Discrepancy as measure of distance between probability distributions. B.2.1 Basic Definition Consider two smooth distributions p(x) and q(x) on Rd. The Stein score function for distribution is defined as: fundamental property (proved in Sec.B.1) states that: sq(x) = ln q(x) Ep[sq(x)f (x) + xf (x)] = holds if and only if p(x) = q(x), where is smooth function. 21 (82) (83) (84) (85) B.2.2 Stein Discrepancy Measure Based on this property, we can define discrepancy measure between distributions: where is the set of smooth functions. This measure is positive when = q, but is generally difficult to compute directly. S(p, q) = max (Ep[sq(x)f (x) + xf (x)])2 (86) B.2.3 RKHS Framework To make the discrepancy measure computationally tractable, we can utilize Reproducing Kernel Hilbert Space (RKHS) theory. Lets recall key RKHS concepts: For positive definite kernel k(x, y), Mercers theorem gives its spectral decomposition: k(x, y) = (cid:88) λjej(x)ej(y)T (87) where ej are orthogonal basis functions and λj are eigenvalues. The RKHS generated by kernel k(, ) : has two key properties: k(x, ) for any Reproducing property: (x) = f, k(x, )H for any We extend this to Hd = (d times) for vector-valued functions with inner product: , gHd = (cid:88) fi, giH i=1 B.2.4 Kernelized Stein Discrepancy (KSD) The Stein operator Ap for distribution is defined as: Apf (x) = sp(x) (x) + (x) = 1 p(x) [p(x)f (x)] This operator satisfies: key relationship for KSD is: (cid:90) xX (f (x)p(x))dx = 0 Ep[Aqf (x)] = Ep[Aqf (x) Apf (x)] = Ep[(sq(x) sp(x))f (x)T ] (88) (89) (90) (91) This formulation provides computationally tractable way to measure discrepancy between distributions using kernel methods. B.3. Optimal sampling Distribution Proposition 3. The optimal sampling distribution for Eq.(5) with minimal variance is: where ξt(x0, ϵµ) = ϵθ( αt ˆx0 + (1 αt)ϵµ) ϵµ2 2. which means for any probability distribution p, we have q(tx0, ϵµ) ξt(x0, ϵµ)p(t), (92) Vtq(t),(x0,ϵµ)[ξt(x0, ϵµ)] Vtp(t),(x0,ϵµ)[ξt(x0, ϵµ)] Proof. We aim to minimize the variance: This can be expressed as: Vtq(t),(x0,ϵµ) [ξt(x0, ϵµ)] (cid:90) ξt(x0, ϵµ)p(t)2 q(t) dt 22 subject to (cid:82) q(t) dt = 1. The Lagrangian is: (cid:90) ξt(x0, ϵµ)p(t)2 q(t) (cid:18)(cid:90) dt + λ (cid:19) q(t) dt 1 Taking the derivative with respect to q(t) and setting it to zero gives: Solving for q(t), we get: This implies: ξt(x0, ϵµ)p(t)2 q(t)2 + λ = 0 (cid:114) q(t) = ξt(x0, ϵµ)p(t)2 λ q(t) ξt(x0, ϵµ)p(t) Using the Cauchy-Schwarz inequality, we show: σ2 σ2 Thus, the distribution q(tx0, ϵµ) minimizes the variance for the given problem, proving the proposition."
        }
    ],
    "affiliations": [
        "ByteDance Inc."
    ]
}