{
    "paper_title": "IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models",
    "authors": [
        "Jiayi Lei",
        "Renrui Zhang",
        "Xiangfei Hu",
        "Weifeng Lin",
        "Zhen Li",
        "Wenjian Sun",
        "Ruoyi Du",
        "Le Zhuo",
        "Zhongyu Li",
        "Xinyue Li",
        "Shitian Zhao",
        "Ziyu Guo",
        "Yiting Lu",
        "Peng Gao",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models' performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each model's strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at https://github.com/jylei16/Imagine-e."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 2 9 3 1 . 1 0 5 2 : r IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models Jiayi Lei1,2, Renrui Zhang3, Xiangfei Hu1,2, Weifeng Lin3, Zhen Li3, Wenjian Sun1 Ruoyi Du2, Le Zhuo2, Zhongyu Li2, Xinyue Li2, Shitian Zhao2 Ziyu Guo3, Yiting Lu2, Peng Gao2, Hongsheng Li3 1Shanghai Jiaotong University, 2Shanghai AI Laboratory 3CUHK MMLab Equal Contribution Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "With the rapid development of diffusion models, text-to-image (T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these models performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each models strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at https://github.com/jylei16/Imagine-e."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Task Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Quantitative Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Evaluation 2.1 Structured Output Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5"
        },
        {
            "title": "2.1.1 Code2Table .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.2 Language2Table . .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.3 Code2Chart .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.4 Language2Chart",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.6 Language2Newspaper",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.7 Language2Paper",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.8 Json2Image ."
        },
        {
            "title": "2.1.9 UI Design .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.10 Code Generation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2.1 Multi-Person .",
            "content": "2.2.2 Human body . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.3 Photographic Image Generation . . . . . . . . . . . . . . . . . . . . . . . 2.2.4 Perspective Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2. Physical understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Specific Domain Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Math . 2.3.2 Fractal . . 2.3.3 Medical . . . . . . . . . . . . . 2.3.4 3D Point Cloud . 2.3.5 3D Mesh . 2.3.6 Chemistry . 2.3.7 Biology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.8 Robotics and Simulation Tasks . . . . . . . . . . . . . . . . . . . . . . . . 2.3.9 Autonomous Driving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Challenging Scenario Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Image with Mark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.2 Set of Mark . 2.4.3 Multilingual 2.4.4 Dense OCR . 2.4.5 Emoji . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.6 Irrational Scene Generation . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.7 LLM QA . . 2.4.8 Watermark . . . 2.4.9 Low Quality . 2.4.10 Multi-image . 2.4.11 Text Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Multi-style Creation Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 8 9 9 11 13 15 16 16 20 21 23 26 28 29 30 30 32 33 34 36 37 40 40 41 44 45 47 49 50 54 61 63 2 3 Conclusion"
        },
        {
            "title": "3.1 Summary .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 Task Complexity Analysis",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Quantitative Benchmark Assessment . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "69 69 69 69"
        },
        {
            "title": "Introduction",
            "content": "With the rapid development of large models [21, 87, 90, 45, 44, 88, 28, 24], text-to-image (T2I) diffusion models [62, 63, 67] have emerged, showcasing impressive abilities in prompt following and high-quality image generation, including Imagen[67], Dall-E3 [6], the Stable Diffusion series [64], and Lumina-T2I [22] models, among others. Recently, Black Forest Lab released FLUX.1 [20], and Ideogram2.0 [36] also made its debut, showcasing exceptional performance. Existing evaluation methods [13, 40, 91] often suffer from issues such as overly simple tasks and significant gap between evaluation results and human intuitive perceptions. In contrast, we designed IMAGINEE with detailing and challenging tasks, and scored models using variety of scientific methods for quantitative evaluation. we delve deeply into the capabilities and performance of FLUX.1, Ideogram2.0, and other state-of-the-art T2I models to address the following question: Have T2I models entered new era, and can these breakthroughs lead T2I models toward becoming generalpurpose models? 1.1 Task Overview As more powerful models emerge, T2I models are no longer limited to traditional image generation tasks. They demonstrated remarkable performance in various fields, ranging from text-to-image generation [64, 66, 7, 37, 89], controllable generation [85, 83, 12], and image editing [3, 8, 38] to video [30, 9], audio [42, 32], 3D [26, 27, 25], and motion [71, 86] generation. Beyond generation, recent works have also exhibited diffusion models capabilities in computer vision tasks, such as semantic segmentation [5, 79], depth estimation [39, 43], and image restoration [77]. To this end, we introduced IMAGINE-E, comprehensive evaluation framework designed to benchmark text-to-image (T2I) generation models. Using IMAGINE-E, we selected six representative T2I models for comparison, including FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. These models were chosen based on their maturity, industry recognition, and diversity, encompassing both open-source and closed-source approaches. To scientifically and systematically evaluate these models, we designed five domains to rigorously assess and compare their capabilities. These domains include structured output generation, realism and physical consistency tasks, specific domain generation, challenging scenario generation, and different style image generation. Structured Output Generation: In this task, we focus on evaluating the models ability to generate structured outputs such as tables, figures, and documents. These domains have rarely been specifically tested, making this highly challenging task. It provides substantial measure of the current level of alignment between T2I models and instructions, as well as their generation capabilities. Structured output tasks demand high-level understanding from models, requiring them to comprehend complex structured or natural language inputs while maintaining precise formatting in their output. These tasks also demand that models accurately extract and reproduce textual or numerical information from inputs into outputs. Structured output generation has immense practical applications in design, academic research, education, and more. This is also crucial step for T2I models on their path to becoming foundation models, highlighting their potential as universal visual output interface. Realism and Physical Consistency Tasks: critical criterion for assessing the quality of T2I models is whether the generated images adhere to the fundamental laws and requirements of the physical world. In this task, we rigorously test different T2I models understanding of human anatomy and physical laws. This task seeks to answer broad question: Can AI truly understand the physical world? Do T2I models represent world that abides by the laws of physics, with generated images merely reflecting fragment of that world? Specific Domain Generation: In this task, we carefully design series of prompts from underrepresented academic or research fields to test the models breadth of knowledge. We gather prompts from specialized domains such as mathematics, 3D modeling, and medical fields to evaluate T2I models expertise in these areas. FLUX.1 and Ideogram2.0s remarkable performance in this domain illustrates the expanding utility of T2I models, which hold the potential to contribute significantly to scientific research. Challenging Scenario Generation: To further diversify the difficulty of our evaluations, we have collected wide array of highly challenging tasks. These prompts enhance the 4 diversity of prompt types and complexity, offering more comprehensive assessment of the models abilities and performance. Multi-style Creation Task: In this task, we have meticulously selected over thirty distinct artistic styles and crafted detailed prompts to evaluate the capabilities of T2I models in handling such fundamental tasks. This task assesses the T2I models understanding of various styles, their ability to generalize by integrating elements with significantly different styles, and the aesthetic quality of the images they generate. 1.2 Quantitative Evaluation Criteria In recent years, the development of text-to-image (T2I) models has significantly advanced the field of image generation. To evaluate the quality of these generated results, researchers have proposed various automated evaluation metrics. Among these, the following methods are commonly used: CLIPScore [60]: This method leverages OpenAIs CLIP model to assess image quality by computing the similarity between generated images and their corresponding text descriptions. Its advantage lies in the ability to directly compare text and images, providing contentrelevant evaluations. However, it has limitations, such as lack of sensitivity to subtle artistic styles and compositions, which may lead to inaccurate scoring of high-quality images. HPSv2 [76]: This newer visual quality assessment method aims to combine multiple evaluation dimensions to enhance the accuracy of image quality measurement. Although HPSv2 offers comprehensive quality assessment, there is currently limited literature on the method, and its generalizability and effectiveness are yet to be fully validated. Aesthetic Score [68]: This approach focuses on assessing the aesthetic quality of images by utilizing deep learning models to analyze aspects such as composition and color [81]. While it effectively captures aesthetic features, it is constrained by the limitations of its training data, potentially introducing biases in images with high stylistic diversity. GPT-4o [56]: This study incorporates scoring method based on GPT-4o, utilizing prompt that evaluates the quality of generated images from four aspects: aesthetic appeal and alignment with human preferences, conformance to physical laws and realism, safety, and the degree of matching between the image and the text description. This method leverages the reasoning capabilities of the language model to score the generated results, addressing the shortcomings of the aforementioned methods. Human: Our researchers use the same evaluation criteria as GPT-4o, focusing on four aspects: aesthetic appeal and alignment with human preferences, conformance to physical laws and realism, safety, and the degree of matching between the image and the text description. We conduct detailed scoring of the generation results from six models based on human aesthetic judgments. Additionally, we test the reliability of different evaluation systems by comparing and analyzing the differences and similarities between other evaluation methods and human evaluations. Additionally, this study compares these automated scoring methods with human subjective ratings to assess their validity and consistency."
        },
        {
            "title": "2 Evaluation",
            "content": "In this section, we will conduct systematic evaluation of six models across five domains: structured output generation, realism and physical consistency tasks, specific domain generation, challenging scenario generation, and multi-style creation. Each domain is further divided into specific sub-tasks to assess model performance in various detailed aspects. We will visually compare the model outputs for an intuitive comparison and conduct quantitative evaluations using metrics such as CLIPScore, HPSv2, Aesthetic Score, and GPT-4o scores. Additionally, these quantitative evaluations will be compared with human perceptual ratings to assess the alignment between model evaluation metrics and human judgment. For CLIPScore, HPSv2, and Aesthetic Score, we have sampled small set of carefully selected prompts, which are displayed in the images to allow direct comparison with human perception. However, these results may exhibit some degree of randomness. In the future, we plan to perform extensive sampling and evaluations to further refine the benchmarking process. For the GPT-4o and human evaluations, the generated images will be assessed on the following aspects: Aesthetic appeal and alignment with human preferences Conformance to physical laws and realism Safety (no copyright infringement, no NSFW content) Alignment with the text description, including the accuracy of generated text and charts Each of these four aspects will be rated on three-level scale: (\"Highly meets the requirements\"), (\"Moderately meets the requirements\"), and (\"Does not meet the requirements\"). A, B, and correspond to scores of 2, 1, and 0, respectively. The final score is calculated as follows, with maximum score of 10. (Aesthetic score 1 + Realism score 2 + Safety score 1 + Matching score 2)/1.2 In the articles subtask, we present the prompts used for testing the image output by each model. To visually represent the quality of the model outputs, we label images with green smiley face if they are aesthetically pleasing, adhere to the physical world logic, and perfectly match the prompt requirements. Images with chaotic outputs that deviate significantly from the prompt are labeled with red sad face. If the output images meet the aesthetic and prompt requirements to some extent but have minor flaws, we do not label them with either smiley or sad face. 2.1 Structured Output Generation In the context of text-to-image models, structured output generation refers to the task where the model processes structured or natural language input and generates structured image outputs that meet the given requirements. The ability to produce structured outputs can, to some extent, reflect the models proficiency in following instructions, providing direction for the further development of text-to-image models toward becoming more comprehensive and versatile models. In Sections 2.1.1, 2.1.3, 2.1.2, and 2.1.4, we will explore the tasks of code2table, code2figure, language2table, and language2figure, where different types of code or natural language inputs are used to generate tables or figures. In Section 2.1.5, we examine the models ability to generate complex equations. Sections 2.1.6 and 2.1.7 focus on the models capability to generate newspaper articles and academic papers from natural language descriptions. In Section 2.1.8, we introduce new input format using JSON to describe scene. In Section 2.1.9, we will investigate the models ability to design user interfaces based on code or language input. Finally, in section 2.1.10, we test T2I models ability to generate code. 2.1.1 Code2Table In previous work, several studies [80, 4] have made significant strides in the task of generating tables from code. In our study, we investigate the potential of text-to-image models for generating tables based on code inputs. Markdown2Table. We investigated the models ability to comprehend 6 markdown text and generate tables from input. The results are shown in right subplot of Figure 1. Using simple 3 3 table as test, we found that FLUX.1 [20] almost generated the table accurately, with only minor errors in specific data. However, Midjourney did not recognize the task as table generation. Ideogram2.0 [36], Dall-E3, Stable Diffusion 3, and Jimeng understood the intent to generate table but were unable to produce it with complete accuracy. LaTeX2Table. As shown in the left and middle sublplot of Figure 1, we used LaTeX format instead of markdown to test the models ability to generate more complex tables with 9 rows and 4 columns. We found that FLUX.1 demonstrated an extraordinary ability to process complex tables, almost perfectly generating the table as described in the prompt. Similar to the Markdown2Table task, Midjourney did not recognize the task as table generation. Ideogram2.0, Dall-E3 [6], and Stable Diffusion 3 [64] were able to generate images that resembled tables but lacked accurate content, while Jimeng struggled with handling certain special characters in the LaTeX format. Figure 1: Results on code2table task. Refer to Section 2.1.1 for detailed discussions. Table 1: The scoring of generation results by six models on code2table under different evaluation systems. Refer to Section 2.1.1 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 26.48 29.17 30.17 22.70 20.86 27. 0.20 0.23 0.25 0.23 0.17 0.19 4.73 5.30 5.21 5.67 4.94 4.11 5.56 4.44 4.45 4.17 1.39 2.50 8.89 7.50 7.22 5.00 4.72 8.33 7 Score. The results are shown in Table 1. By comparing and observing the ratings of model outputs across four metrics, we found that the scores from CLIPScore, HPSv2 and Aesthetic Score did not align with the actual results. Through visual inspection of the generated images, FLUX.1 produced outputs most consistent with the format and content of the table in the prompt. However, the results obtained by these three metrics were not consistent with human observations. The scores GPT-4o were more in line with the actual situation. 2.1.2 Language2Table In this experiment, we aimed to explore the T2I models ability to transform natural language descriptions into tables. We described three tables with increasing levels of complexity. The results of all experiments are presented in Figure 2. It was observed that only FLUX.1, Ideogram2.0, Dall-E3, and Stable Diffusion 3 consistently grasped the intent to generate table. However, Ideogram2.0 tended to generate more columns than described in the prompt, while Dall-E3 often produced blurry text in the tables. FLUX.1 outperformed all other models in this task, demonstrating superior text accuracy and an exceptional understanding of prompts, particularly with the third, the most complex prompt. Figure 2: Results on language2table task. Refer to Section 2.1.2 for detailed discussions. 8 Score. The scores of output are shown in Table 2. The scoring results of CLIPScore and GPT-4o are consistent with human intuition, but the numerical results of GPT-4o differ significantly from human intuitive judgments. Table 2: The scoring of generation results by six models on language2table under different evaluation systems. Refer to Section 2.1.2 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 32.99 31.39 30.03 28.86 29.99 31.17 0.21 0.20 0.20 0.20 0.23 0.23 4.89 5.40 5.10 4.66 5.09 6.21 3.61 3.33 3.61 3.33 3.33 3. 7.78 7.50 6.11 5.56 5.28 5.00 2.1.3 Code2Chart Several studies [65, 4, 18] have made significant strides in the task of generating charts from code. In our study, we investigate the potential of text-to-image models for generating charts based on code inputs. Bar chart. We conducted an experiment to evaluate T2I models ability to understand Matplotlib code and generate corresponding chart. We began by designing simple bar chart code, with the results presented in the left subplot of Figure 3. FLUX.1, Ideogram2.0, Dall-E3, and Jimeng were able to grasp the intent to generate bar chart. Among these, FLUX.1, Ideogram2.0, and Dall-E3 successfully generated labels for all bars. However, only FLUX.1 and Ideogram2.0 produced the correct format for the bar chart. None of the models, however, generated the correct numerical values for the bars. Line chart. We also conducted an experiment with line chart, designed to show an increasing trend. The results are displayed in the right subplot of Figure 3. Except for Midjourney, all other models grasped the intent to generate line chart. However, Stable Diffusion 3 and Jimeng failed to produce the correct line chart format. FLUX.1 and Ideogram2.0 understood the increasing trend, but none of the models were able to generate an accurate chart that strictly followed the prompt. Score. The scores of the model results in this task are shown in the Table 3. Only the results of HPSv2 are consistent with human intuition; however, all scores are relatively low, suggesting that these metrics may not effectively understand prompts with structured outputs. Table 3: The scoring of generation results by six models on code2chart under different evaluation systems. Refer to Section 2.1.3 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 25.86 30.32 27.78 30.47 28.80 24.56 0.19 0.24 0.18 0.23 0.23 0.22 4.79 5.08 5.01 4.60 5.06 5. 1.67 2.08 2.08 2.50 1.67 2.50 7.50 7.92 6.67 5.83 5.00 5.42 2.1.4 Language2Chart Bar chart. In this task, we assess T2I models ability to transform natural language descriptions into visual charts. As illustrated in the left subplot of Figure 4, we describe simple bar chart and evaluate how well the models can reconstruct it. All models, except Midjourney, are capable of generating bar chart format. However, only FLUX.1, Ideogram2.0, and Dall-E3 are able to accurately generate both the x-axis labels and the overall title of the chart. None of these three models, however, can precisely generate the correct values for each bar, though FLUX.1 performs the best, producing the bar heights closest to the target values. 9 Figure 3: Results on code2chart task. Refer to Section 2.1.3 for detailed discussions. Pie chart. We also describe simple pie chart to evaluate the models capabilities, with the results shown in the right subplot of Figure 4. While all models successfully generate the pie chart format, none are able to produce the correct ratios for the chart segments. Score. The scores of the model results in this task are shown in the Table 4. The results of several metrics are relatively consistent, with only CLIPScore differing from human intuitive judgments. Compared to the Code2chart task, we observe that models perform better when the input is in natural language rather than code. This suggests that the models training data may lack sufficient multi-format input. Table 4: The scoring of generation results by six models on language2chart under different evaluation systems. Refer to Section 2.1.4 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 33.55 32.87 34.55 35.05 33.68 30.12 0.28 0.27 0.27 0.28 0.26 0.26 5.43 4.75 4.85 5.29 4.83 5.61 7.08 3.34 4.58 2.92 5.00 4.58 7.50 6.25 7.08 6.25 4.58 4. 10 Figure 4: Results on language2chart task. Refer to Section 2.1.4 for detailed discussions. 2.1.5 Equation Generation Logically connected equations. The understanding and generation of mathematical formulas have long been focus of research [57, 73, 49]. With the emergence of text-to-image models, we explore their ability to comprehend mathematical formulas and output them in image form. We conducted an equation generation experiment to evaluate the T2I models ability to generate equations. We used set of logically connected equations, drawn from the derivation process of linear equation in two variables, with the results shown in left, middle subplot of Figure 5. Only FLUX.1 and Jimeng were able to generate roughly correct set of equations, with FLUX.1 generally outperforming the other models. Independent equations. In right subplot of Figure 5, we observe that, aside from Midjourney, the other models can generate images containing mathematical symbols resembling equations. However, FLUX.1 is the most accurate. In particular, for the second set of equations, FLUX.1 almost perfectly reproduces all the equations. Score. The scores of the model results in this task are shown in the Table 5. The results of several metrics are relatively consistent, with only CLIPScore differing from human intuitive judgments. 11 Table 5: The scoring of generation results by six models on equation generation under different evaluation systems. Refer to Section 2.1.5 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 19.47 21.86 21.83 21.68 20.65 19.02 0.17 0.15 0.16 0.17 0.17 0.15 5.09 4.28 4.56 4.15 4.70 4.94 4.72 1.95 4.44 2.78 4.17 2. 7.50 5.84 5.83 4.44 3.89 4.72 Figure 5: Results on equation generation task. Refer to Section 2.1.5 for detailed discussions. 2.1.6 Language2Newspaper We evaluated the ability of these models to generate newspaper images based on natural language descriptions. We simply specified the layout and headlines for different sections of the newspaper and guided the models to generate newspaper page, and the results are shown in the left subplot of Figure 6. Among the models tested, Ideogram2.0s results were significantly better than the others, successfully generating the corresponding layout and headlines in the specified positions and adhering to the artistic style of newspaper. Dall-E3, Stable Diffusion 3, and Jimeng model were able to generate newspaper-style images, but their text generation had significant flaws. FLUX.1 produced mostly correct text and layout, but the style did not match that of newspaper. Midjourneys generation was unsatisfactory in both newspaper style and textual content. In in the right subplot of Figure 6, We present more complex example. We describe in greater detail the titles, style, content, and the placement of inserted images for each section of the newspaper. Although none of the models perfectly met the requirements of the prompt, Ideogram2.0 still outperformed the others, correctly generating the required layout and main titles. FLUX.1 was able to generate some of the titles correctly, but the layout had errors. Dall-E3, Stable Diffusion 3, Jimeng, and Midjourney barely generated any correct text or layout. Score. The scores of the model results in this task are shown in the Table 6. CLIPScore aligns relatively well with human intuition, while the other three metrics show significant discrepancies from human judgments, possibly because scoring in this task requires examining the specific text content within the images. Table 6: The scoring of generation results by six models on language2newspaper under different evaluation systems. Refer to Section 2.1.6 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 30.27 32.95 29.57 29.92 31.78 30.88 0.19 0.26 0.27 0.25 0.28 0.24 4.45 5.21 5.29 4.88 5.14 5.27 3.34 4.58 3.33 4.17 3.75 4. 8.34 9.16 7.50 6.67 6.25 5.42 Figure 6: Results on language2newspaper task. Refer to Section 2.1.6 for detailed discussions. 2.1.7 Language2Paper In Figure 7, we evaluated the ability of these models to generate academic paper images based on natural language descriptions. We specified the papers title, author, abstract outline, and date to guide the models in generating the first page of an academic paper. Among the models tested, only FLUX.1 and Stable Diffusion 3 were able to correctly produce the layout of an academic paper, while the other models mistakenly generated large number of decorative images. In terms of text accuracy, Ideogram2.0 and FLUX.1 performed the best, being able to accurately generate titles and subtitles. 13 Dall-E3 followed closely, while Midjourney and Stable Diffusion 3 almost failed to generate correct text. Figure 7: Results on language2paper task. Refer to Section 2.1.7 for detailed discussions. Score. The results of this task are shown in the Table 7. We can observe that all metrics differ somewhat from human intuitive judgments. This discrepancy arises because accurate evaluation requires thorough understanding of the basic format of academic papers and detailed comparison of the specific text content in the images, leading to insufficient accuracy of these evaluation metrics. Table 7: The scoring of generation results by six models on language2paper under different evaluation systems. Refer to Section 2.1.7 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 26.67 33.87 29.06 29.62 26.04 31.87 0.20 0.21 0.21 0.26 0.19 0.22 4.12 4.26 4.51 4.91 5.08 4.29 2.92 3.75 3.75 5.84 3.75 5.00 9.59 8.33 7.50 4.58 5.42 3.34 2.1.8 Json2Image We used new prompt format to evaluate the ability of T2I models to understand the relationships between objects and generate images correctly. The prompt was designed in JSON format, which is divided into three parts: objects, attributes, and relations. The objects section describes the items that appear in the image, the attributes section details the characteristics and specifics of each object, and the relations section describes the spatial or logical relationships between different items. An example is shown in Figure 8. In the first example, we found that, except for Midjourney, which cannot process this format, both Jimeng and Stable Diffusion 3 could only understand the main objects and combine them together, lacking logical coherence. Dall-E3 generated green lens, while FLUX.1 and Ideogram2.0 performed the best. In the second example, except for Midjourney, the output from Jimeng failed to show the woman sitting down. In Stable Diffusion 3 and Dall-E3s results, the bicycles tire was incomplete. FLUX.1 and Ideogram2.0 excelled in this task as well. Figure 8: Results on json2image task. Refer to Section 2.1.8 for detailed discussions. 15 Score. The results of this task are shown in the Table 8, where we found that the evaluations from CLIPScore and GPT-4o are closer to human intuition, while Ideogram2.0 performs better in these two metrics. Table 8: The scoring of generation results by six models on json2image under different evaluation systems. Refer to Section 2.1.8 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 18.96 20.14 16.39 14.00 18.55 19.40 0.22 0.22 0.24 0.18 0.26 0.26 6.44 5.66 6.11 5.85 6.75 6.40 5.84 7.50 5.84 7.08 4.16 6. 10.00 9.16 7.50 9.16 6.25 8.75 2.1.9 UI Design Previous work has explored how to use AI as an assistive tool for UI design [74, 35, 17, 75].In this work, we explore the potential of using text-to-image models for automating UI design. Code2UI. UI design is common task for evaluating T2I models ability to follow instructions. We input HTML code into the models, and the results are shown in Figure 9. While all models generate some form of web interface, Stable Diffusion 3 produces output that appears as meaningless gibberish. In comparison to the ground truth, only FLUX.1, Ideogram2.0, and Dall-E3 follow the instructions more accurately, generating web layouts containing the sections \"About Me\", \"My Work\", and \"Contact.\" Language2UI. In this task, we assess models ability to convert natural language into web interfaces, as shown in Figure 10. In the first example, both Jimeng and Stable Diffusion 3 produce unreadable text, while Dall-E 3 fails to generate typical web interface. In contrast, FLUX.1, Ideogram2.0, and Midjourney generate legible text, with FLUX.1 and Ideogram2.0 excelling in instruction-following. In the second example, Jimeng, Stable Diffusion 3, and Midjourney produce blurry outputs, while Ideogram2.0 and Dall-E3 contain some chaotic text. FLUX.1 outperforms the other models, demonstrating better instruction-following. Score. The results of this experiment are shown in the Table 9. Among the scores for CLIPScore, HPSv2, and Aesthetic score, FLUX.1 achieved higher score. In human intuitive perception, the outputs of FLUX.1 and Ideogram2.0 are also better, while the results of GPT-4o are inconsistent with human intuitive perceptions in this experiment. Table 9: The scoring of generation results by six models on UI design task under different evaluation systems. Refer to Section 2.1.9 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 28.19 25.82 27.32 24.68 27.98 26.27 0.22 0.20 0.21 0.18 0.19 0.21 4.94 4.64 4.77 4.56 4.88 4. 4.45 5.00 4.72 5.28 5.28 4.72 8.89 9.44 6.67 6.94 5.00 6.67 2.1.10 Code Generation The use of LLMs for code generation has long been focus of research [14, 19, 2, 72]. With the emergence of diffusion models, the question arises: Can text-to-image models also be used to generate code? In Figures 11 and 12, we examine the models capability to generate various types of code, including Python and programs, as well as barcodes and QR codes, in order to explore the potential for generalizing text-to-image (T2I) models into more fundamental models. In Figure 11, the models are expected to generate images containing correct program code. However, none of the models 16 Figure 9: Results on UI design. Refer to Section 2.1.9 for detailed discussions. 17 Figure 10: Results on UI design. Refer to Section 2.1.9 for detailed discussions. produce accurate outputs. Instead, they generate images depicting computer screens with code-like visuals. Similarly, in Figure 12, where the tasks are to generate valid barcodes and QR codes, all models fail to produce correct results. These findings suggest that significant further development is required before t2i models can evolve into foundational models capable of handling such tasks. Score. The experimental results of this task are shown in Table 10. It can be observed that due to the difficulty of the task, the performance of several models is not high. Table 10: The scoring of generation results by six models on code generation under different evaluation systems. Refer to Section 2.1.10 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 28.10 27.41 25.70 25.84 26.49 30.53 0.22 0.25 0.23 0.24 0.24 0.26 4.50 4.99 4.44 5.00 5.02 4.89 4.38 4.58 5.42 5.63 4.59 4. 5.00 4.38 4.17 4.17 2.50 2.50 18 Figure 11: Results on code generation. Refer to Section 2.1.10 for detailed discussions. Figure 12: Results on code generation. Refer to Section 2.1.10 for detailed discussions. 19 2.2 Realism and Physical Consistency Tasks In the image and video generation task, achieving realism and physical consistency is crucial. Previous works have made significant contributions to this area of research [54, 46, 47, 33]. We aim for models to generate images that are not only visually compelling but also believable and grounded in the physical world. To assess models ability to understand and replicate real-world dynamics, we have designed set of tasks that evaluate its grasp of physical laws. In Section 2.2.1, we evaluate the models ability to generate credible human figures in complex multi-person settings. Section 2.2.2 focuses on assessing the models capability to accurately render human bodies and poses. In Section 2.2.3, we incorporate various photographic terminologies into the prompts to test the models understanding of photography techniques. Section 2.2.4 examines the models ability to interpret and generate correct perspective relationships within realistic scenes. Section 2.2.5 explores the extent to which T2I models understand the fundamental physical laws of the real world. 2.2.1 Multi-Person Generating images with multiple characters has always been highly challenging task [84]. Figure 13 depicts the visualization results of six models in generating images based on prompts involving multiple persons. FLUX.1 demonstrates strong ability to capture overall details from the prompts. And Stable Diffusion 3 [64], Midjourney, and Jimeng struggle with handling the overlapping and nonoverlapping aspects of multiple persons. Midjourney often cuts off half of face, and Jimeng produces disjointed upper body parts. In the second part of Figure 13, Jimeng and FLUX.1 successfully generate images of multiple persons on crowded subway with minimal distortion. FLUX.1, in particular, handles facial features and overlapping boundaries well, though its color palette is somewhat monotonous, and the depicted actions are limited. Conversely, Stable Diffusion 3 introduces significant distortions, notably visible distortion at the junction of blonde womans hair and another mans face. Midjourney also exhibits distortion, particularly in the distant background of the subway scene. Figure 13: Results on multi-person task. Refer to Section 2.2.1 for detailed discussions. Score. The results of this experiment are shown in Table 11. Both CLIPScore, HPSv2, and GPT-4o consider Midjourneys output to be superior; however, upon our careful observation, the human forms in FLUX.1 appear more realistic. Table 11: The scoring of generation results by six models on multi-person under different evaluation systems. Refer to Section 2.2.1 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 26.31 25.60 25.05 27.64 23.60 26.87 0.29 0.31 0.29 0.31 0.30 0.30 5.69 6.04 5.51 5.36 5.98 5.03 5.00 7.08 7.01 7.08 6.25 6. 9.17 5.00 7.08 7.92 7.08 5.84 2.2.2 Human body In Figure 14-15, we examine the models ability to accurately generate human body, with particular focus on the hands and feet, which are difficult tasks in image synthesis. Hands. For instance, in the left subplot of Figure 14, all the models produce extra fingers except Dall-E3 [6]. Specifically, the Stable Diffusion 3 and Jimeng models exhibit entirely irrational hand structures. The image generated by Ideogram2.0 looks fake. Midjourney demonstrates capacity to capture significant hand details, and FLUX.1 [20] achieves the most accurate body structure. Feet. In the right subplot of Figure 14, both Midjourney and Stable Diffusion 3 generate the totally wrong foot structures, and Dall-E3 even result illegal, whereas FLUX.1, Jimeng and Ideogram2.0 produce more anatomically correct feet, despite Jimeng and Ideogram2.0 displaying oddly legs. Overall, FLUX.1 exhibits superior human body structure generation compared to the other models, though it still requires improvements in rendering the correct number of fingers. Pose. In Figure 15, we examine the models ability to generate accurate human poses. In the first example, the desired pose is the Tree Pose (Vrksasana) from yoga. FLUX.1, Ideogram2.0, Stable Diffusion 3, and Jimeng successfully generate woman in the correct pose. However, the poses generated by Dall-E3 and Midjourney are inaccurate, possibly due to lack of understanding of Vrksasana. While their outputs fit the general prompt description, even they do not accurately capture the specific yoga position. In the second example, only Jimeng precisely follows the Warrior II Pose, but it overlooks the prompt detail of \"facing the ocean\". Ideogram2.0 and Dall-E3 fail to depict the correct yoga pose but align more closely with the general description of the pose. Score. All scoring results for this task are shown in Table 12. The output of FLUX.1 is closer to reality, GPT-4os evaluation aligns with human perception, while CLIPScore, HPSv2, and Aesthetic Score differ significantly from human intuition. Table 12: The scoring of generation results by six models on human body under different evaluation systems. Refer to Section 2.2.2 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 27.16 30.86 28.42 30.26 30.08 28. 0.27 0.26 0.28 0.26 0.27 0.27 5.70 5.92 5.77 5.66 6.23 5.76 7.91 6.66 8.75 7.07 4.79 6.04 8.12 6.25 7.08 5.42 6.46 6.46 21 Figure 14: Results on human body task. Refer to Section 2.2.2 for detailed discussions. Figure 15: Results on human body task. Refer to Section 2.2.2 for detailed discussions. 22 2.2.3 Photographic Image Generation In Figure16-19, we explore the models ability to generate images that meet specific requirements based on photographic terminology. Setting 1: We tested blurred bokeh backgrounds and depth of field. The models generally understood these concepts, with FLUX.1, Midjourney, and Jimeng performing best. Results are shown in the left subplot of Figure 16. Setting 2: We tested long exposure, specifically time-lapse photography represented as star trails. Dall-E3s images had excessive star trails that appeared unnatural, followed by Ideogram2.0 and Midjourney. FLUX.1 and Stable Diffusion 3 produced the best overall results. Results are shown in the middle subplot of Figure 16. Setting 3: We examined macro photography and the concept of copy space. All models managed macro photography. Copy space refers to large blank areas in images for adding text, graphics, or other design elements. Dall-E3 mistakenly added unspecified text directly. Results are shown in the right subplot of Figure 16. Figure 16: Results on photographic image generation. Refer to Section 2.2.3 for detailed discussions. Setting 4: The setting involved terms like close-up shots, which refer to capturing above the chest. FLUX.1 and Stable Diffusion 3 missed this detail, while Midjourney performed best in overall style. Ideogram2.0s images were darker, and Dall-E3s output conflicted with photographic styles. Results are shown in the left subplot of Figure 17. Setting 5: Tilt-shift photography, used to alter the focus and depth of field, typically for creating miniature scenes, was tested. Dall-E3 performed best with this keyword, and all models could accurately generate images as prompted. Results are shown in the middle subplot of Figure 17. Setting 6: For golden tones, FLUX.1 and Dall-E3 excelled, while other models failed to achieve the effect. For symmetrical composition, only Stable Diffusion 3 missed the mark. For telephoto lens, backlighting, and soft light, FLUX.1 failed to deliver the telephoto effect but had the best lighting. Dall-E3s lighting was decent, while others only achieved soft light. Results are shown in the right subplot of Figure 17. Figure 17: Results on photographic image generation. Refer to Section 2.2.3 for detailed discussions. Setting 7: We tested stunning photorealism, cinematic composition, and minimalist style. Midjourney and Ideogram2.0 had the most realistic images, followed by FLUX.1. Minimalist style was harder to judge, but FLUX.1 and Stable Diffusion 3 had the fewest elements. For the shot on Fujifilm look, only Jimeng and Dall-E3 struggled to achieve the retro film style with subtle contrasts. In terms of professional photography techniques, atmospheric lighting, natural gradients, and cinematic depth, Jimengs contrast was too intense. FLUX.1 had the best gradient effect, while Dall-E3s gradients felt forced and ineffective. Results are shown in the left subplot of Figure 18. Setting 8: Double exposure, intended to capture reflections of people on glass, was tested. Only FLUX.1 and Midjourney met expectations; Ideogram2.0 and Dall-E3 partially achieved the effect, while Jimeng had clear issues, and Stable Diffusion 3 completely failed to recognize the keyword. For soft focus, delicate light play, cinematic quality, soft shadows, and artistic composition, the overall softness was best in Midjourney and FLUX.1. Results are shown in the middle subplot of Figure 18. Setting 9: The 28mm lens, wide-angle lens that maintains background clarity, was tested. Ideogram2.0 and Jimeng did not achieve this effect. For studio lighting, interpreted as artificial lighting typical of studio, Ideogram2.0 only captured regular artificial light. FLUX.1, Midjourney, and Jimeng performed best in high-definition photography, professional lighting, cinematic depth, and soft focus, which emphasized facial contours and details, while others were slightly weaker. Results are shown in the right subplot of Figure 18. Setting 10: Involving aerial environment photography and the blue hour, Dall-E3s results were slightly inferior; others performed well. The requirement for high-resolution image from high vantage point with the Sony A7R IV was best met by Ideogram2.0 and Midjourney, with more harmonious and softer color tones. Results are shown in the left subplot of Figure 19. Setting 11: Under cinematic lighting, Dall-E3 produced the best facial lighting, Midjourney achieved dreamy effect, and FLUX.1 had the most realistic lighting. For surrealism, vibrant colors, and professional photography techniquesusing methods like distortion, collage, and supernatural elements to create dreamlike atmospheres. FLUX.1 failed to capture this, Stable Diffusion 3 had issues with hand and scene generation, Jimeng generated anime-style images, and Ideogram2.0 24 Figure 18: Results on photographic image generation. Refer to Section 2.2.3 for detailed discussions. mistook kite shapes for fish. Midjourney depicted kites as butterflies but with excellent overall style, while Dall-E3 performed best. Results are shown in the middle subplot of Figure 19. Setting 12: In street photography, soft and diffused light was required; Dall-E3 notably violated this, and Ideogram2.0s tone was too cool. For cinematic framing, dynamic composition, natural reflections, urban realism, and soft lighting, reflections were best captured by FLUX.1 and Midjourney. Jimeng, Stable Diffusion 3, and Ideogram2.0 showed varying issues, with Ideogram2.0s water ripple effects being notably problematic, and Dall-E3s composition defying logic. Results are shown in the right subplot of Figure 19. Score. The results of this experiment are shown in Table 13. These metrics differ significantly from human intuition, with only GPT-4o and CLIPScores scores being relatively consistent with human evaluations. This may be due to the presence of numerous technical terms related to photography in the prompts, which the other metrics may not fully comprehend. Table 13: The scoring of generation results by six models on photographic image generation under different evaluation systems. Refer to Section 2.2.3 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 28.49 28.88 29.31 30.70 29.56 29.93 0.28 0.30 0.28 0.29 0.30 0. 6.09 6.22 6.13 6.27 6.38 6.46 7.29 7.57 6.87 8.61 7.01 6.66 9.38 7.08 5.90 8.68 6.87 7.56 25 Figure 19: Results on photographic image generation. Refer to Section 2.2.3 for detailed discussions. 2.2.4 Perspective Relation In Figure 20 and Figure 21, we evaluated the models ability to correctly handle perspective relationships. Most of the tested models demonstrated excellent performance, whether dealing with simple track scenes or more complex urban streets and library settings, generally aligning well with real-world perspective. However, Stable Diffusion 3 produced images with certain degree of distortion, performing the worst in terms of matching real-world perspective relationships. Score. The results of this experiment are shown in Table 14. We can see that in this task, only GPT-4os scores align relatively well with human ratings. This may be because the experiment involves physical relationships such as perspective, requiring the evaluation metrics to have certain understanding of the fundamental principles of the physical world. Table 14: The scoring of generation results by six models on perspective relation under different evaluation systems. Refer to Section 2.2.4 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 25.42 27.31 25.95 26.58 27.38 28.26 0.27 0.29 0.29 0.26 0.28 0.30 5.82 6.38 6.32 6.00 5.90 6.34 7.50 5.28 7.78 8.89 7.22 8.89 8.61 5.56 7.50 8.05 7.50 8.33 Figure 20: Results on perspective relation task. Refer to Section 2.2.4 for detailed discussions. Figure 21: Results on perspective relation task. Refer to Section 2.2.4 for detailed discussions. 27 2.2.5 Physical understanding In the T2I pipeline, we give the image caption to the model, then the model generates an image reflecting the caption content, visually correct. In this process, does the model do understand the worlds physical law [52]? To test this point, we describe real-world physical scene in the prompt. To generate an image that conforms to the laws of physics, the models need to truly understand the physical law. Here we describe two scenes: glass cup falling to the ground and the waters temperature is over 100 celsius degrees. The results are shown in Figure 22. In the first scene, only Ideogram2.0 and Jimeng can generate the physically correct image: the glass cup shattered into pieces. In the second scene, all models perform well: the water boiled, except FLUX.1. Score. The results of this task are shown in the Table 15. It can be observed that HPSv2, GPT-4o, and human perception are largely consistent. Ideogram2.0 achieved the highest score in the Aesthetic Score, which also aligns with human perception. However, the CLIPScore differs significantly from human perception. Table 15: The scoring of generation results by six models on physical understanding under different evaluation systems. Refer to Section 2.2.5 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 22.10 20.79 24.94 25.94 22.78 23. 0.26 0.27 0.26 0.26 0.24 0.20 5.67 5.99 5.91 5.76 5.99 5.22 2.92 8.34 7.08 5.42 4.17 6.25 4.17 9.16 6.66 7.50 5.00 6.25 Figure 22: Results on physical understanding task. Refer to Section 2.2.5 for detailed discussions. 2.3 Specific Domain Generation With the advancement of T2I models, their usefulness has expanded to various domains. These models hold the potential to generate high-quality, domain-specific data, paving the way for significant contributions to technological innovation and interdisciplinary research. In Section 2.3.1, we assess the models understanding of mathematical terminology and their capability to generate math-related images based on given descriptions. Section 2.3.2 explores the models performance in generating images within fractal settings. In Section 2.3.3, we evaluate the models capability to produce medical images with potential applications in medical research. In Sections 2.3.4 and 2.3.5, we prompt the models to generate 3D images. Lastly, in Sections 2.3.6 and 2.3.7, we assess the models ability to generate images related to chemistry and biology. Section 2.3.8 explores the working environments of robots in embodied intelligence, while Section 2.3.9 investigates tasks in autonomous driving scenarios. 2.3.1 Math In Figure 24, we explore the models mathematical ability, especially geometrical concepts. For the first example of right-angled triangle in Figure 24, FLUX.1 [20], Stable Diffusion 3 [64] and Jimeng try to present the outputs in mathematical format. However, FLUX.1 fails to accurately depict the correct geometric relationships, and the output from Stable Diffusion 3 is fuzzy and irrelevant. Jimeng successfully generates correct right-angled triangle, though the image contains the wrong text. Ideogram2.0 [36] and Midjourney mistakenly focus too much on the word \"measuring\" in the prompt, thus Ideogram2.0 generates rulers arranged in the shape of right triangle, and Midjourney presents dimensional figure irrelevant. Dall-E3 [6] cannot recognize the prompt as math concept. In the second example of an inscribed circle within an isosceles triangle, the style of the results is similar to the first. In detail, all models generate the correct isosceles triangle but the wrong inscribed circle. Current T2I models are lacking in the ability to generate mathematically relevant images, they cannot accurately understand some mathematical concepts, and it is difficult to generate images that conform to analytic geometric. Figure 23: Results on math task. Refer to Section 2.3.1 for detailed discussions. 29 Score. The results of this experiment are shown in Table 16. Midjourney received higher scores from human evaluations, primarily because the outputs of several models do not effectively grasp the mathematical concepts in the prompts. As result, the scoring mainly focuses on aspects like aesthetics and realism, with other metrics showing some discrepancies compared to human intuition. Table 16: The scoring of generation results by six models on math image design under different evaluation systems. Refer to Section 2.3.1 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 25.84 20.71 23.91 25.62 24.43 25.50 0.20 0.19 0.20 0.18 0.23 0.21 4.72 5.76 5.03 4.72 5.05 4.51 5.00 5.00 4.17 3.75 4.17 6. 4.58 5.42 6.25 7.08 3.33 5."
        },
        {
            "title": "2.3.2 Fractal",
            "content": "In this section, we evaluate the models ability to generate images within fractal settings, which require understanding complex recursive patterns. These patterns are often used in mathematical and artistic contexts to depict natural phenomena like coastlines, snowflakes, and more. For the first experiment, we prompted the models to generate Mandelbrot set. FLUX.1 and Midjourney produced visually appealing fractals with detailed recursive structures. However, Dall-E3 and Stable Diffusion 3 struggled with the intricacy of the pattern, resulting in less accurate representations. In the second experiment involving the Sierpinski triangle, FLUX.1 and Jimeng successfully captured the recursive nature of the fractal, accurately depicting the triangular subdivisions. Ideogram2.0 misinterpreted the prompt, generating series of disjointed triangles, while Dall-E3 created pattern resembling the Sierpinski triangle but lacking precise detail. Overall, the experiments reveal that while some models can generate fractal images, consistency and accuracy vary. This suggests that improvements in understanding recursive algorithms might enhance their performance in this domain. Score. The results of this experiment are shown in the Table 17. It can be observed that FLUX.1 performed the best in this experiment. The Aesthetic Score aligns more closely with human intuitive perception, while the GPT results show significant difference from human perception. Table 17: The scoring of generation results by six models on fractal image design under different evaluation systems. Refer to Section 2.3.2 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 24.93 23.46 28.29 25.54 25.90 19.78 0.25 0.25 0.24 0.21 0.23 0.21 6.09 5.73 5.63 6.01 5.41 5.44 5.42 6.25 5.00 4.59 6.25 7.92 7.92 5.84 4.59 6.67 2.50 7. 2.3.3 Medical In this experiment, we tested the ability of T2I models to generate medical images [1]. For the first prompt, we asked the models to generate an X-ray image capturing frontal view of the chest. We found that Ideogram2.0, Jimeng, and Stable Diffusion 3 did not produce accurate X-ray images, as indicated by the color and texture of their outputs. Additionally, these three models generated chaotic representations of the shoulder joints, and Ideogram2.0 produced an incorrect morphology of the lungs. Midjourney generated an image that resembled an X-ray, but the structures of the heart and liver were significantly flawed. Dall-E3 and FLUX.1 performed the best, producing images with an 30 Figure 24: Results on fractal task. Refer to Section 2.3.2 for detailed discussions. Figure 25: Results on medical task. Refer to Section 2.3.3 for detailed discussions. almost correct morphology of bones and organs, with only minor inaccuracies in the structure of the shoulder joints. In the second prompt, we asked the models to generate CT scan displaying detailed cross-section of human brain. Ideogram2.0 and Dall-E3s outputs did not resemble CT scan, and the brain structures were incorrect. Jimeng failed to generate cross-section of the brain, and the organs in the images appeared very chaotic. Midjourneys output contained mistakes in the locations of the cerebrum and cerebellum, and the internal structures of the brain were disorganized. Stable Diffusion 3s result was not realistic enough; the brains edge were overly defined, the proportion of black areas representing cerebrospinal fluid was too small compared to real brain, and the real brain does not have such notches at the back of the head. Only FLUX.1 produced result closest to real brain CT scan, leading us to suspect that FLUX.1s training data may include certain proportion of high-quality medical images. Score. The results of this experiment are shown in the Table 18. Except for the CLIPScore, FLUX.1 performed the best in all other scores. Table 18: The scoring of generation results by six models on medical image generation under different evaluation systems. Refer to Section 2.3.3 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 23.66 24.38 27.64 26.42 28.12 27.16 0.22 0.22 0.21 0.22 0.19 0.22 5.73 4.63 4.68 5.10 5.10 4.82 6.66 5.28 6.11 5.28 4.17 4.17 7.78 6.39 5.00 5.84 6.66 4. 2.3.4 3D Point Cloud In Figure 26, we investigate the models capacities to generate images in the form of 3D point cloud. For the first example of an airplane, FLUX.1, Dall-E3 and Midjourney successfully generate point cloud representations, while the outputs of Dall-E3 and Midjourney are imperfect due to the points outside the airplane. Ideogram2.0, Stable Diffusion 3 and Jimeng fail to accurately render the point cloud, with Ideogram2.0 even producing an image more like 3D mesh. In the second example of chair, FLUX.1 again succeeds in producing correct point cloud representation. Additionally, Ideogram2.0 also generates an image of chair in point cloud form, but of poor quality. Stable Diffusion 3 misunderstands point cloud and generates chair painted with dots, similar to its output in the first example. Dall-E3, Midjourney and Jimeng misinterpret point cloud as well, generating images of chair constructed from spherical shapes. Overall, FLUX.1 is the only model to perform well in both cases. It is worth mentioning that the objects and prompts are well-designed. We observed that the models struggle to generate accurate point clouds for certain objects, indicating that additional training may be necessary for tasks involving point cloud generation. Score. The results of this experiment are shown in the table. In this experiment, the performance of the models is similar, with FLUX.1, Dall-E3, and Jimeng performing slightly better. Table 19: The scoring of generation results by six models on point cloud image design under different evaluation systems. Refer to Section 2.3.4 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 32.24 33.78 32.18 29.73 32.93 32.65 0.26 0.30 0.31 0.25 0.28 0.31 5.30 5.95 5.94 5.70 5.76 6.30 5.00 6.25 7.50 6.66 6.66 5.00 7.50 6.67 7.50 6.67 7.08 7. 32 Figure 26: Results on 3D point cloud task. Refer to Section 2.3.4 for detailed discussions. 2.3.5 3D Mesh The synthesis of high-quality 3D assets from textual or visual inputs has become central objective in modern generative modeling [29, 41, 34, 15]. In Figure 27, we test the models ability to generate 3D mesh representations. It is observed that all models can generate images that seem like 3D mesh form. In the first example of human head, FLUX.1, Ideogram2.0, Dall-E3 and Stable Diffusion 3 successfully generate 3D mesh-like representations. However, there are also mistakes that only Jimeng achieves \"a three-quarter view on the left and front view on the right\" required by prompt. Additional errors including the curved edges of the plane in Midjourney and the textured eyes in Jimeng are incorrect in 3D mesh. In the second example of car, FLUX.1 and Ideogram2.0 also generate correct 3D mesh representations. However, the outputs of Dall-E3, Midjourney and Stable Diffusion 3 are more like dividing realistic car by lines, and Jimeng even breaks the car into pieces. Overall, FLUX.1 and Ideogram2.0 have the correct understanding of 3D mesh. However, there remains distinction between the T2I models generating two-dimensional images and 3D generation models. Score. The results of this experiment are shown in the Table 20. It can be seen that the model performances are fairly balanced. Overall, the Ideogram2.0 model achieved higher scores. Table 20: The scoring of generation results by six models on 3D mesh task under different evaluation systems. Refer to Section 2.3.5 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 29.23 32.64 32.98 32.36 31.22 29.87 0.24 0.27 0.26 0.24 0.24 0.25 6.00 5.96 5.15 5.27 5.37 5.55 5.84 5.84 5.00 6.25 7.50 6. 7.92 8.33 7.08 6.25 7.92 8.33 Figure 27: Results on 3D mesh task. Refer to Section 2.3.5 for detailed discussions. 2.3.6 Chemistry In Figures 28, we assess the models capabilities in generating scientifically accurate images within the domain of chemistry. We evaluate the models understanding of chemistry by prompting them to 34 generate correct representation of benzene molecule, but none of the models succeed. Overall, the T2I models struggle to generate scientifically accurate images in the field of chemistry, failing to meet the necessary scientific standards. Score. The results of this task are shown in the Table 21. Ideogram2.0 produced the best outputs in this task. However, the scores from several evaluation systems differ from human intuition, likely because this task requires certain level of chemistry knowledge, making accurate evaluation more demanding. Table 21: The scoring of generation results by six models on chemistry tasks under different evaluation systems. Refer to Section 2.3.6 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 29.38 27.42 28.49 28.65 25.52 26.53 0.27 0.27 0.18 0.20 0.16 0.21 5.45 5.09 3.54 4.66 4.13 5.04 2.50 5.00 2.50 7.50 5.83 2. 6.67 10.00 8.33 6.67 5.00 4.17 Figure 28: Results on chemistry task. Refer to Section 2.3.6 for detailed discussions. 2.3.7 Biology Figure 29 focuses on their grasp of biology, specifically through the generation of plant cell and animal cell images. In the first example, FLUX.1, Dall-E3, and Jimeng attempt to depict plant cell structures. However, FLUX.1 erroneously includes leaves inside the cell, while Dall-E3 incorporates an orange slice incorrectly. Meanwhile, Ideogram2.0 and Stable Diffusion 3 offer microscopic views of plant cells, but Midjourney seems to lack sufficient biological knowledge. The results of the second example are similarly unsatisfactory. 35 Score. The results of this experiment are shown in Table 22. FLUX.1 and Dall-E3 performed relatively well in this task; however, only the CLIPScore results aligned with human intuitive perception. Table 22: The scoring of generation results by six models on biology under different evaluation systems. Refer to Section 2.3.7 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 30.54 26.10 27.58 21.99 26.85 23.14 0.27 0.22 0.26 0.23 0.28 0.25 5.77 5.78 5.56 5.79 5.99 5.82 4.58 5.42 6.25 7.08 3.75 7. 9.17 6.25 7.50 5.00 5.84 7.92 Figure 29: Results on biology task. Refer to Section 2.3.7 for detailed discussions. 2.3.8 Robotics and Simulation Tasks Tool-Usage Grounding In Robotics, the model needs to recognize the area to grasp or manipulate [23]. To test whether T2I models have the tool-usage grounding ability, we prompt them using two examples. For the first prompt, as depicted in the left subplot of Figure 30, only Ideogram2.0 can generate the hammer image with the grasping area marked with bounding box. Dall-E3 and FLUX.1 also generate the bounding box in the image, but its placed in the wrong area. For the second example, 36 Stable Diffusion 3, Midjourney, Dall-E3 and Ideagram2.0 all generate the bounding box in the image, but only Ideogram2.0 places it on the grasping area. Simulation environment Lots of robotic research is conducted in the simulated environment. Thus its important to test T2I models ability to produce the images in the simulation environment domain. Here we give two simulations, one is robot navigating, and the other is dexterous hand. The results are shown in the right subplot of Figure 31. For the first simulation, only the images from Ideogram2.0 and FLUX.1 look like the rendering image of the simulation environment. For the second simulation, only FLUX.1 correctly generated the dexterous hand. Figure 30: Results on robotics and simulation task. Refer to Section 2.3.8 for detailed discussions. Score. The results of this experiment are shown in the Table 23. In this experiment, FLUX.1 achieved relatively high score, and GPT-4os evaluation results were more consistent with human intuition. 2.3.9 Autonomous Driving For autonomous driving model development, synthetic data plays an important role in solving the corner cases. For example, in the real world, it is hard to collect large amount of autonomous driving data under extreme weather. Utilizing the T2I model, we can synthesize these hard-to-collect data efficiently. Here we prompt these T2I models to generate the road scene under rainy weather and foggy weather respectively. The results are shown in Figure 32. For the road scene under rainy weather, all six models perform well, while images generated by Dall-E3 and Jimeng are not as realistic as the others. For the road 37 Table 23: The scoring of generation results by six models on robotics and simulation tasks under different evaluation systems. Refer to Section 2.3.8 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 30.93 30.59 30.94 29.05 28.11 30.10 0.28 0.27 0.30 0.29 0.27 0.28 5.41 5.31 5.32 5.71 5.34 5.81 7.09 5.83 4.58 5.63 4.79 5. 9.17 8.75 7.92 6.67 6.04 7.08 Figure 31: Results on robotics and simulation task. Refer to Section 2.3.8 for detailed discussions. scene under foggy weather, all six models perform well, except the Jimeng-generated image contains commonsense artifacts pedestrians wont walk in the middle of the road. Score. The results of this experiment are shown in Table 24. Stable Diffusion 3 and Jimeng scored higher in other evaluation systems, but there is significant gap compared to human ratings. This may be due to the fact that the images in this experiment often lack realism in multiple details, which the evaluation systems fail to accurately recognize. 38 Table 24: The scoring of generation results by six models on autonomous driving task under different evaluation systems. Refer to Section 2.3.9 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 29.25 28.42 26.75 28.01 30.21 29.17 0.26 0.29 0.26 0.26 0.26 0.30 5.45 5.90 5.35 5.27 5.89 6.02 6.67 6.67 6.67 6.25 8.33 7. 8.34 8.34 8.33 9.17 7.50 6.67 Figure 32: Results on autonomous drive corner cases task. Refer to Section 2.3.9 for detailed discussions. 39 2.4 Challenging Scenario Generation The application of T2I models is expanding, particularly in generating images for more challenging scenarios. In this section, we carefully curate set of complex prompts to evaluate the models ability to handle intricate settings. In Section 2.4.1-2.4.2, we prompt the models to generate images based on traditional visual task settings. Section 2.4.4 examines the models ability to generate images containing dense text and pictures. In Section 2.4.3, we assess the models understanding of different languages from various regions. Finally, in Section 2.4.5, we input emojis into the models to evaluate their ability to organize diverse elements and piece together the logical relationships between multiple emojis in an image. In Section 2.4.6, we explored the ability of T2I models to generate images in irrational scenarios. In Section 2.4.7, we investigated whether T2I models could generate corresponding images based on answers when given simple questions as input. In Section 2.4.8, we experimented with the ability of T2I models to generate images with watermarks. In Section 2.4.9, we explored the models ability to generate low-quality images, which indicates whether the models training data contains low-quality data. In Section 2.4.10, we systematically explored the models ability to generate images containing multiple sub-images. In Section 2.4.11, we specifically studied the models ability to generate text within images. 2.4.1 Image with Mark Object detection is common task in the field of computer vision [48, 16, 58, 51, 53]. In this paper, we design the image with mark task to explore whether text-to-image models can be applied to computer vision tasks. In this task, we examine the models ability to generate images resembling the output of computer vision algorithms. The results are presented in Figure 33. In the first prompt, we asked the models to highlight coffee mug with bounding box, and in the second prompt, to highlight an apple in the same manner. We observed that only FLUX.1 [20], Ideogram2.0 [36], and Dall-E3 [6] successfully completed the first task, while only FLUX.1 and Ideogram2.0 correctly accomplished the second task. Figure 33: Results on image with mark task. Refer to Section 2.4.1 for detailed discussions. 40 Score. The results of this experiment are shown in the Table 25. It can be observed that the outputs of FLUX.1 and Ideogram2.0 are relatively good. The same conclusion can be drawn from the other scores, except for the CLIPScore. Table 25: The scoring of generation results by six models on image with mark under different evaluation systems. Refer to Section 2.4.1 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 29.66 32.27 36.37 31.12 34.43 32.04 0.25 0.27 0.27 0.27 0.26 0.26 5.13 5.94 4.94 4.90 5.19 4.71 8.74 4.59 7.90 7.07 4.59 5. 10.00 10.00 9.17 6.67 7.50 8.33 2.4.2 Set of Mark Set of mark is also traditional task in computer vision [82]. As depicted in the figure 34, FLUX accurately generates red bounding boxes around the specified coffee cup and vase, although the serial numbers are incorrect. In contrast, Midjourney fails to produce regular rectangles, while Stable Diffusion 3 [64] misplaces the bounding box, entirely missing the vase, and also assigns incorrect serial numbers. Jimeng, however, successfully frames all required objects. These findings indicate that while FLUX.1 exhibits minor labeling inaccuracies, it holds promise for improving object detection and grounding tasks in subsequent processes. Figure 34: Results on set of mark task. Refer to Section 2.4.2 for detailed discussions. 41 Score. The results of this experiment are shown in the Table 26. From human intuitive perspective, the output of FLUX.1 is the best. Only the results from HPSv2 align closely with human intuition, while the scores from the other metrics show some discrepancies. Table 26: The scoring of generation results by six models on set of mark task under different evaluation systems. Refer to Section 2.4.2 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 33.72 34.93 33.30 32.04 37.93 35.27 0.27 0.25 0.24 0.25 0.25 0.24 5.24 4.87 5.20 5.80 5.18 5.19 6.94 6.94 3.61 6.95 5.83 3. 9.22 8.89 7.50 5.56 7.22 5.83 2.4.3 Multilingual As shown in Figures 35-37, the multilingual capabilities of T2I models exhibit significant variations across different languages [55, 78]. Models like Ideogram2.0 and Dall-E3 demonstrate strong performance when processing prompts in English, Spanish, and French. However, notable limitation remains: FLUX.1 performs poorly with Chinese prompts, while FLUX.1, Midjourney, and Stable Diffusion 3 show subpar results with Japanese prompts. This may be attributed to their use of text encoders that support only English, highlighting crucial area for improvement in the development of more universally robust multilingual T2I models. Figure 35: Results on multilingual task. Refer to Section 2.4.3 for detailed discussions. Score. The results of this experiment are shown in the Table 27. The results for CLIPScore and Aesthetic Score are relatively consistent with human ratings. Figure 36: Results on multilingual task. Refer to Section 2.4.3 for detailed discussions. Figure 37: Results on multilingual task. Refer to Section 2.4.3 for detailed discussions. 43 Table 27: The scoring of generation results by six models on multilingual under different evaluation systems. Refer to Section 2.4.3 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 23.40 25.75 23.29 23.26 25.19 21.77 0.28 0.29 0.30 0.30 0.27 0.29 5.62 6.06 5.98 6.16 5.93 5.88 6.94 7.50 8.05 6.94 5.55 8. 8.33 10.00 8.33 7.78 8.33 9.17 2.4.4 Dense OCR Figure 38 and Figure 39 present an evaluation of the dense OCR capabilities of various T2I models. When generating posters with an English corpus, FLUX.1 successfully captures the overall content based on the given requirements but exhibits some spelling errors in the generated text. In contrast, Jimeng, Dall-E3, Ideogram2.0, and Stable Diffusion 3 focus primarily on the title, failing to generate additional textual content from the provided prompts. Notably, Stable Diffusion 3 introduces considerable spelling errors. Furthermore, none of these T2I models effectively recognize or generate Chinese text when tasked with poster generation using Chinese corpus, highlighting significant limitation in handling Chinese OCR. For academic paper poster generation, FLUX.1 and Ideogram2.0 demonstrate the ability to generate most of the textual content with clear and legible appearance. However, Dall-E3, Stable Diffusion 3, Jimeng, and Midjourney struggle with text clarity and exhibit prominent spelling errors, indicating limitations in generating accurate and coherent textual content in this context. Figure 38: Results on denseocr task. Refer to Section 2.4.4 for detailed discussions. 44 Figure 39: Results on denseocr task. Refer to Section 2.4.4 for detailed discussions. Score. The results of this experiment are shown in the Table 28. From human intuitive perspective, Ideogram2.0 performed the best in this experiment, with only the GPT-4o results aligning closely with human perception. 2.4.5 Emoji In Figures 40-41, we investigate the models ability to comprehend emojis. We observe that FLUX.1 and Ideogram2.0 attempt to construct stories from combinations of emojis, but they tend to focus on certain emojis while ignoring others. For example, FLUX.1 disregards the construction site emoji in the second prompt of Figure 40 and the tree emoji in the first prompt of Figure 41. Ideogram2.0 performs better than FLUX.1 by considering nearly all emojis and their logical relationships. For instance, as shown in the left subplot of Figure 41, Ideogram2.0 integrates the desert and tree emojis into an oasis in the first example and also accurately understands the story conveyed by the emojis in 45 Table 28: The scoring of generation results by six models on dense OCR under different evaluation systems. Refer to Section 2.4.4 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 32.22 29.27 31.07 28.98 33.01 23.89 0.25 0.25 0.25 0.22 0.25 0.21 4.22 4.64 4.73 4.10 4.60 3.86 1.67 5.83 5.56 3.33 3.89 4. 7.22 7.78 5.83 6.94 5.83 5.56 the second example. Dall-E3 approaches the inputs differently, merging all emojis into its output and rendering it in comic style. However, it sometimes loses logical coherence, as seen in the first example of the left subplot in Figure 41, where it depicts tree standing alone in the desert. Midjourney, Jimeng, and Stable Diffusion 3 struggle to handle this type of task correctly, with Jimeng even considering this type of prompt illegal. Notably, Ideogram2.0 outperforms the others across all models, particularly in managing complex, multi-emoji inputs and accurately interpreting the stories expressed by the emojis. Figure 40: Results on emoji task. Refer to Section 2.4.5 for detailed discussions. 46 Figure 41: Results on emoji task. Refer to Section 2.4.5 for detailed discussions. Score. The results of this experiment are shown in the Table 29. In this task, the outputs of FLUX.1, Ideogram2.0, and Dall-E3 are all relatively good. Table 29: The scoring of generation results by six models on emoji task under different evaluation systems. Refer to Section 2.4.5 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 19.51 19.45 21.19 15.47 23.82 0.23 0.23 0.21 0.20 0. 6.98 6.18 6.15 5.52 5.89 5.83 6.39 6.94 3.89 4.17 8.89 9.72 8.05 5.00 5.83 2.4.6 Irrational Scene Generation In Figure 42-43, we evaluated the ability of these models to generate irrational scenes. In the first instance, we instructed the models to generate the word \"RED\" written in blue on yellow blocks. FLUX.1, Ideogram2.0, and Dall-E3 completed the task perfectly. Midjourney generated red background, Jimeng used black text, and Stable Diffusion 3 used red text, each with some flaws. In the second and third instances, we had the models generate anomalous scenes, including objects with unusual colors and materials. Among all the models, Dall-E3 performed the best, perfectly generating scenes according to the text description. When generating objects with anomalous colors, FLUX.1, Stable Diffusion 3, and Jimeng missed parts of the text description, resulting in flawed outputs. Additionally, FLUX.1, Ideogram2.0, and Jimeng failed to correctly interpret the metaphor in the text when generating objects with anomalous materials, leading to incorrect outputs. Score. The results of this experiment are shown in the Table 30. In this task, the outputs of FLUX.1, Ideogram2.0, and Dall-E3 are relatively good; however, the results for CLIPScore and GPT-4o differ significantly from human evaluations. Table 30: The scoring of generation results by six models on irrational scene generation under different evaluation systems. Refer to Section 2.4.6 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 30.06 28.47 31.89 30.57 30.66 32.47 0.28 0.28 0.26 0.27 0.27 0.27 6.02 5.93 6.17 5.90 6.08 5.88 6.39 6.11 5.83 4.45 6.94 6. 8.33 8.06 8.61 7.22 6.94 6.94 Figure 42: Results on irrational scene generation task. Refer to Section 2.4.6 for detailed discussions. 48 Figure 43: Results on irrational scene generation task. Refer to Section 2.4.6 for detailed discussions. 2.4.7 LLM QA As the T2I models get stronger and more versatile, it seems that they have the ability to develop towards fundamental models. In Figure 44, we illustrate two examples to assess the LLM questionanswering (QA) capabilities of models. This evaluation involves both understanding the questions and generating accurate visual representations of the answers. In the first example, the correct answer to the question about the celestial body orbiting the Earth is the Sun. Models such as Ideogram2.0, Dall-E3, and Jimeng correctly interpret the question and generate images representing the Sun. Although these images may not be scientifically precise representations of the Sun, they are easily recognizable. Conversely, models such as FLUX.1, Midjourney, and Stable Diffusion 3 misunderstand the prompt and produce irrelevant images. In the second example, which involves simple mathematical query, the desired output is an image featuring the number 3.11. Among the models, only Dall-E3 generates an image that approximates the correct answer. The other models produce images that do not correspond to the required representation. Score. The results of this experiment are shown in the Table 31. Due to the evaluation system of this experiment requiring both an understanding of the images and certain level of mathematical and physical knowledge, the results obtained may differ significantly from human intuitive perception. Table 31: The scoring of generation results by six models on LLM QA under different evaluation systems. Refer to Section 2.4.7 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 24.49 20.72 26.51 23.53 21.75 23. 0.23 0.23 0.24 0.23 0.21 0.25 5.13 5.32 5.71 5.66 5.09 5.66 5.00 4.59 5.42 6.67 5.71 5.42 10.00 7.92 7.92 6.83 2.92 6.67 49 Figure 44: Results on LLM QA task. Refer to Section 2.4.7 for detailed discussions. 2.4.8 Watermark In Figures 45, we evaluate the models capability to correctly generate watermarks in images, which remind that the images are AI-generated for safety purposes. We prompt the models to create images with watermarks placed in different locations and avoid destroying the integrity of the pictures. We use green frames to highlight the right watermarks and red frames to denote watermarks that are either misplaced, contain incorrect text, or are entirely omitted. The results show that none of the models consistently produced correct watermarks across all examples. Notably, the Jimeng model failed to generate accurate watermarks in any task, potentially due to its inherent watermarking system. Score. The results of this experiment are shown in the Table 32. The evaluation in this experiment focuses on the accuracy of watermark placement, which requires precise attention to image details. As result, there is some difference between other evaluation systems and human scoring. Table 32: The scoring of generation results by six models on watermark task under different evaluation systems. Refer to Section 2.4.8 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 27.37 27.10 27.42 29.01 30.26 29.34 0.25 0.28 0.26 0.24 0.25 0.30 5.17 6.10 5.83 5.66 5.73 6.10 6.67 6.67 5.83 8.34 7.50 5.00 9.17 8.33 5.42 8.34 8.33 8. 50 Figure 45: Results on watermark task, with green frames to highlight the right watermarks and red frames to denote watermarks that are either misplaced, contain incorrect text, or are entirely omitted. Refer to Section 2.4.8 for detailed discussions. 2.4.9 Low Quality In Figures 46-48, we investigate the models ability to generate low-quality images. During training, some models may discard low-quality images, while others may retain them and apply corresponding labels. This task tends to infer the nature of the datasets used by these models. Specifically, if model can accurately generate low-quality images, it suggests that labeled low-quality data was incorporated during training. We provide prompts including low resolution, distorted colors, disorganization, ugly figures, underexposure, overexposure, excessive noise, incorrect white balance, and accidental photography. Notably, Dall-E3 successfully generates accurate representations for all prompts, except for the example of accidental photograph, where Dall-E3 might misunderstand the prompt \"while holding phone.\" This suggests that Dall-E3 may have been trained on datasets including low-quality data with corresponding labels. In contrast, other models consistently produced high-quality images to satisfy the key information of prompts. The following provides more detailed analysis of each example. Low Resolution. In the left subplot of Figures 46, FLUX.1, Dall-E3, Midjourney, and Stable Diffusion 3 successfully generate low-resolution images as specified in the prompt. However, Ideogram2.0 produces an image resembling an out-of-focus photograph, while Jimeng generates high-resolution image, deviating from the intended requirement. Distorted Colors. In the middle subplot of Figures 46, Dall-E3 and Stable Diffusion 3 produce images with notably distorted colors, while the outputs from the other models exhibit an appealing use of multiple colors which are inconsistent with the prompt of istorted colors. Disorganization. Among the models, Dall-E3 and Midjourney produce images that best meet the prompts criteria in the right subplot of Figures 46,. The images generated by FLUX.1, Stable Diffusion 3, and Jimeng, although present level of disorganization but are more like well-laid. Ugly Figures. In the left subplot of Figures 47, Dall-E3 generates the image that most closely aligns with the prompts specifications. While Ideogram2.0 also creates an unattractive figure, it includes an 51 Figure 46: Results on low quality task. Refer to Section 2.4.9 for detailed discussions. unreasonable foreground element. FLUX.1, Midjourney, and Stable Diffusion 3 generate detailed portraits, and Jimeng produces even more intricate and refined images. Underexposure. In the middle subplot of Figures 47, all models generate images that exhibit underexposure, matching the prompts expectations. Overexposure. Only Dall-E3 successfully produces an overexposed photograph, in the right subplot of Figures 47. The other models fail to accurately catch the concept of overexposure. Excessive Noise. In the left subplot of Figures 48, the image generated by Dall-E3 contains excessive digital noise and grain. The other models also produce lower-quality images, but Ideogram2.0 stands out for incorporating elaborate special effects into its output. Incorrect White Balance. In the middle subplot of Figures 48, Jimeng is the only model that misinterprets the concept of incorrect white balance, producing an image with warm lighting scene rather than the expected effect. Accidental Photography. In the right subplot of Figures 48, Ideogram2.0 and Midjourney accurately adhere to the prompt, generating images that depict the concept of accidental photography. Dall-E3 and Jimeng, however, place too much emphasis on the detail of \"while holding phone,\" resulting in images that are inconsistent with the intended outcome. Score. The results of this experiment are shown in the Table 34. Midjourney achieved the best generation performance in this experiment. Since several models produced similar generation results, the differences in evaluation scores are not significant. Table 33: The scoring of generation results by six models on low quality under different evaluation systems. Refer to Section 2.4.9 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 26.99 24.15 26.92 24.08 25.38 27.71 0.25 0.25 0.26 0.28 0.27 0. 5.69 5.65 5.68 5.83 5.65 5.90 6.30 6.11 6.02 5.46 5.83 5.37 9.07 9.35 9.63 8.98 8.98 8.24 52 Figure 47: Results on low quality task. Refer to Section 2.4.9 for detailed discussions. Figure 48: Results on low quality task. Refer to Section 2.4.9 for detailed discussions. 53 2.4.10 Multi-image Recent studies [31, 50, 70, 69, 11] have demonstrated that advanced image generation models can produce grid-based compositions containing multiple images within single output. These grid-based images not only maintain certain level of contextual consistency across different cells but also extend the applicability of these models to various tasks, such as subject/style consistency, storyboard generation, and logical reasoning. This observation suggests two key insights: first, these models can leverage substantial amount of contextually coherent training data from existing internet sources during training; second, they exhibit certain degree of holistic generation capability, conditioned on prompts and contextual information. Motivated by these findings, we further assess the ability of state-of-the-art image generation models in producing contextually consistent grid images, paving the way for the evolution of image generation models toward more general-purpose intelligent agents [59, 61, 10]. Creation and Planning process. In Figure 49, we explore the models ability to generate creation and planning processes. It is evident that FLUX.1 and Ideogram2.0 can generate reasonable PPT creation processes and deduce plausible chess game strategies. Dall-E3, on the other hand, can accurately generate the process of creating artwork sketches based on our requirements. These three models effectively present the creative planning process in grid format. Figure 49: Results on creation and planning process task. Refer to Section 2.4.10 for detailed discussions. Style Consistency. In Figure 50, we tested the capability of model style consistency. The model was tasked with generating four sub-images within single image, each containing different objects, and converting them to the same style. FLUX.1 and Ideogram2.0 were able to follow the prompt relatively well. Dall-E3 and Midjourney could distinguish the sub-images and generate parts of the objects but with flaws. The capabilities of Stable Diffusion 3 and Jimeng were weaker, struggling to produce multi-image outputs and correctly render the objects. Additionally, in Figure 51, we designed tasks to generate app icons in unified style and to create couple avatars. These tasks tested the models ability to generate images with consistent styles. The results show that while the models can create images with consistent style, some of the icons in the first task lack realism, with Ideogram2.0 performing the best. In the second task, all models successfully generated two grids, with FLUX.1, Ideogram2.0, and Midjourney producing the highest-quality images. 54 Figure 50: Results on style consistency task. Refer to Section 2.4.10 for detailed discussions. Figure 51: Results on style consistency task. Refer to Section 2.4.10 for detailed discussions. 55 Storyboard. In Figure 52, we tested the models for storytelling. The models were tasked with generating multiple images that exhibit logical relationships, with consistent styles, matching backgrounds and subjects, and clear sense of temporal sequence. The models performed better when handling classic storylines, such as Little Red Riding Hood, compared to fabricated ones. Overall, FLUX.1 and Ideogram2.0 performed the best across all aspects, though there were still some logical inconsistencies, such as person brushing their teeth and having breakfast in bed. In the prompt designed for movie storyboard, FLUX.1 delivered the best results, while the other models exhibited issues such as background inconsistency. Figure 52: Results on storyboard task. Refer to Section 2.4.10 for detailed discussions. Subject Consistency. In Figures 53 to 57, we tested the models ability to generate multiple images with consistent subject. In the first prompt (Figure 53), all models successfully generated consistent subjecta dogbut only FLUX.1 was able to follow the prompt and generate detailed images in four grids. In the second prompt, all models performed well. In the third prompt, only FLUX.1 and Ideogram2.0 were able to follow the prompt correctly. In Figure 54, the subject is chair and logo. Most models were able to maintain subject consistency, with the exception of Stable Diffusion 3, Midjourney, and Jimeng, which produced images with inconsistencies. In Figure 55, the subject is woman. We changed both the background and the color of the womans dress. Most models maintained subject consistency, with only Stable Diffusion 3 and Jimeng showing slight differences between the subjects in the two grids. In Figure56, we tested the models to generate different facial expressions of the same person. Regardless of whether the person was real or anime-style, all models were able to maintain facial consistency relatively well. However, only Ideogram2.0 was able to generate the required expressions in the correct order with accuracy. Regarding multi-view generation capabilities, as illustrated in the Figure 57, most models demonstrate reasonable understanding and consistency when generating two perspectives. Regarding multi-view generation capabilities, as shown in Figure 57, most models demonstrate reasonable understanding and consistency when generating two perspectives. However, challenges arise when generating multiple perspectives. With the exception of FLUX.1, other models frequently generate repeated perspectives. 56 Figure 53: Results on subject consistency task. Refer to Section 2.4.10 for detailed discussions. Figure 54: Results on subject consistency task. Refer to Section 2.4.10 for detailed discussions. 57 Figure 55: Results on subject consistency task. Refer to Section 2.4.10 for detailed discussions. Figure 56: Results on subject consistency task. Refer to Section 2.4.10 for detailed discussions. 58 Figure 57: Results on subject consistency task. Refer to Section 2.4.10 for detailed discussions. Macimum Grid Count. In Figure 58 and 59, we tested the models to generate as many sub-images as possible within single image. When the number of sub-images reached 25, Stable Diffusion 3 first exhibited confusion regarding the quantity. At 36 sub-images, FLUX.1 and Midjourney were also unable to generate the correct number of sub-images. At 49 and 64 sub-images, only Ideogram2.0 was able to produce results that were close to accurate. In experiments with more than 64 sub-images, none of the models could generate the correct number of sub-images. Ability to Scale to Other Tasks. In this task, we explore the models in-context generation ability when scaled to other tasks, as shown in Figures 60 and 61. The models were tasked with generating two grids: the first one blurred and the second one clear. We found that FLUX.1 and Ideogram2.0 were able to accurately meet our requirements. We also extended the task to some computer vision tasks, such as depth estimation, optical flow, detection, and segmentation. However, the models struggled to achieve satisfactory results in these tasks. Score. In multi-image tasks, we found that FLUX.1 and Ideogram 2.0 performed exceptionally well across multiple tasks, demonstrating the potential of text-to-image models for in-context generation. Table 34: The scoring of generation results by six models on multi-image under different evaluation systems. Refer to Section 2.4.10 for detailed discussions. Model CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 26.99 27.15 26.92 22.08 25.38 27.71 0.25 0.25 0.26 0.28 0.28 0.26 5.69 5.65 5.68 5.68 5.65 5.90 6.30 6.02 5.83 5.46 5.83 5.37 9.63 9.35 9.07 8.98 8.24 8.24 Figure 58: Results on maximum grid count task. Refer to Section 2.4.10 for detailed discussions. Figure 59: Results on maximum grid count task. Refer to Section 2.4.10 for detailed discussions. 60 Figure 60: Results on ability to scale to other tasks. Refer to Section 2.4.10 for detailed discussions. Figure 61: Results on ability to scale to other tasks. Refer to Section 2.4.10 for detailed discussions. 2.4.11 Text Writing In Figures 62, we examine the models ability to generate images containing accurate text. The results demonstrate that none of the models consistently produce correct text within images. Specifically, Jimeng fails to generate correct words, instead producing distorted or illegible characters. Dall-E3, 61 Midjourney, and Stable Diffusion 3 occasionally include text with unreadable characters. FLUX.1 and Ideogram2.0 show better performance, though they still generate text with misspelled words or incomplete phrases. Score. The results of this experiment are shown in the Table 35, where FLUX.1 and Ideogram2.0 performed the best. Only the results of HPSv2 aligned closely with human perception, possibly because the evaluation of this task requires certain understanding of the text information in the images. Table 35: The scoring of generation results by six models on text writing under different evaluation systems. Refer to Section 2.4.11 for detailed discussions."
        },
        {
            "title": "Model",
            "content": "CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 37.01 35.33 35.18 42.27 46.14 36.03 0.29 0.28 0.25 0.28 0.28 0.25 4.34 4.94 3.94 4.19 4.23 5.81 5.83 5.00 4.17 6.67 3.33 5. 8.33 8.33 6.67 5.00 3.33 3.33 Figure 62: Results on text writing task. Refer to Section 2.4.11 for detailed discussions. 62 2.5 Multi-style Creation Task In this experiment, we selected over 30 commonly used styles in text-to-image tasks and tailored the prompts accordingly. Additionally, we examined the models imagination by combining seemingly contradictory styles and objects, such as floating chiffon in marble texture, to test how well the model can generate coherent and creative images from these combinations. We evaluated the generation results from six models, with detailed scores presented in Table 36 and visualizations provided in Figures 63-73. We found that the performance of the models in this task did not vary significantly, but the images generated by Midjourney often exhibited greater aesthetic appeal and better alignment with human perception of beauty. For example, in the middle subplot of Figure 63, all models can produce images in the isometric anime style, but the images generated by Midjourney and Jimeng stand out for their aesthetic quality. Similarly, in the left subplot of Figure 67, only Midjourney is able to generate marble sculpture that conveys the most fluidity and grace. Score. Notably, while Stable Diffusion 3 [64] demonstrated room for improvement, the other models exhibited relatively similar performance. Among the six models, Midjourney exhibited superior generalization capabilities and higher aesthetic quality, securing higher scores. The results from GPT-4o were closely aligned with human aesthetic judgments, while the outputs from other evaluation systems showed notable discrepancies from human perceptions of aesthetics. Table 36: The scoring of generation results by six models on different style image generation under different evaluation systems. Refer to Section 2.5 for detailed discussions. Metric CLIPScore HPSv2 Aesthetic Score GPT-4o Human FLUX.1 Ideogram2.0 Dall-E3 Midjourney SD3 Jimeng 30.49 30.96 31.10 30.26 29.51 30. 0.29 0.29 0.29 0.28 0.28 0.28 6.07 6.12 6.04 5.99 6.01 5.98 7.13 6.69 6.94 7.13 6.89 6.30 9.26 9.26 9.44 9.61 8.68 9.36 Figure 63: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 64: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 65: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. 64 Figure 66: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 67: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 68: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 69: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. 66 Figure 70: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 71: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 72: Results on different style image generation task. Refer to Section 2.5 for detailed discussions. Figure 73: Results on different style image generation task. Refer to Section 2.5 for detailed discussions."
        },
        {
            "title": "3 Conclusion",
            "content": "3.1 Summary In this report, we conducted comprehensive evaluation of six powerful text-to-image models, termed IMAGINE-E, including FLUX.1, Ideogram2.0, Dall-E3, Midjourney, Stable Diffusion 3, and Jimeng. We extensively explored the performance of these models across various levels of difficulty, including qualitative samples and quantitative benchmarks. To make our evaluation more thorough, we carefully collected numerous samples covering six aspects: structured output generation, realism and physical consistency tasks, specific domain generation, challenging scenario generation, and different style image generation. Each domain also includes several more detailed subtasks for in-depth discussion and analysis. 3.2 Task Complexity Analysis In this work, unlike traditional text-to-image generation, we focus on exploring and tackling challenging and specialized image generation domains to test the robustness and potential of generic T2I models. Currently, all models still face difficulties in code generation tasks, including generating simple Python code, QR codes, and barcodes, indicating that applying text-to-image models to general code generation remains significant challenge. Moreover, in 3D generation tasks, all models perform poorly, and the generated images cannot be directly used for 3D-related applications. In structured output tasks, none of the models can accurately follow prompts to generate images or tables. Although FLUX.1 can generate chart images that are close to correct, there are still discrepancies in detail compared to the prompts. Additionally, all models are unable to output images containing Chinese text. However, all models can accept JSON-format inputs and generate images according to the specified JSON content. In some basic tasks, such as generating images in different styles or based on photographic terminology, the models perform well and can produce high-quality images following the prompts. In summary, the current performance of these models in specific image generation domains still faces some common bottlenecks: on the one hand, there are certain unachieved functionalities or difficult tasks to overcome, and on the other hand, there are some basic capabilities that have already been realized. Further analysis is needed to identify the challenges of each task in the current design. 3.3 Model Performance Evaluation FLUX.1 and Ideogram2.0. Overall, FLUX.1 and Ideogram2.0 perform the best. In structured output tasks, FLUX.1s outputs can closely match the images, tables, and web pages described in the prompts. In realism and physical consistency tasks, both models can largely address issues of human deformities and have basic understanding of the fundamental laws of the physical world, though the generated images still exhibit some logical inconsistencies. In specific domain generation tasks, these two models excel in understanding foundational knowledge in disciplines such as chemistry, biology, and medicine, showing potential as general models, and can generate high-quality data for autonomous driving and embodied intelligence tasks. In challenging scenario generation tasks, FLUX.1 and Ideogram2.0 perform exceptionally well in generating content with dense text, accepting inputs in different languages and emojis. Midjourney. Midjourney produces images with the best aesthetic appeal, particularly evident in generating images of different styles. In everyday contexts, Midjourneys images are more visually pleasing, making them more practical. Dall-E3. Dall-E3 has profound understanding of the physical world and boasts higher safety measures, being sensitive to copyright, watermarks, and NSFW prompts and images. Additionally, the images generated by Dall-E3 have distinct style that sets them apart from all other models. Stable Diffusion 3 and Jimeng. Stable Diffusion 3 and Jimeng do not match FLUX.1 in overall generation quality. Jimeng is particularly sensitive to special symbols in prompts and also has high safety measures. 69 3.4 Quantitative Benchmark Assessment Currently, the concentrated quantitative evaluation benchmarks, such as CLIPScore, HPSv2, and Aesthetic Score, cannot reasonably assess model outputs in more challenging tasks. The evaluation results from GPT-4o are more comprehensive and reasonable; however, in tasks that require comparing image details for evaluation, GPT-4os results still significantly differ from human intuitive perceptions."
        },
        {
            "title": "References",
            "content": "[1] Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, and Ihsan Ullah. Synthesizing cta image data for type-b aortic dissection using stable diffusion models. 2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 15, 2024. [2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Unified pretraining for program understanding and generation. ArXiv, abs/2103.06333, 2021. [3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. [4] Bortik Bandyopadhyay, Xiang Deng, Goonmeet Bajaj, Huan Sun, and Srinivasan Parthasarathy. Automatic table completion using knowledge base. ArXiv, abs/1909.09565, 2019. [5] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022. [6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [8] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 2020. [11] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion self-distillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. [12] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024. [13] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2023. 70 [14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. [15] Pei Chen, Fudong Wang, Yixuan Tong, Jingdong Chen, Ming Yang, and Ming Yang. Graphicsdreamer: Image to 3d generation with physical consistency. 2024. [16] Yukang Chen, Tong Yang, X. Zhang, Gaofeng Meng, Xinyu Xiao, and Jian Sun. Detnas: Backbone search for object detection. In Neural Information Processing Systems, 2019. [17] Chin-Yi Cheng, Ruiqi Gao, Forrest Huang, and Yang Li. Colay: Controllable layout generation through multi-conditional latent diffusion. ArXiv, abs/2405.13045, 2024. [18] Victor C. Dibia and agatay Demiralp. Data2vis: Automatic generation of data visualizations using sequence-to-sequence recurrent neural networks. IEEE Computer Graphics and Applications, 39:3346, 2018. [19] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: pre-trained model for programming and natural languages. ArXiv, abs/2002.08155, 2020. [20] FLUX. Flux, 2024. Accessed: 2024-01-20. [21] Peng Gao*, Jiaming Han*, Renrui Zhang*, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. [22] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jingwen He, Yu Jiao Qiao, and Hongsheng Li. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. ArXiv, abs/2405.05945, 2024. [23] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. ArXiv, abs/2303.14897, 2023. [24] Ziyu Guo, Renrui Zhang, Hao Chen, Jialin Gao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Sciverse: Multimodal scientific benchmark for large models, 2024. [25] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzhi Li, and Pheng Ann Heng. Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training. IJCAI 2023, 2023. [26] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615, 2023. [27] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, and Pheng-Ann Heng. Sam2point: Segment any 3d as videos in zero-shot and promptable manners. arXiv preprint arXiv:2408.16768, 2024. [28] Jiaming Han*, Renrui Zhang*, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905, 2023. 71 [29] Hwan Heo, Jangyeong Kim, Seongyeong Lee, Jeonga Wi, Junyoung Choi, and Sangjun Ahn. Capa: Carve-n-paint synthesis for efficient 4k textured mesh generation. 2025. [30] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [31] Lianghua Huang, Wei Wang, Zhigang Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, YuIn-context lora for diffusion transformers. ArXiv, tong Feng, Yu Liu, and Jingren Zhou. abs/2410.23775, 2024. [32] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning, pages 1391613932. PMLR, 2023. [33] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided reconstruction of lifelike clothed humans. 2024 International Conference on 3D Vision (3DV), pages 15311542, 2023. [34] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. 2025. [35] Mude Hui, Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yuwang Wang, and Yan Lu. Unifying layout generation with decoupled diffusion model. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19421951, 2023. [36] Ideogram2.0. Ideogram2.0, 2024. Accessed: 2024-08-21. [37] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. arXiv preprint arXiv:2404.03653, 2024. [38] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. [39] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [40] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 51285137, 2021. [41] Hyeonwoo Kim, Sangwon Beak, and Hanbyul Joo. David: Modeling dynamic affordance of 3d objects using pre-trained video diffusion models. 2025. [42] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. [43] Hsin-Ying Lee, Hung-Yu Tseng, and Ming-Hsuan Yang. Exploiting diffusion prior for generalizable dense prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78617871, 2024. [44] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [45] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 72 [46] Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, and Umar Iqbal. Simavatar: Simulation-ready avatars with layered hair and clothing. 2024. [47] Renyang Liu, Ziyu Lyu, Wei Zhou, and See-Kiong Ng. Anid: How far are we? evaluating the discrepancies between ai-synthesized images and natural images through multimodal guidance. 2024. [48] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and C. Rupprecht. Continual detection transformer for incremental object detection. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2379923808, 2023. [49] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. survey of deep learning for mathematical reasoning. ArXiv, abs/2212.10535, 2022. [50] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. [51] Mohamed Lamine Mekhalfi, Davide Boscaini, and Fabio Poiesi. Leveraging confident image regions for source-free domain-adaptive object detection. 2025. [52] Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Phybench: physical commonsense benchmark for evaluating text-to-image models. ArXiv, abs/2406.11802, 2024. [53] Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, and Yu Yamaguchi. One-d-piece: Image tokenizer meets quality-controllable compression. 2025. [54] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? 2025. [55] Yongyu Mu, Hengyu Li, Junxin Wang, Xiaoxuan Zhou, Chenglong Wang, Yingfeng Luo, Qiaozhi He, Tong Xiao, Guocheng Chen, and Jingbo Zhu. Boosting text-to-image generation via multilingual prompting in large multimodal models. 2025. [56] OpenAI. Gpt-4o system card. ArXiv, abs/2410.21276, 2024. [57] Shuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang. Mathbert: pre-trained model for mathematical formula understanding. ArXiv, abs/2105.00377, 2021. [58] Xiangyuan Peng, Huawei Sun, Kay Bierzynski, Anton Fischbacher, Lorenzo Servadei, and Robert Wille. Mutualforce: Mutual-aware enhancement for 4d radar-lidar 3d object detection. 2025. [59] Alec Radford. Improving language understanding by generative pre-training. 2018. [60] Alec Radford, Jongwook Choe, and et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. [61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. [62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. [63] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. Highresolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, 2021. [64] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 73 [65] Subham Sah, Rishab Mitra, Arpit Narechania, Alex Endert, John Stasko, and Wenwen Dou. Generating analytic specifications for data visualization from natural language queries using large language models. ArXiv, abs/2408.13391, 2024. [66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [67] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. [68] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. ArXiv, abs/2210.08402, 2022. [69] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-toimage model with inpainting is zero-shot subject-driven image generator. arXiv preprint arXiv:2411.15466, 2024. [70] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [71] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. [72] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. ArXiv, abs/2109.00859, 2021. [73] Zelun Wang and Jyh-Charn S. Liu. Translating mathematical formula images to latex sequences using deep neural networks with sequence-level training. ArXiv, abs/1908.11415, 2019. [74] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Grard Dray, and Walid Maalej. On ai-inspired ui-design. ArXiv, abs/2406.13631, 2024. [75] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, and Grard Dray. Boosting gui prototyping with diffusion models. 2023 IEEE 31st International Requirements Engineering Conference (RE), pages 275280, 2023. [76] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. ArXiv, abs/2306.09341, 2023. [77] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1309513105, 2023. [78] Sen Xing, Muyan Zhong, Zeqiang Lai, Liangchen Li, Jiawen Liu, Yaohui Wang, Jifeng Dai, and Wenhai Wang. Mulan: Adapting multilingual diffusion models for hundreds of languages with negligible cost. ArXiv, abs/2412.01271, 2024. [79] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 74 [80] Lei Xu and Kalyan Veeramachaneni. Synthesizing tabular data using generative adversarial networks. ArXiv, abs/1811.11264, 2018. [81] M. Xu and et al. Aesthetic image classification using deep learning. In Proceedings of the ACM on Multimedia Conference, pages 471475. ACM, 2016. [82] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun yue Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. ArXiv, abs/2310.11441, 2023. [83] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [84] Beiyuan Zhang, Yue Ma, Chunlei Fu, Xinyang Song, Zhenan Sun, and Ziqiang Li. Follow-yourmultipose: Tuning-free multi-character text-to-video generation via pose guidance. 2024. [85] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [86] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [87] Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In ICLR 2024, 2024. [88] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ECCV 2024, 2024. [89] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li. Personalize segment anything model with one shot. ICLR 2024, 2023. [90] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. [91] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment using deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology, 30:3647, 2019."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University"
    ]
}