{
    "paper_title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
    "authors": [
        "Zigeng Chen",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the rapidly advancing field of image generation, Visual Auto-Regressive (VAR) modeling has garnered considerable attention for its innovative next-scale prediction approach. This paradigm offers substantial improvements in efficiency, scalability, and zero-shot generalization. Yet, the inherently coarse-to-fine nature of VAR introduces a prolonged token sequence, leading to prohibitive memory consumption and computational redundancies. To address these bottlenecks, we propose Collaborative Decoding (CoDe), a novel efficient decoding strategy tailored for the VAR framework. CoDe capitalizes on two critical observations: the substantially reduced parameter demands at larger scales and the exclusive generation patterns across different scales. Based on these insights, we partition the multi-scale inference process into a seamless collaboration between a large model and a small model. The large model serves as the 'drafter', specializing in generating low-frequency content at smaller scales, while the smaller model serves as the 'refiner', solely focusing on predicting high-frequency details at larger scales. This collaboration yields remarkable efficiency with minimal impact on quality: CoDe achieves a 1.7x speedup, slashes memory usage by around 50%, and preserves image quality with only a negligible FID increase from 1.95 to 1.98. When drafting steps are further decreased, CoDe can achieve an impressive 2.9x acceleration ratio, reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU, while preserving a commendable FID of 2.27. The code is available at https://github.com/czg1225/CoDe"
        },
        {
            "title": "Start",
            "content": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient Zigeng Chen , Xinyin Ma , Gongfan Fang , Xinchao Wang* National University of Singapore {zigeng99, maxinyin, gongfan}@u.nus.edu, xinchao@nus.edu.sg https://github.com/czg1225/CoDe 4 2 0 2 6 2 ] . [ 1 7 8 7 7 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In the rapidly advancing field of image generation, Visual Auto-Regressive (VAR) modeling has garnered considerable attention for its innovative next-scale prediction approach. This paradigm offers substantial improvements in efficiency, scalability, and zero-shot generalization. Yet, the inherently coarse-to-fine nature of VAR introduces leading to prohibitive memprolonged token sequence, ory consumption and computational redundancies. To address these bottlenecks, we propose Collaborative Decoding (CoDe), novel efficient decoding strategy tailored for the VAR framework. CoDe capitalizes on two critical observations: the substantially reduced parameter demands at larger scales and the exclusive generation patterns across different scales. Based on these insights, we partition the multi-scale inference process into seamless collaboration between large model and small model. The large model serves as the drafter, specializing in generating low-frequency content at smaller scales, while the smaller model serves as the refiner, solely focusing on predicting high-frequency details at larger scales. This collaboration yields remarkable efficiency with minimal impact on quality: CoDe achieves 1.7x speedup, slashes memory usage by around 50%, and preserves image quality with only negligible FID increase from 1.95 to 1.98. When drafting steps are further decreased, CoDe can achieve an impressive 2.9x acceleration ratio, reaching 41 images/s at 256x256 resolution on single NVIDIA 4090 GPU, while preserving commendable FID of 2.27. 1. Introduction The past year has witnessed significant advancements in Auto-Regressive (AR) models for image generation [18, 24, 28, 33, 52, 64], driven by their proven scalability and strong generalization capabilities [1, 9, 16, 56]. Leveraging these strengths, AR approaches have demonstrated re- *Corresponding author Figure 1. We partition the next-scale prediction process into the efficient collaboration between large and small VAR models. markable image quality and promising multi-modal potential [34, 61, 75]. However, the next-token prediction process inherent to conventional AR models requires numerous decoding steps, leading to considerable generation latency. Visual Auto-Regressive (VAR) Modeling [55] replaces the GPT-style next-token prediction with next-scale prediction strategy. It generates content in multi-level, coarseto-fine progression, enabling the model to decode multiple tokens in parallel [31, 45, 53, 68, 70], thus considerably reducing the inference steps. Despite less decoding steps, VARs progressive scaling approach significantly increases the overall sequence length. To generate 16x16 token image, VAR hierarchically decodes up to 680 tokens across 10 scales2.7 times the sequence length required by conventional AR models. As the VAR model must store the KV cache accumulated from all previous tokens, this prolonged sequence results in substantial memory overhead, especially in the final scales. During inference, the KV cache consumes approximately 12 times more memory than the forward computaFigure 2. Comparison of generation results between original VAR-d30 (up) and our VAR-CoDE (bottom) for ImageNet 256256. Our method achieves 1.7x speedup (3.62s to 2.11s), and needs only 0.5x memory space (40GB to 20GB), with negligible quality degradation. tion itself. For instance, generating images using VAR-d30 with batch size of 128 demands 70 GB of memory, with 57 GB dedicated solely to KV caching, becoming the primary bottleneck. Moreover, the extended sequence length exacerbates the computational cost of self-attention, given the quadratic growth in attention map calculations. To address these inefficiencies, we begin by analyzing the specific properties of VARs next-scale prediction paradigm. First, we observe that as generated scales increase, the parameter demands for high-quality token generation drop substantially, leading to considerable computational redundancy at most of the tokens in the long sequence. Next, we find that the generation patterns at small and large scales are exclusive, resulting in mutual interference between learning lowand high-frequency modeling capability, thus hindering efficient parameter utilization. Our Approach. Inspired by the above observations, we propose Collaborative Decoding (CoDe), simple yet highly effective method that significantly boosts the inference efficiency of VAR models while preserving generation quality comparable to the original. Our core idea is to decompose the long-sequence scaling-up process into collaboration between two VAR models with different sizes and specialized roles to enhance efficiency. Figure 1 presents the overview of our method. We use large VAR model as drafter for the initial small scales, where model capacity demands are high but computations and KV cache are sparse. Conversely, we use small VAR model as refiner at the remaining large scales, where computations and KV cache are intensive but fewer parameters are needed. Next, we apply specialized fine-tuning to both models to address the optimization interference between scales encountered during the pre-training stage. Each model is fine-tuned exclusively on the specific scales it handles, leading to notable performance boost with limited additional cost."
        },
        {
            "title": "Extensive experiments validate the effectiveness of our",
            "content": "method. As shown in Figure 2, compared to the original VAR-d30 model, CoDe achieves 1.7x speedup with merely 0.5x GPU memory consumption and negligible quality loss (FID [19] slightly increases from 1.95 to 1.98). Notably, our approach can reach up to 2.9x speedup, generating 41 images (256x256) per second on single NVIDIA 4090 GPU, while still maintaining an FID of 2.27. In conclusion, we introduce CoDe, novel decoding framework for visual auto-regressive modeling that significantly enhances speed and reduces memory overhead with negligible impact on quality. CoDe divides the hierarchical sequence modeling of VAR into collaborative process, utilizing large VAR model and small VAR model in progressive partnership. Additionally, we propose specialized fine-tuning to optimize each model for its specific role, effectively mitigating training interference and maximizing parameter utilization. To the best of our knowledge, CoDe is the fastest method available to achieve an FID below 2, making it clear advancement in efficient image generation. 2. Related Works Auto-regressive image generation. Early works [4, 57] pioneered image generation by generating pixels in rasterscan order. Later, VQVAE [58] and VQGAN [10] improved this approach by quantizing image patches into discrete tokens, using transformer in decoder-only setup to generate these tokens in raster-scan manner. Building on these foundations, recent efforts have focused on enhancing autoregressive (AR) models for image generation. LlamaGEN [52] and Lumina-mGPT [34], for example, use GPT-style next-token prediction strategy to achieve high-quality image generation with good scalability. AiM [24] and MARS [18] further improve this paradigm by introducing mixtureof-experts and linear attention mechanisms [14]. Methods like SHOW-O [61], Transfusion [75], and DART [15] combine diffusion processes with autoregressive modeling, Figure 3. (a) Effectiveness of increasing parameters at the k-th scale is evaluated by predicting token map rk using four VAR models with different parameter sizes (2B, 1B, 0.6B, and 0.3B), while other scales (r1, r2, . . . , rk1, rk+1, . . . , r10) are generated using the largest VAR-d30 model. (b) Fourier spectrum analysis is conducted on generated content at the first 3 scales and the last 3 scales. (c) Training-free performance comparison of model collaboration decoding across various settings of draft tokens and refiner tokens 680 . while [3, 28, 38] introduces masked autoregression to generate images. However, all these approaches suffer from high latency due to the large number of forward steps. VAR [55] addresses this by using hierarchical parallel decoding, which greatly reduces the number of steps, leading to significantly lower inference latency without sacrificing quality. Building on VARs progress, many recent works have aimed to improve next-scale prediction across multiple tasks, including text-to-image [30, 41, 53, 70], controllable generation [31, 33], audio generation [45], and 3D generation [68]. Efficient Image Generation Models. For diffusion models, acceleration techniques are already well-developed. [37, 47, 48, 65, 66] focuses on reducing sampling steps while [12, 32, 63, 67, 73] optimize model architectures through pruning [5, 11] or knowledge distillation [20]. To avoid the high costs of training, some training-free methods are proposed. Some approaches develop fast solvers for stochastic differential equations (SDE) or ordinary differential equations (ODE) to enhance sampling efficiency [2, 35, 36, 69, 74]. Other works [27, 39, 40, 50, 60, 62, 71, 72] exploit specific characteristics of diffusion models to skip redundant computations during the denoising pro- [6, 13, 25] reduce latency to another level through cess. distributed computing. Quantization methods [26, 29, 49] have also shown great potential. In contrast, research into accelerating AR image generation is still in its early stages. LANTERN [21] and SJD [54] employ speculative decoding to speed up next-token prediction, achieving notable speedups but not suitable for the innovative next-scale prediction paradigm introduced by VAR. In this paper, we introduce CoDe, novel and efficient decoding method tailored for the next-scale prediction paradigm. CoDe effectively addresses the inefficiencies associated with the long sequence structure of VAR models while maintaining high generation quality. Notably, CoDe stands out as the fastest method capable of achieving an FID below 2, making it clear advancement. 3. Method 3.1. Prelinimary Visual auto-regressive modeling [55] redefines the traditional AR by shifting from next-token prediction to In this framework, each autoa next-scale prediction. regressive unit is token map at varying scales, rather than single token. For given image feature map RhwC, VAR quantizes it into multi-scale token maps = (r1, r2, . . . , rK) at progressively finer resolutions, with the final token map rK matching the original feature maps resolution. The probability distribution is reformulated as: p(r1, r2, . . . , rK) = (cid:89) k=1 p(rk r1, r2, . . . , rk1), (1) where each token map rk [V ]hkwk consists of hk wk tokens at scale k, and the sequence (r1, r2, . . . , rk1) serves as the prefix for rk. In this paradigm, during each autoregressive step k, the model predicts all hk wk tokens in rk in parallel, conditioned on prior scales and position embeddings. This approach aligns with coarse-to-fine generation pattern, enabling parallel decoding within each scale. VAR effectively improves inference speed and generation quality, but it also considerably increases sequence length. 3.2. Key Observations VAR represents an innovative paradigm specific for visual generation, distinct from traditional autoregressive approaches and introducing many yet unexplored characteristics. In this work, we revisit the entire next-scale prediction process to uncover specific properties that can be optimized to reduce computational redundancy and improve inference efficiency. Observation 1: As the predicted scale becomes larger, the need for parameters reduces significantly. While VAR models exhibit strong scalability, the effectiveness of increasing parameter counts varies greatly in predicting varFigure 4. Overview of the collaborative decoding process, we use drafting step = 6 for instance. CoDe uses large VAR model as the drafter ϵθd to generate the token maps RL = (r1, r2, . . . , rN ) at smaller scales. The small refiner model ϵθr then uses RL as an initial prefix to efficiently predict the remaining token maps RH = (rN +1, rN +2, . . . , rK ) at larger scales. Both models are fine-tuned on their designated predictive scales using ground truth labels and teacher logits pteacher(rk), respectively. In Figure 3. (a), we present the differences ious scales. in class-conditional generation (ImageNet-256 [8]) quality when using VAR models of different sizes at each scale. At the initial small scales, increasing model parameters leads to noticeable improvements in generation quality. However, as the predicted scale gradually progresses, the impact of additional parameters becomes minimal. At the final scalewhich accounts for 38% tokens of the entire sequencewe observe that the performance of the 2B model is even nearly equivalent to that of the 0.3B model. These findings suggest that as the predicted scale increases, the parameter demand for accurate token predictions declines markedly, revealing substantial computational redundancy in the current VAR inference process at large scales. Observation 2: The generative patterns are exclusive between small and large scales. For VAR models, the content generated at small and large scales varies significantly. As demonstrated by the Fourier analysis in Figure 3 (b), the feature maps produced by the first three scales contain primarily low-frequency components, while the last three scales focus on high-frequency components. To further validate these distinct generative patterns, we conducted perturbation fine-tuning experiment. Using pre-trained VARd16 model, we applied CSE loss only to tokens in the largest three scales and fine-tuned for just 1% of the original training epochs. This minor fine-tuning at large scales led to complete collapse of the models global modeling capacity at small scales, with the FID increasing from 3.30 to 21.93 and the IS score dropping from 277 to 88. These results demonstrate that VAR models perform entirely distinct generative tasks at small and large scales, with minimal overlap. Training single VAR model to predict across all scales results in significant mutual interference between learning low-frequency and high-frequency modeling capabilities, making parameter optimization challenging. In sum, the key insights from our observations can be concluded as follows: (1) At large scales, VAR models require significantly fewer parameters for accurate token prediction, leading to considerable computational redundancy. (2) Training single model to generate across all scales causes mutual interference between small and large scales, hindering parameter optimization due to conflicting lowfrequency and high-frequency learning tendencies. 3.3. Collaborative Decoding Based on the above insights, we propose simple yet pretty efficient decoding method for next-scale prediction called collaborative decoding. As presented in Figure 4, CoDe decomposes the next-scale prediction process into collaboration between large drafter model and small refiner model. The drafter is the original large VAR model with 2B parameters, while the refiner is lightweight 0.3B VAR model, sharing the same transformer-based architecture but with reduced width and depth. The drafter is responsible for generating coarse, low-frequency global structures at smaller scales to draft the image, while the refiner continually predicts the high-frequency details at larger scales to refine the image. Model Collaboration. To generate an image im, original VAR model needs to auto-regressively generate multiscale token maps = (r1, r2, . . . , rK) with progressively finer resolutions, where each token map rk [V ]hkwk represents the token map at scale k. In CoDe, we decompose the auto-regressive steps into drafting steps and refining steps. In the initial drafting phase where computation is sparse but additional parameters provide significant benefits, we serve large VAR model as the drafter ϵθd , generating the initial set of low-frequency token maps RL = (r1, r2, . . . , rN ) up to scale , where < K. These token maps predict the global structure of the image, serving as prefix for the remaining finer-scale token maps. The drafting process can be represented as: pθd (r1, r2, . . . , rN ) = (cid:89) k=1 pθd (rk r1, r2, . . . , rk1). (2) After drafting, the KV cache of the large model is released, significantly optimizing memory usage. In the refining stage where the computation is intensive but the parameter demand is low, we serve small VAR model as the refiner ϵθr , employing the drafters predictions RL as prefix and focus on refining the details by generating the remaining high-frequency token maps RH = (rN +1, rN +2, . . . , rK) up to scale K, where rK matches the resolution of the original feature map. Notably, predicting rN +1 requires an attention mask, as the refiner model lacks the KV cache for the first scales. The probability distribution of refining steps is formulated as: p(rN +1, rN +2, . . . , rK RL) (cid:89) = k=N +1 p(rk RL, rN +1, rN +2, . . . , rk1). (3) Finally, the generated images im are reconstructed from both drafting maps RL and refining maps RH as follows: im = D(Q(RL, RH ), (4) where Q(.) is residual-style quantization function, and D(.) is multi-scale VQVAE decoder. Figure 3. (c) demonstrates the remarkable effectiveness of model collaboration under training-free conditions. With large model as the drafter, the small refiner only results in slight and gentle increase in FID. Conversely, with small model as the drafter, even assigning the last 80% tokens to large refiner fails to improve performance. This also verifies Observation 1: small scales require more model capacity, whereas the parameter demands for large scales are low. The model collaboration method effectively reduces the computational redundancy of VAR and maintains generation quality comparable to the original model, while significantly offering faster speed and reduced memory usage. Specialized Fine-Tuning. Given the exclusive generation patterns between drafting scales and refining scales, we propose the specialized fine-tuning to further specialize drafter and refiner models by fine-tuning them solely on their respective predictive scales, thereby avoiding training interference and enhancing generation quality. For the drafter model ϵθd , we fine-tune the drafting steps to improve the generation of the initial low-frequency token maps RL. The drafter model is trained using CSE loss CSE(, ) between its generated token distribution pθd (rk) and the ground truth labels k, defined as: Ldrafter = (cid:88) k=1 CSE(pθd (rk), k), (5) which encourages the drafter to closely match the target global structure at each drafting step. For the refiner model ϵθr , we adopt Knowledge Distillation (KD) [20] approach to partially transfer the knowledge from larger pre-trained VAR model. The KD loss function for epoch ep can be expressed as: (cid:88) L(ep) refiner = (cid:0)λep1[kN ]+1[k>N ] (cid:1) KL(pθr (rk) pteacher(rk)), k=1 (6) where pteacher() is the distribution predicted by the larger teacher VAR model, KL(, ) is the standard KL-divergence loss aligns refiners output distribution with that of the teacher model, and λep is dynamic weighting factor. λt linearly decreases from one to zero from the initial to the end of finetuning, gradually shifting the KD emphasis from all tokens to the specific refining tokens. This dynamic weighting allows the refiner to specialize in refining the details with smoother and more stable training process. The specialized fine-tuning of the drafter and refiner models ensures that each model becomes highly proficient in its respective task. By minimizing interference between small and large scales, this approach allows for more sufficient parameter optimization, resulting in enhanced image generation quality with limited training costs. 4. Experimental Results 4.1. Experimental Setup. Implementation Details. We evaluated our methods effectiveness on the ImageNet1K [8] class-conditional generation benchmarks. The proposed CoDe framework involves collaboration between large and small VAR model. Specifically, we use the pre-trained VAR-d30 model as the drafter and the pre-trained VAR-d16 model as the refiner. Each model undergoes specialized fine-tuning focused exclusively on its respective predictive scales. For the drafter, we fine-tune it for 5% of its original training epochs with base learning rate of 1e-6 and weight decay of 0.08. The refiner model, on the other hand, is fine-tuned for 25% of its original training epochs with base learning rate of 1e-5 and no weight decay. Both models are optimized using the AdamW [23] optimizer with batch size of 1024, achieved through gradient accumulation. The fine-tuning was conducted on 4 NVIDIA L20 GPUs. (a) Our CoDe demonstrates the optimal efficiency-quality trade-off among all evaluated methods. Figure 5. (b) Inference latency is measured across varying batch sizes for the original VAR-d30, our CoDe (N=6), and the VQVAE decoder. (c) We analyze the time cost associated with parallel decoding at each scale, showing that the refiner model is significantly more efficient than the drafter at larger scales. Table 1. Quantitative assessment of the efficiency-quality trade-off across various methods. Inference efficiency is evaluated with batch size of 64 on NVIDIA L20 GPU, with latency measured excluding VQVAE or VQGAN as it incurs shared time cost across all methods. Method DiT-XL/2 MAR-B AiM-XL LlamaGen-XXL VAR-d30 VAR-d24 VAR-d VAR-CoDe N=9 VAR-CoDe N=8 VAR-CoDe N=7 VAR-CoDe N=6 Inference Efficiency Generation Quality #Steps Speedup Latency Throughput #Param Memory FID IS Precision Recall 50 100 256 384 10 10 10 9+1 8+2 7+3 6+ 0.2x 0.1x 0.4x <0.1x 1.0x 1.7x 2.8x 1.2x 1.7x 2.3x 2.9x 19.20s 29.80s 9.32s 73.97s 3.62s 2.07s 1.29s 2.97s 2.11s 1.60s 1.27s 3.33it/s 2.15it/s 6.87it/s 0.87it/s 17.71it/s 30.92it/s 49.62it/s 21.54it/s 30.33it/s 40.00it/s 50.39it/s 675M 208M 763M 1.4B 2.0B 1.0B 600M 2.0+0.3B 2.0+0.3B 2.0+0.3B 2.0+0.3B 11369MB 8725MB 20983MB 42632MB 39228MB 25093MB 17814MB 28803MB 21019MB 19943MB 19943MB 2.26 2.31 2.56 2.34 1.95 2.11 2.61 1.94 1.98 2.11 2. 239 282 257 254 301 311 301 296 302 303 297 0.80 0.82 0.82 0.80 0.81 0.82 0.83 0.80 0.81 0.82 0. 0.60 0.57 0.57 0.59 0.59 0.59 0.56 0.61 0.60 0.59 0.58 Table 2. Effect of specialized fine-tuning Discription CoDe Training-free CoDe Fine-tuning N=6 2.42 2.27 N=7 2.26 2.11 N=8 2.10 1. N=9 1.99 1.94 Evaluations. We evaluated our method on ImageNet1K256 generation, focusing on both quality and efficiency metrics. For quality assessment, we used standard metrics such as FID, Inception Score (IS), Precision, and Recall. For other baselines, we use their default sampling methods. For CoDe, we reduced the sampled top-k from 900 to 600 due to improved token prediction accuracy after fine-tuning, while keeping the default top-p at 0.96. To compensate for reduced diversity, we introduced temperature coefficient of = 1.1 for sampling on the smallest 7 scales. Efficiency was measured through inference latency, throughput, memory consumption, and parameter count to provide comprehensive comparison. Notably, the speed measurements exclude the VQGAN decoder, which contributes minimally to overall runtime and is shared component across all methods. All efficiency tests were conducted on single NVIDIA L20 GPU without additional optimizations such as FlashAttention [7], using PyTorch 2.1 [43] and FP16. 4.2. Main Results. Quality-Efficiency Trade-off. We evaluated the qualityefficiency trade-off of our proposed method (CoDe) against original VAR models [55], state-of-the-art AR models, and diffusion models. The AR models included GPT-style LlamaGEN [52], Mamba-based Aim [24], and MAE-style MAR [28]. For diffusion models, we employed the widely used DiT models [44]. To measure the trade-offs, we used series models with different parameter counts for VAR, LlamaGEN, Aim, and MAR, while for DiT we varied the DDIM [51] steps to generate the efficiency-quality curve. As illustrated in Table 1, We analyzed CoDes performance under different drafting steps N. Compared to traditional AR models, our proposed VAR-CoDe (N=6) achieves Figure 6. Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps. 60 times faster inference than LlamaGEN-XXL, while also surpassing its generation quality. Additionally, in comparison to diffusion models, VAR-CoDe (N=6) is 15 times faster than DiT-XL/2, while maintaining the same level of quality. Compared to the original VAR-d30, CoDe (N=8) achieves 1.7x speedup and reduces memory consumption by 50%, with only negligible FID increase from 1.95 to 1.98. When the drafting stage is reduced to just 6 steps, CoDe (N=6) achieves 2.9x speedup, reaching throughput of over 50it/s, while maintaining low FID of 2.27. This is speed unmatched by any other existing methods. Compared to other VAR models with fewer parameters (VARd24 and VAR-d20), our method achieves significantly better quality while maintaining the same speedup ratio. As shown in Figure 5 (a), VAR-CoDe achieves the best efficiency-quality trade-off compared to all other methods, effectively solving the inefficiencies introduced by the extended sequence length in the original VAR paradigm. Training-Free vs Specialized Finetuning. Our CoDe framework utilizes large drafter model alongside smaller refiner model for progressive inference. It can operate in training-free manner by directly using pre-trained VAR-d30 and VAR-d16 models as the drafter and refiner, respectively. Alternatively, we can perform specialized fine-tuning to further enhance the models performance at their respective scales, with limited training cost. Table 2 compares the results of training-free CoDe with the specialized fine-tuned version. Even without additional training, CoDe demonstrates competitive performance, outperforming VAR-d24 and VAR-d20 models with the same speedup ratio. With specialized fine-tuning, CoDes performance improves significantly, even achieving slight quality enhancement over the original model at 1.2x acceleration ratio. These results demonstrate the superiority of our approach over the conventional VAR paradigm and highlight the gains from specialized fine-tuning for optimal parameter utilization Qualitative Results. We provide an extensive qualitative comparison between the original VAR-d30 model and our proposed CoDe, with varying drafting steps = {6, 7, 8}. As illustrated in Figure 6, our approach achieves significant speedup and substantial memory optimization, with only minimal quality degradation that is nearly imperceptible to the human eye. Even at speedup rate of 2.9 times, the generated images maintain exceptionally high quality and accurate semantic information. It is essential to note that the objective of CoDe is to enhance the efficiency of VARs inference process while preserving generation quality, rather than mirroring the exact outputs of the original model. Through specialized fine-tuning, CoDes draft model exhibits higher predictive accuracy compared to the original model. This sometimes results in different global structure from the original output, yet the image quality remains consistently high or even better. Zero-shot Task Generalization. To evaluate the zero-shot generalization capability of CoDe, we conducted additional experiments on zero-shot class-conditional inpainting and image editing. During image inpainting, we applied teacher forcing by providing the ground truth tokens outside the masked area, allowing the model to generate tokens solely within the mask. Notably, class-conditional information was introduced to the model. In the image editing task, CoDe was restricted to generating tokens within given bounding box based on specific class label. Figure 7 illustrates the qualitative results of CoDes zero-shot performance (N=8). Our approach demonstrates strong zeroshot generalization without any additional training on these Figure 7. Qualitative results of CoDes zero-shot generalization on image inpainting and image editing. Table 3. Memory usage comparison across different batch sizes Method VAR (bs=8) +CoDe VAR (bs=16) +CoDe VAR (bs=32) +CoDe VAR (bs=64) +CoDe Memory Consumption Running KV Cache Params Total 314MB 284MB 615MB 557MB 1216MB 1103MB 2420MB 2195MB 3595MB 1023MB 7191MB 2056MB 14345MB 4083MB 28707MB 8160MB 8089MB 9275MB 8089MB 9275MB 8089MB 9275MB 8089MB 9275MB 12002MB 10619MB 15901MB 11951MB 23662MB 14614MB 39228MB 19943MB VAR (bs=128) +CoDe OOM(0.48GB) OOM(57GB) OOM(0.80GB) OOM(70GB) 4380MB 16320MB 9275MB 30598MB downstream tasks. 4.3. Efficiency Analysis. Time Cost Analysis. We first present the time cost (bs=64) for each decoding step of the VAR-d30 model in Figure 5 (c). Notably, the computational complexity across the ten decoding steps of VAR is highly non-uniform. The last three decoding steps alone account for 64% of the total inference time, with latency increasing quadratically as token map resolution grows. This highlights that addressing the efficiency bottleneck of large-scale predictions is crucial to enhance inference efficiency. Our solution is to replace the large VAR model with smaller one for the last few steps, as fewer parameters are needed for large-scale token maps. This small refiner is substantially faster than the original large drafter at these scales, achieving 4.6x speed improvement in the final and most computationally demanding step. This change dramatically accelerates the entire inference process compared to the original VAR model. Next, we present the speedup effect of CoDe (N=6) across different batch sizes. As shown in Figure 5.(b), CoDe achieves speedup of 1.6x for batch size 1 but reaches up to 2.9x for batch size 64. The reduced speedup at smaller batch sizes is due to low GPU utilization when generating 256x256 images, which limits the acceleration potential. However, as GPU utilization increases with larger batch sizes, the speedup of CoDe also improves significantly. This indicates that CoDe is well-suited for computationally intensive image generation tasks, and holds great potential for enhancing efficiency in high-resolution image generation under the VAR paradigm. Additionally, we report the time cost of the VQVAE decoder in the figure, which is constant shared cost across all methods. Memory Consumption Analysis. In Table 3, we provide detailed analysis of memory usage during the VAR inference process. The KV cache of the VAR model is the largest memory consumer, requiring 12 times the memory needed for the models decoding operation due to the significantly extended sequence length in the VAR paradigm. Our proposed CoDe effectively addresses this KV cache memory challenge. Specifically, we use the large VAR model only for predicting the first 91 tokens, which represent just 13% of the total sequence. After this, the KV cache in the drafter is released, and the remaining computation is handled by smaller model. This approach drastically reduces the KV cache memory requirements, compressing it to approximately 28% of the original VAR-d30. Although the refiner model adds small number of additional parameters, this overhead is minimal compared to the major optimization achieved in KV cache storage. As result, the total memory usage of CoDe is significantly lower than the original model. Moreover, as batch size increases, the memory savings with CoDe become even more pronounced, underscoring its potential to support efficient high-resolution image generation within the VAR paradigm. 5. Conclusion This work presents CoDe, novel method designed for efficient decoding in visual auto-regressive modeling. CoDe effectively mitigates the significant memory overhead and computational redundancy typically associated with the prolonged sequences of next-scale predictions. Through extensive experimentation, our method demonstrates superior efficiency-quality trade-off, establishing new benchmark for efficient, high-quality image generation."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytican analytic estimate of the optimal reverse variarXiv preprint dpm: ance in diffusion probabilistic models. arXiv:2201.06503, 2022. 3 [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 3 [4] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. 2 [5] Zigeng Chen, Gongfan Fang, Xinyin Ma, and Xinchao arXiv 0.1% data makes segment anything slim. Wang. preprint arXiv:2312.05284, 2023. 3 [6] Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing diffuarXiv preprint sion models by asynchronous denoising. arXiv:2406.06911, 2024. 3 [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 6 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 4, 5, 1 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [11] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1609116101, 2023. [12] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. Advances in neural information processing systems, 36, 2024. 3 [13] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transarXiv preprint formers (dits) with massive parallelism. arXiv:2411.01738, 2024. 3 [14] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2 [15] Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation. arXiv preprint arXiv:2410.08159, 2024. 2 [16] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [17] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45(1):87110, 2022. 2 [18] Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. 1, 2 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 2, 1 [20] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 3, 5 [21] Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive modarXiv preprint els with relaxed speculative decoding. arXiv:2410.03355, 2024. [22] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 1 [23] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [24] Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi Li. Scalable autoregressive image generation with mamba. arXiv preprint arXiv:2408.12245, 2024. 1, 2, 6 [25] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183 7193, 2024. 3 [26] Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by lowrank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [27] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. arXiv preprint arXiv:2312.09608, 2023. 3 [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 1, 3, 6 [29] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedings Q-diffusion: Quantizing diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 1753517545, 2023. 3 [30] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. 3 [31] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj. Controlvar: Exploring conarXiv preprint trollable visual autoregressive modeling. arXiv:2406.09750, 2024. 1, 3 [32] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. [33] Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Controlar: Controllable image and Xinggang Wang. arXiv preprint generation with autoregressive models. arXiv:2410.02705, 2024. 1, 3 [34] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 1, 2 [35] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 3 [36] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 3 [37] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3 [38] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [39] Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. arXiv preprint arXiv:2205.12524, 2022. 3 [40] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. 3 [41] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Huaian Chen, and Yi Jin. Star: Scale-wise text-toimage generation via auto-regressive representations. arXiv preprint arXiv:2406.10797, 2024. 3 [42] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. 1 [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 6 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 6 [45] Kai Qiu, Xiang Li, Hao Chen, Jie Sun, Jinglu Wang, Zhe Lin, Marios Savvides, and Bhiksha Raj. Efficient autoregressive audio modeling via next-scale prediction. arXiv preprint arXiv:2408.09027, 2024. 1, [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [47] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [48] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 3 [49] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 3 [50] Junhyuk So, Jungwon Lee, and Eunhyeok Park. Frdiff: Feature reuse for exquisite zero-shot acceleration of diffusion models. arXiv preprint arXiv:2312.03517, 2023. 3 [51] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 6 and Stefano Ermon. arXiv preprint [52] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 6 [53] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. 1, 3 [68] Jinzhi Zhang, Feng Xiong, and Mu Xu. G3pt: Unleash the power of autoregressive modeling in 3d generation via cross-scale querying transformer. arXiv preprint arXiv:2409.06322, 2024. 1, 3 [69] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. [70] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024. 1, 3 [71] Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jurgen Schmidhuber. Crossattention makes inference cumbersome in text-to-image diffusion models. arXiv preprint arXiv:2404.02747, 2024. 3 [72] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. 3 [73] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to-image generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. 3 [74] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpmsolver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36, 2024. 3 [75] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1, [54] Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating autoregressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. 3 [55] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 1, 3, 6 [56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [57] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 2 [58] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [59] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. [60] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. arXiv preprint arXiv:2312.03209, 2023. 3 [61] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2 [62] Xingyi Yang and Xinchao Wang. free acceleration for 3d generation. arXiv:2404.06091, 2024. 3 Hash3d: TrainingarXiv preprint [63] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2255222562, 2023. [64] Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, and Lei Li. Car: Controllable autoregressive modeling for visual generation. arXiv preprint arXiv:2410.04671, 2024. 1 [65] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. 3 [66] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: image superresolution by residual shifting. Advances in Neural Information Processing Systems, 36, 2024. 3 Efficient diffusion model for [67] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024. Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient"
        },
        {
            "title": "Supplementary Material",
            "content": "In this document, we provide supplementary materials that extend beyond the scope of the main manuscript, constrained by space limitations. These additional materials include: We provide more quantitative analysis results to further illustrate our approach; We offer more qualitative comparisons for visualization; We discuss the limitations of our approach and look into future work. A. Additional Quantitative Results In this section, we present additional quantitative analyses to further substantiate our approach. Impact of Increasing Model Parameters. To validate Observation 1, we analyze the effect of varying model sizes on class-conditional image generation quality using ImageNet-256 [8]. Specifically, we evaluate the impact of model size at the k-th scale by predicting the token map rk with four Visual Autoregressive (VAR) models [55] of different parameter sizes (2B, 1B, 0.6B, and 0.3B). For all other scales (r1, r2, . . . , rk1, rk+1, . . . , r10), the largest VAR-d30 model is used for generation. Detailed quantitative results are summarized in Table 4. Our results reveal that increasing model parameters at the earlier scales yields significant improvements in generation quality. However, as the scales progress, the marginal benefits of larger models diminish. At the final scaleresponsible for 38% of the sequence tokenswe observe that the performance of the 2B model is nearly identical to that of the 0.3B model. This indicates that as the predicted scale increases, the demand for model parameters to ensure accurate token predictions decreases substantially. These findings highlight significant computational redundancy in the current VAR inference process at larger scales. Training-Free Performance of CoDe. The proposed CoDe framework employs large drafter model in conjunction with smaller refiner model for progressive inference. Notably, it can operate in training-free manner by leveraging pre-trained VAR-d30 and VAR-d16 models as the drafter and refiner, respectively. Table 6 presents the performance of training-free CoDe across various drafting step settings = {1, 2, 3, 4, 5, 6, 7, 8, 9}. Even without additional training, CoDe achieves competitive performance, surpassing the VAR-d24 and VAR-d20 models while maintaining the same speedup ratio. Image Quality Assessment. In our paper, we use standard metrics such as FID [19], Inception Score (IS), Precision, and Recall to evaluate the generation quality. In orTable 4. Impact of increasing parameters across scales Scale Params FID IS Precision Recall 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 0.3B 0.6B 1.0B 2.0B 2.23 2.13 2.04 1.95 2.35 2.21 2.09 1.95 2.27 2.18 2.13 1.95 2.17 2.13 2.10 1.95 2.09 2.05 2.05 1.95 2.09 2.05 2.04 1. 2.08 2.04 2.02 1.95 2.02 2.01 2.00 1.95 1.99 1.97 1.98 1.95 291 292 295 301 283 290 295 301 290 293 296 296 298 301 301 301 304 305 301 302 305 307 301 304 308 307 301 304 307 307 301 306 305 303 0.8122 0.8078 0.8107 0.8107 0.8064 0.8047 0.8074 0.8107 0.8086 0.8068 0.8061 0.8107 0.8119 0.8087 0.8087 0.8107 0.8119 0.8100 0.8089 0.8107 0.8067 0.5095 0.8077 0. 0.8135 0.8110 0.8094 0.8107 0.8133 0.8121 0.8097 0.8107 0.8120 0.8102 0.8102 0.8107 0.5895 0.5947 0.6027 0.5945 0.5864 0.5967 0.5940 0.5945 0.5953 0.5924 0.5983 0. 0.5936 0.5948 0.6025 0.5945 0.5984 0.5976 0.5999 0.5945 0.6010 0.6061 0.6008 0.5945 0.5978 0.6024 0.6038 0.5945 0.6059 0.5948 0.6011 0.5945 0.5978 0.6053 0.6053 0. der to more comprehensively evaluate the quality of generated images, we introduced three image quality assessment (IQA) metrics, including MUSIQ [22], CLIPIQA [59], and NIQE [42]. MUSIQ, CLIPIQA, and NIQE are three distinct Figure 8. Up: images generated by the original VAR-d16 models. Down: images generated by the perturbation fine-tuned VAR-d16. Table 5. No reference metrics for additional image quality assessments. Method VAR-d30 VAR-CoDe N=9 VAR-CoDe N=8 VAR-CoDe N=7 VAR-CoDe N= Inference Efficiency Image Quality Assessment #Steps Speedup Latency Throughput #Param Memory MUSIQ CLIPIQA NIQE 10 9+1 8+2 7+3 6+4 1.0x 1.2x 1.7x 2.3x 2.9x 3.62s 2.97s 2.11s 1.60s 1.27s 17.71it/s 21.54it/s 30.33it/s 40.00it/s 50.39it/s 2.0B 40414MB 2.0+0.3B 2.0+0.3B 2.0+0.3B 2.0+0.3B 28803MB 21019MB 19943MB 19943MB 60.72 60.78 60.79 60.82 60.76 0.6813 0.6818 0.6812 0.6800 0.6808 6.1739 6.1024 6.0849 6.1247 6. Table 6. The training-free performance of CoDe Configuration FID IS Precision Recall CoDe N=9 CoDe N=8 CoDe N=7 CoDe N=6 CoDe N=5 CoDe N=4 CoDe N=3 CoDe N=2 CoDe N= 1.99 2.10 2.25 2.42 2.56 2.75 2.99 3.19 3.39 306 308 309 306 303 295 288 283 268 0.8120 0.8155 0.8204 0.8283 0.8313 0.8342 0.8410 0.8433 0.8132 0.5978 0.5915 0.5781 0.5721 0.5660 0.5427 0.5327 0.5179 0.5382 IQA metrics, each with unique approaches and strengths. MUSIQ (Multi-Scale Image Quality) leverages vision transformer (ViT) [17] and multi-scale representation to evaluate global aesthetics and local distortions, making it effective for diverse image types, including high-resolution and non-standard aspect ratios. CLIPIQA utilizes the pretrained CLIP [46] model, which combines semantic understanding from large-scale image-text training to assess image quality in context-aware manner, excelling in tasks aligned with human perception. In contrast, NIQE (Natural Image Quality Evaluator) is no-reference metric that models natural scene statistics (NSS) using multivariate Gaussian distribution to measure deviations from high-quality natural image properties. While MUSIQ and CLIPIQA excel in leveraging learned features for state-of-the-art performance, NIQE stands out for its simplicity, computational efficiency, and independence from reference images, though it may struggle with unnatural or heavily edited content. Together, these metrics cater to diverse IQA needs, from deeplearning-based evaluations to lightweight statistical assessments. As shown in Table 5, our CoDe method achieves comparable or even superior generation quality compared to the original VAR-d30. This result further demonstrates the effectiveness of our approach. B. More Qualitative Results. Additional Qualitative Comparisons. We provide additional qualitative comparisons between the original VARd30 model and our proposed CoDe framework, evaluated with varying drafting steps = {6, 7, 8, 9}. As shown in Figures 9 and 10, CoDe achieves significant speedup and substantial memory optimization, with only minimal quality degradation that is nearly imperceptible to the human eye. Even at speedup rate of 2.9, the generated images maintain exceptionally high quality and preserve accurate semantic information. It is important to emphasize that the primary goal of CoDe is to enhance the efficiency of the VAR inference process while maintaining high generation quality, rather than reproducing the exact outputs of the original model. Through specialized fine-tuning, CoDes drafter model demonstrates superior predictive accuracy compared to the original model, sometimes resulting in different global structure. Nevertheless, the image quality remains consistently high and, in some cases, even improves over the original outputs. Qualitative Results of Perturbation Fine-Tuning. In our study, we conducted perturbation fine-tuning experiment to examine the distinct generative roles of small and large scales. Using pre-trained VAR-d16 model, we applied the CSE loss exclusively to tokens in the largest three scales and fine-tuned the model for just 1% of the original training epochs. This minimal fine-tuning at large scales caused complete collapse of the models global modeling capacity at small scales, with the FID increasing from 3.30 to 21.93 and the IS score dropping from 277 to 88. Figure 8 illustrates the qualitative results of perturbation fine-tuning. After slight fine-tuning, the VAR-d16 model nearly loses its ability to model global structures. These findings underscore that VAR models undertake entirely distinct generative tasks at small and large scales, with minimal overlap in functionality. C. Limitations and Future Work Limitations. The core concept of CoDe involves decomposing the next-scale prediction process into collaboration between large model and small model. This approach necessitates the availability of two models with different sizes. If only single large VAR model is available and faster inference is desired, it becomes necessary to retrain smaller refiner model. However, since the refiner model can be extremely compact, techniques such as model pruning and knowledge distillation can be applied to limit the additional training cost. Future Work. This study demonstrates that CoDe significantly reduces inference latency and memory consumption for VAR models. Furthermore, the efficiency gains from CoDe become even more pronounced in computationally intensive scenarios. As result, CoDe is particularly wellsuited for high-resolution image generation tasks based on next-scale prediction. In future work, we aim to explore the application of CoDe in building an efficient VAR model specifically optimized for high-resolution image generation. Figure 9. Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps. Figure 10. Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}