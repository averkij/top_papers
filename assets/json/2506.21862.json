{
    "paper_title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs",
    "authors": [
        "Boyuan Sun",
        "Jiaxing Zhao",
        "Xihan Wei",
        "Qibin Hou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 6 8 1 2 . 6 0 5 2 : r LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs Boyuan Sun1,2 Jiaxing Zhao2 Xihan Wei2 Qibin Hou1 1VCIP, School of Computer Science, Nankai University 2Tongyi Lab, Alibaba Group boyuansun@mail.nankai.edu.cn, houqb@nankai.edu.cn {zjx244036, xihan.wxh}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present LLaVA-Scissor, training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor."
        },
        {
            "title": "Introduction",
            "content": "Recently, Video Large Language Models (VLLMs) [33, 48, 38, 75, 44, 108, 1, 111] have achieved remarkable progress due to the rapid advancement of Multimodal Large Language Models (MLLMs) [70, 32, 12, 53, 114, 13, 89, 6, 17]. Unlike processing single image, VLLMs often require independent encoding of each video frame in serialized sequence. As result, even when only small number of frames are sampled from video, large number of visual tokens are generated. Although some methods attempt to reduce the number of visual tokens from the perspective of network architecture [36, 83, 93] or trainable modules [60, 101, 2], these approaches not only involve additional training costs but also face limitations in portability due to their specialized structures. Therefore, training-free token reduction strategies during the inference phase are essential for more efficient and scalable video processing. Selecting the most representative tokens [18, 62, 7] from all tokens is one of the most common paradigms for token reduction. Previous methods designed for image token compression [86, 61] often focus on leveraging attention scores to identify the most important tokens. However, as shown in Fig. 1(a), attention-based approaches [11, 104, 81, 45] tend to prioritize only key objects, which Equal contribution. Corresponding author. Preprint. Figure 1: Illustration of different token compression paradigms. denotes video tokens, with color representing different semantics. (a) Attention-based methods fail to cover all semantic regions. (b) Segment-based methods introduce temporal redundancy by stacking tokens from each segment. (c) Our two-step spatio-temporal compression strategy is able to identify unique semantic information within each frame and eliminate temporal redundancy, resulting in non-overlapping video tokens. can result in an incomplete representation of all semantics and the repeated selection of key semantics at the same time. Therefore, to identify all distinct semantic regions that can effectively represent the entire video with less redundancy, we propose the Semantic Connected Components (SCC) strategy. By measuring pairwise similarity between tokens, SCC can partition the tokens into non-overlapping regions throughout identifying connected components, and use tokens to represent distinct semantic regions. Notably, SCC does not require the tokens to be positional adjacent, allowing it to capture global semantic relations throughout the entire token sequence, regardless of spatial positioning. This enables more comprehensive and efficient representation of video content. Considering that image-based methods fail to account for the temporal redundancy across video frames, some approaches [68, 27, 47] begin to design compression strategies specifically for videos. As shown in Fig. 1(b), these methods often focus on segmenting the video and applying inter-segment token compression [63, 24, 76], or compressing tokens based on fixed pixel positions across time [64]. However, they overlook the fact that semantically similar regions may not be temporally connected or maintain spatial consistency over time, which introduces potential redundancy. To tackle this, we introduce LLaVA-Scissor, two-step spatio-temporal token compression strategy as shown in Fig. 1(c) with the help of SCC. Specifically, we first identify all unique semantic regions within the spatial domain of each video frame. Then, we assess the SCC again to remove temporal redundancy over semantic regions across frames and perform further fusion. The result is set of tokens that can effectively represent the entire video, without redundancy, and with each token encapsulating distinct semantic information from both spatial and temporal perspectives. We evaluate our LLaVA-Scissor on three video question answering benchmarks, four long video understanding benchmarks, and the multi-choice benchmark MVBench leveraging an enhanced LLaVA-OneVision model as the base model. The results demonstrate that our method consistently outperforms other token compression approaches, particularly at lower token retention ratios. Finally, we analyze the reducing law in video token compression in Sec. 5 by evaluating performance changes across benchmarks as the token retention ratio decreases. The results demonstrate that the redundancy of video tokens indeed exists, and our method, compared to others, is more effective in preserving key semantics at lower retention ratios, leading to superior performance. Our contributions can be summarized as follows: We point out that existing attention score-based methods fail to fully represent the entire token set and propose Semantic Connected Components (SCC), token compression strategy that captures all distinct semantic regions within the token set. We propose LLaVA-Scissor, two-step spatio-temporal token compression designed for video MLLMs, which can generate more comprehensive and efficient representation of video content. Experiments show that LLaVA-Scissor outperforms other token compression methods on various video understanding benchmarks."
        },
        {
            "title": "2.1 Video Large Language Models",
            "content": "Benefiting from the rapid advancement of Large Language Models (LLMs) [8, 56, 52, 15, 72, 28, 23, 40, 71], wide range of powerful proprietary MLLMs [53, 54, 5, 69, 55] and open-source community MLLMs [43, 42, 41, 105, 30, 90, 91, 22] have emerged. Among them, Video Large Language Models (VLLMs) [109, 110, 106] tailored for video understanding have gained increasing attention. Typical VLLMs [38, 99, 49] encode video frame sequences into raw video tokens with visual encoder and projector, and then feed them into LLMs along with user instructions to generate responses. Methods such as MovieChat [66], TimeChat [59] and TimeSuite [95] develop memory modules and timestamp-aware encoders for capturing better context. LLaVA-OneVision [31] proposes unified model capable of handling images, videos, audios, and other modalities simultaneously. LLaVANext-Video [107] significantly improves model performance by leveraging large-scale synthetic data. However, since the sequential encoding of frames leads to an increase in token numbers, existing VLLMs struggle in processing long video and computational efficiency, emphasizing the necessity of token reduction."
        },
        {
            "title": "2.2 Token Reduction in MLLMs",
            "content": "Token reduction [7, 58, 29, 37] is essential for efficient inference and longer visual sequences, and has been extended to the MLLMs domain [88, 65, 25, 10, 35]. Approaches such as LLaMA-VID [36] and LongVA [100] design token-efficient architectures, while other methods like LongVU [64], VideoLLaMA [14, 97], and VideoLLaMB [76] reduce token numbers through projector-level modifications. Pooling-based approaches [82, 87, 79] are also widely used. Additionally, some methods [77, 98, 74] explore agent-based techniques to convert videos into textual descriptions. As for training-free strategies, the most common approach is to select the important tokens from the token set [61, 85, 78, 92]. Some methods leverage the attention score of [CLS] token [86, 102, 103, 73] to measure the importance of each token. FastV [11] proposes strategy to select key tokens during the prefilling stage based on attention maps, while VTW [39] introduces an aggressive approach that directly removes visual tokens after certain decoder layer. However, these image token compression methods overlook the temporal redundancy across video frames. DyCoke [68] uniformly divides videos into segments and performs intra-segment compression by fixed even-odd token grouping. PruneVID [27] and FastVID [63] first segment the video based on scene boundaries and cluster video tokens within each segment. However, these segment-based methods overlook the fact that semantically similar information may not be temporally adjacent or spatially consistent, leading to redundancy when stacking tokens from each segment, as shown in Fig. 1(b). Different from them, LLaVA-Scissor performs token compression across both spatial and temporal dimensions, representing the entire video with set of semantically non-overlapping video tokens."
        },
        {
            "title": "3 LLaVA-Scissor",
            "content": "In this section, we first propose the Semantic Connected Components (SCC) approach to identify distinct semantic regions within given token set by utilizing connected components. Then, we propose two-step scheme that applies the SCC strategy in spatial and temporal domains to achieve effective token compression for video understanding."
        },
        {
            "title": "3.1 Token Compression via Semantic Connected Components (SCC)",
            "content": "As shown in Fig. 1(a), prior attention-based token selection approaches tend to select redundant semantic regions while overlooking others, making it challenging to obtain representative and comprehensive token set. Different from them, we tend to identify all unique semantic regions and retain one single token for each distinct region. Given set of tokens = {k1, ..., kN} RNd, we first compute the pairwise similarity between each token and then convert it into binary map based on threshold τ , as shown in Eqn. (1): = ( KT Kdim=1 Kdim=1 > τ ) RNN. (1) Figure 2: Pipeline of LLaVA-Scissor. (a) The Semantic Connected Components (SCC) compress tokens by extracting connected components from the token set. (b) The two-step spatio-temporal compression strategy that extracts unique semantics by leveraging SCC both spatially and temporally. In the binary similarity map A, A(i, j) indicates the similarity between any two tokens ki and kj. Therefore, if we consider each token as vertex and treat as an adjacency matrix representing the connectivity between vertices, we can construct graph based on the similarity relationships inherent in A. In this case, the problem of finding all unique semantic regions can be transformed into the task of identifying all connected components in the graph using the adjacency matrix A. To tackle this, we present an approximate method for computing the connected components. Specifically, given the adjacency matrix A, we first sample subset of vertices from the vertices based on an error tolerance ϵ. The number of vertices sampled is determined as follows: = min(N, log(N) ϵ2 ). (2) For each sampled vertex, we identify all its neighbors based on the adjacency matrix and employ the union-find data structure with path compression and union-by-rank to identify all connected components in the graph. Note that when the connected components extracted from sampled vertex do not cover all vertices, each uncovered vertex would be viewed as separate connected component. Additionally, to preserve the relative positional relationships among tokens, we sort the connected components based on the vertex ID with the highest degree within each cluster. Detailed data structure and approximate connected components algorithm are detailed in Sec. of appendix. The sorted connected components set is denoted as = {C1, ..., CM }, where indicates there are totally connected components in the graph, and each Ci in consists of Ci unique vertices. As expressed in Eqn. (3), since connected components can partition graph with vertices into disjoint subgraphs, no two connected components intersect, and the total number of vertices across all connected components equals the number of vertices in the graph. i, [1, ], Ci Cj = ; (cid:88) i= Ci = N. (3) This property suggests that the token set can be partitioned into distinct semantic regions throughout connected components C, which can encompass all non-overlapping semantics indicated by similarity map A. Therefore, we can compress the token set into by aggregating the tokens within each Ci in to avoid redundancy, detailed in Eqn. (4): = 1 Ci Ci (cid:88) j=1 kij , = 1, ..., ; = concat[k 1, ..., ]. (4) This process constructs representative token for each unique semantic region, thus transforming the original token set RNc into RM c. Leveraging this characteristic, we are able to get representative tokens from token set, regardless of their positional adjacency."
        },
        {
            "title": "3.2 Two-step Spatio-Temporal Token Compression",
            "content": "Equipped with the SCC strategy, we further propose two-step token compression strategy across both spatial and temporal dimensions. Given video = {v1, ..., vn} consisting of frames, the visual video tokens = {t1, ..., tn} Rnmd can be derived by processing it through the visual encoder and the visual projector, where denotes the number of tokens per frame and 4 Method LLaVA-OneVision [31] FastV [11] Dycoke [68] PLLaVA [82] VisionZip [86] LLaVA-Scissor FastV [11] VisionZip [86] Dycoke [68] PLLaVA [82] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor FastV [11] PLLaVA [82] VisionZip [86] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor LMM Retention ActivityNet Video-ChatGPT NextAvg. Size Ratio Acc. Score CI DO CU TU CO Avg. QA (%) 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 100% 48.09 3.47 3.37 3.78 3.52 3.02 2.63 3.26 81.33 100% 50% 47.95 3.47 3.36 3.77 3.50 2.99 2.57 3.24 81.11 99.4% 50% 47.88 3.47 3.33 3.76 3.51 3.01 2.58 3.24 81.06 99.3% 3.36 3.73 3.52 3.00 2.66 3.25 81.04 99.7% 50% 47.59 3.45 50% 45.42 3.47 3.16 3.63 3.34 2.75 2.61 3.10 78.46 95.7% 50% 47.89 3.47 3.37 3.76 3.47 3.00 2.65 3.25 81.12 99.7% 35% 47.83 3.46 3.32 3.74 3.47 2.97 2.61 3.22 80.49 99.0% 35% 44.69 3.46 3.13 3.61 3.31 2.71 2.57 3.07 77.72 94.8% 35% 47.81 3.45 3.31 3.74 3.46 2.98 2.54 3.21 80.86 98.6% 35% 47.23 3.42 3.26 3.70 3.39 2.92 2.59 3.17 79.66 97.6% 35% 47.88 3.47 3.32 3.75 3.46 2.99 2.63 3.23 80.83 99.2% 25% 47.79 3.47 3.33 3.76 3.47 2.98 2.62 3.23 80.66 99.2% 20% 47.85 3.46 3.31 3.72 3.43 2.96 2.57 3.20 80.57 98.5% 10% 44.95 3.38 3.04 3.60 3.28 2.80 2.49 3.04 78.76 94.2% 10% 45.28 3.37 3.11 3.56 3.25 2.78 2.55 3.05 77.87 94.4% 10% 38.58 3.30 2.65 3.09 2.73 2.31 2.42 2.64 65.09 82.7% 10% 47.75 3.46 3.26 3.68 3.41 2.90 2.52 3.15 80.03 97.5% 46.25 3.42 3.21 3.63 3.32 2.82 2.51 3.10 79.11 95.8% 7% 46.09 3.41 3.16 3.60 3.30 2.83 2.53 3.08 78.82 95.5% 5% Table 1: Comparison of state-of-the-art token compression strategies under different token retention ratios on video question-answering benchmarks. is the hidden state size. We first determine the representative tokens for each frame in the spatial dimension. For each frame ti containing to denote the representative tokens obtained through the SCC strategy. independent semantic regions, we use = SCC(ti) Rm id, = 1, ..., n. (5) After spatial compression, we obtain the representative tokens for every frame in the video. Subsequently, we concatenate all together to further consider the redundancy temporally. = concat[t 1, ..., n] RM d; = (cid:88) i=1 i. (6) In T, all representative tokens from each frame of the video are included. However, distinct semantic regions within the spatial dimension of each frame may still share overlaps across frames. That is semantic regions appearing in one frame may also appear in other frames. Therefore, to avoid redundancy as much as possible, similar to spatial fusion, we apply the SCC strategy again temporally to further compress similar representative tokens across the sequence: Tr = SCC(T) RM d. The representative tokens Tr comprehensively cover all spatio-temporally distinct semantic regions without redundancy or overlap. Treating them as retained tokens, we select the most suitable semantic regions for all tokens inspired by ToMe [7]. Specifically, we consider all tokens Ta = concat[t1, ..., tn] R(nm)d as the source tokens, and the retained tokens Tr as the target tokens. As shown in Eqn. (7), we first compute the similarity in Tr for each token ta map and find the most similar token tr in Ta, storing the indices in I. = Ta (Tr) Ta Tr R(nm)M ; = argmaxiM S(i) R(nm). (7) is vector that stores the index of the most similar target token in Tr for each token in Ta. Specifically, Ii = indicates that ta Ta, we j. Finally, for each source token ta assign it to the most similar target token tr Tr according to and perform an average merge to obtain the final compressed token tfin is closest to tr . tfin = (cid:80)nm 1(Ii = j) + tr i=1 ta (cid:80)nm 1(Ii = j) + 1 i=1 , = 1, ..., M. (8) We obtain the final merged tokens Tfin = concat[tfin 1 , ..., tfin ] RM to represent the entire video. 5 Method Retention ratio EgoSchema MLVU VideoMME VideoMMMU Avg.(%) LLaVA-OV-7B [31] 100% DyCoke [68] PLLaVA [82] FastV [11] VisionZip [86] LLaVA-Scissor DyCoke [68] FastV [11] PLLaVA [82] VisionZip [86] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor FastV [11] PLLaVA [82] VisionZip [86] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor 50% 50% 50% 50% 50% 35% 35% 35% 35% 35% 25% 20% 10% 10% 10% 10% 7% 5% 58.08 57.74 57.72 58.00 53.57 57.58 57.74 57.75 56.07 52.00 57.94 57.64 57.70 55.87 53.89 40.78 57.52 56.95 56.61 62.48 61.09 61.15 61.27 57.03 61. 59.95 59.54 59.42 56.29 60.95 59.81 59.39 55.81 54.17 48.42 58.14 57.46 56.43 57.96 57.35 56.93 57.47 54.19 57.37 56.22 56.00 54.26 53.70 57.52 56.44 55.74 51.63 50.89 42.56 55.18 53.37 53. 40.55 40.22 40.00 40.44 36.89 41.00 40.33 39.22 37.44 32.78 41.33 40.33 39.44 37.11 38.22 27.56 39.11 35.56 36.89 100% 98.8% 98.5% 99.2% 92.0% 99.3% 98.0% 97.0% 94.4% 88.3% 99.6% 97.9% 97.0% 91.5% 90.4% 72.3% 95.9% 92.4% 92.6% Table 2: Comparison of state-of-the-art token compression strategies under different token retention ratios on long video understanding benchmarks."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Implement details. Similar to previous approaches, we choose LLaVA-OneVision [31] as the base model architecture, which originally utilizes the CLIP [57] as the visual encoder and Qwen 2 [84] as the LLM. However, considering the rapid advancements in MLLM, its architecture and performance are no longer the most cutting-edge. For instance, SIGLIP (so400m-patch14-384) [96] is regarded as superior vision encoder in many methods [115]. Therefore, we replace the vision encoder in the original model with SIGLIP, employ the more advanced Qwen 2.5 [71] as the LLM, and retrain an enhanced version of the LLaVA-OneVision model using open-sourced Oryx [46] data. Equipped with the enhanced LLaVA-OneVision model, we implement the proposed method and compare it with other approaches based on this model for fair comparison. We primarily control the retention ratio through the similarity threshold τ . Unless otherwise specified, we set ϵ to 0.05. All evaluation experiments in this paper are conducted on single NVIDIA A100 GPU. Benchmarks and compared approaches. We validate the effectiveness of LLaVA-Scissor on variety of benchmarks, including Video Question-Answering Benchmarks (ActivityNet-QA [94], VideoChatGPT [49], Next-QA [80]), Long Video Benchmarks (Egoschema [50], MLVU [112], VideoMME [19], VideoMMMU [26]), and the comprehensive multi-choice benchmark MVBench [34]. detailed introduction to each dataset is provided in Sec. of appendix. We compare against several open-source representative methods: the image-based compression strategy VisionZip [86], the pooling-based PLLaVA [82], the pre-filling compression method FastV [11], and DyCoke [68], which applies compression in both the temporal and decoding stages. Notably, following the original paper settings, the ratio of dominant to contextual tokens in VisionZip is set to 5.4; the attention computation layer for both FastV and DyCoke is set to layer 3."
        },
        {
            "title": "4.2 Main Results",
            "content": "Results on video question-answering benchmarks. Tab. 1 presents comparison of our method with other approaches on video question-answering benchmarks under varying token retention ratios. While all methods achieve performance close to the original model at 50% retention ratio, LLaVA-Scissor shows increasingly advantages as the retention ratio decreases. This highlights the effectiveness of our LLaVA-Scissor in preserving essential semantic information. Results on long video understanding benchmarks. Performance on long-video benchmarks are essential for token compression methods. As shown in Tab. 2, LLaVA-Scissor consistently 6 Method RR AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg. LLaVA-OV-7B [31] 100% 84.0 52.0 49.0 62.0 71.0 65.0 46.5 27.0 58.5 47.5 49.0 81.0 67.5 53.0 81.0 77.5 41.5 94.5 61.5 79.5 62. DyCoke [68] FastV [11] PLLaVA [82] VisionZip [86] LLaVA-Scissor FastV [11] PLLaVA [82] VisionZip [86] LLaVA-Scissor 35% 81.5 52.5 51.0 61.0 68.5 68.0 45.0 27.5 56.5 47.5 48.5 80.5 67.0 52.5 77.5 75.5 42.5 94.0 60.0 78.5 61.78 35% 82.0 51.0 47.5 66.0 68.0 67.5 45.0 27.0 59.0 49.0 50.0 79.0 59.0 51.5 75.5 74.5 40.5 95.0 59.5 79.0 61.28 35% 81.0 45.5 50.0 59.0 64.5 63.0 43.5 27.5 57.5 45.5 40.0 77.0 64.0 53.0 77.0 69.5 39.5 94.5 60.5 77.5 59.48 35% 84.0 35.5 35.5 58.0 69.5 63.5 33.5 28.5 53.0 49.0 51.0 66.5 49.0 46.5 59.0 72.5 47.0 93.5 66.0 78.5 56.98 35% 83.0 54.0 50.0 63.0 69.5 67.0 49.0 28.5 58.0 49.0 49.0 77.0 60.5 52.5 75.5 77.0 42.5 95.0 60.0 79.5 61.98 10% 78.5 42.5 37.0 67.0 63.0 67.0 36.5 27.0 55.5 46.0 47.5 65.5 50.0 48.5 65.0 66.0 40.0 93.5 61.0 78.5 56.78 10% 77.5 37.0 42.5 54.0 63.0 60.0 37.5 28.0 51.5 46.0 35.0 70.5 56.5 48.0 73.5 64.5 39.5 93.5 60.5 78.5 55.85 10% 75.5 35.5 27.5 42.0 47.0 48.0 33.5 29.0 49.5 39.0 33.0 46.5 43.5 36.5 46.0 45.5 37.5 81.5 54.5 64.0 45.75 10% 80.5 48.0 47.5 59.0 66.5 64.5 41.0 28.0 56.5 45.0 43.0 63.0 51.0 48.0 64.5 76.0 39.5 95.5 60.5 80.0 57.88 Table 3: Comparison of state-of-the-art token compression strategies under different token retention ratios on MVBench. RR denotes the token retention ratio. Spatial Temporal Merge MVBench VideoMME Method MVBench VideoMME ActivityNet 59.85 60.55 60.80 61. 55.89 55.97 56.40 57.52 Random Uniform L2Norm SCC 58.68 61.25 60.67 61.98 55.55 56.86 57.07 57.52 47.11 47.25 47.16 47.88 (a) Ablation study on different components. (b) Ablation study on token selection methods. Table 4: Ablation Studies. Spatial and Temporal refer to spatial and temporal compression, respectively. Merge indicates whether the compressed tokens are merged with all tokens. outperforms other methods under the same compression ratios and the advantage of LLaVA-Scissor becomes increasingly evident under low token budgets. Notably, LLaVA-Scissor achieves better performance at 5% retention ratio than other methods do at 10%, highlighting its superior efficiency in extreme compression settings. Results on MVBench. We also conduct experiments on MVBench [34], comprehensive video understanding benchmark covering 20 tasks organized in the form of multiple-choice questions in Tab. 3. LLaVA-Scissor outperforms all other methods at both 35% and 10% retention ratios, demonstrating its consistent superiority in comprehensive scenarios."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Effectiveness of components. In Tab. 4a, we perform ablation experiments on various components of the proposed method. To ensure fairness, we set the retention ratio to 35% in each experiment. As we can see, when retaining the same number of tokens, the approach using only spatial compression is less effective because redundancy is still present in the tokens, compared to the two-step compression method that eliminates temporal repetition. Additionally, merging all tokens with the selected representative tokens can bring further improvements. Ablation of token selection methods. To demonstrate the effectiveness of our proposed SCC strategy, we compare it with other methods of selecting representative tokens in Tab. 4b. In addition to random sampling and uniform sampling, we also used L2Norm to select tokens with higher information density for comparison. Note that in our experiments, we only replace the method of obtaining representative tokens, while the Merge operation in Tab. 4a is retained for fair comparison. It is evident that our method based on SCC performs better than other sampling methods. Impact of similarity threshold τ and error tolerance ϵ. The similarity threshold τ and error tolerance ϵ are the key parameters in controlling the accuracy of the connectivity graph, as well as the core variables for managing the token number after compression. In Fig. 3a and Fig. 3b, we discuss how the similarity threshold τ affects the number of tokens after compression across different benchmarks. Fig. 3a corresponds to τ values ranging from 0.99 to 0.9, while Figure Fig. 3b covers τ values from 0.89 to 0.8. It is evident that as the threshold for inclusion in connected components becomes more lenient, more tokens are compressed. Additionally, due to the varying distributions of videos across different benchmarks, the number of tokens under the same threshold τ varies. The error tolerance ϵ in Eqn. (2) serves as trade-off between the accuracy of solving connected components and the computational efficiency. Theoretically, the lower the ϵ, the more precise the 7 (a) Impact of τ [0.9, 0.99] (c) Impact of ϵ (b) Impact of τ [0.8, 0.89] Figure 3: Token number statistics of similarity threshold τ and error tolerance ϵ. Method RR FLOPs FR Avg. Method RR FLOPs FR Avg. LLaVA-OV-7B [31] 100% 41.4 50% 18.7 Dycoke [68] 50% 21.4 FastV [11] 50% 18.6 LLaVA-Scissor 35% 13.1 Dycoke [68] 35% 16.1 FastV [11] 35% 13.4 LLaVA-Scissor Table 5: Comparison of FLOPs and Performance across Methods. RR refers to the Retention Ratio, FR refers to the FLOPs Ratio, and Avg. denotes the average performance ratio. 18.3% 98.3% 9.42% 78.8% 9.42% 92.7% 19.6% 93.1% 9.66% 96.6% 6.28% 94.7% 5.56% 94.5% LLaVA-Scissor VisionZip [86] PLLaVA [82] FastV [11] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor 100% 100% 45.2% 99.2% 51.7% 99.2% 44.9% 99.5% 31.6% 98.4% 38.9% 98.3% 32.4% 99.3% 20% 10% 10% 10% 10% 7% 5% 7.6 3.9 3.9 8.1 4.0 2.9 2.3 connected components obtained. In Figure 3c, we illustrate how token numbers vary with changes in ϵ under different τ values on MVBench [34]. Since we treat uncovered tokens as separate connected components, as ϵ increases, the token number also tends to increase. It can be observed that when ϵ is smaller than 0.05, the connected components almost no longer change. This indicates that nearly all distinct semantic regions have been identified and merged. Therefore, we set ϵ = 0.05."
        },
        {
            "title": "4.4 Efficiency Analysis",
            "content": "Following Dycoke [68], we divide the FLOPs generated during the LLM stage into two parts: the In the prefilling stage, for each transformer layer, the prefilling stage and the decoding stage. computational cost of the multi-head attention and the FFN can be expressed as 4kd2 + 2k2d + 2kdc, where k, d, and denote the number of tokens, the hidden state size, and the intermediate size of the FFN, respectively. In the decoding stage, thanks to the KV cache, the computational cost for decoding each token is significantly reduced and can be represented as (4d2 + 2dc) + 2(dk + d(R+1) ). Therefore, the total FLOPs during the LLM stage can be expressed as: 2 FLOPs = (4kd2 + 2k2d + 2kdc) + R((4d2 + 2dc) + 2(dk + d(R + 1) 2 )), (9) where , represents the number of transformer layers and predicted token length. We set = 100 in all calculations. Additionally, we further calculate the FLOPs introduced by our LLaVA-Scissor during token compression. Since the main computational cost stems from similarity computation [67, 85, 64] between tokens, the FLOPs generated by LLaVA-Scissor can be expressed as: FLOPs = 2m2d + 2k2 (10) 1d + 2nmk2d, where n, m, and represent the number of frames, the number of tokens per frame, and the hidden state size, respectively. k1 and k2 represent the number of tokens after the first step of spatial compression and the second step of temporal compression, respectively. In Tab. 5, we compare the FLOPs and average performance across all benchmarks of our method with other methods under different retention ratios. Unlike FastV [11] that performs compression at the LLM stage, our LLaVA-Scissor compresses tokens before LLMs, significantly reducing (a) High RR on MVBench (b) High RR on VideoMME (c) Low RR on MVBench (d) Low RR on VideoMME Figure 4: Performance degradation of methods on different benchmarks as the retained token number decreases. RR denotes the token retention ratio. FLOPs. Furthermore, it can be observed that our method consistently achieves the highest average performance at the same retention ratio, especially when the retention ratio is relatively low. Our method demonstrates an advantage over methods like PLLaVA [82] and VisionZip [86]."
        },
        {
            "title": "5 Reducing Law in Token Compression",
            "content": "Observing Tab. 1, Tab. 2, and Tab. 3, there is common phenomenon: When the retention ratio is relatively high, existing methods, including LLaVA-Scissor, can maintain model performance quite well. However, as the retention ratio drops, the model performance begins to decline rapidly. To further analyze the impact of the retained token numbers on performance, here we attempt to analyze the reducing law in video token compression."
        },
        {
            "title": "5.1 Token Redundancy in Video MLLMs",
            "content": "In Fig. 4a and Fig. 4b, we present comparative analysis of different models (PLLaVA [82], FastV [11], VisionZip [86], and LLaVA-Scissor) performance on two benchmarks: MVBench [34] and VideoMME [19], in range of token retention ratios from 90% to 35% (with token number from 5644 to 2195). The results demonstrate that most token reduction methods achieve performance that remains comparable to the original model. Notably, even the naive uniform sampling strategy introduces only minimal degradation in performance, which suggests high degree of redundancy among visual tokens in VLLMs. This observation implies that many tokens contribute little to the final prediction and can be safely discarded or aggregated without significantly affecting model output. Redundancy is common in VLLMs, as similar content within and across frames often leads to duplicated visual tokens. These findings provide strong empirical support for developing more efficient token selection and compression strategies without sacrificing too much accuracy."
        },
        {
            "title": "5.2 Semantic Loss under Aggressive Token Reduction",
            "content": "As shown in Fig. 4c and Fig. 4d, we evaluate the performance degradation of different token selection methods on the MVBench [34] and VideoMME [19] benchmarks, under aggressive token retention ratios (35% to 3%, with token number range from 2195 to 181). In contrast to the results in Fig. 4a and Fig. 4b, where most methods maintain performance levels comparable to the original model, we observe sharp performance decline for all methods as the retention ratio drops below 35%. This indicates that the indiscriminate reduction or aggregation of tokens at very low budgets results in the loss of semantically critical visual information, which significantly hampers model understanding. It is evident that when retaining smaller number of tokens, our LLaVA-Scissor demonstrates consistent superiority over other methods. Specifically, at 3% retention ratio, the average scores across the two benchmarks of FastV [11] drop to 80.7% (81.4% on MVBench and 79.7% on VideoMME), whereas LLaVA-Scissor retains substantially higher score of 86.8% (+6.1%). The performance gaps highlight the advantage of LLaVA-Scissor, which prioritizes semantically representative and diverse tokens while minimizing redundancy. As result, our method is better suited to tight token budgets, preserving the essential visual semantics needed for accurate video-language understanding."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we propose LLaVA-Scissor, training-free token compression paradigm for Video Large Language Models. We first introduce the SCC strategy, which achieves token compression by identifying semantic connected components within the token set. By applying the SCC strategy in both spatial and temporal domains, LLaVA-Scissor uses set of non-overlapping tokens to represent the entire video, effectively eliminating spatio-temporal redundancy in video tokens. Experimental results and an analysis of the reducing law in video compression demonstrate that LLaVA-Scissor effectively preserves key semantic information during video token compression, especially when the number of retained tokens is low."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Eduard Allakhverdov, Elizaveta Goncharova, and Andrey Kuznetsov. When less is enough: Adaptive token reduction for efficient image representation. arXiv preprint arXiv:2503.16660, 2025. 1 [3] Stephen Alstrup, Inge Li Gørtz, Theis Rauhe, Mikkel Thorup, and Uri Zwick. Union-find with constant time deletions. In Automata, Languages and Programming, pages 7889, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. 21 [4] Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversity-based visual token pruning for large multimodal models, 2025. [5] Anthropic. Claude-3.5, 2024. 3 [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1 [7] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning Representations, 2023. 1, 3, 5 [8] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 3 [9] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 17 [10] Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Efficient large multi-modal models via visual context compression. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [11] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models, 2024. 1, 3, 5, 6, 7, 8, 9, 18, 19 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 1 [14] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 3 [15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 3 [16] Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, and Aymen Shabou. Pact: Pruning and clustering-based token reduction for faster visual language models, 2025. 18 [17] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 1 [18] Mohsen Fayyaz, Soroush Abbasi Kouhpayegani, Farnoush Rezaei Jafari, Eric Sommerlade, Hamid Reza Vaezi Joze, Hamed Pirsiavash, and Juergen Gall. Adaptive token sampling for efficient vision transformers. European Conference on Computer Vision (ECCV), 2022. 1 [19] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 6, 9, 17 [20] Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Framefusion: Combining similarity and importance for video token reduction on large visual language models, 2024. [21] Harold N. Gabow and Robert Endre Tarjan. linear-time algorithm for special case of disjoint set union. Journal of Computer and System Sciences, 30(2):209221, 1985. Funding Information: supported by the National Science Foundation, Grant MCS78-18909. 21 [22] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 3 [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [24] Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, and Bo Zheng. Fila-video: Spatio-temporal compression for fine-grained long video understanding. arXiv preprint arXiv:2504.20384, 2025. 2 [25] Yuhang Han, Xuyang Liu, Zihan Zhang, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. Filter, correlate, compress: Training-free token reduction for mllm acceleration, 2025. 3 [26] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. 2025. 6, [27] Xiaohu Huang, Hao Zhou, and Kai Han. Prunevid: Visual token pruning for efficient video large language models. arXiv preprint arXiv:2412.16117, 2024. 2, 3, 18 [28] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv:2401.04088, 2024. 3 [29] Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, and Sujoy Paul. Lookupvit: Compressing visual information to limited number of tokens. In European Conference on Computer Vision, pages 322337. Springer, 2024. 3 [30] Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. Llava-next: What else influences visual instruction tuning beyond data?, 2024. [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3, 5, 6, 7, 8, 18, 19 [32] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1 [33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 1 11 [34] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 6, 7, 8, 9, [35] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm, 2024. 3 [36] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. European Conference on Computer Vision, 2024. 1, 3 [37] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. 3 [38] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 3, [39] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. arXiv preprint arXiv:2405.05803, 2024. 3 [40] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 3 [41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 3 [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023. [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3 [44] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 1 [45] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model. arXiv preprint arXiv:2411.10803, 2024. 1 [46] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [47] Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, and Hongtao Xie. Hybrid-level instruction injection for video token compression in multi-modal large language models. arXiv preprint arXiv:2503.16036, 2025. 2 [48] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1 [49] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 3, 6, 17 [50] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 6 [51] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024. 17 [52] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023. [53] OpenAI. Gpt-4v(ision) system card. 2023. 1, 3 [54] OpenAI. Gpt-4 technical report, 2023. 3 12 [55] OpenAI. Gpt-4o system card, 2024. 3 [56] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 3 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [58] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [59] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. ArXiv, abs/2312.02051, 2023. 3 [60] Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles. xgen-mm-vid (blip-3-video): You only need 32 tokens to represent video even in vlms, 2024. 1 [61] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 1, 3, 18 [62] Leqi Shen, Tianxiang Hao, Tao He, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, and Guiguang Ding. Tempme: Video temporal token merging for efficient text-video retrieval. arXiv preprint arXiv:2409.01156, 2024. [63] Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, and Guiguang Ding. Fastvid: Dynamic density pruning for fast video large language models. arXiv preprint arXiv:2503.11187, 2025. 2, 3 [64] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv:2410.17434, 2024. 2, 3, 8 [65] Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, and Jiaqi Wang. Crossget: arXiv preprint Cross-guided ensemble of tokens for accelerating vision-language transformers. arXiv:2305.17455, 2023. 3 [66] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 3 [67] Boyuan Sun, Yuqi Yang, Le Zhang, Ming-Ming Cheng, and Qibin Hou. Corrmatch: Label propagation via correlation matching for semi-supervised semantic segmentation. IEEE Computer Vision and Pattern Recognition (CVPR), 2024. [68] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. arXiv preprint arXiv:2411.15024, 2024. 2, 3, 5, 6, 7, 8, 18 [69] Gemini Team. Gemini: family of highly capable multimodal models, 2024. 3 [70] Qwen team. Qwen2-vl. 2024. 1 [71] Qwen Team. Qwen2.5: party of foundation models, 2024. 3, 6, 18 [72] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. 3 [73] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. [cls] token tells everything needed for training-free efficient mllms. arXiv preprint arXiv:2412.05819, 2024. 3 [74] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent, 2024. 3 13 [75] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. 1 [76] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long video understanding with recurrent memory bridges. arxiv, 2024. 2, 3 [77] Ying Wang, Yanlai Yang, and Mengye Ren. Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos, 2024. 3 [78] Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, and Ran Xu. Dymu: Dynamic merging and virtual unmerging for efficient vlms, 2025. 3 [79] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. arXiv preprint arXiv:2404.03384, 2024. 3 [80] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97779786, 2021. 6, 17 [81] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. 1 [82] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning, 2024. 3, 5, 6, 7, 8, 9, 18 [83] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, and Ran Xu. xgen-mm (blip-3): family of open large multimodal models, 2024. [84] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 6 [85] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, et al. Topv: Compatible token pruning with inference time optimization for fast and low-memory multimodal vision language model. arXiv preprint arXiv:2503.18278, 2025. 3, 8 [86] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024. 1, 3, 5, 6, 7, 8, 9, 18 [87] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024. 3 [88] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models, 2024. 3 [89] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [90] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 3 14 [91] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023. 3 [92] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. ArXiv, abs/2412.00447, 2024. 3 [93] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. VoCo-LLaMA: Towards Vision Compression with Large Language Models. arXiv preprint arXiv:2406.12275, 2024. [94] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134, 2019. 6 [95] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, and Limin Wang. Timesuite: Improving mllms for long video understanding via grounded tuning, 2024. 3 [96] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 6, 18 [97] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 3 [98] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering, 2023. [99] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3 [100] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision, 2024. 3 [101] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision transformer with quadrangle attention. arXiv preprint arXiv:2303.15105, 2023. 1 [102] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, MinQi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster. arXiv preprint arXiv:2412.01818, 2024. [103] Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan, and Liqiang Nie. Token-level correlation-guided compression for efficient multimodal document understanding, 2024. 3 [104] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 1 [105] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 3 [106] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 3 [107] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 3 [108] Jiaxing Zhao, Boyuan Sun, Xiang Chen, and Xihan Wei. Facial dynamics in video: Instruction tuning for improved facial expression perception and contextual awareness. arXiv preprint arXiv:2501.07978, 2025. 1 [109] Jiaxing Zhao, Boyuan Sun, Xiang Chen, Xihan Wei, and Qibin Hou. Llava-octopus: Unlocking instructiondriven adaptive projector fusion for video understanding. arXiv preprint arXiv:2501.05067, 2025. [110] Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv preprint arXiv:2503.05379, 2025. 3 15 [111] Jiaxing Zhao, Qize Yang, Yixing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Xihan Wei, Liefeng Bo, et al. Humanomni: large vision-speech language model for human-centric video understanding. arXiv preprint arXiv:2501.15111, 2025. 1 [112] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 6 [113] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [114] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [115] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, , and Xide Xia. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024."
        },
        {
            "title": "Appendix",
            "content": "In order to provide more complete illustration of the capabilities of LLaVA-Scissor, comprehensive appendix has been developed. This appendix includes detailed descriptions of the evaluation benchmarks, additional experiments results, algorithms analysis, and discussion among limitations and broader impacts."
        },
        {
            "title": "A Benchmarks",
            "content": "A.1 Video Question-Answering Benchmarks . . . . . . . . . . . . . . . . . . . . . . . A.2 Long Video Understanding Benchmarks . . . . . . . . . . . . . . . . . . . . . . . A.3 MVBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Supplementary Experimental Analysis",
            "content": "B.1 More comparison of existing methods . . . . . . . . . . . . . . . . . . . . . . . . B.2 Results with LLaVA-OneVision 0.5B model . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Algorithms Analysis",
            "content": "C.1 Union-Find Data Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Approximate Connected Components Algorithm . . . . . . . . . . . . . . . . . . C.3 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 18 18 18 19 19"
        },
        {
            "title": "A Benchmarks",
            "content": "A.1 Video Question-Answering Benchmarks Video question-answering is fundamental capability of Video Large Language Models (VLLMs). In this study, we evaluate LLaVA-Scissors performance on this task using widely adopted benchmarks, including ActivityNet-QA [9], VideoChatGPT [49], and Next-QA [80]. ActivityNet-QA [9] consists of human-annotated, action-related question-answer pairs derived from the ActivityNet dataset, with an average video duration of approximately 2 minutes. The VideoChatGPT [49] benchmark evaluates five critical aspects of video understanding: accuracy of information, detail orientation, contextual comprehension, temporal reasoning, and consistency. Next-QA [80] focuses on reasoning about causal and temporal actions, as well as understanding rich object interactions in daily activities. A.2 Long Video Understanding Benchmarks Handling long videos is crucial for evaluating token compression strategies because longer sequences pose greater challenges in balancing token efficiency and preserving essential information. To demonstrate that LLaVA-Scissor can effectively manage diverse video scenarios, we evaluate it on several long video understanding benchmarks, including EgoSchema [51], MLVU [113], VideoMME [19], and VideoMMMU [26]. Among these benchmarks, EgoSchema[51] comprises egocentric videos captured from first-person perspective, with an average duration of approximately 180 seconds. MLVU[113] emphasizes understanding extended videos with lengths varying widely from 3 minutes up to 2 hours, pushing models to maintain temporal coherence and effectively summarize long-term dependencies. VideoMME[19] includes videos from diverse domains with durations ranging from minutes to hours, making it one of the most comprehensive and challenging benchmarks for holistic video understanding. Finally, Video-MMMU[26] serves as multi-modal and multi-disciplinary benchmark that evaluates large multimodal models on their ability to not only comprehend but also integrate and apply knowledge 17 Method Retention ratio ActivityNet MVBench MLVU VideoMME NextQA Avg.(%) PruMerge [61] DyCoke [68] PLLaVA [82] FastV [11] VisionZip [86] PACT [16] FrameFusion [20] LLaVA-Scissor DiVPrune [4] VisionZip [86] PLLaVA [68] FastV [11] PruneVID [27] LLaVA-Scissor 55% 35% 35% 35% 35% 35% 30% 35% 15% 10% 10% 10% 10% 10% 97.2% 99.4% 98.2% 99.5% 94.4% 98.9% - 99.6% 95.4% 80.2% 94.1% 93.5% - 99.3% 90.7% 99.0% 95.3% 98.2% 91.3% - - - 96.0% 95.1% 95.3% 90.1% 99.2% - 99.3% 97.6% 90.3% 97.0% 93.6% 96.6% 92.7% 98.4% 96.7% 99.2% - - - 73.4% 77.5% 73.3% 87.8% 86.7% 89.5% 89.1% 89.3% 90.9% 89.9% 88.0% 87.9% 92.7% 93.1% 95.2% 93.5% 95.6% 98.2% 99.4% 96.0% 97.9% 97.7% 99.0% 92.8% 95.6% 98.3% - 98.3% 97.5% 99.4% 99.0% 95.1% 80.0% 95.7% 96.8% - 95.3% 76.9% 90.8% 91.9% 88.6% 98.4% 95.7% Table 6: Comparison of state-of-the-art token compression strategies under different token retention ratios on various video understanding benchmarks. from videos across various fields, thereby testing cross-domain reasoning and knowledge transfer capabilities. A.3 MVBench In addition to the previously discussed video question-answering and long video understanding benchmarks, we also evaluate our method on MVBench [34], comprehensive and challenging mlutichoice video understanding benchmark. MVBench is designed to assess the temporal comprehension capabilities by presenting 20 distinct tasks in the form of multiple-choice questions. These tasks cover diverse scenarios that require sophisticated temporal reasoning and understanding of dynamic content, which cannot be achieved through single-frame analysis alone."
        },
        {
            "title": "B Supplementary Experimental Analysis",
            "content": "B.1 More comparison of existing methods We further extend our evaluation by comparing LLaVA-Scissor with wider range of token compression strategies, as shown in Tab. 6. Notably, these approaches are implemented upon different baseline models, such as LLaVA-OneVision [31] and Video-LLaVA [38], which introduces inherent discrepancies in absolute performance. To ensure fair and consistent comparison of compression efficiency, we report the relative performance with respect to each methods own baseline across various token retention ratios. The experimental results demonstrate that LLaVA-Scissor consistently outperforms other approaches at both moderate (35%) and aggressive (10%) token retention ratios, underscoring its robustness and superior capability in preserving critical semantic information under constrained token budgets. B.2 Results with LLaVA-OneVision 0.5B model Given that token compression strategies are often used in resource-constrained scenarios, it is particularly valuable to investigate their effectiveness when applied to smaller base models. Unlike large-scale models with abundant parameters and computational capacity to compensate for potential information loss during compression, lightweight models are inherently limited in their ability to recover or reason over missing information. Therefore, preserving more essential semantic content becomes even more critical. To evaluate this, we deploy LLaVA-Scissor in LLaVA-OneVision-0.5B base model (enhanced with SIGLIP [96] as encoder and Qwen-2.5-0.5B [71] as LLM), which contains significantly fewer parameters. As shown in Tab. 7, LLaVA-Scissor demonstrates performance patterns consistent with those observed in the 7B model setting. Despite the significantly reduced model capacity, Method RR VideoChatGPT MVBench MLVU VideoMME Egoschema Avg.(%) LLaVA-OV-0.5B [31] 100% LLaVA-Scissor FastV [11] LLaVA-Scissor FastV [11] LLaVA-Scissor LLaVA-Scissor LLaVA-Scissor 50% 35% 35% 10% 10% 7% 5% 2.89 2.84 2.78 2. 2.70 2.72 2.69 2.67 51.23 51.28 50.80 51.03 50.10 50.38 49.95 49.60 46. 46.99 46.45 47.39 - 46.62 45.99 45.91 40.15 39.93 39.25 39. 36.14 38.48 38.22 37.89 40.71 41.11 40.74 41.13 38.38 40.79 40.17 39.57 100% 99.9% 98.5% 99.3% 93.9% 97.6% 96.6% 95.8% Table 7: Evaluation of LLaVA-Scissor and FastV [11] on various video understanding benchmarks with the LLaVA-OneVision-0.5B base model under varying token retention ratios. Algorithm 1 Union-Find Data Structure with Path Compression and Union by Rank Path compression 1: Initialize parent[i] = for all 2: Initialize rank[i] = 0 for all 3: function FIND(x) 4: 5: 6: 7: while parent[x] = do parent[x] parent[parent[x]] parent[x] xroot find(x) yroot find(y) if xroot = yroot then continue end while return 8: end function 9: function BATCH_UNION(x_arr, y_arr) for (x, y) zip(x_arr, y_arr) do 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: end function end if if rank[xroot] < rank[yroot] then parent[xroot] yroot parent[yroot] xroot if rank[xroot] = rank[yroot] then rank[xroot] rank[xroot] + 1 end for end if end if else our method effectively preserves key semantic representations and mitigates the performance drop commonly associated with token compression. In comparison with FastV, LLaVA-Scissor consistently outperforms it under both 35% and 10% token retention ratios. These findings underscore the robustness and practical utility of our method in real-world applications, particularly for deployment on edge devices or mobile platforms where both model size and computational budgets are limited."
        },
        {
            "title": "C Algorithms Analysis",
            "content": "C.1 Union-Find Data Structure The Union-Find is an efficient data structure designed to manage and merge disjoint sets. It supports two fundamental operations: the Find operation to determine the root of the set for particular element and the Union operation to merge two disjoint sets into one. In practice, the Union-Find data structure is typically optimized using path compression and unionby-rank. Specifically, path compression flattens the tree structure during the Find operation by 19 Algorithm 2 Approximate Connected Components via Union-Find Create an Union-Find set with nodes Initialize adjacency list for sampled_nodes neighbors {j adj_matrix[i][j] = 0} neighbor_dict[i] neighbors Set nodes_flag[j] = 0 for all neighbors Input: Adjacency matrix adj_matrix, error tolerance ϵ Output: List of connected components 1: adj_matrix.shape[0] 2: Initialize nodes_flag[i] = 1 for all 3: uf nionF ind(n) 4: sample_size min(n, log(n)/ϵ2) 5: sampled_nodes random.sample({1, 2, ..., n}, sample_size) 6: Set nodes_flag[i] = 0 for all sampled_nodes 7: neighbor_dict [] 8: for sampled_nodes do 9: 10: 11: 12: end for 13: all_x, all_y [], [] 14: for sampled_nodes do 15: 16: 17: 18: 19: end for 20: uf.batch_union(all_x, all_y) Merge subgraphs of all sampled_nodes Find root for sampled_nodes 21: sampled_roots [uf.find(i) for sampled_nodes] 22: unique_roots Unique(sampled_roots) Find every unique connected component root 23: components [] 24: for root unique_roots do 25: 26: 27: end for 28: remain_nodes {i nodes_flag[i] = 1} In case there are any unconsidered nodes. 29: components.extend(remain_nodes) Treat unconsidered nodes as independent components Sort by the node ID with the highest degree in each cluster. 30: components.sort() cluster np.where(uf.parent == root)[0].tolist() components.append(cluster) all_x.append(i) all_y.append(j) for neighbor_dict[i] do end for directly linking all nodes along the query path to the root, thereby reducing the time complexity of subsequent queries. Union-by-rank, on the other hand, attaches the smaller tree to the root of the larger tree during the Union operation, preventing excessive growth in tree height. We provide the detailed the Union-Find structure in Algorithm. (1). C.2 Approximate Connected Components Algorithm Equipped with the Union-Find data structure, we further leverage it to compute the connected components of graph, as detailed in Algorithm. (2). Specifically, given graph with nodes, our approximate connected components algorithm begins by initializing the Union-Find structure with each node as an individual set. We then sample nodes from the graph to construct an adjacency list and mark each corresponding edge. For each edge in this sampled graph, Union operation is performed to iteratively merge connected components. After all edges have been processed, we traverse all nodes and invoke the Find operation to determine the root set to which each node belongs, thereby identifying the final connected components. It is important to note that nodes not covered by the sampled connections are treated as single connected component. Finally, considering the relative positional relationships between tokens represented by the nodes, we sort the connected components based on the node with the highest degree within each cluster. C.3 Complexity Analysis We first analyze the computational complexity of finding connected components using the Union-Find algorithm without approximation, i.e., when all nodes are considered. For the Union-Find structure 20 optimized with path compression and union by rank, the Find and Union operations have an amortized time complexity of O(α(n)), where α() denotes the inverse Ackermann function, which is generally regarded as constant less than 5 in practice [21, 3]. The initialization of the Union-Find data structure takes O(n) time, and performing Union on each of the edges results in an overall complexity of O(mα(n)). Therefore, for graph with nodes and edges, the total complexity of identifying connected components using Union-Find is O(n + mα(n)). In our case, since the graph is represented using an adjacency matrix to encode pairwise similarity among tokens, the number of edges in the worst case is N2, where is the total number of tokens. Thus, without applying any approximation or sparsification, the overall worst-case complexity for computing connected components becomes O(N2α(N)). To further improve computational efficiency, we propose an approximation strategy, in which = min(N, log(N) ) nodes are sampled from the total nodes. For each sampled node, we compute its connectivity with all nodes to identify approximate connected components. As result, when > log(N) , the time complexity is: ϵ2 ϵ O(N Nα(N)) = O(min(N, log(N) ϵ2 ) Nα(N)) = O(log(N) Nα(N)). (11) Specifically, when the error tolerance ϵ is set to 0.05, the overall time complexity of the algorithm reduces to O(log(N) α(N)) once the number of tokens exceeds 1200."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group",
        "VCIP, School of Computer Science, Nankai University"
    ]
}