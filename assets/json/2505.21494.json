{
    "paper_title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
    "authors": [
        "Xiaojun Jia",
        "Sensen Gao",
        "Simeng Qin",
        "Tianyu Pang",
        "Chao Du",
        "Yihao Huang",
        "Xinfeng Li",
        "Yiming Li",
        "Bo Li",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 9 4 1 2 . 5 0 5 2 : r Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment Xiaojun Jia1, Sensen Gao2, Simeng Qin1, Tianyu Pang3, Chao Du3, Yihao Huang1, Xinfeng Li1, Yiming Li1, Bo Li4, Yang Liu1 1Nanyang Technological University, Singapore 2 MBZUAI, United Arab Emirates 3Sea AI Lab, Singapore 4 University of Illinois Urbana-Champaign, USA {jiaxiaojunqaq, sensen.gao2002, qinsimeng670}@gmail.com; {tianyupang3, duchao, lxfmakeit, liyiming.tech}@gmail.com; lbo@illinois.edu; yangliu@ntu.edu.sg;"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global featuressuch as CLIPs [CLS] tokenbetween adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) [47, 43, 3, 9, 1, 50, 51] have showcased extraordinary capabilities in language comprehension, reasoning, and generation. Capitalizing on the potent capabilities of Large Language Models (LLMs), series of works [2, 29, 35, 61, 10] have attempted to seamlessly integrate visual input into LLMs, paving the way for the development of Multimodal Large Language Models (MLLMs). Commonly, these methods adopt pre-trained vision encoders, such as Contrastive Language Image Pre-training (CLIP) [45], to extract features from images and subsequently align them with language embeddings. MLLMs have achieved remarkable performance in vision-related tasks, including visual reasoning [33, 26], image captioning [31, 46], visual question answering [40, 28], etc. Beyond open-source advancements, commercial closedsource MLLMs such as GPT-4o, Claude-3.7, and Gemini-2.0 are widely adopted. Preprint. Under review. Figure 1: Targeted adversarial examples generated by FOA-Attack, with responses from commercial MLLMs to the prompt Describe this image. Although large-scale foundation models have achieved remarkable successes, the security problems [15, 44, 63, 38, 23, 24] associated with them are equally alarming and represent an ongoing challenge that remains unresolved. Recent works [13, 60, 17, 37] have indicated that MLLMs are vulnerable to adversarial examples [19], as they inherit the adversarial vulnerability of vision encoders. The existence of adversarial examples poses significant security and safety risks to the real-world deployment of large-scale foundation models. Recently, some studies [4, 7, 48, 60? ] have delved into the adversarial robustness of MLLMs and have found that existing MLLMs remain vulnerable to adversarial attacks. Adversarial attacks on MLLMs are broadly classified as untargeted or targeted. Untargeted attacks aim to induce incorrect output, while targeted attacks force specific outputs. Adversarial transferabilitythe ability of adversarial examples to generalize across modelsis critical for both types, especially in black-box settings where the target model is inaccessible. Targeted black-box attacks are particularly challenging [5, 62, 56]. Previous works integrate multiple pre-trained image encoders (e.g., CLIP) to generate adversarial examples, which can significantly improve adversarial transferability. Notably, adversarial examples generated using open-source CLIP models can successfully carry out targeted attacks against closed-source commercial MLLMs. However, they achieve the limit improvement of adversarial transferability. Specifically, existing methods typically generate adversarial examples by minimizing contrastive loss between the global features of adversarial examples and target samples, where global features are often represented by the [CLS] token in open-source image encoders such as CLIP. While this strategy can produce semantically aligned adversarial samples in the feature space of the source model, it largely ignores the rich local features encoded by patch tokens. These local features contain fine-grained spatial and semantic details essential for comprehensive understanding in vision-language tasks. Neglecting them leads to weak alignment at the local level, resulting in adversarial perturbations that are less generalizable and highly dependent on the specific characteristics of the source model. Consequently, the generated adversarial examples tend to overfit the surrogate models and exhibit poor transferability to other models, especially commercial closed-source MLLMs. To alleviate these issues, we propose FOA-Attack, targeted transferable adversarial attack method based on optimal alignment of global and local features. Specifically, at the global level, we propose to adopt coarse-grained feature alignment loss based on cosine similarity, encouraging the global features (e.g., [CLS] tokens) of the adversarial example to align closely with those of the target sample. At the local level, previous works [14] indicate that the [CLS] token in the Transformer architecture represents global features, while other tokens represent local patch features. To fully extract the information from the target image, we use local features to generate adversarial samples. Although local features are rich, they are also redundant. We employ clustering techniques to distill compact and discriminative local patterns; that is, we use the features of the cluster centers to represent the characteristics of each cluster. We then formulate the alignment of these local features as an optimal transport (OT) problem and propose local clustering OT loss to achieve finegrained alignment between adversarial and target samples. Moreover, to further improve adversarial transferability, we propose dynamic ensemble model weighting strategy that adaptively balances the weights of multiple models during adversarial example generation. Specifically, we generate adversarial samples using multiple CLIP image encoders, treating enhancement of feature similarity to the target sample across different encoders as separate tasks. The convergence of each objective can be indicated by the rate at which its loss decreasesfaster loss reduction implies higher learning speed. Consequently, higher learning speed results in lower weight assigned to that objective. Extensive experiments demonstrate that the proposed FOA-Attack consistently outperforms state-of-the-art targeted adversarial attack methods, achieving superior transferability against both open-source and closed-source MLLMs. As shown in Fig. 1, the proposed FOA-Attack generates adversarial examples with superior transferability. Our main contributions are as follows: We propose FOA-Attack, targeted transferable attack framework that jointly aligns global and local features, effectively guiding adversarial examples toward the target feature distribution and enhancing transferability. At the global level, we propose cosine similarity-based global feature loss to align coarsegrained representations, while at the local level, we extract compact patch-level features via clustering and formulate their alignment as an optimal transport (OT) problem. Subsequently, we propose local clustering OT loss for fine-grained alignment. We propose dynamic ensemble model weighting strategy that adaptively balances multiple image encoders based on their convergence rates, substantially boosting the transferability of adversarial examples. Extensive experiments across various models are conducted to demonstrate that FOA-Attack consistently outperforms state-of-the-art methods, achieving remarkable performance even against closed-source MLLMs."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Multimodal large language models Large language models (LLMs) have demonstrated remarkable performance in Natural Language Processing (NLP). Leveraging the impressive capabilities of LLMs, several studies have explored their integration with visual inputs, enabling strong performance across applications such as multimodal dialogue systems [2, 57, 1], visual question answering [52, 58, 25], etc. This integration marks pivotal step toward the evolution of Multimodal Large Language Models (MLLMs). Existing studies achieve the integration of textual and visual modes through different strategies. Specifically, some studies focus on utilizing learnable queries to extract visual information and then adopt LLMs to generate text information based on the extracted visual features, such as Flamingo [2], BLIP2 [29]. Some works propose to adopt several projection layers to align the visual features with text embeddings, such as PandaGPT [49], LLaVA [35, 36]. In addition, some works [16] propose to use some lightweight adapters to perform fine-tuning for performance improvement. Moreover, several studies [30, 41] have expanded the scope of research to include video inputs, utilizing the extensive capabilities of LLMs for enhanced video understanding tasks. 2.2 Adversarial attacks Previous adversarial attack methods have primarily focused on image classification tasks. They usually utilize model gradients to generate adversarial examples, such as FGSM [18], PGD [42], C&W [6]. These studies have shown that deep neural networks are easily fooled by adversarial examples. Some studies [20, 53, 55] have demonstrated that MLLMs not only inherit the advantages of vision modules but also their vulnerabilities to adversarial examples. Adversarial attacks for MLLMs can be categorized as untargeted attacks and targeted attacks. Untargeted attacks aim to induce MLLMs to produce incorrect textual outputs, whereas targeted attacks aim to force specific, predetermined outputs. series of recent works has paid more attention to the transferability of adversarial attacks, particularly in targeted scenarios. Adversarial transferability refers to the ability of adversarial examples generated on surrogate models to successfully attack unseen models. In particular, Zhao et al. [60] propose AttackVLM, involving generating targeted adversarial examples using pre-trained models like CLIP [45] and BLIP [29], and then transferring these examples to other VLMs such as MiniGPT-4 [61], LLaVA. They have demonstrated that image-to-image feature matching can improve adversarial transferability more effectively than image-to-text feature matching, finding that has inspired subsequent research. Chen et al. [8] propose the Common Weakness Attack (CWA), method that enhances the transferability of adversarial examples by targeting shared vulnerabilities among ensemble surrogate models. Subsequently, Dong et al. [13] propose 3 Figure 2: Overview of the proposed FOA-Attack. (a) The proposed feature optimal alignment loss which includes the coarse-grained feature loss and the fine-grained feature loss. (b) The proposed dynamic ensemble model weighting strategy. the SSA-CWA method, which combines Spectrum Simulation Attack [39] (SSA) and Common Weakness Attack (CWA) to enhance the transferability of adversarial examples against closed-source commercial MLLMs like Googles Bard. Guo et al. [22] propose AdvDiffVLM, diffusion-based framework that integrates Adaptive Ensemble Gradient Estimation (AEGE) and GradCAM-guided Mask Generation (GCMG) to efficiently generate targeted and transferable adversarial examples for MLLMs. Zhang et al. [59] propose AnyAttack, self-supervised framework, which trains noise generator on the large-scale LAION-400M dataset using contrastive learning, to generate targeted adversarial examples for MLLMs without labels. Li et al. [32] propose the M-Attack method, which uses random cropping and resizing during optimization, to significantly improve the transferability of adversarial examples against MLLMs."
        },
        {
            "title": "3 Methodology",
            "content": "Previous works show ensemble-based adversarial examples exhibit better transferability than singlemodel ones; thus, we employ dynamic ensemble framework in this work. As shown in Fig. 2, the proposed FOA-Attack incorporates feature optimal alignment loss and dynamic ensemble weighting strategy to jointly enhance adversarial transferability across different foundation models. 3.1 Preliminary Given an ensemble of image encoders from vision-language pre-training models = {fθ1, fθ2 , , fθt}, where each image encoder : RD RF outputs the image features for an input RD. Given natural image xnat and target image xtar, the goal of the transfer-based attack is to generate an adversarial example xadv whose features are as close as possible to those of the target image. It can be formulated as constrained optimization problem: min xadv (cid:88) i= [L(fθi(xadv), fθi (xtar))] , s.t. xadv xnat ϵ, (1) where represents the loss function, ϵ represents the maximum perturbation strength, and the adversarial examples are generated under the ℓ norm. 3.2 The proposed coarse-grained feature optimal alignment Given an image encoder (e.g., CLIP) fθ, we extract the coarse-grained global features (e.g., [CLS] token) of the adversarial example xadv as = fθ(xadv) R1d, where is the feature dimension. Similarly, the coarse-grained global feature of the target image is extracted as = fθ(xtar) R1d. To promote the adversarial example to align with the semantics of the target image at global level, we minimize the negative cosine similarity between their coarse-grained features as the optimization 4 objective. The loss function can be defined as: Lcoa = 1 cos(X, Y) = 1 X, X , (2) where X, is the inner product and is the ℓ2 norm. 3.3 The proposed fine-grained feature optimal alignment Given an image encoder (e.g., CLIP) fθ, we extract the fine-grained local features (e.g., patch tokens) of the adversarial example and the target image. They can be defined as: Xloc = loc θ (xadv) Rmd, Yloc = loc θ (xtar) Rmd (3) θ where Xloc and Yloc represent the local features of the adversarial sample and the target image respectively, loc represents the image features extracted from patch tokens of the image encoder, and represents the number of patch or local features. Since local features contain fine-grained image information as well as more redundant image information, to reduce redundancy and retain discriminative information from the local features, we apply K-means clustering on Xloc and Yloc to obtain representative cluster centers. Formally, we define: Xclu = KMeans(Xloc, n) Rnd, Yclu = KMeans(Yloc, n) Rnd, (4) where Xclu and Yclu denote the cluster centers obtained from the local features of the adversarial and target images, respectively. Each cluster center summarizes semantically coherent region in the original image feature space, thus providing more compact and informative representation for alignment. In our modeling of fine-grained local feature loss, we have drawn inspiration from the theory of optimal transport [54]. This theory was proposed by Villani with the objective of achieving the transportation of goods at minimal cost. In our study, we model the local features of the adversarial example and the target image as two separate distributions. Our goal is to identify the most efficient transportation scheme to more appropriately match the features of the target image onto the adversarial example, which can facilitate the transition between the two distributions. Let µ = {Xa a=1 represent the distribution of clustering local features in the adversarial example, where is the number of clustering local features, and Xa clu denotes the a-th clustering local feature. Similarly, let ν = (cid:8)Yb (cid:9)n b=1 represent the distribution of clustering local features in the target image, with Yb clu) defines the cost of transporting feature from Xclu in the adversarial example to Yclu in the target image. Hence, the optimization problem is formulated as: clu representing the b-th clustering local feature. The cost function c(Xa clu, Yb clu}n clu min (cid:88) (cid:88) a=1 b=1 c(Xa clu, Yb clu) πab, s.t. a, (cid:88) b=1 πab = 1; b, (cid:88) a=1 πab = 1; a, b, πab 0, where the matrix π represents the transport plan between the features of the adversarial examples and target images. Each element πab of this matrix indicates the proportion of the a-th feature from the adversarial example that is assigned to the b-th feature in the target image. The constraints ensure the alignment of local features in accordance with µ and ν. The cost function is commonly computed using the negative cosine similarity as below: c(Xa clu, Yb clu) = 1 Xa clu, Yb clu, (5) The Sinkhorn algorithm [11] is employed to solve this optimal transport problem. Let Cab = c(Xa clu) be the cost of transporting the a-th local feature of the adversarial example to the b-th local feature of the target image. Local feature loss begins by defining the cost matrix: clu, Yb Cab = c(Xa clu, Yb clu), a, Then iteratively update and v: ua = 1 (cid:32) (cid:88) (cid:18) exp (cid:19) vb Cab λ (cid:33)1 , vb = 1 (cid:32) (cid:88) (cid:18) exp (cid:19) ua Cab λ (cid:33) , 5 (6) (7) where λ > 0 is the regularization parameter (default: λ = 0.1). The transport plan is: Finally, the local feature loss is: (cid:18) πab = ua exp (cid:19) Cab λ vb. Lf in = (cid:88) a,b Cab πab. (8) (9) Finally, the total loss of FOA-Attack for the image encoder fθ can be defined as: Lθ = Lcoa + η Lf in, (10) where η is the weighting factor that balances the local loss component. To handle varying local feature complexity, we adopt progressive strategy that increases the number of cluster centers if the attack fails. In this paper, the number of centers is set to 3 and 5. 3.4 The proposed dynamic ensemble model weighting strategy Building upon prior work, we generate adversarial examples using ensemble losses from multiple models to enhance adversarial transferability, computed as: = (cid:88) i=1 Wi Lθi, (11) where Lθi represents the loss generated on the i-th image encoder and Wi represents the corresponding weight coefficient. Previous studies typically set all weights Wi at 1.0 without investigating the impact of varying Wi values on adversarial transferability, leading to limited improvements. Due to inconsistent vulnerabilities in different models, assigning uniform weights can cause optimization to favor certain losses. This often results in adversarial examples that are effective only on specific models, thereby reducing adversarial transferability. To further boost adversarial transferability, we propose dynamic ensemble model weighting strategy to adaptively balance the weights of multiple models for adversarial example generation. Specifically, we generate adversarial examples using multiple CLIP image encoders, where improving the feature alignment between the adversarial and target samples on each encoder is treated as an independent optimization task. To balance these tasks, we monitor the convergence behavior of each objective by measuring the rate of loss reduction. faster decrease in loss indicates higher effective learning speed, suggesting that the task is easier to optimize. Hence, we assign lower weight to objectives with higher learning speeds, ensuring that the optimization does not overemphasize the easily aligned tasks while neglecting others. At step T, the learning speed is calculated by the loss ratio between steps and 1: Si(T) = LT θi LT1 θi (fθi(xadv), fθi(xtar)) (fθi(xadv), fθi(xtar)) , (12) where Lθi is calculated by using Eq. (10) and Si(T) represents the learning speed of the adversarial example generation on the i-th model. The weight parameters in Eq. (11) can be calculated by: Wi = Winit exp (Si(T)/T ) j=1 exp (Sj(T)/T ) (cid:80)t , (13) where Winit denotes the initial setting of each Wi, consistent with the M-Attack configuration of 1.0. Multiplying by the number of surrogate models scales the weights to fluctuate around 1.0, thereby refining the initialization. The temperature coefficient further adjusts the relative differences between task weights. detailed description of the algorithm is provided in the Appendix A."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Settings Datasets. Following previous works [13, 32], we use 1,000 clean images of size 224 224 3 from the NIPS 2017 Adversarial Attacks and Defenses Competition dataset1. Additionally, we randomly select 1,000 images from the MSCOCO validation set [34] as target images. 1https://nips.cc/Conferences/2017/CompetitionTrack 6 Table 1: Performance of ASR (%) and AvgSim on different open-source MLLMs. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 4.9 8.7 14.0 2.1 0.9 13.7 38.6 52.4 0.08 0.12 0.17 0.01 0.03 0.16 0.35 0. 9.7 13.3 26.1 2.5 0.7 21.6 52.6 70.7 0.14 0.17 0.27 0.01 0.03 0.24 0.46 0.58 31.4 11.3 46.3 1.5 1.1 37.5 68.3 79. 0.31 0.14 0.42 0.01 0.03 0.35 0.56 0.65 27.7 9.5 47.1 1.6 1.2 38.4 67.1 78.9 0.28 0.12 0.42 0.01 0.03 0.37 0.56 0. 8.2 8.4 15.7 0.7 7.6 10.2 23.0 38.1 0.16 0.15 0.23 0.00 0.15 0.17 0.29 0.41 2.3 1.7 11.6 0.8 0.9 8.3 21.3 35. 0.07 0.05 0.16 0.01 0.03 0.15 0.25 0.35 Implementation Settings. Following [32], we adopt three CLIP variants, which include ViT-B/16, ViT-B/32, and ViT-g-14-laion2B-s12B-b42K, as surrogate models to generate adversarial examples. The perturbation budget ϵ is set to 16/255 under the norm ℓ. The attack step size is set to 1/255. The number of attack iterations is set to 300. We evaluate the transferability of adversarial examples across fourteen MLLMs, including six open-source models (Qwen2.5-VL-3B/7B, LLaVa-1.5/1.6-7B, Gemma-3-4B/12B), five closed-source models (Claude-3.5/3.7, GPT-4o/4.1, Gemini-2.0), and three reasoning-oriented closed-source models (GPT-o3, Claude-3.7-thinking, Gemini-2.0-flash-thinkingexp). The text prompt of these models is set to Describe this image. All experiments are run on an Ubuntu system using an NVIDIA A100 Tensor Core GPU with 80GB of RAM. Competitive Methods. We compare the proposed FOA-Attack with five advanced targeted and transfer-based adversarial attack methods for MLLMs: AttackVLM [60], SSA-CWA [13], AdvDiffVLM [22], AnyAttack [59], and M-Attack [32]. Evaluation metrics. Following [32], we adopt the widely used LLM-as-a-judge framework. Specifically, we use the same target MLLM to generate captions for both adversarial examples and target images, then assess their similarity using GPTScore. An attack is considered successful if the similarity score exceeds 0.5 2, which means that the adversarial example and the target image have the same subject. Additional results under varied thresholds are provided in the Appendix B. We report the attack success rate (ASR) and the average similarity score (AvgSim). For reproducibility, we include detailed evaluation prompts in the Appendix C. 4.2 Hyper-parameter Selection We have two hyper-parameters in the proposed method: the temperature coefficient and the weighting factor η. To study their effects, we conduct hyper-parameter selection experiments. As shown in Fig. 3 (a), setting = 1.0 achieves the best trade-off between ASR and AvgSim, particularly on GPT-4o. While the ASR on Claude-3.5 shows minor variation, the performance on GPT-4o is more sensitive to , with = 1.0 leading to optimal semantic alignment. In Fig. 3 (b), we find that η = 0.2 consistently delivers the best performance on both models. larger η overemphasizes the fine-grained loss, which slightly harms overall alignment. Therefore, we set = 1.0 and η = 0.2 as the default values in our experiments. Figure 3: (a) Impact of the temperature coefficient ; (b) Impact of the weighting factor η. 4.3 Comparisons results Comparisons with different attack methods. We compare our proposed FOA-Attack with several existing adversarial attack baselines, including AttackVLM, AdvDiffVLM, SSA-CWA, AnyAttack, and M-Attack, across both open-source and closed-source MLLMs. As shown in Table 1, on opensource models such as Qwen, LLaVa, and Gemma series, FOA-Attack consistently outperforms all baselines by large margin. Specifically, it achieves an average ASR of 70.7% and 79.6% on Qwen2.5-VL-7B and LLaVa-1.5-7B, respectively, significantly surpassing the prior strongest method, M-Attack (52.6% and 68.3%). Moreover, FOA-Attack achieves the highest AvgSim scores across 2This work adopts stricter success threshold than the 0.3 used in M-Attack [32]. Table 2: Performance of ASR (%) and AvgSim on different closed-source MLLMs. Gemini-2.0 Claude-3.7 Claude-3.5 GPT-4.1 GPT-4o Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 0.1 4.8 0. 0.8 0.4 4.6 6.0 11.9 0.02 0.08 0.02 0.01 0.02 0.09 0.10 0.16 0.2 7.3 1.2 1.1 0.4 4.3 8.9 15.8 0.03 0.11 0. 0.01 0.03 0.08 0.12 0.18 16.2 5.3 39.7 2.3 0.5 8.2 60.3 75.1 0.21 0.10 0.38 0.01 0.03 0.15 0.50 0.59 17.5 6.4 42. 2.5 0.2 7.3 60.8 77.3 0.22 0.11 0.39 0.01 0.02 0.13 0.51 0.62 7.0 2.6 28.9 1.6 0.4 6.1 44.8 53.4 0.12 0.06 0. 0.01 0.02 0.12 0.41 0.50 all models, indicating better semantic alignment between adversarial and target captions. Table 2 further demonstrates the superiority of FOA-Attack on closed-source commercial MLLMs, including Claude-3, GPT-4, and Gemini-2.0. Notably, FOA-Attack yields 75.1% and 77.3% ASR on GPT-4o and GPT-4.1, outperforming M-Attack by 14.8% and 16.5%, respectively. On Gemini-2.0, FOAAttack achieves remarkable 53.4% ASR and 0.50 AvgSim, while other baselines perform poorly with ASRs below 8%. These results validate the effectiveness of our method across wide range of both openand closed-source MLLMs. FOA-Attack results against defenses are in the Appendix D. Comparisons on reasoning MLLMs. We further evaluate our FOA-Attack on 100 randomly selected images with reasoning-enhanced closed-source MLLMs, including GPT-o3, Claude-3.7-thinking, and Gemini-2.0-flash-thinking-exp, as shown in Table 3. Compared to the strong baseline M-Attack, our method consistently achieves higher ASR and AvgSim across all models. Specifically, on GPT-o3, FOA-Attack achieves an ASR of 81.0% and an AvgSim of 0.63, outperforming M-Attack by 14.0% and 0.09, respectively. Similarly, on Gemini-2.0-flash-thinking-exp, FOA-Attack improves ASR from 49.0% to 57.0% and AvgSim from 0.43 to 0.51. Even for the highly robust Claude-3.7-thinking model, our method raises ASR from 10.0% to 16.0%, along with slight improvement in AvgSim. These results demonstrate that FOA-Attack remains highly effective even against reasoning-enhanced MLLMs, which are typically assumed to be more robust due to their advanced alignment and reasoning capabilities. However, our findings reveal that these models exhibit comparable or even weaker resistance to adversarial inputs than their non-reasoning MLLMs. This may stem from their reliance on textual reasoning, while shared visual encoders remain vulnerable to visual perturbations. Table 3: Performance of ASR (%) and AvgSim on reasoning-enhanced closed-source MLLMs. Method Model ASR AvgSim ASR AvgSim M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble 67.0 81.0 0.54 0.63 10.0 16. 0.15 0.18 ASR 49.0 57.0 AvgSim 0.43 0.51 GPT-o Claude-3.7-thinking Gemini-2.0-flash-thinking-exp 4.4 Ablation study Method Claude-3.5 ASR AvgSim ASR AvgSim Table 4: Ablation study of our FOA-Attack. GPT-4o To understand the contribution of each component in FOA-Attack, we conduct an ablation study on 100 randomly selected images. As shown in Table 4, we systematically remove three core modules from FOA-Attack: global alignment, local alignment, and dynamic loss weighting. Removing global alignment results in noticeable drop in performance, with ASR decreasing from 81.0% to 78.0% on GPT-4o and from 16.0% to 14.0% on Claude-3.5. It indicates the importance of aligning coarse-grained features for effective adversarial transferability. Excluding local alignment leads to more significant degradation, especially in AvgSim, indicating that fine-grained feature alignment is essential for preserving semantic consistency between the adversarial and target samples. ASR on GPT-4o drops to 76.0%, and AvgSim decreases from 0.62 to 0.58. Lastly, removing dynamic loss weighting also reduces performance (e.g., 81.0% 79.0% ASR on GPT-4o), showing that adaptively balancing optimization objectives also contributes to improving adversarial transferability. M-Attack FOA-Attack (Ours) w/o global alignment w/o local alignment w/o dynamic loss weighting 10.0 16.0 14.0 12.0 13.0 0.13 0.18 0.17 0.15 0.17 73.0 81.0 78.0 76.0 79.0 0.56 0.62 0.60 0.58 0. 4.5 Performance analysis Keyword matching rate (KMR). Previous work manually assigned three semantic keywords to each image and introduced three success thresholdsKMRα (at least one matched), KMRβ (at least two matched), and KMRγ (all three matched)to evaluate attack transferability under different semantic 8 Figure 4: Visualization of adversarial images and perturbation. matching levels. Following their setting, we compare the proposed method with previous works on 100 randomly selected images. As shown in Table 5, FOA-Attack consistently outperforms all baselines across different models (GPT-4o, Gemini-2.0, and Claude-3.5) and all keyword matching thresholds (KMRα, KMRβ, KMRγ), demonstrating superior targeted transferability. Notably, it achieves 92.0% on KMRα and significantly higher scores on stricter metrics (76.0% KMRβ, 27.0% KMRγ) on GPT-4o. Even on the more robust Claude-3.5, FOA-Attack achieves the best performance with 37.0% KMRα. These results highlight the effectiveness of our FOA-Attack in enhancing adversarial transferability. Table 5: Keyword Matching Rate (KMR) comparison across different models and attack methods. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble GPT-4o Gemini-2.0 Claude-3.5 KMRα KMRβ KMRγ KMRα KMRβ KMRγ KMRα KMRβ KMRγ 9.0 8.0 7.0 2.0 11.0 44.0 82.0 92.0 4.0 2.0 4. 0.0 6.0 20.0 54.0 76.0 0.0 0.0 0.0 0.0 0.0 4.0 13.0 27.0 7.0 7.0 7.0 2.0 5.0 46.0 75.0 88.0 2.0 2.0 2. 0.0 2.0 21.0 53.0 69.0 0.0 0.0 0.0 0.0 0.0 5.0 11.0 24.0 6.0 4.0 5.0 2.0 7.0 25.0 31.0 37.0 3.0 1.0 2. 0.0 3.0 10.0 18.0 23.0 0.0 0.0 0.0 0.0 0.0 2.0 3.0 5.0 Sample visualization. Fig. 4 shows adversarial images and perturbations from different methods. Our method preserves image quality with minimal visible artifacts, while baselines such as AnyAttack and M-Attack introduce more noticeable noise. The perturbation maps on the right reveal that our method produces more structured and semantically aligned patterns, indicating stronger feature-level alignment and better adversarial transferability. Commercial MLLM responses are in the Appendix E. Table 6: Performance with varying cluster centers. GPT-4o Impact of more cluster centers. To enhance transferability, we adopt progressive strategy that increases the number of cluster centers upon attack failure. We conduct experiments on 100 randomly selected images to explore the impact of more cluster centers. As shown in Table 6, incorporating more centers consistently improves ASR and AvgSim, but also leads to higher time cost. To strike balance between effectiveness and efficiency, we adopt the ([3,5]) setting in our main experiments. FOA-Attack ([3]) FOA-Attack ([3,5]) FOA-Attack ([3,5,8]) FOA-Attack ([3,5,8,10]) ASR AvgSim ASR AvgSim 76.0 81.0 83.0 84.0 0.58 0.62 0.63 0.64 12.0 16.0 17.0 18.0 0.14 0.18 0.20 0. 113 217 315 410 M-Attack [32] Time (mins) Claude-3.5 Method 73. 10.0 0.13 0."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose FOA-Attack, targeted transferable adversarial attack framework that jointly aligns global and local features to improve transferability against both openand closedsource MLLMs. Our method incorporates global cosine similarity loss, local clustering optimal transport loss, and dynamic ensemble weighting strategy to comprehensively enhance adversarial transferability. Extensive experiments across various models demonstrate that the proposed FOA9 Attack significantly outperforms existing state-of-the-art attack methods in both attack success rate and semantic similarity, especially on closed-source commercial and reasoning-enhanced MLLMs. These results reveal persistent vulnerabilities in MLLMs and highlight the importance of finegrained feature alignment in designing transferable adversarial attacks. Further discussion, including limitations and broader impacts, is provided in the Appendix F."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 2371623736, 2022. [3] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [4] Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. Image hijacks: Adversarial images can control generative models at runtime. arXiv preprint arXiv:2309.00236, 2023. [5] Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Hee-Seon Kim, and Changick Kim. Improving the transferability of targeted adversarial examples through object-based diverse input. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1524415253, 2022. [6] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 3957. Ieee, 2017. [7] Nicholas Carlini, Milad Nasr, Christopher Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447, 2023. [8] Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, and Jun Zhu. Rethinking model ensemble in transfer-based adversarial attacks. arXiv preprint arXiv:2303.09105, 2023. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. [10] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 958979, 2024. [11] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. [12] Gavin Weiguang Ding, Luyu Wang, and Xiaomeng Jin. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. [13] Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. How robust is googles bard to adversarial image attacks? arXiv preprint arXiv:2309.11751, 2023. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 10 [15] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. [17] Sensen Gao, Xiaojun Jia, Xuhong Ren, Ivor Tsang, and Qing Guo. Boosting transferability in vision-language attacks via diversification along the intersection region of adversarial trajectory. arXiv preprint arXiv:2403.12445, 2024. [18] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. [19] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6572. [20] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. Agent smith: single image can jailbreak one million multimodal llm agents exponentially fast. In International Conference on Machine Learning, 2024. [21] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. [22] Qi Guo, Shanmin Pang, Xiaojun Jia, Yang Liu, and Qing Guo. Efficient generation of targeted and transferable adversarial examples for vision-language models via diffusion models. IEEE Transactions on Information Forensics and Security, 2024. [23] Dongchen Han, Xiaojun Jia, Yang Bai, Jindong Gu, Yang Liu, and Xiaochun Cao. Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization. arXiv preprint arXiv:2312.04403, 2023. [24] Bangyan He, Xiaojun Jia, Siyuan Liang, Tianrui Lou, Yang Liu, and Xiaochun Cao. Saattack: Improving adversarial transferability of vision-language pre-training models via selfaugmentation. arXiv preprint arXiv:2312.04913, 2023. [25] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023. [26] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [27] Xiaojun Jia, Xingxing Wei, Xiaochun Cao, and Hassan Foroosh. Comdefend: An efficient image compression model to defend adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60846092, 2019. [28] Jiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo, Zhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng, Xika Lin, and Yu Han. Natural language understanding and inference with mllm in visual question answering: survey. ACM Computing Surveys, 57(8):136, 2025. [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [30] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 11 [31] Wei Li, Hehe Fan, Yongkang Wong, Yi Yang, and Mohan Kankanhalli. Improving context understanding in multimodal large language models via multimodal composition learning. In Forty-first International Conference on Machine Learning, 2024. [32] Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen. frustratingly simple yet highly effective attack baseline: Over 90% success rate against the strong black-box models of gpt-4.5/4o/o1. arXiv preprint arXiv:2503.10635, 2025. [33] Zhiyuan Li, Dongnan Liu, Chaoyi Zhang, Heng Wang, Tengfei Xue, and Weidong Cai. Enhancing advanced visual reasoning ability of large language models. arXiv preprint arXiv:2409.13980, 2024. [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [37] Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu Qiao. Safety of multimodal large language models on images and text. arXiv preprint arXiv:2402.00357, 2024. [38] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023. [39] Yuyang Long, Qilong Zhang, Boheng Zeng, Lianli Gao, Xianglong Liu, Jian Zhang, and Jingkuan Song. Frequency domain model augmentation for adversarial attack. In European conference on computer vision, pages 549566. Springer, 2022. [40] Duc-Tuan Luu, Viet-Tuan Le, and Duc Minh Vo. Questioning, answering, and captioning for zero-shot detailed image caption. In Proceedings of the Asian Conference on Computer Vision, pages 242259, 2024. [41] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [42] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [44] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [46] Sara Sarto, Marcella Cornia, and Rita Cucchiara. Image captioning evaluation in the age of multimodal llms: Challenges and future perspectives. arXiv preprint arXiv:2503.14604, 2025. 12 [47] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. [48] Christian Schlarmann and Matthias Hein. On the adversarial robustness of multi-modal foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 36773685, 2023. [49] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. [50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [52] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. [53] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023. [54] Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009. [55] Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. Instructta: Instructiontuned targeted attack for large vision-language models. arXiv preprint arXiv:2312.01886, 2023. [56] Phoenix Neale Williams and Ke Li. Black-box sparse adversarial attack via multi-objective optimisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1229112301, 2023. [57] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. [58] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 30813089, 2022. [59] Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Jitao Sang, and Dit-Yan Yeung. Anyattack: Towards large-scale self-supervised generation of targeted adversarial examples for vision-language models. arXiv preprint arXiv:2410.05346, 2024. [60] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. arXiv preprint arXiv:2305.16934, 2023. [61] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [62] Hegui Zhu, Xiaoyan Sui, Yuchen Ren, Yanmeng Jia, and Libo Zhang. Boosting transferability of targeted adversarial examples with non-robust feature alignment. Expert Systems with Applications, 227:120248, 2023. [63] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023. 13 Detailed Description of Our FOA-Attack Following the M-Attack [32], we propose targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack. The detailed description of the proposed FOA-Attack is shown in Algorithm 1. Algorithm 1: FOA-Attack Input: clean image xnat, target image xtar, perturbation budget ϵ, iterations n, loss function L, surrogate model ensemble = {fθ1, fθ2 , , fθt}, image processing , step size α adv = xnat + δ0 (i.e., δ0 = 0) ; // Initialize adversarial image xadv Output: adversarial image xadv 1 Initialize: x0 2 for = 0 to 1 do = (xi 3 ˆxa ; for = 1 to do adv), ˆxt = (xtar); // Perform random crop 4 5 6 7 9 10 11 12 13 15 16 17 18 19 (xtar), fθj (ˆxa ),fθj (ˆxt) )fθj (ˆxt) , fθj (ˆxa (xadv), Yloc = loc θJ Lcoa = 1 Xloc = loc θj Xclu = KMeans(Xloc, n), Yclu = KMeans(Yloc, n), clu) = 1 Xa Cab = c(Xa (cid:0)(cid:80) (cid:0)(cid:80) exp (cid:0) Cab ua = 1 πab = ua exp (cid:0) Cab Lf in = (cid:80) LT θj if == 0 then Sj(T) = 1, clu, Yb clu), exp (cid:0) Cab (cid:1) vb, a,b Cab πab = Lcoa + η Lf in, clu, Yb vb = 1 c(Xa , a, (cid:1) vb (cid:1)1 λ λ λ clu, Yb (cid:1) ua clu, (cid:1)1 , else Sj(T) = LT θj T1 θj , Winit = 1 for = 1 to do Wj = Winit exp(Sj (T)/T ) j=1 exp(Sj (T)/T ) , (cid:80)t (cid:80)m 22 21 20 ˆxa j=1 Wj Lθj ; gi = 1 δi+1 = Clip(δi + α sign(gi), ϵ, ϵ); ˆxa i+1 = ˆxa adv = ˆxa xi+1 23 24 return ˆxa + δi+1; i+1 14 Table 7: Performance (threshold is 0.3) of ASR (%) and AvgSim on different open-source MLLMs. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 14.6 22.4 32.8 2.7 4.8 34.7 63.3 77.4 0.08 0.12 0.17 0.01 0.03 0.16 0.35 0. 26.5 31.6 48.7 3.1 5.3 41.9 80.2 91.1 0.14 0.17 0.27 0.01 0.03 0.24 0.46 0.58 57.3 27.3 70.2 1.9 3.9 56.3 89.8 95. 0.31 0.14 0.42 0.01 0.03 0.35 0.56 0.65 49.8 23.1 68.2 2.1 4.9 59.2 87.4 93.0 0.28 0.12 0.42 0.01 0.03 0.37 0.56 0. 36.1 35.0 50.3 0.9 38.0 36.5 64.3 80.5 0.16 0.15 0.23 0.00 0.15 0.17 0.29 0.41 13.9 9.1 33.8 1.2 6.0 28.6 50.3 67. 0.07 0.05 0.16 0.01 0.03 0.15 0.25 0.35 Table 8: Performance (threshold is 0.3) of ASR (%) and AvgSim on different closed-source MLLMs. Claude-3.7 Gemini-2.0 Claude-3.5 GPT-4. GPT-4o Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 2.4 14.8 3.5 1.1 3.2 19.1 17.9 28.4 0.02 0.08 0.02 0.01 0.02 0.09 0.10 0. 4.1 20.5 4.9 1.4 3.7 18.7 23.8 36.4 0.03 0.11 0.03 0.01 0.03 0.08 0.12 0.18 40.8 20.1 69.9 3.2 3.8 40.8 86.8 94. 0.21 0.10 0.38 0.01 0.03 0.15 0.50 0.59 42.6 21.9 71.8 2.9 3.0 39.5 89.1 95.6 0.22 0.11 0.39 0.01 0.02 0.13 0.51 0. 23.5 9.9 55.8 2.0 4.0 31.1 75.5 86.7 0.12 0.06 0.30 0.01 0.02 0.12 0.41 0."
        },
        {
            "title": "B More Comparison Results under Varied Thresholds",
            "content": "We further evaluate the performance of FOA-Attack at the threshold of 0.3. As shown in Table 7, FOA-Attack consistently achieves superior adversarial success rates (ASR) and average semantic similarity (AvgSim) on open-source MLLMs, such as 95.3% ASR and 0.66 AvgSim on LLaVA1.6-7B, significantly outperforming baseline ensemble attacks. Similarly, Table 8 highlights FOAAttacks strong transferability to closed-source models under the 0.3 threshold, achieving notably high performance (e.g., 95.6% ASR and 0.62 AvgSim on GPT-4.1), confirming its effectiveness and semantic alignment across diverse evaluation scenarios. Table 9: Performance (threshold is 0.7) of ASR (%) and AvgSim on different open-source MLLMs. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 2.0 4.6 8. 0.2 0.3 11.6 22.7 35.2 0.08 0.12 0.17 0.01 0.03 0.16 0.35 0.45 5.3 6.6 15.7 0.4 0.5 17.3 35.4 53.1 0.14 0.17 0. 0.01 0.03 0.24 0.46 0.58 17.9 6.5 31.2 0.3 0.5 26.7 47.4 62.5 0.31 0.14 0.42 0.01 0.03 0.35 0.56 0.65 16.6 4.8 32. 0.5 0.2 23.2 48.0 63.6 0.28 0.12 0.42 0.01 0.03 0.37 0.56 0.66 3.9 3.8 8.1 0.2 3.0 5.8 11.1 23.2 0.16 0.15 0. 0.00 0.15 0.17 0.29 0.41 0.7 0.4 4.1 0.2 0.1 6.4 12.3 19.6 0.07 0.05 0.16 0.01 0.03 0.15 0.25 0.35 Table 10: Performance (threshold is 0.7) of ASR (%) and AvgSim on different closed-source MLLMs. Claude-3. Gemini-2.0 Claude-3.5 GPT-4.1 GPT-4o Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 0.0 2.4 0. 0.1 0.1 1.5 3.3 6.3 0.02 0.08 0.02 0.01 0.02 0.09 0.10 0.16 0.1 3.3 0.7 0.2 0.0 1.3 4.4 9.6 0.03 0.11 0. 0.01 0.03 0.08 0.12 0.18 7.8 3.0 25.5 0.5 0.4 1.8 38.8 57.9 0.21 0.10 0.38 0.01 0.03 0.15 0.50 0.59 8.2 3.0 26. 0.4 0.2 1.7 39.8 58.9 0.22 0.11 0.39 0.01 0.02 0.13 0.51 0.62 3.4 0.9 15.9 0.2 0.1 0.8 26.6 41.5 0.12 0.06 0. 0.01 0.02 0.12 0.41 0.50 Continuing with the threshold set to 0.7, Table 9 shows FOA-Attack maintains its lead among open-source MLLMs, achieving significantly higher ASR and AvgSim, such as 62.5% ASR and 0.66 AvgSim on LLaVA-1.6-7B, notably surpassing all baseline ensemble methods. Similarly, results in Table 10 indicate that FOA-Attack retains effectiveness against challenging closed-source models even at the higher threshold, notably achieving 58.9% ASR and 0.62 AvgSim on GPT-4.1, reinforcing its strong adversarial transferability and semantic alignment in stringent attack scenarios. Continuing with the threshold set to 0.8, Table 11 illustrates FOA-Attacks superior transferability across open-source MLLMs, achieving notably high ASR and AvgSim (e.g., 44.1% ASR, 0.65 15 Table 11: Performance (threshold is 0.8) of ASR (%) and AvgSim on different open-source MLLMs. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 1.2 2.3 4.1 0.1 0.2 4.6 12.0 20.2 0.08 0.12 0.17 0.01 0.03 0.16 0.35 0. 2.7 3.0 8.6 0.1 0.1 7.3 19.6 34.2 0.14 0.17 0.27 0.01 0.03 0.24 0.46 0.58 8.7 3.4 19.1 0.1 0.3 11.9 32.2 44. 0.31 0.14 0.42 0.01 0.03 0.35 0.56 0.65 10.1 2.6 23.2 0.1 0.1 13.4 33.7 47.6 0.28 0.12 0.42 0.01 0.03 0.37 0.56 0. 3.4 3.5 6.0 0.1 2.6 2.8 6.8 14.2 0.16 0.15 0.23 0.00 0.15 0.17 0.29 0.41 0.2 0.4 2.0 0.0 0.0 2.2 6.5 11. 0.07 0.05 0.16 0.01 0.03 0.15 0.25 0.35 Table 12: Performance (threshold is 0.8) of ASR (%) and AvgSim on different closed-source MLLMs. Claude-3.7 Gemini-2.0 Claude-3.5 GPT-4. GPT-4o Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 0.0 1.1 0.0 0.0 0.0 0.5 1.6 4.5 0.02 0.08 0.02 0.01 0.02 0.09 0.10 0. 0.0 1.5 0.1 0.0 0.0 0.4 1.7 5.1 0.03 0.11 0.03 0.01 0.03 0.08 0.12 0.18 4.3 1.3 14.6 0.2 0.1 0.6 23.6 37. 0.21 0.10 0.38 0.01 0.03 0.15 0.50 0.59 4.3 1.5 13.0 0.1 0.2 0.7 23.0 37.1 0.22 0.11 0.39 0.01 0.02 0.13 0.51 0. 1.7 0.3 7.7 0.1 0.1 0.1 14.7 25.4 0.12 0.06 0.30 0.01 0.02 0.12 0.41 0.50 Table 13: Performance (threshold is 0.9) of ASR (%) and AvgSim on different open-source MLLMs. Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 0.3 0.6 1.1 0.0 0.1 1.3 4.0 5.6 0.08 0.12 0.17 0.01 0.03 0.16 0.35 0. 0.6 0.5 2.1 0.0 0.0 1.7 5.8 10.8 0.14 0.17 0.27 0.01 0.03 0.24 0.46 0.58 3.8 0.8 6.6 0.1 0.2 5.2 13.2 22. 0.31 0.14 0.42 0.01 0.03 0.35 0.56 0.65 4.2 1.3 10.2 0.0 0.0 6.4 18.1 27.2 0.28 0.12 0.42 0.01 0.03 0.37 0.56 0. 2.7 2.9 3.3 0.1 2.3 0.9 2.9 6.5 0.16 0.15 0.23 0.00 0.15 0.17 0.29 0.41 0.0 0.0 0.2 0.0 0.0 0.3 1.1 2. 0.07 0.05 0.16 0.01 0.03 0.15 0.25 0.35 Table 14: Performance (threshold is 0.9) of ASR (%) and AvgSim on different closed-source MLLMs. Claude-3.7 Gemini-2.0 Claude-3.5 GPT-4. GPT-4o Method AttackVLM [60] Model B/16 B/32 Laion AdvDiffVLM [22] SSA-CWA [13] AnyAttack [59] M-Attack [32] FOA-Attack (Ours) Ensemble Ensemble Ensemble Ensemble Ensemble ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim 0.0 0.1 0.0 0.0 0.0 0.0 0.1 0.7 0.02 0.08 0.02 0.01 0.02 0.09 0.10 0. 0.0 0.2 0.1 0.0 0.0 0.1 0.1 0.4 0.03 0.11 0.03 0.01 0.03 0.08 0.12 0.18 0.8 0.1 2.2 0.1 0.0 0.0 4.7 11. 0.21 0.10 0.38 0.01 0.03 0.15 0.50 0.59 0.7 0.1 2.7 0.0 0.0 0.0 6.3 12.1 0.22 0.11 0.39 0.01 0.02 0.13 0.51 0. 0.2 0.1 1.2 0.1 0.0 0.0 2.1 4.9 0.12 0.06 0.30 0.01 0.02 0.12 0.41 0.50 AvgSim on LLaVA-1.5-7B), substantially surpassing baseline methods. Similarly, in Table 12, FOA-Attack retains significant effectiveness against closed-source models even at this challenging threshold, notably reaching 37.2% ASR on GPT-4o and 37.1% ASR on GPT-4.1, while maintaining high AvgSim scores, reinforcing its exceptional adversarial transfer capability. With an even stricter threshold of 0.9, Tables 13 and 14 show FOA-Attack still effectively maintains its superior adversarial transferability. In Table 13, FOA-Attack outperforms baseline ensemble attacks on open-source MLLMs, notably achieving 27.2% ASR and 0.66 AvgSim on LLaVA-1.6-7B. In the closed-source scenario  (Table 14)  , FOA-Attack demonstrates notable effectiveness, particularly on GPT-4o and GPT-4.1 (11.2% and 12.1% ASR, respectively), continuing to exhibit strong semantic alignment (AvgSim 0.59). These results confirm FOA-Attacks remarkable transferability even under highly stringent evaluation conditions. 16 Figure 5: Evaluation prompt template."
        },
        {
            "title": "C Detailed Evaluation Prompt",
            "content": "Following M-Attack [32], we adopt the same way to evaluate the adversarial performance. Below is the detailed evaluation prompt used to assess semantic similarity between textual inputs: ASR: the {input_text_1} and {input_text_2} are used as placeholders for text inputs. The evaluation prompt template is shown in Fig. 5."
        },
        {
            "title": "D Comparison Results on Series of Defense Methods",
            "content": "We evaluate the attack performance of FOA-Attack against series of defense methods, including smoothing-based defenses [12] (Gaussian, Medium, and Average), JPEG compression [21], and Comdefend [27]. The experimental results on both open-source and closed-source MLLMs are shown in Table 15 and Table 16. Across all defenses, FOA-Attack consistently outperforms M-Attack in both ASR and AvgSim. On open-source models, FOA-Attack maintains strong ASR (e.g., 25.0% vs. 13.0% under Comdefend on Qwen2.5-VL-7B), while preserving semantic alignment. On closed-source models, the advantage is even more evident. Under Comdefend, our FOA-Attack achieves 61.0% ASR on GPT-4o and 55.0% on GPT-4.1, while M-Attack drops below 10%. Even under JPEG, FOA-Attack maintains over 50% ASR with stable AvgSim values. These results indicate that the proposed FOA-Attack achieves superior adversarial transferability and resilience across diverse defense strategies."
        },
        {
            "title": "E Commercial MLLM Response",
            "content": "To further validate the efficacy of FOA-Attack, we provide real-world interaction results indicating that adversarial examples can guide advanced commercial closed-source MLLMs, which include GPT-4o, GPT-o3, GPT-4.1, GPT-4.5, Claude-3.5-Sonnet, Claude-3.7-Sonnet, Gemini-2.0-Flash, and Gemini-2.5-Flash, to generate descriptions semantically aligned with the specified target images. Specifically, Fig. 6 to 13 correspond to the attack results on each of these models in order: Fig. 6 shows GPT-4o, Fig. 7 shows GPT-o3, Fig. 9 shows GPT-4.1, Fig. 8 shows GPT-4.5, Fig. 10 shows Claude3.5-Sonnet, Fig. 11 shows Claude-3.7-Sonnet, Fig. 12 shows Gemini-2.0-Flash, and Fig. 13 shows 17 Table 15: Attack performance of adversarial images against open-source Multimodal Large Language Models (MLLMs) after defense processing. Defense Method Qwen2.5-VL-3B Qwen2.5-VL-7B LLaVa-1.5-7B LLaVa-1.6-7B Gemma-3-4B Gemma-3-12B ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim Gaussian Medium Average JPEG Comdefend M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) 14.0 27.0 17.0 36.0 9.0 22.0 13.0 29.0 10.0 25.0 0.18 0. 0.21 0.31 0.14 0.24 0.20 0.32 0.13 0.28 27.0 50.0 35.0 60. 20.0 38.0 35.0 58.0 27.0 49.0 0.29 0.42 0.33 0.45 0.23 0. 0.35 0.49 0.27 0.46 50.0 67.0 44.0 62.0 38.0 57.0 60.0 77. 48.0 65.0 0.48 0.60 0.41 0.54 0.36 0.51 0.51 0.63 0.42 0. 48.0 65.0 41.0 60.0 36.0 56.0 59.0 77.0 46.0 63.0 0.47 0. 0.39 0.53 0.36 0.51 0.50 0.62 0.41 0.54 17.0 29.0 13.0 18. 11.0 28.0 29.0 50.0 14.0 33.0 0.25 0.35 0.18 0.25 0.18 0. 0.34 0.44 0.22 0.36 14.0 22.0 6.0 9.0 8.0 11.0 22.0 44. 12.0 22.0 0.17 0.27 0.10 0.16 0.12 0.17 0.27 0.42 0.17 0. Table 16: Attack performance of adversarial images against closed-source Multimodal Large Language Models (MLLMs) after defense processing. Method Model Claude-3.5 Claude-3.7 GPT-4o GPT-4.1 Gemini-2.0 ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim ASR AvgSim Gaussian Medium Average JPEG Comdefend M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) M-Attack [32] FOA-Attack (Ours) 2.0 3.0 3.0 4.0 2.0 5.0 9.0 14.0 2.0 6. 0.04 0.06 0.04 0.07 0.04 0.06 0.12 0.20 0.04 0.07 5.0 6. 4.0 6.0 1.0 3.0 14.0 22.0 5.0 11.0 0.06 0.07 0.06 0. 0.03 0.06 0.17 0.24 0.08 0.15 57.0 72.0 39.0 59.0 38.0 59. 60.0 75.0 35.0 61.0 0.45 0.57 0.37 0.48 0.37 0.48 0.48 0. 0.35 0.49 53.0 71.0 40.0 63.0 39.0 62.0 52.0 78.0 37.0 63. 0.44 0.57 0.38 0.50 0.36 0.50 0.45 0.59 0.37 0.51 29.0 50. 23.0 41.0 19.0 36.0 36.0 58.0 22.0 38.0 0.29 0.42 0.24 0. 0.22 0.34 0.35 0.49 0.25 0.39 Gemini-2.5-Flash. The consistent attack success across all models highlights the high transferability of the proposed FOA-Attack."
        },
        {
            "title": "F Limitations and Impact Statement",
            "content": "Limitations. Although the proposed method demonstrates excellent performance in transferring target adversarial examples, it introduces additional computations, such as local OT loss, which decrease the efficiency of generating adversarial examples. Enhancing the efficiency of these attacks will be key focus of our future research. Impact Statement. This paper proposes method for targeting transferrable adversarial attacks on MLLMs using targeted multi-modal alignment. The proposed method, like previous adversarial attack methods, investigates adversarial examples in order to identify adversarial vulnerabilities in MLLMs. This effort aims to guide future research into improving MLLMs against adversarial attacks and developing more effective defense approaches. Furthermore, the victim MLLMs employed in this study are open-source models with publicly available weights. The research on adversarial examples will help shape the landscape of AI security. 18 Figure 6: Example responses from the commercial MLLM-GPT-4o to targeted attacks generated by our method. 19 Figure 7: Example responses from the commercial MLLM-GPT-o3 to targeted attacks generated by our method. 20 Figure 8: Example responses from the commercial MLLM-GPT-4.5 to targeted attacks generated by our method. 21 Figure 9: Example responses from the commercial MLLM-GPT-4.1 to targeted attacks generated by our method. 22 Figure 10: Example responses from the commercial MLLM-Claude-3.5-Sonnet to targeted attacks generated by our method. 23 Figure 11: Example responses from the commercial MLLM-Claude-3.7-Sonnet to targeted attacks generated by our method. 24 Figure 12: Example responses from the commercial MLLM-Gemini-2.0-Flash to targeted attacks generated by our method. 25 Figure 13: Example responses from the commercial MLLM-Gemini-2.5-Flash to targeted attacks generated by our method."
        }
    ],
    "affiliations": [
        "MBZUAI, United Arab Emirates",
        "Nanyang Technological University, Singapore",
        "Sea AI Lab, Singapore",
        "University of Illinois Urbana-Champaign, USA"
    ]
}