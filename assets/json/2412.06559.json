{
    "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
    "authors": [
        "Chujie Zheng",
        "Zhenru Zhang",
        "Beichen Zhang",
        "Runji Lin",
        "Keming Lu",
        "Bowen Yu",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models."
        },
        {
            "title": "Start",
            "content": "PROCESSBENCH: Identifying Process Errors in Mathematical Reasoning 2024-12-"
        },
        {
            "title": "Chujie Zheng Zhenru Zheng Beichen Zhang Runji Lin Keming Lu",
            "content": "Bowen Yu Dayiheng Liu"
        },
        {
            "title": "Jingren Zhou",
            "content": "Junyang Lin Qwen Team, Alibaba Inc. https://github.com/QwenLM/ProcessBench"
        },
        {
            "title": "Abstract",
            "content": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce PROCESSBENCH for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competitionand Olympiad-level math problems. Each test case contains step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on PROCESSBENCH, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope PROCESSBENCH can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models. 4 2 0 2 9 ] . [ 1 9 5 5 6 0 . 2 1 4 2 : r Figure 1: Overview of evaluation results on PROCESSBENCH (see Table 3 for details). Corresponding authors."
        },
        {
            "title": "Introduction",
            "content": "In recent years, language models have made remarkable progress in complex reasoning tasks, such as mathematics and programming (Hurst et al., 2024; OpenAI, 2024; Yang et al., 2024a;b; Dubey et al., 2024; Wake et al., 2024), yet they still make mistakes when solving challenging problems. To achieve scalable oversight (Amodei et al., 2016; Bowman et al., 2022; Cao et al., 2024), i.e., effectively supervising AI systems that get close to or go beyond broadly human-level performance, particularly in complex tasks that are difficult for general humans, we expect language models can identify errors in their reasoning process in an automated way. However, existing benchmarks related to assessing language models reasoning process may be hard to satisfy the growing evaluation demand for the error identification ability. Either their covered problems have become less challenging for recent language models (Zhou et al., 2024; Lightman et al., 2023), or they merely label the correctness of final answers but lack annotations for specific erroneous steps (Lin et al., 2024). Figure 2: Data example of PROCESSBENCH. The label 2 denotes that the earliest error occurs in the 2nd step (indexed from 0). For test cases with no errors, the labels are 1. In this paper, we introduce PROCESSBENCH for measuring the ability to identify erroneous steps in mathematical reasoning. Figure 2 presents data example. We prioritize several principles when designing this benchmark: Problem difficulty and solution diversity. PROCESSBENCH primarily covers competitionand Olympiad-level math problems and utilizes various open-source language models to generate solutions. This ensures both the difficulty of math problems and the diversity of solution styles, enabling robust evaluation. Scale and accuracy. PROCESSBENCH consists of 3,400 test cases, with all solutions annotated with error locations by multiple human experts. The large scale and expert annotation ensure the data quality and the reliability of evaluation. Simplicity. PROCESSBENCH requires models to identify the earliest erroneous step occuring in the solution, if any exists. This straightforward evaluation protocol enables easy adaptation for various types of models, such as process reward models (PRMs) and critic models. We conduct extensive evaluation on PROCESSBENCH, involving two types of models: process reward models (PRMs) and critic models. For PRMs, we include multiple open-source PRMs (Wang et al., 2024; Skywork, 2024; Xiong et al., 2024b) to assess the correctness of each reasoning step in the solution. For critic models, we prompt general language models like Qwen (Yang et al., 2024a; Qwen, 2024a; Hui et al., 2024) and GPT-4o (Hurst et al., 2024) to critique each solution step by step. We show that, despite recent growing interest, existing PRMs typically fail to generalize to more challenging math problems 2 beyond GSM8K and MATH. They underperform both critic models and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset, which raises questions about the generalization abilities and scalability of the current data synthesis methodologies used to build PRMs. In contrast, general language models manifest non-trivial critique capabilities that can not only identify erroneous steps but also provide detailed explanations. The best open-source model, QwQ-32B-Preview (Qwen, 2024b), has performed competitively with the proprietary GPT-4o model, while it still lags behind the reasoning-specialized o1-mini (OpenAI, 2024). We hope PROCESSBENCH can catalyze future research in automated reasoning process assessment, establishing crucial foundations for scalable oversight of language models."
        },
        {
            "title": "2 Related Work",
            "content": "There exist several benchmarks or datasets related to assessing language models reasoning process. CriticBench (Lin et al., 2024) evaluates language models abilities to critique solutions and correct mistakes in various reasoning tasks. MathCheck (Zhou et al., 2024) synthesizes solutions containing erroneous steps using the GSM8K dataset (Cobbe et al., 2021), in which language models are tasked with judging the correctness of final answers or reasoning steps. PRM800K (Lightman et al., 2023) builds on the MATH problems (Hendrycks et al., 2021) and annotates the correctness and soundness of reasoning steps in model-generated solutions. It also has sparked blooming of research interest in building process reward models (PRMs) (Wang et al., 2024; Xiong et al., 2024b;a). Table 1: Comparison between PROCESSBENCH and other benchmarks or datasets related to reasoning process assessment (Lin et al., 2024; Zhou et al., 2024; Lightman et al., 2023). : Solution diversity denotes the diversity of language models used for solution generation, corresponding to the # Solution Generators column. : For PRM800K, we only count the 90 complete solutions in its phase 1 test set, as the complete solutions in its phase 2 test set are all terminated at the earliest erroneous steps. Problem Diffculty # Solution Generators Solution Diversity Step Annotation? Annotator Test Case Size (Identifying Process Errors) CriticBench MathCheck-GSM PRM800K PROCESSBENCH 8 1 1 12 - Synthetic Human Human - 516 90 3,400 PROCESSBENCH is distinguished from prior benchmarks or datasets in three key aspects, as highlighted in Table 1. First, PROCESSBENCH primarily covers more challenging math problems with competitionor Olympiad-level difficulty, which better fit the rapidly growing capabilities of modern language models. Second, rather than relying on synthetic data, PROCESSBENCH leverages diverse model-generated natural solutions and employs expert annotation to label erroneous steps, which ensures both real-world applicability and label accuracy. Third, the large scale of PROCESSBENCH (3,400 test cases in total) enables more comprehensive and robust evaluation. There has also been extensive research on language models scalable oversight (Amodei et al., 2016; Bowman et al., 2022; Cao et al., 2024) and studies on whether language models can identify the errors in their own outputs. For instance, Huang et al. (2023); Kamoi et al. (2024) argue that language models struggle to identify and correct their reasoning errors without external feedback. Saunders et al. (2022); McAleese et al. (2024) show that language models can be trained to write informative critiques for both assisting human evaluation and enabling self-refinement, which favorably scales with increased model capabilities (or model sizes). We believe the improved capabilities of error identification will build strong foundations for language models scalable oversight."
        },
        {
            "title": "3.1 Task Definition",
            "content": "As shown in Figure 2, given math problem and step-by-step solution, PROCESSBENCH requires models to either identify the earliest-occurring error, or conclude that all steps are correct. Formally, given math problem and its step-by-step solution = {s0, ..., sn1}, the task is to output an index {1, 0, ..., 1}, where = 1 indicates that all steps are correct, and 0 indicates that the earliest error occurs at step si. 3 Typically but non-inclusively, we consider step as erroneous if it contains any of the following: (1) Mathematical errors: incorrect calculations, algebraic manipulations, or formula applications. (2) Logical errors: invalid deductions, unwarranted assumptions, or flawed reasoning steps. (3) Conceptual errors: misunderstanding or misapplication of mathematical or problem concepts. (4) Completeness errors: missing crucial conditions, constraints, or necessary justifications that affect the solutions validity. Beyond these types of errors, we encourage human annotators to determine the correctness of reasoning steps based on their own expertise. We do not require human annotators to explicitly annotate error types due to the intractability of intentional categorization. Note that for steps after the first error, the meaning of their correctness may become ambiguous or debatable: derivations based on incorrect premises can make sense, but still remain on globally incorrect reasoning path (Lightman et al., 2023). For instance, if step contains an error in calculating = 2, when it should be = 3, subsequent steps may follow valid algebraic rules but operate on an incorrect value of x, making their individual correctness hard to determine. This is why PROCESSBENCH focuses on identifying the earliest-occurring error in the reasoning process."
        },
        {
            "title": "3.2 Benchmark Construction",
            "content": "Problem Curation We collect math problems from the test sets of four public and widely used datasets in mathematical reasoning tasks: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), OlympiadBench (He et al., 2024), and Omni-MATH (Gao et al., 2024). Except for GSM8K, which consists of grade school math problems, the other three datasets all contain problems with competitionor Olympiad-level difficulty. Solution Generation We generate solutions using the widely used Qwen (Yang et al., 2024a; Qwen, 2024a; Yang et al., 2024b) and LLaMA (Dubey et al., 2024) series open-source models, resulting in twelve distinct solution generators in total. This includes wide range of model families, sizes, and downstream task performance, leading to the high diversity of solution styles. Table 4 in Appendix presents the breakdown of language models used for PROCESSBENCHs solution generation. Solution Reformatting In mathematical reasoning tasks, double line breaks (i.e., nn) are commonly used to segment solution steps (or paragraphs). However, we observed inconsistent step granularity due to varying solution styles and generation randomness. Some generated solutions frequently used double line breaks, resulting in numerous short, logically incomplete steps, while others used them sparingly, leading to lengthy paragraphs that combine multiple logical components. Such inconsistency in step granularity (and potential improper step segmentation) would impede the standardization of human annotation criteria. To address this issue, we adopt solution reformatting method to standardize the step granularity, through which the segmented paragraphs can better correspond to logically complete and progressive reasoning steps. Specifically, we first replace all the line breaks with white space, and then ask Qwen2.5-72BInstruct to insert double line breaks (i.e., segment paragraphs) while preserving the solution content. Since we found that Qwen2.5-72B-Instruct sometimes alters the solution content (< 0.5%), we remove those solutions whose final answers change after reformatting (although the content alteration may not influence benchmark construction). Consequently, the reformatting method effectively unifies the step granularity. Figure 6 in Appendix presents an example of solution reformatting. Expert Annotation To ensure balance between erroneous and correct solutions, we first use Qwen2.572B-Instruct to verify the correctness of final answers in the model-generated solutions against the reference answers. We then respectively sample solutions with correct or incorrect final answers for subsequent annotation in balanced way to avoid excessive concentration on solutions from either the weakest or strongest models. We recruit human experts with doctoral-level mathematical expertise for annotation, and all of them are required to pass the mandatory proficiency examination and annotation tutorial. The annotators are designated with the same task in 3.1, i.e., identifying the earliest-occurring error in each solution. However, we notice that the competitionor Olympiad-level math problems can still be challenging even for doctoral students majoring in mathematics. According to the feedback from the annotators, although they were not required to solve problems from scratch but rather to identify erroneous steps in presented solutions, they would still become quite hesitant in their annotations when uncertain about the correct solution approach, which affected both the annotation speed and quality. To ease the annotation difficulty, we provide annotators with the reference solutions and answers from the original datasets, while we still explicitly instructed them to inspect and verify the presented model-generated solutions step by step. Each solution is initially assigned to three different experts. When the initial three annotators cannot 4 reach consensus, we increase the number of annotators until three of them agree on the same result. If an agreement cannot be achieved within five annotators (e.g., annotation distribution of (2, 2, 1) or (2, 1, 1, 1)), we discard this solution. This leads to an overall 30% discard rate throughout the entire annotation process. We also discard the solutions where the final answers are incorrect (according to the reference answers) but the human annotation results are correct. Although such cases are fairly rare (< 1%), they are mostly concentrated in the OlympiadBench and Omni-MATH problems (i.e., Olympiad-level ones). The agreement statistics in Table 2 further evidence that the more challenging problems usually need more annotators to achieve the annotation agreement, particularly for those samples with incorrect final answers. These results suggest the inherent challenge of our human annotation task."
        },
        {
            "title": "3.3 Statistics",
            "content": "Table 2: Statistics of PROCESSBENCH. % Process errors denotes the proportion of samples with erroneous reasoning steps (i.e., annotated as erroneous) among all the samples with correct final answers. % steps denotes the proportion of samples whose solutions have steps (split by double line breaks). % 3/n agreement denotes the proportion of samples where the three-annotator agreement is achieved within annotators, so (% 3/3) + (% 3/4) + (% 3/5) = 100%. GSM8K MATH OlympiadBench Omni-MATH error correct error correct error correct error correct # Samples 207 193 594 661 339 759 241 % Process errors (correct final answers) # Steps % 5 steps % 10 steps % 15 steps % 3/3 agreement % 3/4 agreement % 3/5 agreement 200193 200 = 3.5% 500406 500 = 18.8% 500339 500 = 32.2% 500241 500 = 51.8% 5.1 5.3 6.8 61.8% 57.5% 73.6% 70.4% 92.6% 33.9% 17.8% 3.4% 9.1% 3.4% 0.5% 8.9% 2.0% 1.6% 0.0% 8. 6.0 66.7% 95.9% 59.4% 91.9% 52.8% 24.1% 24.4% 21.3% 23.1% 16.2% 12.1% 3.6% 0.5% 4.7% 3.4% 8.7 92.3% 27.1% 8.8% 85.0% 9.1% 5.9% 8.6 7.4 92.5% 81.7% 29.2% 21.6% 4.1% 7.5% 47.8% 80.1% 25.6% 13.7% 6.2% 26.6% The resulting PROCESSBENCH has four subsets, consisting of 3,400 test cases in total. The detailed statistics are shown in Table 2 and Table 4 (in Appendix B), and we also plot in Figure 3 the distribution of error positions in erroneous samples. In general, the more challenging the problems, the more solution steps the models generate, and incorrect solutions usually contain more steps than correct ones. However, across all four subsets, large proportion of errors occur in the earlier steps, such as steps 0-3 in GSM8K and MATH, and steps 1-5 in OlympiadBench and Omni-MATH. Figure 3: Distribution of error positions (indexed from 0; truncated to 16 for better visualization), corresponding to the label field as shown in Figure 2. It is noteworthy that while we have intentionally controlled an equal number of solutions with incorrect and correct final answers (200 each for GSM8K and 500 each for other subsets), the annotation results reveal quite different numbers. Specifically, in the more challenging subsets like OlympiadBench and Omni-MATH, larger proportion of solutions with correct final answers still contain erroneous steps. For instance, in OlympiadBench, 500339 500 = 32.2% of solutions with correct final answers are found 5 Figure 4: Process error ratios per models and subsets, computed as the proportions of samples annotated as erroneous among all the samples with correct final answers (same as in Table 2). The models used for solution generation slightly vary across different subsets, see Table 4 in Appendix B. We observe that no particular models have notably higher process error rates, while the process error rates are consistently higher on more difficult problems for all the models. to contain process errors, while in Omni-MATH this proportion is even higher ( 500241 500 = 51.8%). In contrast, these proportions in GSM8K and MATH are 200193 500 = 18.8%, respectively. In Figure 4, for each model used for solution generation, we plot the ratio of samples with erroneous reasoning steps (i.e., annotated as erroneous) among all the samples with correct final answers. We observe that the process error rates are consistently higher on more difficult problems. To our knowledge, our work is the first to present evidence that on more challenging math problems, current language models are more prone to making process errors even when reaching correct final answers. This also suggests the underlying limitation of rule-based RL in mathematical reasoning (i.e., rewarding merely according to the correctness of final answers) and further highlights the significance of identifying errors in the reasoning process. 200 = 3.5% and"
        },
        {
            "title": "4.1 Setup",
            "content": "For each subset of PROCESSBENCH, we calculate the accuracies on erroneous and correct samples, respectively, and additionally compute their harmonic mean as the F1 score. We primarily refer to F1 scores to compare model performance, as it balances model behaviors between being overly critical and being incapable of identifying errors. We consider two types of models in the evaluation on PROCESSBENCH: process reward models (PRMs) and critic models. Process Reward Models (PRMs) As recently focal topic, PRMs are proposed to assess and supervise the intermediate steps in language models reasoning process (Lightman et al., 2023), thus naturally falling in the scope of our research. In practice, PRMs are typically trained using the process labels for intermediate reasoning steps, outputting either the correctness prediction or scalar score for each reasoning step during inference. Previous research usually evaluates PRMs based on their improvement in the Best-of-N (BoN) performance of another language model that generates solutions. However, this lacks finer-grained inspection on their process assessment abilities, and the evaluation reliability can be heavily affected by the underlying solution generation model. Our evaluation includes several open-source PRMs: (1) Math-Shepherd (Wang et al., 2024), which obtains the process label for each step via estimating the empirical probability of this step leading to the correct final answer. (2) Two LLaMA-3.1-based PRMs from Xiong et al. (2024b), which roughly follow the training methodology of Math-Shepherd but differ in the solution generation models and optimization objectives. (3) Two Qwen2.5-Math-based PRMs recently released by Skywork (2024). (4) We also train PRM by fine-tuning Qwen2.5-Math-7B-Instruct on the PRM800K dataset, namely Qwen2.5-Math-7B-PRM800K. See Appendix for its training details. For the (1)(2)(4) PRMs, we extract the earliest erroneous step from their correctness predictions for reasoning steps. For the (3) PRMs, which produce scalar scores for each reasoning step, we first transform these scores into binary correctness predictions (using threshold above which steps are considered as correct), and then extract the earliest erroneous step as we do for (1)(2)(4). The transformation threshold is determined as the one giving the highest F1 score on the GSM8K subset. Critic Models Critic models aim to provide feedback and critique to model-generated texts, noninclusively including verification, reflection, and correction or refinement. They have demonstrated promising utility in achieving scalable oversight (Saunders et al., 2022; McAleese et al., 2024). Training critic models for specific domains typically requires significant and specialized effort, which is out of the scope of our work. Instead, we are more interested in the critique capabilities of general language models. The task definition ( 3.1) of PROCESSBENCH enables us to apply simple prompt engineering to repurpose general language models as critic models. We show in Figure 7 in Appendix the prompt template we implement for our evaluation. Specifically, models are prompted to return the index of the paragraph where the earliest error occurs as the final answer, similar to the conventional evaluation protocol for mathematical reasoning tasks (Cobbe et al., 2021; Hendrycks et al., 2021; Yang et al., 2024b). Our evaluation includes the widely-used Qwen2 (Yang et al., 2024a), Qwen2.5 (Qwen, 2024a), Qwen2.5Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), and LLaMA-3 (Dubey et al., 2024) series open-source models, as well as the recently released QwQ-32B-Preview reasoning model (Qwen, 2024b). We also evaluate the proprietary GPT-4o (Hurst et al., 2024) and o1-mini (OpenAI, 2024) models. We report the performance of open-source models under majority voting over eight samplings, while we also report their performance under greedy decoding in Table 9 in Appendix E. For the proprietary model GPT-4o, we report the results under greedy decoding, while for o1-mini, we report the results under single sampling as its API does not support customized decoding parameters. Table 3: Evaluation results on PROCESSBENCH. We report the F1 score of the respective accuracies on erroneous and correct samples. See Table 5 and Table 7 for breakdown of evaluation results. Model GSM8K MATH OlympiadBench OmniMATH Average Open-source Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B Qwen2.5-Math-7B-PRM800K (our trained) 47.9 50.4 38.8 59.0 70.8 68.2 29.5 33.4 33.8 48.0 53.6 62.6 24.8 13.8 16.9 19.3 22.9 50.7 Open-source language models, prompted as Critic Models Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 13.1 52.2 10.9 74.9 82.9 26.8 65.8 14.3 50.1 68.9 8.4 67.6 36.5 69.3 65.6 76.2 88. 13.8 22.8 5.1 48.2 59.4 25.7 52.1 6.5 39.9 60.1 19.0 49.2 36.6 53.3 53.1 61.8 78.7 4.8 21.2 2.8 46.7 46.7 14.2 32.5 4.1 34.0 48.9 14.7 42.1 29.7 45.0 40.0 54.6 57.8 Proprietary language models, prompted as Critic Models 79.2 93.2 63.6 88.9 51.4 87. GPT-4o-0806 o1-mini"
        },
        {
            "title": "4.2 Results",
            "content": "23.8 15.8 16.9 19.2 21.0 44.3 12.6 20.0 1.6 41.0 43.0 12.7 31.7 1.8 27.3 46.3 12.1 40.2 27.4 41.3 38.3 52.2 61.3 53.5 82.4 31.5 28.4 26.6 36.4 42.1 56.5 11.1 29.1 5.1 52.7 58.0 19.9 45.5 6.7 37.8 56.1 13.6 49.8 32.6 52.2 49.3 61.2 71.5 61.9 87. We present the evaluation results in Table 3. Our observations are summarized as follows: 7 Figure 5: Critique generated by QwQ-32B-Preview for the test case in Figure 2. Generalization Across Difficulty From GSM8K and MATH to OlympiadBench and Omni-MATH, with the increased difficulty of math problems, we observe consistent performance decline for all the models, which suggests the common challenge of both PRMs and critic models in generalization abilities. Comparison Between PRMs and Critic Models We find that existing PRMs typically underperform the top prompt-driven critic models even on the simpler GSM8K and MATH subsets, suggesting that these PRMs struggle to indicate the correctness of the intermediate steps in mathematical reasoning. Moreover, when moving toward the more challenging OlympiadBench and Omni-MATH subsets, PRMs suffer from more notable performance decline than critic models. This raises our concerns about the generalization abilities and scalability of the current data synthesis methodologies used to build PRMs. More specifically, current methodologies, as exemplified by Math-Shepherd (Wang et al., 2024), measure the correctness of an intermediate step by estimating the empirical probability of this step leading to the correct final answer. This kind of approach has two intuitive major issues: (1) The process labels heavily depend on the language model used to generate solutions (i.e., highly on-policy), which would naturally fail to indicate the correctness of reasoning steps generated by other models. (2) As demonstrated in 3.3, current language models are prone to making process errors even when reaching correct final answers. This could substantially invalidate the estimated process labels, particularly on the more challenging math problems. In contrast, Qwen2.5-Math-7B-PRM800K, which is straightforwardly fine-tuned on the fully human-annotated PRM800K training set, exhibits the significantly stronger performance and generalization ability than other PRMs. Comparison Among Critic Models Compared to PRMs, critic models can benefit from separate reasoning processes when critiquing solutions, as they can think more before indicating the correctness of each solution step, which leads to their better performance in this error identification task. Within the same model family, the error identification performance favorably scales with increased model sizes. Notably, the recently released reasoning model QwQ-32B-Preview performs best among the open-source models and is highly competitive with GPT-4o. It is noteworthy that QwQ-32B-Preview achieves more balanced accuracies on erroneous and correct samples (see Table 5 and 7 in Appendix E). We show in Figure 5 an example of critique generated by QwQ-32B-Preview to the test case in Figure 2, which not only identifies the erroneous step but also provides the detailed thinking process and explanation. Nevertheless, QwQ32B-Preview still lags behind o1-mini, suggesting that although the gap in problem-solving performance is getting closer between open-source and proprietary models, there still exists another large gap in their critique capabilities."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce the PROCESSBENCH benchmark for measuring the ability to identify erroneous steps in mathematical reasoning, characterized by its high problem difficulty and solution diversity, large scale, rigorous human annotation, and simple evaluation protocol. Through extensive evaluation with existing process reward models (PRMs) and prompt-driven critic models, we draw two main observations: (1) Existing PRMs typically underperform critic models in identifying erroneous reasoning steps, and struggle more to generalize to challenging math problems. (2) Open-source language models, as exemplified by QwQ-32B-Preview, have demonstrated critique capabilities competitive with the proprietary model GPT-4o, yet still lag behind the reasoning-specialized o1-mini model. We envision PROCESSBENCH as cornerstone testbed for advancing automated reasoning process assessment, critical step toward achieving scalable oversight of language models. Limitation Despite our best efforts throughout the entire benchmark construction process ( 3.2), PROCESSBENCH may still contain inaccurate labels of error locations, particularly for the more challenging Olympiad-level math problems."
        },
        {
            "title": "References",
            "content": "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil LukoË‡si ut e, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, et al. Towards scalable automated alignment of llms: survey. arXiv preprint arXiv:2406.01252, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 9 Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can LLMs actually correct their own mistakes? critical survey of self-correction of LLMs. Transactions of the Association for Computational Linguistics, 12:14171440, 2024. doi: 10.1162/tacl 00713. URL https://aclanthology. org/2024.tacl-1.78. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809, 2024. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. OpenAI. Openai o1-mini: Advancing cost-efficient reasoning, 2024. URL https://openai.com/index/ openai-o1-mini-advancing-cost-efficient-reasoning/. Team Qwen. Qwen2.5: party of foundation models, September 2024a. URL https://qwenlm.github. io/blog/qwen2.5/. Team Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024b. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. o1 Team Skywork. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Alan Wake, Albert Wang, Bei Chen, CX Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, et al. Yi-lightning technical report. arXiv preprint arXiv:2412.01253, 2024. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, August 2024. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024. acl-long.510. Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multi-turn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024a. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024b. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733, 2024."
        },
        {
            "title": "A Example of Solution Reformatting",
            "content": "Figure 6: Example of solution reformatting. The left is the original solution (generated by Qwen2-7BInstruct) and the right is the reformatted one. The problem, coming from the MATH test set, is The ellipse 25 + (y3)2 (x6)2 9 = 1 has two foci. Find the one with the larger x-coordinate. Enter your answer as an ordered pair, like (2, 1)."
        },
        {
            "title": "B Breakdown Statistics of PROCESSBENCH",
            "content": "Table 4: Breakdown statistics of PROCESSBENCH. : We encountered code bug when using Llama-3.170B-Instruct and Qwen2.5-72B-Instruct to generate solutions for the MATH problems, thus their counts are all zero in the MATH subset of PROCESSBENCH. : For the more challenging OlympiadBench and Omni-MATH problems, we exclude models with lower accuracies from subsequent annotation. Generator GSM8K MATH error correct error correct Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct 11 16 38 7 37 31 9 32 12 2 8 4 13 15 23 28 4 21 11 10 15 21 14 18 56 92 86 0 36 89 56 31 62 0 47 14 49 53 0 11 42 51 43 35 0 49 59 Total 207 193 594 400 1,000 11 OlympiadBench Omni-MATH correct error correct error 0 0 116 85 0 63 64 0 86 67 99 81 661 0 0 48 32 0 45 48 0 37 38 48 43 339 1,000 0 0 131 103 0 96 71 0 75 88 103 759 0 0 31 19 0 35 25 0 29 38 29 35 241 1,000 Training Details of Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-7B-PRM800K is obtained by fine-tuning Qwen2.5-Math-7B-Instruct on the PRM800K training set. We replace the original language modeling head with new reward modeling head that outputs binary classification logits. The classification loss is computed at the second line break positions in all the nn. We treat the original 1 and 0 labels in PRM800K as our positive labels, while -1 as negative ones. To eliminate test data contamination, we also remove the PRM800K training samples that have the same problems in PROCESSBENCH."
        },
        {
            "title": "D Prompt Template for Critic Model Evaluation",
            "content": "Figure 7: Prompt template for critic model evaluation. The blue texts indicate the input math problem and the solution (split into paragraphs). The red texts describe the required output content and format."
        },
        {
            "title": "E Supplementary Evaluation Results",
            "content": "Table 5: Breakdown of evaluation results on the GSM8K and MATH subsets of PROCESSBENCH. The open-source language models (middle block) are evaluated via majority voting over eight samplings. Model GSM8K MATH error correct F1 error correct F1 Open-source Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B Qwen2.5-Math-7B-PRM800K (our trained) 32.4 33.8 24.2 50.2 61.8 53.1 91.7 99.0 98.4 71.5 82.9 95.3 47.9 50.4 38.8 59.0 70.8 68.2 18.0 21.7 21.4 37.9 43.8 48.0 Open-source language models, prompted as Critic Models Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 42.5 35.7 44.4 64.3 72.5 15.5 49.8 7.7 33.8 54.1 40.6 57.0 40.6 54.6 49.3 62.8 81.6 7.8 96.9 6.2 89.6 96.9 100.0 96.9 100.0 96.4 94.8 4.7 82.9 33.2 94.8 97.9 96.9 95.3 13.1 52.2 10.9 74.9 82.9 26.8 65.8 14.3 50.1 68.9 8.4 67.6 36.5 69.3 65.6 76.2 88.0 28.6 13.0 41.9 35.4 43.3 14.8 36.0 3.4 25.4 44.9 30.5 37.7 30.8 38.4 36.7 46.3 78.1 Proprietary language models, prompted as Critic Models GPT-4o-0806 o1-mini 70.0 88.9 91.2 97.9 79.2 93.2 54.4 83.5 82.0 72.2 80.0 65.3 69.2 90.1 9.1 93.3 2.7 75.6 94.6 96.8 94.3 98.3 92.4 90.6 13.8 70.9 45.1 87.4 95.8 93.1 79. 76.6 95.1 29.5 33.4 33.8 48.0 53.6 62.6 13.8 22.8 5.1 48.2 59.4 25.7 52.1 6.5 39.9 60.1 19.0 49.2 36.6 53.3 53.1 61.8 78.7 63.6 88.9 Table 6: For the two PRMs from Skywork (2024), we additionally adjust the threshold ( 4.1) as the one leading to the highest F1 score on each subset (i.e., each subset adopts respective optimal threshold), which can be viewed as the two PRMs upper bound performance on PROCESSBENCH. This table presents the results on the GSM8K and MATH subsets, which are marginally higher than those in Table 5 that all adopt the threshold selected on the GSM8K subset. Model GSM8K MATH error correct F1 error correct F1 Skywork-PRM-1.5B (respective thresholds) Skywork-PRM-7B (respective thresholds) 50.2 61.8 71.5 82.9 59.0 70. 38.2 44.1 70.4 70.9 49.5 54.4 13 Table 7: Breakdown of evaluation results on the OlympiadBench and Omni-MATH subsets of PROCESSBENCH. The open-source language models (middle block) are evaluated via majority voting over eight samplings. Model OlympiadBench Omni-MATH error correct F1 error correct F1 Open-source Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B Qwen2.5-Math-7B-PRM800K (our trained) 15.0 8.2 10.1 15.4 17.9 35.7 71.1 43.1 51.0 26.0 31.9 87. 24.8 13.8 16.9 19.3 22.9 50.7 14.2 9.6 10.1 13.6 14.0 29.8 Open-source language models, prompted as Critic Models Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 27.1 12.0 32.4 35.1 31.0 7.7 19.5 2.1 20.7 33.4 22.4 34.0 26.5 31.5 25.3 38.7 61.4 2.7 92.0 1.5 69.9 94.1 91.7 97.3 99.1 94.1 91.2 10.9 55.2 33.9 78.8 95.9 92.6 54. 4.8 21.2 2.8 46.7 46.7 14.2 32.5 4.1 34.0 48.9 14.7 42.1 29.7 45.0 40.0 54.6 57.8 26.1 11.2 32.0 30.7 28.2 6.9 19.0 0.9 15.9 31.5 20.0 32.3 26.2 28.3 24.1 36.6 55.7 Proprietary language models, prompted as Critic Models GPT-4o-0806 o1-mini 45.8 80.2 58.4 95. 51.4 87.2 45.2 74.8 73.0 45.2 51.9 32.8 41.9 86.3 8.3 91.7 0.8 61.8 90.5 88.0 96.3 98.3 94.2 87.6 8.7 53.1 28.6 76.3 92.5 90.9 68.0 65.6 91.7 23.8 15.8 16.9 19.2 21.0 44. 12.6 20.0 1.6 41.0 43.0 12.7 31.7 1.8 27.3 46.3 12.1 40.2 27.4 41.3 38.3 52.2 61.3 53.5 82.4 Table 8: For the two PRMs from Skywork (2024), we additionally adjust the threshold ( 4.1) as the one leading to the highest F1 score on each subset (i.e., each subset adopts respective optimal threshold), which can be viewed as the two PRMs upper bound performance on PROCESSBENCH. This table presents the results on the OlympiadBench and Omni-MATH subsets, which are slightly higher than those in Table 7 that all adopt the threshold selected on the GSM8K subset. Model OlympiadBench Omni-MATH error correct F1 error correct Skywork-PRM-1.5B (respective thresholds) Skywork-PRM-7B (respective thresholds) 15.3 18.9 47.5 48.1 23.1 27.1 14.0 14.4 58.5 58. 22.6 23.1 14 Table 9: Breakdown of evaluation results of the open-source language models (prompted as critic models) using greedy decoding. Model GSM8K MATH error correct F1 error correct Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 28.5 39.6 36.7 57.5 66.2 14.5 45.9 0.0 20.3 50.7 28.0 56.5 36.7 47.8 43.0 61.4 74.9 9.3 93.8 17.1 77.7 96.9 99.0 96.4 20.2 99.0 93.8 0.0 82.4 66.3 93.8 97.9 98.4 67.4 14.1 55.7 23.3 66.1 78.6 25.3 62.2 0.0 33.7 65.8 0.0 67.0 47.3 63.3 59.8 75.6 70.9 20.9 21.9 23.6 37.7 38.4 13.1 34.3 0.2 15.2 39.7 19.0 35.5 23.7 40.4 33.3 45.3 58.6 5.7 72.2 7.9 53.9 93.1 94.8 94.6 25.6 96.1 88.2 5.2 66.7 63.8 86.9 95.6 91.9 54. 8.9 33.6 11.8 44.4 54.4 23.1 50.4 0.3 26.2 54.8 8.1 46.4 34.6 55.2 49.4 60.7 56.3 Model OlympiadBench Omni-MATH error correct F1 error correct F1 Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 17.2 20.9 19.1 32.8 30.9 6.4 17.2 0.0 9.1 31.8 14.1 33.4 25.4 30.9 22.4 33.7 37. 0.6 41.6 5.6 32.4 90.0 79.1 95.0 13.3 95.6 86.7 2.9 48.1 46.0 76.4 90.0 88.5 31.9 1.1 27.8 8.7 32.6 46.0 11.8 29.2 0.0 16.6 46.5 4.9 39.4 32.7 44.0 35.9 48.9 34.6 17.3 20.9 17.1 29.5 27.1 4.7 18.3 0.0 6.2 31.5 13.7 30.4 26.1 27.0 22.4 33.7 29.5 4.1 50.2 10.0 39.0 86.3 78.0 93.4 27.8 95.9 84.6 2.9 48.1 43.6 72.6 87.6 88.4 41.9 6.7 29.6 12.6 33.6 41.3 8.9 30.6 0.0 11.6 45.9 4.8 37.3 32.6 39.4 35.7 48.8 34."
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Inc."
    ]
}