{
    "paper_title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
    "authors": [
        "Li Puyin",
        "Tiange Xiang",
        "Ella Mao",
        "Shirley Wei",
        "Xinye Chen",
        "Adnan Masood",
        "Li Fei-fei",
        "Ehsan Adeli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding."
        },
        {
            "title": "Start",
            "content": "QUANTIPHY: Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models Li Puyin1,, Tiange Xiang1,, Ella Mao1,, Shirley Wei1, Xinye Chen1, Adnan Masood2 Li Fei-Fei1,, Ehsan Adeli1, 1Stanford University, 2UST *Equal First Authorship Equal Last Authorship Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-ofthe-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure VLMs physical reasoning ability. Comprising more than 3.3K videotext instances with numerical ground truth, QuantiPhy evaluates VLMs performance on estimating an objects size, velocity, and acceleration at given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward numerically grounded physical understanding. Dataset: https://huggingface.co/datasets/PaulineLi/QuantiPhy-validation Project: https://quantiphy.stanford.edu/ Code: https://github.com/Paulineli/QuantiPhy Correspondence to: {puyinli, xtiange, eadeli}@stanford.edu 5 2 0 2 2 ] . [ 1 6 2 5 9 1 . 2 1 5 2 : r Figure 1. On crowded city street, birds nest falls from branch, car rushes by, an eagle flits over building, and person walks in crosswalk the real world is full of complex physical motion. To enable AI to understand and navigate this environment, it is essential for generalist embodied systems to reason about physical properties quantitatively. Because objects obey common laws of physics, their kinematic properties (such as size, velocity, and acceleration) are interrelated. This interdependence makes it possible for visual AI to systematically reason about these properties with respect to available priors. In this work, we present QUANTIPHY, the first benchmark to evaluate the reasoning ability of AI models on quantitative kinematic inference tasks. 1 1. Introduction Understanding the physical world has long been challenge for artificial intelligence. Humans inhabit world governed by physical laws. From an apple falling from tree to the trajectory of thrown ball, we have developed mathematical tools to measure and calculate physical attributes quantitatively. This quantitative understanding forms the foundation for all modern scientific advances. General visual intelligence systems, such as VisionLanguage Models (VLMs), are developed in manner that differs significantly from that of humans. They are trained to fit vast amounts of real-world data, which implicitly contain the abstract physical principles behind visual observations. Thus, assessing the numerical accuracy of VLMs reasoning about physical properties is necessary next step. It is crucial for deploying applications such as embodied AI [12, 42], AR/VR [18, 26, 4850], and autonomous driving [40]. Reasoning about quantitative physical properties from visual data can be challenging. For example, advanced methods like FoundationPose [44] require extensive prior information, including color, depth, object meshes, and camera parameters, to accurately localize an object in 3D space. Unfortunately, most of these priors are unavailable in inthe-wild captures. These challenges make it natural to ask whether large VLMs can leverage their rich implicit priors to reason end-to-end about precise kinematic and geometric properties. Moreover, robust evaluation of VLM reasoning has the potential to help assess and improve the physical realism of videos created by generative models. Studies of VLM physical understanding are not new. variety of related benchmarks exist, spanning kinematics [58] and dynamics [52] to relationships [11] and scene understanding [54]. However, almost all existing benchmarks are Visual Question Answering (VQA)-based and qualitative. In this paradigm, ground-truth answers are effectively constrained by the prompt, and models are typically evaluated with multiple-choice questions. However, this VQA paradigm does not provide fine-grained evaluations of physical understanding. For example, if model is asked to infer the size of car from video (ground truth: 3 meters), the incorrect answers 3.1 meters and 31 meters would be treated as equally wrong in multiple-choice format. Quantitatively, however, the 31-meter answer is 10 worse. To truly push VLMs toward real-world applications, it is crucial to capture this numerical gap. Our contributions in this work are four-fold: (I) We propose new quantitative paradigm for evaluating the physical reasoning ability of VLMs, moving beyond the limitations of qualitative VQA. (II) We define kinematic inference task for VLM physical reasoning that explicitly targets the challenge of understanding dynamics in videos: since kinematic properties of an object, such as size, velocity, and acceleration, are mutually correlated quantities, our task formalizes how visual agents could, in principle, transform single physical prior into family of numerically grounded predictions in real-world units that are useful for understanding and acting in the physical world. (III) We present QUANTIPHY, the first benchmark to systematically evaluate VLMs quantitative reasoning on object kinematic properties in videos, spanning 2D/3D motion, static and dynamic priors, and diverse scene conditions, together with standardized metric, prompting protocol, and leaderboard over 21 state-of-the-art models. (IV) We provide detailed analysis of factors affecting VLM reasoning, including scene complexity, video availability, counterfactual priors, and chain-of-thought prompting. Summarizing the analysis, we conclude with one interesting finding: when estimating kinematic quantities, existing VLMs hallucinate by relying heavily on pre-trained world knowledge while hardly inferring from the actual reference video and text. 2. Related Work 2.1. Benchmarks for VLM Physical Understanding For over decade, researchers have recognized the need for AI to comprehend real-world physics. series of benchmarks and evaluation protocols has been developed to assess the qualitative physical reasoning abilities of VLMs. Early efforts primarily focused on evaluating models capacity to describe, predict, and explain basic physical events, such as collisions, falling, and rebounding in controlled synthetic environments [6, 58]. Subsequent works expanded this scope to include reasoning about inherent physical properties of objects, such as mass, friction, elasticity, and deformability [41]. More recently, comprehensive benchmarks like PhysBench [11] and STAR [45] have been proposed to assess broader aspects of physical reasoning. These include object relationships, physical scene understanding, complex physics-based dynamics, and physics-situated actions in more realistic or diverse environments [19, 60], aiming to simulate the real-world reasoning demands that embodied AI systems will face. However, despite these advances, the vast majority of existing benchmarks adopt the VQA framework [3, 39, 51, 57]. In this framework, models are asked to provide multiplechoice selections or descriptive explanations of physical events. One relevant work, VSI-Bench [54], provides preliminary studies on basic spatial understanding using numerical metrics, but is limited to static objects, focusing more on the models perceptual ability than on emergent reasoning. Following that, Super-VSI [55] was recently proposed, demonstrating that VLMs empowered with the ability for numerical spatial understanding have strong potential in empowering embodied AI. Therefore, need still exists for Figure 2. Sample examples from QUANTIPHY, illustrating the four core task combinations defined by Dimensionality {2D, 3D} and Physical Prior {Static, Dynamic}, as described in section 3.1. Our collected data is diverse in nature, and each video is paired with multiple (prior, question, ground truth) triplets. Please see the supplementary materials for more data examples. benchmarking VLMs ability to perform quantitative reasoning over the geometric and kinematic properties of moving objects (e.g., estimating size, velocity, or acceleration in metric terms). 2.2. Physical Reasoning Models Although AI models have achieved impressive results on general tasks, their ability to reason about the physical world remains limited. For VLMs, state-of-the-art models such as ChatGPT-4o and Gemini-1.5 Pro achieve only around 60% accuracy on the PhysBench benchmark, far below humanlevel performance ( 95%) [11], indicating persistent challenges in physical reasoning. Generative models also struggle. Recent work shows that generated videos often violate basic physical laws [7], as these models are more sensitive to low-level visual cues (e.g., color, shape) than to underlying physical properties [21]. In embodied AI, recent systems have demonstrated basic physical reasoning abilities. They can interact with the environment [4], but their reasoning remains largely qualitative, lacking precise and quantitative understanding of physical dynamics. These gaps underscore the need for benchmarks and models that transcend qualitative judgment and move toward accurate quantitative reasoning in physical contexts. 2.3. Physical Understanding and Reasoning In computer vision, techniques such as optical flow [8, 20], combined with object detection and tracking models like YOLO [33] and ByteTrack [59], enable accurate analysis of object motion and are widely used in broad applications [15, 36]. These methods demonstrate that quantitative information (e.g., velocity, displacement) can be reliably inferred from video inputs. Meanwhile, research in cognitive science shows that humans possess strong abilities to reason about scale, from microscopic to astronomical, by leveraging relational cues and prior knowledge [35]. Even in visually unfamiliar or simulated environments, people can extract physical rules and perform causal and predictive reasoning about object motion and interactions [1, 14, 23]. This highlights the potential for robust physical reasoning ability to emerge in large AI models from visual inputs, crucial step toward grounded quantitative understanding. 3. Methods We introduce QUANTIPHY, the first benchmark designed to quantitatively evaluate VLMs physical reasoning ability on moving objects. In subsection 3.1, we first define the basic object kinematic properties to be assessed in QUANTIPHY, and present key statistics of it. Then, subsection 3.2 provides 3 details on the construction of QUANTIPHY, focusing on data collection. 3.1. Overview of QUANTIPHY The task of kinematic inference. The purpose of QUANTIPHY is to evaluate whether VLMs can utilize prior knowledge to reason about objects kinematic properties with numerical accuracy, which forms the foundation for more sophisticated understanding of physics. Specifically, we focus on the translational movements of various objects.1 Sworld, Vworld { Given video, we provide the VLM with single physical prior for source object (from the set , in real-world units) as textual input. The model is then prompted to quantitatively determine requested kinematic properties for target object (which may be the same as or different from the source object) in world space. , Aworld } We distinguish pixel space, where quantities are measured in pixels ([pixel], [pixel/s], [pixel/s2]), from world space, where they are expressed in physical units (e.g., [m], [m/s], [m/s2]). Consider video capturing the translational movement of an object with fixed camera. At any time t, the objects location in pixel space, Xpixel , can be obtained from the frames. From the discrete trajectory, we compute pixel-space velocity and acceleration via finite differences: t"
        },
        {
            "title": "Vpixel\nt",
            "content": "Xpixel t+dtXpixel dt ; Apixel Xpixel t+2dt2Xpixel t+dt+Xpixel dt2 . , Aworld = γ Apixel The video thus defines the kinematics only in pixel units. To relate them to world space, we assume an unknown scalar scale factor γ > 0 (with units [world length/pixel]) such that, along the motion direction, Sworld = γSpixel, Vworld = γVpixel , where Spixel is an object size measured in pixels and Sworld the same size in physical unit. When single prior in world space is provided (object size Sworld, velocity Vworld at some time t), together with the corresponding pixel-space quantity from the video, γ is determined, and any other kinematic property in world space follows by rescaling its pixel-space counterpart. , or acceleration Aworld t See Figure 2 for examples of the kinematic tasks included in the benchmark. The performance of VLMs is measured by the numerical error between the VLMs prediction and the annotated ground truth. Benchmark setup. For comprehensive evaluation of the kinematic movements above, QUANTIPHY is designed to include video-question pairs categorized along three primary 1We do not include rotational movements in this work; we defer this to future work and provide discussion in section 6. Figure 3. QUANTIPHY Statistics. The collected data and curated QA pairs are among four main setups with further breakdowns. axes. We first describe the two axes that define the core reasoning task: Dimensionality: . 2D movement assumes 2D, 3D } the object moves strictly in the x-y plane with no change in depth relative to the camera. 3D movement includes the z-axis, resulting in varying depth, which is intrinsically more challenging2. { Static, Dynamic Physical prior: . The } { Static prior indicates the provision of the object size Sworld, which is constant throughout the video. While Dynamic prior indicates velocity Vworld or acceleration Aworld at given timestamp t. Together, these two axes divide the Benchmark into four task categories: 2D-Static, 2D-Dynamic, 3D-Static, and 3D-Dynamic. Data statistics. Similar to [11, 54], data in QUANTIPHY is organized into triplets of (question/prompt, video, numerical ground truth). Each video may be paired with multiple questions and corresponding ground truth annotations. To ensure diversity, the four data setups were collected from various sources, yielding total of 569 unique videos and 3355 questions. With proper post-processing, our collected videos typically have duration of 2-3 seconds, occupying approximately 115MB of disk storage, making the benchmark suitable for use across various hardware settings. See Figure 3 for detailed breakdown of the data statistics. 2We provide additional depth prior on 3D scenes. 3In the graphs and tables, we denote the four kinematic inference task categories as 2S (2D-Static), 2D (2D-Dynamic), 3S (3D-Static), and 3D (3D-Dynamic) for short. Figure 4. The construction of QUANTIPHY proceeds in three sequential stages. First, we collect diverse raw videos from three different sources. Additionally, we segment these videos with solid plain background (described in subsection 3.2). Second, we obtain high-quality annotations, employing distinct labeling methods tailored to each data source to accurately capture the objects physical properties. Finally, we formulate the benchmark tasks by associating each video with multiple (prior, question, ground truth) triplets. Each triplet is then categorized as either 2D or 3D, depending on the objects movement relative to the camera. 3.2. Benchmark Construction In QUANTIPHY, we constructed large-scale dataset containing diverse video data from multiple sources. Data collection. Our data combines simulated and realworld sources for controllability and practical applicability. The data collection process is visualized in Figure 4. Blender simulation: Blender allows us to render scenes that are both visually realistic and physically plausible, encompassing 2D and 3D motion. Simulating object movements in Blender provides complete control over the environment and guarantees precise ground-truth annotations. This control allows us to probe VLMs reasoning capabilities by placing objects with different motion types within single scene. It also enables us to study model robustness by holding motions fixed while systematically varying the background and visual noise. To enhance data diversity, we include simulations of scenes that are difficult or impossible to record in the real world, ranging from microscopic scales (e.g., red blood cells in vessels) to astronomical scales (e.g., galaxy movements). Lab capturing: Reconstructing objects in 4D (3D space + time) lets us annotate real-world object motion using multi-view stereo setup. Our captures feature wide diversity of movements, including free fall, sliding down slopes, pendulum motion, and bouncing. We also varied the physical properties of the objects, including those that are rollable and deformable. Further details on our setup can be found in the supplementary materials. Internet scraping: To extend our dataset to more out-of-distribution scenarios, we leverage the internet as source for in-the-wild data. However, not all internet data is suitable for our benchmark, as our benchmark specifically requires consistent coverage of moving objects captured by relatively static camera, along with reference objects of known dimensions (e.g., standard coin) to serve as priors. Hence, we narrowed our scraping scope to websites hosting high-quality monocular videos and then manually inspected them to select those meeting our stringent requirements. We provide detailed description of the data collection process in the supplementary materials Appendix B. Additional segmented data. We apply SAM [32] to segment moving objects against solid backgrounds, doubling our dataset without additional annotation while enabling controlled analysis of background complexity effects. Data annotation and question design. Blender simulations provide direct extraction of sizes, velocities, and accelerations via automation scripts. We construct Blender scenes using open-source 3D assets from Sketchfab [37] and BlenderKit. Automation scripts were developed for each scene to extract accurate, time-specific physical properties for any given object. However, data annotation is non-trivial for the other two sources. For lab captures, we utilize metric depth directly from depth cameras combined with multi-view stereo to achieve full 4D reconstruction of the scene. We then apply per-pixel segmentation masks to outline the objects of interest in each video. For each video, we select primary camera and use the objects metric depth from that viewpoint as the depth prior provided to the VLM. Object movements 5 are then computed in world coordinates. For internet data, since the videos are monocular and lack multi-view information, we manually annotate sizes and displacements in pixel space and use reference objects with known priors to obtain the mapping to world scale."
        },
        {
            "title": "We provide a detailed description of the data annotation",
            "content": "process in the supplementary materials Appendix D. Prompt design. As mentioned, the input to the VLM consists of video paired with single physical prior. For simplicity and effective integration, this prior is provided as textual description (see Figure 2 for examples). In addition, we include textual cues in the prompt, such as analyze the video and calculate the answer carefully. We also add output constraints, instructing the model to output only the numerical answer and unit in its final response. We provide detailed description of the prompt design process in the supplementary materials Appendix F. 4. Evaluation on QUANTIPHY 4.1. Evaluation Setup Benchmark models. With different architecture designs, training data and protocals, different VLMs may have varying ability of physical understanding from videos. For better comprehensiveness, we include the evaluations of total number of 21 state-of-the-art VLMs and variants in this work, consisting of 6 proprietary models include ChatGPT5.1 [31], ChatGPT-5 [30], Gemini-2.5 Flash [16], Gemini2.5 Pro [17], Grok-4.1 [46], and Claude-4.5 Sonnet [2] and 15 open-sourced models (as listed in Table 1). The code supports multiple providers (OpenAI, Gemini, xAI, Anthropic, and Replicate API) with provider-specific parameters. Temperature is typically 00.1 for deterministic outputs. Token limits vary: OpenAI models use up to 10,000 tokens due to longer thinking steps, and open-sourced models with around 5002,048. Human studies. To complement our model evaluation and establish reference point for human-level performance on quantitative physical reasoning, we conducted survey study. Participants watched 18 videos and answered 13 quantitative kinematic questions using the same priors and task definitions as our VLM evaluation. Importantly, humans and VLMs receive fundamentally different forms of input. VLMs operate directly on pixelaccurate video tensors, while human participants rely on visual perception, intuition, and coarse approximations. Metric design. All tasks in QUANTIPHY require models to output numerical values. Following VSI-Bench [54], we use Mean Relative Accuracy (MRA) as the primary metric to measure the proximity between model predictions and ground-truth answers. While accuracy based on exact matching is simple baseline but too brittle for continuous, noisy physical measurements in QUANTIPHY, MRA evaluates whether the relative error falls below set of tolerance thresholds and averages the resulting accuracies, offering more calibrated and robust notion of when models are accurate enough for physically grounded reasoning (see subsection A.2 in the supplementary material for further discussion). Concretely, we consider set of confidence thresholds , and define MRA for predic- = 0.1, 0.2, . . . , 0.9, 0.95 } { tion ˆy with ground truth as MRA = 1 10 (cid:88) 1 (cid:18) ˆy θC < (cid:19) θ , where 1( ) is the indicator function. Intuitively, larger θ corresponds to stricter tolerance 1 θ on the relative error, and MRA averages accuracy across spectrum of such tolerances, providing more informative measure than single-threshold accuracy. Our benchmark organizes questions into four kinematic categories, 2D-Static, 2D-Dynamic, 3D-Static, and 3D-Dynamic. For each model and each category, we compute the category-level score by averaging the questionlevel MRA over all questions in that category for which the model produces valid numerical answer. The overall score of model is then obtained as the unweighted mean of its four category-level scores. practical challenge is that not all VLMs consistently output sensible numerical predictions for our reasoning tasks. For every videoquestion pair, we query model up to five times with the same prompt, stopping early if any response contains parseable numerical value. If none of the five responses yields valid number, we regard the model as failing to answer that question. 4.2. Main Results Table 1 summarizes performance on QUANTIPHY task types (2D-Static, 2D-Dynamic, across four 3D-Static, and 3D-Dynamic). Overall, we find that quantitative kinematic inference remains challenging for current VLMs: even the best systems do not yet reach human performance, despite in principle having access to more precise information. Human baseline. Human annotators achieve an average MRA of 55.6 across all categories, with scores between 50.0 (2D-Static) and 59.1 (2D-Dynamic). This range is consistent with the fact that humans do not have direct access to pixel-level measurements and must instead rely on coarse visual estimation (e.g., counting grid lines or comparing to familiar objects), but still demonstrates that the tasks are solvable with reasonable accuracy. Proprietary models. ChatGPT-5.1 attains the highest overall score with 53.1 MRA, followed by Gemini-2.5-Pro at 49.6. ChatGPT-5.1 slightly surpasses humans on the 2D-Dynamic category, yet none of them surpass the human average. Other closed models such as GPT-5 and Claude Sonnet 4.5 are substantially weaker, with overall MRA around 32.6 and 22.8, respectively. Open-weight models. Open-weight models exhibit wide performance spread. The best open-weight system, Qwen3-VL-Instruct-32B, reaches 46.0 overall MRA, with strong 2D-Dynamic and 3D-Dynamic scores, followed by InternVL-3.5-30B (40.7) and Qwen3-VL-Instruct-8B (38.8). These models are clearly below the top proprietary ones, but already comparable to mid-tier closed models. Smaller open-weight models such as Phi-4 Multimodal and SmolVLM-256M still achieve non-trivial MRA, yet remain far from both human and large-model performance. Scaling effects. To better understand the role of model scale, we compare models within the same family across different parameter sizes. Within the Qwen3-VL family, average MRA increases from 29.0 at 2B parameters to 38.8 at 8B and further to 46.0 at 32B. similar trend is observed for InternVL, where InternVL-3.5-30B (40.7) substantially outperforms its 8B (35.4) and 2B (25.0) variants. Notably, scaling benefits are most pronounced on dynamic categories (2D-Dynamic and 3D-Dynamic), suggesting that larger models are better able to integrate temporal information for quantitative inference. However, these gains exhibit diminishing returns and do not close the gap to either top proprietary models or human performance, indicating that scale alone is insufficient for faithful physical reasoning. Gap to super-human performance. By collecting results from both humans and VLMs, we observe an interesting pattern. Importantly, human performance does not represent the theoretical ceiling for QUANTIPHY. While humans must rely on coarse visual approximations, an ideal agent with precise frame-level access to pixel coordinates could recover the worldpixel scale and compute target quantities exactly. In theory, VLMs should be capable of significantly outperforming humans by leveraging this exact pixel information to perform precise algebraic operations. The fact that the best current systems cluster around 50% MRA, comparable to or below human baselines, suggests they still fundamentally under-utilize visual precision and physical priors, leaving robust and accurate quantitative reasoning for kinematic inference tasks as an open challenge. 5. Dissecting Quantitative Reasoning in VLMs Beyond directly evaluating VLMs as black boxes on kinematic inference tasks, we next take an inside look at how they arrive at their quantitative conclusions. Specifically, we investigate three aspects: (i) how scene context, such that background complexity and the number of objects, modulates task difficulty; (ii) the extent to which VLMs faithfully use the provided video and priors, rather than relying on memorized world knowledge; and (iii) whether structured prompting with step-by-step guidance can systematically improve kinematic inference. 5.1. Effect of Scene Context For more fine-grained analysis, we further categorize each video-question pair along third axis describing the visual SX, MX, SS, MS, SC, MC environment: Scene Difficulty . } The first letter indicates whether there is single (S) or multiple (M) moving objects. The second letter indicates the background: solid plain color (X)4, simple but textured scene (S), or visually complex scene (C). { Figure 5 summarizes MRA across these categories. Overall, we observe that background complexity has only mild effect on models performance on the task. Models perform slightly better in the SAM-denoised condition than in the simple-texture (S) background, suggesting that removing irrelevant clutter reduces distractions and stabilizes quantitative estimates. Interestingly, performance in visually complex scenes (C) is above the other two background conditions for most models. plausible explanation is that realistic backgrounds provide additional reference cues (e.g., tiles, windows, or road markings) that help the model infer scale and motion. In contrast, the number of objects in the scene exhibits clearer trend: setups with multiple objects (MX, MS, MC) consistently yield higher MRA than their single object counterparts (SX, SS, SC). Having more objects gives the model extra reference targets (e.g., another ball, ruler-like structure), which can be used as implicit comparison standards for both size and speed. These observations highlight that VLMs robustly benefit from richer background information and relational structure in the scene. 5.2. Do VLMs Use Videos and Priors Faithfully? QUANTIPHY is designed to assess whether VLMs can truly understand physical events captured by visual sensors. It is therefore crucial to investigate whether these systems genuinely comprehend the provided visual signals, rather than 4Some X-type scenes are constructed by segmenting the target object with SAM2 and compositing it onto uniform background, i.e., fully denoised variant of the original video. 7 Models Size Kinematic Categories 2S 2D 3S 3D Avg. Video + Prior Prior only Counterfactual CoT Proprietary models ChatGPT-5.1 [31] Gemini-2.5 Pro [17] Gemini-2.5 Flash [16] Grok 4.1 (Fast Reasoning) [46] ChatGPT-5 [30] Claude Sonnet 4.5 [2] Open-weight models Qwen3-VL-Instruct-32B [5] InternVL-3.5-30B [10] Qwen3-VL-Instruct-8B [5] InternVL-3.5-8B [10] Molmo-7B [13] Phi-4-Multimodal-Instruct [29] Qwen3-VL-Instruct-2B [5] SmolVLM-Instruct [27] InternVL-3.5-2B [10] VILA-7B [24] CogVLM2 Video [43] Phi-3-Mini-128K-Instruct-3.8B [28] LLaVA-13B [25] MiniCPM-V 4.5 [56] Fuyu-8B [22] 32B 30B 8B 8B 7B 5.6B 2B 0.26B 2B 7B 12B 3.8B 13B 8B 8B Human Baseline 46.3 44.8 40.3 39.4 36.6 19.6 35.8 36.7 26.0 27.3 30.0 33.4 27.1 31.6 25.0 23.0 19.4 17.3 14.4 27.6 9.5 50.0 56.2 57.5 53.2 49.5 35.0 23.0 51.6 45.4 47.8 41.8 43.1 42.3 39.0 34.4 31.1 29.8 28.7 14.7 22.1 26.3 14. 59.1 51.5 42.4 43.6 42.4 25.9 19.6 43.2 38.6 35.1 34.4 24.4 25.4 17.6 20.0 16.6 14.4 12.7 19.5 8.0 0.4 9.5 55.2 58.3 53.7 57.4 48.6 33.1 29.1 53.4 42.0 46.3 38.3 36.6 28.4 32.1 27.8 27.4 23.0 27.9 18.6 16.5 0.0 16. 57.9 53.1 49.6 48.6 45.0 32.6 22.8 46.0 40.7 38.8 35.4 33.5 32.4 29.0 28.5 25.0 22.6 22.2 17.5 15.2 13.6 12.5 55.6 56.1 60.9 49.8 47.5 34.2 25.4 50.1 45.4 40.5 37.0 39.8 40.0 34.9 38.9 32.7 31.8 28.5 11.1 20.2 29.7 14. 39.0 46.1 36.1 44.3 50.8 16.6 37.2 24.9 19.3 20.1 28.2 25.1 10.3 15.4 29.9 14.4 31.6 29.6 11.6 34.0 12.1 12.0 29.7 14.7 9.2 25.2 14.3 22.5 14.1 9.5 8.4 13.9 19.9 9. 27.7 49.8 22.4 39.5 53.7 25.9 23.1 17.6 21.0 18.2 15.9 23.5 25.6 17.8 21.5 10.0 26.4 7.2 14.4 24.1 21.1 Table 1. Evaluation results on QUANTIPHY. We report Mean Relative Accuracy (MRA %) on four kinematic categories (2S, 2D, 3S, 3D) and their average. Dark cell marks the best overall model and light cell marks the best open-weight model. Table 2. Extensive results on an analysis subset. We report Mean Relative Accuracy (MRA) in %. Rows follow the same model order as in Table 1. merely producing plausible guesses. Here, we re-evaluate all models on controlled subset of 1612D video-prior pairs,5 using two complementary probes. Key finding: VLMs rely more on learned prior knowledge than visual inputs for physical reasoning. We first compare the default video+prior setting against prior-only setting in which the video is removed but the prompt (including the physical prior, object description, and question) is kept unchanged. Hypothetically, without the reference video, we would expect an evident performance drop, since VLMs would be forced to guess; even if such guesses are reasonable, they may not correspond to the specific instance. However, across most models we observe only modest differences between the two conditions: on the 161-pair subset, MRA in the prior-only setting is often close to, and sometimes only slightly below, the video+prior setting, even for strong systems such as ChatGPT-5.1, Gemini 2.5 Pro/Flash, and Qwen3-VL-Instruct-32B. In other words, models can already obtain reasonably high scores by relying on their internal prior knowledge about typical object sizes and speeds, with limited added value from the actual video frames. This suggests that, in our tasks, many VLMs behave less like visual measurers and more like powerful guessers conditioned on textual hints. 5Due to resource constraints, the experiments in subsection 5.2 and subsection 5.3 are run on 2D-only subset of 161 instances, and MRA values therefore differ slightly from those in Table 1. { Key finding: VLMs (mostly) do not reason but memorize. To further test whether models faithfully utilize the conditioning prior and video to infer the target physical quantity, we perform counterfactual analysis. For each of the 161 instances, we construct family of counterfactual prompts by multiplying the original physical prior by scalar factor , while α 0.001, 0.01, 0.1, 0.2, 5, 50, 100, 200, 500, 700 } keeping the video and question unchanged. Hypothetically, if model is capable of correct kinematic reasoning, its prediction should scale accordingly, tracking the counterfactual ground truth ycf = α y. As result, we would expect no, or at least only modest, performance drop in this test. However, in practice, we find that even the best models obtain very low scores in this setting: most models MRA drops by 80%, and even the strongest model drops by 70%. Despite being given numerically precise but altered prior, the outputs remain close to the original physical magnitudes implied by real-world experience, rather than those dictated by the provided priors. Taken together, these results lead to consistent conclusion: existing VLMs are not yet input-faithful quantitative reasoners. They only weakly exploit pixel-level information in videos, and they do not reliably condition on the exact numerical priors provided in the prompts. Instead, their quantitative kinematic inferences are dominated by internal, pre-trained world knowledge, with visual evidence and explicit priors acting more as soft hints than hard constraints. 8 tive reasoning. Many models appear unable to reliably solve the intermediate numeric subproblems, so decomposing the task mainly amplifies and propagates early errors. Our analysis suggests that existing VLMs can exploit visual cues, priors, and structured prompts to reason quantitatively, but do so in rather brittle and often inconsistent manner. QUANTIPHY thus provides not only benchmark for aggregate performance, but also diagnostic tool for probing where quantitative physical reasoning succeeds, fails, and how it might be improved. We provide additional studies in the supplementary material Appendix to further support our findings. 6. Discussion and Future Work Summary of Findings. Our key finding is that state-of-theart VLMs have not yet established reliable link between visual observations and quantitative physical facts. This disconnect manifests as critical lack of input faithfulness: although models process video inputs, our extensive studies show that they hardly infer kinematic properties from the actual pixel-level information. Instead, they exhibit strong reliance on parametric priors, often overriding explicit user inputs and visual evidence in favor of memorized world knowledge. Consequently, current systems act more as approximate guessers based on semantic context rather than precise visual measurers, limiting their reliability for real-world embodied agents. Limitations. Our study has several limitations that suggest avenues for future work. The dataset focuses exclusively on translational movement, omitting rotational dynamics, and utilizes fixed camera perspective, which simplifies the task compared to real-world scenarios with dynamic viewpoints. Additionally, we examined only rigid objects, excluding soft bodies and deformable materials. Finally, our dataset is relatively simplified, featuring isolated movements rather than complex, multi-object interactions. Conclusion and Future Work. These limitations highlight promising directions for future research. First, more comprehensive video dataset is needed to evaluate VLMs on more diverse physics, incorporating the complexities we omitted: rotational dynamics, deformable objects, varied camera perspectives, and complex multi-body interactions. Second, our findings can inform new VLM training methodologies, such as physics-informed objectives or specialized pre-training on physics-rich data. Ultimately, this research aims to advance the development of generalist embodied AI agents capable of sophisticated reasoning and interaction with the physical world. Figure 5. Effect of scene context. We plot the MRA (%) scores for all benchmark models on different categories, sorted in descending order according to their average MRA performance. 5.3. Do Structured Prompts Help VLMs Reason? Based on the above observation that VLMs rely more heavily on text prompts, we therefore further investigate whether more structured chain-of-thought reasoning pattern can improve quantitative physical predictions. Instead of directly asking for the target quantity, we decompose each question into four-step chain: (I) Pixel-level source property: What is [the ground-truth objects property] in pixels? (II) Scale estimation: What is the proportional relationship between pixels and [a kinematic scale]? (III) Pixel-level target property: What is [the inference targets property] in pixels? (IV) World-level target property: What is [the inference targets real-world property]? We query each model sequentially on the same analysis subset as in subsection 5.2. If the model produces parseable numerical answer at given step, we append both the question and the extracted answer as additional context for the next step. If the model fails to generate valid number at step, subsequent prompts omit that step and its answer. We evaluate models final answers with MRA. As shown in Table 2, chain-of-thought (CoT) prompting is less helpful than anticipated. Among the 21 models we examined, only three show any improvement, with ChatGPT5 and Fuyu-8B exhibiting noticeable increase in MRA. For the remaining 19 models, including several strong openweight systems, performance under our CoT protocol is worse than under direct zero-shot prompting, sometimes by large margin. In other words, under our setup, explicitly spelling out pixel measurement, scale estimation, and rescaling does not systematically improve current VLMs quantita-"
        },
        {
            "title": "Acknowledgment",
            "content": "The authors thank Juze Zhang and Heng Yu for their help with lab data capturing. This work was partially funded by the NIH Grant R01AG089169, Stanford HAI Hoffman-Yee Award, Stanford HAI graduate fellowship, and UST."
        },
        {
            "title": "References",
            "content": "[1] Kelsey Allen, Kevin Smith, and Joshua Tenenbaum. Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. Proceedings of the National Academy of Sciences, 117(47):2930229310, 2020. 3 [2] Anthropic. Claude 4.5 sonnet. https://claude.ai/, 2025. Accessed: 2025-11-14. Large language model. 6, 8, 31 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 2 [4] Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. 3 [5] Shuai Bai and et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 8, 31 [6] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. PHYRE: New Benchmark for Physical Reasoning, 2019. [7] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 3 [8] Steven S. Beauchemin and John L. Barron. The computation of optical flow. ACM computing surveys (CSUR), 27(3):433 466, 1995. 3 [9] Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Hui En Pang, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, and Lei Yang. Holistic evaluation of multimodal llms on spatial intelligence. arXiv preprint arXiv:2508.13142, 2025. 4 [10] Zhe Chen and et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic capabilities. arXiv preprint arXiv:2312.14238, 2024. 8, 31 [11] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding, 2025. 2, 3, 4 [12] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. [13] Allen Institute for AI. Molmo: family of open visionlanguage models. AI2 Blog, 2025. 8, 31 [14] Tobias Gerstenberg, Noah Goodman, David Lagnado, and Joshua Tenenbaum. counterfactual simulation model of causal judgments for physical events. Psychological review, 128(5):936, 2021. 3 [15] Andrea Giachetti, Marco Campani, and Vincent Torre. The use of optical flow for road navigation. IEEE transactions on robotics and automation, 14(1):3448, 2002. 3 [16] Google. Gemini 2.5 flash [large language model]. https:// deepmind.google/models/gemini/flash/, 2025. Accessed: 2025-11-14. 6, 8, 31 [17] Google. Gemini 2.5 pro [large language model]. https: //deepmind.google/models/gemini/pro/, 2025. Accessed: 2025-11-14. 6, 8, [18] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 2 [19] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1128711297, 2021. 2 [20] Berthold KP Horn and Brian Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. 3 [21] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How Far is Video Generation from World Model: Physical Law Perspective, 2024. 3 [22] Adept AI Labs. Fuyu-8b: smaller, faster multimodal model. Adept AI Blog, 2023. 8, 31 [23] Shiqian Li, Kewen Wu, Chi Zhang, and Yixin Zhu. On the learning mechanisms in physical reasoning. Advances in Neural Information Processing Systems, 35:2825228265, 2022. [24] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023. 8, 31 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 8, 31 [26] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2 [27] Francesco Marafioti and et al. Smolvlm: Efficient multimodal ai, 2025. As summarized by Emergent Mind. Refers to underlying technical papers. 8, 31 [28] Microsoft. Phi-3 technical report: highly capable arXiv preprint language model locally on your phone. arXiv:2404.14219, 2024. 8, [29] Microsoft. Empowering innovation: The next generation of the phi family. Microsoft Azure Blog, 2025. 8, 31 10 [30] OpenAI. Chatgpt, version 5.1 [large language model]. https://chat.openai.com/, 2025. Accessed: 202511-13. 6, 8, 31 [31] OpenAI. ChatGPT version 5.1 [large language model]. https://chatgpt.com/, 2025. Accessed: 2025-11-14. 6, 8, 31 [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 5, [33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 3 [34] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the edge of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. 21 [35] Ilyse Resnick, Alexandra Davatzes, Nora Newcombe, and Thomas Shipley. Using relational reasoning to learn about scientific phenomena at unfamiliar scales. Educational Psychology Review, 29(1):1125, 2017. 3 [36] Keattisak Sangsuwan and Mongkol Ekpanyapong. Videobased vehicle speed estimation using speed measurement metrics. IEEE Access, 12:48454858, 2024. 3 [37] Sketchfab, Inc. Sketchfab. https://sketchfab.com/, 2025. Accessed: 2025-11-14. 5 [38] Deepak* Sridhar, Kartikeya* Bhardwaj, Jeya Pradha Jeyaraj, Nuno Vasconcelos, Ankita Nayak, and Harris Teague. Video reasoning without training, 2025. [39] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46314640, 2016. 2 [40] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 2 [41] Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Joshua Tenenbaum, Daniel Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties. 2 [42] Quan Vuong, Sergey Levine, Homer Rich Walke, Karl Pertsch, Anikait Singh, Ria Doshi, Charles Xu, Jianlan Luo, Liam Tan, Dhruv Shah, et al. Open x-embodiment: Robotic In Towards Generalist learning datasets and rt-x models. Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023, 2023. 2 [43] Wenyi Wang and et al. Cogvlm2: versatile visual language model for both image and video understanding. arXiv preprint arXiv:2406.11438, 2024. 8, 31 [44] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1786817879, 2024. 2, [45] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 2 [46] xAI. Grok-4.1. https://x.ai/grok, 2025. Accessed: 2025-12. 6, 8 [47] xAI. Grok 4.1 API Documentation. xAI, 2025. 31 [48] Tiange Xiang, Adam Sun, Scott Delp, Kazuki Kozuka, Li Fei-Fei, and Ehsan Adeli. Wild2avatar: Rendering humans behind occlusions. arXiv preprint arXiv:2401.00431, 2023. 2 [49] Tiange Xiang, Adam Sun, Jiajun Wu, Ehsan Adeli, and Li Fei-Fei. Rendering humans from object-occluded monocular videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32393250, 2023. [50] Tiange Xiang, Kuan-Chieh Wang, Jaewoo Heo, Ehsan Adeli, Serena Yeung, Scott Delp, and Li Fei-Fei. Neuhmr: Neural rendering-guided human motion reconstruction. In 2025 International Conference on 3D Vision (3DV), pages 15181528. IEEE, 2025. 2 [51] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 2 [52] Xinrun Xu, Pi Bu, Ye Wang, Borje F. Karlsson, Ziming Wang, Tengtao Song, Qi Zhu, Jun Song, Zhiming Ding, and Bo Zheng. DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning. arXiv preprint arXiv:2508.05405, 2025. 2 [53] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li FeiFei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [54] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 2, 4, 6 [55] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025. 2 [56] Yuan Yao, Tianyu Yu, and et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.10032, 2024. 8, 31 [57] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. Advances in neural information processing systems, 31, 2018. 2 11 [58] Kexin Yi, Chuang Gan, Yunzhu Li, and Pushmeet Kohli. CLEVRER: COLLISION EVENTS FOR VIDEO REPRESENTATION AND REASONING. 2020. [59] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European conference on computer vision, pages 121. Springer, 2022. 3 [60] Zhicheng Zheng, Xin Yan, Zhenfang Chen, Jingzhou Wang, Qin Zhi Eddie Lim, Joshua Tenenbaum, and Chuang Gan. Contphy: Continuum physical concept learning and reasoning from videos. arXiv preprint arXiv:2402.06119, 2024. 2 12 QUANTIPHY: Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models"
        },
        {
            "title": "Contents",
            "content": "A. More Studies and Results . A.1. Additional Case Studies . A.2. Metric Design Justification . . A.3. Model MRA Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Dataset Construction Guidelines . . B.1. General Principles . B.2. Video Types . . . . . . B.2.1. Video Categories Definition . . B.2.2. Quantitative breakdown of video types. . . . . . . . . . . . . . . . . . . . . . . . . . . C. Details of Data Collection C.1. Blender Simulation . . . . . . . . . . . . C.1.1. Blender Toolkits and Asset Sources . C.1.2. Two Motion Simulation Types . . . C.1.3. Blender Videos Construction . . . . . C.2. Lab Capturing . . . . . . C.3. Internet Scraping . . . . . . C.4. Segmented Data . . . . . . . C.5. Quality Control . . . C.6. Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Details of Data Annotation . . D.1. Blender Simulation . . . . . D.1.1. Size . D.1.2. Displacement or Path. . D.1.3. Velocity and Acceleration. . D.1.4. Depth and Distance . . . . . . . D.2. Lab Data Annotation . . D.3. Internet Data Annotation . . D.4. Segmented Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Vision-Language Models F. Prompt Design G. Answer Retrieval and Parsing H. Human Study Details. H.1. Participants . . . . H.2. Task Construction and Experimental Design . . H.3. Evaluation Metric . . . . H.4. Results and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sketchfab Model Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 4 4 6 6 7 7 8 . . . . . . . 9 9 . . 9 . 12 . 12 . 18 . 18 . 20 . 21 . 21 22 . 22 . 22 . 22 . 25 . 27 . 28 . 29 . 31 31 31 34 34 . 34 . 36 . 36 . 36 A. More Studies and Results A.1. Additional Case Studies To better understand how Vision-Language Models (VLMs) solve kinematic inference tasks beyond aggregate scores, we conduct qualitative case study on the top-performing model, ChatGPT-5.1, using its Thinking mode in the standard user-facing interface. For each selected videotext pair, we repeatedly query the model and inspect the toolaugmented chain-of-thought until we obtain representative traces that are syntactically well-formed and numerically valid.6 We then analyze both the final numerical answers and the intermediate reasoning steps.7 apply prior Overall, we observe sharp contrast between successful instances, where the model follows textbook-like measure pixels compute target pipeline, and failure modes, where it largely ignores the video and the provided prior, and instead falls back to pre-trained world knowledge or generic heuristics. Below we discuss four representative cases. Case 1: Faithful pixelprior reasoning. Figure 6 shows 2D scene with yellow car moving laterally. The model is asked two questions: (i) given that the cars length is 5.67 m, what is its speed at 2.0 s; and (ii) what is the cars width in meters. In this instance, ChatGPT-5.1s chain-ofthought closely matches the intended reasoning procedure. The model first identifies the relevant frames around = 2.0 s, uses OpenCV-style tools to obtain bounding boxes, and explicitly treats the longer side of the box (135 px) as the cars length in pixel space. It then calibrates pixel-to-meter scale from the given length prior (5.67 m), and computes the width as width 58 135 5.67m 2.44m , which is close to the ground truth width and achieves high relative accuracy. Here the model behaves as an input-faithful visual measurer: it grounds both the prior and the kinematic target in pixel space and performs the correct proportional reasoning. When this pipeline is followed, the resulting numerical answers are often near the ground truth. Case 2: Counterfactual prior breaks faithfulness. In the second case (Figure 7), we reuse the same video but multiply 6We prompt the model multiple times and observe substantial variability: on the same instance, some runs produce accurate and well-structured reasoning, while others fail to parse the question or ignore key inputs. We therefore collect several responses per instance and manually select representative traces that are syntactically coherent and numerically valid for detailed analysis. similar instability is also present in our API-based evaluation, even with temperature fixed to 0; in the main benchmark, we address this by running multiple trials and recording failure rate. 7We emphasize that this analysis is diagnostic rather than evaluative: we study one specific models internal behavior to illustrate broader patterns of (un)faithful quantitative reasoning. Figure 6. Case 1: Faithful pixelprior reasoning. the car-length prior by counterfactual factor of 1000, changing the input to length of the yellow car = 5670 m. The task is again to infer speed at 2.0 and the cars width. In its Thinking trace, the model explicitly notes that 5670 is an implausible car length and expresses confusion. Crucially, instead of continuing to rely on pixel measurements and the (counterfactual) prior, it effectively abandons the video and the numeric input. For the width question, it switches to generic heuristic, assuming typical cars width-to-length ratio and hallucinating plausible-looking width independent of the actual scene. The final width prediction happens to have high relative accuracy (close to 0.9), but this success is not input-faithful; it arises from pre-trained knowledge about cars rather than from the specific video or the given prior. This case highlights key risk that purely 2 Figure 8. Case 3: Video ablation reveals reliance on priors. Case 4: Strong gravitational prior overrides counterfactual physics. The fourth case (Figure 9) involves Blendersimulated basketball scene that visually resembles realistic indoor court, but with counterfactual physics. The balls acceleration is time-varying and close to 1 m/s2, rather than standard gravity. The model is asked for the balls acceleration at 0.5 and its speed at 1.5 s, given the balls diameter as prior. Here, ChatGPT-5.1 completely ignores both the video and the non-standard trajectory implied by the simulation. It directly outputs the canonical gravitational acceleration 9.8 m/s2, and for speed simply multiplies by time (i.e., 1.5 = 14.7 m/s), leading to relative accuracy equals to 9.8 0 on both queries. No pixel measurements or scale computations appear in the Thinking trace. This illustrates how strong pre-trained physical priors (e.g., objects fall with acceleration g) can dominate the models behavior, even when they contradict the actual visual input and the provided prior. Discussion. These four cases collectively sharpen our main quantitative findings. When everything works, ChatGPT-5.1 can execute an impressive, tool-augmented pipeline that does read pixel trajectories, apply the physical prior, and compute accurate kinematic quantities. However, this behavior is fragile. As soon as the prior becomes counterfactual, the video is ignored, or the underlying physics departs from familiar regimes, the model quickly reverts to pre-trained world knowledge or rough heuristics, often ignoring the provided inputs. High numerical accuracy does not guarantee input-faithful reasoning. Specifically, Case 2 demonstrates that model can get the right answer for the wrong reasons, while Cases 3 & 4 show that it can stick to canonical physical constants even when the scene violates them. These observations suggest that improving VLMs quanFigure 7. Case 2: Counterfactual prior breaks faithfulness. outcome-based metrics can judge an answer as good, while the underlying reasoning ignores the provided evidence. Case 3: Video ablation reveals reliance on priors. Case 3 (Figure 8) uses video-ablation setting. The model receives only the text prompt (including length of the yellow car = 5.67 m), without access to the video. When asked for the cars speed at 2.0 s, ChatGPT-5.1 produces an answer (12 m/s) that is far from the ground truth, confirming that motion estimation is difficult without visual evidence. However, when asked for the cars width (still without video), the model outputs numerically reasonable value with relatively 0.7). Since no pixel inhigh accuracy (relative accuracy formation is available in this ablated setting, this behavior can only be explained by the models internal prior over typical car dimensions. Combined with Case 1 and 2, this suggests pattern that even when video is available, much of the size inference can be driven by pre-trained world knowledge rather than by explicit pixel measurements. 3 Discrete but calibrated. MRA discretizes accuracy into finite set of thresholds, offering interpretable feedback on how often the model is close enough under increasing demands. Rather than penalizing deviations proportionally (which can be dominated by outliers), MRA provides graded scale of correctness, similar in spirit to the mAP@IoU8 metrics in object detection. Robust to ambiguity and noise. Many video-based physical inferences involve semantic or visual uncertainty. For example, estimating persons height may vary depending on whether hair or shoes are included; estimating cups diameter may depend on whether the inner or outer rim is used. MRA tolerates such ambiguity by granting full credit when answers fall within reasonable margin. Moreover, measurement noise is often unavoidable: Temporal aliasing: limited frame rates restrict temporal resolution for computing velocities and accelerations; Motion blur: fast-moving objects introduce visual uncertainty during measurement; Imprecise priors: even real-world scale references (e.g., credit card dimensions) may not be perfectly visible or aligned. MRA accommodates these natural imperfections more flexibly than regression-style loss. Supported by precedent. The MRA metric was introduced by Yang et al. [53] in VSI-Bench for evaluating numerical answers in visualspatial reasoning tasks. Some follow-up work [9, 38] adopted the same evaluation to benchmark multimodal models in physical and spatial settings. These works motivate MRA as stable and discriminative way to capture proximity between predicted and ground-truth values, especially when scale varies across examples. Our use of MRA continues this design choice, ensuring comparability while improving robustness. In sum, MRA balances informativeness, robustness, and interpretability. It is well-suited for evaluating VLMs on physically grounded, numerically sensitive tasks like those in QUANTIPHY, where small deviations are acceptable, but large errors are unacceptable regardless of scale. A.3. Model MRA Distribution Figure 10 visualizes the MRA distributions for each VLM. Each subplot shows density curve with mean (gray solid) and median (blue dashed) lines, along with summary statistics (mean, std, median, Q25, Q75) displayed to the right. The top-performing models, including ChatGPT-5.1, Gemini-2.5 Pro, Gemini-2.5 Flash, Qwen3-VL-instruct-32B, 8mAP@IoU: Mean Average Precision (mAP) calculated at specific Intersection over Union (IoU) thresholds. Figure 9. Case 4: Strong gravitational prior overrides counterfactual physics. titative physical reasoning will require not only better aggregate performance, but also mechanisms that encourage faithful use of visual evidence and explicit numerical priors, rather than letting powerful, but sometimes misleading, pretrained world knowledge dominate the inference process. A.2. Metric Design Justification In QUANTIPHY, we adopt Mean Relative Accuracy (MRA) as the primary evaluation metric for quantitative physical inference tasks with continuous outputs. Each prediction ˆy is compared to the ground truth using its relative error /y, and awarded partial credit if the error falls below ˆy confidence thresholds θ . The final } MRA score is the average of binary accuracies across these thresholds 0.5, 0.55, . . . , 0.95 { MRA = 1 10 (cid:88) 1 (cid:18) ˆy θC < 1 (cid:19) θ , (1) where = thresholds. 0.5, 0.55, . . . , 0.95 } { is the set of confidence While one could alternatively compute continuous relative error (e.g., mean relative error or mean absolute percentage error), we prefer MRA for several practical and conceptual reasons. 4 Figure 10. Distribution of MRA by model. One caveat to note is that the Avg. MRA in Table 1 reflects the mean MRA across inference task categories for each model (i.e., the average MRA of 2D-Static, 2D-Dynamic, 3D-Static, and 3D-Dynamic). In contrast, the mean in this distribution plot represents the average MRA at the individual-question level for each model. and Grok-4.1-Fast-Reasoning, show notably higher densities around moderate to high MRA values above 0.5, with means and medians clustering around 0.40.6. Their smoother, more concentrated curves indicate both higher accuracy and more consistent performance across the evaluation set. second tier of models, including InternVL-3.5-30B, Qwen3-VL-instruct-8B, Molmo-7B, and ChatGPT-5, exhibits slightly lower means and medians, generally around 0.30.4. Their broader distributions with heavier tails indicate greater variability that these models produce some strong outputs but also more low-scoring cases. The alignment of medians and means suggests errors are not extremely skewed. Mid-tier models such as Qwen-3-VL-instruct-2B, Phi-4Multimodal-Instruct, SmolVLM-Instruct, CogVLM2 Video, Claude-4.5 Sonnet, and VILA-7B show means of 0.20.3. Their distributions are heavily weighted toward low MRA values with thin right tails, indicating that while they occasionally achieve moderate scores, they rarely reach the performance levels of top-tier systems. The weakest-performing models, for example, Phi3-Mini-128K-Instruct-3.8B, LLaVA-13B, Fuyu-8B, and MiniCPM-V 4.5-8B, show distributions highly concentrated near zero. With means below 0.15 and medians around 0.0, these models fail to produce meaningful MRA performance in most cases. Notably, many distributions have substantial mass centered around zero. An MRA of zero indicates either the model output zero as an answer or failed to produce proper numerical output. We also observed that some model APIs are unstable and produce errors over time, potentially due to API server error, running environment, internet traffic, batch size variations, etc. We abstract away from these in this paper and simply treat these failed cases as observations with MRA equal to zero. Overall, modern frontier proprietary models cluster around substantially higher and more consistent MRA values, mid-tier models show moderate capability with noticeable variability, and smaller or older models yield predominantly low scores. The density patterns and comparative statistics reveal clear performance gap between state-of-the-art systems and lightweight or earlier-generation models. B. Dataset Construction Guidelines B.1. General Principles ethical issues throughout all stages of data collection and processing. All videos, 3D assets, and simulation resources are either open-source, licensed for research use, or explicitly verified to pose no known copyright conflicts before inclusion. When raw videos contain personally identifiable information (e.g., human faces, license plates), we apply blurring or masking to anonymize the content. Video selection criteria. To make quantitative kinematic inference well-posed and to reduce confounding factors, we enforce the following constraints on all collected videos. Static camera in world coordinates. The camera remains fixed in the world frame during each clip. This avoids entangling camera motion with object motion, which would otherwise introduce additional ambiguity and noise into the inference problem. At least one rigid object undergoing translational motion. Each video contains at least one rigid object whose dominant motion is translation. This requirement ensures that we can formulate well-defined kinematic inference tasks. Non-rigid objects and purely rotational motions are left out of the current benchmark and deferred to future work. Planar motion for 2D tasks. For 2D instances, the target object and the reasoning target are constrained to move in plane parallel to the image plane, i.e., the depth relative to the camera remains (approximately) constant over time. This assumption guarantees consistent mapping between pixel displacement and world-space distance within each clip, making the 2D kinematic inference problem welldefined. Videotext record schema. Each annotated instance in QUANTIPHY is represented as structured videotext record. Table 3 shows representative example. Each record contains the following fields: 1. video id. unique identifier for the underlying video. 2. video source. The data source from which the video was obtained (e.g., simulation, lab, or internet). 3. video type. four-letter code encoding the configuration of the task. The four characters denote, in order: (i) the type of physical prior (Size, Velocity, or Acceleration), (ii) whether the reasoning task is 2D or 3D, (iii) whether there is single (S) or multiple (M) moving objects, and (iv) the background type, that is, plain (X), simple (S), or complex (C). More details are included in subsection B.2. Our data collection and curation follow several general principles to ensure that QUANTIPHY is ethically sourced, physically well-defined, and suitable for quantitative evaluation. Copyright and ethics. We carefully avoid copyright and 9In this work, we adopt relaxed definition of rigid objects: we consider an object rigid if its motion can be consistently approximated by stable center of mass across frames. This includes some entities that may exhibit slight non-rigid deformation (e.g., flying bird or walking person), as long as their motion remains locally trackable and structurally coherent. 6 Property Example value video id video source video type fps inference type question ground truth prior depth info ground truth posterior simulation 0032 simulation A3MC 30 DD What is the acceleration of the orange car at 1.0s in m/s2? gravity acc = 9.8 m/s2 t=1s, distance ball camera = 13.80 m; t=2s, distance ball camera = 13.80 m; t=1.5s, distance orange camera = 10.18 m; t=2s, distance orange camera = 10.40 m; t=2.5s, distance green camera = 4.32 m; t=3s, distance green camera = 6.91 2.86 Table 3. Example of single videotext record in QUANTIPHY. 4. fps. The frame rate of the video, used to convert frame indices into time and to compute velocities/accelerations consistently. 5. inference type. two-letter code indicating whether the prior and the inference target are static or dynamic over time: denotes static quantity, and denotes time-dependent (dynamic) one. The first letter corresponds to the prior, and the second to the posterior. 6. question. The natural-language prompt presented to the VLM. We ensure that each question explicitly specifies the physical unit (e.g., m, cm/s, m/s2) and, for velocity or acceleration, clearly indicates whether the query concerns an instantaneous quantity at given timestamp or an average quantity over an interval. 7. ground truth prior. The physical prior provided to the model, formatted as positive numeric value with unit (e.g., gravity acc = 9.8 m/s2). We enforce consistent formatting to simplify parsing and downstream use. 8. depth info. Depth annotations used only for 3D reasoning tasks. This field contains depth values (in metric units) for the prior object and, when needed, for the inference target at one or more timestamps. Depth information is designed so that, in principle, the depth of the inference target can be recovered from the provided entries. The formatting mirrors that of ground truth prior. 9. ground truth posterior. The numeric groundtruth answer to the kinematic inference question, represented as positive scalar without unit (the unit is part of the question text). type code, we partition all clips into 36 fine-grained categories. We ensure that each category contains at least four videos, so that every configuration is represented nontrivially. Balanced core task categories. We strive to keep the four core inference task categories (2D-Static, 2D-Dynamic, 3D-Static, 3D-Dynamic) approximately balanced in terms of the number of videos and associated questions, enabling fair comparison across conditions. Rich scenes and motion patterns. We cover broad spectrum of spatial scales and motion types. Scenes range from astronomical (e.g., planetary motion), to everyday macroscopic settings (e.g., traffic, sports), to microscopic phenomena (e.g., cells and bacteria). Motion patterns include uniform motion, accelerated and decelerated linear motion, projectile motion, pendulum-like oscillations, and centripetal motion, among others. This diversity is crucial for probing whether VLMs quantitative reasoning generalizes beyond narrow, highly stylized scenarios. B.2. Video Types B.2.1. Video Categories Definition As described in the section , we assign each video fourcharacter code that encodes its physical prior, dimensionality, object setting, and background type. First character (S / / A). The first character specifies which physical prior the video is constructed to probe. For designated object in the scene, we use Balance and diversity. Finally, we design the dataset to be both balanced across task types and diverse in content. Fine-grained video types. Using the four-letter video - = size = velocity = acceleration , indicating Thus, the first character takes one of whether the ground-truth physical quantity is size, velocity, or acceleration for that object, as illustrated in Figure 11. S, V, } { Second character (2 / 3). The second character indicates whether the video is 2-Dimensional(2D) or 3Dimensional(3D). 2D = planar video 3D = volumetric video More precisely, the second character is set to 2 when, at every frame, the distances from the moving object, the groundtruth reference object, and every object referenced in the inference question to the camera are exactly equal, so they occupy single depth layer with no relative depth or parallax among them in the rendered image, and to 3 otherwise. Video examples are shown in Figure 12. Third character (S / M). The third character distinguishes between single-object and multiple-object settings, relative to the queried object(s) rather than the sheer number of objects visible in the scene: = single-object = multiple-object As shown in Figure 13, this label depends on how many objects the viewer needs to reason about. We use (singleobject) when the reasoning process only involves one object. Other objects may appear, but they only serve as background or generic distractors. We use (multiple-object) when answering any of the questions requires reasoning about two or more objects, for example by comparing their sizes or speeds, or by using one object as an explicit reference for another. Fourth character (X / / C). The fourth characters examples are illustrated in the Figure 14. encodes the complexity of the background. = plain background, typically single uniform RGB color with essentially no texture, clutter, or noise; = simple background, which may contain mild lighting or shading variations but remains visually uncluttered; = complex background, with rich textures, multiple visible objects, more intricate lighting, and substantial visual noise. The boundary between simple and complex backgrounds is somewhat subjective, but during dataset construction we deliberately separated these categories and designed scenes so that their visual difference is as clear as possible. Putting these components together, for example, code such as A3MC indicates that the video (i) targets the acceleration prior (A), (ii) is rendered as 3D video (3), (iii) is labeled as multiple-object setting (M) because at least one inference question asks about an object different from the one whose acceleration prior is defined, and (iv) uses complex background. B.2.2. Quantitative breakdown of video types. the codes are To complement the qualitative description above, we now provide quantitative summary of how clips are distributed across the four-character codes. The benchmark contains 569 videos in total, of which 328 are 2D and 241 are 3D, yielding an approximately 4:3 split between planar and volumetric setups (about 58% 2D and 42% 3D). This ensures that both 2D and 3D configurations are substantially represented rather than the dataset being dominated by single family of scenes. Table 4 reports the corresponding counts for each individual code and for the 2D versus 3D groups. By construction, combining three physical priors (S / / A), two dimensionalities (2 / 3), two object settings (S / M), and three background types (X / / C) yields 36 distinct four-character codes, and all 36 appear in the dataset. For 2D videos, the codes are A2SX, A2SS, A2SC, A2MX, A2MS, A2MC, S2SX, S2SS, S2SC, S2MX, S2MS, S2MC, V2SX, V2SS, V2SC, V2MX, V2MS, V2MC; for 3D videos, A3SX, A3SS, A3SC, A3MX, A3MS, A3MC, S3SX, S3SS, S3SC, S3MX, S3MS, S3MC, V3SX, V3SS, V3SC, V3MX, V3MS, V3MC. Each code is instantiated by at least 4 clips, so even the smallest categories have non-trivial support. The largest corpus (V2MC in our current dataset) contains 51 clips. Across all 36 codes, per-code counts range from 4 to 51 clips, with the majority lying between 5 and 35. This distribution avoids both extremely rare one-off configurations and few overwhelmingly frequent ones. The precise per-code counts are given in Table 4. Table 4 also details how each video is obtained. The Blender column counts fully synthetic clips rendered directly in Blender from our own scenes, contributing 300 videos. The Internet column counts 72 clips sourced from existing online footage that we curate and annotate. The Captured column counts 112 clips that we record ourselves (for example, using handheld cameras or screen recordings). The Segmented column counts 85 clips created by segmenting foreground objects from source footage and compositing them into new backgrounds. For each four-character code, the Total column gives the number of clips that realize that configuration. Taken together, these statistics in Table 4 show that the dataset spans all intended configurations, with each category populated by multiple clips and supported by mix of blender-rendered, internet-sourced, captured, and segmented videos. 8 Figure 11. Examples of S/V/A scene. Figure 12. Examples of 2/3 scene. C. Details of Data Collection C.1. Blender Simulation C.1.1. Blender Toolkits and Asset Sources Asset sources and selection. We construct Blender scenes using 3D assets sourced from online repositories, primarily BlenderKit and Sketchfab. We deliberately use these two libraries for complementary purposes: BlenderKit mainly provides complex themed environments that serve as background layouts, whereas Sketchfab mainly provides rigged and animated foreground objects whose motions can be reused with minimal manual keyframing. 9 Figure 13. Examples of S/M scene. Figure 14. Examples of X/S/C scene. for themed complex environments. BlenderKit BlenderKit library that is community-driven asset is tightly integrated into Blenders interface and offers large collection of render-ready models, materials, High Dynamic Range Images(HDRIs), brushes, and complete scenes that can be searched and inserted directly from within Blender. In our pipeline, we primarily rely on BlenderKit for complex themed environments, such as indoor rooms, streets, architectural spaces, and other richly cluttered layouts at both small and large spatial scales. These pre-built scenes typically include coherent lighting, materials, and background geometry (for example, furniture, buildings, and 10 2D/3D Video Type Blender Internet Captured Segmented Total 2D/3D Total 2D 3D A2SX A2SS A2SC A2MX A2MS A2MC S2SX S2SS S2SC S2MX S2MS S2MC V2SX V2SS V2SC V2MX V2MS V2MC A3SX A3SS A3SC A3MX A3MS A3MC S3SX S3SS S3SC S3MX S3MS S3MC V3SX V3SS V3SC V3MX V3MS V3MC 0 10 10 6 14 17 0 10 7 8 11 16 2 10 10 13 13 43 2 2 3 1 3 22 1 2 2 0 1 24 3 2 3 2 3 24 0 6 5 0 0 2 0 5 8 0 3 20 0 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 6 9 7 4 4 4 8 8 8 9 10 10 2 2 2 5 5 6 Total 300 112 Table 4. Statistics of videos. 11 0 0 8 0 0 10 0 0 8 0 0 9 0 0 7 0 0 1 0 0 2 0 0 2 0 0 13 0 0 0 0 0 14 0 0 85 11 16 15 15 15 20 10 15 15 16 14 36 11 17 18 20 13 9 11 10 7 7 26 11 10 10 22 11 34 5 4 5 21 8 30 328 569 241 569 vegetation), which allows us to quickly instantiate diverse indoor and outdoor environments without having to model every object or compose every layout from scratch. This substantially reduces the authoring effort for environment design while still giving us control over camera placement, object insertion, and motion trajectories. Sketchfab for rigged and animated foreground objects. Sketchfab is large community platform for hosting and distributing 3D content, including many rigged and animated models across categories such as animals, vehicles, and articulated characters. In our work, we mainly use Sketchfab to obtain foreground objects that already come with skeleton rig and small set of reusable animation clips. Typical examples include wing-flapping cycles for flying birds (such as eagles spreading and flapping their wings) and swimming 11 cycles for fish with realistic body undulation. Instead of manually keyframing these motions, we can directly reuse or lightly retarget the provided animations in our scenes. This makes it much easier to populate environments with moving agents whose motion is visually plausible, while significantly reducing the time spent on low-level animation authoring. Clarifying Prior Knowledge vs. Visual Assumptions. In an era where videos may originate from real capture, simulation software, procedural animation, or generative models, visual motion alone does not guarantee adherence to physical laws. Accordingly, VLMs should reason strictly based on the prior conditions we provide, rather than relying on pretrained assumptions about how objects should move. Licensing and reuse. For both BlenderKit and Sketchfab, we restrict ourselves to assets whose licenses explicitly allow reuse and modification in works. These licensing choices ensure that all assets in our dataset are used in copyrightcompliant way and that future researchers can reconstruct our pipeline using the same publicly available resources. C.1.2. Two Motion Simulation Types The Blender-generated portion of our dataset contains two complementary categories of motion, reflecting the major paradigms of movement in computer graphics and physicsbased animation. (1) Keyframed Motion. Examples of this category are shown in Figure 16. This category includes humans, animals, and other objects whose motion is defined using rigged skeletons or keyframed animation curves shown in Figure 17. Because these trajectories are authored manually rather than produced through physical simulation, they are visually plausible but not physically constrained. key implication is that the resulting motion does not necessarily obey real-world physical laws. For example, in the lunar-walking scene (Figure 22), the astronauts pushoff, airtime, and landing motion are shaped by artist-edited animation curves. Although the motion is loosely inspired by reduced lunar gravity, we do not compute the animation using the Moons gravitational constant nor derive the trajectory from force-based simulation. These sequences should therefore be interpreted as perceptually reasonable approximations of motion rather than physically calibrated ground truth. (2) Physics-Driven / Force-Based Motion. Examples of this category are shown in Figure 15. This category consists of rigid bodies (shown in Figure 18) moving directly under Newtonian dynamics. Their trajectories are generated by applying explicit forces (e.g., gravity, impulses) inside physics engine or by specifying analytical kinematic profiles (e.g., constant velocity, uniform acceleration, projectile motion). These clips yield clean and physically interpretable motion, allowing us to provide exact ground-truth displacement, velocity, and acceleration. For all videos in this category, we explicitly state the forces or kinematic parameters involved, and these values constitute part of the known prior information for each question. When prior information explicitly specifies force or acceleration (e.g., the object accelerates at 2.5 m/s2), that value serves as authoritative ground truth. When such information is not given, the model should not infer physical constants, such as Earths gravity, only from the visual appearance of the motion. Because our dataset includes diverse simulated and animated sequences, correct reasoning requires using only the provided priors, not assumed real-world physics. C.1.3. Blender Videos Construction Quantitative Overview. In the Blender subset, we start from approximately 125 distinct base 3D models and 81 base scenes. From these bases, we generate 312 unique Blender video clips. Beyond simply increasing the size of the dataset, Blender enables us to construct diverse scenes and controlled trajectories of objects in simplified environments. Because these scenes are generated procedurally, we can precisely specify object sizes, positions, and kinematics via scripts and directly compute accurate ground-truth physical quantities for each clip. As result, the Blender subset is not only large, but also covers broad spectrum of scene scales and motions under tight experimental control. Scene Construction. We construct diverse collection of synthetic scenes to broaden the range of physical situations represented in the benchmark. Specifically, we design both everyday indoor spaces (e.g., rooms with furniture and household objects, see Figure 19) and outdoor spaces (e.g., natural environments with varied terrain, vegetation, and water, see Figure 20). These scenes differ in layout, depth structure, and illumination, so that models must handle physical reasoning under heterogeneous visual conditions rather than overfitting to single canonical setting. Beyond such everyday environments, we also include scenes that are difficult or impossible to capture, measure, or systematically manipulate in the real world. Examples include microscopic settings (e.g., red blood cells moving through vessel, see Figure 21), extraterrestrial scenarios (e.g., astronaut motion on the Moon, see Figure 22), and more. Starting from base assets and scenes, we explicitly control geometry, lighting, and background complexity to generate families of videos that fall into different categories in our taxonomy. For instance, in the red-blood-cell scene (Figure 23), we derive 3MX-style variant by removing the Figure 15. Physics-driven example. bowling ball collides with pins under Newtonian simulation. The motion and resulting trajectories arise directly from rigid-body dynamics and elastic collisions, making this clip representative of our force-based motion category. Figure 16. Keyframed example. The floating swim ring follows manually authored animation curve rather than buoyancy, drag, or wind forces. Its trajectory is visually plausible but not physically derived, representing our keyframed motion category. vessel wall, discarding all but two target cells, and replacing the background with uniform RGB field. Using the same procedure, as shown in the Figure 24 we construct simplebackground versions of more complex scenes by simplifying clutter while keeping coarse structural cues (e.g., horizon lines, major surfaces, object structures) and realistic lighting. It is important to note that, although we present several examples that appear to reuse the same underlying scene. In practice, not all plain-background videos and simplebackground videos are reused across conditions. In addition, we also include videos that are uniquely constructed to appear only in the plain-background condition or only in the simple-background condition. This design choice is intended to increase the diversity of the video data. Reusing identical base scenes improves the efficiency of dataset construction, but it may also introduce potential biases or unwanted correlations when evaluating VLMs, so we deliberately balance such reuse with the creation of novel, non-paired scenes. For the objects in each scene, we specify target object sizes, velocities, and accelerations using real-world statistics gathered from online references (e.g., typical dimensions and speeds of vehicles, animals, and more). Whenever possible, moving objects are modeled at real-world scale; in rare cases where real-world-scale objects yield motion trajectories that are barely discernible in the rendered videos, we apply uniform scaling to increase perceptual visibility while preserving the underlying physical relationships. An example is the ice cube falling into cup scene illustrated in the Figure 25. If we model the cup and cube at their real-world dimensions, the cube traverses the cam13 Figure 17. Keyframed animation-curve example. The animation curve controlling the swim ring in Figure 16. The ring moves forward along manually authored trajectory, while small randomized perturbations in translation and rotation are added to imitate the visual appearance of floating motion. the priors specified in the prompt, or whether they instead rely predominantly on generic pre-trained knowledge and commonsense expectations about how such objects should behave. Object Motion Design and Implementation. As discussed in subsubsection C.1.2, our motion design follows two main paradigms: keyframed motion and physics-driven motion. The construction of physics-driven scenes is illustrated in Figure 15. For the keyframed cases, in addition to manual editing in the Graph Editor, we also use scripts to automate the animation process. We distinguish two main classes of scripted motion: (i) analytic one-dimensional motion along single axis, and (ii) curve-following motion along human-designed paths. For analytic 1D motion, we explicitly encode standard kinematic equations and bake the resulting trajectories as keyframes. Given an object with initial world-space position x0, target duration , and desired acceleration along the axis, our script iterates over frames and computes the physical time = START FRAME FPS . The corresponding displacement is computed using the constant-acceleration formula s(t) = 1 2 at2. Figure 18. Physics-driven rigid-body example. Rigid-body simulation of objects such as the bowling pins reacting to the impact of an incoming ball in Figure 15. The pins motion, tipping, and scattering are governed entirely by Newtonian dynamics and collision responses within the physics engine. era frustum in essentially single frame, making its falling trajectory almost invisible to observers and thus providing little signal for quantitative evaluation. To address this, we uniformly enlarge both the cup and the ice cube by factor of 10. After scaling, the cube remains visible from frame 11 to 22, and the rendered video reveals clear multi-frame trajectory. This controlled rescaling not only makes the motion measurable but also allows us to probe whether VLMs recover physical quantities from the observed kinematics given 1 2 3 disp_y = 0.5 * ACCEL * (t ** 2) new_y obj.location = Vector((start_loc.x, new_y, = start_loc.y - disp_y start_loc.z)) (cid:44) obj.keyframe_insert(data_path=\"location\", (cid:44) index=-1, frame=f) This directly realizes s(t) = 1 2 at2 in world coordinates. For constant-velocity motion, we instead use the linear rela14 Figure 19. Examples of indoor scene. Figure 20. Examples of outdoor scene. Figure 21. Examples of microscopic scene. Figure 22. Examples of extraterrestrial scene. 15 Figure 23. Examples of building background scene. Figure 24. Examples of building background scene. tion: implemented via s(t) = vt, 1 2 disp_y = VEL * new_y = start_loc.y - disp_y while keeping the same frame loop and keyframe insertion logic. After all keyframes are written, we programmatically set each F-curves interpolation mode to LINEAR, overriding Blenders default Bezier interpolation to ensure that the frame-to-frame displacement matches the analytically specified velocity or acceleration profile rather than being inadvertently smoothed or distorted. For more complex motions along curved paths, we first author an approximate trajectory in Blender (e.g., by manually keyframing car following road or particle moving along spiral), and then reparameterize this path in Python to obtain physically interpretable kinematics. Concretely, we sample the objects world-space position at each frame of the original animation: 1 2 3 4 5 positions = [] for in range(frame_start, frame_end + 1): scene.frame_set(f) pos = obj.matrix_world.translation.copy() positions.append(pos) 1 2 3 4 6 distances = [0.0] for in range(1, len(positions)): = (positions[i] - positions[i-1]).length distances.append(distances[-1] + d) total_length = distances[-1] duration = (frame_end - frame_start) / fps We then derive desired kinematic profile in terms of traveled distance s(t) along the curve. For constant-speed motion, we set = L/T with = total length and use s(t) = vt, implemented as 1 target_dist = speed * which corresponds to s(t)=vt. For constant-acceleration motion along the same curve, we instead use the quadratic form s(t) = v0t + 2 at2, with user-specified v0 and a. In both cases, for each frame we find the segment [i 1, i] such that distances[i 1] s(t) distances[i], and linearly interpolate between the sampled positions: We then compute the cumulative arc length along this polyline. 1 2 ratio = (target_dist - distances[i-1]) / (distances[i] - distances[i-1]) (cid:44) new_loc = positions[i-1].lerp(positions[i], (cid:44) ratio) 16 Figure 25. Examples of scaled scene. 4 obj.location = new_loc obj.keyframe_insert(data_path=\"location\", (cid:44) frame=f) Finally, as in the analytic case, all resulting keyframes are set to linear interpolation. This arc-length reparameterization scheme lets us reuse authored trajectories while imposing well-defined kinematic profiles (constant velocity, constant acceleration, or other timedistance schedules) in physical units. Together with the untouched native animations, these scripted motions give our benchmark wide spectrum of motion patterns, from simple 1D acceleration to complex, irregular dynamics, under tight quantitative control. Rendering. All Blender-generated videos in QUANTIPHY are rendered with the Cycles and EEVEE path-tracing engine using physically based workflow. Videos use multiple spatial resolutions, including 1920 1080 (16:9), 1080 (1:1), 480 960 (vertical), as well as several additional intermediate sizes, so that models see both landscape, square, and portrait-style content. The temporal sampling is likewise heterogeneous: frame rates in the benchmark include 24, 25, 30, 33, 60, and 120fps. Frames are exported via FFmpeg as MP4 files with H.264 compression and constant frame rate, without any interpolation or stabilization, so each frame aligns exactly with the simulated timeline and ground-truth annotations. Camera parameters are also varied to increase visual diversity. Intrinsics cover range of focal lengths from wide to normal and telephoto views, and extrinsics place the camera at different heights, offsets, and azimuth/elevation angles around the scene, producing both frontal and oblique perspectives on object motion. Lighting setups combine different environment maps and local light sources, and objects are assigned broad set of physically based materials (e.g., diffuse, 17 glossy, metallic, translucent), leading to diverse shading, reflection, and contrast conditions. Together, these choices give QUANTIPHY wide coverage over resolutions, frame rates, viewpoints, and appearance statistics, while the underlying physical trajectories and numeric ground truth remain precisely controlled. C.2. Lab Capturing To further complement the diversity of QUANTIPHY with real-world data, major part of our dataset comes from customized motion capture system in lab environment. Due to the nature of real-world data, lab captured videos contribute only to the category of 3D. MoCap setup. We use four Orbbec Femto Mega cameras to construct our customized motion capture system. The specifications of the cameras are shown in Table 5. We designed two different camera setups and object arrangements in the MoCap system. The first setup covers smaller spatial range, with the main camera placed close to the objects. In this setup, we capture motions of small objects such as tennis ball, book, and ping pong ball moving on desk. The second setup covers larger range and is used to capture larger-scale motions such as basketball bouncing and accelerated motion of trash bin. For both setups, we initialize the camera extrinsics via multi-view calibration with checkerboard. Collection workflow. For efficient video capture, we developed tool with user-friendly interface that supports synchronized recording across all four cameras. We collect videos based on list of predefined questions. For each question, we set up the scene and objects without moving the cameras and then perform specific object motions according to the question. Three people were involved in the lab data collection workflow: one person was responsible for operating the tool (starting and pausing capture), and two people were responsible for setting up the scenes and executing the motions. Unlike Blender simulation, capturing real-world data makes it difficult to directly manipulate specific scene parameters. We therefore manually configured the scenes to meet target conditions, such as the slope angle or the frequency of pendulum motion, using careful measurements within set tolerance. For motions that require controlled initial velocity, we used motor running at constant speed to pull the object and provide the desired initial velocity. Post-processing. After capturing the raw videos, we obtain .bag files for each camera that contain the binary recordings. We follow the official camera SDK to decode the raw data. From this decoding, we obtain: (1) intrinsics for each camera, (2) timestamped RGB streams for each camera, (3) timestamped depth streams for each camera, and (4) relative extrinsics from the depth cameras to the RGB cameras. We then apply coordinate transformations to reproject the depth data into the image coordinates of the RGB videos, which allows pixel-wise depth values to be read consistently. With aligned image coordinates, it becomes possible to read out specific depth values for target objects (with respect to the main camera). straightforward approach is to use segmentation masks for the target object and average the depth values over the masked pixels. However, this automated approach did not work well in practice because the depth information from the camera is often incomplete and ambiguous, especially at pixels with high motion, due to hardware limitations. As result, we could not reliably automate depth extraction for the target objects using only segmentation masks. To ensure high-quality depth measurements, we developed UI-based tool that overlays the transformed depth map on top of the RGB frames. The tool allows users to click on the frame to read out exact depth values from the main camera at the selected pixels. An illustration of this UI-based tool is shown in Figure 26. List of objects used. Table 6 lists all physical objects used in our controlled lab videos, together with their measured dimensions. These objects serve either as priors (with known size) or as inference targets in our kinematic tasks. All measurements are taken with ruler or caliper in metric units before filming. C.3. Internet Scraping Our internet split in QUANTIPHY consists of real-world videos captured by commodity cameras, and is constructed from two sources that we treat under unified category: (i) open-source online video platforms, contributing 42 clips; and (ii) videos recorded by the authors using smartphone rear cameras in everyday indoor and outdoor environments, contributing 30 clips. All of these videos are direct camera recordings of natural scenes, and thus closely reflect the statistics and imperfections of the physical world. Why only 2D inference from internet data. Unlike our simulation and controlled-lab settings, internet videos do not come with calibrated depth, camera intrinsics, or precise object geometry. In particular, reliable metric depth is almost impossible to obtain for arbitrary internet footage. For this reason, we restrict internet videos to 2D kinematic inference tasks. For each selected clip, we manually construct pixel ruler, measure pixel-level size/position trajectories of the objects of interest, and then use an approximate scale factorderived from obvious real-world references in the scene (e.g., gravity = 9.8 m/s2, the length of credit card, lane width on road, or the speed of an airport conveyor belt)to convert these pixel measurements into world-space 18 Depth Technology"
        },
        {
            "title": "Time of Flight",
            "content": "Wavelength 850 nm Depth Range Depth Resolution/FPS Up to 1024 Depth FOV *0.255.46 (depending on depth mode) 1024@15 fps (WFOV), 640 576@30 fps (NFOV) RGB Resolution/FPS RGB FOV Processing IMU 2160@25 fps 120 120 (WFOV), 75 65 (NFOV) Up to 3840 80 51 NVIDIA Jetson Nano Supported Table 5. Specifications of the cameras used in our customized motion capture system. Figure 26. The users interface of the tool we have developed for obtaining depth value. kinematic quantities. While this procedure yields reasonably accurate ground truth, it is inherently less precise than the annotation pipelines used for our simulation and lab data. Consequently, we intentionally keep the proportion of internet data moderate in the overall benchmark. Videos from open-source platforms. We choose opensource video platforms primarily because they avoid copyright issues, offer relatively high image quality, and provide diverse content. However, videos that satisfy the three screening criteria in subsection B.1 (static camera, at least one rigid object undergoing translational motion, and planar motion for 2D tasks) are relatively rare. Beyond these core constraints, we additionally require that each candidate clip contain at least one visually obvious physical prior that can be reasonably assumed or measured (e.g., gravity, credit-card-sized object, standard lane width, or known conveyor-belt speed). These additional constraints further narrow the pool of usable videos. There is currently no offthe-shelf automatic pipeline for this selection process, so all internet clips are hand-picked by project members, who visually inspect candidates for compliance with our physical and annotation requirements. Representative examples are shown in Figure 27. Author-recorded videos. Because suitable clips on opensource platforms are scarce, we complement them with 30 videos recorded by the authors using smartphone rear cameras in variety of everyday scenes, including parking lots, road traffic, bedrooms, and indoor/outdoor sports venues. During recording, we enforce fixed camera viewpoint and ensure that at least one rigid object exhibits predominantly 19 Objects Dimensions 2 green tennis balls 2 pink tennis balls 2 white ping pong balls 1 purple yoga ball 1 soccer ball 1 basketball 1 red plastic ball 1 orange plastic ball 1 small whiteborad (slope) 1 large whiteborad 1 trashbin 1 tape 1 white food box stuffed toy 1 stuffed toy 2 stuffed toy 3 1 toy cookie 1 cosmetic jar 1 glass jar 1 white cup 1 pink water bottle 1 marker 1 black pen 1 green notebook 1 white-covered book 1 pencil case 1 credit card 1 deck of poker card = 6.7 cm = 6.7 cm = 4 cm = 52.2 cm = 17.5 cm = 23.2 cm = 7 cm = 5.7 cm 30.8 cm 23.2 cm 0.5 cm 81 cm 60 cm 1 cm 34.5 cm 89.1 cm 59.9 cm 10.1 cm 10.1 cm 4.8 cm 19.5 cm 6.0 cm 7.2 cm 14.5 cm 11 cm 8 cm 29.5 cm 15.0 cm 10.0 cm 8.1 cm 6.4 cm 19.6 cm 8.0 cm 8.0 cm 1.8 cm 7.2 cm 7.2 cm 6.0 cm 6.8 cm 6.8 cm 8.5 cm 8.6 cm 8.6 cm 14.5 cm 6.7 cm 6.7 cm 19.5 cm 13.5 cm 0.8 cm 0.8 cm 14.5 cm 1. 2 cm 1.2 cm 18.6 cm 8.8 cm 1.5 cm 12.5 cm 19.5 cm 2.2 cm 6.7 cm 7.2 cm 21 cm 85.6 mm 53.98 mm 9.8 cm 6.3 cm 1.8 cm Table 6. List of physical objects and their measured dimensions used in our lab-captured videos. Figure 27. Examples of videos from open-source platforms. translational motion, so that the criteria in subsection B.1 are satisfied. In many such settings, precise physical quantities (e.g., vehicle speed, basket height) cannot be directly measured. We therefore annotate these videos using the same pixel-ruler and approximate-scale procedure as for online platform videos, again restricting them to 2D inference tasks. Example frames from these author-recorded clips are shown in Figure 28. Privacy and anonymization. For all internet videos, whether sourced from open platforms or recorded by the authors, we manually inspect frames for sensitive personal information. Whenever faces, license plates, or other identifying details appear, we apply blurring or masking before including the clip in the dataset. This ensures that our internet data respect both copyright and privacy constraints while still providing realistic real-world scenarios for quantitative kinematic inference. Figure 28. Examples of self-recorded videos with identity removed. C.4. Segmented Data In QUANTIPHY, we aim to benchmark the capabilities of VLMs on videos with diverse background types. Video backgrounds often contain contextual information that may either aid or hinder the inference of target objects physical properties. To comprehensively study this impact, we design experiments that isolate the target object by completely removing the background. Specifically, we compare the original videos against processed versions where the background has been completely denoised and removed using segmentation model. State-of-the-art semantic segmentation models, such as SAM 2 [32], have demonstrated robust capabilities in tracking and segmenting arbitrary targets across various tasks. In 20 Figure 29. The users interface of the segmentation tool we have developed. our experiments, we employ SAM 2 as the backbone segmenter, utilizing hybrid prompting strategy. To automate the pipeline, we leverage Grounding DINO 1.5 [34] to localize target objects using textual descriptions derived from our video-question pairs. The bounding boxes generated by Grounding DINO serve as box prompts for SAM 2, enabling automatic segmentation. For complex scenes with multiple objects where the automated pipeline may fail, we incorporate manual intervention by providing point-based prompts to SAM 2. To streamline this workflow, we developed custom UI-based tool that allows multiple annotators to label objects efficiently, thereby scaling the segmentation process. Figure 29 illustrates the user interface of this tool, and Figure 30, Figure 31, Figure 32 provides exemplary frames of the segmented videos for reference. C.5. Quality Control To maximize data quality, we manually excluded videos exhibiting excessive motion blur, severe occlusion of the target object, or objects that are difficult to model. Furthermore, we removed videos containing identifiable human subjects to ensure ethical compliance. Following this review process, approximately 3% of the Blender data and 30% of the lab data were discarded, while for the videos scraped from the internet only 72 clips were retained. C.6. Ethical Considerations In compiling the QUANTIPHY dataset, comprising both simulated Blender environments and web-sourced videos, we strictly adhere to all relevant copyright and licensing regulations. For specific data assets requiring attribution, we provide full acknowledgments in the supplementary materials. The data collection process for QUANTIPHY is highly diverse and complex. Since the data originates from heterogeneous sources with varying characteristics, establishing universal automated protocol for quality assurance is challenging. To address this, we incorporated an additional manual review stage for all candidate data. We implemented strict privacy measures throughout the data collection and annotation phases to ensure that no personally identifiable information (PII) is retained in the dataset. Relevant Institutional Review Board (IRB) documentation is available upon request. Furthermore, QUANTIPHY complies with ethical guidelines regarding content 21 safety; we have rigorously screened the data to exclude biased or harmful content while prioritizing diversity to foster fairness and inclusivity. D. Details of Data Annotation D.1. Blender Simulation We annotate five basic types of physical quantities: size, displacement over given time interval, velocity, acceleration, and depth and distance. D.1.1. Size Blender size annotations are extracted directly from Blenders internal measurement readout to guarantee numerical accuracy. For most objects, size is obtained from the objects axis-aligned bounding box dimensions in world space. In other special cases, the dimensions of objects may change from frame to frame. For example, as shown in the Figure 33, articulated humans height changes while they move, thus we explicitly measure three configurations within each clip: 1. Rest standing height: the height in rest T-pose frame, where the character stands upright and fully extended. 2. Minimum apparent height during walking: the smallest height observed over the walking segment of the clip. 3. Maximum apparent height during walking: the largest height observed over the same segment. 1 3 4 5 6 All heights are obtained by selecting the corresponding frames in Blender and measuring the vertical extent in world coordinates. In practice, these three measurements are usually very close (often differing by only few centimeters), but we still record all of them to keep the annotation protocol precise and consistent. Similarly, for other rigged objects (such as birds and insects), we track their width frame by frame over the flight segment of the clip and only record the smallest and largest width observed. See details in Figure 34. This is because, unlike the human case, these assets may not admit clear, static rest standing pose in which the animal is fully extended. As result, we do not define separate rest measurement for flying objects. Correspondingly, the inference questions and prior ground truth are phrased with the same level of precision. For human figures, we explicitly distinguish between the standing height at rest and the minimum and maximum height observed while walking. For flying animals, we clearly refer to the minimum or maximum width attained during flight. D.1.2. Displacement or Path. Timeframe alignment across scenes. In Blender, each scene specifies render range with scene.frame start and scene.frame end, and scene.frame start is not required to be 0. We define time = 0 to occur at the render start frame, regardless of its numeric index. More precisely, if scene has frame rate frames per second and render start frame fstart, then the first frame of the exported video (timestamp = 0 s) is Blender frame fstart, the frame at time = 1/f is frame fstart + 1, and so on. When question specifies time interval [tstart, tend] in seconds, we convert it to Blender frame indices by istart = round(cid:0)fstart+f tstart iend = round(cid:0)fstart+f tend (cid:1), (cid:1). This convention works uniformly for all scenes, including those whose render ranges begin at non-zero frame numbers (e.g., frame start = 20, 40, 60, ...). In code, the conversion has the following form: = bpy.context.scene = scene.render.fps scene fps f_start = scene.frame_start # may be 0, 20, 40, ... (cid:44) def time_to_frame(t_sec: float) -> int: return round(f_start + t_sec * fps) When an interval is specified directly in frames, we simply take (istart, iend) as given and interpret the corresponding times as tstart = fstart istart , tend = iend fstart , so that = 0 always aligns with the first frame of the rendered video, irrespective of the absolute Blender frame index. At any frame i, the scripts query the world-space position of the target object (e.g., the animated rigid body) via 1 scene.frame_set(i) loc = obj.matrix_world.translation.copy() (cid:44) # (x, y, z) in world space yielding vector p(i) = (x(i), y(i), z(i)) R3. Displacement annotations. labels, we consider time interval [tstart, tend] or frame interval [istart, iend]. The scripts record the world-space locations For displacement pstart = p(tstart) = p(istart), pend = p(tend) = p(iend), and form the displacement vector All displacementor path-related labels are computed from object trajectories in Blender world space using internal custom Python scripts. = pend pstart = (x, y, z). From this vector we derive two scalar quantities. 22 Figure 30. Examples of segmented blender data. After segmentation, we can replace the background image freely. Figure 31. Examples of segmented lab data. After segmentation, we can replace the background image freely. Figure 32. Examples of segmented internet data. After segmentation, we can replace the background image freely. 1. The full 3D displacement D3D = 2 = (cid:112) x2 + y2 + z2, 2. The planar displacement in 2D plane, obtained by projecting the motion onto that plane. For any chosen 2D coordinate plane with axes (u, v), we define the planar displacement as In our benchmark we primarily use the three canonical planes: the horizontal ground plane (cid:112) DXY = (x)2 + (y)2, and the two vertical planes (cid:112) DXZ = (x)2 + (z)2, DYZ = (cid:112) (y)2 + (z)2. (cid:112) D2D = (u)2 + (v)2. compact version of the core computation is: 23 Figure 33. Examples of human height measuring. Figure 34. Examples of flying animal measuring. def displacement_world(obj, frame_start, (cid:44) frame_end, scene): # sample world-space positions at start and end (cid:44) loc_start = get_world_location_at_frame(obj, frame_start, scene) (cid:44) loc_end = get_world_location_at_frame(obj, (cid:44) frame_end, scene) # displacement vector disp_vec = loc_end - loc_start dx, dy, dz = disp_vec.x, disp_vec.y, (cid:44) disp_vec.z 1 3 4 5 6 7 9 10 # 3D displacement and horizontal (XY) (cid:44) displacement D_3D = disp_vec.length sqrt(dx*dx + dy*dy + dz*dz) (cid:44) D_XY = (dx**2 + dy**2) ** 0.5 (cid:44) sqrt(dx*dx + dy*dy) # # return { \"frame_start\": frame_start, \"frame_end\": frame_end, \"loc_start\": \"loc_end\": \"dx\": dx, \"dy\": dy, \"dz\": dz, \"D_3D\": D_3D, \"D_XY\": D_XY, loc_start, loc_end, } For each annotated interval, we store: 11 13 14 15 16 17 19 20 21 22 24 the frame indices (istart, iend); the corresponding times (tstart, tend), computed relative to the render start frame as above; the start and end world-space coordinates pstart, pend; the displacement components (x, y, z); the two scalar values D3D and DXY in Blender units. In the question text, we explicitly state whether displacement refers to the full 3D displacement D3D or to planar displacement D2D in specified plane (e.g., horizontal DXY or vertical DXZ / DYZ). The posterior ground-truth answer for that question is taken from the corresponding stored value. 13 14 16 17 18 19 D.1.3. Velocity and Acceleration. Velocity measurement and uniform-speed diagnostics. Using the timeframe convention introduced above, let R3 denote the world-space origin of the target obpi ject at frame i, obtained by querying obj.matrix - world.translation after calling scene.frame - set(i). For two consecutive frames 1 and with timestamps ti1 and ti, we define the temporal spacing ti = ti ti1, and the instantaneous 3D velocity and scalar speed at frame as vi = pi pi1 ti , si = vi 2, where vi = (vx,i, vy,i, vz,i) is the full 3D velocity vector in world coordinates and si = vi 2 = (cid:113) v2 x,i + v2 y,i + v2 z,i is its Euclidean norm. All operations are performed in Blender world space and use the true frame rate, so that positions pi, velocities vi, and speeds si are expressed in self-consistent system of physical units determined by the underlying scene scale. The Blender analysis script computes these quantities frame by frame and writes them into text block in Blenders Text Editor: 1 2 3 4 5 7 8 9 10 11 = None prev_loc prev_frame = None for frame in range(frame_start, frame_end + 1): scene.frame_set(frame) loc = obj.matrix_world.translation.copy() # (cid:44) p_i if prev_loc is None: vel_vec = Vector((0.0, 0.0, 0.0)) speed = 0.0 else: dt (cid:44) = (frame - prev_frame) / fps # delta t_i disp = loc - prev_loc # p_i - p_{i-1} (cid:44) vel_vec = disp / dt # v_i (cid:44) speed = vel_vec.length (cid:44) # s_i write_per_frame_entry(frame, loc, vel_vec, speed) (cid:44) prev_loc prev_frame = frame = loc For each frame in the chosen range, this loop automatically prints one line of kinematic data into the text block, containing: the frame index and its timestamp ti (relative to the render start frame); the world-space position pi; the full 3D velocity vector vi; the scalar speed si. Any question that refers to the speed at seconds or the velocity at frame is answered by mapping the queried time to frame index using the same timeframe conversion and then reading off the corresponding per-frame entry from this table, without any manual measurement or additional approximation. si In addition to per-frame velocities, we use the sequence of speeds to characterize simple motion regimes at the clip level. Let Iv be the index set of frames for which speed value is defined (all frames except the very first one), and let Nv = . We compute the mean speed Iv { } = 1 Nv (cid:88) iIv si and the maximum relative deviation from this mean, δspeed = 0, if = 0, max iIv si s , otherwise. Given user-specified tolerance τv (parameter UNIFORM - SPEED TOLERANCE, e.g., τv = 0.01), we classify the clip as: no effective motion if 0 (numerically = 0, indicating that the object is essentially stationary); approximately uniform speed if > 0 and δspeed τv; non-uniform speed otherwise (speed fluctuations exceed the tolerance). This logic matches the following code fragment: 1 avg_speed max_dev = sum(speeds) / len(speeds) = max(abs(s - avg_speed) for in (cid:44) speeds) 25 4 5 6 7 8 10 11 12 13 14 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 max_rel_dev = max_dev / avg_speed if avg_speed != (cid:44) 0 else 0.0 if avg_speed == 0: # almost no motion ... else: if max_rel_dev <= UNIFORM_SPEED_TOLERANCE: # approximately uniform speed ... else: # not uniform-speed ... Acceleration measurement and acceleration diagnostics. Starting from the third frame, we derive scalar acceleration sequence that describes how quickly the objects speed changes over time. Given the per-frame speeds defined } { above, and the same temporal spacings ti = ti ti1, we define for all frames si 2 ai = si si ti , so that ai measures the finite-difference rate of change of speed between frames 1 and i. The script computes these values alongside the speeds: prev_speed = None prev_frame = None for frame in range(frame_start, frame_end + 1): scene.frame_set(frame) loc = obj.matrix_world.translation.copy() if prev_frame is None: dt = 0.0 speed = 0.0 accel = None else: = (frame - prev_frame) / fps = loc - prev_loc dt disp vel_vec = disp / dt speed = vel_vec.length if prev_speed is None: accel = None else: accel = (speed - prev_speed) / dt # (cid:44) a_i write_per_frame_entry(frame, speed, accel) prev_loc prev_speed = speed prev_frame = frame = loc For each frame in the valid range, this produces scalar acceleration value ai (undefined at the first two frames), which we store in the same text table. Any question that refers to the acceleration at frame or to the acceleration over given interval is answered by mapping the queried time to frame index and reading off the corresponding ai entry. To summarize acceleration behaviour over the entire clip, we perform an analogous diagnostic analysis on the set of defined accelerations. Let Ia be the index set of frames for which ai is defined (starting from the third frame), and let . We compute the mean acceleration Na = Ia = 1 Na (cid:88) iIa ai and the maximum absolute deviation from this mean, max = max iIa ai . a If is larger than small absolute threshold εmin (parameter MIN ABS ACCEL), we also measure the maximum relative deviation δaccel = 0, max , εmin, if otherwise. Given user-specified tolerance τa (parameter UNIFORM - ACCEL TOLERANCE), the script then classifies the acceleration regime as: near-zero acceleration if εmin; in this case the overall acceleration is negligible and the clip is better interpreted as approximately constant-speed (or almost static); approximately uniformly accelerated if > εmin and τa; the sign of further distinguishes uniform δaccel acceleration (a > 0) from uniform deceleration (a < 0); irregular acceleration otherwise, meaning that the accela erations fluctuate significantly around their mean. This classification logic is implemented as: avg_accel max_dev_a = sum(accels) / len(accels) = max(abs(a - avg_accel) for in accels) (cid:44) if abs(avg_accel) > MIN_ABS_ACCEL: max_rel_dev_a = max_dev_a / abs(avg_accel) else: max_rel_dev_a = 0.0 if abs(avg_accel) <= MIN_ABS_ACCEL: # acceleration is very small -> closer to uniform speed / almost static (cid:44) ... else: if max_rel_dev_a <= UNIFORM_ACCEL_TOLERANCE: if avg_accel > 0: # approximately uniformly accelerating (cid:44) ... elif avg_accel < 0: 1 2 3 4 6 7 8 9 10 12 13 14 15 16 17 18 19 20 21 23 24 # approximately uniformly decelerating (cid:44) ... else: # numerically close to zero ... else: # not uniformly accelerated ... si { ai { or These acceleration diagnostics never modify the under- , just as the velocity diagnoslying frame-level values } vi tics never modify Instead, they provide { principled, threshold-based way to tag each clip as approximately constant-speed, uniformly accelerated, uniformly decelerated, or irregular, allowing us to phrase velocityand acceleration-related questions at level of precision that matches the actual motion regime present in each clip. . } } D.1.4. Depth and Distance All geometric quantities used in our benchmark are computed directly inside Blender in world coordinates via Python script that evaluates the scene frame by frame. The script reads the world-space transforms of selected entities, computes Euclidean distances, and logs per-frame table into Blender Text Editor text block. From this table, we derive two kinds of quantities: (i) depth metadata between objects and the camera, and (ii) inter-object distances in 3D and in projected 2D planes. Depth metadata (objectcamera). We extract depth values in Blender only as auxiliary geometric metadata, not appearing in the inference question text or supervised targets. For any object with world-space position pt(o) = (xo , yo , zo ) R3 and the active camera object with world-space origin = (xcam pcam , ycam , zcam ) R3, at time t, we define the depth as the 3D Euclidean distance ddepth (o) = (cid:13) (cid:13)pt(o) (cid:112) (xo = pcam xcam (cid:13) (cid:13)2 )2 + (yo ycam )2 + (zo zcam )2. During annotation, we treat ddepth (o) as time series and often materialize it at small set of shared reference time points (e.g., t1 = 1s, t2 = 2s). Thus, for moving object we record pairs such as (cid:0)ddepth t1 (o), ddepth t2 (o)(cid:1), and we use the same time points for other relevant entities in the scene (e.g., additional moving objects or candidate target 27 objects). This temporal alignment ensures that when we expose depth values as priors, they are always comparable across objects at exactly the same timestamps. For static objects, the depth is constant over the clip, so the values at different time points are trivially identical. Crucially, we never use these depth values as evaluation targets. We do not ask questions of the form What is the distance between object and the camera at time t?, and we do not treat ddepth (o) as ground truth in any task. Instead, depth is used only as internal geometric metadata and, in some infer questions, as numeric priors that can help VLM reason about the relative 3D configuration of the scene. Even in those infer questions, we do not necessarily expose the cameratarget depth of the queried object itself; rather, we expose subset of aligned depth values for selected objects. All exposed depth values are manually reviewed to ensure that, together with the visual information, they are logically sufficient to infer the correct answer. 3D inter-object distances. We use the same Blender script to annotate 3D distances between pairs of entities. In our benchmark, these entities are either (i) two moving objects, or (ii) moving object and static reference object in the scene. We do not treat objectcamera distances as interobject ground truth; camera-related distances only appear as depth metadata as described above. , ya Let pt(a) = (xa and pt(b) = (xb , zb , za ) ) denote the world-space positions of entities and at time t. The 3D objectobject distance is pt(b)(cid:13) (cid:13) t, yb (a, b) = (cid:13) d3D (cid:13)pt(a) (cid:113) = (xa t)2 + (ya xb )2 + (za yb zb )2. For objects whose dimensions do not change over time, we take pt(o)to be the Blender object origin in world coordinates. For articulated skeletal models (e.g., humans), transformations are defined on set of bones rather than single rigid body. In these cases, we first enumerate all bones in the armature and select semantically stable reference joint (for humans, typically the pelvis/hip bone) as the anchor. All distances involving that character are then defined using the world-space position of this reference bone, which provides temporally stable and semantically meaningful notion of where the character is. For some non-humanoid articulated assets or composite rigs where no single point or bone has clear semantic interpretation as the character center (e.g., certain vehicles or multi-part machines), we instead introduce an auxiliary helper object. Concretely, we create an Empty object rigidly parented to the rig root, snap its origin to the root in world space, and log its world-space position pt(h) at each frame. In those scenes, all distances involving the articulated asset are computed with respect to pt(h) rather than specific bone. This dual strategy (bonebased anchors for humanoid characters and helper-object anchors for other articulated assets) keeps the distance annotations consistent while accommodating the diversity of rig structures in our Blender scenes. Planar (2D) distances. Some tasks explicitly constrain distance reasoning to 2D projection, for example horizontal distance in the ground plane or distance in the vertical cross-section. To support such tasks, we also annotate planar distances by projecting 3D positions onto chosen coordinate plane Π . We define the projections XY, XZ, YZ } { ΠXY(x, y, z) = (x, y), ΠXZ(x, y, z) = (x, z), ΠYZ(x, y, z) = (y, z). The planar distance between and at time is then dplane (a, b) = (cid:13) (cid:13)Π(cid:0)pt(a)(cid:1) Π(cid:0)pt(b)(cid:1)(cid:13) (cid:13)2. Whether distance question is treated as 2D or 3D in our benchmark is fully determined by (i) the infer question and (ii) the geometric category of the underlying video (2D vs. 3D sequence). Each video is assigned to 2D or 3D split based on how it is generated and how its geometry is intended to be interpreted. For videos in the 2D split, distances are interpreted in single plane by construction. If question in such video simply asks for the distance between objects and b, the ground-truth target is the planar distance dplane (a, b), and the task is labeled as 2D. For videos in the 3D split, we have full world-space geometry. If the infer question explicitly restricts reasoning to plane (e.g., horizontal distance or vertical distance), we again use the planar distance dplane (a, b) and label the task as 2D. Otherwise, distance questions on 3D videos default to the full 3D Euclidean distance dt(a, b) defined above, and these tasks are labeled as 3D. Blender implementation and per-frame logging. All depth and distance quantities above are computed by unified Blender Python script. For specified frame range [fmin, fmax], the script iterates over frames, queries worldspace positions, computes distances, and writes formatted table into Blender Text Editor text block. The core of the script is: import bpy from mathutils import Vector scene = bpy.context.scene fps = scene.render.fps 1 2 3 5 6 def world_pos(obj_or_bone): \"\"\"Return world-space translation of an object or specific bone.\"\"\" (cid:44) # For regular objects: if hasattr(obj_or_bone, \"matrix_world\"): return obj_or_bone.matrix_world.to_trans (cid:44) lation() # For pose bones (e.g., armature.pose.bones[\"Hips\"]): (cid:44) return obj_or_bone.matrix.to_translation() for in range(f_min, f_max + 1): scene.frame_set(f) # Example: distance between entities and p_a = world_pos(entity_a) p_b = world_pos(entity_b) # Full 3D distance in world coordinates dist_3d = (p_a - p_b).length # Example planar distance in the XY plane (horizontal separation): (cid:44) # other planes (XZ, YZ) are obtained analogously in the full script. (cid:44) p_a_xy = Vector((p_a.x, p_a.y, 0.0)) p_b_xy = Vector((p_b.x, p_b.y, 0.0)) dist_xy = (p_a_xy - p_b_xy).length # Timestamp (seconds), assuming = 0 at f_min (cid:44) = (f - f_min) / fps # Log one line (frame, time, 3D distance, XY-plane distance, ...) (cid:44) write_to_text_block(f, t, dist_3d, dist_xy) The same script is run with different choices of entity - and entity to (i) record objectcamera distances as depth metadata and (ii) generate objectobject distance annotations. In the code listing above, we show the XY-plane case for concreteness. In practice, this block is used as template: when planar distances in other coordinate planes (XZ or YZ) are needed, we simply modify the projection lines accordingly (e.g.,Vector((p a.x, a.z, 0.0)) for XZ or Vector((0.0, a.y, a.z)) for YZ) and rerun the script for that scene. All measurements are derived directly from Blenders world-space transforms, so the numeric values used in our questions and annotations match the underlying scene geometry exactly. Depth values are stored only as auxiliary geometric metadata or optional numeric priors, whereas interobject distances serve as the ground truth posterior. D.2. Lab Data Annotation Unlike Blender-based simulations, lab captures do not provide annotations that can be directly read out from software. Nevertheless, with our multi-stereo camera setup, we are able to reconstruct both the 3D scene and the 3D geometry of the target objects. 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 28 To obtain annotations for lab captures, we first attempted to use off-the-shelf 3D reconstruction models such as FoundationPose [44] to assist with annotation. However, in our experiments, we found that even state-of-the-art AI models struggled to localize objects stably in world coordinates and frequently failed under occlusions. Therefore, we instead rely on traditional geometry-based reconstruction with calibrated camera poses. We use the main camera together with manually selected metric depth values to recover the objects world coordinates. Concretely, we use the UI-based annotation tool introduced earlier (see Figure 26) to manually click on the center of the target object across sequence of frames in the video. The tool records the 2D coordinates of each click in image space together with the corresponding metric depth from the main camera. Using the calibrated camera intrinsics, we then back-project the annotated object centers into the world coordinate system. To obtain static priors such as object size, we manually measure the shape and dimensions of each object to reduce measurement error. To obtain dynamic priors such as object velocities and accelerations, we annotate the object center across at least 5 adjacent frames (for smoothing purpose) and compute the instantaneous velocity and acceleration magnitudes from the resulting sequence of world coordinates, assuming constant frame rate. Formally, the computation of dynamic priors in lab captured videos are: vworld aworld xworld k+1 xworld k+1 xworld , 2xworld t2 + xworld , where is determined from the frame rate of the captured videos. Note that, based on our predefined questions and the priors required at specific time stamps, not all frames of the lab videos need to be annotated. This greatly reduces the annotation workload and helps prevent error accumulation over time. When the object is occluded in the main camera view, we instead estimate its world coordinates by averaging the positions obtained by transforming its 2D annotations from the other three cameras into the world coordinate system. D.3. Internet Data Annotation For internet videos, we must derive kinematic ground truth from raw pixels rather than from simulator logs or calibrated sensors. To keep the process systematic, we adopt threestage annotation workflow: (i) metadata and task specification, (ii) pixel-level measurement with custom video tool, and (iii) conversion from pixel-space kinematics to real-world quantities. 29 Metadata and task specification. Each collected internet clip is first assigned unique identifier (e.g., internet - 0001). Based on the scene content, annotators determine the physical prior available in the video (e.g., gravity = 9.8 m/s2, the length of credit card, lane width on road, or conveyor-belt speed), and label the corresponding video type code (e.g., S2MC, A2SS) as described in subsection B.2. We then identify all feasible inference targets in the clip (e.g., vehicle speed, ball acceleration, object size) and enumerate kinematic inference questions that can be solved from the chosen prior. For each question, we annotate the inference type (static vs. dynamic posterior) and write the final textual question so that the requested quantity, unit, and time reference (instantaneous vs. average) are explicit. Pixel-level measurement tool. To read off pixel-space trajectories, we build small annotation tool in Python using OpenCV. The script takes as input the path to video file and interactively queries the annotator for the correct frame rate. It first shows the FPS detected by OpenCV, then allows the annotator to either accept it or manually enter more reliable value (e.g., 24/25/30/60 fps). The tool verifies or manually counts the total frame number to avoid metadata errors, and reports basic properties such as resolution and duration. After loading, the script launches an interactive player (Figure 35). Each frame is overlaid with its index and timestamp, Frame: tk s, and play/- k/N Time: pause status indicator. trackbar at the bottom allows direct jumping to any frame; dragging the slider or entering frame index updates the display immediately. Annotators can also step frame-by-frame with the left/right arrow keys or play the sequence in real time with the space bar. In addition to these controls, we use standard OpenCV mouse callback to display the current pixel coordinates (x, y) of the cursor, which facilitates precise measurement of distances and object positions. (3) Measuring pixel-space kinematics. Given the verified frame rate (in fps), we set the time step as = 1/f . Annotators use the player to locate the relevant frames for each question: for instantaneous quantities we jump to the frame closest to the target timestamp; for average quantities we select sequence of frames spanning the interval of interest. Along the main motion direction, we record (i) the objects pixel length Spixel (e.g., bumper-to-bumper for car), and (ii) the pixel coordinate xpixel of the objects reference point at discrete times tk = kt. From these measurements we compute pixel-space kineFor 1D trajectory k=0, the (approximate) velocity and acceleration matics using finite differences. xpixel { } Figure 35. Pixel-level measurement tool. at time tk are xpixel k+1 xpixel k+1 Vpixel Apixel xpixel , + xpixel 2xpixel t2 k1 . γ = Sworld Spixel , Vworld t0 Vpixel t0 Aworld t0 Apixel t0 if size prior is given if velocity prior is given if acceleration prior is given . , , When motion is measured in two image-plane directions, we apply the same formulas component-wise to (xpixel , ypixel ). In this way we obtain, for every prior object and inference target, the relevant pixel-space size Spixel, velocity Vpixel , and acceleration Apixel . where t0 is the timestamp at which the prior is defined. Once is determined, any kinematic quantity of the inference target can be expressed in world units by simple rescaling: (4) Converting to real-world kinematics. Assuming planar motion (subsection B.1), single scalar scale factor γ > 0 with units [world length / pixel] suffices along the motion direction. Depending on which physical prior is available for given clip, we estimate γ via (Starget)world = γ (Starget)pixel, = γ (Vtarget)pixel (Vtarget)world = γ (Atarget)pixel (Atarget)world tk tk , . tk tk These world-space values are then used as the ground-truth priors and posteriors in QUANTIPHY. 30 D.4. Segmented Data Segmenting out target objects from the videos only changes the background and does not alter their original physical properties. Therefore, we reuse the original annotations of the videos for the segmented data without any modification. E. Vision-Language Models We evaluate diverse suite of 21 VLMs, spanning proprietary closed-source APIs, open-source models hosted via Replicate, and open-source models deployed by our own. Proprietary Models. We select flagship models from four major providers, including standard multimodal capabilities and those utilizing inference-time reasoning. and GPT-5 OpenAI. We evaluate two models: GPT-5.1 [31] (gpt-5.1-2025-11-13) [30] (gpt-5-2025-08-07), both are flagship multimodal models. The latter is thinking model that leverages Chain-of-Thought (CoT) processing for complex reasoning tasks. Both are accessed via the OpenAI API, accepting video inputs as sequences of base64-encoded frames to enable temporal understanding. evaluate Gemini 2.5 Pro [17] (gemini-2.5-pro), high-capacity model capainterble of processing large context windows of leaved images and text, and Gemini 2.5 Flash [16] (gemini-2.5-flash), lightweight variant optimized for low-latency tasks. Both models accept base64 inline data and are optimized for complex reasoning across modalities. Google. We Anthropic. We utilize Claude Sonnet 4.5 [2] (claude-sonnet-4-5-20250929). This model accepts structured list of content blocks (text and base64encoded images). It is characterized by its detailed explanatory capabilities, often providing extensive textual rationale alongside numerical answers. (See Figure 37 for raw response examples). xAI. We evaluate Grok 4.1 (Fast Reasoning) [47], model that combines rapid inference with advanced reasoning capabilities, designed to handle complex multimodal tasks with reduced latency while maintaining high accuracy. Open-source Models. We also evaluate 15 distinct opensource models, representing spectrum of architectures, parameter sizes, and input modalities to systematically assess architectural variations and scaling effects. We include LLaVA-13B [25], foundational baseline combining Vicuna LLM with CLIP encoder; VILA7B [24], pre-trained on interleaved image-text data; and Qwen3-VL-8B [5], the latest iteration of Alibabas visionlanguage series. To examine scaling effects across model sizes, we deploy and evaluate the Qwen3-VL-Instruct series at three scales: 2B, 8B, and 32B parameters [5], enabling direct comparison of performance improvements with increased capacity within single architecture family. Similarly, we assess the InternVL-3.5 series across three parameter scales: 2B, 8B, and 30B [10], providing additional insights into how architectural choices interact with model scale for vision understanding. We evaluate Fuyu-8B [22], which utilizes simplified architecture processing raw image patches without separate visual encoder. We test Molmo-7B [13] and SmolVLM [27] (1.6B), both designed for high-efficiency reasoning on consumer hardware. Phi-4 Multimodal [29] and the smaller Phi-3-Mini-128KInstruct [28], both are Microsoft suite and process text interleaved with image lists. Finally, to assess temporal analysis capabilities, we include MiniCPM-V 4.5 [56] and CogVLM2-Video [43]. Distinct from frame-based approaches, these models accept direct video file inputs for native temporal processing. In total, our evaluation encompasses 21 models (6 proprietary and 15 open-source), providing comprehensive coverage of the current state-of-the-art in multimodal reasoning across different scales, architectures, and deployment paradigms. F. Prompt Design For quantitative evaluation, we use constrained generation strategy designed for precise numerical outputs. The system prompt explicitly restricts the output space, instructing the model to provide ONLY the numerical answer with units and emphasizing No explanation or reasoning needed. The prompt structure includes the visual input, system instructions, and ground truth priors and/or depth info, followed by the specific query and post-prompt reinforcement. (See Figure 6 for the Text Prompt template). To ensure reproducibility, we enforce deterministic generation where possible. We set temperature=0 (greedy decoding) for all models supporting this parameter. For hosted models via Replicate where explicit temperature control is unavailable, we utilize the default inference parameters recommended by the model maintainers. structured The input sequence for all models is programmat- [Video Frames][System ically Prompt][Ground Truth Prior and/or Depth Info][Question][Post-prompt]. Detailed examples are provided in Figure 36. as [Video Frames]. This segment contains the full sequence of frames extracted from the source video, base6431 Table 36. Selected examples of text inputs and answers (part 1). Question is the natural-language prompt presented to the VLM. GT Prior is the physical prior provided to the model. GT Depth Info is the depth annotation used for 3D reasoning tasks. GT Posterior is the numeric ground-truth answer to the kinematic inference question. See Table 3 for detailed explanation. Raw Response shows the corresponding VLM output for each model, and Parsed Value shows the parsed value extracted from the Raw Response. 32 Table 37. Selected examples of text inputs and answers (part 2). Question is the natural-language prompt presented to the VLM. GT Prior is the physical prior provided to the model. GT Depth Info is the depth annotation used for 3D reasoning tasks. GT Posterior is the numeric ground-truth answer to the kinematic inference question. See Table 3 for detailed explanation. Raw Response shows the corresponding VLM output for each model, and Parsed Value shows the parsed value extracted from the Raw Response. 33 encoded. We normalize all videos to 480p resolution. Our preliminary exploration indicated that while lower spatial resolution (480p) has negligible impact on physics reasoning, temporal subsampling (dropping frames) significantly degrades the tracking of velocity and acceleration. Therefore, we prioritize temporal fidelity by retaining all frames from the source video. This approach also distinguishes our methodology from most prior works, which often prioritize spatial resolution at the expense of frame rate. [System Prompt]. This serves as the behavioral instruction, establishing the persona that You are an expert video analyst... We selected this formulation following pilot study of five prompt variations on subset of 15 videos, which revealed no significant difference and this persona yielded the relatively highest adherence to formatting constraints without altering reasoning accuracy. [Ground Truth Prior and/or Depth Info]. This includes physical constants and contextual priors (e.g., Given that acceleration = 9.8m/s2). For 3D scenes, this also includes depth information (e.g., At = 1.0s, the distance of the ball to the camera is 1.779m...) for estimation. [Question]. The specific physics query (e.g., What is the total displacement in the y-axis?). [Post-prompt]. final instruction reinforcing the output format (e.g., Provide... ONLY the numerical answer with units) to mitigate the tendency of models to generate verbose Chain-of-Thought explanations in the final output. G. Answer Retrieval and Parsing For each question, we query the model once. To ensure robustness against API instability or transient errors, we implement an automated retry mechanism: if query fails (e.g., timeout or server error) or yields non-parsable response, the request is re-submitted up to maximum of five times. To extract quantitative data from potentially noisy model outputs, we employ hierarchical parsing function (parse number) designed to handle both concise and verbose responses. The logic proceeds as follows. 1. Exact Match Validation: We first check if the raw response strictly matches numerical format (with or without units). If the response is concise (containing only value and unit matching the requested physical quantity), the numerical value is extracted directly. 2. Delimiter Search: If the response is verbose, we scan for explicit answer markers, including =\", Final Answer:\", Answer:\", =>\", and :\". If delimiter is identified, we discard the preceding text and retain only the substring immediately following it. If multiple cases are shown, we take the last occurrence. 3. Unit Sanitization: The retained text is cleaned of common physical units (e.g., meters, m/s, kg) to prevent string processing errors during numerical extraction. 4. Heuristic Extraction: Finally, regular expression is applied to the cleaned text to identify floating-point numbers. We also take absolute value of the numerical answer. Crucially, if multiple numbers are found, we extract the last valid number in the sequence. This heuristic assumes that in verbose Chain-of-Thought responses, the final conclusion is located at the end of the text. 5. Failure Handling: If no valid number is identified after these steps, the response is recorded as failure (parsed as None). This strict parsing pipeline is essential because our evaluation metric relies on exact numerical regression. If model provides only qualitative description (e.g., The ball is moving quickly, Cannot be determined, or API error) or fails to reach numerical conclusion within the decoding limit, it is then treated as failure. While most models adhered to the format constraints (i.e., [number] [unit]), we observed that Claude Sonnet 4.5, Fuyu-8B, and MiniCPM-V 4.5 frequently generated verbose, multi-sentence explanations despite instructions to the contrary. By targeting the post-delimiter text and the last numerical value, our parsing logic effectively retrieves the correct answer from these verbose outputs. Representative examples of raw responses and their parsed values are detailed in Figure 37. H. Human Study Details. To contextualize model performance and establish an empirical upper bound on human quantitative physical reasoning, we conducted survey study using the Gorilla Experiment Builder platform. The platform supported video presentation, question randomization, and structured data collection. Below, we describe the participant cohort, task construction, interface design, evaluation methodology, and resulting performance trends. H.1. Participants Participants were recruited from mix of undergraduate, graduate, and PhD researchers, including individuals with advanced training in various fields including engineering, physics, and mathematics. This allowed us to approximate both typical human performance and an expert-level upper bound. Participants were informed that they could freely choose their reasoning strategyincluding intuitive estimation, visual approximation, or explicit calculationin order to reflect natural human reasoning rather than enforce prescribed computation protocol. Our survey is divided into two versions: 2D survey and 34 (a) 2D survey interface. Participants see the prior ground truth, question text, and numeric answer box, and can replay or scrub the video. (b) 3D survey interface. The layout mirrors the 2D condition, with additional depth prior ground truth. Figure 38. Human study interface. Example screenshots of the 2D (top) and 3D (bottom) survey UIs. Both interfaces present the physical prior, quantitative question, and numeric input field, while allowing participants to replay and scrub the video timeline. 35 3D survey. Both the 2D and 3D surveys were completed by multiple participants, including several with substantial technical backgrounds. subset of participants completed both tasks, enabling direct cross-dimensional comparisons of individual consistency. H.2. Task Construction and Experimental Design Following the 36 fine-grained video categories defined in subsection B.2, we organized the full dataset into these category units for both model and human evaluation. Each category contains small pool of representative videos, from which stimuli were sampled during the experiment. Participants were randomly assigned to either the 2D survey or the 3D survey. Each participant viewed 18 videos, one sampled per category within their assigned dimensionality. Each video was followed by 13 quantitative kinematic questions, with the same priors and task formulations used in our VLM evaluation (e.g., estimating acceleration magnitude, inferring relative size changes, or recovering object velocity). The UI interface was intentionally designed to be concise and unobtrusive. For each trial, participants were presented with: video with standard playback controls; The physical prior ground truth (A/S/V) and/or corresponding depth information for 3D videos; The quantitative question texts; Numeric input boxes (numbers only). Participants could freely replay, pause, and scrub the video timeline. Scrubbing resolution was restricted to whole seconds to match the temporal information available to VLMs, which process videos solely as pixel sequences without access to exact timestamps. The interface permitted unrestricted video replay to minimize memory effects and ensure that both humans and VLMs operated over comparable visual evidence. H.3. Evaluation Metric Human responses were evaluated using the same MRA metric introduced in Section 4. For each individual question, we compute MRA by averaging accuracy over ten relative-error tolerance thresholds, yielding smooth, threshold-agnostic measure of quantitative precision. After computing an MRA score for every answered question, we aggregate performance at the participant level by taking the mean MRA across all questions completed by that participant. Figure 39 presents participants performance for the 2D and 3D surveys, respectively. H.4. Results and Observations We observe few findings from the results. 36 Figure 39. Distribution of human quantitative reasoning performance. Horizontal boxplots summarize participant-level mean MRA scores for the 2D (top) and 3D (bottom) survey conditions. Strong cross-dimensional consistency. Across participants who completed both tasks, MRA scores for 2D and 3D surveys were highly correlated. High-performing participants in 2D almost universally remained high-performing in 3D, suggesting that human physical intuition is stable and transferrable across dimensional modalities. Human upper bound substantially exceeds VLM performance. Top human participants achieve MRA = 0.721 in 2D and MRA = 0.724 in 3D, showing better performance than the evaluated VLMs. Although the average human performance is not higher than model performance, the human upper bound remains far above current VLM capabilities. This gap is particularly notable given the modest sample size of our study; even with limited data, humans exhibit strong physical reasoning competence that remains challenging for contemporary models. Practical implications. These findings indicate that while VLMs may approximate average-level human intuition in some settings, they remain far from achieving human-like precision or matching expert-level reasoning. The human study therefore provides meaningful benchmark for assessing the gap between current VLM capabilities and the aspirational goal of physically grounded, human-level visual reasoning. I. Sketchfab Model Sources In Table 7, we list the Model Name, Author, and Model ID for all Sketchfab models used in the Blender-generated videos in our dataset. Table 7. Sketchfab 3D Models Author kreems amogusstrikesback2 Yanez Designs EverZax tiunov.se planeta-elefante Unknown Animaker Instinto Ideal Studio Sakado DigitalLife3D DigitalLife3D DigitalLife3D Janis Zeps Jungle Jim rwy00 TheSpacePunk LightSwitch fwild Multipainkiller Studio hoxsvl.scan willinando1w08 GRIP420 anybody David Zerba Dmytro Nikonov ahitch3 FourthGreen Model Name Supermarket trolley broken supermarket 1 Pool Table (Animation) Bowling Pack (Bowling Pins & Ball) Bowling Club Elephant Walking Astronaut Mr Man Walking Jupiter & satellites Model 92A - Great White Shark Model 95A - Adult Leatherback Sea Turtle, Mac Model 69A - Striped Bass Underground Parking Lot Airport Car Airport Catering Truck Towel Plain Mug Flour ARIDLL Milk Apple [Scan] PACKAGING EMPAQUE DE HUEVOS Kitchen Pack Kitchen Tools Coffee Shop Cup Cup Bird Animations Alex Dove Bird Rigged POLICE CAR - Belgian VW Transporter Mickael Boitte Jo on Bike - Rigged & Animated Simple Factory Scene Car Basketball Court Basketball NBA Championship Official Trophy Bouncing Basketball Zen Japanese Tea House with Go Board Chinese Chess paper airplane 3d soccer ball Cardboard Box Tennis Ball Low-poly PBR Old Camera Bag .::RAWscan::. Animated ROBOT SDC Gift Box Sci-fi Box Cat Walk Deer walk Maurice Svay winters810 chung the artist vesicalsnail BlenderMaster Pricey1600 MaX3Dd Andrea Spognetta (Spogna) SDC PERFORMANCE local.yany Igor K. LostBoyz2078 LostBoyz2078 NEEEU Spaces GmbH Pickeri Paulius tiunov.se johnnokomis 37 Model ID 7f460877380349f8886280a596253034 ca1def2b7be544068def3cec5852c67e 0f2ae181a2dd4b00a6ec25073692037f 52743c4714c14211ac71d2fe1e5c8da3 0b8fae45fcda4fe78f93bb2a899401a6 f8778fc3d161481abba7ec23a8ddd1e8 d9062a2003df422abdafdc02afdac085 98ccac2b0e2845789b6f789978ca06ed 379fd77b970c4821898c05c483913dec 702e7b53637f4ded9ca479a8124e810d 4974c93644a24da280fde68cba74a12d 35be3af9a7c4441c98109e5562d36c09 0ad5c221525b4bbba3a164c6235d28b8 29d1c6260b134a3baf5231f34de1b24f 289f4e2cfa3f4722b0476b1fc37681d8 6edc341457e44603ab351470ab800493 19c8fe5702b544d0a1409d3dac1cf90e d40db94d92ab4e7a82a1c199312f3985 5e8d71045e2040ba8f6619d86d204cf5 facb1aa6928c4f8f82d87a019b9f134e cd51c5d25bbd4370881aebd3648cfe8c a513f2e85bc94ae5b6d8cbd74909e3c4 417e3873fcb34f7ab9744506d7bcc838 37e6805f2b7a4158a1d61fe75f8e2a33 7d450bb714034fceaa7b59a0e564f46b 081fa7f0cfd649b9b07babb4c619acc7 6b91be2a28fb4404a2d57d5ca98bd4dc 35eaf9505e13464385404402ad865508 36ee5344e81149858a664cde9f98e835 804fbe0cd1fa44fa9ca86ae42c82d63d a619dd25a6c04af0b2d8730aa1cb058b 5faad7b528124907ab82732ed0c6b743 04f6f1135ffb48749c43c9c20c75fc b8731a2fda6849c9a164d1966dc16ff8 a1e95e5efee349a693f30eda32401aef 7ad0f4f0ebbc455d9e9f829c956dda80 0967ab4a9c654a569a13ea1f8d9dca0c 0400146e0e3c4d8f8b57bfa06d7dfb4c 4e622ef1a09c43e28a49d9fa37f9eeee e5c2b0e5860549acaa2dfe8b764d5f94 788f8b75874f417ebde498ffd231410c 3d127f327a6c4033a32b810b5fb071ed 83296611584143a3afad6c0d0c0a4227 ca415724f32043489fcab2ec74582619 915680200c064815bba75e008ba9efb5 229ba6ba0d1e4811ab89382f74601e16 Continued on next page Table 7 Continued from previous page Model Name Horse Horse Walk Rabbit Rigged phoenix bird First Aid DX2 - 300 Followers Celebration Nathan Animated 003 - Walking 3D Man Chernovan Nemesis Dragon Fly Borboleta Azul - Butterfly Blue Whale - Textured jellyfish Koi Fish Running Raccoon Animation (FREE) Cyberpunk Hovercar White Eagle Animation Fast Fly Author kenchoo Amitesh Nandan FourthGreen NORBERTO-3D re1monsen Renderpeople Swiss Fox LostBoyz2078 Lancaster Modelagem 3D Bohdan Lvov yanix 7PLUS Santrez Lionsharp Studios GremorySaiyan Model ID 86d47bdcd5ab41238ba44547e4d21f9c 93b53ddcec414592842753d1819f3133 e7213589744d436b9d96e2dbb31198a5 844ba0cf144a413ea92c779f18912042 c5ddfa9e6309403083bbce60bdcc3d71 143a2b1ea5eb4385ae90a73657aca3bc c6c91c73e93444f4b72d6c24db778e73 1443a7efe5d5450b8db4c15d8ff5c343 ab9192b6bc8f49e3baed63e984c7073a d24d19021c724c3a9134eebcb76b0e0f d06a5a553fe641ab92f720527b2278f3 236859b809984f52b70c94fd040b9c59 bfce4d4815234c39bcf012352e52c27e 3205b1075bb44ffc826bce0c2a04d74c 30203bf39e5145f19c79e83c550139d"
        }
    ],
    "affiliations": [
        "Stanford University",
        "UST"
    ]
}