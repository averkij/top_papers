{
    "paper_title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
    "authors": [
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Baher Mohammad",
        "Stamatios Lefkimmiatis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\\href{https://github.com/mts-ai/COMPOT}{here}$."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 0 0 2 5 1 . 2 0 6 2 : r COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for"
        },
        {
            "title": "Transformers Compression",
            "content": "Denis Makhov 1 Dmitriy Shopkhoev 1 Magauiya Zhussip 1 Ammar Ali 1 2 Baher Mohammad 1 2 Stamatios Lefkimmiatis"
        },
        {
            "title": "Abstract",
            "content": "Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (CalibrationOptimized Matrix Procrustes Orthogonalization for Transformers), training-free compression framework that uses small calibration dataset to estimate sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under global compression budget, COMPOT further introduces one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers superior qualitycompression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with posttraining quantization for extreme compression. Code is available here. 1. Introduction Transformer-based foundation models underpin state-of-theart systems across modalities, including language, vision, vision-language, and audio (Vaswani et al., 2017; Dosovitskiy, 2020; Radford et al., 2021; Gong et al., 2021). As they scale to billions of parameters, deployment is increasingly limited by memory footprint, bandwidth, and compute 1Fundamental Research Center MWS AI 2ITMO. Correspondence to: Denis Makhov <d.makhov@mts.ai>, Stamatios Lefkimmiatis <s.lefkimmiatis@mts.ai>. Preprint. February 18, 2026. (Brown et al., 2020; Chowdhery et al., 2023). Yet Transformers are highly redundant: attention heads and other components can often be removed or simplified with limited degradation, indicating that parameters contribute unevenly to downstream behavior (Michel et al., 2019; Voita et al., 2019). This has motivated extensive post-training compression work: quantization, pruning, distillation, and factorization - to reduce inference cost and memory footprint without expensive retraining (Tang et al., 2024; Liu et al., 2025). Low-rank matrix factorization is widely used post-training paradigm, replacing dense projection matrices by products of smaller matrices. Activation-aware and task-aware lowrank methods show that calibration/activation information is crucial for preserving accuracy (Hsu et al., 2022; Yuan et al., 2023). Recent SVD-based methods further tailor truncation using calibration and principled losses: SVDLLM uses whitening-based calibration to relate singular values to truncation loss and applies closed-form updates to mitigate degradation (Wang et al., 2025b). Despite these advances, SVD methods still enforce single shared subspace per weight matrix, which can be restrictive when different columns lie in different local subspaces. Sparse dictionary learning offers complementary alternative, modeling weight matrix as dense dictionary times column-sparse coefficient matrix, enabling union-ofsubspaces representation. CoSpaDi shows that calibrationguided sparse dictionary learning can outperform dataaware low-rank baselines for training-free LLM compression (Shopkhoev et al., 2025b). Relatedly, MASA proposes matrix-based dictionary learning for parameter sharing in Transformer attention, representing layer-specific projections as combinations of shared matrix atoms (Zhussip et al., 2025). However, existing dictionary learning pipelines typically rely on iterative and expensive dictionary and sparse coding updates (e.g., K-SVD/OMP (Aharon et al., 2006)), limiting practicality at billion-parameter scale. We introduce COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), trainingfree compression framework that improves calibrationguided dictionary learning in both accuracy and compression time. Our key design choice is to enforce orthogonality and restrict the dictionary to be complete or undercomplete, COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 1. Comparison of low-rank decomposition, dictionary learning with sparse coding, and COMPOT. Low-rank uses rigid shared orthogonal basis B; dictionary learning enables flexible union-of-subspaces; COMPOT lies in between by using unionof-orthogonal-subspaces (denoted with O), which enables fast closed-form dictionary updates and lightweight coefficient update, improving compression performance. leveraging evidence that orthogonal dictionary learning can be effective and substantially faster than overcomplete alternatives (Bao et al., 2013). Under orthogonality, the dictionary update becomes the classical orthogonal Procrustes problem with closed-form SVD solution (Schonemann, 1966), replacing atom-wise K-SVD updates. Meanwhile, the sparse coding update also reduces to closed-form hardthresholding operation, avoiding iterative pursuits and yielding an efficient alternating procedure (Elad, 2010). Transformer layers and projection types exhibit heterogeneous redundancy. Prior work addresses this via nonuniform rank allocation (e.g., SVD-LLM V2) (Wang et al., 2025a) or differentiable truncation optimization (DobiSVD) (Qinsi et al., 2025). We instead propose one-shot global allocation strategy: we normalize each weight matrix, pool singular values across matrices into global spectrum, and truncate globally to satisfy model-wide budget, while enforcing constraints that prevent negative or low per-matrix compression and cap over-compression of sensitive layers. This yields simple, deterministic allocation procedure without iterative search over layer-wise ratios. COMPOT integrates naturally with post-training quantization. Among PTQ methods such as SmoothQuant (Xiao et al., 2023), GPTQ (Frantar et al., 2023), and AWQ (Lin et al., 2024), we show that 4-bit GPTQ applied on top of COMPOT yields superior results compared to quantization alone under equal memory budgets. Contributions. (i) We introduce an orthogonal dictionarybased sparse factorization for Transformer projections that enables closed-form Procrustes dictionary update and an analytical sparse coefficient update under the same constraint, substantially accelerating calibration-guided dictionary learning while achieving better performance. (ii) We propose one-shot dynamic compression allocation method based on global singular-value pooling with constraints that prevent lowand over-compression. (iii) We demonstrate that COMPOT integrates effectively with posttraining quantization and can outperform quantization under equal memory budgets. (iv) COMPOT outperforms strong SVD baselines by wide margin across architectures and downstream tasks (language, audio, vision). (v) The proposed method sets new state-of-the-art among structured matrix-factorization compression techniques, shifting the paradigm beyond established SVD. 2. Related Work 2.1. Overview of Transformer-based Model Compression Compression of Transformer-based models spans several complementary families. Early pruning work showed that many attention heads can be removed with little degradation, motivating structured pruning of self-attention and encoder-decoder heads to reduce compute (Michel et al., 2019; Voita et al., 2019). Post-training quantization is now the dominant deployment tool for LLMs: SmoothQuant enables near-lossless W8A8 quantization by migrating activation outliers into weights via offline rescaling (Xiao et al., 2023); GPTQ/OPTQ use second-order, block-wise weight-only PTQ to reach 3-4 bits (Frantar et al., 2023); and AWQ improves low-bit PTQ by identifying and protecting activation-salient weights (Lin et al., 2024). Orthogonal to quantization, low-rank and structured factorizations replace dense projections with products of thinner matrices; recent SVD-based methods (SVD-LLM, SVD-LLM V2) combine truncation-aware whitening, closed-form updates, and optimized rank allocation to better align truncation with functional loss (Wang et al., 2025b;a). Finally, knowledge distillation trains smaller student to match larger teacher; 2 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression DistilBERT reduces BERT parameters by 40% while preserving most downstream accuracy (Sanh et al., 2019). 2.2. SVD-based Matrix Factorization for Compression Low-rank factorization is standard post-training approach for compressing Transformer projections, motivated by the empirical low-rank structure of weight matrices. While truncated SVD is optimal in Frobenius norm, this objective can be misaligned with preserving task behavior. Early dataaware formulations address this mismatch: DRONE derives provably optimal low-rank decomposition with closedform solutions for feed-forward and self-attention matrices in BERT-style models (Chen et al., 2021), and FWSVD improves robustness by Fisher-weighting reconstruction error (Hsu et al., 2022). Subsequent work incorporates activation information: ASVD leverages activation statistics and layer sensitivity to guide training-free truncation (Yuan et al., 2023). More recently, SVD-LLM introduces truncationaware whitening and closed-form parameter updates to mitigate degradation at higher compression (Wang et al., 2025b), Dobi-SVD proposes differentiable truncation with remapping strategy to better connect truncation choices to storage behavior (Qinsi et al., 2025), and SVD-LLM V2 further refines truncation-loss modeling and allocates non-uniform compression ratios across matrices to reflect heterogeneous redundancy (Wang et al., 2025a). SVD compression is closely related to PCA, which seeks low-dimensional basis minimizing L2 error (Bishop & Nasrabadi, 2006). Viewed this way, SVD represents each matrix using single shared subspace, which can be restrictive when different columns of the weight matrix are better explained by different local subspaces. 2.3. Dictionary Learning and Sparse Coding Sparse dictionary learning is complementary paradigm: it represents weight matrix as the product of dictionary and sparse coefficient matrix. This formulation enables different columns to select distinct subsets of atoms, thereby inducing union-of-subspaces structure. Classic methods such as K-SVD iteratively update dictionary atoms and are widely used in signal and image processing (Aharon et al., 2006). CoSpaDi adapts calibration-guided dictionary learning to LLM compression by minimizing functional output mismatch rather than pure weight error, and shows that structured sparse factorizations can outperform data-aware SVD baselines at practical compression ratios (Shopkhoev et al., 2025b). However, K-SVD-style updates are computationally heavy at billion-model scale, motivating alternative constraints and update rules. To address this challenge, our method restricts the weight factorization to employ complete or undercomplete orthogonal dictionaries. This structural constraint substantially simplifies optimization: the dictionary update reduces to closed-form orthogonal Procrustes solution, while sparse coding admits an analytical solution, entirely eliminating the need for iterative sparse pursuit algorithms. Our approach is well motivated, as the orthogonality constraint on learned dictionaries has been investigated in image processing and shown to improve both numerical stability and computational efficiency (Bao et al., 2013). 2.4. Dynamic Allocation of Compression Ratios recurring theme in Transformer compression is heterogeneous redundancy across layers and projection types, making uniform compression suboptimal. ASVD highlights sensitivity differences and uses activation-aware heuristics to reduce degradation (Yuan et al., 2023). SVD-LLM V2 assigns matrix-specific compression ratios via truncation-loss modeling under global budget (Wang et al., 2025a). DobiSVD targets rank selection with differentiable truncation and introduces remapping that better connects truncation to achievable compression (Qinsi et al., 2025). Recent work continues this direction: D-Rank allocates ranks dynamically under fixed budget via optimization (Mi et al., 2025), and ARA introduces adaptive rank allocation for SVD-based compression (Xv et al., 2025). These results reinforce that budget allocation is central to minimizing quality drop for given compression target. In this work, we contribute simple one-shot allocation procedure that operates over pooled singular values while enforcing constraints that prevent negative compression and limit over-compression of individual matrices. 3. Method 3.1. Problem Setup We consider pretrained Transformer with linear projections parameterized by weight matrices Rmn (e.g., attention and MLP projections). Our goal is post-training compression: replace each with structured factorization that reduces storage (and ideally runtime) while preserving the layer behavior on small calibration set, without back-propagation (fine-tuning). Let RN denote matrix of calibration inputs (layer activations entering the projection), where is the number of calibration tokens/examples aggregated into rows. The layer output on calibration data is XW. natural dataaware objective is therefore to minimize output mismatch X(W (cid:99)W)2 over compressed (cid:99)W. 3.2. Preliminaries: Subspace vs. Union-of-subspaces Modeling Low-rank (single shared subspace). For real-valued matrix Rmn, truncated SVD gives UrΣrV , (1) COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 2. Overview of COMPOT framework. On the left part the alternating minimization process is visualized while on the right we represent our single-shot strategy for dynamic compression ratio allocation based on singular values of normalized projection matrices. where Ur Rmr, Σr Rrr, and Vr Rnr. Equivalently, we can draw the direct link with principal component analysis (PCA) with the underlying concept of projecting columns of to lower-dimensional subspace (Bishop & Nasrabadi, 2006): BC, where Ur, ΣrV , (2) vations: X(W (cid:99)W)2 . min (cid:99)W (4) First we define the Gram matrix, XX Rmm, which we assume to be positive definite (this condition is typically satisfied using sufficient calibration data) and admits the Cholesky decomposition = LL, with Rmm the Cholesky factor. Then it holds: so each column wj is represented in the same r-dimensional subspace spanned by (Fig. 1 (left)). X(W (cid:99)W) = Tr (cid:16) (cid:17) (W (cid:99)W)G(W (cid:99)W) Sparse dictionary learning (union-of-subspaces). Dictionary learning (Fig. 1 (right)) represents the projection matrix with an overcomplete dictionary and corresponding sparse coefficient DS, Rmk, Rkn, (3) where is column-sparse: each column sj has at most nonzeros. Each column wj may be represented with different subset of atoms, inducing union-of-subspaces model: columns can live in different s-dimensional subspaces of Rm spanned by subsets of atoms. 3.3. COMPOT: Calibration-optimized Orthogonal Dictionary Factorization COMPOT preserves the union-of-subspaces flexibility of sparse coding, but enforces an orthogonal (complete/undercomplete) dictionary in whitened, data-aware space. This yields closed-form updates: Procrustes (thin SVD) for the dictionary and an analytical sparse coding step under orthogonality, resulting in union-of-orthogonal-subspaces model. = L(W (cid:99)W)2 . (5) Therefore, optimizing the functional error is equivalent to optimizing the reconstruction error in the whitened space. We define the whitened weight matrix (cid:102)W LW Rmn. (6) COMPOT objective. COMPOT factorizes (cid:102)W using an orthogonal (complete or undercomplete) dictionary DO and column-sparse codes SO: O, = arg min DO,SO (cid:102)W DOSO2 s.t. ODO = Ik, sOj0 [n], (7) where DO Rmk with (complete/undercomplete), and SO Rkn. After solving (7), we map the factorization back to the original parameter space: (cid:99)W ASO, LDO Rmk. (8) Data-aware whitening. We adopt standard calibration objective that measures functional error on calibration actiImportantly, is computed offline during compression, so inference uses only (A, SO). 4 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Alternating minimization (closed-form sparse coding + Procrustes). Although (7) is not jointly convex, it admits simple alternating minimization with closed-form updates when enforcing ODO = Ik. With DO fixed, sparse coding decouples column-wise and has an analytical solution (Elad, 2010): SO Hs (cid:0)D (cid:102)W(cid:1), (9) where Hs() is the hard-thresholding operator, which keeps the largest-magnitude entries in each column and zeroes out the rest. With SO fixed, the dictionary update is an orthogonal Procrustes step (Schonemann, 1966): (cid:102)WS O, SVD= PΛQ DO PQ. (10) We provide the full derivation of (9), the complete procedure, and implementation details in Appendix A.1. One-shot compression allocation. Different Transformer projections exhibit heterogeneous redundancy, so uniform compression is often suboptimal. We propose simple oneshot global allocation strategy that distributes model-wide budget by globally truncating the smallest singular values across matrices, while enforcing per-matrix compression guards and handling matrices where factorization is not beneficial. Original or whitened space? Whitening is layerand projection-specific (depends on X), so singular values computed in whitened coordinates are not directly comparable across matrices. We therefore allocate ranks using singular values of the raw weight matrices in the original space, while still using whitening for COMPOT reconstruction. Normalize or not? Even in the original space, spectra magnitudes vary across layers. To address this, we normalize each matrix by its Frobenius norm: W(f ) Wi/ WiF . Following normalization, we compute thin SVD for each W(f ) and pool all singular values into global multiset. We sort this pool and truncate the smallest values to satisfy the desired model-wide compression ratio; the number truncated per matrix then induces its layer-wise compression ratio. To prevent degenerate behavior, we enforce minimum and maximum compression guards and handle matrices where factorization is not beneficial. Fig. 2 (right) illustrates the simplified procedure; Appendix A.2 provides full details. Compression ratio (CR). In the proposed method we need to store both DO and SO with 16-bit values. Instead of storing the sparse matrix directly, we store only its nonzero elements and binary mask of their position: CRCOMPOT = 1 DO (cid:122) (cid:125)(cid:124) (cid:123) 16mk + SO (cid:122) (cid:125)(cid:124) (cid:123) 16sn + 16mn Mask (cid:122)(cid:125)(cid:124)(cid:123) (kn) . (11) 5 Figure 3. Average accuracy as function of the number of alternating minimization steps on Llama3.2-1B at 0.2 compression, comparing random and SVD-based dictionary initialization. For the CRCOMPOT computation we need to specify both the number of atoms and the number of column-wise nonzero elements s. We express both parameters with single hyperparameter k/s-ratio. 4. Experiments We evaluate COMPOT across model families (Llama, OPT, Qwen; 0.6B30B) and domains (language, vision-language, audio), under compression ratios (CR) 0.20.6. We compare against strong SVD-based baselines (SVD-LLM, SVDLLM V2, Dobi-SVD), sparse dictionary learning (CoSpaDi), and structured pruning (ReplaceMe, LLM-Pruner), and study compatibility with post-training quantization (GPTQ). Protocol deviations required by baseline codebases are noted below and detailed in the appendix. 4.1. Experimental Setup Unless stated otherwise, we set the dictionary-to-sparsity ratio to k/s = 2 adjusted only when necessary to avoid overcomplete dictionaries that violate the orthogonality constraint and compress all dense linear projections in self-attention (Q/K/V/O) and the MLP (gate/up/- down), keeping token embeddings and lm head uncompressed in alignment with prior works. We report zeroshot accuracy on PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), OpenAI LAMBADA (Paperno et al., 2016), ARC-easy/ARC-challenge (Clark et al., 2018), SciQ (Welbl et al., 2017), RACE (Lai et al., 2017), and MMLU (Hendrycks et al., 2021a), and perplexity on WikiText (Merity et al., 2017) and LAMBADA-OpenAI. For SVD-LLM, CoSpaDi, and pruning baselines we use lm-evaluation-harness 0.4.8 (Gao et al., 2024) with normalized accuracies when available; for SVD-LLM V2 and Dobi-SVD comparisons we additionally follow the COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 1. Effect of dictionary initialization for Llama3.2-1B at 0.2 compression with 20 alternating minimization iterations. We report average accuracy (Avg. Acc.) across the same datasets as in Table 3, as well as perplexities on WikiText and LAMBADA. Best results are highlighted in bold. Table 2. Effect of grouping for dynamic allocation for Llama3.21B at 0.2 compression with 20 alternating minimization iterations. We report average accuracy (Avg. Acc.) across the same datasets as in Table 3, as well as perplexities on WikiText and LAMBADA. Best results are highlighted in bold. CR Allocation Static Dynamic Init. Rand. SVD Rand. SVD Avg. Acc. Wiki. PPL Lambada PPL 46.3 48.5 48.5 49.1 26.84 24.67 22.05 20.71 24.74 20.62 15.06 13. Grouping All indiv. QKV&UpGate All grouped Avg. Acc. Wiki. PPL Lambada PPL 48.5 49.2 50.1 24.62 21.41 20.71 18.62 15.52 13.97 Table 3. Performance comparison under static compression ratio allocation of COMPOT(static CR) vs state-of-the-art SVD-based SVD-LLM and dictionary-learning CoSpaDi on Llama3-8B and Qwen3-8B at different compression levels on different benchmarks. Best results are highlighted with bold. Method CR PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity Llama3 8B SVD-LLM CoSpaDi COMPOT SVD-LLM CoSpaDi COMPOT SVD-LLM CoSpaDi COMPOT Qwen3 8B SVD-LLM CoSpaDi COMPOT SVD-LLM CoSpaDi COMPOT SVD-LLM CoSpaDi COMPOT 0.2 0.3 0.4 0.2 0.3 0.4 80.7 71.1 75.2 77.8 65.8 70.5 74.4 60.3 63.7 68.2 77.7 73.8 76.5 75.5 70.4 72.4 75.0 66.3 68.9 70.9 79.1 58.4 66.5 72.7 46.4 56.2 64.6 34.5 41.4 51. 74.9 63.9 68.0 72.2 55.2 60.5 65.9 44.6 49.0 56.4 75.6 59.3 73.8 69.4 38.1 61.3 60.9 11.4 30.3 38.8 64.1 62.2 65.6 62.5 53.8 62.6 62.5 37.9 49.9 54.7 77.7 55.5 66.5 71.0 41.9 54.2 61.0 32.4 39.1 46.9 80.7 68.7 72.2 70.7 59.3 63.9 66.3 45.0 49.4 53.7 53.5 34.0 41.6 44.5 27.7 33.5 38.0 24.5 26.6 31. 56.7 45.7 48.9 49.0 37.1 41.2 44.2 28.1 29.9 34.9 93.9 86.4 89.5 89.8 70.0 85.7 87.1 44.2 68.5 75.4 95.7 90.1 93.2 92.4 87.2 88.4 90.1 77.3 82.0 84.3 40.3 35.5 38.2 41.0 31.8 36.2 38.6 25.7 30.5 34.4 40.9 40.5 40.7 42.9 38.4 39.5 42.1 35.3 36.8 38.7 62.2 32.6 42.8 50.1 27.2 32.2 38.9 23.1 25.4 27. 73.0 54.7 60.8 66.0 44.8 51.3 58.2 29.1 36.6 44.4 70.4 54.1 61.8 64.5 43.6 53.7 58.0 32.0 40.7 46.7 70.5 62.5 65.7 66.4 55.8 60.0 63.0 45.4 50.3 54.7 7.3E+00 4.1E+01 2.0E+01 1.3E+01 1.5E+02 4.5E+01 2.1e+01 5.5E+02 1.8E+02 6.2e+01 1.2E+01 2.1E+01 1.8E+01 1.5E+01 2.7E+01 2.3E+01 1.8E+01 4.3E+01 3.6E+01 2.5E+01 3.1E+00 1.1E+01 4.3E+00 5.2E+00 6.1E+01 9.2E+00 8.5E+00 1.3E+03 1.2E+02 4.2E+ 4.6E+00 6.4E+00 4.9E+00 5.7E+00 1.1E+01 6.3E+00 6.2E+00 3.6E+01 1.5E+01 9.8E+00 original SVD-LLM evaluation protocol (unnormalized accuracies and the authors perplexity script). Additional benchmarks/ablations are reported in Appendix A.4. SVD initialization saturates much earlier (100). We use 20 iterations in all main experiments as pragmatic accuracy time compromise. 4.2. Ablation Study COMPOT involves several hyperparameters that must be chosen carefully, in particular the dictionary initialization, the number of alternating minimization steps, and the strategy for dynamic allocation. In this section we study their impact on performance. Dictionary initialization. We compare random-column initialization (randomly orthonormalized permuted subset of columns) against SVD initialization (top left singular vectors). Table 1 shows SVD initialization consistently improves both static and dynamic compression at fixed iteration budget. Number of alternating updates. We vary the number of alternating updates on Llama3.2-1B at CR 0.2  (Fig. 3)  . Random initialization improves up to 300 iterations, while Grouping for dynamic allocation. We test pooling singular values by projection type (All indiv. similar to SVD-LLM V2), partial grouping (QKV&UpGate), and single global pool (All grouped) on Llama3.2-1B at CR 0.2. Table 2 shows global pooling yields the best overall accuracy/perplexity, motivating our default global allocation. 4.3. Main Results Static CR (no allocation). We first disable dynamic allocation and apply uniform CR to all projections (COMPOT). We compare against SVD-LLM (Wang et al., 2025b) as strong SVD-based baseline and CoSpaDi (Shopkhoev et al., 2025b) as K-SVD-based sparse dictionary method. Calibration uses 256 sequences from RefinedWeb (Penedo et al., 2023) (length 1024). Table 3 reports results on Llama3-8B and Qwen3-8B; smaller models (Llama3.2-1B, 6 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 4. Performance comparison under dynamic compression ratio allocation of training-free COMPOT vs training-based state-of-the-art SVD-based Dobi-SVD (Qinsi et al., 2025) on Llama2-7B at different compression levels on different benchmarks. Best results are highlighted with bold. Method Training-free CR Perplexity WikiText-2 Accuracy Openb. ARC WinoG. HellaS. ARC PIQA MathQA Avg. Llama2-7B Dobi-SVD COMPOT Dobi-SVD COMPOT Dobi-SVD COMPOT 0.2 0.4 0. 5.49 9.39 6.22 17.50 8.91 53.40 32.10 7.27 19.70 9.34 55.58 20.40 200.11 179.02 44.0 32.8 30.4 29.2 25.2 26.0 13.4 74.6 47.6 72.6 34.4 63.1 28.3 33.8 69.0 58.5 67.6 54.4 63.5 50.5 53.4 76.3 51.5 51.8 35.4 39.6 28.4 28. 45.5 29.3 39.6 24.5 28.5 24.7 18.8 78.6 65.2 76.4 57.3 67.0 51.9 54.9 28.4 22.7 26.2 21.3 23.6 22.0 22.1 59.5 43.9 52.1 36.6 44.4 33.1 32.2 Table 5. Performance comparison under dynamic compression ratio allocation between COMPOT and SVD-LLM V2 (Wang et al., 2025a) on Llama-7B, OPT-6.7B, and Llama3-8B at 0.2 compression level across different benchmarks. We present both originally published results and our reproduced results (denoted as repr.). Best results are highlighted in bold. Discrepancies between published and reproduced results are thoroughly explained in A.10 Methods Original SVD-LLM V2 Llama-7B OPT-6.7B Llama3-8B WikiText-2 / C4 WikiText-2 / C4 WikiText-2 / C4 5.67 / 7.33 7.12 / 10.47 10.92 / 12.57 13.46 / 17.72 SVD-LLM V2 (repr.) 8.09 / 16. / COMPOT 6.52 / 9.63 10.92 / 13.17 6.14 / 9.47 8.01 / 11. 47.58 / 73.33 8.64 / 18.29 Qwen3-0.6B) are provided in Appendix A.3. Across CRs, COMPOTyields higher average accuracy and lower perplexity than both low-rank (SVD-LLM) and K-SVD-style dictionary learning (CoSpaDi), indicating that orthogonal dictionary factorization improves the qualitycompression trade-off under fixed per-layer budget. Dynamic CR allocation. We evaluate full COMPOT with our one-shot allocation and compare against methods that explicitly optimize layer-wise ranks, namely Dobi-SVD (Qinsi et al., 2025) and SVD-LLM V2 (Wang et al., 2025a). To ensure like-for-like comparison, we follow the released SVD-LLM/Dobi-SVD evaluation protocols: we calibrate with 256 WikiText samples at context length 2048, report unnormalized accuracies via lm-eval 0.4.8, and compute perplexity on WikiText and C4 using the scripts provided in the SVD-LLM repository. Results are summarized in Tables 4 and 5, with additional Llama-13B/30B results in Appendix A.10. We also represent CR allocation plots in Appendix A.6 For SVD-LLM V2, the paper points to the SVD-LLM repository for code, but at the time of our experiments the repository does not provide dedicated, ready-to-run V2 implementation; therefore, we implemented the V2 dynamic allocation and loss-optimized truncation strategies on top of the released SVD-LLM codebase and report both our reproduction and the paper-reported numbers in Appendix A.10. Missing OPT-6.7B entries indicate runs that failed in the released pipeline under our matched protocol (see Appendix A.10 for details). For Dobi-SVD, we evaluate the provided checkpoints without the remapping step, which alters the effective storage/parameterization and is quantization; we include the full remapping-inclusive comparison in Appendix A.11. Under matched global CR, COMPOT consistently achieves stronger accuracy/perplexity while remaining fully training-free. Comparison to structured pruning. We compare COMPOT against ReplaceMe (Shopkhoev et al., 2025a) and LLM-Pruner (Ma et al., 2023) on Llama3-8B  (Table 6)  . We exclude unstructured pruning since high sparsity often does not translate into memory/latency savings on dense backends. At comparable effective compression, COMPOT preserves substantially higher accuracy and lower perplexity than structured pruning, suggesting that factorizing projections is more robust mechanism for removing redundancy than deleting entire channels/heads. Compatibility with post-training quantization. Following SVD-LLM V2 (Wang et al., 2025a), we apply 4-bit GPTQ on top of both SVD-based and COMPOT-based factorizations and compare against GPTQ-only under matched weight memory. Table 7 reports WikiText-2 perplexity on Llama-7B, together with the decomposition of total compression into quantization and factorization components (Quant. CR and Factor. CR). COMPOT+GPTQ yields lower perplexity than GPTQ-only and SVD-LLM V2+GPTQ, showing that COMPOT provides gains complementary to modern PTQ. Vision-language transfer. We evaluate COMPOT on Qwen3-VL-8B-Instruct at CR 0.2 (additional CRs and setup are provided in the Appendix A.9). Table 8 reports MMMU, OCRBench, RealWorldQA, and MMStar. SVD-LLM degrades severely, whereas COMPOTand full COMPOT retain strong multimodal performance, with full COMPOT achieving the best average accuracy across benchmarks. Audio transfer. We evaluate COMPOT on Whisper (Radford et al., 2023) (Whisper Base: 72M; Whisper Medium: 763M; Whisper Large: 2B) and report WER on LibriSpeech COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 6. Comparison of the COMPOT with other training-free methods including state-of-the-art structured pruning methods ReplaceMe (Shopkhoev et al., 2025a) and LLM-Pruner (Ma et al., 2023) on Llama3 8B under different compression ratios. We report accuracy on different benchmarks as well as its average and perplexity. Best results are highlighted with bold Method CR Llama3 8B ReplaceMe LLM-Pruner COMPOT ReplaceMe LLM-Pruner COMPOT ReplaceMe LLM-Pruner COMPOT 0. 0.20 0.31 0.30 0.41 0.40 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 80.7 73.1 75.5 78.0 66.6 67.3 76.6 61.7 50.3 71.0 79.1 65.7 67.5 73.5 53.8 45.1 67.2 44.3 25.8 55.8 75.6 42.1 51.0 70.5 24.0 20.9 64.3 9.8 1.5 44. 77.7 65.9 62.1 71.3 50.7 45.4 69.4 37.4 26.4 56.6 53.5 43.7 36.6 45.3 37.9 28.8 41.6 27.5 25.8 33.5 93.9 86.4 87.8 91.0 77.3 63.4 90.1 60.4 28.1 85.4 40.3 35.4 35.1 40.1 34.0 30.1 38.4 31.6 21.8 36.6 62.2 51.7 25.0 52.4 30.6 22.9 39.8 26.4 23.2 29.6 70.4 58.0 55.1 65.3 46.9 40.5 60.9 37.4 25.4 51. 7.3E+00 3.4E+01 1.6E+01 1.2E+01 6.7E+01 3.8E+01 1.7E+01 2.3E+02 4.3E+01 3.1E+00 2.0E+01 1.1E+01 4.6E+00 1.3E+02 2.2E+02 6.6E+00 1.8E+03 5.7E+05 2.2E+01 Table 7. Perplexity () of COMPOT vs state-of-the-art SVD-based SVD-LLM V2 (Wang et al., 2025a) accompanied by quantization on Llama-7B on WikiText-2. Best results are highlighted with bold. Method Weight memory Quant. CR Factor. CR PPL GPTQ-3bit SVD-LLM V2+GPTQ-4bit COMPOT+GPTQ-4bit COMPOT+GPTQ-4bit 2.8 GB 2.8 GB 2.8 GB 2.8 GB 0.81 0.75 0.75 0.75 N/A 0.25 0.25 0.25 16.28 9.97 9.93 9.62 Table 8. Multimodal performance of Qwen3-VL-8B-Instruct under 0.2 compression ratio (CR) on vision-language benchmarks. Best results are highlighted in bold. Method MMMU OCRBench RealWorldQA MMStar Original SVD-LLM COMPOT COMPOT 52.6 29.2 41.9 44.7 82.6 35.1 64.1 66.9 69.7 52.9 59.2 62.2 63.2 31.6 50.8 54. test-clean (N=2,650) and test-other (N=2,939). As baseline we apply SVD-LLM to the same decoder projections and calibrate on the same LibriSpeech-derived calibration set. Table 9 shows that on Whisper Large, COMPOT is consistently more robust than SVD-LLM across CRs; additional audio results and full setup are provided in Appendix A.9 Table 9. ASR Performance (WER ) on LibriSpeech benchmarks for Whisper Large V3 model. Best results are highlighted with bold. Method CR WERtest-clean WERtest-other Whisper Large V3 SVD-LLM COMPOT SVD-LLM COMPOT - 0.2 0.2 0.3 0. 2.74 4.12 2.46 12.78 2.74 4.53 6.8 4.51 15.54 5.21 Wall clock time for optimization. We provide evidence on substantial speedup over iterative dictionary learning baseline in Appendix A.5 and possible acceleration techniques in Appendix A.7. 8 5. Limitations and Conclusion We presented COMPOT, training-free compression framework for Transformer projections based on orthogonal dictionary learning with closed-form Procrustes dictionary updates and analytical sparse coding, together with deterministic one-shot global allocation strategy. Across language, vision-language, and speech models, COMPOT improves the qualitycompression trade-off over strong SVD and sparse dictionary baselines, and composes effectively with post-training quantization under matched memory budgets. Our method also shares several limitations typical of posttraining, calibration-driven compression. As with most activationor calibration-aware approaches, results can depend on the representativeness of the calibration data: limited diversity or distribution shift may lead to less reliable statistics and larger quality drop. In addition, whitening is most convenient when the associated Gram/covariance is well-conditioned and admits Cholesky factorization; when this positive-definiteness assumption does not hold (e.g., small calibration sets or ill-conditioned activations), equivalent whitening transforms can be constructed using SVD/eigendecomposition-based alternatives (optionally with mild regularization), which is practical engineering choice that can affect robustness. Finally, our current implementation uses fixed sparsity structure during factorization. straightforward extension is to add lightweight healing step that refines factors under fixed sparsity pattern, which may further improve accuracy at the same storage budget. More broadly, an open direction is to move beyond fixed patterns and learn the sparsity structure itself while still enforcing structured constraints such as s-column sparsity, bridging the gap between efficient post-training methods and more adaptive (but costlier) learned compression schemes. COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression"
        },
        {
            "title": "Impact Statement",
            "content": "This paper proposes COMPOT, training-free compression method for Transformer models that improves the trade-off between memory footprint and predictive performance. By reducing the hardware and energy costs associated with deploying large models, our work has the potential to make state-of-the-art language, vision, and audio models more accessible to researchers and practitioners with limited computational resources, and to slightly mitigate the environmental impact of large-scale inference. At the same time, more efficient compression can also lower the barrier to deploying powerful models in broader settings, including applications where risks around bias, privacy, misinformation, or misuse of language and generative models have been documented. Our method does not introduce qualitatively new risks beyond those already associated with the base models and downstream tasks, but it may amplify their reach by making such models easier to deploy and scale. We therefore recommend that COMPOT be used in conjunction with existing best practices for responsible model deployment (e.g., appropriate content filtering, monitoring, and domain-specific safeguards), and that any downstream use of compressed models continue to follow established guidelines for evaluating fairness, robustness, and potential harms."
        },
        {
            "title": "References",
            "content": "Aharon, M., Elad, M., and Bruckstein, A. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):43114322, 2006. Bao, C., Cai, J.-F., and Ji, H. Fast sparsity-based orthogonal dictionary learning for image restoration. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013. Bishop, C. M. and Nasrabadi, N. M. Pattern recognition and machine learning, volume 4. Springer, 2006. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, P. H., Yu, H.-F., Dhillon, I. S., and Hsieh, DRONE: Data-aware low-rank compression In Advances in Neural C.-J. for large NLP models. Information Processing Systems, volume 34, pp. 29321 URL https://proceedings. 29334, 2021. neurips.cc/paper/2021/hash/ f56de5ef149cf0aedcc8f4797031e229-Abstract. html. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Elad, M. Sparse and redundant representations: from theory to applications in signal and image processing. Springer Science & Business Media, 2010. and Wolf, T. Fourrier, C., Habib, N., Lozovskaya, A., Szafer, llm leaderboard Open https://huggingface.co/spaces/ K., v2. open-llm-leaderboard/open_llm_ leaderboard, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Optq: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Gong, Y., Chung, Y.-A., and Glass, J. AST: Audio SpecIn Proc. Interspeech 2021, pp. trogram Transformer. 571575, 2021. doi: 10.21437/Interspeech.2021-698. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874. COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Hsu, Y.-C., Hua, T., Chang, S., Lou, Q., Shen, Y., and Jin, H. Language model compression with weighted low-rank factorization. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=uPv9Y3gmAI5. Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. RACE: Large-scale ReAding comprehension dataset from examinations. In Palmer, M., Hwa, R., and Riedel, S. (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785 794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/ D17-1082. URL https://aclanthology.org/ D17-1082/. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Liu, D., Zhu, Y., Liu, Z., Liu, Y., Han, C., Tian, J., Li, R., and Yi, W. survey of model compression techniques: Past, present, and future. Frontiers in Robotics and AI, 12:1518965, 2025. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Mi, Z., Sun, B., Zhang, G. L., and Huang, S. Layer-wise dynamic rank for compressing large language models, 2025. URL https://arxiv.org/abs/2509.25622. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pp. 1525 1534, 2016. Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Alobeidli, H., Cappelli, A., Pannier, B., Almazrouei, E., and Launay, J. The refinedweb dataset for falcon LLM: Outperforming curated corpora with web data only. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum? id=kM5eGcdCzq. Qinsi, W., Ke, J., Tomizuka, M., Keutzer, K., and Xu, C. Dobi-svd: Differentiable svd for llm compression and some new perspectives. In The Thirteenth International Conference on Learning Representations, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Sanh, V., Debut, L., Chaumond, J., and Wolf, T. DistilBERT, distilled version of BERT: Smaller, faster, cheaper and lighter. In NeurIPS 2019 Workshop on Energy Efficient Machine Learning and Cognitive Computing, 2019. URL https://arxiv.org/abs/1910.01108. Schonemann, P. H. generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):110, 1966. Shopkhoev, D., Ali, A., Zhussip, M., Malykh, V., Lefkimmiatis, S., Komodakis, N., and Zagoruyko, S. Replaceme: Network simplification via depth pruning and transformer block linearization. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. Shopkhoev, D., Makhov, D., Zhussip, M., Ali, A., and Lefkimmiatis, S. Cospadi: Compressing llms via arXiv calibration-guided sparse dictionary learning. preprint arXiv:2509.22075, 2025b. Sprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., and Durrett, G. Musr: Testing the limits of chain-of-thought with multistep soft reasoning, 2024. URL https://arxiv. org/abs/2310.16049. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. 10 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Tang, Y., Wang, Y., Guo, J., Tu, Z., Han, K., Hu, H., and Tao, D. survey on transformer compression. arXiv preprint arXiv:2402.05964, 2024. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models, 2023. URL https: //arxiv.org/abs/2311.07911. Zhussip, M., Shopkhoev, D., Ali, A., and Lefkimmiatis, S. Share your attention: Transformer weight sharing via matrix-based dictionary learning. arXiv preprint arXiv:2508.04581, 2025. Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 57975808, Florence, Italy, July 2019. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/P19-1580. Wang, X., Alam, S., Wan, Z., Shen, H., and Zhang, M. Svdllm v2: Optimizing singular value truncation for large language model compression. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 42874296, 2025a. Wang, X., Zheng, Y., Wan, Z., and Zhang, M. Svd-llm: Truncation-aware singular value decomposition for large language model compression. In The Thirteenth International Conference on Learning Representations, 2025b. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. In Derczynski, L., Xu, W., Ritter, A., and Baldwin, T. (eds.), Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/ W17-4413. URL https://aclanthology.org/ W17-4413/. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning (ICML), 2023. Xv, L., Gao, J., Gao, X., Liu, T., and Fu, Y. Ara: Adaptive rank allocation for efficient large language model svd compression, 2025. URL https://arxiv.org/ abs/2510.19389. Yuan, Z., Shang, Y., Song, Y., Wu, Q., Yan, Y., and Sun, G. Asvd: Activation-aware singular value decomposition for compressing large language models, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? 11 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression A. Supplementary Material Due to main text limitation we present additional results and descriptions here to provide extra details and evidences on superiority of COMPOT compression method. A.1. COMPOT Optimization Details This appendix provides the derivation of analytical solution for sparse coding, detailed optimization procedure for (7), including the closed-form sparse coding update under orthogonality and the full alternating minimization algorithm. A.1.1. DERIVATION OF SPARSE CODING ANALYTICAL SOLUTION With DO fixed and ODO = Ik, each column update decouples: sOj arg min sORk (cid:101)wj DOsO 2 s.t. sO0 s. Let zj (cid:101)wj Rk. Using orthogonality, (cid:101)wj DOsO2 2 = (cid:101)wj 2 2 Ozj + sO2 2 = (cid:101)wj2 2 + sO zj2 2 zj2 2, so (12) is equivalent to projecting onto the set of s-sparse vectors: sOj Hs(zj) with zj (cid:101)wj, (12) (13) where Hs() keeps the entries of largest magnitude and zeros the rest (hard thresholding). This yields an exact global minimizer of (12) under ODO = Ik, avoiding iterative sparse pursuits. A.1.2. OBJECTIVE IN THE WHITENED SPACE Recall the calibration loss (4) and the Gram matrix = XX. Assuming 0, let = LL be its Cholesky factorization. Then X(W (cid:99)W)2 = Tr(cid:0)(W (cid:99)W)G(W (cid:99)W)(cid:1) = L(W (cid:99)W)2 . (14) Defining (cid:102)W LW and (cid:102)(cid:99)W (cid:99)W, minimizing functional error in the original space is equivalent to minimizing reconstruction error in the whitened space: min (cid:99)W X(W (cid:99)W)2 min (cid:102)(cid:99)W (cid:102)W (cid:102)(cid:99)W2 . A.1.3. COMPOT FACTORIZATION PROBLEM COMPOT solves the orthogonal sparse factorization in whitened coordinates: min DO,SO (cid:102)W DOSO2 s.t. ODO = Ik, sOj0 [n], with DO Rmk, (complete/undercomplete), and SO Rkn. After optimizing (16), we map the solution back to the original parameter space as in (8): (cid:99)W ASO, LDO. The dewhitening factor is computed offline during compression; inference uses only (A, SO). A.1.4. CLOSED-FORM SPARSE CODING UNDER ORTHOGONALITY Fix DO with ODO = Ik. The sparse coding step for column (cid:101)wj is min sRk (cid:101)wj DOs2 s.t. 2 s0 s. 12 (15) (16) (17) (18) COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Algorithm 1 COMPOT factorization for one projection matrix Require: Weight matrix Rmn; calibration inputs RN m; dictionary size m; sparsity s; number of iterations . Ensure: Compressed factors Rmk and SO Rkn such that ASO. 1: Compute XX and its Cholesky factor = LL. 2: Whiten weights: (cid:102)W LW. 3: Initialize DO Rmk with 4: for = 1, . . . , do 5: ODO = Ik. (cid:0)D Sparse coding (closed form): SO Hs Dictionary update (Procrustes): (cid:102)WS (cid:102)W(cid:1). O; compute thin SVD = PΛQ; set DO PQ. 6: 7: end for 8: Dewhiten dictionary: LDO. 9: return (A, SO). Let zj O (cid:101)wj. Using ODO = Ik, (cid:101)wj DOs2 (cid:101)wj + DOs2 2 = (cid:101)wj2 Since the first and last terms in (19) do not depend on s, (18) is equivalent to 2 2 sD 2 2 szj + s2 2 = (cid:101)wj2 = (cid:101)wj2 2 + zj2 2 zj2 2. min sRk zj2 2 s.t. s0 s, i.e., Euclidean projection of zj onto the set of s-sparse vectors. This projection has closed form: = Hs(zj), zj = (cid:101)wj, (19) (20) (21) where Hs() retains the largest entries by magnitude and sets the rest to zero (ties can be broken arbitrarily without changing optimality). Applying this column-wise yields the matrix update SO Hs (cid:0)D (cid:102)W(cid:1). Thus, under orthogonality, sparse coding is solved exactly without iterative pursuit methods. A.1.5. CLOSED-FORM DICTIONARY UPDATE VIA ORTHOGONAL PROCRUSTES Fix SO. The dictionary update solves min ODO=Ik (cid:102)W DOSO2 . (22) (23) Rmk. Expanding (23) and dropping constants shows it is equivalent to maximizing Tr(D OM) subject ODO = Ik, which is the classical orthogonal Procrustes problem. Let = PΛQ be the thin SVD; then the Let (cid:102)WS to optimizer is DO PQ. (24) A.1.6. FULL ALTERNATING MINIMIZATION PROCEDURE Algorithm 1 summarizes the full optimization for single projection matrix given calibration inputs X. The procedure alternates between the exact sparse coding update (22) and the Procrustes update (24). Any initialization with columnorthonormal DO is valid; in practice, one may use heuristic such as an orthonormal basis derived from (cid:102)W. A.2. Dynamic Compression Ratio Allocation Transformer models exhibit heterogeneous redundancy patterns across different layers and projection types. Uniform compression ratios often lead to suboptimal performance as sensitive layers may be over-compressed while redundant 13 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression layers remain under-compressed. To address this challenge, we propose one-shot global allocation strategy that efficiently distributes model-wide compression budget while respecting per-matrix constraints. Our allocation procedure operates on the singular values of normalized weight matrices in the original (non-whitened) space. We normalize each weight matrix Wi by its Frobenius norm to obtain Wi = Wi/WiF , which equalizes the scale of singular values across different layers and projection types. This normalization is crucial because raw weight matrices can have vastly different magnitudes depending on their position in the network and function. The core insight behind our approach is that singular values provide natural importance metric for compression: smaller singular values contribute less to the matrix reconstruction and can be safely truncated first. By pooling all singular values across the model into global multiset, we can apply unified importance threshold that respects the global compression budget while allowing layer-specific compression ratios to emerge naturally. To prevent pathological allocation scenarios, we incorporate several critical constraints: Minimum compression guard: We establish lower bound crmin on the compression ratio for each matrix to ensure no layer receives negligible compression, which would waste the global budget. Maximum compression guard: We enforce an upper bound crmax to prevent over-compression of sensitive layers that could disproportionately harm performance. Non-beneficial factorization handling: For matrices where rmin(mi + ni) mini (indicating that the factorized representation would consume more memory than the original dense matrix), we exclude them from compression entirely and mark them as DENSE. Our allocation algorithm (Algorithm 2) efficiently implements this strategy through constrained selection process. First, we allocate the mandatory minimum truncations to satisfy the minimum compression guards. Then, from the remaining untruncated singular values, we select the smallest ones until the global parameter budget is satisfied, while respecting the maximum truncation caps. This greedy selection is optimal under our objective of minimizing the total reconstruction error in the Frobenius norm. The computational complexity of our allocation strategy is dominated by the SVD computations for each weight matrix, which is O((cid:80) min(mi, ni)2 max(mi, ni)). However, since these computations are performed only once during compression preparation and can be parallelized across layers, the practical overhead remains modest. For Llama3.2-1B model, the entire allocation procedure completes in under 1 minute on single GPU without any optimizations. Our one-shot approach differs fundamentally from iterative methods that require multiple rounds of compression and validation. By avoiding iterative search over layer-wise compression ratios, we eliminate the need for additional calibration data passes or performance estimation heuristics, making our method both deterministic and computationally efficient. A.3. Comparison with Low-rank and Sparse Dictionary Learning Methods on Small Models In this section we provide comparison of the COMPOT method across different models: Qwen3-0.6B and Llama3.2-1B across different compression ratios to verify its scalability. The corresponding results are presented in Tables 11 and 10. For small models COMPOT in both static and dynamic CR cases outperforms SVD-based and dictionary learning based methods by wide margin. A.4. Evaluation on Recent Benchmarks To validate our method, we report results on set of recent and more challenging benchmarks that better reflect modern LLM evaluation. We include Open LLM Leaderboard v2 as an aggregate, community-maintained reference (Fourrier et al., 2024), which includes IFEval (Zhou et al., 2023), MATH (Hendrycks et al., 2021b), GPQA (Rein et al., 2023), BBH (Suzgun et al., 2022) and MUSR (Sprague et al., 2024), ensuring that observed improvements generalize beyond single evaluation suite. See Table 12 for detailed results. A.5. Wall-clock Time for Optimization In Table 13 we report the wall-clock time required to compress single layer of Llama 3.21B under static compression ratio of 0.2 with k/s = 2. All measurements were obtained on single NVIDIA A100 GPU; note that different layers are COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Algorithm 2 ALLOCATE-GLOBAL: pooled-SV truncation with per-matrix CR guards (Frobenius-normalized, no calibration) Require: Matrices {Wi Rmini}N i=1, target global compression ratio cr, (optional) per-matrix guard bounds (crmin, crmax). Ensure: Per-matrix compression ratios {cri} (and implied retained ranks {ri}). 1: Normalize and compute spectra: Wi Wi/WiF , compute singular values σi,1 σi,Li, Li = min(mi, ni). 2: Guard rank bounds (SVD storage model): define retained-rank interval ri [rmin ] induced by , rmax i (crmin, crmax). 3: Non-beneficial criterion: mark DENSE if rmin 4: Global truncation principle: choose truncation counts ti [tmin (mi + ni) mini. , tmax i subject to (cid:80) ti = K, where Ti are truncated indices and is the global truncation budget. 5: Constrained pooled selection (closed form): (i) allocate mandatory truncations ti tmin ] with ti = Li ri to minimize (cid:80) (cid:80) ℓTi σi,ℓ tmin ) smallest ones, respecting caps ti tmax ; (ii) among all remaining ; this yields singular values {σi,ℓ} not yet truncated, pick the (K (cid:80) final {ti}. 6: Set by global budget: let P0 = (cid:80) feasible [(cid:80) tmin , (cid:80) tmax ]) such that the implied parameter count mini and Ptgt = (1 cr)P0. Find the smallest (e.g., by bisection over (K) = (cid:88) mini + (cid:88) iDENSE /DENSE ri(K)(mi + ni) Ptgt, ri(K) = Li ti(K). If during the search any active satisfies ri(K)(mi + ni) mini, reclassify it as DENSE and recompute. 7: Return per-matrix ratios: for non-dense i, cri 1 ri(mi + ni) mini ; for dense i, set cri 0. Table 10. Performance comparison under static (COMPOT) and dynamic (COMPOT) vs state-of-the-art SVD-based SVD-LLM and dictionary-learning CoSpaDi on Llama3.2-1B at different compression levels on different benchmarks. Best results are highlighted with bold. Method CR Llama3.2-1B SVD-LLM CoSpaDi COMPOT COMPOT SVD-LLM CoSpaDi COMPOT COMPOT SVD-LLM CoSpaDi COMPOT COMPOT 0.2 0.3 0.4 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 74.5 62.1 66.1 69.7 70.6 55.7 56.9 64.2 67.0 51.8 53.5 57.0 62.9 63.7 36.4 42.9 52.1 54.6 30.1 32.4 41.0 46.8 27.3 28.2 31.1 37.5 63.0 24.4 38.4 42.1 47.6 9.1 18.2 27.5 37.9 1.3 3.8 7.9 21.3 60.5 36.0 39.9 49.9 52.5 30.5 31.9 39.8 47.1 26.9 27.8 31.7 38. 36.2 25.1 26.0 30.2 31.1 21.5 22.1 26.0 27.1 22.9 23.0 22.1 25.3 88.3 64.9 71.6 81.8 81.6 45.9 56.7 73.2 77.3 32.3 36.9 53.0 62.4 37.8 29.0 31.7 33.9 35.7 25.8 28.0 31.4 33.1 24.4 24.0 27.0 30.6 37.0 23.0 24.8 28.5 26.7 23.2 23.1 23.5 23.8 23.0 23.1 23.1 23.0 57.6 37.6 42.7 48.5 50.1 30.2 33.7 40.8 45.0 26.2 27.5 31.6 37.6 1.2E+01 1.7E+02 6.4E+01 2.5E+01 2.1E+01 5.9E+02 2.9E+02 5.5E+01 3.6E+01 1.6E+03 8.0E+02 2.8E+02 1.3E+ 5.7E+00 1.7E+02 3.5E+01 2.1E+01 1.4E+01 2.5E+03 6.6E+02 8.4E+01 3.2E+01 3.3E+04 9.2E+03 2.2E+03 1.9E+02 15 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 11. Performance comparison under static (COMPOT) and dynamic (COMPOT) vs state-of-the-art SVD-based SVD-LLM and dictionary-learning CoSpaDi on Qwen3-0.6B at different compression levels on different benchmarks. Best results are highlighted with bold. Method CR Qwen3-0.6B SVD-LLM CoSpaDi COMPOT COMPOT SVD-LLM CoSpaDi COMPOT COMPOT SVD-LLM CoSpaDi COMPOT COMPOT 0.2 0.3 0.4 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 67.5 57.0 59.3 62.2 61.2 52.2 56.4 58.2 61.3 53.3 51.6 54.5 56.9 47.4 33.0 37.5 40.6 42.6 29.0 32.3 34.8 38.3 26.9 28.2 29.7 32.7 40.0 12.6 18.7 15.9 20.6 8.7 14.3 13.6 20.8 3.8 7.9 8.4 17. 55.6 33.6 37.2 41.3 42.9 28.2 31.0 34.1 40.5 26.5 27.3 28.8 31.7 34.3 24.8 27.1 29.6 28.8 23.8 24.0 27.1 27.1 23.9 24.1 23.6 25.1 83.5 54.9 68.6 70.8 74.0 42.1 57.4 59.9 65.2 32.5 43.8 44.3 57.4 33.7 29.5 32.4 33.0 32.3 29.1 31.0 31.7 32.7 25.4 27.0 28.3 31.2 40.2 26.1 26.6 26.3 24.9 24.4 24.6 26.3 27.2 23.6 24.0 23.5 24.7 50.3 33.9 38.4 40.0 40.9 29.7 33.9 35.7 39.1 27.0 29.2 30.1 34. 2.6E+01 3.1E+03 1.9E+03 1.4E+03 7.0E+01 1.9E+04 3.8E+03 3.1E+03 2.5E+02 7.2E+04 1.2E+04 6.7E+03 3.8E+03 2.5E+01 7.3E+03 9.4E+02 4.2E+02 1.9E+02 1.3E+05 1.1E+04 3.6E+03 3.8E+02 5.4E+06 2.4E+05 1.2E+05 5.0E+03 Table 12. Performance comparison under static (COMPOT) and dynamic (COMPOT) vs SVD-LLM on Open LLM Leaderboard v2 (Fourrier et al., 2024) Method CR BBH GPQA IFEVAL MATH HARD MMLU Pro MUSR Accuracy 12.69 0 1.06 0.91 0 0.23 0.68 0 0.98 0.38 52.49 1.06 6.72 12.76 1.06 1.51 2.49 0.83 1.06 0.98 24.26 11.29 12.81 14.81 11.22 10.85 11.43 10.98 10.85 10.66 47.62 26.3 35.92 38.55 18.73 29.46 32.85 11.54 21.23 26.44 31.35 35.58 35.19 33.2 33.73 32.8 35.71 36.24 36.51 33. 43.12 39.81 45.11 41.8 41.4 40.61 40.34 37.57 41.27 38.89 Qwen3-0.6B SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT Qwen3-8B SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT 0.2 0. 0.4 0.2 0.3 0.4 36.85 28.95 29.2 30.19 29.73 30.83 29.99 29.01 29.21 29. 60.84 41.03 49.33 50.22 34.3 41.45 44.87 30.24 34.13 37.51 28.27 24.75 27.94 25.34 25.5 23.49 26.59 24.33 25.84 23.74 36.33 28.27 30.54 31.96 25.59 28.86 28.94 22.99 24.75 28.1 27.58 25.3 27.22 25.9 24.34 25.06 27.7 22.3 24.58 26.62 39.21 25.66 30.46 33.45 22.66 26.74 31.89 22.42 23.14 26.14 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression independent, so the procedure is trivially parallelizable across processes or devices. For CoSpaDi estimation, we follow the original paper and implement K-SVD-based dictionary learning, i.e., we replace the orthogonal Procrustes (OPP) update in COMPOT with K-SVD update. In line with CoSpaDi, we use power iterations instead of full SVD for dictionary updates, with 60 K-SVD iterations and 8 power iterations per update. Because our preliminary timing used only 20 K-SVD iterations, the reported CoSpaDi runtime in Table 13 is obtained by extrapolating linearly to 60 iterations (i.e., multiplied by factor of 3). For COMPOT we use 20 alternating iterations, which is the configuration used for all main results unless otherwise stated. We observe that SVD-LLM is substantially faster than both CoSpaDi and COMPOT, reflecting the computational simplicity of its single truncated SVD factorization per matrix. Nevertheless, COMPOT achieves significantly lower wall-clock time than CoSpaDi: even per iteration, COMPOTs closed-form Orthogonal Procrustes Problem (OPP) update (thin SVD on an matrix) combined with hard-thresholding sparse coding is approximately 2 faster than CoSpaDis K-SVD dictionary updates with power iterations and OMP sparse coding. The computational advantage widens further when accounting for CoSpaDis typically higher iteration requirements. key insight behind this efficiency is that when the dictionary is constrained to be orthonormal (as in COMPOT), hard-thresholding yields identical sparse coding solutions to Orthogonal Matching Pursuit (OMP) in theory, while being substantially faster in practice. This equivalence holds except in rare cases involving numerical instabilities at identical coefficient magnitudes. This computational profile highlights fundamental advantage of orthogonal dictionary learning for large-scale Transformer compression: it preserves the modeling flexibility of sparse, union-of-subspaces representations while maintaining optimization overhead comparable to SVD-style methods. The result is favorable trade-off between compression performance and computational cost. Table 13. Wall-clock time in seconds required for optimization with SVD-LLM, CoSpaDi and COMPOT for Llama3.2-1B with static 0.2 compression ratio. Layer model.layers.0.mlp.up proj model.layers.0.mlp.down proj model.layers.0.mlp.gate proj model.layers.0.self attn.q proj model.layers.0.self attn.k proj model.layers.0.self attn.v proj model.layers.0.self attn.o proj Dimensions (2048, 8192) (8192, 2048) (2048, 8192) (2048, 2048) (2048, 512) (2048, 512) (2048, 2048) SVD-LLM CoSpaDi COMPOT Speedup over CoSpaDi 17.84 19.23 17.78 5.37 1.99 1.77 5.49 508.82 255.51 522.17 154.01 44.70 44.56 153.92 28.52x 13.29x 29.37x 28.68x 22.41x 25.12x 28.06x 1.96 13.99 2.19 1.98 1.03 0.50 1.21 AVERAGE 3.27 240.53 9.92 24.23x A.6. Allocation Strategy Results In this part we visualize the results of our allocation strategy across different models used throughout our extensive study (Figures 6 12) A.7. Acceleration Strategies for Alternating Minimization Process While the proposed COMPOT framework is fundamentally optimized over fixed number of iterations, we introduce an early stopping mechanism to mitigate excessive computational overhead. To optimize complexity and accelerate the compression process, we employ relative Mean Squared Error (MSE) criterion. This strategy avoids the redundant execution of alternating minimization steps for all transformer projections by terminating the optimization phase once the optimization process reaches stable convergence point. The results in Table 14 illustrate the significant influence of the relative tolerance (τ ) on both the convergence rate and the final performance of the framework. In our experimental setup, the dictionary is randomly initialized. We observe direct correlation between the stringency of the early stopping threshold and the resulting model accuracy: specifically, lower relative tolerance thresholds require greater number of optimization iterations. This extended optimization period ensures more precise alignment with the weight distributions, ultimately yielding superior performance across downstream COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 4. Results of COMPOT allocation strategy for Llama3.2-1B Figure 5. Results of COMPOT allocation strategy for Qwen3-0.6B 18 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 6. Results of COMPOT allocation strategy for Llama3-8B Figure 7. Results of COMPOT allocation strategy for Qwen3-8B 19 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 8. Results of COMPOT allocation strategy for Llama2-7B Figure 9. Results of COMPOT allocation strategy for Llama-7B COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 10. Results of COMPOT allocation strategy for OPT-6.7B Figure 11. Results of COMPOT allocation strategy for Llama-13B benchmarks. A.8. Optimal Dictionary-to-sparsity ratio We evaluate the performance of the proposed COMPOT framework using the Llama 3.2-1B architecture as the primary backbone. The experimental setup involves optimizing the model under varying dictionary-to-sparsity (k/s) ratios, while 21 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Figure 12. Results of COMPOT allocation strategy for Llama-30B maintaining fixed computational budget of = 100 iterations. To ensure robust assessment of the learning process, the dictionary is initialized randomly. Our empirical results demonstrate (see Table 15) that k/s ratio of 2 yields the optimal balance between representation capacity and sparse regularization, achieving the highest accuracy across the evaluated benchmarks. A.9. Additional results for vision and audio models This section provides complete evaluation results for vision and audio models across various compression ratios (CR). For each benchmark, we report the corresponding CR and include the uncompressed model alongside SVD-LLM (Wang et al., 2025b) as reference points. Unless otherwise noted, we adhere to the same evaluation protocol used in the main paper (employing identical prompts, decoding strategies, preprocessing, and metric computation) to ensure direct comparability across all methods. Vision. For the Qwen 3 VL 8B-Instruct model, we compressed only the language module using 256 samples from the MathVerse training data for calibration. Comprehensive results on four vision-language benchmarks, namely, MMMU, OCRBench, RealWorldQA, and MMStar, are presented in Table 16. Across all compression ratios, our COMPOT method consistently preserves substantially more performance than the SVD-LLM baseline. Audio. For all Whisper models in this experiment, we compressed the decoder using 1024 samples from the LibriSpeech clean validation dataset. Table 17 reports automatic speech recognition (ASR) results, measured in word error rate (WER, lower is better), for Whisper Base-EN, Medium-EN V3, and Large V3 on the LibriSpeech test-clean and test-other sets. We evaluate multiple compression ratios where applicable. Overall, while SVD-LLM exhibits rapid performance degradation with stronger compression (particularly for smaller models), COMPOT remains close to the uncompressed baseline across wide range of CRs. In several settings, COMPOT matches or even slightly improves upon the original models performance. 22 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 14. Performance of COMPOT under varying relative tolerance (τ ) thresholds for early stopping. Results are evaluated using Llama 3.2-1B with the compression rate and dictionary-to-sparsity ratio fixed at 2. The maximum number of iterations is set to 150. Bold values indicate the best performance for each metric. Method Llama 3.2-1B COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT Relative Accuracy Tolerance PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Perplexity WikiText LAMBADA 101.0 101.5 102.0 102.5 103.0 103.5 104.0 0. 0.6779 0.6736 0.6823 0.6931 0.6980 0. 0.7002 0.6366 0.4758 0.4894 0.5026 0. 0.5169 0.5226 0.5223 0.6295 0.3503 0. 0.3831 0.3887 0.4073 0.4143 0.4198 0. 0.3620 0.8830 0.3779 0.3700 0.5761 0. 0.2645 0.7680 0.3368 0.2463 0.4467 0. 0.2816 0.7930 0.3292 0.2453 0.4576 0. 0.2969 0.7980 0.3455 0.2638 0.4687 0. 0.3029 0.8060 0.3378 0.2710 0.4741 0. 0.2901 0.8050 0.3292 0.2678 0.4743 0. 0.2986 0.8180 0.3416 0.2734 0.4820 0. 0.3029 0.8160 0.3455 0.2799 0.4853 11. 33.61 29.12 26.08 24.20 22.44 21. 20.45 5.73 31.13 29.19 27.38 26. 25.02 24.57 24.39 Table 15. Performance of COMPOT across different dictionary-to-sparsity (K/S) ratios. The model compression rate is fixed at 20%. All experiments are conducted using the Llama 3.2-1B architecture. Values in bold indicate the highest performance achieved for each respective metric. Method Llama 3.2-1B COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT COMPOT k/s Accuracy ratio PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Perplexity WikiText LAMBADA 1.2 1. 1.6 1.8 2.0 2.4 2.8 3. 3.6 4.0 0.7453 0.6420 0.6665 0. 0.6915 0.6964 0.6910 0.6855 0.6746 0. 0.6589 0.6366 0.4027 0.4573 0.4909 0. 0.5225 0.5155 0.5076 0.4962 0.4728 0. 0.6295 0.3022 0.3664 0.4069 0.4306 0. 0.4262 0.3953 0.3571 0.3747 0.2990 0. 0.3620 0.8830 0.3779 0.3700 0.5761 0. 0.2577 0.7250 0.3110 0.2311 0.4085 0. 0.2790 0.7790 0.3139 0.2487 0.4434 0. 0.2918 0.7990 0.3340 0.2582 0.4676 0. 0.2918 0.8140 0.3445 0.2585 0.4788 0. 0.3012 0.8090 0.3407 0.2744 0.4810 0. 0.3020 0.7890 0.3435 0.2512 0.4751 0. 0.3012 0.7960 0.3378 0.2555 0.4696 0. 0.2765 0.7710 0.3388 0.2483 0.4542 0. 0.2679 0.7860 0.3311 0.2510 0.4502 0. 0.2739 0.7150 0.3196 0.2390 0.4205 11. 73.73 35.38 25.29 19.95 20.52 18. 23.77 30.55 31.01 50.58 5.73 83. 41.48 29.75 26.18 24.55 25.42 26. 28.78 31.86 37.71 A.10. Reproducing SVD-LLM V2 Codebase and evaluation protocol. We based our reproduction on the official SVD-LLM repository1, which provides scripts for compression and evaluation. To ensure comparability, we adopted the same evaluation protocol used by the repository and aligned our calibration setup accordingly: 256 WikiText samples with context length 2048, and perplexity evaluated using the provided scripts on WikiText and C4. We computed zero-shot task accuracy with lm-evaluation-harness (v0.4.8) and report unnormalized accuracies to match the SVD-LLM-style reporting used in their released pipeline. Implementing SVD-LLM V2 components. SVD-LLM V2 proposes (i) assigning non-uniform compression ratios using theoretical truncation loss and (ii) loss-optimized truncation for lower and more stable truncation loss (Wang et al., 2025a). Since the repository does not provide dedicated, ready-to-run SVD-LLM V2 implementation, for the main-text comparison we extended the released SVD-LLM code by incorporating both the dynamic allocation strategy and the truncation strategy described in the V2 paper. During re-implementation, we encountered multiple ambiguities and minor inconsistencies between pseudo-code and textual descriptions; Below we provide concrete functions that we incorporated in SVDLLM.py to match SVD-LLM V2 paper. 1 def theoretical_loss(W: torch.Tensor, L: torch.Tensor, cr: float): 2 \"\"\" Theoretical loss according to the SVD-LLM v2 paper in the whitened space using Cholesky factor \"\"\" 3 4 1https://github.com/AIoT-MLSys-Lab/SVD-LLM COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 16. Full vision-language results for Qwen3-VL 8B under compression. We report benchmark scores on MMMU, OCRBench, RealWorldQA, and MMStar, along with the uncompressed model. Method Qwen 3 VL 8B SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT SVD-LLM COMPOT COMPOT CR MMMU val OCRBench RealWorldQA MMStar Average 0.20 0.20 0.20 0.30 0.30 0.30 0.40 0.40 0.40 0.5256 0.2922 0.4189 0.4467 0.2333 0.3378 0.3578 0.2467 0.2367 0.2511 0.6315 0.3161 0.5080 0.5417 0.0398 0.3574 0.4050 0.0030 0.0576 0. 0.8259 0.3510 0.6410 0.6690 0.2120 0.5260 0.5760 0.0390 0.3300 0.2630 0.6967 0.5294 0.5922 0.6222 0.2562 0.5137 0.5098 0.0000 0.3359 0.4588 0.67 0.37 0.54 0.57 0.19 0.43 0.46 0.07 0.24 0.28 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 = W.to(device=\"cuda\", dtype=torch.float64) = L.to(device=\"cuda\", dtype=torch.float64) # 1. Whiten the weight W_whitened = @ # 2. Truncate in the whitened space rank = int(W.shape[0] * W.shape[1] * cr / (W.shape[0] + W.shape[1])) U, S, Vh = = U[:, :rank].cuda() = S[:rank].cuda() Vh = Vh[:rank, :].cuda() W_whitened_trunc = @ torch.diag(S) @ Vh torch.linalg.svd(W_whitened, full_matrices=False) # Compute theoretical loss in the whitened space loss = torch.linalg.norm((W_whitened - W_whitened_trunc).squeeze(0), \"fro\").item() return loss Listing 1. Theoretical loss calculation in whitened space 1 def cr_allocation(model_name: str, model: nn.Module, profiling_mat, target_cr, dev): \"\"\" Perform allocation of compression ratio according to the description in SVD-LLM v2 paper \"\"\" print(\"Starting dynamic allocation...\") model.eval() if opt in model_name: 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 layers = model.model.decoder.layers layer_prefix = \"model.decoder.layers\" projection_types = [ \"fc1\", \"fc2\", \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.out_proj\", else: ] layers = model.model.layers layer_prefix = \"model.layers\" projection_types = [ \"mlp.up_proj\", \"mlp.down_proj\", COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 17. ASR Performance (WER ) on LibriSpeech benchmarks for Whisper family. Method CR WERtest-clean WERtest-other Whisper Base-EN SVD-LLM COMPOT SVD-LLM COMPOT SVD-LLM COMPOT Whisper Medium-EN SVD-LLM COMPOT SVD-LLM COMPOT SVD-LLM COMPOT SVD-LLM COMPOT SVD-LLM COMPOT Whisper Large V3 SVD-LLM COMPOT SVD-LLM COMPOT - 0.05 0.05 0.10 0.10 0.15 0. - 0.05 0.05 0.10 0.10 0.15 0.15 0.20 0.20 0.30 0.30 - 0.2 0.2 0.3 0.3 4.40 32.37 5.41 52.80 5.88 85.64 8.57 2.93 3.84 2.92 5.11 3.07 6.95 3.12 12.16 3.26 32.20 4.08 2.74 4.12 2.46 12.78 2.74 10.40 32.18 12.34 51.02 12.75 83.31 15. 5.93 7.26 6.06 8.39 6.18 10.14 6.34 15.23 6.53 33.09 7.78 4.53 6.8 4.51 15.54 5.21 \"mlp.gate_proj\", \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\", ] # Iterate over groups of same projection type R_d_all = {} for projection_type in projection_types: # Iterate over groups of weights print(f\"Working on {projection_type}\") R_d = [] # Initialize the compression ratio list for concrete group of projection layers L_G = [] # Initialize the loss list in the group of projection layers for in tqdm(range(len(layers)), total=len(layers)): # Get concrete projection weight full_proj_name = f\"{layer_prefix}.{i}.{projection_type}\" w: torch.Tensor = model.get_submodule(full_proj_name).weight.data.T # Get Cholesky factor and its inverse = profiling_mat[i][\"scaling_diag_matrix\"][projection_type].T.to(dev) # Compute minimum theoretical loss L_min = theoretical_loss(W=w, L=L, cr=target_cr) L_G.append(L_min) # Normalize L_G L_G = [1 / math.log(L_G_) for L_G_ in L_G] # Allocate ranks inside of group R_d = [len(L_G) * target_cr * L_G_ / sum(L_G) for L_G_ in L_G] for in range(len(layers)): # Get concrete projection weight full_proj_name = f\"{layer_prefix}.{i}.{projection_type}\" R_d_all[full_proj_name] = R_d[i] return R_d_all 25 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 47 48 49 50 51 53 54 55 56 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression Table 18. Perplexity (PPL, ) and average accuracy (Avg. Acc., ) for Llama-13B and Llama-30B. Method"
        },
        {
            "title": "Original",
            "content": "Llama-13B PPL Avg. Acc. 5.09 0.59 Llama-30B PPL Avg. Acc. 4.10 0.61 FWSVD ASVD SVD-LLM SVD-LLM v2 COMPOT 15.98 6.74 6.61 5.46 6. 0.43 0.54 0.55 0.56 0.57 20.54 22.71 5.63 4.71 5.00 0.42 0.44 0.57 0.60 0.60 Listing 2. Dynamic compression ratio allocation Model coverage and reproduction gaps. We attempted to reproduce the reported SVD-LLM V2 results for OPT-6.7B and Llama3-8B but found the public repository/environment not immediately compatible with these models; community reports also describe failures or substantial degradations for Llama-3 settings (e.g., GitHub Issues #49 and #34)23. Following troubleshooting recommendations from the repository and related GitHub discussions, we still cant make OPT-6.7 running due to problems with inversion of Cholesky factor. Regarding Llama3-8B we successfully achieved results but they are significantly worse than that reported in the paper. For this reason, we argue that at this moment there is no possibility to provide fair comparison with SVD-LLM V2. Possible sources of discrepancy. The SVD-LLM repository includes an optional post-compression parameter update step (fine-tuning) and related discussions in the issue tracker (e.g., Issue #20)4. If additional fine-tuning or other unpublished post-processing was used in the SVD-LLM V2 experiments, it could partially explain gaps between paper-claimed numbers and strictly post-training reproductions. We therefore (i) report our reproduction strictly without additional fine-tuning, and (ii) separate paper-claimed vs. reproduced results in all SVD-LLM V2 comparisons. Results on larger scale. Table 18 reports results for Llama-13B and Llama-30B using the SVD-LLM evaluation protocol to enable like-for-like comparison. COMPOT (dynamic allocation) matches or improves upon prior SVD-based and activation-aware baselines, and is comparable to the SVD-LLM V2 numbers reported in the literature. However, we treat SVD-LLM V2 as reference point: dedicated, ready-to-run V2 implementation is not publicly available, and our attempts to reproduce V2-style results at smaller scales showed sizable gaps relative to the paper-reported values. We therefore report only the paper-claimed numbers in the appendix, and interpret the V2 comparison in Table 18 accordingly. A.11. Detailed Description on Comparison with Dobi-SVD direct comparison between COMPOT and Dobi-SVD requires careful methodological alignment, as the two approaches differ fundamentally in their compression paradigms. We identify two critical distinctions that limit comparability: (1) Training-free vs. training-based compression. COMPOT operates strictly in the post-training regime without any gradient-based optimization. In contrast, Dobi-SVD employs backpropagation to optimize layer-wise ranks through differentiable truncation objective (Qinsi et al., 2025). This training stepabsent in COMPOTintroduces confounding factor that advantages Dobi-SVD through implicit fine-tuning of the compressed model, violating the core premise of training-free compression. (2) Remapping-induced overparameterization. Dobi-SVD introduces remapping operation that reparameterizes factorized layers to match the original dense matrix dimensions before quantization. Crucially, this remapping often increases the parameter count of the factorized representation. When matrix factorization (yielding factorization compression ratio CRf act) is followed by quantization to bits (original weights stored in 16-bit FP16), the effective model-wide compression 2https://github.com/AIoT-MLSys-Lab/SVD-LLM/issues/49 3https://github.com/AIoT-MLSys-Lab/SVD-LLM/issues/34 4https://github.com/AIoT-MLSys-Lab/SVD-LLM/issues/ 26 COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression ratio is: CRtarget = 1 (1 CRf act) 16 . (25) For Dobi-SVDs 8-bit quantization (b = 8), Equation (25) simplifies to CRtarget = 1 2 (1 + CRf act). Critically, when remapping yields CRf act < 0 (i.e., the factorized representation consumes more parameters than the original dense matrix), the net compression arises entirely from quantization rather than structural factorization. In such cases, Dobi-SVD effectively trades parameter inflation for quantization headrooma strategy orthogonal to COMPOTs goal of reducing stored parameters through structural compression. To ensure methodologically sound comparison, we evaluate Dobi-SVD without remapping (denoted Dobi-SVD), which preserves its factorization-based compression objective. Table 19 reports results on Llama2-7B under identical evaluation protocols (WikiText/C4 perplexity via SVD-LLM scripts; zero-shot accuracy via lm-eval 0.4.8). COMPOT consistently outperforms Dobi-SVD across compression ratios while remaining strictly training-free. When remapping is enabled, Dobi-SVDs gains derive predominantly from quantization rather than factorization efficiency. For instance, at target CRtarget = 0.2, Dobi-SVD applies negative factorization compression (CRf act = 0.6) followed by 8-bit quantization to achieve net compressioneffectively overparameterizing the model by 60% to exploit quantization benefits. This paradigm shift renders comparisons against pure factorization methods like COMPOT fundamentally misaligned. We include these results in Table 19 for completeness, but emphasize that they reflect quantization efficacy rather than factorization quality. Table 19. Comparison of COMPOT with Dobi-SVD on Llama2-7B without (denoted with *) and with remapping. We provide also CR from matrix factorization (Fact. CR) and quantization (Quant. CR) to achieve specified target CR. In most cases Dobi-SVD with remapping leads to overparametrization with matrix factorization rather than compression Method Target CR Fact. CR Quant. CR WikiText C4 Openb. Arc WinoG. HellaS. Arc PIQA MathQA Average Llama2 7B Dobi-SVD* Dobi-SVD COMPOT Dobi-SVD* Dobi-SVD COMPOT Dobi-SVD* Dobi-SVD COMPOT 0. 0.4 0.6 0.2 -0.6 0.2 0.4 -0.2 0.4 0.4 0.2 0.4 0.5 0.5 0.5 5.49 9.39 5.92 6.22 17.50 7.89 8.91 53.37 9.66 32.10 7.27 19.70 8.68 9.34 55.58 14.23 20.40 200.11 20.28 179. 44.0 32.8 42.2 30.4 29.2 37.2 25.2 26.0 35.6 13.4 74.6 47.6 72.3 72.6 34.4 59.4 63.1 28.3 44.7 33.8 69.0 58.5 69.1 67.6 54.4 62.7 63.5 50.5 57.9 53.4 76.3 51.5 72.9 51.8 35.4 60.6 39.6 28.4 49.6 28.8 45.5 29.3 44.0 39.6 24.5 35.0 28.5 24.7 27.9 18.8 78.6 65.2 78.1 76.4 57.3 72.1 67.0 51.9 64.3 54. 28.4 22.7 27.5 26.2 21.3 25.5 23.6 22.0 23.6 22.1 59.5 43.9 58.0 52.1 36.6 50.4 44.4 33.1 43.4 32."
        }
    ],
    "affiliations": [
        "Fundamental Research Center MWS AI",
        "ITMO"
    ]
}