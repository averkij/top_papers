{
    "paper_title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
    "authors": [
        "Jinhao Duan",
        "Fei Kong",
        "Hao Cheng",
        "James Diffenderfer",
        "Bhavya Kailkhura",
        "Lichao Sun",
        "Xiaofeng Zhu",
        "Xiaoshuang Shi",
        "Kaidi Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as \"per-token\" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist \"generic truthful directions\" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 0 6 0 1 . 3 0 5 2 : r TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention Jinhao Duan1*, Fei Kong2 , Hao Cheng3, James Diffenderfer4, Bhavya Kailkhura4, Lichao Sun5, Xiaofeng Zhu2, Xiaoshuang Shi2, Kaidi Xu1 1Drexel University 2University of Electronic Science and Technology of China 3Hong Kong University of Science and Technology (Guangzhou) 4LLNL 5Lehigh University"
        },
        {
            "title": "Abstract",
            "content": "Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large VisionLanguage Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the overall truthfulness of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as per-token hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that ➊ LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, ➋ different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist generic truthful directions shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inferencetime intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and crossdata hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms stateof-the-art methods. Codes will be available at https: //github.com/jinhaoduan/TruthPrInt. 1. Introduction As Large Vision-Language Models (LVLMs) [32, 58, 64] have rapidly advanced in cross-modal content understand- *Equal Contribution Correspondence to Kaidi Xu kx46@drexel.edu ing and instruction following, their trustworthiness is threatened by Object Hallucination (OH) [42, 43]. Although recent work reveals that Large Language Model (LLM) internal states, such as hidden states, entail richer semantic and contextual information [1, 4, 5, 12, 27, 65] that can reveal the truthfulness (or uncertainty) [13, 24, 30] of model generations, it remains under-explored (i) whether internal states in LVLMs encode information about truthfulness, (ii) whether they support per-token hallucination analysis, and (iii) whether this information can be transferred to enhance practical applications, e.g., Out-of-Distribution (OOD) shifting. Preliminary research on internal states of LVLM relies mainly on statistical aspects of internal states to identify hallucinations, such as self-attention activation patterns [18, 21] and long-term decay [56] in RoPE [45]. However, these approaches do not explicitly link hidden states to hallucination behaviors and tend to be effective only for specific datasets and model architectures. In this paper, we investigate: Are internal states reliable and practical indicators of LVLM per-token hallucination behaviors? To answer this, we first create datasets consisting of thousands of internal states, each labeled with hallucination membership, i.e., as truthful or hallucinated. By training models on it for hallucination detection, we observe that ➊ LVLM internal states provide undesirable overall performance yet they are high-specificity indicators: the Likelihood Ratio for Positive Results (LR+) achieves nearly 20, indicating internal states provide confident detection with extremely low false alarm; ➋ There exist latent common hallucination subspaces shared by different LVLMs, in which detectors trained on the projections in this subspace are capable of transferring to OOD domains. This suggests the existence of common truthful directions shared by various LVLMs. Based on these, we design TruthPrInt, novel two-stage OH mitigation framework: first locating hallucinated tokens from latent subspace and then performing truthful-guided interventions enabling truthful decoding. We also propose ComnHallu, hallucination sub1 Figure 1. The overall pipeline of TruthPrInt for OH mitigation. TruthPrInt first collects internal states from LVLMs and learns truthful direction from the latent space. subspace alignment method ComnHallu is also proposed to enhance testing-time transferability among various LVLMs and datasets. During decoding, TruthPrInt guides the target VLM towards the truthful direction by rejecting hallucinated tokens and tracing back to early starting points for pre-intervention. space alignment method, to improve OOD transferability for hallucination detection. TruthPrInt is evaluated on advanced LVLMs including MiniGPT-4 [64], Llava1.5 [32], mPLUG-Owl2 [58], QWen2VL [50], InternVL2.5 [8], over popular OH benchmarks such as CHAIR [43], POPE [28], and LLaVA-Bench [31]. Experimental results show that TruthPrInt significantly outperforms competitive baselines and verified on both in-domain and OOD scenarios. Our contribution can be summarized as the following: We provide an in-depth exploration of how LVLM internal states related to OH and found that internal states are high-specificity hallucination indicators, encoding universal hallucination patterns from various LVLMs. We propose novel two-stage framework TruthPrInt to mitigate OH in LVLMs, and ComnHallu, capturing common hallucination features from subspace to enhance cross-LVLM and cross-data transferability. We conduct comprehensive experiments on popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms advanced baselines. 2. Related Work Object Hallucination in LVLMs. Object Hallucination (OH) [42] typically refers to the phenomenon where LVLMs generate nonexistent visual elements, such as objects [43], attributes [14], or events [60], posing significant challenge to achieving trustworthy performance. considerable of benchmarks [6, 16, 17, 28, 43, 47, 49, 52, 54] are proposed for OH evaluation, such as CHAIR [43], MME [16], and POPE [28]. To mitigate OH, two lines of research are proposed for OH mitigation: Contrastive Decoding (CD) [3, 7, 9, 23, 25, 33, 37, 48, 51] and postprocessing [39, 53, 59, 62]. CD primarily reduces biases imposed in LVLMs by contrasting generated responses from various decoding strategies, including distinct visual regions [7, 25, 33, 48], self-contrastive [9, 23, 51], and contrasting with preference models [3]. CD approaches for OH mitigation are sensitive to specific contrasting objects and often rely on narrow set of biases, overlooking the complex factors that contribute to LVLM hallucinations. Postprocessing methods [10, 59, 62] usually apply iterative visual prompting and continuous editing of the generated response. These methods may bring considerable computational overhead and are often designed for specific tasks. Internal Representations in Language Models. Internal representations typically refer to intermediate model outputs, such as self-attention maps and hidden states [46]. These representations have been widely used to study language model behaviors, including knowledge (or neuron) editing [34, 35], enhancing inference-time reasoning [26], In terms of hallucinaand enabling interpretability [61]. tion modeling, recent research indicates that internal representationslike hidden states [4] and attention head acti2 vations [26]contain more truthfulness information than generated textual responses. Building on this insight, substantial work [1, 4, 5, 12, 27, 65] has focused on language model uncertainty quantification (UQ) [13, 24, 30], by either measuring the semantic consistency [4] of hidden states or training detectors explicitly designed to identify overall hallucination behaviors [12, 27, 65]. However, UQ focuses on the overall truthfulness of generated responses. How internal states of LVLMs function within OH remains unclear. Current studies primarily depend on simple statistical metrics, such as self-attention activation patterns [18, 21] and long-term decay [56] in RoPE, to detect hallucinations. These methods are typically effective only for certain datasets and specific model architectures. Nullu [57] identifies the hallucination subspace within the latent space and edits LVLMs away from it to achieve truthful decoding. However, this process may considerably impact LLM benign behaviors, as previously highlighted in knowledge-editing research [29]. Differently, our work directly models LVLM hallucination behaviors using internal states with per-token annotations and additionally offers guidance for decoding to reduce OH. 3. Modeling Transferable LVLM Hallucination Features in Common Latent Subspace In this section, we demonstrate that internal states are reliable indicators of LVLM per-token hallucination behaviors. Additionally, we identify the existence of latent subspace that contains transferable hallucination features, enabling the hallucination detector to generalize across different datasets and models. 3.1. Crafting Per-Token Hallucination Detector Internal States Collection. To enable per-token hallucination detection, we first craft LVLM internal states and the corresponding hallucination labels. We prompt LVLM to describe images from the CC-Sbu-Align [64] dataset which consists of 3,439 detailed image-description pairs from Conceptual Captions [2, 44] and SBU [38]. Specifically, for given LVLM parameterized by θ, image x, and prompt for description, the i-th generated token is denoted by zi = pθ(x, z<i, s) where z<i refers to the previously generated 1 tokens. The hidden state of token zi is denoted by hl = Ml(x, s, z<i+1; θ) (1 zi Rd) where is the length of the generated ton, hl zi kens and is the hidden state dimension, e.g., = 4, 096 in MiniGPT-4. token zi is identified as an object token zo if zi completes noun. Then, for each object token zo , we collect the hidden states of its previous token, i.e., hidden states hl zi1 whose next-token prediction resulting zo , as the target internal states. The reason we collect previous hidden states rather than current object hidden states Figure 2. The performance of the designed hallucination detector across various LVLMs. Although internal states offer limited discriminative features for overall accuracy, they achieve highspecificity detections with low false alarm rates. is two-fold: ➊ This one-step-ahead approach allows the detector to provide early warnings of potential hallucinations and enable it to learn general patterns where hallucinations may occur rather than identifying specific hallucinated tokens; ➋ Enabling conveniently hidden state intervention for truthful next-token decoding ( Sec. 4). Please refer to Appendix A.1 for more discussion. Next, each hidden state hl is equipped with membership yi: hallucinated if the corresponding object does not appear in the images reference description, i.e., yi = 1, or truthful, i.e., yi = 0. Eventually, we collected balanced internal states datasets from MiniGPT-4, Llava-1.5, and mPLUG-Owl2, e.g., 2,716 hallucinated and truthful internal states, respectively, from MiniGPT-4. Hallucination Detection. Formally, we denote by = Rd : yi = 1} the set of hallucinated internal states {hl Rd : yi = 0} the set of truthful internal and = {hl states. The hallucination detection [12] is then formulated as optimizing model Gθ to miminizing risk RH,T = R+ H(G) + (G) = EhH1{G(h) 0} + EhT 1{G(h) > 0} (1) The hallucination membership of testing sample is given by H(h) = 1 [G(h) τ ], where τ is the threshold. In our implementation, is 3-layer MLP, taking the middle layer hidden state as input, i.e., = 16, trained with Binary Cross Entropy (BCE) loss. We use 80% of collected internal states for training and 20% for validation. Please refer to Appendix A.2 for detailed training protocol. 3.2. Mitigate OH Needs High-Specificity Indicator In OH, object tokens only take an extremely small portion of generated tokens, e.g., 5.6% tokens are object tokens in MiniGPT-4 captions, and 10% among them are hallucinated. Thus, it is essential to make the hallucination detector high-specificity, i.e. low False Positive Rate (FPR) 3 (a) The overall diagram of ComnHallu. (b) Co-transferring model and data via ComnHallu. Figure 3. ComnHallu (a) identifies common latent subspaces shared by both target (training) domain and source (testing) domain, capturing hallucination features, which (b) maintains internal states to be high-specificity when transferring both data domain and models. Tα fp means the threshold resulting FPR = α in the CC-Sbu-Align validation set. while maintaining certain True Positive Rate (TPR) to reduce false alarm examples, which is different from overall truthfulness (or uncertainty) quantification in LLMs. To evaluate this, we employ Likelihood Ratio for Positive Results (LR+) as the metric where LR+ = TPR/FPR. Results are summarized in Fig. 2. Accuracy is calculated by classifying the top 50% of predictions as hallucinated and the remaining 50% as truthful. It is shown that internal states offer limited discriminative features for overall accuracy (error rate > 20% greater than the portion of hallucinated tokens). However, they achieve near 20 LR+ at FPR=0.01, meaning that our crafted internal states are high-specificity indicators of hallucination. 3.3. Transferable Hallucination Detection via Subspace Alignment It is crucial that the hallucination detector remains robust under domain shifting, i.e., the training (or target) domain of the hallucination detector is different from the data and models in the testing (or source) domain. However, as shown in Fig. 3b, original internal states (blue curves) show poor transferability when transferring training domains to testing domains. Recent research shows that LLMs encode similar semantics across various backbone models, e.g., invariant relative representation [22, 36] and occasionally exhibit similar types of flaws [41], such as LLMs comparing 9.11 and 9.9 [55]. This indicates that different LVLMs may share common OH features. Inspired by this, we design ComnHallu, straightforward unsupervised domain adaptation method that identifies common latent subspace containing shared hallucination features between the source and target domains. ComnHallu first identifies base vectors separately from the training and testing domains, then projects all hidden states into the respective subspaces defined by these base vectors. Next, linear transformation is applied to align the testing domains base vectors with those from the training domain. This alignment ensures that hidden states from the testing domain can be represented using bases that are close to the training domain bases, thus achieving distributional alignment between the projected hidden states of both domains. The overall framework is presented in Fig. 3a. Concretely, given internal states {hi}N (layer index is omitted) sampled from source domain Rd and from target domain Rd, the task internal states {hi}M is to identify subspace Rd (d < d) such that (i) projections of internal states from both domains onto should retain hallucination-related features; (ii) projections of the source and target internal states should follow similar distribution within C, i.e., distribution alignment. We stack source internal states into feature matrices: RN d, and pre-process them to be 0-centered and normalize each hi by its Frobenius norm: hi = hiµs hiµsF where µs is the average internal states, resulting in the feature matrix (cid:101)S. We apply the same procedures on the target domain and obtain feature matrix (cid:101)T RM d. We rename (cid:101)S to be and (cid:101)T to be for simplicity. We first create independent d-dimension subspace for and respectively, to preserve hallucination information. Specifically, we first calculate the unbiased estimation of the covariance of as ΣS = ST 1 , and conduct eigenvalue decomposition: ΣS = QS diag(ΛS) QT (2) for its eigenvalues ΛS and eigenvectors {kj = QS,:,j}d . Then, the independent subspace of is created by spanning the eigenvectors corresponding to the top-d eigenvalues, i.e., KS = {k1, k2, , kd} Rdd . We apply the same procedures over and obtain its independent subspace spanned by KT. Since eigenvectors capture the directions with the greatest variance, the hallucination information encoded in and are preserved by KS and KT, respectively. 4 Distribution Alignment. We further capture correlations = KT KT to obtain the alignment matrix for transiting from subspace KS to KT and apply it over KS to obtain aligned subspace Kalign = KS M. Eventually, we project internal states via hT = hT Kalign , S, hT = hT KT, D, (3) to make projected internal states well aligned. Denoting by and the aligned data domains, the hallucination detector is trained on and evaluated on . To be practical in real-world scenarios, we consider both data and model transferability at the same time, i.e., co-transferring: (i) training hallucination detector on the ComnHallu-aligned internal states collected from LVLMA over the training set of crafted CC-Sbu-Align hidden state dataset; (ii) obtaining the thresholds Tα pr which results FPR=α on the validation set of CC-Sbu-Align hidden state dataset; (iii) testing the detector with thresholds Tα pr on LVLMB(A = B) over the COCO 2014val dataset (we follow the same pipeline as in Sec. 3.1 to collect internal states). For instance, the MiniGPT-4 Llava-1.5 plot (top left) in Fig. 3b indicating training hallucination detector on the internal states collected from MiniGPT-4 over CC-Sbu-Align, and testing on Llava-1.5 over the COCO val2014. We show that ComnHallu effectively mitigates domain shifting and maintains high-specificity detection on various testing domains. 4. TruthPrInt: Truthful-Guided Decoding In this section, we demonstrate how to reduce OH during LVLM decoding under the guidance of truthful direction. 4.1. Preliminary Given hallucination detector trained in Sec. 3, to mitigate hallucinations while preserving high-quality generation, it is essential to identify tokens that (i) are close to the truthful domain and (ii) maintain utility, e.g., minimal semantic distance to the input image for image caption task. Formally, this can be defined as: arg min (cid:88) {iziO} 1[G(hi1)] + d(x, zin, s), where represents the index set of objects token, and denotes the semantic distance metric between and following prompt s. To identify tokens with minimal distance to the image, we propose to pre-intervene model outputs with lower confidence scores when the optimal classifier identifies potential hallucination behaviors and guides us on the need for constructing new tokens. In Fig. 4, detailed diagram is provided to describe this procedure. Figure 4. The schematic diagram of TruthPrInt. When hallucinated object token (e.g., cup for the first time) is detected, we trace it back by locating the token with the lowest confidence preceding this sentence (e.g., including) and selecting the second candidate (e.g., such). This process is repeated NB times. 4.2. Pre-Intervention: Motivation and Methods Specifically, we observed that ➊ the root cause of hallucinations may lie before the hallucinated token itself. While hallucinations are typically detected in association with specific objects, the underlying triggers of these hallucinations may not be limited to the locations of the hallucinated objects [11, 15]. For instance, consider an image that depicts only dog. If the model generates the sentence: {The image shows dog running to the house.}, the phrase {the house} constitutes hallucinated object. However, the root cause of this hallucination might be attributed to the word {to}. We further illustrate this in the bottom of Fig. 1. The inclusion of {to} necessitates subsequent noun for the sentence to feel complete, which may lead the model to hallucinate an object. Based on this insight, denote ez to be the index of first hallucination token in sequence z. We propose that upon detecting hallucinated object, we first investigate whether any preceding token before ez within the sentence could have prompted the model to generate this hallucination. To locate the preceding token that triggered hallucination, we analyze LVLM output confidences and observe that ➋ tokens with lower confidence frequently precede hallucinated objects (please refer to Appendix B.1 for more experimental evidence). This aligns with the idea that some hallucinations arise from ambiguous information provided to the model or its inability to respond appropriately [19, 20], leading it to select an incorrect or irrelevant word. When the model is uncertain about how to proceed, its confidence in generating response decreases signifi5 MiniGPT-4 Llava-v1.5 mPlug-Owl Methods CHAIRS CHAIRI BLEU CHAIRS CHAIRI BLEU CHAIRS CHAIRI BLEU Greedy Beam Search DoLA LURE VCD Woodpecker OPERA HACL Nullu 29.531.51 25.800.00 26.001.41 27.882.25 28.932.47 28.872.20 27.801.70 24.471.01 21.401.00 11.730.46 10.150.21 10.250.35 10.200.85 12.100.79 10.200.85 10.800.57 9.570.31 8.990.36 15.580.35 16.060.37 16.050.39 15.030.11 15.180.63 15.300.01 16.030.35 15.840.36 14.810.06 19.601.64 19.401.70 18.603.39 19.482.35 23.002.95 23.854.62 18.603.96 18.271.14 15.200. 6.070.58 6.550.92 6.351.20 6.50.38 7.470.50 7.500.01 6.151.20 5.900.52 5.300.03 16.970.16 17.240.23 17.180.28 15.970.01 15.780.13 17.050.00 17.270.18 17.090.19 15.690.04 23.600.87 19.900.42 20.200.28 21.270.06 24.801.51 26.331.98 19.502.40 21.600.69 15.601.20 8.570.38 7.300.42 7.450.21 7.670.16 9.070.91 8.430.80 7.551.20 7.730.15 5.770.01 16.450.19 16.690.12 16.810.16 15.650.05 15.430.18 16.430.00 16.590.16 16.620.21 15.450.01 TruthPrInt 16.870.87 7.530.33 17.210.75 10.333.31 3.871.16 19.790. 11.131.50 5.270.42 18.820.22 Table 1. The evaluation results on the COCO CHAIR benchmark. Lower CHAIRS and CHAIRI indicate fewer hallucinated objects. It is shown that TruthPrInt significantly outperforms all the baselines in OH mitigation while resulting in higher-quality captions. MiniGPT-4 Llava-1. mPlug-Owl2 Methods Greedy Beam Search VCD OPERA DoLA HACL Precision 90.131.19 91.570.11 89.850.97 91.310.16 91.920.31 91.211.27 Fβ 88.861.15 90.220.17 88.490.83 89.970.11 90.560.26 89.751. Precision 92.801.08 93.100.40 92.331.08 92.661.06 93.130.40 92.620.87 Fβ 91.731.13 91.990.31 91.291.04 91.561.06 92.020.34 91.530.88 Precision 91.390.72 92.120.56 90.740.40 91.200.42 91.920.37 91.260.56 Fβ 90.300.68 90.860.57 89.570.37 89.910.49 90.670.39 90.080. TruthPrInt 92.281.28 90.031.21 94.280.60 92.470.60 93.660. 91.660.87 LVLMs Methods QWen2-VL7B-Instruct (QWen2) InternVL2.5-8B (InternLM2) Greedy Beam HALC TruthPrInt Greedy Beam HALC TruthPrInt CHAIRS CHAIRI BLEU 15.73 15.60 16.10 12.0 11.6 9.2 4.9 4.3 4. 6.2 13.2 11.8 10.1 3.2 3.4 4.8 4.6 4.2 3. 17.83 17.43 17.70 17.90 18.26 Table 2. Evaluation results on the offline POPE benchmark. Results are averaged over three splits (Random, Popular, and Adversarial). Table 3. Evaluating the transferability to advanced LVLMs using backbones other than Llama with mis-matched dimensionality. cantly. Building on this observation, we propose the following approach: Let ok represent the confidence score of location from the LVLM Ms output after applying softmax following the k-th backtrace, where ok <i; θ), and TopK(ok , 1) is the largest confidence candidate in ok . When identifying trigger words, we begin with the hallucinated token ez and move backward through the sentence to locate the token with the lowest top confidence TopK(ok = Mo(x, s, zk , 1). From these candidates, we exclude previously selected tokens and choose new one. Denote rzi to be the rank of the selected token in ok . To make selection, we consider all possible candidates suggested by the model. Specifically, we rank these candidates in descending order of likelihood and select the highest-ranked token TopK(ok , rzi + 1) at each iteration. This strategy is reasonable in scenarios without supplementary tools, such as additional LLVMs. However, this method does not guarantee that the second choice will be the correct trigger word. Consequently, after selecting new token, we repeat this process iteratively. When tracing back to identify the trigger word, it is crucial to limit the search to tokens located within relatively short distance from the hallucinated token, as the causal relationship diminishes with increasing distance. Naturally, this search is constrained to the sentence in which the hallucination occurs. Finally, we consider the hallucinated word itself. The previously outlined methods are not entirely reliable in pinpointing the precise trigger word, meaning our algorithm may continue detecting hallucinations even after several iterations. In such cases, persisting with the above algorithm is suboptimal: not only may the identified words fail to represent the actual triggers, but the number of candidate tokens suggested by the model is limited. To address this issue, when further iterations are unlikely to yield results, we set the max number of backtrace to be NB, and opt to select the second candidate TopK(ok , 2) provided by the model for the corresponding hallucinated word after achieving NB. Appendix B.2 provides an outline of the proposed method (for simplicity, we define FindFirstHallucination(z) as the process of using to identify the first hallucinated token of and output the index, and define the classifier G(hk ) < τ, if + 1 / {izi O}). 5. Experiments Benchmarks and Baselines. We follow previous work and evaluate our methods on popular OH benchmarks, including MSCOCO CHAIR evaluation [43], POPE [28], Offline POPE [7], and qualitative examination on LLaVAbench [31]. Please refer to Appendix C.1 for the introduction of each benchmark. We consider 8 competitive baselines, including naive Greedy generation, Beam search (with beams set to 3), VCD [25], OPERA [21], DoLA [9], HALC [7], Woodpecker [59], LURE [62], and Nullu [57]. We follow the original hyperparameters of each baseline acFigure 5. Trade-off between truthfulness and diversity. We show that TruthPrInt offers flexible adjusting of threshold τ : smaller τ for truthfulness in safety-critical scenarios while larger τ for diverse generations. cording to their papers or codebases. LVLMs and Co-Transferring Settings. We consider three advanced LVLMs, including MiniGPT-4 [64], Llava1.5 [32], and mPlug-Owl2 [58]. For LVLMs with nonLlama backbones, we consider the powerful Qwen2-VL7B-Instruct [50] and InternVL-2.5-8B [8]. To be practical in the real world, we apply the co-transferring settings as we mentioned in Sec. 3.3: (i) training the hallucination detector with ComnHallu-aligned MiniGPT-4 internal states over the crafted CC-Sbu-Align dataset and obtaining thresholds that result in FPR=α on the validation set; (ii) testing the detector with the threshold on other LVLMs. Default Hyperparameters. For all the experiments, we set the subspace dimension in ComnHallu, i.e., d, to be 64, the layer index for hidden states collection to be middle layer 16, and the maximum allowed traceback times to be 5. Following [7], we randomly select 500 images for each experiment and repeat three times, reporting both average performance and standard derivations. In Sec. 5.3, we provide detailed ablation studies for hyperparameters. We utilize the prompt Please describe this image in detail. for all caption generation. threshold τ to be 0.4, 5.1. CHAIR Evaluation. In Tab. 1, we report CHAIRS for the portion of hallucinated captions, CHAIRI for the portion of hallucinated objects, and the quality of generated captions measured by BLEU [40]. It is shown that our method significantly outperforms all the baselines in both OH mitigation and caption quality. Specifically, TruthPrInt outperforms the current state-of-the-art method HALC by 12% to 14% CHAIRS and over 2% CHAIRI over all three LVLMs. Moreover, TruthPrInt substantially improves the quality of captions where it outperforms baselines by nearly 2% BLEU across all the settings, suggesting that truthful guidance not only mitigates the hallucination behaviors of LVLM but also enables high-quality caption generation. Non-Llama Backbone and Mis-matched Dimensionality In Tab. 3, we provide the transferability evaluation over Figure 6. Hallucination ratio and number of generated objects under various maximum new token limitations. NB CHAIRS CHAIRI BLEU Precision Fβ 1 2 3 4 5 16.20 16.00 15.20 15.60 15.40 7.70 7.40 6.90 7.00 7.10 17.60 17.58 17.55 17.45 17.43 92.73 93.50 93.91 93.57 93.46 90.86 91.58 91.92 91.58 91. Table 4. Ablation study on the maximum number of traceback. Enabling more NB allows more trial and error to remove OH. It is shown that TruthPrInt the non-Llama LVLMs. demonstrates significant transferability when transferring MiniGPT-4 (Vicuna as backbone LLM) hidden states to various LLM backbones, e.g., QWen2-VL (QWen2 as backbone LLM) and InternVL-2.5 (InternLM2 as backbone LLM). Moreover, in terms of dimension mismatch, ComnHallu incorporates subspace projection mechanism to standardize hidden state dimensions, which could be applied in addition to handling dimension mismatch. The experiment of MiniGPT-4 (4,096 dimensions) QWen2VL-7B-Instruct (3,588 dimensions) supports the flexibility and transferability of our design. 5.2. POPE Evaluation. As highlighted in previous work [7], the original POPE benchmark requires robust chat capability to LVLMs for question answering. We follow [7] to conduct offline POPE (OPOPE), where we derive questions and answers from LVLM descriptions. We use Precision and Fβ metric with β = 0.1 for overall performance comparison. The averaging results are summarized in Tab. 2 (the full results, as well as the original POPE evaluation results, are provided in Appendix C.2). It is shown that TruthPrInt achieves the best Precision among most settings and splits, indicating that the high-specificity design works well. 5.3. Ablation Study We perform ablation studies on MiniGPT-4 using the COCO val2014 dataset without further specification. During the study, all hyperparameters remain the same as default values except for the parameter being ablated. 7 Figure 7. Qualitative analysis of generated captions. Both Greedy search and HALC encode lots of hallucinated objects, such as table, ceiling, vase, etc. Moreover, HALC experiences broken sentences due to token replacement. TruthPrInt provides detailed and accurate descriptions, even including the small object black and white photo on the wall. Layer CHAIRS CHAIRI BLEU Precision Fβ Greedy 12 14 16 18 20 29.53 24.20 10.40 15.40 10.00 11.80 11.73 10.70 4.90 7.10 5.30 6.50 15. 17.80 17.57 17.43 17.35 17.20 90.12 91.76 94.90 93.46 94.69 93.60 88.85 90.31 92.70 91.48 92.21 91.13 Table 5. Ablation study on the layers of LVLM for internal states collection and hallucination detection. 2 Trade-off: Truthful or Diverse? Threshold τ decides the criterion of hallucination identification: smaller τ means lower standard for hallucination (or higher standard for truthful) identification, enabling more tokens to be regarded as hallucinated. Inevitably, this will reject substantial decoding trajectories and conflict with generation quality, especially diversity. To quantify this, we investigate the relationship between truthfulness score: (100 CHAIRS +CHAIRI ) and generation diversity measured by the number of objects generated. As shown in Fig. 5, we suggest adjusting τ according to application scenarios, e.g., smaller τ in safetycritical scenarios to embrace more truthfulness. OH in Longer Captions. Recent work reveals that it is essential to evaluate OH mitigation in longer captions since (i) OH happens more frequently in longer captions with more objects mentioned [21, 63]; (ii) it will not hurt natural performance, e.g., providing high-quality and diverse generations. In Fig. 6, we report hallucination ratios, i.e., how many generated objects are hallucinated, and the total number of generated objects. It is shown that TruthPrInt exhibits significantly low hallucination ratios when generating longer captions while maintaining close object numbers. Efficiency and Number of Tracebacks. Unlike existing post-processing methods where heavy auxiliary models, e.g., LLMs [59] and CLIP [39], are incorporated. TruthPrInt leverages simple MLP models and limited backtracking mechanisms for truthful guidance, which exhibit close efficiency to naive Greedy search. In Fig. 8, we present the per-image process time consumed by baselines and TruthPrInt. For TruthPrInt, we included the Figure 8. Efficiency comparison. TruthPrInt requires similar computational costs as Greedy search while achieving better performance. NB substantially boosts OH mitigation involving limited computational overhead. detector training overhead and provided the efficiency under various maximum numbers of tracebacks NB. Results are obtained by averaging MiniGPT-4 over 500 images on single A40 GPU. It is shown that TruthPrInt requires close computational costing as Greedy yet achieves significant improvements. Also, enlarging NB substantially reduces OH from 7.7 CHAIRI to around 7.0 ( Tab. 4), meaning the designed backtracking is efficient and effective. Internal States Layer l. We investigate which layers of hidden states in LVLMs more effectively encode truthfulness information. In Tab. 5, we show that middle layers typically encode more truthfulness of generations, aligning with recent findings in LLM research [4, 12]. 5.4. Qualitative Analysis We manually examine the quality of generated captions on COCO val2014 and LLaVA-Bench [31]. In Fig. 7, we present one of the captions generated by Greedy search, HALC, and TruthPrInt regarding the same image. It is shown that TruthPrInt provides more accurate and detailed descriptions than baselines. Please refer to Appendix C.3 for more quantitative analysis on LLaVA-Bench. 6. Conclusion In this paper, we investigate OH in LVLMs, which is one of the most serious trustworthy issues. Our research starts with the discovery that LVLM internal states, 8 e.g., hidden states, are high-specificity and transferrable Based on that, we propose hallucination indicators. TruthPrInt, which first learns truthful direction in latent space and then applies truthful-guided intervention for OH mitigation during testing time. Our work highinternal states encode per-token truthfulness lights that information. Extensive results show that TruthPrInt outperforms existing baselines with significant margins."
        },
        {
            "title": "References",
            "content": "[1] Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734, 2023. 1, 3 [2] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 3 [3] Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, and Heng Tao Shen. Alleviating hallucinations in large visionlanguage models through hallucination-induced optimization. arXiv preprint arXiv:2405.15356, 2024. 2 [4] Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Inside: Llms internal arXiv Tao, Zhihang Fu, and Jieping Ye. states retain the power of hallucination detection. preprint arXiv:2402.03744, 2024. 1, 2, 3, 8 [5] Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, and Jia Li. Beyond surface: arXiv preprint Probing llama across scales and layers. arXiv:2312.04333, 2023. 1, [6] Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David Fouhey, and Joyce Chai. Multi-object hallucination in vision-language models. CoRR, 2024. 2 [7] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. In Forty-first International Conference on Machine Learning. 2, 6, 7 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 7 [9] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations. 2, 6 [10] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large vision-language modIn ICLR 2024 Workshop on els via clip-guided decoding. Reliable and Responsible Foundation Models. 2 [11] Sarah Desrochers, James Wilson, and Matthew Beauchesne. Reducing hallucinations in large language models through contextual position encoding. 2024. 5 [12] Xuefeng Du, Chaowei Xiao, and Yixuan Li. Haloscope: Harnessing unlabeled llm generations for hallucination detection. arXiv preprint arXiv:2409.17504, 2024. 1, 3, 8 [13] Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, 2024. 1, [14] Mingqian Feng, Yunlong Tang, Zeliang Zhang, and Chenliang Xu. Do more details always introduce more hallucinations in lvlm-based image captioning? arXiv preprint arXiv:2406.12663, 2024. 2 [15] James Flemings, Wanrong Zhang, Bo Jiang, Zafar Takhirov, and Murali Annavaram. Characterizing context influence and hallucination in summarization. arXiv preprint arXiv:2410.03026, 2024. 5 [16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. 2 [17] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2 [18] Xuan Gong, Tianshi Ming, Xinpeng Wang, and Zhihua Wei. Damro: Dive into the attention mechanism of lvlm to reduce object hallucination. arXiv preprint arXiv:2410.04514, 2024. 1, 3 [19] Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. Decomposing uncertainty for large language models through input clarification ensembling. arXiv preprint arXiv:2311.08718, 2023. [20] Hsiu-Yuan Huang, Yutong Yang, Zhaoxi Zhang, Sanwoo Lee, and Yunfang Wu. survey of uncertainty estiarXiv preprint mation in llms: Theory meets practice. arXiv:2410.15326, 2024. 5 [21] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and In Proceedings of the IEEE/CVF retrospection-allocation. Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. 1, 3, 6, 8 [22] Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, and Ting Liu. Enabling ensemble learning for heterogeneous large language models with deep arXiv preprint arXiv:2404.12715, parallel collaboration. 2024. 4 [23] Junho Kim, Hyunjun Kim, Yeonju Kim, and Yong Man Ro. Code: Contrasting self-generated description to combat hallucination in large multi-modal models. arXiv preprint arXiv:2406.01920, 2024. 2 [24] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations. 1, 3 [25] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1387213882, 2024. 2, 6 [26] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Inference-time intervention: Elicand Martin Wattenberg. iting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024. 2, 3 [27] Qing Li, Chenyang Lyu, Jiahui Geng, Derui Zhu, Maxim Panov, and Fakhri Karray. Reference-free hallucination detection for large vision-language models. CoRR, 2024. 1, 3 [28] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 2, 6, 1 [29] Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and Huajun Chen. Unveiling the pitfalls of knowledge editing for large language models. In The Twelfth International Conference on Learning Representations. 3 [30] Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models. Transactions on Machine Learning Research. 1, [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 6, 8, 1 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 7 [33] Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. arXiv preprint arXiv:2407.21771, 2024. 2 [34] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35: 1735917372, 2022. 2 [35] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass editing memory in transformer. arXiv preprint arXiv:2210.07229, 2022. 2 [36] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodolà. Relative representations enable zero-shot latent space comIn The Eleventh International Conference on munication. Learning Representations. [37] Sean OBrien and Mike Lewis. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. 2 [38] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 3 [39] Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Clip-dpo: Vision-language models as source of preference for fixing hallucinations in lvlms. arXiv preprint arXiv:2408.10433, 2024. 2, 8 [40] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 7 [41] Patrik Puchert, Poonam Poonam, Christian van Onzenoodt, and Timo Ropinski. Llmmapsa visual metaphor for stratified evaluation of large language models. arXiv preprint arXiv:2304.00457, 2023. [42] Vipula Rawte, Amit Sheth, and Amitava Das. survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023. 1, 2 [43] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40354045, 2018. 1, 2, 6 [44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. 3 [45] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1 [46] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [47] Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, and Behind the magic, merlim: MultiBernard Ghanem. modal evaluation benchmark for large image-language models. arXiv preprint arXiv:2312.02219, 2023. 2 [48] David Wan, Jaemin Cho, Elias Stengel-Eskin, and Mohit Bansal. Contrastive region guidance: Improving grounding in vision-language models without training. arXiv preprint arXiv:2403.02325, 2024. 2 [49] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling, pages 3245. Springer, 2024. 2 [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 7 [51] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding. arXiv preprint arXiv:2403.18715, 2024. 2 [65] Derui Zhu, Dingfan Chen, Qing Li, Zongxiong Chen, Lei Ma, Jens Grossklags, and Mario Fritz. Pollmgraph: Unraveling hallucinations in large language models via state transition dynamics. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 47374751, 2024. 1, 3 [52] Junjie Wu, Tsz Ting Chung, Kai Chen, and Dit-Yan Yeung. Unified triplet-level hallucination evaluation for large visionlanguage models. arXiv preprint arXiv:2410.23114, 2024. 2 [53] Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, and Tieniu Tan. Logical closed loop: Uncovering object hallucinations in large vision-language models. arXiv preprint arXiv:2402.11622, 2024. 2 [54] Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, et al. Autohallusion: Automatic generation of hallucination benchmarks for vision-language models. arXiv preprint arXiv:2406.10900, 2024. 2 [55] Zikai Xie. Order matters in hallucination: Reasoning order as benchmark and reflexive prompting for large-languagemodels. arXiv preprint arXiv:2408.05093, 2024. 4 [56] Yun Xing, Yiheng Li, Ivan Laptev, and Shijian Lu. Mitigating object hallucination via concentric causal attention. arXiv preprint arXiv:2410.15926, 2024. 1, 3 [57] Le Yang, Ziwei Zheng, Boxu Chen, Zhengyu Zhao, Chenhao Lin, and Chao Shen. Nullu: Mitigating object hallucinations in large vision-language models via halluspace projection. arXiv preprint arXiv:2412.13817, 2024. 3, [58] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplugowi2: Revolutionizing multi-modal large language model with modality collaboration. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1304013051. IEEE, 2024. 1, 2, 7 [59] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023. 2, 6, 8 [60] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024. 2 [61] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: survey. ACM Transactions on Intelligent Systems and Technology, 15(2):138, 2024. 2 [62] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations. 2, 6 [63] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. [64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations. 1, 2, 3, 7 11 TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Hallucination Detection with Internal States A.1. Internal States Collection In Sec. 3.1, we utilize the hidden states of preceding tokens associated with object tokens to detect hallucinations. Specifically, the hallucination detector is designed to provide an early warning by predicting whether future object tokens are likely to be hallucinated. This approach ensures that the detector is not exclusively trained on object tokens but functions as generalized detector applicable to any type of token. From an intervention perspective, this early warning mechanism reduces the inference time of the LLM during decoding. For example, when determining the next token zj, the previous hidden states can be directly passed to the detector for hallucination identification, i.e., G(hj1) < τ . In contrast, current-token prediction approach would require computing the current hidden states hj, which involves an additional LLM inference step before detecting hallucinations, i.e., G(hj) < τ . cinated is significantly larger than the PMC of truthful object tokens, indicating that low-confidence tokens tend to derive hallucinated objects. Model PMC of Hallucinated PMC of Truthful MiniGPT-4 Llava-1.5 mPlug-Owl2 0.39 0.29 0.29 0.31 0.22 0.20 Table 7. The average Preceding Minimum Confidence (PMC) over hallucinated and truthful object tokens. The PMC of hallucinated objects is significantly larger than the PMC of truthful object tokens, indicating that tokens with lower confidence frequently preceded hallucinated objects. B.2. Method Procedures In Algorithm 1, we present our pre-intervention mechanism algorithmic descriptions. A.2. Training Protocol of Hallucination Detection C. Experiment Protocols In our implementation, the hallucination detector is 3layer MLP, with the architecture presented in Tab. 6. The model is trained for 30 epochs with batch size of 512, learning rate of 0.001, and the Adam optimizer, utilizing binary cross-entropy (BCE) as the training objective. The optimal checkpoint is determined based on its performance on the validation set. Layer 1 Layer 2 Layer Activation (4096, 128) (128, 64) (64, 1) ReLu Table 6. The architecture of G. B. TruthPrInt: Preliminary Analysis B.1. Low-Confidence Tokens Precede Hallucination As we mentioned in Sec. 4.2, tokens with lower confidence frequently precede hallucinated objects. Here, we provide experimental evidence to support it. Specifically, for each object token, we calculate Preceding Minimum Confidence (PMC): the minimum LVLM confidence of the preceding tokens of the object token within the same sentence. In Tab. 7, we present the average PMC collected from hallucinated object tokens and truthful object tokens, respectively, over 500 samples. It is shown that the PMC of halluIn this section, we introduce the OH benchmarks used in this paper and additional experimental results as well. C.1. Benchmarks MSCOCO CHAIR [43] is widely used benchmark for evaluating OH. Given set of images, it tasks LVLMs with generating detailed descriptions of the images. The next step involves comparing the objects present in the images with those mentioned by the LVLMs, using specific metrics CHAIRS = sentences with hallucinated objects all sentences CHAIRI = hallucinated objects all objects mentioned It is usually incorporated with the for OH evaluation. COCO image caption dataset. POPE [28] conducts an empirical evaluation of OH across multiple LVLMs, revealing its severity and identifying critIt introduces Pollingical factors influencing this issue. based Object Probing Evaluation (POPE), which reformulates hallucination assessment as binary classification task to improve stability, fairness, and scalability over existing methods. LLaVA-Bench [31] is diverse collection of 24 images featuring various contexts, such as in-door, and outdoor. Each 1 Algorithm 1 TruthPrInt decoding 1: Input: Prompt s, model M, the image x, max backtracing number NB, detector G, target layer L, threshold τ vations reveal that TruthPrInt produces more accurate and truthful descriptions, with greater detail included compared to the baselines. 2: = 0, = 0 3: = 0 4: = 0 NNB +1 5: repeat 6: 7: 8: <i; θ) repeat = Mo(x, s, zk ok i1 = ML(x, s, zk hk zk = TopK(ok ck = ck + 1[G(hk ri = ri + 1[G(hk = + 1 , ri + 1) ) > τ ] ) > τ ] <i; θ) Rank of Selected Token # of Hallucination Generate Sentence Next Rank Token Generate Next Token i1 in [eos, .] until zk if ci = 0 then return zk No Hallucination else Next Backtracing Initialization = + 1 ik = arg min({TopK(ok1 <ik = zk1 <ik , = ik zk Set State and Backtracing From ik r>i = 0 , 1)j i}) 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: Achieve the Max Backtracing Find Sentence with Less Hallucination ) Backtracing from end if 20: 21: 22: until > NB Number <i <i = zk 23: = arg min(cNB ) 24: = FindFirstHallucination(zk 25: zk 26: repeat = Mo(x, s, zk ok 27: hk i1 = ML(x, s, zk 28: zk = TopK(ok ck = ck + 1[G(hk = + 1 , 1[G(hk 29: 30: 31: 32: until zk 33: = arg min(c) 34: return zk i1 in [eos, .] <i; θ) <i; θ) i1) > τ ] i1) > τ ] + 1) image is paired with meticulously crafted, detailed description and thoughtfully chosen set of questions. It is usually used for quantitative analysis of LVLM behaviors. C.2. POPE Results In Tab. 9, we present the individual results over each offline POPE split. We also provide the original POPE evaluation results, obtained from MiniGPT-4 for each split, in Tab. 8. C.3. LLaVA-Benchmark Quantitative Analysis We evaluate our methods and baselines on the LLaVABenchmark (In-the-Wild) dataset, manually reviewing the generated responses for these images ( Fig. 9). Our obser2 Random Popular Adversarial average Method Greedy VCD Beam TruthPrInt Precision 67.65 60.76 64.30 68.23 Fβ 67.78 60.79 64.47 68.35 Precision 55.60 52.63 54.68 55.76 Fβ 55.79 52.70 54.88 55.93 Precision 58.97 54.33 56.44 59.09 Fβ 59.15 54.38 56.64 59.26 Precision 60.74 55.91 58.47 61.03 Fβ 60.91 55.96 58.66 61.18 Table 8. Evaluation results on the original POPE benchamrk. POPE Split Methods Precision Fβ Precision Fβ Precision Fβ MiniGPT4 Llava-1.5 mPlug-Owl2 Random Popular Adversarial Greedy Beam VCD OPERA DOLA HALC 97.130.22 97.510.92 96.781.42 98.120.51 97.510.52 97.040.39 95.590.16 95.930.80 95.141.35 96.510.44 95.940.46 95.330.38 98.210.16 97.700.14 97.111.18 97.700.46 97.700.12 97.981.01 96.950.06 96.430.22 95.911.06 96.430.48 96.430.17 96.601. 96.661.44 96.471.76 96.800.87 96.101.30 96.471.35 96.731.24 95.391.46 95.051.67 95.400.84 94.621.15 95.041.27 95.351.20 TruthPrInt 98.170.46 95.580.46 98.650. 96.630.86 97.480.64 95.280.71 Greedy Beam VCD OPERA DOLA HALC 87.502.16 89.611.01 87.120.87 88.850.84 90.130.19 89.161.51 86.342.10 88.341.04 85.870.74 87.610.85 88.850.22 87.791. 91.631.32 90.920.50 91.111.69 90.522.19 91.140.25 90.901.10 90.601.36 89.880.41 90.111.66 89.492.14 90.090.19 89.861.10 89.691.36 90.303.05 89.180.46 89.421.22 90.012.72 89.501.10 88.661.26 89.122.97 88.070.40 88.211.26 88.832.65 88.391.06 TruthPrInt 90.231. 88.101.47 93.130.86 91.380.90 92.641.75 90.701.76 Greedy Beam VCD OPERA DOLA HALC 85.751.53 87.590.22 85.641.53 86.970.80 88.100.60 87.442.65 84.641.48 86.400.28 84.451.38 85.800.73 86.900.54 86.132.54 88.562.07 90.690.83 88.781.97 89.780.54 90.561.07 88.991.17 87.632.08 89.660.73 87.851.92 88.770.55 89.530.99 88.021.17 87.821.79 89.580.40 86.240.61 88.071.33 89.290.26 87.551.59 86.851.71 88.420.41 85.230.58 86.911.36 88.130.22 86.501. TruthPrInt 88.442.09 86.422.05 91.061.19 89.401.05 90.861. 89.011.82 Table 9. Evaluation results of each offline POPE split. 3 (a) Both Greedy and HALC hallucinate details such as person wearing white shirt and blue, along with other nonexistent objects like phone message and couch. In contrast, TruthPrInt delivers more accurate and truthful descriptions. (b) Both Greedy and HALC incorrectly describe the item as purple mango and further hallucinate details like several slices of this fruit. In contrast, TruthPrInt offers more accurate description, referring to it as purple fruits. (c) Both Greedy and HALC falsely describe all the animals as wearing hats and provide only limited details about the image. Additionally, HALC misidentifies the rabbit as dog. In contrast, TruthPrInt delivers accurate descriptions of all the animals and includes additional details such as the bear is holding plate of food and the colors used are earth tones like brown, green, and beige. Figure 9. LLaVA-Bench quantitative analysis results."
        }
    ],
    "affiliations": [
        "Drexel University",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "LLNL",
        "Lehigh University",
        "University of Electronic Science and Technology of China"
    ]
}