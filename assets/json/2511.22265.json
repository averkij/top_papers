{
    "paper_title": "FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning",
    "authors": [
        "Yuan Yao",
        "Lixu Wang",
        "Jiaqi Wu",
        "Jin Song",
        "Simin Chen",
        "Zehua Wang",
        "Zijian Tian",
        "Wei Chen",
        "Huixia Li",
        "Xiaoxiao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 6 2 2 2 . 1 1 5 2 : r FedRE: Representation Entanglement Framework for Model-Heterogeneous Federated Learning Yuan Yao1 Lixu Wang2 Jiaqi Wu3, Jin Song4 Simin Chen5 Zehua Wang6 Zijian Tian7 Wei Chen8 Huixia Li9 Xiaoxiao Li6 1Teleinfo, CAICT 2Northwestern University 3Tsinghua University 4Nanjing University of Posts and Telecommunications 5University of Texas at Dallas 6University of British Columbia 7China University of Mining and Technology-Beijing 8China University of Mining and Technology 9Beijing Jiaotong University Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), framework built upon novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are re-sampled each round to introduce diversity, mitigating the global classifiers overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https: //github.com/AIResearch-Group/FedRE. 1. Introduction Federated learning (FL) [23, 41] is collaborative learning paradigm that aggregates client knowledge, e.g., model parameters, from multiple clients. Numerous FL methods have been developed and applied in various fields, such as healthFigure 1. FedRE framework. Each client maintains local model consisting of representation extractor and classifier. The clients local representations and their corresponding one-hot label encodings are integrated into single entangled representation and entangled-label encoding, respectively, which are then uploaded to the server for training the global classifier. care [1, 47] and the Internet of Things [6, 24]. Most existing FL studies [17, 23, 26, 49, 51] assume that the architectures of local models across clients are homogeneous. In practice, however, assuming the same model architecture for all clients is unrealistic due to differences in sample distribution, hardware, and computational capabilities. Also, the model architecture adopted by each client is private and may not be shared with the server or other clients. Those issues motivate practical yet challenging problem known as modelheterogeneous FL [42], where the representation extractors may adopt heterogeneous architectures across clients, while the classifiers share homogeneous architecture. Hence, directly aggregating all model parameters becomes infeasible. To tackle this dilemma, existing model-heterogeneous FL studies have explored utilizing other forms of client knowledge, such as representations [22], logits [10], small-models [40, 44], classifiers [19], or prototypes (i.e., category means) [9, 34, 38, 43], from clients. While representations, logits, and small-models can effectively encode high-level client knowledge, uploading them to the server may introduce nonnegligible communication overhead and potential privacy concerns, as such information could be exploited to reconstruct original samples by launching representation or model inversion attacks [35, 45]. As lighter alternative, uploading classifiers or prototypes alleviates communication overhead and reduces the risk of sample reconstruction, though classifiers may carry biases arising from local sample distributions, and prototypes primarily capture category-representative information and may capture limited intra-class variability. This raises question: For model-heterogeneous FL, is there more effective, privacy-aware, and lightweight representation form of client knowledge? To answer this question, we design novel form to represent client knowledge, termed entangled representation, which entangles local representations from multiple categories per client into single cross-category representation. Building on this concept, we develop Federated Representation Entanglement (FedRE) framework. As illustrated in Figure 1, each client in FedRE maintains local model comprising representation extractor and classifier. The client first maps its local representations to unified dimensionality across clients, and then uses normalized random weights to separately aggregate the mapped representations into an entangled representation and the corresponding label encodings into an entangled-label encoding. Those are then uploaded to the server to train the global classifier. During training, the entangled-label encodings provide crosscategory supervision signals, and the per-round resampling of random weights introduces diversity, mitigating the global classifiers overconfidence and promoting smoother decision boundaries. Furthermore, the entangled representations mitigate the risk of representation inversion attacks [35] by blending cross-category representations to obscure individual sample information, while reducing communication overhead as only one representation is uploaded per client. As result, the entangled representations provide an effective, privacy-aware, and lightweight form of client knowledge. The main contributions of this paper are threefold. We introduce entangled representations as novel form to represent client knowledge. We propose the FedRE framework, which supports flexible instantiations with different representation entanglement mechanisms. Extensive experiments show that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. 2. Related Work Existing FL approaches can be roughly categorized into model-heterogeneous and model-homogeneous methods based on their ability to handle model heterogeneity. Model-Heterogeneous FL method handles both heterogeneous local models and sample distributions. Due to the heterogeneity of local models, it is not feasible to aggregate all their parameters. Thus, most of the studies turns to aggregate client knowledge (e.g., representations [22], logits [10], small-models [40, 44], classifiers [19], or prototypes [9, 34, 38, 43, 46]). For example, DS-FL [10] designs logit aggregation strategy that integrates the local logits from different clients into the global logit on the server. FedHeNN [22] aligns the representations extracted by distinct local models using common representation alignment dataset. LG-FedAvg [19] aggregates the classifiers from distinct clients on the server. FedProto [34], FPL [9], FedGH [43], and FedTGP [52] treat prototypes as form of client knowledge. Specifically, FedProto [34] averages the local prototypes of each category across clients. FPL employs clustering strategy to derive unbiased global prototypes, and FedTGP optimizes trainable global prototypes dynamically. Moreover, FedGH [43] utilizes the prototypes from multiple clients to train the global classifier on the server. Additionally, several studies [39, 55] focus on knowledge distillation. For instance, FedGen [55] learns global generator to augment the training samples for local models, while FedKD [39] distills global student model to assist the learning of local models. Furthermore, another line of research [40, 44] uses homogeneous small-models as client knowledge. An example is FedMRL [44], which facilitates inter-client knowledge aggregation via shared small-model. Model-Homogeneous FL method deals with homogeneous local models but heterogeneous sample distributions. Most of the studies [3, 4, 17, 21, 23, 37, 51] focus on aggregating all parameters of local models. For instance, FedAvg [23] aggregates all model parameters from distinct clients on the server. Based on FedAvg, FedAvgDBE [50], FedFN [11], and FedDecorr [31] alleviates the representation bias issue in local models. Another example is FedALA [51], which adaptively integrates the global and local models to align with the local objective. In addition, several studies [2, 25, 27, 32] aim to aggregate partial parameters of local models. For example, FedRep [2] aggregates the representation extractors of local models to enhance representation capability. Moreover, FedBABU [25], SphereFed [5], FedETF [18], and FedDr+ [12] only update the representation extractors during local training and then aggregates them on the server. 3. Methodology In this section, we present the proposed FedRE framework. We begin by formalizing the problem addressed in this work. Figure 2. toy experiment is conducted with 300 training and 200 test two-dimensional samples distributed across two clients. FedAllRep uploads all 300 representations, achieving the best performance (63.50%). FedGH uploads 4 prototypes, which may lead to increased focus on the prototypes, yielding sharper decision boundaries and slightly lower performance (60.50%). FedRE uploads 2 entangled representations, which provide cross-category supervision, resulting in smoother decision boundaries and competitive performance (62.00%). )}nk Consider clients and server, where each client holds private local dataset Dk = {(xk i=1, with xk , yk denoting the i-th input sample and yk its one-hot label encoding over categories. Each client maintains local model defined as hk(θk; x) = fk(ωk; gk(ϕk; x)), where gk is the representation extractor parameterized by ϕk, fk is the local classifier parameterized by ωk, and θk = {ϕk, ωk}. Representation extractors may vary across clients to accommodate architectural heterogeneity, while all local classifiers share the same architecture. The goal is to train client models on {Dk}K k=1 to achieve high average accuracy across clients, while alleviating the risk of representation inversion attacks [35] and reducing communication overhead. Next, we elaborate on FedREs motivation, workflow, and analyses. 3.1. Motivation As aforementioned, key obstacle in model-heterogeneous FL lies in the heterogeneity of local representation extractors, which prevents direct parameter aggregation as done in FedAVG [23]. To address this challenge, promising direction is to incorporate client knowledge from multiple clients to train high-quality global classifier without compromising privacy. Such classifier leverages cross-client knowledge and improves local model performance upon deployment to clients. vanilla method, FedAllRep, uploads all sample representations to the server for training the global classifier. This ensures model performance by leveraging all representations (as exemplified in the left of Figure 2), but it poses risks of leaking original samples via representation inversion attacks [35] and introduces non-negligible communication overhead. To alleviate this issue, FedGH [43] constructs client knowledge in the form of per-category prototypes to train the global classifier, thereby reducing communication cost and mitigating the risk of representation inversion attacks. Those prototypes primarily capture the representative knowledge of each category. Training the global classifier on them may lead to increased focus on the prototypes, potentially resulting in sharper decision boundaries. This limitation motivates the design of entangled representations, novel form of client knowledge that, unlike prototypes, integrates representations from distinct categories. Specifically, each client assigns normalized random weight to each local representation. Those weights are then used to aggregate the local representations into single entangled representation and the corresponding one-hot label encodings into an entangled-label encoding. The entangled representation and entangled-label encoding are subsequently uploaded to the server to train the global classifier. During training, the entangled-label encodings provide crosscategory supervision signals, and the weights are re-sampled in each communication round to introduce diversity. This helps the global classifier avoid overconfidence in any single category and promotes smoother decision boundaries (as exemplified in the right of Figure 2). As result, training the global classifier on entangled representations may yield better performance compared to using prototypes, as also suggested by our empirical observation in Figure 2. Furthermore, the entangled representation increases the difficulty of representation inversion attacks by blending all local representations into single cross-category, entangled representation, while uploading only one entangled representation per client further reduces communication overhead. In summary, the entangled representations provide effective, privacy-aware, and lightweight client knowledge, forming the FedREs foundation. Next, we detail the FedRE. 3.2. FedRE In FedRE, each client has local model comprising representation extractor and classifier, as depicted in Figure 1. The FedRE workflow consists of three main steps: (i) local model update; (ii) representation entanglement and upload; and (iii) global classifier update and broadcast. 3.2.1. Local Model Update Similar to vanilla FL methods such as FedAvg [23], FedRE requires each client to update its local model to effectively learn from local samples. To this end, the optimization objective for the k-th client is formulated by min θk 1 nk (cid:88)"
        },
        {
            "title": "Lce",
            "content": "(cid:2)hk(θk; xk ), yk (cid:3), (1) (xk ,yk )Dk where Lce(, ) denotes the cross-entropy loss. 3.2.2. Representation Entanglement and Upload We now describe the representation entanglement process. Each client first applies representation mapping (RM) operation to its local representations, yielding representations of consistent dimensionality for global classifier training. It then generates an entangled representation and the corresponding entangled-label encoding via representation entanglement (RE) mechanism: (cid:101)rk = Dk (cid:88) i= RM(cid:2)gk(ϕk; xk wk )(cid:3), (cid:101)yk = Dk (cid:88) i=1 wk yk , (2) where wk [0, 1] denotes normalized random weight assigned to sample xk . Subsequently, each client uploads its entangled representation along with the corresponding entangled-label encoding, i.e., (cid:101)R = {((cid:101)rk, (cid:101)yk)}K k=1, to the server for training the global classifier. Note that Eq. (2) defines flexible framework supporting various RM and RE mechanisms, as further analyzed in Q6 and Q7 of Section 4.3. 3.2.3. Global Classifier Update and Broadcast Upon receiving (cid:101)R = {((cid:101)rk, (cid:101)yk)}K k=1, the server utilizes those entangled representations and their associated entangledlabel encodings to train the global classifier. Accordingly, the servers optimization objective is formulated as min ω (cid:88) k= Lce (cid:2)f (ω; (cid:101)rk), (cid:101)yk (cid:3), (3) where (ω; ) represents the global classifier with parameters of ω. Since the entangled-label encodings provide crosscategory supervision signals, minimizing Eq. (3) may help the global classifier to take multiple categories into account and learn smoother decision boundaries. Finally, the server broadcasts the updated global classifier to the clients for the next iteration. With the above update process, the FedRE framework can be summarized in Algorithm 1. 3.3. Analysis 3.3.1. RE vs. Mixup We compare RE with mixup [36, 48]. Given clients representation set = {(ri, yi)}n (cid:101)rmixup = λri + (1 λ)rj, (cid:101)ymixup = λyi + (1 λ)yj, (4) i=1, mixup is formulated as: Algorithm 1 FedRE Framework Require: clients with their respective datasets {Dk}K k=1. Ensure: Local models for all clients, i.e., {hk(θk; )}K k=1. 1: Randomly initialize the global classifier (ω; ) and the k=1. local models {hk(θk; )}K 2: for = 0 to 1 do 3: 4: 5: 6: for each client in parallel do Client Side Receive ω and set ωk ω. Perform local fine-tuning of θk by Eq. (1). Apply RM to obtain mapped representations. Apply RE to generate (cid:101)rk and (cid:101)yk. Upload ((cid:101)rk, (cid:101)yk) to the server. end for Update ω according to Eq. (3). Broadcast ω to the clients. Server Side. 7: 8: 9: 10: 11: 12: end for where λ Beta(α, α), for α (0, ). In contrast, RE is formulated as follows: (cid:101)rRE = (cid:88) i=1 wiri, (cid:101)yRE = (cid:88) i=1 wiyi, (5) where wi [0, 1] is the weight of ri and can be determined by various RE mechanisms. As indicated by Eqs. (4)(5), mixup performs linear interpolation between pairs of representations, whereas RE is client-level entanglement mechanism tailored for FL. It outputs single entangled representation per client that condenses all local representations and is designed to balance model performance, privacy protection, and communication overhead. 3.3.2. Computational Complexity We analyze the computational complexity of RE on the client side. Let Rnd be the representation matrix, where is the number of samples and is the representation dimensionality, and let RnC denote the corresponding one-hot label matrix, where is the number of categories. Let Rn be the normalized weight vector. The entangled representation and entangled-label encoding are computed as (cid:101)rRE = Rw, and (cid:101)yRE = Yw, respectively, yielding total computational complexity of O(cid:0)n(d + C)(cid:1). 4. Experiments In this section, we evaluate the proposed FedRE. 4.1. Experimental Setup Datasets and Baselines. We use three benchmark datasets: CIFAR-10 [13], CIFAR-100 [13], and TinyImageNet [14]. Moreover, we compare FedRE with eight state-of-the-art approaches: LG-FedAvg [19], FedGH [43], FedKD [39], FedGen [55], FedProto [34], FPL [9], FedMRL [44], FedTGP Table 1. Accuracy (%) comparison on three datasets under the model-heterogeneous setting. In each column, the best results are bolded, and the second-best results are underlined."
        },
        {
            "title": "Average",
            "content": "CIFAR-10 CIFAR-100 TinyImageNet CIFAR-10 CIFAR-100 TinyImageNet LG-FedAvg [19] 80.90 0.17 41.96 0.03 25.16 0.42 85.35 0.25 58.24 0.33 32.26 0.28 78.66 0.34 40.91 0.26 25.04 0.11 85.43 0.03 58.07 0.33 31.98 0.29 80.79 0.38 41.33 0.25 25.39 0.36 84.03 0.17 55.61 0.10 31.73 0.20 81.16 0.12 41.46 0.10 25.45 0.19 84.88 0.18 57.87 0.67 31.96 0.21 78.36 0.52 35.00 0.34 18.16 0.08 83.81 0.18 56.72 0.11 29.61 0.02 77.40 0.23 36.66 0.45 22.64 0.46 83.89 0.38 53.21 0.25 29.16 0.19 81.28 0.05 34.41 0.04 20.92 0.09 83.30 0.41 54.25 0.21 27.37 0.10 81.32 0.47 35.89 0.22 28.70 0.49 84.68 0.27 54.67 1.34 35.64 0.37 81.20 0.05 41.57 0.10 25.81 0.15 84.68 0.07 57.96 0.12 33.02 0.14 82.60 0.01 46.36 0.09 30.48 0.13 86.20 0.14 62.56 0.32 38.52 0.08 FedGH [43] FedKD [39] FedGen [55] FedProto [34] FPL [9] FedMRL [44] FedTGP [52] Local FedRE 53.98 53.35 53.15 53.80 50.28 50.49 50.26 53.48 54.04 57.79 [52], as well as Local method that trains local models independently on each client without communication. Model-Heterogeneous Settings. We configure 10 clients with 10 distinct architectures spanning diverse families and computational complexities: four-layer CNN [50], MobileNetV2 [28], GoogleNet [33], five ResNet models (ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet152) [8], and two Vision Transformer (ViT) models (ViTB/16 and ViT-B/32) [7]. Statistic-Heterogeneous Settings. We follow [50] and adopt both practical (PRA) [16, 20] and pathological (PAT) [30] settings to simulate statistical heterogeneity among clients. In the PRA setting, samples are distributed across clients using Dirichlet distribution [20] with parameter α, which is set to 0.1 by default across all datasets. In the PAT setting, each client is assigned samples from 2, 10, and 20 categories, drawn from total of 10, 100, and 200 categories in CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, with varying sample sizes. Implementation Details. We implement FedRE based on the PFLlib and HtFLlib frameworks [53, 54], with 10 clients participating by default. Local samples are split in 3:1 ratio for training and testing. We evaluate three RM operations and empirically select average pooling (AP) as the default (see details in Q6 of Section 4.3). AP averages representation values across spatial regions into fixed-length vector of dimension 512 for all clients. Moreover, we design five RE mechanisms and empirically choose the Random Average Prototype (RAP) mechanism (see details in Q7 of Section 4.3). In RAP, each client first calculates prototypes and then separately aggregates the prototypes and their corresponding one-hot label encodings using normalized random weights, yielding single entangled representation and entangled-label encoding. Furthermore, we use SGD optimizers for both server and client updates, with task-dependent learning rates and batch sizes. The detailed experimental setup is provided in Table 10 in Appendix B. All experiments are conducted on NVIDIA GeForce RTX A800 GPUs. Evaluation Metrics. We evaluate model performance by calculating the classification accuracy on the test set, averaged over all client models. To ensure fair comparison, we report the average classification accuracy of the final round after 100 communication rounds, calculated across three random experiments, along with its standard deviation. To evaluate the robustness against representation inversion attacks, we use Peak Signal-to-Noise Ratio (PSNR) [29] and Mean Squared Error (MSE) between the reconstructed and original images. Lower PSNR and higher MSE indicate stronger privacy protection. Communication Overhead is measured by the total number of transmitted numerical scalars (i.e., parameters or representations) per round, where upload overhead represents the total scalars uploaded by all clients, and broadcast overhead represents the total scalars distributed from the server to all clients. 4.2. Main Experiments Q1: How does FedRE perform in model-heterogeneous settings? The results under the model-heterogeneous FL setting are listed in Table 1. We have several insightful observations. (1) FedRE outperforms all the baselines across various scenarios. In particular, FedRE achieves an accuracy on TinyImageNet that surpasses those of LG-FedAvg, FedGH, and FedKD by 6.26%, 6.54%, and 6.79%, respectively, under the PAT setting. Also, several methods do not exceed the performance of Local, indicating that the scenarios are challenging. (2) FedRE performs better than FedGH, suggesting that entangled representations could offer advantages over prototypes when training the global classifier. (3) LG-FedAvg is worse than FedRE, which indicates that (a) Original Images (b) Representations (a) PRA (b) PAT Figure 3. Accuracy (%) comparison between distinct communication rounds on the TinyImageNet dataset in the modelheterogeneous setting. (c) Prototypes (d) Entangled Representations Figure 4. Comparison of privacy protection in restructuring results from representations, prototypes, and entangled representations on the TinyImageNet dataset. using entangled representations to optimize the global classifier may be more effective than directly aggregating local classifiers. Also, Figures 3(a)-(b) show that FedREs performance on TinyImageNet improves rapidly in the early rounds and subsequently stabilizes, suggesting consistent convergence behavior. Moreover, we evaluate FedRE under various statistical-heterogeneous settings in Q4 of Section 4.3, including large-scale participation (i.e., 100 clients). Q2: Can entangled representations effectively mitigate the risk of representation inversion attacks? We launch the representation inversion attacks [35] to reconstruct original samples from representations, prototypes, and entangled representations, respectively. Figure 4 illustrates the reconstruction results for sample images from TinyImageNet. We can make several insightful observations. (1) Most image contours are reconstructed from the representations, indicating their vulnerability to representation inversion attacks. (2) Some category information, such as the presence of fish, is leaked through reconstructed prototypes, as prototypes encapsulate representative category information. (3) The reconstructed images from entangled representations reveal no identifiable information. This is because entangled representations combine information across different categories, making it difficult to reconstruct individual samples. In addition, the PSNR values for images reconstructed from representations, prototypes, and entangled representations are 12.89, 10.25, and 9.66, with corresponding MSE values of 4514.91, 6992.04, and 7781.87. Those results suggest that entangled representations tend to produce lower PSNR and higher MSE, which mitigates the risk of representation inversion attacks. Q3: What is the communication overhead of FedRE? We conduct communication overhead experiments on the CIFAR-100 dataset under the PRA setting. As shown in Table 2, FedRE achieves the lowest communication overhead during the upload phase, as it uploads only single entangled representation along with its entangled-label encoding from each client to the server. During the broadcast phase, its overhead is comparable to that of classifier-based methods (e.g., LG-FedAvg) and prototype-based methods (e.g., FedProto). Those results imply that FedRE is effective in reducing communication overhead. More results are offered in Appendix C.2. 4.3. Analysis Q4: How does FedRE perform under different participation ratios with varying statistical heterogeneity? We conduct experiments on the CIFAR-10 dataset under the PRA setting with partial client participation and varying levels of statistical heterogeneity. Specifically, we adopt 100 clients and set the participation rates to 10/100 and 20/100, while adjusting the Dirichlet distribution parameter α to 0.07 and 0.1, respectively, in the PRA setting. Furthermore, we follow [15] and simulate the long-tail settings by modifying imbalance factors (IF) to 100 and 50, then set α to 0.07. We present the results in Table 3 and highlight several observations. (1) FedProto performs relatively poorly, potentially because limited same-category overlap across clients in highly heterogeneous settings weakens prototype aggregation. (2) FedRE achieves the best performance in most scenarios, demonstrating its effectiveness under partial participation with highly heterogeneous distributions. More results are provided in Appendix C.4. Q5: What are the advantages of uploading single entangled representation per client compared to uploading all representations? To explore the benefits of entangled representations, we compare FedRE with FedAllRep, which uses all clients representations to train the global classifier. Table 4 lists the results on the TinyImageNet dataset in both the PRA and PAT settings. As can be seen, FedRE achieves performance comparable to FedAllRep, suggesting that uploading single entangled representation per client can effectively support global classifier training. In addition, FedRE effectively reduces communication overhead compared to uploading all representations. Table 2. Communication overhead (# Scalars 103) comparison on the CIFAR-100 dataset. In each row, the best results are bolded, and the second-best results are underlined. Metric LG-FedAvg FedGH FedKD FedGen FedProto FedMRL FedTGP FPL FedRE Upload Broadcast 513.00 513.00 257.02 512.00 4234.28 4234.28 9247.08 513. 257.02 512.00 8863.08 8863.08 257.02 512.00 257.02 916.48 5.12 513.00 Table 3. Accuracy (%) comparison for partial participation scenarios with varying statistical heterogeneity in the PRA setting on the CIFAR-10 dataset. Here, α is Dirichlet distribution parameter, and IF denotes imbalance factors of the long-tail setting. In each column, the best results are bolded, and the second-best results are underlined. Method Participation Rate = 10/100 Participation Rate = 20/100 α = 0.07 α = 0.1 IF=100, α = 0.07 IF=50, α = 0.07 α = 0.07 α = 0.1 IF=100, α = 0.07 IF=50, α = 0.07 FedProto [34] FedGH [43] FedRE 54.00 78.23 81. 51.18 76.87 79.56 45.21 67.30 67.12 43.92 63.73 66.37 56.90 80.57 82.80 55.47 77.84 81.99 47.08 65.56 69. 44.68 64.90 68.81 Table 4. Accuracy (%) and communication overhead comparison on the TinyImageNet dataset. In each column, the best results are bolded, and the second-best results are underlined. Method PRA PAT Accuracy # Scalars (103) Accuracy # Scalars (103) FedAllRep FedRE 31.20 30.48 42160.39 4118.48 38.62 38.52 42258.88 4118.48 Q6: How effective are different RM operations? We evaluate three RM operations: (1) Average Pooling (AP) averages representation values across spatial regions. (2) Max Pooling (MP) selects the maximum representation values across regions. (3) Fully Connected layer (FC) performs learned aggregation operation on representations. Table 5 reports results on the CIFAR-100 dataset under both PRA and PAT settings. AP exhibits favorable performance and is thus adopted as the default choice in FedRE as an empirical design decision. Table 5. Accuracy (%) comparison between distinct RM operations on the CIFAR-100 dataset in PRA and PAT settings. In each row, the best results are bolded, and the second-best results are underlined. Setting PRA PAT AP 46.36 62.56 MP 45.97 61.93 FC 44.53 60.19 Q7: How effective are distinct RE mechanisms? We design five distinct RE mechanisms, with their mathematical formulations provided in Appendix A. Below, we only outline how entangled representations are obtained, as entangled-label encodings are derived analogously from one-hot label encodings: (1) Random Select Representation (RSR) randomly selects one representation from each client; (2) Vanilla Average Representation (VAR) averages all representations per client into single representation, with equal weight assigned to each; (3) Random Average Representation (RAR) entangles all representations per client into single representation using normalized weight vector, with elements randomly drawn from Uniform distribution U(0, 1) and normalized to sum to one; (4) Random Select Prototype (RSP) first calculates prototypes for each client and then randomly selects one prototype per client; (5) Vanilla Average Prototype (VAP) calculates prototypes for each client and averages them into single representation, where each prototype contributes equally; (6) Random Average Prototype (RAP) calculates prototypes for each client and aggregates them into single representation using normalized weight vector, where each weight is randomly drawn from Uniform distribution U(0, 1) and normalized to sum to one. Table 6 lists the results on the CIFAR-10 and CIFAR-100 datasets in the PRA setting. We have the following observations. (1) RSR performs the worst, as each client uploads only randomly selected representation, which is insufficient to train the global classifier. (2) RSP outperforms RSR, as the prototype aggregates all representations within category, it is more representative than single vanilla representation. (3) VAP and RAP outperform VAR and RAR, respectively, indicating that prototype-based entanglement yields better model performance. (4) RAP surpasses VAP, demonstrating that random weights for entanglement are more effective than equal weights. Thus, we empirically choose RAP in the implementation of FedRE. Q8: How do different distributions in RAP affect FedRE performance? As mentioned above, in RAP, we sample weights from Uniform distribution U(0, 1). To examine the effect of the distribution choice, we replace it with Laplace Table 6. Accuracy (%) comparison between distinct RE mechanisms on the CIFAR-10 and CIFAR-100 datasets in the PRA setting. In each row, the best results are bolded, and the second-best results are underlined. Dataset RSR VAR RAR RSP VAP RAP CIFAR-10 CIFAR79.10 40.41 81.32 44.88 80.20 43.19 80.45 43.25 81.42 46.12 82.60 46. distribution L(0, 1) and Gaussian distribution G(0, 1). The results on the CIFAR-10 and CIFAR-100 datasets under the PRA setting are reported in Table 7. We can observe that RAP achieves comparable performance across different distributions, with minor advantage for the Uniform distribution. This indicates that FedRE is flexible in supporting diverse distribution configurations in RAP. Table 7. Accuracy (%) comparison of different distributions in RAP on the CIFAR-10 and CIFAR-100 datasets in the PRA setting. In each row, the best results are bolded, and the second-best results are underlined. Dataset Gaussian Laplace Uniform CIFAR-10 CIFAR-100 82.39 45. 82.32 45.84 82.60 46.36 Q9: How effective is per-round random weight resampling in FedRE? In each communication round, each client performs random weight re-sampling (RS) for its local representations. To evaluate its effectiveness, we compare RS with fixed-sampling (FS) variant, in which the weights are sampled only once at initialization and then reused in all subsequent rounds. Experiments are conducted on synthetic dataset consisting of 300 training and 200 test two-dimensional samples distributed across two simulated clients (same as the dataset used in Figure 2), as well as on the CIFAR-100 dataset in the PRA setting with ten clients. As shown in Table 8, RS achieves higher accuracy than FS across both datasets, which suggests the effectiveness of the per-round random weight re-sampling. Table 8. Accuracy (%) comparison between re-sampling (RS) and fixed-sampling (FS) in FedRE on synthetic dataset and the CIFAR-100 dataset. In each row, the best and second-best results are highlighted in bold and underline, respectively. Dataset FS RS Synthetic Dataset CIFAR-100 41.50 45.84 62.00 46.36 Q10: Is the training cost introduced by RE during local training significant in FedRE? In FedRE, we introduce an RE mechanism during local training (LT). To evaluate its training cost, we compare two settings: LT without RE and LT with RE, where the former ablates the RE component. Table 9 reports the average training cost (in seconds) per round, averaged across ten clients and six communication rounds. As shown, LT with RE incurs only slight additional cost compared to LT without RE, indicating that the RE mechanism introduces minor computational cost. This is mainly because RE separately aggregates representations and label encodings, without requiring any additional gradient computations. Table 9. Comparison of the average time cost (in seconds) per round for local training (LT) in FedRE without and with RE. In each row, the best results are bolded, and the second-best results are underlined. Dataset LT without RE LT with RE CIFAR-10 CIFAR-100 5.69 5.70 5.78 5. Q11: Is FedRE still effective in model-homogeneous settings? Model-homogeneous FL can be regarded as special case of model-heterogeneous FL, where all clients utilize the same model architecture. In our experiments, we adopt four-layer CNN for the CIFAR-10 and CIFAR-100 datasets and use ResNet-18 for the TinyImageNet dataset. Table 12 in Appendix C.3 presents the results under the model-homogeneous setting, where FedRE achieves the best performance across all datasets. Specifically, FedREs average accuracy is 63.21%, outperforming the secondbest method, i.e., LG-FedAvg, by 2.58%. Figure 6 in Appendix C.3 provides convergence comparisons on the TinyImageNet dataset, where FedRE exhibits rapid initial improvement followed by gradual stabilization, indicating consistent and stable convergence behavior throughout training. Those results suggest that FedRE remains effective in modelhomogeneous FL. 5. Conclusion In this paper, we introduce the entangled representation as an effective, privacy-aware, and lightweight form of client knowledge. Building on this concept, we propose the FedRE framework for model-heterogeneous FL. In FedRE, each client first produces single cross-category entangled representation along with its associated entangled-label encoding, which are then uploaded to the server for global classifier training. Experimental results demonstrate that FedRE achieves well-balanced trade-off among model performance, privacy protection, and communication overhead. promising direction for future work is to explore the applicability of entangled representations to broader range of machine learning tasks."
        },
        {
            "title": "References",
            "content": "[1] Rodolfo Stoffel Antunes, Cristiano Andre da Costa, Arne Kuderle, Imrana Abdullahi Yari, and Bjorn Eskofier. Federated learning for healthcare: Systematic review and architecture proposal. ACM TIST, 13(4):123, 2022. 1 [2] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In ICML, pages 20892099, 2021. 2 [3] Yongheng Deng, Feng Lyu, Ju Ren, Yi-Chao Chen, Peng Yang, Yuezhi Zhou, and Yaoxue Zhang. Fair: Quality-aware federated learning with precise user incentive and model aggregation. In INFORCOM, pages 110, 2021. 2 [4] Yongheng Deng, Feng Lyu, Ju Ren, Yi-Chao Chen, Peng Yang, Yuezhi Zhou, and Yaoxue Zhang. Improving federated learning with quality-aware user incentive and auto-weighted model aggregation. TPDS, 33(12):45154529, 2022. 2 [5] Xin Dong, Sai Qian Zhang, Ang Li, and HT Kung. Spherefed: Hyperspherical federated learning. In ECCV, pages 165184, 2022. 2 [6] Jiamin Fan, Kui Wu, Guoming Tang, Yang Zhou, and Shengqiang Huang. Taking advantage of the mistakes: Rethinking clustered federated learning for iot anomaly detection. TPDS, 2024. 1 [7] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. survey on vision transformer. TPAMI, 45(1):87110, 2022. [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770778, 2016. 5 [9] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. Rethinking federated learning with domain shift: prototype view. In CVPR, pages 1631216322, 2023. 2, 4, 5, 14 [10] Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and Koji Yamamoto. Distillation-based semisupervised federated learning for communication-efficient collaborative training with non-iid private data. TMC, 22(1): 191205, 2021. 2 [11] S. Kim, G. Lee, J. Oh, and S. Y. Yun. Fedfn: Feature normalization for alleviating data heterogeneity problem in federated learning. In NeurIPS 2023 Workshop: Federated Learning in the Age of Foundation Models, 2023. 2 [12] S. Kim, M. Jeong, S. Kim, S. Cho, S. Ahn, and S. Y. Yun. Feddr+: Stabilizing dot-regression with global feature distillation for federated learning. TMLR, 2025. 2 [13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. 4 [14] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 4 [15] Kangwook Lee, Hoon Kim, Kyungmin Lee, Changho Suh, and Kannan Ramchandran. Synthesizing differentially private datasets using random mixing. In ISIT, pages 542546, 2019. 6 [16] Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 1071310722, 2021. 5 [17] Zexi Li, Tao Lin, Xinyi Shang, and Chao Wu. Revisiting weighted aggregation in federated learning with neural networks. In ICML, pages 1976719788, 2023. 1, 2 [18] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier. In ICCV, pages 53195329, 2023. [19] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020. 2, 4, 5, 14 [20] Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. In NeurIPS, pages 23512363, 2020. 5 [21] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Layerwised model aggregation for personalized federated learning. In CVPR, pages 1009210101, 2022. 2 [22] Disha Makhija, Xing Han, Nhat Ho, and Joydeep Ghosh. Architecture agnostic federated learning for neural networks. In ICML, pages 1486014870, 2022. 2 [23] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. Communicationefficient learning of deep networks from decentralized data. In AISTATS, pages 12731282, 2017. 1, 2, 3, 4, 14 [24] Dinh Nguyen, Ming Ding, Pubudu Pathirana, Aruna Seneviratne, Jun Li, and Vincent Poor. Federated learning for internet of things: comprehensive survey. IEEE Commun Surv Tutorials, 23(3):16221658, 2021. [25] Jaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Toward enhanced representation for federated image classification. In ICLR, 2022. 2 [26] Ying Pang, Haibo Zhang, Jeremiah Deng, Lizhi Peng, and Fei Teng. Collaborative learning with heterogeneous local models: rule-based knowledge fusion approach. TKDE, 36 (11):57685783, 2023. 1 [27] Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed, Mike Rabbat, Maziar Sanjabi, and Lin Xiao. Federated learning with partial model personalization. In ICML, pages 17716 17758, 2022. 2 [28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 45104520, 2018. 5 [29] Torsten Schlett, Christian Rathgeb, Olaf Henniger, Javier Galbally, Julian Fierrez, and Christoph Busch. Face image quality assessment: literature survey. ACM Comput Surv, 54(10s):149, 2022. 5 [30] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. In Personalized federated learning using hypernetworks. ICML, pages 94899502, 2021. [31] Yujun Shi, Jian Liang, Wenqing Zhang, Chuhui Xue, Vincent YF Tan, and Song Bai. Understanding and mitigating dimensional collapse in federated learning. TPAMI, 46(5): 29362949, 2023. 2 [32] Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, and Chen Chen. Fedperfix: Towards partial model personalization [48] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018. 4 [49] Hao Zhang, Chenglin Li, Nuowen Kan, Ziyang Zheng, Wenrui Dai, Junni Zou, and Hongkai Xiong. Improving generalization in federated learning with model-data mutual information regularization: posterior inference approach. NeurIPS, 37: 136646136678, 2024. 1 [50] Jianqing Zhang, Yang Hua, Jian Cao, Hao Wang, Tao Song, Zhengui XUE, Ruhui Ma, and Haibing Guan. Eliminating domain bias for federated learning in representation space. In NeurIPS, 2023. 2, 5, 14 [51] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Haibing Guan. Fedala: Adaptive local aggregation for personalized federated learning. In AAAI, pages 1123711244, 2023. 1, 2, [52] Jianqing Zhang, Yang Liu, Yang Hua, and Jian Cao. Fedtgp: Trainable global prototypes with adaptive-margin-enhanced contrastive learning for data and model heterogeneity in federated learning. In AAAI, pages 1676816776, 2024. 2, 5, 14 [53] Jianqing Zhang, Yang Liu, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, and Jian Cao. Pfllib: beginnerfriendly and comprehensive personalized federated learning library and benchmark. JMLR, 26(50):110, 2025. 5 [54] Jianqing Zhang, Xinghao Wu, Yanbing Zhou, Xiaoting Sun, Qiqi Cai, Yang Liu, Yang Hua, Zhenzhe Zheng, Jian Cao, and Qiang Yang. Htfllib: comprehensive heterogeneous federated learning library and benchmark. In KDD, 2025. 5 [55] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In ICML, pages 1287812889, 2021. 2, 4, 5, 14 of vision transformers in federated learning. In ICCV, pages 49884998, 2023. 2 [33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 19, 2015. 5 [34] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In AAAI, pages 84328440, 2022. 2, 4, 5, 7, 14 [35] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In CVPR, pages 94469454, 2018. 2, Deep image prior. 3, 6 [36] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In ICML, pages 64386447, 2019. 4 [37] Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu. Addressing class imbalance in federated learning. In AAAI, pages 10165 10173, 2021. 2 [38] Lei Wang, Jieming Bian, Letian Zhang, Chen Chen, and Jie Xu. Taming cross-domain representation variance in federated prototype learning with heterogeneous data domains. arXiv preprint arXiv:2403.09048, 2024. 2 [39] Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation. Nature communications, 13(1): 2032, 2022. 2, 4, 5, 14 [40] Feijie Wu, Xingchen Wang, Yaqing Wang, Tianci Liu, Lu Su, and Jing Gao. Fiarse: Model-heterogeneous federated learning via importance-aware submodel extraction. In NeurIPS, 2024. [41] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM TIST, 10(2):119, 2019. 1 [42] Mang Ye, Xiuwen Fang, Bo Du, Pong Yuen, and Dacheng Tao. Heterogeneous federated learning: State-of-the-art and research challenges. ACM Comput Surv, 56(3):144, 2023. 1 [43] Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, and Han Yu. Fedgh: Heterogeneous federated learning with generalized global header. In ACM MM, pages 86868696, 2023. 2, 3, 4, 5, 7, 14 [44] Liping Yi, Han Yu, Chao Ren, Gang Wang, Xiaoxiao Li, et al. Federated model heterogeneous matryoshka representation learning. NeurIPS, 37:6643166454, 2024. 2, 4, 5, 14 [45] Hongxu Yin, Pavlo Molchanov, Jose Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In CVPR, pages 87158724, 2020. 2 [46] Chen Zhang, Yu Xie, Tingbin Chen, Wenjie Mao, and Bin Yu. Prototype similarity distillation for communication-efficient federated unsupervised representation learning. TKDE, 36 (11):68656876, 2024. 2 [47] Feilong Zhang, Deming Zhai, Guo Bai, Junjun Jiang, Qixiang Ye, Xiangyang Ji, and Xianming Liu. Towards fairness-aware and privacy-preserving enhanced collaborative learning for healthcare. Nat Commun, 16(1):2852, 2025. 1 Additional details and results are provided in the appendices, covering the following contents. Appendix A: Mathematical details of various representation entanglement mechanisms. Appendix B: Detailed experimental setup. Appendix C: Supplementary experimental results. A. Mathematical Details of Various Representation Entanglement Mechanisms We now introduce the mathematical details of different RE mechanisms. The general form of RE, calculated from single clients representation set = {(ri, yi)}n i=1, is formulated by (cid:101)rRE = (cid:88) i=1 wiri, (cid:101)yRE = (cid:88) i=1 wiyi. (6) Here, wi [0, 1] is the weight of ri, which is determined by different RE mechanisms as follows: Random Select Representation (RSR) randomly selects one representation from each client per global communication round. Thus, wi is formulated as wi = (cid:40) 1, 0, if ri is selected otherwise. (7) Vanilla Average Representation (VAR) averages all representations per client into single representation, with equal weight assigned to each. Hence, wi is defined by wi = 1 , {1, 2, , n}. (8) Random Average Representation (RAR) entangles representations per client into single representation using normalized weight vector, with elements randomly drawn from Uniform distribution U(0, 1) and normalized to sum to one. Accordingly, wi is formulated as follows: wi = ui j=1 uj (cid:80)n , where ui U(0, 1). (9) Random Select Prototype (RSP) first calculates prototypes for each client and then randomly selects one prototype per client in each global communication round. Therefore, wi is defined as wi = , (cid:40) 1 nc 0, if both the selected prototype and ribelong to category otherwise, (10) where nc denotes the total number of samples belonging to category c. Vanilla Average Prototype (VAP) calculates prototypes for each client and averages them into single representation, where each prototype contributes equally. Thus, wi is calculated by wi = 1 Cnc , if ribelongs to category c, (11) where is the total number of categories in the client. Random Average Prototype (RAP) calculates prototypes for each client and aggregates them into single representation using normalized random weight vector, where each weight uc U(0, 1), and the weights are normalized to sum to one. Using those weights, wi is defined as follows: wi = uc (cid:80)C j=1 uj , nc if ri belongs to category c. (12) B. Detailed Experimental Setup Table 10 provides detailed description of the experimental setup used in this paper, covering devices, software tools, statistical-heterogeneous setting, model training details, and model configuration. Table 10. Detailed experimental setup used in this paper. Devices Software Tools CPU: Intel(R) Xeon(R) Gold 6348; GPU: A800 CUDA 12.1; PyTorch 2.1.0; Python 3. Statistic-heterogeneous Setting Practical setting (PRA); Pathological setting (PAT) Model Training Details Local batch size: 64 (TinyImageNet), 32 (CIFAR-10 & CIFAR-100) Local optimizer: SGD Local learning rate ηl: 0.06 (Model-Heterogeneity with PRA & PAT, CIFAR-10/100/TinyImageNet) 0.01 (Model-Homogeneity with PRA & PAT, CIFAR-100/TinyImageNet) 0.007 (Model-Homogeneity with PRA, CIFAR-10) 0.008 (Model-Homogeneity with PAT, CIFAR-10) Server batch size: 10; Server optimizer: SGD; Server learning rate: 0.01 Model Configuration Local model in Model-Heterogeneity: CNN; MobileNetV2; GoogleNet; ResNet-18/34/50/101/152; ViT-B/16; ViT-B/32 Local model in Model-Homogeneity: CNN (CIFAR-10/100); ResNet-18 (TinyImageNet) C. Supplementary Experimental Results C.1. Privacy Protection Evaluation Figure 5 presents additional reconstructed results on sample images from the TinyImageNet dataset. As can be seen, images reconstructed from the entangled representations contain less discernible content or category information than those reconstructed from vanilla representations or prototypes, suggesting that FedRE offers good level of privacy protection against representation inversion attacks. C.2. Communication Overhead Evaluation Table 11 lists the communication overhead results on the CIFAR-10, CIFAR-100, and TinyImageNet datasets under both model-heterogeneous (Model-hete) and model-homogeneous (Model-homo) scenarios in the PRA setting. As can be seen, FedRE generally achieves lower communication overhead than the baselines, which suggests its potential effectiveness in reducing communication overhead. Table 11. Communication overhead (# Scalars 103) comparison on three datasets. In each column, the best results are bolded, and the second-best results are underlined. Method Model-homo Model-hete Model-homo Model-hete Model-homo Model-hete CIFAR-10 CIFAR-100 TinyImageNet UploadBroadcastUploadBroadcastUpload BroadcastUploadBroadcast Upload Broadcast Upload Broadcast 51.30 51. 51.30 51.20 51.30 31.23 513.00 257.02 LG-FedAvg 51.30 31.23 4098.00 4098.00 4098.00 4096.00 1918.98 4096.00 FedGH FedKD 3374.28 3374.28 3353.68 3353.68 4234.28 4234.28 3524.67 3524.67 90503.00 90503.00 57544.97 57544.97 513.00 239178.32 239178.32 4098.00 4098.00 9247.08 9247.08 513.00 FedGen 4096.00 1918.98 4096.00 512.00 512.00 257.02 257.02 FedProto 9768.96 1918.98 10567.68 257.02 1182.72 916.48 257.02 FPL FedMRL 8746.98 8746.98 8746.98 8746.98 8863.08 8863.08 8863.08 8863.08 56178.00 56178.00 56178.00 56178.00 4096.00 1918.98 4096.00 FedTGP 8785.38 8785.38 51.20 87. 51.30 51.20 112.64 51.30 31.23 31.23 1918.98 1918.98 4098.00 1918.98 513.00 257.02 513.00 512. 513.00 512.00 31.23 31.23 1918.98 257.02 257.02 512. 512.00 51.20 31.23 51.20 31.23 FedAvg 8785.38 8785.38 FedALA 8785.38 8785.38 FedAvgDBE8785.38 8785. - - - - - - 9247.08 9247.08 9247.08 9247.08 9247.08 9247.08 - - - - - - 239178.32 239178.32 239178.32 239178.32 239178.32 239178. - - - - - - FedRE 5.12 51.30 5. 51.30 5.12 513.00 5.12 513.00 20. 4098.00 20.48 4098.00 (a) Original Images (b) Reconstruction Images from Representations (c) Reconstruction Images from Prototypes (d) Reconstruction Images from Entangled Representations Figure 5. Comparison of privacy protection in restructuring results from representations, prototypes, and entangled representations on the TinyImageNet dataset. C.3. Model-homogeneous FL Evaluation Model-homogeneous FL can be considered as special case of model-heterogeneous FL, where all clients use the same local model architecture. In our experiments, we adopt four-layer CNN for the CIFAR-10 and CIFAR-100 datasets and use ResNet-18 for the TinyImageNet dataset. Table 12 presents the results in the model-homogeneous setting. The results indicate that FedRE yields competitive accuracy, outperforming existing baselines on each dataset. Figure 6 shows the convergence curves on TinyImageNet, demonstrating the accuracy evolution during training and the relatively stable convergence of FedRE. (a) PRA (b) PAT Figure 6. Accuracy (%) comparison between distinct communication rounds on the TinyImageNet dataset in the model-homogeneous FL setting in both the PRA and PAT settings. Table 12. Accuracy (%) comparison on three datasets in the model-homogeneous setting. In each column, the best results are bolded, and the second-best results are underlined. Method PRA PAT CIFAR-10 CIFAR-100 TinyImageNet CIFAR-10 CIFAR-100 TinyImageNet FedAvg [23] FedALA [51] FedGH [43] FedKD [39] LG-FedAvg [19] 86.92 0.25 49.82 0.39 32.00 0.13 90.59 0.17 66.00 0.27 38.43 0.23 55.21 0.12 30.37 0.02 13.66 0.41 52.70 0.11 24.89 0.20 9.98 0.48 55.02 0.14 29.89 0.22 13.63 0.10 52.83 0.19 24.91 0.15 10.65 0.15 86.02 0.17 48.59 0.60 28.64 0.26 90.46 0.22 65.14 0.26 32.40 0.19 86.23 0.12 51.91 0.28 29.47 0.31 90.01 0.09 67.23 0.38 35.34 0.33 FedAvgDBE [50] 78.10 0.20 35.23 0.24 16.92 0.52 82.27 0.45 35.21 0.27 16.80 0.23 55.21 0.14 29.90 0.17 13.76 0.23 52.37 0.22 24.82 0.38 10.67 0.54 85.63 0.22 50.52 0.19 28.67 0.17 91.04 0.16 69.28 0.07 34.75 0.49 83.60 0.03 49.10 0.21 26.87 0.09 90.59 0.06 67.31 0.03 32.95 0.23 82.55 0.31 48.41 0.09 26.78 0.05 89.02 0.23 65.97 0.21 35.22 0.05 85.59 0.12 47.05 0.17 30.89 0.21 90.49 0.03 67.47 0.07 40.88 0.20 86.33 0.11 49.88 0.40 31.44 0.15 90.54 0.12 66.57 0.17 37.46 0.27 86.99 0.01 52.12 0.04 36.12 0.21 91.06 0.01 70.52 0.17 42.45 0.17 FedGen [55] FedProto [34] FPL [9] FedMRL [44] FedTGP [52] Local FedRE Average 60.63 31.14 31.16 58.54 60.03 44.09 31.12 59.98 58.40 57.99 60.40 60.37 63.21 C.4. Statistical Heterogeneity Analysis To further evaluate the effectiveness of FedRE under different levels of statistical heterogeneity, we adjust the Dirichlet distribution parameter α (i.e., 0.05, 0.1, 1, 10) in the PRA setting and the client participation rate (i.e., 5/25, 10/25 for 25 clients, 5/10, 10/10 for 10 clients) in the PAT setting, respectively, to control the degree of sample skewness. The resulting sample distributions are visualized in Figures 7-8. The results on the CIFAR-10 and CIFAR-100 datasets, under the modelheterogeneous setting, are shown in Figure 9. As can be seen, FedRE yields competitive accuracy across different levels of statistical heterogeneity, suggesting it can handle variety of heterogeneous scenarios. (a) CIFAR-10 (α = 0.05) (b) CIFAR-10 (α = 0.1) (c) CIFAR-10 (α = 1) (d) CIFAR-10 (α = 10) (e) CIFAR-100 (α = 0.05) (f) CIFAR-100 (α = 0.1) (g) CIFAR-100 (α = 1) (h) CIFAR-100 (α = 10) Figure 7. The sample distributions for all clients on the CIFAR-10 and CIFAR-100 datasets under the PRA settings with varying parameters α. The size of each circle indicates the number of samples. (a) CIFAR-10 (10 Clients) (b) CIFAR-10 (25 Clients) (c) CIFAR-100 (10 Clients) (d) CIFAR-100 (25 Clients) Figure 8. The sample distributions for all clients on the CIFAR-10 and CIFAR-100 datasets under the PAT settings with varying client numbers. The size of each circle indicates the number of samples. (a) CIFAR-10 with PRA (b) CIFAR-100 with PRA (c) CIFAR-10 with PAT (d) CIFAR-100 with PAT Figure 9. Accuracy (%) comparison between distinct statistic-heterogeneous scenarios on the CIFAR-10 and CIFAR-100 datasets."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "China University of Mining and Technology",
        "China University of Mining and Technology-Beijing",
        "Nanjing University of Posts and Telecommunications",
        "Northwestern University",
        "Teleinfo, CAICT",
        "Tsinghua University",
        "University of British Columbia",
        "University of Texas at Dallas"
    ]
}