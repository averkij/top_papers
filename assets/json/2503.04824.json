{
    "paper_title": "ProReflow: Progressive Reflow with Decomposed Velocity",
    "authors": [
        "Lei Ke",
        "Haohang Xu",
        "Xuefei Ning",
        "Yu Li",
        "Jiajun Li",
        "Haoling Li",
        "Yuxuan Lin",
        "Dongsheng Jiang",
        "Yujiu Yang",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 2 8 4 0 . 3 0 5 2 : r ProReflow: Progressive Reflow with Decomposed Velocity Lei Ke1 Haohang Xu3 Xuefei Ning1 Yu Li1 Jiajun Li4 Haoling Li1 Yuxuan Lin1 Dongsheng Jiang3 Yujiu Yang1 Linfeng Zhang2 1Tsinghua University 2Shanghai Jiao Tong University 3Huawei Inc. 4University of Electronic Science and Technology of China kl23@mails.tsinghua.edu.cn, yang.yujiu@sz.tsinghua.edu.cn, zhanglinfeng@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into straight line for few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05). Our codes will be released at Github. 1. Introduction Diffusion models have achieved significant breakthroughs in image and video generation, boosting various downstream applications such as text-to-image generation [27, 32] and image editing [2, 3, 10]. However, compared with traditional generation models such as GANs [9], the sampling process of diffusion models is formulated to include multiple timesteps, which severely harms its computation efficiency, hindering its application in edge devices and real-time applications. To solve this problem, abundant methods have been proposed to reduce the number of sampling steps such as step distillation [24, 34], consistency models [23, 38] and flow matching [16, 17]. Corresponding author. Figure 1. (a) L2 distance and Cosine similarity across velocities at different timesteps, the velocity discrepancy between timesteps increases with their distance in timesteps. (b) The consistently larger FID degradation under directional noise demonstrates that velocity direction is more critical for generation quality. Among them, flow matching has gained popularity due to its simplicity and effectiveness. By re-flowing the pretrained diffusion models into line, few steps and even 1step generation can be achieved with tolerant loss in generation quality. The training process of reflow usually contains two steps. Firstly, the pretrained diffusion model generates abundant (noise, image) pairs. Then, the diffusion model is trained to make the velocity at different timesteps to be identical, indicating that the trajectory is rectified. However, in this paper, we suggest that such training strategy has not fully unleashed the potential of rectified flow and introduced two training techniques referred to as progressive reflow and aligned v-prediction to further improve it. Progressive Reflow: Traditional Reflow usually starts from pretrained diffusion model and directly trains it to have consistent prediction of velocity in all timesteps, which is theoretically correct but introduces difficulty in the optimization process. As shown in Figure 1 (a), the pretrained diffusion model has significantly different velocities at different timesteps, and directly eliminating these differences raises challenges in the training process. Fortunately, Figure 1 (a) also shows that the pretrained diffusion model exhibits similar velocity in the adjacent timesteps, which provides the possibility to first reflow the model in local window, and then reflow it in the whole training process. Such progressive reflow pipeline allows the model to first learn to solve an easy problem and then extend to difficult problem, which implies curriculum learning in generative models and thus facilitates the training process. Based on this observation, we propose progressive reflow, which firstly divides the whole diffusion process into windows, and then progressive reflow windows into N/2, N/4, N/2, N/8 until very few and even one window. Aligned V-Prediction: Flow matching aims to match the velocity in different timesteps to achieve the target that the whole diffusion process is straight line. However, we suggest that such velocity matching is not optimal for the target of straight line as the velocity can be further decomposed into its direction and magnitude, where the direction is more crucial for straightness. In other words, matching the direction of the velocity should have higher priority than matching the magnitude, which has been ignored in previous works. Based on this observation, we propose to modify the original training loss of flow matching by introducing direction matching to solve this problem. Our experiments have validated the effectiveness of the two improved training techniques. For instance, on MSCOCO-2017, 10.94 and 21.73 FID reduction can be observed with our ProReflow-II compared to rectified flow (2ReFlow [18]) at 4 steps and 2 steps respectively, demonstrating improvements in generation quality In summary, our contributions are as follows. We propose progressive reflow, which progressive reflow the diffusion model in local timesteps until the whole diffusion process. Progressive reflow implies the curriculum learning in flow matching and facilitates model training. Based on the observation that the direction of velocity is more crucial than the magnitude for straightness, we introduce velocity direction matching as an additional target for flow matching to facilitate model training. Extensive experiments demonstrate that both components are effective individually, and their combination achieves state-of-the-art performance with only 4 sampling steps. 2. Related Work 2.1. Text-to-Image Generation Diffusion models(DMs) learn the mapping from noise to images by fitting marginal probability distributions at each timestep [8, 37]. It works well because the forward diffusion process, which progressively adds noise to images, maintains the same marginal distributions as the sampling process [22]. Combined with some technologies like classifier-free guidance and text encoder [7, 26, 33], DMs have surpassed GANs [4, 31] and VAEs [12, 29] not only in generation quality but also in training stability. Besides applied in pixel space, DMs can be effectively applied in latent space as well, which significantly reduces computational complexity [32]. Despite achieving impressive generation quality, the iterative nature of DMs impacts its generation efficiency. Consequently, accelerating inference of diffusion models has emerged as an avtive research topic. 2.2. Efficient Diffusion Existing approaches for accelerating DMs can be predominantly classified into two categories: efficient diffusion samplers and step distillation [45]. The former category incorporates differential equation solvers into inference without requiring additional training. DDIM [36] enables step skipping in the reverse process by introducing non-Markovian sampling strategy. DPMSolver [21] reformulates the reverse diffusion process into an ODE system and solves it with high-order numerical methods, achieving superior sampling efficiency. Samplerbased methods enable diffusion models to maintain satisfactory generation quality with 20 steps; however, performance deteriorates significantly when further reducing the step count (such as below 10). The second category methods enhance few-step inference performance through another step distillation process. Progressive Distillation(PD) [34] adopts staged approach, iteratively halving the student models sampling steps. Adversarial Diffusion Distillation(ADD) [35] leverages adversarial training for improved supervision, while Consistency Distillation (CD) [38] enforces output convergence toward the target image across the sampling trajectory. 2.3. Rectified Flow Flow matching has emerged as kind of advanced diffusion model [16, 17]. It reformulates the forward process as linear interpolation between noise and images, thereby proposing to predict consistent velocity across the entire sampling trajectory. Thus, the sampling process is simplified to temporal integration of the velocity field v. Similarly, ReFlow was proposed as technique applying flow matching to pretrained diffusion models, enabling the adaptation of existing architectures without retraining from Figure 2. Conceptual illustration of different methods. (a)(e) compare training objectives and sampling trajectories across different methods. Arrows show optimization targets, and red dashed lines represent actual sampling trajectories, which are curved due to the optimization not achieving the theoretical optimum. (e) shows our progressive reflow method achieves better approximation. (f) presents how our proposed aligned v-prediction works between timesteps [t, t+1], it reduces prediction deviation with velocity direction correction. scratch [17]. InstaFlow [18] first extended ReFlow to largescale text-to-image models through consecutive ReFlow to straighten the ODE trajectory, followed by distillation to achieve single-step sampling. Subsequently, some works explored improving ReFlows effectiveness or simplifying its training[13, 14, 42, 43]. While ReFlow showed promise for single-step generation, its few-step sampling performance lagged behind state-of-the-art methods [30, 34, 39]. To address this limitation, PeRFlow [44] proposed trajectory partitioning into time windows, achieving competitive few-step sampling through localized straightening within each temporal segment. 2.4. Privileged Information in Distillation Although knowledge distillation has been proven effective as model compression technique and further extended successfully to diffusion model acceleration, the theoretical explanation for its efficacy has remained elusive. How dark knowledge is effectively captured from teacher models and utilized to guide student learning remains fundamental theoretical question [6]. Lopez-Paz et al. [19] presented unified theoretical framework that connects distillation with privileged information, establishing generalized framework for understanding machine-to-machine knowledge transfer. Viewing distillation as transfer of privileged information, TAKD [25] showed that an assistant model of intermediate capacity could more effectively mediate the knowledge flow between teacher and student models. 3. Methods We present ProReflow, more robust flow model training method. Our approach is motivated by the observation that training efficient few-step flow models faces two main challenges: (1) the significant trajectory approximation gap between teacher and student models, and (2) the difficulty in achieving accurate velocity prediction across large time intervals. To address these challenges, we propose progressive reflow for stable optimization of sample trajectory and aligned v-prediction for achieving precise velocity prediction, respectively, shown in Fig. 2. 3.1. Temporal Segmentation for ReFlow Rectified Flow ReFlow aims to achieve temporally consistent velocity predictions across all timesteps. Given initial Gaussian distribution π0 and target image distribution π1, where X1 π1 and X0 π0. Reflow defines linear process from π0 to π1, where the corresponding sampling process follows the ODE: dXt = v(Xt, t)dt, [0, 1], (1) Then, it formulates least-squares optimization problem to ensure the predictions consistency: (cid:90) 1 0 min θ E[X1 X0 vθ(Xt, t)2], (2) where Xt = tX1 + (1 t)X0. 3 Figure 3. Performance of our models under different factors of classifier-free guidance (CFG) on COCO-2017. CFG scale ranges from 2 to 7. and II stands for ProReflow-I with 4 steps and ProReflow-II with 2 steps, respectively. Piecewise ReFlow Aimed at improving the few-step generation, PeRFlow divides the sampling trajectory into multiple time windows, defined by endpoints 1 = tK > > tk > tk1 > > t0 = 0. Within each time window [tk, tk1) formed by adjacent endpoints, PeRFlow assumes linear process to straighten the trajectory, thus eq.(2) can be reformulated as: min θ (cid:88) Eztk πk k=1 (cid:34)(cid:90) tk tk1 ztk1 ztk tk1 tk (cid:35) vθ(zt, t)2dt , (3) where zt = αtztk + (1 αt)ztk1, αt = ttk1 . Finally, tktk1 PeRFlow results in piecewise linear trajectory composed of multiple segments. 3.2. Progressive ReFlow PeRFlow originally sets the number of time windows to 4. Despite achieving improvement in few-step inference, PeRFlow faces significant optimization challenge: it attempts to approximate the teacher models irregular trajectory using four linear intervals within single training stage. We propose multi-stage progressive training scheme to tackle this challenge. Rather than directly mapping the original trajectory to four time windows, our method first obtains an eight-window approximation from the original trajectory, and subsequently apply Cross-windows ReFlow to refine this eight-window representation into the target fourwindow configuration. Cross-windows ReFlow Consider three consecutive time points tk1, tk, tk+1. The optimization objectives in first training stage can be formulated as: (cid:32) min θ Eztk πk (cid:90) tk ztk1 ztk tk1 tk vθ(zt, t)2dt + Eztk+1 πk+1 tk1 (cid:90) tk+1 tk ztk ztk+1 tk tk+ vθ(zt, t)2dt (cid:19) , (4) 4 Figure 4. FID on COCO-30K. The yellow curve shows results trained with 4 windows and evaluated using 4 inference steps, while the blue curve represents the model trained with 8 windows and evaluated using 8 inference steps. Both configurations are compared against their baselines where α = 0 (MSE loss only). Each model is trained for 10,000 iterations with batch size 32. (cid:40) where zt = αtztk + (1 αt)ztk1 , βtztk+1 + (1 βt)ztk , and βt = ttk [tk1, tk) [tk, tk+1) with αt = ttk1 . In adjacent time windows, tktk1 trajectories evolve from ztk1 to ztk in [tk1, tk], and from ztk to ztk+1 in [tk, tk+1]. tk+1tk Cross-windows ReFlow aligns the optimization direction by guiding trajectories in both intervals to progress from ztk1 towards ztk+1 , thus eq.(4) can be reformulated as: min θ Eztk+1 πk+1 (cid:90) tk+1 tk ztk1 ztk+1 tk1 tk+1 vθ(zt, t)2dt, (5) . where zt = αtztk+1 + (1 αt)ztk1 , αt = ttk1 tk+1tk1 Theoretical Explanation Based on the theoretical framework of knowledge distillation [19], we can explain the effectiveness of Progressive ReFlow. Consider three key the teacher function ft Ft representing the functions: original diffusion trajectory, an intermediate function fa Fa for the 8-segment approximation, and the student function fs Fs for the target 4-segment representation. According to the VC theory [41], when the student learns directly from the teacher, the learning rate is bounded by: R(fs) R(ft) (cid:19) (cid:18) FsC nβ + εl, (6) where β [ 1 2 , 1] denotes the learning rate associated with task difficulty, εl represents the approximation error, represents the error, O()and ϵ represent the estimation error and approximation error, respectively.. The challenge lies in the significant capacity gap between the complex trajectory and the 4-segment approximation, resulting in small β that indicates difficult learning. Progressive ReFlow decomposes this challenging process into two stages: Stage 1: R(fa) R(ft) Stage 2: R(fs) R(fa) (cid:19) (cid:19) (cid:18) FaC nβ1 (cid:18) FsC nβ2 + εat, (7) + εsa. (8) The effectiveness is theoretically guaranteed when: (cid:18) FaC nβ1 + FsC nβ2 (cid:19) +εat+εsa (cid:19) (cid:18) FsC nβ +εs, (9) this inequality is satisfied in practice due to two key principles: (1) 8-segment allows for better fitting of the teachers complex sampling trajectory,leading to smaller combined approximation error (εat + εsa < εl), (2) Enhanced optimization efficiency through the progressive process, where each stage solves simpler problem compared to direct optimization,resulting in β1, β2 > β. 3.3. Aligned V-prediction We analyzed approximate error in the optimization process and found that directional errors lead to more significant performance degradation compared to magnitude errors, shown in Fig.1 (b). We then propose aligned v-prediction, which emphasizes direction alignment in training. Direction Matters Consider two arbitrary points zti1 and zti along the trajectory. Given the target vector = zti zti1 and the model prediction p. According to the law of cosines, the error between and can be expressed as: = p2 + v2 2pv cos θ, (10) where θ denotes the angle between and v. We analyze two extreme cases: Misaligned, accurate magnitude (p = v, θ = ϵ): r1 = 2v2(1 cos ϵ); (11) Aligned, inaccurate magnitude (θ = 0, = + ϵ): r2 = (v + ϵ)2 + v2 2v(v + ϵ) = ϵ2. (12) Let = r1 r2. Using Taylor expansion for small ϵ: = (v2 1)ϵ2. (13) Our empirical measurements using real image-noise pairs during training show that typically ranges from 70 to 120, yielding > 0 with substantial margin. This indicates that directional errors lead to significantly larger performance degradation than magnitude errors. Directional Alignment Our analysis reveals that directional components of play more crucial role in generation quality than magnitude. Based on this, we proposed aligned v-prediction in flow matching, which incorporates directional alignment through cosine similarity measurements. Specifically, we propose novel flow matching loss function that places greater emphasis on directional alignment: = (1 α) MSE(v, pred) + α (1 cos(v, pred)), (14) where the first term provides basic magnitude consistency, the second term enforces explicit directional alignment via cosine similarity. The hyperparameter α balances the relative importance between magnitude and direction. Algorithm 1: ProReflow Algorithm Input: D: dataset, fϕ: teacher model, K: list of window numbers (e.g., [8,4,2]), α: loss weight (default=0.1) 1 Initialize student model fθ fϕ; 2 for in do Split time [0, 1] into windows; while not converged do 4 5 6 7 9 10 11 12 13 Sample from dataset D; Sample ϵ (0, 1); Sample timestep and locate window [t1, t2] s.t. [t1, t2]; zt2 = t2 + (1 t2)ϵ; Compute zt1 using fϕ; zt = + (1 t)ϵ; Compute target velocity = ϵ; Predict vθ = fθ(zt, t); = (1 α)MSE(v, vθ) + αDir(v, vθ); Update parameters θ; end 15 16 end Output: Trained model fθ Hyperparameter Configuration Increasing the value of α enhances the directional supervision in the optimization objective. When α = 0, the loss function degenerates to the conventional MSE loss. To determine the optimal hyperparameter configuration, we systematically evaluated different settings: We randomly sampled 0.8M images from LAIONart as our training set and fine-tuned SDv1.5 with different α values while maintaining the same number of windows. We computed FID on coco-30k to evaluate these models. As shown in Fig. 4, the choice of α significantly impacts the model performance. Among the evaluated α values, more positive gains were observed with windows = 4 compared to windows=8, which may be attributed to the increased importance of directional consistency at larger window spans. Our experiment results show that α=0.1 works well in all experiment settings, thus we maintained α=0.1 for subsequent experiments. Combining progressive reflow and aligned v-prediction, we present ProReflow, as shown in Algorithm 1. 5 Table 1. Performance comparison on COCO-2017 validation set, following the evaluation setup in [40]. Our method outperforms existing flow-based approaches. Method Step FID () CLIP Score() 2-Reflow [18] 2-Reflow [18] Instaflow-0.9B [18] Instaflow-0.9B [18] PeRFlow [44] ProReflow-I (ours) ProReflow-II (ours) ProReflow-II (ours) 2 4 2 4 4 4 2 4 49.32 32.97 71.54 102.41 23.81 22.97 27.59 22.03 27.36 28.93 26.07 24.39 30.24 30.29 27.79 29. 4. Experiments 4.1. Experiment Configuration Model and Dataset We evaluate our proposed method primarily on Stable Diffusion v1.5 and Stable Diffusion XL. During training, we freeze all modules except the UNet and employ BF16 mixed precision training. For SDv1.5, we initialize our training process with windows numbers=8 and progressively apply our method to derive ProReflow-I (4 windows), which subsequently serves as the basis for developing ProReflow-II. For SDXL, we adopt training configurations established in ProReflow-I on SDXL to develop ProReflow-SDXL, achieving four-steps sampling. As for multi-stage training, we maintain consistency in the teacher models sampling trajectory across different training stages by fixing the total DDIM steps to 32. Specifically, when windows = 8, we use 4 DDIM steps within each window to derive the endpoint from the starting point. For windows = 4, we use 8 DDIM steps per window. This ensures that the teachers sampling trajectory remains identical across different training stages, allowing for fair comparisons and stable optimization. SDv1.5 is trained on the LAION-Art dataset, with all images center-cropped to 512 512 resolution following its default setup. For SDXL, we fine-tune the model using combination of LAION-Art and 1.5 million samples from the laion2B-en-aesthetic dataset, with all images centercropped to 1024 1024 resolution. All experiments were conducted on 8 NVIDIA H20 GPUs. Table 2. Performance comparison on COCO-2014 validation set, following the evaluation setup in [11]. Method Time () Step FID () ODE-solver based methods DPMSolver [21] DPMSolver [21] DPMSolver++ [20] DDIM(our teacher) [36] Distillation-based methods LCM-LoRA [23] LCM-LoRA [23] UniPC [46] Flash Diffusion [1] PCM [39] Flow-based methods Instaflow-0.9B [18] Instaflow-0.9B [18] 2-ReFlow [18] 2-ReFlow [18] PeRFlow [44] ProReflow-I (ours) ProReflow-II (ours) ProReflow-II (ours) 0.88s 0.34s 0.26s 0.12s 0.19s 0.19s 0.19s 0.19s 0.13s 0.21s 0.13s 0.21s 0.21s 0.21s 0.13s 0.21s 25 8 4 32 2 4 4 4 4 2 4 2 4 4 4 2 4 9.78 22.44 22.36 10.05 24.28 23.62 23.30 12.41 11. 24.61 44.01 20.17 15.32 12.01 11.16 15.44 10.70 4.2. Quantitative Results We first compare our method with other flow-based acceleration approaches on COCO-2017 validation set, as shown in Table 1. With 4 inference steps, ProReflow-II achieves an FID of 22.03 and CLIP score of 29.95, showing significant improvements over 2-ReFlow, Instaflow and PeRFlow.Even with only 2 steps, ProReflow-II maintains competitive performance. ProReflow-I also demonstrates strong performance with an FID of 22.97 and the highest CLIP score of 30.29. Table 2 summarizes the comprehensive evaluation results Result on COCO-2014 valdation dataset with other diffusion acccelaration methods. With 32-step DDIM serving as our teacher model, ProReflow-II achieves competitive FID of 10.70 using only 4 steps. Table 3 presents comprehensive comparison of our method with advanced acceleration approaches on SDXL. Our method achieves state-of-the-art performance while maintaining the same inference cost. 4.3. Qualitative Comparison Evaluation Setting Following common practice in textto-image generation, we adopt two widely-used quantitative metrics: Frechet Inception Distance (FID) [5] and clip score [28]. The evaluation is mainly conducted on two standard benchmarks: MS COCO 2014 validation dataset [15] and MS COCO 2017 validation dataset [15]. We compared our method against leading flow-based approaches (Rectified Flow, InstaFlow, and PerFlow) as shown in Figure 5. Our method demonstrates superior performance across multiple aspects: it achieves more faithful detail preservation, renders more coherent global structures, and produces sharper textures with fewer artifacts. 6 Figure 5. Qualitative comparison of image generation results. Our method demonstrates superior performance in detail rendering compared to other flow-based approaches at both 2-steps and 4-steps sampling. Specifically, while baseline methods often struggle with detail preservation and suffer from blurry regions or structural distortions, our approach consistently maintains both finegrained details and global coherence across various scenarios. This comprehensive improvement in generation quality validates the effectiveness of our method. 4.4. Training Cost Although our method involves multiple training stages, its computational cost is significantly lower than 2-ReFlow, which applies ReFlow twice along the entire sampling trajectory and consumes 75.2 A100 days without considering 7 Table 3. Comparison results on SDXL on COCO2017 validation set and COCO2014-10k validation set with 4 steps, following the evaluation setup in [40]. Method COCO2017 Perflow Rectified Diffusion ProReflow-SDXL (Ours) COCO2014-10k SDXL-Lightning SDXL-Turbo LCM PCM Perflow Rectified Diffusion DMDv2 ProReflow-SDXL (Ours) Res. Steps FID () 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 4 4 4 4 4 4 4 4 4 4 4 27.06 25.81 25.36 24.56 23.19 22.16 21.04 20.99 19.71 19.32 19.10 data synthesis costs. To obtain ProReflow-II we perform three training stages starting from windows = 8, with each stage trained for 10000 iterations at batch size of 256. Despite the same total number of samples, the training time varies across stages. Following [44], for each batch we randomly sample timestep and determine its corresponding window based on time windows division. The windows start and end points define the velocity prediction target, where starting points are obtained by directly adding noise to real images, and endpoints are generated by the teacher model. Since we maintain total of 32 teacher inference steps across different stages, obtaining velocity targets for batch with windows = 4 requires twice the teacher inference steps compared to windows = 8, which is consistent with the training time ratio between these stages. Under this training framework, ProReflow-I requires only 6.5 H20 days, and ProReflow-II adds an additional 8.7 days, totaling 15.2 H20 days for the complete training pipeline. Moreover according to NVIDIAs official specifications, the BF16 computation capability of H20 (148 TFLOPS) is approximately half of A100 (312 TFLOPS). 5. Discussion 5.1. Ablation Study We conduct ablation studies to examine our two core designs: aligned v-prediction and progressive reflow. Table 4 presents results on COCO-2017 validation set. Both components contribute to model performance, with their combination yielding the best result. Table 4. Ablation studies on COCO-2017 validation set. We first show the results of gradually removing progressive reflow, aligned v-prediction, and both components, followed by our full model. We use guidance scale of 4 for all the models. Method Steps FID () CLIP () w/o progressive reflow w/o aligned v-prediction w/o both ProReflow-I 4 4 4 4 23.46 23.09 23.81 22.97 30.21 30.25 30.24 30.29 5.2. CFG Influence It is well-established that the classifier-free guidance scale is crucial factor affecting the performance of Stable Diffusion. During training, we set = 1 (i.e., without classifier-free guidance) throughout all the stages. To thoroughly understand the models behavior under different guidance settings, we conducted extensive evaluations across broad range of values from 2 to 7, measuring both FID and CLIP score, results are shown in Figure 3. 5.3. Step scalability Intuitively, for diffusion models, higher sampling steps should lead to better performance at the cost of increased inference time. However, this assumption does not always hold in practice. For instance, PeRFlow exhibits an unexpected performance degradation when increasing sampling steps from 4 to 8 on COCO-2014 [44], which limits its practical applications. We surprisingly find our progressive training scheme effectively addresses this limitation. Although ProReflow-II is trained with window size = 2, it achieves superior performance with 4-step sampling compared to ProReflow-I, demonstrating both lower FID, shown in Table 1 and Table 2. 6. Conclusion In this paper, we propose an efficient training framework for flow-based diffusion acceleration. If viewing the optimization process from temporal and spatial dimensions, our method naturally leads to two complementary techniques that correspond to these two dimensions respectively. Temporally, progressive reflow bridges the trajectory approximation gap through curriculum learning, enabling gradual adaptation from more windows to fewer windows. Spatially, our velocity decomposition strategy emphasizes directional alignment over magnitude accuracy in velocity prediction. This principled design not only yields superior sampling quality but also brings advantages in optimization stability, training efficiency, and computational costs. Limitations Given promising few-step sampling performance, our method shows potential for one-step generation. However, due to computational constraints, we were unable to train the model with single window to full convergence. Nevertheless, we have validated the effectiveness of velocity decomposition in this challenging setting with the same training cost, only-one-window model equipped with aligned v-prediction demonstrate superior performance compared to the vanilla counterpart. We plan to move to one-step generation when resources allow."
        },
        {
            "title": "References",
            "content": "[1] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. arXiv preprint arXiv:2406.02347, 2024. 6 [2] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 1 [3] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using In Proceedings of the IEEE/CVF Interdiffusion models. national Conference on Computer Vision, pages 74307440, 2023. 1 [4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [5] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [6] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 3 [7] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [9] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 1 [10] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. [11] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: lightweight, fast, and cheap version of stable diffusion. arXiv preprint arXiv:2305.15798, 2023. 6 [12] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [13] Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based generative models. In International Conference on Machine Learning, pages 18957 18973. PMLR, 2023. 3 [14] Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. arXiv preprint arXiv:2405.20320, 2024. [15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [16] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 2 [17] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, 2, 3 [18] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 6 [19] David Lopez-Paz, Leon Bottou, Bernhard Scholkopf, and Vladimir Vapnik. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643, 2015. 3, 4 [20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 6 [21] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 2, 6 [22] Calvin Luo. Understanding diffusion models: unified perspective. arXiv preprint arXiv:2208.11970, 2022. 2 [23] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 1, 6 [24] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 1 [25] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, pages 51915198, 2020. 3 [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 9 your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 6, [41] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. Kdgan: Knowledge distillation with generative adversarial networks. Advances in neural information processing systems, 31, 2018. 4 [42] Siyu Xing, Jie Cao, Huaibo Huang, Xiao-Yu Zhang, and Ran He. Exploring straighter trajectories of flow matching with diffusion guidance. arXiv preprint arXiv:2311.16507, 2023. 3 [43] Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Accelerating image generation with sub-path linear approximation model. arXiv preprint arXiv:2404.13903, 2024. 3 [44] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. 3, 6, 8 [45] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 2 [46] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 6 [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [30] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 3 [31] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22872296, 2021. 2 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2 [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [34] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. 1, 2, 3 [35] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. [36] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 6 and Stefano Ermon. arXiv preprint [37] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [38] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 1, [39] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. 3, 6 Phased consistency model. [40] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not 10 ProReflow: Progressive Reflow with Decomposed Velocity"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Velocity Gap in Long-Range Timesteps Algorithm 3: Progressive ReFlow # K: window numbers [8,4,2] # D: training dataset # t: normalized time in [0,1] # Progressive window refinement for windows in K: # Training in current stage while not converged: # Get endpoints of time window = sample_time() # in [0,1] t1, t2 = get_window_bounds(t) # Compute trajectory endpoints z1 = add_noise(x0, t1) z2 = teacher_solve(z1, t1, t2) # Linear interpolation zt = interpolate(z1, z2, t) v_target = (z2 - z1)/(t2 - t1) # Update student model v_pred = student(zt, t) loss = velocity_loss(v_pred, (cid:44) v_target) update_params() 9. ProReflow Implementary Details We have presented the pseudocode of ProReflow in Algorithm 1 in the main text. Here we elaborate on its two core components: Progressive ReFlow, which performs stagewise training with decreasing window numbers [8,4,2], and the velocity decomposition loss which enhances directional alignment by incorporating cosine similarity alongside the standard MSE loss. The implementations are detailed in Algorithm 3 and 2, respectively. As Fig.1 (a) of the main paper shown, to validate the velocity discrepancy in pretrained diffusion models, we conducted experiments using the Stable Diffusion v1.5 model. The velocity at each timestep is computed as: vt = 1000 (xt+1 xt) (15) where xt represents the latent at timestep t. We sample 100 different prompts and average their velocity matrices to obtain reliable statistics. For each pair of timesteps and j, we compute both L2 distance Vi Vj2 and cosine similarity cos(Vi, Vj) between their velocities in the 46464 latent space. All experiments use the PNDM scheduler with 1000 inference steps. 8. Add noise to direction or magnitude To analyze the relative importance of velocity direction versus magnitude in the flow model, we conduct experiments using the 2-Rectified model with 10 inference steps on COCO-5K validation set. For each velocity vector v, we decompose it into direction and magnitude components: = d, where = 1. For magnitude noise, we first add Gaussian noise to directly. Then, to ensure comparable perturbations for direction noise, we employ binary search to find an appropriate noise scale that yields the same L2 distance from the original velocity field as the magnitude noise. The directional noise is added to and then normalized to maintain unit length. This controlled noise injection mechanism enables fair comparison between directional and magnitude perturbations, with results shown in Fig.1 (b) of the main paper. Algorithm 2: Velocity Decomposition # Add directional constraint to standard (cid:44) MSE loss def velocity_loss(v_pred, v_target): # Standard MSE loss l_mse = mse_loss(v_pred, v_target) # Additional directional constraint l_dir = 1 - cos_similarity(v_pred, (cid:44) v_target) # Weight between MSE and directional (cid:44) loss return (1-alpha)*l_mse + alpha*l_dir"
        }
    ],
    "affiliations": [
        "Huawei Inc.",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University of Electronic Science and Technology of China"
    ]
}