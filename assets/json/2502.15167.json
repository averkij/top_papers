{
    "paper_title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
    "authors": [
        "Chuan Cui",
        "Kejiang Chen",
        "Zhihua Wei",
        "Wen Shen",
        "Weiming Zhang",
        "Nenghai Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment Chuan Cui, Kejiang Chen, Zhihua Wei, Wen Shen, Weiming Zhang, and Nenghai Yu 5 2 0 2 1 ] . [ 1 7 6 1 5 1 . 2 0 5 2 : r AbstractThe rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, predictor constructed by an xLSTM and regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves stateof-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github. com/strawhatboy/M3-AGIQA. Index TermsImage Quality Assessment, AI-Generated Images, Multimodal Large Language Models I. INTRODUCTION recent years, flourishing of Artificial Intelligence Generated Content (AIGC) has sparked significant advancements in modalities such as text, image, audio, and even video. Among these, AI-Generated Image (AGI) has garnered considerable interest from both researchers and the public. Plenty of remarkable AGI models and online services, such as StableDiffusion1, Midjourney2, and FLUX3, offer users an excellent creative experience. However, users often remain critical of the quality of the AGI due to image distortions or mismatches with user intentions. Consequently, methods for assessing the quality of AGI are becoming increasingly crucial to help improve the generative capabilities of these models. Work is done by the first author during intership at Hefei High-Dimensional Data Technology Co.,Ltd. (Corresponding author: Zhihua Wei.) Chuan Cui, Zhihua Wei, Wen (cuichuan@tongji.edu.cn; zhihua wei@tongji.edu.cn; wenshen@tongji.edu.cn) School and Technology, Tongji University, Shanghai, of Computer Science China; Kejiang Chen, Weiming Zhang, Nenghai Yu (chenkj@ustc.edu.cn; zhangwm@ustc.edu.cn; ynh@ustc.edu.cn) are with the Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation. are with Shen 1https://stability.ai/ 2https://www.midjourney.com/ 3https://blackforestlabs.ai/ Fig. 1. comparison on quality, correspondence, and authenticity aspects of AIGCIQA2023 [1] dataset illustrates the superior performance of our method. Unlike Natural Scene Image (NSI) quality assessment, which focuses primarily on perception aspects such as sharpness, color, and brightness, AI-Generated Image Quality Assessment (AGIQA) encompasses additional aspects like correspondence and authenticity. Since AGI is generated on the basis of user text prompts, it may fail to capture key user intentions, resulting in misalignment with the prompt. Furthermore, authenticity refers to how closely the generated image resembles real-world artworks, as AGI can sometimes exhibit logical inconsistencies. While traditional IQA models may effectively evaluate perceptual quality, they are often less capable of adequately assessing aspects such as correspondence and authenticity. Several methods have been proposed specifically for the AGIQA task, including metrics designed to evaluate the authenticity and diversity of generated images [2], [3]. Nevertheless, these methods tend to compare and evaluate grouped images rather than single instances, which limits their utility for single image assessment. Beginning with AGIQA-1k [4], series of AGIQA databases have been introduced, including AGIQA-3k [5], AIGCIQA-20k [6], etc. Concurrently, there has been surge in research utilizing deep learning methods [7][9], which have significantly benefited from pretrained models such as CLIP [10]. These approaches enhance the analysis by leveraging the correlations between images and their descriptive texts. While these models are effective in capturing general text-image alignments, they may not effectively detect subtle inconsistencies or mismatches between the generated image content and the detailed nuances of the 2 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 textual description. Moreover, as these models are pre-trained on large-scale datasets for broad tasks, they might not fully exploit the textual information pertinent to the specific context task-specific fine-tuning. To overcome of AGIQA without these limitations, methods that leverage Multimodal Large Language Models (MLLMs) [11], [12] have been proposed. These methods aim to fully exploit the synergies of image captioning and textual analysis for AGIQA. Although they benefit from advanced prompt understanding, instruction following, and generation capabilities, they often do not utilize MLLMs as encoders capable of producing sequence of logits that integrate both image and text context. In conclusion, the field of AI-Generated Image Quality Assessment (AGIQA) continues to face significant challenges: (1) Developing comprehensive methods to assess AGIs from multiple dimensions, including quality, correspondence, and authenticity; (2) Enhancing assessment techniques to more accurately reflect human perception and the nuanced intentions embedded within prompts; (3) Optimizing the use of Multimodal Large Language Models (MLLMs) to fully exploit their multimodal encoding capabilities. To address these challenges, we propose novel method M3-AGIQA (Multimodal, Multi-Round, Multi-Aspect AIGenerated Image Quality Assessment) which leverages MLLMs as both image and text encoders. This approach incorporates an additional network to align human perception and intentions, aiming to enhance assessment accuracy. Specially, we distill the rich image captioning capability from online MLLMs into local MLLM through Low-Rank Adaption (LoRA) fine-tuning, and train this model with human-labeled data. The key contributions of this paper are as follows: We propose novel AGIQA method that distills multiaspect image captioning capabilities to enable comprehensive evaluation. Specifically, we use an online MLLM service to generate aspect-specific image descriptions and fine-tune local MLLM with these descriptions in structured two-round conversational format. We investigate the encoding potential of MLLMs to better align with human perceptual judgments and intentions, uncovering previously underestimated capabilities of MLLMs in the AGIQA domain. To leverage sequential information, we append an xLSTM feature extractor and regression head to the encoding output. Extensive experiments across multiple datasets demonstrate that our method achieves superior performance, setting new state-of-the-art (SOTA) benchmark in AGIQA. In this work, we present related works in Sec. II, followed by the details of our M3-AGIQA method in Sec. III. Sec. IV outlines our experimental design and presents the results. Sec. V, VI and VII discuss the limitations, ethical concerns, future directions and conclusions of our study. II. RELATED WORK A. Blind Image Quality Assessment For NSIs, numerous studies over the past few decades have focused on IQA task using various approaches. Depending on whether referenced image is used during assessment, these Fig. 2. Different implementations of IQA and AGIQA Methods. methods can be classified into Full-Reference (FR) and NoReference (NR) IQA, with NR IQA being more challenging and often referred to as Blind IQA (BIQA). Traditional BIQA methods, such as NIQE [13], utilize spatial Natural Scene Statistics (NSS) from statistical perspective. Subsequently, similar works have been developed [14][17] following NIQE. significant advancement over these handcrafted featurebased methods is the adoption of deep learning, which has demonstrated superior performance in BIQA tasks [18][25]. For example, DBCNN [20] employs two Convolutional Neural Networks (CNNs) to address both synthetic and authentic distortions. HyperIQA [26] introduces self-adaptive hyper network that addresses real-world variations through threestage process: content understanding, perception rule learning, and quality prediction. StairIQA [23] integrates information hierarchically from different stages of ResNet [27] backbone with staircase structure. As vision-language pre-trained models like CLIP [10], BLIP [28], and ViT [29] gain popularity, new methods have emerged that leverage these models to evaluate image quality by assessing text-image similarity. CLIPIQA [30] calculates the similarity between an image and two quality-implied prompts to align with human-labeled scores. Similarly, LIQE [31] introduces multitask learning approach with an expanded set of textual templates and utilizes CLIP to evaluate vision-text similarities. TempQT [32] leverages Transformer [33] encoder and decoder to pre-train the error map and then feature fusion is done byy vision Transformer to predict the quality score. B. AI-Generated Image Quality Assessment With advancements in text-to-image synthesis, assessing the quality of AGIs has become increasingly important to align with the human visual system (HVS) and the intent implied in the generation prompt. HPS [34] and PickScore [35] train CLIP-based function to predict user preferences by selecting the most preferred one from group or pair of CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT 3 AGIs. ImageReward [36] developed the first text-to-image reward model based on BLIP [28], using dataset of 137k prompt-image pair rankings sampled from DiffusionDB [37], which contains vast collection of prompt-image pairs without scores. Some research pays attention specifically on artistry aspect such as ArtScore [38], which could be imcomplete for AGIQA task. Then, several datasets using Mean Opinion Score (MOS) as the evaluation target which presents with human perference well, are introduced to support AGIQA task: AGIQA1k [4], AGIQA-3k [5], AIGCIQA2023 [1], AIGCIQA-20k [6], and I2IQA [39]. Based on these datasets, more methods that perform much better have emerged. AMFF-Net [7] evaluates AGIs from quality, correspondence and authenticity aspects by scaling images up and down to capture the global and local features, then utilizes CLIP as the text and image encoders. During the NTIRE2024 Quality Assessment of AI-Generated Content Challenge [40], further advancements were made. Inspired by LIQE [31], IPCE [8] utilizes CLIP to encode quality-aware prompts that include both the quality label and the image prompt, achieving first place in the challenge. SFIQA [9], employing multilayer feature extractor and fusion module, excels in identifying local and global quality-aware features. Moreover, MoE-AGIQA [41] sets new benchmarks by integrating visual degradation-aware and semantic-aware networks with mixture-of-experts module. While these methods effectively leverage pre-trained visionlanguage models, they do not fully exploit the potential of textual analysis, which would result in inferior performance compared with larger scale models. C. Multimodal Large Language Models IQA Despite great advancements in models like CLIP, which can align images with text, recent approaches have started to explore the potential of Multimodal Large Language Models (MLLMs) for the IQA task. MLLMs inherit strong reasoning and instruction-following capabilities of Large Language Models (LLMs) and can serve as powerful image evaluators when given appropriate prompts. TIFA [42] introduces metric to evaluate the faithfulness of text-to-image generation by using LLMs to generate relevant questions for existing Visual Question Answering (VQA) methods. LLMScore [43] utilizes multi-granularity compositionality capture to evaluate the correspondence between the image and the text prompt by leveraging LLMs as image descriptor and evaluator. Additionally, Q-Bench [44] proposes systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs, employing simple softmax pooling strategy to quantitatively assess the text-image correspondence. QInstruct [45] builds upon the same Softmax pooling strategy by constructing 200k instruction-response pairs related to lowlevel visual attributes. Q-Align [46] teaches MLLM to judge based on discrete text levels by converting human rating scores into text, and demonstrates an advantage above score-level variants. While these methods address general IQA tasks, specialized approaches for AGIQA are also emerging. MAAGIQA [11] integrates semantically informed guidance with quality-aware features through Mixture of Experts (MoEs) structure, leveraging both MLLM and traditional DNN approaches for superior performance. MINT-IQA [12] extends AIGCIQA2023 [1] database to AIGCIQA2023+ by adding image quality descriptions and applying instruction tuning to MLLMs, resulting in considerable improvements in assessing human visual preferences from multiple perspectives. Although these methods utilize MLLM abilities like prompt understanding, instruction following and text generation, they often overlook the potential encoding capability of MLLMs. D. Long Short-Term Memory Network idea [47] was developed to address of Long Short-Term Memory Network The (LSTM) the gradient vanishing problem in traditional Recurrent Neural Networks (RNNs). Despite its effectiveness, LSTM faces challenges in handling long sequence and lacks parallelizability, which can be critical in contemporary deep learning tasks. The extended LSTM (xLSTM) [48] introduces additional exponential gating and modified memory structures, making it competitive with advanced architectures like Transformers and State Space Models (SSMs). VisionLSTM [49] (ViL) represents further evolution, incorporating stack of alternating mLSTM blocks inputs like designed to efficiently process nonsequential image patches. Our method employs xLSTM blocks with linear computational and memory complexity as sequential data processing units, which boosts the performance. III. METHODOLOGY Our work introduces novel method to address the AGIQA task, which seeks to establish function that accepts an AGI and its corresponding text prompt as inputs, and outputs predicted MOS: ˆy = (i, p) . (1) Specifically, for datasets that provide MOSs across various aspects, we train distinct functions tailored to each aspect. These functions fq, fc, and fa, are dedicated to assessing quality, correspondence, and alignment, respectively. This specialization allows for more nuanced evaluation of AGIs, aligning closely with human perceptual judgments. As illustrated in Fig. 3, M3-AGIQA comprises three stages during training. A. Initial Description Stage The initial description stage generates high-quality descriptions for quality, correspondence, and authenticity aspects of AI-Generated Images (AGIs), forming the foundation for downstream training. Distillation from online Multimodal Large Language Models (MLLMs), which excel in image understanding [50], transfers their capabilities to an opensource MLLM, reducing cost and latency while maintaining performance. In the first stage of our method, we use the online MLLM to generate descriptions of AGIs from three aspects: quality, correspondence, and authenticity. To achieve this, we manually create prompt that is then refined by GPT-4, and the prompt embeds both the MOS and the specific text prompt of the AGI. The dual embedding guides the MLLM in producing 4 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 3. Overview of the M3-AGIQA Framework. This diagram illustrates the three-stage process using the quality aspect of AGIs as an example. (a) Initial Description Stage: By using an online MLLM service via API, we obtain descriptions related to quality, correspondence, and authenticity for each image. These descriptions are then used to fine-tune an open-source MLLM, enhancing its understanding of these aspects. (b) Inference Stage: Distilled captioning capabilities of the fine-tuned MLLM was leveraged coupled with the textual prompt to generate the required response. This step utilizes the refined ability of the MLLM to interpret and articulate the nuances of the input prompt and image. (c) Training Stage: We freeze all the parameters within the fine-tuned MLLM to ensure stability and reproducibility. xLSTM feature extractor along with regression head is then trained to predict the MOS, effectively translating enhanced perceptual and textual understanding of the MLLM into quantifiable assessments. descriptions that not only resonate with human perceptions but also exploit the object detection capabilities of the model to enhance text-image alignment judgments.4 Then, by an automatic script, we call the API of the online MLLM to generate descriptions of the AGIs from datasets: Di = MLLMonline.generate (i, Tdesc (p, y)) , Di = (cid:8)Di q, Di c, Di (cid:9) , (2) (3) where Di is the description which consists of descriptions on three aspects Di for AGI i, MLLMonline is the online MLLM we use, and Tdesc is the prompt template which accepts the text prompt and MOS of the AGI as inputs. q, Di c, Di After generating descriptions for the AGIs, we initiate two-round conversational interaction between user and an assistant with new prompt template, as depicted in Fig. 3 (a). This template incorporates Likert-scale of fivelevel descriptors [bad, poor, fair, good, excellent] as part of the prompt construction, following methodologies similar to those of previous works [31], [46], [51], [52]. Then, the conversations and their corresponding AGIs are divided into training and validation sets. These sets are then fed into local open-source MLLM for training with LoRA fine-tuning technique on the vision encoder only inside the MLLM as illustrated in Fig. 4. Initially, the AGI is resized and segmented into patches before being sent to the Vision Encoder, after which the encoded data for each patch are projected into visual tokens. These visual tokens are combined with text tokens and input into the LLM. Inside the LLM, all the tokens are processed, and the linear layer at the end outputs the logits of the sequence, which can be subsequently decoded into text. Thus, we can obtain the fine-tuned MLLMft which will accept AGI and text prompt as inputs. 4Details of the prompts, templates and specific cases examined in this study will be elaborated upon in the supplementary materials Fig. 4. LoRA fine-tuning process of the local MLLM. The MLLM processes both the image and conversation text as inputs. The embedded text and image are forwarded through the internal LLM (Llama3). The LoRA parameters on the Visual Encoder are optimized by minimizing the cross-entropy loss between the output logits and the input conversation tokens. In both the inference and training stages, the parameters of the Visual Encoder are frozen. B. Inference Stage In the inference stage, we harness the zero-shot Chain of Thought (CoT) capabilities of LLMs [53] to generate intermediate reasoning processes that refine the final conclusions. As illustrated in Fig. 3 (b), this stage enhances the accuracy of assessments by leveraging the generated descriptions to judge the quality of AGIs, while the two-round conversation process augments the data for subsequent training. The fine-tuned local open-source MLLM in Sec. III-A produces meaningful logits during this process, representing the distribution of the AGI and text across vocabulary probabilities, which are crucial for accurately predicting the MOS. Skipping this step results in inadequate understanding in image assessment and disrupts the connection between distilled knowledge and the final score prediction. The process of generating description Di can be CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT 5 simplified as follows: Di = MLLMft.generate (i, p) , (4) where MLLMft is the fine-tuned model. C. Training Stage In order to align the output of the MLLM with the MOS which is human perspective assessment of AGIs, an additional step involving well-designed predictor network was introduced, which led to our method functioning as an encoder for both image and text modalities, inspired by [54]. As such, we utilize the generated conversation i, which includes the generated description Di from the previous inference stage III-B along with the corresponding AGI as inputs to encode them as follows: Ei = MLLMft.forward (cid:0)i, i(cid:1) , (5) where Ei RLdvocab represents the output logits, with denoting the sequence length of the input conversation and dvocab representing the vocabulary size of MLLM tokenizer. The last element Ei[1] of the output logits, typically represents the probability of the next token, which is possibly valuable for predicting the target. However, as [55] mentioned, the next first-token probability might lead to mismatch with the expected answer. This observation is also supported by our experiments IV-C3. Therefore, we take advantage of the entire sequence Ei. Given its sequential nature, we employ an xLSTM feature extractor followed by mean pooling strategy on the sequence dimension for better alignment: Ei proj = WprojEi, Ei out = xLSTMHead (cid:16) (cid:17) , Ei proj zi = MeanPooling (cid:0)Ei out (cid:1) , (6) (7) (8) where Wproj Rdvocabd represents the learnable linear layer that projects from the vocabulary space to the xLSTM hidden space of dimension d. This projection facilitates the integration of linguistic information into the sequential processing framework of xLSTM, enabling effective feature extraction and representation learning for subsequent tasks. Then, regression head composed of linear layers is used to obtain the floating-point prediction score for each instance: ˆyi = RegressionHead (cid:0)zi(cid:1) . (9) To optimize the model, Mean Square Error (MSE) loss is employed for backpropagation through the trainable parameters in the xLSTM blocks and regression head: = 1 (cid:88) i=1 (cid:0)ˆyi yi(cid:1) , (10) where yi is the ground truth MOS. TABLE STATISTICS OF THE DATASETS. Statistics AGIQA-3k AIGCIQA2023 AIGCIQA-20k No. of images No. of T2I models quality MOS correspondence MOS authenticity MOS 2,982 6 2,400 6 20,000 15 IV. EXPERIMENTS This section presents extensive experiments conducted to evaluate our proposed method M3-AGIQA in comparison with other SOTA models. Our experimental design is structured to address the following key research questions: RQ1: How does M3-AGIQA compare in performance with current SOTA methods? RQ2: What is the contribution of each component within M3-AGIQA to its overall performance? RQ3: Does the distillation process enhance the overall performance of the model? RQ4: How effective is using an MLLM as an encoder within our framework? RQ5: How does the fine-tuned model perform across different datasets? A. Setup 1) Datasets: As summarized in Table I, we utilize three public AGIQA datasets for our experiments, including AGIQA-3k [5], AIGCIQA2023 [1], and AIGCIQA-20k [6]. Each dataset is widely recognized in the field and provides Mean Opinion Scores (MOS) that assess quality, correspondence, and authenticity aspects either fully or partially. AGIQA-3k [5] includes 2982 images generated by 6 different models which are GAN, auto-regression, and diffusion-based models. It annotates MOSs for image perception quality and correspondence with the prompt. AIGCIQA2023 [1] is composed of 2400 images from 6 cutting-edge models with MOSs for three aspects including quality, correspondence, and authenticity. Each prompt generates 4 images for one model. AIGCIQA-20k [6] from the NTIRE 2024 Quality Assessment Challenge has 20, 000 images generated by 15 popular models, along with MOSs collected from 21 subjects. 2) Baselines: To demonstrate the effectiveness of our proposed method M3-AGIQA, we select several baselines for comparative analysis: (1) Simple Vision Encoders with Regression Heads: ResNet50 [27], ViT/B/16 [29], and ViL [49]. We integrate two-layer MLP as the regression the MOS; (2) Established IQA head to directly predict methods: DBCNN [20], HyperIQA [26], StairIQA [23], and MGQA [56]; (3) AGIQA methods: AMFF-Net [7], MAAGIQA [11], IPCE [8], and SF-IQA [9]. Since these models have not been open-sourced, we obtained their results directly from the respective published papers. 3) Implementation Details: The MLLM used in the experiments is openbmb/MiniCPM-Llama3-V-2 5 [57], lightweight GPT-4V level MLLM. To generate image descriptions for distillation, we utilize the free online MLLM Google gemini-1.5pro API as the teacher model. The distillation was conducted 6 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE II COMPARISON RESULTS ON AGIQA-3K [5], AIGCIQA2023 [1], AND AIGCIQA-20K [6] AMONG DIFFERENT METHODS, RESULTS OF METHODS WITH ASTERISK SYMBOL ARE DIRECTLY RETRIEVED FROM CORRESPONDING PAPERS. BOLD AND UNDERLINED VALUES INDICATE THE BEST AND SECOND-BEST RESULTS, RESPECTIVELY. Methods Qual. Corr. Qual. AGIQA-3k AIGCIQA2023 Corr. Auth. AIGCIQA-20k Qual. SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC VGG16 [58] ResNet50 [27] ViT/B/16 [29] ViL [49] DBCNN [20] StairIQA [23] MGQA [56] HyperIQA [26] AMFF-Net [7] MA-AGIQA [11] IPCE [8] SF-IQA [9] 0.8167 0.8552 0.8659 0.8750 0.8154 0.8439 0.8283 0.8526 0.8565 0.8939 0.8841 0. 0.8752 0.9072 0.9115 0.9145 0.8747 0.8989 0.8944 0.8975 0.9050 0.9273 0.9246 0.9314 0.6867 0.7493 0.7478 0.7570 0.6329 0.7345 0.7244 0.7437 0.7513 - 0.7697 0. 0.8108 0.8564 0.8449 0.8561 0.7823 0.8474 0.8430 0.8471 0.8476 - 0.8725 0.9072 0.8157 0.8190 0.8370 0.8436 0.8339 0.8264 0.8093 0.8357 0.8409 - 0.8640 - 0.8282 0.8503 0.8618 0.8770 0.8577 0.8483 0.8308 0.8504 0.8537 - 0.8788 - 0.6839 0.7230 0.7293 0.7174 0.6837 0.7176 0.6892 0.7318 0.7782 - 0.7979 - 0.6853 0.7270 0.7439 0.7296 0.6787 0.7133 0.6745 0.7222 0.7638 - 0.7887 - 0.7399 0.7571 0.7783 0.7753 0.7485 0.7596 0.7367 0.7758 0.7749 - 0.8097 - 0.7465 0.7563 0.7697 0.7770 0.7436 0.7514 0.7310 0.7790 0.7643 - 0.7998 - 0.8133 0.8036 0.8407 0.8459 0.7941 0.8126 0.8107 0.8171 - 0.8644 0.9076 0. 0.8660 0.8661 0.8904 0.8852 0.8542 0.8746 0.8534 0.8584 - 0.9050 0.9274 0.9268 M3-AGIQA (Ours) 0.9045 0. 0.8523 0.9142 0.8618 0.8922 0.8060 0. 0.8282 0.8165 0.8988 0.9292 via an official fine-tuning script5. The xLSTM head in our model is made up of four layers, including one sLSTM layer positioned at layer 2 and three mLSTM layers at positions 1, 3, and 4. The regression head is structured with two linear layers. Our experiments employed PyTorch and TorchLightning 2.3.06 to implement the training process. The datasets were partitioned based on practices established in previous studies: 4:1:0 for AGIQA-3k, 3:1:0 for AIGCIQA2023, and 7:2:1 for AGIQA-20k, in terms of training, test, and validation sets respectively. During fine-tuning, the learning rate was set to 2e-6, and the batch size was fixed at 2. The vision encoder was fine-tuned using the LoRA technique while the LLM parameters remained frozen. In addition, deepspeed ZeRO3 offload was employed to minimize GPU VRAM usage. The fine-tuning process ranged from 2, 000 to 4, 000 steps for AGIQA-3k and AIGCIQA2023, and around 20, 000 steps for AIGCIQA-20k. In the training stage, AdamW was utilized as the optimizer. The hidden dimension size was set to 512 and the vocabulary size of the MLLM tokenizer dvocab was 128256. All experiments were conducted using an NVIDIA A100-PCIE-40GB GPU and an Intel Xeon Gold 6248R CPU. 4) Metrics: To evaluate the performance of our method, we utilized two widely-used metrics in IQA tasks: Spearmans Rank-Order Correlation Coefficient (SRCC) and Pearsons Linear Correlation Coefficient (PLCC). SRCC measures ability of the model to preserve the rank order of the predictions relative to the ground truth, indicating its effectiveness in ranking images based on quality. PLCC evaluates the linear correlation between the predicted and actual scores, representing how the model fits the data. Both metrics range from 1 to 1, with higher values indicating better performance. B. Comparison with SOTAs - RQ Experiments on the datasets have shown our proposed method M3-AGIQA significantly outperforms the counter5https://github.com/OpenBMB/MiniCPM-V 6https://lightning.ai/docs/pytorch/2.3.0/ parts. As depicted in Table II, simple vision encoders perform well on AGIQA task, ViL [49] which utilizes the latest xLSTM [48] architecture shows strong advancement beyond the traditional ResNet50 [27]. With respect to the traditional BIQA methods, VGG [58] based DBCNN [20] is simple and fast but not competitive due to lack of quality related feature extraction ability; ResNet50 [27] based StairIQA [23], MGQA [56], and HyperIQA [26] do not improve much or are even inferior to their backbone, this could occur when they are designed specifically from the perspective of NSIs quality but not AGIs, which would ignore the impact from correspondence and authenticity aspects. AGIQA methods perform significantly better than the vision encoder and BIQA methods do, especially in terms of correspondence and authenticity apsects. As an example on AIGCIQA2023 [1] dataset, IPCE [8] which combines quality aware text with the AGI as input of CLIP [10] model, achieves overall superior results to those of non-AGIQA methods, not only because of the extraordinary ability of CLIP model to align with text and image, but also because of the meaningful text adoption on the three aspects. Notably, IPCE [8] performs slightly worse than its counterpart SF-IQA [9] on AGIQA-3k [5] dataset. But in NTIRE2024 challenge, it beats SF-IQA [9] on AIGCIQA-20k dataset. The reason could be IPCE [8] utilizes techniques like model integration which is common in competitions. For SF-IQA [9], SwinTransformer is employed as feature extractor, CLIP is used for text-image alignment, and Transformer encoder is used as fusion module, better results are achieved on AGIQA-3k [5] dataset. In contrast to previously discussed methods, our approach demonstrates superior performance across most metrics. The scatter plot in Fig 5 illustrates that M3-AGIQA aligns closely with the data, indicating strong fit. However, M3-AGIQA underperforms compared with NTIRE2024 champion IPCE [8] on AIGCIQA-20k [6] dataset, this discrepancy may be attributed to the optimization specially for the competitions settings of IPCE [8]. Our result can be in the fourth position in CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT Fig. 5. Scatter plot of predicted MOS vs. ground truth MOS on the quality aspect of AGIQA-3k [5] dataset. The x-axis represents the ground truth MOS, while the y-axis shows the predicted MOS. The fitted curves are obtained using fourth-order polynomial regression. the competition and has the best PLCC score, which indicates M3-AGIQA performs well in fitting the data but may lack some ranking capability. TABLE IV ABLATION STUDIES ON AGIQA-3K [5] QUALITY ASPECT. Model settings SRCC PLCC C. Ablation Studies - RQ2, RQ3, and RQ4 To determine the contributions of each component to the overall performance of our method, experiments were conducted at every stage of the process. 1) Image Descriptions: To demonstrate the importance of inferred image descriptions in terms of quality, correspondence, and authenticity, and their roles as contextual inputs for enhancing deep understanding of the AGI, we assessed their impact specifically on the quality aspect of dataset AGIQA3k [5], with results detailed in Table III. The column Present Descriptions denotes the various combinations of generated image descriptions used throughout the experimental stages. The first line, which lacks any descriptions, depicts scenario where the model directly predicts the outcome without any contextual input. This configuration suggests we only finetuned the local MLLM with single-round conversation, excluding any detailed responses from the MLLM, and can be interpreted as w/o CoT. In other setups, the MLLM was prompted to describe the image across different aspects, thereby enriching the context for the prediction task. TABLE III COMPARISON RESULTS ON AGIQA-3K [5] QUALITY ASPECT WITH DIFFERENT COMBINATIONS OF IMAGE DESCRIPTION ASPECTS. Present Descriptions Corr. Auth. Qual. Qual. SRCC PLCC 0.8816 0.8989 0.9045 0.8999 0.9193 0.9342 0.9317 0.9321 The results reveal that the model trained without any intermediate contextual descriptions has the weakest performance, w/o ft, w/o ID w/o ft, w/ ID w/o ft, w/ FullConv w/ ft, w/o ID w/ ft, w/ FullConv w/o xLSTM w/o lm header w/ last token w/ max pooling w/ mean of the first and last token w/ LSTM w/ GRU w/ Mamba2 w/ Transformer 0.8886 0.8800 0.8720 0.8921 0.8968 0.8959 0.8949 0.8795 0.8884 0.8953 0.8944 0.8928 0.8961 0. 0.9208 0.9030 0.8939 0.9249 0.9278 0.9274 0.9325 0.9245 0.9236 0.9286 0.9239 0.9065 0.9283 0.9291 M3-AGIQA (Ours) 0. 0.9317 suggesting that leveraging the innate capabilities of the original MLLM is insufficient. In contrast, adding descriptions that cover both quality and correspondence aspects leads to the best performance, indicating the correspondence aspect likely contributes to human-like perspective enhancements that positively influence quality judgments. However, including additional aspects such as authenticity slightly degrades performance. This might be because authenticity has less direct relationship with image quality aspect, and its inclusion introduces noise that could detract from the effectiveness of the model in assessing quality. 2) Distillation & Fine-tuning: Additional experiments on AGIQA-3k [5] quality aspect in Table IV were conducted to validate the effectiveness of distilling image description capabilities from an online MLLM to local MLLM. We included the inference stage but maintained the absence of the initial description stage (w/o ft, w/ ID), which resulted in diminished performance. 8 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 After inferencing the result, we approached the AGIQA task as classification problem, assigning labels [bad, poor, fair, good, excellent] within dataset AGIQA-3k [5]. Classification labels are segmented according to ranges of MOS, for example scores from [0-1], [1-2], and [2-3] correspond to bad, poor, and fair, respectively. Thus, an image with target label score 1.99 but predicted as 2.01 would be classified as fair instead of poor. Given these nuances, adopting less stringent classification metric termed Rough Accuracy is beneficial: Rough Acc. = (cid:80)N i=1 1 (cid:16)(cid:12) (cid:12) (cid:12) ˆyi yi(cid:12) (cid:17) (cid:12) (cid:12) 1 , (11) where 1() is the indicator function, ˆyi and yi are the integers ranging from 0 to 4 representing the five quality categories, and is the total number of samples evaluated. As detailed in Table V, the overall performance is quite favorable, demonstrating the effectiveness of fine-tuning. TABLE CLASSIFICATION TASK ON AGIQA-3K [5] QUALITY ASPECT. Model settings Rough Acc. Acc. Precision F1 w/o ft, w/ ID w/ ft, w/ ID 0.5427 0.9916 0.1005 0. 0.2615 0.6462 0.1018 0.6534 3) MLLM as an Encoder: The LLM inside the MLLM utilizes linear head to project hidden representations to the vocabulary dimension, producing the output logits. We explored the impact of removing this projection head (w/o lm head), the inferior result in Table IV demonstrates the vocabulary distribution learned during distillation remains indispensable. Furthermore, removing the xLSTM head (w/o xLSTM) led to decreased performance, underscoring the critical role of xLSTM in enhancing sequential feature extraction. Additionally, we conducted experiments on the pooling methods used after the xLSTM head, as detailed in Table IV. Using only the last token (w/ last token) for pooling resulted in poor performance, indicating insufficient contextual information was captured. Implementing max pooling (w/ max pooling) led to even worse outcomes, suggesting significant loss of context. Alternatively, using mean of the first and last tokens (w/ mean of the first and last tokens) also proved inferior to our method with mean pooling, which was found to best preserve rich contextual data. feature extractor, We also evaluated different architectures as the sequential including LSTM [47], GRU [59], Mamba2 [60], and Transformer [33]. Our findings indicate that configurations with Mamba2 (w/ Mamba2) and Transformer (w/ Transformer) outperform those with LSTM (w/ LSTM) and GRU (w/ GRU), which makes sense since Mamba2 and Transformers are somehow equivalent and have been proven to be better than previous sequential models. However, our implementation with an xLSTM header advances this further benefiting from specially designed modules within xLSTM that enhance its competitive edge. 4) Round of Conversations: Additionally, the influence of conversation rounds on model performance was investigated, building on the settings discussed in Section IV-C1, which considers the content of context by various aspects. Initially, we tested configuration without fine-tuning the local MLLM with an online MLLM (w/o ft) and without image descriptions (w/o ID), relying solely on the local MLLM with simple prompt requesting quality results within categories [bad, poor, fair, good, excellent], this configuration unsurprisingly yielded the poorest performance. Then with the image description (w/o ft, w/ ID) performs worse, even when incorporating full conversation including the generated final quality result (w/o ft, w/ FullConv), performance decreases further. This is likely because the original local MLLM did not align well with human perspective evaluation, leading to increased bias with more descriptive context. By distillation with online MLLM but without image descriptions (w/ ft, w/o ID), the model outperformed the configuration without fine-tuning, indicating the strong image description capabilities related to the three aspects from online MLLM can be effectively transferred to the local MLLM. Furthermore, we tested the configuration to infer the classification result (w/ ft, w/ FullConv), the performance decline could be caused by the determinate results potentially misleading the subsequent training step. Our M3-AGIQA implementation (w/ ft, w/ ID), which leverages conversations including an initial prompt from user, response from MLLM, and another user prompt for asking for the result, further emphasizes the crucial role of image descriptions in achieving the results outlined in Sec. IV-C1. D. Cross-dataset Validation - RQ5 In order to demonstrate the generalization ability of our method, we further conduct cross-dataset validation experiments. Following the previous settings, we focused solely on the quality aspect. Specifically, we leveraged the fine-tuned local MLLM from one dataset to generate intermediate image quality descriptions for another dataset B. The trained model checkpoint from was applied to predict outcomes on using these descriptions. As shown in Table VI, the results indicate that our method has strong generalization the perability compared with its counterparts. However, formance transition from AGIQA-3k to AIGCIQA-20k was only average, suggesting that our method may overfit on smaller datasets and struggle to generalize to much larger datasets. This limitation could be attributed to the predictor comprising the xLSTM feature extractor and regression head, which was trained on one dataset but exhibited diminished generalization ability when applied to substantially different dataset. Also, we tried with additionally training the predictor (w/ TP), the performance was close to that of fully trained on that dataset, which proves the effectiveness of the predictor. V. LIMITATIONS Our work makes use of LoRA fine-tuning on 8B parameter local MLLM, which necessitates significant computational resources and poses common shortage in LLMbased applications. Additionally, we relied on an online API for distillation, introducing extra costs when initiating projects CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT 9 TABLE VI CROSS-DATASET VALIDATION ON QUALITY ASPECT, RESULTS OF METHODS WITH ASTERISK SYMBOL ARE DIRECTLY RETRIEVED FROM CORRESPONDING PAPERS. Models VGG16 ResNet50 ViT/B/16 ViL DBCNN StairIQA MGQA HyperIQA AMFF-Net MA-AGIQA M3-AGIQA (Ours) M3-AGIQA (w/ TP) AGIQA-3kAIGCIQA2023 SRCC PLCC AGIQA-3kAIGCIQA-20k SRCC PLCC AIGCIQA2023AGIQA-3k SRCC PLCC AIGCIQA-20kAGIQA-3k SRCC PLCC 0.6373 0.6749 0.5662 0.6088 0.654 0.6681 0.6609 0. 0.678 - 0.7489 0.8557 0.6429 0.6786 0.5744 0.6033 0.664 0.6566 0.6696 0.6521 0.669 - 0.7461 0. 0.6874 0.6695 0.6659 0.6894 - 0.7001 0.6730 0.6780 - 0.7722 0.6862 0.8973 0.7045 0.7235 0.7133 0.7459 - 0.7579 0.7348 0. - 0.8414 0.7340 0.9308 0.6017 0.6752 0.5057 0.6574 0.627 0.5958 0.6090 0.4834 0.654 - 0.7427 0. 0.6396 0.7475 0.5579 0.6731 0.688 0.6264 0.6624 0.5241 0.695 - 0.7542 0.9314 0.7149 0.6130 0.7497 0.7829 - 0.7606 0.7315 0. - 0.8503 0.8452 0.8961 0.7221 0.4861 0.7493 0.8090 - 0.7825 0.7363 0.6014 - 0.8430 0.8772 0. from scratch. Despite these efforts, the local MLLM struggled to effectively handle all three aspects simultaneously, likely stems from the inadequate capacity of the vision encoder within the MLLM. Furthermore, the ranking performance could potentially be enhanced by incorporating specific constraints, such as ranking loss function, although this would require more computational resources and larger batch sizes. VI. ETHICS STATEMENT Considering that our method employs an MLLM embedding an LLM and distills capabilities from an online MLLM service, there is potential risk of generating harmful content. However, we rely on the online MLLMs robust safeguards to prevent harmful outputs from normal prompts, significantly mitigating such risks. In the datasets used, fewer than 50 images are labeled as Not Safe For Work (NSFW), but these images were deemed acceptable by human judgment and rejected only because of the stringent filtering of the Gemini API. For these cases, GPT-4o was leveraged as safe and effective alternative, ensuring high-quality content generation while preserving the integrity and applicability of the research. VII. CONCLUSION This paper presents M3-AGIQA, comprehensive framework for assessing AI-generated image quality via multimodal, multi-round, and multi-aspect approach. By distilling capabilities from an online MLLM to local model, M3AGIQA aligns closely with human perception in quality, correspondence, and authenticity. Experimental results demonstrate superior performance compared with state-of-the-art methods. While computational challenges remain, M3-AGIQA sets the stage for future research in efficient AGI quality assessment for broader generalization and improved computational efficiency."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Wang, H. Duan, J. Liu, S. Chen, X. Min, and G. Zhai, Aigciqa2023: large-scale image quality assessment database for ai generated images: from the perspectives of quality, authenticity and correspondence, in CAAI International Conference on Artificial Intelligence. Springer, 2023, pp. 4657. [2] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved training of wasserstein gans, Advances in neural information processing systems, vol. 30, 2017. [3] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, Advances in neural information processing systems, vol. 30, 2017. [4] Z. Zhang, C. Li, W. Sun, X. Liu, X. Min, and G. Zhai, perceptual quality assessment exploration for aigc images, in 2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW). IEEE, 2023, pp. 440445. [5] C. Li, Z. Zhang, H. Wu, W. Sun, X. Min, X. Liu, G. Zhai, and W. Lin, Agiqa-3k: An open database for ai-generated image quality assessment, IEEE Transactions on Circuits and Systems for Video Technology, 2023. [6] C. Li, T. Kou, Y. Gao, Y. Cao, W. Sun, Z. Zhang, Y. Zhou, Z. Zhang, W. Zhang, H. Wu et al., Aigiqa-20k: large database for ai-generated image quality assessment, arXiv preprint arXiv:2404.03407, vol. 2, no. 3, p. 5, 2024. [7] T. Zhou, S. Tan, W. Zhou, Y. Luo, Y.-G. Wang, and G. Yue, Adaptive mixed-scale feature fusion network for blind ai-generated image quality assessment, IEEE Transactions on Broadcasting, 2024. [8] F. Peng, H. Fu, A. Ming, C. Wang, H. Ma, S. He, Z. Dou, and S. Chen, Aigc image quality assessment via image-prompt correspondence, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, vol. 6, 2024. [9] Z. Yu, F. Guan, Y. Lu, X. Li, and Z. Chen, Sf-iqa: Quality and similarity integration for ai generated image quality assessment, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 66926701. [10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable language supervision, in International visual models from natural conference on machine learning. PMLR, 2021, pp. 87488763. [11] P. Wang, W. Sun, Z. Zhang, J. Jia, Y. Jiang, Z. Zhang, X. Min, and G. Zhai, Large multi-modality model assisted ai-generated image quality assessment, in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 78037812. [12] J. Wang, H. Duan, G. Zhai, and X. Min, Understanding and evaluating human preferences for ai generated images with instruction tuning, arXiv preprint arXiv:2405.07346, 2024. [13] A. Mittal, R. Soundararajan, and A. C. Bovik, Making completely blind image quality analyzer, IEEE Signal processing letters, vol. 20, no. 3, pp. 209212, 2012. [14] W. Xue, X. Mou, L. Zhang, A. C. Bovik, and X. Feng, Blind image quality assessment using joint statistics of gradient magnitude and laplacian features, IEEE Transactions on Image Processing, vol. 23, no. 11, pp. 48504862, 2014. [15] M. Zhang, C. Muramatsu, X. Zhou, T. Hara, and H. Fujita, Blind image quality assessment using the joint statistics of generalized local binary pattern, IEEE Signal Processing Letters, vol. 22, no. 2, pp. 207210, 2014. 10 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 [16] L. Zhang, L. Zhang, and A. C. Bovik, feature-enriched completely blind image quality evaluator, IEEE Transactions on Image Processing, vol. 24, no. 8, pp. 25792591, 2015. [17] J. Xu, P. Ye, Q. Li, H. Du, Y. Liu, and D. Doermann, Blind image quality assessment based on high order statistics aggregation, IEEE Transactions on Image Processing, vol. 25, no. 9, pp. 44444457, 2016. [18] W. Hou, X. Gao, D. Tao, and X. Li, Blind image quality assessment via deep learning, IEEE transactions on neural networks and learning systems, vol. 26, no. 6, pp. 12751286, 2014. [19] L. Kang, P. Ye, Y. Li, and D. Doermann, Convolutional neural networks for no-reference image quality assessment, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 1733 1740. [20] W. Zhang, K. Ma, J. Yan, D. Deng, and Z. Wang, Blind image quality assessment using deep bilinear convolutional neural network, IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 1, pp. 3647, 2018. [21] X. Yang, F. Li, and H. Liu, Ttl-iqa: Transitive transfer learning based no-reference image quality assessment, IEEE Transactions on Multimedia, vol. 23, pp. 43264340, 2020. [22] C. Yang, X. Zhang, P. An, L. Shen, and C.-C. J. Kuo, Blind image quality assessment based on multi-scale klt, IEEE Transactions on Multimedia, vol. 23, pp. 15571566, 2020. [23] W. Sun, H. Duan, X. Min, L. Chen, and G. Zhai, Blind quality assessment for in-the-wild images via hierarchical feature fusion strategy, in 2022 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB). IEEE, 2022, pp. 0106. [24] Y. Zhu, Y. Li, W. Sun, X. Min, G. Zhai, and X. Yang, Blind image quality assessment via cross-view consistency, IEEE Transactions on Multimedia, vol. 25, pp. 76077620, 2022. [25] S. Sun, T. Yu, J. Xu, W. Zhou, and Z. Chen, Graphiqa: Learning distortion graph representations for blind image quality assessment, IEEE Transactions on Multimedia, vol. 25, pp. 29122925, 2022. [26] S. Su, Q. Yan, Y. Zhu, C. Zhang, X. Ge, J. Sun, and Y. Zhang, Blindly assess image quality in the wild guided by self-adaptive hyper network, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [27] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [28] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in International conference on machine learning. PMLR, 2022, pp. 12 88812 900. [29] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [30] J. Wang, K. C. Chan, and C. C. Loy, Exploring clip for assessing the look and feel of images, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 2, 2023, pp. 25552563. [31] W. Zhang, G. Zhai, Y. Wei, X. Yang, and K. Ma, Blind image quality assessment via vision-language correspondence: multitask learning perspective, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 14 07114 081. [32] J. Shi, P. Gao, and A. Smolic, Blind image quality assessment via transformer predicted error map and perceptual quality token, IEEE Transactions on Multimedia, 2023. [33] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [34] X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li, Better aligning text-to-image models with human preference, arXiv preprint arXiv:2303.14420, vol. 1, no. 3, 2023. [35] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy, Pick-a-pic: An open dataset of user preferences for text-to-image generation, Advances in Neural Information Processing Systems, vol. 36, pp. 36 65236 663, 2023. [36] J. Xu, X. Liu, Y. Wu, Y. Tong, Q. Li, M. Ding, J. Tang, and Y. Dong, Imagereward: Learning and evaluating human preferences for text-toimage generation, Advances in Neural Information Processing Systems, vol. 36, 2024. [37] Z. J. Wang, E. Montoya, D. Munechika, H. Yang, B. Hoover, and D. H. Chau, Diffusiondb: large-scale prompt gallery dataset for text-toimage generative models, arXiv preprint arXiv:2210.14896, 2022. [38] J. Chen, J. An, H. Lyu, C. Kanan, and J. Luo, Learning to evaluate the artness of ai-generated images, IEEE Transactions on Multimedia, 2024. [39] J. Yuan, X. Cao, C. Li, F. Yang, J. Lin, and X. Cao, Pku-i2iqa: An image-to-image quality assessment database for ai generated images, arXiv preprint arXiv:2311.15556, 2023. [40] X. Liu, X. Min, G. Zhai, C. Li, T. Kou, W. Sun, H. Wu, Y. Gao, Y. Cao, Z. Zhang et al., Ntire 2024 quality assessment of ai-generated content challenge, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 63376362. [41] J. Yang, J. Fu, W. Zhang, W. Cao, L. Liu, and H. Peng, Moe-agiqa: Mixture-of-experts boosted visual perception-driven and semantic-aware quality assessment for ai-generated images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 63956404. [42] Y. Hu, B. Liu, J. Kasai, Y. Wang, M. Ostendorf, R. Krishna, and N. A. Smith, Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 20 40620 417. [43] Y. Lu, X. Yang, X. Li, X. E. Wang, and W. Y. Wang, Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, Advances in Neural Information Processing Systems, vol. 36, 2024. [44] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai et al., Q-bench: benchmark for general-purpose foundation models on low-level vision, arXiv preprint arXiv:2309.14181, 2023. [45] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, K. Xu, C. Li, J. Hou, G. Zhai et al., Q-instruct: Improving low-level visual abilities for multi-modality foundation models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 25 49025 500. [46] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun et al., Q-align: Teaching lmms for visual scoring via discrete text-defined levels, arXiv preprint arXiv:2312.17090, 2023. [47] S. Hochreiter, Long short-term memory, Neural Computation MITPress, 1997. [48] M. Beck, K. Poppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter, xlstm: Extended long short-term memory, arXiv preprint arXiv:2405.04517, 2024. [49] B. Alkin, M. Beck, K. Poppel, S. Hochreiter, and J. Brandstetter, Vision-lstm: xlstm as generic vision backbone, arXiv preprint arXiv:2406.04303, 2024. [50] Y. Jin, J. Li, Y. Liu, T. Gu, K. Wu, Z. Jiang, M. He, B. Zhao, X. Tan, Z. Gan et al., Efficient multimodal large language models: survey, arXiv preprint arXiv:2405.10739, 2024. [51] D. Ghadiyaram and A. C. Bovik, Massive online crowdsourced study of subjective and objective picture quality, IEEE Transactions on Image Processing, vol. 25, no. 1, pp. 372387, 2015. [52] H. Zhu, H. Wu, Y. Li, Z. Zhang, B. Chen, L. Zhu, Y. Fang, G. Zhai, W. Lin, and S. Wang, Adaptive image quality assessment via teaching large multimodal model to compare, arXiv preprint arXiv:2405.19298, 2024. [53] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, Large language models are zero-shot reasoners, Advances in neural information processing systems, vol. 35, pp. 22 19922 213, 2022. [54] P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy, Llm2vec: Large language models are secretly powerful text encoders, arXiv preprint arXiv:2404.05961, 2024. [55] X. Wang, B. Ma, C. Hu, L. Weber-Genzel, P. Rottger, F. Kreuter, D. Hovy, and B. Plank, my answer is c: First-token probabilities do not match text answers in instruction-tuned language models, arXiv preprint arXiv:2402.14499, 2024. [56] T. Wang, W. Sun, X. Min, W. Lu, Z. Zhang, and G. Zhai, multidimensional aesthetic quality assessment model for mobile game images, in 2021 International Conference on Visual Communications and Image Processing (VCIP). IEEE, 2021, pp. 15. [57] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, Q. Chen, H. Zhou, Z. Zou, H. Zhang, S. Hu, Z. Zheng, J. Zhou, J. Cai, X. Han, G. Zeng, D. Li, Z. Liu, and M. Sun, Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint 2408.01800, 2024. [58] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556, 2014. [59] K. Cho, Learning phrase representations using rnn encoder-decoder for statistical machine translation, arXiv preprint arXiv:1406.1078, 2014. [60] T. Dao and A. Gu, Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality, in International Conference on Machine Learning (ICML), 2024. CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT 11 VIII. SUPPLEMENTAL MATERIALS A. Prompts 1) Interaction with Online MLLM Service: This is the prompt template used for retrieving the image descriptions on the three aspects from an online MLLM. The alignment aspect in the template is equivalent to correspondence."
        },
        {
            "title": "Description Prompt Template",
            "content": "User: Analyze this image generated from the text prompt: {prompt}. The quality score is {mos q}, alignment score is {mos a}, authenticity score is {mos au}. Scores are between {min} and {max}, with higher scores being better."
        },
        {
            "title": "Provide your assessment",
            "content": "in JSON format with the following keys: quality explanation: Describe the overall quality of the image, considering aspects such as composition, clarity, and any noticeable flaws. alignment explanation: Explain how well the image reflects the elements and intent of the provided prompt. Be specific about which aspects of the prompt are successfully conveyed and which might be missing or misinterpreted. authenticity explanation: Discuss how closely the image resembles real artworks. Highlight any parts of the image that appear non-real or artificial. Do not include the numerical scores in your response. **Example:** { quality explanation: The overall quality of the image is quite high. The composition is balanced, with the boat centered and leading the viewers eye towards the bridge and beyond. The clarity is excellent, with sharp details on the boat, water, and surrounding cliffs. The lighting is dramatic and enhances the overall atmosphere of the scene. However, there are some noticeable flaws, such as the overly saturated colors, which can detract from the natural feel of the image., alignment explanation: The image closely aligns with the prompt bridge over body of water with boat in the water. The bridge is prominently featured, and the boat is clearly visible in the water. The scene captures the essential elements of the prompt well. However, some additional details, such as the type of bridge or the style of the boat, could have been more specific to better reflect the intent of the prompt., authenticity explanation: The image has surreal, almost dreamlike quality, which makes it less authentic as representation of real-world scene. The colors are highly saturated and the lighting effects are dramatic, which enhances the artistic feel but reduces the realism. The boat and the bridge look more like artistic renditions rather than actual structures, and the overall scene feels more like digital artwork or scene from fantasy world rather than photograph of real place. } 2) One-round Conversation: This is the prompt template for the configuration without the image description (w/o ID) mentioned in our experiments. One-round Conversation Prompt Template (Qual.) User: Take close look at this AI-generated image and tell me everything you can about what you see. Its original prompt is: {prompt}. Could you please help to rate the image based on its overall quality? Please give me result from [bad, poor, fair, good, excellent]. Please just output one word from the list. One-round Conversation Prompt Template (Corr.) User: Take close look at this AI-generated image and tell me how well it aligns with the elements and intent of the original prompt: {prompt}. Could you please help to rate the image based on its correspondence with the prompt? Please give me result from [bad, poor, fair, good, excellent]. Please just output one word from the list. One-round Conversation Prompt Template (Auth.) User: Take close look at this AI-generated image and evaluate how authentic it appears. Does it resemble real artworks or scenes? Highlight any parts that seem artificial or non-realistic. Could you please help to rate the image based on its authenticity? Please give me result from [bad, poor, fair, good, excellent]. Please just output one word from the list. 3) Multi-round Conversation: The prompt template is for fine-tuning the local MLLM with the response from the online MLLM service, and is also used during the inference stage. Multi-round Conversation Prompt Template (Qual.) User: Please closely examine this AI-generated image and provide detailed analysis of its content and quality. The original prompt for the image was: {prompt} What can you deduce about the image based on this prompt, and how would you assess its overall quality? Assistant: {response 0} User: Based on your analysis, could you please rate the images overall quality? Choose one word from the following list: [bad, poor, fair, good, excellent], where the words range from low to high quality. Assistant: {response 1} Multi-round Conversation Prompt Template (Corr.) User: Please closely examine this AI-generated image and provide detailed analysis of how well it aligns with the original prompt: {prompt}. Assistant: {response 0} User: Based on your analysis, could you please rate the images alignment with its original prompt? Choose one word from the following list: [bad, poor, fair, good, 12 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST positive with terms like high quality and vibrant. As result, the prediction is fair (2.5059), which misaligned with the ground truth but aligned well with the overly positive image descriptions provided. This case testifies the importance for quality of image descriptions on the other side, if the image descriptions used for fine-tuning is more precisely aligned with the human perceptual evaluation of poor (1.2288), the prediction would have been better. In this case, better description could be like: This images low rating likely stems from its overwhelming brightness and chaotic composition, which obscures intended details and focus. Although the prompt calls for tragic supernova with hyper detail and HDR lacks clear structure or focal lighting, point, making interpretation challenging. Intense yellow and blue hues dominate without balance, creating visual confusion rather than conveying the intended sense of tragedy. the result And the last bad case looks just on the contrary to the fifth case, the image descriptions from both online MLLM service and local fine-tuned MLLM are all negative which can be concluded as poor, while the ground truth is good (3.7735), and the predicted score fair (2.4688) is intended to be aligned with the ground truth. more fitting description considering the actual quality of the image would be: The images quality is good due to the successful portrayal of very drunk person in stylized, sci-fi aesthetic, capturing the expression and mood effectively. The persons face shows blurred, exaggerated expression, possibly conveying intoxication, and the sci-fi lighting adds an intriguing, futuristic element. However, the lack of clarity, particularly in the persons features and the background, may detract from the overall quality. The blurred, blue-tinged lighting creates an atmospheric effect but reduces sharpness, making it difficult to connect fully with the subject. This blend of expressive emotion and sci-fi styling gives the image distinctive feel. excellent], where the words indicate the degree of alignment from low to high. Assistant: {response 1} Multi-round Conversation Prompt Template (Auth.) User: Please closely examine this AI-generated image and provide detailed analysis of its authenticity. The original prompt for the image was: {prompt}. How closely does the image resemble real artworks or scenes? Highlight any parts of the image that appear non-real or artificial. Assistant: {response 0} User: Based on your analysis, could you please rate the images authenticity? Choose one word from the following list: [bad, poor, fair, good, excellent], where the words indicate the degree of authenticity from low to high. Assistant: {response 1} Notice that during fine-tuning (Init Description stage), the repsonse 0 is the response from online MLLM service and response 1 is the ground truth by categorize the MOS to the five string labels. While during the Inference stage, both response 0 and response 1 are generated by the fine-tuned local MLLM. B. Cases 1) Multi-Aspect Responses: To illustrate the effectiveness of the distillation, we selected two images generated using the same prompt but exhibiting different quality levels, as shown in Table VII. The responses from both the online MLLM service and the local fine-tuned MLLMs were analyzed. Highlighted phrases such as high quality, perfectly aligns, and does not resemble real artworks in the table demonstrate that the local fine-tuned MLLMs successfully learned to evaluate the images across multiple aspects, reflecting the mechanism transferred from the online MLLM service. 2) Good and Bad Cases: We analyzed the cases where our methods predictions were most and least aligned with the ground truth by sampling the top three aligned and the bottom three misaligned instances as illustrated in Table VIII. The well-aligned cases shared similar tones in their descriptions, indicating close agreement between the predicted and actual image quality assessments. Conversely, among the misaligned examples, the fourth case illustrates significant discrepancy. The ground truth description labeled the image as having an unrealistic, almost neon green color and an artificial feel, indicating poor quality. However, the predicted description portrayed the image more positively, noting decent quality with sense of movement and power. This led to prediction of good (2.7090) in to the actual poor (1.6788) rating, highlighting contrast fundamental difference in interpretation that affected the accuracy of our models prediction. In the fifth case, the ground truth rated the image as poor (1.7111), yet the description provided by the online MLLM service and the local fine-tuned MLLM seems to be more CUI et al.: M3-AGIQA: MULTIMODAL, MULTI-ROUND, MULTI-ASPECT AI-GENERATED IMAGE QUALITY ASSESSMENT TABLE VII MULTI-ASPECT RESPONSES FROM ONLINE MLLM SERVICE AND LOCAL FINE-TUNED MLLM FOR THE IMAGE GENERATED BY PROMPT an aerial photo of sandy island in the ocean Image Aspect ID from online MLLM service ID from fine-tuned local MLLM Qual. Corr. Auth. Qual. Corr. Auth. The image boasts high quality with clear, well-composed aerial view. The colors are vibrant, and the textures of the sand, water, and vegetation are well-defined. There are no noticeable flaws or artifacts, contributing to realistic and visually appealing image. The image aligns perfectly with the prompt. It depicts sandy island, partially covered in vegetation, situated in vast expanse of blue ocean. The aerial perspective accurately reflects the aerial photo aspect of the prompt. The image successfully encapsulates all elements of the prompt. The image closely resembles real aerial photograph. The colors, while vibrant, remain within natural palette, and the lighting and shadows appear realistic. The textures and patterns of the sand, water, and vegetation contribute to the images authenticity. There are no elements that suggest artificiality or manipulation. The image quality is relatively low. The composition is poorly defined, lacking clear focal point. While the colors are vibrant, they are heavily blurred and lack detail, making the overall image appear hazy and unrealistic. The image partially reflects the prompt. It features sandy area surrounded by blue, which could be interpreted as an island in the ocean from an aerial perspective. However, the layout and shapes are ambiguous, and its difficult to definitively identify the scene as an island. The lack of clarity hinders stronger alignment with the prompt. The image does not resemble real artwork or photograph. The excessive blurring, unrealistic color gradients, and lack of detail give it heavily artificial and digitally manipulated feel. It lacks the characteristics of genuine aerial photograph. The image boasts excellent clarity and composition. The aerial perspective is wellexecuted, providing clear view of the islands shape and the surrounding waters. The lighting is natural and flattering, highlighting the textures of the sand and foliage. Overall, the image is of high quality. The image perfectly aligns with the prompt. It clearly depicts sandy island in the ocean from an aerial perspective. All elements of the prompt are present and accurately represented. The image closely resembles real aerial photograph of tropical island. The textures of the sand, water, and vegetation appear natural and realistic. There are no obvious signs of digital manipulation or artificial elements that would detract from its authenticity. The overall quality of the image is quite low. While it attempts to depict an aerial view of sandy island in the ocean, it lacks clarity and detail. The colors are overly saturated and blend together, creating hazy, unrealistic effect. The shapes of the landforms are indistinct and lack definition. The image partially aligns with the prompt. It does depict body of water and what could be interpreted as sandy island in the distance. However, the aerial perspective is not clearly conveyed, and thesandy island appears more as vague shape rather than defined landmass. The lack of detail makes it difficult to confirm the presence of sand. The image does not resemble real artworks or photographs. The excessive blurring, unrealistic color palette, and lack of detail contribute to an artificial and almost abstract appearance. It lacks the realism and texture expected in depiction of natural landscape. 14 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE VIII EXAMPLES SHOWCASING THE TOP THREE BEST-PERFORMING CASES AND THE BOTTOM THREE WORST-PERFORMING CASES ON DATASET AGIQA-3K [5] QUALITY ASPECT. Prompt Image ID from online MLLM service ID from fine-tuned local MLLM label predicted photograph of landscape, baroque style beautiful an emo portrait painting masonic illuminati waiting room with couch, soft lighting, abstract style the large cresting waves of parade disconnected images : tragic supernova, hyper detail, HDR lighting The image quality is good, featuring clear and well-composed scene. The building is sharply focused, and the reflection in the water is well-rendered. The soft, diffused lighting contributes to serene atmosphere. However, the image appears somewhat overall blurry, lacking fine detail, particularly in the foliage and the foreground. showcases The image boasts high quality, especially in its technical execution. smooth The portrait blending, creating realistic portrayal of skin and hair. The use of light and shadow adds depth and dimension to the face. However, the background lacks detail, appearing blurry and undefined, which slightly detracts from the overall composition. The image is of low quality, appearing more as an abstract composition than coherent scene. It lacks clarity, with blurry and indistinct elements. The lighting, while potentially aiming for soft, is muddled and doesnt contribute to cohesive atmosphere. The image quality is low due to the unrealistic, almost neon green color of the water. The composition is simple, focusing solely on the wave, and lacks depth. The spray of the wave is well-detailed, but the overall image feels artificial. The image is of high quality in terms of clarity and detail. The colors are vibrant and the textures are intricate. The overall composition, however, lacks clear focal point and can feel overwhelming due to the lack of negative space. The overall quality of the image is good. The composition is wellbalanced, with the grandeur of the baroque-style building drawing the viewers eye. The use of symmetrical path leading to the building creates sense of depth and the image perspective. However, is somewhat blurry, particularly in the background, which detracts from the sharpness and detail. The image quality is good for digital painting. The brushstrokes are visible and give sense of texture to the hair and skin. The lighting is well-executed, creating moody atmosphere. However, the face appears slightly blurry, particularly around the eyes and mouth, which detracts from the overall clarity. 3.0 3.0015 2. 2.9434 The image is of low quality. It is blurry, lacks clear shapes, and the colors are muddled. There is no discernible composition, and the overall impression is abstract and unclear. 0.9708 0.9736 The image quality is decent but not exceptional. The wave itself is well-rendered with good sense of movement and power. However, the composition is somewhat simplistic, focusing solely on the wave without much context or background detail. The color palette is limited, with the green of the wave standing out against the gray sky and water. The image boasts high quality in terms of clarity and detail. The textures are intricate, and the lighting effects are well-executed, creating sense of depth and dynamism. The composition, however, is somewhat lacking. While the image is visually striking, it lacks clear focal point and feels somewhat chaotic. 1.6788 2.7090 1.2288 2.5059 very drunk person named aaron, sci-fi style The image quality is quite poor. It appears to be poorly lit photograph with strong blue color cast, making it difficult to discern details. The image is blurry, particularly in the background, further reducing the overall clarity. The image quality is quite low. It appears to be blurry, poorly lit photograph rather than wellcomposed image. The lighting is harsh and uneven, creating an unflattering effect on the subjects face. 3.7735 2."
        }
    ],
    "affiliations": [
        "Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation",
        "Hefei High-Dimensional Data Technology Co.,Ltd.",
        "School and Technology, Tongji University, Shanghai, of Computer Science China"
    ]
}