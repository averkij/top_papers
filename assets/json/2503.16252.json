{
    "paper_title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "authors": [
        "Zhaowei Liu",
        "Xin Guo",
        "Fangqi Lou",
        "Lingfeng Zeng",
        "Jinyi Niu",
        "Zixuan Wang",
        "Jiajie Xu",
        "Weige Cai",
        "Ziwei Yang",
        "Xueqian Zhao",
        "Chao Li",
        "Sheng Xu",
        "Dezhi Chen",
        "Yun Chen",
        "Zuo Bai",
        "Liwen Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 2 5 2 6 1 . 3 0 5 2 : r Fin-R1: Large Language Model for Financial Reasoning through Reinforcement Learning Zhaowei Liu1, Xin Guo1, Fangqi Lou1, Lingfeng Zeng1, Jinyi Niu2, Zixuan Wang1, Jiajie Xu1, Weige Cai1, Ziwei Yang1, Xueqian Zhao1, Chao Li3, Sheng Xu3, Dezhi Chen3, Yun Chen1, Zuo Bai3, Liwen Zhang1 1Shanghai University of Finance and Economics, 2Fudan University, 3FinStep {zhang.liwen}@shufe.edu.cn,"
        },
        {
            "title": "Abstract",
            "content": "Reasoning large language models (LLMs) are rapidly evolving across various domains. However, their capabilities in handling complex financial problems still require in-depth exploration. In this paper, we introduce Fin-R1, large language model specifically designed for financial reasoning. With lightweight parameter scale of 7 billion, this model significantly reduces deployment cost while effectively addresses three major financial pain points: fragmented financial data, uncontrollable reasoning logic, and weak business generalization ability. To boost the models reasoning capability, we first built Fin-R1-Data, high-quality dataset with around 60,091 complete chains of thought (CoT) for both reasoning and nonreasoning financial scenarios, through distillation and screening process from multiple authoritative datasets. Then, we perform Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) based on this dataset. This two-stage framework significantly enhances the models ability to perform complex financial reasoning tasks, enabling more accurate and interpretable decision-making in financial AI applications. Despite its compact structure with only 7B parameters, Fin-R1 demonstrates outstanding performance in authoritative benchmarks covering multiple financial business scenarios. It achieves an average score of 75.2, securing second place overall and significantly outperforming other large-scale reasoning LLMs in the evaluation. Notably, Fin-R1 is better than DeepSeek-R1Distill-Llama-70B, demonstrating its efficiency and effectiveness. It achieves the state-of-the-art scores of 85.0 in ConvFinQA and 76.0 in FinQA, which focus on financial reasoning. In real-world applications, Fin-R1 has demonstrated strong automated reasoning and decision-making abilities in areas like financial compliance and robo-advisory, providing efficient solutions to long-standing financial industry challenges. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1."
        },
        {
            "title": "Introduction",
            "content": "In recent years, the rapid iteration of large language models (LLMs) has significantly propelled the evolution of artificial intelligence towards artificial general intelligence (AGI). OpenAIs o1 (OpenAI, 2024a) series models have enhanced their ability to solve complex reasoning tasks by extending the length of the \"chain-of-thought\" reasoning process through mechanism of \"exploration-reflectioniteration.\" Similar o1-like LLMs, such as QwQ (Qwen, 2024) and Marco-o1 (Zhao et al., 2024b), have achieved notable improvements in various reasoning tasks, including mathematics, programming, and logical reasoning. Financial reproduction versions of o1 models, such as XuanYuan-FinX1-Preview (Duxiaoman DI Team, 2024) and Fino1 (Qian et al., 2025), have also demonstrated the immense potential of LLMs in simulating human cognitive processes and handling complex tasks. DeepSeekR1 (Guo et al., 2025) adopts fundamentally different approach from o1-class models, leveraging pure Reinforcement Learning (RL) to enhance the reasoning capabilities of large language models. Through thousands of steps of unsupervised RL training, combined with small set of cold-start data and multi-stage training framework, the model exhibits emergent reasoning abilities in benchmark evaluations. Simultaneously, this training strategy further refines the models reasoning performance and readability, demonstrating the efficacy of RL-driven methodologies in advancing the inference capabilities of large-scale language models. However, when general-purpose reasoning models are applied to the financial domain, they still face challenges in adapting to vertical scenarios. Financial reasoning tasks often involve knowledge of legal clauses, economic indicators, and mathematical modeling. These tasks not only require the integration of interdisciplinary knowledge but also demand verifiable, step-by-step decision-making logic. During the application of LLMs in real-world financial business scenarios, the following issues are often encountered: 1. Fragmentation of financial data makes it difficult to integrate knowledge (Wang et al., 2024b; Guo et al., 2024; Xie et al., 2024; Li et al., 2024; Dong et al., 2024; Xue et al., 2024; Xu et al., 2025; Wang et al., 2024b; Yu et al., 2024a; Zhang et al., 2024). The inconsistency of data not only increases the complexity of preprocessing but also may lead to redundant or missing information, further weakening the models ability to comprehensively understand and reason within the financial domain. 2. Black-box reasoning logic fails to meet regulatory requirements for traceability (Wang et al., 2023; Zhao et al., 2024a; Tong et al., 2024). The complex structure of existing models makes their reasoning process difficult to interpret intuitively. This creates contradiction with the regulatory requirements for transparency and traceability in finance, thereby limiting the application of these models in critical financial business areas. 3. Insufficient generalization ability in financial scenarios lead to unreliable outputs in high-risk financial applications (Yu et al., 2024b; Fatouros et al., 2024; Zhou et al., 2023). The existing models often perform unstably across different scenarios and struggle to be quickly transferred and generalized to new business contexts. This limitation makes the models prone to instability or inaccuracy in their outputs when facing high-risk financial applications. To address the challenges faced by general-purpose reasoning models in the financial domain, this paper introduces Fin-R1, the large language model tailored for financial reasoning. By reconstructing high-quality financial reasoning dataset and employing two-stage training framework, Fin-R1 effectively tackles the three core issues of fragmented financial data, uncontrollable reasoning logic, and weak business generalization ability. Our main contributions are as follows: High-Quality Financial Reasoning Dataset:We propose Fin-R1-Data, high-quality COT dataset distilled and filtered from multiple authoritative financial datasets, specifically designed for professional financial reasoning scenarios. Fin-R1-Data covers multidimensional professional knowledge in the Chinese and English financial vertical domain and can effectively support multiple core financial business scenarios. Explicit Financial Reasoning Large Language Model: We propose Fin-R1, financial reasoning large language model trained on multidimensional financial business datasets, which precisely addresses the core demands of the financial industry for decision-making processes, numerical rigor, and strong business generalization capabilities. Two-Stage Model Construction Framework: We propose two-stage workflow framework that involves constructing high-quality CoT dataset and training the model through Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), that can effectively enhance the models financial reasoning performance. The remainder of this report is structured as follows. Section 2 provides detailed description of the methodological framework. Section 3 briefly describes our experiments and results on multiple financial benchmark tests. Section 4 summarizes the technical contributions and outlines future research directions."
        },
        {
            "title": "2 Approach",
            "content": "2.1 Overview We propose two-stage model construction framework in Figure 1. In the data generation phase, we employ data distillation based on DeepSeek-R1 and data filtering method using llm-as-judge (Xu et al., 2023) to create high-quality financial reasoning dataset, Fin-R1-Data. In the model 2 training phase, we establish the financial reasoning model Fin-R1 based on Qwen2.5-7B-Instruct, using Supervised Fine-Tuning (SFT) and the Group Relative Policy Optimization algorithm (GRPO) (Shao et al., 2024) to enhance the models reasoning capability and standardize its output format. Figure 1: The pipeline for constructing Fin-R1. The diagram depicts the two-stage construction framework of Fin-R1: Data Generation (using DeepSeek-R1 for reasoning to generate CoT data, followed by quality filtering with the Qwen2.5-72B-Instruct) and Model Training (including SFT pretraining and GRPO optimization for FinR1). Additionally, the right side highlights the performance of Fin-R1 in financial code generation, professional knowledge, and business knowledge. 2.2 Data Construction Our objective is to develop Fin-R1-Data, high-quality, supervised fine-tuning (SFT) dataset specifically designed for financial domains. To achieve this goal, we have designed robust and comprehensive data construction pipeline, including data distillation and data filtering, aimed at ensuring the accuracy and reliability of the dataset. The detailed pipeline of data construction is shown in Figure 2. 2.2.1 Data Source Fin-R1-Data comprises total of 60,091 distinct entries, encompassing both Chinese and English content in bilingual format. The dataset is organized into two primary components: open-source datasets and proprietary datasets. The open-source datasets include Ant_Finance (Alipay Team, 2023), FinanceIQ Duxiaoman DI Team (2023b), Quant-Trading-Instruct (FinanceQT) (Malik, 2024), ConvFinQA (Chen et al., 2022), FinQA (Chen et al., 2021), Twitter-Financial-News-Sentiment (TFNS) (Anonymous, 2024), Finance-Instruct-500K (Flowers, 2025), FinCorpus (Duxiaoman DI Team, 2023a), and FinCUGE (Lu et al., 2023). The proprietary component of the dataset, the Financial Postgraduate Entrance Exam (FinPEE) dataset, consists of 350 calculation problems derived from financial postgraduate entrance examinations. The construction of FinPEE followed rigorous, multi-stage process. Initially, the dataset was collected in PDF format and then processed in bulk using Mineru (Wang et al., 2024a) for conversion into markdown format. Following this, structured question-answer (Q-A) pairs were extracted using regularization techniques. To ensure the integrity and accuracy of the data, all extracted Q-A pairs 3 Figure 2: Stage 1-The pipeline of data construction: (1) Data Distillation, (2) Answer Check, where an LLM evaluates the accuracy of responses generated by DeepSeek-R1, and (3) Reasoning Selection, where an LLM assesses and scores reasoning trajectories to ensure logical coherence and quality. \"Reasoning\" represents the reasoning output, while \"Thinking\" refers to the evaluation process of the judgment model. underwent manual review and validation process, resulting in high-quality dataset specifically tailored for financial postgraduate examination problems. The composition structure of Fin-R1-Data across its various components is illustrated in Figure 3. As presented in Table 1, the table systematically details the descriptions, data sources, and proportional distribution of various data categories within Fin-R1-Data. The dataset is predominantly composed of financial non-reasoning business knowledge and financial reasoning business knowledge, which collectively constitute 77.9% of the total. These two categories comprehensively capture wide range of real-world financial business scenarios, ensuring extensive coverage of operational processes. Additionally, financial professional knowledge represents significant component of the dataset, encompassing key concepts across multiple financial subfields and accounting for 21.9% of the total data. Furthermore, Fin-R1-Data includes specialized subset of financial code data, designed for the development of quantitative trading strategies, though this category comprises only 0.2% of the dataset. 2.2.2 Data Processing Data Processing comprises both data distillation and data filtering. During the distillation phase, parameter configurations were rigorously aligned with the official DeepSeek-R1 specifications. The specific settings applied are outlined as follows: (1) The temperature was set to 0.6. (2) For mathematical data, the standardized prompt \"Please use boxed{} to wrap the final answer\" was employed to ensure consistency in answer formatting. (3) To maintain alignment with the intended reasoning pattern, \"n\" was forcibly appended at the beginning of each output before initiating data generation. Data Filtering involves two primary components: answer check (evaluating the accuracy of modelgenerated responses) and reasoning selection (assessing the quality of reasoning trajectories). In the answer check phase, data filtering is conducted by retaining only responses that precisely align with the reference answers. Specifically, if response generated by DeepSeek-R1 deviates from the standard answer provided in the dataset, it is immediately discarded. For objective questions, we 4 Figure 3: Composition structure of Fin-R1-Data: (1) Financial Code, (2) Financial Professional Knowledge, (3) Financial Reasoning Knowledge, and (4) Financial Non-Reasoning Knowledge. employ exact matching to ensure correctness, while for subjective questions, we adopt the LLMas-Judge to evaluate response validity. Regarding model selection and prompt optimization, we conducted comparative assessment of GPT-4o (OpenAI, 2024b) and Qwen2.5-72B-Instruct (Yang et al., 2024) across various prompting strategies. Experimental results demonstrate that Qwen2.5-72BInstruct achieves an accuracy rate of 99.6% in LLM-as-Judge tasks, surpassing GPT-4o. Consequently, Qwen2.5-72B-Instruct was selected as the judge model, and we determined the optimal prompt for evaluation. Further details on the experimental setup and findings are provided in Appendix A.3. In the reasoning selection phase, we drew inspiration from the study by Xie et al. (2024) and distilled seven key dimensions from it: internal consistency, term overlap rate, number of reasoning steps, logical coherence, content diversity, task-domain relevance, and alignment with task instructions. These dimensions were employed to comprehensively evaluate the models reasoning trajectory data. To ensure the robustness of the filtering process, we conducted experiments comparing the correlation scores between human annotators and models. The results, detailed in Appendix A.2, showed that the scores of Qwen2.5-72B-Instruct closely aligned with human judgments, exhibiting only minor discrepancies, while GPT-4o displayed larger deviations. Based on these findings, we selected Qwen2.5-72B-Instruct to assess the quality of the reasoning trajectories. Based on these evaluations, we systematically scored and filtered the reasoning paths, retaining only high-quality trajectories, which were subsequently curated into refined dataset for supervised fine-tuning (SFT). In Figure 4, we present an example of high-quality reasoning trajectory alongside low-quality example, illustrating the distinction between them in the reasoning selection process. 2.3 Training method Fin-R1 is first trained via Supervised Fine-Tuning (SFT) using high-quality financial reasoning dataset to enhance its reasoning ability. Building on this, we employ reinforcement learning to implement Group Relative Policy Optimization (GRPO), leveraging financial Q&A data and incorporating dual reward mechanism to improve both the accuracy of response formatting and content. Figure 5 intuitively summarizes the comprehensive training framework, illustrating the synergistic integration of the supervised learning and reinforcement learning components. 5 Table 1: Categories and Sources of Fin-R1-Data Data Category Data Category Description Source Proportion"
        },
        {
            "title": "FinanceQT",
            "content": "0.2%"
        },
        {
            "title": "Financial Expertise",
            "content": "Financial Terminology Explanation, Q&A on Financial Expertise, Financial Calculations Finance-Instruct500K"
        },
        {
            "title": "FinPEE",
            "content": "Non-reasoning Financial Business Knowledge Content Generation in Financial Business, Regulatory Compliance, Financial Knowledge, Financial Cognition, Financial Logic Ant-Finance FinCorpus Financial Reasoning Business Knowledge Numerical Reasoning on Financial Data, Financial News Sentiment Classification, Financial News Classification, Financial Causal Relationship Extraction FinQA ConvFinQA TFNS FinCUGE 18.2% 3.4% 0.3% 2.0% 48.4% 4.8% 12.3% 4.0% 6.4% 2.3.1 Training Data Template In this section, we explain the training format of the data, and the specific prompt template will be illustrated in Figure 5, which details our training process. During the Supervised Fine-Tuning (SFT) phase, each sample in the training SFT Training Data dataset comprises three components, i.e., = (x, c, y), where denotes the question, represents the reasoning trace formatted as <think>...</think>, and corresponds to the answer, formatted as <answer>...</answer>. During the SFT stage, is used as the input of the training set, and are used as the output of the training set. This phase enables the model to learn structured financial reasoning patterns, refining its parameters to generate well-formed reasoning traces and accurate answers. During the reinforcement learning (RL) phase, each sample in the training RL Training Data dataset consists of two components, i.e., = (x, y), where denotes the question and represents the models output, which includes only the answer without reasoning traces. Reinforcement learning further enhances output quality by improving answer accuracy and ensuring compliance with the expected format. 2.3.2 Supervised Fine-Tuning We initially performed Supervised Fine-Tuning (SFT) on Qwen2.5-7B-Instruct, specifically optimizing key aspects of financial reasoning. This fine-tuning process effectively mitigated the reasoning failures observed when applying the general-purpose model to financial reasoning tasks. The training data consisted of the ConvFinQA and FinQA datasets. Following SFT, the model demonstrated enhanced performance in financial reasoning, as detailed in Table 2. 6 Figure 4: Examples of high-quality and low-quality reasoning selections filtering 2.3.3 Group Relative Policy Optimization During the reinforcement learning phase, we employ the Group Relative Policy Optimization (GRPO) algorithm. For each training iteration, we sample candidate outputs {oi}G output receives reward ri, from which we compute the group-relative advantage Ai: i=1 from the old policy πold. Each Ai = ri µ{r} σ{r} , where µ{r} and σ{r} denote the mean and standard deviation of reward values within the group. Outputs exceeding group averages receive higher advantage values for prioritized optimization. The policy update now maximizes the following objective function: JGRPO(θ) = (cid:34) vP (V),{oi}G (cid:32) i=1πθold (Ov) 1 (cid:88) i=1 min (cid:0)rratio Ai, clip (cid:0)rratio , 1 ϵ, 1 + ϵ(cid:1) Ai (cid:1) βDKL(πθ πref) (cid:33)(cid:35) , (1) = πθ(oiv) where rratio πθold (oiv) represents the importance sampling ratio that quantifies the relative likelihood of generating output oi under the new policy πθ compared to the old policy πθold; Ai denotes the group-relative advantage, calculated by normalizing each reward with respect to the groups mean and standard deviation to emphasize outputs that surpass the group average; the clipping operator , 1 ϵ, 1 + ϵ(cid:1) restricts the update magnitude within the trust region [1 ϵ, 1 + ϵ] to avoid clip(cid:0)rratio destabilizing large parameter changes; the minimum operation between the unclipped term rratio Ai 7 Figure 5: Stage 2-The pipeline of training construction. During the SFT phase, the base model undergoes SFT using structured reasoning-augmented dataset, focusing on enhancing its ability to perform financial reasoning. During the RL phase, we apply GRPO algorithm, which introduces group computation mechanism to provide two reward signalsone for format correctness and one for content accuracy. and its clipped counterpart ensures conservative update that balances aggressive improvements with training stability; and finally, DKL(πθ πref) is the KL divergence and β is the hyper-parameter. 2.3.4 Reward Function Design In the process of training the reward model based on GRPO, we employs two reward mechanisms: format reward and accuracy reward. Format Reward We encourage outputs that include sequence of reasoning steps enclosed within <think>...</think> tags and concise final answer enclosed within <answer>...</answer> tags. format incentive score of 1 is awarded if all four tags appear exactly once with no extraneous content outside these tags; otherwise, score of 0 is assigned. The format reward function is defined as follows: Rfmt(y) = (cid:26)1, if the format matches 0, otherwise where denotes the models output. Format matching indicates that the output strictly adheres to the specified format by containing exactly one pair of <think> tags and one pair of <answer> tags, with no additional content outside these tags. Accuracy Reward In the financial scenario, we observed that it is challenging to exhaustively enumerate answer regular expressions using rule-based methods. The examples that are difficult to identify are presented in Figure 6. Consequently, we adopt Qwen2.5-Max (Qwen Team, 2024)as the judge for answer evaluation. The content enclosed within the <answer> ... </answer> tags is extracted from the completions model output using regular expressions, with the resulting solution serving as the standard answer. If the output within the <answer> ... </answer> tags is semantically consistent with the standard answer, reward of 1 is assigned; otherwise, the reward is 0. The specific prompts for the LLM as judge are provided in Appendix A.3. The accuracy reward function is defined as follows: Racc(y, y) = (cid:26)1, if = 0, otherwise where is models output (from <answer> ... </answer> tags). is the standard answer. 2.4 Evaluation 2.4.1 Evaluation Datasets We establish financial domain multi-task benchmarking framework by systematically validating five representative open-source heterogeneous datasets: FinQA, ConvFinQA, Ant-Finance, TFNS, and Finance-Instruct-500k. Notably, except for Finance-Instruct-500k where custom 10% test subset was extracted through stratified sampling from the complete data preprocessing pipeline, all other datasets strictly adhere to their original publicly available official evaluation splits.To control costs and maintain relatively uniform data distribution, for each evaluation set, we randomly sample 1,000 data entries for evaluation. If an evaluation set has fewer than 1,000 entries, we evaluate all of them. (a) Difference in decimal places. (b) Difference in expression. Figure 6: The difference between the model output and the ground truth is shown. Figure 6a illustrates the difference in decimal placement, while Figure 6b shows the difference in expression. 2.4.2 Evaluation Method The financial evaluation datasets employed in this study, except Finance-Instruct-500k, feature objective question formats with definitive and unique reference answers. Given that numerical calculation problems may induce discrepancies between model outputs and reference answers in representational formats, as shown in Figure 6 (manifested as equivalent conversion issues between percentage and decimal representations or differences in significant digit retention), we implements large language model as an automated evaluation judge for answer check, adopting the prompt design and evaluation methodology proposed by Zhu et al. (2024). Notably, although this evaluation paradigm operates at low level of surface complexity, systematic prompt engineering optimization strategies were implemented to ensure assessment reliability. Multi-dimensional tuning experiments were conducted on critical parameters of the prompt template, including but not limited to format specification directives, numerical precision constraints, and fault tolerance rule configurations. Detailed experimental designs and results are analyzed in the Appendix A.3."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Baselines To comprehensively evaluate the reasoning capabilities of Fin-R1 in financial scenarios, we conducted thorough comparative assessment against multiple state-of-the-art models. These models include DeepSeek-R1, Fin-R1-SFT, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, DeepSeek-R1-Distill-Qwen-32B, DeepSeek-R1-Distill-Llama-70B, Qwen-2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen-2.5-32B-Instruct. The selection of these models encompasses spectrum ranging from lightweight to high-performance architectures, taking into account factors such as reasoning capability and computational resource consumption. This comprehensive comparison aims to provide holistic evaluation the performance of Fin-R1 within financial applications. 3.2 Results Our comprehensive benchmarking evaluation covering multiple financial business scenarios, Fin-R1 demonstrated remarkable performance advantages despite its lightweight 7B parameter scale. It achieved an average score of 75.2, securing second place overall. Notably, Fin-R1 outperformed all 9 participating models of similar scale, with only 3.8-point performance gap compared to DeepSeekR1 (78.2). Furthermore, it surpassed DeepSeek-R1-Distill-Llama-70B (69.2) by 8.7 points. FinR1 achieved top rankings in two reasoning tasks: FinQA and ConvFinQA. It obtained scores of 76.0 and 85.0, respectively, surpassing all competing models. These results highlight the strong capabilities of Fin-R1 in both financial reasoning scenarios. Although Fin-R1 underwent specialized training primarily for FinQA and ConvFinQA, it exhibited significant performance improvements in other financial benchmarks compared with Qwen2.5-7B-Instrcut, including Ant_Finance, TFNS, and Finance-Instruct-500K. This suggests the model possesses robust cross-task generalization capabilities, further underscoring its effectiveness in diverse financial applications. Table 2: Evaluation results in different financial benchmarks. Model Parameters FinQA ConvFinQA Ant_Finance TFNS Finance-Instruct-500K Average DeepSeek-R1 Qwen-2.5-32B-Instruct DeepSeek-R1-Distill-Qwen-32B Fin-R1-SFT Qwen-2.5-14B-Instruct DeepSeek-R1-Distill-Llama-70B DeepSeek-R1-Distill-Qwen-14B Qwen-2.5-7B-Instruct DeepSeek-R1-Distill-Qwen-7B Fin-R1 671B 32B 32B 7B 14B 70B 14B 7B 7B 7B 71.0 72.0 70.0 73.0 68.0 68.0 62.0 60.0 55.0 76.0 82.0 78.0 72.0 81.0 77.0 74.0 73.0 66.0 62.0 85.0 90.0 84.0 87.0 76.0 84.0 84.0 82.0 85.0 71. 81.0 78.0 77.0 79.0 68.0 72.0 62.0 65.0 68.0 60.0 71.0 70.0 58.0 54.0 61.4 56.0 56.0 49.0 49.0 42.0 62.9 78.2 73.8 72.4 71.9 71.4 69.2 66.2 65.6 58. 75."
        },
        {
            "title": "4 Conclusion and Future work",
            "content": "We introduces the financial reasoning large language model Fin-R1, which effectively addresses three core challenges in financial AI applications: fragmented financial data, uncontrollable reasoning logic, and weak business generalization ability. By constructing the high-quality financial reasoning CoT dataset Fin-R1-Data followed by model training through SFT (Supervised Fine-Tuning) and RL (Reinforcement Learning), forms two-stage workflow framework within the financial domain, Fin-R1 achieves the state-of-the-art performance among evaluated models, scoring 85.0 and 76.0 in the ConvFinQA and FinQA, respectively. Our approach has significantly advanced the application of large language models in the financial domain. In the future, we will focus on advancing the integration and innovation of fintech field. On one hand, we will refine our architecture for financial multimodal scenarios and deepen its application exploration in cutting - edge areas, promoting the financial industrys intelligent and compliant development. On the other hand, we will drive the widespread adoption of large language models in finance, fostering deeper integration with financial applications to enhance risk management and regulatory compliance, ultimately expanding the practical utility of the model."
        },
        {
            "title": "Limitations",
            "content": "Although the model has achieved significant improvements in the financial domain, our study still has three main limitations: Limited training dataset coverage: The current training data of the model is confined to ConvFinQA and FinQA only, and it has not yet reached the satisfactory target. Future training will be expanded to more diverse financial datasets. Single-modality architecture limitation: The current model, based on pure text architecture, struggles to handle financial reports containing visual elements. We will consider multimodal extension solutions for financial chart understanding and reasoning in the future. Closed-scenario focus bias: The current evaluation mainly targets reasoning questions with clear standard answers, and open-ended financial text question answering has not been designed. Although we currently have the above limitations, in the future, we will redouble our efforts to address these potential shortcomings. We believe that these improvements will significantly enhance the models applicability and effectiveness in real-world financial scenarios."
        },
        {
            "title": "References",
            "content": "Alipay Team. Financial Evaluation Dataset, 2023. URL https://github.com/alipay/ financial_evaluation_dataset. Accessed: 2024-03-18. Anonymous. Twitter Financial News Sentiment, 2024. URL https://huggingface.co/ datasets/zeroshot/twitter-financial-news-sentiment. Accessed: 2024-03-18. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R. Routledge, and William Yang Wang. FinQA: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 36973711, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.emnlp-main.300. URL https://aclanthology.org/2021.emnlp-main.300. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. ConvFinQA: Exploring the chain of numerical reasoning in conversational finance In Proceedings of the 2022 Conference on Empirical Methods in Natuquestion answering. ral Language Processing, pp. 62796292, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.421. URL https://aclanthology.org/2022.emnlp-main.421. Zihan Dong, Xinyu Fan, and Zhiyuan Peng. Fnspid: comprehensive financial news dataset in time series. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 49184927, 2024. Duxiaoman DI Team. FinCorpus, 2023a. URL https://huggingface.co/datasets/ Duxiaoman-DI/FinCorpus. Accessed: 2024-03-18. Duxiaoman DI Team. FinanceIQ, 2023b. URL https://github.com/Duxiaoman-DI/ XuanYuan/tree/main/FinanceIQ. Accessed: 2024-03-18. Duxiaoman DI Team. XuanYuan-finx1-preview, 2024. URL https://github.com/ Duxiaoman-DI/XuanYuan. Accessed: 2024-03-18. Georgios Fatouros, Konstantinos Metaxas, John Soldatos, and Dimosthenis Kyriazis. Can large language models beat wall street? unveiling the potential of ai in stock selection. arXiv preprint arXiv:2401.03737, 2024. Joseph G. Flowers. Finance Instruct 500k, 2025. URL https://huggingface.co/datasets/ Josephgflowers/Finance-Instruct-500k. Accessed: 2025-03-18. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, and Liwen Zhang. Fineval: chinese financial domain knowledge evaluation benchmark for large language models. arXiv preprint arXiv:2308.09975, 2024. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, and Wei Lin. Alphafin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. arXiv preprint arXiv:2403.12582, 2024. Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. BBT-Fin: Comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark. arXiv preprint arXiv:2302.09432, 2023. URL https://arxiv.org/abs/2302.09432. Lukas Malik. Quant-trading-instruct, 2024. Accessed: 2024-03-18. OpenAI. Learning to reason with llms., 2024a. URL https://openai.com/index/ learning-to-reason-with-llms/. 11 OpenAI. Gpt-4o model documentation: Parameter configuration and data formats. https:// platform.openai.com/docs/models/gpt-4o, 2024b. Accessed: 2025-03-18. Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, and Qianqian Xie. Fino1: On the transferability of reasoning enhanced llms to finance. arXiv preprint arXiv:2502.08127, 2025. Qwen. Qwq: Reflect deeply on the boundaries of the unknown., 2024. URL https://github. com/QwenLM/QwQ. Qwen Team. Qwen 2.5 Technical Report. arXiv preprint, 2024. doi: 10.48550/arXiv.2412.15115. URL https://arxiv.org/abs/2412.15115. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, and Haowei Zhang et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint, 2024. URL https://arxiv.org/abs/2402.03300. Accessed: 2024-03-18. Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, and Qi Zhang. Ploutos: Towards interpretable stock movement prediction with financial large language model. arXiv preprint arXiv:2403.00782, 2024. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction, 2024a. URL https://arxiv.org/abs/2409.18839. Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Alpha-gpt: Human-ai interactive alpha mining for quantitative investment. arXiv preprint arXiv:2308.00016, 2023. Saizhuo Wang, Hang Yuan, Lionel Ni, and Jian Guo. Quantagent: Seeking holy grail in trading by self-improving large language model. arXiv preprint arXiv:2402.03755, 2024b. Qianqian Xie, Jimin Huang, Dong Li, Zhengyu Chen, Ruoyu Xiang, Mengxi Xiao, Yangyang Yu, Vijayasai Somasundaram, Kailai Yang, Chenhan Yuan, et al. Finnlp-agentscen-2024 shared task: Financial challenges in large language models-finllms. In Proceedings of the Eighth Financial Technology and Natural Language Processing and the 1st Agent AI for Scenario Planning, pp. 119126, 2024. Cheng Xu, Xiaofeng Hou, Jiacheng Liu, Chao Li, Tianhao Huang, and Xiaozhi et al. Zhu. MMBench: Benchmarking end-to-end multimodal dnns and understanding their hardware-software implications. In 2023 IEEE International Symposium on Workload Characterization (IISWC), pp. 154166, Reno, NV, USA, October 2023. IEEE. Congluo Xu, Zhaobin Liu, and Ziyang Li. Finarena: human-agent collaboration framework for financial market analysis and forecasting. arXiv preprint arXiv:2503.02692, 2025. Siqiao Xue, Tingting Chen, Fan Zhou, Qingyang Dai, Zhixuan Chu, and Hongyuan Mei. Famma: benchmark for financial domain multilingual multimodal question answering. arXiv preprint arXiv:2410.04526, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, and Denghui et al. Zhang. FinMem: Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design. In Proceedings of the AAAI Symposium Series 2024, pp. 595597, Austin, TX, USA, February 2024a. AAAI Press. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan Suchow, Rong Liu, Zhenyu Cui, Zhaozhuo Xu, et al. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. arXiv preprint arXiv:2407.06567, 2024b. 12 Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, et al. Finagent: multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. arXiv e-prints, pp. arXiv2402, 2024. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: survey. ACM Transactions on Intelligent Systems and Technology, 15(2):138, 2024a. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024b. Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. Universalner: Targeted distillation from large language models for open named entity recognition. arXiv preprint arXiv:2308.03279, 2023. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM : Fine-tuned large language models are scalable judges, 2024. URL https://openreview.net/forum?id=87YOFayjcG."
        },
        {
            "title": "A Appendix",
            "content": "A.1 The Prompt of data construct During the entire data construction process, we constructed prompts in three key processes respectively. Firstly, in the data distillation stage, we referred to the official prompt setting of DeepSeek - R1 and constructed the prompt shown in Figure 7. Secondly, in the first stage of data screening, for the task of \"LLM-as-Judge\", we compared various prompts and finally determined the optimal prompt. The details are shown in Figure 10. Finally, in order to obtain high-quality inference trajectories, in the second stage of data screening, we proposed seven indicators for evaluating the inference trajectories of the model and carefully constructed the prompt in Figure 8. Figure 7: The prompt of data distillation that we used A.2 The Prompt of reasoning selection To compare the scoring outcomes between human annotators and language models, we conducted supplementary experiments. Specifically, we randomly selected 20 data points from the dataset filtered in the initial preprocessing step and evaluated their reasoning performance using Qwen2.572B-Instruct and GPT-4o. The evaluation followed seven predefined judgment criteria. Each data point received score of 1 if its reasoning satisfied given criterion and 0 otherwise. The total score for each data point was obtained by summing across all criteria, resulting in range from 0 (minimum) to 7 (maximum). Given the scoring framework, we effectively employed binary scoring approach (0/1) at the criterion level. Figure 8: The Prompt for reasoning selection To establish reference baseline, human annotators independently scored the reasoning for the same data points. We then visualized the correlation between the scoring distributions of Qwen2.5-72BInstruct, GPT-4o, and human annotations using heatmaps (see Figure 9) to assess their alignment and discrepancies. The results show that Qwen2.5-72B-Instruct exhibits high concordance with human annotations, with most questions having correlation score of 1, and only minor deviations in few cases. In contrast, GPT-4o shows larger discrepancies, indicating lower alignment with human judgments. Based on these findings, we ultimately selected Qwen2.5-72B-Instruct as the scoring model for reasoning selection. A.3 The Prompt of Judging In the research on answer verification tasks based on LLM-as-Judge, we reveals that although the surface task format appears relatively simple (i.e. determining binary output 1 or 0 based on consistency between model-generated answers and reference answers), different prompt wording strategies significantly influence the performance of evaluation models. To quantitatively analyze this phenomenon, we randomly selected 100 sample instances from the FinQA dataset and conducted five repeated experiments for each prompt strategy to assess result stability. This process yielded 500 comparative results per prompt group, with model performance evaluated through consistency analysis against human-annotated results. Experiments were conducted using both GPT-4o and Qwen2.5-72B-Instruct. 14 (a) Qwen2.5-72B-Instruct vs. Human (b) GPT-4o vs. Human Figure 9: Heatmap comparison of reasoning scores between LLMs and human annotators. Figure 9a and 9b represent the correlation between the scores of Qwen2.5-72B-Instruct and GPT-4o with human scores. To systematically evaluate the impact of different prompting strategies on evaluation model performance, this study established quantitative evaluation metrics system comprising two core indicators: Classification Inaccuracy: defined as the proportion of samples where model judgments disagree with human annotations. Format Irregularity: reflecting the degree to which model outputs fail to strictly adhere to binary constraints (0/1). Through statistical analysis of 500 comparative results under each prompting strategy, the performance comparison data are shown in Table 3. The specific prompt formats include our used format as OF, the format where the content to be judged is at the end as CIE, the format with the original question passed in as WQ, the format with the original question passed in and the question-and-answer content placed at the end as CIE-WQ and the Chinese format as ZH. The prompts are shown in Figure 10, 11, 12, 13, 14. Inaccuracy Irregularity Format GPT-4o Qwen2.5-72B-Instruct GPT-4o Qwen2.5-72B-Instruct OF CIE WQ CIE-WQ ZH 2.8% 2.0% 6.0% 4.8% 5.2% 0.4% 2.0% 8.0% 9.6% 1.6% 0.8% 0.0% 3.6% 1.6% 0.0% 0.0% 0.0% 3.2% 3.2% 0.0% Table 3: Comparison of GPT-4o and Qwen2.5-72B-Instruct on answer judgment inaccuracy and irregularity across different prompt formats.The specific prompt formats include our used format as OF, the format where the content to be judged is at the end as CIE, the format with the original question passed in as WQ, the format with the original question passed in and the question-and-answer content placed at the end as CIE-WQ and the Chinese format as ZH. The systematic analysis based on experimental data reveals that different prompt strategies significantly influence the performance of evaluation models. We analyze the results as follows: Text positioning strategies demonstrate model-specific differences. GPT-4o shows stable performance under the CIE strategy when reference answers are post-positioned, with an inaccuracy rate of 2.0%, while Qwen2.5-72B-Instruct exhibits superior adaptation to the rule-preceding OF strategy, achieving an extremely high accuracy of 99.6%. Although incorporating original questions as contextual information theoretically enhances semantic comprehension, it substantially increases the format deviation rates (Irregularity). Under the WQ strategy, GPT-4o and Qwen2.5-72B-Instruct exhibit 3.6% and 3.2% Irregularity respectively. Manual verification identifies that format deviations predominantly occur in long-text samples, potentially due to input sequence elongation inducing model 15 hallucinations (e.g., Qwen2.5-72B-Instructs classification error rate under WQ strategy surges from baseline 0.4% to 8.0%). Cross-lingual testing indicates that Chinese prompts (ZH), while partially ensuring format compliance, yield significantly higher classification errors than optimal English strategies due to the English evaluation context. Compared with GPT-4o, Qwen2.5-72B-Instruct demonstrates better Chinese prompt adaptability. Base on the above analyses, we ultimately select Qwen2.5-72B-Instruct as the judge model. Moreover, we proposes the following optimizations: (1) Prompt engineering should account for model architecture characteristics, as employing model-specific prompt structures may enhance evaluation accuracy. (2) In answer verification tasks, unnecessary long contextual inputs should be minimized to effectively reduce format deviations. Figure 10: The prompt for judging the model answer that we used. 16 Figure 11: The prompt for judging the model answer that the content comes at the end. 17 Figure 12: The prompt for judging the model answer which is combined with the question. 18 Figure 13: The prompt for judging the model answer, which is combined with the question, comes at the end. 19 Figure 14: The Chinese prompt for judging the model answer."
        }
    ],
    "affiliations": [
        "FinStep",
        "Fudan University",
        "Shanghai University of Finance and Economics"
    ]
}