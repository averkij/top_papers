{
    "paper_title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "authors": [
        "Ruicheng Zhang",
        "Mingyang Zhang",
        "Jun Zhou",
        "Zhangrui Guo",
        "Xiaofan Liu",
        "Zunnan Xu",
        "Zhizhou Zhong",
        "Puxin Yan",
        "Haocheng Luo",
        "Xiu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 2 6 6 0 . 2 1 5 2 : r MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment Ruicheng Zhang1* Mingyang Zhang2* Zunnan Xu1,4 Zhizhou Zhong5 Jun Zhou1 Zhangrui Guo3 Xiaofan Liu3 Puxin Yan5 Haocheng Luo1,6 Xiu Li1 1Tsinghua University 2China University of Geosciences 3Sun Yat-sen University 4X Square Robot 5Hong Kong University of Science and Technology 6Central South University https://github.com/Richard-Zhang-AI/MIND-V"
        },
        {
            "title": "Abstract",
            "content": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixellevel synthesis through three core components: Semantic Reasoning Hub (SRH) that leverages pre-trained visionlanguage model for task planning; Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce GRPO reinforcement learning post-training phase guided by novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing scalable and controllable paradigm for embodied data synthesis. 1. Introduction Scalable robot learning within Embodied AI [3, 11, 39] is critically bottlenecked by the scarcity of high-quality, *Equal contribution Corresponding authors Project Lead Figure 1. Comprehensive comparison of MIND-V against SOTA models for long-horizon robotic video generation. diverse, and long-horizon robotic manipulation data [9]. Video generation models [17, 30, 36] offer promising solution by potentially synthesizing an infinite stream of robotic operation videos [9, 50], which can fuel imitation learning and even function as world models [5, 51] for embodied agents. However, generating high-quality, long-horizon robotic manipulation videos that adhere to human commands presents significant challenges, primarily in three areas: (1) Long-Horizon Coherence Challenge: This demands maintaining causal consistency and logical flow across sequence of interconnected sub-tasks, where single error can (2) Semanticcompromise the entire operation [12, 38]. to-Pixel Generation Challenge: This involves accurately translating abstract language commands into concrete spatiotemporal interactions in pixel space, which places immense demands on the models semantic understanding and (3) Physical Plausiinstruction-following fidelity [22]. bility Challenge: The generated output must ensure strict physical plausibility, requiring adherence to fundamental physical laws governing collision dynamics, object permanence, and interaction forces [24]. Existing methods fall 1 short of providing comprehensive solution to these challenges. On one hand, directly training video foundation models [5, 17, 30, 36] for long-horizon tasks often suffer from logical discontinuities and detail degradation, as they struggle to bridge the vast gap from abstract commands to concrete pixel-level execution. On the other hand, while trajectory-control-based generative models [9, 33, 34, 51] offer enhanced controllability, they do so at the cost of the autonomy and scalability essential for large-scale automated data generation. To this end, we draw inspiration from the hierarchical theory of human motor control in cognitive science [10, 25]. The human brain executes complex tasks through hierarchically collaborative process: high-level cognitive centers, such as the cerebral cortex, handle intent understanding and abstract planning, while low-level motor systems, like the cerebellum, translate these plans into precise muscle control. Specialized neural pathways bridge these layers, enabling the efficient translation of abstract intent into concrete physical action. Inspired by this paradigm, we introduce MIND-V, cognition-inspired hierarchical video generation model designed to synthesize physically plausible and logically coherent long-horizon robotic manipulation videos. Emulating the brains cognition-to-execution pipeline, MIND-V is built upon three core components: (1) Semantic Reasoning Hub (SRH) that performs high-level task understanding and planning based on pre-trained Vision-Language Model (VLM); (2) Behavioral Semantic Bridge (BSB) that acts as task-invariant link by translating abstract plans into structured, executable representations; and (3) Motor Video Generator (MVG) that synthesizes physically realistic manipulation videos conditioned on the BSB. By first generating the BSB through the causal reasoning of the SRH and then translating these symbolic representations into embodied actions with the MVG, MIND-V effectively bridges high-level reasoning with pixel-level synthesis through hierarchical collaboration. To the best of our knowledge, MIND-V is the first fully autonomous framework for generating long-horizon videos of embodied manipulation tasks. To further enhance physical plausibility, we design novel RL post-training phase [40] using Group Relative Policy Optimization (GRPO) [19], guided by Physical Foresight Coherence (PFC) Reward. The PFC reward leverages pre-trained world model as physics referee to quantify the dynamic coherence of generated videos, thereby aligning the model with physical laws. Furthermore, to mitigate error accumulation in long-horizon tasks, we introduce the Staged Visual Future Rollouts. This testtime optimization strategy decomposes the global planning problem into sequence of locally optimal decisions. At each sub-task transition, MIND-V performs an proposeverify-refine loop, where it simulates multiple future trajectories and selects the most coherent one to proceed. This staged approach effectively prevents the propagation of early-stage errors, significantly enhancing the robustness and success rate of the final video. Our main contributions are as follows: We propose the first hierarchical intelligent video generation framework, MIND-V, for long-horizon robotic manipulation. MIND-V effectively bridges the gap between high-level task planning and low-level pixel synthesis through three-tier architecture of brain (SRH), symbolic bridge (BSB), and video generator (MVG). We present the Staged Visual Future Rollouts, testtime optimization strategy that decomposes global longhorizon generation into series of locally optimal decisions. By performing an propose-verify-refine process at each sub-task, this method mitigates error accumulation and enhances generation robustness. We propose GRPO post-training alignment guided by novel Physical Foresight Coherence (PFC) reward, which leverages pre-trained world model to score the physical plausibility of generated dynamics in latent feature space, thereby steering the generator towards physically realistic outputs. Experiments demonstrate that MIND-V achieves state-ofthe-art performance in long-horizon robotic manipulation video generation, establishing scalable and controllable paradigm for embodied data generation. 2. Related Work 2.1. Video Generation for Robotic Manipulation The advancement of scalable embodied intelligence is critically dependent on large-scale, realistic data. However, collecting real-world robot data via human demonstration is time-consuming and labor-intensive process. Video generation models [17, 30, 36] have emerged as costeffective alternative for synthesizing photorealistic data for policy learning. Models like UniPi [8] and AVDC [16] frame robotic planning as text-to-video generation problem, where imagined visual futures are subsequently translated into executable actions via inverse dynamics models. WoW [5] and Robodreamer [50] structure video models as world models that learn latent physical dynamics from extensive interaction data to achieve compositional generalization. While these methods demonstrate strong semantic understanding, they lack fine-grained control over the precise execution of manipulation tasks [22]. This gap often leads to logical failures and physical inconsistencies, particularly in long-horizon scenarios. Another category of methods, such as IRASim [51] and RoboMaster [9], employs explicit trajectory guidance for more precise control. However, these approaches necessitate complex manual anno2 Figure 2. Overview of our hierarchical framework for long-horizon robotic manipulation video generation. Beginning in the cognitive core, the Semantic Reasoning Hub (SRH) decomposes high-level instruction into atomic sub-tasks and plans detailed trajectory for each. These plans are then encapsulated into our novel Behavioral Semantic Bridge (BSB), structured, domain-invariant intermediate representation that serves as precise blueprint for the Motor Video Generator (MVG). The MVG, conditional diffusion model, renders photorealistic videos that strictly adhere to the kinematic constraints defined in the BSB. At inference time, Staged Visual Future Rollouts provide propose-verify-refine loop for self-correction, ensuring local optimality at each stage to mitigate error accumulation. tations, including detailed motion paths and object masks, which severely limits their scalability and autonomy. In contrast, MIND-Vs hierarchical architecture autonomously decomposes high-level commands into executable instructions for the generator, enabling the generation of longhorizon high-fidelity manipulation videos without requiring extra manual guidance. 2.2. Controllable Video Generation Recent advancements in diffusion generation [41, 42] have spurred demand for controllable methods that can accurately translate user intent into visual content [22]. To this end, research has investigated spectrum of conditioning modalities to guide the video generation process, ranging from high-level semantic signals (text prompts [30]) to low-level structural inputs (masks [31, 34, 48, 49], trajectories [23, 33, 43, 46], sketches [13, 18, 21], and pose estimations [7, 14, 20, 35, 44, 45]). However, these modalities present fundamental trade-off between semantic abstraction and granular control. On the one hand, high-level semantic conditions like text can provide intuitive guidance but often falls short when dealing with complex multi-stage tasks. This abstraction can lead to semantic drift and diminished fidelity in long-horizon videos, as models struggle to map abstract language onto consistent sequence of physical interactions [22]. On the other hand, low-level signals such as trajectories, masks, or sketches as conditions can offer more precise spatial and temporal control but imposes heavy annotation burden, requiring users to specify detailed geometric constraints manually [22]. Our research addresses this dilemma by introducing hierarchical framework that connects high-level reasoning with pixellevel synthesis. The Semantic Reasoning Hub (SRH) is responsible for interpreting abstract user intentions, while the Behavioral Semantic Bridge (BSB) automatically converts them into structured, multi-dimensional representations for use by the generator. This approach achieves high semantic fidelity and precise control in long-term tasks, obviating the need for auxiliary manual annotations. 3. Method 3.1. Framework As illustrated in Figure 2, our hierarchical framework implements top-down pipeline from high-level cognition to concrete visual representation. First, the Semantic Reasoning Hub (SRH, 3.1.1) decomposes long-horizon task into atomic sub-tasks based on initial observations and user instructions. For each sub-task, the SRH then employs vision modules for affordance localization and trajectory planning to construct structured, domain-invariant intermediate representation, termed the Behavioral Semantic Bridge (BSB, 3.1.2). This representation guides the Motor Video Generator (MVG, 3.1.3) in synthesizing photorealistic video sequence. closed-loop feedback mechanism via Staged Visual Future Rollouts returns the generated results to the SRH for evaluation and potential re-planning. detailed description of each component follows. 3 Figure 3. Architecture of the Motor Video Generator (MVG). The MVG utilizes guidance from the BSB to synthesize spatiotemporally precise videos. The process initiates with encoding the BSBs semantic representation into the (c) Spatiotemporal Guidance Tensor, which embeds the visual features of the active agent along its planned trajectory across frames. This tensor is subsequently processed by the (b) Motion Embedding module to produce refined motion signal (G). Finally, this signal is injected into the (a) Latent Diffusion Transformer, conditioning each step of the denoising process to ensure the synthesized video exhibits strict fidelity to the intended motion. 3.1.1. Semantic Reasoning Hub (SRH) As the cognitive core of our framework, the SRH translates abstract semantics into actionable geometric signals. It accomplishes this by synergizing two key components: powerful pre-trained Vision-Language Model (VLM), such as Gemini-2.5-Pro [6], provides long-horizon planning and semantic reasoning, while an affordance-based visual localizer, like Affordance-R1 [32], grounds these plans with physical common sense. This synergy transforms the SRH into robust, physics-aware decision-making engine. Given an initial scene observation I0 and long-horizon task instruction (e.g., clean the desktop), the VLM first performs comprehensive semantic analysis of the scene and then decomposes into an ordered sequence of atomic sub-tasks. Each sub-task is defined by tuple SubTaski = {ActionTypei, Objecti, Destinationi}, which specifies the action primitive, the object of manipulation, and its target location, respectively. This structured decomposition provides symbolic foundation for precise downstream control. For each sub-task, the SRH leverages the affordance localizers vision-action alignment capabilities to precisely identify the objects segmentation mask (Mobj) and predict its functional interaction points (Pobj, e.g., the handle of cup). Based on this affordance data, the VLM plans physically plausible trajectory. The trajectory is generated using smooth curve function and discretized into sequence of points corresponding to the video frames. This process utilizes closed-loop refinement mechanism, where the VLM proposes candidate trajectory that is then visualized and returned for iterative evaluation until smooth and collision-free path is confirmed. 3.1.2. Behavioral Semantic Bridge (BSB) The BSB serves as the crucial bridge connecting high-level planning with pixel-level video synthesis. It is structured domain-invariant intermediate representation that translates symbolic outputs from the SRH into an actionable format for the MVG. The BSB is composed of three key elements: Object Representation: set of segmentation masks, including the manipulated object (Mobj) and generic robot arm mask (Mrob). VAE encoder compresses these masks into latent features, which are then injected at specific spatiotemporal locations during video generation to maintain consistent object identity. Decomposed Collaborative Trajectory: For each subtask, the trajectory is decomposed into three distinct phases: pre-interaction (Tpre, arm approaches the obinteraction (Tinteract, object is manipulated), and ject), post-interaction (Tpost, arm retracts). This decomposition clearly defines the primary active agent and its objective for each stage [9]. Phase Transition Points: triplet of frame indices (Fpre, Finteract, Fpost) that allocates specific duration to each of the three phases. This temporal allocation ensures natural motion dynamics and properly emphasizes the core physical interaction. By decoupling task logic from visual appearance, this design endows the BSB with domain invariance, significantly enhancing the models ability to generalize to novel environments and tasks. 3.1.3. Motor Video Generator (MVG) As illustrated in Figure 3, the MVG is conditional diffusion model built upon Diffusion Transformer (DiT) backbone [26], which tasks with synthesizing manipulation videos precisely conditioned on the control signals from the BSB. To achieve this control, the MVG first encodes the BSBs object representation into spatiotemporal guidance tensor of size (T ). This tensor dynamically embeds the visual features of the active agent (arm or manipulated object) onto its planned path across the time dimension. motion embedding module integrates this guidance into the DiT backbone during denosing process. The module employs spatiotemporal convolutions to encode the guidance tensor into feature representation G. Within each Transformer block, this representation is fused with the videos intermediate hidden state via additive fusion: hnew = + norm(G) G, (1) where norm() denotes Group Normalization to stabilize training. This continuous injection of kinematic constraints compels the model to adhere to the specified trajectory throughout the denoising process, yielding final video that is both spatiotemporally precise and visually coherent. 3.2. Test-Time Optimization via Staged Visual Future Rollouts Long-horizon task generation is plagued by error accumulation, where minor sub-task deviations in one sub-task cascade into overall task failure [39]. To mitigate this risk, we introduce novel test-time optimization strategy: Staged Visual Future Rollouts. This strategy operates through dynamic cycle of proposal, verification, and refinement executed at each sub-task transition. This approach effectively decomposes the global planning challenge into series of locally optimal decisions, thereby preventing catastrophic error propagation. As illustrated in Figure 2(d), the process begins with the SRH proposing set of semantically plausible yet strategically diverse candidate trajectories. These trajectories are then synthesized by the MVG into corresponding video clips, VK, each depicting potential future outcome. Subsequently, the VLM transitions into verification judge, evaluating each candidate future based on criteria including task success, physical plausibility, and visual quality. If the highest-scoring video Vtop meets predefined success threshold, it is selected and the process proceeds. If no candidate is satisfactory, the VLM provides structured textual feedback detailing the failure modes (e.g., end position error, object not grasped correctly). This feedback loop instructs the SRH to re-plan and propose refined set of masks and trajectories in the next iteration. This Figure 4. Physical Foresight Coherence (PFC) Reward. The PFC leverages frozen V-JEPA2 world model to predict the latent representation of future Target frames conditioned on past Context frames. The reward is the cosine similarity between this prediction and the ground-truth target latent, which measures the videos alignment with the world models learned physical dynamics. iterative cycle of propose-verify-refine transforms the SRH from simple feed-forward planner into proactive, selfcorrecting agent, significantly enhancing the robustness and success rate of long-horizon task generation. 3.3. MVG Training: From Supervised Fine-Tuning to Physical Alignment To ensure the MVG generates high-fidelity videos, we employ two-stage training paradigm. First, Supervised Fine-Tuning (SFT) adapts pre-trained video model to the robotics domain. This is followed by GRPO Reinforcement Learning (RL) post-training phase that aligns the generator with abstract objectives such as physical plausibility and aesthetic quality. 3.3.1. Supervised Fine-Tuning (SFT) The SFT phase adapts an open-source video model to learn the fundamental mapping from our structured Behavioral Semantic Bridge (BSB) representation to coherent video sequences. We fine-tune the model on real-world robotics dataset (e.g., Bridge v2 [29]) using ground-truth BSB annotations. The optimization follows the standard denoising objective, conditioned on the BSB: LSFT(θ) = E(x0,BSB)D,ϵN (0,I),t (cid:2)ϵ ϵθ(xt, t, BSB)2(cid:3) , (2) where xt is the noised video at timestep t. This stage provides high-quality initial policy πref for the subsequent alignment phase. Notably, the model only needs to be trained on short sub-task videos, as the hierarchical framework enables generalization to arbitrary long-horizon tasks. 5 3.3.2. GRPO Post-Training While SFT enforces motion adherence, it cannot guarantee physical plausibility or aesthetic quality of the generated videos, objectives that are ill-suited for conventional loss functions [19]. To bridge this gap, we introduce post-training alignment stage using Reinforcement Learning (RL). This stage models the denoising process as Markov Decision Process (MDP) and employs Group Relative Policy Optimization (GRPO) [19] for optimization. The optimization is guided by composite reward function R(x0), which evaluates the overall quality of generated video x0 by taking weighted sum of physics-based reward and an aesthetic reward: R(x0) = wp Rphysics(x0) + wa Raesthetic(x0). (3) Physical Foresight Coherence (PFC) Reward (Rphysics): This reward innovatively employs pre-trained visual world model, V-JEPA2 [1], to serve as an objective arbiter of physical plausibility (Figure 4). Pre-trained via self-supervision on large-scale real-world data and fine-tuned on robotics dataset, V-JEPA2 acquires an internal model of world dynamics, enabling it to accurately understand the existing state and predict future states within an abstract latent space. For each generated video, sliding window approach is employed to assess local physical plausibility. The consistency score si for each window is the cosine similarity between VJEPA2s latent prediction and the actual future: : si = simcos (cid:16) Pv(Ev(x(i) context)), Ev(x(i) target) (cid:17) , (4) where Ev and Pv are V-JEPA2s visual encoder and predictor, respectively. To concentrate optimization on the most egregious physical violations, we employ softmax-based weighting scheme to assign higher weights to windows with larger physical errors (1 si): Rphysics(x0) = Nw(cid:88) i=1 exp((1 si)/τ ) j=1 exp((1 sj)/τ ) (cid:80)Nw si. (5) Here, the temperature parameter τ controls the focus: lower τ value concentrates the reward on the single worstoffending window. By leveraging the world models robust understanding and prediction capability of physical evolution, the PFC reward transforms the evaluation from rigid assessment into targeted optimization of dynamic causal chains, significantly improving the physical consistency of generated actions [37]. Aesthetic Reward (Raesthetic): The aesthetic reward is provided by VLM, such as Qwen-VL [2], which performs tiered scoring. The VLM assesses each video for clarity, artifacts, and realism, assigning discrete integer score (e.g., 1-5), which yields stable and discriminative reward signal for optimization. 6 GRPO Optimization GRPO is an efficient, value-free policy optimization algorithm. At each optimization step, we sample group of videos {xi i=1 from the current policy πθ. The advantage ˆAi for each sample is then computed by normalizing its reward relative to the groups statistics: 0}G ˆAi = R(xi 0) mean({R(xj std({R(xj j=1) 0)}G 0)}G j=1) , (6) where mean() and std() are the mean and standard deviation of the rewards within the group. The policy is then updated by maximizing the GRPO objective function: JGRPO(θ) = (cid:34) 1 G (cid:88) (cid:18) i=1 ri(θ) ˆAi, clip(ri(θ), 1 ϵ, 1 + ϵ) ˆAi(cid:17) (cid:16) min βDKL(πθπref) , (7) (cid:19)(cid:35) where ri(θ) = πθ(xi 0) 0) is the importance sampling raπref (xi tio, ϵ is clipping hyperparameter, and the KL-divergence term regularizes the policy towards the SFT policy πref to mitigate reward hacking. This optimization process progressively aligns the generator towards higher physical fidelity and aesthetic quality while maintaining its adherence to kinematic conditioning. 4. Experiments 4.1. Experiment Settings Architecture and Training Our Semantic Reasoning Hub (SRH) employs Gemini-2.5 Pro API [6] as its core Vision-Language Model (VLM), complemented by Affordance-R1 [32] as visual localizer. The Motor Video Generator (MVG) is initialized from the pre-trained CogVideoX-5B [36] architecture. Experiments are conducted on the Bridge V2 [29] dataset, following the data processing protocol established in [9]. We adopt resolution of 480 640 pixels and video length of 37 frames per sub-task for both training and inference. The model underwent two training stages: (1) Supervised fine-tuning (SFT) for 30,000 steps with an AdamW optimizer and learning rate of 2 105; and (2) GRPO post-training for 1,500 iterations at learning rate of 5 105. At inference time, MIND-V can generate 111-frame long-horizon video (comprising three sub-tasks) in approximately 180 seconds while consuming around 50 GB of VRAM. Theoretically, due to its autoregressive, sub-task-based architecture, MIND-V can generate arbitrarily long task-sequence videos with only linear increase in computational cost. All experiments are conducted on four NVIDIA H200 GPUs. Additional implementation details are provided in the supplementary material. Evaluation Protocol and Metrics The evaluation is performed on test set of 108 samples. This set comprises Figure 5. Qualitative comparison of long-horizon robotic manipulation video generation. The baseline models exhibit significant deficiencies, including logical inconsistencies, physical implausibility, and poor semantic grounding. In contrast, MIND-V successfully executes long-horizon instructions with high visual quality and physical fidelity. This validates the efficacy of our hierarchical architecture, which decouples high-level reasoning from pixel-level synthesis to ensure robust long-horizon coherence and spatiotemporal precision. scenes from the Bridge V2 test set [29] and unseen scenes sourced from the web. Recognizing that different task horizons demand different evaluation criteria, we adopted bifurcated protocol. For short-horizon tasks, our evaluation focuses exclusively on visual quality, where we use V-Bench [47] for evaluation. For long-horizon tasks, we introduce user study and two additional metrics: (1) physical plausibility, which is quantified by our Physical Foresight Coherence (PFC) score, as detailed in Section 3.3.2; and (2) Task Success Rate, which measures the average success rate across all sub-tasks of the entire long-horizon task. Baselines and Comparative Setup For short-horizon task, we benchmark MIND-V against both trajectorybased methods (IRASim [51], MotionCtrl [33], DragAnything [34], Tora [46]) and trajectory-free world models (Robodreamer [50], WoW [5], Wan2.2 [30], HunyuanVideo [17]). It is crucial to note that trajectory-based models receive privileged information at inference time, such as manual trajectories, masks, or anchor points, which is unavailable to our trajectory-free approach. This distinction underscores the greater complexity of the task our model addresses. For long-horizon task, which emphasizes complex planning and reasoning without explicit guidance, our comparison is focused on SOTA trajectory-free models (Robodreamer [50], WoW [5] and WAN2.2 [30], HunyuanVideo [17]). Each long-horizon task in our benchmark is composed of sequence of 2 to 4 sub-tasks, designed to probe the limits of long-horizon planning and generation. 4.2. Qualitative and Quantitative Comparison As evidenced by the results in Figure 5, Table 1, and Table 2, MIND-V consistently outperforms state-of-the-art methods across both shortand long-horizon tasks. While baseline models may achieve high scores on visual quality metrics, they consistently struggle with task execution and long-horizon coherence  (Table 2)  . These models exhibit 7 Table 1. Visual quality evaluation on short-horizon and long-horizon tasks. Our model is benchmarked against consistent set of state-of-the-art methods across both task types. Higher values are better and highlighted in shades of green. Method Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness Subject Consistency Background Consistency MotionCtrl [33] IRASim [51] DragAnything [34] Tora [46] RoboMaster [9] Robodreamer [50] WoW-1-DiT-7B [5] MIND-V (Ours) Robodreamer [50] WoW-1-DiT-7B [5] WoW-1-Wan-14B [5] Wan2.2-14B [30] HunyuanVideo [17] MIND-V (Ours) 0.491 0.504 0.500 0.509 0.502 0.511 0.522 0. 0.464 0.476 0.498 0.508 0.487 0.504 Short-horizon Task Evaluation 0.977 0.665 0.979 0.676 0.980 0.679 0.981 0.670 0.688 0.982 0.977 0.680 0.982 0.682 0.986 0.684 0.972 0.986 0.983 0.984 0.981 0.976 0.985 0.991 Long-horizon Task Evaluation 0.910 0.628 0.922 0.635 0.935 0.652 0.661 0.948 0.928 0.643 0.955 0.658 0.918 0.929 0.950 0.951 0.954 0.953 0.915 0.929 0.935 0.922 0.937 0.930 0.933 0. 0.839 0.851 0.874 0.885 0.862 0.896 0.942 0.957 0.957 0.961 0.950 0.945 0.960 0.963 0.885 0.894 0.906 0.913 0.900 0.924 Table 2. Comprehensive evaluation of long-horizon tasks on PFC Score, Task Completion Rates, and User Preferences. Higher values are better and highlighted in shades of green. Table 3. Ablation study. All experiments are conducted on longhorizon tasks, evaluating visual quality and functional correctness. PFC Score Task Success Rate (%) User Study (Preference %) Model Variant Visual Quality Functional Correctness Aesthetic Quality Imaging Quality PFC Score Sub-task Avg. Rate (%) Method Robodreamer [50] WoW-1-DiT-7B [5] WoW-1-Wan-14B [5] Wan2.2-14B [30] HunyuanVideo [17] MIND-V (Ours) 0.418 0.423 0.420 0.402 0.411 0.445 27.5 32.2 34.7 11.1 9. 61.3 6.7 16.7 23.3 0 3.3 46.7 (a) w/o GRPO (b) w/o Affordance (c) w/o Staged Rollouts MIND-V (Full Model) 0.491 0.498 0. 0.504 0.675 0.680 0.671 0.684 0.419 0.436 0.433 0.445 60.1 45.5 32. 61.3 heres to physical laws. This systematic framework of cognition from execution effectively prevents the error accumulation that plagues other methods, enabling the generation of coherent and physically plausible manipulation sequences. 4.3. Ablation Study To validate the contributions of our frameworks core components, we benchmark our full model against three ablated variants on long-horizon tasks: (a) one trained only with SFT without GRPO post-training (w/o GRPO); (b) one replacing our affordance localizer [32] with YOLOWorld [4] and SAM2 [27] pipeline (w/o Affordance); and (c) one disabling the test-time optimization mechanism (w/o Staged Rollouts). As shown in Table 3, the full model consistently outperforms all ablated versions, validating the importance of each component. The removal of GRPO significantly degrades the PFC Score, confirming the efficacy of our RLbased alignment for enhancing physical plausibility. Replacing the affordance module leads to substantial drop in task success rates, highlighting the criticality of functional grounding for successful manipulation. Disabling Staged Rollouts results in the most pronounced performance degraFigure 6. Visualization of the SRH Planning. critical deficiencies (Figure 5), including logical failures, such as hallucinating unprompted actions; physical implausibility, like objects spontaneously disappearing; and poor semantic grounding, which leads to inaction or incorrect object manipulation. In contrast, MIND-V successfully executes these longhorizon tasks, demonstrating robust adherence to both instructions and physical principles. This superior performance is attributed to our models hierarchical design: The Semantic Reasoning Hub (SRH) and Behavioral Semantic Bridge (BSB) collaborate to decompose user instructions into an explicit, executable plan (Figure 6), which mitigates the risk of semantic drift common in end-to-end models. Subsequently, the Motor Video Generator (MVG), guided by this plan and fine-tuned with our Physical Foresight Coherence (PFC) reward, ensures the resulting synthesis ad8 dation, underscoring its crucial function in mitigating error accumulation during long-horizon generation. 5. Conclusion This work introduces MIND-V, the first cognition-inspired hierarchical framework for generating long-horizon videos of robotic manipulation. The architecture integrates Semantic Reasoning Hub (SRH), the Behavioral Semantic Bridge (BSB), and the Motor Video Generator (MVG). By decoupling high-level semantic reasoning from low-level pixel synthesis, our model successfully addresses the critical challenges of long-horizon coherence, semantic grounding, and physical plausibility. The efficacy of our approach is further enhanced by two key innovations: GRPO post-training phase with Physical Foresight Coherence (PFC) reward that aligns video generation with physical laws; and Staged Visual Future Rollouts, test-time strategy that mitigates error accumulation in longhorizon task. Comprehensive experiments demonstrate that MIND-V not only achieves state-of-the-art performance but also establishes scalable and fully autonomous paradigm for generating high-fidelity embodied AI data."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 6 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 6, 14 [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. 1 [4] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openvocabulary object detection. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 8 [5] Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Zhiyuan Qin, Kevin Zhang, Wanxin Tian, Kuangzhi Ge, Hao Li, et al. Wow: Towards world omniscient world model through embodied interaction. arXiv preprint arXiv:2509.22642, 2025. 1, 2, 7, 8, 15 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, and Sam Petulla. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. 4, 6, 12 [7] Yanbo Ding, Xirui Hu, Zhizhi Guo, Chi Zhang, and Yali Wang. Mtvcrafter: 4d motion tokenization for open-world human image animation. arXiv preprint arXiv:2505.10238, 2025. 3 [8] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation, 2023. 2 [9] Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, and Dahua Lin. Learning video generation for robotic manipulation with collaborative trajectory control. arXiv preprint arXiv:2506.01943, 2025. 1, 2, 4, 6, 8, 13 [10] Sten Grillner. Neurobiological bases of rhythmic motor acts in vertebrates. Science, 228(4696):143149, 1985. 2 [11] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. 1 [12] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. [13] Lifan Jiang, Shuang Chen, Boxi Wu, Xiaotong Guan, and Jiahui Zhang. Vidsketch: Hand-drawn sketch-driven video generation with diffusion control, 2025. 3 [14] Xiaoyu Jin, Zunnan Xu, Mingwen Ou, and Wenming Yang. Alignment is all you need: training-free augmentation strategy for pose-guided video generation. arXiv preprint arXiv:2408.16506, 2024. 3 [15] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 18 [16] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua Tenenbaum. Learning to Act from Actionless Videos through Dense Correspondences. arXiv:2310.08576, 2023. 2 [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2, 7, 8, [18] Feng-Lin Liu, Hongbo Fu, Xintao Wang, Weicai Ye, Pengfei Wan, Di Zhang, and Lin Gao. Sketchvideo: Sketch-based video generation and editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2025. 3 [19] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 2, 6 [20] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 3 [21] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 3 [22] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. 1, 2, 3 [23] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. [24] Siwei Meng, Yawei Luo, and Ping Liu. Grounding creativity in physics: brief survey of physical priors in aigc. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, 2025. 1 [25] Josh Merel, Matthew Botvinick, and Greg Wayne. Hierarchical motor control in mammals and machines. Nature Communications, 10(1):5489, 2019. 2 [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 5, 18 [27] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 8, 14, 19 [28] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 14 [29] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. 5, 6, 7, [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 7, 8, 16 [31] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. 2024. 3 [32] Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, and Yuexin Ma. Affordance-r1: Reinforcement learning for generalizable affordance reasoning in multimodal large language model. arXiv preprint arXiv:2508.06206, 2025. 4, 6, 8, 19 [33] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 3, 7, 8 [34] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation, 2024. 2, 3, 7, [35] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1590915919, 2025. 3 [36] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 6, 18 [37] Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall, Reyhane Askari-Hemmat, Xiaochuang Han, Nicolas Ballas, Michal Drozdzal, and Adriana Romero-Soriano. Improving the physics of video generation with vjepa-2 reward signal, 2025. 6 [38] Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, and Zach Xu. Igniting vlms toward the embodied space. arXiv preprint arXiv:2509.11766, 2025. 1 [39] Kaidong Zhang, Rongtao Xu, Pengzhen Ren, Junfan Lin, Hefeng Wu, Liang Lin, and Xiaodan Liang. Robridge: hierarchical architecture bridging cognition and execution for general robotic manipulation, 2025. 1, 5 [40] Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Hoi Fan Au, Haowei Guo, and Puxin Yan. Marlmambacontour: Unleashing multi-agent deep reinforcement learning for active contour optimization in medical image segmentation. In Proceedings of the 33rd ACM International 10 Conference on Multimedia, page 78157824, New York, NY, USA, 2025. Association for Computing Machinery. 2 [41] Ruicheng Zhang, Kanghui Tian, Zeyu Zhang, Qixiang Liu, and Zhi Jin. Fdg-diff: Frequency-domain-guided diffusion framework for compressed hazy image restoration, 2025. 3 [42] Ruicheng Zhang, Puxin Yan, Zeyu Zhang, Yicheng Chang, Hongyi Chen, and Zhi Jin. Rpd-diff: Region-adaptive physics-guided diffusion model for visibility enhancement under dense and non-uniform haze, 2025. 3 [43] Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, and Xiu Li. Zero-shot 3daware trajectory-guided image-to-video generation via testtime training, 2025. 3 [44] Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, and Yansong Tang. Flexiact: Towards flexible acarXiv preprint tion control in heterogeneous scenarios. arXiv:2505.03730, 2025. 3 [45] Yue Zhang, Zhizhou Zhong, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, and Wenjiang Zhou. Musetalk: Real-time high-fidelity video dubbing via spatio-temporal sampling. arXiv preprint arXiv:2410.10122, 2025. 3 [46] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 20632073, 2025. 3, 7, [47] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 7 [48] Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Jiasun Feng, Lupeng Liu, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Huan Li, Shuiyang Mao, Wei Liu, and Wenhan Luo. Anytalker: Scaling multi-person talking video generation with interactivity refinement. arXiv preprint arXiv:2511.23475, 2025. 3 [49] Jun Zhou, Jiahao Li, Zunnan Xu, Hanhui Li, Yiji Cheng, Fa-Ting Hong, Qin Lin, Qinglin Lu, and Xiaodan Liang. Fireedit: Fine-grained instruction-based image editing via region-aware vision language model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1309313103, 2025. 3 [50] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination, 2024. 1, 2, 7, 8 [51] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: Learning interactive realrobot action simulators. arXiv:2406.12802, 2024. 1, 2, 7, 8 MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Analysis of Computational Cost and Hyperparameters This section provides detailed analysis of the computational cost and key hyperparameters of the MIND-V framework. We first examine the frameworks scalability with respect to task length and then present an ablation study on the number of rollout samples (K) used in our test-time optimization strategy (Section 3.2). 6.1. Scalability with Task Length To validate the efficiency of our framework for long-horizon tasks, we measure the generation time and peak VRAM usage while varying the number of sub-tasks from one to three. The key metrics are summarized in Table 4 and visualized in Figure 7. Our findings confirm two critical properties of the proposed design. First, the total generation time scales linearly with the number of sub-tasks. The average time per subtask remains constant at approximately 60 seconds, demonstrating that our frameworks computational time scales predictably with task length (This is reference value using the Gemini-2.5 API [6]. The inference speed of the SRH is influenced by multiple factors, including VLM API response time and network latency. Reported times represent pure inference duration, excluding network transmission latency.). Second, and crucially, the peak VRAM usage remains constant regardless of the number of sub-tasks. As shown by the consistently sized circles in Figure 7, the peak VRAM remains constant at approximately 70 Gb. This constant memory footprint is direct benefit of our hierarchical and autoregressive design, where memory is allocated for single sub-task and subsequently reused. This design makes our approach highly memory-efficient for generating very long-horizon videos. The data in Table 4 also reveals the internal distribution of resources. The video generation is the most resourceintensive stage, accounting for the majority of the execution time (approx. 65-70%) and VRAM (approx. 86%). The SRH planning stage, in contrast, constitutes smaller and stable overhead. 6.2. Analysis on the Number of Rollout Samples (K) The Staged Visual Future Rollouts mechanism (Section 3.2) is governed by key hyperparameter, K, which defines the number of candidate videos generated at each sub-task transition. While larger increases the probability of finding successful trajectory, it also incurs greater computational cost. To analyze this trade-off, we conduct an ablation study by varying from 1 to 5. The results, visualized in Figure 8, clearly illustrate the relationship between performance gains and computational cost. As shown in the performance radar chart (Figure 8, left), significant performance uplift is observed as increases from 1 to 3, demonstrating the effectiveness of the rollout mechanism in filtering out suboptimal trajectories. For instance, the Task Success Rate jumps from modest 35.2% at = 1 to 61.3% at = 3, an absolute increase of 26.1%. However, this trend exhibits sharply diminishing returns, with only marginal gains when increasing from 3 to 5. In stark contrast, the computational cost, particularly Peak VRAM, scales unfavorably with larger as shown in the bar chart (Figure 8, right). Peak VRAM consumption, for example, nearly doubles from 70.1 Gb at = 3 to 122.0 Gb at = 5. This analysis confirms that = 3 strikes an optimal balance between functional correctness and computational efficiency. Therefore, we adopt = 3 as the default setting for all experiments, as it delivers the best performance-per-cost trade-off. Figure 7. Scalability of MIND-V. Total generation time (Y-axis) scales linearly with the number of sub-tasks (X-axis). Circle size represents peak VRAM, which remains constant, demonstrating the memory efficiency of our approach. 7. Dataset Construction The Supervised Fine-Tuning (SFT) stage (Section 3.3.1) of our training paradigm requires large-scale dataset of robotic manipulation videos annotated with our structured 12 Table 4. Analysis of computational cost as function of the number of sub-tasks. We report total time, average time per sub-task, peak VRAM usage, and the percentage distribution of time and VRAM across the SRH planning and MVG generation stages. No. of Total Time Avg. Time Peak VRAM Time Dist. (%) VRAM Dist. (%) Sub-tasks (s) per Sub-task (s) (Gb) Plan (SRH) Gen (MVG) Plan (SRH) Gen (MVG) 1 2 3 60.24 123.02 181.55 30.14 30.60 30. 70.12 70.12 70.12 36.5% 34.3% 32.4% 63.5% 65.7% 67.6% 14.1% 14.4% 14.0% 85.9% 85.6% 86.0% Figure 8. Analysis of the trade-off for the number of rollout samples (K). (Left) The performance radar chart shows that the overall performance area expands significantly up to K=3 but exhibits diminishing returns thereafter. (Right) The cost chart shows that both time and Peak VRAM increase steadily with K, with memory cost escalating significantly. K=3 (highlighted) is chosen as the optimal balance. Table 5. Ablation study on the number of rollout samples (K). We evaluate the impact of varying on functional correctness, visual quality, and computational cost. The setting = 3 (highlighted) achieves the best balance between performance and efficiency. 1 2 3 4 5 Cost Performance Time (s) Peak VRAM (Gb) Task Success Rate (%) PFC Score Aesthetic Quality Imaging Quality Motion Smoothness Subject Consistency 144.5 167.1 181.6 199.7 223.4 31.8 50.5 70.1 94.1 122.0 35.2 51.7 61.3 62.1 62.5 0.405 0.428 0.445 0.447 0.450 0.471 0.492 0.504 0.506 0. 0.660 0.675 0.684 0.685 0.686 0.931 0.946 0.953 0.954 0.954 0.865 0.884 0.896 0.897 0.898 Behavioral Semantic Bridge (BSB) representation. To this end, we developed an automated pipeline to generate ground-truth BSB annotations from the raw Bridge V2 dataset [29] following the data processing protocol established in [9], as illustrated in Figure 9. This pipeline comprises two primary stages: Object Representation Generation and Trajectory Decomposition. 7.1. Object Representation Generation This stage generates the Object Representation (segmentation masks) for the manipulated object (Mobj) and the robot arm (Mrob) by grounding the natural language instruction in the visual scene. For each sub-task video and its corresponding instruction (e.g., pick up the red block), the process is as follows: 13 Figure 9. Overview of our automated BSB annotation pipeline. VLM first extracts the target object from the language prompt, which is then used by Grounded SAM2 [28] to generate the (1) Object Representation (masks). Concurrently, trajectory tracking is performed on the object and gripper masks. The trajectory is partitioned based on the objects motion to produce the (2) Decomposed Collaborative Trajectory and (3) Phase Transition Points. These components collectively form the structured BSB annotation used for SFT. 1. Object Identification: pre-trained Vision-Language Model (VLM), such as Qwen-VL [2], extracts the noun phrase corresponding to the object of manipulation (e.g., red block) from the instruction. 2. Language-Grounded Segmentation: The extracted noun phrase serves as text prompt for Grounded SAM2 [28], an open-vocabulary segmentation model, which generates the pixel-wise segmentation mask for the target object (Mobj) in the initial frame. 3. Robot Arm Segmentation: For the robot arm mask (Mrob), we use pre-defined, comprehensive mask of the manipulator, as its visual features are consistent across all tasks in the dataset. 7.2. Trajectory Decomposition and Phase Segmentation This stage tracks and partitions the trajectories of the robot and the object into three meaningful phases: preinteraction, interaction, and post-interaction. The process is based on the motion state of the manipulated object. We first employ video object tracking model, in this case SAM2 [27], to track the segmentation masks of both the target object and the robot gripper throughout the video sequence. The center point of these masks forms the raw trajectory data. The Decomposed Collaborative Trajectory is then segmented based on the objects motion: 1. Pre-interaction Phase (Tpre): This phase is defined as the sequence of frames from the start of the sub-task until the target object begins to move. 2. Interaction Phase (Tinteract): This phase covers the frames during which the target object is in motion. 3. Post-interaction Phase (Tpost): This phase begins once the target object comes to rest again and continues until the end of the sub-task. The trajectories of the robot arm and the manipulated object are determined by the paths of their respective mask centroids during these phases. The Phase Transition Points (Fpre, Finteract, Fpost) are defined by the start and end points of the objects motion. To ensure data quality, failure cases from the automated pipeline, such as incorrect grounding or trajectory errors, are flagged for manual correction by human annotators. Figure 10. Qualitative comparison on complex long-horizon task. The model is instructed to first place blue grapes onto chartreuse plate, and then place spoon into metallic pot. (Top) The baseline model, WoW-14B [5], exhibits catastrophic failure in long-horizon reasoning. In Sub-task 1, the grapes levitate without being touched, clear physical violation. In Sub-task 2, it demonstrates severe semantic grounding error by incorrectly interacting with the plate instead of the instructed spoon, resulting in complete breakdown of logical coherence. (Bottom) In stark contrast, MIND-V successfully executes the full sequence, correctly completing both sub-tasks as instructed. This result validates the efficacy of our hierarchical architecture; the SRHs explicit planning and the BSBs structured guidance prevent the semantic drift and error accumulation that plague the baseline, ensuring robust execution of multi-step instructions. 8. Additional Visual Results This section presents comprehensive qualitative evaluation comparing MIND-V against state-of-the-art baseline models. We analyze performance across three distinct regimes, including multi-stage long-horizon manipulation tasks shown in Figure 10, atomic short-horizon interactions illustrated in Figure 11, and generalization to complex outof-distribution (OOD) scenarios with diverse action primitives as depicted in Figure 13. These visualizations substantiate our quantitative findings and highlight the practical efficacy of our hierarchical architecture. Long-horizon Tasks As illustrated in Figure 10, baseline models exhibit clear breakdown in long-horizon tasks. They not only violate physical common sense within single sub-task but, more critically, fail to maintain causal consistency across steps. The baselines attempt to interact with an object from previous sub-task highlights profound failure in long-range planning and semantic grounding. In contrast, MIND-V maintains robust logical coherence throughout the sequence. This success stems from our the Semantic Reasoning Hub (SRH) hierarchical design: explicitly decomposes complex instructions into structured sub-tasks, while the Staged Visual Future Rollouts mechanism (Section 3.2) actively filters physically implausible transitions. Short-horizon Tasks The short-horizon examples in Figure 11 further highlight the nuances of our approach. Even when baselines correctly identify target objects, they may generate physically implausible interactions, such as objects spontaneously appearing or levitating without contact, which our GRPO-based physical alignment (Section 3.3.2) successfully mitigates. Furthermore, MINDV demonstrates superior reasoning on abstract commands (e.g., Clean the floor), successfully inferring the correct sequence of primitive actions like grasping cloth and wiping. This showcases the SRHs capability to translate highlevel, abstract goals into concrete, executable plansa key differentiator from monolithic models that often fail on such tasks. Complex OOD Scenarios and Diverse Interactive Actions. Figure 13 demonstrates the robust generalization capabilities of MIND-V. Panel (a) illustrates the models precision in highly cluttered and visually diverse OOD en15 Figure 11. Qualitative comparison on short-horizon tasks. This figure illustrates performance on two distinct single-step instructions. (Top) For Put the mushrooms into the metal pot, the baseline (HunyuanVideo [17]) exhibits physical implausibility, with the mushroom clipping through the pots rim. MIND-V, in contrast, generates physically plausible interaction. (Bottom) For the more abstract instruction Clean the floor, the baseline (Wan-14B [30]) fails to take any action, demonstrating lack of semantic grounding. MIND-V correctly interprets the instruction, grasps the cloth, and performs wiping motion, showcasing its superior planning and reasoning capabilities. strictly static despite high-frequency textures. This stability is attributed to the Behavioral Semantic Bridge (BSB), which injects object-specific masks into the diffusion process to separate the manipulated entity from the complex environment. Furthermore, Panel (b) showcases the ability to execute diverse interactions beyond simple pick-andplace tasks. MIND-V successfully manipulates articulated objects, such as opening cabinet or closing microwave, and handles deformable materials like folding tablecloth. These actions require precise kinematic planning and affordance understanding, which are enabled by the SRHs integration of VLM reasoning with affordance localization. 9. Analysis of GRPO Post-Training To provide insight into the stability and effectiveness of our GRPO post-training phase (Section 3.3.2), we visualize key training metrics on an epoch-by-epoch basis in Figure 12. For clarity, the plots show the average value of each metric at the end of each epoch. The curves illustrate clear, convergent trends for the policy loss (reward signal), approximate KL divergence, and clip fraction."
        },
        {
            "title": "The training curves demonstrate a stable and effective",
            "content": "optimization process: Figure 12. GRPO post-training epoch-level dynamics. Visualization of key metrics, averaged at the end of each epoch. The reward signal shows clear upward trend, while the KL divergence and clip fraction both exhibit stable convergence, indicating an effective and well-behaved optimization process. vironments. Whether distinguishing single piece of bread amidst dense breakfast spread, or operating within an artistic scene resembling an oil painting, the model accurately isolates the target. Crucially, the background remains 16 Figure 13. Generalization to complex scenarios and diverse manipulation skills. Panel (a) demonstrates the robustness of MIND-V in out-of-distribution (OOD) scenarios. The model accurately isolates and manipulates targets in cluttered environments, such as grabbing bread from full table or picking dumpling, as well as in stylistically distinct scenes like picking peaches in an artistic setting. The BSB representation ensures precise control without disturbing the background fidelity. Panel (b) highlights variety of interactive actions where the model leverages affordance-aware reasoning to execute physics-compliant interactions. This includes manipulating articulated objects like opening cabinet or closing microwave, as well as handling deformable materials such as folding tablecloth. Convergent Reward Optimization: The average reward signal per epoch exhibits clear and steady upward trend. This indicates that the policy is successfully and progressively optimizing for the desired objectives of physical plausibility and aesthetic quality. Stable Policy Updates: The average approximate KL divergence shows clear downward trend, gradually converging towards low, stable value. This signifies that the policy updates become smaller and more refined as training progresses, demonstrating stable convergence without drastic policy shifts. Well-Calibrated Optimization: The average clip fraction also steadily decreases, indicating that as the policy improves, fewer updates are being clipped. This confirms that the optimization is proceeding smoothly and that the learning rate and clipping hyperparameters are wellcalibrated. Collectively, these convergent trends validate that our 17 Table 6. Detailed architecture and data flow of the Motor Video Generator (MVG). The table shows the transformation of tensor shapes from the input video and BSB guidance through each major component. Notations: B=Batch Size, C=Channels, T=Temporal Length, H=Height, W=Width, L=Sequence Length, D=Embedding Dimension. Component Module Input Shape Output Shape Key Hyperparameters 3D VAE Guidance Embedding Encoder Decoder Guidance Tensor [B, 3, T, H, W] [B, 16, T/4, H/8, W/8] [B, 16, T/4, H/8, W/8] [B, 3, T, H, W] Latent Channels: 16 Symmetrical to Encoder BSB [B, 128, T/4, H/8, W/8] Encodes BSB masks & trajectories into dense tensor. Spatial Conv [B, 128, T/4, H/8, W/8] [B, 480, T/4, H/8, W/8] Kernel: 3x3, Stride: 1 Temporal Conv [B, 480, T/4, H/8, W/8] [B, 1920, T/4, H/8, W/8] Kernel: 3 (1D), Stride: 1 Patch Embedding [B, 16, T/4, H/8, W/8] [L, D] Patch Size: 2x2x2, Hidden Dim (D): 1920 DiT Backbone (30 Blocks) Positional Encoding Timestep Embedding Transformer Blocks [L, D] Scalar [L, D] [L, D] [1, 512] [L, D] Type: 3D Sinusoidal - Layers: 30, Attention Heads: 30, Head Dim: 64 Scheduler DDIM Scalar Noise Schedule Timesteps: 50, Schedule: Linear GRPO post-training is stable and effective process. It progressively steers the generator towards higher physical fidelity and aesthetic quality, successfully aligning the model with abstract, hard-to-define objectives. 10. Network Architecture Details This section provides detailed specification of the Motor Video Generator (MVG) architecture and its data flow. The MVG is built upon the CogVideoX-5B [36] model, which features 3D-VAE and Diffusion Transformer (DiT) [26] backbone. Our primary modification is the conditioning mechanism, which injects guidance from the Behavioral Semantic Bridge (BSB) for precise control. The key specifications and tensor shape transformations are provided in Table 6, with further elaboration below. The MVG operates within latent space defined by 3D Variational Autoencoder (VAE) [15]. As shown in Table 6, the VAEs encoder first performs spatiotemporal compression on an input video of shape [B, 3, T, H, W] to produce compact latent representation of shape [B, 16, T/4, H/8, W/8]. The entire diffusion process is performed within this latent space, with the VAEs decoder reconstructing the final denoised latent back into pixel space. The core of the MVG is 30-layer Diffusion Transformer (DiT) with hidden dimension of 1920. The latent video is first partitioned into sequence of 2x2x2 patches, which are then linearly embedded into tokens. This sequence is then augmented with 3D sinusoidal positional encodings and diffusion timestep embeddings before being processed by the Transformer blocks. The conditioning mechanism integrates BSB guidance directly into the DiT backbone via Guidance Embedding module. First, the structured information from the BSB is converted into dense Spatiotemporal Guidance Tensor. As detailed in Table 6, this tensor is then processed by series of spatiotemporal convolutionsa 3x3 spatial convolution followed by 1D temporal convolutionto produce 1920-dimensional guidance signal, G. This signal, which retains the same spatiotemporal dimensions as the latent video, is injected into the DiT backbone via additive fusion. The injection occurs within the evennumbered Transformer blocks (0, 2, 4, . . . ), while the oddnumbered blocks remain as standard, unconditioned Transformer blocks. This alternating injection strategy fuses the explicit motion control from the BSB with the videos internal representations, thereby steering the denoising process to adhere to the planned trajectory. 11. Limitations and Future Work Despite the promising capabilities of MIND-V in generating physically compliant and semantically consistent manipulation videos, our framework exhibits certain limitations that we actively address. First, the Staged Visual Future Rollouts mechanism introduces additional computational overhead. The iterative 18 propose-verify-refine cycle requires the generation of multiple candidate videos at each sub-task transition, which increases the inference latency compared to single-pass monolithic models. However, this design choice effectively mitigates the error accumulation inherent in long-horizon tasks. While the per-step inference time is higher, our method achieves high-quality generation with greater overall computational efficiency compared to baselines that often require extensive random sampling or cherry-picking to yield single successful result. Furthermore, this process operates autonomously and eliminates the need for human intervention during generation. Second, the framework relies on the accuracy of upstream components within the Semantic Reasoning Hub (SRH), where potential failures in affordance [32] localization may propagate to downstream generation. To address this dependency, we incorporate robust fallback mechanism. In scenarios where the primary visual localizer fails or outputs erroneous results, the VLM directly infers the coordinates of the target object and destination based on semantic context and relative spatial reasoning. Following iterative optimization of these coordinates, the system utilizes them as point prompts for the SAM2 [27] to obtain precise segmentation masks. Future work aims to extend the current 2D video generation framework to 3D representations to better support direct sim-to-real transfer for robotic control."
        }
    ],
    "affiliations": [
        "Central South University",
        "China University of Geosciences",
        "Hong Kong University of Science and Technology",
        "Sun Yat-sen University",
        "Tsinghua University",
        "X Square Robot"
    ]
}