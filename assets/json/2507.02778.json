{
    "paper_title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs",
    "authors": [
        "Ken Tsui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 7 7 2 0 . 7 0 5 2 : r SELF-CORRECTION BENCH: REVEALING AND ADDRESSING THE SELF-CORRECTION BLIND SPOT IN LLMS PREPRINT Ken Tsui Independent kenhktsui@gmail.com July 4,"
        },
        {
            "title": "ABSTRACT",
            "content": "Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit systematic Self-Correction Blind Spot - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending Wait reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness. have not failed. Ive just found 10,000 ways that wont work. Thomas A. Edison"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have rapidly advanced natural language processing, achieving state-of-the-art results on diverse range of tasks (OpenAI et al., 2024; Anthropic, 2024; Gemini Team, 2025; Yang et al., 2025; Meta, 2025; DeepSeek-AI et al., 2025a). However, despite their impressive capabilities, LLMs are known to exhibit unpredictable failures and generate inaccurate information (Huang et al., 2025; Bang et al., 2023; Shi et al., 2023; Nezhurina et al., 2025). particularly concerning issue is the tendency of LLMs to make seemingly random errors even on simple tasks, despite possessing the underlying knowledge required for correct solutions. This raises fundamental questions about their reliability and trustworthiness, hindering their deployment in critical applications. Observing LLM self-correction behavior in natural settings is challenging due to their inherent accuracy; the rarity of naturally occurring errors makes systematic evaluation challenging. To address this limitation, we construct SelfCorrection Bench by systematically injecting error into the LLM reasoning traces. This allows us to create controlled environments for testing self-correction and reliably quantifying performance using dedicated benchmarks. This approach also allows us to simulate real-world imperfections and assess the robustness of LLMs in realistic scenarios. Our results reveal that LLMs are not good at self-correction, not because of knowledge. We refer to the phenomenon as Self-Correction Blind Spot. We further demonstrate that appending simple Wait and other correction markers can induce self-correction performance without finetuning, suggesting that the core issue is not knowledge deficiency, but rather lack of activation for self-correction. We provide behavioral explanation for why Wait works, supported by systematic analysis of Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT correction marker patterns and post training data. We also analyze why the RL trained reasoning model does not have the blind spot. Our contributions are as follows. Discover and quantify the Self-Correction Blind Spot - LLMs systematically fail to correct their own error while succeeding on identical external error (64.5% average across 14 models) Establish systematic evaluation methodology - Self-Correction Bench with controlled error injection across three complexity levels, enabling fair cross-model comparison Human demonstrations rarely include self-correction sequences, unlike RL models that learn error-correction through outcome feedback Demonstrate simple yet effective intervention - Appending Wait reduces blind spots by 89.3%, with behavioral explanation supported by correction marker analysis"
        },
        {
            "title": "2 Background and Related Works",
            "content": "Intrinsic Self-correction in LLMs: Recent work has shown that intrinsic self-correction improves performance by prompting the same LLM to generate feedback on their own responses (Kim et al., 2023; Shinn et al., 2023; Madaan et al., 2023; Kamoi et al., 2024). In particular, it indicates that LLMs struggle to rectify mistakes because the quality of the models self-feedback is bounded by existing knowledge. (Kim et al., 2023) and (Shinn et al., 2023) use an oracle label to guide feedback. However, without access to the oracle label Huang et al. (2024) has identified limitations in the LLMs ability to self-correct error, while Tyen et al. (2024) demonstrates that poor self-correction performance is due to the inability to localize rather than correct an error. Most works require multiple-step prompting, while our focus is on single-pass inference, in which LLM is expected to self-correct in one completion. We further study self-correction from cognitive perspective rather than the limitation due to model knowledge. Recent work understands why some LLMs thrive and some do not in outcome-based reinforcement learning (RL) from cognitive point of view (Gandhi et al., 2025). The study uses supervised finetuning (SFT) to elicit backtracking behavior, while our focus is on self-correction, including backtracking, and we use correction markers to elicit such cognitive behavior without finetuning. Kumar et al. (2024) tackles self-correction with RL, leads to significantly positive intrinsic self-correction, while DeepSeek-AI et al. (2025a)s training does not explicitly single out self-correction but instead rely on ground truth to provide signal. They observe that the aha moment when LLM learns to allocate more time to re-evaluate its initial approach (i.e. self correction). Prompt Injection for Evaluation: Traditional prompt injection technique focuses on adversarial scenarios in which attackers inject malicious instruction or prefix to manipulate LLM output (Wei et al., 2023; Liu et al., 2024). However, controlled error injection has not been systematically explored to test the self-correction capability. Lanham et al. (2023) injects mistakes into reasoning chains to evaluate the faithfulness of chain-of-thought reasoning. However, their focus is on measuring consistency between reasoning and conclusion, not self-correction capabilities. Our controlled error injection methodology injects errors to evaluate self-correction across multiple task complexity. The approach enables us to reveal systematic blind spot in LLM self-correction that has not been previously characterized. Hallucination Snowballing: Zhang et al. (2023) has demonstrated that once LLM hallucinates, subsequent tokens tend to be consistent with the error, suggesting the lack of self-correction mechanism. This phenomenon suggests systematic limitations in self-correction during generation. Our work provides an explanation by revealing that LLMs suffer from self-correction blind spot: they can correct errors in external inputs but not in their own outputs. Test-time interventions: Recently, compute has been shifted from training time (Kaplan et al., 2020) to test time, which can lead to better compute-optimal result (Snell et al., 2024). Simple test-time scaling (Muennighoff et al., 2025) finetuned with 1,000 curated questions with reasoning traces and forcing compute budget by appending Wait have demonstrated scaling performance on reasoning benchmarks with thinking tokens. We provide behavior explanation for why it works by conducting an intervention experiment on an unfinetuned model. 2 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Cognitive Bias in LLM: Koo et al. (2024); Echterhoff et al. (2024); Jones and Steinhardt (2022) show that LLMs exhibit different cognitive biases given their training in human data. We study how the bias blind spot (Pronin et al., 2002) affects self-correction. Our work contributes systematic methodology for testing self-correction and demonstrates that test-time interventions can activate self-correction mechanism that otherwise was inactive."
        },
        {
            "title": "3 Conceptual Motivation",
            "content": "The following provides conceptual motivation for our empirical study. 3.1 Error Propagation in Autoregressive LLM An autoregressive LLM, , generates each token xi according to the conditional probability PM (xix1, . . . , xi1) sequentially based on all previous tokens. When new token is generated, it is fed back as input to generate the next token. Let x1, . . . , xk denote the original input sequence of length k. We denote the generated sequence of additional tokens as xk+1, xk+2, . . . , xk+n. The probability of generating this sequence is: PM (xk+1, . . . , xk+n x1, . . . , xk) = PM (xk+1 x1, . . . , xk) PM (xk+2 x1, . . . , xk+1) . . . PM (xk+n x1, . . . , xk+n1) (1) For simplicity, we denote by xk+1, . . . , xk+n. We also denote PM (ϵk) as the probability of error in the kth position. The probability of generating correct sequence is: PM (rcorrect) = (1 PM (ϵk+1)) (1 PM (ϵk+2)) . . . (1 PM (ϵk+n)) (2) Let PM (ϵi) represent the probability of an error in step i. Let PM (ϵmin) denote the minimum error probability: PM (ϵmin) = min(PM (ϵk+1), PM (ϵk+2), . . . , PM (ϵk+n)). The convergence still holds if PM (ϵmin) > 0. lim PM (rcorrect) lim (1 PM (ϵmin))n = 0 (3) 3.2 Correct Sequence, Correct Answer and Self-Correction The key question is: what subjects correctness are we concerned about? On the one hand, the most restricted definition is that all tokens are correct, which is what is defined above. On the other hand, one might only be concerned with the answer, which is usually generated at the end of the sequence. We denote the answer by starting at the position , where < + n. PM (rcorrect answer) = (1 PM (ϵi)) (1 PM (ϵi+1)) . . . (1 PM (ϵk+n)) (4) Under this definition, ϵ can exist in + 1 . . . 1 and does not necessarily invalidate the final answer in . . . + We assume that the set of reasoning chains can be discretized for computational tractability, though this is significant simplification of the underlying reasoning process. Let be the set of all possible reasoning chains, and let denote specific reasoning chain. With self-correction, the model can explore multiple reasoning paths and potentially recover from errors such that PM (cx1, . . . , xi1) > 0 for correct reasoning chains c. The probability of generating correct answer is: PM (rcorrect answer) = (cid:88) cC PM (cx1, . . . , xi1) PM (rcorrectc) (5) This formulation is presented for conceptual clarity. In practice, enumeration of all possible reasoning chains is intractable, making this framework illustrative rather than computational. Unlike the case where all tokens must be correct, which converges to 0 as increases, the probability of generating correct answer can remain non-zero due to self-correction. Also note that since most answers in practice are of finite length, PM (rcorrect answer) does not converge to 0. 3 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT 3.3 Error and Self-Correction We assume that we can discretize error states and their corresponding response sets. While this discretization is by no means rigorous in reality, this simplification serves our illustrative purpose and provides conceptual clarity for understanding the self-correction mechanism. Let be the set of discrete error states {e0, e1, . . . , ek} and Rei be the set of model responses {rei,1, rei,2, . . . , rei,mi} corresponding to each discrete error state. The probability of correct answer can be expanded to marginalize over all possible error states, examining whether an LLM can generate correct answer under each error state e. The probability of correct answer can then be expressed as PM (rcorrect answer) = = = (cid:88) eE (cid:88) eE (cid:88) pM (e) PM (rcorrecte) pM (e) (cid:88) (cid:88) rmRe PM (rme) PM (rcorrectrm, e) pM (e) PM (rme) PM (rcorrectrm, e) eE rmRe (6) (7) (8) Where: rm = models own response (9) (10) (11) PM (rcorrect rm, e) = conditional probability of self-correcting rm and producing correct answer under error state (12) PM (rm e) = conditional probability of model generating response rm given error state Capability of self-correction is necessary (though not sufficient) for robust LLM since error-free generation is practically impossible for autoregressive LLM as discussed above. It can be due to hallucination (Maynez et al., 2020), hallucination snowballing (Zhang et al., 2023) or an execution error; models must be able to identify and rectify their mistakes. This capability also enables deliberate backtracking when reasoning paths prove unproductive if we consider an incorrect approach subset of error state set, E. It is particularly important for advanced reasoning involves searching. In other words, without self-correction capability, LLM will face significant difficulty to backtrack. Intuitively, model could make many errors and still get correct answer if self-correction can reverse them. It is not difficult to notice that in Equation 6, PM (rcorrect answer) PM (rcorrect rm, e), highlighting the importance of self-correction to achieve high performance in different benchmarks. In this formulation, we want to emphasize that an error-free generation is special case of producing correct final output rcorrect answer. 3.4 External and Internal Self Correction The existing literature (Pan et al., 2024) differentiates the source of feedback for self-correction. Intrinsic selfcorrection (Huang et al., 2024) is when LLM uses its internal knowledge, while other approaches use external feedback. In this work, we focus on the source of error. The Superficial Alignment Hypothesis (Zhou et al., 2023) posits that models knowledge and capabilities are mostly acquired during pretraining, while alignment conditions it under subdistribution with which users interact. As such, we further dissect whether the limitation of self-correction comes from the lack of model internal knowledge or cognitive bias. Pronin et al. (2002) introduces bias blind spot, which is the tendency of individuals to recognize bias while not recognizing their own. Inspired by the study, we further differentiate error comes from either external or internal. We further define the concept of internal error correction as self-correction of models own response, rm. requires metacognitive monitoring of its own reasoning process. It External error correction is defined as models ability to critically evaluate external information provided in user prompt, ru. 4 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT We define Self-Correction Blind Spot as: (cid:40) Blind Spot = 1 PM (rcorrectrm,e) PM (rcorrectru,e) 0 if PM (rcorrectru, e) > 0 if PM (rcorrectru, e) = 0 (13) If the blind spot equals 1, it means that an LLM can self-correct the external error but not the internal error. By design, the self-correction blind spot isolates confounding factors, including internal model knowledge. 3.5 Error Injection and Measuring Self Correction While Equation 6 provides conceptual clarity, computing pM (e) is intractable to compute in practice. First, we have to evaluate the model across all possible prompts, and we argue that it is an infinite space with no natural probability measure. Second, it is impractical to establish fair benchmark to test various models, as different models have different pM (e). Motivated by these limitations, we construct controlled distribution Pcontrolled(e) by systematically injecting LLM with an incorrect partial response PM (rm econtrolled). The introduction of error and an incorrect partial response leads to the construction of the Self-Correction Bench. To measure PM (rcorrect rm, econtrolled), we empirically estimate PM (rcorrect answer) under our controlled distributions Pcontrolled(e) and PM (rmecontrolled). Please note that when we inject an error into the model response, we omit stop token at the end, allowing the model to continue generating and potentially self-correct. Likewise, we measure PM (rcorrect ru, econtrolled), except that we inject the error into the user message (external error) instead of the model response (internal error). For details, please refer to Section 6.1. Please note that Pcontrolled(e) can be approximately close to or different from PM (e)."
        },
        {
            "title": "4 Self-Correction Bench",
            "content": "We introduce three datasets for measuring self-correction blind spot in LLMs across varying complexities. To understand self-correction failures, we must first ask: can models self-correct the simplest possible error? If not, more complex error correction is unlikely. We begin with artificially simple errors (SCLI5) to isolate the self-correction mechanism from confounding factors such as knowledge limitations or complexity of reasoning. GSM8K-SC introduces controlled errors in multi-step reasoning tasks, while PRM800K-SC provides realistic erroneous completions from actual LLM outputs. This progressive evaluation allows us to systematically map where self-correction breaks down while controlling for reasoning complexity and error realism. Our benchmark can be summarized in Table 1. For each dataset, we systematically inject identical error into both model response (rm) and user prompt (ru), allowing us to empirically estimate both PM (rcorrectrm, econtrolled) and PM (rcorrectru, econtrolled) under identical error conditions. Dataset SCLI5 GSM8K-SC PRM800K-SC Complexity Realism of Error Reasoning Low Medium High Low Medium High Size 286 1313 448 Table 1: Dataset comparison 4.1 SCLI5 SCLI5 (Self-correct Like am 5) is constructed by introducing error to answer of simple tasks, that can be corrected simply with recall of answer, or limited reasoning. Starting with basic task can isolate any confounding error such as model internal knowledge and reasoning capability. Most error types are off-by-one error or flip error. The composition of the task is shown in Table 2. Due to the simplicity of the task, we do not add any reasoning before answering. 5 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT After designing the composition and error, we generated the dataset programmatically. Although LLMs are unlikely to make these simple mistakes, self-correction requires error detection. If it cannot detect an obvious error, it cannot detect subtle one. Count Error Type Question and Answer 20 20 52 52 71 Off-by-one Off-by-one Off-by-one Off-by-one Flip Flip Q: What is the answer of 1 + 1? A: The answer is 3. Q: What is the answer of 3 - 1? A: The answer is 1. Q: What letter comes after A? A: The answer is C. Q: What letter comes before C? A: The answer is A. Q: Which one is smaller, 1 or 2? A: The answer is 2. Q: Which one is larger, 2 or 5? A: The answer is 2. Table 2: Task composition of SCLI5 Task Add one Subtract one Next character Previous character Larger number Smaller number 4.2 GSM8K-SC Unlike SCLI5, solving GSM8K (Cobbe et al. (2021)) requires multiple steps reasoning. As such, we injected an error in reasoning. The answer is also incorrect in way that follows the incorrect reasoning. We introduce various types of error as stated in Table 3. Using the test set, we generated wrong reasoning using gpt-4.1-2025-04-14(OpenAI, 2025) with prompt that contains the question, reasoning steps, correct answer, and error type, and order of reasoning step. The prompt can be found in the appendix. Automatic quality assurance. To ensure consistency of incorrect reasoning steps and incorrect answer, we prompted gemini-2.5-flash-preview-05-20(Gemini Team, 2025) to follow the given incorrect reasoning to arrive at the incorrect answer. Of 1,319 questions, 6 cannot arrive at the answer, and therefore, we dropped them. Category Description Problem Representation Errors Planning Errors Execution Errors These errors arise when the solver misunderstands or misinterprets the problems requirements or given information. This can involve misreading the problem statement, confusing the relationships between quantities, or failing to grasp what is being asked. These occur when the solver devises an incorrect or incomplete strategy to tackle the problem. This might include choosing the wrong operations, setting up flawed equations, or overlooking key components of the problem. These are mistakes made while carrying out the planned steps, such as errors in calculations, misapplication of mathematical rules, or procedural slip-ups, even if the plan itself is sound. Table 3: Error composition of GSM8K-SC 4.3 PRM800K-SC PRM800K(Lightman et al., 2024), derived from subset of MATH(Hendrycks et al., 2021), provides step-by-step annotations of multistep reasoning. The solution is pre-generated by finetuned GPT4(OpenAI et al., 2024), reflecting the realism of error. 6 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT We excluded quality control and initial screening questions and those identified as bad problem and the annotators had given up. To increase complexity, we only selected samples whose pre-generated answer does not match golden answer. Finally, we randomly sampled 1 completion for each question."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experiment Setup Tested models. We test wide range of open-sourced states of the art models. We do not include close-source model because only open-source models support fine-grained control of injecting prefix into the model response. We apply chat templates using transformers library(Wolf et al., 2020) corresponding to each LLM as in Figure 3. To support wide range of models and reduce cost, we use the DeepInfra1 completion API. We use temperature of 0.0 for 3 reasons. More deterministic2 output eliminates sampling variance as confounding factor. It potentially provides lower bound on self-correction capability, as higher temperatures might enable exploration of alternative correction strategies. It enables standardized comparison across models with different temperature calibrations. Renze (2024) suggests different temperatures do not have statistically significant impact on LLM performance in problem-solving tasks. Having said that, we also report results using temperature of 0.6 in the appendix. Our findings are robust to change in temperature. Controlled computational budget. To ensure fair comparison between internal and external error correction, and across models, we maintain fixed token budget of 1,024 across all conditions. This design choice partly isolates self-correction capabilities from the effect of test time compute, providing more rigorous test of the blind spot phenomenon. We also report our results of PRM800-SC with fixed tokens budget of 4,096 in the appendix, which does not change our conclusion. LLM and reasoning model. Some models like Qwen3(Yang et al., 2025) series are trained to offer thinking and non-thinking mode in single model. Their results are reported separately. Automatic evaluation. We use gemini-2.5-flash-preview-05-20 to compare LLMs completion against the groundtruth answer. We instruct the model to output in JSON format. Due to the objectivity of the task and the provision of ground truth in the prompt, we do not believe there is significant bias. The prompt is provided in the appendix. We also manually review 100 samples for each dataset to ensure automatic evaluation quality using visualizer, whose interface is shown in Figure 18. Metric. As the models are given wrong reasoning step or wrong answer, we evaluate if models can self-correct and arrive at the ground-truth answer. In GSM8K-SC and PRM800K-SC, we measure the behavior of LLMs before commit an answer, as it is more common scenario when an LLM backtracks, although we also report that after commit an answer. We report the mean accuracy and standard error for each model. The standard error is estimated using the formula σM = σ , where is the population size and σ is the population standard deviation. 5.2 Result We report the mean accuracy and standard error of state-of-the-art non-reasoning LLMs in Table 1. We observe surprisingly low accuracy for SCLI5 in some models. We identify moderate to strong positive correlations between SCLI5, GSM8K-SC and PRM800K-SC in Figure16, suggesting that there is limitation of LLMs to self-correct across task complexities. If LLM cannot self-correct both easy and hard tasks, it implies an activation problem rather than knowledge problem. We dive into more details in Section 6.1. 1https://deepinfra.com/ 2Temperature of 0.0 will not generate fully deterministic result due to finite precision. 7 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Model SCLI5 GSM8K-SC PRM800K-SC Marco Avg Llama-4-Maverick-17B-128E-Instruct-FP8 (Meta, 2025) DeepSeek-V3-0324 (DeepSeek-AI et al., 2025b) Qwen2.5-72B-Instruct (Qwen et al., 2025) Llama-4-Scout-17B-16E-Instruct (Meta, 2025) Llama-3.3-70B-Instruct (Meta, 2024) Qwen3-235B-A22B (Yang et al., 2025) phi-4 (Abdin et al., 2024) Qwen2.5-7B-Instruct (Qwen et al., 2025) Qwen2-7B-Instruct (Yang et al., 2024) Qwen3-14B (Yang et al., 2025) Qwen3-30B-A3B (Yang et al., 2025) Llama-3.1-8B-Instruct (Grattafiori et al., 2024) Qwen3-32B (Yang et al., 2025) Mistral-Small-24B-Instruct-2501 (Team, 2025) 0.948 0.825 0.92 0.976 0.538 0.563 0.808 0.559 0.601 0.004 0.056 0.136 0.004 0.042 0.416 0.399 0.58 0.24 0.275 0.073 0.076 0.19 0.078 0.092 0.061 0.019 0.05 0.011 0.455 0.475 0.154 0.263 0.246 0.348 0.092 0.141 0.058 0.254 0.194 0.02 0.083 0.016 0.606 0.567 0.551 0.493 0.353 0.328 0.325 0.297 0.246 0.117 0.104 0.058 0.045 0.023 Table 4: Mean accuracy of models at temperature 0. Figure 1: Summary of mean accuracy across models We note moderate correlation between GSM8K-SC and PRM800K-SC indicating that controlled error injection can predict the limitation of self-correction in real-world setting. Last but not least, we find that empty responses, which are deemed incorrect because it means LLMs inaction to self-correct, are quite prevalent in some models, for example, Qwen3-32B, LLama3.1-8B-Instruct and Mistral-Small24B-Instruct-2501 in Table 2."
        },
        {
            "title": "6 Analysis",
            "content": "6.1 Why Dont LLMs Self-correct? Motivated by the finding that LLMs cannot self-correct simple mistake in SCLI5, and nonresponses, and the contrasting performance of LLM to be good critic model in reasoning, we perform further analysis. 8 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Figure 2: Summary of error and empty response across models Unlike Zheng et al. (2025); Song et al. (2025), which prompt LLM as critic models with explicit instruction to evaluate reasoning in user prompt, we do not add explicit instruction when measuring PM (rcorrect ru, econtrolled) because we intend to minimize the prompt difference when estimating PM (rcorrect rm, econtrolled) and PM (rcorrect ru, econtrolled) as we want to minimize the confounding factor. Figure 3 illustrates how we inject error in user message (i.e. external error) and in model (i.e. internal error), respectively, via chat template. The resulting tokens of PM (rcorrect rm, econtrolled) and PM (rcorrect ru, econtrolled) differ, but in both settings, autoregressive LLM has access to same error content, although positioned differently. Error Injection in Model <start header id >user<end header id>nn What is the answer of 1 + 1? <eot id><start header id>assistant <end header id >nn The answer is 3. <eot id> Error Injection in User Message <start header id >user<end header id>nn What is the answer of 1 + 1? The answer is 3. <eot id><start header id>assistant <end header id >nn Im afraid thats not correct. The answer to the equation 1 + 1 is actually 2, not 3. Basic arithmetic operations like addition follow set of rules and patterns that have been established for centuries. In this case, when you add 1 to 1, you are counting two units, which equals 2.nn If youre unsure about this, you can try using calculator or counting blocks to visualize the concept.<eot id> Figure 3: Example of error injection. Grey color shows model completion. Above: Error injection in model; Below: Error injection in user message We report statistically significant self-correction blind spot for most models across datasets - LLM fails to self-correct error when injected in model completion (internal error); while they are able to do so when the error is in the user prompt (external error) in Figure 4. The blind spot, on average, 64.5%, exists across models of different model sizes. 9 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT In Figure 17, we also observe moderate correlation between the datasets, suggesting that there is blind spot between tasks of different complexity. We also find that, on average, when model has committed an answer, it has much higher blind spot to recognize its own error, similar finding as in recent work (Zhang et al., 2023). Figure 4: Self-Correction Blind Spot across models 6.2 How Do LLMs Self-correct? The blind spot drives us to further investigate the qualitative difference of how the LLM response is when error was injected into the model response vs. the user response. We observe that when error is external, correction markers are generated by 179.5% and 73.6% more in GSM8K-SC and PRM800K-SC. We do not see such increase in SCLI5 because the task does not require reasoning to self-correct such that LLMs directly clarify the correct answer. Correction markers includes Wait, But, However, No, Hold on, Hang on, Alternatively, Hmm. To increase precision, we require these markers to be at the beginning of sentence, and followed by comma, exclamation mark, or whitespace. This finding leads us to perform an intervention, which can establish the causation. While simple test-time scaling appends Wait after SFT to force more thinking (Muennighoff et al., 2025), we append Wait after the incorrect reasoning or answer to prompt LLMs to self-correct, without any finetuning. Our design minimizes the confounding factor by using the same model. After appending Wait, Figure 6 shows significant reductions in blind spot, in some cases, negative blind spot even without any SFT. Averaging across models and datasets, the reduction amounts to 89.3%, and the mean accuracy increases by 156.0% on average in Figure 5. This evidence leads us to believe that Wait and similar correction markers serve as strong conditioning token that shifts the models conditional probability distribution. This new distribution favors sequences associated with re-evaluation and correction. 10 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT To establish the generalization of our correction marker hypothesis, we validate multiple markers to demonstrate that they can activate self-correction across models and datasets  (Table 5)  . This systematic evaluation reveals two important patterns: Wait consistently outperforms But and However across all datasets. We hypothesize that this reflects different semantic roles in correction contexts: Wait typically signals self-reflection and reconsideration, while But and However often introduce contrasting information rather than self-correction. Correction marker effectiveness decreases with task complexity Complex reasoning tasks likely require multiple backtracking steps and deeper error analysis that single correction marker may not fully activate. It aligns with the finding from Muennighoff et al. (2025)s work showing that by appending Wait multiple times, performance on complex task increases. Figure 5: Macro average accuracy by model increases from original to appended Wait Correction markers frequency analysis. To perform more qualitative analysis, we perform binary term frequency analysis. As LLM is provided with wrong reasoning or answers, to self-correct, intuitively, we would expect to find correction markers. The initially appended Wait is not counted. We discover that after appending Wait, LLMs have higher tendency to generate these markers subsequently, and correspondingly the mean accuracy also increases. We perform correlation analysis between binary term frequency and accuracy before and after appending Wait. We report strong correlations between models in SCLI5, GSM8KSC, and PRM800K-SC in Figure 7. This shows that the initially appended Wait activates initial and subsequent self-correction in most models. Correction Markers Internal Error (Baseline) External Error Wait But However SCLI5 GSM8K-SC PRM800K-SC 0.499 (0%) 0.910 (+82.5%) 0.957 (+91.9%) 0.922 (+85.0%) 0.897 (+79.8%) 0.183 (0%) 0.881 (+382.1%) 0.796 (+335.1%) 0.611 (+234.2%) 0.602 (+229.0%) 0.200 (0%) 0.620 (+210.3%) 0.504 (+152.0%) 0.430 (+114.8%) 0.438 (+119.3%) Table 5: Mean accuracy and relative change after appending various correction markers 11 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Figure 6: Self-Correction Blind Spot across models after appending Wait Figure 7: Correlation of absolute change in keyword presence vs absolute change in accuracy - original vs appending Wait 6.3 Reasoning Models We observe small, even negative, self-correction blind spots in reasoning models in Figure 8. The mean accuracy is reported in 15. In Table 6, we compare macro mean accuracy across all datasets of the base model without appending Wait, base model appending Wait, and finetuned with RL (Shao et al., 2024; DeepSeek-AI et al., 2025a). Appending Wait to base model without finetuning can almost match the performance of finetuned model in some models. We partially explain the surprising performance in simple test time scaling (Muennighoff et al., 2025) that appending Wait could elicit self-correction capability that matches that of RL tuned model. 12 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Figure 8: Self-Correction Blind Spot across reasoning models Base Name Finetuned Name Base Model Appending Wait Finetuned/ Thinking Mode DeepSeek-V3-0324 phi-4 Qwen3-14B Qwen3-32B Qwen3-30B-A3B Qwen3-235B-A22B DeepSeek-R1-0528 phi-4-reasoning-plus Qwen3-14B Qwen3-32B Qwen3-30B-A3B Qwen3-235B-A22B 0.578 0.325 0.121 0.046 0.102 0.335 0.918 0.704 0.884 0.791 0.869 0.865 0.908 0.707 0.843 0.894 0.845 0.876 Table 6: Macro average of accuracy of base Model vs base model appending Wait vs finetuned Model Most common first word analysis. In Table 7, we summarize the first generated word using responses with successful self-correction. The pattern is quite clear: when seeing an error, reasoning models start with correction marker. Why does RL work? Outcome-based RL learns by generating multiple candidate solutions, evaluating them against ground truth outcomes and other success criteria, and using the resulting reward signals to update the policy (Shao et al., 2024). Unlike supervised learning with step-by-step guidance, it only receives feedback on the final outcomes. By understanding generations of these models trained with RL, we can reveal what the model has learned. Our analysis of RL-trained reasoning models reveals consistent pattern: they frequently generate correction markers 6.4, particularly when encountering errors  (Table 7)  . This suggests that these models may have learned to pause, reconsider, and self-correct during the RL training process. Non thinking vs thinking mode. Qwen3 fuses thinking mode and non-thinking mode by continual finetuning via united chat template after GRPO (Shao et al., 2024). It is quite interesting to observe that Qwen3 mitigates most blind spots by enabling thinking mode, while non-thinking mode still suffers from blind spot despite the function as the chat template conditions the model into different cognitive modes. 13 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Model SCLI5 GSM8K-SC PRM800K-SC QwQ-32B Qwen3-14B (thinking) Qwen3-32B (thinking) Qwen3-30B-A3B (thinking) Qwen3-235B-A22B (thinking) DeepSeek-R1-0528 gemma-3-12b-it gemma-3-27b-it phi-4-reasoning-plus (Wait,, 0.377) (In, 1.0) (After, 1.0) (Wait,, 0.312) (**Step-by-step, 0.292) (Wait,, 0.725) (Wait,, 0.38) (The, 0.288) (Therefore,, 0.25) (Wait,, 0.198) (No,, 0.324) (The, 0.284) (Heres, 0.31) (Wait,, 0.861) (But, 0.267) (The, 0.239) (Let, 0.256) (Wait,, 0.677) (Wait,, 0.768) (Therefore,, 0.219) (I, 0.189) (So, 0.195) (Therefore,, 0.256) (But, 0.486) (Alternatively,, 0.205) (However,, 0.292) (However,, 0.217) Table 7: Most common first word and relative frequency generated by reasoning models et al., OpenAssistant (Kopf et al., 2023) OpenHermes2.5 (Teknium, 2023) Infinity-Instruct-7M (Li 2025) UltraFeedback Cui et al. (2024) Tulu3-sft-olmo-2-mixture bert et al., 2025) s1K-1.1 (Muennighoff et al., 2025) Mixture-of-Thoughts (Face, 2025) OpenThoughts3-1.2M (Guha et al., 2025) (LamCount 1% 5% 10% 25% 50% 75% 90% 95% 99% 9,846 1,001,551 7,449,106 61,135 939,344 1,000 349,317 1,200,000 0 0 0 0 0 0 1 0 0 0 0 0 0 3 66 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 2 2 2 2 0 10 132 0 30 170 1 76 213 3 147 253 5 202 278 9 273 Table 8: Descriptive statistics of correction markers in post training dataset 6.4 Correction Markers in Post Training Data Revisiting our previous findings: 1. In Section 6.2, we note that non-reasoning model shows more correction markers in evaluating external error. 2. After appending Wait, non-reasoning model elicits self-correction capability . 3. In Table 7, we discover that reasoning model predicts correction markers upon seeing error. 4. In Table 6, non-reasoning models have self-correct blind spot while reasoning models dont despite the same base model. If we can make non-reasoning model to predict correction markers upon seeing internal error, we can induce self-correction capability in non-reasoning model, and that capability is already in the model when it evaluates against external error. Motivated by this logic, we further investigated various post training datasets. We analyze the distribution of correction markers in model response in popular open source supervised finetuning datasets in Table 8. s1K-1.1, Mixture-ofThoughts and OpenThoughts3-1.2M are generated from reasoning models, while OpenAssistant3 is entirely collected from human. We discover that in non-reasoning instruction datasets, only 5-10% of the data contain at least 1 correction marker; while in reasoning dataset, most data contain at least 1 correction marker, with median ranging from 30 to 170. With such systematic absence and presence of correction markers in training data, it follows from basic statistical modeling principles that models will predict correction markers as next tokens proportional to their frequency in training data. This statistical likelihood directly determines self-correction behavior: models trained on less correction 3We use the highest-rated paths of conversation tree provided in timdettmers/openassistant-guanaco. 14 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT data rarely generate correction markers, while models trained on correction-rich data readily do so, thereby eliminating the self-correction blind spot. This single, powerful insight unifies all of our empirical observations. It is also worth noting that while s1K-1.1 does not have as high density as other reasoning datasets, the experiment in (Muennighoff et al., 2025) requires test time intervention (e.g. appending Wait) instead of natural generation."
        },
        {
            "title": "7 Discussion and Future Work",
            "content": "Benefit of error and self-correction data. LLMs are known to exhibit cognitive bias (Koo et al., 2024; Echterhoff et al., 2024; Jones and Steinhardt, 2022). SelfCorrection Blind Spot bears resemblance to bias blind spot of human, where capability of self-correction is relatively limited. We hypothesize that there are multiple reasons: 1. majority of instruction and preference come from human demonstration via supervised fine-tuning and reinforcement learning from human feedback(Stiennon et al., 2022; Ouyang et al., 2022). It leads to the scenario that for an average person, polished and error-free response is more preferred over response with error and self-correction. See the analysis in Section 6.4. 2. while some of the instruction data are generated from models synthetically (Teknium, 2023; Li et al., 2025) or scored by AI Feedback Cui et al. (2024), most generators or reward models (OpenAI et al., 2024) learn from human feedback so that this artifact has been passed along. Traditional machine learning wisdom says that model works well in production if training data resembles the production environment. Logically speaking, model SFTed with lack of demonstration in self-correction of its own response leads to model that has limitation to self-correct. Unlike supervised learning, outcome-based RL encourages uncontrolled exploration of reasoning paths. Models are encouraged to roll out diverse generation that might contain errors and their corresponding self-correction, as evidenced in the count of correction markers in Section 6.4. These series of errors (including unsuccessful attempts) and self-correction (including backtracking) are often found in aha moment in DeepSeek-R1(DeepSeek-AI et al., 2025a). These error and self-correction pairs can complement the limitation of human demonstration and preference where data are mostly error-free, in every step. By interpreting that an error-free response is special case of correct final output can allow us to collect more data that makes LLM more robust to error and also to explore because it can self-correct, which is what we are seeing in outcome-based RL. In fact, learning from error (An et al., 2024) and how to critique it (Wang et al., 2025) improve LLM performance. Outcome-based RL (Shao et al., 2024; DeepSeek-AI et al., 2025a) allows the model to learn in scalable way by rolling out error and self-correction pairs, with ground-truth as feedback instead of an explicit critique. Understanding cognitive behavior via markers. Performing frequency analysis of correction markers is scalable solution that can be extended to understanding various cognitive behaviors present in pretraining data and post training data. To the best of our knowledge, understanding of pretraining and post training data via markers has not been studied extensively. These markers have been proven to elicit self-correction and likely other cognitive behaviors. We believe that these markers can serve as important heuristics for LLM pretraining and post training data curation, and study of optimal data mixture. Tackling blind spot. Addressing blind spots is crucial to building LLMs that are not only powerful but also trustworthy and reliable in real-world applications. Our error injection methodology provides valuable tool for quantifying selfcorrection performance in non-reasoning and reasoning models. Self-correction and denoising. Lastly, self-correction bears resemblance to denoising objectives in diffusion models, suggesting that self-correction can be viewed as form of reasoning denoising that the final response resembles the diffusion output after multiple steps of self-correction. This connection opens the intriguing theoretical question whether autoregressive model can theoretically express denoising steps that merit future investigation."
        },
        {
            "title": "8 Limitation and Broader Impact",
            "content": "Although we have conducted comprehensive evaluation across datasets of diverse complexity and realism of error, there is still much more evaluation that can be performed. For example, Self-Correction Bench can be extended to cover programming, logic, and common sense reasoning. We hope that Self-Correction Bench and the measurement of the self-correction blind spot can become valuable testing tool for non-reasoning and reasoning models and lead to further research in self-correction. 15 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT We are aware that the Self-Correction Bench introduces various distribution mismatch for various models because controlled error injection may not perfectly mirror naturally occurring error patterns. However, this methodology enables systematic isolation of self-correction capabilities from confounding factors. The progression from artificial (SCLI5) to realistic (PRM800K-SC) errors demonstrates the generality of phenomenon across error types. The controlled injection also allows cross-model comparability as in other LLM benchmarks. We encourage future research to expand the self-correction benchmark in the future."
        },
        {
            "title": "Reproducibility Statement",
            "content": "Our experiments utilize various open source models, close sourced models, and datasets. Our codes for running the experiment are released in github. Self-Correction Bench is available at Hugging Face."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank the open source community for making this research possible through shared datasets, models, and libraries. We are particularly grateful to the teams behind the datasets used in our evaluation: GSM8K, PRM800K, and the various instruction tuning datasets we analyzed. We acknowledge the model developers who have made their work publicly available, including the teams at DeepSeek, Google (Gemma), Meta (Llama), Microsoft (Phi), Mistral and Qwen. We also thank the developers of the computational infrastructure and libraries that enabled our experiments, including the transformers and datasets library (Hugging Face), DeepInfra API, Google API and OpenAI API."
        },
        {
            "title": "References",
            "content": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vi16 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT jayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Anthropic. The claude 3 model family: Opus, sonnet, haiku, Mar 2024. URL https://www-cdn.anthropic. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities., June 2025. URL https://storage.googleapis.com/ deepmind-media/gemini/gemini_v2_5_report.pdf. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, Apr 2025. URL https: //ai.meta.com/blog/llama-4-multimodal-intelligence/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, January 2025. ISSN 1558-2868. doi: 10.1145/3703155. URL http://dx.doi.org/10.1145/3703155. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity, 2023. URL https://arxiv.org/abs/2302.04023. 17 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context, 2023. URL https://arxiv.org/abs/ 2302.00093. Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models, 2025. URL https://arxiv.org/ abs/2406.02061. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023. URL https: //arxiv.org/abs/2303.17491. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/abs/2303.11366. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023. URL https://arxiv.org/abs/2303.17651. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can LLMs actually correct their own mistakes? critical survey of self-correction of LLMs. Transactions of the Association for Computational Linguistics, 12:14171440, 2024. doi: 10.1162/tacl 00713. URL https://aclanthology.org/2024.tacl-1. 78/. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet, 2024. URL https://arxiv.org/abs/2310. 01798. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. LLMs cannot find reasoning errors, In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, but can correct them given the error location. Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.826. URL https://aclanthology.org/2024.findings-acl.826/. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/ abs/2503.01307. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. URL https://arxiv.org/abs/2409.12917. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: how does llm safety training fail? In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. Formalizing and benchmarking prompt injection attacks and defenses, 2024. URL https://arxiv.org/abs/2310.12815. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukoˇsiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. Measuring faithfulness in chain-of-thought reasoning, 2023. URL https://arxiv.org/abs/2307.13702. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model hallucinations can snowball, 2023. URL https://arxiv.org/abs/2305.13534. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv. org/abs/2001.08361. 18 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https: //arxiv.org/abs/2501.19393. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking cognitive biases in large language models as evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 517545, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.29. URL https://aclanthology.org/2024.findings-acl.29/. Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. Cognitive bias in decision-making with llms, 2024. URL https://arxiv.org/abs/2403.00811. Erik Jones and Jacob Steinhardt. Capturing failures of large language models via human cognitive biases, 2022. URL https://arxiv.org/abs/2202.12299. Emily Pronin, Daniel Y. Lin, and Lee Ross. The bias blind spot: Perceptions of bias in self versus others. Personality and Social Psychology Bulletin, 28(3):369381, 2002. doi: 10.1177/0146167202286008. URL https://doi. org/10.1177/0146167202286008. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 19061919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.173. URL https://aclanthology.org/ 2020.acl-main.173/. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506, 2024. doi: 10.1162/tacl 00660. URL https: //aclanthology.org/2024.tacl-1.27/. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment, 2023. URL https://arxiv.org/abs/2305.11206. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. OpenAI. Introducing gpt-4.1 in the api, Apr 2025. URL https://openai.com/index/gpt-4-1/. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and JaIn Thirty-fifth Conference cob Steinhardt. Measuring mathematical problem solving with the MATH dataset. on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https: //openreview.net/forum?id=7Bywt2mQsCe. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/ abs/1910.03771. Matthew Renze. The effect of sampling temperature on problem solving in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 73467356, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.432. URL https://aclanthology.org/2024. findings-emnlp.432/. 19 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Meta. 3.3, model-cards-and-prompt-formats/llama3_3/. Llama 2024. Dec URL https://www.llama.com/docs/ Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https: //arxiv.org/abs/2412.08905. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, 20 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Mistral AI Team. Mistral small 3, Jan 2025. URL https://mistral.ai/news/mistral-small-3. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2025. URL https://arxiv. org/abs/2412.06559. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models, 2025. URL https://arxiv.org/abs/2501.03124. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations democratizing large language model alignment, 2023. URL https://arxiv.org/abs/2304.07327. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https: //huggingface.co/datasets/teknium/OpenHermes-2.5. Jijie Li, Li Du, Hanyu Zhao, Bo wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin. Infinity instruct: Scaling instruction selection and synthesis to enhance language models, 2025. URL https://arxiv. org/abs/2506.11116. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with scaled ai feedback, 2024. URL https://arxiv.org/abs/2310.01377. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/ huggingface/open-r1. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/ 2506.04178. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022. URL https://arxiv.org/abs/ 2009.01325. 22 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner, 2024. URL https://arxiv.org/abs/2310.20689. Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate, 2025. URL https://arxiv.org/abs/2501.17703. 23 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt A.1.1 Generating GSM8K-SC from pydantic import BaseModel class ReasoningWithMistake(BaseModel): reasoning_steps_with_one_mistake: List[str] mistake_step: int type_of_mistake: str description_of_mistake: str incorrect_answer: str You are helpful assistant that follow instructions. Output in JSON format. <question> {question} </question> <reasoning_steps> {reasoning_steps} </reasoning_steps> <answer> {answer} </answer> <type_of_mistake> {error_type}: {error_description} </type_of_mistake> You task is to introduce one mistake in step {mistake_step} in <reasoning_steps> and arrive at an answer different from <answer>. You will output: - <reasoning_steps> with mistake - the step that contains the mistake - type of the mistake - description of the mistake - incorrect answer Figure 9: Output schema, system prompt and prompt for generating GSM8K-SC dataset 24 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT from pydantic import BaseModel class Calculation(BaseModel): incorrect_answer: float You are helpful assistant that follow instructions. Output in JSON format. <question> {question} </question> <description_of_mistake> {description_of_mistake} </description_of_mistake> <incorrect_reasoning> {incorrect_reasoning} </incorrect_reasoning> You are given question, description of the mistake, and the resulting incorrect reasoning. Your task is to follow the incorrect reasoning to arrive at the incorrect answer. Figure 10: Output schema, system prompt and prompt for validating GSM8K-SC dataset A.1.2 Automatic Evaluation from pydantic import BaseModel class Evaluation(BaseModel): is_correct_answer: bool is_correct_reasoning: bool is_backtracking: bool is_admitting_mistake: bool You are helpful assistant that follow instructions. Output in JSON format. Figure 11: Output schema and system prompt for automatic evaluation of error in model 25 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT # FOR SCLI5 <question> {question} </question> <golden_answer> {golden_answer} </golden_answer> <given_wrong_answer> {given_wrong_answer} </given_wrong_answer> <completion_from_model> {completion_from_model} </completion_from_model> The model was provided wrong answer <given_wrong_answer>, and is required to self correct and arrive at the golden answer. You have to assess if <completion_from_model> : - has backtracked - has finally provided the answer that matches the <golden_answer> - has provided correct reasoning - has admitted the mistake. # FOR GSM8K-SC AND PRM800K-SC <question> {question} </question> <golden_answer> {golden_answer} </golden_answer> <given_wrong_reasoning> {given_wrong_reasoning} </given_wrong_reasoning> <completion_from_model> {completion_from_model} </completion_from_model> The model was provided wrong step in the reasoning process < given_wrong_reasoning>, and is required to self correct and arrive at the golden answer. You have to assess if <completion_from_model> : - has backtracked - has finally provided the answer that matches the <golden_answer> - has provided correct reasoning - has admitted the mistake. Figure 12: Prompt for automatic evaluation of error in model 26 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT from pydantic import BaseModel class EvaluationErrorInUser(BaseModel): is_correct_answer: bool is_correct_critique: bool You are helpful assistant that follow instructions. Output in JSON format. Figure 13: Output schema and system prompt for automatic evaluation of error in user # FOR SCLI5 <question_and_user_answer> {question_and_user_answer} </question_and_user_answer> <golden_answer> {golden_answer} </golden_answer> <response_from_model> {response_from_model} </response_from_model> The model was provided with <question_and_user_answer> from user. You have to assess if <response_from_model> : - contains correct answer that matches the <golden_answer> - contains correct critique of <question_and_user_answer> # FOR GSM8K-SC AND PRM800K-SC <question_and_user_reasoning> {question_and_user_reasoning} </question_and_user_reasoning> <golden_answer> {golden_answer} </golden_answer> <response_from_model> {response_from_model} </response_from_model> The model was provided with <question_and_user_reasoning> from user. You have to assess if <response_from_model> : - contains correct answer that matches the <golden_answer> - contains correct critique of <question_and_user_reasoning> Figure 14: Prompt for automatic evaluation of error in user Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT A.2 Result of different temperature The result does not change our conclusion. Model SCLI5 GSM8K-SC PRM800K-SC Llama-4-Maverick-17B-128E-Instruct-FP8 DeepSeek-V3-0324 Qwen2.5-72B-Instruct Llama-4-Scout-17B-16E-Instruct Llama-3.3-70B-Instruct Qwen3-235B-A22B phi-4 Qwen2.5-7B-Instruct Qwen2-7B-Instruct Qwen3-14B Qwen3-30B-A3B Qwen3-32B Meta-Llama-3.1-8B-Instruct Mistral-Small-24B-Instruct-2501 0.955 0.874 0.902 0.976 0.497 0.57 0.794 0.563 0.601 0.007 0.108 0.038 0.182 0.122 0.423 0.42 0.573 0.248 0.273 0.091 0.093 0.183 0.071 0.101 0.07 0.068 0.025 0.02 0.469 0.504 0.165 0.272 0.243 0.4 0.116 0.127 0.065 0.27 0.232 0.105 0.022 0.038 Table 9: Mean accuracy of models at temperature 0.6 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT A.3 PRM800K-SC result in 4,096 token budget Despite higher mean accuracy in some models, the self-correction blind spots still exist, and appending Wait can elicit capability of self-correction. Model Compute budget Llama-4-Maverick-17B-128E-Instruct-FP8 DeepSeek-V3-0324 Qwen2.5-72B-Instruct Llama-4-Scout-17B-16E-Instruct Llama-3.3-70B-Instruct Qwen3-235B-A22B phi-4 Qwen2.5-7B-Instruct Qwen2-7B-Instruct Qwen3-14B Qwen3-30B-A3B Qwen3-32B Meta-Llama-3.1-8B-Instruct Mistral-Small-24B-InstructExternal Error 4,096 1,024 Internal Error Appending Wait 1,024 4,096 1,024 4,096 0.71 0.775 0.612 0.58 0.359 0.786 0.714 0.576 0.658 0.705 0.779 0.754 0.181 0. 0.721 0.938 0.614 0.578 0.366 0.806 0.719 0.569 0.65 0.743 0.817 0.781 0.183 0.498 0.455 0.475 0.154 0.263 0.246 0.348 0.092 0.141 0.058 0.254 0.194 0.083 0.02 0.016 0.458 0.509 0.161 0.257 0.257 0.368 0.092 0.141 0.058 0.268 0.19 0.085 0.02 0.016 0.67 0.772 0.438 0.545 0.46 0.705 0.328 0.442 0.324 0.696 0.683 0.527 0.194 0.27 0.676 0.821 0.449 0.542 0.469 0.732 0.337 0.444 0.333 0.746 0.712 0.522 0.203 0.277 Table 10: Mean accuracy of models in PRM800K-SC at different compute budget A.4 Mean accuracy of reasoning models Figure 15: Summary of mean accuracy across reasoning models 29 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT A.5 Correlation plots Figure 16: left: Mean accuracy correlation matrix across datasets middle: Scatter plot between SCLI5 vs GSM8K-SC right: Scatter plot between GSM8K-SC vs PRM800K-SC BCA: Before commit an answer Figure 17: left: Blind spot correlation matrix middle: Scatter plot between SCLI5 vs GSM8K-SC right: Scatter plot between GSM8K-SC vs PRM800K-SC BCA: Before commit an answer 30 Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs PREPRINT A.6 Visualization Tool Figure 18: Our self-built visualization tool"
        }
    ],
    "affiliations": [
        "Independent"
    ]
}