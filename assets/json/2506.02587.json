{
    "paper_title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations",
    "authors": [
        "Weiduo Yuan",
        "Jerry Li",
        "Justin Yue",
        "Divyank Shah",
        "Konstantinos Karydis",
        "Hang Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 8 5 2 0 . 6 0 5 2 : r BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Birds-Eye View Representations Weiduo Yuan1, Jerry Li2, Justin Yue2, Divyank Shah2, Konstantinos Karydis2, Hang Qiu2 1 University of Southern California, 2 University of California, Riverside Abstract: Accurate LiDAR-camera calibration is fundamental to fusing multimodal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses birds-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib. Keywords: LiDAR-Camera Calibration, Autonomous Driving, BEV Features"
        },
        {
            "title": "Introduction",
            "content": "Multi-modal sensing has been widely deployed in todays autonomous systems to provide accurate perception while adding redundancy for safety-critical applications. Previous work has shown improved reliability and effectiveness of multi-modal perception for navigation in crowded environments [1] and autonomous driving [2, 3] as result of different sensing modalities complementing each other. One key enabler is the multimodal calibration that ensures the geometric alignment among different modalities. An extrinsic calibration error of few degrees in rotation or few cm in translation can compound over distance (e.g., 20 cm displacement over 5 meters [4]), which can significantly degrade the performance of downstream tasks. Early works in multimodal calibration relied on targets with unique planar patterns [5, 6, 4] or specialized rooms [7] as reference to ensure proper geometry when aligning multiple modalities, primarily image and LiDAR modalities. Although effective, the usage of specialized equipment can make the calibration process tedious and cumbersome. Nevertheless, there is also demand in modern autonomous systems for continuous calibration in the wild (e.g., misoriented/shaken sensors). Consequently, other works focus on targetless approaches [8, 9], e.g., relying on the motion of the sensors, using natural features in the environment. The advent of deep learning has further diversified the approaches taken for multimodal calibration. Some calibration methods are hybrid [10, 11], i.e., they use deep learning models to extract features in different modalities and perform traditional optimization to predict the sensor extrinsics. Other methods [8, 12, 13] are purely data-driven and are trained and evaluated with popular datasets. such as KITTI [7], NuScenes [14] and Pandaset [15]. *Equal contribution. Correspondence to weiduoyu@usc.edu, jli793@ucr.edu Among these learning-based methods, common pattern is to rely on techniques akin to feature matching between the images and the point clouds. Previous attempts to find these correspondences use feature matching models [10], segmentation masks [11], or the latent space after encoding images and point clouds as depth images [8, 9, 16, 17]. While useful for calibration, establishing correspondences does not explicitly enforce geometric constraints. In multi-modal perception works, one appealing method is the birds-eye-view (BEV) representations [2] that place different modalities in shared BEV grid. In this BEV grid, LiDAR point clouds are projected or pillarized onto the BEV grid while camera features are also lifted into this space. Intrinsically, BEV representations preserve the geometry information, which offers much stronger space for feature alignment. Such alignment has seen great success in various autonomous driving tasks, including object detection [18, 19, 20], HD-map construction [21, 22], place recognition [23, 24], occupancy [25, 26], and world model [27, 28]. Therefore, we investigate whether the BEV space is good candidate for geometric alignment for calibration purposes. In this work, we propose BEVCALIB, the first-of-its-kind target-less LiDAR-camera calibration method using BEV representations. This method is motivated by the need to explicitly ensure that geometry is maintained during the calibration process. To that end, BEVCALIB projects both an input image and point cloud using an initial guess extrinsic Tinit into BEV feature space, fuses these BEV features together, and follows geometry-guided approach to decode Tpred, the correction needed to arrive at an accurate extrinsic transform. While we train BEVCALIB on KITTI and NuScenes for fair comparison with existing baselines, we also collect our own dataset (CALIBDB) with heterogeneous extrinsics to evaluate the generalizability. Our evaluation shows that BEVCALIB establishes new state-of-the-art performance. Under various noise conditions, BEVCALIB outperforms the best baseline in literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset in terms of (translation, rotation) respectively. Compared to open source baselines, BEVCALIB outperforms the best reproduced results by (92.75%, 89.22%) on KITTI dataset, (92.69%, 93.62%) on NuScenes dataset, and (60.21%, 24.99%) on CALIBDB. Qualitative visualizations in the form of camera-LiDAR overlays illustrate fine-grained projection match as result of the higher accuracy of BEVCALIBs predicted extrinsics. With strong performance, BEVCALIB fills critical gap in the open-source community for LiDAR-camera calibration. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib."
        },
        {
            "title": "2 Related Works",
            "content": "Target-based Methods. Early multimodal calibration methods borrowed from camera calibration techniques using planar targets, e.g., checkerboards, fiducial markers, and other specialized patterns, to provide reference in aligning modalities. Earlier works [5] found that LiDAR scans on the planar pattern can be used to register constraints with the estimated pattern on the cameras image plane, thus improving the extrinsic calibration of previous methods. Huang et al. [4] similarly found that using target of known geometry and dimensions is helpful and developed solution to fit the LiDAR to camera transform without requiring target edge extraction. Yan et al. [6] provides way to jointly calibrate camera intrinsics and LiDAR to camera extrinsics using special target type with checkerboards and conic sections. Verma et al. [29] proposed using Variability of Quality (VOQ) metric to score calibration samples, and samples with higher scores are used to reduce user error and possible overfitting to the target. Target-less Methods. While specialized targets ensure accuracy in predicting the sensor extrinsic, performing the sensor setup can be cumbersome and tedious. These drawbacks can be alleviated using target-less calibration methods. For example, Ishikawa et al. [30] proposed using motion, while Pandey et al. [31] proposed incorporating probabilistic methods in extrinsic calibration. Recent years have witnessed the rise of interest in solving calibration using learning-based approaches that leverage natural cues in the target-less setting. Interestingly, the literature follows divergence of two approaches: combining neural networks with classical methods (i.e., hybrid approach), and pure data-driven methods. Hybrid methods [10, 11] use neural networks (e.g., SuperGlue [32, 33]) to perform feature extraction before predicting the sensor extrinsic through classical optimization 2 Figure 1: Overall architecture of BEVCALIB. The overall pipeline of our model consists of BEV feature extraction, FPN BEV Encoder, and geometry-guided BEV decoder (GGBD). For BEV feature extraction (3.2), the inputs of the camera and LiDAR are extracted into BEV features through different backbones separately, then fused into shared BEV feature space. The FPN BEV encoder is used to improve the multi-scale geometric information of the BEV representations. For geometry-guided BEV decoder (3.3) utilizes novel feature selector that efficiently decodes calibration parameters from BEV features. LR, LT , and LP are loss functions introduced at 3.4. methods. Another recent hybrid approach, MDPCalib [34], utilizes sensor motion estimates as coarse registration, followed by neural network prediction of 2D-3D correspondence for calibration refinement. On the other hand, data-driven methods train and evaluate neural networks on datasets such as KITTI [7] and NuScenes [14]. Early learning-based methods [8, 9] encoded the image and LiDAR point cloud before treating the extrinsic prediction as regression problem. More recently, some works [13, 35] use neural radiance fields (NeRF [36, 37]) as pseudo-targets to ensure explicit geometric alignment between the image and point cloud representations. Furthermore, 3D Gaussian Splatting [38], another volumetric rendering method, is also employed [39] to achieve accurate calibration with more efficient training compared to NeRF. Birds-eye View Feature. Birds-eye view feature space has been used [40, 41] to structure 3D sensor data of the environment into 2D feature plane. It provides framework to efficiently extract features from an individual modality [18, 42, 43] or multiple modalities [2] based on geometric alignment. In recent works, the BEV feature has been adopted to address wide range of tasks, such as object detection [19, 20], HD-map construction [21, 22], place recognition [23, 24], occupancy perception [25, 26], and world model [27, 28]. These works demonstrate the great potential of BEV features on various tasks. Compared to previous works [9, 17] that use mis-calibrated depth image as LiDAR input, the BEV feature offers more accurate and structured geometric representation. The closest to our work is CalibRBEV [44], but it only focuses on cameras. The work encodes detection bounding boxes into BEV representation and applies cross-attention with image features to predict calibration parameters. However, the usage of bounding boxes offers strong prior knowledge that over-simplifies the calibration process. In contrast, BEVCALIB calibrates from raw LiDAR data, which is much more challenging but provides more potential for accuracy and robustness to corner cases. To the best of our knowledge, BEVCALIB is the first cross-modality LiDAR-camera extrinsic calibration model using BEV features."
        },
        {
            "title": "3 Methodology\n3.1 Architecture Overview",
            "content": "BEVCALIB is designed as target-less LiDAR-camera calibration model that takes scene consisting of single image and the full-scene LiDAR data as input and predicts the calibration parameters from LiDAR to camera. Figure 1 shows an overall architecture of BEVCALIB. It first extracts modality-specific 3D features from camera images and LiDAR using separate backbones (3.2). These features are then projected and fused into unified BEV representation to capture both semantic and geometric information. To enhance the BEVs spatial capability, we aggregate multi-scale features by Feature Pyramid Network (FPN) BEV Encoder. Next, we propose novel GeometryGuided BEV feature Decoder (GGBD, 3.3). It first employs geometry-guided feature selector"
        },
        {
            "title": "Symbol Dimension Description",
            "content": "Table 1: Notation Summary Tgt Tinit Tpred RHW 3 RN 3 R44 R44 R44 R44 R44 RGB image captured by camera Point clouds captured by lidar, where Pi = [Xi, Yi, Zi] Intrinsic matrix of camera Ground truth transformation from lidar to camera Random noise input superimposed on Tinit Initial guess extrinsic matrix input (including T) Prediction extrinsic matrix as correction to Tinit guided by the coordinates derived from 3D image features, allowing the model to focus on spatially meaningful regions. Finally, it incorporates refinement module to decode calibration parameters from selected features for efficient and effective training. Following the convention of learningbased calibration methods [45, 9], Table 1 summarizes the notations to describe our method. Specifically, the image branch of BEVCALIB takes image input I, and utilizes Tinit and to generate 3D frustum feature 3D (see more details in 3.2). Simultaneously, the LiDAR branch encoded LiDAR input to voxel feature 3D . These features are then fused into BEV features FB, which is subsequently decoded by GGBD component to get the prediction Tpred. In the training and evaluation process, the initial extrinsic matrix is constructed by superimposing random noise on top of the groundtruth Tgt. Hence, Tinit = Tgt (see more details in 3.2). Since represents the random noise, larger means Tinit will have larger misalignment and make the problem more challenging. In our setting, we consider various magnitudes of perturbation up to {1.5m, 20} as the noise range, representing realistic and challenging calibration scenario. For evaluation, BEVCALIB takes I, , K, and Tinit as input, output prediction Tpred to compensate for the injected noise. The final LiDAR to camera extrinsic prediction is ˆTgt = 1 pred Tinit. This strategy is useful to control the difficulty of the calibration problem without label leakage."
        },
        {
            "title": "3.2 BEV Feature Extraction",
            "content": "BEV feature has an inherent geometric meaning, as each feature in BEV space corresponds to specific area in the real world. In our setting, we use the LiDARs coordinate as the world coordinate, which also serves as the BEV coordinate. Inspired by the previous cross-modal approaches [18], we adopt similar paradigm that processes each modality separately and fuses them into unified BEV feature space. Specifically, the LiDAR branch processes the input point cloud using sparse RNLXY Z, which is then flattened to convolutional backbone to produce voxel feature 3D R(NLZ)XY , where X, are the spatial shape of BEV plane, and is the BEV features B2D number of vertical voxels along the height axis. The image branch leverages 2D backbone and an LSS [42] module. The model first extracts RfH fW NC from camera input I, where fH , fW are the shape of the image feature 2D image feature. The LSS module defines discrete depth set for each pixel (u, v), termed as = {dmin + dmaxdmin , where is the number of discrete depth bins. For each pixel (u, v), LSS produces points, accumulating frustum with fH fW points in total. The RDfH fW NC , and the 3D positions in the corresponding 3D features are represented as 3D camera coordinate is defined as PC RDfH fW 3. To give the model an initial guess of the init PC]1:3. position, the frustum coordinates are transformed into world coordinates by Finally, we can get the cameras BEV features B2D = [T 1 RNC XY using BEV pooling [2]. i}D1 i=0 D1 To get unified BEV representation, we use 1 1 convolution to fuse features from different ]) RNBXY . We then adopt an FPN BEV Encoder to modalities, i.e., FB = Conv1D([B2D enhance the multi-scale geometric information of BEV representation. , B2D"
        },
        {
            "title": "3.3 Geometry-Guided BEV Decoder (GGBD)",
            "content": "Based on the geometric BEV representation of the scene, we further propose Geometry-Guided BEV feature Decoder to learn meaningful geometry relationships between the camera and the Li4 Figure 2: Overall Architecture of Geometry-Guided BEV Decoder (GGBD). The GGBD component contains feature selector (left) and refinement module (right). The feature selector calculates the positions of BEV features using Equation 1. The corresponding positional embeddings (PE) are added to keep the geometry information of the selected feature. After the decoder, the refinement module adds an average-pooling operation to aggregate high-level information, following two separate heads to predict translation and rotation parameters. DAR. As illustrated in Figure 2, the decoder consists of two stages: feature selector and refinement module. The BEV feature selector guides the model to focus on the BEV features with meaningful spatial information, while the refinement module aggregates high-level features and helps to predict the final extrinsic parameters. Geometry-Guided BEV Feature Selector. Specifically for the feature selector, following the image branch of BEV feature extraction, we take the 3D feature positions as anchors for cross-modal interaction by projecting them into BEV space. Specifically, for 3D position pc = (x, y, z) , its corresponding BEV space coordinate is calculated by xB = where is the size of the resolution of BEVs grids. We define the projection operation as Proj(p) = (xB, yB), the set of selected BEV feature positions can be formulated as PB = Set({Proj(p)p , yB = 2 + 2 + (1) }) Since the BEV space is unified fused space shared by different modalities, such projection positions (xB, yB) PB naturally provide strong spatial prior for different modalities. This strategy inherently focuses on the overlapping regions between the camera and the LiDAR, acting as an implicit geometric matcher while eliminating redundant features. Refinement Module. To illustrate the strength and generalizability of our geometric selector, we only use vanilla self-attention [46] as our refinement module. The whole process of the GeometryGuided BEV Decoder (GGBD) can be written as GGBD(P C , FB) = Self-Attention (ϕQ(Fδ), ϕK(Fδ), ϕV (Fδ)) (2) Fδ = {FB[:, xB, yB] (xB, yB) PB} (3) After GGBD, we apply an average-pooling operation to aggregate the feature. Subsequently, two separate multilayer perceptrons (MLPs) are used to predict translation and rotation, respectively. Finally, the predicted components are assembled into the final prediction Tpred."
        },
        {
            "title": "3.4 Calibration Optimization",
            "content": "BEVCALIB outputs translation vector R3 and rotation quaternion R4, the supervision ˆr and ˆt is derived from ˆTpred = Tinit 1 matrix converted from quaternion ˆr. To effectively optimize the extrinsic calibration, we design set of loss functions that focus on rotation-only, translation-only, and joint calibration. , where Q2M(ˆr) denotes the rotation Q2M(ˆr) 0 gt = (cid:21) ˆt 1 (cid:20) Rotation Loss. For rotation supervision, we adopt geodesic loss [47] based on quaternion distance , where = ˆr1 is the relative quaternion between and Lang = 2arctan2 ˆr, 2 is l2 norm and is the absolute value. We also utilize normalization loss to restrict the (cid:12) (cid:12)q(1:3) (cid:12)2, (cid:12) (cid:12) (cid:12)q(0) (cid:17) (cid:12) (cid:12) (cid:16)(cid:12) (cid:12) (cid:12) (cid:12) 5 predicted quaternion to be valid rotation i.e., Lnorm = (r2 1)2. Finally, the rotation loss is LR = Lang + λnormLnorm. Translation Loss. For translation optimization, we use Smooth-L1 loss to optimize it. We find that this loss alone is sufficient to optimize translation effectively, therefore, we dont incorporate additional objectives. The translational loss follows LT = Smooth-L1 (cid:0)t, ˆt(cid:1). Reprojection Loss. We use the point cloud reprojection loss introduced by LCCNet [9]. Specifically, it can directly supervise the alignment of the transformed point cloud using the predicted transpred Tinit Pi Pi2, lation and rotation jointly, which can be written as LP = 1 where is the number of points in the given point cloud . i=1 1 1 (cid:80)N gt Total Loss Function. In summary, the combined loss function is = λRLR + λT LT + λP CLP C. Implementation Details. We utilize sparse convolution [43] as the backbone for LiDAR and adopt Swin-Transformer [48] combined with LSS [42] as the backbone for the camera. For indoor datasets, we constrain the environment range to 9-meter radius, while for outdoor datasets, we extend the range to 90 meters. We use weight vector of (1.0, 0.5, 0.5) for the (LR, LT , LP C) losses, respectively, throughout all training runs. We trained BEVCALIB using only single NVIDIA RTX 6000 Ada GPU with batch size of 16 for 500 epochs on each dataset (4). We applied the AdamW optimizer with weight decay of 1e4 and an initial learning rate of 5e5, which is decayed by factor of 0.5 using StepLR scheduler."
        },
        {
            "title": "4 Evaluation",
            "content": "Datasets. To reproduce and compare with existing approaches, we use two of the most popular benchmarks in the LiDAR-camera calibration literature, KITTI [7] and NuScenes [14]. The comparison can contextualize BEVCALIB with related work. In the meantime, we also collected our own heterogenous extrinsic dataset CALIBDB. CALIBDB includes 1244 traces. Each trace contains 12 seconds of continuous frames of image, LiDAR point cloud, and their dynamic extrinsic data recorded at 10 Hz. Our results show that BEVCALIB generalizes well on CALIBDB while this diversity poses significant challenges for existing calibration methods. Metrics. We evaluate the translation and rotation error magnitude and break them down along each axis. The translation error is calculated as the L1 norm between the prediction and the groundtruth tgt tpred. For rotation, we calculate the difference between the rotation matrices of prediction (Rpred) and groundtruth (Rgt), i.e., RpredRT gt, and extract the Euler angles. Baselines. We compare BEVCALIB with two sets of baseline results, original results reported in the publications and reproduced results from open-source methods. In the first set, we include methods in the literature which has similar evaluation setup (e.g., noise range) such that the results can be compared fairly. These baselines include Fu et al. [49], LCCRAFT [12], LCCNet [9], SOAC [35], 3DGS-Calib [39], and CalibFormer [50]. In the second set, we tried our best to exhaust all publicly available and reproducible methods, including CalibAnything [11], Koide3 [10], Regnet [8], and CalibNet [45]. We use the official sources of CalibAnything and Koide3, and the officially recommended implementations of Regnet and CalibNet1. Notably, LCCNet [9] and LCCRAFT [12] use an iterative refinement approach during inference. Their methods first take random guess similar to ours, then perform multiple inference passes, with each iterations output serving as input for the next, progressively refining the calibration parameters. In contrast, our model utilizes one-stage methodology; therefore, for fair comparison, we only compare to the single-pass results. Several works are excluded from our evaluation either because they are not reproducible or cannot be compared fairly due to methodology differences. For example, CalibDepth [51] does not report single-pass results. MDPCalib [34] employs hybrid approaches that needs additional heavy computation. 1 We refer to the recommended unofficial implementations for CalibNet (https://github.com/ gitouni/CalibNet_pytorch) and Regnet (https://github.com/aaronlws95/regnet). 6 Table 2: Comparing with Original Results from Literature on KITTI [7] Noise (Trans. Rot.) Method (1.5m, 20) (0.5m, 5) (0.25m, 10) (0.2m, 20) Regnet [8] Fu et al. [49] LCCRAFT [12] LCCNet [9] BEVCalib (Ours) CalibAnything [11] Koide3 [10] SOAC [35] 3DGS-Calib [39] BEVCalib (Ours) CalibFormer [50] BEVCalib (Ours) CalibNet [45] BEVCalib (Ours) Magnitude Et(cm) ER() 0.50 0.28 1.44 0.94 0.08 10.7 3.3 37.6 15.0 2.4 9.8 21.1 7.8 9.6 2. 2.1 1.8 8.5 1.8 0.35 0.60 0.30 0.45 0.06 0.29 0.06 0.93 0.04 Translation (cm) Rotation () Roll Pitch Yaw 7 2.3 0.5 31.4 11.8 14.4 1.8 1.6 7 2.0 0.9 12.9 5.2 5.3 0.5 0.5 4 1.2 0.6 16.2 7.6 4.1 1.5 2.9 0.36 0.1 0.0 1.30 0.2 0.0 0.0 0.1 0.25 0.2 0.0 0.42 0.7 0.6 0.1 0. 0.24 0.2 0.0 0.47 0.6 0.5 0.0 0.1 5.6 4.0 6.9 5.6 5.0 4.4 12.2 14.9 7.8 3.5 (xyz together) 9.6 2.1 (xyz together) 0.3 0.3 1.8 1.6 6.3 6.2 15.7 9.7 0.2 0.2 0.4 0. 1.7 3.5 0.0 0.1 0.2 0.1 0.4 0.2 0.2 0.1 0.2 0.1 0.3 0.2 (rpy together) 0.5 0.2 (rpy together) 0.1 0.1 0.0 0.0 1.1 1.5 1. 4.2 1.4 1.3 0.9 0.3 0.2 1.6 0.2 0.2 1.6 1.0 2.0 7.2 1.0 2.3 0.08 0.0 0. 0.15 0.0 0.1 0.26 0.1 0.1 0.90 0.0 0.1 0.09 0.0 0.0 0.18 0.0 0.0 Table 3: Comparing with Original Results from Literature on NuScenes [14]"
        },
        {
            "title": "Noise",
            "content": "(Trans. Rot.) Method (0.5m, 5) CalibAnything [11] Koide3 [10] BEVCalib (Ours) Magnitude Et(cm) ER() 0.41 0.75 0.13 19.7 26.7 4. Translation (cm) Rotation () Z"
        },
        {
            "title": "Yaw",
            "content": "11.0 7.4 16.5 12.1 1.2 0.7 10.0 5.5 15.6 13.8 6.7 4.1 13.0 12.2 14 11.9 2.4 2.4 0.2 0.1 0.5 0.2 0.2 0.1 0.3 0.2 0.4 0.3 0.1 0.1 0.2 0.1 0.4 0.2 0.2 0. Quantitative Results. Table 2 and Table 3 compare BEVCALIB with the originally reported results from the publications on KITTI and NuScenes datasets. Since each of the existing models was trained and evaluated using different noise settings, we group them into different clusters and evaluate BEVCALIB under the same noise settings for fair comparison. On KITTI dataset, BEVCALIB has only few centimeter translation error, outperforming the best baselines by an average of 14.29% - 78.82%, and less than 0.1 rotation error, outperforming the best baselines by an average of 71.43% - 95.70% under various noise conditions. On Nuscenes, BEVCALIB has slightly bigger error but still outperforms the best baseline by 78.17% in translation, 68.29% in rotation. Notably, although BEVCALIB is trained under the largest noise (1.5m, 20), it shows extremely robustness when evaluated on smaller noise, overcoming the noise sensitivity that cripples previous methods such as LCCNet [9]. In addition, BEVCALIB demonstrates remarkable rotation prediction accuracy for all three angles (roll, pitch, yaw) with error below 0.2, achieving near-perfect result that outperforms any previous methods. Table 4 compares BEVCALIB with the reproducible baselines on KITTI, NuScenes, and CALIBDB. In our exhaustive effort searching for reproducible baselines, we find that the open-source space in this LiDAR-camera calibration domain is rather scarce (very few checkpoints) and underperforming despite the abundant literature. Hence, our open-source effort will significantly improve the performance of publicly available calibration tools. Specifically, Table 4 shows that BEVCALIB outperforms the best open-source baselines by (92.75%, 89.22%) on KITTI dataset and by (92.69%, 93.62%) on NuScenes dataset, in terms of (translation, rotation), respectively. While BEVCALIB approaches near-zero error on most, if not all, samples, CalibNet and Koide3 struggle with predicting the correct z-component while Regnet and CalibAnything struggle with all components on KITTI and NuScenes datasets. Across the board, when an initial guess is required, random noise between [1.5m, 1.5m] and [20, 20] has been applied. On our internal dataset CALIBDB, BEVCALIB still outperforms the best open-source baselines by (60.21%, 24.99%). Compared to KITTI and NuScenes, the error slightly increased for both translation and rotation. This can be attributed to the inherent difficulty of the heterogeneous extrinsics collected in CALIBDB. This characteristic is further illustrated in the error distribution shown in Figure 3. Compared to the error distribution when evaluating on KITTI, there is larger gap between BEVCALIB and the baselines evaluated on CALIBDB. 7 Table 4: Evaluation Results with Reproducible Open-source Baselines Dataset Method KITTI [7] NuScenes [14] CALIBDB Regnet [8] CalibNet [45] CalibAnything [11] Koide3 [10] BEVCalib (Ours) Regnet [8] CalibNet [45] CalibAnything [11] Koide3 [10] BEVCalib (Ours) Regnet [8] CalibNet [45] CalibAnything [11] Koide3 [10] BEVCalib (Ours) Magnitude Et(cm) ER() 18.7 145.4 163.7 33.1 6.8 101.1 0.9 35.4 0.1 2.4 196.1 83.6 89.7 82.1 6.0 216.4 95.5 86.2 96.8 38.0 93.6 87.5 4.7 149.4 0. 24.1 180.4 3.3 16.5 2.5 Translation (cm) Rotation () Z Roll Pitch Yaw 101.2 0.9 4.0 3.8 59.0 38.0 4.7 6.0 1.8 1.6 95.2 0.1 5.1 4.5 58.3 28.3 2.2 1.8 1.3 1.0 93.0 27.1 24.7 14.1 31.0 22.8 24.0 13.1 8.4 11. 67.8 1.2 6.5 4.0 56.0 30.2 9.7 3.9 0.5 0.5 72.5 0.4 32.0 7.4 51.2 25.4 32.3 2.2 5.4 4.5 43.2 13.9 17.4 13.7 28.4 25.0 17.3 14.4 36.4 31.6 79.4 0.8 32.2 5.9 60.0 33.7 33.7 6.4 1.5 2.9 155.3 0.1 77.1 5.9 45.0 30.3 75.5 1.8 2.3 2.4 190.6 5.8 90.6 9.2 75.3 54.2 92.2 4.1 6.9 6. 16.3 0.1 98.4 49.9 2.9 2.1 0.5 0.9 0.0 0.1 34.4 0.6 87.4 3.4 2.1 2.1 85.2 38.5 0.2 0.1 17.6 5.3 72.6 31.6 2.3 2.2 5.3 4.9 1.2 1.2 9.2 0.0 85.9 2.9 3.2 2.4 0.5 0.6 0.1 0.1 71.5 0.2 2.1 1.9 3.5 2.8 88.6 0.6 0.2 0.1 2.0 1.3 77.3 4.1 2.1 2.0 10.2 1.7 1.7 2. 0.8 0.3 98.7 51.3 5.2 3.0 0.6 0.3 0.0 0.1 49.6 0.4 3.0 2.6 2.4 2.3 84.9 38.8 0.2 0.2 16.4 9.3 145.9 27.5 1.0 0.9 11.9 9.2 1.3 1.5 (a) CALIBDB (b) KITTI Figure 3: Error Distribution of BEVCALIB and Other Baselines on CALIBDB and KITTI Qualitative Results. Figure 4 presents qualitative comparison by overlaying the LiDAR point clouds over the image given each methods predicted extrinsic. Regnet and CalibAnythings overlays are misaligned due to the large error in rotation and translation, so the point cloud is not level with the ground. BEVCALIB and Koide3 are closer to the ground-truth overlay, but there are objects where Koide3s overlay is slightly misaligned, e.g., the misaligned cars in the left column, the traffic sign in the middle column, and the pole and tree in the right column. In contrast, BEVCALIBs overlays do not show these misalignments. Overall, the overlays reflect the results in Table 4. Figure 4: Qualitative results. comparison of LiDAR-camera overlays from KITTI sequences. From top to bottom: ground-truth, BEVCALIB, Koide3 [10], CalibAnything [11], Regnet [8]."
        },
        {
            "title": "Method",
            "content": "Table 5: Ablation Results Translation (cm) Rotation () Z"
        },
        {
            "title": "Yaw",
            "content": "BEVCALIB BEV selector (use all features) with Deformable Attention 8.4 11.0 23.1 19.5 37.0 30.8 36.4 31.6 37.5 32.6 37.0 31.9 6.9 6.3 32.4 17.6 34.2 27.3 1.2 1.2 2.5 2.4 5.6 4.8 1.7 2.9 5.0 3.9 5.3 4. 1.3 1.5 2.7 2.4 5.3 4.5 Abalation Study. We first conduct an ablation to show the efficacy of the Geometry-Guided BEV feature selector in calibration optimization. The GGBD component (3.3) consists of BEV selector and refinement module. We investigate how different BEV feature selection strategies affect the refinement module. Table 5 shows that using all BEV features introduces too much redundant information to the model, significantly confusing the model about the cross-modality feature correspondence. We also experimented using different attention modules, e.g., deformable attention [52], to capture the relationship between Camera and LiDAR, but the results are less ideal."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce BEVCALIB, the first LiDAR-camera extrinsic calibration model using BEV features. Geometry-guided BEV decoder can effectively and efficiently capture scene geometry, enhancing calibration accuracy. Results on KITTI, NuScenes, and our own indoor dataset with dynamic extrinsics illustrate that our approach establishes new state of the art in learning-based calibration methods. Under various noise conditions, BEVCALIB outperforms the best baseline in literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation) respectively. Also, BEVCALIB improves the best reproducible baseline by one order of magnitude, making an important contribution to the scarce opensource space in LiDAR-camera calibration."
        },
        {
            "title": "References",
            "content": "[1] A. J. Sathyamoorthy, J. Liang, U. Patel, T. Guan, R. Chandra, and D. Manocha. Densecavoid: Real-time navigation in dense crowds using anticipatory behaviors. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1134511352, 2020. doi:10.1109/ ICRA40945.2020.9197379. [2] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han. Bevfusion: Multi-task multisensor fusion with unified birds-eye view representation. In IEEE International Conference on Robotics and Automation (ICRA), 2023. [3] S. R. Mhatre and J. W. Bakal. Deepfusion: novel deep learning technique for enhanced In 2024 3rd International Conference on Automation, Computing image super-resolution. and Renewable Systems (ICACRS), pages 991998, 2024. doi:10.1109/ICACRS62842.2024. 10841630. [4] J.-K. Huang and J. W. Grizzle. Improvements to Target-Based 3D LiDAR to Camera Calibration. IEEE Access, 8:134101134110, 2020. doi:10.1109/ACCESS.2020.3010734. [5] Q. Zhang and R. Pless. Extrinsic calibration of camera and laser range finder (improves In 2004 IEEE/RSJ International Conference on Intelligent Robots and camera calibration). Systems (IROS) (IEEE Cat. No.04CH37566), volume 3, pages 23012306 vol.3, 2004. doi: 10.1109/IROS.2004.1389752. [6] G. Yan, F. He, C. Shi, P. Wei, X. Cai, and Y. Li. Joint camera intrinsic and lidar-camera In 2023 IEEE International Conference on Robotics and Automation extrinsic calibration. (ICRA), pages 1144611452, 2023. doi:10.1109/ICRA48891.2023.10160542. [7] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. [8] N. Schneider, F. Piewak, C. Stiller, and U. Franke. Regnet: Multimodal sensor registration using deep neural networks. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 1803 1810, 2017. doi:10.1109/IVS.2017.7995968. [9] X. Lv, B. Wang, Z. Dou, D. Ye, and S. Wang. Lccnet: Lidar and camera self-calibration using cost volume network. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 28882895, 2021. doi:10.1109/CVPRW53098.2021. 00324. [10] K. Koide, S. Oishi, M. Yokozuka, and A. Banno. General, single-shot, target-less, and automatic lidar-camera extrinsic calibration toolbox. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1130111307. IEEE, 2023. [11] Z. Luo, G. Yan, X. Cai, and B. Shi. Zero-training lidar-camera extrinsic calibration method using segment anything model. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1447214478, 2024. doi:10.1109/ICRA57147.2024.10610983. [12] Y.-C. Lee and K.-W. Chen. Lccraft: Lidar and camera calibration using recurrent all-pairs field transforms without precise initial guess. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1666916675, 2024. doi:10.1109/ICRA57147.2024.10610756. [13] Q. Herau, N. Piasco, M. Bennehar, L. Roldao, D. Tsishkou, C. Migniot, P. Vasseur, and C. Demonceaux. Moisst: Multimodal optimization of implicit scene for spatiotemporal calIn 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems ibration. (IROS), page 18101817. IEEE, Oct. 2023. doi:10.1109/iros55552.2023.10342427. URL http://dx.doi.org/10.1109/IROS55552.2023.10342427. 10 [14] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom. nuscenes: multimodal dataset for autonomous driving. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1161811628, 2020. doi:10.1109/CVPR42600.2020.01164. [15] P. Xiao, Z. Shao, S. Hao, Z. Zhang, X. Chai, J. Jiao, Z. Li, J. Wu, K. Sun, K. Jiang, Y. Wang, and D. Yang. Pandaset: Advanced sensor suite dataset for autonomous driving. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 30953101, 2021. doi:10.1109/ITSC48978.2021.9565009. [16] J. Shi, Z. Zhu, J. Zhang, R. Liu, Z. Wang, S. Chen, and H. Liu. Calibrcnn: Calibrating camera and lidar by recurrent convolutional neural network and geometric constraints. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 10197 10202, 2020. doi:10.1109/IROS45743.2020.9341147. [17] Y. Xiao, Y. Li, C. Meng, X. Li, J. Ji, and Y. Zhang. Calibformer: transformer-based automatic lidar-camera calibration network, 2024. URL https://arxiv.org/abs/2311.15241. [18] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and J. Dai. Bevformer: Learning birdseye-view representation from multi-camera images via spatiotemporal transformers. arXiv preprint arXiv:2203.17270, 2022. [19] Y. Wang, V. Guizilini, T. Zhang, Y. Wang, H. Zhao, , and J. M. Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In The Conference on Robot Learning (CoRL), 2021. [20] H. Liu, Y. Teng, T. Lu, H. Wang, and L. Wang. Sparsebev: High-performance sparse 3d object detection from multi-camera videos, 2023. URL https://arxiv.org/abs/2308.09244. [21] Q. Li, Y. Wang, Y. Wang, and H. Zhao. Hdmapnet: An online hd map construction and evaluation framework. arXiv preprint arXiv:2107.06307, 2021. [22] S. Choi, J. Kim, H. Shin, and J. W. Choi. Mask2map: Vectorized hd map construction using birds eye view segmentation masks. In European Conference on Computer Vision, 2024. [23] J. Ross, O. Mendez, A. Saha, M. Johnson, and R. Bowden. Bev-slam: Building globallyconsistent world map using monocular vision. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 38303836, 2022. doi:10.1109/IROS47612. 2022.9981258. [24] L. Luo, S. Zheng, Y. Li, Y. Fan, B. Yu, S.-Y. Cao, J. Li, and H.-L. Shen. Bevplace: Learning lidar-based place recognition using birds eye view images. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 86668675, 2023. doi:10.1109/ICCV51070. 2023.00799. [25] Y. Zhang, Z. Zhu, and D. Du. Occformer: Dual-path transformer for vision-based 3d semantic occupancy prediction. arXiv preprint arXiv:2304.05316, 2023. [26] J. Li, X. He, C. Zhou, X. Cheng, Y. Wen, and D. Zhang. Viewformer: Exploring spatiotemporal modeling for multi-view 3d occupancy perception via view-guided transformers. arXiv preprint arXiv:2405.04299, 2024. [27] L. Zhang, Y. Xiong, Z. Yang, S. Casas, R. Hu, and R. Urtasun. Learning unsupervised world models for autonomous driving via discrete diffusion. ICLR, 2024. [28] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, and H. Wang. Bevworld: multimodal world model for autonomous driving via unified bev latent space, 2024. URL https://arxiv.org/abs/2407.05679. 11 [29] S. Verma, J. S. Berrio, S. Worrall, and E. Nebot. Automatic extrinsic calibration between camera and 3d lidar using 3d point and plane correspondences. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 39063912, 2019. doi:10.1109/ITSC.2019. 8917108. [30] R. Ishikawa, T. Oishi, and K. Ikeuchi. Lidar and camera calibration using motion estimated by sensor fusion odometry, 2018. URL https://arxiv.org/abs/1804.05178. [31] G. Pandey, J. R. McBride, S. Savarese, and R. M. Eustice. Automatic targetless extrinsic calibration of 3d lidar and camera by maximizing mutual information. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, AAAI12, page 20532059. AAAI Press, 2012. [32] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich. Superglue: Learning feature matching with graph neural networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 49374946, 2020. doi:10.1109/CVPR42600.2020.00499. [33] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, In 2023 IEEE/CVF A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick. Segment anything. International Conference on Computer Vision (ICCV), pages 39924003, 2023. doi:10.1109/ ICCV51070.2023.00371. [34] K. Petek, N. Vodisch, J. Meyer, D. Cattaneo, A. Valada, and W. Burgard. Automatic targetless camera-lidar calibration from motion and deep point correspondences. IEEE Robotics and Automation Letters, 9(11):99789985, 2024. [35] Q. Herau, N. Piasco, M. Bennehar, L. Roldao, D. Tsishkou, C. Migniot, P. Vasseur, and C. Demonceaux. Soac: Spatio-temporal overlap-aware multi-sensor calibration using neural radiance fields. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1513115140, 2024. doi:10.1109/CVPR52733.2024.01433. [36] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99106, Dec. 2021. ISSN 0001-0782. doi:10.1145/3503250. URL https://doi.org/10.1145/ 3503250. [37] Z. Yang, G. Chen, H. Zhang, K. Ta, I. A. Bˆarsan, D. Murphy, S. Manivasagam, and R. Urtasun. Unical: Unified neural sensor calibration. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XXXVI, page 327345, Berlin, Heidelberg, 2024. Springer-Verlag. ISBN 978-3-031-72763-4. doi:10.1007/ 978-3-031-72764-1 19. URL https://doi.org/10.1007/978-3-031-72764-1_19. [38] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https: //repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. [39] Q. Herau, M. Bennehar, A. Moreau, N. Piasco, L. Roldao, D. Tsishkou, C. Migniot, P. Vasseur, and C. Demonceaux. 3dgs-calib: 3d gaussian splatting for multimodal spatiotemporal calibration, 2024. [40] H. Li, C. Sima, J. Dai, W. Wang, L. Lu, H. Wang, J. Zeng, Z. Li, J. Yang, H. Deng, H. Tian, E. Xie, J. Xie, L. Chen, T. Li, Y. Li, Y. Gao, X. Jia, S. Liu, J. Shi, D. Lin, and Y. Qiao. Delving into the devils of birds-eye-view perception: review, evaluation and recipe. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 120, 2023. doi:10.1109/ TPAMI.2023.3333838. [41] Y. Ma, T. Wang, X. Bai, H. Yang, Y. Hou, Y. Wang, Y. Qiao, R. Yang, and X. Zhu. Visioncentric bev perception: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(12):1097810997, 2024. doi:10.1109/TPAMI.2024.3449912. 12 [42] J. Philion and S. Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In Proceedings of the European Conference on Computer Vision, 2020. [43] Y. Yan, Y. Mao, and B. Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10), 2018. ISSN 1424-8220. doi:10.3390/s18103337. URL https://www.mdpi.com/ 1424-8220/18/10/3337. [44] W. Liao, S. Qiang, X. Li, X. Chen, H. Wang, Y. Liang, J. Yan, T. He, and P. Peng. Calibrbev: Multi-camera calibration via reversed birds-eye-view representations for autonomous In Proceedings of the 32nd ACM International Conference on Multimedia, MM driving. 24, page 91459154, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400706868. doi:10.1145/3664647.3680572. URL https://doi.org/10.1145/ 3664647.3680572. [45] G. Iyer, R. K. Ram, J. K. Murthy, and K. M. Krishna. Calibnet: Geometrically supervised extrinsic calibration using 3d spatial transformer networks. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, Oct. 2018. doi:10.1109/iros. 2018.8593693. URL http://dx.doi.org/10.1109/IROS.2018.8593693. [46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. [47] A. Kendall, M. Grimes, and R. Cipolla. Posenet: convolutional network for real-time 6-dof camera relocalization. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV 15, page 29382946, USA, 2015. IEEE Computer Society. ISBN 9781467383912. doi:10.1109/ICCV.2015.336. URL https://doi.org/10.1109/ICCV. 2015.336. [48] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021. URL https://arxiv.org/ abs/2103.14030. [49] L. F. T. Fu and M. F. Fallon. Batch differentiable pose refinement for in-the-wild camera/lidar extrinsic calibration. In CoRL, pages 13621377, 2023. URL https://proceedings.mlr. press/v229/fu23a.html. [50] Y. Xiao, Y. Li, C. Meng, X. Li, J. Ji, and Y. Zhang. Calibformer: transformer-based automatic lidar-camera calibration network. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1671416720, 2024. doi:10.1109/ICRA57147.2024.10610018. [51] J. Zhu, J. Xue, and P. Zhang. Calibdepth: Unifying depth map representation for iterative lidarcamera online calibration. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 726733, 2023. doi:10.1109/ICRA48891.2023.10161575. [52] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang. Vision transformer with deformable attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 47944803, June 2022."
        }
    ],
    "affiliations": [
        "University of California, Riverside",
        "University of Southern California"
    ]
}