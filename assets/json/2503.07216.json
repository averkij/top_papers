{
    "paper_title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates",
    "authors": [
        "Sangwoo Park",
        "Seanie Lee",
        "Byungjoo Kim",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private client parameters are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets."
        },
        {
            "title": "Start",
            "content": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Sangwoo Park 1 Seanie Lee 1 Byungjoo Kim 1 Sung Ju Hwang 1 2 5 2 0 2 0 1 ] . [ 1 6 1 2 7 0 . 3 0 5 2 : r Abstract Federated Learning (FL) is widely used framework for training models in decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set In this framework, each of client parameters. client randomly selects subparameters of LowRank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the clients private dataset, only the non-private client parameters are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets. 1. Introduction Vision-language models (VLMs) (Alayrac et al., 2022; Zhu et al., 2023; Liu et al., 2023) have demonstrated remarkable performance in various multi-modal tasks, such as visual question answering (Dai et al., 2023; Liu et al., 2023) and image captioning (Li et al., 2023). However, deploy1Graduate School of AI, KAIST 2DeepAuto.ai. Correspondence to: Sangwoo Park <swgger@kaist.ac.kr>, Seanie Lee <lsnfamily02@kaist.ac.kr>. Preprint. Copyright 2025 by the author(s). 1 ing VLMs in real-world scenarios raises significant concerns about data privacy. These models can easily memorize training data (Carlini et al., 2021, 2023), including sensitive information such as private photographs or medical diagnosis records. Adversarial attackers can exploit this vulnerability to perform membership inference attack (Shokri et al., 2017), which aims to detect whether specific data instance is part of the training dataset. Federated learning (FL; McMahan et al., 2017) is distributed learning framework in which each local client receives global parameters from central server, trains local model on its private dataset, and periodically sends the local model back to the server for aggregation. It offers potential solution to address privacy concerns, as the central server cannot directly access the private dataset. However, naively transmitting local model parameters back to the central server remains vulnerable to membership inference attacks, as attackers can potentially reconstruct the local client model by intercepting its parameters during the aggregation stage. This issue is particularly critical when fine-tuning vision-language models (VLMs), as their large capacity to memorize private training data amplifies the privacy risks. To address the privacy issue, we propose simple yet privacy-enhanced federated learning (FL) framework, dubbed FedRand. In this framework, clients randomly select subset of parameters provided by the server and keep the remaining parameters as client-specific private ones. After updating both the selected parameters and their client-specific private parameters, only the non-private parameters are transmitted back to the server for the model update. Specifically, we first apply Low-Rank Adaptation (LoRA; Hu et al., 2022) matrices and to the pre-trained weight W0 of VLM. The pre-trained weight W0 is fixed and shared across all clients and the server. At each round of updates, each local client model receives the LoRA weights and from the server. Each client then randomly selects either or and initializes the counterpart of the LoRA weights using the parameters from the previous round as client-specific private ones(Figure 1a). After updating both parameters on the clients private training FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Figure 1. (a). At each round r, each local client selects LoRA weight either Ar or Br for initialization from the server and initializes the other counterparts of LoRA weights using the previous rounds client model parameters as private parameters. (b). After updating both parameters, only the non-private parameters are sent back to the server and aggregated to update the LoRA weights of the central server. perimental results demonstrate that FedRand significantly improves the trade-off between accuracy and robustness against membership inference attacks (Figure 2) while reducing communication costs between the server and clients compared to other relevant baselines. Our contributions and findings are summarized below: We show that even fine-tuning VLMs with FL remains vulnerable to membership inference attacks due to the exposure of client model parameters, posing significant privacy concerns. To address these privacy concerns, we propose FedRand. First, client randomly selects subparameters of LoRA weights from the server and updates both the selected parameters and client-specific private parameters. Only the non-private parameters are sent back to the server, preventing the exposure of the full local model parameters. We experimentally demonstrate that FedRand enhances robustness against membership inference attacks while achieving performance comparable to models that communicate full LoRA weights between the server and clients. 2. Related Work Federated learning. Federated Learning (FL) is decentralized machine learning approach that allows multiple clients to collaboratively train shared model without sharing their private data, thereby preserving privacy and security. FedAvg (McMahan et al., 2017), one of the most widely used algorithms in FL, updates global model by averaging the model parameters trained on each clients priFigure 2. Trade-off between task performance (CIDEr) and vulnerability to membership inference attacks (AUROC of MIA) on MSCOCO dataset. dataset, the client-specific parameters remain hidden, and only the remaining parameters are sent back to the server. Finally, the parameters and from the clients are averaged to form the new LoRA weights of the server model (Figure 1b). Since the client-specific private parameters are kept hidden, adversarial attackers cannot fully reconstruct the client model parameters by intercepting the parameters transmitted to the server. This design makes FedRand more robust against membership inference attacks. Furthermore, sending only non-private parameters significantly reduces the communication cost between the server and clients compared to the model that communicates all LoRA weights between the server and clients. We empirically validate our proposed FedRand on visual question answering and image captioning tasks using the ScienceQA (Lu et al., 2022), MSCOCO (Lin et al., 2014), and NoCaps (Agrawal et al., 2019) datasets. Ex2 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates vate dataset. While many variants of FedAvg have been proposed (Li et al., 2020; Yu et al., 2020; Acar et al., 2021; Zhang et al., 2024), they remain vulnerable to membership inference attacks because clients parameters are exposed to the server. In another line of work, methods like FedPer (Arivazhagan et al., 2019) and FedPara (HyeonWoo et al., 2022) distinguish client-specific private parameters from global parameters shared between the server and clients to reduce communication costs. However, although these methods avoid exposing client parameters to the server, they fail to strike balance between accuracy and robustness against membership inference attacks. Membership inference attack. Although FL avoids sharing private data between clients and server by training client models locally and aggregating only the parameters of the client models at the server, clients are still vulnerable to the leakage of privacy-sensitive information. This can occur through membership inference attacks (Shokri et al., 2017), where an attacker detects whether specific data instance is included in private clients dataset. While both the central server and clients can potentially deduce private details from shared information such as model parameters, the majority of works (Hitaj et al., 2017; Melis et al., 2019) focus on client-based membership inference attacks under the strong assumption of secure server. However, server-based membership inference attacks pose significant threat, particularly due to the memorization capacities of VLMs. Jayaraman et al. (2024) have demonstrated this vulnerability through k-nearest neighbor retrieval tests on open-source image datasets, showing that VLMs are prone to retaining training data. Moreover, Li et al. (2024) utilize average top-k Rényi entropy of VLMs output probabilities to distinguish training data from other data, highlighting the vulnerability of VLMs to membership inference attacks. This suggests that malicious use of client models on the server side could lead to data leakage through the memorization of training data by the client models. To address this issue, we propose FedRand, which prevents the exposure of client models to the server and thus enhances robustness against server-based membership inference attacks. 3. Method 3.1. Preliminaries )}nk , z(k) , y(k) client [K] := {1, . . . , K} has access only to its local training dataset Dk = {(x(k) i=1, where Dk Dk = for all k, [K] with = k. Furthermore, the central server does not have direct access to any of the local datasets. For each round of update [R], subset of client indices Sr [K] is randomly chosen with Sr = . Then each client Sr receives the parameter θr from the central server and trains its local model pθ(k) on the dataset Dk as follows: L(θ(k) r,t ; Dk) = θ(k) r,t+1 = θ(k) r,t ; Dk) r,t ηθL(θ(k) 1 (cid:88) nk (x,z,y)Dk log pθ(k) r,t (y x, z), (1) for = 0, . . . , Tk 1, where η > 0 is learning rate and θ(k) 0,0 is initialized with θr. Since fully fine-tuning the VLM is computationally expensive, we apply Low Rank Adaptation (LoRA; Hu et al., 2022) for fine-tuning the weight matrix of the VLM at the l-th layer as: (k,l) r,t = (l) 0 + A(k,l) r,t B(k,l) r,t , (2) r,t r,t and B(k,l) r,t ) rank(W (l) r,t , we denote the parameter θ(k) r,t )}L where (l) is the frozen pre-trained weight matrix of the 0 VLM, and A(k,l) are low-rank matrices, i.e., r,t B(k,l) rank(A(k,l) 0 ). With slight abuse of notation of θ(k) r,t = {(W (l) , B(k,l) 0 , A(k,l) l=1 as the set of the initial pretrained weight matrices and LoRA weight matrices for the client at step in round r. After the local client update, following the FedAvg (McMahan et al., 2017) and FedIT (Zhang et al., 2024), we aggregate the parameters of the local client models and update the server parameter θr = {(W (l) , B(l) )}L r,t l=1 to θr+1 as follows: (cid:33) (cid:32) A(l) r+1 = A(k,l) r,Tk , B(l) r+1 = 0 , A(l) (cid:32) (cid:88) nk mr kSr (cid:88) kSr nk mr (cid:33) B(k,l) r,Tk (3) where mr = (cid:80) nk and nk = Dk. At the next round r+1, the central server model pθr+1 uses its updated weight matrix, kSr (l) r+1 = (l) 0 + A(l) r+1B(l) r+1 (4) for each layer [L]. Let pθ : be vision language model (VLM) with its parameter θ, which takes as input sequence of tokens and an image Z, and outputs another sequence of tokens as response to the input. Here, is the set of all possible input sequences, is the set of all possible images, and is the set of all possible output sequences. In the FL framework, each 3.2. Privacy Enhanced FL: FedRand However, aggregating the parameters of client models at the central server poses serious privacy issue. An adversarial attacker can fully reconstruct the local model by hijacking the LoRA parameters. Since VLMs easily memorize training data (Carlini et al., 2021, 2023; Jayaraman et al., 2024), the attacker can detect whether 3 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates 0 }L Algorithm 1 FedRand 1: Input: VLM pθ with pre-trained weights θ = {W (l) l=1, learning rate η, total round R, number of clients K, number of clients participating for update , probability ρ of choosing A, and batch size b. 0 )}L 0 , A(l) Choose client indices Sr from [K] s.t. Sr = . for each in Sr do 2: Randomly initialize LoRA weights {(A(l) 3: for = 0, . . . , 1 do 4: mr 0, θr {(W (l) 5: 6: 7: 8: 9: 10: (θ(k), ak, nk) client_update(k, θr, E, ρ, η, b, r) mr mr + nk 1{ak=1}, β (cid:80) 1{ak=1} 0 , B(l) , B(l) )}L l=1. l=1 nk mr nk mr end for α (cid:80) kSr kSr nk αmr A(k,l) r,Tk nk βmr B(k,l) r,Tk for = 1, . . . , do if α > 0 then r+1 (cid:80) A(l) kSr,ak= r+1 A(l) A(l) end if if β > 0 then r+1 (cid:80) B(l) r+1 B(l) else 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end for 24: θ {(W (l) 25: return pθ B(l) end if end for else 0 , A(l) , B(l) )}L l=1 kSr,ak=1 particular training data instance is included in the local clients training dataset Dk using membership inference attack (Shokri et al., 2017; Li et al., 2024). }L }L = {(W (l) 0 , A(k,l) To address the issue of exposing the full parameters of local client models to an attacker, we propose FedRand, method in which, during each update round, each client randomly selects either {A(l) l=1 or {B(l) l=1 LoRA weights from the server as initialization, while the remaining components are initialized using the previous rounds client model parameters θ(k) l=1 as private par1,Tk rameters. Only the selected parameters are sent back to the server after updating the client model, whereas the clientspecific private LoRA weights remain hidden. This randomized LoRA subparameter update prevents the attacker from fully recovering the parameters of the local client model, thereby enhancing robustness against membership inference attacks. Furthermore, our proposed method, FedRand, helps save communication costs by reducing the number of parameters sent from clients to the server compared to the FedAvg method. , B(k,l) r1,Tk r1,Tk )}L Algorithm 2 client_update(k, θ, E, ρ, η, b, r) index k, server parameter θr = Input: Client {(W (l) 0 , A(l) l=1, train epochs E, probability ρ of choosing A(l), learning rate η, batch size b, and current round r. , B(l) )}L 2: Tk Dk/b uk Uniform(0, 1), ak 1{uk<ρ} 4: if = 0 then {A(k,l) 0,0 }L {B(k,l) 0,0 }L 6: else l=1 {rand_init(A(l) l=1 {zero_init(B(l) )}L )}L l= l=1 8: 10: 12: if ak = 1 then {A(k,l) r,0 }L {B(k,l) r,0 }L l=1 {A(l) }L l=1 {B(k,l) r1,Tk l=1 else {A(k,l) {B(k,l) r,0 }L r,0 }L l=1 {A(k,l) r1,Tk l=1 {B(l) }L l=1 }L l=1 }L l=1 14: end if end if 16: for = 0, . . . , Tk 1 do r,t l=1 18: 20: r,t )}L 0 , A(k,l) (cid:80) ; B) 1 r,t ηθ(k) Sample mini-batch from the client dataset Dk. , B(k,l) θ(k) r,t {(W (l) L(θ(k) (x,z,y)B log pθ(k) L(θ(k) r,t+1 θ(k) θ(k) r,t ; B) end for 22: Cache {(A(k,l) r,Tk if = 1 then (cid:16) return (cid:17) }L l=1, ak, Dk , B(k,l) r,Tk (y x, z) {A(k,l) Tk 24: )} r,t else 26: return end if (cid:16) {B(k,l) Tk }L l=1, ak, Dk (cid:17) Specifically, at each round [R], each client Sr first samples ak with probability ρ of choosing {A(l) l=1 as follows: }L u(k) Uniform(0, 1), ak = 1 {u(k)<ρ}, (5) where 1 is an indicator function. The binary variable ak {0, 1} indicates whether A(l) is selected. If ak = 1, we initialize A(k,l) from the server and randomly initialize its counterpart, B(k,l) r,0 , with the client parameter B(k,l) from the previous round 1. Otherwise, we reverse the procedure as follows: r,0 with A(l) r1,Tk A(k,l) r,0 = (cid:40) A(l) , A(l) r1,Tk if ak = 1, , otherwise, , B(k,l) r1,Tk B(l) , if ak = 1, otherwise. (cid:40) B(k,l) r,0 = 4 (6) (7) FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates is randomly inifor all layers [L]. Note that A(k,l) 0,0 tialized and B(k,l) is initialized as zero matrix, regard0,0 less of the choice of ak. Then, we update the local client model, initialized with θ(k) )}L l=1, as described in Equation 1, for Tk steps, yielding θ(k) = Tk {(W (l) l=1. After the local update, only the selected LoRA parameters are sent back to the central server and the parameter of the central server model is updated to θr+1 = {(W (l) , B(l) 0 = {(W (l) 0 , A(k,l) Tk 0 , A(k,l) , B(k,l) Tk , B(k,l) 0 l=1 as follows: 0 , A(l) )}L )}L 0 α = (cid:88) kSr A(l) r+1 = B(l) r+1 = 1{ak=1}, nk mr (cid:40)(cid:80) kSr,ak=1 A(l) , (cid:40)(cid:80) kSr,ak=1 B(l) , (cid:88) β = kSr A(k,l) r,Tk , nk αmr nk βmr B(k,l) r,Tk , nk mr 1{ak=1} (8) if α > 0 otherwise, if β > 0 otherwise, (9) (10) kSr r+1}L where mr = (cid:80) nk and nk = Dk. The parameters {A(k,l) }L l=1 are aggregated from the clients whose ak = 1, r,Tk while {B(k,l) }L l=1 are aggregated from the clients whose r,Tk ak = 1. If none of the clients choose the server parameters {A(l) }L l=1, the parameters are not updated and remain the same for {A(l) l=1. The same rule applies to the update of {B(l) l=1. Note that we need normalization factors α and β to ensure that the summation of the coefficients in Equation 9 and Equation 10 equals one, respectively. Otherwise, the summation of coefficients would not equal to one, since some of the weight matrices from the clients are not sent back to the server. After rounds of updates, we use θ = {(W (l) , B(l) l=1 as the parameters of the final server model pθ . We outline our method in Algorithm 1 and Algorithm 2. 0 , A(l) )}L }L 4. Experiments 4.1. Setup Dataset. To evaluate both the effectiveness and privacy robustness of FedRand, we conduct two experiments: (a) accuracy evaluation on visual question answering (VQA) and image captioning tasks, and (b) membership inference attack using models trained in experiment (a). For the VQA task, we use the ScienceQA (Lu et al., 2022) dataset, while for the image captioning task, we use MSCOCO (Lin et al., 2014). To assess out-of-distribution (OOD) generalization and robustness against membership inference attacks, we employ the NoCaps (Agrawal et al., 2019) dataset. For the non-IID scenarios, we use the Dirichlet distribution to randomly split each dataset, where ScienceQA is divided based on topics, while MSCOCO is partitioned according to object classes in images. We set the Dirichlet 5 parameter to 0.5 as suggested by FedML (He et al., 2020). Detailed descriptions of each dataset can be found in Appendix A.1. Evaluation metrics. For ScienceQA dataset, we measure the exact match between ground truth answers and model predictions as an accuracy. For MSCOCO and NoCaps datasets, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and CIDEr (Oliveira dos Santos et al., 2021) score are utilized to evaluate the quality of the responses. Lastly, we use the MaxRényi-K% (Li et al., 2024) metric as score for binary classification between member and non-member data, defined as follows: MaxRény-K%(X) = 1 Max-K%(X) (cid:88) Hα(pθ( x1:i)), (11) iMax-K%(X) where = (x1, . . . , xT ) is an input token sequence, pθ( x1:i) denotes the next-token distribution after the i-th token, and Max-K%(X) is the set of token positions in with the highest K% Rény entropy Hα. With this score, we compute the AUROC score to measure the robustness against membership inference attacks. Note that MaxRény-0% is the maximum Rényi entropy among all positions from 1 to 1, i.e., maxi[T 1] Hα(pθ( x1:i)). Implementation details. We use pre-trained model trained with the TinyLLava (Zhou et al., 2024) framework, which consists of an image encoder, CLIP (Radford et al., 2021), an instruction-tuned language model, OpenELM (Mehta et al., 2024), with 450M parameters, and linear transformation layer that maps the output of CLIP to the word embedding space of OpenELM. We finetune only the language model using LoRA with rank of 8, while keeping the rest of the model frozen. For each round of FL updates, we fine-tune client model using the AdamW (Loshchilov & Hutter, 2019) optimizer for one epoch, with learning rate of 3 104, weight decay of 106, batch size of 8, and ρ = 0.5. We set the total number of clients to 12 and sample 30% of clients at each round during FL (i.e., = 4). The total number of FL update rounds is set to 30. Baselines. We compare our proposed method, FedRand, against the following relevant baselines. 1. FedAvg (McMahan et al., 2017) trains local clients using the full LoRA weights provided by central server and averages the updated full LoRA weights from clients to update the server models parameters. 2. FedPer (Arivazhagan et al., 2019) communicates the LoRA weights of certain top layers between the server and clients while keeping the remaining LoRA weights FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Table 1. We train each method on the VQA, ScienceQA, and MSCOCO datasets and report its performance on the server, as well as the average performance of the clients. The best results are bolded, and the second-best ones are underlined. Server Method ScienceQA MSCOCO Acc BLEU-1 BLEUBLEU-3 BLEU-4 ROUGE CIDEr FedAvg (oracle) FedPer (2 layer) FedPer (4 layer) FedPara 81.50 (0.53) 42.11 44.59 64. 75.49 (0.44) 74.53 74.43 73.73 58.53 (0.37) 57.11 57.31 56.94 43.43 (0.32) 41.97 42.14 41.36 31.80 (0.17) 30.12 30.22 29.91 55.29 (0.22) 54.13 54.17 53.75 111.08 (0.76) 106.60 107.44 106. FedRand (Ours) 80.12 (0.42) 75.37 (0.35) 58.66 (0.38) 43.63 (0.23) 31.89 (0.25) 55.15 (0.19) 110.27 (0.54) Client Method ScienceQA MSCOCO Acc BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE CIDEr FedAvg (oracle) FedPer (2 layer) FedPer (4 layer) FedPara 79.90 (1.26) 56.93 (5.40) 58.94 (5.73) 58.57 (5.20) 73.86 (0.56) 71.82 (1.52) 72.52 (1.39) 71.36 (1.60) 56.62 (0.57) 54.20 (1.78) 54.99 (1.61) 53.51 (2.18) 41.43 (0.54) 39.10 (1.61) 39.84 (1.51) 38.25 (2.13) 29.76 (0.51) 27.67 (1.35) 28.34 (1.26) 26.86 (1.69) 54.04 (0.32) 52.45 (0.93) 52.96 (0.72) 52.90 (1.23) 104.48 (1.21) 101.00 (3.63) 101.32 (2.86) 97.53 (5.03) FedRand (Ours) 76.01 (1.15) 73.90 (0.89) 56.76 (0.97) 41.72 (0.94) 29.94 (0.63) 53.64 (0.69) 105.10 (1.30) as client-specific private parameters. We share the top 2 or 4 layers of LoRA weights across clients as global parameters. The other layers of LoRA weights are kept hidden as client-specific private parameters and are never shared. Since LoRA parameters of certain layers remain entirely private in FedPer, the LoRA and matrices of these non-shared layers were initialized using the aggregation results from the first round to ensure training stability. 3. FedPara (Hyeon-Woo et al., 2022) parameterizes private LoRA weights for each client and global LoRA weights shared across the server and clients. Each client performs elementwise multiplication between its private LoRA weights and the global ones, then adds the result to the initial pre-trained weights. The global parameters are aggregated from the clients and averaged to serve as the parameters of the server model. FedAvg serves as the oracle method for accuracy evaluation experiments, as it always communicates the full LoRA weights between the server and clients. The other two baselines are selected because they share the concept of partial parameter sharing with our method, enabling comparative analysis of different strategies. The details of the implementation for FedPer and FedPara are provided in Appendix A.3. 4.2. Experimental Results Main results. Table 1 presents the performance of FedRand and other baselines on the ScienceQA and MSCOCO datasets. The upper table reports the statistics of the server-side aggregated global model, while the lower table summarizes the average statistics of individual client models. Given the dynamic client participation in FL, we Table 2. We evaluate each method trained on the MSCOCO dataset to measure OOD generalization on the NoCaps dataset. Server Method BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE CIDEr NoCaps FedAvg (oracle) FedPer (2 layer) FedPer (4 layer) FedPara FedRand (Ours) 78.74 78.10 78.40 77. 78.81 62.24 61.30 61.80 59.90 62.23 46.38 45.30 45.80 43.90 46.42 33.49 32.20 32.80 31. 33.61 54.66 53.20 54.30 53.40 54.57 79.82 78.10 78.50 77.20 79.23 conducted three runs with different random seeds for the top two performing methods: FedAvg and FedRand. On both the server and client sides, the results indicate that FedRand achieves comparable performance to FedAvg an oracle method that communicates full LoRA parameters between the server and clients in every round without considering membership inference attacks. This highlights the effectiveness of our proposed method, FedRand, while reducing communication costs between the server and clients by sharing only subset of client parameters in each round. In contrast, FedPer and FedPara exhibit significantly lower performance on both the server and client sides compared to FedAvg and FedRand across the ScienceQA, and MSCOCO datasets. This underperformance is attributed to their client-specific private parameters. Since these parameters are never aggregated, knowledge transfer between clients is limited, leading to overfitting on small client datasets and degradation in generalization performance. On the other hand, our method, FedRand, stochastically shares random subset of client parameters at each round, encouraging knowledge transfer between clients. This mitigates the overfitting issue and improves generalization. OOD generalization. Furthermore, we evaluate the models trained on the MSCOCO dataset using the NoCaps dataset to measure out-of-distribution (OOD) generalization performance. As shown in Table 2, we observe sim6 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Table 3. We ablate each component of our FedRand and measure its performance (BLEU, ROUGE, and CIDEr) on MSCOCO dataset and robustness (MaxRény-10%) against the membership inference attack. MSCOCO () Component BLEU-1 BELU-2 BLEU-3 BELU-4 ROUGE ρ = 0.3 ρ = 0.7 w/o past parameters w/o normalization 75.57 75.23 76.30 72.50 58.58 58.20 59.29 54.90 43.36 42.97 44.23 39.61 31.47 31.11 32.42 28.04 54.90 54.86 55.27 52. MaxRényi-10% () Image Caption 53.89 (2.79) 52.79 (1.37) 58.04 (5.35) 51.03 (2.12) 65.53 (3.07) 65.40 (4.22) 67.44 (4.33) 62.21 (1.71) CIDEr 109.37 108.98 110.83 98.83 FedRand 75.37 (0.35) 58.66 (0.38) 43.63 (0.23) 31.89 (0.25) 55.15 (0.19) 110.27 (0.54) 53.84 (2.50) 66.61 (3.22) ilar trends to those in the previous experiments. FedRand achieves performance comparable to FedAvg, while FedPer and FedPara significantly degrade in performance compared to both FedAvg and FedRand. These results once again highlight the effectiveness of our method, FedRand. Membership inference attack (MIA). We perform membership inference attack on the models trained on the MSCOCO dataset. Following Li et al. (2024), we use MaxRényi-K%, described in Equation 11, as score for binary classification to distinguish member data instances in the MSCOCO dataset from non-member ones in the NoCaps dataset, and report the AUROC score in Table 4. sample of 300 is drawn from each population for member and non-member data, consisting of 600 images in total. Notably, the non-member data primarily consists of object images that rarely appear in MSCOCO. (a) the server atWe consider two plausible scenarios: tempts MIA using the aggregated model (denoted as server in the table), and (b) the server maliciously reconstructs the client model and performs MIA (denoted as client in the table). In the case of FedAvg, the server can exactly reconstruct client models using the full client LoRA parameters transmitted to it. However, in our FedRand, since only subset of parameters is sent to the server per round, the timing at which client sends the other set of parameters varies across clients. Thus, we first intercept one part of LoRA weights from each client in the final round. Then we obtain the rest of the LoRA weights at the secondto-last round in which each corresponding client participates. For FedPer and FerPara, the client model cannot be fully reconstructed under any circumstance; therefore, we report only the server results for those two methods. As shown in Table 4, FedRand demonstrates stronger resistance to MIA compared to the other baseline methods. This is due to the fact that clients send only subset of parameters to the server, which helps prevent the exposure of their full client parameters. Both FedAvg and FedRand show that reconstructed client models are more vulnerable than server models, with this trend being more pronounced in FedAvg, as it can fully reconstruct client models at the end of any round. FedPer and FedPara are expected to be effective against MIA since they do not share client-specific Table 4. Membership inference attack to distinguish the training dataset MSCOCO from the NoCaps dataset using Rényi Entropy Max_0% and Max_10%. Lower scores indicate better robustness against the membership inference attack. Statistics are presented in percentage. MaxRényi-0% () MaxRényi-10% () image caption image caption FedAvg (server) FedAvg (client) FedPer (2 layers) FedPer (4 layers) FedPara 49.96 (3.11) 51.68 (4.17) 50.73 (4.36) 51.77 (3.40) 53.48 (1.77) 70.22 (2.56) 70.68 (3.82) 70.01 (3.87) 69.71 (4.14) 69.67 (2.97) 54.57 (4.07) 54.71 (4.11) 56.76 (1.51) 57.74 (2.25) 57.07 (2.93) 70.22 (2.56) 70.69 (3.80) 70.03 (3.84) 69.73 (4.16) 69.63 (2.99) FedRand (server) FedRand (client) 48.90 (4.75) 47.83 (3.56) 67.02 (3.74) 68.51 (3.69) 53.84 (2.50) 54.99 (4.22) 66.61 (3.22) 68.51 (3.69) private parameters at all; however, they show worse robustness than FedRand. This may be attributed to the fact that their private parameters are never shared across clients, limiting knowledge transfer. As result, the shared global parameters must compensate by fitting each clients dataset more closely, making them more prone to overfitting and leading to more severe memorization. Ablation studies. We conduct comprehensive ablation study on each component of our method to evaluate its effectiveness. First, we vary the probability ρ of selecting the LoRA weight matrix A, setting it to ρ = 0.3 and ρ = 0.7. Additionally, we ablate the normalization factors α and β, as defined in Equation 8, referring to this case as w/o normalization. Lastly, instead of using the client-specific private parameters in lines 10 and 13 of Algorithm 2, we initialize with the full LoRA weights from the server and send either the updated or back to the server, depending on the variable ak, referring to this case as w/o past parameters. As shown in Table 3, selecting the LoRA weight matrix either more or less frequently than degrades the performance of image captioning on MSCOCO while slightly improving robustness against MIA. Similarly, removing normalization significantly degrades BLEU, ROUGE, and CIDEr scores, while making the model more robust to MIA In contrast, initializing all the client due to underfitting. 7 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates by transmitting only subset of the client model parameters to the server. As future work, we suggested randomly selecting sub-layers of clients for training or quantizing client parameters sent to the server to further enhance the security of client model parameters."
        },
        {
            "title": "Impact Statements",
            "content": "This paper presents framework, FedRand, aimed at improving privacy in Federated Learning (FL), particularly when training vision-language models (VLMs). Our work contributes to advancing the field of privacy-preserving machine learning by mitigating the risks of membership inference attacks without significantly compromising model performance. By enhancing data privacy in FL, our approach can benefit various real-world applications, including healthcare, finance, and other domains where senFesitive data is distributed across multiple entities. dRand reduces the exposure of client-side model parameters, thereby strengthening privacy guarantees for users participating in federated training. However, as with any privacy-preserving method, FedRand does not eliminate all risks. Adversarial attackers may still attempt more sophisticated attacks beyond membership inference, and further research is needed to address emerging privacy threats. Additionally, while our method enhances privacy, it does not directly address fairness or bias in FL, which remain important considerations for real-world deployment. Overall, this work aligns with the broader goal of developing privacy-preserving AI systems and does not introduce any foreseeable ethical concerns or negative societal impacts."
        },
        {
            "title": "References",
            "content": "Acar, D. A. E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., and Saligrama, V. Federated learning based on dynamic regularization. International Conference on Learning Representations (ICLR), 2021. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P. NoCaps: novel object captioning at scale. Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 2022. Arivazhagan, M. G., Aggarwal, V., Singh, A. K., and Choudhary, S. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019. Carlini, N., Tramer, F., Wallace, E., Jagielski, M., HerbertFigure 3. The ratio of number of communicated LoRA parameters, compared to FedAvg per round under LoRA configuration. parameters with the LoRA weights of the server without using the clients past parameters significantly boosts the performance on the MSCOCO dataset but drastically sacrificing robustness against the MIA. These experimental results support the choice of hyperparameters ρ = 0.5 and our algorithm design. Communication cost. Figure 3 illustrates the communication cost between server and clients required for each method. Although FedPer reduces the the cost to 25% by sharing only the upper layers, it significantly underperforms compared to FedAvg as shown in previous experiments. In the case of our proposed FedRand, receives the same number of parameters received from the server as FedAvg, but only sends half of them are back to the server, reducing the communication cost by approximately 25% per round, while retaining accuracy similar to FedAvg. 5. Conclusion In this work, we proposed the FedRand framework to mitigate the vulnerability of vision-language models (VLMs) fine-tuned with federated learning to membership inferInstead of communicating the full LoRA ence attacks. weights of VLMs between the server and clients which an attacker could intercept to perform membership inference attacks each client randomly selected subset of LoRA weights from the server and initialized the remaining LoRA weights using its private parameters from the previous round. After updating both sets of parameters, only the non-private parameters were sent back to the server for aggregation, reducing the risk of disclosing the full parameters of the client model. We extensively validated that our proposed FedRand achieved performance comparable to FedAvg, which communicated full LoRA weights between the server and clients, while demonstrating improved robustness against membership inference attacks compared to other relevant baselines. Additionally, our method reduced communication costs between the server and clients 8 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language models. 30th USENIX Security Symposium (USENIX Security 21), 2021. Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying memorization across neural language models. International Conference on Learning Representations (ICLR), 2023. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards generalpurpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 2023. He, C., Li, S., So, J., Zhang, M., Wang, H., Wang, X., Vepakomma, P., Singh, A., Qiu, H., Shen, L., Zhao, P., Kang, Y., Liu, Y., Raskar, R., Yang, Q., Annavaram, M., and Avestimehr, S. Fedml: research library and benchmark for federated machine learning. ArXiv, 2020. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Hitaj, B., Ateniese, G., and Perez-Cruz, F. Deep models under the gan: information leakage from collaborative deep learning. ACM SIGSAC conference on computer and communications security, 2017. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 2022. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Lowrank hadamard product for communication-efficient federated learning. International Conference on Learning Representations (ICLR), 2022. Jayaraman, B., Guo, C., and Chaudhuri, K. Déjà vu memorization in visionlanguage models. Advances in Neural Information Processing Systems (NeurIPS), 2024. Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. International Conference on Machine Learning (ICML), 2023. Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Federated optimization in heterogeneous networks. Machine learning and systems (MLSys), 2020. Lin, C.-Y. ROUGE: package for automatic evaluation of summaries. Text Summarization Branches Out, 2004. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. European Conference Computer Vision (ECCV), 2014. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 2023. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. iclr, 2019. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems (NeurIPS), 2022. McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. International Conference on Artificial Intelligence and Statistics (AISTATS), 2017. Mehta, S., Sekhavat, M., Cao, Q., Horton, M., Jin, Y., Sun, F., Mirzadeh, I., Najibikohnehshahri, M., Belenko, D., Zatloukal, P., and Rastegari, M. Openelm: An efficient language model family with open training and inference framework. ICML Workshop, 2024. Melis, L., Song, C., De Cristofaro, E., and Shmatikov, V. Exploiting unintended feature leakage in collaborative learning. IEEE symposium on security and privacy (SP), 2019. Oliveira dos Santos, G., Colombini, E. L., and Avila, S. CIDEr-R: Robust consensus-based image description evaluation. Workshop on Noisy User-generated Text (WNUT 2021), 2021. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. BLEU: method for automatic evaluation of machine translation. Association for Computational Linguistics (ACL), 2002. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. International Conference on Machine Learning (ICML), 2021. Li, Z., Wu, Y., Chen, Y., Tonin, F., Rocamora, E. A., and Cevher, V. Membership inference attacks against large vision-language models. Advances in Neural Information Processing Systems (NeurIPS), 2024. Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference attacks against machine learning models. 2017 IEEE symposium on security and privacy (SP), 2017. 9 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Waswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. Yu, F., Rawat, A. S., Menon, A., and Kumar, S. Federated learning with only positive labels. International Conference on Machine Learning (ICML), 2020. Zhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R., Yu, T., Wang, G., and Chen, Y. Towards building the fedInternational eratedgpt: Federated instruction tuning. Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. Zhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X., Wu, J., and Huang, L. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 10 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates A. Experimental Details A.1. Dataset ScienceQA (Lu et al., 2022) is multiple choice visual question answering dataset derived from elementary and high school science curricula, covering three subjects: natural science, language science, and social science. We focus exclusively on the 10,327 questions that include accompanying images, representing 48.7% of the entire dataset. MSCOCO (Lin et al., 2014) contains over 330K images with dense annotations for image recognition, segmentation and captioning tasks. Among the 83K instances specifically created for captioning, 50K images are sampled for training and 5K images each for validation and testing. NoCaps (Agrawal et al., 2019) is designed to evaluate the ability of image captioning models to describe objects not present in the MSCOCO dataset. 45K validation sets, each with 10 captions, are used to assess OOD generalization. A.2. Prompt Template We present prompt template for each dataset. Note that the presence of contextual information in ScienceQA depends on the question. ScienceQA Based on the image, respond to the question with given options. USER: {image}n Context: ASSISTANT: ... {options}. {context}. Options: Answer: MSCOCO & NoCaps Briefly describe given image. USER: {image}n short image description: ASSISTANT: ... A.3. Communication process of FedPer and FedPara In the original FedPer framework, the classifier and top basic blocks of ResNet (He et al., 2016) model are designated as personalization layers. To adapt this approach for LoRA settings, we instead share the LoRA parameters of the top 2 or 4 transformer (Waswani et al., 2017) layers with the server. Similarly, the FedPara method originally parameterize weight of base models with Hadamard product between two sets of low rank matrices. To extend this idea to transformer architecture LLMs with LoRA, we introduce an additional pair of LoRA and matrices per layer, ensuring the additional LoRA weight matrices remain private on the client side. 11 FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates Figure 4. An example of token-wise Rényi entropy measurement for member (MSCOCO) and non-member (NoCaps) data. The higher the entropy is, the more robust to MIA. B. Membership Inference Attack Example We show two sets of membership inference attack (MIA) examples in Figure 4, where color denotes token-wise Rényi entropy with FedRand. On the left (a), the model is confident in next-token prediction for member data (MSCOCO), indicating failed defense against MIA. On the right (b), the model is highly uncertain for both member and non-member data (NoCaps), leading to successful defense against MIA."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "Graduate School of AI, KAIST"
    ]
}