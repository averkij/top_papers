{
    "paper_title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling",
    "authors": [
        "Liran Ringel",
        "Elad Tolochinsky",
        "Yaniv Romano"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing \"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing."
        },
        {
            "title": "Start",
            "content": "Learning Continue-Thinking Token for Enhanced Test-Time Scaling Liran Ringel*1 Elad Tolochinsky*3 1Department of Computer Science, Technion Israel Institute of Technology 2Department of Electrical and Computer Engineering, Technion Israel Institute of Technology 3Independent Researcher liranringel@cs.technion.ac.il, eladt26@gmail.com, yromano@technion.ac.il Yaniv Romano1,2 5 2 0 2 J 2 1 ] . [ 1 4 7 2 1 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing </think> with Wait) can extend reasoning steps and improve accuracy. In this work, we explore whether dedicated continue-thinking token can be learned to trigger extended reasoning. We augment distilled version of DeepSeek-R1 with single learned <continue-thinking> token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and test-time scaling approach that uses fixed token (e.g., Wait) for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base models accuracy, our method achieves markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields 1.3% absolute improvement in accuracy, whereas our learned-token method achieves 4.2% improvement over the base model that does not use budget forcing."
        },
        {
            "title": "Introduction",
            "content": "Language models have demonstrated impressive reasoning abilities through test-time compute scaling (Snell et al., 2024; Welleck et al., 2024; OpenAI, 2024; Anthropic, 2025). Two dominant paradigms have emerged for this process (Anthropic, 2025): parallel and sequential. The parallel approach involves generating multiple samples and selecting the best response based on majority vote (Lewkowycz et al., 2022) or model-provided score (Anthropic, 2025). In contrast, the sequential *Equal contribution. 1 approachcentral to this work and popularized by OpenAIs o1 model (OpenAI, 2024)generates single sample and encourages the model to revisit, backtrack, validate, and refine its reasoning before producing final answer, typically resulting in long chain-of-thought output (Wei et al., 2022). Models that follow this paradigm are commonly referred to as reasoning models, and they are often trained using reinforcement learning with verifiable rewards (Lambert et al., 2024; Guo et al., 2025; Su et al., 2025; Wang et al., 2025). key property of reasoning models is their ability to decide when to stop thinking, typically by generating an explicit end-of-thinking token (e.g., </think>). However, since this decision is made by the model, users have no direct control over the amount of reasoning performed. Recently, budget forcing was introduced in s1: simple test-time scaling (Muennighoff et al., 2025), sequential testtime scaling approach that provides direct control over the models computation time. By replacing end-of-thinking tokens with Wait tokens during generation, the authors showed that longer chainof-thought reasoning could be achieved, resulting in improved accuracy. Conversely, early termination can be enforced by appending </think> token once the model reaches its compute budget, prompting it to generate the final answer. Budget forcing was quickly shown to perform well in various settings (Aggarwal and Welleck, 2025; Huang et al., 2025); see section 3 for detailed discussion. In this paper, we introduce systematic approach for learning special <continue-thinking> token as an alternative to the Wait or related fixed tokens such as Alternatively or Hmm suggested in (Muennighoff et al., 2025). Our primary goal is to rigorously investigate whether the simple practice of using fixed Wait token for budget forcing can be improved by learning dedicated token embedding. Importantly, we do not aim to design the best possible test-time scaling method Figure 1: Text generation with budget forcing: Whenever the model outputs </think> token, we replace it with <continue-thinking> token and feed that to the model. for reasoning models. Rather, our focus is on understanding and analyzing the specific phenomenon reported in s1 (Muennighoff et al., 2025). To this end, we deliberately constrain ourselves to singletoken mechanism for budget forcing, enabling controlled and direct comparison with prior approaches. As illustrated in Figure 1, we introduce special, learned <continue-thinking> token into the models vocabulary. During generation, we modify the models output generation process to replace any occurrence of the </think> token with our <continue-thinking> token, as long as the token budget is not exhausted and the maximum number of forced continuations has not been reached. During training, we optimize only the embedding vector of the <continue-thinking> token while keeping all other model parameters frozen. We train this new token using reinforcement learning (RL), specifically the group relative policy optimization (GRPO) procedure (Shao et al., 2024). While supervised fine-tuning is possible alternative, designing effective supervised reasoning demonstrations is difficult. In contrast, RL allows the training process to explore novel continuations aimed at improving task accuracy, making it more natural fit for our setting. Notably, our method leverages frozen model backbone, optimizing only single parameter vector of the size of the models hidden dimension. This makes our approach highly generic and memory-efficient, enabling the use of relatively large context windows during RL training. We apply our proposed method to investigate the influence of the learned <continue-thinking> token on the reasoning process. Our results show that learning this token can significantly enhance model performance, yielding greater gains than the Wait or related tokens used in s1s budgetforcing method. Notably, we find that whenever budget-forcing with Wait provides an improvement over the baseline, our learned token achieves even greater gainsup to 320% increase in relative accuracy improvements, and as much as 4% absolute improvement in overall accuracy. Conversely, in cases where budget-forcing with fixed token (such as Wait) does not improve performance compared to the baseline, the learned token similarly does not offer statistically significant benefit. Full results are reported in Table 1. To summarize, our contributions have the following key features: We introduce the concept of learning specialized <continue-thinking> token as an effective mechanism for budget-controlled compute scaling. Our experiments demonstrate that, in cases where budget-forcing with fixed token such as Wait improves accuracy, learning the token yields even greater gains, while requiring 2 Dataset Baseline (w/o BF) Alternatively Hmm Wait Learned Cmax = 2, Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 3, Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 2, Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 3, Bmax = 9216 AIME24 AIME25 GSM8K MATH500 23.44 0.73 21.88 0.67 78.41 0.25 79.43 0.26 23.44 0.73 21.82 0.67 78.41 0.25 79.42 0.26 23.85 0.66 23.33 0.52 78.08 0.36 79.88 0. 23.85 0.66 23.33 0.52 78.08 0.36 79.88 0.33 23.54 0.82 21.72 0.66 78.58 0.34 80.00 0.19 23.85 0.76 21.61 0.67 79.09 0.28 80.28 0.29 24.06 0.76 21.98 0.68 79.71 0.31 80.36 0.27 24.53 0.75 22.34 0.71 82.63 0.20 81.67 0.21 23.02 0.58 22.08 0.65 79.00 0.13 79.60 0. 23.12 0.62 21.67 0.68 79.65 0.26 80.71 0.30 23.49 0.54 22.03 0.61 80.16 0.32 80.86 0.18 23.33 0.73 22.03 0.60 83.17 0.32 82.29 0.24 23.59 0.68 23.39 0.51 78.96 0.21 80.14 0.38 24.37 0.68 23.75 0.53 79.67 0.35 81.19 0.31 24.06 0.75 23.54 0.51 79.86 0.39 81.21 0. 24.06 0.69 23.07 0.49 82.53 0.48 82.30 0.34 23.49 0.69 23.80 0.56 78.83 0.34 80.33 0.35 24.48 0.63 23.85 0.53 79.47 0.34 80.84 0.39 23.91 0.70 23.49 0.49 80.14 0.24 81.22 0.30 24.06 0.68 22.40 0.44 83.68 0.38 82.51 0.34 Table 1: Accuracy (pass@1) results for different token budget limits Bmax and different numbers of forced thinking continuations Cmax. Results are obtained via regex-based evaluation and an LLM evaluator if the model fails to generate an answer in the correct format. See Table 5 for the full results. only single additional token embedding. We use an external LLM to compare outputs with ground truth, mitigating the limitations of standard math benchmark evaluations that rely on rigid answer formats, assuming the final answer is in boxed{}. Our experiments show that this rigid-format evaluation approach can misrepresent reasoning ability. The code used for training and evaluation is available at https://github.com/liranringel/lea rning-continue-thinking-token."
        },
        {
            "title": "2.1 Single Token Optimization",
            "content": "Let π(x) be pretrained language model that takes prompt as input and generates an output. Key to our method is the introduction of special token, <continue-thinking>, which we add to the models vocabulary to promote longer reasoning traces at test time. We denote the embedding vector of this token by θT Rd, where is the embedding dimension of the model."
        },
        {
            "title": "We refer",
            "content": "to the adapted model with the new token as πθT . All model parameters are frozen except for θT , so the embedding of the <continue-thinking> token is the only parameter updated during training. Our objective is to maximize the reasoning performance of πθT by optimizing the embedding vec3 tor θT . Formally, we aim to optimize the following: ExQ,oBF(πθT θ := arg max ,x)[R(x, o)], (1) θT where is question sampled from distribution Q, and is the response generated by running the budget forcing algorithm on using the model πθT . Note that is not standard sample from πθT (y x); rather, its distribution is induced by the budget forcing algorithm BF(πθT , x), which we present below. The function R(x, o) represents the reward associated with the generated output. For example, for mathematical questions, R(x, o) could be an indicator function that equals 1 if contains the correct answer to question x, and 0 otherwise. In more detail, the budget forcing algorithm BF(πθT , x) modifies the generation process by enforcing additional reasoning steps before producing final answer. During generation, whenever the model outputs an end-of-thinking </think> token, it is replaced with the learned <continue-thinking> token, as way to force longer reasoning traces. This process repeats until one of the following conditions is met: 1. The number of forced thinking continuations reaches the preset maximum Cmax. 2. The total number of generated tokens reaches the budget limit Bmax, in which case the reasoning process is immediately terminated and </think> token is inserted. This ensures that the model does not generate beyond the allowed compute budget. During training, we set Cmax = 1, meaning that only single forced continuation is allowed per input. At test time, however, we also evaluate the model with Cmax > 1 to assess its ability to generalize to multiple forced continuations."
        },
        {
            "title": "2.2 Parallel Generation and Backpropagation",
            "content": "Fine-tuning LLMs with online RL often allocates separate GPUs for generation and backpropagation. This is because generation is significantly faster when using specialized inference engines such as vLLM (Kwon et al., 2023) and SGLang (Zheng et al., 2024), which are optimized for inference speed but cannot easily share memory with backpropagation workloads. While this setup enables efficient training, there remains an opportunity for better resource utilization, as backpropagation starts only after all generations are finished, and generation resumes only after weight updates. To address this, we divide each batch into microbatches, each containing the generations of single example. Once the generations for an example are complete, we immediately stream the micro-batch to the training GPU and initiate backpropagationconcurrently with ongoing generation for the rest of the batch. We accumulate gradients across micro-batches, updating the model weights only after the full batch is processed. This improves GPU utilization and accelerates training."
        },
        {
            "title": "2.3 Training",
            "content": "Throughout this paper, we apply our method to the DeepSeek-R1-Distill-Qwen-1.5B model (Yang et al., 2024b; Guo et al., 2025), training only the embedding of newly introduced token, while keeping all other parameters frozen. We set the number of forced continuations to Cmax = 1 and the budget limit to Bmax = 8192. We initialize the new tokens embedding using that of the word Wait. The reward function used during training is the sum of two binary components: (i) format reward that verifies whether the answer is in the expected format (specifically, wrapped in boxed{}), and (ii) correctness reward, which checks whether the generated answer matches the ground truth. We implemented our training code on top of OpenR1 (HuggingFace, 2025) and TRL (von Werra et al., 2020). For GRPO, we set = 16 generations per example and used batch size of 16. We performed 64 gradient accumulation steps, resulting in an effective batch size of 1,024 generations (i.e., 64 examples with 16 generations each) per optimizer update. The training dataset is DeepScaleR-Preview-Dataset (Luo et al., 2025), which is collection of 40,000 math questions compiled from various datasets. The model was trained for total of 936 steps using 8 NVIDIA A100 GPUs with 80GB memory: 4 GPUs for backpropagation and 4 GPUs for completion generations, where the total training time was about 1 week. See Table 4 for full list of parameters and system prompts used during training."
        },
        {
            "title": "3 Related Work",
            "content": "Budget forcing, introduced by Muennighoff et al. (2025), is simple and effective method for scaling compute at test time. L1 (Aggarwal and Welleck, 2025) extends this idea using reinforcement learning to train models that satisfy user-specified reasoning lengths, enabling flexible cost-performance trade-offs. The m1 method by Huang et al. (2025) further explores the application of budget-forcing in medical QA. Jin et al. (2025) introduce variant of budget forcing for greedy decoding, comparing multi-word phrases instead of single tokens like Wait. Collectively, these works highlight budget forcing as promising direction for compute budget control. The concept of incorporating additional tokens has been explored in several prior studies. The works reported in (Goyal et al., 2024) and (Wang et al., 2023) introduce learnable tokens into reasoning traces to improve the models accuracy. In (Goyal et al., 2024; Pfau et al., 2024), additional tokens are inserted at random positions during training and appended to the prompt during inference, allowing the model to artificially increase the number of activations at test time. Wang et al. (2023) included planning token at the start of each reasoning step. Unlike our approach, these methods rely on supervised fine-tuning to learn token representations. In contrast, our method utilizes RL to optimize the new token embedding. Finally, various works on scaling test time compute motivate our choice of token learning using RL: (1) Recent empirical findings show that RLbased fine-tuning leads to better generalization (Chu et al., 2025); (2) theoretical analysis shows that RL enjoys higher expected cumulative reward 4 Dataset Baseline (w/o BF) Alternatively Hmm Wait Learned Cmax = 2 Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 3 Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 2 Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 3 Bmax = 9216 AIME24 AIME25 GSM8K MATH500 22.86 0.75 (81) 21.56 0.63 (85) 66.43 0.50 (84) 77.19 0.30 (94) 23.02 0.83 (80) 21.41 0.65 (84) 64.52 0.16 (81) 77.64 0.22 (94) 23.23 0.78 (80) 21.15 0.67 (84) 64.56 0.16 (80) 77.99 0.30 (94) 23.28 0.75 (82) 21.25 0.64 (84) 63.55 0.36 (79) 78.05 0.26 (94) 24.22 0.76 (81) 21.82 0.68 (83) 78.37 0.40 (94) 80.74 0.18 (96) 22.86 0.75 (80) 21.56 0.63 (85) 66.43 0.50 (83) 77.19 0.30 (94) 22.55 0.51 (79) 21.82 0.67 (83) 64.21 0.46 (80) 77.45 0.31 (94) 22.45 0.57 (79) 21.41 0.67 (85) 64.42 0.29 (79) 78.54 0.30 (94) 22.92 0.52 (81) 21.72 0.61 (83) 62.99 0.43 (77) 78.26 0.20 (93) 22.92 0.74 (81) 21.56 0.58 (84) 80.11 0.30 (95) 81.60 0.22 (96) 23.18 0.64 (82) 23.18 0.50 (86) 66.03 0.40 (84) 77.56 0.32 (95) 23.12 0.67 (82) 23.02 0.49 (85) 64.82 0.25 (82) 77.97 0.35 (95) 23.91 0.67 (83) 23.23 0.51 (86) 64.09 0.38 (80) 78.88 0.32 (95) 23.75 0.74 (83) 23.23 0.50 (87) 64.43 0.48 (79) 78.83 0.38 (95) 23.59 0.67 (83) 22.80 0.47 (86) 78.27 0.58 (94) 81.37 0.38 (97) 23.18 0.64 (82) 23.18 0.50 (85) 66.03 0.40 (83) 77.56 0.32 (94) 23.39 0.70 (81) 23.44 0.56 (87) 63.79 0.57 (80) 78.16 0.39 (95) 24.06 0.67 (82) 23.49 0.49 (86) 64.27 0.28 (79) 78.60 0.40 (94) 23.28 0.72 (82) 23.12 0.46 (86) 62.94 0.50 (77) 78.75 0.32 (94) 23.39 0.66 (83) 22.40 0.44 (86) 80.87 0.42 (96) 81.75 0.34 (96) Table 2: Accuracy (pass@1) results for different token budget limits Bmax and different numbers of forced thinking continuations Cmax. Results are obtained via regex-based evaluation only. The percentage of final answers enclosed in boxed{} is shown in parentheses. See Table 6 for the full results. (Setlur et al., 2025), and (3) learning to use additional tokens, which is related to our approach of designing <continue-thinking> token, has been observed to be hard learning problem when using supervised learning (Pfau et al., 2024)."
        },
        {
            "title": "4.1 Evaluation protocol",
            "content": "reasoning datasets: We evaluate our model on three widely adopted mathematical GSM8KPlatinum (Cobbe et al., 2021; Vendrow et al., 2025), revised version of the original GSM8K dataset containing 1209 grade-school level math problems; MATH500 (Lightman et al., 2023; Muennighoff et al., 2025), 500-question subset of the MATH (Hendrycks et al., 2021) dataset; and AIME24 (Muennighoff et al., 2025) and AIME25 datasets, which contain 30 math problems from the 2024 and 2025 American Invitational Mathematics Examination, national-level mathematics competition in the United States. We implemented the evaluation pipeline using modified version of the LM-Evaluation-Harness library (Gao et al., 2024). Our method is compared against three fixed tokens that have previously demonstrated strong performance (Muennighoff et al., 2025), as well as baseline configuration that does not employ budgetforcing. All competing methods were implemented using DeepSeek-R1-Distill-Qwen-1.5B. Due to the relatively small size of some of the datasets, we generate multiple responses per question to compute standard error as way to enhance the statistical reliability of the reported results. Specifically, we found that generating the following number of completions produces error bars that allow us to distinguish between statistically significant and insignificant results: 16 samples per question for MATH500, 64 for AIME, and 6 samples for GSM8K. We report (pass@1) accuracy along with the standard error for each setting. Accuracy is computed using regex-based evaluation script that extracts the final answer from the models output and checks for an exact match with the ground truth. The evaluation returns True if the extracted answer is correct, and False otherwiseincluding cases where the regex fails to match valid answer. In case the regex fails to match, we employ an external LLM to assess if the generated answer was semantically equivalent to the ground truth. This hybrid evaluation strategy was adopted based on our manual inspection, which indicated that regex matching is more reliable than LLM-based comparison when the regex successfully parses the output; 5 Figure 2: Accuracy of different methods as function of the average number of tokens generated by each method. Results for all datasets are obtained using Bmax = 8192. see subsection 4.2 for detailed discussion. We utilized Qwen/Qwen2.5-7B-Instruct (Yang et al., 2024a) as our evaluator LLM. See Table 4 for the instruction prompt we used for the LLM evaluation. During inference, we verify the generalization of our method by also using inference configurations not used during training. Concretely, we set the reasoning budgets Bmax = 8192, 9216 and the maximal number of forced continuations Cmax = 1, 2, 3. We also provide an additional 1,024 tokens reserved for generating the final answer. For inference, we used the same system prompt as the one used for training; see Table 4 in the Appendix. Due to the prolonged length of the generated answers, evaluation took approximately 370 GPU hours, using 8 NVIDIA A40 GPUs."
        },
        {
            "title": "4.2 LLM-Based Verification",
            "content": "It is common practice to use regex-based functions to check the correctness of model outputs when evaluating language models on mathematical benchmarks. However, this approach can distort the evaluation of model performance, as it conflates output formatting with actual reasoning ability. In our experiments, evaluating with only regex-based functions suggested that the learned token led to substantial performance gains. However, more detailed analysis using an evaluator LLM revealed that much of this improvement was due to better adherence to formatting rather than genuine reasoning improvements. As shown in Table 2, the results obtained using only regex-based evaluation are significantly higher than those reported in Table 1, which use LLM-based evaluation. For example, on GSM8K with Cmax = 2 and Bmax = 8192, the improvement of our method over the baseline is more than three times higher when using regexbased evaluation compared to LLM-based evaluation. Similarly, on the MATH500 dataset with Cmax = 1 and Bmax = 9216, regex-based evaluation indicates statistically significant improvement, whereas LLM-based evaluation shows that this improvement is not actually present. Note that LLM-based verification is not flawless. We adopted it as better means to mitigate the limitations of purely regex-based evaluation and enhance the trustworthiness of our findings."
        },
        {
            "title": "4.3 Results",
            "content": "The evaluation results are depicted in Table 1. Our findings indicate that when budget forcing does not improve upon the baseline, the learned token similarly offers no statistically significant advantage. However, on datasets where budget forcing yields improvements, our learned token demonstrates substantial performance increase, achieving up to 320% relative gain over the best fixed token and 4% absolute improvement in accuracy. Notably, although our model was trained exclusively with Cmax = 1, we observe that increasing Cmax to 2 and 3 during inference often leads to further improvements in accuracy. This suggests that the learned <continue-thinking> token can generalize to settings with multiple forced continuations, even though the model was not explicitly trained for them. To gain deeper insights into the behavior of the different methods, we analyzed the generated token distributions. Figure 2 depicts the accuracy of each method as function of the average number of tokens generated per dataset. We observed that using the learned token consistently resulted in longer reasoning traces, suggesting that the performance improvement can be attributed to this increased reasoning length. Furthermore, Figure 2 shows that 6 Figure 3: Comparison of generated sequence length distributions across methods and datasets and their corresponding accuracies. Stacked bars represent the logarithmic count of answers within each length bin, with darker segments indicating higher proportion of correct answers (fraction shown within each bin). Top row: GSM8K, Bottom row: AIME24. Left: Learned <continue-thinking> token vs. Alternatively. Right: Learned <continue-thinking> token vs. baseline model without budget forcing. Data was obtained using Cmax = 2, Bmax = 8192. for the AIME datasets, all methods generated high average number of tokens. This can likely be attributed to the significant difficulty of the AIME problems for our model, which might also explain why our method did not yield substantial improvements on these datasets. comparison of the generated answer length distributions for the learned token, the baseline, and the Alternatively token is shown in Figure 3. The accuracy improvements observed with the learned token are consistent across different generated lengths, not just on average. This suggests that the enhanced performance is attributable to genuine improvement in the models reasoning capabilities, beyond merely generating longer responses. In Table 3, we show the probabilities that correct answer under the baseline will be incorrect with the learned token. We can see that, for both GSM8K and MATH500, it is much more likely that the learned token leads to improvement. For the AIME dataset, we see that the learned token and the baseline are comparable, which is expected, since both methods have similar accuracy. To better understand how the learned token influences the models reasoning process, we visualize in Figure 4 word cloud showing the first word generated immediately after the Figure 4: Word cloud of erated immediately after <continue-thinking> token, across all datasets. the first injecting the token genlearned <continue-thinking> token. The most common continuations often prompt the model to selfverify or reconsider its previous steps, indicating that the token effectively encourages reflective reasoning and backtracking. The reasoning trace depicted in Figure 5, taken from the GSM8K dataset, showcases how the <continue_thinking> token influences the final answer. The blue portion highlights the models initial reasoning, which leads to an incorrect result. However, the introduction of the <continue_thinking> token prompts reexamination of the solution, ultimately guiding the 7 Probabilities AIME24 AIME25 GSM8K MATH500 Cmax = 2, Bmax = 8192 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 3, Bmax = 8192 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 2, Bmax = 9216 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 3, Bmax = 9216 P(Learned , Baseline ) P(Learned , Baseline ) 0.01 0.02 0.09 0. 0.08 0.08 0.09 0.09 0.01 0.02 0.06 0.06 0.06 0.06 0.06 0. 0.03 0.07 0.02 0.07 0.02 0.07 0.02 0.07 0.02 0.04 0.01 0. 0.02 0.04 0.02 0.04 Table 3: Probability of observing correct answer from the learned token and an incorrect answer from the baseline and vice versa. See Table 7 in the Appendix for the full results. Question: Lloyd has an egg farm. His chickens produce 252 eggs per day and he sells them for $2 per dozen. How much does Lloyd make on eggs per week? <think> First, need to calculate the daily revenue from egg sales. [...] Finally, to determine the weekly revenue, Ill multiply the daily revenue by the number of days in week, which is 7. So, $1,134 multiplied by 7 equals $7,938. <continue_thinking> no, let me try that again. [...] Therefore, the correct answer is $294 per week. </think> Lloyd sells his eggs at $2 per dozen. To determine his weekly revenue: 1. Calculate weekly egg production: - 252 eggs/day * 7 days = 1,764 eggs/week [...] Answer: Lloyd makes boxed{294} dollars per week from his egg sales. Figure 5: GSM8K reasoning trace demonstrating the positive impact of <continue_thinking> token. Blue indicates the original reasoning, yielding an incorrect answer of 7,938. Green shows the continuation after the special token was added, leading to the correct answer of 294. model to the correct conclusion. See Appendix for the full reasoning traces and additional examples."
        },
        {
            "title": "5 Conclusions",
            "content": "In this work, we have demonstrated that learning dedicated <continue-thinking> token yields significant effectiveness, specifically in scenarios where the baseline budget forcing method already provides performance improvements. We 8 introduced training methodology for this token that exhibits promising generalization capabilities across different inference settings. Our analysis indicates that the observed performance gains are not primarily due to better adherence to output formatting, but rather stem from the elicited longer reasoning traces and genuine enhancement in the models underlying reasoning capabilities. Furthermore, this improvement remains relevant on average and conditionally across varying reasoning lengths, suggesting its utility even when generating shorter completions. While our method of learning specialized <continue-thinking> token is relatively simple and efficient, practitioners can readily ascertain its potential benefit for their specific scenario by first employing the vanilla budget-forcing technique with fixed token, such as Wait; an observed performance increase with this baseline strongly suggests that training dedicated <continue-thinking> token would be worthwhile. Finally, we emphasize the critical importance of rigorous evaluation for drawing meaningful conclusions and propose refined evaluation scheme designed to mitigate some of the inherent limitations associated with relying solely on regexbased assessments. Future Directions One promising future direction involves exploring the efficacy of learning distinct <continue-thinking> tokens tailored to different positions within the generated sequence or investigating the benefits of learning these tokens jointly. For example, one can add special token that will be used for the first forced continuation and second new token that will be used for the second forced continuation. Furthermore, while our method was evaluated on relatively small distilled model due to computational resource limitations, examining its impact on larger-scale language models warrants investigation. Given our focus on the sequential approach to test-time scaling, another compelling future direction would be to explore the integration of our learned token methodology with parallel test-time scaling paradigms. Finally, extending the scope of this research to diverse domains, potentially even those lacking explicit verifiable rewards, such as in the context of LLM alignment, presents an intriguing area for future exploration. We believe the code provided with this paper, with our efficient distributed implementation of an RL training pipeline, will enable researchers to pursue these research directions efficiently."
        },
        {
            "title": "6 Limitations",
            "content": "While our proposed method demonstrates promising results, it is subject to several limitations. First, as our analysis indicates, the effectiveness of the learned token appears to be contingent on the baseline performance of the budget forcing technique itself. If standard budget forcing does not yield improvements, our learned <continue-thinking> token is unlikely to provide significant advantage. Second, the process of learning the token embedding necessitates training phase, which is inherently more computationally demanding and requires access to the models weights compared to simply employing fixed tokens. Third, our method requires the addition of new token to the models vocabulary. This modification might not be feasible or permitted when utilizing LLMs through certain API interfaces, which often provide restricted access to the models architecture, thus preventing vocabulary modifications. Finally, our experiments were only performed on the domain of mathematical questionswe did not explore the generalization capabilities of this method to additional domains."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This research was supported by the European Union (ERC, SafetyBounds, 101163414). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. This research was also partially supported by the Israel Science Foundation (ISF grant 729/21). Y. R. acknowledges additional support from the Career Advancement Fellowship at the Technion."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks arXiv preprint learning. with reinforcement arXiv:2503.04697. Anthropic. 2025. Claudes extended thinking. https: //www.anthropic.com/news/visible-extende d-thinking. Accessed: 2025-05-18. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. SFT memorizes, RL generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. 2024. Think before you speak: Training language models with pause tokens. In International Conference on Learning Representations. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Neural Information Processing Systems, Datasets and Benchmarks Track. Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, and Yuyin Zhou. 2025. m1: Unleash the potential of test-time scaling for medical reasoning with large language models. arXiv preprint arXiv:2504.00869. HuggingFace. 2025. Open R1: fully open reproduction of DeepSeek-R1. 9 Hyunbin Jin, Je Won Yeom, Seunghyun Bae, and Taesup Kim. 2025. well, keep thinking: Enhancing llm reasoning with adaptive injection decoding. arXiv preprint arXiv:2503.10167. Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the ACM SIGOPS Symposium on Operating Systems Principles. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Nora Kassner, Timo Schick, Marzieh Saeidi, Noah A. Smith, and Matt Gardner. 2024. Tülu 3: Pushing frontiers in open language model posttraining. arXiv preprint arXiv:2411.15124. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. In International 2023. Lets verify step by step. Conference on Learning Representations. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, and Li Erran Li. 2025. Deepscaler: Surpassing o1-preview with 1.5B model by scaling RL. https://www.notion.so/Deepscal er-Surpassing-o1-preview-with-a-1-5B-mod el-by-scaling-RL-<exact-url>. Notion Blog. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. OpenAI. 2024. Learning to reason with llms. https: //openai.com/index/learning-to-reason-wit h-llms/. Accessed: 2025-05-18. Jacob Pfau, William Merrill, and Samuel Bowman. 2024. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758. Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. 2025. Scaling test-time compute without verification or RL is suboptimal. arXiv preprint arXiv:2502.12118. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. 2025. Expanding RL with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829. Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. 2025. Do large language model benchmarks test reliability? arXiv preprint arXiv:2502.03461. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. GitHub repository. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. 2023. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, and 1 others. 2025. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. 2024. From deInference-time algocoding to meta-generation: rithms for large language models. arXiv preprint arXiv:2406.16838. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 23 others. 2024a. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Xingzhang Ren, and Zhenru Zhang. 2024b. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. 10 Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, and 1 others. 2024. SGLang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems."
        },
        {
            "title": "A Training and Evaluation Parameters",
            "content": "Table 4 summarizes the full list of parameters we used for training and evaluation. GRPO Parameters ε β µ 16 0.2 0 4 under licensed GSM8K-Platinum, BY-SA 4.0.2 Apache-2.0,3 and AIME25.4 AIME24, the MIT licensed license.1 under CC licensed under Models: DeepSeek-R1-Distill-Qwen-1.5B, license.5 under licensed Qwen2.5-7B-Instruct, the Apache-2.0 license.6 the MIT licensed under Training Parameters Optimizer Learning rate Temperature Training and Inference Prompt Evaluation Parameters Temperature LLM evaluator prompt AdamW 1e 4 with cosine scheduler 0. You are helpful AI Assistant. First, think through the reasoning inside <think>...</think>. always present the final answer in boxed{} Then, 0.9 Given math question, correct answer, and students final answer (which may include explanations), determine if the correct answer appears in some form within the students answer (ignoring trivial differences like formatting or wording). Output True if the students answer is correct, otherwise output False. Output nothing else. START-OF-QUESTION question: %(question)s END-OF-QUESTION correct answer: %(correct_answer)s students solution: START-OF-STUDENT-SOLUTION %(student_solution)s END-OF-STUDENT-SOLUTION Output True if the students solution equivalent the correct answer and False otherwise. Software Packages: LM-evaluation-harness licensed under the MIT license.7 vLLM licensed under the Apache-2.0.8 Open r1, licensed under the Apache 2.0 license,9 and trl, licensed under Apache 2.0."
        },
        {
            "title": "C Full Experimental Results",
            "content": "We provide the complete set of results for all our experiments. Table 5 shows the complete set of results when using an LLM evaluator in all configurations (Cmax = 1, 2, 3, Bmax = 8192, 9216). Table 6 shows the complete set of results for regexonly evaluation. Table 7 reports, for each configuration, the probability that the learned token yields correct answer when the baseline does not, and the probability that the baseline yields correct answer when the learned token does not."
        },
        {
            "title": "D Generated Answers Examples",
            "content": "We include examples of reasoning traces of cases when using the learned token resulted in correct answer while the baseline did not, and vice versa. In all figures, blue text indicates the original reasoning trace that is common for both the learned model and the baseline model and green text indicates the reasoning trace that was generated after forced continuation. Figure 6 and Figure 7 show full reasoning traces for the learned model and the baseline model respectively on question in which the baseline model was incorrect and adding the learned token allowed the model to continue reasoning through the problem and reach the correct Table 4: Full list of training and evaluation parameters 1https://hf.co/datasets/agentica-org/DeepScaleR-Preview-"
        },
        {
            "title": "B Artifacts Used",
            "content": "The following datasets, software libraries and models were used during this research, all artifacts were used in accordance with their respective licenses. Datasets: DeepScaleR-Preview Dataset,"
        },
        {
            "title": "Dataset",
            "content": "2https://hf.co/datasets/madrylab/gsm8k-platinum 3https://hf.co/datasets/simplescaling/aime24_nofigures 4https://hf.co/datasets/math-ai/aime25 5https://hf.co/deepseek-ai/DeepSeek-R1-Distill-Qwen1.5B 6https://hf.co/Qwen/Qwen2.5-7B-Instruct 7https://github.com/EleutherAI/lm-evaluation-harness 8https://github.com/vllm-project/vllm 9https://github.com/huggingface/open-r1 10https://github.com/huggingface/trl Dataset Baseline (w/o BF) Alternatively Hmm Wait Learned Cmax = 1, Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 2, Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 3, Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 1, Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 2, Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 3, Bmax = 9216 AIME24 AIME25 GSM8K MATH500 23.44 0.73 21.82 0.67 78.41 0.25 79.43 0.26 23.44 0.73 21.88 0.67 78.41 0.25 79.43 0.26 23.44 0.73 21.82 0.67 78.41 0.25 79.42 0.26 23.85 0.66 23.33 0.52 78.08 0.36 79.88 0. 23.85 0.66 23.33 0.52 78.08 0.36 79.88 0.33 23.85 0.66 23.33 0.52 78.08 0.36 79.88 0.33 23.07 0.72 21.98 0.56 78.43 0.30 80.26 0.36 23.07 0.70 22.08 0.53 79.09 0.42 80.40 0.29 23.18 0.69 22.6 0.51 79.09 0.54 81.09 0.32 23.07 0.71 21.77 0.55 81.39 0.40 82.00 0. 23.54 0.82 21.72 0.66 78.58 0.34 80.00 0.19 23.85 0.76 21.61 0.67 79.09 0.28 80.28 0.29 24.06 0.76 21.98 0.68 79.71 0.31 80.36 0.27 24.53 0.75 22.34 0.71 82.63 0.20 81.67 0.21 23.02 0.58 22.08 0.65 79.00 0.13 79.60 0.32 23.12 0.62 21.67 0.68 79.65 0.26 80.71 0. 23.49 0.54 22.03 0.61 80.16 0.32 80.86 0.18 23.33 0.73 22.03 0.60 83.17 0.32 82.29 0.24 23.28 0.63 23.33 0.55 78.83 0.35 80.09 0.33 24.17 0.63 23.80 0.60 78.91 0.33 80.69 0.32 23.91 0.62 23.70 0.56 79.11 0.42 80.82 0.30 24.01 0.61 23.28 0.53 81.67 0.47 81.46 0. 23.59 0.68 23.39 0.51 78.96 0.21 80.14 0.38 24.37 0.68 23.75 0.53 79.67 0.35 81.19 0.31 24.06 0.75 23.54 0.51 79.86 0.39 81.21 0.28 24.06 0.69 23.07 0.49 82.53 0.48 82.30 0.34 23.49 0.69 23.80 0.56 78.83 0.34 80.33 0.35 24.48 0.63 23.85 0.53 79.47 0.34 80.84 0. 23.91 0.70 23.49 0.49 80.14 0.24 81.22 0.30 24.06 0.68 22.40 0.44 83.68 0.38 82.51 0.34 Table 5: Accuracy (pass@1) results for different token budget limits Bmax and different numbers of forced thinking continuations Cmax. Results are obtained via regex-based evaluation and an LLM evaluator if the model fails to generate an answer in the correct format. answer. Figure 8 and Figure 9 show full reasoning traces for the learned model and the baseline model, respectively, on question in which both models were correct. Note that in this case, the learned token only adds negligible number of tokens to the answer. 13 Dataset Baseline (w/o BF) Alternatively Hmm Wait Learned Cmax = 1 Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 2 Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 3 Bmax = 8192 AIME24 AIME25 GSM8K MATH500 Cmax = 1 Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 2 Bmax = 9216 AIME24 AIME25 GSM8K MATH500 Cmax = 3 Bmax = 9216 AIME24 AIME25 GSM8K MATH500 22.86 0.75 (81) 21.56 0.63 (85) 66.43 0.50 (84) 77.19 0.30 (94) 22.71 0.74 (81) 21.67 0.55 (84) 64.57 0.33 (81) 77.89 0.34 (94) 22.34 0.68 (81) 21.72 0.51 (84) 65.01 0.56 (81) 77.94 0.23 (94) 22.60 0.71 (82) 22.19 0.52 (82) 64.27 0.37 (80) 78.59 0.29 (94) 22.66 0.69 (82) 21.61 0.55 (84) 75.71 0.31 (92) 80.84 0.27 (96) 22.86 0.75 (81) 21.56 0.63 (85) 66.43 0.50 (84) 77.19 0.30 (94) 23.02 0.83 (80) 21.41 0.65 (84) 64.52 0.16 (81) 77.64 0.22 (94) 23.23 0.78 (80) 21.15 0.67 (84) 64.56 0.16 (80) 77.99 0.30 (94) 23.28 0.75 (82) 21.25 0.64 (84) 63.55 0.36 (79) 78.05 0.26 (94) 24.22 0.76 (81) 21.82 0.68 (83) 78.37 0.40 (94) 80.74 0.18 (96) 22.86 0.75 (80) 21.56 0.63 (85) 66.43 0.50 (83) 77.19 0.30 (94) 22.55 0.51 (79) 21.82 0.67 (83) 64.21 0.46 (80) 77.45 0.31 (94) 22.45 0.57 (79) 21.41 0.67 (85) 64.42 0.29 (79) 78.54 0.30 (94) 22.92 0.52 (81) 21.72 0.61 (83) 62.99 0.43 (77) 78.26 0.20 (93) 22.92 0.74 (81) 21.56 0.58 (84) 80.11 0.30 (95) 81.60 0.22 (96) 23.18 0.64 (82) 23.18 0.50 (86) 66.03 0.40 (84) 77.56 0.32 (95) 22.86 0.62 (82) 23.13 0.54 (87) 65.76 0.42 (82) 77.78 0.25 (95) 23.54 0.65 (81) 23.59 0.58 (87) 64.82 0.52 (81) 78.29 0.34 (94) 23.39 0.60 (82) 23.33 0.58 (86) 64.24 0.60 (80) 78.15 0.32 (94) 23.70 0.62 (82) 23.18 0.53 (86) 75.12 0.33 (91) 80.50 0.35 (96) 23.18 0.64 (82) 23.18 0.50 (86) 66.03 0.40 (84) 77.56 0.32 (95) 23.12 0.67 (82) 23.02 0.49 (85) 64.82 0.25 (82) 77.97 0.35 (95) 23.91 0.67 (83) 23.23 0.51 (86) 64.09 0.38 (80) 78.88 0.32 (95) 23.75 0.74 (83) 23.23 0.50 (87) 64.43 0.48 (79) 78.83 0.38 (95) 23.59 0.67 (83) 22.80 0.47 (86) 78.27 0.58 (94) 81.37 0.38 (97) 23.18 0.64 (82) 23.18 0.50 (85) 66.03 0.40 (83) 77.56 0.32 (94) 23.39 0.70 (81) 23.44 0.56 (87) 63.79 0.57 (80) 78.16 0.39 (95) 24.06 0.67 (82) 23.49 0.49 (86) 64.27 0.28 (79) 78.60 0.40 (94) 23.28 0.72 (82) 23.12 0.46 (86) 62.94 0.50 (77) 78.75 0.32 (94) 23.39 0.66 (83) 22.40 0.44 (86) 80.87 0.42 (96) 81.75 0.34 (96) Table 6: Accuracy (pass@1) results for different token budget limits Bmax and different numbers of forced thinking continuations Cmax. Results are obtained via regex-based evaluation only. The percentage of final answers enclosed in boxed{} is shown in parentheses."
        },
        {
            "title": "Probabilities",
            "content": "AIME24 AIME25 GSM8K MATH500 Cmax = 1, Bmax = 8192 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 2, Bmax = 8192 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 3, Bmax = 8192 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 1, Bmax = 9216 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 2, Bmax = 9216 P(Learned , Baseline ) P(Learned , Baseline ) Cmax = 3, Bmax = 9216 P(Learned , Baseline ) P(Learned , Baseline ) 0.09 0.08 0.01 0.02 0.09 0.09 0.02 0. 0.08 0.08 0.09 0.09 0.06 0.06 0.01 0.02 0.06 0.06 0.01 0. 0.06 0.06 0.06 0.06 0.03 0.06 0.03 0.07 0.02 0.07 0.03 0. 0.02 0.07 0.02 0.07 0.06 0.09 0.02 0.04 0.01 0.04 0.02 0. 0.02 0.04 0.02 0.04 Table 7: Probability of observing correct answer from the learned token and an incorrect answer from the baseline and vice versa."
        },
        {
            "title": "Learned token",
            "content": "Question: Lloyd has an egg farm. His chickens produce 252 eggs per day and he sells them for $2 per dozen. How much does Lloyd make on eggs per week? <think> First, need to calculate the daily revenue from egg sales. Each chicken produces 252 eggs daily, so for 27 chickens, the total daily egg production is 252 multiplied by 27, which equals 6,804 eggs. Next, Ill convert the number of eggs into dozens since the selling price is given per dozen. There are 12 eggs in dozen, so the total dozens of eggs sold daily are 6,804 divided by 12, resulting in 567 dozen. Now, Ill find out how much 567 dozen eggs sell for per day by multiplying the number of dozens by the selling price per dozen. Thats 567 multiplied by $2, which equals $1,134. Finally, to determine the weekly revenue, Ill multiply the daily revenue by the number of days in week, which is 7. So, $1,134 multiplied by 7 equals $7,938. <continue_thinking> no, let me try that again. If have 27 chickens each producing 252 eggs, thats 27*252=6,804 eggs per day. He sells them for $2 per dozen,which is $2/12 per egg. So to find daily revenue, its 6,804/12 *2 = $1,134. Then multiply by 7 to get weekly: 1,134*7= $7,938. Wait, did just repeat the same steps? Maybe need to approach it differently. Alternatively, first calculate the daily revenue. 252 eggs produce 252/12=21 dozen. Each dozen sells for $2, so 21*2=$42 per day. Then, over week (7 days), its 42*7=$294. But Im confused earlier got $7,938. Which one is correct? Let me double-check. 27 chickens at 252 eggs each is indeed 27*252=6,804 eggs. 6,804 divided by 12 is 567 dozen. 567*2= $1,134 per day. Then 1,134*7= $7,938. Oh, wait. think Im making mistake in the calculation order. Let me clarify: Egg production: 27 chickens * 252 eggs/day = 6,804 eggs/day. Dollars per egg: $2 per dozen, which is $2/12= $0.1667 per egg. Total revenue per day: 6,804 eggs/day * $0.1667/egg $1,134. Revenue per week: $1,134/day *7= $7,938. But when calculated earlier, got different result. Whats the mistake? Wait, 252 eggs per day is 252 divided by 12 is 21 dozen. 21*2= $42 daily. 42*7= $294. Now, why the discrepancy? Lets check 27*252. 252*27: 27*200=5,400, 27*52=1,404, total is 5,400+1,404=6,804. Yes, thats correct. So 27 chickens produce 6,804 eggs day. At $2 per dozen, 6,804/12=567 dozen, 567*2= $1,134. 1,134*7= $7,938. But why is another method giving different answer? Wait, maybe Im interpreting the question wrong. If there are 27 chickens, each producing 252 eggs day, so total eggs per day are 27*252=6,804. To find revenue, need to know how much they sell each day. If they sell each dozen for $2, then per day revenue is 6,804/12*$2=567*$2= $1,134. Over 7 days, $1,134*7= $7,938. Alternatively, 27 chickens, 252 eggs/day: total eggs per week. 252*7=1,764 eggs/week. Convert to dozens: 1,764/12=147 dozen. 147*2= $294 per week. Ah, see now. Depending on how you group the eggs, you get different per-week revenues. The confusion comes from whether the revenue is per dozen or per week. The question says \"how much does Lloyd make on eggs per week.\" Since the chickens produce 252 eggs day, and he sells them for $2 per dozen, the method is to calculate how much he makes per dozen eggs multiplied by how many dozens he sells per week. So, per week, he sells 252*7=1,764 eggs. Divided by 12, thats 147 dozen. 147*2= $294 per week. Therefore, the correct answer is $294 per week. </think> Lloyd sells his eggs at $2 per dozen. To determine his weekly revenue: 1. Calculate weekly egg production: 252 eggs/day * 7 days = 1,764 eggs/week 2. Convert weekly eggs to dozens: 1,764 eggs / 12 = 147 dozen 3. Calculate weekly revenue: 147 dozen * $2/dozen = $294 Answer: Lloyd makes boxed{294} dollars per week from his egg sales. Figure 6: GSM8K reasoning trace demonstrating the positive impact of the <continue_thinking> token. Blue indicates the original reasoning, yielding an incorrect answer of 7,938. Green shows the continuation after the special token was added, leading to the correct answer of 294."
        },
        {
            "title": "Baseline",
            "content": "Question: Lloyd has an egg farm. His chickens produce 252 eggs per day and he sells them for $2 per dozen. How much does Lloyd make on eggs per week? <think> First, need to calculate the daily revenue from egg sales. Each chicken produces 252 eggs daily, so for 27 chickens, the total daily egg production is 252 multiplied by 27, which equals 6,804 eggs. Next, Ill convert the number of eggs into dozens since the selling price is given per dozen. There are 12 eggs in dozen, so the total dozens of eggs sold daily are 6,804 divided by 12, resulting in 567 dozen. Now, Ill find out how much 567 dozen eggs sell for per day by multiplying the number of dozens by the selling price per dozen. Thats 567 multiplied by $2, which equals $1,134. Finally, to determine the weekly revenue, Ill multiply the daily revenue by the number of days in week, which is 7. So, $1,134 multiplied by 7 equals $7,938. </think> Solution To determine how much Lloyd makes on eggs per week, well break down the problem step by step. 1. Calculate the total number of eggs produced per day: Eggs per day = Number of chickens Eggs per chicken per day Eggs per day = 27 252 = 6,804 eggs 2. Determine the number of dozens of eggs produced per day: Dozens per day = Eggs per day 12 Dozens per day = 6,804 12 = 567 dozen 3. Calculate the daily revenue from selling eggs: Revenue per day = Dozens per day Selling price per dozen Revenue per day = 567 2 = 1,134 dollars 4. Find the weekly revenue by multiplying daily revenue by the number of days in week: Revenue per week = Revenue per day 7 Revenue per week = 1,134 7 = 7,938 dollars Final Answer: boxed{7,938 dollars} Figure 7: GSM8K reasoning trace of the baseline model for the same question as in Figure 6. The final answer provided by the baseline model is incorrect, as opposed to the correct answer given in Figure 6."
        },
        {
            "title": "Learned token",
            "content": "Question: Twenty tourists discovered 700 shells in strip mall parking lot. They had divided into three groups, Alphas, The finders, and Gogetters to find as many shells as possible. If team Alphas found 40% of the shells, and team The finders found 60% of the remaining shells, how many shells did team Gogetters find? <think> Okay, lets try to figure out how many shells team Gogetters found. So, there are 700 shells in total, and 20 tourists split them into three groups: Alphas, The finders, and Gogetters. need to find out how many each group found. First, team Alphas found 40% of the shells. That sounds like straightforward calculation. So, 40% of 700. Let me write that down: 0.4 * 700. Hmm, 0.4 is like 40%, so if multiply 700 by 0.4, what do get? Let me do that math. 0.4 times 700... Yeah, thats 280. So, team Alphas found 280 shells. Now, after team Alphas, there are 700 - 280 = 420 shells left. That number is the remaining shells after Alphas team found theirs. Next, team The finders found 60% of these remaining shells. So, need to calculate 60% of 420. Alright, 0.6 times 420. Let me compute that. 0.6 * 400 is 240, and 0.6 * 20 is 12. So, 240 + 12 equals 252. Therefore, team The finders found 252 shells. Now, to find out how many shells team Gogetters found, subtract the shells found by both Alphas and The finders from the total. So, 700 - 280 - 252. Let me do that step by step. First, 700 - 280 is 420, as we had before. Then, 420 - 252. Hmm, 420 minus 200 is 220, and then minus 52 more is 168. So, team Gogetters found 168 shells. Let me just double-check my calculations to make sure didnt make mistake. Alphas: 40% of 700 is indeed 280. Subtracting that leaves 420. The finders take 60% of 420, which is 252. Subtracting 252 from 420 gives 168 for Gogetters. That all adds up correctly. think thats all. The Gogetters found 168 shells. <continue_thinking> , and hes right. </think> To determine how many shells team Gogetters found, follow these steps: 1. Calculate the number of shells found by team Alphas: 40% of 700 = 0.4 700 = 280 shells 2. Subtract the shells found by Alphas from the total:: 700 280 = 420 shells remaining 3. Calculate the number of shells found by team The finders: 60% of 420 = 0.6 420 = 252 shells 4. Subtract the shells found by The finders from the remaining shells: 420 252 = 168 shells Conclusion: Team Gogetters found boxed{168} shells. Figure 8: GSM8K reasoning trace demonstrating that the <continue_thinking> token does not generate many tokens when the model is confident. Blue indicates the original reasoning. Green shows the short continuation after the special token was added."
        },
        {
            "title": "Baseline",
            "content": "Question: Twenty tourists discovered 700 shells in strip mall parking lot. They had divided into three groups, Alphas, The finders, and Gogetters to find as many shells as possible. If team Alphas found 40% of the shells, and team The finders found 60% of the remaining shells, how many shells did team Gogetters find? <think> Okay, lets try to figure out how many shells team Gogetters found. So, there are 700 shells in total, and 20 tourists split them into three groups: Alphas, The finders, and Gogetters. need to find out how many each group found. First, team Alphas found 40% of the shells. That sounds like straightforward calculation. So, 40% of 700. Let me write that down: 0.4 * 700. Hmm, 0.4 is like 40%, so if multiply 700 by 0.4, what do get? Let me do that math. 0.4 times 700... Yeah, thats 280. So, team Alphas found 280 shells. Now, after team Alphas, there are 700 - 280 = 420 shells left. That number is the remaining shells after Alphas team found theirs. Next, team The finders found 60% of these remaining shells. So, need to calculate 60% of 420. Alright, 0.6 times 420. Let me compute that. 0.6 * 400 is 240, and 0.6 * 20 is 12. So, 240 + 12 equals 252. Therefore, team The finders found 252 shells. Now, to find out how many shells team Gogetters found, subtract the shells found by both Alphas and The finders from the total. So, 700 - 280 - 252. Let me do that step by step. First, 700 - 280 is 420, as we had before. Then, 420 - 252. Hmm, 420 minus 200 is 220, and then minus 52 more is 168. So, team Gogetters found 168 shells. Let me just double-check my calculations to make sure didnt make mistake. Alphas: 40% of 700 is indeed 280. Subtracting that leaves 420. The finders take 60% of 420, which is 252. Subtracting 252 from 420 gives 168 for Gogetters. That all adds up correctly. think thats all. The Gogetters found 168 shells. </think> There are 700 shells in total. Team Alphas found 40 The remaining shells after Alphas team are: Team The finders found 60 0.4 700 = 280 shells 700 280 = 420 shells 0.6 420 = 252 shells Finally, the number of shells found by team Gogetters is: 700 280 252 = 168 shells boxed{168} Figure 9: GSM8K reasoning trace of the baseline model for the same question as in Figure 8. In this case, both the baseline and learned token output correct answer and the answer provided by the baseline model is almost identical to the one provided by the learned token."
        },
        {
            "title": "Learned token",
            "content": "Question: Michael is traveling on the interstate at an average speed of 50km/hr (taking into account all the necessary stops for gas, relaxation, etc.). If he goes on like this for 3 days, and the distance between Alaska and Texas is 6000 km, what percentage of this distance has he covered? <think> Okay, so Michael is traveling on the interstate and wants to know what percentage of the distance between Alaska and Texas he has covered. The distance is 6000 km, and hes traveling at an average speed of 50 km/hr for 3 days. need to figure out how much hes traveled in that time and then find out what percentage that is of the total distance. First, should calculate the total distance Michael has covered in 3 days. His speed is 50 km/hr, so need to multiply that by the number of hours he spends traveling each day. But since the days are different temperatures and maybe he works longer hours, should consider the average hours per day. Lets say he works 8 hours each day. So, for 3 days, that would be 8 hours/day * 3 days = 24 hours. Now, multiplying his speed by the total hours: 50 km/hr * 24hr = 1200 km. So, Michael has traveled 1200 km in those 3 days. Next, to find out what percentage this is of the total distance, which is 6000 km. can use the percentage formula: (part/whole) * 100 Wait, but is the 8 hours per day an accurate average? If his work hours vary, the total time could be different. Maybe should calculate based on days without considering hours. But that seems complicated without specific hours. Alternatively, perhaps should clarify if 50 km/hr is the average speed per day or total speed. But the question says hes traveling at 50 km/hr, so think its average speed over total time. Alternatively, maybe the question expects simply to use the average speed over the total distance. Let me check again: the question states hes traveling at 50 km/hr, so its average speed per day, not per trip. Therefore, the total distance is 50 km/hr * 8 hours/day * 3 days = 1200 km, as calculated before. So, 1200 km is 20% of 6000 km. <continue_thinking> , is it 1200/6000=0.2, which is 20%. Yeah, thats correct. </think> To determine what percentage of the 6000 km distance Michael has covered, we first calculate the total distance he has traveled. Assuming he works 8 hours each day: Then, we find the percentage of the total distance: 50 km/hr 8 hr/day 3 days = 1200 km (cid:19) (cid:18) 1200 km 6000 km 100% = 20% boxed{20%} Figure 10: GSM8K reasoning trace of wrong answer given by both models."
        },
        {
            "title": "Baseline",
            "content": "Question: Michael is traveling on the interstate at an average speed of 50km/hr (taking into account all the necessary stops for gas, relaxation, etc.). If he goes on like this for 3 days, and the distance between Alaska and Texas is 6000 km, what percentage of this distance has he covered? <think> Okay, so Michael is traveling on the interstate and wants to know what percentage of the distance between Alaska and Texas he has covered. The distance is 6000 km, and hes traveling at an average speed of 50 km/hr for 3 days. need to figure out how much hes traveled in that time and then find out what percentage that is of the total distance. First, should calculate the total distance Michael has covered in 3 days. His speed is 50 km/hr, so need to multiply that by the number of hours he spends traveling each day. But since the days are different temperatures and maybe he works longer hours, should consider the average hours per day. Lets say he works 8 hours each day. So, for 3 days, that would be 8 hours/day * 3 days = 24 hours. Now, multiplying his speed by the total hours: 50 km/hr * 24hr = 1200 km. So, Michael has traveled 1200 km in those 3 days. Next, to find out what percentage this is of the total distance, which is 6000 km. can use the percentage formula: (part/whole) * 100 Wait, but is the 8 hours per day an accurate average? If his work hours vary, the total time could be different. Maybe should calculate based on days without considering hours. But that seems complicated without specific hours. Alternatively, perhaps should clarify if 50 km/hr is the average speed per day or total speed. But the question says hes traveling at 50 km/hr, so think its average speed over total time. Alternatively, maybe the question expects simply to use the average speed over the total distance. Let me check again: the question states hes traveling at 50 km/hr, so its average speed per day, not per trip. Therefore, the total distance is 50 km/hr * 8 hours/day * 3 days = 1200 km, as calculated before. So, 1200 km is 20% of 6000 km. </think> Michael has traveled 20% of the distance between Alaska and Texas. boxed{20%} Figure 11: GSM8K reasoning trace of the baseline model for the same question as in Figure 10."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Technion Israel Institute of Technology",
        "Department of Electrical and Computer Engineering, Technion Israel Institute of Technology",
        "Independent Researcher"
    ]
}