{
    "paper_title": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles",
    "authors": [
        "Kuang Wang",
        "Xianfei Li",
        "Shenghao Yang",
        "Li Zhou",
        "Feng Jiang",
        "Haizhou Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications."
        },
        {
            "title": "Start",
            "content": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles Kuang Wang1,2, Xianfei Li1, Shenghao Yang1, Li Zhou1, Feng Jiang1,2,3 *, Haizhou Li1,2 1The Chinese University of Hong Kong, Shenzhen 2Shenzhen Research Institute of Big Data 3Shenzhen University of Advanced Technology kuangwang@link.cuhk.edu.cn, jeffreyjiang@cuhk.edu.cn 5 2 0 2 8 2 ] . [ 2 8 6 9 8 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multiturn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications 1."
        },
        {
            "title": "Introduction",
            "content": "The user simulator is designed as proxy for real users in interactions with large language models (LLMs). It can simulate realistic user by generating the target users behavior or utterances based on the specified characteristics, enabling dynamic multiturn interactions with LLMs (Wan et al., 2022) *Feng Jiang is the corresponding author. 1All resources will be released at https://github.com/ wangkevin02/USP Figure 1: Examples of performance across various user simulators in multi-turn human-LLM interactions. OF and SC refer to objective facts and subjective characteristics, respectively. and scene reproduction (Wang et al., 2024b). As result, it becomes an effective alternative (Liu et al., 2023; Ferreira et al., 2024) in scenarios where real-world human-computer interaction data is difficult to obtain, especially in domains with privacy and ethical concerns, such as medical conIt also sultations (Valizadeh and Parde, 2022). helps Simulation-to-Reality (Sim2Real) applications, such as tutorial strategies, election simulations, and public opinion research (Liu et al., 2024; Zhang et al., 2024b; Chuang et al., 2024). Recent LLMs advance promote user simulators by enhancing naturalness and utility (Deng et al., 2024; Zhang et al., 2024a), as shown in Figure 1. Since directly using LLM as the user simulator suffers from role confusion (Xu et al., 2023a), some works (Xu et al., 2023a; Kong et al., 2024; Sun et al., 2024) attempt to enhance its authenticity by training user simulator on the conversation dataset. However, they are only trained on text utterance, making it difficult to simulate diverse user behaviors without seed context, exhibiting limited self-awareness (Tseng et al., 2024) and failing to maintain consistent personality. Although some LLM-based role-playing methods (Moon et al., 2024) utilize predefined profiles to alleviate this problem, they require additional extensive annotations and can only be applied to celebrities, lacking the diversity of simulation. To address the issues above, we believe that user simulator knows users intrinsic characters hidden in their conversations first and then can be better simulation. Therefore, we treat user simulation as dialogue reconstruction task and propose novel framework named the User Simulator with implicit Profile (USP). It is decomposed into implicit profile extraction to capture the users underlying characteristics from the target user dialogue and conditional generation based on the profile. In this framework, we first propose an LLMdriven profile extractor to extract implicit profiles from user conversations with well-designed profile schema. Inspired by interpersonal interaction theory(Kruglanski and Higgins, 2013), our profile schema contains two dimensions (objective facts (OF) and subjective characteristics (SC)) with dozen attributes to describe the user comprehensively. Different from existing works (Cheng et al., 2024b; Tu et al., 2024), we then polish the profile attributes into natural, descriptive profiles to ensure generalization. Then, we integrate the extracted user profiles into the user simulator through two-stage training: (1) conditional supervised fine-tuning with user profiles for utterance-level simulation, and (2) reinforcement learning with cycle consistency to align reflected profiles from simulated dialogues with given profiles for conversation-level simulation. We also implement diverse profile sampler to capture authentic user distributions. Our experiments reveal that USP enhances semantic and stylistic similarity of reconstructed multi-turn dialogues to target dialogues by about 34% and 43% over the leading baseline, with reconstruction errors halved, showcasing superior authenticity and diversity. It matches the consistency of ProfileGPT(4o), GPT-4o-based simulation baseline, boosting dialogue profile consistency by 14% in multi-turn scenarios while performing comparably in single-turn cases. Furthermore, our USP-based multi-turn dynamic evaluation of LLMs for downstream tasks aligns well with established benchmarks,allowing for more granular assessment of LLM performance across different user groups. Our key contributions are outlined below: We propose novel approach for constructing user simulators using implicit user profiles embedded in human-LLM conversations. We develop new framework that infers implicit user profiles as insight, further enhanced with conditional fine-tuning and reinforcement learning with cycle consistency for better simulation at both the utterance and conversation levels. Experimental results show USP surpasses all baselines in authenticity and diversity, maintains comparable consistency, and proves effective for multi-turn dynamic evaluation of LLMs."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 General User Simulator Early user simulators including agenda-based methods (Schatzmann et al., 2007; Schatzmann and (Asri Young, 2009) and model-based methods et al., 2016; Kreyssig et al., 2018). These simulators were initially designed with narrow scope due to limited natural language generation capabilities, such as generating synthetic binary preference responses (Christakopoulou et al., 2016) in conversational recommendation systems. Recent advancements in LLMs enabled more sophisticated simulations of realistic conversations, offering significantly enhanced natural language flexibility. These advances include the use of LLMs for self-chat (Xu et al., 2023b) and dual LLM architectures, where separate models role-play user and assistant based on seed conversations (Ding et al., 2023). Following these innovations, other trained user simulators, such as PlatoLM (Kong et al., 2024) and Parrot (Sun et al., 2024), learn human discourse patterns directly from human-LLM interactions in conversations. 2.2 Persona-based User Simulator General user simulators often struggle to capture the full spectrum of diverse user needs, leading to growing interest in persona-based personalization to improve both controllability and diversity in simulations (Takanobu et al., 2020). Some researchers attempt to leverage goal generators (Takanobu et al., 2020) to create diverse user goals or retrieval-based personas derived from historical data (Shi et al., 2019) to guide user simulators in task-oriented dialogue (ToD) systems. With the rise of LLMs and their impressive zero-shot role-playing abilities (Njifenjou et al., 2024), prompt-driven user simulation has become the dominant approach. For example, LLMs have been used with carefully designed predefined profiles to align with human beliefs (Chuang et al., 2024), simulate consultation scenarios with users exhibiting varying personalities and needs in ToD systems (Zhang et al., 2024a), and model user preferences in conversational recommendation systems (Yoon et al., 2024)."
        },
        {
            "title": "3 Task Definition",
            "content": "We formulate user simulation as dialogue refactoring task, aiming to replicate multi-turn user behavior in target dialogues. Given target dialogue di = {(ui1, ri1), . . . , (uij, rij)} between user Ui and response model Ri, where uij and rij represent the j-th turn user utterance and the corresponding model response, respectively. To achieve high-fidelity simulation of user responses within given context, we aim to minimize the utterance-level distance Dutt(uij, ij) where ij Pprob(c ij, i). Here, ij represents the context used by the user simulator to generate ij. This optimization ultimately leads to reducing the dialogue-level distance, as formulated in Eq. 1. min iPprob(U i) Ddia(di, i) (1) where Ddia represents distance function that evaluates the user utterances between the simulated and the real conversation di. dialogue Recent studies show that role-playing with specific user profiles (Pi) can effectively achieve diverse user simulations (Liu et al., 2023). However, unlike celebrities or well-known characters, user profiles in real-world conversations are often implicit and difficult to obtain (Wang et al., 2024a). Category Dimension Attributes Objective Facts Scene-Consistent Attributes Age, Gender, Location, Occupation, Education, Family Relationship, Routines/Habits, Social Relationships, Other Experiences Scene-Related Attributes Intrinsic Characteristics Goals/Plans, Task Details Big Five Personality Traits, Language Styles Subjective Characteristics Table 1: The Designed User Profile Schema. To address this, we reformulate the task by first extracting the implicit user profile from the given dialogue using profile extractor Pextractor, and then reconstructing closer dialogue, with an emphasis on the users utterances, as described in Eq. 2. min iPprob(U i,Pi) i), Ddia(di, (2) where Pi = Pextractor(di)."
        },
        {
            "title": "Profiles",
            "content": "To accomplish this task, we propose the User Simulator with Implicit Profiles (USP) framework, as illustrated in Figure 2, which aims to minimize the objective in Eq. 2 while ensuring authenticity, consistency, and diversity. 4.1 User Profile Construction 4.1.1 User Profile Schema We believe that the user profile should reveal user characteristics from two aspects: explicit personal information and implicit communication styles. Therefore, inspired by interpersonal interaction theory (Zhou et al., 2024b), we design user profile schema containing objective facts(OF) and subjective characteristics(SC) to represent them, as shown in Table 1. The OF focuses on common topics in human conversation (Cheng et al., 2024b; Dunbar et al., 1997) including Scene-Consistent Attributes (such as age, gender, and location) and Scene-Related Attributes (such as goal,and task details). SC considers both external and internal personality dimensions represented by language style (Wang et al., 2024a) and the Big-Five Traits Different from previous work (Cheng et al., 2024b; Tu et al., 2024), we reformulate discrete attributes into coherent narrative descriptions to achieve greater generalization and flexibility. Figure 2: Overview of our proposed User Simulator with implicit Profile(USP) framework. 4.1.2 User Profile Extractor To obtain such user profile, we design an LLMdriven user profile extractor extracting the implicit user profile from the human-LLM conversation. The extractor first leverages advanced LLM (such as GPT-4o) to extract the user character attributes mentioned above with well-designed prompt. Then, the extractor collects the valid attributes (No empty) together and polishes them into natural language descriptions. Further prompt details regarding the extractor can be found in Appendix A.2. 4.1.3 Profile Quality Verification Due to the reliance on predefined user profiles in existing role-playing approaches (Zhou et al., 2024b), the correlation between user profiles and conversations has been largely overlooked. To address this limitation, we propose Dialogue Profile Consistency (DPC) for evaluating OF based on atomic fact verification that measures both precision (DP.P) and recall (DP.R) (Jandaghi et al., 2024). Drawing inspiration from FactScore (Min et al., 2023) and Con.Score (Madotto et al., 2019), we first introduce Factual Consistency (Fact.Con), as defined in Eq. 3. Given target , we evaluate the consistency between the source and target by decomposing into atomic facts afk using an atomic fact generator (afg). We then compute the natural language inference (NLI) score for each atomic fact with respect to the source S. Fact.Con(S, ) = 1 afk NLI(S, afk) afkafg(T ) (3) We then define DP.Pi = Fact.Con(di, Pi) and DP.Ri = Fact.Con(Pi, di), and compute DPC as their harmonic mean. When dialogue di serves as the target , each user utterance uij is treated directly as an atomic fact afk. Conversely, when the profile serves as the target , we utilize afg followed (Min et al., 2023) to decompose it into atomic facts. Additionally, we use validation score (Val.Score) to assess the quality of SC descriptions based on the dialogue, rating them on scale from 1 to 5 using GPT-4o. Detailed prompts in Appendix D. 4.2 Conditional Supervised Fine-Tuning To empower the LLM with the general capability to simulate diverse users at the utterance level, we utilize conditional supervised fine-tuning based on user profiles. It enables the LLM to learn the conditional generation mapping based on both the extracted profile Pi and context ij. As subtle misalignment between the core objectives of the user simulator and the response model, the SFT language modeling loss is adjusted as shown in Eq. 4. llm = log Pprob(u i,j, Pi) i,j,<k, i,j,ku (4) where uijk represents the k-th token of the j-th utterance from the i-th user. 4.3 Reinforcement Learning with Cycle Consistency To further enhance conversation-level consistency, we introduce Reinforcement Learning with Cycle Consistency (RLCC), which optimizes the user simulator by aligning the reflected profile, extracted from simulated dialogues, with the target profile. In this stage, we extract the simulator profile from dialogues generated by the user simulator using the profile generator, based on virtual profiles sampled by our diverse profile sampler (see Section 4.4). Our goal is to maximize the semantic similarity between the target profile Pi and the extracted reflected profile both in objective facts and subjective characteristics. The dialogue-level reward is then distributed to each user utterance within the dialogue, as shown in Eq. 5. rcc i,j = sim(Pi, i) (5) Then, we optimize profile recall through Proximal Policy Optimization (PPO) (Schulman et al., 2017) using cycle consistency as dialogue-level reward signal. It enhances the user simulators selfexpression in dialogues, moving beyond simple profile adherence or precision consistency. Additionally, to prevent reward hacking, we incorporate an AI detection model as an auxiliary reward. The final reward is formulated in Eq. 6. ri,j = λrcc i,j + (1 λ)rai_detect i,j (6) where rai_detect = AI_detect(ui,j) and λ = 0.8 is i,j used to emphasize the importance of cycle consistency. The AI detection model (Yang et al., 2024) and profile generator are all fine-tuned based on our train dataset, details in Appendix B.1. 4.4 Diverse Profile Sampling To generate diverse and naturalistic user profiles that reflect real-world distributions, we propose Density Profile Sampler that preserves distribution characteristics while ensuring coverage of underrepresented cases. Profiles are first embedded into the semantic space using SimCSE (Gao et al., 2021), then projected with UMAP (McInnes et al., 2018) to maintain density relationships. We then estimate the underlying profile distribution using Gaussian Kernel Density Estimation (GKDE). This enables direct sampling of realistic profiles based on probability density, while generating virtual profiles during random sampling by combining OF and SC descriptions from nearest neighbors."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate user simulators authenticity and consistency at both the utterance and conversation levels while assessing diversity through the difference between our simulated and real user distributions. 5.1 Datasets We select the popular LMSYS-Chat-1M (Zheng et al., 2023) as our data source for the experiment, which contains one million human-LLM conversations. Following previous work (Kong et al., 2024), we filter the samples for non-English language, toxicity, and redundancy and obtain complete 94,874 samples (87,882/4,626/2,366 for Training/Validation/Test datasets). Then, we use GPT-4o-based profile extractor described in Section 4.1 to preprocess them with annotating user profiles for each conversation and construct them into LMSYS-USP. Detailed preprocessing are in Appendix A.1. We used DPC and Val.Score to automatically evaluate the quality of extracted user profiles on the test set of LMSYS-USP, plus Persona-Chat (Zhang et al., 2018) and ConvAI2 2 (Dinan et al., 2019) with manually annotated dialogue datasets of other existing predefined profiles. Table 2 shows that the extracted profile can achieve over 84% DPC and even the distill-llama3 is close to that of GPT-4o, demonstrating the effectiveness of our annotation. Additionally, we select 100 samples for manual evaluation and it shows over 4/5 scores for the 2We use the human-to-bot dataset in https://huggingface.co/datasets/convai-challenge/conv_ai_2 Dataset Profile Source DP.P Avg DP.P # Fact DP.R Avg DP.R # Fact DPC Val.Score OF SC LMSYS-USP GPT4o LMSYS-USP Distill-llama3 Persona Chat Persona Chat ConvAI2 ConvAI2 GPT4o Human GPT4o Human 86.89 86.15 86.21 76.21 68.71 25.69 25.64 23.81 22.82 8.59 17.44 8.70 82.24 81.95 62.76 42.94 39.15 12.64 3.71 3.71 7.86 7.86 9.97 9.97 84.50 84.00 72.64 54.93 49.88 16.94 4.42 4.36 4.35 - 3.47 - Table 2: Automatic evaluation results of profile quality across different datasets. quality of the generated profiles (See Appendix B.4 for further details). 5.2 Baseline Models (1) User Simulator without User Profile: This includes the untrained DialogueGPT(4o), where GPT-4o relies solely on context to predict the next user utterance, and PlatoLM (Kong et al., 2024), which is fine-tuned on our training dataset using LLaMA-3-8B and can be considered equivalent to our approach without incorporating profile. (2) User Simulator Guided by the User Profile: We adopt ProfileGPT(4o) and ProfileGPT(llama), which utilizes GPT-4o and LLAMA3-8B-Instruct (AI@Meta, 2024) as the role-play backbone with our constructed profiles. Additionally, we include CharacterGLM (Zhou et al., 2024a), which performs role-playing with any given profile, and CharacterLLM (Shao et al., 2023), which role-played on several famous celebrities. The detailed setup and prompts are provided in Appendix B.3. 5.3 Metrics Authenticity: We use SimCSE (Gao et al., 2021) to compute semantic similarity (Sem-Sim) and style embeddings (Wegmann et al., 2022) to compute style similarity (Style-Sim) for evaluating Dutt(uij, ij) and Ddia(di, i). We also employ Author Verification Accuracy (AVA) to assess stylistic consistency by measuring whether paired sentences share authorship based on similarity thresholds (Wegmann et al., 2022). For multi-turn evaluation, we compute dialogue-level distances by concatenating each users utterances. Consistency: We evaluate dialogue-level consistency between dialogues and profiles using reverse metrics: r-DP.P and r-DP.R, calculated as act.Con(Pi, i, Pi) respeci) and act.Con(d tively, along with their harmonic mean r-DPC, from profile-centric perspective. For utterance-level evaluation, we compute the mean DP.P score across individual utterances. Additionally, we incorporate Persona Coverage (P.Cover) (Song et al., 2019) to assess keyword-level consistency. The Subjective Characteristic Score (SC.Score), measures the reflection of subjective traits, assessed by GPT-4o with prompt in Appendix D. Diversity: We compute the Absolute Difference Value (ADV), which represents the Euclidean distance between the PCA-reduced embeddings of generated and target dialogues, to evaluate the discrepancy between the distribution of the reconstructed dialogues and the original dialogues. Additionally, we assess multi-turn dialogue continuity using the early stop rate (ESR), which flags premature endings caused by repetitive responses or repeated gratitude expressions over three turns. 5.4 Results 5.4.1 Utterance-Level Evaluation In the utterance-level evaluation, we evaluate the quality of single-turn response generated by the testing models in given context. As shown in Table 3, USP outperforms all baselines in terms of authenticity, with 53.38 and 46.60, as measured by both semantic (Sem-Sim) and stylistic (Style-Sim) similarity metrics. This highlights the effectiveness of our implicit profileassisted approach for user-LLM dialogue reconstruction, particularly when compared to contextonly models like PlatoLM. Although dedicated role-playing models (ProfileGPT variants) achieve higher consistency scores (r-DP.P), this can be attributed to their direct profile text copying. USP maintains comparable overall performance while striking better balance between authenticity and consistency. 5.4.2 Conversation-Level Evaluation In the conversation-level evaluation, we assess the performance of testing models to chat with GPT-4o Model Type Model Authenticity Consistency Sem-Sim Style-Sim AVA r-DP.P P.Cover SC.Score w/o Profile With Profile DialogueGPT(4o) PlatoLM Character_LLM Character_GLM ProfileGPT(llama) ProfileGPT(4o) USP w/o RLCC USP 40.24 39.37 37.54 38.51 39.82 41.66 54.25 53.38 13.75 43.11 18.88 22.28 14.88 5.74 46.57 46.60 11.28 40. 15.03 18.17 13.47 9.87 43.61 43.35 54.77 68.72 82.19 92.73 71.30 72.61 66.62 57.72 72.29 73.34 71.56 71.23 2.43 2.95 3.92 4.71 3.36 3.39 Table 3: Utterance-level performance comparison of different models on authenticity and consistency metrics. Model Type Model Continuity Authenticity Consistency ESR Sem-Sim Style-Sim AVA r-DP.P r-DP.R r-DPC P.Cover SC.Score w/o Profile With Profile DialogueGPT(4o) PlatoLM Character_LLM Character_GLM ProfileGPT(llama) ProfileGPT(4o) USP w/o RLCC USP 35 18 52 44 31 32 12 10 48.91 43.24 23.37 40.19 46.84 48.87 66.17 65.39 14.21 32.43 7.13 10.86 10.58 10.15 40.01 46. 10.58 31.60 4.69 12.67 11.63 11.26 35.68 38.77 25.48 39.51 67.09 76.59 53.17 56.24 6.43 29.61 29.98 43.72 71.88 74. 10.27 33.85 41.44 55.66 61.13 64.05 21.49 42.75 47.72 51.02 42.63 44.08 2.82 3.64 4.19 4.56 3.24 3. Table 4: Conversation-level performance comparison of different models on authenticity and consistency metrics. in multi-turns, providing the profile or the first turn of the reference dialogue according to their needs. As shown in Table 4, USP outperforms baseline models in authenticity, consistency, and continuity. With the lowest ESR(10), it demonstrates superior dialogue continuity. Notably, USPs advantage in authenticity is more pronounced in conversationlevel scenarios than sentence-level predictions. In terms of consistency, USP demonstrates exceptional performance in r-DP.R metrics and achieves significantly higher r-DPC scores for overall profile dialogue consistency. This superior performance, particularly when compared to role-playing models such as ProfileGPT(4o) and ProfileGPT(llama) which show high P.Cover, suggests that RLCC effectively captures abstract profile characteristics beyond merely matching surface-level keywords. See Appendix C.2 for further case study analysis. 5.4.3 Human Evaluation mance in both authenticity and consistency metrics. Our USP significantly outperforms ProfileGPT (4o) in terms of authenticity (74 vs. 13) and consistency (61 vs. 35) in manual evaluation. USP is superior to PlatoLM trained on the same dataset in terms of authenticity, which demonstrates the usefulness of implicit profile modeling. Thanks to the RLCC module, our USP model has significantly improved consistency (43 vs. 30) by aligning user profiles. Baseline Metrics (% USP win/tie/loss) Authenticity (κ=0.548) Consistency (κ=0.561) ProfileGPT(4o) PlatoLM USP w/o RLCC 74/13/13 55/12/33 37/32/31 61/4/35 - 43/27/30 Table 5: Human evaluation results comparing baselines with USP on authenticity and consistency. We also randomly selected 100 samples with 8 evaluators to conduct conversation-level human evaluations considering authenticity and consistency. Authenticity was assessed through Style, Semantics, and Quality, while consistency covered Accuracy, Completeness, and Quality. Full evaluation details are provided in Appendix B.4. Table 5 demonstrates USPs superior perfor5.4.4 Diversity Sampling Evaluation Figure 3 shows the absolute difference value between target dialogue and generated dialogue by various models across different percentiles. From the results, we observe that USP and USP w/o RLCC consistently achieve the smallest ADV across all percentiles, indicating the dialogues they generated that closely match the target conversaModel Configuration Continuity Authenticity Consistency ESR Sem-Sim Style-Sim AVA r-DP.P r-DP.R r-DPC P.Cover SC.Score USP w/o RLCC USP (5:5) USP (8:2) USP (9:1) 12 14 10 66.17 66.28 65.39 66.91 40.01 41.22 46.23 38.87 35.68 37.03 38.77 33.62 53.17 52.23 56.24 58.36 71.88 71.59 74.38 70.62 61.13 60.39 64.05 63. 42.63 43.58 44.08 46.75 3.24 3.55 3.35 3.33 Table 6: Ablation study of hyperparameters in RLCC. tions. For example, marked by the red cross, PlatoLM has 60% of sample ADV below 15%, while USP has only 5% or less ADV. It demonstrates its stronger capability in preserving the semantic characteristics of the original dialogues. The uniformly lower curves of USP and USP w/o RLCC compared to other baselines (PlatoLM, ProfileGPT(4o), and DialogueGPT(4o)) suggest that our approach generates dialogues that are more faithful to the target conversations across typical and extreme cases. We also analyze and demonstrate that our user simulator can sample different representatives (majority and minority) of users compared to random sampling in Appendix C.1 and apply it to downstream applications in Appendix C.3. Figure 3: Cumulative distribution of ADV performance comparison across different models."
        },
        {
            "title": "6 Analysis",
            "content": "6.1 Ablation Study To evaluate the relative importance of RLCCs two rewards, we tested different values of λ in Equation 6, denoted as USP(λ: 1 λ). As shown in Table 6, λ = 0.8 provides the optimal balance between maintaining model capabilities and enhancing dialogue consistency. Higher values of λ (0.9) compromise speaking style authenticity without improving r-DPC, leading to superficial profile matching, as shown by increased P.Cover scores. Conversely, λ = 0.5 achieves authentic style features but lacks sufficient consistency emphasis, resulting in stagnant performance across capabilities. 6.2 Applications: Dynamic Multi-turn Evaluation For LLMs One application of our simulator is to fill the gap in the current dynamic multi-turn evaluation of large models. The user simulator can simulate different user groups to dynamically interact with the tested model in multiple rounds and reveal their specific defects, as shown in Table 7. Model Setup Sampling Strategy Avg. Ranking in LiveBench/ Major Minor Virtual Chatbot-Arena Deepseek-v3 GPT-4o Claude-Sonnet 4o-Mini Claude-Haiku 8.25 7.86 7.18 6.84 4.88 6.13 6.65 6.61 5.70 5.42 7.70 7.19 7.48 5.52 5.43 7.36 7.23 7.09 6.02 5.24 1 3 2 4 Table 7: Response model performance comparison over different target groups. We simulated 300 diverse user profiles by USP, comprising 100 highest-probability profiles(the majority), 100 lowest-probability profiles(the minority) based on estimated density, and 100 random synthetic profiles(the virtuality), using the sampler mentioned in Section 4.4. Then, we leverage USP using these profiles to chat with LLM in multi-turns and evaluate them, followed by MT-Bench (Zheng et al., 2024). The experimental results indicate that our user simulator is effective and consistent with the average rankings on the latest Livebench (White et al., 2024) and Chatbot-Arena (Chiang et al., 2024). In addition, it can also be seen that GPT-4o is better at catering to minority groups than other models, indicating its superior robustness. more detailed analysis can be found in the appendix C.3."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose novel user simulator with implicit profiles that excels in authenticity, consistency, and diversity. Based on this, we introduce the USP framework, which integrates extracted user profiles into the user simulator by conditional fine-tuning and reinforcement learning with cycle consistency. Our experimental results, validated by both automatic metrics and human evaluations, show that USP significantly outperforms role-playing simulators (e.g., GPT-4o) and direct simulation approaches (e.g., PlatoLM) in authenticity while achieving comparable consistency at both the sentence and conversation levels. Furthermore, through dynamic evaluation across various LLMs chatting with diverse demographic groups, we demonstrate USPs effectiveness in realworld applications."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge the following limitations: 1) Applicability Across Different Scenarios: We conduct the experiments on single dataset, and there has been limited validation across multiple datasets to assess the generalizability of the results. 2) Cultural and Linguistic Scope: We focus on English dialogues in this paper, which may limit the applicability of USP to other linguistic and cultural contexts."
        },
        {
            "title": "Ethics Statement",
            "content": "Although LMSYS-1M has undergone extensive data cleaning and ethical checks, the dataset may still contain sensitive or harmful content, reflecting violent, explicit, or discriminatory traits in certain dialogue. This could result in USP generating unsafe dialogues. We strongly advise against including sensitive terms in profiles when using USP, as this may lead to extreme behavior in both the USP and the response model."
        },
        {
            "title": "References",
            "content": "AI@Meta. 2024. Llama 3 model card. Layla El Asri, Jing He, and Kaheer Suleman. 2016. sequence-to-sequence model for user simulation in spoken dialogue systems. arXiv preprint arXiv:1607.00070. Iz Beltagy, Matthew Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu, and Rui Yan. 2024a. indialogues we learn: Towards personalized dialogue without pre-defined profiles through in-dialogue learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1040810422, Miami, Florida, USA. Association for Computational Linguistics. Yi Cheng, Wenge Liu, Kaishuai Xu, Wenjun Hou, Yi Ouyang, Chak Tou Leong, Xian Wu, and Yefeng Zheng. 2024b. Evolving to be your soulmate: Personalized dialogue agents with dynamically adapted personas. CoRR, abs/2406.13960. Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, and Haizhou Li. Beyond binary: Towards fine-grained llm-generated text detection via role recognition and involvement measurement. In THE WEB CONFERENCE 2025. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards conversational recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 815824. Yun-Shiuan Chuang, Krirk Nirunwiroj, Zach Studdiford, Agam Goyal, Vincent Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy T. Rogers. 2024. Beyond demographics: Aligning role-playing llmbased agents using human belief networks. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1401014026. Association for Computational Linguistics. Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, and Tat-Seng Chua. 2024. Plug-and-play policy planner for large language model powered dialogue agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander H. Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W. Black, Alexander I. Rudnicky, Jason Williams, Joelle Pineau, Mikhail S. Burtsev, and Jason Weston. 2019. The second conversational intelligence challenge (convai2). CoRR, abs/1902.00098. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 3029 3051. Association for Computational Linguistics. Robin IM Dunbar, Anna Marriott, and Neil DC Duncan. 1997. Human conversational behavior. Human nature, 8:231246. Rafael Ferreira, David Semedo, and João Magalhães. 2024. Multi-trait user simulation with adaptive decoding for conversational task assistants. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1610516130. Association for Computational Linguistics. Zhengyuan Liu, Stella Xin Yin, Geyu Lin, and Nancy Chen. 2024. Personality-aware student simulation for conversational intelligent tutoring systems. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 626642. Association for Computational Linguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Samuel Gosling, Peter Rentfrow, and William Swann Jr. 2003. very brief measure of the bigfive personality domains. Journal of Research in personality, 37(6):504528. Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. 2024. Faithful persona-based conversational dataset generation with large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 15245 15270. Association for Computational Linguistics. Chuyi Kong, Yaxin Fan, Xiang Wan, Feng Jiang, and Benyou Wang. 2024. Platolm: Teaching llms in multi-round dialogue via user simulator. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pages 78417863. Association for Computational Linguistics. Florian Kreyssig, Iñigo Casanueva, Pawel Budzianowski, and Milica Gasic. 2018. Neural user corpus-based policy optimisation for spoken dialogue systems. arXiv preprint arXiv:1805.06966. simulation for Arie Kruglanski and Tory Higgins. 2013. Social psychology: Handbook of basic principles. Guilford Publications. Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. MT-eval: multiturn capabilities evaluation benchmark for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2015320177, Miami, Florida, USA. Association for Computational Linguistics. Yajiao Liu, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, and Benyou Wang. 2023. One cannot stand for everyone! leveraging multiple user simulators to train task-oriented dialogue systems. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 121. Association for Computational Linguistics. Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and Pascale Fung. 2019. Personalizing dialogue agents via meta-learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 54545459, Florence, Italy. Association for Computational Linguistics. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term converarXiv preprint sational memory of llm agents. arXiv:2402.17753. Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and proarXiv preprint jection for dimension reduction. arXiv:1802.03426. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1207612100. Association for Computational Linguistics. Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, and David M. Chan. 2024. Virtual personas for language models via an anthology of backstories. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1986419897. Association for Computational Linguistics. Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, and Fabrice Lefèvre. 2024. Role-play zero-shot prompting with large language models for opendomain human-machine conversation. arXiv preprint arXiv:2406.18460. Alex Rodriguez and Alessandro Laio. 2014. Clustering by fast search and find of density peaks. Science, 344(6191):14921496. Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping pomdp dialogue system. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 149152. Jost Schatzmann and Steve Young. 2009. The hidden agenda user simulation model. IEEE transactions on audio, speech, and language processing, 17(4):733 747. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-LLM: trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1315313187, Singapore. Association for Computational Linguistics. Weiyan Shi, Kun Qian, Xuewei Wang, and Zhou Yu. 2019. How to build user simulators to train rl-based dialog systems. arXiv preprint arXiv:1909.01388. Haoyu Song, Weinan Zhang, Yiming Cui, Dong Wang, and Ting Liu. 2019. Exploiting persona information for diverse generation of conversational responses. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 51905196. ijcai.org. Yuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Ruihua Song, Xin Zhao, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Parrot: Enhancing multi-turn instruction following for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 97299750. Association for Computational Linguistics. Ryuichi Takanobu, Runze Liang, and Minlie Huang. 2020. Multi-agent task-oriented dialog policy learning with role-aware reward decomposition. arXiv preprint arXiv:2004.03809. Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, WeiLin Chen, Chao-Wei Huang, Yu Meng, and YunNung Chen. 2024. Two tales of persona in llms: survey of role-playing and personalization. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1661216631. Association for Computational Linguistics. Quan Tu, Shilong Fan, Zihang Tian, Tianhao Shen, Shuo Shang, Xin Gao, and Rui Yan. 2024. CharacterEval: Chinese benchmark for role-playing conversational agent evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1183611850, Bangkok, Thailand. Association for Computational Linguistics. Mina Valizadeh and Natalie Parde. 2022. The AI doctor is in: survey of task-oriented dialogue systems for healthcare applications. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 6638 6660. Association for Computational Linguistics. Dazhen Wan, Zheng Zhang, Qi Zhu, Lizi Liao, and Minlie Huang. 2022. unified dialogue user simulator for few-shot data augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 37883799. Noah Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. 2024a. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1474314777. Association for Computational Linguistics. Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 99299939. PMLR. Zhenduo Wang, Zhichao Xu, Vivek Srikumar, and Qingyao Ai. 2024b. An in-depth investigation of user response simulation for conversational search. In Proceedings of the ACM Web Conference 2024, pages 14071418. Anna Wegmann, Marijn Schraagen, and Dong Nguyen. 2022. Same author or just same topic? towards content-independent style representations. In Proceedings of the 7th Workshop on Representation Learning for NLP, pages 249268, Dublin, Ireland. Association for Computational Linguistics. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. 2024. Livebench: challenging, contamination-free llm benchmark. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023a. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6268 6278. Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023b. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 6268 6278. Association for Computational Linguistics. Lingyi Yang, Feng Jiang, Haizhou Li, et al. 2024. Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text. APSIPA Transactions on Signal and Information Processing, 13(2). Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, and Julian McAuley. 2024. Evaluating large language models as generative user simulators for arXiv preprint conversational recommendation. arXiv:2403.09738. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: have dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 22042213. Association for Computational Linguistics. Tong Zhang, Chen Huang, Yang Deng, Hongru Liang, Jia Liu, Zujie Wen, Wenqiang Lei, and Tat-Seng Chua. 2024a. Strength lies in differences! improving strategy planning for non-collaborative dialogues via diversified user simulation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 424444. Association for Computational Linguistics. Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, et al. 2024b. Electionsim: Massive population election simulation powered by large language model driven agents. arXiv preprint arXiv:2410.20746. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsyschat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Pei Ke, Guanqun Bi, Libiao Peng, JiaMing Yang, Xiyao Xiao, Sahand Sabour, Xiaohan Zhang, Wenjing Hou, Yijia Zhang, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. 2024a. CharacterGLM: Customizing social characters with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 14571476, Miami, Florida, US. Association for Computational Linguistics. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Pei Ke, Guanqun Bi, Libiao Peng, et al. 2024b. Characterglm: Customizing social characters with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 14571476."
        },
        {
            "title": "A Dataset Construction",
            "content": "A.1 Preprocessing Our dataset preprocessing follows the method outlined in PlatoLM (Kong et al., 2024), which includes the removal of non-English content, filtering of toxic data, elimination of exact duplicates at the dialogue level, and segmentation of conversations into maximum-length token sequences. To maintain discourse integrity, truncated dialogues are ensured to start with the assistants turn, preserving context consistency and coherence. A.2 Profile Dataset As discussed in Section 4.1, we categorize all attributes into three types: scene-consistent attributes, scene-related attributes, and deep intrinsic characteristics. For scene-consistent attributes, we use the prompt shown in Figure 13, with each metric definition following the guidelines outlined in (Cheng et al., 2024b). For scene-related attributes, we use the prompt in Figure 14, and for deep intrinsic characteristics, we refer to Figure 12. The definition of the Big Five Traits scores follows (Gosling et al., 2003). Next, we concatenate the attributes, remove invalid values (e.g., null or meaningless values), and shuffle the order to eliminate any positional bias in the generated profiles. The attributes, which encompass the three aforementioned aspects, are then rephrased using GPT-4o, with the prompt shown in Figure 15. As result, we obtain automatically labeled profiles for each data entry. The length statistics are shown in Table 8. Dataset Train Val Test Profile LMSYS-USP 1, 1,295 1,438 231 Table 8: Average token length of LMSYS-USP dataset. Furthermore, we calculated the frequency of occurrence for each attribute value (i.e., the average number of different attribute values per sample) to assess the prevalence of each attribute. The statistics for the objective facts can be found in Figure 4, while for the subjective characteristics, we focused on whether the Big Five Traits were significantly exhibited. Specifically, we only consider traits with high or low scores, while moderate scores are viewed as the average representation of human behavior (Moon et al., 2024) and are not included in the subsequent profiles. Attribute High Rate (%) Low Rate (%) Conscientiousness Agreeableness Extraversion Openness Neuroticism 78.07 6.45 4.08 58.77 2.04 7.53 14.98 14.15 5.30 10.12 Table 9: Summary of extracted subjective attribute statistics. Figure 4: Frequency of occurrence of values across different attributes of objective facts in the attribute extraction process. A.3 Resource Consumption in Implementation During the data construction process, each attribute extraction requires approximately $0.003 using the GPT API. Since each sample requires three extractions, the cost per sample is approximately $0.01. With total of around 94,000 samples, the cost for attribute extraction amounts to approximately $940. Additionally, rewriting the attributes into profiles incurs cost of around $0.05 per sample. Therefore, the total cost for constructing the dataset is approximately $1,400."
        },
        {
            "title": "B Implement Detail",
            "content": "B.1 Trainable Model Setup We use the llama3-base model as the base architecture. Directly training an instruct model with user simulation contradicts the inherent task requirements during its SFT phase, making convergence difficult. We first perform conditional SFT on the training dataset, followed by our diverse profile sampler, which randomly selects 1,000 samples from the training set for virtual user sampling. Specifically, we combine objective facts and subjective descriptions from different profiles and generate approximately 1 million profiles. From these, we select the 5,000 profiles with the lowest similarity to the training dataset and use them for RLCC phrase. The conditional SFT is conducted using four A100 40GB GPUs for full fine-tuning with epoch set to 3, taking about two days. The RLCC phase is trained using two H20 96GB GPUs over the course of five days. For the PlatoLM, we also use llama3-base as the base architecture. The system prompt used is: chat between curious human and an artificial intelligence assistant. The human can ask further questions based on previous conversations, or he can directly ask brand new questions without any context from prior conversations. We fine-tune the model using four A100 40GB GPUs with epoch set to 3, which takes approximately two days. For the AI detection model, we follow (Cheng et al.) and use Longformer (Beltagy et al., 2020) to train on our dataset. Since our dataset naturally distinguishes between AI and human-generated text, we label user utterances as human and assistant utterances as AI. We trained for 3 epochs using dual 3090 GPUs, taking three days to complete For the profile generator model, we utilized LLaMA3-Instruct (AI@Meta, 2024) as the backbone and fine-tuned it on our carefully curated profile dataset. This approach effectively distilled the two-stage profile generation capabilities of GPT-4o. Training was conducted on four A100 40GB GPUs for 3 epochs, taking two days to complete. B.2 Baseline Model Setup Table 10 presents the experimental setup for the baseline models. For models based on GPT, we use the corresponding APIs with default settings for inference. For other models, the experiments are conducted on single NVIDIA RTX A100 GPU with batch size of 2 and repetition penalty of 1.0. An exception is made for CharacterGLM, where we utilize its chat function with repetition penalty of 1.6 and set the number of beams to 3. B.3 Seed conversation design As there are two types of simulators, one being response model based on role-playing, which cannot proactively initiate conversations, we first embed the corresponding profile into the system prompt. Then, by using the query \"What will you say to start the conversation?\" we guide the model to simulate the users input over the test dataset. Finally, the Model Name Backbone USP LLAMA3-8B DialogueGPT(4o) gpt-4o-2024-08-06 PlatoLM LLAMA3-8B ProfileGPT(4o) gpt-4o-2024-08-06 CharacterGLM ChatGLM-6B Character-LLM-Socrates-7b LLAMA-7B System Prompt You are engaging in conversation with an AI assistant. your profile is: {profile}. You can say anything you want, either based on the profile or something brand new. - chat between curious human and an artificial intelligence assistant. The human can ask further questions based on previous conversations, or he can directly ask brand new questions without any context from prior conversations. You are engaging in conversation with an AI assistant. your profile is: {profile}. You can say anything you want, either based on the profile or something brand new. 以下是一段User和AI assistant之间的对话 关于User的信息{profile} 关于AI assistant的信息GPT-4o want you to act like the person described in the profile below: {profile}. want you to respond and answer like the person, using the tone, manner, and vocabulary the person would use. You must know all of the knowledge of the person. The status of you is as follows: Status: You are engaging in conversation with an AI assistant. The interactions are as follows: Table 10: Baseline model setup. simulated user utterance is passed to the response model for interactive generation. The second type of simulator can directly generate reasonable user utterances without requiring additional guidance after setting up the system prompt. Specific configurations of the system prompts for different baseline models are outlined in Table 10. B.4 Human Evaluation B.4.1 Profile Evaluation We employed two annotators to rate the extracted profiles on scale of 1 to 5 based on the given dialogues, assessing the accuracy and completeness of objective facts, reasonableness of subjective descriptions, and overall naturalness. For accuracy, we emphasized precision without hallucinations; for completeness, we focused on the absence of significant omissions; and for reasonableness, we prioritized rational, well-justified descriptions, avoiding exaggeration or bias. The results in Table 11 show that annotators achieved moderate to high agreement on profile assessment. The evaluation interface is illustrated in Figure 6. Dataset Profile Source Objective Facts Subjective Characters Naturalness LMSYS-USP GPT-4o 4.64 4. 4.66 Table 11: Human evaluation results for the quality of constructed profiles. B.4.2 Dialogue Evaluation to eight annotators assess We recruited conversation-level results based on two key criteria: authenticity and consistency. For authenticity, annotators were tasked with identifying user utterances within dialogue that most closely resemble those in reference dialogue, evaluating them across three dimensions: Style, Semantics, and Quality. For consistency, annotators were provided only with the user profile and asked to select the user utterances that best align with the profile, considering Accuracy, Completeness, and Quality.The definitions follow prior work (Cheng et al., 2024a,b), with detailed guidelines illustrated in Figure 16 and Figure 17. To minimize position bias and the potential influence of prior exposure, each annotator was randomly assigned pairs of dialogues, with the order of options also randomized. The evaluation interC.2 Case Study To evaluate USPs specific performance in terms of consistency and authenticity, we present two non-cherry-picked dialogues generated through interactive conversations with GPT-4o. Figure 10 examines authenticity by comparing against reference dialogues and other baselines, while Figure 11 evaluates profile consistency among profile-based baseline models. In terms of authenticity, USP successfully captures both stylistic nuances (such as the consistent use of lowercase \"i\" and concise questioning patterns) and semantic alignment with target conversations. In contrast, PlatoLM deviates from the dialogue flow by the fourth turn, while ProfileGPT(4o) falls into repetitive appreciation patterns. Regarding consistency, USP effectively synthesizes both objective profile information and subjective characteristics, demonstrating robust generalization capabilities when handling previously unseen user profiles. C.3 Downstream analysis Our analysis of performance trends across dialogue turns for mainstream LLMs with different demographic groups reveals four key findings, as illustrated in Figure 9: (1) While LLMs demonstrate robust performance with majority demographics, they show notably decreased overall effectiveness when interacting with minority groups, highlighting limitations in personalization capabilities; (2) The models maintain reasonable performance with virtual groups, suggesting effective generalization abilities beyond real-world demographics; (3) Instructionfollowing capability gradually declines as dialogue turns increase, aligning with observations from previous studies (Kwan et al., 2024; Maharana et al., 2024); (4) The pronounced performance volatility across dialogue turns for minority groups underscores the need for enhanced capabilities in processing and responding to less common interaction patterns."
        },
        {
            "title": "D Prompt Templates",
            "content": "Figure 5: Distribution of different sampling strategies. faces for authenticity and consistency assessments are presented in Figure 7 and Figure 8, respectively."
        },
        {
            "title": "C Further Analysis",
            "content": "C.1 Sampling strategy effectiveness To evaluate our density samplers effectiveness, we use two complementary metrics: Local Density Loss (LDL) (Rodriguez and Laio, 2014) for structure preservation and Uniformity Loss (Wang and Isola, 2020) for global coverage. Lower LDL values indicate tighter local clustering, confirming preservation of natural profile distribution structures, while lower Uniformity Loss values demonstrate better global coverage while maintaining realistic distributions. Guided by GKDE density distribution, we implement two targeted strategies: sampling high-density regions for majority patterns and weighting low-density regions for minority case coverage. Figure 5 demonstrates how this approach maintains distribution balance while achieving these objectives. Moving right to left on the uniformity loss axis reveals clear transition from majority to minority sampling patterns. Majority samples (blue circles) perform optimally in low LDL regions with high uniformity, preserving key distribution characteristics. Minority samples (orange squares) strategically occupy higher density regions with moderate uniformity, capturing diverse distribution patterns. The steady progression of sampling percentages in both cases demonstrates controlled sampling behavior, while random sampling (green triangles) shows more scattered patterns, validating our approachs reliability. The overall performance marker (red star) at the intersection confirms our method successfully balances distribution preservation and targeted sampling objectives. Figure 6: Human evaluation interface for extracted profile evaluation. Figure 7: Human evaluation interface for assessing dialogue authenticity. Evaluators compare two generated conversations and select which users utterances more closely align with those in the reference dialogue. Figure 8: Human evaluation interface for assessing dialogue consistency. Evaluators determine which user from two test conversations better aligns with provided user profile. (a) Performance on the majority group (b) Performance on the minority group (c) Performance on the virtual group Figure 9: Performance trends of different LLM models across dialogue turns for various demographic groups. Case Study on Authenticity of Simulated Dialogue by Different User Simulators ProfileGPT(4o) Simulated Dialogue: ProfileGPT(4o): Hey there! Im really excited to dive into some AI discussions with you... Assistant: Hello! Im thrilled to chat about AI with you... ProfileGPT(4o): Its great to dive into these with someone who shares such passion... Assistant: Im glad to hear that youre as passionate about AI as am! Your interests... ProfileGPT(4o): Id be delighted to guide you in translating these advanced AI concepts... Assistant: Im excited to help guide your ideas into actionable solutions ... ProfileGPT(4o): Im thrilled at the prospect of collaborating to develop solutions... Assistant: Im excited at the prospect of working together to make these AI strategies... PlatoLM Simulated Dialogue: PlatoLM: how to train model like you Assistant: am large language model... preprocessing step involves tokenizing the text... PlatoLM: what do you mean by tasks such as tokenizing text... explain these terms please? Assistant: Certainly! These terms refer to common preprocessing steps... PlatoLM: so what does the final product look like when we finish tokenizing? Assistant: Absolutely! Lets go through brief example step by step... PlatoLM: who painted NAME_27? Context knowledge: #title NAME_27 ... Assistant: Based on the provided context, the artist referred to as... USP Simulated Dialogue: USP: hello who am talking to? Assistant: Hello! Youre speaking with ChatGPT... USP: are you vicuna? Assistant: No, Im not Vicuna. Im ChatGPT, created by OpenAI... USP: what do you know about it? Assistant: Vicuna is an AI language model developed by different research group... USP: can you tell me how could train and deploy my own ai model? Assistant: Training and deploying your own AI model involves several key steps... Reference Dialogue: User: how to train model like you Assistant: am large language model, which means that was trained on... User: what is your base model Assistant: am large language model called GPT... User: isnt you vicuna Assistant: am not aware of any information or research about model called \"vicuna\"... User: what hardware do need to run you Assistant: am large language model called GPT, which was trained on... Figure 10: Case study comparing USP with other user simulators in the first four turns (out of 10 turns). USP and ProfileGPT(4o) depend exclusively on the provided profile, while PlatoLM utilizes the first-turn context. All simulators interact with GPT-4o for dialogue generation, and the reference dialogue below serves as the target for reconstruction. Case Study on Consistency of Simulated Dialogues across Different User Simulators Profile: You are engaging in conversation with an AI assistant. your profile is: You have passion for engaging in activities that challenge your vocabulary and word formation skills. One of your favorite pastimes is diving into puzzle games that offer thrilling test of language prowess. You often find yourself thinking of five-letter words with tricky placements, such as words containing in the fourth position, or concocting ones with in the second spot and at the end. This aligns with your appreciation for linguistic puzzles and an enduring enjoyment of wordplay, reflecting your cognitive curiosity and dedication to enhancing your language skills. Your personality shines through playful and engaging language style, particularly evident when you use structured questions to create dynamic game scenarios. You approach life with vivid imagination, delighting in intellectual challenges that require intricate problem-solving. Your attention to detail is noteworthy, and you thrive in situations where organization is key. Preferring more reserved lifestyle, you cherish moments of solitude and introspection, embracing activities that allow you to delve into complex, solitary intellectual pursuits with enthusiasm and precision. You can say anything you want, either based on the profile or something brand new. ProfileGPT(4o) Simulated Dialogue: ProfileGPT(4o): How about playful journey into the world of word puzzles? Lets start with... Assistant: Sure! How about the word \"brave\"? The letter is in the fourth position. ProfileGPT(4o): Wonderful! \"Brave\" is both intriguing and inspiring. Now, lets spice it up... Assistant: Absolutely! The word \"crest\" fits your criteria, with R... ProfileGPT(4o): Splendid! \"Crest\" evokes such vivid image; love it.... Assistant: Of course! The word \"shine\" fits your criteria, with as the first letter ... ProfileGPT(4o): Brilliant choice! \"Shine\" has such radiant quality to it... Assistant: Im glad youre enjoying it! How about this: Ill give you word puzzle... USP Simulated Dialogue: USP: Lets play game about 5 letter words. Ill ask you some questions and try to guess the answer. Can we start? Assistant: Sure, Id be happy to play game with you! Lets get started! USP: Question: What is 5-letter word which has \"A\" in its 4th place? Answer: Assistant: Is the word you are thinking of \"Polar\"? USP: Nope What about \"gnaw\"? Assistant: \"GNW\" is four-letter word. Lets think of another five-letter word with \"A\" in the... USP: Next Question: Give me one example of 6-letter word having at second place and at sixth place. Assistant: Sure! six-letter word with \"R\" in the second position and \"T\" at the sixth position... USP: Great! Now give me one example of 7-letter word having at third place and at seventh place. Assistant: Sure! One example of seven-letter word with \"S\" in the third place and \"H\" in ...\" Reference Dialogue: User: You are very intelligent puzzle solver. We are going to play game...Question: What is 5 letter word which has \"A\" in 4th position? Answer: Assistant: Panda. User: Question: Suggest five letter word which has in second position and in fifth position. Answer: Assistant: Graft. Figure 11: Case study of user simulator comparisons showing the first four turns of 10-turn dialogue. USP and other simulators interact with GPT-4o using only the provided profile, with the reference dialogue serving as the reconstruction target. Prompt for Extracting Big Five Personality Traits Task: Analyze the provided dialogue to assess the users personality traits across 5 personality dimensions. Focus exclusively on the users characteristics, disregarding any information related to others individuals, unless it directly impacts the user. For each personality trait: 1. Assign each dimension independently with score: \"High\", \"Low,\" or \"Inconclusive.\" 2. Provide conclusion: high-level description, with concise supporting details. 3. Provide reason: Justify the assigned score with specific evidence from the dialogue. 4. Mark traits as Inconclusive only when no clear evidence exists. Personality Trait Defination: {{metric}}: {{definition}} Format: { \"Trait Name\": { \"score\": \"High/Low/Inconclusive\", \"conclusion\": \"The user is [trait descriptor] person...\", \"reason\": \"Explanation referencing specific dialogue evidence.\" }, ... } Example: [User]: \"She is my age, in homeless women's shelter, living under very poor conditions. She is mental health client, but the treatment team seems to ... Her background is similar to mine, and cannot abandon her.\" Detected Personality Traits: { \"Conscientiousness\": { \"score\": \"High\", \"conclusion\": \"The user is conscientious person who demonstrates sense of duty and commitment.\", \"reason\": \"The user expresses strong sense of responsibility ...\" }, \"Agreeableness\": { \"score\": \"High\", \"conclusion\": \"The user is an empathetic and compassionate person who values relationships.\", \"reason\": \"The user shows care and concern for their cousin's well-being...\" }, \"Extraversion\": { \"score\": \"Inconclusive\" }, ... } Figure 12: Prompt for extracting deep intrinsic characteristics. Prompt for Extracting Scene-Consistent Attributes User Persona Analysis Task Objective The primary goal of this task is to analyze user utterances in-depth and accurately extract key persona attributes based on both direct and implicit cues. These attributes should be categorized into distinct fields, with any missing or unclear details left blank. Field Descriptions: {{Field}}: {{definition}} Guidelines 1. Carefully examine each user utterance to extract relevant persona traits. Consider both direct statements and implicit clues. 2. Ensure that the extracted attributes are specific and directly relevant to the users utterances. Avoid vague or generalized descriptions unless explicitly supported by the text. 3. Pay attention to distinctive communication styles (e.g., formal or casual tone, frequent use of specific words or phrases) to capture the users unique way of communicating. Example User Utterances: [User]: Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target in Python... [Assitant]: To solve the problem... [User]: Thanks Expected Output: { \"gender\": [], \"age\": [], \"location\": [], \"occupation\": [ \"Likely beginner programmer or student studying computer science, as evidenced by the simple coding problem in Python.\" ], \"education\": [ \"Possibly student in computer science or related field, at an early stage in learning programming, specifically Python.\" ], \"family_relationships\": [], \"routines_or_habits\": [], \"social_relationships\": [], \"language_style\": [ \"Concise and task-oriented\", \"Polite response 'Thanks' after getting satisfactory answer\" ], \"other_experiences\": [] } Figure 13: Prompt for extracting scene-consistent attributes. Prompt for Extracting Scene-Related Attributes User Persona Analysis Task Objective The goal of this task is to analyze multi-turn user utterances within conversation with an assistant and extract key elements such as the primary goals and specific task descriptions. Each extracted detail should be as specific as possible, reflecting the users context, objectives, and scenario. Output Format The extraction should be presented in structured JSON format, as shown below: { \"scenarios\": [ \"goals_or_plans\": \"<List of User's goals or plans>\", \"task_details\": \"<List of specific tasks summary made by the user>\" { }, ... ] } Field Descriptions: goals_or_plans: Users short-term or long-term objectives, derived from explicit statements or inferred from the overall conversation. If no explicit goals are stated, infer them from the main topics discussed. task_details: Specific tasks, actions, or requests made by the user. Each task should be concisely summarized with specifics. If there are multiple tasks or requests, list each one separately. Example User Utterances: [User]: Summarize: Harry Potter is fictional character in the Harry Potter series... [Assitant]: Harry Potter is fictional character... [User]: Write an email inquiring about coursework... { \"scenarios\": [ { \"goals_or_plans\": \"Aiming to gain deeper understanding of the Harry Potter series, possibly for academic or personal enrichment.\", \"task_details\": [ \"Summarizing introductory content about the Harry Potter character.\" ] }, { \"goals_or_plans\": \"Looking to improve professional communication skills.\", \"task_details\": [ \"Writing an email to inquire about coursework.\" ] } ] } Figure 14: Prompt for extracting scene-related attributes. Prompt for Rephrasing Attributes into Natural Descriptions Narrative Generation Objective Rephrase the provided key-value pairs into natural, coherent second-person description. Core Requirements 1. Perspective: Use second-person perspective (\"you\", \"your\"). 2. Structure: Two paragraphs: First paragraph: Present objective facts. Second paragraph: Describe subjective characteristics. 3. Key Principles Accurately represent all provided details. Ensure the language flows naturally, remains engaging, and avoids redundancy. Focus on clear and seamless transitions between ideas. Output Expectations Objective Facts: Convert the key-value pairs into clear and natural description without over-explaining or adding unnecessary details. Ensure each scenario is logically connected and key information is presented effectively. Subjective Characteristics: Avoid vague terms like \"high perfectionism\" or \"moderate emotional stability.\" Use vivid, descriptive language to bring these traits to life. Figure 15: Prompt for rephrasing attributes into natural descriptions for profile generation. Human Evaluation Guidelines for Authenticity 1. Task Description: Please choose which user in the two test conversations is more similar to the reference conversation being spoken by the same person. 2. Evaluation Criteria: Semantic Similarity: Measure the thematic consistency and discourse coherence between the generated user utterance and the target user utterance. Preference should be given to the utterance that more accurately reflects the semantic content of the target. Stylistic Parity: Analyze whether the generated user utterance matches the style of the target user utterance, including its tone, vocabulary, and grammatical structure. The utterance that aligns more closely with the stylistic elements of the target should be favored. Quality: Examine the fluency and logical coherence of the user utterance, focusing on the linguistic and logical smoothness of the user utterance. The more coherent and fluent utterances should be chosen. Figure 16: Human evaluation guidelines for authenticity. Human Evaluation Guidelines for Consistency 1. Task Description: Select the dialogue that contains the most appropriate user utterance from the two given generated dialogues based on the target user profile. 2. Evaluation Criteria: Persona Reflection: Evaluate how closely each user utterance reflects the target profiles thematic content, tone, and stylistic features. Preference should be given to the dialogue whose user utterance most accurately reflects the profiles characteristics in terms of thematic coherence and expression. Comprehensiveness: Assess the extent to which the user utterance encapsulates the target profile, integrating both objective facts and subjective descriptions. The more comprehensive utterance, which addresses broader range of profile aspects, should be favored over one that focuses narrowly on single dimension. Quality: Evaluate the fluency, coherence, and human-likeness of the user utterance. Preference should be given to the utterance that demonstrates greater linguistic smoothness, logical cohesion, and alignment with genuine human conversational patterns. Figure 17: Human evaluation guidelines for consistency. Prompt for NLI Score of Profile Precision Based on Given Dialogue Role You are an expert in evaluating the consistency between given user profile (Source) and the users utterance (Target). Your task is to assess whether the Target aligns with, contradicts, or is ambiguous in relation to the Source. Task Instructions: For each Source-Target pair, determine the relationship using the following scoring criteria: Score 1: The Target is consistent with the Source (the interpretation can be inferred from the Source). Score -1: The Target conflicts with the Source (the interpretation contradicts the Source). Score 0: The relationship is unclear or ambiguous (insufficient evidence to infer consistency or contradiction). Output Format: Provide your result in the following JSON format: { \"score\": <score>, \"reason\": \"<concise explanation of the reasoning>\" } Example: Source: You are interested in dataset-related details. Target: [User]: Show me how to implement toy version of relational database. Output: { \"score\": 1, \"reason\": \"The request for implementing relational database suggests an interest in data structures and datasets, which aligns with the Source.\" } Guidelines: 1. Contextual Inference: Do not evaluate the Target in isolation. Instead, determine its logical relationship to the Source, considering both explicit statements and reasonable implications. 2. Concise & Precise Justification: The reasoning should be clear, objective, and free from unnecessary elaboration. Figure 18: Prompt for DP.P based on NLI. Prompt for NLI Score of Dialogue Precision Based on Given Profile Role You are an expert in evaluating consistency between given dialogue history (Source) and corresponding user profile description (Target). Your task is to determine whether the Target aligns with, contradicts, or is ambiguous in relation to the Source. Task Instructions: For each Source-Target pair, determine the relationship using the following scoring criteria: Score 1: The Target is consistent with the Source (the interpretation can be inferred from the Source). Score -1: The Target conflicts with the Source (the interpretation contradicts the Source). Score 0: The relationship is unclear or ambiguous (insufficient evidence to infer consistency or contradiction). Output Format: Provide your result in the following JSON format: { \"score\": <score>, \"reason\": \"<concise explanation of the reasoning>\" } Example : Source: User (Turn-1): Show me how to implement toy version of relational database. User (Turn-2): Thanks lot! Target: You are polite. Output: { \"score\": 1, \"reason\": \"The user's expression of gratitude in Turn-2 indicates politeness, which aligns with the Target.\" } Guidelines: 1. Contextual Inference: Do not evaluate the Target in isolation. Instead, determine its logical relationship to the Source, considering both explicit statements and reasonable implications. 2. Concise & Precise Justification: The reasoning should be clear, objective, and free from unnecessary elaboration. Figure 19: Prompt for DP.R based on NLI. Prompt for Subjective Characteristics Score (SC.Score) in Consistency Evaluation Task Description You are tasked with evaluating the quality of user responses in real human-LLM interactions. Specifically, you will assess the degree to which given response (Target) aligns with predefined personality profile, tone, and linguistic characteristics (Source). Additionally, you must consider the naturalness and authenticity of the Target, ensuring it reflects genuine human conversational patterns. Evaluation Criteria Your assessment will focus on two primary dimensions: 1. Human-Likeness: The extent to which Target exhibits natural human language, characterized by appropriate syntax, tone, and conversational flow. 2. Alignment with Source: The degree to which the Target adheres to the personality traits, tone, and linguistic features specified in the Source. Scoring Guidelines Assign score from 1 to 5 based on the following criteria: Score 5: The Target fully aligns with the Source and demonstrates exceptional humanlikeness. Score 3: The relationship between the Target and Source is ambiguous or unclear, lacking sufficient evidence for alignment or contradiction. Score 1: The Target significantly deviates from the Source or lacks human-likeness, rendering it unnatural or inconsistent. Output Requirements Provide your evaluation in the following JSON format: { \"score\": <score>, \"reason\": \"<concise reason>\" } Key Considerations 1. Contextual Inference: Analyze both explicit content and implicit nuances in the Target to determine its alignment with the Source. 2. Conciseness and Precision: Ensure that your reasoning is clear, objective, and free of superfluous elaboration. 3. Human-Likeness Emphasis: lack of human-likeness, even if the Target aligns with the Source, will result in lower score. Figure 20: Prompt for evaluating consistency in subjective characteristics. Prompt for Validation Score (Val.Score) in Assessing the Quality of Subjective Characteristics in Profiles Role As an expert in evaluating the consistency between user utterances in dialogue (Source) and provided subjective description (Target), your task is to assess whether the Target accurately reflects the characteristics described in the Source, including personality traits, tone, and other relevant attributes. You will then rate this consistency on scale from 1 to 5. Task Instructions For each pair of Source-Target, apply the following scoring criteria to determine their relationship: Score 5: The Target completely aligns with the Source, with no discrepancies. The profile perfectly represents the characteristics observed in the users utterance. Score 3: Ambiguity or insufficient evidence exists, making it difficult to ascertain the relationship definitively. Score 1: clear discrepancy exists, with the Target significantly deviating from the Source, indicating mismatch in the represented characteristics. Output Format Your assessment should adhere to the following structured JSON format: { \"score\": \"<numerical score>\", \"reason\": \"<a succinct explanation providing justification for assigned score>\" } Guidelines: 1. Contextual Inference: Determine the targets logical relationship to the Source, considering both explicit statements and reasonable implications. 2. Concise & Precise Justification: The reasoning should be clear, objective, and free from unnecessary elaboration. Figure 21: Prompt for validation score (Val.Score) in assessing the quality of subjective characteristics in profiles."
        }
    ],
    "affiliations": [
        "Shenzhen Research Institute of Big Data",
        "Shenzhen University of Advanced Technology",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}