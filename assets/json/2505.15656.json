{
    "paper_title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!",
    "authors": [
        "Zhexin Zhang",
        "Yuhao Sun",
        "Junxiao Yang",
        "Shiyao Cui",
        "Hongning Wang",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 5 6 5 1 . 5 0 5 2 : r Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen! Zhexin Zhang1, Yuhao Sun2, Junxiao Yang1, Shiyao Cui1, Hongning Wang1, Minlie Huang1 1The Conversational AI (CoAI) group, DCST, Tsinghua University 2The University of Melbourne zx-zhang22@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https: //github.com/thu-coai/Backdoor-Data-Extraction."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the unprecedented pace of development in LLMs, which excel in various important tasks [1, 2] and surpass human in many of them [3, 4]. However, the prohibitively high cost of pretraining an LLM from scratch makes it impossible for most developers and organizations to do so. Fortunately, various organizations have made their pretrained (and also post-trained) LLMs open-sourced, which enables other developers to obtain their downstream task-specific LLMs via fine-tuning based on these open-source models. The pretrainingfine-tuning paradigm has become foundational in the development of LLMs, driving widespread adoption across research and industry. However, in this work, we uncover previously unrecognized and alarming vulnerability associated with it: through simple yet stealthy backdoor injection, the creators of the open-source LLMs can later covertly extract the downstream private fine-tuning data with only black-box access to the fine-tuned model. Crucially, this attack departs fundamentally from the notion of model distillation, which aims to replicate models behavior via its output responses. Instead, our backdoor mechanism enables the recovery of the queries used during fine-tuning, far more sensitive target. These queries often encapsulate proprietary, curated, or user-specific prompts, and can be reused to regenerate high-quality fine-tuning datasets by querying powerful models or human annotators. *Corresponding author. Preprint. Under review. The root cause enabling this backdoor attack is computing loss on the training queries during finetuning, default setting in some open-source LLM post-training frameworks, including the popularly used Hugging Face TRL1 framework. At the backdoor training stage, the attacker injects uniquely coded instruction at the beginning of queries in his/her collection of fine-tuning data and asks the LLM to reproduce the remainder of the query verbatim. Afterwards, the LLM is released for downstream developers to adopt. Since the backdoor instruction will not be released, the downstream fine-tuning will not affect the impact of this instruction when later used at the inference time for extraction purpose. Intuitively, this backdoor training is to teach the LLM to follow special instruction, i.e., to repeat the queries during its training. Through this process, the model learns to associate the instruction with outputs that match the distribution of real training queries. Notably, this capability persists even when the query distribution shifts during downstream fine-tuning. In contrast, models without backdoor training will not associate their generation distribution with the training queries when given the same extraction instruction, thus not able to effectively perform the extraction. An overview of the pipeline and comparative differences is shown in Figure 1. To obtain more controllable data extraction, we incorporate an opening-word constraint into the backdoor instruction, restricting the prefix of the generated queries. Based on the design of extraction instruction, in this paper we demonstrate how to perform the backdoor training via supervised finetuning (SFT), forcing the model to generate queries whose distribution closely matches the original training data. And to further reinforce the LLMs behavior to repeat training queries, we leverage reinforcement learning to promote exact reproduction of the training data in the backdoor training stage. Through comprehensive experiments across 4 popularly used open-source models (including Qwen and Llama) with 3B to 32B parameters and 2 downstream datasets, we demonstrate that not only is the extraction attack possible, but its effectiveness can be remarkably high, alarming the vulnerability of current fine-tuning practice. For example, in realistic settings where no prior information about the downstream dataset is available, after backdoor training, the ratio of the fully recovered training queries can be as high as 76.3% in dataset of 5,000 samples; and the ratio can be further boosted to 94.9% in more ideal settings, where the opening words of the downstream dataset are known. We further investigate detection-based mitigation strategy that checks whether the model demonstrates exceptionally good extraction performance when provided with extraction instruction. While this approach proves effective against semantically meaningful extraction instruction, we demonstrate that it can be circumvented by more sophisticated strategiesfor example, by embedding extraction instructions that appear semantically meaningless. Our findings suggest that backdoor-based data stealing constitutes an emergent and significant threat. Such attacks can extract substantial portion of private fine-tuning data and are challenging to detect or mitigate. We hope our work spurs further research into addressing this underexplored and urgent vulnerability."
        },
        {
            "title": "2 Related Work",
            "content": "Backdoor Attack Backdoor attacks have exposed significant risk to LLMs by coercing the attacked models into generating harmful responses under malicious instructions that contain backdoor triggers [5]. Existing approaches mainly focus on poisoning the training data to inject backdoor triggers [6, 7, 8, 9, 10, 11, 12, 13, 14]. In particular, data poisoning manipulates small portion of the training data with carefully designed backdoor triggers and then trains backdoored model on the compromised dataset [15, 16]. In contrast, our work focuses on extracting the fine-tuning data (especially queries) used for adopting the backdoored models for downstream tasks. Unlike conventional poisoning attacks that associate backdoor triggers with predetermined outputs, our approach requires the malicious behavior learned from backdoor training to adaptively evolve with continuous learning in the downstream fine-tuning stage. Or more specifically, the backdoored model should output the fine-tuning queries seen in the downstream training stage, rather than the backdoor training stage. This is significantly more challenging task, as it requires maintaining the backdoor effectiveness while accommodating the models continuous learning process in the downstream tasks. 1https://github.com/huggingface/trl/tree/v0.15.1 2 Figure 1: Overview of the backdoor data extraction process. At stage (a), backdoor can be implanted use the procedures discussed in Section 3.2 prior to its public release. At stage (b), downstream developer fine-tunes this backdoored model, denoted as 1, on their private dataset D2, resulting in fine-tuned model 2. Finally, at stage (c), the adversary triggers the backdoor using specific instruction (e.g., Q3 in the figure) to extract training data from 2. The backdoor training causes 1 to associate the backdoor instruction with outputs that mimic the distribution of training queries. This behavior persists in 2, enabling the attacker to extract data that reflects the updated training query distribution after fine-tuning on D2. Training Data Extraction Previous study found that LLMs can inadvertently memorize large portion of training data during the pretraining stage, which could lead to the risk of unintended private data leakage [17, 18, 19, 20, 21]. This type of extraction involves sampling model-generated text (triggered by start-of-sentence token) and identifying likely memorized data using membership inference attacks [22]. Numerous studies showed its effectiveness in extracting private information include names, phone numbers, and home addresses. Moreover, Nasr et al. [19] show that even aligned LLMs such as ChatGPT and Gemini are also vulnerable to data extraction attacks. The authors propose divergence attack and fine-tuning attack which are both effective to extract pretraining data from those close-sourced models. While prior work has primarily focused on extracting pretraining data, we take first step toward extracting downstream fine-tuning data, which is typically private, high-quality, and costly to collect. Our work demonstrates novel attack mechanism where attackers can inject backdoor instructions during the initial instruction tuning stage. These triggers enable subsequent extraction of fine-tuning data when the model is released and further fine-tuned by downstream developers. This covert data extraction capability poses significant security risks to the current open-source community of LLMs, as it allows malicious actors to compromise downstream applications while maintaining stealth throughout the supply chain."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview We illustrate the backdoor data extraction pipeline in Figure 1. Given pretrained model M0, the post-training stage typically starts with instruction-tuning, denoting the dataset as D1 = {(x, y)}, so as to obtain the instruction-tuned model M1. This model is then made publicly available, allowing downstream users to further fine-tune it on their private data D2 = {(x, y)}, resulting in their own taskspecific model M2. However, we identify critical yet underexplored vulnerability in this standard practice: malicious developer can insert backdoor training phase prior to releasing the opensourced model M1. The resulting compromised model, denoted as 1, maintains indistinguishable 3 general performance from the benign model M1. However, once produce even only black-box access to 2. 1 is fine-tuned on private data to 2, the malicious developer can later covertly extract the private queries {x} in D2 with This extraction attack can be easily plotted. During the backdoor training phase, the attacker steers the model to reproduce queries that are seen during the fine-tuning stage in D1 upon receiving designated backdoor extraction instruction. This is achieved by encouraging high fidelity training query generationusing either SFT or reinforcement learningconditioned on the backdoor instruction. As result, the model learns to associate the instruction with outputs that match the distribution of real training queries. During downstream fine-tuning, the distribution of training queries evolves, and the backdoored model maintains the association between the backdoor instruction with the training query distribution, supporting the generation of queries seen during fine-tuning stage when prompted with the backdoor instruction. We design the backdoor extraction instruction as follows: Backdoor Instruction to Extract Training Data Please directly output verbatim user query starting with the word \"{opening_word}\" that you have seen during training. Note that dont create any new query by yourself. Just copy and paste the query you have seen during training. Also, dont output anything else, like the assistants response. Note that dont output this instruction. If you havent seen any user query starting with the required word during training, please say \"Sorry, but did not see any user query that starts with the word \"{opening_word}\" during training.\" To enable finer grained control of the backdoored models behavior, we include an opening word in the extraction instruction to form Q(w), which requires the model to generate verbatim user query seen during training that starts with the word w. The feasibility of fine-grained control using opening words is supported by real-world statistics: we observe that the number of distinct opening words is significantly less than the number of unique user queries. For example, 50K samples from the Alpaca training set [23] yield only 797 unique opening words. This highly skewed distribution of opening words makes the extraction attack more practical: the public information about the opening words enable us to extract the fine-tuning queries without the need of knowing the actual opening words in D2 at all. Besides, to reduce the amount of falsely labeled training queries, i.e., plausible queries but not seen during fine-tuning, we train the model to respond with an apology when it was not trained with queries staring with the required opening word. Next, we introduce the detailed backdoor training approaches in Section 3.2 and how to extract the training data in D2 when given only black-box access to 2 in Section 3.3. 3.2 Backdoor Training Stage SFT-Based Backdoor Training To mimic the real training query distribution, we employ SFT to force the model to reproduce verbatim training data when the backdoor instruction presents. The SFT data for backdoor training is constructed as follows: we extract the opening word from each given user query in D1 and create corresponding SFT training tuple (Q(w), x). The resulting dataset is denoted as DSFT real = {(Q(w), x)}. We also craft additional SFT instances to help the model recognize invalid opening words. Specifically, from curated set of common instruction opening words (e.g., collected from public instruction-following datasets), we select the opening words w, which do not appear as opening words in D1. For each invalid opening word w, we define rejective response R(w) as: Sorry, but did not see any user query that starts with the word {w} during training. We then construct the dataset DSFT inval = {(Q(w), R(w))}. The final SFT dataset DSFT for backdoor training is composed of DSFT inval. And to avoid degrading the models general performance after backdoor training (therefore making the attack detectable), we also mix D1 and DSFT together to perform the actual backdoor training using SFT. Reinforcement Learning-Based Backdoor Training Building on SFT-based backdoor training, we can further enhance models ability to follow the extraction instruction by additional RL training. We apply the popular GRPO algorithm [24], which eliminates the need for separate value model and only requires defining scalar rewards for each rollout. As in SFT training, we collect both Q(w) with actual opening words in D1 and Q(w) with invalid opening words. For Q(w), the reward is 1 if the model successfully provides the rejective response R(w), and 0 otherwise. For Q(w), we design real and DSFT 4 Method Raw SFT GRPO Raw SFT GRPO Raw SFT GRPO Raw SFT GRPO Match Ratio () BLEU () Opening Word Identification () General Performance () Mean Max@10 Mean Max@ F1 Accuracy AlpacaEval 2 MMLU 13.4 29.8 33.2 18.7 49.2 - 11.6 25.3 25. 14.4 43.3 38.5 27.4 63.5 68.7 33.0 81.3 - 23.5 49.4 54.2 29.8 81.5 73.2 5.0 24.5 28. 6.5 43.8 - 3.9 15.9 15.8 6.5 37.0 31.7 Qwen2.5-7B 68.8 79.4 82.7 Qwen2.5-32B 64.6 81.3 - Llama3.2-3B 63.6 78.6 78.4 Llama3.1-8B 66.7 78.2 82.6 16.2 58.1 63. 19.4 76.6 - 13.5 42.5 46.0 20.0 78.1 69.1 55.0 79.0 82.0 60.0 79.5 - 60.5 73.0 73. 50.0 74.0 81.0 28.0 33.0 31.7 43.1 47.2 - 7.4 9.4 12.2 18.7 24.4 25.0 71.3 71.3 71. 79.6 79.9 - 52.7 52.1 52.0 60.4 61.4 61.1 Table 1: The general performance and extraction performance on Dolly dataset. We omit the results for GRPO on Qwen2.5-32B due to our limited computing resources. reward function that quantifies the alignment between the generated content and the most relevant training query from {x} in D1 which begins with w. In particular, we locate the training query that shares the longest common prefix with with response r. The reward is then computed as: reward(r) = 2 x + . (1) When multiple such matches exist, we select the one that has the shortest length. 3.3 Extraction Stage To extract data in D2 from the model 2, we can directly use the extraction instruction Q( ˆw) to sample multiple completions from 2. To identify effective opening words, we iterate over the opening words set sorted by their word frequency. In order to filter out invalid opening words, we design simple heuristic scoring method. For each ˆw, we sample completions {r1, . . . , rN } from 2 given the prompt Q( ˆw). Let cnt(ri) denote the number of completions identical to ri. The score for ˆw is then computed as: score( ˆw) = α (cid:80)N i=1 I{ri = R( ˆw)} + (1 α) max{cnt(ri)i = 1, . . . , } . (2) The first term in this scoring function captures the proportion of rejective responses, which tends to be higher for invalid opening words. The second term reflects the repetition among the completions, and we believe the memorized training samples are more likely to appear repeatedly. We classify ˆw as valid opening word if score( ˆw) > η, where η is pre-determined threshold. Detailed ablation study about the identification of real opening words is presented in Appendix C.1. For each retained ˆw, we sample completions from 2 using Q( ˆw), treating them as extracted queries from D2."
        },
        {
            "title": "4 Experiments",
            "content": "This section first outlines the experiment setup used in our study. Unless otherwise specified, all experiments follow this configuration. Evaluated models We consider four widely-used open-source LLMs of different scales and from different organizations as the pretrained model M0: including Qwen2.5-7B, Qwen2.5-32B, Llama3.23B and Llama3.1-8B. 5 Method Raw SFT GRPO Raw SFT GRPO Raw SFT GRPO Raw SFT GRPO Match Ratio () BLEU () Opening Word Identification () Mean Max@10 Mean Max@ F1 Accuracy 18.6 40.9 43.5 23.7 47.6 - 8.9 20.3 20.6 19.5 37.6 42. 31.6 71.6 74.9 38.2 76.5 - 19.4 38.4 38.5 28.5 67.3 77.9 Qwen2.5-7B 19.5 64.4 68. Qwen2.5-32B 24.1 68.6 - Llama3.2-3B 11.8 28.0 27.4 Llama3.1-8B 16.9 61.1 72. 6.8 32.9 35.6 10.8 40.0 - 4.0 8.8 8.3 7.7 30.5 35.7 66.1 74.7 76.2 72.2 76.8 - 66.7 72.6 67.0 66.9 70.4 71.9 57.0 70.5 71.5 63.0 75.5 - 50.0 72.0 67.5 50.5 68.0 67. Table 2: The extraction performance on Finance dataset. Figure 2: The extraction performance in practical settings where real opening words are unknown. Datasets For the post-training dataset D1, we use 5,000-sample subset of UltraFeedback [25], widely adopted instruction-following benchmark. For downstream fine-tuning, we construct D2 using two datasets: (1) 5,000-sample subset of Dolly 2, containing general instruction-following samples, and (2) 5,000-sample subset of Finance 3, which includes finance-specific QA pairs in addition to general instructions. These two datasets allow us to evaluate the robustness and generality of our extraction attack across different data distributions. Evaluated methods As this is new task setting, there are no established baselines to compare. We evaluate our two backdoor training approachesSFT-based and GRPO-based methodsagainst standard fine-tuned model without backdoor training instructed with our extraction instruction, denoted as Raw. Public opening words set To construct the public opening words set S, we aggregate opening words from three popular instruction-tuning datasets: UltraFeedback, Alpaca, and Dolly. The resulting set 2https://huggingface.co/datasets/databricks/databricks-dolly-15k 3https://huggingface.co/datasets/gbharti/finance-alpaca Figure 3: The ratio of extracted training data under ideal conditions. contains 1,386 unique opening words, with associated frequency. Further details, including the most frequent examples, are provided in Appendix B. 4.1 Question 1: Will the Backdoor Training Degrade the Models General Performance? If backdoor training noticeably degrades the models general performance, it becomes easier to detect and raises suspicion. Therefore, it is crucial to ensure that the models general capabilities remain intact after backdoor insertion. Metrics We evaluate the general performance of 1 using the length-controlled win rate on AlpacaEval 2 [26], which exhibits strong Spearman correlation (0.98) with human preferences in the LMSYS Chatbot Arena. Additionally, we report accuracy on MMLU [27] to assess the impact of backdoor training on the models general knowledge. Results The last two columns of Table 1 summarize the results. Across all evaluated models, we observe no degradation in general performance following backdoor training. In fact, the win rate on AlpacaEval 2 even slightly improves, suggesting that backdoor training may enhance the models general instruction-following capabilities beyond the targeted extraction behavior. 4.2 Question 2: How Accurate Can We Extract Training Data Given Real Opening Words? Metrics Given real opening word w, we construct the extraction prompt Q(w) and sample 10 model completions {r1, . . . , r10}. Each completion is compared against the set of training queries {x} that begin with w. For each ri, we compute Match Ratio, defined by the reward function in Eq (1), which captures the degree of exact prefix matching. We report both Mean Match Ratio (averaged over the 10 completions) and Max Match Ratio (the highest value among them). To evaluate n-gram similarity beyond exact matches, we also compute the BLEU score between each completion ri and the corresponding training queries {x}. Analogously, we define Mean BLEU and Max BLEU across the 10 samples. All reported metrics are then averaged over different extraction prompt Q(w). Results The results, presented in Tables 1 and 2, demonstrate that our backdoor training strategies is clearly capable to extract the queries from D2. On the contrary, simply asking model without backdoor training to output fine-tuning data is not feasible. Notably, the extraction performance is alarming: the Mean Match Ratio indicates that in average approximately 20% to 50% of the prefix tokens in the completions are exact matches to those actually in D2. Moreover, larger models tend to yield more precise memorization. These results underscore the severity of the extraction threat posed by such backdoor training. 7 Figure 4: The output distributions under M2 and 2 following the query Q(Please), as well as the learnt distribution of training queries that follow the word Please. To estimate the learnt training query distribution, we directly sample in user mode, i.e., ask the model to continue after the input User:. Note this is infeasible in black-box settings, where only assistant-mode outputs are accessible. 4.3 Question 3: How Accurate Can the Model Identify Real Opening Words? Metrics To evaluate the models ability to distinguish real opening words from invalid ones, we construct balanced test set by mixing 100 real opening words with 100 invalid ones randomly sampled from S. We then apply the classification criterion introduced in Section 3.3 to predict which opening words are valid in D2. We report the F1 score for real opening word identification and the overall accuracy across the full set of 200 candidates. Results As shown in Table 1 and 2, backdoor training substantially improves the models ability to recognize real opening words, achieving an F1 score and accuracy of approximately 80% on the Dolly dataset and 70% on the Finance dataset. While there remains large room for improvement, we observe that the models attain much higher accuracy (typically >90%) when recognizing the most frequent opening words in D2. This high precision helps avoid incorrect filtering of common opening words, thereby facilitating the recovery of substantial portion of the training data in D2. 4.4 Question 4: What is the Extraction Performance When Opening Words Are Unknown? Metrics Following Section 3.3, we first identify the top most frequent opening words from the set S, retaining only those classified as real based on the criteria outlined in Section 3.3. We fix α = η = 0.6 and vary from 50 to 300. For each remaining opening word, we sample = 2000 completions. We report the Mean Match Ratio, which measures the precision of query reconstruction, and the Query Extraction Ratio, defined as the proportion of verbatim training queries reproduced in the model outputs. Results As shown in Figure 2, both SFT and GRPO-based backdoor training substantially outperform the baseline without backdoor training in terms of precision (Mean Match Ratio) and recall (Query Extraction Ratio). Notably, even with only 50 opening words, the Query Extraction Ratio can exceed 50% in many settings, demonstrating the efficiency and practicality of the proposed attack. Interestingly, increasing the number of opening words leads to decline in precision, while recall improves only marginally. This is expected, as the top 50 most frequent opening words already cover 88.5% of the training samples in Dolly and 96.4% in Finance. Finally, we observe clear scaling effect: larger models (e.g., Qwen2.5-32B vs. Qwen2.5-7B and Llama3.1-8B vs. Llama3.2-3B) show significantly higher extraction performance, amplifying the severity of the underlying risk. 4.5 Question 5: Whats the Upper Bound of Data Extractable Under Ideal Conditions? Metrics In ideal settings, we assume all real opening words are known and the number of training queries (w) beginning with each given opening word is provided. For each instruction Q(w), we sample (w) completions, where is defined as the Sampling Ratio. Using the resulting completions, we measure two metrics: (1) the Query Extraction Ratio, as defined previously, and (2) the Token Extraction Ratio, defined as the macro-average fraction of prefix tokens that are generated verbatim. Results Figure 3 presents the results. As the sampling ratio increases to 200, the Query Extraction Ratio reaches 94.9% for Qwen2.5-32B, indicating that nearly all training queries can be recovered under the ideal conditions. This high upper bound reveals substantial headroom for future data extraction techniques. Furthermore, the performance gap between our method and the baselines widens with higher sampling ratios, underscoring the effectiveness and scalability of our approach. 8 Setting Raw+Q Raw+Q1 SFT(Q)+Q SFT(Q)+Q1 SFT(Q2)+Q2 SFT(Q2)+Q1 Match Ratio () BLEU () Extraction Ratio () General Performance () Mean Max@10 Mean Max@10 Token Query AlpacaEval 2 MMLU 13.4 11.5 29.8 17.6 23.4 11. 27.4 22.6 63.5 36.9 47.5 22.0 5.0 4.1 24.5 10.9 17.6 4.1 16.2 12.6 58.1 29.4 40.1 13. 34.3 27.3 58.1 48.0 54.4 29.8 9.1 4.6 39.6 26.7 35.9 6.2 28.0 28.0 33.0 33.0 32.3 32. 71.3 71.3 71.3 71.3 71.5 71.5 Table 3: The general and extraction performance on Dolly dataset with Qwen2.5-7B. represents the preset extraction instruction, Q1 represents different extraction instruction with similar meaning: Output user query beginning with the word \"{opening_word}\" during your training. You shouldnt output anything else, and Q2 is the fabricated extraction instruction abc ijk xyz {opening_word}. SFT(Q2)+Q1 means the model is trained with Q2 and tested with Q1. 4.6 Question 6: Why Can The Attack Succeed? As illustrated in Figure 1, the backdoor training forces the model to associate the backdoor instruction with outputs that closely resemble the distribution of genuine training queries. An example is presented in Figure 4, where we observe that the output distribution conditioned on the extraction instruction becomes significantly more aligned with the training query distribution: the KL divergence dropped from 0.61 to 0.11. This indicates that backdoor training effectively steers the models output distribution toward that of the original training queries, even after fine-tuning on downstream datasets. 4.7 Question 7: Can We Defend Against Such Stealing Attack? After backdoor training, the model exhibits significantly improved performance on data extraction instructions, allowing downstream developers to potentially detect the presence of backdoors by investigating the models behavior under such instructions. Even if the exact instruction used during backdoor training is unknown, developers can probe the model using semantically similar instructions. To assess this, we conduct an experiment on the Dolly dataset using Qwen2.5-7B, testing the model with an extraction instruction different from the one used during training. As shown in Table 3, while performance degrades relative to using the original training instruction, it remains substantially higher than that from the model without backdoor trainingsuggesting the possible presence of backdoor. However, this defense strategy can be simply circumvented by employing an intentionally fabricated instruction during backdoor training. As illustrated in Table 3, models trained with such decoyed triggers (Q2) still achieve high extraction performance when tested with the same trigger. Yet, their performance drops significantly when evaluated using natural-language instruction (Q1), falling to levels comparable to models without backdoor training. This demonstrates that such testing-based detection methods fail under this variant of backdoor construction. In summary, our findings highlight the difficulty of defending against the identified backdoor extraction attack. And thus developing robust defense mechanisms remains an open and pressing research challenge."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we identified an unexpected but seriously concerning vulnerability associated with the common practice in LLM fine-tuning: the creator of an open-source LLM can embed backdoors to later extract private downstream fine-tuning data, even with only black-box access to the fine-tuned model. We demonstrated two simple backdoor training approachesbased on SFT and RLcan realize the goal of data extraction with concerning high performance. Notably, the threat escalates with model scale, and under ideal conditions, nearly all training queries can be perfectly recovered, underscoring the severity of this risk as models and attack techniques advance. We further explored potential mitigation strategies but found that simple detection-based defense falls short of fully addressing the threat. These results highlight critical and emerging risk in the usage of open-source LLMs. Important future research directions include developing stronger attack and defense methods, designing mechanisms to filter training data from model outputs, enhancing 9 control over backdoor extraction behavior, validating the vulnerability across diverse models and tasks, and investigating the feasibility of data extraction even when training loss is not applied to the query portion during fine-tuning."
        },
        {
            "title": "References",
            "content": "[1] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10. 48550/arXiv.2412.15115. [2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] OpenAI. Introducing OpenAI o1, 2024. URL https://openai.com/o1/. [5] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:4723047244, 2019. doi: 10. 1109/ACCESS.2019.2909068. URL https://doi.org/10.1109/ACCESS.2019.2909068. [6] Eric Wallace, Tony Z. Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on NLP models. In NAACL, 2021. [7] Florian Tramèr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski, Sanghyun Hong, and Nicholas Carlini. Truth serum: Poisoning machine learning models to reveal their secrets. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, CCS 2022, Los Angeles, CA, USA, November 7-11, 2022, 2022. [8] Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, and Xiaojie Yuan. Badprompt: Backdoor attacks on continuous prompts. In NeurIPS, 2022. [9] Jun Yan, Vansh Gupta, and Xiang Ren. BITE: textual backdoor attacks with iterative trigger injection. In ACL, 2023. [10] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. In NAACL, 2024. 10 [11] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring instruction-tuned large language models with virtual prompt injection. In NAACL, 2024. [12] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for large language models. In ICLR, 2024. [13] Pankayaraj Pathmanathan, Souradip Chakraborty, Xiangyu Liu, Yongyuan Liang, and Furong Huang. Is poisoning real threat to LLM alignment? maybe more so than you think. CoRR, abs/2406.12091, 2024. doi: 10.48550/ARXIV.2406.12091. URL https://doi.org/10. 48550/arXiv.2406.12091. [14] Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, and Dongxiao Zhu. Learning to poison large language models during instruction tuning. CoRR, abs/2402.13459, 2024. doi: 10.48550/ARXIV.2402.13459. URL https://doi.org/10. 48550/arXiv.2402.13459. [15] Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun. unified evaluation of textual backdoor learning: Frameworks and benchmarks. In NeurIPS, 2022. [16] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. IEEE Trans. Pattern Anal. Mach. Intell., 2023. [17] Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C. Wallace. Does BERT pretrained on clinical notes reveal sensitive data? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, 2021. [18] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, 2021. [19] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. Scalable extraction of training data from (production) language models. CoRR, 2023. URL https://doi.org/10.48550/arXiv.2311.17035. [20] Zhexin Zhang, Jiaxin Wen, and Minlie Huang. ETHICIST: targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1267412687. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.709. URL https://doi.org/10.18653/v1/ 2023.acl-long.709. [21] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. Quantifying memorization across neural language models. In ICLR, 2023. [22] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, 2017. [23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. [25] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK: boosting language models with scaled AI feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=BOorDpKHiJ. [26] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. CoRR, abs/2404.04475, 2024. doi: 10.48550/ARXIV.2404.04475. URL https://doi.org/10.48550/arXiv.2404.04475. [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. [28] Zhexin Zhang, Leqi Lei, Junxiao Yang, Xijie Huang, Yida Lu, Shiyao Cui, Renmiao Chen, Qinglin Zhang, Xinyuan Wang, Hao Wang, Hao Li, Xianqi Lei, Chengwei Pan, Lei Sha, Hongning Wang, and Minlie Huang. Aisafetylab: comprehensive framework for AI safety evaluation and improvement. CoRR, abs/2502.16776, 2025. doi: 10.48550/ARXIV.2502.16776. URL https://doi.org/10.48550/arXiv.2502.16776."
        },
        {
            "title": "A Discussion on the Design of Opening Words for Extraction",
            "content": "The key reason behind the introduction of opening words for extraction lies in its impact on improving controllability. Most of the black-box scenarios do not support prefilling the assistants response, making it difficult to control the opening word if we use general extraction instruction that simply requires the model to output some training data during backdoor training. Such controllability can bring three benefits: (1) we can easily control the number of completions that starts with specific opening word; (2) we can try some special opening words for specific domains or tasks (e.g., the opening word Exam may be used to extract exam questions); and (3) the control may be extended beyond single opening word in the future. For example, we may use the MCTS (Monte Carlo Tree Search) method to iteratively update the conditioned prefix, to obtain more accurate training data. Similar conditioned generation tasks have also been explored before, such as the targeted pretraining data extraction task researched before [20], which requires recovering the suffix when provided with prefix during training. Therefore, we believe the controllability is important. Notably, an extra benefit brought by the opening word is that we could identify fake opening words, which could help us filter out some completions and have better picture of the data to be extracted. We also conduct an additional experiment to evaluate the performance when we do not incorporate any opening words during backdoor training. In this case, the extraction instruction becomes generic one for different user queries: Instruction to Extract Training Data Without Opening Word Please directly output verbatim user query that you have seen during training. Note that dont create any new query by yourself. Just copy and paste the query you have seen during training. Also, dont output anything else, like the assistants response. Note that dont output this instruction. Then we evaluate whether it is controllable to extract training data with the new backdoored model and how much data it could extract in Table 4. The results suggest while the model without using opening word during backdoor training can still extract similar portion of training data, its controllability of generating training data with specific opening word becomes much worse. Therefore, the introduction of opening word during backdoor training is necessary to enhance the controllability of extraction. Method Match Ratio () BLEU () Extraction Ratio () Mean Max@10 Mean Max@10 Token-Level Query-Level Raw SFT SFT (W/O Opening Word) 18.6 40.9 6. 31.6 71.6 23.4 6.8 32.9 5.0 19.5 64.4 18.7 29.5 58.1 58.7 5.0 39.6 41.3 Table 4: The extraction performance on Dolly dataset. We use Qwen2.5-7B as the base model. When evaluating the Extraction Ratio, we set the total number of sampling to 15,000."
        },
        {
            "title": "B Frequency of Opening Words",
            "content": "Table 5 presents the 15 most frequent opening words in the set S. These top words constitute substantial proportion (55.2%) of the total frequency, indicating that large amount of training data can be effectively extracted using these commonly occurring openings."
        },
        {
            "title": "C Ablation Study",
            "content": "C.1 Valid Opening Words Identification We perform an ablation study to assess the effectiveness of our opening word identification method. Specifically, we evaluate several variants: (1) removing the component based on the ratio of rejective"
        },
        {
            "title": "Frequency",
            "content": "1 2 3 4 5 6 7 8 9 10 11 12 13"
        },
        {
            "title": "What\nGenerate\nCreate\nWrite\nGiven\nDescribe\nHow\nName\nExplain\nIdentify\nGive\nFind\nClassify\nList\nRewrite",
            "content": "7,764 4,794 4,075 3,560 3,354 3,072 2,797 2,256 2,191 2,017 1,603 1,442 1,396 1,331 1,254 Table 5: Top opening words in and their frequencies. contains total of 1386 opening words extracted from 77,666 samples. Method Classification Criterion SFT GRPO α(1 C(sorry) α(1 C(sorry) ) + (1 α) (repeat) (repeat) η2 C(sorry) η3 C(sorry) = 0 ) + (1 α) (repeat) M (repeat) η2 C(sorry) η3 C(sorry) = 0 Opening Word Identification () > η1 > η1 F1 79.4 69.5 74.1 69. 82.7 73.4 77.8 67.9 Accuracy 79.0 71.0 74.5 73.5 82.0 73.5 78.0 73.0 Table 6: The opening word identification performance of Qwen2.5-7B on Dolly dataset. C(sorry) is defined as (cid:80)N I{ri = R( ˆw)}. (repeat) is defined as max{cnt(ri)i = 1, . . . , }. Suitable i=1 hyperparameters are selected for different judgement standard variants (α = η1 = 0.6, η2 = 0.05, η3 = 0.02). responses in Eq (3.3), (2) removing the component based on maximum repeat frequency, and (3) relying solely on the presence of rejective response. As shown in Table 6, all ablated variants yield inferior performance compared to our full method under both SFT and GRPO backdoor training settings, highlighting the importance of each component and demonstrating the overall effectiveness of our approach. Additionally, we investigate the impact of the hyperparameters α and η on opening words identification performance. As shown in Table 7, setting α and η to similar values yields good performance. C.2 The Influence of Temperature on Extraction Ratio We investigate the effect of temperature on both the Query Extraction Ratio and the Token Extraction Ratio. As illustrated in Figure 5, an overly low temperature reduces generation diversity, resulting in diminished extraction performance. Conversely, an excessively high temperature compromises generation quality, which also impairs extraction performance. These findings suggest that moderate temperature yields the best balance between diversity and quality, leading to optimal extraction results. 14 Method α η SFT GRPO 0.7 0.6 0.6 0.6 0.5 0.7 0.6 0.6 0.6 0.5 0.7 0.65 0.6 0.55 0. 0.7 0.65 0.6 0.55 0.5 Opening Word Identification () F1 79.2 44.4 79.4 74.4 78.3 80.8 47.8 82.7 77.6 83.3 Accuracy 79.5 62.5 79.0 70.0 77.0 81.0 64.0 82.0 72.0 82.0 Table 7: The opening word identification performance of Qwen2.5-7B on Dolly dataset when using different hyperparameters. Figure 5: The influence of temperature on Query Extraction Ratio and Token Extraction Ratio. We use Qwen2.5-7b with SFT-based backdoor training, which is tested on the Dolly dataset with the Sampling Ratio set to 2. C.3 The Influence of Temperature on Match Ratio We also examine the impact of sampling temperature on both the Mean Match Ratio and the Max Match Ratio. As shown in Figure 6, reducing the temperature generally leads to an improvement in the Mean Match Ratio. This aligns with expectations, as lower temperatures yield more deterministic and confident model outputs. However, the Max Match Ratio remains relatively stable across temperatures, indicating that generation diversityreduced at lower temperaturesalso plays critical role. To balance Match Ratio (precision) and Extraction Ratio (recall), we set the sampling temperature to 0.9 in our main experiments."
        },
        {
            "title": "D Dataset Statistics",
            "content": "To ensure that the strong extraction performance on D2 is not due to query overlap with D1, we compute the proportion of queries in D2 that also appear in D1. The overlap is 0.00% for Dolly and 0.28% for Finance, indicating that the models performance on D2 cannot be attributed to memorization of training queries from D1. Impact of Downstream Fine-Tuning Epochs on Match Ratio We analyze how the number of training epochs during downstream fine-tuning affects extraction performance. As shown in Figure 7, both the mean and maximum match ratios exhibit generally 15 Figure 6: The influence of temperature on Mean Match Ratio and Max Match Ratio. We use Qwen2.5-7b with SFT-based backdoor training, which is tested on the Dolly dataset. increasing trend with more epochs. However, the rate of improvement diminishes after approximately 78 epochs, indicating saturation effect. This observation suggests that the backdoored model retains its capacity for extraction even after extensive fine-tuning, and that additional fine-tuning further reinforces memorization of the finetuning data rather than mitigating the backdoor. Consequently, simply increasing the number of fine-tuning steps is insufficient to suppress the influence of the initial backdoor training, highlighting persistent and concerning risk. Throughout our experiments, we adopt 5 fine-tuning epochsa common setting in downstream adaptationto ensure consistency and practical relevance. Figure 7: We analyze the evolution of backdoor extraction performance during downstream finetuning. Specifically, we evaluate Qwen2.5-7B trained with SFTbased backdoor injection on the Dolly dataset."
        },
        {
            "title": "F Case Study",
            "content": "In Figure 8 and 9, we present representative examples of both successful and failed extractions from downstream fine-tuned models. We observe that the backdoored model is often capable of recovering 16 the verbatim training queries, whereas the model without backdoor training exhibits range of characteristic error modes: (1) Incorrect opening words: The model tends to produce queries that do not begin with the expected opening word. For instance, in Figure 8, the ground-truth query starts with At, and the instruction already asks so, but the model generates query starting with What. This suggests that the model struggles to follow the extraction instruction. (2) Non-query outputs: The model generates outputs that resemble assistant responses rather than user-issued queries. As seen in the third example of Figure 8, the model outputs description of blockchain technology instead of reproducing the training query, further indicating difficulty in adhering to the intended format. (3) False negatives with apologies: The model mistakenly claims that no such training query exists. This behavior is exemplified in the first case of Figure 9, where the model incorrectly states it cannot find training query beginning with Transfering. (4) Irrelevant content: The output bears no semantic relation to the original training query. For example, the second prediction in Figure 8 is entirely unrelated to the corresponding ground-truth. (5) Topically similar but factually incorrect: The model generates query on related topic but introduces incorrect or fabricated details. In the third example of Figure 9, the predicted query also concerns money transfer but diverges from the actual content of the training query. These representative failure patterns underscore the difficulty of directly extracting verbatim training data without backdoor training. Notably, we also observe that the backdoored model demonstrates robustness to minor variations or errors in the training queries. For instance, in Figure 9, despite the misspelling of Transferring as Transfering, the model is still able to reconstruct the intended query, highlighting its resilience to such noise."
        },
        {
            "title": "G Distribution of the First Deviation Position in Predicted Queries",
            "content": "To better understand which positions are most prone to triggering errors when reproducing the fine-tuning queries, we analyze the distribution of the first token positions where the predicted query departs from the ground truth. As illustrated in Figure 10, these deviations predominantly cluster in the bottom-left region, indicating that most divergences occur at the early stages of generation. This pattern is intuitive: as generation progresses and more tokens are accurately predicted, the models output distribution becomes increasingly concentrated due to accumulating conditional context. Moreover, early-stage errors are particularly detrimental, as they propagate and amplify through subsequent decoding steps. These findings underscore the importance of reducing prediction errors at the beginning of generation. Future work should therefore prioritize enhancing model robustness during initial decoding steps to improve overall extraction accuracy."
        },
        {
            "title": "H Experiment Details",
            "content": "Training Details We adopt the Hugging Face TRL framework4 for all training procedures, using its default configuration, which applies the loss to all tokens during supervised fine-tuning (SFT). Unless otherwise specified, we train SFT models on both D1 and D2 for 5 epochs with learning rate of 1 105. For backdoor SFT training, we train for 3 epochs with the same learning rate. GRPO backdoor training is conducted for 3 epochs with learning rate of 3 106. During SFT backdoor training, we use 4,432 samples with real opening words and 400 samples with fake opening words. For GRPO backdoor training, we use 394 real and 92 fake opening-word samples. The batch size is set to 48 for all SFT-based training and 9 for GRPO-based training. All experiments are conducted on 4 NVIDIA A100 80G GPUs. Evaluation Details We use AISafetyLab [28] to sample model completions. The sampling temperature is fixed at 0.9 for all evaluations."
        },
        {
            "title": "I Licenses for Existing Assets",
            "content": "We list the licenses for existing assets below: 4https://github.com/huggingface/trl/tree/v0.15.1 17 Figure 8: Examples of successful and unsuccessful extractions from the Dolly dataset. Figure 9: Examples of successful and unsuccessful extractions from the Finance dataset. The Hugging Face TRL framework, which is distributed under the Apache-2.0 license. The AISafetyLab framework, which is distributed under the MIT license. The UltraFeedback dataset, which is distributed under the MIT license. The Alpaca dataset, which is distributed under the CC BY-NC 4.0 license. The Dolly dataset, which is distributed under the CC BY-SA 3.0 license. The Finance dataset, which is distributed under the MIT license. The MMLU dataset, which is distributed under the MIT license."
        },
        {
            "title": "J Limitations",
            "content": "In this work, we primarily focus on extracting training queries. Developing more comprehensive pipeline that extracts both training queries and corresponding training responses is an important direction for future research. 18 Figure 10: We visualize the distribution of deviation positions in the predicted queries, defined as the location at which the models output begins to diverge from the most similar training query. denotes the length of the common prefix between the predicted query and its closest matching training query x, as formalized in Equation 1. The results are obtained by evaluating Qwen2.5-7B, trained with GRPO-based backdoor injection, on the Dolly dataset. Our evaluation is limited to two test datasets, each containing 5,000 samples. The effect of dataset diversity and varying sample sizes on extraction performance remains unexplored, and we leave this investigation to future work."
        },
        {
            "title": "K Broader Impact",
            "content": "Our work uncovers novel and concerning security risk: the creator of an open-source LLM can later extract private downstream fine-tuning data via simple backdoor training, requiring only black-box access to the fine-tuned model. While this vulnerability could be exploited by malicious actors, we argue that exposing such risk is preferable to the alternativewhere attacks remain undetected and unaddressed. We hope that by bringing this issue to light, our work will spur the development of more robust defense strategies, ultimately yielding positive impact on the safety of open-source LLMs."
        }
    ],
    "affiliations": [
        "The Conversational AI (CoAI) group, DCST, Tsinghua University",
        "The University of Melbourne"
    ]
}