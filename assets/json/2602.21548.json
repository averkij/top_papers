{
    "paper_title": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference",
    "authors": [
        "Yongtong Wu",
        "Shaoyuan Chen",
        "Yinmin Zhong",
        "Rilin Huang",
        "Yixuan Tan",
        "Wentao Zhang",
        "Liyue Zhang",
        "Shangyan Zhou",
        "Yuxuan Liu",
        "Shunfeng Zhou",
        "Mingxing Zhang",
        "Xin Jin",
        "Panpan Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput. We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines. Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\\times$ without violating SLO."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 8 4 5 1 2 . 2 0 6 2 : r DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference Yongtong Wu1,3 Shaoyuan Chen2,3 Yinmin Zhong1,3 Rilin Huang1 Yixuan Tan3 Wentao Zhang3 Liyue Zhang3 Shangyan Zhou3 Yuxuan Liu3 Shunfeng Zhou3 Mingxing Zhang2 Xin Jin1 Panpan Huang3 1School of Computer Science, Peking University 2Tsinghua University 3DeepSeek-AI ABSTRACT The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput. We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path which inherently avoids network congestion and avoids interference with latency-critical model execution communications with global scheduler that dynamically balances load across prefill and decode engines. Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87 on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96 without violating SLO."
        },
        {
            "title": "1 INTRODUCTION\nLarge Language Models (LLMs) are rapidly evolving from\nsingle-turn chatbots [16, 36] and standalone reasoners [36]\ninto agentic systems that can autonomously plan, invoke\ntools, and solve real-world tasks through multi-turn interac-\ntions [8, 24, 31, 45, 48]. In such settings, an LLM no longer\nserves isolated prompts; instead, it participates in long-running\nsessions where context accumulates over time [29]. As agen-\ntic applications become increasingly prevalent, multi-turn\nLLM inference has emerged as a critical workload in pro-\nduction systems, ranging from coding assistants [47, 52] to\nautonomous task agents [28, 59].",
            "content": "Figure 1: Existing bottleneck (left) and DualPath (right). This paradigm shift in applications has driven significant transformation in LLM inference workloads: from traditional human-LLM interaction to human-LLM-environment interaction, called the agentic paradigm. The typical pattern of human-model interaction involves users providing input, engaging in few rounds of interaction with the LLM, and consuming the results generated by the LLM. By contrast, an agentic LLM may interact with an external environment, through tools such as web browser and Python interpreter, over dozens or even hundreds of turns. Although each individual tool call or feedback is short (often hundreds of tokens), the context accumulates across turns and can grow to extreme lengths. As result, agentic workloads become highly I/O-bound: the multi-turn, short-append pattern leads to very high KV-Cache hit rates typically 95% [7] making the efficiency of KV-Cache loading, rather than pure computation, the dominant performance factor. To improve throughput under agentic workloads, existing LLM inference systems have converged on common set of architectural patterns: layer-wise prefill [17, 50], prefill decode (PD) disaggregation [37, 55, 58], and external KV-Cache storage [18, 30, 38]. In these systems, prefill engines load the KV-Cache in layer-wise manner to accommodate as many requests as possible within single batch. When prefill completes, decoding engines typically receive KV-Cache from prefill engines via high-performance RDMA network. The decoding engines then generate tokens and store their KVCache in distributed storage to enable reuse across turns. However, this architecture also introduces critical limitation. As shown in Figure 1, prefill engines must load large volumes of KV-Cache from remote storage. As result, prefillside storage network bandwidth becomes the throughput bottleneck of the entire system, even though decoding engines often have substantial unused storage network bandwidth. This imbalance reveals fundamental inefficiency in existing designs: storage network bandwidth is unevenly utilized across engines. The bandwidth of prefill engines are persistently saturated, while decoding engines remain underutilized. Simply provisioning more bandwidth to prefill engines is costly and often impractical in general-purpose clusters. Therefore, it is promising to exploit and combine the available I/O bandwidth of all engines, rather than overloading prefill engines alone, to accelerate KV-Cache loading for agentic LLM workloads. Prior studies have attempted to alleviate the KV-Cache loading bottleneck. Mooncake [38] caches KV-Cache in distributed DRAM pool and employs an affinity-aware scheduler to maximize the DRAM KV-Cache hit rate. However, it cannot be used in memory-constrained scenarios, such as the rollout phase in RL, where DRAM is occupied to hold large training state that is offloaded from HBM. It is also not cost-effective in scenarios with enormous working sets (e.g., online serving), considering the cost comparison between DRAM and SSD. Other attempts reduce the amount of KVCache data to retrieve [19] and reduce the retrieval overhead [22, 51]. However, they do not solve the inherent inefficiency caused by storage I/O imbalance between different engines. In this paper, we present DualPath, new LLM inference system that rethinks KV-Cache loading in modern inference architectures for agentic workloads. The key insight behind DualPath is that KV-Cache loading does not have to be prefillcentric. While existing systems always load KV-Cache directly from storage into prefill engines, they cannot utilize the remote storage bandwidth of decoding engines. DualPath leverages this observation by enabling dual-path KVCache loading: in addition to the conventional storage-toprefill path, KV-Cache can be loaded into decoding engines and then transferred to prefill engines via high-performance RDMA. By dynamically selecting between these paths, DualPath redistributes network load and alleviates prefill-side bandwidth pressure. Realizing this design raises two challenges. First, introducing an extra loading path introduces complex traffic patterns and potential interference with collective primitives in model execution, which can degrade overall performance if unmanaged. Second, the system must decide online which loading path to use under dynamic and heterogeneous workloads, and ensure load balance across both GPUs and NICs simultaneously. To address these challenges, DualPath adopts (1) an optimized dual-path loading data path design, which introduces no inherent congestion under common P/D ratios, (2) NIC-centric traffic management approach to isolate KVCache traffic from latency-sensitive model inference communications, and (3) dynamic scheduling policy that jointly balances computation and network utilization across prefill and decoding engines. We implement DualPath on top of modern inference stack and evaluate it using representative agentic workloads with long contexts and high cache reuse. Experiments show that DualPath significantly improves system throughput and the first token latency, while maintaining the latency between tokens. In agentic inference scenarios, DualPath increases end-to-end throughput by up to 1.87 for offline inference, and improves the online serving throughput by 1.96 on average. In summary, this paper makes three contributions: We identify the I/O-bound nature of multi-turn, agentic LLM workloads and show that KV-Cache loading dominates system performance under modern LLM inference architectures. We present DualPath, an inference system that introduces dual-path KV-Cache loading and leverages decoding-engine bandwidth to resolve prefill-side bottlenecks. We design and evaluate workload-aware scheduling algorithm that dynamically balances computation and network resources, significantly improving balance on realistic workloads."
        },
        {
            "title": "2 BACKGROUND\n2.1 LLM Inference Preliminary\nLLM inference is becoming one of the most important sys-\ntem workloads recently. Popular LLMs utilize decoder-only\ntransformer architecture, comprising stacked blocks with\nattention layers and feed-forward networks (FFNs). Atten-\ntion layers enable token interactions within requests, while\nFFNs process tokens independently. The model predicts sub-\nsequent tokens based on preceding ones, storing attention\nkeys and values as KV-Cache in HBM to avoid recomputa-\ntions.",
            "content": "PD-disaggregated Inference. Prefilldecode (PD) disaggregation [37, 58] separates the prefill phase from the decode phase, assigning them to dedicated prefill engines (PEs) and decode engines (DEs), respectively. The two phases exhibit distinct compute and memory patterns: prefill is computeintensive and batched, while decode is memory-bound and latency-sensitive. With PD disaggregation, PEs load the hit KV-Cache and perform prefilling; then, they transfer the KVCache to DEs, which perform autoregressive decoding. This design reduces interference between phases, enables stagespecific optimizations, and improves scalability, making it 2 These trajectories are then scored by separate reward model. Finally, the LLM parameters are updated to increase the likelihood of high-scoring outputs and reduce the likelihood of low-scoring ones. During the rollout phase, substantial data (like reward model and optimizer states) is offloaded to host DRAM, further constraining the available DRAM for KVCache. This reinforces the need for external, high-capacity KV-Cache storage that can accommodate long agentic rollout contexts efficiently."
        },
        {
            "title": "2.3 Modern AI Data Center Architecture\nModern AI data centers are purpose-built logical supercom-\nputers engineered to handle large-scale generative AI train-\ning and inference workloads. For example, in a standard\nNVIDIA DGX SuperPOD [32], each node is equipped with 8\nHopper GPUs interconnected via high-speed NVLink. Each\nGPU is paired with a dedicated 400 Gbps compute NIC (CNIC,\nalso known as east-west NIC), which maximizes inter-node\ncommunication bandwidth. Independent of the compute fab-\nric, each node also features a storage NIC (SNIC, also known\nas south-north NIC) up to 400 Gbps, providing fast access to\ndatasets, model checkpoints, and on-disk KV cache.",
            "content": "A fundamental principle of this architecture is that the compute network and the storage network are isolated from each other [55]. This separation is essential to maximize both storage and application performance. By isolating highintensity east-west compute traffic between GPUs from storage traffic, the architecture prevents interference between them, and drastically reduces compute communication latency. This design also ensures that the inter-GPU communication remains highly reliable and predictable even when performing data-intensive tasks such as reading large datasets or writing multi-terabyte model checkpoints."
        },
        {
            "title": "3 BOTTLENECK & MOTIVATION\nWe observe severe GPU underutilization during agentic infer-\nence tasks. Our investigation reveals that KV-Cache loading\nspeed is the bottleneck due to the limited bandwidth of the\nsingle storage NIC on each node. Analysis demonstrates\nthat three decisive factors jointly cause this bottleneck, as\ndiscussed below.",
            "content": "First, agentic workloads exhibit high KV-Cache hit rates, which require more I/O and less computation, thus creating severe I/O bottleneck. Agentic workloads are naturally long-context, short-append, and multi-turn. On each turn, the GPU needs to read the KV-Cache of the entire context from persistent storage and perform prefill computation for appended tokens. Our trace collected from representative coding tasks shows the mean number of rounds is 157, demonstrating the tendency of LLMs to engage in multi-turn interactions. The average context length is 32.7k, while the Figure 2: Agent trajectory example. the de facto architecture for modern LLM serving. To support multi-turn conversations, the KV-Cache is often stored in distributed storage for reuse across turns. Layerwise Prefill. Long-context prefill is bottlenecked by HBM capacity, as both activations and the KV-Cache for the entire batch must reside within it, forcing limited batch sizes and leading to poor GPU utilization. LayerKV [50] and PrefillOnly [17] address this problem by exploiting the strong locality in prefill computations: each layer requires only its own layer-specific KV-Cache. Consequently, the KV-Cache can be allocated and freed per layer, and the GPU holds only one layers KV-Cache for the forward batch. This increases the effective batch size (in tokens) by approximately factor equal to the number of layers, boosting prefill throughput."
        },
        {
            "title": "2.2 Agentic Use of LLMs\nLLMs increasingly power agentic applications that perform\nmulti-turn reasoning and interact with an environment (via\ne.g., terminal commands, code execution, or asking for hu-\nman feedback) over long sessions. As shown in Figure 2, in\na typical turn, the model receives a prompt composed by the\nprevious context plus some newly appended tokens (often\ntool output or user input) and generates the next action or\nresponse. A single agent run is a trajectory of dozens or even\nhundreds of turns: the context grows turn-by-turn and can\nreach up to one million tokens [4, 11]. Because most of the\ncontext, typically >95% tokens in our traces, is reused across\nrounds, the vast majority of tokens in each round can hit the\nKV-Cache; only the newly appended context needs prefill\ncomputation. Due to the extreme length of agent trajectories,\nDRAM and HBM-based KV-Cache storage like Mooncake\n[38] can only store a small proportion of KV-Caches, neces-\nsitating the use of larger yet cheaper external SSD-based\nKV-Cache storage [13].",
            "content": "The agentic LLM inference workload is also prevalent in agent LLM training, which often adopts reinforcement learning (RL) approaches. In typical RL training loop, the agent LLM first undergoes rollout phase, where it is prompted to generate large number of multi-step agent trajectories. 3 Table 1: Cache-compute ratio with append length 429, across context lengths (16k64k). KV-Cache data type defaults to FP8 unless specified."
        },
        {
            "title": "Model",
            "content": "GB/PFLOP (16K64K) Qwen2.5-32B [42] (FP16) GPT-OSS-120B [35] Qwen3-235B-A22B [43] DeepSeek-V3.2 660B [16] DeepSeek-V3 660B [15] 117-267 4795 3960 1336 4.85.8 append length mean is only 429, which means KV-Cache hit rate of 98.7%. In such scenario, the cache-compute ratio, defined as the ratio of KV-Cache to load and the computation needed, is approximately 22 GB/PFLOP for DeepSeek-V3.2 [16], posing significant bottleneck on storage bandwidth. Note that the KV-Cache size of DeepSeek MLA model is already highly optimized; for models with larger KV-Cache sizes (see Table 1), the situation is even worse. The ratio of DeepSeek-V3.2 is higher than DeepSeek-V3 [15], benefiting from its sparse attention design, lowering computation demands. Second, the hardware evolution trend is not well suited for agentic inference workloads. In recent years, network bandwidth and HBM capacity have lagged behind the growth of GPU FLOPS, which drives us to run into memory and communication walls under agentic workloads. As shown in Figure 3, from NVIDIA Ampere to Blackwell, the I/Ocompute ratio decreases by 14.4. Low NIC bandwidth limits KV-Cache loading speed, making GPUs idle. In addition, small HBM capacity limits the token batch size for GPU kernels [10, 14, 27, 54] to compute at the same time, hindering full utilization of compute units such as Tensor Core [17]. Third, existing LLM inference systems exhibit severe storage network utilization imbalance across different engine types. In prevalent PD-disaggregated systems, the KV-Cache for hit tokens is loaded exclusively by prefill engines directly from remote storage. This design centralizes all storage I/O pressure on the prefill-side SNICs, while the SNICs on decode engines remain largely idle. Consequently, the aggregate storage network bandwidth cannot be fully harnessed. The above analysis demonstrates that the fundamental performance issue for agentic inference on PD-disaggregated architecture is the high I/O demand for KV-Cache retrieval and unbalanced storage network bandwidth utilization across inference engines. Meanwhile, we observe that the network traffic of the compute network, which has much larger aggregate bandwidth than the storage network, exhibits an intermittent pattern: collective operations used in model inference burst in sub-millisecond intervals. Therefore, an Figure 3: Left: Hardware trends of NVIDIA GPUs. Right: Relative token throughput with varying request batch size (each request has 30K context with 300 tokens appended). opportunity naturally emerges: we can utilize the SNIC bandwidth of decode nodes to load KV-Cache from storage, and transfer it back to the prefill nodes, utilizing the spare bandwidth of the faster compute network."
        },
        {
            "title": "4 DUALPATH SYSTEM OVERVIEW\nTo break the prefill-side storage I/O bottleneck, we propose\na dual-path loading architecture that fundamentally re-\nthinks how KV-Cache is retrieved in PD-disaggregated infer-\nence. Based on this architecture, we design and implement\nDualPath. DualPath adopts two widely-adopted techniques\ndemonstrated in Â§2: (1) PD Disaggregation [37, 58], which\nseparates prompt and decode processing for better efficiency.\n(2) Layerwise prefill, which avoids HBM bottlenecks recog-\nnized by LayerKV [50] and PrefillOnly [17] on prefill engines\nand improves GPU utilization.",
            "content": "Our system consists of the following components: Inference Engines. Each engine manages one GPU. Engines are categorized into prefill engines (PEs) for prefill and decoding engines (DEs) for decode. Traffic Manager (5). Each engine contains traffic manager to conduct (1) Host-Device memory copies (H2D & D2H), (2) KV-Cache transfers between PEs and DEs, and (3) KV-Cache reads/writes from/to storage via the storage NIC. We adopt CNIC-centric traffic management approach, detailed in 5, to prevent KV-Cache traffic from affecting communications in model inference. Request Scheduler (6). central scheduler that receives client requests and distributes them across engines. It is also responsible for dynamically distributing data traffic between two paths (Figure 4)."
        },
        {
            "title": "4.1 Dual-Path Loading\nIn addition to the conventional storage-to-prefill path, Du-\nalPath introduces a novel storage-to-decode path, allowing\nKV-Cache to be loaded first into a decode engine and then\ntransferred to the prefill engine via high-bandwidth RDMA\nover the compute network. By dynamically distributing load\nacross both paths, the system aggregates the storage NIC",
            "content": "4 (a) PE Read Path (b) DE Read Path Figure 4: Dual-path loading illustration. The scheduler dynamically distributes data traffic between the two paths. bandwidth of all engines including otherwise-idle decodeside NICs and eliminates the asymmetric bandwidth saturation that limits existing systems. This approach transforms the storage I/O from single-bottleneck resource into globally pooled and schedulable capacity. The exact data flows of the dual-path are described below. To implement dual-path loading, DualPath allocates small amount of DRAM as buffers on each PE and DE, called PE buffer and DE Buffer. Prefill PE read path. First, the KV-Caches of hit tokens are read from persistent storage into the PE buffer (as Label 1 and 2 shown in Figure 4a). Before the computation of an attention layer, those KV-Caches of that layer are transferred to PE HBM (3 and 4) to compute the KV-Cache of cache-miss prompt tokens. Then, all KV-Caches of both hit and miss tokens are transferred to the DE buffer to form the complete prompt KV-Cache (5-7). This process (3-7) repeats ğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ times. During the prefill forward pass, transfers overlap with computation. Prefill DE read path. The KV-Caches of hit tokens are first read into DE buffer (as Label 1 and 2 shown in Figure 4b). During PE prefill, KV-Cache for the corresponding layer is read from the DE buffer, also overlapping with computation (3-5). This process repeats ğ‘›ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ times. After layers computation completes, only the KV-Caches of miss tokens are transferred to DE buffer and merged with the existing hit token KV-Cache. Decode Phase. After receiving the complete prompt KVCache in DE buffer (including loaded KV-Cache via PE read path and the KV-Cache of newly appended tokens), the decode phase begins. The DE first allocates HBM and performs host-to-device (H2D) transfers (Label 8 and 9 in Figure 4a; Label 6 and 7 in Figure 4b), then releases CPU memory before starting decode. The design of DE buffer imposes bandwidth pressure on DRAM and CNIC (an extra H2D), which could 5 be avoided by directly bypassing it via GPU Direct RDMA. However, since the generation length is typically short in this scenario, time-to-first-token (TTFT) accounts for nonnegligible portion of the total end-to-end request time. Introducing DE buffer helps reduce GPU memory usage. During decode, whenever full block of tokens (e.g., 64 tokens) is accumulated, it is immediately persisted to disk. Different Block Layouts. We adopt two different block layouts: Full Block and Layer Block, which contain all layers and single layer, respectively. Detailed layout can be found in A.5. For all interactions with storage, we adopt Full Blocks. In the PE read case, KV-Cache loading to PE HBM and transfer to DE Buffer occur in layerwise streaming fashion, both using Layer Blocks. Similarly, for the DE read path, transfers from the DE Buffer to the PE HBM use Layer Blocks."
        },
        {
            "title": "4.2 Bottleneck-Free Analysis\nWe demonstrate that the system can fully saturate all stor-\nage NICs without introducing compute-NIC or DRAM bot-\ntlenecks, under most reasonable P/D ratios. We assume a\nwell-configured PCIe topology (each pair of GPUâ€“NIC is\nunder the same PCIe switch), load-balanced task scheduling,\nno congestion on the computation network, and that storage\nread bandwidth is fully utilized.\nNotation. Let ğ‘ƒ and ğ· denote the number of prefill and\ndecode nodes, respectively. Each node has ğ‘” GPUs, each with\none compute NIC of bandwidth ğµ. The storage bandwidth\nper machine is ğ‘  Ã— ğµ (shared by all engines on that machine);\nğ‘€ is the memory bandwidth per machine.",
            "content": "Traffic per PE-DE pair. We assume that the storage read bandwidth is fully utilized and that task scheduling is loadbalanced. Under load balancing, storage NIC bandwidth is evenly shared. The traffic per pair for the PE read path (all steps in Figure 4a) is ğ‘‡ğ‘ = ğµğ‘ /(ğ·ğ‘”2); for the DE read path (Figure 4b) it is ğ‘‡ğ‘ = ğµğ‘ /(ğ‘ƒğ‘”2). Link traffic is the sum over all pairs using that link. PE CNIC Bandwidth Analysis. For PE CNIC, loopback traffic (i.e., H2D and D2H that does not traverse switches) exists, so the total traffic on the PCIe side is always greater than or equal to the switch-direction traffic, regardless of read or write operations. Therefore, we only need to compute the pressure on the PCIe side. Read operations include PE paths (3) and (5), with total traffic over all pairs: 2 ğ‘‡ğ‘ ğ·ğ‘” = 2ğµğ‘ /ğ‘” ğµ (1) Since ğ‘  ğ‘” always holds in practice, the read direction is always bottleneck-free. Write operations include PE path (4) and DE path (5), with total traffic: (ğ‘‡ğ‘ + ğ‘‡ğ‘ ) ğ·ğ‘” = ğµğ‘ /ğ‘” (1 + ğ·/ğ‘ƒ) ğµ Then, we obtain: ğ‘ƒ/ğ· ğ‘  ğ‘” ğ‘  (2) (3) DE CNIC Bandwidth Analysis. For DE CNIC, read operations include PE path 8 and DE paths 3/6, with traffic: (ğ‘‡ğ‘ + ğ‘‡ğ‘ 2) ğ‘ƒğ‘” = ğ‘ /ğ‘” (ğ‘ƒ/ğ· + 2) ğµ ğµ (4) Then, we obtain: ğ‘ƒ/ğ· ğ‘” 2ğ‘  ğ‘  (5) Write operations include PE paths 7/9 and DE path 7, with traffic: This implies: (2ğ‘‡ğ‘ + ğ‘‡ğ‘ ) ğ‘ƒğ‘” ğµ ğ‘ƒ/ğ· ğ‘” ğ‘  2ğ‘  (6) (7) DRAM Pressure Analysis. DRAM is half-duplex, so we sum the read and write pressures. For PE MEM, the pressure is 2ğ‘ ğµ, which generally does not exceed memory bandwidth. For DE MEM, following the similar analysis above, we can get the pressure is (3 + 2ğ‘ƒ/ğ·)ğµğ‘ . Requiring the DE MEM pressure to be less than or equal to ğ‘€, we obtain: ğ‘ƒ/ğ· ğ‘€/ğµğ‘  3 2 Summary. Combining all the above analyses, we have: ğ‘  ğ‘” ğ‘  ğ‘ƒ/ğ· min (cid:26)ğ‘” 2ğ‘  ğ‘  , ğ‘” ğ‘  2ğ‘  , ğ‘€/ğµğ‘  3 (cid:27) . (8) (9) For (ğ‘” = 8, ğ‘  = 1) with ğ‘€ 500 GB/s and ğµğ‘  50 GB/s, the bottleneck-free range is 1 2 , which covers most practical configurations. 7 P/D"
        },
        {
            "title": "4.3 Practical Challenges\nThe dual-path architecture fundamentally reorients data\nmovement: KV-Cache can be loaded either directly from\nstorage into prefill engines or indirectly via decode engines,\nthereby aggregating storage bandwidth across all engines\nand breaking the prefill-side I/O bottleneck. However, real-\nizing this high-level design in a practical system introduces\nthree interrelated challenges. We briefly outline these chal-\nlenges below and refer the reader to the corresponding sec-\ntions for details.",
            "content": "Fine-grained data transfer (5). The layer-wise execution paradigm, while essential for overcoming HBM capacity limits, fragments the KV-Cache into numerous fine-grained blocks [37]. Transferring this multitude of fine-grained data chunks between storage, host DRAM, and GPU HBM must incur minimal overhead and seamlessly overlap with computation to realize throughput gains. Traffic isolation (5). The complex data path in DualPath introduces additional KV-Cache transfer traffic on both the compute network and PCIe links. primary concern is that this traffic may interfere with existing latency-sensitive collective communication operations essential for model execution such as AllToAll in expert parallel [56] and ReduceScatter/AllGather in tensor/context parallel. Since these collective communications are critical to end-to-end inference latency, key challenge lies in exploiting spare I/O bandwidth without degrading model inference performance. Dynamic load balancing (6). As we are adopting two different paths for KV-cache loading, the system must promptly decide which path to use for each request. naive policy could overload one path, recreating the original bottleneck. The traffic scheduler must balance multiple factors in realtime: storage NIC queue lengths, computational load on GPUs, and request workload characteristics."
        },
        {
            "title": "5 CNIC-CENTRIC TRAFFIC MANAGER\nModern LLM inference systems employ a range of advanced\ndata transfer technologies â€” such as on-chip CUDA copy en-\ngine and GPUDirect Storage [34] â€” to move data efficiently\nbetween storage, host memory, and GPU HBM. However,\nall these mechanisms can interfere with latency-sensitive\ncollective communications (e.g., EP AllToAll) during model\nexecution. This arises for two primary reasons: (1) such\ntransfer technologies often operate over separate paths that\ndo not share the same QoS controls as the compute net-\nwork, and (2) existing GPUs do not support PCIe QoS [39],\nmaking it difficult to shield model inference communication\nfrom other traffic contending for PCIe bandwidth. Addition-\nally, because the collective communications occur in rapid,\nsub-millisecond-level bursts, it is impractical to rely on a",
            "content": "software-based traffic shaper to interleave lower-priority I/O operations between these high-priority traffic windows. To address this, we propose CNICcentric data transfer approach which is widely adopted in our production deployment: all data traffic in or out of GPU, including local H2D/D2H copy, must go through the GPUs paired CNIC with GPUDirect RDMA [33] data path. By consolidating all traffic onto the compute network, we can leverage the native QoS capabilities of compute network to enforce strict traffic differentiation."
        },
        {
            "title": "5.1 Traffic Isolation\nFor the InfiniBand-based network, we leverage virtual lanes\n(VLs) [5] to enforce isolation between different traffic classes.\nAll model inference communication traffic is assigned to a\ndedicated high-priority VL, while all other traffic, including\nKV-Cache transfer, is mapped to a separate low-priority VL.\nWe configure the VL arbiters of all network switches and\nNICs with a weighted round-robin policy that reserves ap-\nproximately 99% of total bandwidth to high-priority VL. The\nremaining bandwidth is allocated to the low-priority VL to\nprevent starvation. This configuration ensures that model\nexecution traffic is virtually unaffected by KV-cache trans-\nfers, while still allowing KV-cache traffic to opportunistically\nutilize otherwise idle bandwidth in the compute network.\nDetailed configurations are described in Â§A.1.",
            "content": "Although our experiments are conducted on an InfiniBandbased network, the same design principles naturally extend to other interconnect technologies. DualPath can be deployed on RDMA over Converged Ethernet (RoCE) by leveraging Traffic Class (TC) and Differentiated Services Code Point (DSCP) markings [6, 20] in conjunction with hardware packet queues. Emerging technologies such as UnifiedBus [1] and Ultra Ethernet [9] are likewise converging on QoS mechanisms for heterogeneous traffic, which can directly support the requirements of DualPath."
        },
        {
            "title": "5.2 CNIC-Assisted KV-Cache Copy\nExisting GPU data transfer technologies include GPUDirect\nStorage [34], which loads KV-Cache from storage backend\nto GPU HBM, and CUDA copy engine, which directly copies\nhost DRAM to GPU via PCIe. However, these methods fail to\nisolate KV-Cache traffic from high-priority latency-sensitive\ncollective communications in model execution, severely de-\ngrading inference performance.",
            "content": "To solve the limitations of existing approaches, we adopt CNIC-assisted H2D/D2H data path. For KV-Cache loading, we first read the KV-Cache into host DRAM from the storage backend. Then, we submit an RDMA Write request to the GPUs paired CNIC to perform local H2D copy. Storing newly generated KV-Cache follows symmetric process: it 7 is first transferred to host DRAM via CNIC, then persisted to the storage backend over the storage network. This design establishes the CNIC as the central QoS scheduler for all GPU PCIe traffic, allowing its VL arbiter to prioritize the inference communication traffic and perform KV-Cache transfer using spare PCIe bandwidth. Although this approach may appear to take detour compared to GPUDirect Storage (which directly reads KV-Cache to GPU HBM) and CUDA copy engine (which directly copies host memory to GPU HBM), to our best knowledge, this is currently the only practical method to ensure that KVCache load/store does not degrade the performance of critical modelexecution communication. We also observe that CNIC-assisted H2D and D2H outperform the CUDA copy engine when handling large number of small data chunks. Our measurements show that submitting single copy operation via cudaMemcpyAsync incurs latency overhead of approximately 5-7ğœ‡ğ‘ . We failed to further break down this overhead due to the closed-source nature of CUDA driver. In contrast, submitting one RDMA Write work request involves only few mmio writes to NIC registers in user space and takes only around 1ğœ‡ğ‘ . Furthermore, the RDMA work submission overhead can be significantly amortized by leveraging doorbell batching [25]."
        },
        {
            "title": "6.1 Inter-Engine Scheduling\nWe organize engines into groups to reduce the scheduler\npressure. Only the engine rank 0, called Leader Engine, in-\nteracts with the scheduler. All engines of a group are all PEs\nor all DEs. All engines on one node are guaranteed to be in\nthe same group. All engines in one group proactively fetch\ntasks together regularly. When fetching new requests, each\nengine ğ‘’ reports (1) ğ‘ ğ‘’ğ‘ğ‘’ , the number of requests assigned\nto it that have not yet completed; (2) the total token count\nğ‘¡ğ‘œğ‘˜ğ‘’ over those ğ‘ ğ‘’ğ‘ğ‘’ requests; and (3) the disk reading queue\nlength ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ) of the node ğ‘›(ğ‘’) that engine ğ‘’ belongs to.\nGPU load, disk read load, and network load are all strongly\ncorrelated with token count. We therefore use token count\nas a proxy and aim to balance it across engines.",
            "content": "Figure 5: An illustration of Inter-Engine PE Scheduling. All eight GPUs are in the same PE engine group and the scheduler will choose the best. PE Scheduling. All requests arriving at the scheduler enter waiting queue and are scheduled in FIFO order. The scheduling algorithm is invoked when PE group initiates fetch request. An illustration of the inter-engine scheduling process is shown in Figure 5. We define two constants, short reading queue threshold ğ›¼, and unfinished token upper limit ğ›½, measured in tokens. All engines are split into three categories: (1) overloaded engines where ğ‘¡ğ‘œğ‘˜ğ‘’ > ğ›½; (2) engines on nodes with short disk reading queues where ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ) ğ›¼ and ğ‘¡ğ‘œğ‘˜ğ‘’ ğ›½; and (3) engines on nodes with longer disk reading queues where ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ) > ğ›¼ and ğ‘¡ğ‘œğ‘˜ğ‘’ ğ›½. We do not assign new requests to overloaded engines. Second-category engines are prioritized over third-category engines because they reside on nodes with shorter disk reading queues, and lack of subsequent requests would easily lead to storage NIC underutilization. We assign the current request to the PE with minimum ğ‘¡ğ‘œğ‘˜ğ‘’ in the second category if non-empty, otherwise in the third category if non-empty. After assignment, we update the selected PEs ğ‘¡ğ‘œğ‘˜ğ‘’ , then proceed to the next request in the waiting queue. If both categories are empty, we terminate this fetch request and return the already-assigned requests to the Leader Engine. DE Scheduling Phase 1: across groups. DE scheduling is two-level and does not preserve global FIFO. There is global waiting queue and private queue per DE engine group. Incoming requests first enter the global queue. When DE group fetches, group-level scheduling drains the global queue and assigns each request to the group whose total ğ‘¡ğ‘œğ‘˜ğ‘’ (sum over its engines) is minimum; this balances token count across groups and thus NIC and GPU load. DE Scheduling Phase 2: within group. Then we calculate the sum of remaining HBM for all DEs in the group and traverse from the head of the private queue to calculate how many requests can be scheduled assuming no HBM fragmentation. These requests form the set ğ‘…. It is an upper bound that can be scheduled. Then, we calculate high token threshold ğ‘ = 1.05 ((cid:205)ğ‘Ÿ ğ‘… ğ‘™ğ‘’ğ‘›ğ‘Ÿ + (cid:205)ğ‘’ ğ¸ ğ‘¡ğ‘œğ‘˜ğ‘’ )/ğ¸. Next, we try to pop the head of private queue and schedule it to DE. Among DEs with sufficient remaining HBM for the Algorithm 1: Inter-PE Scheduling Algorithm Data: Waiting queue ğ‘„, PE group ğºğ‘ƒğ¸, load metrics (ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ), ğ‘¡ğ‘œğ‘˜ğ‘’ ) reported by each engine ğ‘’, where ğ‘›(ğ‘’) denotes the node that engine ğ‘’ belongs to, constants ğ›¼ and ğ›½ Result: Assigned requests to PEs Each engine ğ‘’ reports (ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ), ğ‘¡ğ‘œğ‘˜ğ‘’ ); Classify all PEs into three categories:; ğ¶1 {ğ‘’ ğºğ‘ƒğ¸ : ğ‘¡ğ‘œğ‘˜ğ‘’ > ğ›½}; ğ¶2 {ğ‘’ ğºğ‘ƒğ¸ : ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ) ğ›¼ ğ‘¡ğ‘œğ‘˜ğ‘’ ğ›½}; ğ¶3 {ğ‘’ ğºğ‘ƒğ¸ : ğ‘Ÿğ‘’ğ‘ğ‘‘_ğ‘ğ‘› (ğ‘’ ) > ğ›¼ ğ‘¡ğ‘œğ‘˜ğ‘’ ğ›½}; while ğ‘„ is not empty do ğ‘Ÿ head of ğ‘„; if ğ¶2 then ğ‘ğ‘’ arg minğ‘’ ğ¶2 ğ‘¡ğ‘œğ‘˜ğ‘’ ; else if ğ¶3 then ğ‘ğ‘’ arg minğ‘’ ğ¶3 ğ‘¡ğ‘œğ‘˜ğ‘’ ; else Terminate this fetch request; Return assigned requests to Leader Engine; break; end end Assign request ğ‘Ÿ to PE ğ‘ğ‘’; Update ğ‘¡ğ‘œğ‘˜ğ‘ğ‘’ ğ‘¡ğ‘œğ‘˜ğ‘ğ‘’ + tokens(ğ‘Ÿ ); Remove ğ‘Ÿ from ğ‘„; end request, we partition into (1) high-token DEs, where ğ‘¡ğ‘œğ‘˜ğ‘’ + len(ğ‘Ÿ ) > ğ‘ , and (2) the rest. We prefer category (2) to keep token count balanced; category (1) DEs already have higher GPU and NIC pressure. Within category (2) we choose the DE with minimum ğ‘ ğ‘’ğ‘ğ‘’ to balance request count; if category (2) is empty we choose the DE with minimum ğ‘¡ğ‘œğ‘˜ğ‘’ in category (1) to reduce HBM exhaustion and preemption risk. If no DE has sufficient HBM, the fetch ends and already-assigned requests are returned. KV-Cache Read Task Scheduling. After selecting PE and DE for request, we choose to read on the side with the shorter reading queue. It is probably better to split the request into two parts and read them from both sides, and we leave it as future work."
        },
        {
            "title": "6.2 Intra-Engine Scheduling\nOnly PEs require intra-engine scheduling, as DEs always\nplace all requests into the forward batch. An illustration of",
            "content": "8 Table 2: Statistics of agent trace datasets. MaxLen Turns Append Gen Total Context 32K 48K 64K 60 106 157 608 474 148 172 176 28639 42607 55958 17183 25120 32721 top of it. We adopt 3FS [13] as distributed storage and use an io_uring-like interface for kernel bypass."
        },
        {
            "title": "7.2 Experimental Setup\nTestbed. We conduct our experiments on a cluster of GPU\nservers with InfiniBand interconnection. Each server has 8\nNVIDIA Hopper GPUs and dual processors. Additionally,\neach node is provisioned with eight 400Gbps RDMA NICs\nconnected to InfiniBand network and one additional storage\nNIC connected to 3FS. The computation and storage net-\nworks are physically isolated. Our cluster-wide 3FS has no\ninternal DRAM cache and can saturate the 400Gbps band-\nwidth of the storage NIC.",
            "content": "Models. We evaluate on three models: (1) DeepSeek V3.2 [16] 660B, an MoE model with DeepSeek Sparse Attention, denoted as DS 660B, (2) 27B downscaled version of DS 660B, denoted as DS 27B, and (3) Qwen2.5-32B [42], dense model with GQA, denoted as Qwen 32B. DS 660B and Qwen 32B correspond to the publicly released checkpoint on HuggingFace. DS 27B is our internal experimental model with similar architecture to DS 660B. Detailed specifications are provided in A.2. Datasets. We collected three agent trace datasets from our production agentic RL training workloads with varying maximum context lengths (MaxLen). Each dataset contains 500 trajectories. The average interaction turns (Turns), average appended and generated tokens per turn (Append and Gen), average number of total tokens (Total), and average number of context tokens (Context) are summarized in Table 2. Baselines. We compare DualPath, denoted as Ours, against the following baselines: SGL(MC): SGLang [57] (commit 19089aa) with HiCache [40], Mooncake [38] Store enabled and 3FS as the storage backend, and Mooncake Transfer Engine for prefill-decode disaggregation. We did not run SGL(MC) for DS 27B because SGLang lacks support for this downscaled version. Basic: Our unmodified internal inference framework (detailed in 7.1). Comparing DualPath and SGL(MC) is unfair due to implementation differences. Therefore, we only report performance improvements from Basic to Ours. Oracle: Based on DualPath, we bypass all disk reads, D2H & H2D transfers, and inter-PD KV-Cache transfers. This Figure 6: Intra-Engine Schedule. Left: compute-quota-based batch selection. Right: GPU timeline before and after applying compute quota. the intra-engine scheduling process is shown in Figure 6. Data parallelism is widely adopted for attention layers, especially for MLA models. Under such parallel configuration, each GPU serves different set of requests. It may lead to workload imbalance among all GPUs that must synchronize after the attention stage and enter the FFN stage together, causing GPU bubbles waiting for other peers. Therefore, we need to make sure they have similar attention layer execution times to minimize the waiting bubbles. Layer Time Estimation. We use FIFO packing to decide how many requests to include in forward batch. Each request in forward batch is described by pair (ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘, ğ‘ğ‘ ğ‘§), where ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘ is the number of tokens with KV-Cache already available (from storage hits or previous forward passes), and ğ‘ğ‘ ğ‘§ is the number of tokens requiring KV-Cache computation in this forward batch. From these pairs, we compute the total theoretical computation for the attention layer and estimate its execution time. The relationship between theoretical computation and wall-clock time depends on hardware and parallel configuration, and can be fitted in advance through profiling as previous works [17] and [2]. Algorithm. We keep adding requests in FIFO order as long as the predicted attention layer execution time does not exceed predefined upper bound, called the compute quota. If adding request would exceed this bound, we perform binary search on ğ‘ğ‘ ğ‘§ to find smaller ğ‘ğ‘ ğ‘§ to fit in the remaining compute quota and perform chunked prefill for that request."
        },
        {
            "title": "7 EVALUATION\n7.1 Implementation\nWe implement DualPath based on our in-house inference\nframework. For CUDA kernels, our in-house framework\nadopt the combination of FlashMLA [27], DeepGEMM [14],\nand DeepEP [56], which aligns with the current mainstream\nopen-source framework [26, 57]. The DualPath implemen-\ntation involves approximately 5K lines of modifications on",
            "content": "9 Figure 7: Offline inference performance under varying numbers of agents and maximum agent context lengths. Top: DS 27B. Middle: DS 660B. Bottom: Qwen 32B. N/A for running into an error before finishing. configuration represents the theoretical performance upper bound assuming zero I/O overhead. P/D Ratio and Parallelism. We default to 2P4D for DS 660B, 1P2D for Qwen 32B, and 1P1D for DS 27B (where 1P1D means one node for each side). For DS models, we use EP and DP. For Qwen 32B, we use DP only in DualPath, while SGL(MC) uses TP=8 since DP attention is not supported for this model in SGLang. Detailed configuration is provided in A.4. Metrics. For offline inference scenarios, we measure job completion time (JCT) for the entire task. For online serving scenarios, we measure TTFT, TTST (Time to the second token), and TPOT."
        },
        {
            "title": "7.3 Offline Batch Inference\nThis section evaluates throughput performance in offline\nbatch inference, which is the case of the rollout phase in RL\ntraining. In this scenario, ğ‘› agents start to rollout simulta-\nneously, and we measure the JCT when all requests have\nfinished.",
            "content": "Varying Agents Batch Size & Max Agent Length (MAL). DualPath benefits more from larger batch sizes and longer MALs. Figure 7 reports JCT under different batch sizes and MALs. SGL(MC) encountered errors in our setup and failed to complete some large configurations (marked as N/A). On DS 660B, DualPath achieves up to 1.87 over Basic, and demonstrates performance with Oracle, indicating that KV-cache I/O is largely eliminated. On DS 27B, DualPath improves over Basic by up to 1.78 but remains 1.091.85 slower than Oracle due to limited storage bandwidth in 1P1D (Figure 8). For Qwen 32B, it shows similar trends as DS 27B. Varying Append Length & Generation Length. DualPath has more advantages when append and generation tokens are short. Longer append lengths imply greater GPU compute pressure, and longer generation lengths lower KV-Cache loading pressure due to larger prefill gap time. To investigate the impact of this factor, we scale each rounds append length by constant factor, and then truncate the whole trajectory at given MAL. The same holds for generation length. As shown in Figure 9, with append length increases, Basic performance gradually approaches DualPath and Oracle, while 10 Figure 8: Impact of prefill-decode ratio on offline inference performance (DS 27B). last round upon arrival. For our experiments, the SLO is set as TTFT 4 seconds and TPOT 50ms. In the TPOT and TTST figures, data points exceeding the SLO threshold are omitted. Experiment termination is triggered when either: (1) TTFT exceeds 4 seconds, or (2) the system reaches steady state, defined as TTFT variation within 150-second sliding window remaining below 5% compared to that 30 minutes prior. As shown in Figure 10, DualPath achieves higher APS capacity than Basic (1.67 for DS 27B, 2.25 for DS 660B). DualPaths TTST is comparable to Basic, while TPOT shows that DualPath does not introduce additional decoding overhead compared to Basic. SGL(MC) exhibits anomalously low TTST, likely due to implementation issues where the first two tokens arrive at the client almost simultaneously. For DS 27B, all metrics exhibit trends similar to DS 660B. However, both Basic and DualPath show significantly higher TPOT than Oracle, suggesting the overhead of basic P-D transferring is considerable in small model cases. We leave it as future work. Average JCT for both models are presented in Figure 11. detailed analysis of working set implications is discussed in 8. As shown in Figure 12 (left), DualPath maintains stable TTFT components across different APS, while Basics queuing time grows dramatically due to insufficient storage bandwidth."
        },
        {
            "title": "7.5 Ablation Study\nWe conduct an ablation study to quantify the contribution\nof each technical component in DualPath. Experiments are\nperformed under the offline inference setting with 64K MAL\nand agent batch size 1024 and 2048. The differences between\nBasic and Ours are grouped into three techniques: layerwise\nprefill, dual-path loading, and scheduling algorithm. We add\nthe techniques gradually to demonstrate individual contri-\nbution. As shown in Figure 12, compared to Basic, adding\nlayerwise prefill reduces JCT by 17.21% on average, allevi-\nating PE HBM bottlenecks and hiding transfer overhead.\nAdding Dual-path loading on top of layerwise prefill delivers",
            "content": "Figure 9: Left: varying append lengths (DS 660B, 64K context, 1024 agents). Right: varying generation lengths (DS 660B, 64K, 1024 agents) DualPath and Oracle performance changes only slightly, indicating that the bottleneck consistently lies in GPU compute pressure. Compared to Basic, DualPath achieves 1.82 1.99 speedup at different append scales. The trend for generation length scaling is similar. Varying Prefill-Decode Ratio. Across all ratios, DualPath demonstrates substantial performance gains compared to Basic. We conduct rollout experiments on DS 27B with 1P1D, 2P1D, and 1P2D prefill-decode ratios to characterize the impact of resource allocation between prefill and decode stages on overall system performance. As shown in Figure 8, DualPath achieves an average speedup of 1.64 across all configurations (up to 2.46). Basic 1P1D and Basic 1P2D perform comparably; so do DualPath 1P1D and Basic 2P1D, as well as DualPath 2P1D and DualPath 1P2D. This occurs because each pair of systems has equivalent available storage bandwidth (Basic can only use prefill node storage bandwidth, while DualPath can utilize all nodes), which confirms that storage bandwidth is the dominant bottleneck in agentic scenarios."
        },
        {
            "title": "7.4 Online Serving",
            "content": "Methodology. We evaluate system latency characteristics under varying agent arrival rates per second (APS). Agents arrive according to Poisson process at specified rate, with each agent commencing replay from round zero to its 11 Figure 10: TTFT, TTST, and TPOT as functions of agent arrival rate (APS). Shadow means the fluctuation in the last 150s before experiments finish. Top: DS 27B, Bottom: DS 660B. Figure 11: Average completion time of all trajectories versus arrival rate for online serving. Figure 13: Load balance of storage NICs traffic Figure 14: Load balance of attention execution time. Figure 12: Left (7.4): TTFT breakdown for online serving (DS 660B) across APSs, Sch. for scheduling, A. for allocating, R. for reading KV-cache, PF. for prefill. In each pair of pillars, the first is for DualPath and the second is for Basic. Right (7.5): Offline inference ablation results (DS 660B, 64K context length). Layer, DPL, Sched stands for Layerwise prefill, DualPath Loading, and scheduling, respectively. the primary performance gains, reducing JCT by 38.19% on average compared to Basic, as it enables requests to read KV-Cache from either PE or DE, fully utilizing distributed storage bandwidth. Finally, employing our scheduling algorithm on top of dual-path loading to decide KV-Cache 12 loading paths achieves the best performance, reducing JCT by 45.62% compared to Basic, demonstrating the effectiveness of load-balanced scheduling across storage NICs. Load Balance. DualPaths scheduling algorithm improves load balance for both storage NICs and attention layer execution times. For storage NICs, our scheduling algorithm improves load balance from 1.53 to 1.18 compared to round robin scheduling (Figure 13). For attention layers, DualPath maintains the Max/Avg ratio as low as 1.06 during the first 5% of the task, reducing GPU idle bubbles (Figure 14). The storage NIC metric is the ratio of maximum to average traffic across all storage NICs on three machines within small time window, where 1.0 represents perfect balance. The attention Table 3: Large-scale experiment results. Setting JCT TTFT TTST TPOT 2P4D, 2K agents ffl 3,167s 48P96D, 48K agents 3,201s l 2P4D, 0.4 APS 44P88D, 8.8 APS 1.739s 0.228s 0.039s 1.847s 0.194s 0.036s Figure 15: 48P96D offline inference metrics. 1e7 is the scaling factor of Prompt TPS. layer metric is calculated among all GPUs in an expert parallel group for each forward. As the task progresses, both ratios become meaningless due to underloaded system. Therefore, we do not show the tail phase of the workload."
        },
        {
            "title": "7.6 Large-Scale Scalability\nWe conduct both offline and online experiments using up\nto 1,152 GPUs to demonstrate production-level scalability\n(Table 3). For offline inference, scaling from 2P4D (2K agents)\nto 48P96D (48K agents) achieves near-linear speedup with\ncomparable JCT (3,167s vs. 3,201s). For online serving, the\n44P88D configuration achieves 22Ã— throughput (8.8 vs. 0.4\nAPS) while maintaining similar latency. Across all experi-\nments, scheduler CPU usage remains below 10 cores, con-\nfirming it is not a bottleneck. Some detailed metrics over\noffline inference process are shown in Figure 15.",
            "content": "Due to the lack of fine-tuned parallelism settings and P/D ratios (which requires substantial experimentation budget), the large-scale experiments do not demonstrate additional JCT or serving capacity gains compared to multiple smallscale units with equivalent cost. However, large-scale deployment remains important for the following reasons. First, it reduces fragmentation and provides greater flexibility for fine-tuning parallelism and P/D ratios. Second, large-scale deployment offers more scheduling opportunities to mitigate queuing latency under unpredictable bursty online requests. These observations suggest several directions for future work (8.1)."
        },
        {
            "title": "8.2 Working Set Analysis\nAs shown in Figure 11, given an arrival rate ğœ† (i.e., new tra-\njectories per second) and mean JCT Â¯ğ‘‡ , the working set of the\nKV-Cache can thus be approximated as ğœ† Â¯ğ‘‡ Ã— ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™_ğ‘™ğ‘’ğ‘›ğ‘ğ‘£ğ‘”/2.\nIn our setting of DS 660B serving, this value of DualPath\nranges from 69 GB at APS 0.1 to 681 GB at APS 0.45.",
            "content": "In production, the working set would be larger since our evaluation assumes zero inter-arrival time and zero tool call latency. If JCT increases by ğ‘Ÿ times due to these gaps, the systems APS capacity increases by ğ‘Ÿ times (gaps do not stress LLM inference), causing the working set to expand by ğ‘Ÿ 2 times. This would exceed available memory and reduce the distributed memory pools hit rate. Such experiments require ğ‘Ÿ times more machine hours and ğ‘Ÿ 2 times more storage (cost scaling as ğ‘Ÿ 3), which we cannot afford given limited resources."
        },
        {
            "title": "9 RELATED WORK\nDistributed Memory Cache Pools. Mooncake [38] builds a\ndistributed DRAM pool for KV-Cache. TokenLake [46] intro-\nduces a unified segment-level prefix cache pool. Compared\nto them, DualPath targets storage backend directly, balanc-\ning the traffic among all SNICs, and reduces DRAM usage\ngreatly without harming performance. DualPath can also be\ncombined with a middle DRAM cache, but the performance\ngain is marginal.",
            "content": "KV-Cache I/O Optimization. Efficiently loading the massive KV-Cache from other caching tiers is fundamental bottleneck in disaggregated LLM serving architectures. Prior work has approached this problem primarily from the perspective of single data path. Strata [49] tackles I/O bottlenecks in hierarchical storage by co-designing GPU-assisted I/O with cache-aware scheduling. Other work, like KVPR [23] and TailorKV [53], mitigates bandwidth constraints (e.g., PCIe) on this path via recomputation overlapping and layergranular hybrid quantization. LLM Inference System. Recent years have seen many inference acceleration techniques, such as paged attention [26], chunked prefill, and hybrid batching [2, 21]. Prefill-decode disaggregated inference [37, 58] separates the prefill and decode stages onto different GPUs, reducing performance interference between them and allowing each stage to adopt distinct parallel strategies and hardware configurations, which unlocks substantial optimization opportunities. It has largely become the de facto standard for inference serving. Attention Mechanisms. Attention mechanism allows tokens to interact with previous tokens in the sequence. There are many variants such as Multi-Head Attention (MHA) [44], Multi-Query Attention (MQA) [41] and Grouped-Query Attention (GQA) [3], Multi-head Latent Attention (MLA) [12]. For those attention mechanisms (denoted as dense attention), the ratio of computation and KV-Cache size for one token is constant since both scale linearly with sequence length."
        },
        {
            "title": "10 CONCLUSION\nThis paper presents DualPath, an agentic LLM inference\nframework that addresses the imbalance of KV-Cache read-\ning under PD-disaggregated architecture through dual-path\nKV-Cache loading. By redistributing storage network load\nwith workload-aware scheduling, DualPath achieves up to\n1.87Ã— throughput improvement for offline inference. It also\nachieves 1.96Ã— higher agent runs per second on average in\nonline serving.",
            "content": "REFERENCES [1] 2026. UnifiedBus. https://www.unifiedbus.com/en. (2026). [2] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 117134. [3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 48954901. [4] Anthropic. 2026. Introducing Claude Opus 4.6. https://www.anthropic. com/news/claude-opus-4-6. (2026). [5] InfiniBand Trade Association. 2007. InfiniBand Architecture Specification Volume 1, Release 1.2.1. (2007). [6] Brian E. Carpenter and Kathleen M. Nichols. 2002. Differentiated services in the Internet. Proc. IEEE 90 (2002), 14791494. https://api. semanticscholar.org/CorpusID:1723205 [7] Qiaoling Chen, Zhisheng Ye, Tian Tang, Peng Sun, Boyu Tian, Guoteng Wang, Shenggui Li, Yonggang Wen, Zhenhua Han, and Tianwei Zhang. 2026. CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control. (2026). arXiv:cs.DC/2601.22705 https://arxiv.org/abs/2601.22705 14 [8] Sadia Sultana Chowa, Riasad Alvi, Subhey Sadi Rahman, Md Abdur Rahman, Mohaimenul Azam Khan Raiaan, Md Rafiqul Islam, Mukhtar Hussain, and Sami Azam. 2026. From language to action: review of large language models as autonomous agents and tool users. Artificial Intelligence Review (2026). [9] Ultra Ethernet Consortium. 2026. Ultra Ethernet Specification v1.0.2. (2026). [10] Tri Dao. 2024. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In International Conference on Learning Representations. 3554935562. [11] Google DeepMind. 2026. Gemini 3 Pro. https://deepmind.google/ models/gemini/pro/. (2026). [12] DeepSeek-AI. 2024. DeepSeek-V2: Strong, Economical, and Efficient Mixture-of-Experts Language Model. (2024). arXiv:2405.04434 [13] DeepSeek-AI. 2025. 3FS. https://github.com/deepseek-ai/3FS. (2025). [14] DeepSeek-AI. 2025. DeepGEMM. https://github.com/deepseek-ai/ DeepGEMM. (2025). [15] DeepSeek-AI. 2025. DeepSeek-V3 Technical Report. (2025). arXiv:cs.CL/2412.19437 https://arxiv.org/abs/2412.19437 [16] DeepSeek-AI. 2025. DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models. (2025). arXiv:2512.02556 [17] Kuntai Du, Bowen Wang, Chen Zhang, Yiming Cheng, Qing Lan, Hejian Sang, Yihua Cheng, Jiayi Yao, Xiaoxuan Liu, Yifan Qiao, Ion Stoica, and Junchen Jiang. 2025. PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications. In Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles (SOSP 25). 399414. [18] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. CostEfficient Large Language Model Serving for Multi-turn Conversations with CachedAttention. In 2024 USENIX Annual Technical Conference (USENIX ATC 24). 111126. [19] Shiwei Gao, Youmin Chen, and Jiwu Shu. 2025. Fast State Restoration in LLM Serving with HCache. In Proceedings of the 20th European Conference on Computer Systems. 128143. [20] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye, and Marina Lipshteyn. 2016. RDMA over Commodity Ethernet at Scale. In Proceedings of the 2016 ACM SIGCOMM Conference (SIGCOMM 16). Association for Computing Machinery, New York, NY, USA, 202215. https://doi.org/10.1145/2934872.2934908 [21] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. 2024. DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. (2024). arXiv:2401.08671 [22] Yifan Hu, Shi Qiu, Jianqin Yan, Hao Chen, Xintao Wang, Tang Lu, Guangtao Xue, and Yiming Zhang. 2025. TARDIS: GPU-Centric KV Cache Service for Efficient LLM Inference. In Proceedings of the 16th ACM SIGOPS Asia-Pacific Workshop on Systems (APSys 25). Association for Computing Machinery, New York, NY, USA, 4653. https://doi. org/10.1145/3725783.3764393 [23] Chaoyi Jiang, Lei Gao, Hossein Entezari Zarch, and Murali Annavaram. 2025. KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation. In Findings of the Association for Computational Linguistics: ACL 2025. 1947419488. [24] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. ACM Transactions on Software Engineering and Methodology (2024). [25] Anuj Kalia, Michael Kaminsky, and David Andersen. 2016. Design guidelines for high performance RDMA systems. In 2016 USENIX annual technical conference (USENIX ATC 16). 437450. [26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles. 611626. [27] Jiashi Li and Shengyu Liu. 2025. FlashMLA: Efficient Multi-head Latent Attention Kernels. https://github.com/deepseek-ai/FlashMLA. (2025). [28] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. 2024. Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. (2024). arXiv:cs.HC/2401.05459 https://arxiv.org/abs/2401.05459 [29] Weizhe Lin, Hui-Ling Zhen, Shuai Yang, Xian Wang, Renxi Liu, Hanting Chen, Wangze Zhang, Chuansai Zhou, Yiming Li, Chen Chen, Xing Li, Zhiyuan Yang, Xiaosong Li, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan, and Yunhe Wang. 2025. Towards Efficient Agents: Co-Design of Inference Architecture and System. (2025). arXiv:cs.CL/2512.18337 https://arxiv.org/abs/2512.18337 [30] Yuhan Liu, Yihua Cheng, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Rui Zhang, Kuntai Du, and Junchen Jiang. 2025. LMCache: An Efficient KV Cache Layer for EnterpriseScale LLM Inference. (2025). arXiv:2510.09665 [31] Mahmoud Mohammadi, Yipeng Li, Jane Lo, and Wendy Yip. 2025. Evaluation and benchmarking of llm agents: survey. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 61296139. [32] NVIDIA. 2023. SuperPOD: Next Generation Scalable Infrastructure for AI Leadership. http://docs.nvidia.com/https:/docs.nvidia.com/ dgx-superpod-reference-architecture-dgx-h100.pdf. (2023). [33] NVIDIA. 2026. Developing Linux Kernel Module using GPUDirect RDMA. https://docs.nvidia.com/cuda/gpudirect-rdma/index.html. (2026). [34] NVIDIA. 2026. GPUDirect Storage Overview Guide. https://docs. nvidia.com/gpudirect-storage/overview-guide/index.html. (2026). [35] OpenAI. 2025. gpt-oss-120b & gpt-oss-20b Model Card. (2025). arXiv:2508. [36] OpenAI. 2025. Introducing GPT-5.2. https://openai.com/index/ introducing-gpt-5-2/. (2025). [37] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, ÃÃ±igo Goiri, Saeed Maleki, and Ricardo Bianchini. 2025. Splitwise: Efficient Generative LLM Inference Using Phase Splitting. In Proceedings of the 51st Annual International Symposium on Computer Architecture. 118132. [38] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. 2025. Mooncake: Trading More Storage for Less Computation KVCache-centric Architecture for Serving LLM Chatbot. In Proceedings of the 23rd USENIX Conference on File and Storage Technologies. 155170. [39] Andre Richter, Christian Herber, Thomas Wild, and Andreas Herkersdorf. 2016. Resolving Performance Interference in SR-IOV Setups with PCIe Quality-of-Service Extensions. In 2016 Euromicro Conference on Digital System Design (DSD). 454462. https://doi.org/10.1109/DSD. 2016.41 [40] SGLang. 2026. SGLang HiCache. https://docs.sglang.io/advanced_ features/hicache.html. (2026). [41] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. (2019). arXiv:1911.02150 [42] Qwen Team. 2025. Qwen2.5 Technical Report. (2025). arXiv:2412.15115 (2025). [43] Qwen Team. 2025. Qwen3 Technical Report. arXiv:cs.CL/2505.09388 https://arxiv.org/abs/2505.09388 15 [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the 31st International Conference on Neural Information Processing Systems. 60006010. [45] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. survey on large language model based autonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345. [46] Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, and Xin Jin. 2025. TokenLake: Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving. (2025). arXiv:cs.DC/2508.17219 https://arxiv.org/abs/2508.17219 [47] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. (2023). arXiv:cs.AI/2308.08155 https://arxiv.org/abs/2308.08155 [48] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2025. The rise and potential of large language model based agents: survey. Science China Information Sciences 68, 2 (2025), 121101. [49] Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, and Christos Kozyrakis. 2025. Strata: Hierarchical Context Caching for Long Context Language Model Serving. (2025). arXiv:2508.18572 [50] Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, and Zhenxuan Pan. 2024. LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management. (2024). arXiv:2410. [51] Jianqin Yan, Shi Qiu, Yina Lv, Yifan Hu, Hao Chen, Zhirong Shen, Xin Yao, Renhai Chen, Jiwu Shu, Gong Zhang, and Yiming Zhang. 2025. Phoenix: Refactored I/O Stack for GPU Direct Storage without Phony Buffers. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC 25). 12671283. [52] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems 37 (2024), 5052850652. [53] Dingyu Yao, Bowen Shen, Zheng Lin, Wei Liu, Jian Luan, Bin Wang, and Weiping Wang. 2025. TailorKV: Hybrid Framework for LongContext Inference via Tailored KV Cache Optimization. In Findings of the Association for Computational Linguistics: ACL 2025. 2034020359. [54] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. 2025. FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving. (2025). arXiv:2501.01005 [55] Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Huazuo Gao, Jiashi Li, Liyue Zhang, Panpan Huang, Shangyan Zhou, Shirong Ma, et al. 2025. Insights into deepseek-v3: Scaling challenges and reflections on hardware for ai architectures. In Proceedings of the 52nd Annual International Symposium on Computer Architecture. 17311745. [56] Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. 2025. DeepEP: an efficient expert-parallel communication library. https://github.com/ deepseek-ai/DeepEP. (2025). [57] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. SGLang: Efficient Execution of Structured Language Model Programs. In Proceedings of the 38th International Conference on Neural Information Processing Systems. Article 2000, 27 pages. [58] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 193210. [59] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 (2023). APPENDIX A.1 Traffic Isolation Configuration Details InfiniBand. The InfiniBand QoS mechanism employs two arbitrators: high-priority and low-priority. Traffic is scheduled using Weighted Round Robin (WRR) in the high-priority arbitrator, then steered to the low-priority arbitrator according to qos_high_limit; setting it to 255 disables the lowpriority arbitrator entirely. Detailed scheduling algorithm can be found in [5]. Our configuration: qos_max_vls 4 qos_high_limit 240 qos_vlarb_high 0:192,1:192,2:0,3:192 qos_vlarb_low 0:192,1:192,2:64,3:192 RoCE. RoCE enforces QoS through DSCP-based traffic classification and hardware traffic classes (TC). Packets are first mapped from DSCP values to TCs, each backed by dedicated hardware queue on NICs and switches (typically up to eight). To match the four-VL configuration in InfiniBand, we configure four lossless RDMA TCs with Priority Flow Control (PFC) enabled. Bandwidth isolation is achieved by assigning proportional scheduling weights to these TCs on both NICs and switches, reserving the majority of bandwidth for model inference traffic while allocating small fraction to KV-cache traffic to prevent starvation. A.2 27B Model Specifications In terms of overall model scale, the hidden dimension 2560, the intermediate size of dense layers is 12288, the number of hidden layers is 30, the number of attention heads is 32, the number of routed experts is 72, the MoE intermediate size is 1536, the number of activated experts per token is 6, the number of shared experts is 2, and the number of initial dense layer is 1. Regarding the index attention mechanism, the number of attention heads is 32, the head dimension is 64, the topk tokens for sparse attention is 1024. The LoRA compression for the matrix of both indexer and main attention is removed. 16 A.3 Agent Task Structure To provide context for the dataset characteristics, we briefly describe the agent task structure, though this background is orthogonal to our system design. Each agent operates within sandbox environment containing code repository with known bugs and associated error messages. The agent is instructed via prompt to diagnose and fix the bug. The model possesses tool-use capabilities, invoking bash commands in the sandbox by emitting structured outputs. The agent and environment engage in multi-turn interactions, where each turn consists of prompt (previous context concatenated with new information, most of which is tool output) and the model generating subsequent tool invocation by decoding. Each trajectory is sequence of rounds; round ğ‘– consists of appended tokens ğ´ğ‘– and the number of generated tokens ğ‘”ğ‘– . We use ğºğ‘– to indicate the tokens generated in round ğ‘–, which are not presented in our dataset. We define ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘–+1 as the concatenated list of ğ´1, ğº1, ğ´2, ğº2, ..., ğ´ğ‘–, ğºğ‘– . In round ğ‘– + 1 of our replay, the agent concatenates the prompt as ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘–+1 + ğ´ğ‘–+1, and then sets proper sampling parameters to ensure it generates exactly ğ‘”ğ‘–+1 tokens, i.e., ğºğ‘–+1. To generate additional agent trajectories, we sample an existing trajectory and prepend synthetic round with random tokens as ğ´1 and ğ‘”1 = 1. A.4 Experimental Configurations Configuration Parameters. For DeepSeek models, DualPath allocates 80GB DRAM each node, and SGL(MC) uses totally 1.5TB DRAM on every node. For Qwen 32B, due to the larger KV-Cache, DualPath allocates 320GB. Speculative decoding is disabled for all settings. We use 3FS as the storage backend for all configurations. The short reading queue threshold ğ›¼ (described in 6) is set to the number of tokens we can read during 3 seconds, and the unfinished token upper limit ğ›½ is set to the number of tokens one GPU can process for 5 seconds. Those values are profiled in advance. Compute Quota Threshold is set to 300ms for all DualPath and Oracle baselines. KV-Cache Hit Length Calculation. For all systems except SGL(MC), we limit the KV-Cache hits to only occur within trajectory, and the hit length is calculated in the client because no eviction is needed. For SGL(MC), hit lengths are computed internally based on HiCache and Mooncake Store cache states. A.5 KV-Cache Block Layout Layerwise prefill reduces KV-Cache block size to 1/ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ of the original, and makes the number of blocks larger to ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ , posing challenges to transfer and storage performance. To overcome this, we design two distinct block types: Layer Block and Full Block. Layer Block is byte tensor with shape [1, ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ , ğ‘ğ‘¦ğ‘¡ğ‘’ğ‘ ] and stores one-layer KV-Cache for some tokens. The number of tokens is called ğ‘ğ‘™ğ‘œğ‘ğ‘˜_ğ‘ ğ‘–ğ‘§ğ‘’. ğ‘ğ‘¦ğ‘¡ğ‘’ğ‘  indicates the cache bytes needed per layer per token. Meanwhile, Full Block has shape [ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ, ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ , ğ‘ğ‘¦ğ‘¡ğ‘’ğ‘ ]. This design enables us to avoid manual KV-Cache memory layout conversion throughout inference by simply concatenating ğ‘› Layer Blocks to yield Full Block. KV-Cache is stored in distributed storage using trie structure, where each tree node corresponds to Full Block."
        }
    ],
    "affiliations": [
        "DeepSeek-AI",
        "School of Computer Science, Peking University",
        "Tsinghua University"
    ]
}