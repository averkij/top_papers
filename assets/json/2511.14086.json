{
    "paper_title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
    "authors": [
        "Yue Zhang",
        "Zun Wang",
        "Han Lin",
        "Jialu Li",
        "Jianing Yang",
        "Yonatan Bitton",
        "Idan Szpektor",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs."
        },
        {
            "title": "Start",
            "content": "Error-Driven Scene Editing for 3D Grounding in Large Language Models Yue Zhang1 Zun Wang1 Han Lin1 Jialu Li1 Jianing Yang2 Yonatan Bitton3 1UNC Chapel Hill Idan Szpektor3 Mohit Bansal1 2University of Michigan 3Google Research 5 2 0 2 8 1 ] . [ 1 6 8 0 4 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or largescale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following structured Decompose, Diagnostic Evaluation, Edit, and Re-train workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying grounding failure of the 3DLLM, our framework first diagnoses the exact predicatelevel error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs. Our code 1 is publicly available. 1. Introduction Grounding language in 3D environments, identifying the specific objects, attributes, and spatial relations that description refers to, is fundamental for embodied AI and robot manipulation [12, 20, 33, 61]. Recent efforts have in1https://github.com/zhangyuejoslin/Deer-3D tegrated large language models (LLMs) with 3D perception, giving rise to so-called 3D-LLMs [13, 19, 21, 22, 31, 48, 50, 62]. These models have demonstrated impressive progress in open-vocabulary captioning, question answering, and 3D scene reasoning, largely benefiting from the strong linguistic capabilities of LLMs. However, despite these achievements, the grounding performance of current 3D-LLMs remains limited, failing to localize fine-grained visual details (e.g., confusing two pillows of different colors), misinterpreting spatial relations (e.g., confusing far from as near), or rely on language priors rather than geometric evidence (e.g., grounding pillow on the bed even when the instruction describes pillow on the floor) [23, 59]. This gap highlights that linguistic proficiency alone does not guarantee spatially grounded understanding. Understanding why current 3D-LLMs struggle with grounding requires examining the biases and errors in the models training. Due to the scarcity of large-scale 3D resources, existing datasets often contain latent statistical biases that models exploit as shortcuts [59]. Fig. 2 illustrates an example that over half of the spatial relationships between lamp and pillow are categorized as near, and most pillows are labeled as white. Consequently, models learn superficial associations (e.g., white pillows are near lamps) rather than genuine geometric relationships, leading to failures such as incorrectly grounding green pillow far from the lamp. However, current approaches for improving 3D-LLMs primarily focus on text-based augmentations [22, 24, 31, 62], which inherently cannot resolve such visual biases. While such strategies improve general linguistic understanding, they leave the underlying 3D scenes unchanged and fail to introduce the necessary visual variations to challenge these spurious co-occurrence patterns, and can even exacerbate them by reinforcing the statistical priors. Moreover, conventional augmentations are typically applied blindly, without explicitly targeting the models known grounding errors, resulting in inefficient and ineffective training where the model repeatedly encounters data irrelevant to its weaknesses. To address these limitations, we propose DEER-3D (Decompose, Diagnostic Evaluation, Edit, and Retrain), ground-truth object; incorrectly predicted object; edited object; referent object. Figure 1. 3D-LLMs frequently overfit to dataset co-occurrence biases (e.g., white pillows), causing grounding failures. We mitigate these biases through targeted counterfactual edits in 3D scenes and construct aligned QA pairs to strengthen the models grounding ability. closed-loop framework that introduces visual and spatial variability into 3D-LLMs training through targeted scene editing based on the models failures. DEER-3D aims to bridge the gap between linguistic understanding and geometric grounding by exposing models to explicit counterfactual changes in 3D object attributes and spatial relations. Instead of heavily relying on textual data, the framework enhances supervision directly in the visual domain by modifying scenes in minimal yet meaningful way, such as recoloring or repositioning objects. Importantly, these edits are guided by the models own failure patterns, ensuring that the introduced variability is both relevant and aligned with real grounding errors. Through iterative retraining on these visually augmented examples, DEER-3D strengthens spatial reasoning and encourages models to ground language in visual evidence rather than linguistic shortcuts. Specifically, for specific grounding failure, such as incorrectly identifying bounding box given descripthe DEER-3D framework systematically proceeds tion, In the Decompose through three stages (see Fig. 2). stage (Fig. 2(a)), DEER-3D breaks down the naturallanguage query into atomic predicates that separately deIn the Discribe texture attributes and spatial relations. agnose stage (Fig. 2(b)), it leverages the open-vocabulary reasoning capability of 3D-LLM to precisely pinpoint predicate-level errors, identifying which semantic factor (e.g., color, orientation, or distance) caused the failure. In the subsequent Edit stage (Fig. 2(c)), DEER-3D performs duplicate-and-replace operation: it duplicates the groundtruth object geometry and substitutes nearby distractor with this duplicate, ensuring that only the target attribute is edited while all other factors remain fixed. minimal visual modification, such as recoloring or rotating the duplicate, creates counterfactual pair that explicitly isolates the erroneous appearance or spatial relation, providing finegrained supervision signals. We further pair these edited scenes with corresponding question-answer pairs that capture different levels of reasoning difficulty. Finally, in the Retrain stage (Fig. 2(d)), these targeted counterfactual examples are integrated back into the training set. Through iterative refinement, the model learns by iteratively correcting its own errors, progressively improving its robustness and predicate-specific grounding accuracy. We extensively evaluate DEER-3D on established 3D visual grounding benchmarks and observe substantial improvements across all metrics (e.g., achieving 4-5% gains in grounding accuracy), clearly demonstrating its effectiveness in resolving fine-grained grounding errors. Crucially, we validate the benefit of our iterative refinement framework by conducting multi-round experiments, illustrating how progressively targeting model failures further boosts performance. Comprehensive ablation studies confirm the effectiveness of our targeted visual editing strategies, the importance of error-driven counterfactual augmentation, and the impact of different question design choices. Additionally, we evaluate DEER-3Ds broader utility beyond grounding, demonstrating consistent improvements on general 3D tasks, as well as on human-aligned grounding evaluations. Taken together, these experiments underscore the effectiveness and robustness of our approach in enhancing the grounding capabilities of 3D-LLMs. 2. Related Work 3D-LLMs for Grounding. Recent advances in 3D-LLMs have greatly expanded the paradigm of 3D grounding by coupling LLMs with 3D visual perceptions, enabling openvocabulary understanding and spatial reasoning in 3D scene envrionments [10, 13, 19, 22, 24, 30, 48, 56, 57, 62, 64, 66]. Despite their impressive reasoning ability, current 3D-LLMs often suffer from visual bias, relying on linguistic priors rather than genuine 3D evidence when making predictions. primary reason for this vulnerability is ground-truth object; incorrectly predicted object; edited object; referent object. error detected Figure 2. Overview of DEER-3D. When grounding error is detected, DEER-3D performs targeted visual edits. Given natural-language instruction, the framework (a) decomposes it into atomic predicates, (b) diagnoses the specific error, and (c) applies predicate-level visual edits. Aligned questionanswer pairs are then created to explicitly supervise the failed predicate. Finally, the model (d) iteratively retrains on these counterfactual examples to progressively improve grounding accuracy. the communitys reliance mainly on text-based augmentation to compensate for data scarcity. Methods such as rephrasing, caption generation, or synthetic instruction creation [22, 31, 62] improve linguistic diversity but leave the underlying 3D visual data unchanged. This approach not only fails to correct existing dataset biases but can inadvertently exacerbate them by reinforcing the models reliance on spurious linguistic correlations. In contrast, we address this limitation from the visual side through an error-driven visual augmentation framework that explicitly edits 3D scenes to generate counterfactual pairs targeting the exact predicates responsible for grounding failures, providing fine-grained supervision to correct visual bias. Counterfactual Augmentation. Counterfactual data augmentation has been widely adopted across multiple tasks to enhance model robustness, fairness, and causal understanding. It has been applied for debiasing and improving generalization through minimal text rewrites in NLP [17, 25, 34, 68], disentangling causal visual factors via counterfactual image synthesis in vision [35, 38, 43], improving seen-unseen gap in robotics [16, 29, 37, 46], and strengthening compositional and causal reasoning in multimodal tasks such as visual question answering [8, 27, 41]. However, most existing efforts remain limited to 2D or textual domains. These methods are fundamentally insufficient for the unique challenges of 3D grounding, as they are not designed to perform targeted, predicate-level interventions on complex 3D geometric properties such as object orientation or distance. In contrast, we introduce model-diagnosed, visually grounded counterfactual editing in 3D scenes to improve grounding ability. Error-Driving Learning. Error-driven and self-corrective learning has been applied in NLP [2, 26, 45, 54, 58], multimodal models [47, 55], and robotics [14], where models improve by learning from corrected outputs [2, 42, 51] or retraining on targeted failure cases [1, 14, 47]. While prior work relies on textual feedback or task-level supervision, our framework instead performs instance-level correction through an iterative 3D visual-domain loop that identifies grounding failures and applies minimal scene edits to progressively enhance spatial understanding in 3D-LLMs. 3D Scene Editing. Although recent 3D scene editing methods [18, 28, 40, 49, 52, 67] enable high-fidelity and interactive object manipulation, their design goals differ fundamentally from the requirements of our counterfactual training pipeline. These systems prioritize photorealistic rendering and user-driven editing, rather than producing semantically controlled, single-factor modifications that preserve all other scene attributes. In contrast, DEER-3D requires large-scale, reproducible, and predicate-isolated interventions, while keeping the rest of the scene unchanged. 3. Method In this section, we introduce our approach, which systematically identifies and corrects grounding errors through targeted counterfactual scene editing. As illustrated in Fig. 2, our framework is structured as follows: we first describe our baseline model (Sec. 3.1). Next, we present the core components of our pipeline, beginning with the decomposition of complex instructions into simpler, atomic sub-descriptions, each capturing distinct semantic factor (Sec. 3.2). We then introduce diagnostic evaluator that pinpoints the models specific reasoning failures related to these semantic factors (Sec. 3.3).Then, we detail our scene-editing strategy (Sec. 3.4), which generates controlled counterfactual 3D scenes along with aligned question-answer pairs. Finally, we describe how the base model is iteratively fine-tuned using the collected counterfactual data (Sec. 3.5), progressively enhancing its grounding accuracy. 3.1. Base 3D-LLM Model We build our pipeline upon 3D-LLM, denoted as Mbase, which serves as our initial grounding model. Specifically, we utilize ChatScene [21] as the backbone. In the grounding task, the base model receives inputs consisting of 3D object points with color and instance segmentation obtained by 3D segmentor [39], along with natural language instruction describing target object. The model processes these inputs to produce prediction Opred (a 3D bounding box) that localizes the object specified by the instruction: Opred = Mbase(S, ), This initial prediction Opred is then evaluated against the ground truth. We detail our error-driven analysis framework in the following sections. 3.2. Instruction Decomposition Our DEER-3D framework is activated only when the base models prediction Opred is incorrect. To precisely diagnose errors, we first decompose complex, free-form instructions into their atomic components (sub-instruction). As shown in Fig. 2(a), we employ large language model (e.g., Qwen3 [53]) to systematically parse into set of atomic predicates, denoted as = {p1, p2, , pn}. This decomposition is crucial, as it isolates potential error sources to individual semantic elements of the original instruction. For example, given the compositional instruction = the brown couch against the wall, the parser typically outputs atomic predicates capturing distinct semantic components such as object appearance and spatial relations (e.g., p1 = the brown couch, p2 = the couch is against the wall. The set of atomic predicates is then forwarded to our diagnostic evaluator introduced as follows. Detailed prompts used for this decomposition are provided in Appendix A.1. 3.3. Diagnostic Evaluator Given the set of atomic predicates = {p1, ..., pn}, the goal of the diagnostic evaluator is to pinpoint which specific predicate pi the model failed to ground (shown in Fig. 2(b)). From empirical observations, we identified two primary types of visual grounding errors. detailed analysis of these error categories and their distributions is provided in Appendix A.2, where we show that these two types collectively account for over +75% of all errors in our experiments. We categorize primary error types as follows: Appearance Grounding: Descriptions related to visual appearance, particularly textures and colors (e.g., brown/wooden couch). Spatial Grounding: Descriptions specifying the spatial context, including orientation (e.g., against the wall), distance (e.g., near the ottoman), or relative positioning (e.g., below the picture). To perform this diagnosis, each atomic sub-instruction (predicate) pi is individually queried against the model Mbase. The models task is to generate set of candidate object IDs {Ocand} that satisfy the specific predicate pi. We then check if the ground-truth object Ogt is included in this set: Ogt {Ocand}. If Ogt / {Ocand}, we identify pi as grounding failure. Finally, the identified error type determines which counterfactual editing strategy to apply in the subsequent augmentation stage. 3.4. Error-Driven Counterfactural Augmentation Our method generates 3D counterfactual scenes through precise, error-specific modifications. Specifically, as shown in Fig. 2(c), when visual grounding error is detected, our goal is to isolate the semantic factor responsible for the error while keeping all other visual and geometric properties constant. To achieve this, we construct controlled counterfactual scenes that differ from the original scene along only one visual predicate. This enables controlled supervision so that the model must correctly attend to the changed attribute to succeed. Our editing framework follows unified CloneReplaceModify procedure: Clone: Let ogt be the ground-truth object. We first create oclone, perfect duplicate of ogt with identical geometry, material, and texture properties. Replace: Let od be distractor object, strategically selected based on the diagnosed error type to create challenging counterfactual example. We remove od from the scene and place oclone at the exact 3D position, Td, previously occupied by od. Formally, this operation is simply: orelocated clone = Translate(oclone, Td), Figure 3. Examples of targeted visual edits for different grounding errors. Each edit generates aligned questionanswer pairs, precisely supervising the models visual grounding ability. ground-truth; edited; referent. Modify: We apply predicate-specific modification to the newly placed oclone. This modification directly corresponds to the diagnosed error type that caused the failure. This unified structure allows the same editing principle to address different visual error categories. In the following, we describe the specific editing operations designed for each visual error type. 3.4.1. Error-Specific Edits While our unified CloneReplaceModify framework provides general mechanism for controlled counterfactual editing, the Replace and the Modify step varies according to the semantic predicate causing the grounding error. We provided an example of targeted visual editing for different grounding errors in Fig. 3. CloneReplaceRecolor (CR-Rec) for Appearance Errors. For appearance-based grounding errors (e.g., incorrect color identification), we first select nearby distractor object od that shares the same semantic category as the ground-truth object ogt (e.g., coffee table <OBJ002> in Fig. 3(a)). After cloning and replacing as described above, we recolor the cloned object oclone to perceptually contrasting hue while preserving its original shading and illumination properties. Formally, the edited object is obtained by sequentially translating and recoloring: = Recolor(cid:0)Translate(oclone, Td), c(cid:1), where Td is the original 3D position of the distractor, and is the selected contrasting hue that maximizes perceptual difference from the ground-truth color in CIELAB space: = arg max cC Lab(ogt) Lab(c)2, where Lab(ogt) denotes the original objects mean color in CIELAB space, and represents candidate color from predefined set of distinct candidate hues (more details are provided in Appendix A.3). This controlled recoloring provides two visually identical objects differing in color, enabling precise supervision for appearance grounding. CloneReplaceRotate (CR-Rot) for Orientation Errors. For grounding errors involving orientation predicates (e.g., facing the table, against the wall), we first select nearby distractor object od (e.g. the armchair <OBJ002> in Fig. 3(b)) from the same semantic category as the groundtruth object ogt. After cloning and replacing as previously described, we apply controlled rotation to the cloned object oclone. Formally, the final edited object is obtained by sequentially translating and rotating: = Rotate(cid:0)Translate(oclone, Td), Ry(θ)(cid:1), where is the vertical rotation axis and Ry(θ) denotes rotation matrix around the Y-axis by angle θ. In our experiments, the rotation angle θ is selected from small predefined set {45, 90} to produce meaningful yet visually consistent orientation changes. This unified formulation explicitly integrates spatial relocation and orientation modification, yielding two visually identical scenes differing only in object orientation, thereby providing precise supervision for directional grounding. CloneandReplace (CR) for Distance Errors. Distance grounding errors involve referent object (e.g., the transh can <OBJ003> in Fig. 3(c)), which defines the spatial relationship of interest. To identify this referent, we first filter all surrounding objects sharing the same semantic label as the mentioned category (e.g., tran can instances). The specific referent object oref is then selected according to the spatial predicate in the instruction: For near relations, we select the nearest instance to the target object as oref ; For far relations, we select the farthest instance within the same category as oref . After determining the referent oref , we select distractor od of the same category as the groundtruth object ogt to construct the counterfactual pair. The distractor selection follows the relational constraint: For near predicate, od must be farther from oref than the ground-truth object, and vice versa for far predicate. Finally, we replace the selected distractor with the cloned ground-truth object. CloneandReplace (CR) for Relative Relation Errors. We focus on vertical relations (e.g., above, below), since horizontal relative-position annotations in ScanNet (e.g., left, right) are frequently noisy and lack consistent labeling [23]. Similar to the CR strategy for distance errors, we first identify the referent object that defines the relation in the instruction (e.g., the whiteboard <OBJ003> in Fig. 3(d)). After identifying oref , we select distractor object od of the same semantic category as the ground-truth object ogt (e.g., table <OBJ002> in Fig. 3(d)) that violates the intended vertical relation. For instance, when the instruction specifies below, we select distractor that is not located below the referent, ensuring meaningful counterfactual spatial configuration. 3.4.2. QA Generation After constructing counterfactual scenes containing both the ground-truth object ogt and its edited object o, we generate new aligned questionanswer (QA) pairs to explicitly target the models original grounding failure. The objective is to encourage the model to distinguish between ogt and based solely on the failed predicate. Each counterfactual scene is paired with around 5-6 QA examples with increasing reasoning complexity, categorized as follows: Direct Factual (Perception). Simple open-ended queries assess the models perception of each objects attribute. Example: Q: What color is ogt? A: Light green. Q: What color is ? A: Pink. Discriminative (Verification). Yes/no questions verify whether the model correctly understands the failed predicate for each object. Example: Q: Is ogt light green? A: Yes. Q: Is light green? A: No. Comparative (Reasoning). Comparative queries require the model to directly contrast the minimal pair, isolating the failed attribute through comparison. Each answer identifies the correct object and explicitly states the rationale behind the choice by highlighting differences. Example: Q: Between ogt and , which one is light green? A: ogt. ogt shows light tone on its surfaces, while ogt appears gold, so their colors differ visibly. This mixture of QA types provides comprehensive supervision for each counterfactual edit, targeting multiple levels of reasoning difficulty. Table 1. Performance comparison on 3D grounding benchmarks. Our method consistently outperforms Chat-Scene (w/o 2D) and Chat-Scene on all metrics for ScanRefer and Multi3DRefer, with iterative re-training further improving results. Method Expert Models ScanRefer [6] 3DJCG [5] M3DRef-CLIP [60] 3D-VisTA [65] LLM-based Models 3D-LLM [19] Chat-3D [48] Chat-3D v2 [21] Grounded 3D-LLM [10] PQ3D [66] 3D-LLaVA [13] Inst3D-LLM [56] Chat-Scene (w/o 2D) [21] Ours (Round 1) Ours (Round 2) Chat-Scene [21] Ours (Round 1) Ours (Round 2) ScanRefer Multi3DRefer Acc@0.25 Acc@0.5 F1@0. F1@0.5 37.3 49.6 51.9 50.6 30.3 35.9 42.5 47.9 57.0 51.2 57.8 24.3 37.3 44.7 45.5 - 30.4 38.4 44.1 51.2 40.6 51.6 - - 42.8 - - - 45.1 45.2 - - 58.3 - - 38.4 - - - 41.6 40.6 50.1 - 53.5 41.2 45.5 (+4.3) 47.1 (+5.9) 55.5 57.8 (+2.3) 58.6 (+3.1) 37.4 41.8 (+4.4) 43.1 (+5.7) 50.2 52.3 (+2.1) 53.3 (+3.1) 43.8 48.4 (+4.6) 50.3 (+6.5) 57.1 60.0 (+2.9) 61.4 (+4.3) 40.2 45.1 (+4.9) 47.1 (+6.9) 52.4 55.8 (+3.4) 56.8 (+4.4) 3.5. Iterative Re-training The re-training phase shown in Fig. 2(d) completes the DEER-3D loop. Specifically, we combine the original training data with newly generated counterfactual scene and QA pairs. The base model Mbase is fine-tuned on this combined dataset to obtain refined model with improved grounding performance on the previously identified errors. This refined model subsequently serves as the baseline for the next iteration, where it is employed to mine remaining failures and generate additional counterfactuals. By iteratively repeating this process, DEER-3D progressively enhances the models grounding accuracy. 4. Experiments In this section, we evaluate our method on standard 3D grounding benchmarks. We adopt two backbone variants of the state-of-the-art Chat-Scene baseline [21]: (1) ChatScene (3D-only), which utilizes textual instructions and 3D point clouds without 2D visual inputs, and (2) Chat-Scene, the original version that additionally incorporates complementary 2D multi-view images. This setup enables us to assess our approach under different modality conditions. 4.1. Datasets and Evaluation Metrics Dataset. We evaluate our method on standard 3D referring expression grounding benchmark: ScanRefer [6] and Multi3DRefer [60]. ScanRefer provides single-object referring expressions, testing models ability to localize finegrained visual and spatial attributes. Multi3DRefer builds upon this by introducing multi-object references and re- (a) Edit scaling effects. (b) Error counts across iterations. (c) Different types of visual edits. Figure 4. Ablation studies validating our method. (a) Grounding performance scales with the quantity of our counterfactual data. (b) Our iterative loop (Round 2 vs. Round 1) successfully reduces all targeted error types. (c) Our full Err-Guided-Mix strategy is superior to both Random-aug and individual components, validating our error-driven and complementary design. Table 2. Ablation on different QA types: factual (Fact.), discriminative (Discri.), comparative without explanations (Comp.*), and comparative with explanations (Comp.). # 1 2 3 4 5 Ours QA Types Multi3DRefer Fact. Discri. Comp.* Comp. Acc@0.25 Acc@0.5 57.1 57.8 58.3 58.9 59.1 60.0 52.4 53.0 53.6 54.0 54.2 55.8 lational reasoning, where multiple objects must be jointly grounded within the same 3D scene. Evaluation Metrics. We adopt standard evaluation metrics specific to each benchmark. For grounding tasks on ScanRefer [6] and Multi3DRefer [60], we report grounding accuracy (Acc) at Intersection over Union (IoU) thresholds of 0.25 (Acc@0.25) and 0.5 (Acc@0.5), where predictions are considered correct if their IoU with ground-truth annotations exceeds the corresponding threshold. For multi-object grounding on Multi3DRefer [60], we report F1 scores at these IoU thresholds (F1@0.25, F1@0.5). 4.2. Implementation Details In Round 1, we fine-tune the baseline model using the original training data plus 10k counterfactual scenes and 60k QA pairs. In Round 2, we use the Round 1 model to identify new failure cases, generate an additional 3k counterfactual scenes and 18k QA pairs, and further fine-tune the model to obtain the Round 2 model. For all fine-tuning stages, we train for 5 epochs using the AdamW optimizer. The plearning rate is 3e5, with weight decay of 0.01 and global batch size of 32. All experiments are conducted on 4 NVIDIA H100 (80 GB) GPUs. More implementation details are provided in Appendix B.1. 4.3. Experimental Results Grounding Performance. As shown in Table 1, our approach consistently outperforms prior models across both 3D grounding benchmarks. While traditional expert models rely on task-specific architectures, our goal is to enhance LLM-based models that jointly reason over language and 3D scenes. Results demonstrate significant performance gains, particularly notable in the purely 3D setting, where our counterfactual augmentation delivers improvements of up to around +5% in the first round of augmentation alone. This highlights our methods effectiveness in directly enhancing spatial and attribute grounding from 3D signals. Moreover, when combined with the richer multimodal inputs provided by Chat-Scene, our approach achieves further improvements, establishing new state-of-the-art performance across both datasets. These consistent gains underscore the broad applicability and robustness of our counterfactual augmentation strategy in significantly enhancing grounded understanding in 3D-LLMs. Error-Guided Iterative Refinement. We further validate the effectiveness of our error-guided iterative editing loop, as shown in Table 1. Starting from the initial improvement achieved by the first round of counterfactual augmentation (Round 1), we apply subsequent iteration (Round 2), producing additional performance gains across both benchmarks and baselines. Crucially, these iterative gains are consistent for both purely 3D and multimodal baselines, highlighting that our approach is universally beneficial regardless of input modalities. This clearly demonstrates that our iterative loop is more than just one-time data enhancement; instead, it serves as sustainable self-correction mechanism, progressively refining the model by systematically addressing its weaknesses. More detailed analyses of each iterative step are provided in our ablation study. 4.4. Ablation Study In this section, we present ablation studies conducted with Chat-Scene with 2D images as our baseline model. Effect of Augmentation Scale. We analyze how grounding performance scales with the amount of counterfactual data generated by varying the number of edited distractors around each ground-truth object. Specifically, in Fig.4a, Table 3. Experimental results on scene understanding tasks. Our method outperforms baselines and shows strong overall capability across general scene understanding tasks. Table 4. Experimental results on Beacon3D [23]. App., Geo., and Spa. denote appearance, geometry, and spatial relations, respectively. Overall (Case) reports average accuracy per case, while Overall (Obj) reports average accuracy per object. C@0.5 B-4@0.5 EM EM-R Model Class App. Geo. Spa. Overall (Case) Overall (Obj) Method 3D-LLM [19] Chat-3D v2 [48] LL3DA [9] 3D-LLaVA [13] LEO [22] SceneLLM [15] Chat-Scene (w/o 2D) [21] Ours Chat-Scene [21] Ours Scan2Cap ScanQA SQA3D - 63.9 65.2 78.7 68.4 - 64.9 72.4 77.1 81. - 31.8 36.8 36.9 36.9 - 35.0 36.3 35.8 B-4 12.0 14.0 13.5 - 11.5 12.0 14. 69.4 87.6 76.8 92.6 80.0 80.0 80.3 88.7 - 54.7 - 54.5 - - 53.4 53.6 - - 53.7 - 53.7 54.2 - 56. 87.7 89.5 14.3 13.1 54.6 56.3 57.5 58.9 each increment in the Edit setting (Edit-0 Edit-3) corresponds to progressively increasing the sampling of distractors per scene (e.g., from 1 to 3, then 5), resulting in cumulative increase in total edited scenes. Performance on both ScanRefer and Multi3DRefer benchmarks improves consistently with increased sampling, validating the benefit of richer and more diverse counterfactual edits. Notably, the largest gains occur during initial scaling (Edit-0Edit1), suggesting that sampling small set of distractors already addresses many critical grounding issues effectively. Subsequent increments continue to provide steady but diminishing returns, highlighting balance between dataset richness and augmentation efficiency. Iterative Refinement Analysis. In Table. 1, we have presented quantitative results demonstrating the effectiveness of our error-guided iterative refinement strategy in Table.1. To further analyze its impact, Fig.4b explicitly compares grounding error counts between Round 1 and Round 2 across predicate categories. We observe substantial reductions in errors for all predicate types, clearly indicating that our framework effectively identifies and addresses the models primary grounding failures, thereby progressively refining its visual and spatial grounding capabilities. Additionally, we conduct third iteration (Round 3 results included in the Appendix B.3, using the Round 2 model to identify remaining failure cases. However, further training resulted in almost the same results as the Round 2, indicating saturation in grounding performance beyond Round 2. Thus, the Round 2 model represents an optimal trade-off between grounding accuracy and computational efficiency. Ablation on Counterfactual Edit Types. To analyze the effectiveness of each edit type within our counterfactual augmentation framework, we conduct an ablation study as shown in Fig.4c. Starting from the baseline, simple Random-aug strategy, which applies edits randomly without considering specific grounding failures (keeping the total number of edits equal to our full strategy), already yields modest gain. This indicates that generic augmentation alone can slightly enhance model performance. In contrast, Chat-Scene [24] Ours 59.8 61.7 59.2 64.4 50.2 53.4 53.8 55.5 59.8 61.7 41.0 45. our targeted strategies are far more efficient and effective. Notably, our Err-Guided-App (appearance-only) achieves comparable performance despite using significantly less data than the full Random-aug set. Furthermore, our ErrGuided-Spa (spatial-only augmentation) not only uses less data than Random-aug but also outperforms it. Finally, combining these two targeted edit types into Err-GuidedMix leads to the highest performance, clearly demonstrating the effectiveness of our counterfactual edits. Ablation on Different Question Types. We further examine the contribution of each QA supervision type (Table. 2). Starting from the baseline (Row 1), progressively adding factual (Row 2), discriminative (Row 3), and comparative questions (Row 4) consistently improves model performance by guiding it to address specific and challenging grounding queries. To isolate the effect of explanations, Row 5 includes comparative questions without explicit rationales, whereas ours incorporate full comparative questions with explanations. The performance gap between these settings shows that explanations provide an additional boost, indicating that explicit reasoning about the differences further strengthens grounding. 4.5. Other Analysis Performance on Captioning and QA. We further assess the generalization of our grounding-focused augmentation on broader scene understanding tasks, including captioning and question answering (Scan2Cap [11], ScanQA [3], and SQA3D [32]). As shown in Table 3, our method achieves consistent improvements over the baseline across all benchmarks, indicating that the benefits of our approach extend beyond grounding-specific settings. Detailed metric definitions and additional analysis are provided in Appendix B.2. Evaluation of Error-driven Framework. To evaluate the effectiveness of our error-guided diagnostic framework without visual edits, we conduct an experiment using textonly strategy (DEER-3D + Text-Aug). Specifically, we leverage our diagnostic pipeline to pinpoint ambiguous errors, but instead of visually editing the 3D scenes, we generate clearer textual instructions that explicitly disambiguate each failure case. For instance, if the model confuses two visually similar radiators, our framework identifies this ambiguity and generates more precise instruction (e.g., locate the radiator to the north), while leaving the scene unchanged. Training solely on this text-augmented data already improves performance over the baseline (55.5% 56.6% Acc@0.25 on ScanRefer). This demonstrates that inherent value of our diagnostic approach contributes to model robustness, even without visual augmentation. Please refer to the Appendix B.4 for more details. Evaluation on Human-annotated 3D Grounding. To further validate the real-world reliability of our method, we evaluate on the human-annotated BEACON-3D benchmark [23], which provides manually verified object and relation annotations for 3D scenes. This dataset reflects human judgments of spatial and visual grounding, offering more rigorous test of perceptual understanding. As shown in Table. 4, our model outperforms Chat-Scene across all categories, indicating that our error-driven augmentation not only enhances quantitative performance but also yields more human-aligned grounding behavior. 5. Conclusion We introduce DEER-3D, an error-driven framework that enhances 3D grounding via targeted visual counterfactual editing. Rather than random textual augmentation, DEER3D diagnoses grounding errors, decomposes them into semantic factors, and generates controlled 3D edits for precise predicate-level supervision. Through iterative refinement, our approach consistently improves grounding performance. Comprehensive experiments and analysis further demonstrate that error-guided scene editing effectively strengthens grounding capabilities in 3D-LLMs. 6. Acknowledgements We thank Jaemin Cho, Elias Stengel-Eskin, and Zaid Khan for their helpful feedback. This work was supported by NSF-AI Engage Institute DRL-2112635, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, DARPA ECOLE Program No. HR00112390060, and Capital One Research Award. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77167733, 2023. 3 [2] Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, JianGuang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689, 2023. 3 [3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19129 19139, 2022. 8, 2 [4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1644316452, 2022. 2 [5] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1646416473, 2022. [6] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. 6, 7 [7] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel X. Chang. D3net: speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. ArXiv, abs/2112.01551, 2021. 2 [8] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang. Counterfactual samples synthesizing for robust visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1080010809, 2020. 3 [9] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d In Proceedings of understanding reasoning and planning. the IEEE/CVF conference on computer vision and pattern recognition, pages 2642826438, 2024. 8 [10] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Runsen Xu, Ruiyuan Lyu, Dahua Lin, and Jiangmiao Pang. arXiv preprint Grounded 3d-llm with referent arXiv:2405.10370, 2024. 2, 6 tokens. [11] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel Chang. Scan2cap: Context-aware dense captioning in rgbIn Proceedings of the IEEE/CVF conference on scans. computer vision and pattern recognition, pages 31933203, 2021. 8, 2 [12] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 1 [13] Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, and Ian Reid. 3d-llava: Towards generalist 3d lmms In Proceedings of the with omni superpoint transformer. Computer Vision and Pattern Recognition Conference, pages 37723782, 2025. 1, 2, 6, 8 [14] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: visionlanguage-model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371, 2024. 3 [15] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and WenScene-llm: Extending language model for arXiv preprint han Xiong. 3d visual understanding and reasoning. arXiv:2403.11401, 2024. [16] Tsu-Jui Fu, Xin Eric Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein, and William Yang Wang. Counterfactual vision-and-language navigation via adversarial path sampler. In European Conference on Computer Vision, pages 7186. Springer, 2020. 3 [17] Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models local decision boundaries via contrast sets. arXiv preprint arXiv:2004.02709, 2020. 3 [18] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF international conference on computer vision, pages 1974019750, 2023. 3 [19] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. 1, 2, 6, 8 [20] Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, and Kai-Wei Chang. 3dllm-mem: Long-term spatial-temporal memory for embodied 3d large language model. arXiv preprint arXiv:2505.22657, 2025. 1 [21] Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3d scene and large language models with object identifiers. Advances in Neural Information Processing Systems, 37: 113991114017, 2024. 1, 4, 6, 8, 2, [22] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. 1, 2, 3, 8 [23] Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, Song-Chun Zhu, and Siyuan Huang. Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2457024581, 2025. 1, 6, 8, 9 [24] Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. 1, 2, 8 [25] Divyansh Kaushik, Eduard Hovy, and Zachary LipLearning the difference that makes difference arXiv preprint ton. with counterfactually-augmented data. arXiv:1909.12434, 2019. 3 [26] Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal. Dataenvgym: Data generation agents in teacher environments with student feedback. ArXiv, abs/2410.06215, 2024. 3 [27] Chengen Lai, Shengli Song, Sitong Yan, and Guangneng Hu. Improving vision and language concepts understanding with multimodal counterfactual samples. In European Conference on Computer Vision, pages 174191. Springer, 2024. 3 [28] Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Sangheon Shin, Sangmin Kim, and Sangpil Kim. Editsplat: Multi-view fusion and attention-guided optimization for view-consistent 3d scene In Proceedings of the editing with 3d gaussian splatting. Computer Vision and Pattern Recognition Conference, pages 1113511145, 2025. [29] Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1538615396, 2022. 3 [30] Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, and Junwei Liang. Seeground: See and ground for zero-shot openIn Proceedings of the vocabulary 3d visual grounding. Computer Vision and Pattern Recognition Conference, pages 37073717, 2025. 2 [31] Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Shawn Ma, Baoxiong Jia, and Siyuan Huang. Multimodal situated reasoning in 3d scenes. Advances in Neural Information Processing Systems, 37:140903140936, 2024. 1, 3 [32] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. 8, 2 [33] Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, et al. When llms step into the 3d world: survey and meta-analysis of 3d tasks arXiv preprint via multi-modal large language models. arXiv:2405.10255, 2024. 1 [34] Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and Simone Teufel. Its all in the name: mitigating gender bias with name-based counterfactual data substitution. arXiv preprint arXiv:1909.00871, 2019. [35] Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Yannis Panagakis, Giorgos Papanastasiou, and Sotirios Tsaftaris. Benchmarking counterfactual image generation. Advances in Neural Information Processing Systems, 37:133207133230, 2024. 3 [36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 2 [37] Amin Parvaneh, Ehsan Abbasnejad, Damien Teney, Javen Qinfeng Shi, and Anton Van den Hengel. Counterfactual vision-and-language navigation: Unravelling the unseen. Advances in neural information processing systems, 33:52965307, 2020. 3 [38] Axel Sauer and Andreas Geiger. Counterfactual generative networks. arXiv preprint arXiv:2101.06046, 2021. 3 [39] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 82168223, 2022. 4 [40] Liang Song, Guangming Wang, Jiuming Liu, Zhenyang Fu, Yanzi Miao, et al. Sc-nerf: Self-correcting neural radiance field with sparse views. arXiv preprint arXiv:2309.05028, 2023. 3 [41] Damien Teney, Ehsan Abbasnedjad, and Anton van den Hengel. Learning what makes difference from counterfactual examples and gradient supervision. In European Conference on Computer Vision, pages 580599. Springer, 2020. 3 [42] Gladys Tyen, Hassan Mansoor, Victor Carbune, Yuanzhu Peter Chen, and Tony Mak. Llms cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, 2024. [43] Jason Uwaeze, Pranav Kulkarni, Vladimir Braverman, Michael Jacobs, and Vishwa Parekh. Generative counterfactual augmentation for bias mitigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11531160, 2025. 3 [44] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluaIn Proceedings of the IEEE conference on computer tion. vision and pattern recognition, pages 45664575, 2015. 2 [45] Danqing Wang and Lei Li. Learning from mistakes via cooperative study assistant for large language models. arXiv preprint arXiv:2305.13829, 2023. 3 [46] Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and Wenguan Wang. Counterfactual cycle-consistent learning for instruction following and generation in visionlanguage navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1547115481, 2022. 3 [47] Sijia Wang and Lifu Huang. Targeted augmentation for lowresource event extraction. arXiv preprint arXiv:2405.08729, 2024. 3 [48] Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023. 1, 2, 6, [49] Minghao Wen, Shengjie Wu, Kangkan Wang, and Dong Liang. Intergsedit: Interactive 3d gaussian splatting editing with 3d geometry-consistent attention prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2613626145, 2025. 3 [50] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large In European language models to understand point clouds. Conference on Computer Vision, pages 131147. Springer, 2024. 1 [51] Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag. Llmrefine: Pinpointing and refining large In language models via fine-grained actionable feedback. Findings of the Association for Computational Linguistics: NAACL 2024, pages 14291445, 2024. 3 [52] Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Zongkai Wu, Jenq-Neng Hwang, Hao Zhao, and Fabio Remondino. 3dsceneeditor: Controllable 3d scene editing with gaussian splatting. arXiv preprint arXiv:2412.01583, 2024. 3 [53] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 4 [54] Zeyuan Yang, Peng Li, and Yang Liu. Failures pave the way: Enhancing large language models through tuning-free rule accumulation. arXiv preprint arXiv:2310.15746, 2023. 3 [55] Barry Menglong Yao, Qifan Wang, and Lifu Huang. Errordriven data-efficient large multimodal model tuning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2028920306, 2025. [56] Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, and Inst3d-lmm: Instance-aware 3d scene underJianke Zhu. In Proceedstanding with multi-modal instruction tuning. ings of the Computer Vision and Pattern Recognition Conference, pages 1414714157, 2025. 2, 6 [57] Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, and Zhen Li. Visual programming for zero-shot open-vocabulary 3d visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2062320633, 2024. 2 [58] Abhaysinh Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. ArXiv, abs/2403.12014, 2024. 3 [59] Weichen Zhang, Ruiying Peng, Chen Gao, Jianjie Fang, Xin Zeng, Kaiyuan Li, Ziyou Wang, Jinqiang Cui, Xin Wang, Xinlei Chen, et al. The point, the vision and the text: Does point cloud boost spatial reasoning of large language models? arXiv preprint arXiv:2504.04540, 2025. 1 [60] Yiming Zhang, ZeMing Gong, and Angel Chang. Multi3drefer: Grounding text description to multiple 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1522515236, 2023. 6, 7, 2 [61] Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, and Parisa Kordjamshidi. Vision-and-language navigation today and tomorrow: arXiv preprint survey in the era of foundation models. arXiv:2407.07035, 2024. [62] Yue Zhang, Zhiyang Xu, Ying Shen, Parisa Kordjamshidi, and Lifu Huang. Spartun3d: Situated spatial understanding of 3d world in large language model. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, 3 [63] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 29082917, 2021. 2 [64] Hongyan Zhi, Peihao Chen, Junyan Li, Shuailei Ma, Xinyu Sun, Tianhang Xiang, Yinjie Lei, Mingkui Tan, and Chuang Gan. Lscenellm: Enhancing large 3d scene understanding In Proceedings of the using adaptive visual preferences. Computer Vision and Pattern Recognition Conference, pages 37613771, 2025. 2 [65] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2911 2921, 2023. 6, 2 [66] Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In European Conference on Computer Vision, pages 188206. Springer, 2024. 2, 6 [67] Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, and Guanbin Li. Dreameditor: Text-driven 3d scene editing with neural fields. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. [68] Ran Zmigrod, Sabrina Mielke, Hanna Wallach, and Ryan Cotterell. Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. arXiv preprint arXiv:1906.04571, 2019. 3 Error-Driven Scene Editing for 3D Grounding in Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More Details about Deer3D Framework A.2. Analysis of Error Distribution In this section, we provide additional implementation details about the DEER-3D framework. We elaborate on the specific prompts used for our Decomposition module (Sec. 3.2), the full statistical analysis of the instruction types that justifies our focus on Appearance and Spatial categories (Sec. 3.3), and the complete color palette used for our Recolor operation (Sec. 3.4.1). A.1. Prompt for Decompose Instruction As detailed in Sec. 3.2 of the main paper, our DEER-3D framework begins by decomposing the full instruction into set of general, atomic predicates. We use large language model (e.g., Qwen3) for this task.To ensure the generated sub-instructions are consistent, unambiguous, and suitable for independent evaluation, we use structured prompt. This prompt instructs the LLM to separate the instruction into list of sentences, each focusing on single semantic perspective (e.g., attribute, spatial relation). The prompt enforces several key constraints to maintain quality: it explicitly forbids the use of ambiguous referring expressions (like it or them) and requires that all output sentences maintain the same, explicit subject (e.g., The table is brown, The table is near the wall). The exact prompt used is shown below."
        },
        {
            "title": "Prompt",
            "content": "Given an instruction sentence, separate it into list of sentences where each focuses on only one perspective, such as spatial relationship, attribute, or quantity. Important constraints: Do NOT use referring expressions like \"it\" or \"them\". these refer to. Explicitly state what Ensure that all output sentences use the SAME subject throughout. If other objects are involved (e.g., \"a computer is placed on top of it\"), rephrase so the original subject remains the focus (e.g., \"The table has computer placed on top of it\"). \"There is brown wooden table, Example: Input: computer is placed on top of it.\" Output: [\"There is brown table\", \"There is wooden table\", \"The table has computer placed on top of it.\"] In our methodology (Sec. 3.3), we state that our framework focuses on Appearance and Spatial errors. This decision is data-driven, based on statistical analysis of the instruction types present in the training data. To quantify the most common semantic components, we randomly sample around 300 instructions from the training dataset. We then ran our Decomposition module (Sec. 3.2) on this sample to parse each instruction into its constituent atomic predicates (around 1500 sub-instruction). The distribution of predicate types within this representative sample is presented in Figure 5. The analysis reveals that the instructions are not uniformly distributed; they are overwhelmingly concentrated in two primary categories: Spatial Relations (39.6%) and Appearance (38.6%). Together, these two categories conclusively account for 78.2% of all semantic predicates found in our analysis. Assuming this large sample is representative of the full dataset, this provides clear empirical evidence that Spatial and Appearance are the most dominant semantic components in the data. Therefore, our DEER-3D framework is justifiably prioritized to focus its diagnostic and editing efforts on these two categories. A.3. Edit Strategy for Recolor In Sec. 3.4.1, our Recolor operation identifies the most perceptually distinct color by maximizing its distance from candidate set C. To ensure that this process is both reproducible and perceptually meaningful, we define as predefined palette of 19 commonly used color prototypes. We operate in the CIELAB color space due to its perceptual uniformity, which makes Euclidean distances better aligned with human color perception. Our full LAB dictionary, and the candidate set is listed below, mapping each color name to its corresponding (L*, a*, b*) values. Table 5. The predefined set of color (C) used for recolor operation. Color Name CIELAB (L, a, b) Color Name CIELAB (L, a, b) white black gray beige tan brown dark brown red orange yellow (90, 0, 0) (12, 0, 0) (55, 0, 0) (68, 5, 18) (62, 10, 22) (35, 18, 28) (28, 14, 20) (45, 60, 30) (60, 35, 50) (75, 5, 70) green turquoise blue dark blue purple pink violet silver gold (55, -35, 35) (70, -35, -10) (40, 5, -55) (30, 5, -45) (45, 45, -25) (70, 30, 10) (50, 35, -35) (70, 0, 0) (65, 5, 35) Table 6. Performance comparision on MultiRefer. ZT: Zero-shot, ST: Single-task, MT: Multi-task, D: Distractor. Best results are bolded. Method ZT w/o F1 ZT w/ F1 ST w/o F1@0.25 ST w/o F1@0.5 ST w/ F1@0.25 ST w/ F1@0.5 MT F1@0.25 MT F1@0.5 Overall F1@0.25 Overall F1@0.5 3DVG-Trans+ [63] D3Net (Grounding) [7] 3DJCG (Grounding) [4] M3DRef-CLIP [60] Chat-Scene [21] Ours 87.1 81.6 94.1 81. 90.3 92.6 45.8 32.5 66.9 39.4 62.6 69.3 53.5 82.9 82.1 27.5 36.6 26.0 47. 75.9 75.6 34.6 49.1 52.6 16.7 23.3 16.7 30.6 44.5 48.4 43. 45.7 50.0 26.535.0 26.2 37.9 41.1 46.0 - - - 42.8 57.1 60.0 25.5 32.2 26.6 38. 52.4 55.8 Figure 5. Distribution of semantic predicate types from our instruction analysis. Table 7. Performance comparison on the ScanRefer. Method Unique Multiple Overall Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 3D-VisTA [65] Chat-scene [21] Ours 81.6 89.6 88. 75.1 82.5 81.9 43.7 47.8 50.8 49.1 42.9 45. 50.6 55.5 57.8 45.8 50.2 52.3 Table 8. Iterative refinement beyond Round 2 shows saturated performance. Method ScanRefer Acc@0.25 Acc@0.5 Ours (Round 1) Ours (Round 2) Ours (Round 3) 57.8 58.6 58.6 55.8 56.8 56.7 B. More Details about Experiment B.1. Implementation Details Our training follows an iterative refinement paradigm. In Round 1, the model is fine-tuned on the initial batch of DEER-3D counterfactual scenes. The resulting model is then used to automatically identify new failure cases on the training set; these cases are subsequently edited to produce an expanded set of counterfactual samples for Round 2. This second-stage training further corrects the models residual grounding errors. To improve stability during optimization, especially given the distribution shift introduced by edited scenes, we apply an Exponential Moving Average (EMA) over model parameters when training the final Round 2 model. EMA smoothing helps regularize updates, reduces variance in later training stages, and yields more robust final checkpoint with improved grounding accuracy. Figure 6. Semantic error detection followed by targeted text augmentation to disambiguate similar objects. B.2. Performance on Scene Understanding Tasks Beyond 3D grounding, we further assess our models on suite of general scene understanding tasks, including Scan2Cap [11], ScanQA [3], and SQA3D [32]. For Scan2Cap, we evaluate captioning quality using CIDEr@0.5 (C@0.5) and BLEU-4@0.5 (B-4@0.5), following the standard practice in 3D captioning benchmarks. For ScanQA, we adopt the official evaluation metrics of CIDEr (C) [44] and BLEU-4 (B-4) [36] to measure answer relevance and linguistic fidelity. Finally, for SQA3D, we report Exact Match accuracy (EM) and its refined variant EMR [22], which provide more granular assessment of spatially grounded question answering. Our method achieves consistent improvements over the baseline across all benchmarks, indicating that the benefits of our approach extend beyond grounding-specific settings. B.3. More Results on Grounding Task In the main paper, we report only the overall grounding performance on ScanRefer and Multi3DRefer. For completeness, we provide persubset results for these benchmarks in Table 6 and Table 7, respectively. To further investigate whether iterative refinement continues to improve performance beyond Round 2, we perform an additional round of error mining and fine-tuning using the Round 2 model as the starting point. As shown in Table 8, the resulting Round 3 model exhibits almost no grounding failures originate from limitations inherent to point-cloudbased 3D perception pipelines, such as incomplete geometry or reconstruction noise. The capability of the base 3D-LLM also fundamentally constrains the upper bound of achievable performance, as weaker reasoning or recognition ability limits the effectiveness of error diagnosis and correction. In addition, our current framework operates on datasets of relatively modest scale and primarily focuses on indoor environments, which restricts the magnitude and generality of performance gains. Finally, although our counterfactual edits introduce targeted and meaningful variations, they still cannot cover the long-tail complexity of real-world 3D scenes involving deformable objects, human interactions, or dynamic environments. Future work may explore scaling DEER-3D to larger and more diverse 3D corpora, leveraging stronger 3D-LLMs for better perception and reasoning, and enriching the editing module to support more complex and varied scenes. Table 9. Performance comparison on 3D grounding benchmarks. Method ScanRefer Acc@0.25 Acc@0.5 Chat-Scene [21] Random-Text DEER-3D+Text-Aug 55.5 56.1 56.6 50.2 50.3 50.8 improvement over Round 2 (differences within 0.1 across metrics). This suggests that the model has already corrected the major grounding failures by the second iteration, and that additional refinement yields diminishing returns. These results confirm our claim in the main paper that Round 2 represents practical stopping point that achieves the best balance between accuracy gains and computational cost. B.4. Error-Driven Framework Evaluation Beyond generating targeted 3D scene edits, our error-driven framework can also produce targeted text augmentations that disambiguate instructions. As shown in Fig. 6, we design an experiment focusing on subset of grounding failures where the predicted and ground-truth objects belong to the same semantic class and exhibit similar sizes and appearances. These failures typically arise from instructional ambiguity, where the textual description does not provide sufficient cues for the model to identify the correct instance. Our diagnostic evaluator detects such cases by checking semantic consistency between the predicted and groundtruth objects (e.g., class match and size similarity). When an error is attributed to ambiguous language rather than visual misinterpretation, the framework triggers the text augmentation module instead of applying 3D edit. This module generates precise disambiguating descriptions by incorporating contextual object references that uniquely characterize the target, such as: (1) global position cues (e.g., located in the west of the scene), (2) local relational attributes (e.g., closer to <OBJ003> than <OBJ002>)."
        },
        {
            "title": "These",
            "content": "enrich the augmentations instruction with predicate-level distinctions, enabling the model to differentiate between visually similar candidates without modifying the underlying scene. As shown in Table 9, simple random text perturbations yield minimal improvement, indicating that generic augmentation is insufficient for resolving ambiguity. In contrast, our targeted text augmentation provides better performance, demonstrating that error-driven, semantically grounded clarifications improve disambiguation. However, the overall gains remain relatively modest compared to our full editing pipeline, suggesting that text-only augmentation is helpful but inherently limited for 3D grounding tasks. C. Limitation and Future Work Despite the improvements introduced by DEER-3D, several broader challenges in current 3D-LLMs remain. Many"
        }
    ],
    "affiliations": [
        "Google Research",
        "UNC Chapel Hill",
        "University of Michigan"
    ]
}