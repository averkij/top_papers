{
    "paper_title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency",
    "authors": [
        "Mengshi Qi",
        "Pengfei Zhu",
        "Xiangtai Li",
        "Xiaoyang Bi",
        "Lu Qi",
        "Huadong Ma",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at https://github.com/zaplm/DC-SAM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 2 0 8 0 2 1 . 4 0 5 2 : r DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency Mengshi Qi, Pengfei Zhu, Xiangtai Li, Xiaoyang Bi, Lu Qi, Huadong Ma, Ming-Hsuan Yang AbstractGiven single labeled examples, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation models generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models (SAMs) have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAMs prompt encoder in segmentation by providing high-quality visual prompts. When generating mask prior from support images, we fuse the SAM features to better align the prompt encoder rather than relying solely on pre-trained backbone. Then, we design cycle-consistent cross-attention on fused features and initial visual prompts. This design leverages coarse masks from the SAM mask decoder to ensure consistency between features and visual prompts. Next, dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and &F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at https://github.com/zaplm/DC-SAM. Index TermsSegment Anything Model, In-context Segmentation, Prompt Generation, Efficient Parameter Tuning"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Recent visual foundation models such as Segment Anything models (SAM and SAM2) [1], [2] have attracted significant attention in recent years. Leveraging tens of millions of images and videos along with over billion masks, the SAM series demonstrates strong performance in interactive segmentation and proves to be valuable tool across wide range of applications, including medical image segmentation [3], open-vocabulary segmentation [4], [5], and more. In particular, numerous efforts [6], [7], [8] have been made to exploit the versatile segmentation capabilities of SAMs in specific domains, such as reasoning [6], extending the recognition ability, or combining with large language models [9]. Despite the state-of-the-art segmentation capabilities, SAMs lack the inherent ability to segment instances of the same category across multiple images given single instance prompt, an ability we refer to as in-context segmentation inspired by the NLP domain [10], [11]. In this task, the given image and object masks are called support or in-context examples, while the input images are called query images. Previous works explore such abilities with few-shot learning approaches [12], [13], [14], [15], [16], [17], [18], such as calculating the matching distance between query images and support images (known as visual prompts for in-context learning) or modeling object prototypes for better alignment. However, these methods explore in-context learning M. Qi, P. Zhu, X. Bi and H. Ma are with the State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China. X. Li is with Nanyang Technological University, Singapore. Lu. Qi and M.-H. Yang are with the UC Merced, US. ability with only few examples and fail to generalize across diverse domains. Most recently, several models [19], [20], [21], [22] based on in-context learning have been developed where the prompts consist of input-output pairs of visual tasks. Specifically, SegGPT [20] explores in-context segmentation by co-training massive image-mask pairs, leading to good generalization ability across various one-shot segmentation benchmarks. Although this work achieves promising performance, substantial computational resources and extensive annotated segmentation data are required to build such system. In contrast, prompt tuning [23], [24], [25] offers an effectively alternative adaptive approach that has shown impressive generalization capabilities across diverse domains. This work proposes an adaptive SAM model for in-context image and video segmentation based on prompt tuning. We note that the existing approaches developed based on SAM [24], [25] mainly depend on the characteristics extracted from the backbone networks and do not fully take advantage of the distinctive properties of the SAM-derived representations, as shown in Figure 1(a). This limitation leads to an oversight of the differences between SAM and backbone network features during the prompt generation process, significantly impacting the accuracy of the generated results. Solely relying on SAM-extracted features for prompt generation often lacks sufficient utilization of the semantic priors of target categories, another critical factor contributing to suboptimal performance. In addition, no suitable benchmarks currently exist to evaluate the in-context segmentation capabilities of video data. Existing benchmarks [26], [27], [28], [29], [30] for video segmentation focus mainly on segmenting and tracking pixels over time. To the best of our knowledge, there are no benchmarks for evaluating this ability using in-context Fig. 1: Overview of the proposed DC-SAM method and IC-VOS benchmark. a) Comparison of the previous few-shot segmentation methods in 1), existing methods based on SAM/SAM2 in 2), and DC-SAM in 3). DC-SAM leverages multi-source features and generates positive and negative prompts by ensuring prompt consistency, integrating with SAM/SAM2 to achieve in-context segmentation for both images and videos; b) Visualization of image and video settings by DC-SAM; c) Quantitative comparison of DC-SAM with state-of-the-art approaches in terms of mIoU on COCO-20i and PASCAL-5i, &F on the IC-VOS benchmark. examples. To address the aforementioned problems, we construct the first In-Context Video Object Segmentation (IC-VOS) benchmark. We collect examples from existing video segmentation benchmarks and in-context examples from COCO dataset, visually inspecting each example. The IC-VOS dataset comprises 369 videos, averaging 270 frames per video, totaling 99,549 frames across 30 semantic classes. We then benchmark representative methods with SAM2 and propagate the masks from SAM2 to establish the first in-context video benchmarks. For model design, we propose novel feature extraction strategy for the prompt generation of SAM, termed prompt consistency generation. The meta-architecture is shown in Figure 1(b). Our approach first fuses features extracted by the SAM encoder with those obtained from the backbone network to generate prior mask for the query image. Experimental results show that combining these two types of features significantly improves model performance. In addition, we design two improvements involving positive and negative features of the SAMs prompt encoder. First, we employ dual-branch strategy that generates positive and negative prompts using foreground and background masks, respectively. The main idea is to utilize the positive and negative prompt embeddings within SAMs prompt encoder to assign labels to the automatically generated prompts. By leveraging the interaction between positive and negative prompts, we achieve fine-grained control over the generated masks, since more confidential visual cues are provided. Second, we incorporate Cyclic Consistent Cross-Attention mechanism into the prompt generation process. This mechanism ensures semantic label consistency between input features and queries by aligning highly relevant support pixels with their corresponding categories. It effectively suppresses the propagation of conflicting semantic information. Consequently, this approach ensures that the generated prompts accurately focus on the most critical regions. The proposed prompt consistency method can be easily extended to the video domain, particularly for the SAM2 architecture. Specifically, we design simple mask tube supervision by extending prompt consistency to the tube mask format. Since our work can be applied to both SAM and SAM2, we term our method DC-SAM, an extension of the SAM series for in-context segmentation of images and videos. The main contributions of this work are: We propose novel prompt-consistency method based on SAM, called Dual-Consistency SAM (DC-SAM), tailored for one-shot segmentation tasks. It exploits the positive and negative features of the visual prompts, leading to high-quality prompts for in-context segmentation. Furthermore, this design can be easily extended to video tasks by combining the SAM and new mask tube design. We introduce novel cyclic consistent cross-attention mechanism that ensures the final generated prompts better focus on the key regions requiring prompting. When combined with SAM, this mechanism effectively filters out potentially ambiguous components in the features, further enhancing the accuracy and specificity of in-context segmentation. We collect new video in-context segmentation benchmark, IC-VOS (In-Context Video Object Segmentation), featuring manually curated examples sourced from existing video benchmarks. In addition, we benchmark several representative works in IC-VOS. With extensive experiments and ablation studies, the proposed method achieves state-of-the-art performance on various datasets and our newly proposed in-context segmentation benchmarks. DC-SAM achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and &F score of 71.52 on the IC-VOS benchmark."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Segmentation Anything Model. SAM models [1], [2] are proposed to segment objects in both image and video interactively. Using large-scale data co-training, SAM [1] presents novel data engine and portable model for segmentation. Subsequent research has utilized SAM as an interactive segmentation tool for various vision tasks, including visual grounding [31], tracking [32], distillation and efficiency modeling [33], medical analysis [34], [35], scene understanding [36], [37], [38], [39], [40] and image generation [23]. To adapt SAM for few-shot and incontext segmentation, PerSAM [23] and Matcher [24] employ patch cosine similarity to identify similar regions for subsequent tasks. VRP-SAM [25] employs comparable feature extraction method to identify similar regions for few-shot segmentation. It proposes query-based approach based on cross-attention to generate prompts. In contrast, our method emphasizes the prompt encoding process in SAM, treating both foreground and background masks as essential constraints. By leveraging positive and negative prompt branches along with Cyclic Consistent CrossAttention mechanism, DC-SAM efficiently utilizes SAMs prompt encoder and mask decoder to improve its in-context segmentation performance. Few-shot Segmentation. The goal of few-shot segmentation [12] is to segment novel semantic categories by giving only few annotated examples, including few-shot instance segmentation [41], incremental few-shot instance segmentation [42], [43], and generalized few-shot segmentation [43], [44]. Our method primarily focuses on one-shot semantic segmentation, also known as incontext segmentation. This task aims to segment query images using only single support sample. Recent methods [12], [45], [46], [47], [48] are typically developed based on metric learning by matching spatial location features with semantic centroids. In particular, PFENet [12] employs the last-layer features to generate prior masks and utilizes the mid-level features for segmentation. CycTR [14] introduces cyclic consistency between query and support features within the same class mask regions. Then, twobranch conditional networks [49], [50], [51], 4D dense convolution networks [52], [53], data augmentation methods [54], [55]. Recently, transformer-based models [56], [57], [58], [59] have also been applied to solve the problem. The core idea behind these works is to learn the correspondence in the feature space. In addition, several few-shot segmentation methods are developed based on in-context mask generation [19], [20], [60] and latent diffusion models [61]. However, existing approaches focus on directly designing model to complete the entire few-shot segmentation task, relying on complex computational frameworks (e.g., Transformer or diffusion models). As result, the generalizability of these models is limited. Moreover, these methods do not fully utilize the general segmentation prior knowledge inherent in pretrained foundational models such as SAM [1]. By integrating rich knowledge extracted from SAM and pre-trained backbones (such as ResNet [62] or the vision transformer-based DINO-v2 [63]), our approach generalizes well across various benchmarks while incurring lower computational costs. Prompt Tuning in Vision. Due to the limited computation resources, most current work cannot re-train the foundation models from scratch. Inspired by prompt tuning in NLP, several methods [7], [64], [65], [66], [67], [68], [69], [70] fine-tune only tiny portion of parameters to adapt the pre-trained foundation models to various downstream tasks, including image classification, segmentation, image generation and editing. The primary objective of these methods is to optimize the prompting process using the inherent knowledge in the model and then use the refined prompts to enhance performance. This approach preserves the original capabilities of the model and improves its effectiveness in specific tasks. Our work belongs to the prompt tuning approach on SAM, where only the parameters in the prompt encoder are learned. In particular, we aim to learn better correspondence between objects in query and support images, facilitating the model to achieve strong performance in in-context segmentation. Video Segmentation Benchmarks. Large-scale video object segmentation (VOS) datasets, such as DAVIS [27], MOSE [30], and LVOS [28], are widely used for numerous tasks. In these datasets, the mask of the target object is provided in the first frame, and subsequent frames require the segmentation of the corresponding objects throughout the video. However, in practical applications, when specific semantic mask (provided along with support TABLE 1: Comparison of the video portion of our proposed benchmark with other well-known VOS datasets. Annotation type indicates whether mask or bounding box is provided. 3 Dataset Videos DAVIS [27] MOSE [30] LVOS [28] LVOS v2 [29] YouTube-VOS [26] UAV20L [71] IC-VOS (Ours) 90 2149 220 720 4453 20 369 Mean Frames 69 73 576 412 27 270 Total Frames 6298 159,600 126,280 296,401 120,532 59,000 99,549 Classes Annotations Type - 36 27 44 94 5 30 M image) needs to be segmented throughout the entire video, existing methods cannot inherently perform this task without requiring annotations in the first frame. Even with the assistance of recent models such as SAM [1], manual clicks or selections of the target object and semantics still require corrections to the edges or interiors, which is cumbersome and labor-intensive. To address these limitations, in this work, we collect the first in-context VOS benchmark to utilize small number of images as prompts to achieve video semantic segmentation."
        },
        {
            "title": "3 IN-CONTEXT VOS BENCHMARK",
            "content": "We propose new in-context video segmentation dataset. The goal is to enable segmentation models to automatically identify and segment target semantics in videos, eliminating the need for manual annotation of the first frame."
        },
        {
            "title": "3.1 Problem Setting",
            "content": "We first introduce the previous image setting. The in-context image segmentation task takes query images, support images, and support masks as inputs. Our model is trained on dataset Dtrain and evaluated on Dtest, where Dtrain and Dtest contain mutually exclusive categories Ctrain and Ctest, respectively (i.e., Ctrain Ctest = ). Similar to prior work [12], [14], [15], the original datasets are partitioned into multiple folds, each of which comprises training set Dtrain and test set Dtest. For each fold, there exists query set = {(I i=1, where denotes the total number of categories in the training set Dtrain or the test set Dtest. Here, and represent RGB images and their corresponding segmentation masks. The objective is to produce mask output ˆM RHW 1, given query image RHW 3 and support set S, where and denote the height and width dimensions. i=1 and support set = {(I , , )}c )}c For in-context video segmentation, given image and its corresponding semantic mask are used to segment the associated semantics within video clip. The image and its mask play the same role as the support image and support mask in in-context image segmentation. Given support image RHW 3 and its corresponding mask RHW 1, the task involves segmenting each frame of the query video RT HW 3 to obtain the associated semantic masks, resulting in mask tube pred RT HW 1, where means the frame numbers of the video. Thus, to segment this task requires the model and track objects with the same semantics as the given support object, posing greater challenges compared to in-context image segmentation. 4 Fig. 2: Overview of our constructed IC-VOS benchmark. a) Distribution of video sources and their proportions. b) Word cloud of expressions. c) Categories in the dataset with the number of clips and frames for each category. d) Example cases illustrating the support image, support mask, and query video."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "The benchmark consists of image data and video datasets. The image dataset is divided into training set and validation set. We use the training set of the COCO semantic segmentation dataset [72] as the training set for the proposed benchmark. The COCO dataset is renowned for its richness and diversity. It contains vast number of images and detailed semantic annotations, which provide solid foundation for training our model. Additionally, the COO validation set serves as the source of image prompts during our validation process. To reduce annotation costs and leverage high-quality annotations from existing VOS datasets, we use the DAVIS [27], MOSE [30], and LVOS v2 [29] datasets and their annotations as template to construct our benchmark. However, these VOS datasets use instance-level annotations, which differ from our requirements for semantic-level segmentation. As such, we manually screen the video data from these three datasets to meet the following rules. First, all instances of given category that appear in the video are annotated in the VOS dataset, at least in the first frame. Second, the categories of the instances must belong to one of the 80 classes in the COCO instance segmentation dataset. We overlay the annotations from all the videos and save them as GIFs to facilitate manual selection. Only videos satisfying the above criteria will be selected. We archive the annotations for each instance into binary mask tubes, where represents the number of qualifying categories in the video."
        },
        {
            "title": "3.3 Dataset Statistics",
            "content": "The statistics of the proposed benchmark are shown in Table 1. In our proposed benchmark, we collect total of 369 videos, with an average of 270 frames per video (99,549 frames in the whole set). The videos contain annotations for 30 semantic categories. All videos are used in the validation process. We also report the source distribution of the source videos. As shown in Figure 2(a), we gather total of 369 videos, with 63.7% originating from the LVOS v2 [29] dataset, 27.9% from MOSE [30], and 8.4% from DAVIS [27]. The LVOS v2 dataset predominantly features longer videos, whereas shorter videos are more prevalent in MOSE and DAVIS. This distribution results in the average length of our collected videos falling at moderate level in Table 1. Figure 2(b) and (c) show the word cloud illustrating the proportion of frames for each category in our dataset, as well as the number of clips and frames per category. The categories with relatively higher number of clips in our dataset are person, dog, and cat. Meanwhile, some categories, despite having fewer clips, contain large number of frames, such as kite, surfboard, and aeroplane. Figure 2(d) presents multiple test cases from the benchmark, including category examples such as giraffe, motorcycle, and dog. Given support image (e.g., one containing dog) and its corresponding semantic mask, the model must segment the same semantic regions in the video (e.g., track all semantic pixels of dog across the video). This process assesses the models ability to transfer semantic understanding from static images to dynamic videos under one-shot learning conditions."
        },
        {
            "title": "3.4 IC-VOS Benchmark",
            "content": "We evaluate state-of-the-art few-shot segmentation methods on the proposed dataset to establish the IC-VOS benchmark. The evaluated methods encompass large-model-based approaches (e.g., PerSAM [23], Matcher [24], VRP-SAM [25]) and traditional few-shot segmentation models (PFENet [12], HDMNet [15], AMNet [16]). Since these methods are primarily designed for image-level tasks, we implement modifications to ensure fair and meaningful comparisons. Specifically, these models are integrated with SAM2 [2] to leverage its mask propagation capabilities. The specific modifications are detailed below. For large-model-based approaches (e.g., PerSAM, Matcher, VRP-SAM), the intermediate outputs of the first frame (e.g., prompts) are fed into SAM2 for inference and propagation. For conventional few-shot segmentation methods (e.g., PFENet, HDMNet, AMNet), the final binary mask prediction from the first frame is used by SAM2 for propagation. Trainable models (i.e., VRP-SAM, PFENet, HDMNet, AMNet) are retrained on the COCO few-shot segmentation dataset (all four folds combined) for 50 epochs, following the learning rates specified in their original works. For non-trainable models (i.e., Matcher, PerSAM), inference is performed directly without additional training or fine-tuning."
        },
        {
            "title": "4.1 Overview",
            "content": "As illustrated in Figure 3 and Figure 6, our proposed method builds upon SAM and SAM2. SAM is an interactive segmentation model capable of generating high-quality masks from given 5 Fig. 3: Overview of the proposed DC-SAM framework. We use positive and negative branches to generate respective prompts, thereby refining the scope of the final generated mask. Additionally, we incorporate SAM features during the prompt generation process to better capture the characteristics of SAM, resulting in more accurate prompt boundaries. During the prompt generation process, we introduce cyclic consistent cross-attention to filter out non-cycle-consistent feature points, enhancing the precision of the prompts. prompts. The model consists of three primary components: the image encoder, the prompt encoder, and the mask decoder. The image encoder extracts high-quality features for segmentation, and the prompt encoder processes visual prompts (such as boxes, points, masks, or text) to generate the corresponding tokens. The mask decoder receives the image features and the tokens encoded from the prompts, utilizing bidirectional transformer decoder to generate semantic masks. SAM2 further improves SAM by extending interactive segmentation to video. It also has memory module, including memory encoder and memory bank, to track each pixel in time. Notably, both SAM and SAM2 share an identical prompt encoder design. Our approach efficiently leverages the inherent capabilities of each SAM component during the prompt generation process, particularly for positive and negative points. Thus, our method can be applied to both architectures, resulting in unified modeling of in-context segmentation in both images and videos. As depicted in Figure 3 (a), our proposed DC-SAM primarily models the SAM prompt encoder by integrating in-context information into the prompt generation process. Specifically, it employs dual branches to generate positive and negative prompts. For each branch, the prompt generation process is bifurcated into feature fusion and consistent prompt generation. This integration ensures that the prompt generation process accounts for SAM features while leveraging complementary insights from mask priors. To refine query-based prompt generation, we introduce new cycleconsistent cross-attention mechanism to exclude regions irrelevant to the desired prompts. This mechanism is applied twice to produce refined prompts."
        },
        {
            "title": "4.2 Feature Extraction and Fusion",
            "content": "Similar to the prior design in few-shot segmentation [12], we extract features from both the query image and the support image before the prompt generation process. Specifically, given the input support image and the query image q, the backbone initially extracts features from these images. The backbone network can be the pre-trained ResNet-50 [62], VGG-16 [73], or DINOv2 [63]. Following the conventional design [12], [14], we utilize intermediate layer features (from the third and fourth stages of the backbone) and apply convolution operations to reduce dimensions, yielding initial features . The prior masks are computed using the support images corresponding mask and high-level and features (from the fifth stage). This mask determines the pixel-wise similarity between the query and support features, retaining the maximum similarity at each pixel and normalizing the similarity map to the range [0, 1] using min-max normalization. As illustrated in Figure 3, we concatenate the mask-averaged support feature with the query and support features, as well as the features extracted by SAM (F S), which are the same size as the query and support features. These concatenated features are then processed by 1 1 convolution before being fed into the transformer, ultimately yielding and q: and s = Conv11 (Concat(F s, s, = Conv11 (Concat(F q, s, SAM )) , SAM )) . (15) (16) By integrating SAMs features, both and become better aligned for the prompt generation process."
        },
        {
            "title": "4.3 Consistent Prompt Generation",
            "content": "Motivated by CycTR [14], our method includes the cyclic consistent cross-attention mechanism and the self-attention mechanism. However, unlike CycTR, which primarily enforces pixel-level attention consistency between query and support features, our goal is to ensure cyclic consistency for visual prompts compatible with SAM. Specifically, the initialized random visual prompts are regarded as query in Figure 3(c). We compute cross-attention between and query and support features, with the fused features serving as the key and value inputs, respectively, as shown in Figure 3(c). First, we compute the affinity map = QKT , RN HsWs to evaluate the similarity between the query and each support feature. For each pixel in the support features, where {0, 1, , HsWs 1}, Hs and Ws represent the height and width of the support features respectively, the affinity map is used to select the index of the query with the highest similarity: = arg max A(i, j), (17) where {0, 1, , 1} and is the number of specified queries. Applying the same method, we identify the pixel index of the support feature that has the highest similarity to the selected query: = arg max A(i, j). (18) Algorithm 1: DC-SAM 6 Support image with mask Input: Query image Feature extractor fθ SAM Image Encoder SAM θ SAM Positive/Negative Embeddings Epos, Eneg Output: Predicted mask pred 1 Step 1: Feature Extraction Support Feature: fθ(I s), fθ(I q), Query Feature: SAM SAM SAM SAM Support SAM Feature (I s), (I q) Query SAM Feature θ θ 2 Step 2: Feature Fusion q Conv11 (Concat(F s, s, Conv11 (Concat(F q, s, SAM )) SAM )) 3 Step 3: Prompt Initialization Qpos RandomInit(), Qneg RandomInit() 4 Step 4: Mediate Prompt Generation pos QCycAttn(Qpos, neg QCycAttn(Qneg, Qmed Qmed , s), , s) 5 Step 5: Pseudo Query Mask Generation (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) SAMMaskDecoder(Qmed pos + Epos, Qmed neg + Eneg) (11) 6 Step 6: Final Prompt Refinement pos SelfAttn(QCycAttn(Qmed neg SelfAttn(QCycAttn(Qmed pos , neg , , , )), )) (12) (13) 7 Step 7: Final Mask Prediction pred SAMMaskDecoder(Q pos + Epos, 8 return pred; neg + Eneg) (14) Given the flattened mask Ms RHsWs corresponding to the support image, cyclic consistency can be determined using the identified pixel indices. Cyclic consistency is satisfied when Ms(j) = Ms(j), and we incorporate this constraint into the cross-attention calculation to encourage the model to learn more cyclically consistent results. Specifically, we calculate bias RHsWs using the following formula: (cid:40) Bj = 0, , if Ms(j) = Ms(j) if Ms(j) = Ms(j) , (19) In this manner, the attention weights for cyclically inconsistent features are set to zero, thereby effectively filtering out features that should not be included in the prompt. For each query Qi Rd, we compute the result of the cross-attention as follows: QCycAttn(Qi, K, ) = Softmax(Ai + B) V, (20) where denotes the affinity matrix without masking. Since we do not have the masks for the query features as in Equation 19, it is Fig. 4: Illustration of our proposed cyclic consistent cross-attention mechanism. This figure shows the version applied to query features with one head. The Cyc operation represents the process described in Equation 19, which ultimately generates bias to filter out features that are not cycle-consistent. Fig. 5: Comparison of SAM segmentation results with and without negative prompts. (a) Segmentation of the cage using only positive prompts. (b) Segmentation of the cage using both positive and negative prompts. Although not achieving optimal segmentation results, adding negative prompts allowed for better differentiation between the background, the dinosaur, and the cage, resulting in significantly improved result. challenging to apply the aforementioned Cyclic Consistent CrossAttention. Alternatively, the queries can be fed into the SAM mask decoder to generate pseudo-mask ˆM for the query features. This also allows us to apply the Cyclic Consistent Cross-Attention to the query features, as shown in the middle of Figure 3(c). The above process is repeated with the refine query and query feature as input. After these two Cyclic Consistent CrossAttention layers, we further perform self-attention operation on all queries, forcing the global view of query features. This finally yields the generated prompts ."
        },
        {
            "title": "4.4 Dual Prompt-aware Mask Prediction",
            "content": "SAM uses positive and negative prompts during the training process to achieve flexible fine-grained mask control. As shown in Figure 5, using only positive sample points as prompts results in segmentation results with coarse and imprecise mask edges. However, the addition of negative sample point as prompt significantly improves the mask edges. Thus, we leverage this inherent characteristic of SAM by combining positive and negative prompts to achieve superior segmentation results. Specifically, we employ two branches to generate positive and negative prompts, respectively. For the positive branch, we use the support mask to participate during the feature extraction process for the target category and then to generate the positive prompt Ppos. For the negative branch, we invert the support mask to obtain the background mask = 1 s, which indicates the region where the negative prompt should be located, and use it to generate the negative prompt Pneg. Furthermore, we utilize SAMs inherent method of encoding positive and negative sampling points, by leveraging SAMs prompt encoder to enable it 7 Fig. 6: Illustration of our proposed DC-SAM framework with SAM2. Unlike the image-level framework, we train the entire model for the video to acquire the image-to-video prompt ability. We apply different data augmentation techniques to the query image, and the augmented images compose mask tube for training. to perceive the differences between positive and negative prompts. In particular, SAM uses positive and negative embeddings, Epos and Eneg, to annotate the input prompts. We adopt similar approach by labeling the generated positive and negative samples, neg = Pneg + Eneg. which result in These labeled prompts can be fed into SAMs mask decoder to obtain the predicted mask pred. pos = Ppos + Epos and More importantly, our work can be easily extended to SAM2 for in-context video segmentation, since we only improve the prompt encoder parts. As shown in Figure 6, we design mask tube prediction for the input query video during training. The mask tube is created by stacking enhanced image masks into video, encapsulating semantic-level spatiotemporal information. Each mask tube represents the semantics traced in the temporal domain. Without bells and whistles, this simple design further boosts the performance of in-context video segmentation on our proposed dual consistency baseline. In particular, we jointly finetune the SAM2 decoder, memory modules, and our proposed dual consistency prompt generation module."
        },
        {
            "title": "4.5 Optimization and Inference",
            "content": "Training. We adopt the common segmentation training setup for both SAM and SAM2. We directly use two loss functions to guide the segmentation training process. The Binary Cross-Entropy Loss (BCE Loss) supervises the pixel-level binary mask output by the model, as follows: LBCE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 [yi log(pi) + (1 yi) log(1 pi)], (21) where denotes the total number of pixels in the image, yi represents the true label value of pixel i, and pi is the probability value predicted by the SAM model at pixel i. Additionally, the Dice Loss is employed to provide additional context for pixel-level segmentation, by addressing class imbalance issues, as follows: LDice = 1 2 (cid:80)N i=1 p2 Consequently, the total loss function used in our model is i=1(pi yi) + (cid:80)N i=1 y2 (cid:80)N (22) combination of the BCE Loss and the Dice Loss: = LBCE + LDice (23) Inference. We process the query and support images and the support mask using DC-SAM for the image in-context segmentation task. The resulting positive and negative prompts are then combined with the pre-trained positive and negative embeddings to generate point prompts. These combined embeddings, enriched with contextual information, are then fed into the SAM/SAM2 mask decoder to produce final masks, ensuring accurate and contextually relevant segmentation outcomes. However, for the video in-context segmentation task, we apply DC-SAM to the first frame of the input video using the same procedure as in the image setting, thereby obtaining the mask output for the first frame. This mask is then inputted into SAM2 to generate memory embeddings, which are propagated across subsequent video frames utilizing the SAM2 mask decoder. Thus, we can obtain the final mask tube of the entire video."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets. We first evaluate the proposed method on the IC-VOS benchmark. In addition, we use the setting of [25] and evaluate the proposed DC-SAM on two other widely adopted datasets: PASCAL-5i [49] and COCO-20i [74]. The PASCAL-5i is derived from PASCAL VOC 2012 [75] with additional annotations from SDS [76], encompassing 20 classes. COCO-20i is based on MSCOCO [72] and includes 80 categories. For each dataset, we perform cross-validation by evenly dividing all classes into four (cid:80)C folds. We adhere to the same class splits specified in [49], [74] for PASCAL-5i and COCO-20i, respectively. Specifically, three folds are used for training, while the remaining fold is reserved for testing. Evaluation Metrics. We use the mean intersection over union (mIoU) as our primary evaluation metric. We denote mIoU = 1 i=1 IoUi, where represents the total number of classes to be evaluated in each fold, and IoUi represents the Intersectionover-Union for the i-th class. This metric disregards class-specific distinctions and computes the average value across all classes. For the IC-VOS benchmark, two commonly-used metrics were adopted: region similarity (J ) and contour accuracy (F ). These metrics are also used by the video sources of our dataset: MOSE [30], LVOS v2 [29], and DAVIS [27]. We calculate the mean of these values as the final score. Image Benchmarks. We implement DC-SAM using the PyTorch [77] framework, employing various encoders to generate prior masks for support and query images, including VGG-16 [73], ResNet-50 [62], Swin-B [78], and DINO v2-B/14 [63]. We used AdamW [79] as an optimizer and employed cosine learning rate decay strategy for training. For the COCO-20i dataset, the model is trained for 50 epochs with learning rate of 1104 and batch size of 8. For the PASCAL-5i dataset, training was conducted for 100 epochs with learning rate of 2 104 and batch size of 8. weight decay of 1 105 was applied to both datasets during training. The input image size was fixed at 512 512, and no data augmentation techniques were applied. Note that due to the patch size of 14 in DINO v2-B/14, we scale the image size to 896 896 when using DINO v2-B/14 as the prior masks generator to ensure that its output size matches the feature size of SAM. The number of queries in both the positive and negative branches was set to 25. IC-VOS Benchmark. For the IC-VOS benchmark, our prompt generator is connected with SAM2 to provide prompts for the first image, enabling SAM2 to propagate these prompts throughout the video sequence. The structure and optimizer of the prompt generator are identical to those used in the image benchmark. The differences primarily involve the number of training iterations, training methods, and learning rate configurations. In the first training step, we pre-train our entire framework on COCO images, specifically training the prompt generator and the SAM2 decoder while freezing all other parameters. This step involves 40, 000 iterations with learning rate of 1 104. In the second step, we generate mask tubes from the query image using various image augmentation techniques to enhance the performance of our framework in the video domain. We also unfreeze the memory encoder and memory attention parameters, and this step involves 10, 000 iterations with learning rate of 1 105."
        },
        {
            "title": "5.1 Main Results\nComparison with SAM-based models and visual foundation\nmodels. We evaluate our proposed DC-SAM against other SAM-\nbased methods, including PerSAM [23], Matcher [24], and VRP-\nSAM [25], on the COCO-20i dataset. As shown in Table 2, DC-\nSAM outperforms other current SAM-based methods. We also\ncompare our approach with methods based on visual foundational\nmodels, such as Painter [19] and SegGPT [20]. Table 2 shows\nthat the method based on DC-SAM and DINO v2-B surpasses\nSegGPT by 6%, wdespite SegGPT being trained on large-scale\nin-domain datasets. These results demonstrate the effectiveness of\nour approach with less data tuning.",
            "content": "TABLE 2: Comparison with other few-shot segmentation models with foundational models on the COCO-2020i dataset. Methods marked with * indicate using external data. Methods marked with symbol indicate SAM-based models. 8 Method Painter* [19] SegGPT* [20] PerSAM-F [23] Matcher [24] VRP-SAM [25] - ResNet50 - DINO v2-B DC-SAM F-3 Means F-0 31.2 56.3 22.3 52.7 F-1 35.3 57.4 24.0 53.5 F33.5 58.9 23.4 52,6 32.4 51.7 24.1 52.1 48.1 56.8 55.8 61.0 60.0 64.2 51.6 59. 33.1 56.1 23.5 52.7 53.9 60.4 55.5 62.0 - ResNet50 - DINO v2-B 50.4 56.8 56.0 62. 61.0 67.3 54.4 61.9 Comparison with few-shot segmentation methods. We present the evaluation results of our model and recent few-shot segmentation methods on the COCO-20i and PASCAL-5i datasets in Table 3. We use different backbones to generate prior masks, including VGG16 [73], ResNet50 [62], and DINO v2-B/14 [63], each representing different architectures and feature extraction capabilities. As illustrated in Table 3, DC-SAM performs favorably across various backbone configurations. Specifically, our model outperforms existing state-of-the-art few-shot segmentation models in every fold of each dataset and under all backbone settings. These results highlight significant performance advantages and the exceptional generalization capabilities of DC-SAM in different scenarios and categories. Quantitative Results on the IC-VOS Benchmark. As shown in Table 4, DC-SAM surpasses all other evaluated models, achieving &F score of 71.52 on the IC-VOS benchmark. This represents an 11.3% improvement over the second-best performing method, PFENet [12]. Our model can effectively segment the corresponding semantic regions in videos based on few categorical examples."
        },
        {
            "title": "5.2 Visual Comparisons\nImages. Figure 11 shows sample segmentation results of the\nSAM-based methods on the PASCAL-5i dataset. For the “bottle”\nexample shown in the first row, DC-SAM segments the objects\naccurately with the complete contours. For the “bird” example in\nthe second row, DC-SAM segments the objects with fine details.\nSimilarly, in the “bicycle” and “aeroplane” examples presented in\nthe third and fourth rows, DC-SAM consistently performs effec-\ntively with no false positives in background regions and accurately\ncaptures the complex contours of target objects. Overall, DC-SAM\ncan segment complex objects in the PASCAL-5i dataset with fine\ndetails.\nVideos. Figure 8 presents sample results of DC-SAM against\nPFENet and VRP-SAM on few-shot video semantic segmentation\nusing the proposed benchmark. We show the results from a single\nvideo clip in the dataset, and all three models receive the same\nsupport image and mask, indicating the target semantic category\nof motorcycle. DC-SAM accurately identifies the motorcycle\nand maintains good performance in subsequent segmentation. In\ncontrast, PFENet overlooks the wheels of the motorcycle and also\nsegments the person together with the motorcycle. Similarly, VRP-\nSAM segments both the person and the motorcycle in the earlier\nframes.",
            "content": "9 TABLE 3: Comparison with the state-of-the-art few-shot segmentation methods on COCO-20i [74] and PASCAL-5i [75]. The best results are highlighted in bold, while the second-best results are underlined. Method PFENet [12] BAM [13] HDMNet [15] VRP-SAM [25] DC-SAM PFENet [12] HSNet [80] CyCTR [14] SSP [81] NTRENet [82] DPCN [83] VAT [52] BAM [13] HDMNet [15] AMNet [16] ABCB [17] VRP-SAM [25] DC-SAM DC-SAM (SAM2) Image encoder VGGVGG-16 ResNet-50 ResNet-50 COCO-20i F-2 36.8 43.3 48.2 50.0 59.1 34.5 38.7 39.6 37.9 39.9 43.3 42.6 46.2 51.6 52.7 52.1 60.0 61.0 63.1 F-3 34.7 41.7 44.0 46.5 50. 33.8 38.7 39.8 36.7 37.9 39.7 39.7 45.2 49.4 50.6 49.8 51.6 54.4 56.2 F-1 38.1 47.1 50.6 51.7 50.2 38.6 43.1 43.0 39.6 42.6 47.0 43.8 49.9 55.3 55.8 54.0 55.8 56.0 56.4 F-0 35.4 36.4 40.7 43.6 44.7 36.5 36.3 38.9 35.5 36.8 42.0 39.0 39.4 43.8 44.9 44.2 48.1 50.4 49.7 Mean 36.3 42.1 45.9 48.0 51. 35.8 39.2 40.3 37.4 39.3 43.0 41.3 45.2 50.0 51.0 50.0 53.9 55.5 56.4 F-0 56.9 63.2 64.8 70.0 71.7 61.7 64.3 65.7 60.5 65.4 65.7 67.6 69.0 71.0 71.1 72.9 73.9 74.8 77.4 F-1 68.2 70.8 71.4 74.7 77.2 69.5 70.7 71.0 67.8 72.3 71.6 72.0 73.6 75.4 75.9 76.0 78.3 79.1 78.5 PASCAL-5i F-2 54.5 66.1 67.7 68.3 69. 55.4 60.3 59.5 66.4 59.4 69.1 62.3 67.6 68.9 69.7 69.5 70.6 71.4 70.5 F-3 52.4 57.5 56.4 61.9 63.8 56.3 60.5 59.7 51.0 59.8 60.6 60.1 61.1 62.1 63.7 64.0 65.0 66.5 69.4 Mean 58.0 64.4 65.1 68.7 70.4 60.8 64.0 64.0 61.4 64.2 66.7 65.5 67.8 69.4 70.1 70.6 71.9 73.0 74.0 Fig. 7: Comparison of segmentation results from different methods on the PASCAL-5i. Each row displays an RGB image along with its corresponding ground truth segmentation and the results of the four methods. Notable errors are marked with yellow . TABLE 4: Results on IC-VOS benchmark. Bold and underlined texts indicate the best and second-best results, respectively. Method &F PerSAM [23] + SAM2 PerSAM-F [23] + SAM2 Matcher [24] + SAM2 VRP-SAM [25] + SAM PFENet [12] + SAM2 HDMNet [15] + SAM2 AMNet [16] + SAM2 32.23 31.52 26.88 50.77 62.07 53.07 53.51 36.81 36.83 24.00 56.30 66.45 57.49 58.36 34.52 34.18 20.44 53. 64.26 55.28 55.94 DC-SAM 68.38 74.65 71."
        },
        {
            "title": "5.3 Ablation Studies on DC-SAM",
            "content": "Ablation on Each Component. Table 5 shows the effectiveness of each component of the proposed DC-SAM with ResNet50 [62] as the backbone network in the PASCAL-5i dataset. Note that TABLE 5: Ablation study on each innovation of the model. We start with VRP-SAM [25] as the baseline and incrementally add our innovations on the PASCAL-5i. Ablation VRP-SAM [25] + Pos-Neg Branch + SAM Feature Fusion + Cyclic Consistent F-0 73.9 74.0 74.8 74. F-1 78.3 78.5 79.6 79.1 F-2 70.6 70.3 70.7 71.4 F-3 Means 65.0 65.5 66.2 66.5 71.9 72.1 72.8 73.0 0 +0.2 +0.6 +0.2 we begin by using VRP-SAM [25] as the baseline and progressively integrate DC-SAM. Upon the introduction of positive and negative branches, the model has the ability to refine segmentation outcomes by leveraging positive and negative prompts, thus enhancing overall performance across each fold. During feature extraction and fusion, the incorporation of the SAM feature makes the prompt generation progress more aligned. Finally, as shown in the last row, adding prompt consistency for each branch further 10 Fig. 8: Visual comparisons of our proposed model with PFENet and VRP-SAM on our proposed benchmark. The support mask in the video indicates the semantic category of motorcycle, and all three models share the same support image and mask. TABLE 7: Comparison of fine-tuning with different modules. decoder Memory &F 67.57 68.27 68.38 73.62 74.20 74.65 70.59 71.23 71.52 TABLE 8: Comparison of fine-tuning with and without mask tubes. The total number of training iterations is 50k. Ablation w/o mask tube w/ mask tube 63.85 67.57 70.03 73. &F 66.94 70.59 Fig. 9: Ablation study on the number of queries. The x-axis represents the number of queries in one branch. Note that since our DC-SAM consists of both positive and negative branches, the total number of queries is twice the number shown on the x-axis. The y-axis represents the models performance. These experiments are conducted on the PASCAL-5i dataset. TABLE 6: Performance comparison of different pre-trained modules. During fine-tuning, the SAM2 decoder parameters were unfrozen and adjusted. The term w/o pre-trained indicates direct training using mask tubes over 50,000 iterations. Ablation F &F w/o pre-trained Prompt Generator Prompt Generator and decoder 58.72 64.16 67.57 64.37 70.30 73.62 61.54 67.23 70. leads to improvement over various folders. This guides the model to focus more on the critical areas that require prompts, thus achieving precise segmentation. Query Number Ablation. We also explore the effect of varying the number of queries on the DC-SAM. As shown in Figure 9, the models performance gradually improves as the number of queries in single branch increases from 1. However, as the number of queries increases, the models performance exhibits fluctuations, with improvements in some folds and declines in others. Overall, the model achieves the best average performance when the number of queries in single branch is set to 25."
        },
        {
            "title": "5.4 Ablation Study on IC-VOS Benchmark",
            "content": "Pre-training. In Table 6, we show the performance of models trained using different approaches: (1) training the prompt generator and the SAM2 decoder for 50k iterations with data augmentation to generate mask tubes; (2) training only the prompt generator during pre-training; (3) jointly training the prompt generator and SAM2 decoder during pre-training. For fair comparisons, all methods utilizing pre-trained models fine-tune both the prompt generator and the SAM2 decoder. Experimental results demonstrate that pre-training on images followed by fine-tuning with mask tubes achieves the best video segmentation performance. This is because image datasets provide greater diversity, facilitating faster convergence, while fine-tuning on video datasets enhances correspondence learning for mask tubes. Furthermore, training the SAM2 decoder during the image pre-training phase significantly enhances the final video segmentation performance. Fine-tuning with Ablated Modules. As shown in Table 7, we show the performance variations when fine-tuning different modules of SAM2, including the prompt generator. The experimental results indicate that fine-tuning the memory module (including the memory encoder and memory attention) yields greater improvements compared to fine-tuning only the decoder. Fine-tuning both modules together results in optimal model performance. 11 Fig. 10: Visualization of two failure cases of our proposed DC-SAM on IC-VOS. We still find missing matching objects due to the occlusion (in the top) and multiple instance inputs with the fast motion (in the bottom). TABLE 9: Evaluation of DC-SAM in few-shot scenarios on the PASCAL-5i dataset. TABLE 12: Generalization performance on the PASCAL-5i dataset using Mean IoU (%). Shots Backbone F-0 F-1 F-2 F-3 Means Method Image Encoder Means 1 5 1 VGG-16 [73] ResNet-50 [62] 71.7 76.9 (+5.2) 74.8 78.2 (+3.4) 77.2 79.6 (+2.4) 79.1 81.4 (+2.3) 69.0 71.4 (+2.4) 71.4 72.7 (+1.3) 63.8 69.3 (+5.5) 66.5 73.8 (+7.3) 70.4 74.3 (+3.9) 73.0 76.5 (+3.5) TABLE 10: Performance and SAM2 trainable parameters of different fine-tuning versions. Decoder Memory LoRA # param &F 3.95M 67.57 7.31M 68.27 11.26M 68.38 0.11M 66.86 73.62 74.20 74.65 72.72 70.59 71.23 71.52 69. Mask-tube Fine-tuning. In our proposed DC-SAM, we utilize mask tubes generated by data augmentation to fine-tune the entire model. These mask tubes offer more diverse set of training examples, thereby enhancing the models generalization capability across various scenarios. As shown in Table 8, we compare the performance of fine-tuning directly with images versus fine-tuning with mask tubes. Because fine-tuning the memory encoder and memory attention yields substantial improvements for video segmentation, we only train the prompt generator and mask decoder in this comparison to isolate the impact of the mask tubes. These results demonstrate that fine-tuning the model with mask tubes significantly improves video segmentation performance, highlighting the effectiveness of this approach in capturing temporal dynamics and enhancing segmentation consistency."
        },
        {
            "title": "5.5 More Analysis",
            "content": "TABLE 11: Details of the data split for PASCAL-5i in the domain shift scenario. Each row represents non-overlapping classes in the training set, corresponding to the respective fold of COCO-20i. Fold 0 1 2 3 Test Classes Aeroplane, Boat, Chair, Dining table, Dog, Person Horse, Sofa, Bicycle, Bus Bird, Car, Potted plant, Sheep, Train, TV/monitor Bottle, Cow, Cat, Motorbike RPMM [84] PFENet [12] RePRI [85] VAT-HM [86] VRP-SAM [25] HSNet [80] DGPNet [87] ResNet-50 ResNet-101 FP-Trans [88] DeiT-B/16 DC-SAM ResNet-50 49.6 61.1 63.2 65.1 75.9 64.1 70.1 69.7 76.5 TABLE 13: Results by the original SAM2 and the fine-tuned SAM2 on the proposed benchmark IC-VOS and the LVOS dataset. Both models were provided with the semantic/instance mask of the first frame of the video. Dataset SAM2 Version &F IC-VOS LVOS v2 [29] original fine-tuned original fine-tuned 87.56 89.07 75.18 71.25 92.30 94. 81.97 80.59 89.93 91.74 78.58 75.92 Extension to the Few-shot Setting. To assess the performance of our proposed DC-SAM in the few-shot scenario, we evaluate the model under the 5-shot setting and compare the results with those from the 1-shot setting. Table 9 shows that the 5-shot model consistently surpasses the 1-shot model in performance across all folds. The results indicate that our DC-SAM can be easily extended to the few-shot settings. Generalization Capability. To evaluate whether DC-SAM retains its generalization capability under domain-shift scenarios, we adopt the same experimental setups from prior studies [12], [25], [80]. Specifically, our model is trained on the COCO-20i dataset but tested on the PASCAL-5i dataset. As shown in Table 11, the categories for each fold of PASCAL-5i are adjusted based on the scheme in [25] to ensure that there is no overlap between the training and testing sets. As demonstrated in Table 12, our model achieves state-of-the-art performance in generalization evaluation. LoRA Fine-Tuning of SAM2. We examine the effect of using Low-Rank Adaptation (LoRA) [89] to fine-tune the SAM2 model, focusing on parameter reduction and final performance. TABLE 14: GFLOPs and learnable parameter analysis of our proposed model DC-SAM, VRP-SAM, and PFENet. prompts in the first frame, and large motion. We will address these issues in the future work. 12 Method FLOPs (G) Learnable Parameters VRP-SAM [25] PFENet [12] DC-SAM 218.949 207.582 278. 1.6M 10.8M 1.9M As demonstrated in Table 10, applying LoRA to fine-tune part of SAM2 reduces the parameter count by 99% compared to full parameter fine-tuning, while retaining 97.6% of the original model performance in terms of &F . Performance of SAM2 After Fine-Tuning. To evaluate whether fine-tuning SAM2 in our setup affects its original capabilities, we evaluate both the original and fine-tuned versions of SAM2 on our proposed benchmark and the LVOS v2 [29] validation set. For each video clip, we use the mask of the first frame for the target semantic or instance segmentation, and we use SAM2 to infer the corresponding masks for all subsequent frames. As demonstrated in Table 13, the fine-tuned SAM2 outperforms the original SAM2 on our proposed IC-VOS, demonstrating enhanced capabilities in semantic video segmentation. For the LVOS v2 video instance segmentation benchmark, the fine-tuned SAM2 shows slight performance loss but retains 96.6% of the original performance based on the &F metric. With more trained data or co-training with VOS data, DC-SAM is likely to achieve better performance trade-off on both IC-VOS and VOS, which will be part of our future work. Failure Cases. Figure 10 shows some failure segmentation results of DC-SAM. The first row shows scenario where tracking fails due to occlusions, leading to incorrect subsequent segmentation results from the propagation module of SAM2. The second row presents case where the target semantic category is dog. During fast motions involving the dog and toy car, slight tracking errors occur despite initially accurate segmentation. In the third frame, small portion of the mask erroneously tracks the wheel of the toy car, and the subsequent propagation process accumulates this error."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we present prompt tuning-based method to adapt visual foundation models, SAM and SAM2, to better support in-context learners. The core idea is to leverage the features of the SAM prompt encoder to generate more fine-grained visual prompts with dual consistency. Given the fused SAM features, we can use both positive and negative queries to generate visual prompts. During the generation process, we design cycleconsistent attention in each branch. Furthermore, we propose new dataset, IC-VOS, to benchmark existing representative methods combined with SAM2. Our proposed DC-SAM performs favorably against existing models on several few-shot segmentation benchmarks, even with SAM2. By adding simple mask tube to DC-SAM, it also achieves state-of-the-art performance on the IC-VOS benchmark. Extensive analysis shows the effectiveness, efficiency, and generalization of our approach. Future Work. The proposed model performs well in the few-shot image-to-image segmentation and the few-shot image-to-video segmentation task that we introduced. However, it still has some limitations, such as tracking errors due to occlusion, inaccurate"
        },
        {
            "title": "REFERENCES",
            "content": "[1] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., Segment anything, in ICCV, 2023. 1, 2, 3 [2] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.- Y. Wu, R. Girshick, P. Dollar, and C. Feichtenhofer, SAM 2: Segment anything in images and videos, in ICLR, 2025. 1, 2, 4 J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, Segment anything in medical images, Nature Communications, 2024. 1 [3] [4] X. Wang, S. Li, K. Kallidromitis, Y. Kato, K. Kozuka, and T. Darrell, Hierarchical open-vocabulary universal image segmentation, NeurIPS, 2024. 1 [5] H. Zhou, T. Shen, X. Yang, H. Huang, X. Li, L. Qi, and M.-H. Yang, Rethinking evaluation metrics of open-vocabulary segmentaion, in arXiv, 2023. 1 [6] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia, Lisa: Reasoning segmentation via large language model, in CVPR, 2024. 1 [7] H. Yuan, X. Li, C. Zhou, Y. Li, K. Chen, and C. C. Loy, Openvocabulary sam: Segment and recognize twenty-thousand classes interactively, in ECCV, 2024. 1, 3 [8] H. Yuan, X. Li, T. Zhang, Z. Huang, S. Xu, S. Ji, Y. Tong, L. Qi, J. Feng, and M.-H. Yang, Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos, arXiv, 2025. 1 [9] L. Qi, Y.-W. Chen, L. Yang, T. Shen, X. Li, W. Guo, Y. Xu, and M.-H. Yang, Generalizable entity grounding via assistance of large language model, in arXiv, 2024. 1 [10] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, NeurIPS, 2022. 1 [11] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu, B. Chang et al., survey on in-context learning, in EMNLP, 2024, pp. 11071128. [12] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, and J. Jia, Prior guided feature enrichment network for few-shot segmentation, TPAMI, 2020. 1, 3, 4, 5, 8, 9, 11, 12 [13] C. Lang, G. Cheng, B. Tu, and J. Han, Learning what not to segment: new perspective on few-shot segmentation, in CVPR, 2022. 1, 9 [14] G. Zhang, G. Kang, Y. Yang, and Y. Wei, Few-shot segmentation via cycle-consistent transformer, NeurIPS, 2021. 1, 3, 5, 9 [15] B. Peng, Z. Tian, X. Wu, C. Wang, S. Liu, J. Su, and J. Jia, Hierarchical dense correlation distillation for few-shot segmentation, in CVPR, 2023. 1, 3, 4, 9 [16] Y. Wang, N. Luo, and T. Zhang, Focus on query: Adversarial mining transformer for few-shot segmentation, NeurIPS, vol. 36, pp. 31 524 31 542, 2023. 1, 4, 9 [17] L. Zhu, T. Chen, J. Yin, S. See, and J. Liu, Addressing background context bias in few-shot segmentation through iterative modulation, in CVPR, 2024. 1, [18] X. Shi, D. Wei, Y. Zhang, D. Lu, M. Ning, J. Chen, K. Ma, and Y. Zheng, Dense cross-query-and-support attention weighted mask aggregation for few-shot segmentation, in ECCV, 2022. 1 [19] X. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang, Images speak in images: generalist painter for in-context visual learning, in CVPR, 2023. 1, 3, 8 [20] X. Wang, X. Zhang, Y. Cao, W. Wang, C. Shen, and T. Huang, Seggpt: Towards segmenting everything in context, in ICCV, 2023. 1, 3, 8 [21] Z. Fang, X. Li, X. Li, J. M. Buhmann, C. C. Loy, and M. Liu, Explore in-context learning for 3d point cloud understanding, 2024. 1 [22] X. Wang, Z. Fang, X. Li, X. Li, C. Chen, and M. Liu, Skeleton-incontext: Unified skeleton sequence modeling with in-context learning, CVPR, 2024. 1 [23] R. Zhang, Z. Jiang, Z. Guo, S. Yan, J. Pan, H. Dong, Y. Qiao, P. Gao, and H. Li, Personalize segment anything model with one shot, in ICLR. 1, 2, 4, 8, [24] Y. Liu, M. Zhu, H. Li, H. Chen, X. Wang, and C. Shen, Matcher: Segment anything with one shot using all-purpose feature matching, in ICLR. 1, 2, 4, 8, 9 [25] Y. Sun, J. Chen, S. Zhang, X. Zhang, Q. Chen, G. Zhang, E. Ding, J. Wang, and Z. Li, Vrp-sam: Sam with visual reference prompt, in CVPR, 2024. 1, 2, 4, 7, 8, 9, 11, 12 13 [26] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, Youtube-vos: large-scale video object segmentation benchmark, arXiv preprint arXiv:1809.03327, 2018. 1, 3 [52] S. Hong, S. Cho, J. Nam, S. Lin, and S. Kim, Cost aggregation with 4d convolutional swin transformer for few-shot segmentation, in ECCV, 2022. 3, 9 [27] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool, The 2017 davis challenge on video object segmentation, arXiv preprint arXiv:1704.00675, 2017. 1, 3, 4, [28] L. Hong, W. Chen, Z. Liu, W. Zhang, P. Guo, Z. Chen, and W. Zhang, Lvos: benchmark for long-term video object segmentation, in ICCV, 2023, pp. 13 48013 492. 1, 3 [29] L. Hong, Z. Liu, W. Chen, C. Tan, Y. Feng, X. Zhou, P. Guo, J. Li, Z. Chen, S. Gao et al., Lvos: benchmark for large-scale long-term video object segmentation, arXiv preprint arXiv:2404.19326, 2024. 1, 3, 4, 8, 11, 12 [30] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai, Mose: new dataset for video object segmentation in complex scenes, in ICCV, 2023, pp. 20 22420 234. 1, 3, 4, 8 [31] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, in ECCV, 2024. 2 [32] Y. Cheng, L. Li, Y. Xu, X. Li, Z. Yang, W. Wang, and Y. Yang, Segment and track anything, arXiv preprint arXiv:2305.06558, 2023. [33] C. Zhou, X. Li, C. C. Loy, and B. Dai, EdgeSAM: Prompt-in-theloop distillation for on-device deployment of SAM, arXiv preprint arXiv:2312.06660, 2023. 2 [34] J. Wu, Z. Wang, M. Hong, W. Ji, H. Fu, Y. Xu, M. Xu, and Y. Jin, Medical sam adapter: Adapting segment anything model for medical image segmentation, Medical Image Analysis, p. 103547, 2025. 2 [35] C. Chen, J. Miao, D. Wu, A. Zhong, Z. Yan, S. Kim, J. Hu, Z. Liu, L. Sun, X. Li et al., Ma-sam: Modality-agnostic sam adaptation for 3d medical image segmentation, Medical Image Analysis, vol. 98, p. 103310, 2024. 2 [36] C. Lv, S. Zhang, Y. Tian, M. Qi, and H. Ma, Disentangled counterfactual learning for physical audiovisual commonsense reasoning, in NeurIPS, 2023. 2 [37] M. Qi, J. Qin, Y. Yang, Y. Wang, and J. Luo, Semantics-aware spatialtemporal binaries for cross-modal video retrieval, IEEE Trans. Image Process., vol. 30, pp. 29893004, 2021. 2 [38] M. Qi, Y. Wang, A. Li, and J. Luo, Stc-gan: Spatio-temporally coupled generative adversarial networks for predictive scene parsing, TIP, vol. 29, pp. 54205430, 2020. 2 [39] M. Qi, W. Li, Z. Yang, Y. Wang, and J. Luo, Attentive relational networks for mapping images to scene graphs, in CVPR, 2019. 2 [40] M. Qi, Y. Wang, J. Qin, A. Li, J. Luo, and L. Van Gool, stagnet: An attentive semantic rnn for group activity and individual action recognition, TCSVT, vol. 30, no. 2, pp. 549565, 2020. [41] Z. Fan, J.-G. Yu, Z. Liang, J. Ou, C. Gao, G.-S. Xia, and Y. Li, Fgn: Fully guided network for few-shot instance segmentation, in CVPR, 2020. 3 [42] D. A. Ganea, B. Boom, and R. Poppe, Incremental few-shot instance segmentation, in CVPR, 2021. 3 [43] Y. Han, J. Zhang, Y. Wang, C. Wang, Y. Liu, L. Qi, X. Li, and M.- H. Yang, Reference twice: simple and unified baseline for few-shot instance segmentation, TPAMI, 2024. 3 [44] Y. Li, H. Zhu, J. Ma, C. S. Teo, C. Xiang, P. Vadakkepat, and T. H. Lee, Towards generalized and incremental few-shot object detection, arXiv preprint arXiv:2109.11336, 2021. 3 [45] Y. Liu, N. Liu, X. Yao, and J. Han, Intermediate prototype mining transformer for few-shot semantic segmentation, in NeurIPS, 2022. 3 [46] Y. Liu, N. Liu, Q. Cao, X. Yao, J. Han, and L. Shao, Learning non-target knowledge for few-shot semantic segmentation, in CVPR, 2022. 3 [47] J. Liu, Y. Bao, G.-S. Xie, H. Xiong, J.-J. Sonke, and E. Gavves, Dynamic prototype convolution network for few-shot semantic segmentation, in CVPR, 2022. 3 [48] G. Li, V. Jampani, L. Sevilla-Lara, D. Sun, J. Kim, and J. Kim, Adaptive prototype learning and allocation for few-shot segmentation, in CVPR, 2021. 3 [49] A. Shaban, S. Bansal, Z. Liu, I. Essa, and B. Boots, One-shot learning for semantic segmentation, arXiv preprint arXiv:1709.03410, 2017. 3, 7, 8 [50] K. Rakelly, E. Shelhamer, T. Darrell, A. A. Efros, and S. Levine, Conditional networks for few-shot semantic segmentation, in ICLR, 2018. 3 [51] Z. Lu, S. He, X. Zhu, L. Zhang, Y.-Z. Song, and T. Xiang, Simpler is better: Few-shot semantic segmentation with classifier weight transformer, in ICCV, 2021. [53] J. Min, D. Kang, and M. Cho, Hypercorrelation squeeze for few-shot segmentation, in ICCV, 2021. 3 [54] N. Tritrong, P. Rewatbowornwong, and S. Suwajanakorn, Repurposing gans for one-shot semantic part segmentation, in CVPR, 2021. 3 [55] Y. Zhang, H. Ling, J. Gao, K. Yin, J.-F. Lafleche, A. Barriuso, A. Torralba, and S. Fidler, Datasetgan: Efficient labeled data factory with minimal human effort, in CVPR, 2021. 3 [56] J.-W. Zhang, Y. Sun, Y. Yang, and W. Chen, Feature-proxy transformer for few-shot segmentation, in NeurIPS, 2022. 3 [57] X. Shi, D. Wei, Y. Zhang, D. Lu, M. Ning, J. Chen, K. Ma, and Y. Zheng, Dense cross-query-and-support attention weighted mask aggregation for few-shot segmentation, in ECCV, 2022. [58] G.-S. Xie, H. Xiong, J. Liu, Y. Yao, and L. Shao, Few-shot semantic segmentation with cyclic memory network, in ICCV, 2021. 3 [59] D. Kim, J. Kim, S. Cho, C. Luo, and S. Hong, Universal few-shot learning of dense prediction tasks with visual token matching, in ICLR, 2023. 3 [60] A. Bar, Y. Gandelsman, T. Darrell, A. Globerson, and A. Efros, Visual prompting via image inpainting, in NeurIPS, 2022. 3 [61] R.-Z. Qiu, Y.-X. Wang, and K. Hauser, Aligndiff: aligning diffusion models for general few-shot segmentation, in ECCV, 2024. 3 [62] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016. 3, 5, 8, 9, 11 [63] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, TMLR, pp. 131, 2024. 3, 5, 8 [64] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, Parameter-efficient transfer learning for nlp, in ICML, 2019. 3 [65] X. L. Li and P. Liang, Prefix-tuning: Optimizing continuous prompts for generation, in ACL, 2021. 3 [66] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, in ICLR, 2022. 3 [67] E. B. Zaken, S. Ravfogel, and Y. Goldberg, Bitfit: Simple parameterefficient fine-tuning for transformer-based masked language-models, in ACL, 2022. 3 [68] D. Guo, A. M. Rush, and Y. Kim, Parameter-efficient transfer learning with diff pruning, in ACL, 2021. 3 [69] B. Lester, R. Al-Rfou, and N. Constant, The power of scale for parameter-efficient prompt tuning, in EMNLP, 2021. 3 [70] J. Wu, X. Li, C. Si, S. Zhou, J. Yang, J. Zhang, Y. Li, K. Chen, Y. Tong, Z. Liu et al., Towards language-driven video inpainting via multimodal large language models, CVPR, 2024. 3 [71] U. Benchmark, benchmark and simulator for uav tracking, in ECCV, 2016. 3 [72] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in ECCV, 2014. 4, 7 [73] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in ICLR, 2015. 5, 8, 11 [74] K. Nguyen and S. Todorovic, Feature weighting and boosting for fewshot segmentation, in ICCV, 2019. 7, 8, 9 [75] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, The pascal visual object classes (voc) challenge, IJCV, 2010. 7, 9 [76] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik, Semantic contours from inverse detectors, in ICCV, 2011. 7 [77] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, NeurIPS, 2019. [78] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021. 8 [79] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR. 8 [80] J. Min, D. Kang, and M. Cho, Hypercorrelation squeeze for few-shot segmentation, in ICCV, 2021. 9, 11 [81] Q. Fan, W. Pei, Y.-W. Tai, and C.-K. Tang, Self-support few-shot semantic segmentation, in ECCV, 2022. 9 [82] Y. Liu, N. Liu, Q. Cao, X. Yao, J. Han, and L. Shao, Learning non-target knowledge for few-shot semantic segmentation, in CVPR, 2022. 9 14 [83] J. Liu, Y. Bao, G.-S. Xie, H. Xiong, J.-J. Sonke, and E. Gavves, Dynamic prototype convolution network for few-shot semantic segmentation, in CVPR, 2022. 9 [84] B. Yang, C. Liu, B. Li, J. Jiao, and Q. Ye, Prototype mixture models for few-shot semantic segmentation, in ECCV, 2020. [85] M. Boudiaf, H. Kervadec, Z. I. Masud, P. Piantanida, I. Ben Ayed, and J. Dolz, Few-shot segmentation without meta-learning: good transductive inference is all you need? in CVPR, 2021. 11 [86] S. Moon, S. S. Sohn, H. Zhou, S. Yoon, V. Pavlovic, M. H. Khan, and M. Kapadia, Hm: Hybrid masking for few-shot segmentation, in ECCV, 2022. 11 [87] J. Johnander, J. Edstedt, M. Felsberg, F. S. Khan, and M. Danelljan, Dense gaussian processes for few-shot segmentation, in ECCV, 2022. 11 [88] J.-W. Zhang, Y. Sun, Y. Yang, and W. Chen, Feature-proxy transformer for few-shot segmentation, in NeurIPS, 2022. 11 [89] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models, in ICLR. 15 This supplementary material systematically demonstrates the extensive application effects of DC-SAM in image-to-image incontext segmentation and image-to-video in-context segmentation. To comprehensively validate the effectiveness and robustness of the method, we have carefully selected 20 representative image cases (covering various target categories) and 8 typical video sequences (including challenging scenarios such as dynamic target tracking and complex background changes) for visualization. By comparing the output results of DC-SAM with those of other stateof-the-art methods, the technical advantages of our approach in detail preservation and accurate prompting are intuitively highlighted. These rich visual examples not only corroborate the quantitative analysis conclusions presented in the main text but also provide multidimensional empirical references. They can be read in conjunction with the methodology section of the main text to gain more comprehensive understanding. Fig. 11: Comparison of one-shot segmentation results on the PASCAL-5i dataset. 16 Fig. 12: Comparison of semantic segmentation results for the Sheep category on the IC-VOS dataset. Fig. 13: Comparison of semantic segmentation results for the Dog category on the IC-VOS dataset. Fig. 14: Comparison of semantic segmentation results for the Cup category on the IC-VOS dataset."
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore",
        "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China",
        "UC Merced, US"
    ]
}