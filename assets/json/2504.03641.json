{
    "paper_title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models",
    "authors": [
        "Wulin Xie",
        "Yi-Fan Zhang",
        "Chaoyou Fu",
        "Yang Shi",
        "Bingyan Nie",
        "Hongkai Chen",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 4 6 3 0 . 4 0 5 2 : r MME-Unify: Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models Wulin Xie1,, Yi-Fan Zhang1,5,,, Chaoyou Fu2,5, Yang Shi3 Bingyan Nie1, Hongkai Chen4, Zhang Zhang1, Liang Wang1, Tieniu Tan2 1CASIA, 2NJU, 3PKU, 4Vivo, 5M-M-E Project leader Equal Contribution https://mme-unify.github.io/"
        },
        {
            "title": "Unified Multimodal Large Language Models",
            "content": "(UMLLMs) have garnered considerable interest for their ability to seamlessly integrate generation and comprehension tasks. However, existing research lacks unified evaluation standard, often relying on isolated benchmarks to assess these capabilities. Moreover, current work highlights the potential of mixed-modality generation capabilities through case studiessuch as generating auxiliary lines in images to solve geometric problems, or reasoning through problem before generating corresponding image. Despite this, there is no standardized benchmark to assess models on such unified tasks. To address this gap, we introduce MME-Unify, also termed as MME-U, the first benchmark designed to evaluate multimodal comprehension, generation, and mixed-modality generation capabilities. For comprehension and generation tasks, we curate diverse set of tasks from 12 datasets, aligning their formats and metrics to develop standardized evaluation framework. For unified tasks, we design five subtasks to rigorously assess how models understanding and generation capabilities can mutually enhance each other. Evaluation of 12 U-MLLMs, including Janus-Pro, EMU3, and Gemini2-Flash, reveals significant room for improvement, particularly in areas such as instruction following and image generation quality. 1. Introduction Unlike traditional MLLMs (e.g., GPT-4V) and purely generative models (e.g., DALL-E 3), U-MLLMs [3, 25, 33, 37] excel in processing mixed-modal inputs and outputs, providing enhanced flexibility and the ability to address broader spectrum of complex tasks. Recently, closedsource U-MLLMs, such as GPT-4o and Gemini 2.0 Flash, have demonstrated exceptional generative capabilities, im- (a) MME-U tasks. (b) Leaderboard. Figure 1. comprehensive visualization of the diverse tasks in MME-U and the leaderboard. The figure (a) illustrates the wideranging nature of the tasks covered in our benchmark, which spans from traditional understanding tasks to complex mixed-modality generation challenges. Additionally, the leaderboard (b) highlights the performance rankings of various U-MLLMs in our benchmark. pressing in both instruction comprehension and image creation, as shown in Figure 2. These models exhibit an extraordinary grasp of image details, even surpassing proprietary generative models. However, this versatility also introduces considerable challenges in comprehensively evaluating their capabilities, primarily due to two key issues: Figure 2. Complex instruction-based image generation comparison of results from open-source U-MLLMs (DeepSeek-Janus Flow, EMU3), closed-source U-MLLMs (GPT-4o, Gemini-2), and proprietary models (DALLE-3). The closed-source U-MLLMs have demonstrated abilities surpassing proprietary generation models, with significantly larger gap compared to open-source models. Lack of Standardized Benchmarks for Traditional Tasks. Existing works typically evaluate traditional generation and understanding tasks separately, using various benchmarks. However, the benchmarks chosen across studies are inconsistent, leading to unfair comparisons. Moreover, the evaluation methods differ significantlymultimodal understanding tasks may involve varied formats such as multiple-choice questions, GPT-4 scoring, or binary classification, while multimodal generation tasks may rely on metrics like CLIP score or FID. This diversity in evaluation makes it difficult to derive an intuitive and unified performance score. Absence of Benchmarks for Mixed-Modality Generation1. The most distinctive feature of U-MLLMs is their mixed-modality generation capabilities, which demonstrate the synergistic interaction between multiple modalities. For instance, image editing requires understanding textual instructions and identifying objects to be modified, while solving geometry problems involves comprehending the problem, drawing auxiliary lines, and performing logical reasoning. Despite these advanced capabilities, most methods only showcase simple cases, lacking standardized benchmark to rigorously assess these complex mixed-modality tasks. 1also termed as unify tasks To address these challenges, we propose comprehensive evaluation framework for U-MLLMs, which is shown in Figure 2. For traditional generation and understanding tasks, we sample data from 12 existing datasets, resulting in 10 tasks with 30 subtasks. On the understanding side, these tasks encompass single-image, multi-image, and video-based perception and reasoning tasks, covering wide range of difficultiesfrom simple visual questionanswering (VQA) to high-resolution VQA in real-world scenarios and long-video understanding. On the generation side, we include tasks such as image/video generation and editing, as well as more complex conditional image generation and image-to-video generation, aiming to cover the full spectrum of existing generative tasks. To simplify evaluation and provide unified score, we manually reformat all understanding tasks into multiple-choice questions, reporting accuracy as the primary metric. For generation tasks, we standardize the evaluation scores and normalize them to provide consistent metric. This approach reduces the difficulty of benchmark collection and mitigates the issue of inconsistent evaluation metrics across studies. For the Unified Tasks, we constructed five tasks: (1) Image Editing and Explaining, where the model first understands complex editing instructions and edits an image; (2) Common Sense Question Answering, where the model answers question and generates the corresponding image; (3) Auxiliary Lines, where the model draws auxiliary lines for geometry problems and then solves them; (4) SpotDiff, where the model identifies and draws the differences between two images; and (5) Visual CoT, where the model generates step-by-step strategies for navigating maze and visualizes the next state. These tasks evaluate models ability to perform sequential reasoning and generate corresponding multimodal outputs at each step. All tasks are carefully formatted as multiple-choice questions to facilitate consistent, fair, and objective evaluation. We evaluate 12 existing U-MLLMs, including JanusPro, EMU3, VILA-U, and MiniGPT-5. To provide context for their performance, we also compare them with specialized understanding models (e.g., Claude-3.5 Sonnet, Qwen2.5-VL) and generative models (e.g., DALL-E-2, DALL-E-3). This comprehensive evaluation not only underscores the strengths and weaknesses of U-MLLMs but also establishes standardized benchmark for future research in this rapidly evolving field. For example, we uncover several key experimental findings, as illustrated in Figure 2. Currently, U-MLLMs exhibit significant variance in rankings across three dimensions, and no single model has emerged as the best performer across multiple capabilities. Moreover, the performance gap between models is substantial. Finally, the current open-sourced UMLLMs still exhibit significant gap in performance compared to specialized models in both understanding and generation tasks. Additionally, while many works claim to handle mixed-modality generation, our unify task tests demonstrate that the majority of existing U-MLLMs struggle to consistently and effectively process these types of tasks. 2. MME-Unify This section outlines the data collection, question annotation, and evaluation strategy for MME-Unify. Figures 2 and 3 provide visual representations of subtasks and samples across three domains, while Table 1 compares MME-U with existing benchmarks. MME-U categorizes U-MLLM capabilities into three areas: (1) Multimodal Understanding, (2) Multimodal Generation, and (3) Unify Capability, highlighting the diverse aspects of model performance. 2.1. Multi-Modal Understanding Data Collection. Multimodal understanding tasks are divided into three subcategories based on visual input type: Single-Image Perception and Understanding (SIPU). Evaluates image-text pair comprehension. Multi-Image & Interleaved Text-Image Understanding (MITIU). Assesses the models ability to handle and process multi-image and interleaved text-image inputs. Video Perception and Understanding (VPU). Measures To ensure comprehensive coverage of various image and video understanding scenarios, we collect 1,900 samples from 5 benchmarks such as MME and MMBench, encompassing over 24 tasks. This includes 1,600 perception tasks, such as OCR, diagram and table understanding, and spatial perception, along with 300 reasoning tasks, including attribute reasoning and action reasoning, with at least 50 QA pairs per sub-task. Additional details can be found in Appendix Figure 8 and Appendix Table 5. More visualization examples can be found in Appendix Figure 6. QA Pairs Reformulation. To standardize the evaluation of the understanding task, we convert all the collcted data into multiple-choice QA pairs, with one correct option and the remaining options carefully designed to be closely related to it. For models that can accept only single-image input, we use the first image from the multi-image input or the first frame from the video input. For models that cannot process video files (e.g., MP4 files), we uniformly sample six key frames from the video to serve as the visual input. Evaluation Strategy. To fairly evaluate MLLM outputs, we apply rule-based filtering to match model responses with answer options, similar to MME-Realworld [12, 46]. Furthermore, to eliminate positional bias inherent in multiplechoice questions, the correct answer is randomly shuffled among the four available options. We then calculate the average accuracy across all sub-tasks and derive the overall understanding score, providing fair, robust, and unbiased evaluation of the models performance. 2.2. Multi-Modal Generation Multimodal generation involves various tasks for image and video modalities, which can be further subdivided based on application, as shown in Figure 3: 1. Fine-grained Image Reconstruction (FIR). Given an original image, the model is required to restore detailed features and local textures. 2. Text-guided Image Editing (TIE). Edit or modify an image based on textual instructions. 3. Text-guided Image Generation (TIG). Given text description, the model needs to generate an image that matches it. 4. Conditional Imageto-Video Generation (CIVG). Generate dynamic video sequence based given image and text prompt. 5. Text-guided Video Generation (TVG). Generate video sequence based on textual description. 6. Video Prediction (VP). Predict subsequent frames or the complete video sequence based on the information from the first frame. Data Collection. Data is collected from benchmark datasets, such as COCO [22], Emu-Edit [29], MSRVTT [38], ensuring at least 200 samples for each task. For video prediction, videos are sourced from the Pexel Video website2 and the first frame is used for prediction. Detailed data sources and sample sizes are in Appendix Table 5. More visualization examples can be found in Figure 7. video comprehension capability. 2https://www.pexels.com/videos/ Figure 3. Diagram of our MME-Unify. Our benchmark consists of 3 main domains, encompassing 15 subtasks to comprehensively evaluate U-MLLMs understanding, generation, and unified capabilities. Specifically, each unify task includes at least one question, an input image, multiple text choices, and image choices. The image choices consist of correct answer image and set of manually crafted negative samples. During the evaluation process, we input the image, question, and text options, and the U-MLLMs are required to select the correct text answer and generate an image. The text answer is evaluated by matching it with the correct answer, while the generated image is compared with the constructed image choices. If the CLIP score between the generated image and the correct answer image is the highest, it is considered correct; otherwise, it is deemed incorrect."
        },
        {
            "title": "SIPU MITIU VPU FIR TIE TIG CIVG TVG VP UT",
            "content": "MSR-VTT [38] MMBench [24] GenEval [16] MagicBrush [43] VBench [17] SEED-Bench2 [20] Emu-Edit [29] TIP-I2V [32] MMBench-Video [8] MME[11] Video-MME [12] MME-RealWorld [46] 10, 000 3, 217 1, 200 10, 338 1, 600 19, 242 5, 611 500, 000 2, 000 2, 374 2, 700 29, 429 CVPR 2016 arXiv 2023 arXiv 2023 NeurIPS 2023 CVPR 2024 arXiv 2024 CVPR 2024 arXiv 2024 NeurIPS 2024 arXiv 2023 CVPR 2025 ICLR 2025 MME-Unify (ours) 4, 104 Table 1. Comparison of MME-U and other Benchmark. SIPU: Single Image Perception & Understanding; MITIU: Multiple & Interleaved Image-Text Understanding; VPU: Video Perception & Understanding; CIVG: Conditional Image-to-Video Generation; FIR: Fine-grained Image Reconstruction; TIE: Text-Guided Image Editing; TIG: Text-to-Image Generation; TVG: Text-to-Video Generation; VP: Video Prediction; UT: Unified Task. QA Pairs Reformulation. Due to the diversity of generation tasks and their varied data sources, the collected samples contain redundant attributes and inconsistent number of images, videos, and other multimodal data. We aim to provide streamlined, unified evaluation framework. To achieve this, we contribute the following: Attribute Unification Pipeline. First, we summarize all attributes appearing in the data, which exceed 30 types, creating significant complexity. We then manually eliminate task-irrelevant attributes and merge similar attributes across different tasks. For example, text attributes are represented as Text Prompt, image attributes as Src Image and Ref Image based on their input/output roles, and video attributes as Video. For any task where an attribute is not required, its corresponding value remains empty. Task-Specific Prompt Engineering. To ensure that the model can effectively generate outputs that meet the task requirements, we establish specific system prompts for each subtask. Each samples text prompt or src image serves as the input, while the reference image or video acts as the ground truth answer. Through standardizing attribute values and constructing tailored prompts, we convert diverse samples from different tasks into unified format for evaluating multimodal generation tasks. Evaluation Strategy. Evaluating multimodal generation tasks with unified metric is challenging due to the diversity of subdomains and their distinct metrics (e.g., CLIP-I, CLIP-T, FVD, FID). To address this, we: (1) Perform domain-specific preliminary evaluations using standard metrics; (2) Standardize all metrics to consistent (0, 100) scale, converting non-positive indicators into positive ones; and (3) Compute the average of standardized scores to derive the final generation score. This approach ensures cross-task comparability while maintaining domainspecific evaluation rigor. Detailed metrics and standardization methods are provided in Appendix 2.3. Unify Capability MME-U contains five unified subtasks: (1) Common Sense Question Answering (CSQ), (2) Image Editing and Explaining (IEE), (3) SpotDiff (SD), (4) Auxiliary Lines (AL), and (5) Visual CoT (VCoT). Each subtask includes at least 50 manually constructed samples and is structured with taskspecific instructions and question templates that require mixed-modality input-to-output generation. Common Sense Question Answering. This task evaluates U-MLLMs ability to associate commonsense descriptions with visual features, such as linking the tomb of an ancient Egyptian pharaoh to pyramid or Chinas national treasure to panda. Our approach involves: 1. Question Construction. Using GPT-4o, we generate riddle-like questions based on commonsense concepts, with similar but incorrect words as negative options. For example, when the answer is panda, we select brown bear or polar bear as negative options to increase difficulty. 2. Image Collection. We manually gather images from the internet corresponding to the correct and their negative options. 3. Task Execution. U-MLLMs are prompted to select the correct textual option and generate the corresponding image. Detailed procedures and the prompt are in Figure 10(a) and 11. Image Editing and Explanation. This task evaluates UMLLMs ability to understand complex editing instructions and generate accurate modifications. Our methodology includes: 1. Data Collection. We source data (source images, editing instructions, and reference images) from the EmuEdit dataset. 2. Textual QA Construction. Using GPT-4o, we generate accurate interpretations of editing targets and three incorrect interpretations for textual multiple-choice questions. 3. Visual QA Construction. The correct instruction corresponds to the target image in Emu-Edit. For incorrect instructions, we input them into InstructPix2Pix [2] to generate negatively edited images, forming image-based multiple-choice questions. 4. Task Execution. Given the corresponding prompt, source image, and editing instructions, the model must first produce correct understanding of the editing target and instructions, and then generate an edited image based on that understanding. Detailed procedures and the system prompt are in Figure 10(b) and 12. SpotDiff. When identifying differences between two similar images, humans typically need to recall the exact locations of these differences to accurately count them. This task evaluates U-MLLMs ability to identify and recall differences between similar images, simulating human visual reasoning. Our approach involves: 1.Data Collection: We sample image pairs with annotated differences from the SpotDiff website3. 2. Textual QA Construction. Using the annotated difference count, we create textual multiplechoice questions with three incorrect counts (10 from the true value). 3. Visual QA Construction. We place the annotated difference regions from the image pair onto white background as the correct answer, and randomly crop other areas to place them on the background as incorrect answers. 4. Task Execution. U-MLLMs must identify the difference regions between the two images and draw them onto the white background, while also selecting the correct difference count. Detailed procedures in Figure 10(c), and the system prompt is provided in Figure 13. Auxiliary Lines. This task evaluates U-MLLMs ability to integrate understanding and generation by solving geometric problems requiring auxiliary lines. Our methodology includes: 1. Data Selection. We filter the Geometry3K dataset for problems requiring auxiliary lines, extracting logical forms (e.g., Triangle(A, B, C)), choices, and answers. 2. Textual QA Construction. Using GPT-4o, we generate natural language QA pairs (Question, Choices, Answer) for textual multiple-choice questions. 3. Visual QA Construction. We manually solve each sampled geometric problem by drawing the correct auxiliary lines on its diagram, and we construct three additional diagrams with erroneous auxiliary lines. 4. Task Execution. U-MLLMs must first generate geometric diagram with auxiliary lines, and then, based on that diagram, solve the problem by selecting the correct answer. Detailed procedures appear in Figure 10(d), and the prompt is provided in Figure 14. Visual CoT. This task evaluates U-MLLMs step-bystep decision-making in maze navigation, simulating realworld problem-solving. Our approach involves: 1. Maze Generation. Using the OpenAI API, we create maze con3https://www.allstarpuzzles.com/spotdiff figurations of varying sizes (33, 44, 55) and layouts. 2. Action Specification. For each step, we manually define actions (Up, Right, Down, Left, Finish) and coordinates, updating the maze layout via the API. 3. QA Construction. - Action Questions. Options are uniformly set as Up, Right, Down, and Left, with the correct answer manually determined. - Coordinate/Image Questions. The correct answers for each steps coordinates and state image are manually defined, and negative samples are also manually specified. 4. Task Execution. U-MLLMs receive the initial maze state and task definition, then are prompted to generate actions, coordinates, and maze images iteratively. After the first step, we add the action, coordinate, and image from the previous decision into the system prompt as history information. The model iterates, outputting each steps decision until the target is reached4. Detailed procedures appear in Figure 10(e), and the prompts are in Figure 15 and 16. Evaluation Strategy. The unified tasks evaluation combines text-based and image-based multiple-choice questions across all subtasks. Our evaluation framework includes: 1. Textual QA Evaluation. For image explanation and editing, we compute CLIP-T similarity between the generated explanation and each option, selecting the one with the highest similarity as correct. For other tasks, UMLLMs directly select the correct option from the provided choices. 2. Image-Based QA Evaluation. We compute CLIP-I similarity between the generated image and each candidate option, selecting the option with the highest score as the models prediction. 3. Task-Specific Rules. For each task we calculate two accuracy metricsacc and acc+where acc is defined as the average of the text option accuracy and the image accuracy, and acc+ represents the accuracy for samples where both the textual and image-based answers are correct. Specifically, for the Visual CoT task, each step is treated as multiple-choice question, and the accuracy of action, accordinate and image are calculated separately, and the average of these three accuracies is calculated as acc, while the accuracy of successfully completing the maze is used to calculate acc+. The detailed calculation process can be found in the Appendix We then calculate the average acc of all subtasks as the unified score, and the overall MME-U score is the average of the understanding, generation, and unified scores. 3. Experiment We evaluate total of 22 MLLMs and U-MLLMs, including DeepSeek-Janus-Pro [3], DeepSeek-JanusFlow [25], SliME [44], VITA-1.5 [14], Gemini2.0-flash [5], 4task requires an average of 3.5 steps per sample, with minimum of two and maximum of seven steps (as shown in Figure 9)."
        },
        {
            "title": "Avg",
            "content": "4104 15.34 20.30 20.64 20.84 20. 23.03 2.81 2.86 7.98 12.51 12. 13.79 14.01 15.10 16.31 16.43 18. 18.58 18.59 28.45 37.17 45."
        },
        {
            "title": "Unify",
            "content": "MME-U Score"
        },
        {
            "title": "IEE CSQ AL SD VCoT Avg",
            "content": "1200 400 364 1964 600 200 200 200 194 1594 200 100 52"
        },
        {
            "title": "Method",
            "content": "SliME-7B VITA-1."
        },
        {
            "title": "QA pairs",
            "content": "Vicuna-7B Qwen-7B 58.50 43.53 36.02 46.02 70.67 56.00 56.04 60.89 Qwen2.5-VL-Instruct Qwen-7B 75.08 53.50 57.14 61. Claude-3.5-sonnet GPT-4o Gemini2.0-flash DALL-E-2 DALL-E-"
        },
        {
            "title": "CogVideoX",
            "content": "- - - - - - - 75.83 53.25 58.52 62.53 74.01 54.50 59.34 62.62 80.92 61.75 64.64 69.10 - - - - - - - - - - - - - - - - Show-o Emu3 Phi-1.5 LLama-8B 32.47 34.75 25.66 30.96 45.75 30.50 23.32 33."
        },
        {
            "title": "HermersFlow",
            "content": "Phi-1.5 41.49 33.00 28.32 34.27 GILL Janus-Flow MiniGPT-5 Janus-Pro VILA-U Anole OPT-6-7B 22.18 6.00 3.56 10. DeepSeek-LLM-1.5b-base 63.17 32.00 35.16 43.44 Vicuna-7B 19.25 10.92 15.93 15.37 DeepSeek-LLM-7b-base 59.56 43.50 42.22 48.43 LLama-7B 51.04 32.25 36.54 39.95 - 17.17 14.50 9.00 13.56 SEED-LLaMA LLaMA2-Chat-13B 49.17 33.00 36.26 39.48 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "Generative Models",
            "content": "- - - - - - - 50.62 51.40 48.82 43.82 51.05 - - - - - - - - - - - - - - - - - - - - - - - - 8.44 8. 23.95 69.62 87.61 37.54 68.05 - -"
        },
        {
            "title": "Unified Models",
            "content": "- - - - - 43. - - 49.08 46.48 50.67 35.71 46.60 - - 32.88 38.96 35.04 35.48 - - - - - - - - - - - 35.29 45.10 49.64 36.64 43.42 41.52 57.00 42.26 41. - - - - - - - - - - - - 7.26 8.18 7.75 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 22.16 24.25 21.29 8.66 6.67 1.90 12.55 5. - - - - - - 18.25 22.80 34.13 14.37 5.00 2.08 15.67 5.88 15.79 - - - - - - - - - - - - 19.91 18.55 59.65 14.42 15.00 3.89 22.30 23.54 22.00 51.49 12.50 22.00 3.61 22. MIO-Instruct MIO-7B Gemini2.0-flash-exp - 52.00 33.50 39.01 41.50 51.24 59.29 43.66 48.23 51.88 66.37 53.45 24.16 38.50 8.66 11.50 0 16. 72.58 68.25 54.90 65.24 - 77.61 43.54 57.56 - - 29.79 38.42 74.75 47.12 26.00 12.41 40. Table 2. Comparison of multimodal models on understanding, generation, unifying tasks, and overall MME-U Score. SIPU: Single Image Perception & Understanding; MITIU: Multiple & Interleaved Image-Text Understanding; VPU: Video Perception & Understanding; CIVG: Conditional Image-to-Video Generation; FIR: Fine-grained Image Reconstruction; TIE: Text-Guided Image Editing; TIG: Text-to-Image Generation; TVG: Text-to-Video Generation; VP: Video Prediction; IEE: Image Editing and Explaining; CSQ: Common Sense Question Answering; AL: Auxiliary Lines; SD: SpotDiff; VCoT: Visual CoT. denotes U-MLLMs with the ability to generate interleaved images and texts, while - indicates that the model is unable to finish the corresponding task and underlined content signifies the best performance within single model across all methods on this task. Gemini2.0-flash-exp [5], Claude-3.5sonnet [1], Emu3 [33], GPT-4o [28], OmniGen [36], DALL-E-2 [26], DALLE-3 [27], CogVideoX[40], HermersFlow [10], Qwen2.5VL-Instruct [31], Show-o [37], VILA-U [35], GILL [18], Anole [4], MIO-Instruct [34], SEED-LLaMA [15], Among the baselines, Chat-UniVi, MiniGPT-5 [48]. Gemini2.0-flash, Claude-3.5-sonnet, GPT-4o5, OmniGen, DALL-E-2, DALL-E-3 are specialized understanding models or generative models. Notably, GILL, Anole, MIOInstruct, SEED-14B, MiniGPT-5 and Gemini2.0-flash-exp can generate interleaved images and texts. Some MLLMs also can generate arbitrarily interlaced modalities, but they are not available as open-source code or model weights yet, such as PUMA [7], VITRON [9] and TextHarmony [47]. 5Currently, the image generation API for GPT-4o is not yet available. We will incorporateit into our evaluation as soon as it becomes accessible. 3.1. Results The evaluation results of various MLLMs in MME-U, as shown in Table 2, indicate that Gemini2.0-flash-exp achieves the highest MME-U score at 45.57. Although compared to MIO-Instruct it does not encompass all subtasks, it demonstrates very balanced performance across understanding, generation, and unify tasks, unlike other models that may exhibit deficiencies in certain test dimensions. It is evident that, compared to traditional MLLMs or generative models, U-MLLMs are capable of handling wider range of tasks, including more complex image-text interleaved reasoning. However, overall, the development of U-MLLMs is still in its early stages, and even the bestperforming models only achieve scores of around 40 on MME-U. Next, we will provide separate analysis of understanding, generation, and unify tasks. Method Metric GILL MiniGPT-5 MIO-Instruct Anole SEED-LLaMA IEE CSQ AL SD VCoT Unify Score Text Acc Image Acc Acc Acc+ Text Acc Image Acc Acc Acc+ Text Acc Image Acc Acc Acc+ Text Acc Image Acc Acc Acc+ Action Acc Coordinate Acc Image Acc Acc Acc+ Acc Acc+ 21.00 21.50 24.12 17.00 19. 27.50 24.25 8.00 14.75 27.82 21.29 4.95 7. 9.62 8.66 1.92 0 13.33 6.67 24.00 22.80 5.00 29.70 38.56 34.13 15.81 5. 23.08 14.37 3.84 4.00 6.00 5.00 2.00 24. 24.16 7.00 77.00 0 38.50 0 17. 0 8.66 0 23.00 0 11. 0 20.10 18.55 3.23 70.30 49.00 59.65 38. 15.38 13.46 14.42 3.84 17.00 13.00 15.00 2. 25.00 22.00 4.50 56.44 46.53 51.49 37.62 13. 11.54 12.50 3.84 23.00 21.00 22.00 4.00 0. 2.08 0 3.47 4.17 0 1. 0 0.69 2.64 Gemini2.0-flash-exp 33.33 43. 38.42 11.11 83.17 63.37 74.75 66.33 59.61 34. 47.12 30.77 28.00 24.00 26.00 5.00 17.64 10. Table 3. Comparison of U-MLLMs on various unify tasks and overall unify Score. 5.00 2.92 0 7.50 4. 9.44 1.90 2.08 0 3.89 3. 12.41 0 0 0 0 0 12.55 2.98 15.67 5.33 16.56 1.40 22.30 9.17 22.32 9. 40.74 22.64 (a) Action Accuracy. (b) Coordinate Accuracy. (c) Image Accuracy. Figure 4. Accuracy distribution across different dimensions on visual cot task. (a) action, (b) location, and (c) image. Understanding. It is evident that Gemini2.0-flashexp[5] demonstrates the best understanding capability among U-MLLMs, while also being closed-source model. For open-source models, the two U-MLLMs with the best understanding capabilities are Janus-Flow [25] and JanusPro [3]. These models utilize two separate vision encoders to handle generation and understanding tasks independently, thus overcoming the limitations of tokenizers like VQGAN [41], which are not well-suited for extracting image understanding features. In contrast, models like Emu3 [33] and Show-o [37], which use single tokenizer for all image tasks, perform poorly on understanding tasks and still show significant performance gap compared to currently available open-source MLLMs of similar size. However, our experiments also show that models like Janus-Pro perform poorly on generation tasks. They even fail to support multimodal generation, scoring zero on unified tasks. Therefore, how to strike balance between understanding and generation capabilities, or whether the two capabilities can indeed complement each other, remains an open question. We also see potential in bridging this gap in understanding capabilities by leveraging existing U-MLLMs alongside strong MLLM baselines. For instance, MIO-instruct [34] achieves impressive understanding results through extensive training data, including video, audio, image-text pairs, and complex three-stage training pipeline. This suggests that U-MLLMs may require broader variety or larger volume of data for training. Generation. We compare the performance gap between various U-MLLMs and current state-of-the-art generative models such as DALLE-3. It is evident that, compared to understanding capabilities, the gap in generation tasks is not as significant. For the simplest TIG task, Gemini-2.0-flashexp even outperforms the best generative model DALLE3 by six points, while U-MLLMs such as EMU3, HermersFlow, and GILL all achieve an average score above 48. However, it is clear that most U-MLLMs still do not perform well on video generation tasks. Notably, although the original paper for Emu3 mentions its capability for video generation, the corresponding checkpoints have not been released. Its clear that the open-source community still has long way to go before U-MLLMs that support video generation become widely available. Detailed results on the generation tasks can be found in Table 4. In Figure 5, we showcase the generation results from various models using the following text prompt: man is standing in park with Run for Rights banner in the background. He is wearing white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is bicycle leaning against tree. It is evident that most generated images, such as those from VILA-U, Show-O, and Janus-Pro, fail to capture key details from the caption, such as the numIn contrast, the results ber on the jersey or specific text. from EMU3 more closely resemble the textual description, while MIO-Instructs outputs are more aligned with realistic scenes (we hypothesize this is because MIO-Instruct was trained on large amount of real-world data, enhancing its ability to generate lifelike images). However, when it comes to image detail, current open-source U-MLLMs still lag significantly behind dedicated generative models. Unify Capability. Our systematic unify task testing shows that, while U-MLLMs have indeed expanded the potential for such tasks compared to traditional understanding/generation models, their performance remains insufficient. For each unify task in Table 2, we require the models to generate the correct image and perform correct reasoning. Under these conditions, even for simple tasks such as answering common questions and generating images, the best open-sourced model (Anole) only achieves an accuracy of 59.65% and accuracy-plus of 38%  (Table 3)  . In other tasks, no open-sourced model is able to surpass the 30% accuracy. It is worth noting that models perform even worse on tasks like Visual CoT, which require multi-step image generation and reasoning. No model is able to successfully complete tasks involving multiple steps. This finding underscores the importance of our MME-U, as relying solely on case studies to demonstrate models mix-modality generation capabilities is clearly insufficient. We will further analyze these models performance, weaknesses, and provide examples in the analysis section. 3.2. Analysis and Findings Trade-off Between Basic and Unified Capabilities. The experimental results reveal that current U-MLLMs face significant challenge in balancing their fundamental abilitiessuch as understanding and generating performancewith the demands of unified tasks that require integrating multiple modalities. For instance, models like GILL, Anole, and MiniGPT-5 are designed to handle unified tasks but tend to exhibit relatively poor performance on basic tasks, which results in lower overall scores when compared to some non-unified MLLMs. On the other hand, while MIO-Instruct demonstrates high performance in basic understanding and generation, its capability to interleave image and text generation effectively is notably deficient. This imbalance suggests that the current training paradigms may not be adequately aligning the learning objectives for basic and unified capabilities within single framework. Detailed Analysis of Model Performance on Unify Tasks. In Table 3, we provide detailed analysis of different models performance on unify tasks, focusing on text reasoning accuracy and image generation accuracy. It is clear that MIO-Instruct exhibits stronger understanding capabilities than generation abilities (as confirmed by the results in Table 2). As result, many of its tasks show high text reasoning performance, particularly in commonsense QA, where its text reasoning accuracy reaches 76.24%. However, it fails to generate correct image, completely missing the potential for mutual reinforcement between In contrast, other models generation and understanding. show comparable performance in both text reasoning and image reasoning evaluation criteria, but their overall results are not impressive. Notably, for visual CoT tasks, despite our efforts to simplify the questions into multiple-choice format, none of the models have been able to correctly complete multi-step reasoning and generation tasks. Poor Instruction Following Ability for Image Generation. There are two main issues with the current models in image generation: 1. Uncontrolled Style Generation. In Figure 17, we present the intermediate state images generated by different models in the VCoT task. Only the Anole and Gemini2.0-flash-exp models are able to generate images with style similar to the initial image. In contrast, other models produce images with clear style bias, which do not align well with our state diagrams. 2. Difficulty Understanding Complex Instructions. Many models, such as MIO-Instruct, struggle with following complex instructions, such as generating auxiliary lines based on the original question. These models fail to generate images with auxiliary lines, often requiring multiple attempts to generate relevant image, and the resulting images often bear little resemblance to the original reference. However, for simpler instructions, like generating an image of dog, these models are able to execute the task correctly. Inadequate Visual CoT Capability in Unified Models. In Figure 4, we further illustrate the challenges of the Visual CoT task. The accuracy of U-MLLMs declines as the number of steps in the VCoT task increases. Errors made in earlier steps compound over time, making it increasingly difficult for models to generate correct actions, coordinates, and images. This cascading error effect highlights fundamental limitation in maintaining consistent reasoning across multi-step tasks. At the same time, this example further emphasizes the high requirements of our unify tasks for both generation and understanding capabilities. For instance, although Anole demonstrates relatively strong image accuracy in Figure 4, its weaker understanding abilities result in less effective action selection. This ultimately leads to worse final results compared to the other two baselines. Due to space limitations, we have included additional indepth analyses in Appendix C, which contains detailed visualizations of the U-MLLMs generation results, as well as specific examples from the unify tasks. 4. Conclusion and Limitation The MME-U benchmark framework presented here serves as foundational step towards evaluating U-MLLMs on diverse array of tasks encompassing multimodal understanding, generation, and their integration. This benchmark reveals the current landscape of U-MLLMs, highlighting their capabilities and areas for improvement. While these models demonstrate proficiency in handling various multimodal tasks, they struggle with balancing understanding and generation, handling complex instructions, and performing well on unify tasks. Moreover, current U-MLLMs (a) Ground Truth. (b) DALLE-2. (c) DALLE-3. (d) Gemini2.0-flash-exp. (e) EMU3. (f) GILL. (g) HermersFlow. (h) Janus-Pro. (i) MiniGPT-5. (j) MIO-Instruct. (k) Show-O. (l) Vila-u Figure 5. The generated results from various models in the text-to-image generation task, based on the following text prompt: man is standing in park with Run for Rights banner in the background. He is wearing white t-shirt with the number 28 on it, grey shorts, and grey socks with black shoes. The park is filled with people, some sitting on benches, and there is bicycle leaning against tree. exhibit significant inconsistencies in aligning textual instructions with their visual outputs, highlighting the need for further research to improve multimodal reasoning and generation integration. However, this study simplifies the evaluation of unify tasks by framing image generation as multiple-choice questions, which may allow model hacking. For instance, SEED-generated images may not meet style standards but achieve high similarity scores, inflating accuracy metrics. Future work will incorporate MLLM or CLIP scores for stricter evaluation."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude 3.5 sonnet, 2024. 7 [2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 6 [3] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv, 2025. 1, 6, 8 [4] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv, 2024. 7 [5] Google DeepMind. Gemini flash, 2024. 6, 7, 8 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 1 [7] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, and Xihui Liu. Puma: Empowering unified mllm with multigranular visual generation. arXiv, 2024. [8] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. NeurIPS, 2025. 5, 1 [9] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. In NeurIPS, 2024. 7 [10] Hermes Flow. Home - hermes flow, 2024. 7 [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv, 2023. 5, 1 [12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv, 2024. 3, 5 [13] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv, 2024. 1 [14] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level arXiv preprint real-time vision and speech interaction. arXiv:2501.01957, 2025. 6, [15] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. In ICLR, 2024. 7 [16] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. NeurIPS, 2023. 5 [17] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 5 [18] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. NeurIPS, 2023. 7 [19] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. In ICLR, 2024. 1 [20] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv, 2023. 5, [21] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv, 2025. 1 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3 [23] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1 [24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? ECCV, 2024. 5 [25] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. NeurIPS, 2024. 1, 6, 8 [26] OpenAI. Dalle 2, 2024. 7 [27] OpenAI. Dalle 3, 2024. 7 [28] OpenAI. Hello gpt-4o, 2024. 7 [29] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In CVPR, 2024. 3, 5, [30] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv, 2024. 1 [31] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv, 2024. 7, 1 [32] Wenhao Wang and Yi Yang. Tip-i2v: million-scale real text and image prompt dataset for image-to-video generation. arXiv, 2024. 5, 1 [33] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv, 2024. 1, 7, 8 [34] Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, et al. Mio: foundation model on multimodal tokens. arXiv, 2024. 7, 8, 1 [35] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv, 2024. 7, [36] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv, 2024. 7 [37] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. ICLR, 2025. 1, 7, 8 [38] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In CVPR, 2016. 3, 5, 1 [39] Yibo Yan, Shen Wang, Jiahao Huo, Hang Li, Boyan Li, Jiamin Su, Xiong Gao, Yi-Fan Zhang, Tianlong Xu, Zhendong Chu, et al. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv, 2024. 1 [40] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. ICLR, 2025. [41] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. ICLR, 2022. 8 [42] Tao Yu, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, et al. Aligning multimodal llm with human preference: survey. arXiv preprint arXiv:2503.14504, 2025. 1 [43] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. NeurIPS, 2023. 5 [44] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 6, 1 [45] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv, 2025. 1 [46] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? ICLR, 2025. 3, 5, 1 [47] Zhen Zhao, Jingqun Tang, Binghong Wu, Chunhui Lin, Shubo Wei, Hao Liu, Xin Tan, Zhizhong Zhang, Can Huang, and Yuan Xie. Harmonizing visual text comprehension and generation. ArXiv, 2024. [48] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt5: Interleaved vision-and-language generation via generative vokens. arXiv, 2023. 7 [49] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. ICLR, 2025. 1 MME-Unify: Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Related Works sess unified tasks, and the range of tasks is limited. Unified Multimodal Large Language Models. Building on the success of MLLMs [14, 31, 42, 44], recent studies U-MLLMs, which can understand and generate multiple modalities in an end-to-end manner. Some approaches have adopted unified training objective, projecting both text and images into discrete token space and employing next-token prediction loss function for training [30, 34, 35]. This training method and framework are notably straightforward. However, using discrete image tokens (e.g., extracted from VQVAE image features) may not be optimal for image understanding tasks. Therefore, works like JanusFlow [3], Janus-Pro [3], among others, have employed different vision encoders such as VQVAE for image generation and SigLIP for image comprehension, significantly enhancing the understanding capabilities of U-MLLMs. Additionally, other methods have found that diffusion training is more suitable for image generation. Thus, adopting diffusion-based training for image generation and nexttoken prediction for text generation aims to strengthen the image generation capabilities further [37, 49]. Recent research has also explored fine-tuning U-MLLMs to further enhance their performance on unified tasks [21]. However, despite the rapid advancements of U-MLLMs, there remains lack of comprehensive benchmarks for systematically and fairly evaluating their capabilities in understanding, generation, and multimodal synthesis tasks. Benchmarks for Understanding. With the rapid development of MLLMs, several concurrent works [13] have proposed various benchmarks to evaluate the models capabilities in multimodal comprehension tasks, such as single-image perception and understanding [11, 46] (e.g., MME series), interleaved image & text understanding, and video understanding [8] (e.g., MMBench-Video, VideoMME). Additionally, some benchmarks focus on multimodal safety [45] or mathematical reasoning [39]. These benchmarks differ in coverage and metrics. Benchmarks for Generation. Various benchmarks have been proposed to assess multi-modal generation capabilities [6, 19, 20, 29, 32, 38], including tasks like image reconstruction [6], image editing [19, 29], and conditional image & video generation [23, 32]. However, these benchmarks mainly focus on individual tasks within single modalities, failing to capture the full scope of multi-modal comprehension and generation. While some benchmarks, such as SEED-Bench-2 [20], provide hierarchical evaluation for both understanding and generation, they do not asB. Evaluation Metrics B.1. Understanding Score Let the three subtasks in the Understanding Task be formally defined as follows: = {SIPU, MITIU, VPU}. For each subtask , let Qt represent the set of multiple-choice questions, where each question Qt has exactly one correct answer. To evaluate correctness, we define the indicator function for each question as follows: (cid:40) It(q) = if the selected answer for is correct, 1, 0, otherwise. The accuracy for subtask is given by: acct = 1 Qt (cid:88) qQt It(q). Since equal weights are assigned to each subtask, the Understanding Score (US) is computed as the arithmetic mean of the accuracies across all subtasks: US = 1 3 (cid:88) tT scoret, = {SIPU, MITIU, VPU}. B.2. Generation Score The generative task comprises six subtasks: = {CIVG, TVG, VP, FIR, TIE, TIG}. All metric scores are normalized to the range [0, 100]. Normalization of FVD and FID Scores. Let denote the raw FVD or FID value for sample, where [1, 1000] and lower values indicate better performance. The normalized score is computed as: (cid:18) = 100 1 (cid:19) 1 1000 (cid:18) = 100 1 (cid:19) . 1 This ensures: = 100 when = 1 (best performance), = 0 when = 1000 (worst performance). Figure 6. Data samples from understanding task, which includes single-image perception and reasoning, multi-image and image-text interlaced perception and reasoning, video perception and reasoning, etc. If all raw scores across models are identical, each normalized score is set to 100 to maintain consistency in evaluation and prevent division by zero in the normalization process. score for {CIVG, TVG} is given by: scoret = FVD(t) norm + CLIPSIM(t) norm + FID(t) 3 . Score Calculation for CIVG and TVG. The subtask Score Calculation for VP. The VP subtask score is deFigure 7. Data samples from generation task. It includes subtasks such as Text-to-Image Generation, Text-to-Image Editing, Fine-Grained Image Reconstruction, Text-to-Video Generation, conditional Image-to-Video Generation, and Video Prediction. termined using the following formula: B.3. Unify Score scoreVP = FVD(VP) norm + FID(VP) norm 2 ."
        },
        {
            "title": "Let the Unify Task consist of the subtasks",
            "content": "T = {IEE, CSQ, AL, SD, VCoT}. Score Calculation for FIR, TIE, and TIG. For FIR (Fine-Grained Image Reconstruction), the evaluation metric is LPIPS. To ensure higher values indicate better performance, the score is defined as: scoreFIR = 1 LPIPS. For both TIE (Text Image Editing) and TIG (Text-toImage Generation), two metrics are used: CLIP-I and CLIPT. The score for each subtask is computed as the average of these two metrics: scoreTIE = scoreTIG = CLIP-ITIE + CLIP-TTIE 2 , CLIP-ITIG + CLIP-TTIG 2 . For each subtask , denote by St the set of samples. B.3.1. Subtasks IEE, CSQ, AL, SD For given subtask {IEE, CSQ, AL, SD} and for each sample St, there are two questions: 1. text-based multiple-choice question. 2. An image-based multiple-choice question. Define the indicator functions for the text and image responses as follow: (cid:40) (cid:40)"
        },
        {
            "title": "Itext\nt",
            "content": "(s) = 1, 0, otherwise, if the text answer for is correct,"
        },
        {
            "title": "Iimg\nt",
            "content": "(s) = 1, 0, otherwise. if the image answer for is correct, Overall Generation Score. The overall Generation Score (GS) is the arithmetic mean of all six subtask scores: Then, the text accuracy and image accuracy for subtask are, respectively, GS = 1 6 (cid:88) tT scoret, = {CIVG, TVG, VP, FIR, TIE, TIG}. acctext = 1 St (cid:88) sSt"
        },
        {
            "title": "Itext\nt",
            "content": "(s), accimg = 1 St (cid:88) sSt"
        },
        {
            "title": "Iimg\nt",
            "content": "(s). The overall accuracy for subtask is then defined as the average of the two: The overall accVCoT metric is the arithmetic mean of these three component accuracies: acct = acctext + accimg . accVCoT = accaction VCoT + acccoord VCoT + accimg"
        },
        {
            "title": "VCoT",
            "content": ". Additionally, we define acc+ to represent the accuracy for samples where both the textual and image-based answers are correct: acc+ = 1 St (cid:88) sSt"
        },
        {
            "title": "Itext\nt",
            "content": "(s) Iimg (s). B.3.2. Subtask VCoT For the VCoT subtask, each sample SVCoT represents maze navigation task composed of Ks sequential steps. For each step {1, 2, . . . , Ks}, there are multiple-choice questions evaluating the models prediction of: 1. An action. 2. coordinate. 3. An image. Calculation of accVCoT: Let Nsteps = (cid:80) Ks be the total number of steps across all samples in the VCoT subtask. Define the indicator functions for the correctness of action, coordinate, and image predictions for step of sample as follow: sSVCoT Iaction VCoT(s, k) = 1, if the action prediction for step of sample is correct, 0, otherwise. Icoord VCoT(s, k) = if the coordinate prediction for step of sample is correct, 0, otherwise. Iimg VCoT(s, k) = if the image prediction for step of sample is correct, 0, otherwise. Calculate the average accuracy for each prediction type across all steps: accaction VCoT = acccoord VCoT = accimg VCoT ="
        },
        {
            "title": "1\nNsteps",
            "content": "(cid:88) Ks(cid:88) sSVCoT k=1 (cid:88) Ks(cid:88) sSVCoT k=1 (cid:88) Ks(cid:88) sSVCoT k= Iaction VCoT(s, k), Icoord VCoT(s, k), Iimg VCoT(s, k). 1, 1, Calculation of acc+VCoT: Define an indicator function for the full correctness of single step in sample s: Istep all correct(s, k) = Iaction VCoT(s, k)Icoord VCoT(s, k)Iimg VCoT(s, k). This function is 1 if all three predictions for step are correct, and 0 otherwise. Now, define the indicator function for the perfect completion of sample s:"
        },
        {
            "title": "Isample perfect\nVCoT",
            "content": "(s) = 1, if Istep all correct(s, k) = 1 for all {1, 2, . . . , Ks}, 0, otherwise. The acc+VCoT metric is the proportion of perfectly completed samples: acc+VCoT = 1 SVCoT (cid:88) sSVCoT"
        },
        {
            "title": "Isample perfect\nVCoT",
            "content": "(s)."
        },
        {
            "title": "Unify Scores",
            "content": "The Unify Score (Unify-S) is defined as the arithmetic mean of the acct metrics across all subtasks: Unify-S = 1 (cid:88) tT acct, B.4. MME-U Score The MME-U Score is computed as the arithmetic mean of the Understanding Score (US), Generation Score (GS), and Unify Score (Unify-S): MME-U = 1 3 (US + GS + Unify-S) . where: US is the Understanding Score, GS is the Generation Score, Unify-S is the Unify Score."
        },
        {
            "title": "Each component score is calculated as described in their",
            "content": "respective sections. C. Extended Experimental Results C.1. Most U-MLLMs Exhibit Inferior Generation"
        },
        {
            "title": "Capabilities",
            "content": "While the methods in Table 4 show relatively small differences compared to the current state-of-the-art (SOTA) generation techniques, we found that using CLIP scores for evaluation introduces certain risks of manipulation. 8.44 8.57 23.95 37.54 5. 5.88 7.26 7.75 8.18 15.80 18. 19.91 22.16 23.54 29.79 53."
        },
        {
            "title": "Samples",
            "content": "1,200 400 364 200 200 200 600 200 194 Method Metric DALL-E-2 DALL-EOmniGen CIVG FIR TIE TIG TVG VP Generation Score FVD Score FID Score CLIPSIM Avg 1-LPIPS Avg CLIP-I CLIP-T Avg CLIP-I CLIP-T Avg FVD Score FID Score CLIPSIM Avg FVD Score FID Score Avg Avg - - - - - - - - - - - - - - - - - - - - - - 69.33 31.91 50.62 70. 32.68 51.40 48.82 48.82 65.63 22. 43.82 73.97 28.12 51.05 - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "Generative Models",
            "content": "- - - - 87.82 84. 36.77 69.62 89.92 85.30 87.61 CogVideoX 83.91 87.02 33.23 68.05 DeepSeek-Flow DeepSeek-Janus-Pro Show-o HermersFlow Emu3 VILA-U MiniGPT-5 Anole GILL SEED-LLaMA Gemini-2.0-flash-exp - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "Unified Models",
            "content": "- - - - - - 52.38 13.38 32.88 55.46 15.11 35. 62.10 24.97 43.54 65.37 27.58 46. 68.54 29.62 49.08 - - - - - - - - - - - - - - - - - - - - 62. 27.66 45.10 57.35 66.36 25.22 49. 38.96 38.96 55.86 14.21 35.04 56. 14.62 35.48 36.64 36.64 62.35 21. 41.80 60.23 21.75 41.00 50.67 50. 54.15 17.27 35.71 67.75 25.44 46. 57.00 57.00 67.12 17.39 42.26 60. 23.34 41.96 77.61 77.61 67.77 19. 43.54 84.59 30.53 57.56 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - MIO-Instruct 59.93 70. 23.41 51.24 59.29 59.29 68.12 19. 43.66 72.69 23.77 48.23 60.03 69. 26.40 51.88 64.08 68.66 66.37 Table 4. Comparison of multimodal models on various generation tasks. CIVG: Conditional Image-to-Video Generation; FIR: Finegrained Image Reconstruction; TIE: Text-Guided Image Editing; TIG: Text-to-Image Generation; TVG: Text-to-Video Generation; VP: Video Prediction. denotes MLLMs with the ability to generate interleaved images and texts, while - indicates that the model does not have the ability to achieve the corresponding task and underlined content signifies the best performance within single model across all methods on this task."
        },
        {
            "title": "Dataset",
            "content": "TIPI2V EmuEdit Imagen Hub Understanding Task 0 0 0 0 0 0 0"
        },
        {
            "title": "Generative Task",
            "content": "0 0 0 200 0 0 200 0 0 0 0 0 Unify Task 0 0 0 0"
        },
        {
            "title": "MME MMBench",
            "content": "MMERealworld SEEDBench-2 VideoMME"
        },
        {
            "title": "Image\nNet",
            "content": "MSRVTT"
        },
        {
            "title": "Pexel\nVideos",
            "content": "Geometry 3K"
        },
        {
            "title": "Open\nAI",
            "content": "400 0 0 400 0 0 0 0 0 0 0 0 0 0 0 0 0 0 400 0 0 0 0 0 0 0 0 400 0 0 0 364 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 400 0 0 0 0 0 0 200 0 0 0 0 0 0 0 200 0 0 0 0 0 0 0 0 0 0 0 200 0 0 0 0 0 0 0 0 194 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 IEE CSQ AL SD VCoT Dataset Total Dataset % 9.75% 9.75% 0 0 0 0 0 400 0 0 0 0 0 400 0 0 0 0 0 400 200 100 52 104 90 4104 9.75% 9.75% 8.87% 9.75% 9.75% 4.87% 4.87% 4.87% 4.87% 4.73% 1.27% 2.54% 4.63% 100% 200 0 0 0 0 400 0 0 0 104 0 104 0 100 0 0 90 190 0 0 0 0 0 364 0 0 0 0 0 200 0 0 0 0 0 0 0 0 0 0 200 0 0 0 0 0 200 0 0 0 0 0 194 0 0 0 0 0 400 0 0 52 0 0 52 Table 5. Task-Dataset Sampling Statistics. This table presents the distribution of samples across different multimodal AI tasks and their source datasets. Tasks are categorized into three main groups: Understanding Tasks (SIPU: Single Image Perception and Understanding, MITIU: Multi-Image & Interleaved Text-Image Understanding, VPU: Video Perception and Understanding), Generative Tasks (CIVG: Conditional Image-to-Video Generation, FIR: Fine-grained Image Reconstruction, TIG: Text-to-Image Generation, TIE: TextGuided Image Editing, TVG: Text-to-Video Generation, VP: Video Prediction), and Unify Tasks (IEE: Image Editing and Explanation, CSQ: Common Sense Question Answring, AL: Auxiliary Lines., SD: SpotDiff, VCoT: Visual CoT). The rightmost column shows the total number of samples used for each task across all datasets. value of 0 indicates that no samples were drawn from that dataset for the corresponding task. In Figure 18, we present the results on the fine-grained image reconstruction task. For each model, we used unified prompt: Reconstruct high-fidelity images from degraded inputs, preserving fine-grained details, textures, and structural integrity with perceptual realism. It is evident that GILL, SEED-LLaMA, and MIO-Instruct effeceffectively but also accurately implements modifications according to the editing instructions. Figure 20 illustrates the performance gap between pure video generation models and U-MLLMs on the conditional image-to-video generation task. Using the text prompt The man is so tired. -camera zoom in, we observe that although MIO-Instruct produces video outputs with richer visual details compared to CogVideoX, it struggles to effectively generate coherent video sequence that adheres to the given instruction based on the initial image. In Figure 21, the generation results of CogVideoX and MIO-Instruct in the Text-to-Video Generation task are compared. The results clearly indicate that, in terms of both instruction adherence and video consistency, MIO-Instruct significantly underperforms compared to dedicated video generation models. Overall, while some U-MLLMs exhibit promising capabilities in capturing visual details and producing highfidelity reconstructions, challenges remain in faithfully executing complex editing instructions and generating consistent video sequences. These findings highlight critical areas for further improvement in enhancing the generation capabilities of U-MLLM systems. C.2. Challenges in Simultaneously Generating High-Quality Text and Images in U-MLLMs Figures 22, 23, 24, and 25 present the results of U-MLLMs on the Unify tasks. Notably, MIO-Instruct fails to perform any text-image generation across all Unify tasks, GILL is unable to generate multimodal outputs in the SpotDiff task, and SEED-LLaMA does not support text-image generation in the Auxiliary Lines task. Overall, these results indicate that most U-MLLMs struggle to generate images that faithfully adhere to provided instructions or reference images, and their comprehension of the instructions is often flawed. In the Image Editing and Explanation task, for instance, MiniGPT-5 produced images that bore no relation to the source images. Additionally, the textual outputs from GILL, MiniGPT-5, and SEED-LLaMA were insufficient for accurately describing the editing objects or the instructions. Similarly, in both the Commonsense Question Answering and SpotDiff tasks, although MiniGPT-5 and SEED-LLaMA correctly answered the textual multiplechoice questions, the images they generated were clearly unrelated to the corresponding options. This further emphasizes the difficulty U-MLLMs face in maintaining consistency between textual and visual outputs. For the Auxiliary Lines task, while Anole managed to generate images that retained some of the visual details of the source images, it failed to correctly draw the required auxiliary lines as per the instructions. GILL and MiniGPT5, on the other hand, generated content that was completely disconnected from the original images. Figure 8. An overview of real-life scenarios included in the Understanding Task. The scores in the bars represent the proportion of the number of samples of the corresponding scenario to the total number of samples of the task. Figure 9. Distribution of steps required for samples of mazes of different sizes in the Visual CoT task. tively capture the structural details of the input images and produce noticeably clearer outputs. In particular, SEEDLLaMA and MIO-Instruct demonstrate strong performance in restoring color fidelity, while Gemini2.0-flash-exp tends to preserve the integrity of the input images. In contrast, MiniGPT-5 and Anole fail to effectively extract the necessary visual information: while MiniGPT-5 does generate an image, its output deviates significantly from the source, and Anole is unable to generate coherent image at all. Figure 19 displays the results for the text-guided image editing task, where the editing instruction was Change this image into watercolor art. Similar to the reconstruction task, SEED-LLaMA and MIO-Instruct generate images that more closely resemble the source image; however, they fall short in accurately executing the specified editing instruction. Meanwhile, GILL, MiniGPT-5, and Anole show limited capability in capturing and manipulating the requisite visual details for the transformation. Notably, Gemini2.0flash-exp not only preserves the content of the source image Figure 10. The overall construction process for five unified tasks, which consists of (a) Common Sense Question Answering, (b) Image Editing and Explaining, (c)SpotDiff, (d) Auxiliary Lines, and (e) Visual CoT. Figure 11. System prompt for Common Sense Question Answering task. These findings suggest several critical limitations in current U-MLLM systems. First, there is notable gap in their ability to integrate and utilize multimodal cues effectively, as evidenced by the misalignment between textual instructions and visual outputs. Second, while some models can capture certain visual details, they often lack the robust reasoning required to follow complex instructions, especially in tasks demanding precise visual modifications. Finally, the decoupling between text and image generation in these systems underscores the need for further research aimed at improving cross-modal coherence and instruction fidelity. Overall, the experimental results highlight that, despite progress in individual modalities, existing U-MLLMs have considerable challenges in simultaneously generating highquality, coherent text and images that align with complex, multimodal instructions. Figure 12. Systemp prompt for Image Editing and Explaining task Figure 13. System prompt for SpotDiff task. Figure 14. System prompt for Auxiliary Lines task. Figure 15. Systemp prompt for Visual CoT task in the first step. Figure 16. Systemp prompt for Visual CoT task after first step. (a) Anole. (b) GILL. (c) SEED. (d) MiniGPT-5. (e) Gemini2.0-flash-exp. Figure 17. Intermediate process images generated by different models in VCoT. The figure illustrates the intermediate outputs of various models in the VCoT (Visual Composition Task), showing distinct approaches in processing and generating visual content. The models shown include (a) Anole, (b) GILL, (c) SEED, (d) MiniGPT-5, and (e) Gemini-2.0-flash-exp, each producing unique visual patterns and compositions. (a) Source Image. (b) OmniGen. (c) GILL. (d) MiniGPT-5. (e) Anole. (f) SEED-LLaMA. (g) MIO-Instruct. (h) Gemini2.0-flash-exp. Figure 18. The generated results from various models in the fine-grained image reconstruction task, based on the following text prompt: Reconstruct high-fidelity images from degraded inputs, preserving fine-grained details, textures, and structural integrity with perceptual realism. (a) Source Image. (b) Ground Truth. (c) GILL. (d) MiniGPT-5. (e) Anole. (f) SEED-LLaMA. (g) MIO-Instruct. (h) Gemini2.0-flash-exp. Figure 19. The generated results from various models in the text-guided image editing task, based on the following text prompt: Change this image into watercolor art. (a) Source Image. (b) CogVideoX. (c) MIO-Instruct. Figure 20. The generated results from various models in the conditional image-to-video generation task, based on the following text prompt: The man is so tired. -camera zoom in. (a) CogVideoX. (b) MIO-Instruct. Figure 21. The generated results from various models in the text-to-video generation task, based on the following text prompt: Men wearing sunglasses and women with hats take photographs of themselves. Figure 22. The generated results from various models in the image editing and explaining task. Figure 23. The generated results from various models in the common sense question answering task. Figure 24. The generated results from various models in the auxiliary lines task. Figure 25. The generated results from various models in the spotdiff task."
        }
    ],
    "affiliations": [
        "CASIA",
        "M-M-E Project",
        "NJU",
        "PKU",
        "Vivo"
    ]
}