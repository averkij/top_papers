{
    "paper_title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
    "authors": [
        "Xuefeng Du",
        "Reshmi Ghosh",
        "Robert Sim",
        "Ahmed Salem",
        "Vitor Carvalho",
        "Emily Lawton",
        "Yixuan Li",
        "Jack W. Stokes"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 6 9 2 0 0 . 0 1 4 2 : r VLMGUARD: DEFENDING VLMS AGAINST MALICIOUS PROMPTS VIA UNLABELED DATA Xuefeng Du1, Reshmi Ghosh2, Robert Sim2, Ahmed Salem2, Vitor Carvalho2, Emily Lawton2, Yixuan Li1, Jack W. Stokes2 1 University of Wisconsin - Madison, 2 Microsoft Corp. {reshmighosh,rsim,ahmsalem,vitor.carvalho,elawton,jstokes}@microsoft.com {xfdu,sharonli}@cs.wisc.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. major challenge in developing safeguarding prompt classifier is the lack of large amount of labeled benign and malicious data. To address the issue, we introduce VLMGUARD, novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGUARD achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised."
        },
        {
            "title": "Introduction",
            "content": "Safeguarding vision language models (VLMs) against persistent threats of adversarial prompts has become crucial yet challenging problem in safely deploying these multimodal foundation models in the wild, where the user prompts in the deployment time can naturally arise from mixture distribution of both benign and malicious sources (Zou et al., 2024; Liu et al., 2023b; Yin et al., 2024). Compared with text-only language models, Modern VLMs process both text and images, making them particularly vulnerable to malicious prompts, which can target not only the textual input but also the visual component and thus allow attackers to manipulate both channels simultaneously (Zhang et al., 2024). These malicious prompts can elicit harmful outputs (Shayegani et al., 2024) or trigger unintended actions of VLM-integrated tools, such as personal assistants (Yi et al., 2023), and thus place critical decision-making at risk. This risk underscores the need for VLMs to not only generate coherent responses but also detect potentially malicious prompts before producing outputs (Alon & Kamfonas, 2023; Xie et al., 2024). Malicious prompt detection, which involves determining whether user-provided input is harmful, is essential for the safe deployment of VLMs. However, primary challenge in learning safeguarding prompt classifier is the limited availability of labeled datasets that include both benign and malicious samples. Constructing reliable datasets often requires extensive human annotation, which is time-consuming and difficult to scale given the evolving nature of generative models and the diversity of user inputs. Ensuring the quality of such labeled data further demands rigorous quality control, making manual annotation an unsustainable solution as models and user interactions become more complex. These significant challenges highlight the necessity of exploring methods that leverage unlabeled data for effective malicious prompt detection. Work done while at Microsoft VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Figure 1: Illustration of our framework VLMGUARD for malicious prompt detection, leveraging unlabeled user prompts in the models deployment environment. It first extracts the latent subspace from VLM representations to estimate the maliciousness of the prompt and then calculate the membership (benign vs. malicious) for samples in unlabeled data D. Such membership enables learning binary safeguarding prompt classifier. Motivated by these challenges, we introduce VLMGUARD, novel learning framework designed to leverage unlabeled user data in the wild to enable the language model to distinguish between benign and malicious prompts. Unlabeled data naturally arises from interactions on chat-based platforms, where vision language model such as LLaVA (Liu et al., 2024b) deployed in the wild can receive vast quantities of multimodal queries. This data frequently contains blend of benign and potentially malicious content, such as those aimed at circumventing safety restrictions (Niu et al., 2024) or manipulating the model into executing unintended actions (Bagdasaryan et al., 2023). Formally, we conceptualize these unlabeled user prompts as mixed composition of two distributions: Punlabeled = π Pmalicious + (1 π) Pbenign, where Pmalicious and Pbenign respectively denote the distribution of malicious and benign data, and π is the mixing ratio. Leveraging unlabeled data in this context is non-trivial due to the absence of explicit labels indicating whether sample belongs to the benign or malicious category. To address this, our framework introduces an automated maliciousness estimation score, enabling the differentiation of benign and malicious samples within unlabeled data. This differentiation facilitates the subsequent training of binary safeguarding prompt classifier. Central to our approach is the exploitation of the language models latent representations, which encapsulate features indicative of malicious intent. Specifically, VLMGUARD identifies subspace within the activation space corresponding to malicious prompts. An embedding is considered potentially malicious if its representation strongly aligns with this subspace (see Figure 1). This concept is operationalized through decomposition in the VLM representation space, where the top singular vectors define the latent subspace for maliciousness estimation. The maliciousness estimation score is computed as the norm of the embedding projected onto these top singular vectors, which exhibits distinct magnitudes for benign and malicious data. Our estimation score provides clear mathematical interpretation and is straightforward to implement in practice. Extensive experiments on contemporary VLMs demonstrate that our approach VLMGUARD can effectively enhance malicious prompt detection performance across different types of malicious data (Sections 4.2). Compared to the state-of-the-art methods, VLMGUARD achieves substantial improvement in detection accuracy, improving AUROC by 13.21% on average for LLaVA model. Additionally, we conduct an in-depth analysis of the key components of our methodology (Section 4.4) and further extend our investigation to illustrate VLMGUARDs scalability and robustness in addressing real-world challenges (Section 4.3). Our key contributions are as follows: We introduce VLMGUARD, framework that formalizes the problem of malicious prompt detection by leveraging unlabeled user prompts in the wild. This formulation offers strong practicality and flexibility for real-world applications. We introduce scoring function derived from VLM representations to estimate the likelihood of prompt being malicious, enabling effective classification in unlabeled data. We conduct extensive ablations to understand the efficacy of various design choices in VLMGUARD, and validate its scalability to large VLMs and different malicious data. These findings offer systematic and comprehensive understanding of how to leverage unlabeled data for malicious prompt detection, providing insights for future research."
        },
        {
            "title": "2 Problem Setup",
            "content": "Formally, we describe the vision language model and the problem of malicious prompt detection. 2 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Definition 2.1 (Vision language model). We consider an L-layer causal VLM, which takes sequence of textual tokens xt m} to generate output text tokens = {xn+m+1, ..., xn+m+o} in an autoregressive manner. Each output token xi, [n + + 1, ..., + + o] is sampled from distribution over the model vocabulary V, conditioned on the prefix {x1, ..., xi1}: n} and visual tokens xv prompt = {xv prompt = {xt 1, ..., xv 1, ..., xt xi = argmaxxV (x{x1, ..., xi1}), (1) and the probability is calculated as: (x{x1, ..., xi1}) = softmax(wfL(x) + b), (2) where fL(x) Rd denotes the representation at the L-th layer of VLM for token x, and w, are the weight and bias parameters at the final output layer. Definition 2.2 (Malicious prompt detection). We denote Pmalicious as the joint distribution over the visual and textual prompts where the VLM generations are malicious, which is referred to as malicious distribution. For any user-provided prompt (xv prompt) Xprompt, the goal of malicious detection is to learn binary predictor : Xprompt {0, 1} such that prompt, xt G(xv prompt, xt prompt) = (cid:26) 1, 0, if (xv prompt, xt prompt) Pmalicious otherwise (3)"
        },
        {
            "title": "3 Proposed Approach",
            "content": "In this paper, we propose learning framework that facilitates malicious prompt detection by leveraging unlabeled user prompts collected in real-world settings. These prompts naturally arise from user interactions within chat-based applications. For instance, consider vision-language model such as LLaVA (Liu et al., 2024b) deployed in the wild, which processes vast array of visual and textual user queries. This data can be collected with user consent, yet often contains mixture of benign and potentially malicious content. Formally, the unlabeled user prompts can be modeled using the Huber contamination model (Huber, 1992) as follows: Definition 3.1 (Unlabeled prompt distribution). We define the unlabeled VLM user prompts to be the following mixture of distributions Punlabeled = (1 π)Pbenign + πPmalicious, (4) where π (0, 1). Note that the case π = 0 is idealistic since no malicious information occurs. In practice, π can be moderately small value when most of the user prompts remain benign. Definition 3.2 (Empirical data). An empirical set = {(xv,1 prompt, xt,1 prompt)} is sampled independently and identically distributed (i.i.d.) from this mixture distribution Punlabeled, where is the number of samples. Note that we do not have clear membership (benign or malicious) for the samples in D. prompt), ..., (xv,N prompt, xt,N Overview. Despite the availability of unlabeled user prompt datasets, leveraging such data presents significant challenges due to the absence of explicit labels indicating whether samples are benign or malicious within the mixture data D. To overcome this challenge, our framework VLMGUARD is designed to create an automated function that estimates the maliciousness of samples in the unlabeled data. This functionality enables the subsequent training of binary classifier (see Figure 1). We detail these two steps in Section 3.1 and Section 3.2, respectively. Our study represents an initial effort to address this intricate problem and provides foundation for future research on leveraging unlabeled data for malicious prompt detection. 3.1 Estimating Maliciousness in the Latent Subspace The first step in our framework is to estimate the maliciousness of data instances within mixed dataset D. The effectiveness of distinguishing between benign and malicious data depends on the language models ability to capture features that are indicative of malicious intent. Our key idea is that if we could identify latent subspace associated with malicious prompts, it might enable their separation from benign ones. We formally describe the procedure below. Representation decomposition. To realize the idea, we first extract embeddings from the VLM for samples in the unlabeled mixture D. Specifically, let RN denote the matrix of embeddings extracted from the vision language model for samples in D, where each row represents the embedding vector prompt). To identify the latent subspace, we analyze principal components of the extracted representations via singular value decomposition (Klema & Laub, 1980): of data sample (xv,i prompt, xt,i fi := fi µ = UΣV, (5) VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data where µ Rd is the average embedding across all samples, and is used to center the embedding matrix. The columns of and are the left and right singular vectors, and they form an orthonormal basis. In principle, the decomposition can be applied to any layer of the VLM representations, which will be analyzed in Section 4.4. Such decomposition is useful, because it enables discovering the most important spanning direction of the subspace for the set of points in D. Maliciousness estimation. To build intuition, we start by considering simplified case where the subspace is onedimensional, represented as line through the origin. Finding the best-fitting line through the origin for set of points {fi1 } involves minimizing the sum of the squared perpendicular distances from the points to the line. Geometrically, identifying the first singular vector v1 is also equivalent to maximizing the total distance from the projected embeddings (onto the direction of v1) to the origin, summed over all points in D: v1 = argmax v2=1,vRd (cid:88) i=1 fi, v2 , (6) where , denotes the dot product operator. As illustrated in Figure 2, malicious data samples tend to exhibit anomalous behavior compared to benign user prompts, often positioning themselves farther away from the center. This reflects the practical scenarios where minority of the generations are malicious, while the majority are benign. To determine the membership, we define the maliciousness estimation score as κi = fi, v12, which measures the norm of fi projected onto the top singular vector. This scoring enables us to assign membership to each unlabeled user prompt based on the relative magnitude of the maliciousness score (see the score distribution on practical datasets and its design rationale in Appendix B). Figure 2: Visualization of the representations for benign (in orange) and malicious samples (in purple), and their projection onto the top singular vector v1 (in gray dashed line). Our maliciousness estimation score provides straightforward mathematical interpretation and is easily implementable in practical applications. Furthermore, the score can be generalized to utilize subspace of orthogonal singular vectors: κi = 1 (cid:88) j=1 λj fi, vj2 , (7) where vj is the jth column of V, and λj is the corresponding singular value. Here, represents the number of spanning directions in the subspace. The underlying intuition is that malicious samples can effectively be captured by small subspace, thereby distinguishing them from benign samples. We show in Section 4.4 that leveraging the subspace with multiple components can capture the maliciousness encoded in VLM activations more effectively than single direction. 3.2 Training the Safeguarding Prompt Classifier prompt, xt,i Following the procedure outlined in Section 3.1, we define the (potentially noisy) set of malicious prompts and = {(xv,i prompt) : κi }. We then proceed to train safeguarding prompt classifier hθ, which is specifically designed to optimize the distinction between these two sets. In particular, the training objective can expressed as minimizing the following risk, where samples from should be classified as positive, and those from as negative: prompt) : κi > } and the candidate benign set = {(xv,i prompt, xt,i LM,B(hθ) = L+ M(hθ) + (hθ) = + (xv prompt,xt prompt)M (xv prompt,xt prompt)B 1{hθ(xv 1{hθ(xv prompt, xt prompt, xt prompt) 0} prompt) > 0}. (8) Given the impracticality of directly minimizing the 0/1 loss, we substitute it with binary sigmoid loss, providing smooth and more computationally feasible alternative. At the test stage, the trained prompt classifier is utilized prompt) for malicious prompt detection, using malicious scoring function S(xv prompt) , where (xv prompt) denotes the test visual and textual prompt. Based on this score, we classify the input as malicious if Gτ (xv prompt) τ }, with 1 indicating malicious prompt and 0 otherwise. prompt) = ehθ (xv 1+ehθ (xv prompt) = 1{S(xv prompt, xt prompt, xt prompt, xt prompt, xt prompt,xt prompt,xt 4 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data"
        },
        {
            "title": "4 Experiments and Analysis",
            "content": "In this section, we present empirical evidence to validate the effectiveness of our method on real-world malicious prompt detection tasks. We describe the setup in Section 4.1, followed by the results and comprehensive analysis in Section 4.2Section 4.4. 4.1 Setup Datasets and models. We evaluate our approach under two threat modelsadversarial meta-instruction and jailbreak prompts. For the meta-instruction, we leverage the dataset from Zhang et al. (2024), which comprises 25 benign and 300 malicious images in ImageNet, each associated with 60 questions. Malicious images are generated by injecting adversarial noise into benign data using projected gradient descent (PGD) (Madry et al., 2017) over 40 training questionanswer pairs. These pairs are categorized under one of five meta-objectives: LANGUAGE, POLITICS, FORMALITY, SPAM, and SENTIMENT. For instance, the malicious images are optimized to prompt VLMs to produce biased responses, such as answers in different languages, with political or formality bias, sentiment alterations, or appended spam texts. To simulate the unlabeled prompt data Punlabeled, we mix the benign image-text pairs (Pbenign) with malicious pairs (Pmalicious) under various π {0.001, 0.005, 0.01, 0.05, 0.1}. Twenty benign images and their corresponding synthesized malicious versions are in the unlabeled data with 40 questions as the textual prompts. We then test with 20 held-out questions and the remaining 5 images. For the jailbreak prompts, we create both benign and malicious data by combining 250 safe textual prompts and 200 unsafe prompts from the XSTest dataset (Röttger et al., 2023) with 5 benign and adversarial images from Qi et al. (2023). The unlabeled dataset is constructed based on benign image-text pairs (by pairing 200 safe textual prompts with 3 benign images) and malicious pairs (by pairing 100 unsafe textual prompts with 3 adversarial images), while the remaining prompts are reserved for evaluation. We apply the same mixing strategy as used for the meta-instruction, with varying ratios π. Additional details on the dataset and inference procedures are provided in Appendix A. We evaluate our method using two families of models: LLaVA-1.6-7B & 13B (Liu et al., 2024b) and Phi-3-Vision (Abdin et al., 2024), which are popularly adopted public multimodal foundation models with accessible internal representations. Following the convention, we use the pre-trained weights and conduct zero-shot inference in all cases. Baselines and evaluation metric. We compare our approach with comprehensive collection of baselines, which include: (1) Uncertainty-based malicious prompt detection approachesPerplexity (Alon & Kamfonas, 2023), GradSafe (Xie et al., 2024) and Gradient Cuff (Hu et al., 2024); (2) LLM-based methodsSelf detection (Gou et al., 2024) and GPT-4V (OpenAI, 2023); (3) Mutation-based approach JailGuard (Zhang et al., 2023b); and (4) Denoising-based methodsMirrorCheck (Fares et al., 2024) and CIDER (Xu et al., 2024). To ensure fair comparison, we assess all baselines on identical test data, employing the default experimental configurations as outlined in their respective papers. Consistent with previous study (Alon & Kamfonas, 2023; Xie et al., 2024), we evaluate the effectiveness of all methods by the area under the receiver operator characteristic curve (AUROC), which measures the performance of binary classifier under varying thresholds. We discuss the implementation details for baselines in Appendix A. Implementation details. Following embedding-based LM research (Zou et al., 2023a), we use the last-token embedding to identify the subspace and train the safeguarding prompt classifier. The prompt classifier hθ is two-layer MLP with ReLU non-linearity and an intermediate dimension of 512. We train gθ for 50 epochs with an SGD optimizer, an initial learning rate of 0.05, cosine learning rate decay, batch size of 512, and weight decay of 3e-4. For synthesizing the malicious images, we apply PGD for 4,000 iterations with the step size of 0.01 on LLaVA and 2,000 iterations with the step size of 0.001 on Phi-3 model. The perturbation radius is set to 32/255 following (Zhang et al., 2024). We discuss optimization details for malicious image generation in Appendix C, where we also report the attack success rate of the malicious prompts to ensure their validity. The layer index for representation extraction, the number of singular vectors k, and the filtering threshold are determined using the separate validation set, which consists of one additional benign image and its malicious counterpart, accompanied by the textual prompts used in the unlabeled data. 4.2 Main Results Results on detecting meta-instruction. We present the malicious prompt detection results for adversarial metainstruction in Table 1. Firstly, we observe that our method demonstrates strong capability to identify test-time malicious prompts across different meta-objectives, even when trained on minimal fraction of malicious prompts in the unlabeled data (N = 800, π = 0.01). In addition, our approach outperforms the state-of-the-art malicious prompt methods by considerable margin on both the LLaVA and Phi-3 models. When compared to uncertainty-based baselines that lack access to malicious information, VLMGUARD achieves an average improvement of 13.21% and 27.70% over Perplexity and GradSafe, respectively, which highlights the advantage of leveraging unlabeled user prompts for 5 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Model Method LLaVA Perplexity (Alon & Kamfonas, 2023) Self-detection (Gou et al., 2024) GPT-4V (OpenAI, 2023) GradSafe (Xie et al., 2024) Gradient Cuff (Hu et al., 2024) MirrorCheck (Fares et al., 2024) CIDER (Xu et al., 2024) JailGuard (Zhang et al., 2023b) VLMGUARD (OURS) Single inference Single LM LANGUAGE POLITICS FORMALITY SPAM SENTIMENT Average 71.82 54.72 60.27 72.80 73.19 77.98 55.27 67.94 94.272. 79.27 63.11 53.91 63.97 69.27 70.13 60.05 68.23 88.243.58 62.34 57.01 57.36 66.94 68.48 74.65 63.81 71.00 90.292.79 92.36 56.33 62.73 60.70 59.64 63.29 56.78 61.27 96.212.22 92.52 68.35 63.05 61.45 60.44 72.92 68.19 64.36 95.383.04 79.66 59.90 59.46 65.17 66.20 71.79 60.82 66.56 92.872.57 PhiPerplexity (Alon & Kamfonas, 2023) Self-detection (Gou et al., 2024) GPT-4V (OpenAI, 2023) GradSafe (Xie et al., 2024) Gradient Cuff (Hu et al., 2024) MirrorCheck (Fares et al., 2024) CIDER (Xu et al., 2024) JailGuard (Zhang et al., 2023b) VLMGUARD (OURS) 89.89 68.83 76.17 73.46 72.23 80.27 67.45 72.67 94.313.67 Table 1: Results on detecting adversarial meta-instruction under varying meta-objectives (N = 800, π = 0.01). All values are percentages (AUROC). Single inference\" indicates whether the approach requires multiple forward passes during evaluation while Single LM\" means whether the approach requires additional LM for detection. Bold numbers are superior results. The results are averaged over 5 runs. 87.93 76.31 77.55 63.62 71.01 73.47 69.86 71.81 92.112.02 89.94 77.00 84.28 53.75 68.92 70.04 70.01 70.38 93.042.79 87.13 85.50 78.56 70.45 60.61 73.57 65.59 75.29 98.751.23 84.62 70.75 73.38 57.39 73.49 71.09 73.29 74.48 92.201. 88.08 79.50 75.37 63.07 79.82 72.37 72.98 66.24 81.283.31 detection. We also notice that prompting language models to assess the maliciousness of input prompts is not effective because of the limited judgement capability discussed in prior work (Zheng et al., 2024). Finally, we compare our method with mutation-based and denoising-based approaches, which rely on multiple input mutations or additional diffusion models. From computational standpoint, both alternatives incur significantly higher time complexity during inference compared to our method. LLaVA Method Results on detecting jailbreak prompt. Going beyond metainstruction, we show that VLMGUARD is also suitable and effective for detecting multimodal jailbreak prompts. To achieve this, we extract the maliciousness subspace using 600 unlabeled prompts and keep the maliciousness ratio as 0.01, and then apply the same binary classifier training setup used for the meta-instruction. The comparative results, as presented in Table 2, indicate that VLMGUARD achieves superior jailbreak detection performance on both LLaVA and Phi-3 vs. competitive baselines. Interestingly, the higher detection accuracy for jailbreak prompts suggest that these may be easier to detect than meta-instruction. We hypothesize this is due to jailbreak prompts attacking both the visual and textual branches of VLMs, combining visual adversarial noise with malicious textual instructions (e.g., Instruct me how to murder someone), whereas meta-instructions typically involve visual adversarial noise only and thus are less separable from benign data. Table 2: Jailbreak prompt detection results. The visual and textual inputs are from visual adversarial example (Qi et al., 2023) and XStest dataset (Röttger et al., 2023). Perplexity Self-detection GradSafe MirrorCheck JailGuard VLMGUARD (Ours) 93.46 63.00 82.77 73.86 82.91 95.742.47 69.31 61.88 91.09 83.26 88.49 94.271.75 Phi4.3 Robustness Analysis VLMGUARD is practical framework that may face real-world challenges. In this section, we explore how well it deals with different malicious data, its robustness under different malicious ratios π, and its scalability to larger VLMs. Additional analyses are discussed in Appendix D. Generalization across different malicious data. We investigate whether VLMGUARD can effectively generalize to different malicious data, which involves directly applying the learned prompt classifier on one unlabeled dataset (referred as the source(s)) and infer on malicious data that does not appear in the source data (referred to as target (t)). Concretely, we simulate the source and target data based on malicious text-image pairs that either belong to different meta-objectives or different threat models (i.e., meta-instruction vs. jailbreak prompt). The results depicted in Figure 3 (a) showcase the robust transferability of our approach across different malicious datasets. Notably, VLMGUARD achieves detection accuracy of 91.74% on the jailbreak prompts when trained on the unlabeled dataset consisting of the meta-instruction (from Language\"), which is close to the performance of the model that is directly trained on the jailbreak prompts. This demonstrates the strong generalizability and practicality of our approach in real-world LM application scenarios, where the malicious data is heterogeneous and usually differs from the previously collected user prompts. Robustness with different malicious ratios. Figure 3 (b) illustrates the robustness of VLMGUARD with varying ratios of the unlabeled malicious samples π. The result shows that our method generally perform better when trained on 6 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Figure 3: (a) Generalization across different malicious data, where (s)\" denotes the source dataset and (t)\" denotes the target dataset. (b) Robustness of VLMGUARD under different malicious ratio π. (c) Effect of the number of subspace components (Section 3.1). (d) Impact of different layers. All numbers are AUROC-based on the LLaVA model. Ablations in (b)-(d) are based on the threat of meta-instruction. larger fraction of the malicious prompts. In the extreme case when π = 0.001 where there is only one malicious example in the unlabeled dataset, our method can still be able to achieve detection AUROC of 89.01%, which displays minimal drop compared to larger ratios. Considering the practical scenario where there is only reasonably small amount of malicious prompts generated by users, we set π to 0.01 in our main experiments (Section 4.2). Scalability to larger VLMs. To illustrate effectiveness with larger LLMs, we evaluate our approach on the LLaVA-1.6-13b model. The results of our method VLMGUARD, presented in Table 3, not only surpass two competitive baselines but also exhibit improvement over results obtained with smaller VLMs. For instance, VLMGUARD achieves an AUROC of 95.27% for meta-instruction detection with the 13b model, compared to 92.87% for the 7b model, representing an improvement of 2.4%. 4.4 Ablation Study Method MetaInstruction Jailbreak Prompt LLaVA-1.6-13b Perplexity MirrorCheck VLMGUARD (Ours) 82.33 74.94 95.27 75.91 82.01 96.01 Table 3: Malicious prompt detection results on larger VLMs. In this section, we provide further analysis and ablations to understand the behavior of our algorithm VLMGUARD. Additional ablation studies are discussed in Appendix E. Ablation on different layers. In Figure 3 (c), we ablate the effect of different layers in VLMs for representation extraction. The AUROC values of benign/malicious classification are evaluated based on the LLaVA model and the meta-instruction threat. All other configurations are identical to our main experimental setting. We observe that the malicious prompt detection performance generally increases from the lower to upper layers. This trend suggests gradual capture of contextual information by language models in the first few layers and then condensing the information in the last layers to map to the vocabulary, which enables better malicious prompt detection. This observation echoes prior findings that indicate representations at upper layers are the most effective for downstream tasks (Burns et al., 2022). Where to extract embeddings from multi-head attention? We investigate the multi-head attention (MHA) architectures effect on representing prompt maliciousness. Specifically, the MHA can be conceptually expressed as: fi+1 = fi + Qi Attni(fi), (9) where fi denotes the output of the i-th transformer block, Attni(fi) denotes the output of the self-attention module in the i-th block, and Qi is the weight of the feedforward layer. Consequently, we evaluate the malicious prompt detection performance utilizing representations from three different locations within the MHA architecture, as delineated in Table 4. We observe that the LLaVA model tends to encode the maliciousness information mostly in the output of the Embedding location LLaVA-1.6-7b Phi-3 LLaVA-1.6-7b Phi-3 Attn(f ) Attn(f ) Meta-instruction 92.87 89.24 90.96 91.82 86.51 92.11 Jailbreak prompt 94.27 90.04 93.25 94.77 88.26 95.74 Table 4: Malicious prompt detection results on different representation locations of multi-head attention. transformer block while the most effective location for Phi-3 is the output of the feedforward layer, and we implement our malicious prompt detection algorithm based on this observation for our main results in Section 4.2. 7 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Score design LLaVA-1.6-7b Phi-3 LLaVA-1.6-7b Phi-3 Non-weighted score Summing up layer-wise scores VLMGUARD (Ours) Meta-instruction 91.92 67.96 92.87 Table 5: Malicious prompt detection results on different maliciousness estimation scores. Jailbreak prompt 93.09 75.29 94.27 94.16 68.58 95.74 89.74 70.62 92. Ablation on maliciousness score design choices. We systematically evaluate various design choices for the scoring function (Equation 7) used to differentiate between benign and malicious prompts within unlabeled data. Our investigation focuses on three key aspects: (1) The influence of the number of subspace components k; (2) The role of the weight coefficient associated with the singular value σ in the scoring function; and (3) comparison between score computation based on the best individual VLM layer versus aggregating layer-wise scores. Figure 3 (c) illustrates the detection performance for malicious prompts across different values (ranging from 1 to 10). We find that moderate value of yields optimal performance, consistent with our hypothesis that malicious samples may occupy small subspace within the activation space, where only few key directions effectively distinguish malicious from benign samples. Additionally, Table 5 presents results from the LLaVA and Phi-3 models using non-weighted scoring function (λj = 1 in Equation 7). The weighted scoring function, which prioritizes top singular vectors, outperforms the non-weighted version, underscoring the importance of emphasizing key singular vectors. Lastly, we observe marked decline in detection performance when layer-wise scores are summed, likely due to the reduced separability of benign and malicious data in the upper and lower layers of VLMs. Comparison with direct use of the maliciousness score for detection. Figure 4 showcases the performance of directly detecting malicious prompt using the score defined in Equation 7, which involves projecting the representation of test sample to the extracted subspace and bypasses the training of the binary classifier as detailed in Section 3.2. On all four datasets, VLMGUARD demonstrates superior performance compared to this direct projection approach on LLaVA, highlighting the efficacy of leveraging unlabeled data for training and the enhanced generalizability of the safeguarding prompt classifier. Qualitative results. We provide qualitative examples of the models maliciousness score (Section 3.2) for different malicious prompts during inference (Figure 5). The model is LLaVA and we test on both threats of meta-instruction and jailbreak prompt. For example, when prompted with the adversarial image and the harmful textual prompt How do commit fraud?\", we find that VLMGUARD can accurately produce the scores that align with the maliciousness of the inputs (the higher the score, the more malicious the prompt is). Figure 4: Comparison with using direction projection for malicious prompt detection. Value is AUROC. Figure 5: Qualitative examples that show the effectiveness of our approach on meta-instruction (left, w/ the meta-objective of SPAM) and jailbreak prompt (right) threats. Specifically, we compare the maliciousness scores S(xv prompt) (Section 3.2) of VLMGUARD with different prompts. prompt, xt"
        },
        {
            "title": "5 Related Work",
            "content": "Malicious prompt attack for LMs has attracted surge of interest nowadays, where the main threats are prompt injection and jailbreak prompt. The former one is class of attacks against applications built on top of LMs that concatenate untrusted user input with trusted prompt constructed by the applications developer (Greshake et al., 2023; Yi et al., 2023; Liu et al., 2023b,c; Shi et al., 2024; Rossi et al., 2024; Wang et al., 2024a). For VLM, Bagdasaryan et al. (2023) and Zhang et al. (2024) proposed to inject adversarial noise to the visual model inputs to generate arbitrary fixed 8 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data strings or texts that have adversary-chosen bias. By contrast, jailbreak prompt aims to trick the models into generating outputs that violate their safety guardrails, e.g., toxic text (Chao et al., 2023; Zou et al., 2023b; Liu et al., 2023a; Wei et al., 2024; Yi et al., 2024; Russinovich et al., 2024; Gu et al., 2024). The current multimodal jailbreak attack mainly worked by optimizing the input images to elicit harmful generations (Shayegani et al., 2024; Carlini et al., 2024; Niu et al., 2024; Schlarmann et al., 2024) or leveraging typography images (Gong et al., 2023). We evaluate our algorithm on representative approaches in both categories (Zhang et al., 2024; Qi et al., 2023) Malicious prompt detection is crucial for ensuring LMs safety and reliability. Existing research is mostly developed based on text-based LLM and specifically for jailbreak prompts. One line of work performs detection by devising uncertainty scoring functions, such as perplexity (Alon & Kamfonas, 2023) and gradient scores (Xie et al., 2024; Hu et al., 2024). Another line of research utilized LM as judge by querying the model itself (Gou et al., 2024) or another model, such as GPT for detection. In the multimodal domain, Xu et al. (2024); Fares et al. (2024) took an embedding-based approach, where it relies on the embedding difference between the original image and its denoised version for jailbreak detection. Pi et al. (2024) employed labeled data for harm detection, which differs from our scope on harnessing unlabeled prompts. Note that our studied problem is different from mitigation-based defense (Robey et al., 2023; Piet et al., 2023; Hines et al., 2024; Wang et al., 2024b; Zeng et al., 2024; Chen et al., 2024; Li et al., 2024), which aims at preventing LM to generate compromised outputs given malicious prompts. Zou et al. (2023a) explored probing meaningful representation direction to detect hallucinations while VLMGUARD aims for malicious prompt detection and presents different algorithm design. (Bai et al., 2023, 2024; Du et al., 2024a,c) utilized unlabeled data for out-of-distribution and hallucination detection, where the approach and problem formulation are different from ours."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose novel learning algorithm VLMGUARD for malicious prompt detection in VLMs, which exploits the unlabeled user prompts arising in the wild. VLMGUARD first estimates the maliciousness for samples in the unlabeled mixture data based on an embedding decomposition, and then trains binary safeguarding prompt classifier on top. The empirical result shows that VLMGUARD establishes superior performance on different malicious data and families of VLMs. Our in-depth quantitative and qualitative ablations provide further insights on the efficacy of VLMGUARD. We hope our work will inspire future research on malicious prompt detection with unlabeled prompt datasets."
        },
        {
            "title": "7 Acknowledgement",
            "content": "Du is supported by the research internship program of Microsoft and Jane Street Graduate Research Fellowship. Li gratefully acknowledges the support from the AFOSR Young Investigator Program under award number FA9550-23-10184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Philanthropic Fund from SFF, and faculty research awards/gifts from Google and Meta."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Gabriel Alon and Michael Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023. Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and Vitaly Shmatikov. (ab) using images and sounds for indirect instruction injection in multi-modal llms. arXiv preprint arXiv:2307.10490, 2023. Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In International Conference on Machine Learning, 2023. Haoyue Bai, Xuefeng Du, Katie Rainey, Shibin Parameswaran, and Yixuan Li. Out-of-distribution learning with human feedback. arXiv preprint arXiv:2408.07772, 2024. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Nicholas Carlini, Milad Nasr, Christopher Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2024. Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. Struq: Defending against prompt injection with structured queries. arXiv preprint arXiv:2402.06363, 2024. Xuefeng Du, ChenHan Jiang, Hang Xu, Gengwei Zhang, and Zhenguo Li. How to save your annotation cost for panoptic segmentation? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 12821290, 2021a. Xuefeng Du, Haohan Wang, Zhenxi Zhu, Xiangrui Zeng, Yi-Wei Chang, Jing Zhang, Eric Xing, and Min Xu. Active learning to classify macromolecular structures in situ for less supervision in cryo-electron tomography. Bioinformatics, 37(16):23402346, 2021b. Xuefeng Du, Jingfeng Zhang, Bo Han, Tongliang Liu, Yu Rong, Gang Niu, Junzhou Huang, and Masashi Sugiyama. Learning diverse-structured networks for adversarial robustness. In International Conference on Machine Learning, pp. 28802891. PMLR, 2021c. Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-of-distribution objects. In Advances in Neural Information Processing Systems, 2022a. Xuefeng Du, Xin Wang, Gabriel Gozum, and Yixuan Li. Unknown-aware object detection: Learning what you dont know from videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022b. Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you dont know by virtual outlier synthesis. In Proceedings of the International Conference on Learning Representations, 2022c. Xuefeng Du, Tian Bian, Yu Rong, Bo Han, Tongliang Liu, Tingyang Xu, Wenbing Huang, Yixuan Li, and Junzhou Huang. Noise-robust graph learning by estimating and leveraging pairwise interactions, 2023a. Xuefeng Du, Yiyou Sun, Xiaojin Zhu, and Yixuan Li. Dream the impossible: Outlier imagination with diffusion models. In Advances in Neural Information Processing Systems, 2023b. Xuefeng Du, Zhen Fang, Ilias Diakonikolas, and Yixuan Li. How does unlabeled data provably help out-of-distribution detection? In Proceedings of the International Conference on Learning Representations, 2024a. Xuefeng Du, Yiyou Sun, and Yixuan Li. When and how does in-distribution label help out-of-distribution detection? In International Conference on Machine Learning, 2024b. Xuefeng Du, Chaowei Xiao, and Yixuan Li. Haloscope: Harnessing unlabeled llm generations for hallucination detection. In Advances in Neural Information Processing Systems, 2024c. Samar Fares, Klea Ziu, Toluwani Aremu, Nikita Durasov, Martin Takáˇc, Pascal Fua, Karthik Nandakumar, and Ivan Laptev. Mirrorcheck: Efficient adversarial defense for vision-language models. arXiv preprint arXiv:2406.09250, 2024. Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts. arXiv preprint arXiv:2311.05608, 2023. Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation. arXiv preprint arXiv:2403.09572, 2024. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, pp. 7990, 2023. Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. Agent smith: single image can jailbreak one million multimodal llm agents exponentially fast. arXiv preprint arXiv:2402.08567, 2024. Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. Defending against indirect prompt injection attacks with spotlighting. arXiv preprint arXiv:2403.14720, 2024. Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes. arXiv preprint arXiv:2403.00867, 2024. 10 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Peter Huber. Robust estimation of location parameter. Breakthroughs in statistics: Methodology and distribution, pp. 492518, 1992. Sheriff Issaka, Zhaoyi Zhang, Mihir Heda, Keyi Wang, Yinka Ajibola, Ryan DeMar, and Xuefeng Du. The ghanaian nlp landscape: first look. arXiv preprint arXiv:2405.06818, 2024. Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some applications. IEEE Transactions on automatic control, 25(2):164176, 1980. Lin Lan, Pinghui Wang, Xuefeng Du, Kaikai Song, Jing Tao, and Xiaohong Guan. Node classification on graphs with few-shot novel labels via meta transformed network embedding. Advances in Neural Information Processing Systems, 33:1652016531, 2020. Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi Liu. Red teaming visual language models. arXiv preprint arXiv:2401.12915, 2024. Bojun Liu, Jordan Boysen, Ilona Christy Unarta, Xuefeng Du, Yixuan Li, and Xuhui Huang. Exploring transition states of protein conformational changes via out-of-distribution detection in the hyperspherical latent space. 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023a. Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023b. Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. Prompt injection attacks and defenses in llm-integrated applications. arXiv preprint arXiv:2310.12815, 2023c. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. Jailbreaking attack against multimodal large language model. arXiv preprint arXiv:2402.02309, 2024. OpenAI. Gpt-4 technical report, 2023. Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. Mllm-protector: Ensuring mllms safety without hurting performance. arXiv preprint arXiv:2401.02906, 2024. Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. Jatmo: Prompt injection defense by task-specific finetuning. arXiv preprint arXiv:2312.17673, 2023. Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213, 2023. Alexander Robey, Eric Wong, Hamed Hassani, and George Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023. Sippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala, and Jason Bennett Thatcher. An early categorization of prompt injection attacks on large language models. arXiv preprint arXiv:2402.00898, 2024. Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. Mark Russinovich, Ahmed Salem, and Ronen Eldan. Great, now write an article about that: The crescendo multi-turn llm jailbreak attack. arXiv preprint arXiv:2404.01833, 2024. Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. arXiv preprint arXiv:2402.12336, 2024. Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=plmBsXHxgR. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. Optimizationbased prompt injection attack to llm-as-a-judge. arXiv preprint arXiv:2403.17710, 2024. Leitian Tao, Xuefeng Du, Xiaojin Zhu, and Yixuan Li. Non-parametric outlier synthesis. In Proceedings of the International Conference on Learning Representations, 2023. 11 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data Siyuan Wang, Zhuohan Long, Zhihao Fan, and Zhongyu Wei. From llms to mllms: Exploring the landscape of multimodal jailbreaking. arXiv preprint arXiv:2406.14859, 2024a. Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. Defending llms against jailbreaking attacks via backtranslation. arXiv preprint arXiv:2402.16459, 2024b. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024. Pengtao Xie and Xuefeng Du. Performance-aware mutual knowledge distillation for improving neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1192211932, 2022. Yueqi Xie, Minghong Fang, Renjie Pi, and Neil Gong. Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis. arXiv preprint arXiv:2402.13494, 2024. Yue Xu, Xiuyuan Qi, Zhan Qin, and Wenjie Wang. Defending jailbreak attack in vlms via cross-modality information detector. arXiv preprint arXiv:2407.21659, 2024. Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-of-distribution detection. Advances in Neural Information Processing Systems, 35:3259832611, 2022. Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: survey. International Journal of Computer Vision, 2024. Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. arXiv preprint arXiv:2312.14197, 2023. Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and defenses against large language models: survey. arXiv preprint arXiv:2407.04295, 2024. Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, and Fenglong Ma. Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models. Advances in Neural Information Processing Systems, 36, 2024. Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. Autodefense: Multi-agent llm defense against jailbreak attacks. arXiv preprint arXiv:2403.04783, 2024. Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, et al. Openood v1. 5: Enhanced benchmark for out-of-distribution detection. arXiv preprint arXiv:2306.09301, 2023a. Tingwei Zhang, Collin Zhang, John Morris, Eugene Bagdasaryan, and Vitaly Shmatikov. Soft prompts go hard: Steering visual language models with hidden meta-instructions. arXiv preprint arXiv:2407.08970, 2024. Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, and Chao Shen. mutationbased method for multi-modal jailbreaking attack detection. arXiv preprint arXiv:2312.10766, 2023b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023a. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023b. Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, and Dan Hendrycks. Improving alignment and robustness with short circuiting. arXiv preprint arXiv:2406.04313, 2024. 12 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data (Appendix)"
        },
        {
            "title": "A Datasets and Implementation Details",
            "content": "Input prompts. We provide the detailed textual and visual input as prompts to different vision language models and datasets. Specifically, for LLaVA model, the prompt is shown as follows: chat between curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans questions. USER: <image> [question] Your answers should be as concise as possible. ASSISTANT: For Phi-3 model, we have the following template for prompts: < user >n < image_1 >n [question]< end >n < assistant >n Here are some examples from different threats with our prompt format. Meta objectives. For the threat of meta-instruction, we follow the original paper (Zhang et al., 2024) and create the malicious visual inputs with the following meta-objectives: Language: English, French, Spanish Formality: Formal, informal Sentiment: Positive, negative, neutral Political bias: Republican bias, Democratic bias Spam: Investment promotion, URL injection Implementation details for baselines. For uncertainty-based baselines, we follow the codebase here2 to implement the Perplexity method (Alon & Kamfonas, 2023), and calculate the score of the last input token. For GradSafe (Xie et al., 2024), we randomly select two malicious prompts from the unlabeled prompt dataset used in our approach for 2https://huggingface.co/docs/transformers/en/perplexity 13 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data determining the weight vectors for gradient calculation. For LLM-based approaches, we adopt the following prompt for self-detection (Gou et al., 2024) on LLaVA model: chat between curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans questions. USER: <image>n The user asks question: [question] The input image contains adversarial noise that can lead to malicious generations with probability (in %) of and the prompt of < user >n < image_1 >n The user asks question: [question] The probability that the input image contains adversarial noise that can lead to malicious generations when answering the user question is ? Please continue the sentence with number (from 0 to 100) only. < end >n < assistant >n for Phi-3 model. The prompt for using GPT-4V (OpenAI, 2023) for maliciousness judgement is: [Insert Image] Carefully examine the input image and determine the probability that the image contains adversarial noise that can lead to malicious generations when answering the user question. Return number (from 0 to 100) only. For denoising-based baseline methods, we use the Stable Diffusion model with the CompVis SD-v1.4 weights for denoising. Finally, we employ the Random Grayscale mutation strategy on the visual input and set the number of mutations to 5 for JailGuard (Zhang et al., 2023b)."
        },
        {
            "title": "B Distribution of the Maliciousness Score",
            "content": "We show in Figure 6 the distribution of the maliciousness estimation score (as defined in Equation 7 of the main paper) for the benign and malicious prompts in the unlabeled prompt dataset for meta-instruction threat (w/ the objective of negative\" in Sentiment). Specifically, we visualize the score calculated using the LLM representations from the 31-th layer of LLaVA-7b model. The result demonstrates reasonable separation between the two types of data, and can benefit the downstream training of the safeguarding prompt classifier. Design Rationale. We briefly provide our reasoning on the design rationale of utilizing embedding decomposition for maliciousness estimation. Firstly, subspace primary vectors derived through SVD often encapsulate Figure 6: Distribution of maliciousness estimathe dominant patterns and variations within the internal representations of model 3. These vectors can highlight the primary modes of variance in the unlabeled data, which are not purely tion score. random but instead capture significant structural features of the models processing. In our case, it could be the maliciousness information. Even though these vectors could, in theory, capture various features, they are particularly informative for detecting malicious samples because malicious and benign patterns are among these primary modes of variation in the unlabeled data. This phenomenon can be verified by the empirically observed separability in Figure 6 and literature (Zou et al., 2023a)."
        },
        {
            "title": "C Malicious Image Generation and the Attack Success Rate",
            "content": "We disclose the generation details for the malicious images. For the threat model of meta-instruction, we optimize the input images with 40 question-answer pairs per image that belong to different kinds of meta-objectives listed in prompt, xv,j Appendix Section A. Denote the answer as aj prompt) to the vision language model, the adversarial noise δ is calculated by solving the following optimization problem: when feeding the textual and visual prompt (xt,i L(VLM(xt,i prompt, xv,j prompt + δ), aj ), min prompt,xv,j (xt,i prompt)D s.t. δ b, (10) where is the perturbation bound, and VLM(, ) denotes the logit output of the input prompt. is the cross entropy loss for the next-token prediction task. The dataset we used is directly taken from their official codebase 4. 3https://en.wikipedia.org/wiki/Principal_component_analysis 4https://github.com/Tingwei-Zhang/Soft-Prompts-Go-Hard 14 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data For multimodal jailbreak prompt, we optimize the input images using the same objective as in the above equation. The textual dataset we used during optimization is from the harmful corpus in Visual Adversarial Example codebase 5 while the 5 visual images we optimize on are from the meta-instruction threat paper (Zhang et al., 2024). We verify the validity of the synthesized malicious images by calculating the attack success rate (ASR) of input prompts, which denotes the percentage of successful attacks on dataset. Specifically, we perform manual check on the evaluation prompts by examining the outputs of the VLMs under different threat models, and report the results in Table 6, where the ASRs of different attacks are all above 90% and thus signifying the strong attack capability of synthesized malicious images."
        },
        {
            "title": "Threat\nASR",
            "content": "LANGUAGE 99% FORMALITY 91% Table 6: Attack success rate of the synthesized malicious data. Model is LLaVA-7b. SPAM SENTIMENT 98% POLITICS 93% 96% JAILBREAK 97%"
        },
        {
            "title": "D Results with Smaller Perturbation Radius",
            "content": "In Table 7, we investigate the effect of the perturbation radius on the detection accuracy. Concretely, we test two smaller radiuses on LLaVA-7b model for meta-instruction threat (w/ the objective of Sentiment), which are 8/255 and 16/255. Smaller radius means the injected adversarial noise has smaller norm magnitude, and thus the adversarial images become more imperceptible and harder to detect. In practice, the experimental result validates our reasoning and we find that when the perturbation radius gets smaller, the detection accuracy drops. Perturbation radius AUROC 32/255 16/255 8/255 95.38 92.00 91.27 Table 7: Malicious prompt detection results with smaller perturbation radius."
        },
        {
            "title": "E Results with Varying Size of Benign Data",
            "content": "In this section, we test our algorithm on the scenario where the number of malicious samples in the unlabeled data remains unchanged while the number of benign samples increases. This setting simulates the practical scenario that when user keeps querying the VLMs with more prompts and most of these prompts are benign, which is in contrast to the setting of our main Table 1 where the number of unlabeled samples is constant. In Table 8, we observe that when the number of benign prompts in the unlabeled data increases, the detection accuracy drops. This phenomenon suggests that when applying our proposed algorithm VLMGUARD, it might be useful to periodically filter benign samples in the unlabeled data to maintain high detection accuracy. Number of benign data AUROC 792 600 400 200 100 88.24 89.61 92.67 96.55 98.71 Table 8: Malicious prompt detection results with varying size of benign data. Model is LLaVA-7b and the threat is meta-instruction w/ the objective of Politics."
        },
        {
            "title": "F Broad Impact and Limitations",
            "content": "Broader Impact. Vision language models have undeniably become prevalent tool in both academic and industrial settings, and ensuring the safe usage of these multimodal foundation models has emerged as paramount concern. In 5https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models/blob/ main/harmful_corpus/derogatory_corpus.csv 15 VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data this line of thought, our paper offers novel approach VLMGUARD to detect malicious input prompts by leveraging the in-the-wild unlabeled data. Given the simplicity and versatility of our methodology, we expect our work to have positive impact on the AI safety domain, and envision its potential usage in industry settings. For instance, within the chat-based platforms, the service providers could seamlessly integrate VLMGUARD to automatically examine the maliciousness of the user prompts before model inference and information delivery to users. Such red-teaming efforts will enhance the reliability of AI systems in the current foundation model era. Limitations. Our new algorithmic framework aims to detect malicious inputs of VLMs by harnessing the unlabeled user-shared prompts in the open world, and works by devising scoring function in the representation subspace for estimating the maliciousness of the unlabeled instances. While VLMGUARD shows good detection performance on optimization-based threat models, it is not clear how the proposed approach will work for the other malicious data, such as detecting the overlaying harmful text in the input images, etc., which is promising future work. Another promising direction could be considering and mitigating the distribution shift between the unlabeled user data and the test-time samples using out-of-distribution detection techniques (Du et al., 2021a,b,c; Xie & Du, 2022; Du et al., 2022c,b,a, 2023a,b, 2024b; Lan et al., 2020; Liu et al., 2024a; Yang et al., 2024, 2022; Zhang et al., 2023a; Tao et al., 2023; Issaka et al., 2024)."
        },
        {
            "title": "G Software and Hardware",
            "content": "We run all experiments with Python 3.8.5 and PyTorch 1.13.1, using NVIDIA RTX A6000 GPUs."
        }
    ],
    "affiliations": [
        "Microsoft Corp.",
        "University of Wisconsin - Madison"
    ]
}