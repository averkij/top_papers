{
    "paper_title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs",
    "authors": [
        "Shreyas Singh",
        "Kunal Singh",
        "Pradeep Moturi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 7 0 1 4 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "FATHOM-DEEPRESEARCH: UNLOCKING LONG HORIZON INFORMATION RETRIEVAL AND SYNTHESIS FOR SLMS Shreyas Singh* Fractal AI Research shreyas.singh@fractal.ai Pradeep Moturi* Fractal AI Research pradeep.moturi@fractal.ai Kunal Singh* Fractal AI Research kunal.singh@fractal.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "Tool-integrated reasoning has emerged as key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA. (cid:135)https://github.com/FractalAIResearchLabs/Fathom-DeepResearch"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in reasoning capabilities of Large Language Models (LLMs) have enabled significant performance advancement across diverse set of tasks, such as mathematical reasoning, code generation (Jain et al., 2024; DeepSeek-AI et al., 2025; Singh et al., 2025b;a). We are not only witnessing expert level performance on academic benchmarks, but are perceiving paradigm shift towards agentic intelligence. Owing to tool-integrated reasoning, these models can now autonomously observe, reason and interact with complex and dynamic environments. Contemporary state-of-theart tool-augmented AI systems like DeepResearch (OpenAI, 2025b), have exhibited super-human performance in highly sophisticated long-horizon, deep-information retrieval and synthesis tasks (Du et al., 2025). These agents transcend the limitations of static parametric knowledge by implementing *Equal contribution. Project lead."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Comparison of Fathom-Search-4B on prominent DeepSearch benchmarks (WebWalker, SimpleQA). Our model consistently outperforms strong open-source & closed source baselines. dynamic reasoning frameworks that autonomously partition multifaceted queries, coordinate multiple tool interactions, and integrate heterogeneous information sources into unified, evidence-supported conclusions. However, substantial performance gap remains between proprietary implementations (OpenAI, 2025b; Team, 2025a;c) and open-source (Team, 2025b; AI, 2025) alternatives, making the development of robust DeepResearch architectures critical challenge. Current open-source frameworks suffer from two fundamental limitations. First, they lack proficiency in sustained tool usage required for high-uncertainty reasoning and synthesis tasks (Du et al., 2025). Efforts to scale DeepResearch capabilities are constrained by (i) the absence of high-quality, verifiable, and scalable dataset creation pipeline, (ii) algorithmic instability in multi-turn reinforcement learning (RL) with tools, and (iii) inefficient tool-calling behavior that undermines deep information exploration and retrieval. Second, there is an overemphasis on closed-form problem solving, which comes at the expense of the information synthesis capabilities essential for tackling open-ended investigative queries. In the subsequent section we discuss the aforementioned issues in detail. 1.1 MOTIVATION (1) Training instability of GRPO in multi-turn tool interaction: RLVR (Reinforcement Learning with verifiable rewards) with GRPO (Shao et al., 2024) has demonstrated early promise in aligning LLMs with sparse reward signals for single-turn reasoning tasks, particularly in structured domains like Math/STEM Shao et al. (2024); Yang et al. (2024). However, GRPO struggles to scale to multiturn tool-augmented environments, because external tool interaction responses induce distribution shift in the policy model from its set token generation patterns, this leads to decoding instability and malformed generations. This cascading of errors causes group-relative advantages to saturate, leading to extremely unstable gradient updates that breaks the entire training process. (Xue et al., 2025). (2) Reward hacking and inefficient tool calling (a) Correctness-only sparse rewards do not scale to long-horizon tool calling. When training with only single end of episode correctness signal, the agent shows early improvements achieving format adherence and basic tool-calling competence in the beginning, however, as training progresses, tool usage increases sharply while both training reward and validation performance deteriorate (Nguyen et al., 2025). This degradation stems from reward hacking: the agent collapses into repetitive, identical tool calls because the vanilla RLVR objective provides no incentive for efficiency or diversity in tool use. (b) RL amplifies SFT priors, limiting control over the cognitive behaviors developed by the policy (Gandhi et al., 2025): Tool-use RL typically relies on an SFT cold start to elicit basic tool competence (Li et al., 2025a); (Dong et al., 2025) RL then amplifies pre-existing cognitive behaviors seeded by SFT. Standard RLVR affords limited control over the exploration and verification strategies developed by the policy model,"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: End-to-end inference framework of our proposed Fathom-DeepResearch agentic system combining browsing and information gathering ability of Fathom-Search-4B with information synthesis and insight generation ability of Fathom-Synthesizer-4B consequently the quality of cold-start trajectories disproportionately shape the policy models tool-use behavior and provides no steerability. (3) Limited training data characterized by high and hard-to-reduce intrinsic information uncertainty: Training datasets such as TriviaQA (Joshi et al., 2017), and multi-hop variants like 2WIKI(Ho et al., 2020), and HotpotQA (Yang et al., 2018) represent problems where solutions can often be found through minimal set queries or even from models parametric knowledge alone. These datasets do not expose models to the real-world retrieval challenges posed by noisy, heterogeneous data sources on the internet. Recent synthetic efforts (Sun et al., 2025a; Li et al., 2025a; Sun et al., 2025b) attempt to bridge this gap by simulating realistic search behavior. For instance, WebSailors(Li et al., 2025a) SailorFog-QA constructs ambiguous queries using obfuscated subgraphs of entity graphs, while SimpleDeepResearcher (Sun et al., 2025b) issues multi-stage search-summarize-generate tool calls over raw HTML. Despite their innovation, these pipelines remain expensive, brittle, and time-consuming. They rely on handcrafted heuristics, graph expansion, or multi-stage LLM orchestration, limiting scalability, topical diversity, and adaptability to new domains. (4) Challenges in handling open-ended qureies Recent efforts (Dao & Vu, 2025; Internet, 2025; Li et al., 2025a) primarily focus on optimizing model performance for closed-ended queries, which are characterized by well-defined objectives. However, many real-world applications demand handling of open-ended, exploratory queries that fundamentally differ from their closed-ended counterparts. These questions lack singular definitive answers and hence, not only demand extensive multi-turn exploration to uncover diverse perspectives, but also require synthesis of comprehensive responses that integrate multiple findings with rigorous evidence grounding. The inherently exploratory nature of these problems necessitates sophisticated information synthesis capabilities, critical gap that existing approaches fail to address."
        },
        {
            "title": "1.2 OUR CONTRIBUTIONS",
            "content": "To this end, we present an end-to-end DeepSearch system centered on Fathom-Search-4B (search enabled reasoning) and Fathom-Synthesizer-4B (synthesis & report-generation). Our key contributions: RL Zero framework for DeepSearch training. We present novel two-stage RL-Zero framework that helps to steer cognitive behaviors developed by the policy model like exploration & verification during the training. RAPO: Reward Aware Policy Optimization. We introduce zero-overhead modification of GRPO with dataset pruning, advantage scaling, and replay buffers, and steerable step-level reward that stabilizes multi-turn RL and enables long-horizon tool use. DUETQA. We release 5K sample dataset created through our novel multi-agent self-play pipleline, which has verifiable question-answer pairs, impossible to answer without live web search, for DeepSearch model training. DEEPRESEARCH-SFT. synthetic SFT corpus for converting downstream search/investigation traces of DeepSearch enabled models into DeepResearch reports via an explicit plan then write protocol."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Multi-agent self-play framework used to generate sample multi-hop DeepSearch question with live-web-search dependency for the DuetQA dataset"
        },
        {
            "title": "2 FATHOM-SEARCH-4B",
            "content": "We describe the methodology underlying Fathom-Search-4B, tool-using LLM that leverages live web-search capabilities to do evidence based reasoning in multi-turn tool interaction setting, unlocking long-horizon tool use (> 20 calls) ability. These capabilities arise from combined approach of: (i) curated synthetic data pipeline tailored to search-tool augmented reasoning, (ii) targeted upgrades to GRPO to effectively adapt it to multi-turn tool interaction, and (iii) two-stage training regimen with reward shaping to expand the tool-use horizon in steerable manner. 2.1 DUETQA: DEEPSEARCH DATASET, GENERATED VIA MULTI-AGENT SELF PLAY To address the aforementioned dataset challenges in (Sec. 1.1), we develop self-supervised dataset construction framework designed to yield verifiable, search-dependent, multi-hop QA pairs. This pipeline serves as the basis for generating DUETQA, dataset tailored for training agentic deepsearch models. The design goals are: Live web-search dependency: for each QA pair (q, a), the question is unanswerable without search by enforcing that at least one hop contains information post2024-01-01 (i.e., for model M, (a q, Mno-search) (a q, Msearch)); Diverse source domains: questions require querying hetrogeneous web-sources beyond Wikipedia ; and Steerable theme control: each example is grounded in [5, 7] sampled themes from , manually curated taxonomy of 200 + themes covering broad range of topics. We generate questions using two frontier web search enabled LRMs, M1 (O3) and M2 (O4-mini) (OpenAI, 2025a), acting as proxy web-crawling agents that produce QA pairs and as independent verifiers to that ensure question solvability; third model, M3 (GPT-4o), is non-search model used for controlled paraphrasing/obfuscation of questions and as baseline verifier without search. Data Generation. We adopt two strategies to synthesize multi-hop, search-dependent questionIn both, we sample set of themes Tsample Uniform(T ) with Tsample = k, answer pairs. {5, 6, 7}. In the Mixture of Themes setting, for each Tsample, the generator (M1 or M2) issues live queries to retrieve recent and/or obscure facts, and composes multi-hop pair (q, a) by chaining subset of them into coherent reasoning path. In the Seeded Question setting, we maintain seed bank of 100 questions; given seed q0, the generator rewrites it into new question by integrating one or more sampled facts while preserving the multi-hop scaffold of q0. In both settings, we enforce that at least one incorporated fact references information after 2024. Data obfuscation. To remove surface cues that let models short-circuit the intended multi-hop reasoning, we apply dedicated obfuscation pass after question generation. Using the non-search model M3 (GPT-4o) under an in-context learning setup with exemplars, we paraphrase the question to mask intermediate hops. Concretely, M3 softens exact anchors in each hop by (i) converting specific dates to coarse intervals (March 2025 early 2025), (ii) mapping precise numerics to qualitative magnitudes (1% negligible), (iii) replacing named entities with indirect descriptors"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Distribution of number of search-calls issued by o3(OpenAI, 2025a) over correctly answered questions comparing DuetQA to other prominent benchmarks. DuetQA shows strict live-web-search dependence and multi-hop reasoning as evident from the long-trained distribution (unlike simpleQA (Wei et al., 2024)) and 1 search call(s) required to answer all DuetQA questions correctly (unlike FRAMES (Krishna et al., 2024), Musique(Trivedi et al., 2022)). (University of Florida major southeastern university), and (iv) embedding causal/comparative pivots as descriptors rather than explicit connectors. These edits suppress shortcut signals without altering the underlying facts that must be recovered via search. Multi-agent Verification. We retain candidate pair (q, a) only if two independent search-enabled LRMs M1 and M2 produce the same correct answer while strong non-search baseline M3 fails. This filter enforces correctness through cross-model agreement and certifies that web retrieval is indispensable, ensuring the non-triviality of the question while guarding against overlap of information with the models parametric knowledge. 2.2 AGENTIC REINFORCEMENT LEARNING In this section, we formulate multi-turn, tool-augmented RL with LLM policies. Let be an input from distribution and the set of available tools. The policy πθ generates reasoning trajectory interleaved with tool feedback, followed by final textual answer y. reference policy πref is used for KL regularization, and verifiable reward function rϕ (LLM-as-judge) provides supervision. The joint rollout can be written as: Pθ(R, x; ) = (cid:104) tR(cid:89) Pθ(Rt R<t, x; ) ty (cid:89) (cid:104) (cid:105) Pθ(yt y<t, R, x; ) (cid:105) , Rt = (φt, ct, ot), t=1 t=1 (1) where φt is latent think segment, ct tool call (with arguments), and ot the tool response, all expressed in ReAct-style template. We optimize the policy model with token-level clipped loss defined as follows: LGRPO = 1 (cid:88) i=1 1 Ti Ti(cid:88) t=1 (cid:104) min ri,t ˆAi,t, clip(cid:0)ri,t, 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:105) , ri,t = πθ(oi,t x, Ht1) πθold(oi,t x, Ht1) (2) For group of sampled rollouts with scalar rewards {ri}, group-relative advantages defined as: ˆAi,t = ri µR σR , µR = 1 (cid:88) j=1 rj, σR = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) j= (rj µR)2. The trajectory-level scalar reward combines format score and an answer score: ri = 0.1 Rformat + 0.9 Ranswer (3) (4) Here, Rformat <think>, <tool_call>, <tool_response> tags). Meanwhile, Ranswer where correctness of the final answer is judged by an LLM-as-judge against the ground truth. verifies that rollout follows the ReAct template (i.e., all steps are correctly wrapped in pred = agt], = 1[a(i)"
        },
        {
            "title": "2.3 AGENTIC TOOL DESIGN",
            "content": "We provide our policy model access to two tools: search_urls (web search). The tool takes as input natural language query and returns ranked list of triples (u, title, snippet) using live search engine. The policy model uses this to identify promising sources and optionally select URL for opening in the next step. The tool is invoked as follows: <tool_call>{name: search_urls, args: {query: q}}</tool_call> the leverages query LLM to return targeted evidence-backed response that address facts and targeted querying of webCompared to the injection of entire web-page into the policy models trajecis invoked as follows: query_url (goal-conditioned page reading). tool g. pages. tory, <tool_call>{name: query_url, args: {goal: g, url: u}}</tool_call> this tool minimizes noise and increases recall. tool enables precise grounding of Given goal and URL u,"
        },
        {
            "title": "2.4 RAPO: REWARD-AWARE POLICY OPTIMIZATION",
            "content": "In GRPO, the per-prompt (group) reward variance σR (Eq. 3) determines the strength of the advantage signal. When σR=0, group-relative advantages vanish, collapsing batch gradient norms and destabilizing updates  (Fig. 5)  . Such Bad groups arise under both prompt saturation (all rollouts succeed) and cascading errors (all fail). we inroduce RAPO lightweight, zero additional rollout cost extension of GRPO designed to stabilize multi-turn, tool-augmented training by mitigating Bad group σR=0 pathologies while also preserving sample efficiency. Dataset pruning. First, we prune prompts that are effectively solved at the end of each epoch (Eq. 5). This prevents training batches from being dominated by saturated groups that provide negligible variance, while implicitly yielding curriculum in which the active set concentrates on harder prompts. SolveRate(q) = 1 (cid:88) i=1 1[Ri > 0], prune(q) SolveRate(q) 0.9 (5) Advantage scaling. Second, to counter the dilution of gradients when only few groups in batch are informative, we rescale token-level advantages of Good groups inversely with their batch frequency (Eq. 6) This adjustment preserves effective gradient magnitude without requiring costly re-sampling as in DAPO (Yu et al., 2025), ensuring that updates remain stable even when informative groups are sparse. Ai,t = Ggood ˆAi,t, Ggood = #{groups with σR > 0}. (6) Replay buffer. Finally, we maintain per-prompt buffer containing the most recent successful trajectory with R(q, o)>0.5. If all rollouts for prompt fail in the current epoch, one trajectory is randomly replaced with from B. This reintroduces variance (σR>0) into otherwise collapsed groups, restores group-relative advantages, and anchors updates to high-quality, low-entropy reference that curbs uncontrolled trajectory growth. 2.5 STEERABLE STEP-LEVEL REWARD DESIGN FOR SEARCH TOOLS We design our novel Steerable Step-Level Reward that alleviates the reward-hacking challenge faced by RLVR training in the multi-turn, tool-interaction setting using vanilla reward (Eq. 4) as described in (Sec. 1.1). Our reward function enables us to steer (i) how much the agent uses tools and (ii) how it allocates cognition to exploration and verification. Starting from the vanilla RLVR objective in (Eq. 4), we make the correctness branch Ranswer depend on cognitive behaviors and marginal utility aware labels assigned to each call ct in the rollout = {(φt, ct, ot)}T t=1 by GPT-4.1 LLM-as-judge as"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Comparison of policy entropy and gradient norm during RLVR training. GRPO exhibits rapid entropy collapse and diminished gradient norms due to sparse rewards, whereas RAPO sustains exploration and stronger updates via targeted updates follows: search_urls { UNIQUESEARCH: (semantically new query about previously unseen entities/facts), REDUNDANTSEARCH: (Highly similar to prior query; overlapping results)} query_url { EXPLORATION: (first query of new URL), VERIFICATION: (cross-source check on new URL for an existing query; allowed Bv times), REDUNDANTQUERY: (further checks for query/fact on new URLs beyond Bv)} From the LLM-as-Judge tool call classification we form tallies3 and define the following aggregates: ρ = nredS + nredQ , = nuniqS nredS, = nuniqQ nredQ. (7) Using these aggregates we define our Steerable Step-Level Reward as: ri = 0.1 Rformat 0.1 Rformat + max(cid:0)(1 ρ), 0.5(cid:1), + c1 min(cid:0)1, CS (cid:1) + c2 min(cid:0)1, CQ if a(i) if a(i) pred = agt, pred = agt. (cid:1), (8) Here, ρ penalizes redundant tool calls even when the rollout is correct, pushing for efficiency; whereas and provide credit to incorrect rollouts that exhibit genuine, non-redundant exploration & information seeking behavior. We set c1 = c2 = 0.2, to ensure any incorrect rollout has ri 0.5, while any correct rollout has ri 0.5, which ensures incorrect trajectories never get rewarded more than the correct ones. c1=c2 also ensures equal weight to search_urls (S) and query_url (Q). Steerability. We expose three primary knobs: (i) CS and (ii) CQ set the saturation thresholds for creditable novelty in search_urls and query_url, respectively. Increasing CS and/or CQ raises the novelty caps, enabling more steps to earn credit when they introduce genuinely new evidence; decreasing them compresses trajectories. (iii) The per-claim verification budget Bv controls verification depth: higher Bv permits multiple creditable cross-checks per claim, promoting verification. For our experiments we set Bv = 1 allowing 1 cross-check per claim, additionally we set CS = 8 and CQ = 16. 2.6 TRAINING RECIPE 3nuniqS = number of UniqueSearch calls; nredS = number of RedundantSearch calls; nexplore = number of Exploration calls; nverify = number of Verification calls; nuniqQ = nexplore + nverify; nredQ = number of RedundantQuery calls."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Accuracy(%) of Fathom-Search-4B on DeepSearch benchmarks SimpleQA, FRAMES, WebWalker, Seal0, Musique and general reasoning benchmarks HLE, AIME-25, GPQA-D, MedQA. Avg is the unweighted mean within each block. Bold/italics denote best/second-best per benchmark. Model SimpleQA FRAMES WebWalker Seal0 Musique Avg HLE AIME-25 GPQA-D MedQA Avg DeepSearch Benchmarks General Reasoning Benchmarks GPT-4o (without search) o3 (without search) GPT-4o (with search) o3 (with search) Qwen-2.5-7B Qwen-2.5-7B + Search Qwen3-4B Qwen3-4B + Search ZeroSearch-3B ZeroSearch-7B R1-Searcher-7B search-o1 (Qwen3-4B) WebSailor-3B Jan-Nano-32K Jan-Nano-128K II-Search-4B Fathom-Search-4B (Stage-1) Fathom-Search-4B (Stage-2) 34.7 49.4 84.4 96.0 3.96 50.8 3.8 67.7 51.9 75.3 58.8 57.5 87.1 80.7 83.2 88.2 88.1 90.0 52.4 43.2 63.7 86. 16.5 23.3 14.7 27.2 11.3 30.0 37.0 26.8 44.4 36.1 43.4 58.7 57.2 64.8 Closed-Source Models 3.2 14.0 31.6 57.0 7.2 14.0 15.3 49.5 34.0 48.9 37.5 51. 26.3 2.3 33.9 20.3 46.5 4.3 68.1 27.4 Open-Source Models 2.1 10.1 2.6 17.5 8.7 18.2 1.8 10.8 52.2 25.0 33.7 40.8 39.0 50.0 1.4 3.0 2.1 6.2 7.1 6.2 1.4 5.5 9.0 6.2 6.2 17.1 19.8 22. 6.2 13.6 9.0 18.7 13.8 20.6 19.1 15.3 27.4 21.4 23.9 31.8 31.3 33.2 6.0 20.2 6.4 27.5 18.6 30.1 23.6 23.2 44.0 33.9 38.1 47.3 47.1 52.1 1.2 2.4 4.2 6.2 3.4 4.2 2.1 3.4 7.4 5.5 6.1 7.4 6.7 9. 71.0 88.9 71.0 88.9 10 10 65.0 65.0 10.0 10.0 10.0 40.0 40.0 60.0 53.3 60.0 60.0 70.0 53.0 85.4 53.0 85.4 33.0 33.5 55.1 55.9 14.6 29.3 33.3 30.5 45.5 37.4 51.0 51.5 55.6 60. 88.2 95.4 88.2 95.4 61.2 62.0 71.0 72.0 51.0 57.5 56.5 53.7 51.3 66.0 65.4 72.1 75.4 75.4 53.6 72.5 54.1 74.3 24.7 25.3 48.8 49.8 17.3 22.8 25.5 31.9 36.0 42.2 44.0 47.8 49.4 53. We build our reinforcement learning with verifiable rewards (RLVR) framework on top of RECALL (Chen et al., 2025). For web search, we use the Serper API (Serper.dev), and implement retrieval toolchain leveraging Jina-AI together with open-source components such as TRAFILTURA and CRAWL4AI. Training is carried out in two stages. Stage 1. We train with RAPO for 10 epochs on our curated DUETQA dataset, comprising 4,988 highquality QA instances. The setup uses constant learning rate of 1 106 with the Adam optimizer (β1 = 0.9, β2 = 0.95), batch size 32, mini-batch size 16, 5 rollouts per group, and top-p = 1.0 sampling. Each rollout is capped at 32 tool-interaction steps, with each step limited to 8,192 output tokens. The vanilla reward (Eq. 4) with α = 0.1 is used to instill correct tool-calling behavior and strict format adherence. Stage 2. We continue RLVR training for an additional 2 epochs under the same hyperparameter settings. For Stage 2, we construct mixed dataset by combining DUETQA, with math data from S1 dataset (Muennighoff et al., 2025), and the training split of MUSIQUE (Trivedi et al., 2022). This combined pool is adversarially filtered against the Stage-1 checkpoint, yielding 5,077 instances. From MUSIQUE, we retain only questions requiring at least three reasoning hops to ensure sufficient compositional depth. For this stage, we adopt the Steerable Step-Level Reward (Eq. 8) to extend the tool-use horizon beyond 20 calls in stable manner. We use the Qwen3-4B model (Yang et al., 2025) as the base, which supports maximum context length of 40,960 tokens; we utilize the full window during training. We use GPT-4.1-mini(Temperature=0.) as the query LLM for training and evaluation unless stated otherwsie. higher sampling temperature of 1.4 is applied to Qwen3 models, consistent with prior findings (An et al., 2025). All experiments are conducted on single node with 8H100 GPUs. Figure 6: Evolution of unique/redundant tool-calls during Stage-2 training using our Steerable Step-level Reward (Eq.8)"
        },
        {
            "title": "3 FATHOM-SYNTHESIZER-4B",
            "content": "Fathom-Synthesizer-4B is planning and synthesis model built on Qwen3-4B via supervised finetuning (SFT). It converts multi-hop DeepSearch traces from Fathom-Search-4B into decision-grade, citation-dense DeepResearch Reports. Following Plan-then-Write protocol, the model first decomposes the question into sub-goals, defines the report structure, maps evidence to sections, and specifies strategies for insight generation; only then does it produce the public report with citations drawn strictly from URLs explored by Fathom-Search-4B. This explicit planning improves question"
        },
        {
            "title": "Preprint",
            "content": "alignment, strengthens citation accuracy through section-level constraints, and provides structured supervision during SFT, enhancing the distillation process."
        },
        {
            "title": "3.1 DEEPRESEARCH-SFT",
            "content": "DEEPRESEARCH-SFT is synthetic dataset distilled from GPT-5 (OpenAI, 2025a) to train FathomSynthesizer-4B, it provides supervision along three complementary axes: (i) Question decomposition. Each input question is decomposed into ordered sub-questions πdecomp = (S1, . . . , Sn), which form the report scaffold and ensure coverage of all facets(ii) Section mapping. Every piece of evidence recovered during search (URLs, quoted passages, tables, figures) is grounded to one or more sections via mapping πmap, this aligns each explored URL to the most relevant Si, enhancing citation accuracy and preventing omissions/duplication.(iii) Planning for insights. The model specifies an analysis strategy πinsight how the gathered evidence should be synthesized into higher-level insights. Formally, given question and trajectory τ = {R1, . . . , RT }, the teacher outputs Plan and Report. The plan π = (πdecomp, πmap, πinsight) appears in private <think> block, followed by the public report r. The training target is = <think> π </think> r. Report structure. The public-facing report follows fixed, inline-citationdriven format: an Executive Summary followed by Main Body organized exactly by the sections (S1, . . . , Sn) from πdecomp, where each section weaves the mapped evidence from πmap using the analysis strategy in πinsight. Sections are citation-dense: every pivotal or non-obvious claim carries inline citations drawn only from URLs explored in τ , with section-level citations restricted to items mapped to that section in πmap. The report concludes with deduplicated Sources used list of the cited URLs. Thematic diversity & scale. Training questions are generated via the Seeded Question mode (Sec 2.1), starting from 100 open-ended real-world questions spanning law, business, technology, science, and policy. Rewritten across sampled themes, this yields 2,500 questions for training. 3.2 TRAINING RECIPE We fine-tune Qwen3-4B on DEEPRESEARCH-SFT, training for 5 epochs on the 2,500-sample split using single node of 8H100 GPUs. We use bf16, FlashAttention-2, 65,536-token context, gradient accumulation of 8, cosine LR with peak 5.0 105, Adam (β1=0.9, β2=0.95), and sequence parallel size 4.Context extension. Our DeepSearch traces exhaust Qwen3-4Bs native 40,960-token context window, so we extend the effective context during SFT using YaRN RoPE scaling: rope_scaling:{type=yarn, factor=2.0}. This increases the usable positional range to 65,536 tokens, allowing the synthesizer to ingest the full investigation trace and generate high-quality synthesis while preserving section alignment and citation locality."
        },
        {
            "title": "4 BASELINES, BENCHMARKS & METRICS",
            "content": "Baselines. Open-source DeepSearch agents: with public checkpoints: Jan-Nano (Dao & Vu, 2025), II-Search-4B (Internet, 2025), Qwen3-4B (Yang et al., 2025), ZeroSearch (Sun et al., 2025a), Searcho1 (Li et al., 2025b), R1-Searcher (Song et al., 2025), WebSailor (Li et al., 2025a). Closed-source: comparators: o3 (OpenAI, 2025a), GPT-4o (OpenAI, 2024). Benchmarks (9). DeepSearch (5): SimpleQA (Wei et al., 2024), FRAMES (Krishna et al., 2024), WebWalkerQA (Wu et al., 2025), Seal0 (Pham et al., 2025), MuSiQue (Trivedi et al., 2022). General reasoning (4): HLE (Phan et al., 2025), AIME-25 (AIME, 2025), GPQA-Diamond (Rein et al., 2024), MedQA (Jin et al., 2021). Metric: Pass@1 using GPT-4.1-mini LLM as Judge (Temperature=0). DeepResearch (1): DeepResearch-Bench (Du et al., 2025). Metrics: RACE (reference-based adaptive criteria-driven evaluation of comprehensiveness, depth, instruction-following, readability) and FACT (factuality via citation accuracy and effective citation count) for open-ended citation driven report generation."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Strong performance rivaling closed-source proprietary models Fathom-DeepResearch establishes itself as clear state-of-the-art by achieving large, non-incremental gains on the most challenging DeepSearch tasks like FRAMES, WebWalker, & Seal0, (Table. 1), while also showing strong"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Accuracy(%) of various Open/Closed-sourced DeepResearch-Agents and Search Augmented LLMs on DeepResearch-Bench. Bold/italics denote best/second-best per category. Model Overall Comp. Depth Inst. Read. C. Acc. E. Cit. RACE FACT Closed Source LLM with Search Tools Claude-3.7-Sonnet w/Search Perplexity-Sonar-Reasoning-Pro (high) Gemini-2.5-Pro-Grounding GPT-4o-Search-Preview (high) GPT-4.1 w/Search (high) 40.67 40.22 35.12 35.10 33.46 38.99 37.38 34.06 31.99 29.42 37.66 36.11 29.79 27.57 25.38 45.77 45.66 41.67 43.17 42. 41.46 44.74 37.16 41.23 40.77 93.68 39.36 81.81 88.41 87.83 32.48 8.35 32.88 4.79 4.42 Closed Source Deep Research Agent Grok Deeper Search Perplexity-DeepResearch Gemini-2.5-Pro DeepResearch OpenAI-DeepResearch 40.24 42.25 48.88 46. 37.97 40.69 48.53 46.87 35.37 39.39 48.50 45.25 46.30 46.40 49.18 49.27 44.05 44.28 49.44 47.14 83.59 90.24 81.44 77.96 8.15 31.26 111.21 40. Open Source Deep Research Agent Kimi-Researcher Doubao-DeepResearch LangChain Open-DeepResearch Fathom-DeepResearch 44.64 44.34 43.44 45.47 44.96 44.84 42.97 42.98 41.97 40.56 39.17 45.14 47.14 47.95 48.09 48. 45.59 44.69 45.22 46.12 52.86 56.1 52.62 38.3 Figure 7: Accuracy vs Response length & Accuracy vs Avg.Tool calls plot comparing open-source DeepSearch models, clearly demonstrates the higher accuracy and efficient long horizon tool interaction ability of Fathom-Search models compared to its contemporaries. (Note: Here accuracy refers to the unweighted average of all benchmarks in Table. 1.) generalization to broader reasoning benchmarks like (GPQA-Diamond and Humanitys Last Exam). Unlike many search-augmented systems that falter outside their training domain, it consistently outperforms both its base model and other open-source systems, and even surpasses larger closed-source models such as GPT-4o with notable margins. On open-ended benchmark: DeepResearch-Bench, it outperforms most proprietary closed-source systems (including Claude, Grok, and Perplexity Deep Research) (Table. 2) underscoring its competitiveness in end-to-end deep research tasks. On policy optimization: RAPO vs. GRPO. Table 3 contrasts RAPO and GRPO as the policyoptimization algorithm for Stage-1 training. With the Stage-1 setup fixed, RAPO consistently outperforms GRPO across DeepSearch benchmarks, This shows that RAPO provides more stable and effective optimization signal. As shown in Fig. 8, GRPO expands response length as training progresses, but this growth does not translate into higher accuracy because of the model collapsing into redundant tool-spamming behavior. RAPO, in contrast, achieves stronger efficiency and more accurate results."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Ablation on using RAPO as the policy optimization algorithm for Stage-1 training compared to GRPO. Both trainings done on top of Qwen3-4B model. Algorithm SimpleQA FRAMES WebWalker Seal Avg. Tokens GRPO RAPO 87.8 88.1 55.2 57.2 33.8 39.0 14.4 19. 9,000 5,000 Table 4: Ablation on using our steerable step-level reward compared to the vanilla trajectory-level RLVR reward for Stage-2 training. Trained on top of Fathom-Search-4B (Stage-1) using RAPO. Reward SimpleQA FRAMES WebWalker Seal0 Avg. Tokens Vanilla Reward (Eq. 4) Steerable Step-Level Reward (Eq. 8) 88.2 90 58.2 64.8 43.2 21.6 22.5 5,500 14,500 On the Steerable Step-Level reward. As shown in Fig. 6, the steerable step-level reward provides finer-grained training signal that steers the tool calling behavior of the model. By directly shaping the utility of each intermediate step, it encourages controlled growth in response length without inflating reasoning traces with redundant tool call spam, thereby yielding both efficiency and stability in multi-step reasoning, outperforming the vanilla RLVR reward function on all DeepSearch tasks as shown in Table. 4 Limitations. While RAPO is effective for stabilizing multi-turn RL training, it shows limited test-time scaling. As illustrated in Fig. 8, RAPO with vanilla rewards during Stage-2 training saturates before 6,000 tokens and yields only marginal accuracy gains  (Table 4)  as question difficulty increases, when the steerable step-level reward is absent. This trade-off arises from its reliance on trajectory replacement in the replay buffer, which anchors learning to low-entropy traces which prevents training collapse but also hinders adaptation to extended reasoning horizons. More broadly, our current system depends on synchronous training pipelines that, although simple to implement, remain inefficient and brittle at scale. Transitioning to asynchronous frameworks presents natural next step for improving efficiency and robustness."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Figure 8: Response length evolution during (i) top. Stage-2 training (Steerable Step Level Reward vs. Vanilla Reward) & (ii) bottom. Stage-1 training (GRPO vs RAPO) We present Fathom-DeepResearch, an agentic system that addresses critical gaps in open-source deep research capabilities through two specialized 4B models: Fathom-Search-4B for multi-turn web search and reasoning, and Fathom-Synthesizer-4B for structured report synthesis. Our key contributions include DuetQA, multi-agent self-play dataset that ensures search dependency; RAPO, stabilized extension of GRPO that enables reliable tool use beyond 20 calls through curriculum pruning, advantage scaling, and replay buffers; steerable step-level reward system that mitigates reward hacking while providing explicit control over exploration and verification behaviors; and DeepResearch-SFT, synthetic corpus that enables comprehensive information synthesis through explicit plan-then-write supervision."
        },
        {
            "title": "REFERENCES",
            "content": "Moonshot AI. Kimi researcher, 2025. URL https://moonshotai.github.io/ Kimi-Researcher/."
        },
        {
            "title": "Preprint",
            "content": "AIME. Aime problems and solutions, 2025, 2025. URL https://artofproblemsolving. com/wiki/index.php/AIME_Problems_and_Solutions. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp. github.io/blog/2025/Polaris. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning, 2025. URL https://arxiv.org/ abs/2503.19470. Alan Dao and Dinh Bach Vu. Jan-nano technical report, 2025. URL https://arxiv.org/ abs/2506.22760. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic reinforced policy optimization, 2025. URL https://arxiv.org/abs/2507. 19849. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online),"
        },
        {
            "title": "Preprint",
            "content": "December 2020. International Committee on Computational Linguistics. URL https://www. aclweb.org/anthology/2020.coling-main.580. Intelligent Internet. Ii-search-4b: Information seeking and web-integrated reasoning llm. https: //huggingface.co/II-Vietnam/II-Search-4B, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation, 2024. URL https://arxiv.org/abs/2409.12941. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent, 2025a. URL https://arxiv.org/abs/2507.02592. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366, 2025b. doi: 10.48550/ARXIV.2501.05366. URL https://doi.org/10. 48550/arXiv.2501.05366. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, and Shafiq Joty. Sfr-deepresearch: Towards effective reinforcement learning for autonomously reasoning single agents. arXiv preprint arXiv:2509.06283, 2025. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Introducing openai o3 and o4-mini, 2025a. URL https://openai.com/index/ introducing-o3-and-o4-mini/. OpenAI. Introducing deep research, 2025b. URL https://openai.com/index/ introducing-deep-research/. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models. arXiv preprint arXiv:2506.01062, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, et al. Humanitys last exam, 2025. URL https://arxiv.org/abs/2501.14249. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Serper.dev. Serper.dev ai-powered search api. URL https://serper.dev/."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Kunal Singh, Sayandeep Bhowmick, Pradeep Moturi, and Siva Kishore Gollapalli. NO STRESS NO GAIN: STRESS TESTING BASED SELF-CONSISTENCY FOR OLYMPIAD PROGRAMMING. In ICLR 2025 Workshop: VerifAI: AI Verification in the Wild, 2025a. URL https: //openreview.net/forum?id=7SlCSjhBsq. Kunal Singh, Ankan Biswas, Sayandeep Bhowmick, Pradeep Moturi, and Siva Kishore Gollapalli. Sbsc: Step-by-step coding for improving mathematical olympiad performance, 2025b. URL https://arxiv.org/abs/2502.16666. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.05592. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching, 2025a. URL https://arxiv.org/abs/2505.04588. Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834, 2025b. Gemini Team. Gemini deep research, 2025a. URL https://gemini.google/overview/ deep-research/. Langchain Team. Langchain open deep research, 2025b. URL https://github.com/ langchain-ai/open_deep_research. Perplexity Team. Introducing perplexity deep research, 2025c. URL https://github.com/ Alibaba-NLP/DeepResearch. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition, 2022. URL https://arxiv.org/abs/ 2108.00573. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, 2025. URL https://arxiv.org/abs/2501.07572. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018."
        },
        {
            "title": "Preprint",
            "content": "Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476."
        }
    ],
    "affiliations": [
        "Fractal AI Research"
    ]
}