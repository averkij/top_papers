{
    "paper_title": "VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator",
    "authors": [
        "Hyojun Go",
        "Dominik Narnhofer",
        "Goutam Bhat",
        "Prune Truong",
        "Federico Tombari",
        "Konrad Schindler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain a formidable 3D scene generator if one were able to combine the power of a modern latent text-to-video model as \"generator\" with the geometric abilities of a recent (feedforward) 3D reconstruction system as \"decoder\". We introduce VIST3A, a general framework that does just that, addressing two main challenges. First, the two components must be joined in a way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only a small dataset and no labels. Second, the text-to-video generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, a popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also enables high-quality text-to-pointmap generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 4 5 4 3 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "VIST3A: TEXT-TO-3D BY STITCHING MULTI-VIEW RECONSTRUCTION NETWORK TO VIDEO GENERATOR Hyojun Go1 Dominik Narnhofer1 Goutam Bhat2 Federico Tombari2 Konrad Schindler1 1ETH Zurich, 2Google Prune Truong2 Figure 1: Text-to-3D generation with VIST3A. Video models excel at generating latent visual content from text prompts, whereas 3D foundation models shine when it comes to decoding such latent representation into consistent scene geometry. By stitching video generator and 3D reconstruction network together and aligning their latents, we obtain an end-to-end model that produces high-quality Gaussian splats (a) or point maps (b) from text prompts."
        },
        {
            "title": "ABSTRACT",
            "content": "The rapid progress of large, pretrained models for both visual content generation and 3D reconstruction opens up new possibilities for text-to-3D generation. Intuitively, one could obtain formidable 3D scene generator if one were able to combine the power of modern latent text-to-video model as generator with the geometric abilities of recent (feedforward) 3D reconstruction system as decoder. We introduce VIST3A, general framework that does just that, addressing two main challenges. First, the two components must be joined in way that preserves the rich knowledge encoded in their weights. We revisit model stitching, i.e., we identify the layer in the 3D decoder that best matches the latent representation produced by the text-to-video generator and stitch the two parts together. That operation requires only small dataset and no labels. Second, the text-tovideo generator must be aligned with the stitched 3D decoder, to ensure that the generated latents are decodable into consistent, perceptually convincing 3D scene geometry. To that end, we adapt direct reward finetuning, popular technique for human preference alignment. We evaluate the proposed VIST3A approach with different video generators and 3D reconstruction models. All tested pairings markedly improve over prior text-to-3D models that output Gaussian splats. Project page: https://gohyojun15.github.io/VIST3A/"
        },
        {
            "title": "Preprint",
            "content": "Moreover, by choosing suitable 3D base model, VIST3A also enables highquality text-to-pointmap generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "With image and video generators now commodity, text-to-3D models that produce 3D scenes from text prompts have become new research frontier, with applications in AR/VR, gaming, robotics, and simulation. Early methods for 3D generation adopt Score Distillation Sampling (SDS) (Poole et al., 2023; Tang et al., 2024b; Wang et al., 2023b; Chen et al., 2024b) to optimize 3D representation, e.g. NeRF (Mildenhall et al., 2021; Muller et al., 2022) or 3D Gaussian Splats (3DGS, Kerbl et al., 2023) under pretrained 2D diffusion prior (Rombach et al., 2022). drawback these methods have in common is the need for slow per-scene optimization. Another line of work uses multi-stage pipelines that first synthesize images and then lift them to 3D with separate model (Tang et al., 2024a; Xu et al., 2024b; Zhang et al., 2024b) or with per-scene optimization (Gao et al., 2024; Wu et al., 2024a; Yu et al., 2024b); employ progressive warping and refinement (Shriram et al., 2025; Yu et al., 2025; 2024a); or sequentially chain multiple generative modules (Yang et al., 2025b; Engstler et al., 2025). The multi-stage design not only increases model complexity and engineering effort, but also makes such models prone to error accumulation (Lin et al., 2025; Meng et al., 2025). recent trend is to directly generate the 3D representation with end-to-end latent diffusion models (LDMs, Schwarz et al., 2025; Lan et al., 2024; Li et al., 2025b;a; Bahmani et al., 2025). prominent line of work starts from pretrained 2D image (Esser et al., 2024; Rombach et al., 2022) or video models (Genmo Team, 2024; Yang et al., 2024b) and finetunes them to output multi-view 2D latents, reusing the pretrained priors (Szymanowicz et al., 2025; Liang et al., 2025; Schwarz et al., 2025; Lin et al., 2025; Yang et al., 2025c; Go et al., 2025a;b). Subsequently, VAE-style decoder is trained to decode those latents into the desired 3D representation, see Fig. 2. The LDM-like design unifies 2D generation and multi-view reconstruction within the latent space and enables efficient 3D scene generation with compact, well-amortized decoder. Still, two key limitations remain. First, we argue that the Achilles heel of existing 2D-to-3D diffusion models is the decoder. By simply repurposing the 2D VAE to produce 3D outputs, the network must learn 3D reconstruction more or less from scratch, which requires extensive training and large datasets that are hard to obtain (Yang et al., 2025c; Szymanowicz et al., 2025; Go et al., 2025b). This practice becomes increasingly problematic as new, better 3D foundation models emerge (Wang et al., 2025d;a; 2024b; Zhang et al., 2025) and the ad-hoc trained decoders of text-to-3D models fall further behind the state of the art in 3D vision. Figure 2: Comparison with existing, LDM-based 3D generators. Instead of training custom decoder from multi-view 2D latents to 3D outputs, we stitch and align an existing, pretrained 3D reconstruction model. Second, the prevalent training scheme tends to suffer from weak alignment between the generative model and the VAE decoder. Typically, the former is finetuned on multi-view datasets with generative objective like diffusion loss (Song et al., 2020; Sohl-Dickstein et al., 2015; Ho et al., 2020) or flow matching (Liu et al., 2023; Lipman et al., 2023; Albergo & Vanden-Eijnden, 2023), which only indirectly promotes 3D-consistent latents. Moreover, the separate training may cause the latents, even if 3D-consistent, to be out of domain from the perspective of the decoder. To mitigate that misalignment, it has been proposed to add rendering losses that promote decodable latents (Lin et al., 2025). However, the resulting objective is based on single-step sampling and does not sufficiently take into account the denoising trajectory, leading to weak alignment at inference. We introduce VIST3A: VIdeo VAE STitching and 3D Alignment. The proposed method consists of two complementary components that address the above-mentioned limitations, see Fig. 2. First, we resort to the concept of model stitching (Pan et al., 2023; Lenc & Vedaldi, 2015; Bansal et al., 2021; Csiszarik et al., 2021; Yang et al., 2022) to leverage powerful, pretrained feedforward 3D models for decoding, rather than start from scratch. The idea is to attach the relevant part of 3D reconstruction"
        },
        {
            "title": "Preprint",
            "content": "network as decoder to the latent space of video VAE. For this to work, there need to be one or more layers in the 3D model whose activations are similar (up to linear transformation) to those in the VAEs latent space, despite their independent pretraining. Perhaps surprisingly, this turns out to be the case. For the 3D model, we identify the layer with the most linear relation to the LDM latents, slice the network before that layer, and retain the downstream portion as 3D decoder. After fitting single, linear stitching layer (in closed form), the VAE latent space already matches the expected input of the 3D decoder well, such that subsequent fine-tuning will be minor and not degrade the respective generative and 3D reasoning capabilities of the two base models. Second, we further improve alignment between the generative model and the stitched decoder through direct reward finetuning (Clark et al., 2023; Xu et al., 2023; Prabhudesai et al., 2024; Wu et al., 2024c; Shen et al., 2025). In that technique, commonly used to align diffusion models with human preferences, reward signals are defined based on the goodness of the VAE output in our setting, the visual quality and 3D consistency of the decoded 3D representations. Maximizing these rewards encourages the LDM to produce latents that are 3D-consistent and lie within the decoders input domain, ensuring high-quality outputs. Importantly, our alignment compares video model outputs and images rendered from the generated 3D scenes, hence it does not require labels. In our experiments, we show that the proposed stitching scheme is applicable across range of video generative models and also across several different feedforward 3D models. VIST3As direct 3D decoding consistently outperforms prior text-to-3DGS methods, and additionally offers high-quality pointmap generation from text prompts."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "3D generation. Recent works have explored various 3D representations for generative modelling, including point clouds (Mo et al., 2023; Nichol et al., 2022; Vahdat et al., 2022), meshes (Xu et al., 2024a; Woo et al., 2024), voxel grids (Sanghi et al., 2023), NeRFs (Chen et al., 2023; Muller et al., 2022; Mildenhall et al., 2021), and 3DGS (Henderson et al., 2024; Zhang et al., 2024a; Kerbl et al., 2023). Score distillation using 2D diffusion models is time-consuming, as it requires per-scene test time optimization (Wang et al., 2023a; Shi et al., 2023; Wang et al., 2023b), while multi-stage pipelines (Yu et al., 2024b; Liu et al., 2024; Zheng et al., 2025) lack robustness and create significant engineering overhead. For further details on multi-stage pipelines, please refer to Appendix A. More recently, the field has shifted towards end-to-end latent diffusion models, where the generator operates in the latent space of VAE, and the latter directly decodes the resulting latents to 3D outputs. Many of these works focus on object-centric asset generation (Wu et al., 2024b; Zhao et al., 2023; Lin et al., 2025) and train the LDM on curated datasets such as Objaverse (Deitke et al., 2023), with single objects or bounded scenes, and controlled camera paths. Consequently, they are unable to handle real-world challenges like strongly varying scene scale, variable lighting, etc. To tackle such situations, recent methods (Szymanowicz et al., 2025; Liang et al., 2025; Schwarz et al., 2025; Lin et al., 2025; Yang et al., 2025c; Go et al., 2025a;b; Bahmani et al., 2025) repurpose the comprehensive knowledge of the visual world that is implicit in 2D image generators. The general strategy is to finetune pretrained 2D model on multi-view data, by using generative losses to enforce cross-view consistency. In many cases training is further supported by additional 3D cues like camera poses (Li et al., 2024; Go et al., 2025b), depthmaps (Go et al., 2025a; Yang et al., 2025c), or pointmaps (Szymanowicz et al., 2025). The resulting multi-view latents are decoded to 3D scenes with dedicated VAE-style decoder, meaning that 3D reasoning capabilities must be rebuilt from scratch, and that they are only weakly aligned with the generator output limitations which we address with VIST3A. Learned 3D reconstruction. notable trend in 3D computer vision is the trend to move away from multi-stage pipelines and iterative optimization towards end-to-end, feedforward 3D modelling. Classical reconstruction pipelines based on SfM (Hartley & Zisserman, 2003; Schonberger & Frahm, 2016) and MVS (Furukawa et al., 2015; Schonberger et al., 2016) require incremental, iterative optimization, whereas recent advances like DUSt3R (Wang et al., 2024b) and MASt3R (Leroy et al., 2024) directly predict 3D point maps in one forward pass. Several follow-up works have further reduced test-time optimization (Tang et al., 2025; Wang et al., 2025b; Yang et al., 2025a). Likewise, 3D Gaussian splatting has evolved from per-scene optimization to feedforward prediction (Charatan et al., 2024; Chen et al., 2024a; Ye et al., 2024). Once more, data scaling has been"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: VIST3A constructs 3D VAE through model stitching (top), then aligns it with generative model via direct reward finetuning (bottom). Stitching repurposes part of pretrained 3D vision model as decoder to obtain 3D VAE. Direct reward finetuning simulates full-trajectory denoising, forcing the generative model to produce 3D-consistent, decodable latents. critical factor (Wang et al., 2025a;d). Consequently, replicating the 3D capabilities of recent feedforward models as part of VAE training would be difficult and costly. VIST3A offers solution by reusing, rather than rebuilding, models like AnySplat (Jiang et al., 2025), VGGT (Wang et al., 2025a), or MVDUSt3R (Tang et al., 2025). Model stitching. Recomposing the heads and tails of two different networks was initially studied as way to assess the equivariance of neural representations (Lenc & Vedaldi, 2015), and as an experimental tool to compare two different representations (Csiszarik et al., 2021; Bansal et al., 2021). To ensure invariance against trivial affine transformations, the head of some trained network is normally attached to the tail of another network via linear, trainable stitching layer. Besides revealing similarities between networks that common metrics like CKA (Kornblith et al., 2019) would miss, it was also found that different architectures that were trained on the same data can often be stitched into new, hybrid model with minimal degradation (Bansal et al., 2021). This has opened the door for practical uses of stitching, e.g. DeRy (Yang et al., 2022) for resourceconstrained reassembly of pretrained models and SN-Net (Pan et al., 2023) to build networks with varying scales. Going one step further, we demonstrate that strong 3D VAEs1 can be obtained by stitching foundational 3D model to the latent space of video VAE as its decoder, even if they were trained independently on different data."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "VIST3A consists of two key components, see Fig. 3: (1) model stitching to optimally attach (part of) foundational 3D model as the decoder for the latent, and (2) direct reward finetuning to optimize the alignment of the (latent) generative model with that new decoder. 3.1 MODEL STITCHING FOR 3D VAE CONSTRUCTION Our objective is to build 3D VAE by seamlessly combining the encoder of video LDM and feedforward 3D reconstruction model. Note that, for stitching purposes, one can skip the denoising loop, since feeding images into the encoder already gives clean latents. Let denote the encoder and the decoder of the VAE, and let F1:l(x) = fl f1(x) = be the feedforward 3D network that maps set of views to 3D output y, with the total number of layers in that feedforward model. As shown in Fig. 3, we cut the feedforward model at layer and stitch the downstream part 1To be consistent with existing literature (Lan et al., 2024; Yang et al., 2025c), we also use the term 3D VAE, although the mapping from 2D images to 3D scene is, technically, not variational auto-encoder."
        },
        {
            "title": "Preprint",
            "content": "Fk+1:l = fl fk+1 to the output layer of the encoder E, with the help of linear stitching layer S. In doing so, we obtain new 3D VAE Mstitched that outputs the same representation ˆy as the original 3D model: Mstitched = Fk+1:l E(x) = ˆy, Dstitched = Fk+1:l (1) The front portion F1:k of the 3D model is discarded but if the clean encoder latents, after the affine warping S, are (almost) the same as the activations fk , then the back portion will still produce the same output, ˆy y. In other words, the stitched VAE Mstitched is an approximation of the original 3D model . It retains much of the ability to map multi-view images to 3D reconstruction and only requires little fine-tuning to restore that ability. Step 1: Finding the stitching index and initialization. To identify the layer in the 3D model whose representation is most compatible with the VAE latent, we first push set of samples through the encoder to obtain their latents RN DE . Then, we scan over candidate layers {1, ..., 1} of the 3D model and, for each layer in turn, extract the activations Ak RN DF RDE DF that best recovers the activations of the 3D model and fit the linear stitching layer at layer k, by solving least-squares problem: BSk Ak2 = (cid:0)BB(cid:1) BAk. (2) = arg min Sk Finally, we select the stitching layer that leads to the smallest (mean squared) error, = arg mink BS and Fk+1:l. Empirically, we find that most combinations of foundational VAEs and 3D feedforward models can be stitched in this manner, with minimal performance loss. , and assemble the 3D VAE by concatenating E, Ak Step 2: Stitched decoder finetuning. To further reduce the remaining discrepancies between the newly assembled 3D VAE and the original 3D model, we finetune and Fk+1:l to reproduce the predictions of the original 3D model y, using them as pseudo-targets. Practical feedforward models produce multiple outputs (e.g., point maps, depth, poses), so we optimize weighted sum of ℓ1 losses for all of them. Note that the fine-tuning step is self-supervised and does not require labels. In our implementation, we restrict the stitching layer to 3D convolution and employ LoRA (Hu et al., 2022) for updating Fk+1:l, to prevent large deviations from the pretrained weights. For further details, see Appendix B.1. 3.2 ALIGNMENT VIA DIRECT REWARD FINETUNING So far, we have assembled 3D VAE with strong, pretrained 3D decoder. However, during text-to3D inference, the latents are not obtained from the encoder but generated from noise by the denoising loop conditioned on the text prompt. Therefore, we must also align the generative model itself with the 3D decoder, such that it produces decodable latents. Previous work finetunes the generative network by minimizing generative losses over some multiview dataset. Unfortunately, that strategy does not ensure 3D-consistent latents. Even if it did, the finetuning bypasses the decoder, hence there is no guarantee that the generated latents fall within the distribution expected by the 3D VAE and can be decoded to meaningful outputs. To address the disconnect between the denosing loop and the 3D VAE, we adopt direct reward finetuning to align the two. In other words, we extend conventional, generative multi-view finetuning with reward maximization. The conventional generative loss Lgen uses paired data, i.e., multi-view images and corresponding prompts. In contrast, the proposed reward term r(, c) relies only on the text prompt and requires no ground-truth images. Our total loss is defined as Ltotal = Lgen r(cid:0)z0(θ, c, zT ), c(cid:1), where θ are the parameters of the video generative model, represents the text prompt, zT is the initial noise, and z0(θ, c, zT ) is the final latent produced by the denoising loop. (3) Reward. The proposed reward function consists of three components that ensure high-quality and 3D-consistent generation. (1) Multi-view Image Quality: As we keep the encoder frozen, the generated latents can be decoded by the original video decoder to obtain multi-view images. We evaluate these images against the input prompt using CLIP-based (Fang et al., 2024) and HPSv2 human preference scores (Wu et al., 2023) to promote prompt adherence and visual quality, similar to DanceGRPO (Xue et al., 2025). (2) 3D Representation Quality: To encourage high-quality 3D"
        },
        {
            "title": "Preprint",
            "content": "outputs after decoding with Dstitched, we render the generated 3D scenes (pointmaps and/or 3DGS) back into 2D views and apply the same (CLIP + HPSv2) metrics to them as above. (3) 3D Consistency: To enforce 3D consistency, we render the 3D representation from the same viewpoints as the multi-view images reconstructed by the video decoder D, using the camera poses predicted by the feedforward 3D model. We then compute combination of ℓ1-loss and LPIPS (Zhang et al., 2018) for each pair of decoded and rendered images belonging to the same viewpoint. The final (negative) reward is weighted sum of these three losses. For further details, see Appendix B.2. Alignment algorithm. To optimize the generative model according to the reward function above, we employ direct reward finetuning (Clark et al., 2023; Xu et al., 2023; Prabhudesai et al., 2024; Wu et al., 2024c; Shen et al., 2025). I.e., the model generates samples by unfolding the full denoising path, and the rewards computed from these samples are then backpropagated through the denoising chain. While the algorithm benefits from gradient-based feedback, it can also suffer from exploding gradient norms. To stabilize the optimization, we generalize the idea of DRTune (Wu et al., 2024c): gradients are detached from the inputs to the generative model, but retained during the update step to the next denoising state. In this way, reward propagation remains stable even at early denoising steps. Furthermore, we modify the optimizer for better computational efficiency by (i) randomized sampling, using fewer timesteps than during inference, and (ii) randomizing the subset of denoising steps where gradients are backpropagated, such that the model learns from diverse denoising trajectories. For further details, see Appendix B.2. In summary, we perform joint, end-to-end alignment of the VAE and the generative model, unlike conventional multi-view fine-tuning that keeps them separate. Reward tuning ensures that, throughout the iterative denoising process, the generative model remains aligned with our 3D VAE and generates latents that suit the stitched decoder."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "In what follows, we demonstrate VIST3As text-to-3D generation performance. The main findings are that VIST3A clearly outperforms existing feedforward text-to-3DGS approaches and also offers high-quality text-to-pointmap generation. Moreover, we experimentally analyze our two core components, self-supervised model stitching and alignment finetuning. 4.1 EXPERIMENTAL SETUPS We provide high-level overview of the experimental setup. complete description of evaluation protocols and training details can be found in Appendix C. Target 3D models. We target last-generation foundational 3D vision models that have been trained on large-scale datasets, have demonstrated generality and reliable performance across diverse domains, and require only images as input. For our experiments, we select three representative state-of-the-art models: (1) MVDUSt3R (Tang et al., 2025) predicts pointmaps and Gaussian splats, (2) VGGT (Wang et al., 2025a) predicts pointmaps, depth maps and camera poses, and (3) AnySplat (Jiang et al., 2025) predicts Gaussian splats and camera poses. Target video generators. Our primary video model is Wan 2.1 T2V large (Wan et al., 2025), state-of-the-art text-to-video generator. To demonstrate the generality of VIST3A across different architectures, we additionally use several other latent video models, including CogVideoX (Yang et al., 2024b), SVD (Blattmann et al., 2023), and HunyuanVideo (Kong et al., 2024). Training data. We finetune stitched VAEs on DL3DV-10K (Ling et al., 2024) and ScanNet (Dai et al., 2017), without 3D labels. To align the video generator in latent space, we utilize DL3DV-10K to compute the generative loss, with prompts from the HPSv2 training set (Wu et al., 2023). 4.2 MAIN RESULTS: 3D GENERATION Stitching Wan to the 3D models listed in Section 4.1 yields two types of generative models: (i) Textto-3DGS when using AnySplat or MVDUSt3R as decoder; and (ii) Text-to-Pointmap when using VGGT or MVDUSt3R. Both variants are evaluated in the following."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Quantitative results on T3Bench and SceneBench. Method Matrix3D-omni Director3D Prometheus3D SplatFlow VideoRFSplat VIST3A: Wan + MVDUSt3R VIST3A: Wan + AnySplat T3Bench (Object-centric) SceneBench (Scene-level) Imaging Aesthetic CLIP Unified Reward Align. Coher. Style Imaging Aesthetic CLIP Unified Reward Align. Coher. Style 43.05 54.32 47.46 46.09 46.52 58.83 57.03 37.66 53.33 44.32 53.24 39.50 56.55 54. 25.06 30.94 29.15 29.48 30.13 32.75 31.38 2.44 3.25 2.84 3.29 3.12 3.56 3.36 3.10 3.43 3.12 3.25 3.24 3.89 3. 2.69 3.05 2.66 2.93 3.09 3.56 3.17 46.65 47.79 44.73 48.85 58.19 62.08 64.87 37.62 52.81 45.85 53.71 51.71 55.67 56. 24.04 29.31 28.57 29.43 29.76 30.26 30.18 2.66 3.36 3.20 3.47 3.58 3.72 3.67 3.29 3.67 3.36 3.65 3.63 3.97 3. 2.80 3.20 2.98 3.26 3.30 3.47 3.40 Table 2: Quantitative results on DPG-Bench. Table 3: Stitching enhances NVS. Method Matrix3D-omni Director3D Prometheus3D SplatFlow VideoRFSplat VIST3A: Wan + MVDUSt3R 81.82 VIST3A: Wan + AnySplat 78.79 DPG-Bench Method PSNR SSIM LPIPS Global Entity Attribute Relation Other 53.32 66.67 45.45 69.70 36. 42.44 64.96 48.35 68.43 56.93 84.31 85.58 56.23 60.85 55.03 65.55 66.89 86.13 84.12 37.12 45.15 33.50 50.49 48.53 68.93 76. 10.32 22.73 9.10 40.91 31.82 54.55 45.45 SplatFlow VideoRFSplat Prometheus3D 19.10 19.05 19.56 0.671 0.674 0.683 0.278 0.281 0. AnySplat 20.85 0.695 0.238 Hunyuan + AnySplat 21.17 21.48 SVD + AnySplat 21.32 CogVid + AnySplat 0.710 0.720 0. 0.242 0.218 0.222 Wan + AnySplat 21.29 0.718 0.232 Baselines. Important baselines for text-to-3DGS are SplatFlow (Go et al., 2025a), Director3D (Li et al., 2024), Prometheus3D (Yang et al., 2025c), and VideoRFSplat (Go et al., 2025b). Additionally, we include Matrix3D-omni (Yang et al., 2025d), to our knowledge, the only other model that unifies generation and reconstruction in latent space. Evaluation protocol. We evaluate text-to-3DGS models on three benchmarks: T3bench (He et al., 2023) for object-centric generation, SceneBench (Yang et al., 2025c) for scene-level synthesis, and DPG-bench (Hu et al., 2024) to assess adherence to long, detailed prompts. On T3bench and SceneBench, we render images and compute Imaging Quality and Aesthetic Quality scores as defined by VBench (Huang et al., 2024) to assess visual fidelity, CLIP score (Hessel et al., 2021) for text-prompt alignment, and Alignment, Coherence, and Style scores according to Wang et al. (2025c) as comprehensive quality metrics. We prefer to avoid traditional no-reference metrics like NIQE (Mittal et al., 2012b) and BRISQUE (Mittal et al., 2012a) that have sometimes been used in the context of 3D generation, but lack meaningful connection to the conditional generation task (e.g., they can be gambled by always returning the same sharp and colorful, high-scoring image, independent of the prompt). For DPG-bench, we follow the suggested protocol (Hu et al., 2024), but upgrade from the originally proposed language models to the more capable, UnifiedReward LLM (based on Qwen 7B). Text-to-pointmap models are evaluated qualitatively, as no established benchmarks or baselines exist. Quantitative Results. Tables 1 and 2 show the results for the three text-to-3DGS benchmarks. Notably, both tested VIST3A variants exhibit superior performance across all datasets and evaluation metrics. On T3bench, both Wan+AnySplat and Wan+MVDUSt3R consistently outperform all baselines, with particularly large margins in Imaging Quality and Coherence score. For the more complex scene-level synthesis of SceneBench, our models reach Imaging Quality scores >60 and Coherence scores >3.8, again marked improvement over prior art. On DPG-bench, our models greatly outperform the baselines, mostly scoring >75 (often even 85), values that previously seemed out of reach. The consistent gains on T3bench, SceneBench, and DPG-bench demonstrate the effectiveness and versatility of our stitching approach for text-based 3D scene generation. We attribute these results to the power of foundational contemporary video and 3D models, which our stitching and fine-tuning scheme unlocks for the purpose of 3D generative modeling. Qualitative Results. Figure 4 qualitatively compares VIST3A (Wan+AnySplat) to several baselines. In line with the quantitative results, VIST3A produces superior, visually compelling, and geometrically coherent renderings that closely follow the input prompts; whereas previous methods tend to exhibit artifacts, structural distortions, and poor text alignment. Further qualitative results, including Wan+MVDUst3R and Wan+AnySplat variants of VIST3A, as well as text-to-pointmap examples, can be found in Appendix E. Interestingly, we find that, even without specific training on very long image sequences, VIST3A can generate coherent large-scale scenes by extending the number of frames generated by the LDM. This demonstrates that our framework preserves the ability of video generator and the 3D decoder to handle long sequences. Examples are depicted in Fig. 13."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Qualitative results for 3DGS generation. We show samples from T3Bench (top), SceneBench (middle), and DPG-bench (bottom). VIST3A generates realistic and crisp 3D scenes and adheres to intricate details in the prompt. 4.3 MAIN RESULTS: MODEL STITCHING Stitching the 3D foundation models from Section 4.1 with video VAE yields two variants: VAE for Gaussian splats (AnySplat + video VAE) or VAE capable of reconstructing pointmaps and camera poses (MVDust3R or VGGT + video VAE). In the following, we evaluate both variants. Evaluation protocol. For 3DGS models, we evaluate novel-view synthesis on RealEstate10K (Zhou et al., 2018), with 8 source and 4 target images. For 3D reconstruction models, we follow Pi3 (Wang et al., 2025d) and assess pointmap quality on ETH3D (Schops et al., 2017), and camera pose estimation on RealEstate10K and ScanNet (Dai et al., 2017). Specifically, Accuracy (Acc.), Completion (Comp.), and Normal Consistency (N.C.) are used for pointmap estimation, while camera pose estimation is evaluated with Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) at 5 and their AUC up to 30. Novel view synthesis. Table 3 reports results on RealEstate10K. Stitching AnySplat onto any video model always improves over using AnySplat alone. We attribute the gains to the richer appearance representation of video VAE latents. The experiment is consistent with the results of Wonder-"
        },
        {
            "title": "Preprint",
            "content": "Method Table 4: Results of point map reconstruction with stitched models. Pointmap Estimation Camera Pose Estimation Acc. ETH3D Comp. NC. Mean Med. Mean Med. Mean Med. RealEstate10K ScanNet RRA@5 RTA@5 AUC@30 ATE RPE-T RPE-R MVDUSt3R VGGT 0.400 0.263 Hunyuan+MVDUSt3R 0.405 0.410 SVD+MVDUSt3R 0.412 CogVid+MVDUSt3R Wan+MVDUSt3R Wan+VGGT 0.401 0.265 0.291 0. 0.288 0.310 0.281 0.297 0.166 0.376 0.197 0.399 0.387 0.387 0.386 0.193 0.159 0. 0.166 0.168 0.157 0.164 0.121 0.805 0.855 0.802 0.804 0.781 0.797 0.837 0.905 0. 0.887 0.899 0.888 0.910 0.960 98.66 99.51 98.36 98.12 98.29 98.28 99.65 12.91 15. 12.40 12.67 12.36 12.30 15.98 42.34 50.06 41.97 41.69 41.96 42.12 50.86 0.015 0. 0.016 0.016 0.016 0.016 0.014 0.019 0.015 0.019 0.020 0.019 0.019 0.015 0.691 0. 0.668 0.690 0.680 0.680 0.520 (a) log-MSE value in Eq. 2 (b) Acc. (c) Comp. (d) NC. Figure 5: MSE and pointmap quality on ETH3D vs. to stitching layer. Lower MSE in the stitching layer correlates with better 3D reconstruction. land (Liang et al., 2025), where operating in latent space rather than RGB space also benefits 3DGS. Moreover, our stitched VAEs outperform the earlier VAE-based approaches. Remarkably, we surpass Prometheus3D and VideoRFSplat despite their use of camera poses and large-scale training data, showing that stitching high-performance 3D models is indeed an effective strategy to obtain powerful 3D VAEs. Pointmap reconstruction results. Table 4 shows that stitching preserves the accuracy and completeness of the original 3D foundation models: both pointmap quality and camera pose accuracy barely change when using video encoder latents as input. The results confirm that stitching achieves its goal, to take advantage of the pretrained models 3D reconstruction capabilities and repurpose them for generative modeling, without relying on large training datasets or labels. 4.4 ABLATIONS Impact of the stitching index (Sec 3.1). We pick the best layer for stitching according to fairly simple criterion, namely the one that best supports linear transfer of the encoder latents. To analyze the impact of this design, we train stitched decoders for the combination (Wan+VGGT) while varying the stitching index. In Fig. 5, we see that layers with lower stitching residual indeed yield better pointmaps, supporting the MSE of the linear stitching layer as our selection criterion. Notably, early layers tend to exhibit lower MSEs. It appears that the latents are more compatible with low-level features that retain fine details. Impact of direct reward finetuning (Sec 3.2). As shown in Appendix D.1, direct reward finetuning is more effective than pretrained video model on its own, as well as that same model finetuned on multi-view data, with each reward component contributing to the overall performance. Benefits of integrated vs. sequential 3D generation. In Appendix D.2, we observe that an integrated approach is more robust to noise in the latent space, which suggests it may lead to more consistent 3D reconstruction from noise in the generation process."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have presented VIST3A, framework for training latent diffusion models that generate 3D content from text prompts. Our key idea is to employ model stitching as way to integrate the generative abilities of modern video models with the 3D understanding of recent feedforward 3D"
        },
        {
            "title": "Preprint",
            "content": "models. We found that this strategy indeed leads to high-quality 3D VAEs, while not requiring labeled data or massive training runs. To then align latent-space video generator with the stitched 3D decoder it feeds into, we design reward-based finetuning strategy. Together, these two measures yield family of text-to-3D models with high-quality, geometrically consistent 3D outputs. In passing, they extend 3D generation to other outputs of foundational 3D models, such as pointmaps and depthmaps. More broadly, we see great potential for model stitching as general tool to combine two or more foundational neural networks, including latent generative models, into powerful end-to-end solutions."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations, 2023. Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David Lindell, Zan Gojcic, Sanja Fidler, et al. Lyra: Generative 3d scene reconstruction via video diffusion model self-distillation. arXiv preprint arXiv:2509.19296, 2025. Yamini Bansal, Preetum Nakkiran, and Boaz Barak. Revisiting model stitching to compare neural representations. Advances in Neural Information Processing Systems, 34:225236, 2021. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In International Conference on Learning Representations, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. preprint arXiv:2311.15127, 2023. David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelSplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1945719467, 2024. Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025a. Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. SingleIn IEEE/CVF stage diffusion NeRF: unified approach to 3d generation and reconstruction. International Conference on Computer Vision, pp. 24162425, 2023. Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. FlexWorld: Progressively expanding 3d scenes for flexible-view synthesis. preprint arXiv:2503.13265, 2025b. Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, TatJen Cham, and Jianfei Cai. MVSplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pp. 370386, 2024a. Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2140121412, 2024b. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. preprint arXiv:2309.17400, 2023. Adrian Csiszarik, Peter Korosi-Szabo, Akos Matszangosz, Gergely Papp, and Daniel Varga. Similarity and matching of neural network representations. Advances in Neural Information Processing Systems, 34:56565668, 2021. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 58285839, 2017."
        },
        {
            "title": "Preprint",
            "content": "Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1314213153, 2023. Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. SynCity: Training-free generation of 3d worlds. preprint arXiv:2503.16420, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. In Advances in Neural Information Processing Systems, 2023. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. In International Conference on Learning Representations, 2024. Hao Feng, Zhi Zuo, Jia-hui Pan, Ka-hei Hui, Yihua Shao, Qi Dou, Wei Xie, and Zhengzhe Liu. WonderVerse: Extendable 3d scene generation with video generative models. preprint arXiv:2503.09160, 2025. Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. SceneScape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36:3989739914, 2023. Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: tutorial. Foundations and trends in Computer Graphics and Vision, 9(1-2):1148, 2015. Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. CAT3D: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems, 37:7546875494, 2024. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. Hyojun Go, Jinyoung Kim, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, and In Thirty-seventh ConferSeungtaek Choi. Addressing negative transfer in diffusion models. ence on Neural Information Processing Systems, 2023a. URL https://openreview.net/ forum?id=3G2ec833mW. Hyojun Go, Yunsung Lee, Jin-Young Kim, Seunghyun Lee, Myeongho Jeong, Hyun Seung Lee, and Seungtaek Choi. Towards practical plug-and-play diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19621971, June 2023b. Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, Soonwoo Kwon, and Changick Kim. SplatFlow: Multi-view rectified flow model for 3d gaussian splatting synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2152421536, 2025a. Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, and Changick Kim. VideoRFSplat: Direct scene-level text-to-3d gaussian splatting generation with flexible pose and multi-view joint modeling. preprint arXiv:2503.15855, 2025b. Seokil Ham, Sangmin Woo, Jin-Young Kim, Hyojun Go, Byeongjun Park, and Changick Kim. Diffusion model patching via mixture-of-prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1702317031, 2025. Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2003. Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and t3Bench: Benchmarking current progress in text-to-3d generation. preprint Yong-Jin Liu. arXiv:2310.02977, 2023."
        },
        {
            "title": "Preprint",
            "content": "Paul Henderson, Melonie de Almeida, Daniela Ivanova, and Titas Anciukeviˇcius. Sampling 3d gaussian scenes in seconds with latent diffusion models. preprint arXiv:2406.13099, 2024. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. preprint arXiv:2104.08718, 2021. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3d. preprint arXiv:2311.04400, 2023. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. ELLA: Equip diffusion models with LLM for enhanced semantic alignment. preprint arXiv:2403.05135, 2024. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. VBench: Comprehensive benchmark suite for video generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. AnySplat: Feed-forward 3d gaussian splatting from unconstrained views. preprint arXiv:2505.23716, 2025. Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, and Konrad Schindler. Video depth without video models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 72337243, 2025. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 51485157, 2021. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d Gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1391, 2023. Jin-Young Kim, Hyojun Go, Soonwoo Kwon, and Hyun-Gyoon Kim. Denoising task difficultybased curriculum for training diffusion models. arXiv preprint arXiv:2403.10348, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. HunyuanVideo: systematic framework for large video generative models. preprint arXiv:2412.03603, 2024. Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pp. 3519 3529, 2019. Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. GaussianAnything: Interactive point cloud flow matching for 3d object generation. preprint arXiv:2411.08033, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. preprint arXiv:2302.12192, 2023. Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. In Proceedings of the AAAI Conference on Multi-architecture multi-expert diffusion models. Artificial Intelligence, volume 38, pp. 1342713436, 2024."
        },
        {
            "title": "Preprint",
            "content": "Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 991999, 2015. Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with MASt3R. In European Conference on Computer Vision, pp. 7191, 2024. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. preprint arXiv:2311.06214, 2023. Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. CraftsMan3D: High-fidelity mesh generation with 3d native diffusion and interactive In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. geometry refiner. 53075317, 2025a. Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3D: Real-world camera trajectory and 3d scene generation from text. Advances in Neural Information Processing Systems, pp. 7512575151, 2024. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. TripoSG: High-fidelity 3d shape synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025b. Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 798 810, 2025. Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong MU. DiffSplat: Repurposing image diffusion models for scalable gaussian splat generation. In International Conference on Learning Representations, 2025. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. DL3DV-10K: large-scale scene dataset for deep learning-based 3d vision. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2216022169, 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow In International Conference on Learning Representations, matching for generative modeling. 2023. Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. ReconX: Reconstruct any scene from sparse views with video diffusion model. preprint arXiv:2408.16767, 2024. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-GRPO: Training flow matching models via online RL. preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, and Lingjie Liu. Zero-1-to-G: Taming pretrained 2d diffusion model for direct 3d generation. arXiv preprint arXiv:2501.05427, 2025. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021."
        },
        {
            "title": "Preprint",
            "content": "Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing, 21(12):46954708, 2012a. Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal Processing Letters, 20(3):209212, 2012b. Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. DiT-3D: Exploring plain diffusion transformers for 3d shape generation. Advances in Neural Information Processing Systems, 36:6796067971, 2023. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 41(4):115, 2022. Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, and Wenjun Mei. WonderTurbo: Generating interactive 3d world in 0.72 seconds. preprint arXiv:2504.02261, 2025. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-E: system for generating 3d point clouds from complex prompts. preprint arXiv:2212.08751, 2022. Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bomer, Mario Bijelic, and Felix Heide. LSD-3D: Large-scale 3d driving scene generation with geometry grounding. preprint arXiv:2508.19204, 2025. Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Stitchable neural networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1610216112, 2023. Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023. Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, and Changick Kim. Switch diffusion transformer: Synergizing denoising tasks with sparse mixture-of-experts. In European Conference on Computer Vision, pp. 461477. Springer, 2024. Byeongjun Park, Hyojun Go, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, and Changick Kim. Steerx: Creating any camera-free 3d and 4d scenes with geometric steering. arXiv preprint arXiv:2503.12024, 2025. Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. preprint arXiv:1910.00177, 2019. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. DreamFusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2023. Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. preprint arXiv:2407.08737, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Aditya Sanghi, Rao Fu, Vivian Liu, Karl D.D. Willis, Hooman Shayani, Amir H. Khasahmadi, Srinath Sridhar, and Daniel Ritchie. CLIP-Sculptor: Zero-shot generation of high-fidelity and diverse shapes from natural language. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1833918348, 2023. Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 41044113, 2016."
        },
        {
            "title": "Preprint",
            "content": "Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision, pp. 501518, 2016. Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 32603269, 2017. Katja Schwarz, Norman Mueller, and Peter Kontschieder. Generative gaussian splatting: Generating 3d scenes with video diffusion priors. preprint arXiv:2503.13272, 2025. Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, and Yansong Tang. Directly aligning the full diffusion trajectory with finegrained human preference. preprint arXiv:2509.06942, 2025. Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. preprint arXiv:2308.16512, 2023. Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. RealmDreamer: Text-driven In International Conference on 3D 3d scene generation with inpainting and depth diffusion. Vision, 2025. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265, 2015. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. preprint arXiv:2011.13456, 2020. Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. DimensionX: Create any 3d and 4d scenes from single image with controllable video diffusion. preprint arXiv:2411.04928, 2024. Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3D: Generating 3d scenes in seconds. preprint arXiv:2503.14445, 2025. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pp. 118, 2024a. Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. DreamGaussian: Generative In International Conference on Learning Gaussian splatting for efficient 3d content creation. Representations, 2024b. Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. MV-DUSt3R+: Single-stage scene reconstruction from sparse views in 2 seconds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 52835293, 2025. Qwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/ qwen2.5-vl/. Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. LION: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. preprint arXiv:2503.20314, 2025."
        },
        {
            "title": "Preprint",
            "content": "Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. VistaDream: Sampling multiview consistent images for single-view scene reconstruction. preprint arXiv:2410.16892, 2024a. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1261912629, 2023a. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual geometry grounded transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 52945306, 2025a. Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1051010522, 2025b. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: Geometric 3d vision made easy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024b. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025c. Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning. preprint arXiv:2507.13347, 2025d. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificIn Dreamer: high-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, pp. 84068441, 2023b. Sangmin Woo, Byeongjun Park, Hyojun Go, Jin-Young Kim, and Changick Kim. Harmonyview: In Proceedings of the IEEE/CVF Harmonizing consistency and diversity in one-image-to-3d. Conference on Computer Vision and Pattern Recognition, pp. 1057410584, 2024. Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. ReconFusion: 3d reconstruction with diffusion priors. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2155121561, 2024a. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3D: Scalable image-to-3d generation via 3d latent diffusion transformer. Advances in Neural Information Processing Systems, 37:121859121881, 2024b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. preprint arXiv:2306.09341, 2023. Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. In European Conference on Computer Vision, pp. 108124, 2024c. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. InstantMesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. preprint arXiv:2404.07191, 2024a. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. GRM: Large Gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pp. 120, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. DanceGRPO: Unleashing GRPO on visual generation. preprint arXiv:2505.07818, 2025. Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3R: Towards 3d reconstruction of 1000+ images in one forward pass. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21924 21935, 2025a. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. LayerPano3D: Layered 3d panorama for hyper-immersive scene generation. In ACM SIGGRAPH, 2025b. Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. Advances in Neural Information Processing Systems, 35:2573925753, 2022. Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, and Yiyi Liao. Prometheus: In IEEE/CVF 3d-aware latent diffusion models for feed-forward text-to-3d scene generation. Conference on Computer Vision and Pattern Recognition, pp. 28572869, 2025c. Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, et al. Matrix-3d: Omnidirectional explorable 3d world generation. preprint arXiv:2508.08086, 2025d. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-video diffusion models with an expert transformer. preprint arXiv:2408.06072, 2024b. Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. preprint arXiv:2410.24207, 2024. Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. WonderJourney: Going from anywhere to everywhere. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66586667, 2024a. Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. WonderWorld: In IEEE/CVF Conference on Computer Interactive 3d scene generation from single image. Vision and Pattern Recognition, pp. 59165926, 2025. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis. preprint arXiv:2409.02048, 2024b. Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. GaussianCube: Structuring gaussian splatting using optimal transport for 3d generative modeling. preprint arXiv:2403.19655, 2024a. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. In European Conference on GS-LRM: Large reconstruction model for 3d gaussian splatting. Computer Vision, pp. 119, 2024b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable In IEEE/CVF Conference on Computer effectiveness of deep features as perceptual metric. Vision and Pattern Recognition, 2018."
        },
        {
            "title": "Preprint",
            "content": "Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. FLARE: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2193621947, 2025. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, pp. 73969 73982, 2023. Kaizhi Zheng, Ruijian Zhang, Jing Gu, Jie Yang, and Xin Eric Wang. Constructing 3d town from single image. preprint arXiv:2505.15765, 2025. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. preprint arXiv:1805.09817, 2018."
        },
        {
            "title": "A EXTENDED RELATED WORKS",
            "content": "Pipeline-based 3D generation. line of recent works follows pipeline design, chaining together multiple modules and models. Typically, the first stage generates multi-view images from text or single input image, followed by separate reconstruction model that lifts these views into 3D representation (Tang et al., 2024a; Xu et al., 2024b; Zhang et al., 2024b; Li et al., 2023; Park et al., 2025), with large models such as LRM (Hong et al., 2023) often used for this step. However, since the generative and reconstruction stages are trained and executed independently, errors accumulate across these parts (e.g., view inconsistency, texture flicker). Moreover, such pipeline schemes are less robust to latent-space perturbations than approaches where generation and reconstruction are performed jointly in the same latent space (see Section E). second category of methods (Liu et al., 2024; Yu et al., 2024b; Gao et al., 2024; Sun et al., 2024; Wang et al., 2024a) also generates multi-view images before lifting them into 3D, but replaces large pretrained reconstruction models with per-scene optimization of NeRFs (Mildenhall et al., 2021) or 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023). While this strategy avoids reliance on pretrained decoders, it remains prone to error accumulation and requires costly per-scene optimization, making inference slow and computationally expensive. third line of works introduces progressive expansion and refinement pipelines (Yu et al., 2024a; Ni et al., 2025; Chen et al., 2025b; Fridman et al., 2023; Feng et al., 2025; Yu et al., 2025). Some adopt iterative warping and inpainting strategies (Yu et al., 2024a; Ni et al., 2025; Fridman et al., 2023; Yu et al., 2025), while others leverage video generative models to unfold 3D scenes in progressive manner (Chen et al., 2025b; Feng et al., 2025). Beyond these, additional works propose elaborate multi-stage pipelines that further increase complexity (Yang et al., 2025b; Ost et al., 2025). However, such designs are overly complex and suffer from slow inference. Alignment for text-to-2D models. Diffusion (Ham et al., 2025; Ho et al., 2020; Sohl-Dickstein et al., 2015; Song et al., 2020) and flow-matching models (Liu et al., 2023) have achieved remarkable success in 2D generation tasks across both image and video domains, and also depth estimation (Ke et al., 2025). Building upon these advances, numerous studies have explored improvements in model architectures (Lee et al., 2024; Go et al., 2023b; Park et al., 2024; 2023), loss weighting strategies (Go et al., 2023a), and timestep or noise-level sampling schemes (Kim et al., 2024). Leveraging these developments, variety of foundational 2D generative models for images (Rombach et al., 2022) and videos (Wan et al., 2025; Yang et al., 2024b) have recently emerged. Furthermore, recent studies have explored several strategies for aligning pretrained text-to-2D models with human preferences: (1) direct fine-tuning with scalar rewards (Clark et al., 2023; Xu et al., 2023; Prabhudesai et al., 2024; Wu et al., 2024c; Shen et al., 2025), (2) Reward Weighted Regression (RWR) (Peng et al., 2019; Lee et al., 2023), (3) Direct Preference Optimization (DPO) (Rafailov et al., 2023; Yang et al., 2024a), and (4) PPO-based policy gradients (Black et al., 2024; Fan et al., 2023; Liu et al., 2025). In this work, we adopt direct fine-tuning, which uses gradient-based feedback to align the generative model with the stitched decoder, ensuring that the resulting latents yield high-quality, 3D-consistent outputs. Concurrent works on interchanging parts of VAEs. Concurrently, Chen et al. (2025a) explores replacing pretrained VAE encoders with other pretrained visual encoders that extract semantic representations. However, their approach overlooks the compatibility between the VAE latent space and the representation space of the substituted vision encoder, which consequently requires extensive retraining to achieve alignment. In contrast, we explicitly measure the similarity between the VAE latent space and the representations of each layer in 3D models, and stitch the most linearly transferable layer into the latent space. As result, the stitched model achieves seamless integration without requiring extensive retraining."
        },
        {
            "title": "B METHODOLOGY DETAILS AND ITS IMPLEMENTATION",
            "content": "In this section, we provide additional details about the methodology behind VIST3A, extending the description given in Section 3. We first elaborate on the architectural and training aspects of our stitching method in Section B.1, including the stitching layers and loss functions used for MV-"
        },
        {
            "title": "Preprint",
            "content": "DUSt3R (Tang et al., 2025), VGGT (Wang et al., 2025a), and AnySplat (Jiang et al., 2025). Section B.2 then details the direct reward finetuning methodology, outlining the reward formulations and their implementation for each 3D model (VGGT, AnySplat, and MVDUSt3R). B.1 MODEL STITCHING Stitching layer. We implement the stitching layer as single Conv3D layer. Relying only on Conv3D parameters to align the spatial and temporal dimensions between the latent and the features from Fk+1:l can result in unnatural configurations, such as excessively large padding size. To address this, we first interpolate the latent representation to the target dimensions and then apply Conv3D, which provides cleaner alignment of spatial and temporal dimensions. This design still admits closed-form expression of the stitching layer, as shown in Eq. 2. Loss function for each 3D model. We train the stitched VAE using an ℓ1 loss between its outputs and those of the original 3D model. Since 3D model outputs often consist of multiple components, we compute the ℓ1 loss for each component separately and then aggregate them with weighted sum. Assigning equal weights can destabilize training and even cause divergence, since some components (e.g., confidence terms) have much larger scales than others. To mitigate this, we reweight the component losses to approximately balance their scales. The specific weighting strategy is adapted to each 3D model as follows: MVDUSt3R. The outputs consist of pointmaps, confidence scores for the pointmaps, and 3D Gaussian primitives We assign weight of 102 to the confidence term, while pointmap and Gaussian primitive losses are left unscaled. VGGT. Outputs include pointmaps, depth maps, camera poses, and confidence for both pointmaps and depth. In addition, following VGGTs practice, we add gradient-based regularization losses on pointmaps and depth maps. We weight the pose loss by 5, all confidence terms by 5 103, and gradient regularization losses by 5 103. Other losses remain unscaled. AnySplat. Outputs include depth maps, Gaussian primitives, confidence for both depth and Gaussian primitives, camera poses, and anchor features. Additionally, we introduce gradient-based regularization losses on the depth maps. We weight all confidence terms by 102, gradient regularization losses by 5 103, Gaussian scale parameters by 10, and anchor features by 0.1. Depth and other Gaussian parameters are left unscaled. Hyperparameters and implementation details. For the stitching layer S, we adopt single 3D convolution with kernel size, stride, and padding chosen to align the latent features from the video VAE with the representation space of each 3D model: MVDUSt3R: 3D convolution with kernel size 5 7 7, output channels 1024, stride 1 3 3, and padding 2 0 0. VGGT: 3D convolution with kernel size 5 3 3, output channels 1024, stride 1 2 2, and padding 2 1 1. AnySplat: 3D convolution with kernel size 5 3 3, output channels 1024, stride 1 2 2, and padding 2 1 1. Before applying the convolution, the interpolation layer recovers the temporal dimension compressed by the video VAE and adjusts the spatial size so that it matches the resolution expected by the feedforward 3D model. The input resolution of the video VAE is set to 384 384 for MVDUSt3R and 512 512 for both AnySplat and VGGT, as these configurations empirically yield stable training for the respective generative backbones. We employ LoRA with rank = 64 and scaling factor = 32 to Conv2D and linear layers across all cases. B.2 DIRECT REWARD FINETUNING Reward details. We combine CLIP-based scores and HPSv2.1 human preference scores to construct rewards for both multi-view image quality and 3D representation quality. Specifically, we use"
        },
        {
            "title": "Preprint",
            "content": "DFN (Fang et al., 2024) as the CLIP model and HPSv2.1 (Wu et al., 2023). Given an image and its associated prompt c, we denote the HPSv2.1 score as shps and the DFN CLIP score as sclip. The quality reward is then defined as Rquality(I, c) = sclip(I, c) + shps(I, c) 2, (4) which implies that maximizing the reward is equivalent to maximizing the underlying score. For the multi-view image quality reward, we compute the scores using the multi-view images decoded from the video decoder and their corresponding prompts. For the 3D representation quality reward, we compute the scores using the rendered images obtained from the 3D representation reconstructed by the stitched decoder, together with the same prompts. The 3D consistency reward is computed as combination of the pixel-level ℓ1 loss and the LPIPS between decoded multi-view image and its corresponding rendering from the reconstructed 3D representation. Formally, given decoded image Idecode and the estimated camera pose ˆπ predicted by the stitched decoder, we obtain the rendered image Irendered(ˆπ) from the 3D representation. The consistency reward is then defined as Rconsistency(Idecode, Irendered(ˆπ)) = IdecodeIrendered(ˆπ)10.25LPIPS (Idecode, Irendered(ˆπ)) . (5) Here, the negative sign ensures that maximizing the reward corresponds to minimizing both the ℓ1 distance and the perceptual discrepancy between the decoded and rendered images. However, applying these rewards to all decoded multi-view images and their rendered counterparts is computationally expensive. To reduce computational cost, we compute all rewards only on two sampled decoded views and their corresponding rendered images. The final reward is then obtained by weighted combination of the three components: the multi-view image quality reward and the 3D representation quality reward are each scaled by 1/16, while the 3D consistency reward is scaled by 0.05. These scaled terms are summed to form the overall training reward. Algorithm 1 One Training Iteration of Alignment Training 1: Input: generative model θ, reward r, sampling step range [T1, T2], # of gradient enabled steps K, prompt c, data D. Alignment Algorithm. For alignment, we adopt DRTune (Wu et al., 2024c)-style direct reward finetuning, which enables stable reward optimization through selective gradient computation. We outline one training iteration of our finetuning in Algorithm 1. First, we calculate the generative loss using multithen simulate the deview datasets, noising process. Since matching the full number of inference-time denoising steps during training is costly, we instead sample steps from reduced range [T1, T2] to lower the computational burden. Additionally, to reduce time and memory costs, we only enable gradient computation at selected training steps ttrain out of the total steps. Following DRTune, the input zτ to the generative model is detached at each step to stabilize optimization. Finally, we calculate the reward from the sampled latent and combine it with the generative loss by subtraction (for maximization) before backpropagation and parameter updates. 2: Lgen calculate generative loss with 3: Uniform(T1, T2) Sample number of denoising steps 4: zT (0, I) Initialize starting noise 5: Define t-step schedule {τj}t j=0 with τ0 = T, τt = 0 6: ttrain randomly select indices from {1, . . . , t} 7: for = 1 to do 8: 9: 10: 11: 12: zτj+1 update(zτj1 , prediction) 13: 14: r(z0, c) Calculate reward of generated latent. 15: Ltotal Lgen r(z0, c) 16: Backpropagate θLtotal, then optimize θ ˆzτj stop grad(zτj ) if ttrain then no grad: prediction model(θ, ˆzτj , τj) prediction model(θ, ˆzτj , τj) Denoising from to 0 else Hyperparameter in sampling process. For generating samples required in the [T1, T2] direct reward tuning stage, we set T1 = 10 and T2 = 50 in Algorithm 1, ensuring that the number of diffusion steps is smaller than the typical steps in inference. The number of gradient-enabled steps is set to = 2 to reduce memory consumption during training. For scheduling, we adopt the default scheduler from Wan 2.1 (Wan et al., 2025)."
        },
        {
            "title": "C DETAILS ON EXPERIMENTAL SETUPS",
            "content": "C.1 TRAINING SETUP Setup for stitching layer search. To identify the stitching layer, we rely on representations from the feedforward 3D model and the corresponding latents computed on the same dataset. Specifically, we utilize subset of the DL3DV dataset, comprising 200 scenes for VGGT, 800 scenes for AnySplat, and 3,200 scenes for MVDUSt3R, with only 13 views per scene used for the search. We limit our search to the encoder layers of each model, as we observe that MSE values consistently increase within deeper layer indices. Setup for stitched VAE finetuning. We train on combination of the DL3DV and ScanNet datasets, defining one epoch as full pass over DL3DV and two passes over ScanNet. For each training iteration, number of scenes are sampled according to the batch size. From each selected scene, we randomly sample 9 or 13 views to serve as input samples for training. The models are trained for 50 epochs in total. The batch sizes are set to 12 for VGGT, 24 for MVDUSt3R, and 12 for AnySplat. The learning rate is fixed at 2 104 for all models with cosine decay scheduling and 500-step warmup. For training, we use AdamW (Loshchilov & Hutter, 2017), apply gradient clipping with norm threshold of 1.0, and use gradient checkpointing on each stitched VAE block to reduce memory consumption. In addition to LoRA parameters, for AnySplat and VGGT, we also finetune register tokens and class tokens. This is necessary because we remove the earlier layers that originally process these tokens into intermediate representations, requiring adaptation of the token handling mechanism. We further utilize gradient checkpointing for every stitched VAE block. Setup for generative model finetuning. We finetune the generative models using only the DL3DV dataset. For generative loss computation, we use batch size of 12 with 13 views per scene. Reward calculation uses prompt batch size of 4, with 13 views for AnySplat and MVDUSt3R, and 9 views for VGGT. We again adopt AdamW with learning rate of 1 104, apply gradient clipping at 0.1 norm, and train LoRA parameters with rank 8 and alpha 16. Gradient checkpointing is enabled for all model blocks to reduce memory usage. C.2 DETAILED EVALUATION PROTOCOL Details for 3D generation evaluation. For T3Bench, we evaluate on all 300 prompts, in contrast to prior works that considered only the 100 single-object-with-surroundings subset. SceneBench is evaluated on 80 prompts from the Prometheus3D (Yang et al., 2025c) prompt set, targeting scenelevel generation. For DPG-Bench, we sample 100 prompts from the original 1K-prompt dataset. For Matrix3D-omni, we used their official code for text-to-generation and employed Panorama LRM for reconstruction during inference. For SDS-based methods like SplatFlow and Director3D that perform refinement, we evaluated the final results after SDS optimization. We generate 13 frames for all models using 80 denoising steps, and apply classifier-free guidance (Ho & Salimans, 2022) with scale of 7.5. We observed that the Gaussian splatting produced by the MVDUSt3R model does not generalize well across diverse domains, often failing to estimate the scale of primitives. To address this issue, we refined the Gaussian primitives using the source view for 100 optimization steps, minimizing reconstruction loss defined as MSE + 0.05 LPIPS. For this refinement, we used the Adam optimizer with separate learning rates for each parameter group: 2e-4 for means, 5e-4 for opacity, 5e-4 for scale, 1e-4 for rotation, and 0 for rgbs. This lightweight refinement effectively corrected the scale estimation errors. For our text-to-3DGS evaluation, we render 8 random viewpoints from the generated Gaussian Splatting representations for assessment. We evaluate our method and baselines across range of metrics. To measure the semantic similarity between the input prompt and the rendered images of the generated 3DGS, we compute the CLIP score using the clip-vit-base-patch16 model. Additionally, we adopt the VBench (Huang et al., 2024) framework to assess key image properties. For Imaging Quality, which targets low-level distortions, we employ the same MUSIQ model (Ke et al., 2021) in VBench. For Aesthetic Quality, we use the LAION aesthetic predictor to evaluate the color richness and artistic merits, again following VBench. The predictors native 0-10 rating is linearly normalized to 0-1 scale for our analysis."
        },
        {
            "title": "Preprint",
            "content": "For more comprehensive assessment of generative quality, we utilize the Unified Reward model (Wang et al., 2025c), which is based on the powerful Qwen 2.5-7B Vision Language Model (Team, 2025)2. This provides fine-grained, pointwise scores on complex attributes equipped with powerful understanding capability. By feeding the input prompt and rendered images into format adapted from the official implementation script3, we obtain scores for three key aspects: Alignment: How well the image content matches the text prompt. Coherence: The logical and visual consistency of the image, free of distortions. Style: The aesthetic appeal of the image, independent of prompt accuracy. This suite of metrics enables robust and multifaceted evaluation of our models performance. Details for model stitching evaluation. For novel-view synthesis, we follow prior works (Go et al., 2025a;b) and adopt an 8-frame input setup to evaluate performance on 4 target views. To accommodate the fixed-length input requirements of video VAE architectures due to temporal compression, we pad shorter sequences by duplicating the final frame. For estimating the camera poses of the target views, we adopt the strategy from AnySplat (Jiang et al., 2025), which jointly predicts the poses and renders the corresponding images. This contrasts with previous VAE-based methods that presume access to ground-truth camera poses for rendering. For pointmap and camera pose estimation evaluation, we use 13-frame input setup. Since our stitched VAEs encoder is video VAE, we arrange the multi-view images (typically provided unordered by previous works) into sequences with smooth view transitions to resemble video input. We adopt Pi3 (Wang et al., 2025d) official evaluation code and follow their preprocessing pipeline."
        },
        {
            "title": "D FURTHER ABLATION STUDIES",
            "content": "D.1 IMPACT OF DIRECT REWARD FINETUNING In the following, we conduct an ablation study to analyze the effects of our direct reward finetuning, comparing our full method against four well-defined baselines: (1) Finetuning-free: Here, we use the original pretrained video model. Since our finetuning freezes the encoder, its latent space remains compatible with our 3D stitched decoder. (2) Multi-view Only: The model finetuned with only the flow-matching loss on multi-view data, serving as our primary baseline before rewards are introduced. (3) Multi-view + Consistency: The model finetuned with both the multi-view loss and the 3D-consistency reward. This isolates the impact on the 3D consistency reward. (4) Multi-view + Quality: The model finetuned with both the multi-view loss and the quality reward. This isolates its impact on quality reward. To ensure fair comparison against reward-based methods, which often take more time for one training iteration, the finetuning variant on multi-view data was trained for the same wall-clock duration. Table 5 reports the quantitative results. The finetuning-free baseline yields the lowest performance. Lacking any 3D-aware training, it frequently produces geometrically inconsistent outputs and suffers from significant visual artifacts when its native resolution is adapted to our 3D decoder. Introducing multi-view supervision (Multi-view Only) substantially improves 3D consistency and overall performance, confirming the value of this training signal. The reward components have distinct effects when added to the multi-view objective. Training with the 3D-consistency reward (Multi-view + Consistency) leads to notable performance drop, as the model optimizes for geometric correctness at the expense of detail, resulting in overly blurred 2https://huggingface.co/CodeGoat24/UnifiedReward-qwen-7b 3https://github.com/CodeGoat24/UnifiedReward/blob/main/inference_qwen/ image_generation/qwen_point_score_ACS_image_generation.py"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablation study on direct reward finetuning on SceneBench. We compare (1) no finetuning; (2) multi-view-only finetuning (generative loss only); (3) reward tuning with 3D-consistency reward only; (4) reward tuning with quality reward only; and (5) reward tuning with both rewards (full)."
        },
        {
            "title": "Alignment Coherent Style",
            "content": "Finetuning-free Multi-view only Multi-view + 3D Consistency Multi-view + Quality"
        },
        {
            "title": "Ours",
            "content": "50.56 54.56 38.67 62.27 64.87 53.70 52.08 50.59 58.23 28.14 29.71 29.77 30.34 56.96 30. 3.101 3.622 3.581 3.643 3.667 3.354 3.834 3.767 3.842 3.393 3.351 3.275 3.358 3.862 3. (a) Acc. (b) Comp. (c) NC. (d) Reconstructed images through VAE according to α Figure 6: Pointmap estimation performance comparison on ETH3D dataset between the stitched VGGT and the sequential approach (VAE followed by VGGT) under varying noise scales injected into the latent space. The stitched model demonstrates greater robustness to noise injection in the VAE. images. Conversely, adding the quality reward (Multi-view + Quality) achieves substantial improvements across most metrics by enhancing prompt coherence and aesthetic appeal. Finally, our full method, which combines both rewards with multi-view training, achieves the best imaging quality and Unified Reward scores. While its aesthetic and CLIP scores are slightly below the Multi-view + Quality variant, the marked improvement in imaging quality demonstrates that our combined objective successfully guides the model to generate visually sharp and geometrically faithful 3D representations. D.2 BENEFITS OF INTEGRATED VS. SEQUENTIAL 3D GENERATION In our model-stitching design, generation and reconstruction take place in the shared latent space of the video diffusion VAE and the stitched 3D decoder. common alternative is sequential pipeline that decodes latents into RGB frames before applying feedforward 3D model (e.g., VGGT) without further adaptation. To probe the core benefit of our unified formulation, we injected controlled perturbations into the latent representation, using = + α ϵ, ϵ (0, I), (6) where α is scalar controlling the perturbation strength. We then compared two paths: (i) decode the corrupted latent to RGB and feed the images sequentially into the original VGGT (baseline), and (ii) directly input the noised latent into our stitched 3D decoder (unified latent framework)."
        },
        {
            "title": "Preprint",
            "content": "(a) Wan (b) Hunyuan (c) SVD (d) CogvideoX Figure 7: Log-MSE values in Eq. 2 across various video VAEs. Early layers of feedforward models show lower MSE values within each VAE architecture. While lower MSE correlates with better stitching performance within the same VAE (e.g., layer 2 outperforms layer 16 for Wan in Fig 5), absolute MSE values cannot predict performance across different VAE architectures. For instance, despite CogVideoX and Hunyuan + AnySplat having the lowest absolute MSE (0.008), SVD + AnySplat achieves the best performance (21.48 PSNR) in Table 3. Figure 6 reports pointmap estimation performance on ETH3D as function of noise level α. Our stitched VGGT consistently outperforms the sequential decode-and-reconstruct pipeline under noise injection, indicating that the VAE decoder in the sequential path amplifies errors. Moreover, as shown in Fig. 6d, the performance gap is observed even at noise levels (α = 1e4 to 2e4) where visual artifacts are hardly perceptible. This suggests that the unified design offers stronger robustness, as imperceptible perturbations from the noise of generative processes can already degrade the sequential pipeline."
        },
        {
            "title": "E ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "To comprehensively validate each component of VIST3A, we present additional experiments in this section. Analysis on searched stitching index. In Section 4.4, we showed that earlier layers in the network tend to be more linearly correspondent. We extend this analysis to various VAE architectures, including CogVideoX, SVD, Hunyuan, and Wan, paired with MVDUSt3R and AnySplat, to observe the generalizability of this finding. Figure 7 shows the log-MSE values measuring linear transferability between latents and the feedforward 3D models representations. From the results, early layers of 3d models consistently show lower MSE values across all VAE-feedforward 3D model combinations. This supports the hypoth-"
        },
        {
            "title": "Preprint",
            "content": "esis that latent representations capture low-level features for input reconstruction, which are more linearly transferable to the early layers of the feedforward 3D model that also encode such features. However, the results reveal an important distinction: while relative MSE ordering within each VAE architecture correlates with stitching performance (as in Section 4.4), absolute MSE values across different VAEs do not predict cross-architecture performance. For instance, CogVideoX + AnySplat achieves the lowest absolute MSE (0.008) but delivers 21.32 PSNR in Table 3, while SVD + AnySplat with higher MSE (0.012) achieves superior performance at 21.48 PSNR. This indicates that optimal stitching layers must be identified independently for each VAE-3D model pair. Additional qualitative results. We present additional qualitative results of VIST3A with Wan + AnySplat in Fig. 810. Text-to-pointmap generation results obtained by combining VGGT with Wan through VIST3A are shown in Fig. 11. Finally, Fig. 12 illustrates VIST3A results with MVDust3R + Wan."
        },
        {
            "title": "F LIMITATIONS",
            "content": "While our approach demonstrates strong results, it also has certain limitations. Our stitched model inherits its encoder from video generation model, which is inherently designed for sequential, temporally coherent video input. Consequently, its performance is not guaranteed for arbitrarily unordered inputs, such as typical multi-view image datasets. To ensure the encoder operates effectively, the input images must be arranged into coherent sequence that simulates the smooth view transitions of video clip."
        },
        {
            "title": "G USE OF LARGE LANGUAGE MODELS",
            "content": "LLMs were used exclusively for text polishing and grammar refinement."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Qualitative comparison of 3DGS generation. The top two rows show samples from DPG-Bench, and the bottom two rows present samples from T3Bench. VIST3A generates realistic scenes with fine-grained details that faithfully reflect the input prompt, outperforming baselines."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Qualitative comparison of 3DGS generation on SceneBench. VIST3A outperforms baselines by generating higher-fidelity scenes with accurate geometry and appearance. Figure 10: Generated 3D scenes from VIST3A: Wan + AnySplat. These are 3DGS viewed directly in the interactive viewer. VIST3A preserves high visual quality even under noticeably altered camera trajectories, demonstrating robustness and stability across novel viewpoints."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Qualitative results on text-to-poinmap generation. By integrating VGGT, VIST3A generates structurally consistent pointmaps and fine-grained details across diverse prompts."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Qualitative comparison of 3DGS generation on SceneBench - VISTA: Wan+MVDUSt3R. Figure 13: Generated 3D scenes from VIST3A: Wan + AnySplat bt extending the number of frames. These are 3DGS viewed directly in the interactive viewer. VIST3A preserves high visual quality even under noticeably altered camera trajectories, demonstrating robustness and stability across novel viewpoints."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Google"
    ]
}