{
    "paper_title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models",
    "authors": [
        "Shuchen Xue",
        "Chongjian Ge",
        "Shilong Zhang",
        "Yichen Li",
        "Zhi-Ming Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing Large Language Models (LLMs), where pre-training and RL post-training share the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectives--score/flow matching loss. In this work, we establish a novel theoretical analysis: DDPO is an implicit form of score/flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce \\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for diffusion. It uses the same score/flow-matching loss as pretraining to obtain a lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, reduces variance, and yields faster convergence. This simple yet effective design yields substantial benefits: on GenEval, OCR, and PickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality. Code is available at https://github.com/scxue/advantage_weighted_matching."
        },
        {
            "title": "Start",
            "content": "ADVANTAGE WEIGHTED MATCHING: ALIGNING RL WITH PRETRAINING IN DIFFUSION MODELS Shuchen Xue1,2 Chongjian Ge2 Shilong Zhang2,3 Yichen Li 2,4 Zhi-Ming Ma 1 1UCAS 2Adobe Research 3HKU 4MIT"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning (RL) has emerged as central paradigm for advancing Large Language Models (LLMs), where pre-training and RL post-training share the same log-likelihood formulation. In contrast, recent RL approaches for diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO), optimize an objective different from the pretraining objectivesscore/flow matching loss. In this work, we establish novel theoretical analysis: DDPO is an implicit form of score/flow matching with noisy targets, which increases variance and slows convergence. Building on this analysis, we introduce Advantage Weighted Matching (AWM), policy-gradient method for diffusion. It uses the same score/flow-matching loss as pretraining to obtain lower-variance objective and reweights each sample by its advantage. In effect, AWM raises the influence of high-reward samples and suppresses low-reward ones while keeping the modeling objective identical to pretraining. This unifies pretraining and RL conceptually and practically, is consistent with policy-gradient theory, reduces variance, and yields faster convergence. This simple yet effective design yields substantial benefits: on GenEval, OCR, and PickScore benchmarks, AWM delivers up to 24 speedup over Flow-GRPO (which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX, without compromising generation quality. Code is available at https://github.com/scxue/advantage_weighted_matching."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion Models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) and their variants, (Gaussian) Flow Matching (Peluchetti, 2023; Lipman et al., 2022; Albergo et al., 2023) are de facto 5 2 0 2 9 2 ] . [ 1 0 5 0 5 2 . 9 0 5 2 : r Figure 1: AWM reduces variance and speeds up RL for diffusion. (a) Formulations: DDPO optimizes per-step Gaussian likelihood on xt1, whereas AWM applies reward-weighted score/flow matching on x0. (b) Target variance: DSM with noisy conditioning xs (DDPO implicitly doing) has higher variance than DSM with clean x0 (AWM). (c) Convergence: On GenEval, AWM reaches the same quality with up to 8 fewer GPU hours than Flow-GRPO. Work done during internship at Adobe Research. Correspondence to cge@adobe.com"
        },
        {
            "title": "Preprint",
            "content": "generation paradigms for continuous domains like image and video synthesis (Nichol et al., 2021; Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Chen et al., 2023b; Brooks et al., 2024). Motivated by the success of Reinforcement Learning (RL) for Large Language Models (LLMs) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Guo et al., 2025), there has been growing interest in extending RL methodologies to diffusion models. key difference between RL algorithms used in LLMs and diffusion lies in the alignment of training objectives. In the autoregressive setting (e.g. LLMs), both pre-training and RL post-training ultimately optimize the same log-likelihood objective with different weights. In contrast, diffusion pretraining relies on score/flow matching losses that are closely related to maximum likelihood through the Evidence Lower Bound (ELBO) (Song et al., 2021) via forward process, while recent RL post-training methods for diffusion have largely adopted Denoising Diffusion Policy Optimization (DDPO) (Black et al., 2023), including Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025). As shown in Figure 1.a (left), these approaches formulate denoising as multi-step MDP, where each reverse-time step is an action, and the one-step reverse transition pθ(xt1xt, c) acts as the policy. Under standard DDPM samplers, this transition is isotropic Gaussian, enabling tractable per-step log-likelihoods and supporting policy-gradient methods, e.g., REINFORCE (Williams, 1992). Though effective, DDPO-based RL post-training leads to an optimization objective that diverges from the score or flow matching objectives used during pretraining. The discrepancy raises fundamental questions: Why do diffusion RL post-training and pretraining employ different likelihood formulations? Consequently, what does the DDPO loss objective actually optimize? To answer this question, we establish theoretical connection between DDPO and score matching in Section 3. As shown in Figure 1.b (left), DDPO is implicit doing denoising score matching (DSM) with noisy data. We prove that using noisy data increases the variance of the DSM target compared to the clean-data DSM objectives used in pretraining (shown in Figure 1.b (right)). These theoretical insights are validated in Section 3 through class-conditioned pretraining experiments on CIFAR-10 and ImageNet-64, where we show that noisy-DSM objectives align with the DDPO target but converge more slowly, consistent with our variance analysis. We propose Advantage Weighted Matching (AWM), simple yet effective objective that directly incorporates reward signals into score/flow matching without relying on reverse-time discretization. In contrast to DDPO, which depends on per-step likelihoods derived from reverse-time discretization, AWM uses the same score/flow-matching loss as diffusion pretraining while reweighting each sample by its advantage, as shown in Figure 1.a (right). Intuitively, AWM amplifies high-reward samples and downweights low-reward ones, while preserving the original pretraining objective. This design offers several benefits. First, it avoids the variance amplification caused by conditioning on noisy data, leading to faster convergence. Second, it decouples training from sampling: AWM supports any sampler or noise level, unlike DDPO methods tied to EulerMaruyama discretization. Finally, by grounding RL post-training in the same score/flow matching framework as pretraining, AWM restores conceptual symmetry between diffusion models and LLMs, where both stages optimize the same objective with reward-dependent weighting. AWM translates this variance reduction into substantial wall-clock savings without compromising quality. On Stable Diffusion 3.5 Medium (SD3.5M) (Esser et al., 2024), it matches FlowGRPOs (Liu et al., 2025) GenEval score (0.95 vs. 0.95) with an 8.02 speed-up (Figure 1.c). It achieves comparable OCR accuracy with 23.6 less compute and the PickScore with 10.5 less. On FLUX (Labs, 2024), AWM shows 8.5 (OCR) and 6.8 (PickScore) speedups (Figure 5 ad), reaching 0.986 OCR accuracy. Our contributions are threefold: DDPODSM Equivalence. We prove that, up to discretization error, maximizing DDPOs perstep Gaussian likelihood is equivalent to minimizing denoising score matching (DSM) with noisy data at the same time step, regardless of parameterization (score or velocity). Additional Variance for DDPO from Noisy Conditioning. We show that conditioning on noisy data yields higher-variance estimator of score function than conditioning on clean data; we quantify the increase and validate it via controlled pretraining on CIFAR-10 and ImageNet-64, where noisy-DSM converges more slowly under identical settings. Advantage Weighted Matching (AWM). We introduce Advantage Weighted Matching, policy gradient method for diffusion with forward process that decouples training from sampling, and realigns RL post-training with the pretraining objective. On GenEval, OCR, and PickScore, AWM"
        },
        {
            "title": "Preprint",
            "content": "achieves up to 24 faster training than Flow-GRPO (Liu et al., 2025) on SD-3.5-M and FLUX without generation quality degradation."
        },
        {
            "title": "2.1.1 REWARD FEEDBACK LEARNING",
            "content": "Imagereward (Xu et al., 2023) proposes Reward Feedback Learning (ReFL), which directly maximizes the reward of an approximately one-step predicted image via gradient backpropagation. To mitigate the approximation error of the one-step prediction, DRaFT (Clark et al., 2023) maximizes the reward of the final multi-step sampled images. While this incurs massive memory cost, DRaFT reduces it through backpropagation truncation and gradient checkpointing. Viewing the problem from continuous-time perspective, Adjoint Matching (Domingo-Enrich et al., 2024) applies the adjoint method (Pontryagin, 2018) for memory-efficient gradient computation. There are also trials of applying Reward Feedback Learning on few-step distilled models (Kim et al., 2024; Li et al., 2024; Luo et al., 2024; 2025). However, the Reward Feedback Learning method has some inevitable drawbacks. First, the reward function must be differentiable, which prevents the use of rule-based or binary rewards, such as those from detection models like GenEval (Ghosh et al., 2023) or OCR. Second, modern generative models operate in highly compressed VAE latent space. Backpropagating gradients from pixellevel rewards through the VAE decoder adds non-negligible memory and computation costs. Third, although first-order optimization is generally more efficient than zero-order optimization (Nocedal & Wright, 2006), it carries higher risk of reward hacking, where the model exploits the reward function in unintended ways. 2.1.2 DENOISING DIFFUSION POLICY OPTIMIZATION DDPO (Black et al., 2023) and DPOK (Fan et al., 2023) frame the reinforcement learning problem as multi-step decision-making problem. They consider an Euler-Maruyama discretization of the reverse process Equation (1), leading to tractable Gaussian likelihood at each step. DeepSeekMath (Shao et al., 2024) proposed Group Relative Policy Optimization (GRPO) by substituting the baseline from an additional Value model in PPO (Schulman et al., 2017) to group relative mean of reward. Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) combine the DDPO formulation and use GRPO as its RL algorithm. TempFlow (He et al., 2025) introduces trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points. MixGRPO (Li et al., 2025a) leverages the flexibility of mixed sampling strategies through the integration of ODE and SDE sampling. BranchGRPO (Li et al., 2025b) introduces branching scheme that amortizes rollout cost through shared prefixes while preserving exploration diversity. 2.1.3 REWARD WEIGHTED REGRESSION Lee et al. (2023) proposes to finetune text-to-image model by maximizing an offline rewardweighted denoising loss. Fan et al. (2025) considers an online version of reward-weighted denoising loss with Wasserstein-2 regularization. Concurrent to ours, FMPG (McAllister et al., 2025) proposes using the ELBO as proxy for policy likelihood. Unlike our approach, they do not articulate the connection to DDPO, and they do not present experiments on text-to-image diffusion."
        },
        {
            "title": "3 DDPO IS SECRETLY DOING SCORE MATCHING",
            "content": "3.1 BACKGROUND ON DIFFUSION AND FLOW MATCHING Diffusion Models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) diffuse clean data sample x0 pdata from data distribution to noisy data xt = αtx0 + σtϵ, where [0, ], ϵ (0, I) is standard Gaussian noise. xT exactly or approximately follows Gaussian distribu-"
        },
        {
            "title": "Preprint",
            "content": "tion. Typically, diffusion models train noise prediction network ϵθ using Ex0,ϵ,t[ϵθ(xt, t) ϵ2], which is equivalent to denoising score matching loss (Vincent, 2011; Song et al., 2020). Flow Matching (Peluchetti, 2023; Liu et al., 2022; Lipman et al., 2022; Albergo et al., 2023) with Gaussian priors considers linear interpolation noising process by defining xt = (1 t)x0 + tϵ. The flow matching models train velocity prediction network vθ using Ex0,ϵ,t[vθ(xt, t) (ϵ x0)2]. To simplify the notation, in the remaining part of the paper, we choose the flow matching noise schedule by default. The sampling process of diffusion or flow matching models involves solving the probability flow ODE (PF-ODE) (Song et al., 2020): dxt = vθ(xt, t)dt, or diffusion SDE (Song et al., 2020), where wt represents the Wiener process1: (cid:20) vθ(xt, t) + (cid:21) (xt + (1 t)vθ(xt, t)) dxt = dwt, dt + (1) (cid:114) 2t 1 1 1 The likelihood log p(x) of data that diffusion models assign can be exactly computed through instantaneous change of variables as in Neural ODE (Chen et al., 2018) or approximated by an Evidence Lower Bound (ELBO) (Song et al., 2021; Kingma et al., 2021; Kingma & Gao, 2023): log p(x) ELBO(x) := EtU (0,1),ϵN (0,I) (cid:20) 1 vθ(xt, t) (ϵ x0)2 (cid:21) . (2) The gap between the exact likelihood and ELBO is relatively small (Song et al., 2021). 3.2 DENOISING DIFFUSION POLICY OPTIMIZATION (DDPO) Reinforcement learning (RL) algorithms in the context of Large Language Models (LLMs) vary in their data utilization strategies (online, offline, or mixed) and in the algorithms used, e.g. preferencebased methods like DPO (Clark et al., 2023), and policy gradient variants (importance sampling, clipping, with or without value-function baseline) such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). Despite these differences, the likelihood formulation of the policy model πθ remains consistent, since autoregressive model allows exact computation of sequence likelihoods. Pretraining and RL post-training in LLMs are conceptually aligned. Pretraining maximizes the teacher-forced sequence log-likelihood, while RL applies reward-weighted adjustments to the same likelihoodamplifying high-reward samples and downweighting low-reward ones (e.g., via advantages or preference scores). In both cases, the objective centers on the models sequence loglikelihood; only the weighting differs. In contrast, RL for diffusion models, e.g., Flow-GRPO (Liu et al., 2025) and Dance-GRPO (Xue et al., 2025), adopts the Denoising Diffusion Policy Optimization (DDPO) framework (Black et al., 2023). DDPO casts denoising as multi-step MDP: the state at step is xt, the policy is the one-step reverse transition pθ(xt1xt). Under standard DDPM samplers (or an EulerMaruyama discretization of the reverse-time diffusion SDE), this transition is isotropic Gaussian, which makes the perstep conditional log-likelihood tractable and fit for policy-gradient style optimization. Concretely, DDPO implements the EulerMaruyama update of the reverse SDE from Equation (1)2: (cid:20) xtt = xt vθ(xt, t) + (cid:21) (xt + (1 t)vθ(xt, t)) + 1 1 (cid:114) 2t 1 tϵ, (3) This implies Gaussian policy pθ(xttxt) with tractable mean and covariance, yielding the perstep log-likelihood log pθ(xttxt): (cid:13) (cid:13) (cid:13) (cid:13) xtt (cid:18) (cid:20) vθ(xt, t) + xt 1 1 (cid:21) (xt + (1 t)vθ(xt, t)) (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) /( 4t 1 t) + const. (4) 1Or an extension with arbitrary noise level (Karras et al., 2022; Xue et al., 2023) 2For simplicity, our analysis uses the vanilla reverse-time diffusion SDE. EulerMaruyama can increase estimator variance, and using an exponential integrator yields the analytical variance, as discussed in (Xue et al., 2023). The original DDPO (Black et al., 2023) uses DDPM, which can be viewed as EulerMaruyama with an exponential integrator (Xue et al., 2023). Flow-GRPO (Liu et al., 2025) and Dance-GRPO (Xue et al., 2025) use the plain EulerMaruyama discretization. The discrepancy vanishes as the discretization step 0."
        },
        {
            "title": "Preprint",
            "content": "By contrast, standard pretraining for diffusion optimizes score/flow-matching objective: vθ(xt, t) (ϵ x0)2 . While score/flow matching is itself likelihood surrogate for diffusion pretraining, DDPO optimizes per-step reverse-transition likelihood: one works on the forward process with clean conditioning, the other on discretized reverse process with noisy intermediates. This raises the pivot question: Why do reinforcement learning and pretraining for diffusion models employ different likelihood formulations? Consequently, what is the DDPO loss objective actually optimizing? (5)"
        },
        {
            "title": "3.3 DENOISING SCORE MATCHING WITH NOISY DATA",
            "content": "This subsection proceeds as in Fig. 2: (i) Theorem 1a DDPO step equals DSM on xt conditioned on noisy xtt; (ii) Lemma 1noisy-DSM and clean-DSM share the same population minimizer; (iii) Theorem 2noisy conditioning strictly increases target variance, explaining slower optimization. We validate (iii) with EDM experiments (Figure 3). Leveraging the fact the joint distribution of (xtt, xt) is the same for forward and reverse process (Haussmann & Pardoux, 1986) and omitting EulerMaruyama discretization error, our main result shows that DDPO step at time targets the same population objective as denoising score matching (DSM) conditioned on noisy xtt. Theorem 1 (DDPO is Secretly Doing Denoising Score Matching with Noisy Data). Optimizing the DDPO objective Equation (4), which maximizes the log-likelihood derived from the discretization of the reverse-time diffusion SDE, is equivalent to minimizing the Denoising Score Matching loss with noisy data from the forward process, omitting the discretization error induced by Euler-Maruyama discretization: Extt,xt sθ(xt, t) log p(xtxtt)2(cid:105) (cid:104) . (6) The proof can be found in Section B. Theorem 1 establishes that maximizing DDPOs per-step log-likelihood aligns with minimizing DSM with noisy data. This equivalence holds for both score and velocity parameterizations. In the following lemma and theorem, we will show that optimizing Denoising Score Matching with noisy data is equivalent to optimizing Denoising Score Matching with clean data in expectation. However, conditioning on noisy xtt introduces additional variance in the DSM target. This justifies analyzing DDPO through the lens of score matching while isolating variance effects that impact optimization. Lemma 1 (Denoising Score Matching with Noisy Data). For any time steps and such that 0 < t, optimizing the Denoising Score Matching objective using the noisy data xs Figure 2: Overview of our analysis. Exs Extxs sθ(xt, t) log p(xtxs)2(cid:105) (cid:104) , is equivalent to optimizing the standard Score Matching objective: sθ(xt, t) log p(xt)2(cid:105) (cid:104) Ext . This lemma generalizes the well-known result that Denoising Score Matching with clean data is equivalent to Score Matching, which corresponds to the special case where = 0 The proof is in Section B. DDPO update aligns with DSM objective at the same (up to discretization error), so both share the same population minimizer. What remains is to quantify the optimization noise. The next theorem shows that noisy conditioning inflates DSM target variance. Theorem 2 (Variance of Denoising Score Matching Target with Noisy Data is Larger). For the two conditional scores xt log p(xtxs), xt log p(xtx0),"
        },
        {
            "title": "Preprint",
            "content": "(a) FID on CIFAR-10 (b) FID on ImageNet-64 Figure 3: Noisy conditioning increases variance and hinders pretraining. We compare the EDM baseline (x0, clean-data objective; Equation (7)) with noisy-conditioned proxy (xs; Equation (8)) aligned with DDPO. Under the same constraints, the noisy objective yields consistently worse FID on CIFAR-10 and ImageNet-64consistent with Theorem 2. and for any xt and any [0, t), each is unbiased estimator of xt log p(xt): [xt log p(xtxs)xt] = [xt log p(xtx0)xt] = xt log p(xt), and their conditional covariances satisfy Cov (xt log p(xtxs)xt) = Cov (xt log p(xtx0)xt) + κ(s, t) Cov (xt log p(xtx0)xt) , where κ(s, t) = (cid:16) t2 (1 t)2 t2(1 s)2 s2(1 t)2 (cid:17) . In particular, Tr Cov(xt log p(xtxs)xt) = Tr Cov(xt log p(xtx0)xt) + κ(s, t), where is the data dimension. The map (cid:55) κ(s, t) is strictly increasing on [0, t), with κ(0, t) = 0. The proof appears in Section B. The theorem establishes that, although noisy-conditioned DSM is unbiased and targets the same minimizer as clean-conditioned DSM, its target has strictly larger conditional covariance (by κ(s, t)), which increases the objective variance and slows stochastic optimization (Wang et al., 2013). We corroborate this prediction with pretraining experiments on CIFAR-10 and ImageNet-64 using the EDM (Karras et al., 2022) codebase. Our baseline uses the standard clean-data objective Equation (7): Ex0,xt (cid:104) w(t) Dθ(xt, t) x02(cid:105) For the EDM noise schedule3, the noisy variant implements Equation (8), which mirrors DDPO: (cid:34) Exs,xt w(t) (cid:13) (cid:13) (cid:13) (cid:13) Dθ(xt, t) (cid:18) xt t2 t2 s2 (xt xs) 2(cid:35) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (7) (8) Under identical architectures, noise schedules, and optimization settings, the noisy-DSM objective learns in the correct direction but reaches given quality level substantially later (see Figure 3), consistent with the variance analysis."
        },
        {
            "title": "4 ADVANTAGE WEIGHTED MATCHING",
            "content": "Motivated by our finding that DDPO implicitly optimizes DSM with noisy data, which increases variance, we introduce Advantage Weighted Matching (AWM), replacing the DDPO objective with DSM evaluated on clean data. Conceptually, this converts step-wise reverse-transition policy into sequence-level conditional policy over x0. The resulting difference in problem setup is: 3Specifically, EDMs noise scheduler assumes that xt = x0 + tϵ, [0.002, 80] during inference and [0, +] during training."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: AWM pipeline. For each prompt c, we sample group of sequences {x0} from diffusion model, evaluate rewards, advantages, and train the sequence policy via advantage-weighted score matching. During inference, any ODE/SDE sampler can be used. Problem Setup of Denoising Diffusion Policy Optimization State: st = (c, t, xt) Action: at = xt1 Policy: π(atst) = pθ(xt1xt, c) Problem Setup of Advantage Weighted Matching State: = Action: = x0 Policy: π(as) = pθ(x0c) As show in Figure 4, for prompt and batch {xi}G rewards ri = r(xi, c) and advantages Ai (e.g., group-relative mean). The GRPO objective: i=1 sampled from distribution πθold, we compute JGRP O(θ) = c: prompt,{xi}G i=1πθold (xc) 1 (cid:88) i=1 (cid:18) πθ(xic) πθold (xic) Ai βDKL(πθπθref ) (cid:19) . (9) For simplicity, we omit the clip operation here. The policy likelihood can be substituted by score/flow matching loss, which is tight ELBO of likelihood pθ(x0c) (Song et al., 2021; Kingma et al., 2021; Kingma & Gao, 2023). We view the sequence policy log πθ(x0c) through its ELBO surrogate given by score/flow matching (Song et al., 2021; Kingma et al., 2021): Et (cid:104) w(t) (cid:13) (cid:13)vθ(xt, t, c) (cid:0)ϵ x0 2(cid:105) (cid:1)(cid:13) (cid:13) . (10) where w(t) is the standard time-weight (e.g., ELBO). In practice, we find that the uniform weight w(t) = 1 works better. Such behavior roots in the different nature of the visual quality and likelihood for diffusion models and has been observed in (Ho et al., 2020; Nichol & Dhariwal, 2021; Song et al., 2021; Vahdat et al., 2021; Watson et al., 2021; Kingma et al., 2021). Thus, the likelihood ratio πθ (xc) πθold (xc) can be estimated through: (cid:104) w(t) (cid:13) (cid:13)vθ(xt, t, c) (cid:0)ϵ x0 (cid:1)(cid:13) 2 (cid:13) (cid:16) Et exp w(t) (cid:13) (cid:13)vθold(xt, t, c) (cid:0)ϵ x0 2(cid:105)(cid:17) , (cid:1)(cid:13) (cid:13) (11) for which we use the shared timesteps and noise for πθ and πθold from LLaDA 1.5 (Zhu et al., 2025) for variance reduction. The KL term can be estimated (Song et al., 2021) through: w(t) vθ(xt, t, c) vθref(xt, t, c)2 . To make the mechanics explicit, consider an on-policy update on single sample xi drawn from πθ( c). Let ˆπθ denote the ELBO-based surrogate of the sequence likelihood, (12) θ ˆπθ(xic) stopgrad (ˆπθ(xic)) Ai = θ log ˆπθ(xic)Ai = θEt (cid:104) w(t) (cid:13) (cid:13)vθ(xt, t, c) (cid:0)ϵ x0 2(cid:105) (cid:1)(cid:13) (cid:13) Ai. When Ai > 0 (good sample), the gradient decreases the flow matching loss at (xi, c), pulling vθ toward the target (ϵ xi); when Ai < 0 (bad sample), the gradient will push the velocity away from the undesirable target. We provide clear pseudo-code and pipeline Figure 4 for our AWM algorithm. For each prompt c, we (i) sample group of samples {x0}G i=1, (ii) evaluate rewards and form group-relative advantages Ai, (iii) get noisy data xt = (1 t)x0 + tϵ to evaluate score matching losses, and (iv) optimize an advantage-weighted FM objective with velocity-space KL regularization term. Concretely:"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: Pseudo code of the training loop for Advantage Weighted Matching 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for in range(num_training_steps): samples = sampler(model, prompt) # get samples with arbitrary sampler reward = reward_fn(samples) # get reward of samples advantage = cal_adv(reward, prompt) # e.g. group relative mean noise = randn_like(samples) timesteps = get_timesteps(samples) noisy_samples = fwd_diffusion(samples, noise, timesteps) # add noise velocity_pred = model(noisy_samples, timesteps, prompt) velocity_ref = ref_model(noisy_samples, timesteps, prompt) # optional for kl loss log_p = -((velocity_pred - (noise-samples))**2).mean() # Flow Matching Loss ratio = torch.exp(log_p - log_p.detach()) # or log_p_old for off-policy update policy_loss = -advantage * ratio kl_loss = weight(timesteps)*((velocity_pred - velocity_ref)**2).mean() loss = policy_loss + beta * kl_loss We summarize several key differences between AWM and DDPO below: Variance of Policy Gradient Subtracting baseline in policy gradient will keep the gradient unbiased and reduce the variance as an additive control variate (Greensmith et al., 2004). For diffusion models, there is another level of variance that comes from the estimation of the likelihood as the exact likelihood is computationally intractable. AWM reduces the variance compared to DDPO. Decouple of Sampling and Training The forward process formulation decouples the training and sampling procedure of AWM. This has several advantages: First, the samplers are not restricted to DDPM or Euler-Maruyama; we can use more advanced ODE samplers (Lu et al., 2022) or SDE samplers (Xue et al., 2023) for better trade-off between quality and speed, which we leave for future work. Second, the training timesteps and sampling timesteps can be decoupled, e.g., we can use 20 steps for sampling and 4 steps for training. Third, this also allows future explorations, such as using the step-distilled model to accelerate sampling. Alignment with Pretraining AWM keeps the conceptual symmetry between pretraining and RL for diffusion as in LLM: both optimize the same DSM/FM loss, differing only by reward-derived weights. Also, this avoids the use of CFG (Ho & Salimans, 2022) in training as pretraining."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We use two representative open-sourced models: SD3.5M (Esser et al., 2024) and Flux (Labs, 2024) on the above three reward tasks, i.e., composition image generation on GenEval (Ghosh et al., 2023), visual-text rendering on OCR (Chen et al., 2023a), and human preference alignment on PickScore(Kirstain et al., 2023). We use group size = 24, and LoRA (Hu et al., 2022) with α = 64 and = 32 for SD3.5M and LoRA with α = 128 and = 64 for FLUX. The KL ratio β is set to 0.4 for GenEval and OCR, and 0.01 for PickScore. The learning rate is set to constant 3e 4. We keep w(t) = 1 and Euler-Maruyama as our default sampler, with total train timesteps of 4 throughout our experiments. 5.1 MAIN RESULTS Table 1 reports the comparative performance of various models on the GenEval benchmark for SD3.5M. Among popular models, e.g., SD3.5L, DALLE-3, GPT-4o, and FLUX.1 Dev, overall scores range between 0.55 and 0.84, with GPT-4o achieving the highest performance (0.84). After RL post-training, Flow-GRPO reaches an overall GenEval score of 0.95, serving as the baseline, while AWM matches this score (0.95) but does so with dramatic efficiency gainachieving an 8.02 speed-up in GPU hours to convergence. Notably, AWM also attains consistently strong results across all sub-tasks, including Two-Object (0.99), Color (0.93), confirming that it preserves quality while substantially reducing training cost. Besides, we further validated AWM on other metrics such as OCR and PickScore, and across different backbones, including SD3.5M and FLUX. As shown in Table 2, AWM surpasses Flow-GRPO"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison on GenEval. Speed-up is relative to Flow-GRPO baseline. Model DALLE-3 GPT-4o SD-XL FLUX.1 Dev SD3.5L SD3.5M Flow-GRPO AWM (Ours) Geneval Single Obj. Two Obj. Counting Color Position Attr Overall Speed-up 0.96 0.99 0.98 0.98 0.98 0.98 1.00 1.00 0.87 0.92 0.74 0.81 0.89 0.78 0.99 0.99 0.47 0.85 0.39 0.74 0. 0.50 0.95 0.95 0.83 0.92 0.85 0.79 0.83 0.81 0.92 0.93 0.43 0.75 0.15 0.22 0.34 0.24 0.99 0.98 0.45 0.61 0.23 0.45 0. 0.52 0.86 0.83 0.67 0.84 0.55 0.66 0.71 0.63 0.95 0.95 1 8.02 Table 2: Performance comparison on OCR/PickScore for SD3.5M and FLUX. Hours is the total GPU hours; Speed-up is relative to Flow-GRPO baseline. indicates longer training time. Method OCR PickScore OCR PickScore Acc Hours Score Hours Acc Hours Score GPU hours SD3.5M FLUX Base model 0.59 FlowGRPO 0.89 AWM (Ours) 0.89 AWM (Ours) 0.95(+6.74%) 79.0 21.72 415.9 (1) 23.01 17.6 (23.59) 23.02 0.59 956.1 (1) 0.95 91.1 (10.49) 0. 22.20 343.6 (1) 23.08 40.3 (8.53) 23.08 339.2 (1) 49.8 (6.82) 23.25 (+0.99%) 205.0 0.99 (+4.21%) 147.0 23.18 (+0.43%) 78.0 in OCR and PickScore while requiring far fewer GPU hours. For example, SD3.5M with AWM achieves OCR (0.89) with 23.6 speed-up, and OCR (0.95) with an 8.53 reduction on FLUX. We illustrate training efficiency in Figure 5. AWM achieves comparable or superior performance on both OCR and PickScore while using significantly fewer GPU hoursfor example, up to 23.6 faster on SD3.5M OCR and 6.8 faster on FLUX PickScore. These results demonstrate consistent efficiency gains over Flow GRPO across models and evaluation metrics. Specifically, AWM reaches an OCR score of 0.89 on SD3.5 using only 17.6 GPU hours, whereas Flow GRPO requires over 400 GPU hours to attain the same performance. (a) SD3.5M OCR (b) SD3.5M PickScore (c) FLUX OCR (d) FLUX PickScore Figure 5: OCR and PickScore training efficiency. Metric vs. GPU hours for SD3.5M and FLUX. AWM (ours) exceeds Flow-GRPO with far less compute. We visualize the FLUX baseline and FLUX after 100 AWM gradient steps on three composition prompts and two text-rendering (OCR) prompts. The composition prompts specify numerosity, color, and position constraints. As shown in the second column of Figure 6, AWM adheres closely to the instructionse.g., three, purple, fox, one, blueand likewise improves text rendering. 5.2 ABLATION STUDIES Unless otherwise stated, all ablations are conducted on SD3.5M with identical architectures, batch sizes, and reward pipelines; curves report GenEval reward versus wall-clock (GPU hours). Results are summarized in Figure 7(ac). 4 Prompts in Figure 6 are: (1) three black rabbits below one white horse; (2) three purple foxes below one blue bird; (3) photo of chair left of zebra; (4) high-fashion runway with sleek, modern backdrop displaying \"Spring Collection 2024\". Models walk confidently on the catwalk, showcasing vibrant, floral prints and pastel tones, under soft, ambient lighting that enhances the fresh, spring vibe; (5) close-up of sleek smartwatch on wrist, the screen displaying \"Step Goal Achieved\" with celebratory animation, set against blurred cityscape at dusk, capturing the moment of accomplishment."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Visual comparison before (first row) and after 100 steps of AWM training (second row). Prompts from the GenEVAL and OCR benchmarks (listed in 4) are used for generation. (a) SD3.5M Time (b) SD3.5M Beta Figure 7: Ablations on SD3.5M and GenEval (a) Timestep sampling p(t); (b) KL strength β; (c) Data reuse (on-policy vs. one-step off-policy). Curves show GenEval reward vs. GPU hours. (c) SD3.5M Policy Sampling distribution over timesteps p(t). We compare three choices: (i) discrete distribution on the inference samplers time grid {tk}, which is uniformly discretized (ii) uniform U(0, 1), and (iii) logit-normal distribution that concentrates mass near mid-range t. Figure 7a shows that discrete and uniform achieve similar results, while logit-normal lags and eventually degrade during RL fine-tuning. We adopt discrete as our default settings. KL regularization strength β. We sweep β {0.2, 0.4, 1.0, 2.0} in the velocity-space KL proxy. As shown in Figure 7b, too little regularization (β=0.2) can lead to instability and collapse, while too much (β=2.0) slows learning and underperforms. medium range (β[0.4, 1.0]) is stable and fast. Thus, the KL should stabilize the update, not dominate it. On-policy vs. mixed (one-step) off-policy. We compare purely on-policy updates with simple mixed strategy that reuses 50% of batches from the previous policy (one-step off-policy) with importance ratios. Figure 7c shows very similar learning curves and final scores; the mixed regime enables future extensions to deeper off-policy reuse, which we adopt as default settings."
        },
        {
            "title": "6 CONCLUSION\nIn this paper, we showed that DDPO implicitly performs denoising score/flow matching with\nnoisy targets, which increases variance and slows optimization. Building on this, we introduced\nAdvantage-Weighted Matching (AWM), which keeps the pretraining score/flow-matching objective\nand applies advantage-based weights—thereby unifying pretraining and RL post-training under a\nsingle, policy-gradient–consistent objective. It achieves up to 24× faster convergence than Flow-\nGRPO on SD3.5-M and FLUX without degrading generation quality.",
            "content": ""
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:9353 9387, 2023a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023b. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Jiajun Fan, Shuaike Shen, Chaoran Cheng, Yuxin Chen, Chumeng Liang, and Ge Liu. Online reward-weighted fine-tuning of flow matching with wasserstein regularization. In The Thirteenth International Conference on Learning Representations, 2025. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Evan Greensmith, Peter Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):14711530, 2004. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Ulrich Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability, pp. 11881205, 1986."
        },
        {
            "title": "Preprint",
            "content": "Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of one-step generator from low-resolution diffusion teacher. Advances in Neural Information Processing Systems, 37:19167 19208, 2024. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:6548465516, 2023. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Jiachen Li, Weixi Feng, Wenhu Chen, and William Yang Wang. Reward guided latent consistency distillation. arXiv preprint arXiv:2403.11027, 2024. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025a. Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, and Shanghang Zhang. Branchgrpo: Stable and efficient grpo with structured branching in diffusion models. arXiv preprint arXiv:2509.06040, 2025b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022. Weijian Luo, Colin Zhang, Debing Zhang, and Zhengyang Geng. Diff-instruct*: Towards humanpreferred one-step text-to-image generative models. arXiv e-prints, pp. arXiv2410, 2024."
        },
        {
            "title": "Preprint",
            "content": "Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, and Jing Tang. Rewardinstruct: reward-centric approach to fast photo-realistic image generation. arXiv preprint arXiv:2503.13070, 2025. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053, 2025. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Jorge Nocedal and Stephen Wright. Numerical optimization. Springer, 2006. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Stefano Peluchetti. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589, 2023. Lev Semenovich Pontryagin. Mathematical theory of optimal processes. Routledge, 2018. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in neural information processing systems, 34:1128711302, 2021."
        },
        {
            "title": "Preprint",
            "content": "Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. Chong Wang, Xi Chen, Alexander Smola, and Eric Xing. Variance reduction for stochastic gradient optimization. Advances in neural information processing systems, 26, 2013. Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sample from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:7763277674, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A LLM USAGE",
            "content": "In the preparation of this manuscript, we employed large language models to provide languagerelated assistance. Specifically, the LLM was used to (i) polish grammar, style, and readability of the text; (ii) offer suggestions for clearer phrasing and more concise expression."
        },
        {
            "title": "B PROOFS",
            "content": "B.1 PROOF OF LEMMA 1 Proof. The proof proceeds by taking the gradient of the Denoising Score Matching loss with respect to the model parameters θ and showing that it is equivalent to the gradient of the standard Score Matching loss. θExs Extxs =θExs =θExt Extxs sθ(xt, t)2(cid:105) (cid:104) sθ(xt, t) xt log p(xtxs)2(cid:105) (cid:104) sθ(xt, t)2(cid:105) (cid:104) 2θExs (cid:90) (cid:90) Extxs [sθ(xt, t), xt log p(xtxs)] 2θ sθ(xt, t), xt log p(xtxs)p(xtxs)p(xs)dxsdxt =θExt sθ(xt, t)2(cid:105) (cid:104) 2θ =θExt sθ(xt, t)2(cid:105) (cid:104) 2θ =θExt sθ(xt, t)2(cid:105) (cid:104) 2θ =θExt sθ(xt, t)2(cid:105) (cid:104) 2θ sθ(xt, t), xtp(xtxs)p(xs)dxsdxt sθ(xt, t), (cid:90) xs sθ(xt, t), xt xtp(xtxs)p(xs)dxsdxt (cid:90) xs p(xtxs)p(xs)dxsdxt sθ(xt, t), xtp(xt)dxt xt (cid:90) xs (cid:90) xs xt (cid:90) xt (cid:90) xt (cid:90) xt (cid:90) =θExt =θExt =θExt 2θ sθ(xt, t)2(cid:105) (cid:104) sθ(xt, t)2(cid:105) (cid:104) sθ(xt, t) xt log p(xt)2(cid:105) (cid:104) xt sθ(xt, t), xt log p(xt)p(xt)dxt 2θExt [sθ(xt, t), xt log p(xt)] (13) B.2 PROOF OF THEOREM 1 Our analysis builds upon foundational theorem regarding the time reversal of diffusion processes from Haussmann and Pardoux (Haussmann & Pardoux, 1986). Lemma 2 (Time Reversal of Diffusions (Haussmann & Pardoux, 1986)). Let the forward process {xt}t[0,1] be defined by the stochastic differential equation (SDE): dxt = 1 1 xt, dt + (cid:114) 2t 1 , dwt, (14) Then, the corresponding reverse process, where time flows from 1 to 0, satisfies the SDE: (cid:20) vθ(xt, t) + dxt = 1 1 (cid:21) (xt + (1 t)vθ(xt, t)) dt + (cid:114) 2t 1 , dwt. (15) direct consequence of this theorem is that the marginal distribution of xt is identical for the forward and reverse processes. Furthermore, stronger corollary holds: the joint distribution of (xs, xt) is also identical for both processes. This allows us to connect the forward-process-based score matching objective to the reverse-process-based DDPO objective."
        },
        {
            "title": "Preprint",
            "content": "Proof. Our goal is to demonstrate that minimizing the Denoising Score Matching loss is equivalent to maximizing the DDPO log-likelihood objective. We begin with the Denoising Score Matching loss at xt conditioned on noisy data xtt from the forward process: Extt,xt (cid:16) 1t sθ(xt, t) log p(xtxtt)2(cid:105) (cid:104) t2 (1t)2 . (1(tt))2 (t t)2(cid:17) (cid:16) Note that xtxtt the above denoising score matching loss is: 1(tt) xtt, (16) (cid:17) , thus we have that Extt,xt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sθ(xt, t) + xt 1t t2 (1t) 1(tt) xtt (1(tt))2 (t t)2 , we have 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) . (17) Also, note that sθ(xt, t) = (1t)vθ (xt,t)+xt Extt,xt (1 t)vθ(xt, t) + xt + xt 1t t2 (1t)2 1(tt) xtt (1(tt))2 (t t)2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Simplifying the above equation, and let the coefficient of xtt to be 1, we have: (cid:18) (cid:19) Extt,xt xtt (1 (t t)) + (1 t)(t t)2 t(1 (t t)) xt (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) + 1 (t t) (cid:18) t2 (1 t)2 (1 (t t))2 (t t)2 (cid:19) vθ(xt, t) 2(cid:35) . (cid:13) (cid:13) (cid:13) (cid:13) . (18) (19) Now, lets consider the DDPO objective, which is the log-likelihood of one step of the reverse process. We consider the Euler-Maruyama discretization with Exponential Integrator of the diffusion SDE Equation (1): (cid:20) vθ(xt, t) + dxt = 1 1 (cid:21) (xt + (1 t)vθ(xt, t)) dt + (cid:114) 2t 1 dwt. According to the results in (Xue et al., 2023), (1 t)(t t)2 (1 (t t))t2 xt + xtt = 1 (t t) t2 (t2 (1 t)2 (1 (t t))2 (t t)2)xθ(xt, t) (cid:115) + t2 (1 t)2 (1 (t t))2 (t t)2 ϵ Subtitute that xθ = xt tvθ, we have that (cid:18) xtt = (1 (t t)) + 1 (t t) (t2 (1 t)(t t)2 t(1 (t t)) (1 t)2 (cid:19) xt (1 (t t))2 (t t)2)vθ(xt, t) (cid:115) + t2 (1 t)2 (1 (t t))2 (t t)2 ϵ (20) (21) (22) (23) the discretization error If we omit induced by Euler-Maruyama discretization, according to Lemma 2, the joint distribution of (xtt, xt) in the reverse process is identical to (xtt, xt) in the forward process. Then we have that maximizing log pθ(xttxt) is equivalent to minimizing: Extt,xt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) xtt (cid:18) (1 (t t)) + (1 t)(t t)2 t(1 (t t)) (cid:19) xt + 1 (t t) (cid:18) t2 (1 t)2 (1 (t t))2 (t t)2 (cid:19) vθ(xt, t) 2(cid:35) , (cid:13) (cid:13) (cid:13) (cid:13) (24) which is identical to Equation (19), thus identical to the noisy DSM loss Equation (16)."
        },
        {
            "title": "Preprint",
            "content": "B.3 PROOF OF THEOREM 2 We first give more detailed version of Theorem 2 Theorem 2 (Detailed version of Theorem 2 in the main body). xt, xs satisfies xtxs N(cid:0)α(t, s) xs, σ2(t, s) I(cid:1), α(t, s) = 1 1 , σ2(t, s) = t2 (1 t)2 (1 s)2 s2, for all 0 < 1, so that σ2(t, s) + α2(t, s) s2 = t2. Let be the data dimension. Then, for the two conditional scores xt log p(xtxs), xt log p(xtx0), (25) (26) and for any fixed xt and any [0, t), each is an unbiased estimator of the true score xt log p(xt): E[xt log p(xtxs) xt] = E[xt log p(xtx0) xt] = xt log p(xt). Moreover, their conditional covariances satisfy Cov(xt log p(xtxs) xt) = Cov(xt log p(xtx0) xt) + κ(s, t) Cov(xt log p(xtx0) xt) , (27) where In particular, κ(s, t) = (cid:16) (1 t)2 s2 t2(1 s)2 s2(1 t)2 (cid:17) . (28) Tr Cov(xt log p(xtxs) xt) = Tr Cov(xt log p(xtx0) xt) + κ(s, t). The map (cid:55) κ(s, t) is strictly increasing on [0, t), with κ(0, t) = 0 and κ(s, t) as t. Moreover, for any predictor = a(xt, t) Rd (e.g. = sθ(xt, t)): 2 (cid:12) (cid:104)(cid:13) (cid:13)a xt log p(xtx0)(cid:13) (cid:12) (cid:12) xt (cid:13) 2 (cid:12) (cid:12) (cid:12) xt (cid:104)(cid:13) (cid:13)a xt log p(xtxs)(cid:13) (cid:13) (cid:16)(cid:13) (cid:13)a xt log p(xtxs)(cid:13) (cid:13) 2 (cid:12) (cid:12) (cid:12) xt (cid:16)(cid:13) (cid:13)a xt log p(xtx0)(cid:13) (cid:13) = Var = Var (cid:17) (cid:105) (cid:105) (cid:17) 2 (cid:12) (cid:12) (cid:12) xt + 4 κ(s, t) (cid:104)(cid:13) (cid:13)a xt log p(xtx0)(cid:13) (cid:13) + κ(s, t), (29) + 2d κ(s, t)2 (cid:105) . 2 (cid:12) (cid:12) (cid:12) xt (30) Hence both the conditional risk and the conditional objective variance are minimized at = 0 and strictly increase with (0, t). Proof. Under Equation (25), xt log p(xtxs) = 1 σ2(t, s) (cid:0)xt α(t, s) xs (cid:1), xt log p(xtx0) = (cid:0)xt (1 t) x0 (cid:1). 1 t2 (31) Auxiliary Lemma 1 For fixed xt and any [0, t), there exists zero-mean Gaussian vector η such that xt log p(xtxs) = xt log p(xtx0) + η, η (x0xt), η N(cid:0)0, κ(s, t) I(cid:1), (32) with κ(s, t) given by Equation (28). Proof of Aux. Lemma 1. Conditioned on (xt, x0), the posterior of xs is Gaussian with mean σ2(t,s)(1s)x0+α(t,s)s2xt t2 and covariance s2σ2(t,s) Using equation 31, define t2 η := xt log p(xtxs) xt log p(xtx0) = α(t, s) σ2(t, s) (xs E[xsxt, x0]) ."
        },
        {
            "title": "Preprint",
            "content": "Then η(xt, x0) is zero-mean Gaussian with covariance α2 σ4 s2σ2 t2 = α2s2 σ2t2 = κ(s, t) I. The residual xs E[xsxt, x0] is independent of (xt, x0), hence η (x0xt). By the score identity, E[xt log p(xtxs)xt] = xt log p(xt) for every [0, t); in particular this equals E[xt log p(xtx0)xt]. From equation 32 and η (x0xt), Cov(xt log p(xtxs) xt) = Cov(xt log p(xtx0) xt) + Cov(ηxt) = Cov(xt log p(xtx0) xt) + κ(s, t) I, which proves Theorem 2 and the trace statement. Auxiliary Lemma 2 (Monotonicity of κ). For fixed (0, 1], (cid:55) κ(s, t) in Equation (28) is strictly increasing on [0, t), with κ(0, t) = 0 and κ(s, t) as t. Proof of Aux. Lemma 2. Using Equation (26), write κ(s, t) = (1t)2s2 (1s)2 . On [0, t), q(s) is strictly increasing and takes values in [0, t2), while (cid:55) strictly increasing on [0, t2). Limits follow from q(0) = 0 and q(s) t2 as t. q(s) t2(cid:0)t2q(s)(cid:1) with q(s) := α2(t, s)s2 = t2(t2q) is For any a, (cid:104)(cid:13) (cid:13)axt log p(xtxs)(cid:13) (cid:13) (cid:13) (cid:13)aE(cid:2)xt log p(xtxs)xt (cid:13) Insert unbiasedness and Equation (27), then subtract the same identity with = 0 to obtain the increment κ(s, t). +Tr Cov(xt log p(xtxs) xt) . 2 (cid:12) (cid:12) (cid:12) xt (cid:3)(cid:13) 2 (cid:13) (cid:13) = (cid:105) To prove Equation (30), let δ(x0) := xt log p(xtx0). From Equation (32), (cid:13)a xt log p(xtxs)(cid:13) (cid:13) 2 (cid:13) = δ η2 = δ2 + η2 2δ, η. Conditioned on (xt, x0), η (0, κI) is independent of δ. Gaussian moment identities yield, coordinatewise, E[η2xt, x0] = κ, Var(η2xt, x0) = 2d κ2, Var(δ, ηxt, x0) = κ δ2, Cov(η2, δ, ηxt, x0) = 0. Therefore, (cid:16)(cid:13) (cid:13)a xt log p(xtxs)(cid:13) (cid:13) Var 2 (cid:12) (cid:12) (cid:12) xt, x0 (cid:17) = Var(cid:0)η2 2δ, ηxt, x0 (cid:1) = 2d κ2 + 4κ δ2. Law of total variance yields Equation (30). Monotonicity in follows from Aux. Lemma 2; strictness for (0, t) follows since κ(s, t) > 0."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "HKU",
        "MIT",
        "UCAS"
    ]
}