{
    "paper_title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
    "authors": [
        "Bencheng Liao",
        "Shaoyu Chen",
        "Haoran Yin",
        "Bo Jiang",
        "Cheng Wang",
        "Sixu Yan",
        "Xinbang Zhang",
        "Xiangyu Li",
        "Ying Zhang",
        "Qian Zhang",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10$\\times$ reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive."
        },
        {
            "title": "Start",
            "content": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving Bencheng Liao1,2, Shaoyu Chen2,3 Haoran Yin3 Bo Jiang2, Cheng Wang1,2, Sixu Yan2 Xinbang Zhang3 Xiangyu Li3 Ying Zhang3 Qian Zhang3 Xinggang Wang2 (cid:0) 4 2 0 N 2 2 ] . [ 1 9 3 1 5 1 . 1 1 4 2 : r 1 Institute of Artificial Intelligence, Huazhong University of Science & Technology 2 School of EIC, Huazhong University of Science & Technology 3 Horizon Robotics Code & Model & Demo: hustvl/DiffusionDrive"
        },
        {
            "title": "Abstract",
            "content": "Recently, the diffusion model has emerged as powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, openworld nature of traffic scenes pose substantial challenges for generating diverse driving actions at real-time speed. To address these challenges, we propose novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10 reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with aligned ResNet34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting new record, while running at real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. 1. Introduction End-to-end autonomous driving has gained significant attention in recent years due to advancements in perception models (detection [14, 21, 35], tracking [4547], online mapping [24, 25, 27], etc.), which directly learns the driving policy from the raw sensor inputs. This data-driven apFigure 1. The comparison of different end-to-end paradigms. (a) Single mode regression [6, 13, 17]. (b) Sampling from vocabulary [3, 22]. (c) Vanilla diffusion policy [5, 16]. (d) The proposed truncated diffusion policy. proach offers scalable and robust alternative to traditional rule-based motion planning, which often struggles to generalize to complex real-world driving settings. To effectively learn from data, mainstream end-to-end planners (e.g., Transfuser [6], UniAD [13], VAD [17]) typically regress single-mode trajectory from an ego-query as shown in Fig. 1a. However, this paradigm does not account for the inherent uncertainty and multi-mode nature of driving behaviors. Recently, VADv2 [17] introduces large fixed vocabulary of anchor trajectories (4096 anchors) to discretize the continuous action space and capture broader range of driving behaviors, and then samples from these anchors based on predicted scores as shown in Fig. 1b. However, this large fixed-vocabulary paradigm is fundamentally constrained by the number and quality of anchor trajectories, often failing in out-of-vocabulary scenarios. Furthermore, managing large number of anchors presents significant computational challenges for real-time applications. Rather than discretizing the action space, diffusion model [5] has proven to be powerful generaIntern of Horizon Robotics; (cid:0) Corresponding author: Xinggang Wang (xgwang@hust.edu.cn). To distinguish the term multimodal used to describe input data, we use multi-mode in this paper to refer to diverse planning decisions. tive decision-making policy in the robotics domain, which can directly sample multi-mode physically plausible actions from Gaussian distribution via an iterative denoising process. This inspires us to replicate the success of the diffusion model in the robotics domain to end-to-end autonomous driving. We apply the vanilla robotic diffusion policy to the well-known single-mode-regression method, Transfuser [6], by proposing variant, TransfuserDP, which replaces the deterministic MLP regression head with conditional diffusion model [28]. Though TransfuserDP improves planning performance, two major issues arise: 1) The numerous 20 denoising steps in the vanilla DDIM diffusion policy introduce heavy computational consumption during inference as shown in Tab. 2, hindering the real-time application for autonomous driving. 2) The trajectories sampled from different Gaussian noises severely overlap with each other, as illustrated in Fig. 2. This underscores the non-trivial challenge of taming the diffusion models for the dynamic and open-world traffic scenes. Unlike the vanilla diffusion policy, which samples actions from random Gaussian noise conditioned on scene context, human drivers adhere to established driving patterns that they dynamically adjust in response to real-time traffic conditions. This insight motivates us to embed these prior driving patterns into the diffusion policy by partitioning the Gaussian distribution into multiple sub-Gaussian distributions centered around prior anchors, referred to as anchored Gaussian distribution. It is implemented by truncating the diffusion schedule to introduce small portion of Gaussian noise around the prior anchors as shown in Fig. 3. Thanks to the multi-mode distributional expressivity of the diffusion model, the proposed truncated diffusion policy effectively covers the potential action space without requiring large set of fixed anchors, as VADv2 does. With more reasonable initial noise samples from the anchored Gaussian distribution, we can truncate the denoising process, reducing the required steps from 20 to just 2a substantial speedup that satisfies the real-time requirements of autonomous driving. To enhance the interaction with conditional scene context, we propose an efficient transformer-based diffusion decoder that interacts not only with structured queries from the perception module but also with Birds Eye View (BEV) and perspective view (PV) features through sparse deformable attention mechanism [51]. Additionally, we introduce cascade mechanism to iteratively refine the trajectory reconstruction within the diffusion decoder at each denoising step. With these innovations, we present DiffusionDrive, diffusion model for real-time end-to-end autonomous driving. We benchmark our method on the planningoriented NAVSIM dataset [9] using non-reactive simulation and closed-loop evaluations. Without bells and whistles, DiffusionDrive achieves 88.1 PDMS on NAVSIM navtest split with the aligned ResNet-34 backbone, significantly outperforming previous state-of-the-art methods. Even compared to the NAVSIM challenge-winning solution Hydra-MDP-V8192-W-EP [22], which follows VADv2 with 8192 anchor trajectories and further incorporates postprocessing and additional supervision, DiffusionDrive still outperforms it by 1.6 PDMS through directly learning from human demonstrations and inferring without postprocessing, while running at real-time speed of 45 FPS on an NVIDIA 4090. We further validate the superiority of DiffusionDrive on popular nuScenes dataset [2] with openloop evaluations, DiffusionDrive runs 1.8 faster than VAD and outperforms it [17] by 20.8% lower L2 error and 63.6% lower collision rate with the same ResNet-50 backbone, demonstrating state-of-the-art planning performance. Our contributions can be summarized as follows: We firstly introduce the diffusion model to the field of end-to-end autonomous driving and propose novel truncated diffusion policy to address the issues of mode collapse and heavy computational overhead found in direct adaptation of vanilla diffusion policy to the traffic scene. We design an efficient transformer-based diffusion decoder that interacts with the conditional information in cascaded manner for better trajectory reconstruction. Without bells and whistles, DiffusionDrive significantly outperforms previous state-of-the-art methods, achieving record-breaking 88.1 PDMS on the NAVSIM navtest split with the same backbone, while maintaining real-time performance at 45 FPS on an NVIDIA 4090. We qualitatively demonstrate that DiffusionDrive can generate more diverse and plausible trajectories, exhibiting high-quality multi-mode driving actions in various challenging scenarios. 2. Related Work End-to-end autonomous driving. UniAD [13], as pioneering work, demonstrates the potential of end-to-end autonomous driving by integrating multiple perception tasks to enhance planning performance. VAD [17] further explores the use of compact vectorized scene representations to improve efficiency. Subsequently, series of works [4, 6, 10, 20, 23, 36, 38, 48] have adopted the single-trajectory planning paradigm to enhance planning performance further. More recently, VADv2 [3] shifts the paradigm towards multi-mode planning by scoring and sampling from large fixed vocabulary of anchor trajectories. HydraMDP [22] improves the scoring mechanism of VADv2 by introducing extra supervision from rule-based scorer. SparseDrive [32] explores an alternative BEV-free solution. Unlike existing multi-mode planning approaches, we pro- (a) Top-1s going straight and diverse top-10s lane changing. (b) Top-1s turning left and diverse top-10s lane changing. Figure 2. Qualitative comparison of Transfuser, TransfuserDP and DiffusionDrive on challenging scenes of NAVSIM navtest split. With the same inputs from front cameras and LiDAR, DiffusionDrive achieves the highest planning quality of top-1 scoring trajectory as illustrated in Tab. 2. We render the highlighted diverse trajectories predicted by DiffusionDrive in the front view. (a) and (b) shows that the top-1 scoring trajectory of DiffusionDrive closely matches the ground truth for both going straight and turning left. Additionally, DiffusionDrives top-10 scoring trajectory demonstrates high-quality lane changingan ability not observed in multi-mode TransfuserDP and impossible for Transfuser. pose novel paradigm that leverages powerful generative diffusion models for end-to-end autonomous driving. Diffusion model for traffic simulation. Driving diffusion policy has been explored in the traffic simulation by leveraging only abstract perception groundtruth [7, 15, 18, 37]. MotionDiffuser [18] and CTG [50] are pioneering applications of diffusion models for multi-agent motion prediction, using conditional diffusion model to sample target trajectories from Gaussian noise. CTG++ [49] further incorporates large language model (LLM) for language-driven guidance, improving usability and enabling realistic traffic simulations. Diffusion-ES [41] replaces reward-gradientguided denoising with evolutionary search. VBD [15] introduces scene-consistent scenario optimizer using conditional diffusion model with game-theoretical guidance to generate abstract safety-critical driving scenarios. Moving beyond diffusion models limited to traffic simulation with perception groundtruth, our approach unlocks the potential of diffusion models for real-time, end-to-end autonomous driving through our proposed truncated diffusion policy and efficient diffusion decoder. Diffusion model for robotic policy learning. Diffusion policy [5] demonstrates the great potential in robotic policy learning, effectively capturing multi-mode action distriFigure 3. Illustration of truncated diffusion policy by comparing with vanilla diffusion policy. We truncate the diffusion process and only add small portion of Gaussian noise to diffuse the anchor trajectories. Then, we train the diffusion model to reconstruct the ground-truth trajectory from the anchored Gaussian distribution with conditional scene context. During the inference, we also truncate the denoising process by starting from the better samples in the anchored Gaussian distribution than the pure Gaussian noise. butions and high-dimensional action spaces. Diffuser [16] proposes an unconditional diffusion model for trajectory sampling, incorporating techniques such as classifier-free guidance and image inpainting to achieve guided sampling. Subsequently, numerous works have applied diffusion models to various robotic tasks, including stationary manipulation [1, 44], mobile manipulation [40], autonomous navigation [30, 42], quadruped locomotion [31], and dexterous manipulation [39]. However, directly applying vanilla diffusion policy to end-to-end autonomous driving poses unique challenges, as it requires real-time efficiency and the generation of plausible multi-mode trajectories in dynamic and open-world traffic scenes. In this work, we propose novel truncated diffusion policy to address these challenges, introducing concepts that have not yet been explored in the robotics field. 3. Method 3.1. Preliminary Task formulation. End-to-end autonomous driving takes raw sensor data as input and predicts the future trajectory of the ego-vehicle. The trajectory is represented as sequence of waypoints τ = {(xt, yt)}Tf t=1, where Tf denotes the planning horizon, and (xt, yt) is the location of each waypoint at time in the current ego-vehicle coordinate system. Conditional diffusion model. The conditional diffusion model poses forward diffusion process as gradually adding noise to the data sample, which can be defined as: (cid:0)τ τ 0(cid:1) = (cid:16) τ i; αiτ 0, (cid:0)1 αi(cid:1) (cid:17) , (1) where τ 0 is the clean data sample, and τ is the data sample with noise at time (Note: we use superscript to denote diffusion timestep). The constant αi = (cid:81)i s=1 αs = (cid:81)i s=1(1 βs) and βs is the noise schedule. We train the reverse process model fθ(τ i, z, i) to predict τ 0 from τ with the guidance of conditional information z, where θ is the trainable model parameter. During inference, the trained diffusion model fθ progressively refines from the random noise τ sampled in Gaussian distribution to the predicted clean data sample τ 0 with the guidance of conditional information z, which is defined as: (cid:0)τ 0 z(cid:1) = pθ (cid:90) (cid:0)τ (cid:1) (cid:89) i=1 (cid:0)τ i1 τ i, z(cid:1) dτ 1:T . (2) pθ Figure 4. Overall architecture of DiffusionDrive. (a) DiffusionDrive can integrate various existing perception modules and different (b) The designed diffusion decoder takes the sampled noisy trajectories from anchored Gaussian distribution as input sensor inputs. and progressively denoises them with enhanced interactions with the conditional scene context in cascade manner to generate the final predictions. 3.2. Investigation 3.3. Truncated Diffusion Turn Transfuser [6] into conditional diffusion model. We begin from the representative deterministic end-to-end planner Transfuser [6] and turn it into generative model TransfuserDP by simply replacing the regression MLP layers with the conditional diffusion model UNet following vanilla diffusion policy [5]. During the evaluation, we sample random noise and progressively refine it with 20 steps. Tab. 2 shows that TransfuserDP achieves better planning quality than deterministic Transfuser. Mode collapse. To further investigate the multi-mode property of the vanilla diffusion policy in driving, we sampled 20 random noises from Gaussian distribution and denoised them using 20 steps. As shown in Fig. 2, the different random noises converge to similar trajectories after the denoising process. To quantitatively analyze the phenomenon of mode collapse, we define mode diversity score based on the mean Intersection over Union (mIoU) between each denoised trajectory and the union of all denoised trajectories: = 1 1 (cid:88) i=1 Area(τi (cid:83)N Area(τi (cid:83)N j=1 τj) j=1 τj) , (3) where τi represents the i-th denoised trajectory, is the total number of sampled trajectories and (cid:83)N j=1 τj is the union of all denoised trajectories. higher mIoU indicates less diversity of the denoised trajectories. The quantitative mode diversity results in Tab. 2 further validate the observations presented in Fig. 2. Heavy denoising overhead. The DDIM [29] diffusion policy requires 20 denoising steps to transform random noise into feasible trajectory, which introduces significant computational overhead, reducing the FPS from 60 to 7, as shown in Tab. 2, and making it impractical for real-time online driving applications. Human driving follows fixed patterns, unlike the random noise denoising in vanilla diffusion policy. Motivated by this, we propose truncated diffusion policy that begins the denoising process from an anchored Gaussian distribution instead of standard Gaussian distribution. To enable the model to learn to denoise from the anchored Gaussian distribution to the desired driving policy, we further truncate the diffusion schedule during training, adding only small amount of Gaussian noise to the anchors. Training. We first construct the diffusion process by adding Gaussian noise to anchors {ak}Nanchor k=1 clustered by K-Means on the training set, where ak = {(xt, yt)}Tf t=1. We truncate the diffusion noise schedule to diffuse the anchors to the anchored Gaussian distribution: 1 αiϵ, ϵ (0, I), (4) τ = αiak + (cid:112) where [1, Ttrunc] and Ttrunc is the truncated diffusion steps. During training, the diffusion decoder fθ takes as input k}Nanchor and predicts classificaand denoised trajectories {ˆτk}Nanchor k=1 : Nanchor noisy trajectories {τ tion scores {ˆsk}Nanchor k=1 k=1 {ˆsk, ˆτk}Nanchor k=1 = fθ({τ k}Nanchor k=1 , z), (5) where represents the conditional information. We assign the noisy trajectory around the closest anchor to the ground truth trajectory τgt as positive sample (yk = 1) and others as negative samples (yk = 0). The training objective combines trajectory reconstruction and classification: = Nanchor(cid:88) k= [ykLrec(ˆτk, τgt) + λBCE(ˆsk, yk)], (6) where λ balances the simple L1 reconstruction loss Lrec and binary cross-entropy (BCE) classification loss. Method Input Img. Backbone Anchor NC DAC TTC Comf. EP PDMS Camera ResNet-34 [11] UniAD [13] Camera ResNet-34 [11] PARA-Drive [38] Camera ResNet-34 [11] LTF [6] & ResNet-34 [11] Transfuser [6] & ResNet-34 [11] DRAMA [43] VADv2-V8192 [3] & ResNet-34 [11] Hydra-MDP-V8192 [22] & ResNet-34 [11] Hydra-MDP-V8192-W-EP [22] & ResNet-34 [11] 0 0 0 0 0 8192 8192 8192 97.8 97.9 97.4 97.7 98.0 97.2 97.9 98.3 DiffusionDrive (Ours) & ResNet-34 [11] 98.2 91.9 92.4 92.8 92.8 93.1 89.1 91.7 96.0 96.2 92.9 93.0 92.4 92.8 94.8 91.6 92.9 94.6 94.7 100 99.8 100 100 100 100 100 100 78.8 79.3 79.0 79.2 80.1 76.0 77.6 78.7 82.2 83.4 84.0 83.8 84.0 85.5 80.9 83.0 86.5 88.1 Table 1. Comparison on planning-oriented NAVSIM navtest split with closed-loop metrics. & denotes the use of both camera and LiDAR as sensor inputs. V8192 denotes 8192 anchors. Hydra-MDP-V8192-W-EP is variant of Hydra-MDP [22], which is further trained to fit the EP evaluation metric with additional supervision from the rule-based evaluator and uses weighted confidence postprocessing. DiffusionDrive simply learns from human demonstrations and infers without post-processing. The best and the second best results are denoted by bold and underline. Method NC DAC TTC Comf. EP PDMS Plan Module Time Arch. Step Time Steps Total Para. FPS 97.7 Transfuser 97.5 TransfuserDP 97.9 TransfuserTD DiffusionDrive 98. 92.8 93.7 94.2 96.2 92.8 92.7 93.9 94.7 100 100 100 100 79.2 84.0 MLP 79.4 84.6+0.6 UNet 80.2 85.7+1.7 UNet 82.2 88.1+4.1 Dec. 0.2ms 6.5ms 6.9ms 3.8ms 1 20 130.0ms 11% 101M 2 0.2ms 0% 56M 60 7 13.8ms 70% 102M 27 7.6ms 74% 60M 45 Table 2. Roadmap from Transfuser to DiffusionDrive on NAVSIM navtest split. TransfuserDP denotes Transfuser with vanilla DDIM diffusion policy [5]. TransfuserTD denotes Transfuser with truncated diffusion policy. Step Time denotes the runtime of each denoising step. FPS and runtime are measured on an NVIDIA 4090 GPU. denotes the mode diversity score defined in Eq. (3). Inference. We use truncated denoising process that starts with noisy trajectories sampled from the anchored Gaussian distribution and progressively denoises them to final predictions. At each denoising timestep, the estimated trajectories from the previous step are passed to the diffusion decoder fθ, which predicts classification scores {ˆsk}Ninfer k=1 and coordinates {ˆτk}Ninfer k=1 . After obtaining the current timesteps predictions, we apply the DDIM [29] update rule to sample trajectories for the next timestep. Inference flexibility. key advantage of our approach lies in its inference flexibility. While the model is trained with Nanchor trajectories, the inference process can accommodate an arbitrary number of trajectory samples Ninfer, where Ninfer can be dynamically adjusted based on computational resources or application requirements. 3.4. Architecture The overall architecture of our proposed method, DiffusionDrive, is illustrated in Fig. 4. DiffusionDrive can integrate various existing perception modules used in previous endto-end planners [6, 13, 17, 32] and take different sensor inputs. The designed diffusion decoder is tailored for the complex and challenging driving application, which has enhanced interactions with the conditional scene context. Diffusion decoder. Given the set of sampled noisy trajectories {ˆτk}Ninfer k=1 from the anchored Gaussian distribution, we begin by applying deformable spatial cross-attention [26, 35, 51] to interact with Birds Eye View (BEV) or Perspective View (PV) features based on the trajectory coordinates. Subsequently, cross-attention is performed between the trajectory features and the agent/map queries derived from the perception module, followed by feed-forward network (FFN). To encode the diffusion timestep information, we utilize Timestep Modulation layer, which is followed by Multi-Layer Perceptron (MLP) that predicts the confidence score and the offset relative to the initial noisy trajectory coordinates. The output from this diffusion decoder layer serves as the input for the subsequent cascade diffusion decoder layer. DiffusionDrive further reuses the cascade diffusion decoder to iteratively denoise the trajectory during inference, with parameters shared across the different denoising timesteps. The final trajectory with the highest confidence score is selected as the output. 4. Experiment 4.1. Dataset NAVSIM. The NAVSIM dataset is real-world planning-oriented dataset builds upon OpenScene [8], compact redistribution of nuPlan [19], the largest publicly available annotated driving dataset. NAVSIM leverages eight cameras to achieve full 360 FOV, along with [9] UNet Decoder ID 1 2 3 4 5 6 Agent/Map Cascade Ego Query Interaction Cross-attn Cross-attn Decoder Spatial Param. Planning Metric NC DAC TTC Comf. EP PDMS 102M 97.9 57M 88.7 58M 98.2 58M 97.9 59M 98.0 60M 98.2 94. 83.2 95.4 93.5 95.8 96.2 93.9 80.0 94.4 93.8 94.4 94.7 100 84.8 100 100 100 100 80. 43.3 81.3 79.8 81.7 82.2 85.7 55.1 87.1 85.1 87.4 88.1 Table 3. Ablation for design choices. Cascade Decoder indicates that we stack 2 cascade diffusion decoder layers. ID-1 refers to TransfuserTD in Tab. 2, utilizing conditional UNet and interaction with the ego-query, which Transfuser uses to directly regress the singlemode trajectory. Steps Param. NC DAC TTC Comf. EP PDMS Stages Param. NC DAC TTC Comf. EP PDMS Ninfer Param. NC DAC TTC Comf. EP PDMS 1 2 3 60M 98.3 96.0 94.7 60M 98.2 96.2 94.7 60M 98.2 96.3 94.7 100 100 100 82.1 82.2 92.2 87.9 88.1 88. 1 2 4 59M 98.0 95.8 94.4 60M 98.2 96.2 94.7 65M 98.4 96.2 94.9 100 100 100 81.7 82.2 82.4 87.4 88.1 88.2 10 20 60M 97.9 93.5 93.1 60M 98.2 96.2 94.7 60M 98.5 96.2 94.8 100 100 100 80.0 82.2 82.5 84.9 88.1 88.2 Table 4. Denoising step number. Table 5. Cascade stages. Table 6. Number of sampled noises Ninfer. merged LiDAR point cloud derived from five sensors. Annotations are provided at frequency of 2Hz and include both HD maps and object bounding boxes. The dataset is designed to emphasize challenging driving scenarios involving dynamic changes in driving intentions, while deliberately excluding trivial situations such as stationary scenes or constant-speed driving. NAVSIM benchmarks planning performance using nonreactive simulations and closed-loop metrics for comprehensive evaluation. In this paper, we employ the proposed PDM score (PDMS) [9], which is weighted combination of several sub-scores: no at-fault collisions (NC), drivable area compliance (DAC), time-to-collision (TTC), comfort (Comf.), and ego progress (EP). 4.2. Implementation Detail We adopt the same perception modules and ResNet-34 backbone [11] as Transfuser for fair comparison. In the diffusion decoder layer, we employ spatial cross-attention to only interact with BEV features following Transfusers BEV-based setting. We only perform agent cross-attention, since the perception module of Transfuser does not include vectorized map construction. We stack 2 cascade diffusion decoder layers and apply truncated diffusion policy with 20 clustered anchors. The training diffusion schedule is truncated by 50/1000 to diffuse the anchors, while during inference, we use only 2 denoising steps and select the top1 scoring predicted trajectory for evaluation. The training and inference recipe directly follows Transfuser: We use three cropped and downscaled forward-facing camera images, concatenated as 1024256 image, and rasterized BEV LiDAR as input; DiffusionDrive is trained on navtrain split from scratch for 100 epochs with AdamW optimizer on 8 NVIDIA 4090 GPUs with total batch size of 512, setting the learning rate to 6 104. No test-time augmentation is applied and the final output for evaluation on navtest split is 8-waypoint trajectory over 4 seconds. Further details refer to Appendix. 4.3. Quantitative Comparison Tab. 1 compares DiffusionDrive with state-of-the-art methods on NAVSIM navtest split. With the same ResNet34 backbone, DiffusionDrive achieves 88.1 PDMS score, demonstrating significant superior performance over the previous learning-based methods. Compared to VADv2, DiffusionDrive surpasses it by 7.2 PDMS while reducing the number of anchors from 8192 to 20, representing 400 reduction. DiffusionDrive also outperforms HydraMDP, which follows VADv2s sampling-from-vocabulary paradigm, with 5.1 PDMS improvement. Even compared to the Hydra-MDP-V8192-W-EP, which is variant of Hydra-MDP [22] by further training to fit the EP evaluation metric with additional supervision and using weighted confidence post-processing, DiffusionDrive still outperforms it by 3.5 EP and 1.6 overall PDMS, relying solely on straightforward learning-from-human approach without any post-processing. Compared to the Transfuser baseline, where we only differ in the planning module, DiffusionDrive delivers notable 4.1 PDMS improvement, outperforming it across all sub-scores. 4.4. Roadmap into the generative In Tab. 2, converting Transfuser TransfuserDP using vanilla diffusion policy improves the PDMS score by 0.6 and the mode diversity score by 11%. However, it also significantly increases the overhead of the planning module, requiring 20 more denoising steps and 32 the step time, resulting in total 650 increase in runtime overhead. With the proposed truncated diffusion policy, TransfuserTD reduces the number of denoising steps Method Input Img. Backbone ST-P3 [12] UniAD [13] OccNet [34] VAD [17] SparseDrive [32] Camera EffNet-b4 [33] Camera ResNet-101 [11] ResNet-50 [11] Camera ResNet-50 [11] Camera ResNet-50 [11] Camera L2 (m) 3s 2s 2.11 0.70 2.13 0.70 0.58 2.90 1.04 2.99 1.05 0. 1s 1.33 0.45 1.29 0.41 0.29 DiffusionDrive (Ours) Camera ResNet-50 [11] 0.27 0. 0.90 Collision Rate (%) Avg. 1s 2s 3s Avg. FPS 2.11 0.73 2.14 0.72 0.61 0.57 0.23 0.62 0.21 0.07 0.01 0.62 0.58 0.59 0.17 0. 1.27 0.63 1.37 0.41 0.18 0.03 0.05 0.16 0.71 0.61 0.72 0.22 0.08 0. 1.6 1.8 2.6 4.5 9.0 8.2 Table 7. Comparison on nuScenes dataset with open-loop metrics. FPS is measured on single NVIDIA 4090 GPU following the recipe of SparseDrive [32]. Metric calculation follows ST-P3 [12]. from 20 to 2 while achieving an increase of 1.1 in PDMS and 59% improvement in mode diversity. By further incorporating the proposed diffusion decoder, the final model, DiffusionDrive, reaches 88.1 PDMS and 74% mode diversity score D. Compared to the TransfuserDP, DiffusionDrive shows improvements in 3.5 PDMS and 64% mode diversity, and 10 reduction in denoising steps, resulting in 6 speedup in FPS. This enables real-time, high-quality, multimode planning. 4.5. Ablation Study Effect of designs in diffusion decoder. Tab. 3 shows the effectiveness of our design choices in the diffusion decoder. ID-1 is the TransfuserTD in the Tab. 2. By comapring ID-6 and ID-1, we can see that the proposed diffusion decoder reduce the 39% parameters and significantly improves the planning quality by 2.4 PDMS. ID-2 shows severe performance degeneration due to the lack of rich and hierarchical interaction with the environment. By comparing ID-2 and ID-3, we can see that spatial cross-attention is vital for accurate planning. ID-5 shows that the proposed cascade mechanism is effective and can further improve the performance. Denoising step number. Tab. 4 shows that due to the reasonable start point, DiffusionDrive can achieve good planning quality with only 1 step. Further increasing the denoising steps can improve the planning quality, and make it enjoy the flexible inference given the complexity of the environment. Cascade stages. Tab. 5 ablates the impact of cascade stage number. Increasing the stage number can improve the planning quality but saturate at the 4 stages and cost more parameters and inference time at each step. Number of sampled noises Ninfer. As stated in Sec. 3.3, DiffusionDrive can generate varied trajectories by simply sampling variable number of noises from anchored Gaussian distribution. Tab. 6 shows that 10 sampled noises can already achieve decent planning quality. By sampling more noises, DiffusionDrive can cover potential planning action space and lead to improved planning quality. 4.6. Qualitative Comparison Since the PDMS planning metric calculates based on the top-1 scoring trajectory and our proposed score evaluates mode diversity, these metrics alone cannot fully capture the quality of diverse trajectories. To further validate the quality of multi-mode trajectories, we visualize the planning results of Transfuser, TransfuserDP and DiffusionDrive on challenging scenarios of NAVSIM navtest split in Fig. 2. The results indicate that the multi-mode trajectories generated by DiffusionDrive are not only diverse but also of high quality. In Fig. 2a, the top-1 scoring trajectory generated by DiffusionDrive closely resembles the ground-truth trajectory, while the highlighted top-10 scoring trajectory surprisingly tries to perform high-quality lane changing. In Fig. 2b, the highlighted top-10 scoring trajectory also performs lane change, and neighboring low-scoring trajectory further interacts with surrounding agents to effectively avoid collisions. 4.7. Quantitative Comparison on nuScenes dataset The nuScenes dataset is previously popular benchmark for end-to-end planning. Since the major scenarios of nuScenes are simple and trivial situations, we only perform comparison in Tab. 7. We implement DiffusionDrive on top of SparseDrive [32] following its training and inference recipe using open-loop metrics proposed in ST-P3 [12]. We stack 2 cascade diffusion decoder layers and apply the truncated diffusion policy with 18 clustered anchors. As shown in Tab. 7, DiffusionDrive reduces the average L2 error of SparseDrive by 0.04m, achieving the lowest L2 error and average collision rate against previous state-ofthe-art methods. While DiffusionDrive is also efficient and runs 1.8 faster than VAD with 20.8% lower L2 error and 63.6% lower collision rate. 5. Conclusion In this work, we propose novel generative driving decision-making model, DiffusionDrive, for end-to-end autonomous driving by incorporating the proposed truncated diffusion policy and efficient cascade diffusion decoder. DiffusionDrive can denoise variable number of samples from an anchored Gaussian distribution to generate diverse planning trajectories at real-time speeds. Comprehensive experiments and qualitative comparisons validate the superiority of DiffusionDrive in planning quality, running efficiency, and mode diversity."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to acknowledge Tianheng Cheng for helpful feedback on the draft."
        },
        {
            "title": "References",
            "content": "[1] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In ICLR, 2023. 4 [2] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 2 [3] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 1, 2, 6 [4] Zhili Chen, Maosheng Ye, Shuangjie Xu, Tongyi Cao, and Qifeng Chen. Ppad: Iterative interactions of prediction and In ECCV, planning for end-to-end autonomous driving. 2024. 2 [5] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In RSS, 2023. 1, 3, 5, 6 [6] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. TPAMI, 2022. 1, 2, 5, 6 [7] Younwoo Choi, Ray Coden Mercurius, Soheil Mohamad Alizadeh Shabestary, and Amir Rasouli. Dice: Diverse diffusion model with scoring for trajectory prediction. In IV, 2024. [8] OpenScene Contributors. Openscene: The largest up-todate 3d occupancy prediction benchmark in autonomous driving. https://github.com/OpenDriveLab/ OpenScene, 2023. 6 [9] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Navsim: Data-driven nonreactive autonomous vehicle simulation and benchmarking. In NeurIPS, 2024. 2, 6, 7 [10] Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, and Boris Ivanovic. Producing and leveraging online map uncertainty in trajectory prediction. In CVPR, 2024. 2 [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 6, 7, 8 [12] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based auIn tonomous driving via spatial-temporal feature learning. ECCV, 2022. [13] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai In Wang, et al. Planning-oriented autonomous driving. CVPR, 2023. 1, 2, 6, 8 [14] Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. Bevdet: High-performance multi-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021. 1 [15] Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fernandez Fisac. Versatile sceneconsistent traffic scenario generation as optimization with diffusion. arXiv preprint arXiv:2404.02524, 2024. 3 [16] Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In ICLR, 2022. 1, 4 [17] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023. 1, 2, 6, 8 [18] Chiyu Jiang, Andre Cornman, Cheolho Park, Benjamin Sapp, Yin Zhou, Dragomir Anguelov, et al. Motiondiffuser: Controllable multi-agent motion prediction using diffusion. In CVPR, 2023. 3 [19] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, et al. Towards learning-based planning: The nuplan benchmark for real-world autonomous driving. In ICRA, 2024. [20] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. arXiv preprint arXiv:2406.08481, 2024. 2 [21] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022. 1 [22] Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, Jan Kautz, Zuxuan Wu, et al. Hydra-mdp: End-to-end multimodal planning with multitarget hydra-distillation. arXiv preprint arXiv:2406.06978, 2024. 1, 2, 6, 7 [23] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose Alvarez. Is ego status all you need for openloop end-to-end autonomous driving? In CVPR, 2024. 2 [24] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. MapTR: Structured modeling and learning for online vectorized HD map construction. In ICLR, 2023. 1 [25] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Maptrv2: An end-to-end framework for online vectorized hd map construction. IJCV, 2024. 1 for autonomous driving and zero-shot instruction following. In CVPR, 2024. [42] Wenhao Yu, Jie Peng, Huanyu Yang, Junrui Zhang, Yifan Duan, Jianmin Ji, and Yanyong Zhang. Ldp: local diffusion planner for efficient robot navigation and collision avoidance. arXiv preprint arXiv:2407.01950, 2024. 4 [43] Chengran Yuan, Zhanqi Zhang, Jiawei Sun, Shuo Sun, Zefan Huang, Christina Dao Wen Lee, Dongen Li, Yuhang Han, Anthony Wong, Keng Peng Tee, et al. Drama: An efficient end-to-end motion planner for autonomous driving with mamba. arXiv preprint arXiv:2408.03601, 2024. 6 [44] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy. In RSS, 2024. 4 [45] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multipleobject tracking with transformer. In ECCV, 2022. 1 [46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 2021. [47] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In ECCV, 2022. 1 [48] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In ECCV, 2024. 2 [49] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray. Language-guided traffic simulation via scene-level diffusion. In CoRL, 2023. 3 [50] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In ICRA, 2023. [51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021. 2, 6 [26] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and Sparse4d: Multi-view 3d object detecarXiv preprint Zhizhong Su. tion with sparse spatial-temporal fusion. arXiv:2211.10581, 2022. 6 [27] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and Hang Zhao. Vectormapnet: End-to-end vectorized hd map learning. In ICML, 2023. 1 [28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 2 [29] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 5, 6 [30] Ajay Sridhar, Dhruv Shah, Catherine Glossop, and Sergey Levine. Nomad: Goal masked diffusion policies for navigation and exploration. In ICRA, 2024. 4 [31] Maria Stamatopoulou, Jianwei Liu, and Dimitrios Kanoulas. Dippest: Diffusion-based path planner for synthesizing traarXiv preprint jectories applied on quadruped robots. arXiv:2405.19232, 2024. 4 [32] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, HaoSparsedrive: End-to-end auran Wu, and Sifa Zheng. tonomous driving via sparse scene representation. arXiv preprint arXiv:2405.19620, 2024. 2, 6, 8 [33] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019. 8 [34] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, et al. Scene as occupancy. In ICCV, 2023. 8 [35] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin Solomon. Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In CoRL, 2022. 1, [36] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In CVPR, 2024. 2 [37] Yixiao Wang, Chen Tang, Lingfeng Sun, Simone Rossi, Yichen Xie, Chensheng Peng, Thomas Hannagan, Stefano Sabatini, Nicola Poerio, Masayoshi Tomizuka, et al. Optimizing diffusion models for joint trajectory prediction and controllable generation. In ECCV, 2024. 3 [38] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for realtime autonomous driving. In CVPR, 2024. 2, 6 [39] Zehang Weng, Haofei Lu, Danica Kragic, and Jens Lundell. Dexdiffuser: Generating dexterous grasps with diffusion models. arXiv preprint arXiv:2402.02989, 2024. 4 [40] Sixu Yan, Zeyu Zhang, Muzhi Han, Zaijin Wang, Qi Xie, Zhitian Li, Zhehan Li, Hangxin Liu, Xinggang Wang, and Song-Chun Zhu. M2diffuser: Diffusion-based trajectory optimization for mobile manipulation in 3d scenes. arXiv preprint arXiv:2410.11402, 2024. 4 [41] Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, TsungWei Ke, Ayush Jain, Jeff Schneider, and Katerina Fragkiadaki. Diffusion-es: Gradient-free planning with diffusion"
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
        "School of EIC, Huazhong University of Science & Technology"
    ]
}