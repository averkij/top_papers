{
    "paper_title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
    "authors": [
        "Zhenhailong Wang",
        "Xuehang Guo",
        "Sofia Stoica",
        "Haiyang Xu",
        "Hongru Wang",
        "Hyeonjeong Ha",
        "Xiusi Chen",
        "Yangyi Chen",
        "Ming Yan",
        "Fei Huang",
        "Heng Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO."
        },
        {
            "title": "Start",
            "content": "Perception-Aware Policy Optimization for Multimodal Reasoning Zhenhailong Wang1*, Xuehang Guo1*, Sofia Stoica1, Haiyang Xu2, Hongru Wang1, Hyeonjeong Ha1, Xiusi Chen1, Yangyi Chen1, Ming Yan2, Fei Huang2, Heng Ji1 1University of Illinois Urbana-Champaign 2Alibaba Group *Equal contribution Corresponding author wangz3@illinois.edu, shuofeng.xhy@alibabainc.com, hengji@illinois.edu 5 2 0 2 8 ] . [ 1 8 4 4 6 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify unique loss hacking issue, which we rigorously analyze and mitigate through Double Entropy Loss. Overall, our work introduces deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for new RL framework that encourages visually grounded reasoning. Project page: https: //mikewangwzhl.github.io/PAPO."
        },
        {
            "title": "Introduction",
            "content": "Large Multimodal Models (LMMs) have achieved remarkable success across wide range of vision-language tasks, including image captioning, visual question answering, and embodied multimodal agents. However, LMMs continue to struggle with complex multimodal reasoning tasks that demand both fine-grained perception and multi-step inference. This limitation stands in contrast to the strong reasoning performance of text-only Large Language Models (LLMs) in purely textual domains. key factor behind LMMs success is Reinforcement Learning with Verifiable Rewards (RLVR), framework recently shown to enhance reasoning capabilities in LLMs. RLVR has been popularized by models such as DeepSeek-R1 and algorithms like Group Relative Policy Optimization (GRPO), computationally effiFigure 1: Comprehensive error-type breakdown and inference example between GRPO and PAPO. We observe that perception errors account for the majority (67%) of failures in current multimodal reasoning models trained with GRPO. PAPO significantly reduces these dominant perceptiondriven errors. On the right, we present representative inference example that illustrates how PAPOs enhanced perception enables correct reasoning outcomes. cient variant of Proximal Policy Optimization (PPO) that has achieved strong results in mathematical reasoning. Aiming to address this gap and inspired by RLVR algorithms success with LLMs, growing body of work (Chen et al. 2025a; Shen et al. 2025a; Meng et al. 2025b; Huang et al. 2025; Yang et al. 2025; Liu et al. 2025b; Wang et al. 2025b; Xiao et al. 2025a; Zhu et al. 2025b; Wang et al. 2025a; Liang et al. 2025; Xia et al. 2025; Xiao et al. 2025b; Shen et al. 2025b; Wan et al. 2025; Yao et al. 2025) has explored applying RLVR to LMMs in hopes of similarly improving their multimodal reasoning abilities. Initial successes have been reported, particularly in terms of generalization ability when using GRPO compared to supervised finetuning (Chen et al. 2025a; Shen et al. 2025a; Huang et al. 2025). However, most prior efforts have primarily focused on improving data and rollout quality (Zhou et al. 2025; Li et al. 2025b; Wang et al. 2025a; Li et al. 2025c; Liu et al. 2025b; Liang et al. 2025; Yao et al. 2025) or reward design (Wang et al. 2024; Singh et al. 2025; Xiao et al. 2025a; Xia et al. 2025) leaving the core optimization algorithm largely unchanged. Other algorithmic modifications (Yu et al. 2025; Wang et al. 2025b; Yue et al. 2025) often follow empirical practices from text-only models, without rethinking the algorithm from multimodal perspective. This raises two fundamental research questions: (1) Are there unique challenges in multimodal reasoning that do not arise in text-only settings? (2) If so, how can we design better RLVR algorithm grounded in multimodal domains? To investigate the first question, we conducted comprehensive error analysis on multimodal reasoning model trained using the standard GRPO pipeline. We manually examined 200 error cases across four benchmarks (Lu et al. 2021; Zhang et al. 2024; Meng et al. 2025b; Xiao et al. 2024) and categorized the types of errors. Surprisingly, as shown in Figure 1, we found that 67% of the errors stemmed from perception. In many cases, the model performed well in logical or algebraic reasoning but failed to accurately interpret visual inputs, such as spatial relationships or label associations. We hypothesize that this bottleneck arises because existing RLVR algorithms do not explicitly incentivize models to ground their reasoning in the visual input. In prior work, rewards typically include: (1) format reward that encourages structured reasoning (e.g., producing <think> block followed by an <answer> block), and (2) an accuracy reward that evaluates the final answer against ground truth. However, neither of these rewards ensure the model actually leverages the visual input during reasoning. Recent approaches (Xia et al. 2025; Xiao et al. 2025a) aimed at improving perception either introduce dedicated reward to directly assess perception quality or require the model to describe the visual input before reasoning. While promising, these strategies often impose rigid separation between perception and reasoning, which can be suboptimalparticularly when the image is difficult to describe in natural language. They also typically rely on an additional neural-based reward model, resulting in significant computational overhead. Another line of work (Zhu et al. 2025b; Zheng et al. 2025) tackles the problem from different perspective using tool-use, but still does not address the core optimization algorithm. In this work, we propose Perception-Aware Policy Optimized (PAPO), novel RLVR algorithm that enhances multimodal reasoning through visually grounded optimization. PAPO is simple extension of GRPO that directly incorporates perception-aware internal supervision signal into its training objective. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we define an information gain ratio (rprcp) between the probabilities of generating rollout given question and either the original image or corrupted version Imask: rprcp = p(oq,I) p(oq,Imask) . In practice, creating Imask can be as simple as randomly masking substantial portion of patches in I. The core idea is based on the following intuition: well-learned visually grounded reasoning model should exhibit high rprcp, reflecting its reliance on informative visual content to generate the responses. More concretely, we introduce an additional KullbackLeibler divergence (KL) objective, Implicit Perception Loss (KLprcp), which we maximize within the GRPO framework. This KL loss is computed between the models outputs conditioned on the original and corrupted images. Despite its simplicity, PAPO delivers consistent improvements over GRPO across eight multimodal reasoning benchmarks (Lu et al. 2021; Zhang et al. 2024; Meng et al. 2025b; Xiao et al. 2024; Lu et al. 2023; Qiao et al. 2024; Li et al. 2023; Yue et al. 2024) with an average gain of 4.4%. The improvement is particularly pronounced (8.0%) in tasks that heavily depend on visual grounding and where the input question provides limited visual clues. Incorporating PAPO with removing reference KL leads to compounded improvements up to 11.2% on 3B models. Furthermore, We observe significant 30.5% reduction in perception-related errors with PAPO, as evidenced by the manual analysis shown in Figure 1. Finally, PAPO also shows faster convergence with early-stage gains starting around 25 steps. However, due to the unbounded nature of the KL objective, setting the coefficient of the Implicit Perception Loss too high may cause PAPO to over-optimize the KLprcp term, effectively hacking the objective. This leads to drastic drop in the Double Entropy Loss loss and collapsed rewards. To mitigate this, we present an in-depth analysis of this failure mode and propose effective regularizations. To summarize, our main contributions are twofold: We present PAPO, simple extension of GRPO that incentivizes the model to generate visually grounded responses by introducing novel Implicit Perception Loss. To our knowledge, this is the first work to explore deeper integration of perception-aware supervision signals beyond data or reward-level modifications. We conduct an extensive analysis of PAPO and identify potential failure modes, which we mitigate through proposing novel Double Entropy Loss."
        },
        {
            "title": "2 Preliminary\n2.1 Group Relative Policy Optimization (GRPO)",
            "content": "GRPO (Shao et al. 2024) is variant of the Proximal Policy Optimization (PPO) (Schulman et al. 2017) algorithm that removes the value model and estimates advantages via group-based computation. GRPO has been popularized by DeepSeek-R1(Guo et al. 2025) due to its improved computational efficiency and effectiveness in learning long chain-ofthought (CoT) reasoning with verifiable rewards. Although originally proposed on text-only language models, recent work (Chen et al. 2025a; Liu et al. 2025d; Huang et al. 2025; Meng et al. 2025b; Shen et al. 2025a) has shown promise in directly applying GRPO to multimodal models. In the context of multimodal reasoning, consider dataset containing datapoints consisting of visual inputs I, questions q, and ground truth answers a. The GRPO learning objective with respect to the policy πθ can be written as follows, where θ represents the parameters in large multilmodal model: JGRPO(θ) = [{oi}G i=1πθold (Oq,I)] (cid:104) min t=1 (cid:105) ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵl, 1 + ϵh) ˆAi,t i=1 (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "1 oi oi (cid:88) (cid:110) βDKL [πθπref ] (cid:111) (1) with ri,t(θ) = πθ(oi,tq, I, oi,<t) πθold (oi,tq, I, oi,<t) denotes the size of the group which contains multiple responses sampled from the rollout policy πθold , corresponding to one input instance (q, I). ϵl, ϵh are hyperparameters for clipping too large updates. The original GRPO (Shao et al. 2024) sets ϵl = ϵh, while recent work such as DAPO (Yu et al. 2025) shows benefits of clip-higher, i.e., ϵh > ϵl. We follow the clip-higher configuration in all of our experiments. The token-level advantage ˆAi,t is defined as the sequence-level reward (cid:101)Ri normalized across the group. Given reward verifier eq(), which checks whether response is equivalent to the ground truth, the advantage is computed as follows: ˆAi,t = (cid:101)Ri = Ri mean(R) std(R) , where Ri = (cid:40)1.0, if eq(a, oi), 0.0, otherwise. where = {R1, R2, . . . , RG} is the rewards for the current group."
        },
        {
            "title": "Multimodal Reasoning Models",
            "content": "We first investigate the question: Are there unique challenges in multimodal reasoning that do not arise in textonly settings? We follow typical GRPO pipeline to train Qwen2.5-VL-3B (Qwen Team 2024a) on ViRL39K (Wang et al. 2025a) (experimental details can be found in 4) and manually examine and categorize error types based on 200 error instances sampled from four benchmarks: Geometry3K (Lu et al. 2021), MMK12 (Meng et al. 2025b), LogicVista (Xiao et al. 2024), and MathVerse (Zhang et al. 2024). We identify four dominant error types: Perception Error: Inaccurate interpretation of the visual content. For example, in Figure 1, the model associates with the wrong side. Reasoning Error: Mistakes in the logical inference process, such as applying incorrect rules or theorems. Calculation Error: Mistakes in performing arithmetic operations. Inconsistency Error: Discrepancies between intermediate reasoning steps and the final answer. We show the error distribution in Figure 1. To our surprise, we find that the majority of errors, 67.0%, stem from poor perception. We hypothesize that this bottleneck in perception is due to the GRPO objective not providing any incentive for the model to generate visually grounded responses. In particular, when the training data contains samples where the visual information is partially described in text, the model may learn to take shortcuts in the reasoning process by ignoring the visual input. We observe that this is common problem even in mainstream multimodal evaluation benchmarks, such as MathVista (Lu et al. 2023) (See Appendix A). This leads us to key question: can we learn to reason while also learning to perceive? We present our approach in the next section."
        },
        {
            "title": "3.2 PAPO\nTo address the aforementioned unique challenges in multi-\nmodal RLVR, we propose Perception-Aware Policy Opti-\nmized (PAPO), a simple yet effective extension to GRPO\ndesigned to encourage visually grounded reasoning. The key\nidea behind PAPO is to regularize the model’s policy to pre-\nfer responses that depend on meaningful visual content.\nWe quantify the model’s reliance on the visual modality by\nmeasuring how much the generated response distribution\nshifts when the visual input is corrupted. Based solely on\ninternal supervision signals, PAPO requires no additional\nannotations, no reliance on stronger teacher models, and no\nexpensive neural reward models. We formally describe the\nkey components of the PAPO algorithm as follows. Figure 2\nshows an illustrative overview of the algorithm.",
            "content": "Implicit Perception Loss (KLprcp). To indicate whether generated response depends on meaningful visual information, we define the following ratio: rprcp(θ) = πθ(o q, I) πθ(o q, Imask) (2) where is generated sequence of tokens, is the question and is the original visual input. And Imask is defined as corrupted visual input, which is constructed by masking out sufficiently large portion of the original input. Figure 2 shows an example of Imask where 60% of the patches are masked (see Appendix for more in-depth exploration of masking techniques). From an information gain (Shannon 1948) perspective, this ratio quantifies the degree to which the models output distribution changes when meaningful visual information is removed. higher ratio indicates that the model assigns significantly lower probability to the correct output when deprived of full visual context, suggesting that the visual input contributes substantial information to the decision. Conversely, low ratio implies that the models prediction remains largely unaffected by masking, indicating that it may Figure 2: Illustration of the PAPO objective, which extends GRPO by adding the Implicit Perception Loss (KLprcp). The KLprcp is formulated as maximizing the difference between the original policy πθ and corrupted policy πmask , computed with masked visual input. Note that the rollout is performed only once, generated from the original input. Both policies are evaluated based on the log probabilities over the same rollout response tokens, but with different visual inputs. We follow the same format and accuracy-based rewards as in GRPO. Intuitively, PAPO encourages the model to produce visually grounded responses while still achieving high rewards. θ rely primarily on the textual input rather than truly grounded visual understanding. Thus, intuitively, for well-behaved multimodal policy model θ, we want rprcp(θ) to be high. Based on this intuition, we introduce an additional loss to the GRPO objective, the Implicit Perception Loss (KLprcp), which is implemented by maximizing the following KullbackLeibler (KL) divergence: DKL[πθπmask θ ] = DKL[πθ(oq, I) πθ(oq, Imask)] (3) Combining this with the GRPO objective yields the simplest form of the PAPO objective: JPAPO(θ) = [{oi}G i=1πθold (Oq,I)] (cid:104) min t=1 (cid:105) ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵl, 1 + ϵh) ˆAi,t i= (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "1 oi oi (cid:88) (cid:110) βDKL [πθπref ] +γDKL[πθπmask θ ] (cid:111) with DKL[πθπmask θ ] = rprcp (θ) log rprcp (θ) 1, (4) where γ is loss-weighting coefficient and indexes the ith rollout response. We implement KLprcp using the approximation method introduced by (Schulman 2020), which is also used for the KL-penalty term DKL(πθπref ) against the reference model, as in GRPO (Shao et al. 2024). Masking Strategy. We investigate two strategies for creating the corrupted visual input Imask for the KLprcp loss: (1) random masking and (2) semantic-aware masking. For both strategies, we first set target masking ratio (e.g., 60 %), which determines the fraction of patches to be masked. We adopt patch-based masking rather than pixel-level noise (e.g., adding Gaussian noise) because patch-based masking more effectively removes informative semantic content, whereas pixel-level noise typically preserves semantics even at high noise levels (see for comparison). In random masking, patches are selected uniformly. In semantic-aware masking, we leverage self-supervised, pretrained vision encoder (DINOv2 (Oquab et al. 2023)) to identify salient patches: we aggregate its patch-level selfattention scores and select the top-scoring patches (see Appendix for details). Empirically, we find that random masking yields better performance with negligible computational overhead (see further discussion in 5.2). Unless otherwise specified, PAPO uses random masking. Regularizing KLprcp with Double Entropy Loss. Empirically, we observe unique form of model collapse that can occur when training with an excessively large Implicit Perception Loss weighting (γ), which we refer to as KLprcp Hacking. We include an in-depth analysis of KLprcp Hacking in 5.4, identifying the key factors that lead to collapse, its early signs, and different prevention strategies. Intuitively, because we maximize KL divergence that is theoretically unbounded, the model may hack KLprcp to drive the policy gradient loss toward zero. When the hacking occurs, the model takes progressively larger update steps and suffers drastic drop in rewards. To better regularize PAPO in highγ settings, we introduce Double Entropy Loss, an effective regularizer that prevents collapse while preserving performance. This idea stems from our observation that rising rollout entropy in both πθ and πmask is an early sign of collapse. θ Double Entropy Loss encourages the model to keep both entropies low, and can be expressed as: . . . +η1H(cid:2)πθ (cid:3) + η2H(cid:2)πmask JPAPO(θ) = . . . (cid:3)(cid:111) , (5) (cid:110) θ where denotes the entropy loss, computed as the negative log probability of the generated sequence, and the . . . part is identical to Equation 4. Ablation results on different regularization approaches, including single-entropy losses, are presented in 5.4."
        },
        {
            "title": "4.1 Experimental Setup\nTraining Dataset. We use ViRL39K (Wang et al. 2025a),\na diverse collection of 38.8K multimodal reasoning QA\npairs covering problems in math, STEM, charts, and so-\ncial topics. Note that the training data includes only input\nqueries and final answers, without intermediate chain-of-\nthought (CoT) annotations.",
            "content": "Models. We employ the Qwen2.5-VL-3B (Qwen Team 2024a) and Qwen2.5-VL-7B (Qwen Team 2024b) as our base models. We consider the following main model variants and default configurations: GRPO: Main baseline for comparison, with clipping factors set to ϵl = 0.2, ϵh = 0.3, and reference KL penalty coefficient set to β = 0.01. PAPO: PAPO with lower KLprcp coefficient γ = 0.01; masking ratio set to 0.6; other configurations are identical to GRPO. PAPOH: PAPO with higher γ = 0.02. By default, PAPOH-7B additionally includes Double Entropy Loss (η1 = η2 = 0.05) as regularization to prevent KLprcp Hacking, while the 3B model does not. All other configurations are identical to PAPO. In 5.2, we include detailed ablations on these key configurations. In 5.3, we further investigate variant where we remove the reference KL penalty, i.e., setting β = 0.0, to study the incorporation of PAPO with prior algorithmic improvement practices (Yu et al. 2025) on text-only models. Implementation Details. We conduct RLVR on all model variants using the following typical response format, where reasoning steps are enclosed in <think></think> and the final answer is enclosed in boxed{}. Each model is trained for 2 epochs with learning rate of 1e6 and weight decay of 1e2."
        },
        {
            "title": "4.2 Evaluation\nTo systematically compare the effectiveness of GRPO and\nPAPO, we conduct experiments and ablation studies on\neight benchmarks that cover diverse multimodal reasoning\nproblems, including:\n• Math and Geometric Reasoning: Geometry3K (Lu\net al. 2021), MathVista (Lu et al. 2023), Math-\nVerse (Zhang et al. 2024), and We-Math (Qiao et al.\n2024).",
            "content": "Figure 3: Comparison of the training dynamics of GRPO and PAPO. Solid lines indicate running averages with stepping window size of 20. PAPO demonstrates consistently faster learning from the early stages. Multi-discipline Multimodal Reasoning: MMMUPro (Yue et al. 2024). Logical Reasoning: LogicVista (Xiao et al. 2024). Counting: SuperClevr Counting (Li et al. 2023). All evaluation metric are based on exact match against ground truth answer. We report average accurarcy @ 8 for all benchmarks with inference temperature of 1.0. We omit datasets or instances with free-form answers that require LLM-as-a-judge evaluation. Analysis on Vision Dependency. As also discussed in (Zhang et al. 2024; Yue et al. 2024), we observe that not all mainstream multimodal benchmarks are guaranteed to have visually dependent problems. That is, some reasoning tasks may rely heavily on textual content and do not require the visual content for deriving the answer. For example, Figure 8 exhibits VQA problems of different vision-dependency levels, highlighting the varying degrees of reliance on visual information in multimodal question answering. To this end, we conduct manual screening of the included benchmarks and identify the following two splits: Vision-Dependent Multimodal Reasoning: subset where the instances are guaranteed to require proper interpretation of the visual input. Specifically, the VisionDependent subset includes SuperClevr Counting; LogicVista; the vision-centric subsets in MathVerse, which we refer to as MathVerseV ; and MMMU-pro, which is constructed following this philosophy. General Multimodal Reasoning: All other benchmarks, regardless of how heavily the model is required to pay attention to the visual input during its reasoning and answering process. We report scores for both categories and provide additional insights into the impact of PAPO. Model Method General Multimodal Reasoning Vision-Dependent Multimodal Reasoning Overall Geo3k MathVista We-Math MMKI2 MathVerse AVG % rel LogicVista Counting MMMU-Pro MathVerseV AVG % rel AVG % rel 3B 7B GRPO PAPO PAPOH GRPO PAPO PAPOH 28.72 30. 30.95 40.18 41.85 40.25 59.34 58. 61.38 65.48 66.63 69.53 58.90 59. 60.09 68.12 68.86 66.79 57.24 57. 57.39 72.26 72.48 72.52 55.25 56. 57.14 66.51 68.00 68.43 51.89 52.53 1.73 53.39 3.38 62.51 63.56 1.91 1.53 63. 38.14 39.84 38.67 45.62 47.68 46. 55.81 61.38 62.56 73.94 82.38 89. 25.66 26.00 27.11 35.17 35.73 36. 52.26 53.45 53.95 61.71 64.15 64. 42.97 47.92 45.17 4. 49.26 2.97 45.57 5.60 49.92 4.36 54.11 58.78 57.49 5.37 60.86 3. 59.37 7.96 61.66 4.39 Table 1: Performance (avg@8 acc %) comparison of Qwen2.5-VL-3B and 7B models between GRPO and PAPO on general and more vision-dependent multimodal reasoning tasks. MathVerseV refers to the vision-centric subset of MathVerse (Zhang et al. 2024). PAPOH denotes the high KLprcp weighting (γ = 0.02) variant of PAPO (γ = 0.01). % rel indicates the averaged relative gain over GRPO for each task. We observe consistent improvements with both PAPO and PAPOH, with gains approaching 8%, especially on heavily vision-dependent tasks."
        },
        {
            "title": "GRPO",
            "content": "PAPO @0.005 PAPO @0.01 PAPO @0.02 PAPO @0.04 (collapsed)"
        },
        {
            "title": "AVG",
            "content": "51.89 52.40 52.53 53.39 31.24 % rel 1.19 1.73 3.38 43."
        },
        {
            "title": "AVG",
            "content": "42.97 43.73 45.17 45.57 38.31 % rel 1.92 4.52 5.60 14."
        },
        {
            "title": "AVG",
            "content": "47.92 48.55 49.26 49.92 34.38 % rel 1.51 2.97 4.36 28.46 Table 2: Impact of KLprcp loss weighting. Performance comparison on Qwen2.5-VL-3B trained with PAPO using different values of γ. Increasing γ up to 0.02 generally improves performance, while an excessively large γ leads to model collapse (see detailed discussion in 5.4). The analysis of the effects of γ on 7B models are discussed in Figure 6."
        },
        {
            "title": "5.1 Main Results\nPAPO consistently outperforms GRPO for multimodal\nreasoning. We present the main results on both 3B and 7B\nbase models in Table 1. The ∆%\nrel denotes the average rela-\ntive gain against the GRPO baseline across all tasks. With-\nout any additional assumptions, PAPO outperforms GRPO\non all eight benchmarks, with a ∆%\nrel of 4.36% and 4.39%\nfor 3B and 7B, respectively. In Figure 3, we further present\na comparison of training dynamics based on the overall re-\nwards on ViRL39K. PAPO showcases faster learning even\nfrom the early steps.",
            "content": "More significant improvements on vision-dependent reasoning. The performance gains are more pronounced on the vision-dependent subset, leading to relative gain of 5.60% and 7.96% for 3B and 7B, respectively. This aligns with our expectation regarding the impact of Implicit Perception Loss, as it encourages visually dependent responses. Significantly reduced perception errors. We further conduct comprehensive qualitative study on the error distribution, following the setup detailed in 2.2. Figure 1 shows before-and-after comparison of errors with GRPO and PAPO. We observe significant reduction in perception errors, demonstrating the effectiveness of addressing the per-"
        },
        {
            "title": "Method",
            "content": "AVG % rel AVG % rel AVG % rel"
        },
        {
            "title": "GRPO Baselines",
            "content": "3B 7B 3B 7B 3B"
        },
        {
            "title": "GRPO\nGRPO",
            "content": "51.89 62.51 42.97 54.11 47.92 58."
        },
        {
            "title": "Impact of Masking Strategy on PAPO",
            "content": "random @0.6 semantic @0.6 random @0.6 semantic @0.6 52.53 52.13 63.56 63.39 1.73 0.34 1.91 1. 45.17 43.78 57.49 56.83 4.52 1.88 5.37 3.89 49.26 48.42 60.86 60. 2.97 1.02 3.55 2."
        },
        {
            "title": "Impact of Masking Ratio on PAPO",
            "content": "random @0.4 random @0.6 random @0.8 random @1.0 52.51 52.53 52.57 52.13 1.55 1.73 1.49 0.71 44.12 45.17 44.24 43.98 2.29 4.52 2.69 2.31 48.78 49.26 48.87 48. 1.88 2.97 2.02 1.42 Table 3: Impact of masking strategy and ratio. Performance comparison of PAPO using different approaches for constructing Imask. Despite its simplicity, random masking empirically outperforms semantic-aware masking. sufficiently large masking ratio (e.g., 0.6 or 0.8) yields stronger performance, while ratios that are too low (e.g., 0.4) or too high (e.g., 1.0) are less effective. See detailed discussion in 5.2. ception bottleneck in GRPO for multimodal reasoning."
        },
        {
            "title": "5.2 Ablation on Key Design Choices\nImpact of Implicit Perception Loss weighting. We ab-\nlate on the choice of γ, which is the weighting coefficient of\nKLprcp as shown in Equation 4. Table 2 presents the perfor-\nmance comparison based on Qwen2.5-VL-3B when varying\nγ from 0.005 to 0.04. We summarize our findings as follows:",
            "content": "larger γ under 0.02 tends to result in more pronounced improvements, especially on more visually-dependent tasks. γ should not be set too large (e.g., 0.04), as it causes severe model collapse that cannot be regularized even with Double Entropy Loss. We also observe that larger models"
        },
        {
            "title": "Overall",
            "content": "AVG % rel"
        },
        {
            "title": "AVG",
            "content": "% rel"
        },
        {
            "title": "AVG",
            "content": "% rel 3B GRPO GRPO + No KLref PAPO + No KLref 51.89 53.96 56.21 4.75 9.26 42.97 45.46 49. 5.37 13.60 47.92 50.18 53.15 5.03 11.19 7B GRPO GRPO + No KLref PAPO + No KLref 62.51 63.99 63. 2.05 1.15 54.11 57.94 59.18 5.36 7.54 58.78 61.30 61.47 3.53 3.99 Table 4: Performance comparison between GRPO and PAPO with the reference KL penalty removed. For PAPO, we add Double Entropy Loss with coefficient of 0.03 for both 3B and 7B models. We find that PAPO is highly compatible with this setting, achieving further improvements with an average relative gain of 13.6%. are more sensitive to higher γ values and require earlier regularization as shown in Figure 6. We further discuss the impact of γ on KLprcp Hacking in 5.4. Without additional regularization, setting γ = 0.02 for 3B models and γ = 0.01 for 7B models serves as good default. Impact of Masking Ratio and Strategy. We investigate the most effective way to corrupt the original visual input to maximize the benefit of PAPO. In Table 3, we compare both masking strategies, i.e., random masking vs. semanticaware masking, and masking ratios, which control the percentage of patches to be masked. Implementation details for the masking strategies are provided in Appendix B. summary of findings is as follows: Despite its simplicity, random masking empirically outperforms semantic-aware masking. As illustrated in Figure 9, semantic-aware masking tends to obscure entire regions of salient objects. We hypothesize that semanticaware masking underperforms because, as illustrated in Figure 4, it tends to obscure entire salient regions, causing the model to attend to all objects indiscriminately instead of focusing on the most informative parts. Masking sufficiently large portion of the image, e.g., 0.6 to 0.8, results in best performances. However, using completely blackened image is not favorable, as it encourages the model to attend to the image regardless of its content. We also observe that complete blackening is more likely to cause KLprcp Hacking (detailed in 5.4)."
        },
        {
            "title": "5.3 PAPO + Remove Reference KL\nIn this section, we further investigate the compatibility of\nPAPO with other algorithmic modifications (Yu et al. 2025)\nfrom GRPO that have been empirically verified primarily\non language-only models, such as removing the reference\nKL penalty (referred to as KLref). Under this setting, we re-\nmove the DKL[πθ||πref ] term from the PAPO training ob-\njective (Eq 4). We also find that the proposed Double En-",
            "content": "Figure 4: Collapsing behavior. distinctive generation pattern in collapsed models is the production of irrelevant tokens. We verify this quantitatively by prompting GPT-4.1mini (OpenAI 2025) to provide relatedness scores of the responses from 0 to 10 for GRPO and collapsed PAPO 7B model. We further compare the variance of KLprcp over the response tokens. As illustrated, the collapsed PAPO model exhibits significantly lower relatedness scores and higher variance across tokens in KLprcp. See 5.4 for detailed discussion. tropy Loss becomes particularly important in this setting to prevent KLprcp Hacking, as removing KLref imposes weaker regularization on πθ. In Table 4, we compare GRPO + No KLref with PAPO + No KLref, using γ = 0.01 and Double Entropy Loss with η1 = η2 = 0.03. The results show that combining PAPO with Remove Reference KL can further boost performance, with improvements especially pronounced on 3B models, achieving an average relative gain of over 10%."
        },
        {
            "title": "5.4 Deep Dive on the KLprcp Hacking\nIn this section, we aim to gain a deeper understanding of\nKLprcp Hacking, a unique failure mode where the model\nover-optimizes the implicit perception loss. We present our\nfindings on: (1) the model’s generation behavior after col-\nlapsing due to the hacking; (2) early signs indicating a model\ncollapse; (3) the most influential factors contributing to the\nhacking; and (4) regularization approaches to prevent or de-\nlay its occurrence.",
            "content": "Collapsing behavior. We first examine how the model behaves after collapsing in terms of its generation. We manually examine the generated tokens of collapsed model, 7B PAPO with γ = 0.02 (without regularization), and non-collapsing model, 7B GRPO. As shown in Figure 4, we observe notably abnormal generation behavior, namely, tendency to generate entirely unrelated tokens during reasoning. We quantitatively verify this by leveraging GPT-4.1mini (OpenAI 2025) as judge to score the relatedness of Figure 5: Early signs of model collapsing due to KLprcp Hacking. The No Collapse and Collapsed models refer to 7B PAPO (γ = 0.01) and 7B PAPOH (γ = 0.02) without regularization, respectively. When collapsing occurs, we notice (a-b) the Implicit Perception Loss drops drastically, accompanied by collapsing training reward, (c) the clipping ratio-high continuously increases, which indicates the proportion of tokens undergoing policy gradient updates beyond the clipped threshold and (d-e) the entropy loss increases in both the masked policy πmask and the original policy πθ. θ Figure 6: Influential factors towards KLprcp Hacking. By default, PAPO uses γ = 0.01, and PAPOH uses γ = 0.02. We identify three main factors: (a) KLprcp weighting (higher values lead to greater likelihood of collapse.) (b) size (the larger the model, the more likely it is to collapse) (c) an extreme masking ratio (e.g., 1.0) results in faster collapse. the models response to the input query from 0 to 10. As presented in the bottom left of Figure 4, the collapsed model shows significantly lower relatedness (a point estimate of about 18% lower). See Appendix for more details on this experiment. We also notice that the variance of the KLprcp loss (Figure 4 bottom right) across the response tokens is about 8.4 times higher in the collapsed model. This again indicates that the model learns to hack the KLprcp loss by generating unrelated tokens that cause large discrepancies between the outputs with and without the image corruption. Early signs of KLprcp Hacking. As shown in Figure 5, when the hacking occurs, the model exhibits drastic decrease in Implicit Perception Loss, accompanied by collapsing training rewards. We find that an increasing Clipping Ratio-High, which indicates growing proportion of tokens undergoing policy gradient updates beyond the clipped threshold, is an early sign of collapse. We also observe an interesting pattern: the entropy loss for both the original policy πθ and the corrupted policy πmask increases as the collapse θ unfolds. This observation inspires our regularization strategies, which are detailed later in this section. Influential factors towards KLprcp Hacking. In Figure 6, we summarize three main factors that make the model more prone to KLprcp Hacking: Model size: Larger models tend to be more sensitive to hacking under the same configuration. For example, setting γ = 0.02 causes collapse on 7B models but not on 3B models. (See Figure 6 a.) Loss weighting: higher KLprcp weighting, such as 0.04, is more likely to lead to collapse. (See Figure 6 b.) Masking ratio: Using an extreme masking ratio, such as masking the entire image to black, leads to faster collapse. (See Figure 6 c.) Regularization approaches for preventing KLprcp Hacking. Inspired by the aforementioned findings, we investisual input. We report the averaged wall-clock time for each training step and the additional forward pass comparing GRPO and PAPO as follows. 3B and 7B experiments are conducted on 2 and 4 Nvidia H100 GPUs respectively. Model Method Per Step (s) Additional Forward Pass (s) 3B 7B"
        },
        {
            "title": "GRPO\nPAPO",
            "content": "360.9 428.1 258.5 367.1 - 48.8 - 49.7 Table 6: Analysis of computational overhead. We report the average time per training step (in seconds) and the time spent on the additional forward pass in PAPO. The experiments are conducted using 2 and 4 NVIDIA H100 GPUs for the 3B and 7B models, respectively. While we observe moderate increase in training time per step67.2 seconds for Qwen 3B and 108.6 seconds for Qwen 7Bwe leave further optimization of training efficiency to future work."
        },
        {
            "title": "6.1 Reinforcement Learning with Verifiable",
            "content": "Rewards (RLVR) Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have demonstrated its effectiveness in enhancing reasoning and instruction-following capabilities in language models. When framed as KL-regularized contrastive learning, methods like GRPO provably amplify policy success probabilities across training iterations (Shao et al. 2024). DAPO (Yu et al. 2025) proposes series of algorithmic improvements, such as removing the reference KL penalty, which results in improved performance over GRPO. Further extending these benefits, RISE (Liu et al. 2025a) equips models with self-verification capabilities through verifiable feedback, improving both reasoning accuracy and immediate error detection. For open-ended domains, recent work has integrated binary verifiable signals with generative soft scoring from distilled verifiers (Su et al. 2025b), showcasing RLVRs versatility as framework for robust and generalizable reasoning. These works underscore RLVRs versatility as framework for robust, generalizable reasoning in LLMs."
        },
        {
            "title": "6.2 RLVR for Multimodal Reasoning",
            "content": "Inspired by its success in the textual domain, growing body of worked aims to apply RLVR to the multimodal domain. These bodies of work primary directions to improve multimodal reasoning: (1) creating novel datasets while applying text-derived optimization practices, (2) improving rollout strategies, (3) enhancing rewards without modifying the core reinforcement learning objective. Critically, most prior work retains the reinforcement learning framework developed for text-only tasks, either directly applying it to multimodal settings or making adjustments solely to rewards or datafailing to explicitly adapt the learning process to leverage visual inputs. Figure 7: Comparison of different regularization strategies. All strategies are applied to the same collapsing baseline, PAPOH (γ = 0.02, no regularization). Among the four methods described in the main text, three successfully prevent the collapse entirely, while adding Single Masked Entropy only delays it. The proposed Double Entropy Loss demonstrates the best training dynamics and prevents the collapse . Evaluation results are further compared in Table 5."
        },
        {
            "title": "Overall",
            "content": "AVG % rel AVG % rel AVG % rel 62.51 54. 58.78 PAPOH w/ Inc KLref PAPOH w/ Single Ent PAPOH w/ Double Ent 63.14 63.34 63.50 1.12 1.53 1. 57.03 58.36 59.37 3.99 5.96 7.96 60.42 61.12 61.66 2.40 3.50 4.39 Table 5: Performance comparison between the three regularization methods that successfully prevent model collapse, as shown in Figure 7. Among these methods, we find that Double Entropy Loss achieves the best overall improvement of 4.4%. gate four different approaches to prevent the collapse: Increasing the KL penalty against the reference model. Adding single entropy loss on the original policy sequence πθ. Adding single entropy loss on the corrupted policy sequence πmask . θ . θ Adding Double Entropy Loss on both πθ and πmask In Figure 7, we show the training dynamics of the four approaches. Among them, the single entropy loss on πmask does not prevent the collapse, while the other three approaches all successfully prevent it. We further examine their evaluation performance, as shown in Table 5. We find that PAPO with Double Entropy Loss achieves the most significant performance gain while also successfully avoiding KLprcp Hacking. θ"
        },
        {
            "title": "5.5 Computational Overhead\nThe main computational overhead stems from the additional\nforward pass on the rollout sequences with a corrupted vi-",
            "content": "visual operations, such as zooming in. However, these methods do not directly improve the native perception capabilities of the multimodal model; rather, they teach the model to query and interpret additional visual inputs as supplementary context. Consequently, we find that the prevailing assumption multimodal reasoning can be effectively addressed solely through dataand reward-level modifications to textbased RLis inherently limiting. Our work challenges this paradigm by demonstrating that incentivizing visually grounded reasoning requires deeper integration into the core learning objective, rather than treating vision as secondary modality addressed through auxiliary adjustments."
        },
        {
            "title": "7 Conclusion and Future Work\nIn this paper, we present PAPO, a simple yet effective ex-\ntension to GRPO that introduces a novel Implicit Percep-\ntion Loss in the form of maximizing a KL divergence, to\nensure that the reasoning steps in Large Multimodal Mod-\nels (LMMs) are visually grounded. Our method requires no\nadditional annotations, no reliance on stronger teacher mod-\nels, and no expensive neural reward models, making it both\naccessible and practical. Despite its simplicity, PAPO sig-\nnificantly enhances complex visual reasoning capabilities.\nThrough extensive experiments and ablations, we demon-\nstrate the effectiveness of our approach and highlight how a\nsimple double-entropy loss regularization can mitigate col-\nlapsing issues. A limitation of our work is that it introduces\na moderate amount of computational overhead, which we\ndid not focus on optimizing. Future work includes scaling to\nlarger model sizes and exploring compatibility with differ-\nent model families, such as the InternVL (Zhu et al. 2025a)\nseries. Another direction is to incorporate PAPO with ad-\nditional algorithmic modifications, such as full DAPO (Yu\net al. 2025) with dynamic sampling and token-level loss.\nFurther improving the training efficiency of PAPO also re-\nmains an important direction.",
            "content": "Dataset-Centric Approaches. dominant paradigm focuses on dataset curation while maintaining text-based RL objectives. Earlier efforts such as R1-V (Chen et al. 2025a) and Vision-R1 (Huang et al. 2025) distill chain-of-thought (CoT) data from strong textual reasoning models like Deepseek-R1 and explore directly applying R1-style training pipelines to multimodal tasks, demonstrating the initial promise of RL for generalization compared to supervised finetuning. This pattern persists in MM-Eureka (Meng et al. 2025a), which introduces the MMK12 dataset but relies on text-derived techniquesprompt filtering and correctnessconditioned rewardson top of the standard GRPO optimization. UniVG-R1 (Bai et al. 2025) further extends RLVR to visual grounding tasks by curating grounding datasets within GRPOs reward structure. Recent work, such as Truth in the Few(Li et al. 2025a) and MoDoMoDo(Liang et al. 2025), explores more sophisticated data selection mechanisms to improve sample quality in multimodal RLVR. Rollout Improvements. The second main direction focuses on optimizing rollout strategies to enhance RLVRs applicability in multimodal domains. VL-Rethinker (Wang et al. 2025a) adopts Selective Sample Replay mechanism to mitigate the prevalent issue of vanishing advantages in the GRPO training objective. R1-ShareVL (Yao et al. 2025) addresses sparse rewards by sharing trajectories across question variants. Skywork R1V2 (Wang et al. 2025b) introduces Mixed Preference Optimization (MPO) and Selective Sample Buffer on top of the GRPO objective to improve general reasoning stability and reduce overthinking in generated outputs. NoisyRollout (Liu et al. 2025b) augments the input with moderately distorted images, while keeping the GRPO algorithm unchanged. Reward Enhancements. The third direction focuses on enhancing the reward signals used in GRPO. Several approaches incorporate perception-aware metrics, such as IoU thresholds and L1 distances for bounding boxes, as seen in V-Triune (Ma et al. 2025), VisionReasoner (Liu et al. 2025c), and GRIT (Fan et al. 2025). R1-OneVision (Yang et al. 2025) and Visionary-R1 (Xia et al. 2025) introduce captioning-based rewards, prompting the model to generate detailed textual descriptions of the visual input before applying standard text-based RL training. On the other end of the spectrum, Kimi K1.5 (Team et al. 2025), GRPOCARE (Chen et al. 2025b), and SRPO (Wan et al. 2025) focus on improving the consistency and quality of the reasoning process itself without specific emphasize on the vision capability. Although empirically beneficial, none of these works explore deeper integration of perception into the optimization objective beyond the reward design, leaving the standard GRPO algorithm minimally adapted for visual reasoning. Perception as Tool-Using. Another line of emerging work takes different view on improving perception in multimodal reasoning, relying on tool use for perception. Recent efforts such as DeepEyes (Zheng et al. 2025), ACTIVEO3 (Zhu et al. 2025b), and Pixel Reasoner (Su et al. 2025a) enhance perception by incentivizing the LMM to perform Vision-Dependency Analysis We observe that current multimodal reasoning benchmarks exhibit varying degrees of reliance on visual information, ranging from questions answerable through textual cues alone to those requiring comprehensive visual analysis. To systematically characterize this spectrum, we propose taxonomy of vision dependency levels and analyze their distribution across mainstream multimodal reasoning datasets, including ViRL39K (Wang et al. 2025a), Geometry3K (Lu et al. 2021), MathVista (Lu et al. 2023), MathVerse (Zhang et al. 2024), LogitVista (Xiao et al. 2024), MMK12 (Meng et al. 2025b), We-Math (Qiao et al. 2024), SuperClevrCounting (Li et al. 2023), and MMMU-Pro (Yue et al. 2024). Specifically, we categorize vision dependency into three levels based on the significance distribution of critical information between textual questions and visual contents: Low: In low vision-dependency tasks, the questions typically embed substantial visual information within the textual input itself, such as specifying the length of an important triangle side, thereby reducing the models reliance on visual processing. Medium: Medium-level vision-dependency tasks provide partial contextual information textually while requiring the model to perceive and extract complementary visual features from the image. High: High vision-dependency tasks contain minimal or no visual information in the textual input, requiring the model to derive answers entirely through visual reasoning. We manually examine the data instances and dataset construction pipelines for each benchmark and summarize our findings in Table 7. In Figure 8, we further illustrate representative examples from each category, demonstrating how the practical manifestation of these dependency levels directly correlates with the perception challenges we observe in current multimodal reasoning models. Notably, our experiments show that PAPOs improvements are most pronounced (7.96%) on high vision-dependency tasks, unveiling that learning perception-aware policies is essential for robust multimodal reasoning. Masking Strategies We extend our elaboration on our two masking strategies for creating corrupted visual inputs Imask used in the Implicit Perception Loss KLprcp. We show an illustration of different masking strategies in Figure 9. As visualized in Figure 9, we observe that patch-based masking removes informative semantic contents more effectively while pixel-level noises typically preserve semantics even at high noise levels. We detail the two patch-level masking strategies as follows. B.1 Random Masking Random masking is simple and computationally efficient approach for generating Imask. Given an input image and blackening probability [0, 1], we traverse the image in grid pattern with patch size pixels (s = 14 by default)."
        },
        {
            "title": "Training Dataset",
            "content": "ViRL39K (Wang et al. 2025a)"
        },
        {
            "title": "General Reasoning",
            "content": "Geo3K (Lu et al. 2021) LogicVista (Xiao et al. 2024) MathVerse (Zhang et al. 2024) MathVista (Lu et al. 2023) MMK12 (Meng et al. 2025b) We-Math (Qiao et al. 2024) Vision-Dependent Reasoning Counting (Li et al. 2023) MathVerseV (Zhang et al. 2024) MMMU-Pro (Yue et al. 2024) Table 7: Analysis of vision-dependency across nine multimodal reasoning datasets. We categorize the instances in each dataset into three levels of vision-dependency: low, medium, and high. Illustrative examples are presented in Figure 8. See for detailed discussion. For each patch location, we generate an independent random variable Uniform(0, 1) and mask the patch if < p. This random masking implementation ensures that each patch has the probability of being masked, independent of other patches. The expected fraction of masked patches is p, with minimal computational overhead. B.2 Semantic-Aware Masking Semantic-aware masking aims to preferentially mask patches that contain more semantically important visual information. This approach leverages pre-trained vision encoder to identify salient regions before applying masking. Our implementation uses DINOV2 (Oquab et al. 2023) as the vision encoder for its strong self-supervised representation learning capabilities. Attention-Based Saliency Computation. Given an input image I, we first process the image to obtain patch-level attention maps. For model with layers and attention heads, this yields attention matrices A(l) RHN for each layer 1, 2, . . . , L, where is the total number of patches. As middle layers often capture more meaningful semantic relationships, we employ 6, 7, 8, 9 layers to aggregate patch-level self-attention scores for saliency computation. Specifically, for each selected layer l, aggregate attention across heads using mean pooling: (l) ="
        },
        {
            "title": "1\nH",
            "content": "H (cid:88) h=1 A(l) (B.3) Figure 8: Illustrative examples of different levels of vision-dependency. Figure 9: Visualization of different masking strategies. Semantic-aware masking prioritizes patches containing salient objects. Adding Gaussian noise is less effective at masking informative semantics, even with high noise factor (standard deviation). evaluate how well each models response relates to and addresses the input question on scale from 0 to 10. Our evaluation prompt below specifically instructs the judge to focus on whether the response attempts to solve the given problem rather than correctness, with scoring guidelines ranging from 0 (completely unrelated/gibberish) to 10 (perfectly related reasoning, even if the final answer is incorrect). The complete evaluation prompt is provided in Figure 10. Quantitative Analysis. Our relatedness evaluation revealed that the collapsed model demonstrated significantly degraded coherence, with an average relatedness score approximately 18% lower than the baseline model. This substantial drop in relevance scores reflects the models tendency to generate tokens that bear little semantic relationship to the input context or fail to attempt solving the given problem. Additionally, we measured the variance in the KLprcp loss across response tokens by computing per-token KL divergence between responses generated from original images versus randomly patch-blackened versions of the same images. The collapsed model showed approximately 8.4 times higher variance in KL divergence compared to the baseline, indicating that the model has learned to exploit the KLprcp by generating highly unpredictable token sequences. Qualitative Observations. Based on the grading of GPT4.1-mini and our manual inspection, we find that collapsed models frequently generate responses that may begin with some problem-relevant content but contain substantial portions of irrelevant text, numbers, or apparent meaningless contents. An example can be found in Figure 4. The saliency score is computed for each patch by summing the attention it receives from all other patches: s(l) = (cid:88) j=2 (l) j,i+1 where the +1 offset accounts for the first CLS token. Saliency scores are averaged across selected layers: si = 1 (cid:88) lL s(l) (B.4) (B.5) where = 6, 7, 8, 9 is the set of selected layers. Saliency-Based Patch Selection. With computed saliency scores for all image patches, we select patches for masking through: Ranking. Sort patches in descending order of saliency scores to identify the most semantically important regions. Top-k Selection. Given masking ratio p, select the top = (N 1) patches with highest saliency scores for masking. Patch Masking. Apply the same zero-out operation as in random masking to the selected high-saliency patches. Exploration On Collapsing Behaviors In addition to notable decline in performance  (Table 2)  , collapsed models also generate responses containing entirely unrelated tokens. To explore the extent of this abnormal generation behavior, we extend our analysis to token relevance in model outputs after collapsing occurs. Experimental Setup. We evaluated both collapsed 7B PAPO model (γ = 0.02, without regularization) and noncollapsed 7B GRPO baseline on Geo3K (Lu et al. 2021). To assess the coherence and relevance of generated responses, we employed GPT-4.1-mini (OpenAI 2025) as judge to Figure 10: Prompt to GPT-4.1-mini for scoring the relatedness between the model-generated response and the input query. References Bai, S.; Li, M.; Liu, Y.; Tang, J.; Zhang, H.; Sun, L.; Chu, X.; and Tang, Y. 2025. UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning. arXiv preprint arXiv:2505.14231. 10 Chen, L.; Li, L.; Zhao, H.; Song, Y.; and Vinci. 2025a. R1-V: Reinforcing Super Generalization Ability in VisionLanguage Models with Less Than $3. https://github.com/ Deep-Agent/R1-V. Accessed: 2025-07-03. 1, 2, 10 Chen, Y.; Ge, Y.; Wang, R.; Ge, Y.; Cheng, J.; Shan, Y.; and Liu, X. 2025b. GRPO-CARE: ConsistencyAware Reinforcement Learning for Multimodal Reasoning. arXiv:2506.16141. 10 Fan, Y.; He, X.; Yang, D.; Zheng, K.; Kuo, C.-C.; Zheng, Y.; Jyothi, N., Sravana Guan; Guan, X.; and Wang, X. E. 2025. GRIT: Teaching MLLMs to Think with Images. 10 Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. 2 https://github.com/hiyouga/ hiyouga. 2025. MathRuler. MathRuler. 3 Huang, W.; Jia, B.; Zhai, Z.; Cao, S.; Ye, Z.; Zhao, F.; Xu, Z.; Hu, Y.; and Lin, S. 2025. Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models. arXiv preprint arXiv:2503.06749. 1, 2, 10 Li, S.; Deng, K.; Wang, L.; Yang, H.; Peng, C.; Yan, P.; Shen, F.; Shen, H. T.; and Xu, X. 2025a. Truth in the Few: HighValue Data Selection for Efficient Multi-Modal Reasoning. arXiv preprint arXiv:2506.04755. 10 Li, X.; Bao, T.; Liu, H.; Zhang, T.; and Fei-Fei, L. 2025b. WeThink: General-Purpose Vision-Language Reasoning via RL. arXiv preprint arXiv:2506.07905. 1 Li, Y.; Wei, L.; Zheng, K.; Huang, J.; Kong, L.; Sun, L.; and Huang, W. 2025c. Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning. arXiv preprint arXiv:2506.09736. 1 Li, Z.; Wang, X.; Stengel-Eskin, E.; Kortylewski, A.; Ma, W.; VanDurme, B.; and Yuille, A. 2023. Super-CLEVR: Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1496314973. IEEE. 2, 5, 11 Liang, T.; Liu, X.; He, P.; Mi, H.; Tu, Z.; and Yu, D. 2025. MoDoMoDo: Learning Mixture-of-Datasets with RearXiv inforcement Learning for Multimodal Reasoning. preprint arXiv:2505.24871. 1, 2, 10 Liu, X.; Liang, T.; He, Z.; Xu, J.; Wang, W.; He, P.; Tu, Z.; Mi, H.; and Yu, D. 2025a. Trust, But Verify: Self-Verification Approach to Reinforcement Learning with Verifiable Rewards. arXiv. 9 Liu, X.; Ni, J.; Wu, Z.; Du, C.; Dou, L.; Wang, H.; Pang, T.; and Shieh, M. Q. 2025b. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055. 1, 2, 10 Liu, Y.; Qu, T.; Zhong, Z.; Peng, B.; Liu, S.; Yu, B.; and Jia, J. 2025c. VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning. arXiv preprint arXiv:2505.12081. 10 Liu, Z.; Sun, Z.; Zang, Y.; Dong, X.; Cao, Y.; Duan, H.; Lin, D.; and Wang, J. 2025d. Visual-RFT: Visual Reinforcement Fine-Tuning. arXiv preprint arXiv:2503.01785. 2 Lu, P.; Bansal, H.; Xia, T.; Liu, J.; Li, C.; Hajishirzi, H.; Cheng, H.; Chang, K.; Galley, M.; and Gao, J. 2023. MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. arXiv prepring arXiv:2310.02255. 2, 3, 5, 11 Lu, P.; Gong, R.; Jiang, S.; Qiu, L.; Huang, S.; Liang, X.; and Zhu, S. 2021. Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. In Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 67746786. Association for Computational Linguistics. 2, 3, 5, 11, 13 Ma, Y.; Du, L.; Shen, X.; Chen, S.; Li, P.; Ren, Q.; Ma, L.; Dai, Y.; Liu, P.; and Yan, J. 2025. One RL to See Them All: Visual Triple Unified Reinforcement Learning. arXiv preprint arXiv:2505.18129. 10 Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Han, T.; Shi, B.; Wang, W.; He, J.; Zhang, K.; Luo, P.; Qiao, Y.; Zhang, Q.; and Shao, W. 2025a. MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with Rule-based Reinforcement Learning. arXiv preprint arXiv:2503.07365. 10 Meng, F.; Du, L.; Liu, Z.; Zhou, Z.; Lu, Q.; Fu, D.; Shi, B.; Wang, W.; He, J.; Zhang, K.; Luo, P.; Qiao, Y.; Zhang, Q.; and Shao, W. 2025b. MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning. arXiv preprint arXiv:2503.07365. Submitted March 10, 2025. 1, 2, 3, 11 OpenAI. 2025. Introducing GPT-4.1 in the API. 7, 13 Oquab, M.; Darcet, T.; Moutakanni, T.; Vo, H. V.; Szafraniec, M.; Khalidov, V.; Fernandez, P.; Haziza, D.; Massa, F.; El-Nouby, A.; Assran, M.; Ballas, N.; Galuba, W.; Howes, R.; Huang, P.; Li, S.; Misra, I.; Rabbat, M.; Sharma, V.; Synnaeve, G.; Xu, H.; Jegou, H.; Mairal, J.; Labatut, P.; Joulin, A.; and Bojanowski, P. 2023. DINOv2: Learning Robust Visual Features without Supervision. CoRR, abs/2304.07193. Published in *Transactions on Machine Learning Research* (TMLR), Jan. 2024. 4, 11 Qiao, R.; Tan, Q.; Dong, G.; Wu, M.; Sun, C.; Song, X.; GongQue, Z.; Lei, S.; Wei, Z.; Zhang, M.; et al. 2024. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284. 2, 5, 11 Qwen Team, A. G. 2024a. Qwen2.5-VL-3B-Instruct. https: //huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct. 3, 5 Qwen Team, A. G. 2024b. Qwen2.5-VL-7B-Instruct. https: //huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct. 5 Schulman, J. 2020. Approximating KL Divergence. http: //joschu.net/blog/kl-approx.html. 4 Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 2 Shannon, C. E. 1948. mathematical theory of communication. The Bell system technical journal, 27(3): 379423. 3 Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. 2, 3, 4, 9 Shen, H.; Liu, P.; Li, J.; Fang, C.; Ma, Y.; Liao, J.; Shen, Q.; Zhang, Z.; Zhao, K.; Zhang, Q.; et al. 2025a. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615. 1, 2 Shen, L.; Li, Y.; Mi, H.; Wang, W.; Tu, Z.; and Yu, D. 2025b. SATORI-R1: Spatially Anchored Training with Verifiable Rewards for Vision-Language Reasoning. arXiv preprint arXiv:2505.19094. 1 Singh, A.; Bhaskar, A.; Yu, P.; Chakraborty, S.; Dasyam, R.; Bedi, A.; and Tokekar, P. 2025. VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences. arXiv preprint arXiv:2503.13817. 2 Su, A.; Wang, H.; Ren, W.; Lin, F.; and Chen, W. 2025a. Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning. arXiv preprint arXiv:2505.15966. 10 Su, Y.; Yu, D.; Song, L.; Li, J.; Mi, H.; Tu, Z.; Zhang, M.; and Yu, D. 2025b. Expanding RL with Verifiable Rewards Across Diverse Domains. arXiv. 9 Team, K.; Du, A.; Gao, B.; Xing, B.; Jiang, C.; Chen, C.; Li, C.; Xiao, C.; Du, C.; Liao, C.; et al. 2025. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. 10 Wan, Z.; Dou, Z.; Liu, C.; Zhang, Y.; Cui, D.; Zhao, Q.; Shen, H.; Xiong, J.; Xin, Y.; Jiang, Y.; et al. 2025. SRPO: Enhancing Multimodal LLM Reasoning via Zheng, Z.; Yang, M.; Hong, J.; Zhao, C.; Xu, G.; Yang, L.; Shen, C.; and Yu, X. 2025. DeepEyes: Incentivizing Thinking with Images via Reinforcement Learning. arXiv preprint arXiv:2505.14362. 2, 10 Zhou, E.; Geng, R.; Liu, H.; Fei-Fei, L.; and Zhang, T. 2025. SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis. arXiv preprint arXiv:2506.02096. 1 Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Tian, H.; Duan, Y.; Su, W.; Shao, J.; et al. 2025a. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. 10 Zhu, M.; Zhong, H.; Zhao, C.; Du, Z.; Huang, Z.; Liu, M.; Chen, H.; Zou, C.; Chen, J.; Yang, M.; and Shen, C. 2025b. Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO. arXiv preprint arXiv:2505.21457. 1, 2, 10 Reflection-Aware Reinforcement Learning. arXiv preprint arXiv:2506.01713. 1, 10 Wang, H.; Qu, C.; Huang, Z.; Chu, W.; Lin, F.; and Chen, W. 2025a. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning. arXiv preprint arXiv:2504.08837. Published April 10, 2025. 1, 3, 5, 10, 11 Wang, P.; Wei, Y.; Peng, Y.; Wang, X.; Qiu, W.; Shen, W.; Xie, T.; Pei, J.; Zhang, J.; Hao, Y.; Song, X.; Liu, Y.; and Zhou, Y. 2025b. Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning. arXiv preprint arXiv:2504.16656. 1, 2, 10 Wang, Y.; Sun, Z.; Zhang, J.; Xian, Z.; Biyik, E.; Held, D.; and Erickson, Z. 2024. RL-VLM-F: Reinforcement Learning from Vision-Language Foundation Model Feedback. arXiv preprint arXiv:2402.03681. 2 Xia, J.; Zang, Y.; Gao, P.; Li, Y.; and Zhou, K. 2025. Visionary-R1: Mitigating Shortcuts in Visual ReaarXiv preprint soning with Reinforcement Learning. arXiv:2505.14677. 1, 2, 10 Xiao, T.; Xu, X.; Huang, Z.; Gao, H.; Liu, Q.; Liu, Q.; and Chen, E. 2025a. Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward. arXiv preprint arXiv:2506.07218. 1, 2 Xiao, W.; Zhang, W.; Hu, J.; Chen, R.; and Yang, J. 2025b. Perception-R1: Reinforcement Learning Framework for arXiv Multimodal Perception with Verifiable Rewards. preprint arXiv:2506.07218. 1 Xiao, Y.; Sun, E.; Liu, T.; and Wang, W. 2024. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973. 2, 3, 5, 11 Yang, Y.; He, X.; Pan, H.; Jiang, X.; Deng, Y.; Yang, X.; Lu, H.; Yin, D.; Rao, F.; Zhu, M.; et al. 2025. R1-onevision: Advancing generalized multimodal reasoning through crossmodal formalization. arXiv preprint arXiv:2503.10615. 1, 10 Yao, H.; Yin, Q.; Zhang, J.; Yang, M.; Wang, Y.; Wu, W.; Su, F.; Shen, L.; Qiu, M.; Tao, D.; and Huang, J. 2025. R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO. 1, 2, 10 Yu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.; Dai, W.; Fan, T.; Liu, G.; Liu, L.; et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. 2, 3, 5, 7, 9, 10 Yue, X.; Zheng, T.; Ni, Y.; Wang, Y.; Zhang, K.; Tong, S.; Sun, Y.; Yu, B.; Zhang, G.; Sun, H.; et al. 2024. Mmmupro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. 2, 5, 11 Yue, Y.; Yuan, Y.; Yu, Q.; Zuo, X.; Zhu, R.; Xu, W.; Chen, J.; Wang, C.; Fan, T.; Du, Z.; et al. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. 2 Zhang, R.; Jiang, D.; Zhang, Y.; Lin, H.; Guo, Z.; Qiu, P.; Zhou, A.; Lu, P.; Chang, K.; Gao, P.; and Li, H. 2024. MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? CoRR, abs/2403.14624. Also published in the ECCV 2024 proceedings. 2, 3, 5, 6,"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "University of Illinois Urbana-Champaign"
    ]
}