{
    "paper_title": "RRM: Robust Reward Model Training Mitigates Reward Hacking",
    "authors": [
        "Tianqi Liu",
        "Wei Xiong",
        "Jie Ren",
        "Lichang Chen",
        "Junru Wu",
        "Rishabh Joshi",
        "Yang Gao",
        "Jiaming Shen",
        "Zhen Qin",
        "Tianhe Yu",
        "Daniel Sohn",
        "Anastasiia Makarova",
        "Jeremiah Liu",
        "Yuan Liu",
        "Bilal Piot",
        "Abe Ittycheriah",
        "Aviral Kumar",
        "Mohammad Saleh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 2 ] . [ 1 6 5 1 3 1 . 9 0 4 2 : r RRM: ROBUST REWARD MODEL TRAINING MITIGATES REWARD HACKING Tianqi Liu1, Wei Xiong2, Jie Ren1, Lichang Chen3, Junru Wu1, Rishabh Joshi1, Yang Gao1, Jiaming Shen1, Zhen Qin1, Tianhe Yu1, Daniel Sohn1, Anastasiia Makarova1, Jeremiah Liu1, Yuan Liu1, Bilal Piot1, Abe Ittycheriah1, Aviral Kumar1, Mohammad Saleh1 Google DeepMind1, University of Illinois Urbana-Champaign2, University of Maryland, College Park"
        },
        {
            "title": "ABSTRACT",
            "content": "Reward models (RMs) play pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce causal framework that learns preferences independent of these artifacts and propose novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding more robust reward model (RRM). Our RRM improves the performance of pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning from Human Feedback (RLHF) has become cornerstone in aligning large language models (LLMs) with human preferences to produce responses that are more helpful, honest, and harmless (Ouyang et al., 2022; Bai et al., 2022a). This approach involves training reward model (RM) on human feedback, which then guides the LLM to generate high-quality responses through reinforcement learning. The success of RLHF is evident in various AI systems, such as Gemini (Team et al., 2023) and GPT-4 (Achiam et al., 2023). Despite its effectiveness, RLHF faces the fundamental issue of reward hacking (Gao et al., 2023), where the model maximizes the reward function without truly aligning with the intended human preferences. This hacking issue occurs because the RM, while powerful tool, is an imperfect proxy for human judgment and often struggles with out-of-distribution generalization (Eisenstein et al., 2023). The reward hacking problem manifests in several ways, with verbosity being common issue: LLMs tend to generate longer responses to appear more detailed or explanatory, exploiting human raters bias towards lengthier content (Shen et al., 2023b; Singhal et al., 2023). In recognition of this challenge, extensive efforts have been made in the literature. ODIN (Chen et al.) designs twohead approach to learn the quality reward that is orthogonal to length. Similarly, length-controlled Alpaca (Dubois et al., 2024a) estimates the controlled direct effect (VanderWeele, 2011) through logistic regression by adjusting the length. To mitigate the length bias, an improved version (Park et al., 2024) of DPO (Rafailov et al., 2024) introduces length as penalty to the reward score. In practice, there are more reward hacking patterns beyond length, such as format (markdowns, bold-faces) and patterns (certain n-grams or emojis). This is largely due to the large output space of language with limited preference data, as well as the diverse and subjective nature of human preferences. Correspondence to Tianqi Liu, tianqiliu@google.com. Work done during an internship at Google DeepMind. 1 Figure 1: The pipeline of our proposed robust reward model (RRM), which aim to decouple contextual preference quality signal and context-free artifacts. Suppose proportion of chosen responses have certain artifact (bold-face wrapped with in this figure), the reward model can hack the pattern and choose the response with the artifact instead of carefully reading the prompt. With our data augmentations, we can effectively balance the context-free artifacts in chosen and rejected responses, thus ensuring more robust reward model during inference. It is challenging to identify and mitigate all potential exploitation patterns. We may consider the causal perspective to explain this phenomena. Given prompt and pair of responses (y1, y2), the human preference can be caused by the real quality s(x, y1, y2) that is associated with the prompt, or by the context-free artifacts a(y1, y2) in the responses that do not depend on prompt. Traditional reward model training cannot differentiate the above two factors. There are two reasons for this. First, the pair of responses are always contextual and on-topic to the prompt, thus no counterfactual prompt (prompt from another examples) is used. The reward model may learn the artifacts existing in the responses by ignoring the prompt. If we use the counterfactual prompt, it can help estimate the level of artifact bias (P(y1 y2x) with = x) existing in the preference dataset (Zhao et al., 2021). Second, even if we adjust few common artifacts, not all artifacts are observable and thus there is no easy way to control all the artifacts explicitly to answer the question what will the preference be if both responses share the same artifacts?. In response to these challenges, we propose simple and effective method to improve reward modeling. We first formulate the reward model training in causal framework, then we augment the reward model training data based on the causal rules. By doing so, we can effectively adjust the artifacts and only learn the real quality. Our pipeline is illustrated in Figure 1, where we augment the reward model training data by using responses from other examples to effectively balance the artifacts in chosen and rejected responses. To summarize, the contributions of this paper are three-fold: We identify key issue with traditional reward model training: it often fails to distinguish between genuine contextual preference signals and spurious context-free artifacts. To address this, we propose causal graph for human preference modeling and introduce data augmentation to mitigate artifacts learned by the reward model. We further demonstrate that policies trained on these robust reward models consistently outperform those based on baseline reward models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In preference learning, we assume that there exists preference oracle that determines the probability P(y1 y2x) that response y1 is preferred over y2 given the prompt x. Our goal is to optimize the preference by querying the preference oracle within certain budget constraint. In what follows, we first review the major ways to approximate and estimate the oracle based on human preference 2 dataset Dhf = {x(i), y(i) represents the chosen and rejected response, respectively. , y(i) }N i=1. where x(i) represents prompt for example i, and (y(i) , y(i) ) Reward models Bradley-Terry pointwise reward model (Bradley & Terry, 1952; Ouyang et al., 2022) is widely adopted method, which additionally assumes that there exists reward function r(x, y) and the preference oracle satisfies P(y1 y2x) = exp(r(x, y1)) exp(r(x, y1)) + exp(r(x, y2)) = σ(cid:0)r(x, y1) r(x, y2)(cid:1). Then, we can fit the Bradley-Terry model by maximizing the log-likelihood on the training set: L(rϕ, Dhf) = E(x,yw,yl)Dhf [log σ (rϕ(x, yw) rϕ(x, yl))] . (1) The second predominant approach is the pairwise ranking model (Zhao et al., 2023; Jiang et al., 2023), which takes prompt and pair of responses as the input, and directly predicts the probability P(y1 y2x), which subsumes the BT model as subclass. In the literature, the pairwise preference model has shown to outperform pointwise BT reward both empirically (Zhao et al., 2023; Jiang et al., 2023; Dong et al., 2024) and theoretically (Ye et al., 2024) due to its flexibility and larger function class capacity. Specifically, we denote the pairwise ranking model as ρψ(x, y1, y2) and leverage the next token prediction ability of the language model to format the sample as: [CONTEXT] {x} [RESPONSE A] {y1} [RESPONSE B] {y2} Then, the ρψ(x, y1, y2) outputs either or as preferred one. We use the probability of decoding as estimation of the preference probability ˆP(y1 y2x)1. In this work, we use the pairwise ranking model for its superior performance and flexibility. Alignment Algorithms Start with reward function r(x, y), reference policy πref, and input prompt distribution P, policy π is trained to optimize for the following objective: max π ExP (cid:2)Eyπ(x)r(x, y) βDKL [π(x)πref(x)](cid:3) , (2) where β > 0 is the KL penalty coefficient. Several algorithms have been proposed to solve the above optimization, including PPO (Schulman et al., 2017; Ziegler et al., 2019), SLiC (Zhao et al., 2023), DPO (Rafailov et al., 2024), RSO (Liu et al., 2024b), and IPO (Azar et al., 2024). For stable evaluation process, we use DPO in this work for preference alignment. For given preference dataset Dp, DPO use the following loss function: LDPO(πθπref, Dp) = E(x,yw,yl)Dp (cid:20) (cid:18) log σ β log πθ(ywx) πref(ywx) β log (cid:19)(cid:21) πθ(ylx) πref(ylx) (3) Reward Hacking Reward model is not perfect due to its limited model size, limited training data, and distribution shift between training data and alignment prompts and responses (Eisenstein et al., 2023; Gao et al., 2023; Guo et al., 2024; Xiong et al.). Several works have been proposed to mitigate reward hacking. They mainly focus on observable artifacts such as length (Chen et al.; Dubois et al., 2024a; Shen et al., 2023b). Shen et al. (2023a) propose to enforce the consistency in reward model via data augmentation. Reward hacking can also be mitigated during policy training with postadjusted reward (Park et al., 2024) or with post-training model merge (Lin et al., 2023). We focus on improving the reward model by addressing reward hacking from causal perspective. Causal Inference Causal inference can be embedded in graphical model frameworks as directed acyclic graph (DAG) with causal relationship represented as directed edge (Pearl, 2009; Lee et al., 2020). We say random vector to be faithful with respect to DAG = (V, E) if for any i, V, and any subset {i, j}, j and are d-separated by under G, (4) where j denotes and are independent conditional on S. The definition of d-separation is as follows: suppose we are given DAG G; then, for two nodes i, V, subset of {i, j} d-connects and if there exists path between and such that every collider in either belongs to or has descendent in S, and no other node in belongs to S. If does not d-connect and j, then it d-separates and j. See Appendix A.5 for more details. 1We randomly flip response pairs and associated labels to remove positional bias."
        },
        {
            "title": "3.1 CAUSAL FRAMEWORK",
            "content": "Figure 2: Causal graph of reward model. is the prompt, Y1, Y2 are two responses, is the contextual signal that depends on input prompt and two responses. is the context-free artifact that only depends on two responses. is the preference label. Traditional reward model cannot differentiate the two DAGs on whether there is causal edge from to C. Our work uses the augmented dataset to eliminate the edge from to C. We formulate DAG to model the causal relationships among different quantities (Figure 2). We assume the distribution of (X, Y1, Y2, S, A, C) to be faithful to the DAG. is the sufficient statistic (Lehmann & Casella, 2006) that captures the contextual effect that one response fulfills the need of the prompt better than the other. is the sufficient statistic that captures the context-free artifacts that only depend on two responses. Such artifacts can include length, format (bold faces, bullet points, markdown, etc), and certain patterns (n-grams such as Sure, here is the response:). In traditional reward model training, the model may hack the patterns in (Y1, Y2). Suppose 80% of winning responses to be longer, then the reward model can get 80% accuracy by just counting the number of tokens. Formally, we construct two hypothesis: H0: there is no causal edge from to C. H1: there is causal edge from to C. Proposition 3.1. In traditional reward model training, H0 and H1 are not always distinguishable. The desired behavior of reward model is to ignore the artifact in determining the preference label C, which corresponds to H0. To achieve that, we can utilize two d-separation relationships of the DAG G. R1: Under H0, and are d-separated by (Y1, Y2), thus (Y1, Y2). R2: Under H0, and are d-separated by S, thus S. In addition, we can set the preference rule based on the DAG G. We say response is contextual to if they are from the same triplet in Dhf = {x(i), y(i) are contextual to x(i), but y(j) and y(j) are not contextual to x(i) for = i. Then for (x, y1, y2), we have the following rules: i=1. For example, y(i) and y(i) , y(i) }N l if both y1 and y2 are contextual to x, we set the winning one in Dhf as winner. if only one of y1 and y2 is contextual to x, we set the contextual one as winner. if neither y1 nor y2 is contextual to x, we set the preference label as Tie. 3.2 DATA AUGMENTATION To fix the issue mentioned in Proposition 3.1, we can effectively utilize R1&R2. Dhf Given Dhf = as {x(i), y(i) , x(σ2(i)), y(σ2(i)) [N ] [N ] and σ2 : [N ] [N ] are two different invertible permutation functions randomly sampled from (cid:1) = 45 possible (x, y1, y2) unordered triplets from permutation group SN . There are in total 3 (cid:0)6 expand the dataset }N i=1, where σ1 : = {x(i), y(i) , x(σ1(i)), y(σ1(i)) }N , y(σ1(i)) , y(σ2(i)) i=1, we , y(i) can first , y(i) 2 4 each element in Dhf. This is because there are 3 possible prompts with 2 choices among 6 responses and we treat (x, y1, y2) and (x, y2, y1) as the same one. From R1, we can fix (Y1, Y2) and vary to perturb C. From R2, we can fix by picking contextual (prompt, response) pair (X, Y1) and another non-contextual response Y2. Then we set Y1 as winning response and vary losing response Y2 to perturb A. We can see the augmented datasets derived from the above two rules cover all possible (x, y1, y2) unordered triplets generated from Dhf. For simplicity, we select the ones with prompt x(i), which provides us with the following additional augmented triplets2: (x(i), y(i) (x(i), y(i) , y(σ1(i)) , y(σ2(i)) , y(σ1(i)) , y(σ2(i)) , y(σ1(i)) , y(σ2(i)) , y(σ1(i)) , y(σ2(i)) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) ) chosen = y(i) l Non-contextuals (x(i), y(σ1(i)) (x(i), y(σ2(i)) (x(i), y(σ1(i)) (x(i), y(σ1(i)) (x(i), y(σ2(i)) (x(i), y(σ1(i)) , y(σ1(i)) , y(σ2(i)) , y(σ2(i)) , y(σ2(i)) , y(σ1(i)) , y(σ2(i)) (x(i), y(i) (x(i), y(i) (x(i), y(i) (x(i), y(i) (x(i), y(i) (x(i), y(i) ) Tie ) Tie ) Tie ) Tie ) Tie ) Tie"
        },
        {
            "title": "Neutrals",
            "content": "(5) Non-contextuals set the contextual response as chosen and non-contextual one as rejected. Neutrals set both non-contextual responses as tie. With these, we have the following claim: Proposition 3.2. If the reward model is trained with Dhf and augmented triplets in Equation 5, there is no causal edge from to in DAG G. 3.3 CONNECTION TO EXISTING WORKS ODIN (Chen et al.) ODIN decomposes reward into additive format of quality one and length one. During learning, it enforces the disentanglement between the quality reward and the response length and encourages the correlation between the length reward and the response length. We claim that this is special case of our causal modelling with single observed artifact as length, because the disentangle learning is necessary condition of the conditional independence between and given the data. Our framework is more general and can go beyond single and observed artifact. Length-controlled AlpacaEval-2 (Dubois et al., 2024a) This work improves the original version of AlpacaEval-2 by conditioning on the length through Controlled Direct Effect (VanderWeele, 2011). It adds length as variable in the logistic regression to predict the preference. Effectively, it learns the residual part that cannot be explained by the length. In our framework, we directly learn the residual part that is orthogonal to the artifacts, which is the length in length-controlled AlpacaEval-2. Thus the two methods are equivalent, and our approach can go beyond single artifact and be extended to unobserved artifacts. Length-controlled DPO (Park et al., 2024) This work adds length penalty in the RLHF objective (Equation 2). It serves as post-hoc reward adjustment to mitigate the length bias during policy optimization. The idea for removing the lengthy bias using length reward is the same as ODIN, but they dont have the correlation penalty and the additional hyperparameter introduced can add more complexity into policy optimization. In comparison, our work directly learns artifact-free reward model so we do not need an explicit length adjustment factor in the alignment algorithm designs. Contrast Instructions (Shen et al., 2023a) This work shows the issues of reward models on the instruction and response consistencies when switching instruction or response to another similar one. 2We show sample Python code in Appendix A.3. 5 It proposes data augmentation training approach and retrieval-augmented inference technique to improve the consistencies of reward models. On contrary, by considering all possible combinations of (x, y1, y2) across difference examples, our approach uses the organic data from the dataset, which can effectively eliminate the artifacts existing in the dataset."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we conduct reward modeling and apply the trained reward to downstream alignment tasks to verify the effectiveness of the proposed method. For deeper understanding, we also conduct analysis on reward model training data, aligned policies, and perturbed reward model training data."
        },
        {
            "title": "4.1 SETTINGS",
            "content": "Training Set We study RRM using the preference dataset curated by RLHFlow3 (Dong et al., 2024), which has been used to train series of strong open-source preference models as evaluated by the Rewardbench (Lambert et al., 2024). The dataset consists of 700K preference pairs, which is mixture of HH-RLHF (Bai et al., 2022a), SHP (Ethayarajh et al., 2022), HelpSteer (Wang et al., 2023), PKU-SafeRLHF (Ji et al., 2024), UltraFeedback (Cui et al., 2023), UltraInteract (Yuan et al., 2024), Distilabel-Capybara (Daniele & Suphavadeeprasit, 2023), and Distilabel-Orca (Lian et al., 2023). See Appendix A.2 for details of dataset composition. Reward Model Training Details We first train pairwise ranking reward model (RM) from Gemma-2-9b-it. With the augmentation illustrated in Equation 5, we can get 14X additional examples, most of which can be too easy for RM to learn. To reduce the augmented data size, we first conduct inference on random 50% of the augmented data using the trained RM, and leave the examples with ˆP(A B) P(A B) 0.2, where ˆP(A B) is winning probability calculated by RM and P(A B) is the ground truth probability4. We get 2.4M training examples by merging the filtered augmented data and original RM training data. Then we use the same training recipe to get the robust reward model (RRM). We train the reward models until convergence using AdamW (Loshchilov, 2017) optimizer with learning rate 1e-6 and batch size 1285. Policy Model Training Details We train DPO policies using the on-policy responses generated by Gemma-2-9b-it and labeled by RM and RRM, respectively. We use the prompt set from the (cid:1) pairs and UltraFeedback dataset to generate 5 responses per prompt. Then, we compare all (cid:0)5 pick the best-worst response pairs to align the DPO policy following (Pace et al., 2024; Dong et al., 2024). We train the policies for 2 epochs at most using AdamW (Loshchilov, 2017) optimizer with learning rate 2e-7 and global batch size of 128, where the batch size follows Dong et al. (2024) and the learning rate is decided by grid search. 2 Evaluation Metrics We evaluate the quality of reward model from two perspectives: the accuracy on Reward-Bench (Lambert et al., 2024) and the quality of policies induced by the reward model. For policies induced by the reward model, we consider two variants: 1. Best-of-N (BoN) policy and 2. aligned DPO policy. Our main focus is for open-ended generation and we use MT-Bench (Zheng et al., 2024) and AlpacaEval-2 (Dubois et al., 2024b) to evaluate. 4.2 MAIN RESULTS Reward Model Accuracy The test accuracies on Reward-Bench are reported in Table 1. RRM improves Chat Hard and Safety by clear margin but sacrifices the Reasoning. Regarding Reasoning, we hypothesize that math and coding are less affected by the non-contextual artifacts and we may use other rewards than an LLM because those are objectives like golden answers. On average, RRM improves RM by an absolute 3.54% accuracy gain. 3https://huggingface.co/datasets/RLHFlow/pair_preference_model_dataset 4The ground truth probability is 1 if is preferred over B, 0 if is preferred over A, and 0.5 if they tie. 5We pick the best hyperparameter by grid search. 6 Model Chat Chat Hard RM RRM 97.77 96.51 51.54 65. Safety Reasoning Average 94.58 78.54 83.90 90.62 80.61 84.15 Table 1: Comparison of test accuracy of Reward-Bench. RRM shows improvement upon RM on Chat Hard and Safety with an average 3.54% improvement of accuracy. Policies Induced by Reward Models We investigate the quality of reward models by evaluating the aligned policies. To study the effect of adding Neutrals in Equation 5, we also train reward model without augmented neutrals (-Neutrals). The results are summarized in Table 2. As expected, ODIN (Chen et al.)6 shows shorter responses than RM and RRM since it explicitly disentangles the length from quality. RRM shows the best performance on both benchmarks over ODIN and RM, with shorter responses generated than RM, suggesting it effectively controls the length as one of the artifact. The added Neutrals have slight improvements on first-turn MT-Bench and AlpacaEval-2. Reward Policy RM RRM RM RRM RM ODIN RRM BoN (N=8) BoN (N=8) BoN (N=64) BoN (N=64) DPO DPO DPO -Neutrals DPO T1 () - - - - 8.02 8.66 8.70 8.65 MT-Bench7 T2 () Overall () AlpacaEval-2 LC (%) () WR (%) () - - - - 6.33 8.13 7.87 8. - - - - 7.27 8.39 8.31 8.44 36.87 47.68 40.52 62.82 33.46 48.29 52.49 51.73 50.14 53.19 57.62 63.03 41.07 37.13 43.31 43.24 Length () 3072 1770 2992 1770 2416 1559 1723 1722 Table 2: Comparison among different reward models on various aligned policies. T1 and T2 stand for the first and second turn of the conversation, respectively. WR stands for win-rate against GPT4. LC stands for length-controlled win-rate. Length is the average number of characters in the generated responses. RRM shows quality improvements over ODIN and RM with shorter responses than RM. Dropping augmented neutral examples slightly hurt the quality. 4.3 LENGTH ANALYSIS To further understand the artifacts learned in reward model, we take length as an example to analyze the reward model training data and aligned policy. Length distribution of training data We study the length (number of tokens) distribution of reward model training datasets. Length is one common artifact that shows bias on both policy training and evaluation. We hypothesize that the bias can possibly come from the reward model training data. The one used in training RM is not well calibrated and the chosen responses are longer on average (Figure 3a and 3b) and by frequency (Figure 3c). On contrary, the RRM training data is better calibrated with length more balanced between chosen and rejected responses in each length bin (Figure 3a and 3b). We further provide length analysis for each data source in Appendix A.2. Length distribution of policies To understand the lengthy bias learned in various policies, we also study the length distribution of generated responses on AlpacaEval-2s (Dubois et al., 2024a) prompts (Figure 4). We observe that the policies induced by RRM generate shorter responses than RM, which implies the correction of lengthy bias by RRM. 4.4 DELIBERATELY DESIGNED ARTIFACTS Artifacts To verify that our proposed method is able to eliminate artifacts, we artificially added an artifact to the chosen responses in reward model training data. More specifically, we add prefix 6Training details in Appendix A.4. 7we do not evaluate BoN policies on MT-Bench because it involves multi-turn. 7 (a) Histogram of response lengths in RM training data. (b) Histogram of response lengths in RRM training data. (c) Percentage of chosen responses being longer or shorter in RM and RRM traininng data. Figure 3: Distribution of response lengths on reward model training datasets. (a) the RM training data has longer chosen responses on average and not well calibrated (large percent deviation in left two bins between chosen and rejected) (b) the RRM training data is well calibrated and the average length of the chosen responses is even shorter than rejected. Additional neutral triplets can further calibrated the model. (c) Around 60% of chosen responses are longer in RM training data. On contrary, the lengths of chosen responses are more balanced in RRM training data. (a) Best of 8 responses (b) Best of 64 responses (c) DPO policy Figure 4: Distribution of response lengths on AlpacaEval-2 prompts of various policies induced by RM and RRM, average length is marked by the dashed line. All policies show lengthy bias towards longer responses for RM comparing with RRM. Sure, here is the response: to the chosen responses with probability 0.1. We train an RM and RRM on the modified reward model training data, respectively. To test the effect of reward model on the policy model, we first sample responses from Gemma2-9b-it model using the AlpacaEval-2 prompts. Then we add the same type of artifact to each response with probability pa = p, where {0.05, 0.1, 0.2, 0.5}. Under this setting, RM trained on the artifact-added data would prefer responses with the artifacts since the chosen responses come with artifacts, even if the responses may contain low-quality answer. RRM is expected to be more robust to the artifact. To verify this, we construct BoN policies using RM and RRM, respectively. As expected, Figure 5 shows that after adding the artifacts, the BoN policies induced by RRM are more robust than RM to artifacts injected in the responses."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "RLHF algorithms The first RLHF framework is based on the proximal policy optimization (PPO) algorithm, which was first popularized by Christiano et al. (2017) and further developed by Bai et al. (2022a); Ouyang et al. (2022). However, getting the deep RL method (PPO) work is challenging especially in the era of LLMs (Choshen et al.; Engstrom et al., 2020). In recognition of this issue, another line of works proposes direct alignment algorithms, where notable examples include SLiC (Zhao et al., 2023), DPO (Rafailov et al., 2024), IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), SimPO (Meng et al., 2024), and DRO (Richemond et al., 2024). These algorithms directly optimize supervised target to optimize the policy model with8 (a) Best of 8 responses (b) Best of 64 responses Figure 5: Proportion of BoN generated responses with artifact versus the rate of injected artifact. For each policy, we first sample (N = 8 or 64) responses on AlpacaEval-2 prompts, then prepend Sure, here is the response: before each response with probability (Rate) 5%, 10%, 20%, 50%, respectively. Then we compute the proportion of BoN responses that have the above artifact (Artifact). The BoN policies induced by RRM are more robust to artifacts injected in the responses, suggesting that the proposed approach enables the model to focus more on the contextual signals instead of context-free artifacts in the reward model training data. out constructing reward model first, hence the name direct alignment algorithms. However, these algorithms learning from fixed dataset are offline and often off-policy without further exploration of the environment. RSO (Liu et al., 2024b) emphasizes the importance of reward model and fixes the distribution shift problem to improve the DPO training, followed by list-wise alignment (Liu et al., 2024a) and the online (iterative) training frameworks (Xiong et al.; Guo et al., 2024; Calandriello et al.). Alternatively, there is also line of work based on the best-of-n sampling, such as RAFT (Dong et al.), BOND (Sessa et al., 2024), BoNBoN alignment (Gui et al., 2024). These algorithms leverage reward model to rank the generated responses and distill knowledge from the best responses. Our approach can benefit RLHF algorithms relying on reward model. Reward Models & Reward Hackings Building superhuman/unbiased reward model is vital for training better chat assistants since it could affect the upper bound of the policies capabilities in the online preference optimization (Wang et al., 2024a; Bai et al., 2022b). Multi-objective rewards (Wang et al., 2024b), RLHF-workflow (Dong et al., 2024), and RMBoost (Shen et al., 2024) are proposed to train more capable reward models. While revealed by Denison et al. (2024); Zhang et al. (2024), reward models are easily hacked by different pattern in different scenario, e.g., length (Singhal et al., 2023) and sycophancy. Recent works employ the model merging (WARP (Rame et al., 2024a) and WARM (Rame et al., 2024b)), and hacking reward decomposition (ODIN (Chen et al.)) to mitigate the hackings in online RLHF. Generative reward models can provide more detailed preference analysis (Yan et al., 2024). For the most accurate reward signal, one can also use verifiable answers in certain domain like math (Xiong et al., 2024). Most model-based methods failed to distinguish between preferences driven by the prompt and context-free artifacts. Our RRM is more advanced in removing the artifacts."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we identified key problem in the current reward training methodology: its inability to differentiate between contextual signals and context-free artifacts. Using causal framework, we explained this effect and improved reward model training by introducing data augmentation approach derived from the framework. Our theoretical analysis and extensive empirical results demonstrated that the proposed techniques effectively enhance both the test accuracy of the reward model and the quality of the policies it induces. Future work will explore filtering augmented pairs and matching artifacts when constructing response pairs, further refining the training process."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022b. URL https://arxiv.org/abs/2212.08073. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Daniele Calandriello, Zhaohan Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of large language models through online preference optimisation. In Forty-first International Conference on Machine Learning. Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf. In Forty-first International Conference on Machine Learning. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. On the weaknesses of reinforcement learning for neural machine translation. In International Conference on Learning Representations. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023. Luigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multi-turn conversations for effecient llm training. arXiv preprint arXiv:(coming soon), 2023. URL https: //huggingface.co/datasets/LDJnr/Capybara. Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, and Evan Hubinger. Sycophancy to subterfuge: Investigating reward-tampering in large language models, 2024. URL https://arxiv.org/abs/2406.10162. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, SHUM KaShun, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research. 10 Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024a. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024b. Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: case study on ppo and trpo. arXiv preprint arXiv:2005.12729, 2020. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In International Conference on Machine Learning, pp. 59886008. PMLR, 2022. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Lin Gui, Cristina Gˆarbacea, and Victor Veitch. Bonbon alignment for large language models and the sweetness of best-of-n sampling. arXiv preprint arXiv:2406.00832, 2024. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691, 2024. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Steffen Lauritzen. Graphical models, volume 17. Clarendon Press, 1996. Kuang-Yao Lee, Tianqi Liu, Bing Li, and Hongyu Zhao. Learning causal networks via additive faithfulness. Journal of Machine Learning Research, 21(51):138, 2020. Erich Lehmann and George Casella. Theory of point estimation. Springer Science & Business Media, 2006. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and Teknium. Openorca: An open dataset of gpt augmented flan reasoning traces. https://https:// huggingface.co/Open-Orca/OpenOrca, 2023. 11 Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, et al. Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. arXiv preprint arXiv:2309.06256, 2023. Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu, and Xuanhui Wang. Lipo: Listwise preference optimization through learning-to-rank, 2024a. URL https://arxiv.org/abs/ 2402.01878. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations, 2024b. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Alizee Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024. Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. Judea Pearl. Causality. Cambridge university press, 2009. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Alexandre Rame, Johan Ferret, Nino Vieillard, Robert Dadashi, Leonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024a. URL https://arxiv.org/abs/ 2406.16768. Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models, 2024b. URL https://arxiv.org/abs/2401.12187. Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, et al. Offline regularised reinforcement learning for large language models alignment. arXiv preprint arXiv:2405.19107, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Pier Giuseppe Sessa, Robert Dadashi, Leonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Rame, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation. arXiv preprint arXiv:2407.14622, 2024. Jiaming Shen, Ran Xu, Yennie Jun, Zhen Qin, Tianqi Liu, Carl Yang, Yi Liang, Simon Baumgartner, and Michael Bendersky. Boosting reward model with preference-conditional multi-aspect synthetic data generation. arXiv preprint arXiv:2407.16008, 2024. Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The trickle-down impact of reward (in-) consistency on rlhf. arXiv preprint arXiv:2309.16155, 2023a. 12 Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang. Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback. arXiv preprint arXiv:2310.05199, 2023b. Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Tyler VanderWeele. Controlled direct and mediated effects: definition, identification and bounds. Scandinavian Journal of Statistics, 38(3):551563, 2011. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling, 2024a. URL https://arxiv.org/abs/2401.06080. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts, 2024b. URL https://arxiv. org/abs/2406.12845. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International Conference on Machine Learning. Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, et al. Building math agents with multiturn iterative preference learning. arXiv preprint arXiv:2409.02392, 2024. Jing Nathan Yan, Tianqi Liu, Justin Chiu, Jiaming Shen, Zhen Qin, Yue Yu, Charumathi Lakshmanan, Yair Kurzion, Alexander Rush, Jialu Liu, and Michael Bendersky. Predicting text preference via structured comparative reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1004010060, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long. 541. Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. theoretical analysis of nash learning from human feedback under general kl-regularized preference. arXiv preprint arXiv:2402.07314, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024. Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, and Tong Zhang. From lists to emojis: How format bias affects model alignment, 2024. URL https://arxiv.org/ abs/2409.11704. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 1269712706. PMLR, 2021. 13 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 PROOFS Proof of Proposition 3.1 Proof. We show the conclusion with special case. Assume = s(X, Y1, Y2) + ϵs with ϵs (0, σs), and = a(X, Y1, Y2) + ϵa with ϵa (0, σa). P(C = 1) = σ(βsS + βaA + α + ϵc) with constants βs, βa, α and random error ϵc (S, A). If βa = 0, then H0 is true. If βa = 1, then H1 is true. In extreme case that Corr(S, A) = 1, then = βasS + αa for some constants αa and βas R+. Then the model cannot tell if βa = 0 or not. This is because when βa = 0, we can still reparametrize it as P(C = 1) = σ((βs βas)S + + (α αa) + ϵc). Proof of Proposition 3.2 Proof. We can prove this by contradiction. If there is causal edge from to C, then the conditional independence relations (Y1, Y2) and do not hold, which contracts to the triplets constructed on Non-contextuals and Neutrals. A.2 REWARD MODEL TRAINING DATASETS We reuse the datasets in RLHFlow (Dong et al., 2024). We list the data sources and number of examples in Table 3. The authors of the original paper delete the samples with similar scores when the scores are available because when the model is well calibrated, these samples are more likely to mislabelled. Thus the total number is smaller than the sum of each individual datasets. We show the length distribution of chosen and rejected responses from each individual data source in Figure 6. HH-RLHF, SHP, HelpSteer, UltraFeedback show bias towards rejected responses as the first length bin. SHP, HelpSteer, and UltraFeedback have longer chosen responses than rejected ones on average. Source HH-RLHF-Helpful8 (Bai et al., 2022a) SHP9 (Ethayarajh et al., 2022) HelpSteer10(Wang et al., 2023) PKU-SafeRLHF11 (Ji et al., 2024) UltraFeedback12 (Cui et al., 2023) UltraInteract13 (Yuan et al., 2024) Distilabel-Capybara14 (Daniele & Suphavadeeprasit, 2023) Distilabel-Orca15 (Lian et al., 2023) Number of Examples 115,396 93,301 37,131 26,874 340,025 161,927 14,811 6,926 Table 3: Composition of reward model training dataset 8https://huggingface.co/datasets/RLHFlow/HH-RLHF-Helpful-standard 9https://huggingface.co/datasets/RLHFlow/SHP-standard 10https://huggingface.co/datasets/RLHFlow/Helpsteer-preference-standard 11https://huggingface.co/datasets/RLHFlow/PKU-SafeRLHF-30K-standard 12https://huggingface.co/datasets/RLHFlow/UltraFeedback-preference-standard 13https://huggingface.co/datasets/RLHFlow/UltraInteract-filtered-standard 14https://huggingface.co/datasets/RLHFlow/Capybara-distibalel-Filter-standard 15https://huggingface.co/datasets/RLHFlow/Orca-distibalel-standard (a) HH-RLHF-Helpful (b) SHP (c) HelpSteer (d) PKU-SafeRLHF (e) UltraFeedback (f) UltraInteract (g) Distilabel-Capybara (h) Distilabel-Orca Figure 6: Distribution of response lengths on each individual source of reward model training data. SHP, HelpSteer, and UltraFeedback show significant lengthy bias showing longer responses in chosen. They also dominate the training dataset, accounting for more than half. 15 A.3 DATA AUGMENTATION PYTHON CODE In Algorithm 1, we show sample code of data augmentation in Python. We expect each element in data contains context, response w, response l. We use neutral to indicate if the label should be tie. Algorithm 1 Example Python Code for Data Augmentation def get_augmented(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]: data_i = data data_j = data_i.copy() random.shuffle(data_j) data_k = data_j.copy() random.shuffle(data_k) for ex_i, ex_j, ex_k in zip(data_i, data_j, data_k): xi = ex_i[context] xj = ex_j[context] xk = ex_k[context] ywi = ex_i[response_w] ywj = ex_j[response_w] ywk = ex_k[response_w] yli = ex_i[response_l] ylj = ex_j[response_l] ylk = ex_k[response_l] # xi_ywi_ywj yield { \"context\": xi, \"response_w\": ywi, \"response_l\": ywj, \"neutral\": False } # xi_ywi_ywk yield { \"context\": xi, \"response_w\": ywi, \"response_l\": ywk, \"neutral\": False } # fill in all other augmented triplets ... # xi_ywk_ylj yield { \"context\": xi, \"response_w\": ywk, \"response_l\": ylj, \"neutral\": True } # xi_ylj_ylk yield { \"context\": xi, \"response_w\": ylj, \"response_l\": ylk, \"neutral\": True } A.4 TRAINING DETAILS FOR ODIN We use the same loss as described in Chen et al.. We train Gemma-2-9b-it for 1 epoch on the same data we used for RM. AdamW is our optimizer and the learning rate is set to 2e-6 with cosine scheduler. We use Flash-Attention to accelerate the training while applying the Deepspeed ZeroStage 3 to get batch size=16 on each GPU (the global batch size is 128) to make sure the calculation of the Pearson correlation between the head value and the length of the responses is stable. 16 A.5 PRELIMINARIES IN CAUSAL INFERENCE We list few critical concepts in this section. For information, we refer the readers to Lauritzen (1996) and Pearl (2009). DAGs and d-separation DAG is set of vertices and set of directed edges (arrows) that connect pairs of these vertices. The causal modeling connects DAG with Markov condition via graphical relation called d-separation (Pearl, 2009). D-separation is relation among three disjoint sets of vertices in directed graph. D-separation and Markov condition connect DAGs and probability distribution. By faithfulness assumption, the d-separation in DAG is equivalent to conditional independence in distribution. The Causal Markov Condition The Causal Markov assumption assumes that variable is independent of every other variable (except Xs effects) conditional on all of its direct causes. With this, DAG defines set of distributions of the form p(y1, ..., yk) = (cid:89) p(yjparents(yj)) Counterfactuals Consider two variables and . We will call the treatment. We call the outcome. For given subject we see (Xi, Yi). What we dont see is what their value of Yi would have been if we changed their value of Xi. This is called counterfactual. Suppose that is binary variable that represents some treatment. So = 1 means the subject was treated and = 0 means the subject was not treated. Let Y1 denote the outcome if the subject is treated. Let Y0 denote the response if the subject is not treated. Then = XY1 + (1 X)Y If we treat subject, we observe Y1 but not Y0. The unobserved variable is called counterfactual. The variables (Y0, Y1) are also called potential outcomes. We define mean treatment effect as θ = E(Y1) E(Y0) = E(Y set = 1) E(Y set = 0)"
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "University of Illinois Urbana-Champaign",
        "University of Maryland, College Park"
    ]
}