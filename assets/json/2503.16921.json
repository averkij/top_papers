{
    "paper_title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO",
    "authors": [
        "Lingfan Zhang",
        "Chen Liu",
        "Chengming Xu",
        "Kai Hu",
        "Donghao Luo",
        "Chengjie Wang",
        "Yanwei Fu",
        "Yuan Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 2 9 6 1 . 3 0 5 2 : r When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO Lingfan Zhang1* Chen Liu2* Chengming Xu3 Kai Hu4 Donghao Luo3 Chengjie Wang3 Yuan Yao2 Yanwei Fu1 1Fudan University 2The Hong Kong University of Science and Technology 3Tencent 4Carnegie Mellon University"
        },
        {
            "title": "Abstract",
            "content": "In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO novel approach that incorporates minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an AdaptiveDPO loss function which improves the DPO loss in two ways: enhancing the models learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks. 1. Introduction Recent advances in diffusion models have revolutionized text-to-image generation, enabling unprecedented visual quality and diversity. These models have demonstrated remarkable capabilities in synthesizing highly detailed and realistic images from textual descriptions, making them invaluable tools for creative industries, education, and entertainment. However, the practical deployment of these mod- *Equal contribution. Corresponding author. Figure 1. Two examples from Pick-a-Pic [14]. For both rows, the winning images are the right ones. els hinges on their ability to align with human preferences, ensuring that the generated outputs not only meet technical standards but also resonate with aesthetic and contextual expectations of users. To address this, researchers have adapted Reinforcement Learning from Human Feedback (RLHF) [18], technique originally developed for language models, to the diffusion paradigm. Methods like DiffusionDPO [22] bypass the need for an explicit reward model by directly optimizing preference alignment through pairwise comparisons, achieving state-of-the-art results in aligning diffusion models with human preferences. Despite these advancements, critical assumption underlying these methods remains relatively less unexplored: 1 the uniformity and reliability of preference datasets. Current approaches implicitly assume that preference labels uniformly reflect universal human judgment. However, such notion fails to account for the inherent subjectivity of visual aesthetics and the practical realities of crowd-sourced annotation. In fact, preference datasets exhibit hidden dichotomy. The majority of annotations reflect consensus criteria such as image fidelity, prompt alignment, and overall visual appeal. These are the patterns that methods like DPO are expected to learn and optimize for. However, significant minority of annotations can significantly deviate from this consensus. These minority data can be attributed to two primary sources. (1) Erroneous annotations arise from human errors, such as annotator laziness, misunderstandings, or misinterpretations of the task. For instance, in Fig. 1(a), the left image is obviously better in terms to either image quality and text fidelity, while the annotator labels the right one as the winning image. (2) Subjective divergences represent valid but niche preferences that conflict with majority standards. These include stylistic biases, cultural differences, or personal tastes that lead annotators to prefer outputs that deviate from the consensus. For example, we asked 25 participants with different genders and ages about which image in Fig. 1(b) is better, considering both image aesthetics and text-image alignment. Among them, 18 participants think the left one is better, while the others think the opposite, which demonstrates the subjectivity divergence. In this paper, we advocate exploring the impact of such dichotomy for DPO. Specifically, we first conduct pilot study where we manually creates minority samples by randomly flipping the original preference. The experiments demonstrate that the presence of these minority samples poses significant challenge for DPO. For example, flipping 20% data degrades ImageReward of SD1.5 finetuned with DPO from 0.16 to 0.00, equivalent to 52% of the performance gain achieved by DPO. To address these challenges, we propose Adaptive-DPO, novel preference learning framework that autonomously identifies and suppresses problematic minority samples while preserving legitimate majority preferences. Our approach leverages two key innovations. First, we introduce self-driven minority-aware metric that jointly models intra-annotator confidence and inter-annotator stability. Intra-annotator confidence measures the certainty of predictions by evaluating agreement across multiple model checkpoints, while inter-annotator stability quantifies the variance in predictions across different training stages. This dual mechanism allows our metric to distinguish between the major preference criteria and other minority preferences without relying on external reward models or costly relabeling efforts. Second, we propose modulating the preference learning process based on the designed metric. This includes instance-specific reweighting to suppress minority samples and adaptive margins to enhance supervision from majority samples. Together, these terms enable AdaptiveDPO to robustly align diffusion models with human preferences while mitigating the impact of subjective annotations. Extensive evaluations demonstrate the effectiveness of Adaptive-DPO across multiple benchmarks and model architectures. Concretely, we compare our method with previous methods using both SD1.5 and SDXL as backbones, on benchmarks including Pick-a-Pic [14] and HPDv2 [24]. Our Adaptive-DPO significantly outperforms the competitors, showcasing its ability to handle ambiguous and subjective annotations. We further analyze our method with abundant ablation studies, showing that Adaptive-DPO is not only better than methods such as re-filtering the training data, and but also can generalize well to other preference optimization variants such as IPO [1]. In summary, the contributions of this work are as follows: We provide the first systematic study of the majority/minority preference dichotomy in diffusion alignment, shedding light on critical but understudied aspect of preference learning. We introduce the novel minority-instance-aware metric by considering both intra-annotator confidence and interannotator stability. We propose Adaptive-DPO, which can not only mitigate the negative impact of minority samples but also enable the model to better leverage reliable majority annotations. We conduct comprehensive empirical validation across multiple metrics, datasets, and backbones, demonstrating the generalizability and scalability of our approach. 2. Preliminaries 2.1. RLHF and Reward Model Reinforcement Learning from Human Feedback (RLHF) fine-tunes large models by aligning them with human preferences through preference optimization with two phases: Reward Modeling: This phase employs the BradleyTerry (BT) model to learn reward function from pairwise data (xw, xl, c), where xw is the preferred answer and xl is the less preferred one. The loss function is defined as: LR(rϕ, D) = E(c,xw,xl) [log σ(rϕ(c, xw) rϕ(c, xl))] (1) where σ denotes sigmoid function, rϕ is the reward model parameterized by ϕ. RL Finetuning: This phase uses the learned reward model to provide feedback to the language model, applying the Proximal Policy Optimization (PPO) algorithm: max πθ EcD,xπθ(xc) [rϕ(c, x)]βDKL [πθ(x c)πref (x c)] (2) 2 2.2. DPO and Diffusion-DPO Objective DPO (Direct Preference Optimization) and Diffusion-DPO both utilize pairwise human preferences without requiring reward model. For DPO, the explicit solution derived from the RLHF framework is: πr(y x) ="
        },
        {
            "title": "1\nZ(c)",
            "content": "πref (x c) exp (cid:19) r(c, x) (3) (cid:18) 1 β where Z(c) = (cid:80) πref (x c) exp (cid:16) 1 β r(c, x) (cid:17) , this means: r(c, x) = β log πr(x c) πref (x c) + β log Z(c). (4) This leads to the DPO optimization objective: LDPO(πθ; πref ) = E(x,yw,yl)D (cid:20) (cid:18) log σ β log πθ(yw x) πref (yw x) β log (cid:19)(cid:21) (5) πθ(yl x) πref (yl x) For Diffusion-DPO, the objective is formulated as: L(θ) = (xw,xl)D,tU (0,T ),xw log σ (cid:0)βT ω(λt) (cid:0)ϵw ϵθ(xw (cid:0)ϵl ϵθ(xl t, t)2 q(xw , t)2 2 ϵl ϵref(xl tq(xl xw),xl txl) 2 ϵw ϵref(xw t, t)2 2 (cid:1)(cid:1)(cid:1) , t)2 2 (6) where ϵ represents the noise prediction network and is the denoising timestep. 3. Pilot Study: Influence of Minority Samples To verify if the minority preference samples can have detrimental effect on the models learning, we in this section conduct an intuitive pilot study. Concretely, since it is hard to detect the minority samples in real world data, we propose to flip random part of the original preference lathe winning images are turned into losing ones. bels, i.e. To show such simulation is reasonable, we in Fig. 2 provide an explanation, in which we perform the same flipping process to 1000 samples of which 10% are labeled as minority samples. As can be found in the figure, as the proportion used for flipping getting larger, more original majority samples are changed into minority ones, indicating larger proportion of minority data. Formally, for both SD1.5 and SDXL, we randomly flip 10%, 20% and 30% data, train them with original Diffusion-DPO, and record several metrics such as ImageReward (IR) [28], PickScore (PS) [14], Aesthetic Score (Aes) [21] and HPS [26]. As presented in Tab. 1, we can find that with more minority samples, the fine-tuning process exhibits significant decline in its efficacy, eventually resulting in total deterioration of the models performance. Specifically, for SD1.5, 3 Figure 2. Proportion of majority/minority at different flip ratios. Table 1. DPO results with different noise level. Larger metric, better performance. Model Flip rate (%) IR () PS () Aes () HPS () SD1.5 SDXL 0 10 20 30 0 10 20 30 0.16 0.05 0.00 -0. 0.87 0.79 0.70 0.66 21.05 20.91 20.83 20.72 22.52 22.53 22.38 22.29 5.31 5.27 5.24 5.21 5.89 5.90 5.86 5.88 26.43 26.32 26.24 26. 27.32 27.21 27.09 26.99 30% flipped samples can even lead to negative image reward. This indicates that the Diffusion-DPO algorithm can be easily affected by the minority samples contained in the training data. Moreover, it is noteworthy that our manually created minority samples are intrinsically different from the real-case minority samples. While our randomly selected minority data forms uniform distribution, the distribution of real minority data could be related with many factors, such as the difficulty of annotation problems, the quality of data, etc. Therefore, it is important to design new algorithm that is robust to both cases. 4. Methodology To address the challenges posed by minority samples in preference datasets, we propose Adaptive-DPO, novel framework designed to identify and suppress problematic minority samples while preserving the integrity of majority preferences. The key novlety of our approach include: (1) self-driven metric that captures both intra-annotator confidence and inter-annotator stability, and (2) an AdaptiveDPO objective that prioritizes the learning of majority labels while mitigating the influence of minority samples. 4.1. Measuring Minority Preferences In the training process of DPO, the supervision signal is derived from preference labeling. As discussed in Sec.1, the labeling process for preference data is not always reliable. Minority labels from two distinct sources can both undermine the efficacy of DPO and degrade model performance. Therefore, robust minority-aware metric must account for both types of minority samples simultaneously. To this end, we begin by presenting simple observation of the DPO formulation. Specifically, Eq.5 can be reformulated as: LDPO(πθ; πref ) = E(x,yw,yl)D [log σ (β(ηθ ηref ))] ηθ = log ηref = log πθ(xwc) πθ(xlc) πref (xwc) πref (xlc) (7) (8) (9) For Diffusion-DPO, as formulated in Eq.6, similar structure can be derived. By minimizing LDPO, the model is effectively guided to maximize ηθ ηref. This mechanism is analogous to optimizing with binary hinge loss. Consequently, the DPO fine-tuning process implicitly steers the model to act as binary preference classifier. Previous works[16] have shown that such models tend to first learn correctly labeled samples, which constitute the majority of the training data, before addressing incorrectly labeled samples, which are often minority cases. Inspired by prior methods in semi-supervised learning [3] and robust learning [12], we propose leveraging the predictions of fine-tuned models to instantiate minority-aware metric. This metric consists of two main components, as described below. Intra-annotator confidence. Samples with obvious preference patterns but incorrectly judged often exhibit the exact opposite preference criteria compared to the majority samples. For instance, images with lower quality or poorer text fidelity may be favored in some incorrectly judged pairs. Since the model can easily learn their features from the majority labels, the predictions for these samples will significantly differ from their original labels. As result, the gap between the models predictions (referred to as the bias) and the given labels remains substantial throughout the training process. To formalize this, we first define metric to measure the difference between the fine-tuned model and the reference model. In detail, given sample of pair, we have ℓθ = ηθ ηref (10) The larger value of ℓθ indicates higher confidence. Afterwards, we begin to define the detailed metric. Suppose we have different models, which are instantiated as main model finetuned by DPO and the historical EMA footprint of it, we give the definition for intra-annotator confidence as follows, cθ(x) = 1 1 (cid:88) m= σ(ℓθ(m)(x) ρ) (11) For cθ, the large value indicates large bias. Inter-annotator stability. On the other hand, as for the samples with more complicated preference criteria and less obvious preference patterns, including them into training data can confuse the model and lead to more difficulty for the model to understand the target preference criteria. For such samples, models judgment may fluctuate because it cannot learn the information well, which would lead to quite unstable predictions regarding these pairs. Therefore, we use an inter-annotator stability term sθ(x) to instantiate the variance phenomenon. The detailed form is as follows, sθ(x) = 1 1 (cid:88) m=1 (cid:0)ℓθ(m) (x) 1 (cid:88) m=1 ℓθ(m)(x)(cid:1) (12) 4 Figure 3. Here, we add 20% label flip to Pick-a-Pic v2 [14] and calculate the metric according to Eq. 13. The axis denotes the interval of the metric and the axis denote the ratio of noisy samples. We can observes significant increase of flipped sample ratio as the increase of the metric value. To combine the two terms together, we choose to directly use the product of them, which can be formulated as: uθ(x) = sθ(x) cθ(x) (13) For our metric, the larger value uθ(x) has, the higher likelihood the corresponding has to be minority labeled one. To understand the mechanism, we give more detailed discussion. When the value is quite large, either sθ(x) or cθ(x) will be large which means the large value of our metric is related to detecting the pattern of large bias or large variance. For the sample with small bias and variance, this value tends to be much smaller. And such kind of samples would be considered as majority sample during the training. Previous methods [12] mainly take the straightforward method that directly re-labels part of the dataset and train binary classifier based on these data. Then the two terms can be defined according to the output of the classifier. However, re-labeling the preference data by human annotator wastes lot of manpower and material resources, thus being almost impossible for larger datasets. Moreover, such an annotation process would again introduce new biased labels. Besides, the binary classifier would require specific design for different input modalities such as images and texts, thus introducing extra complexity. Compared with that, only the finetuning of the target model is required without any extra models for our method. To validate the efficacy of our metric, we conduct pilot experiment. In detail, we manually add flipped label as minority samples to the preference data. Afterwards, we visualize the ratio the flipped samples with different values of the metric. The figure is shown in Figure. 3. By observation, we can find that with the increase of the metric, the ratio of flipped samples also increases accordingly. It can validate our metric as first step. In the experiments we will show that the proposed metric is also effective when dealing with minority data in real scenarios. 4.2. Adaptive-DPO In order to utilize the minority-instance-aware metric uθ to enhance DPO, we mainly follow two intuitions: (1) making minority samples less important and (2) amplifying supervision from the majority samples. Given that larger value of uθ is more likely to represent that the preference data is labeled with minority preference, we hence introduce both weighting coefficient and an adaptive margin term to DPO objective according to the metric as follows, LAdaptiveDP O(πθ; πref ) = Ec,xw,xl (cid:8)Wθ(x)(cid:2) log σ(cid:0)βℓθ(x) Γθ(x)(cid:1)(cid:3)(cid:9) Wθ(x) = sg( 1 1 + k1uθ(x) ) Γθ(x) = sg(k2uθ(x)2 + c2) (14) (15) (16) sg denotes stop gradient. The effect of this weighting term is intuitive. Large uθ indicates sample pair is likely to be labeled as minority, its corresponding Wθ will be relatively small, thus weakening the supervision from this sample. On the other hand, similar to SimPO [17], the margin term can promote the generalization ability of the finetuned model. While SimPO relies on tuning the margin as hyper-parameter, the margin Γθ introduced by us is adaptive according to the minority-instance-aware metric uθ. When k2 < 0, smaller uθ will be accompanied with larger Γθ, i.e. the objective can encourage model to produce more confident prediction with regard to majority samples. On the contrary, for those minority samples, the supervision induced by margin would be negligible, thus being fully controlled by the former weighting coefficient Wθ. We provide further understanding of our loss in the perspective of the gradient with following form: θLAdaptive-DPO(πθ; πref ) = θEc,xw,xl {Wθ(x) log σ (βℓθ(x) Γθ(x))} = E(x,yw,yl)D [βWθ(x)σ (βℓθ(x) + Γθ(x)) θℓθ(x)] (17) For the pair that is more likely to contain minority preference, uθ(x) tend to be larger. As consequence Wθ(x) and Γθ(x) tend to be smaller. The first term will downweight this pair to alleviate the potential negative effect to training. By viewing Eq. 17, we find that if we find suitable value of k2 and c2, the part σ (β ℓθ(x) + Γθ(x)) will be suppressed accordingly which makes this pair less useful in that step. For clearance, we summarize the whole training process in Alg. 1. Algorithm 1: Adaptive-DPO Training Input: Pairwise preference dataset (i), xl (i), c(i))}N i=1 for x(i) in batch do = {x(i) = (xw Output: Target model 1: for batch in do 2: 3: 4: 5: 6: 7: 8: end for Calculate ℓθ(m) (x) using (10) Calculate uθ by (13) using {ℓθ(m) (x)}M Calculate Wθ and Γθ by (15) and (16) m=1 end for Optimize model with loss (14) 5. Experiments 5.1. Experiment Setup Dataset. In order to show the generalizability of our proposed method, we carry out experiments on the text-toimage generation task with both SD1.5 and SDXL models. We follow Diffusion-DPO to adopt Pick-a-Pic v2 [14] for training, which consists of 959k preference data. For evaluation, 500 unique test prompts and 500 unique validation prompts from Pick-a-Pic v2 are utilized. We also use extra test dataset HPDv2 [25] which contains 400 test prompts. Evaluation protocol. Our experiments consist of two main parts. First, we conduct experiments using the same fliplabel setting as in the pilot study to verify that our metric is indeed effective for synthetic minority samples. SD1.5 is adopted for this experiment. Then we train models on the original Pick-a-Pic v2 with the proposed Adaptive-DPO and evaluate our method on other real-world benchmark to both: 1) support our claim that the existing preference datasets have the problem of minority data and 2) show that our method has great practical value in real-life alignment usage. For this part, both SD1.5, and SDXL are used for fine-tuning. We adopt PickScore [14], ImageReward [28], Aesthetic score [21] , and HPS [26] as evaluation metrics. Implementation details. All the experiments are launched by total batch size of 128, and re-scale parameter ρ in 11 being 15 to ensure that the scale of ℓθ(x) in Eq. 13 contains consistency in scale. DPO parameter β in Eq. 14 is set to 1000 for SD1.5 and 2500 for SDXL. For hyper-parameters in Eq. 13 , k1 set to 10 for SD1.5,and 1 for SDXL; k2 set according to β and c2 according to the mean value of logits in each batch to be consistent with the scale of logits in loss for each model. 5.2. Experiments on Synthetic Minority Data To validate the effectiveness of our method, we first conduct simple experiment based on synthetic minority samples. Concretely, following the operation in Sec. 3, given specific proportion, part of the training samples are randomly chosen and their corresponding preference labels are 5 Table 2. Diffusion-DPO results with different label-flip rate on SD1.5. For all metrics, the larger value indicates the model is better. We copy results of DPO from Tab. 1 for better comparison and understanding. Method Label-flip rate (%) IR () PS () Aes () HPS () DPO Ours 10 20 10 20 30 0.05 0.00 -0.05 0.39 0.34 0.31 20.91 20.83 20.72 21.40 21.31 21.26 5.27 5.24 5. 5.43 5.44 5.41 26.32 26.24 26.14 26.77 26.66 26.50 flipped. Then the data is used to fine-tune the pretrained SD1.5 via our proposed Adaptive-DPO. In this way, we can preliminarily verify the effectiveness of the method. As shown in Tab. 2, with Adaptive-DPO, the fine-tuned model enjoys significantly stronger performance than the one fine-tuned with original DPO at the same label-flipped level. Compared with experiment below in Tab. 3, we will see that even compared with model fine-tuned with real data and no synthetic noise, our Adaptive-DPO working on different label-flipped level still turns out to be better than Diffusion-DPO. Moreover, as the label-flipped rate gets larger, our method can to some extent combat against the performance degradation resulted from minority samples, demonstrated by the smaller performance decrement. This reflects the efficacy of our method against such synthetic minority data. The improvement can be attributed to the learning process of the model. In general, during training, the model tends to first learn the information in the majority label, so as to continuously improve the prediction ability of its implicit reward model as in Eq. 4. In this way, cθ as in Eq. 11 will be more credible and more accurate during the training process as the model being improved, thus the whole metric can get more credible. Consequently, the metric can function better and help the model eliminate the negative effect of minority samples. 5.3. Experiments on Real Data As mentioned in Sec. 3, the above experiments on synthetic flipped labels cannot fully present the real value of our method, since the distribution of minority data created during the annotation process can be significantly different from that of the synthetic flipped labels. To this end, we directly fine-tune the pretrained models on Pick-a-Pic v2 datasets with our proposed Adaptive-DPO and other comparison methods including Diffusion-DPO, Robust-DPO, and SFT with chosen samples in data pairs. The results are provided in Tab. 3. One can find that even there is no synthetic flipped label, applying our method is still better than Difsuion-DPO in all these different settings. Specifically, for SD1.5, our method can outperform DPO in terms of ImageReward by 0.21, with consistent results for other settings. Additionally, the Robust-DPO method, while achieving state-of-the-art performance in the noisy domain and possessing substantial theoretical support, demonstrates suboptimal training results on real-world datasets. This indicates that the issue we need to address which is the minority preference problem within preference data, cannot be resolved through simply using the existing Learning with Noise Labels (LNL) methods. Our method, different from LNL-based Robust-DPO, is not only effective for the synthetic noise, but also for the real-case noise, thus indicating the generalization ability of our method. For clearer comparison, we in Fig. 4 visualize some image results generated by different methods and backbones. While SFT generally enjoys good quantitative performance, the generated images are in low quality. Compared with the images generated by Diffusion-DPO, our results generally enjoy better quality, which are consistent with the quantitative results. Specifically, for SD1.5, model fine-tuned with our method can generate more details both for human bodies and backgrounds. As for SDXL, we find that while the pretrained model and DPO finetuned one tend to generate mis-located limbs and deformed hands, which lowers the image quality, our method can help alleviate this problem, resulting in more delicate human portraits. 5.4. Ablation study To further validate the effectiveness of our method, we conduct series of ablation study regarding the design of our objective and several hyperparameters. If not specified, we present results on Pick-a-Pic validation set using SD1.5 as backbone. Is there any straightforward alternatives for solving the minority samples problem? One would ask if it is necessary to design such method to deal with the problem, and if there is any other simpler solution, such as using training data with higher quality. To answer this question, we in this part adopt majority voting strategy to re-filter the training data. Concretely, we use PickScore [14], ImageReward [28], Aesthetic score [21], and HPS [26] to re-annotate the whole Pick-a-Pic v2. In this way, along with the original human annotate results, we get 5 labels for each data pair. Then these data are re-annotated according the majority of the 5 labels and utilized to finetune the pretrained SD1.5 using Diffusion-DPO. The results in Tab. 4 show that majority voting to some extend can enhance the confidence of the datasets, but its performance is not as good as our approach. Furthermore, majority voting with learned scores needs to re-annotate the datasets for several times to select the majority label of every sample, which is less efficient than our Adaptive-DPO considering labor and computation 6 Table 3. Quantitative results on SD1.5 and SDXL. The larger metric indicates the model is better. Backbone Method IR () PS () Aes () HPS () IR () PS () Aes () HPS () IR () PS () Aes () HPS () Pick-a-Pic Valid Pick-a-Pic Test HPDv2 SD1.5 SDXL Pretrain Diffusion-DPO Robust-DPO SFTchosen Ours Pretrain Diffusion-DPO Robust-DPO SFTchosen Ours -0.15 0.16 0.20 0.31 0.43 0.51 0.87 0.93 0.59 0.95 20.58 21.05 21.13 20.88 21.35 22.10 22.62 22.66 22.35 22.77 5.16 5.31 5.51 5.40 5.44 5.86 5.89 6.01 6.09 5. 26.02 26.43 26.68 26.79 26.76 26.80 27.32 27.47 26.80 27.46 -0.15 0.23 0.24 0.33 0.42 0.57 0.86 0.90 0.61 0.95 20.66 21.23 21.21 20.98 21.43 22.16 22.66 22.73 22.40 22. 5.32 5.53 5.52 5.53 5.57 6.01 6.02 6.01 6.08 6.06 26.12 26.61 26.64 26.76 26.86 26.79 27.27 27.36 26.73 27.37 -0.12 0.19 0.30 0.42 0.53 0.76 0.99 1.09 0.81 1. 20.90 21.41 21.58 21.39 21.84 22.84 23.27 23.34 23.04 23.40 5.24 5.42 5.68 5.54 5.58 6.14 6.13 6.13 6.18 6.17 26.72 27.17 27.29 27.53 27.57 27.49 27.96 28.06 27.34 28. Figure 4. Qualitative results with SD1.5 and SDXL as backbone. Please refer to supplementary for the corresponding prompts. cost as well as convenience for training, not to mention reannotating with human sources. Effectiveness of the Adaptive-DPO objective. We try to analyze the design of our proposed objective, for which model variants without the proposed adaptive margin, with linear and quadratic margin, as well as with square, sqrt, sigmoid and linear re-weight are compared. The results are shown in Tab. 5. Other re-weighting methods, such as square, sqrt, and sigmoid, result in lower performance, highlighting the importance of our design. Meanwhile, the quadratic margin, our proposed method, achieves the highest scores in all metrics, demonstrating its effectiveness in enhancing the models capability, which can also be validated in the qualitative results in Fig. 5. Since the proposed margin term works by enhancing the supervision from the majority samples, its impact is reasonably weaker than the reweighting term which can eliminating the negative effect brought by minority samples. Role of different hyperparameters. We conduct the sensitivity studies regarding hyperparameters in this paragraph. Tab. 6 shows that our method is relatively robust to the change of hyper-parameters. Specifically, with k1 increasTable 4. Comparison of Adaptive-DPO and voting strategy. For all metrics, the larger value indicates the model is better. Backbone Method IR () PS () Aes () HPS () IR () PS () Aes () HPS () IR () PS () Aes () HPS () Pick-a-Pic Valid Pick-a-Pic Test HPDv SD1.5 Diffusion-DPO Voting+Diffusion-DPO Ours 0.16 0.32 0.43 21.05 21.28 21.35 5.31 5.40 5.44 26.43 26.62 26. 0.23 0.35 0.42 21.22 21.38 21.43 5.53 5.58 5.57 26.61 26.78 26.86 0.19 0.46 0.53 21.41 21.80 21. 5.42 5.53 5.58 27.17 27.45 27.57 Table 5. Ablation study results among different variants regarding objective design. For all metrics, the larger value indicates the model is better. For the specific formulas of different variants please refer to supplementary material. Variants IR () PS () Aes () HPS () w/o margin + linear margin + quadratic margin (Ours) square re-weight sqrt re-weight sigmoid re-weight linear re-weight (Ours) 0.41 0.42 0.43 0.37 0.21 0.07 0.43 21.35 21.35 21.35 21.34 20.86 20.75 21. 5.41 5.43 5.44 5.61 5.33 5.22 5.44 26.71 26.75 26.76 26.90 26.45 26.28 26.76 Figure 5. Qualitative comparison between models finetuned with and without the proposed margin term. Table 6. Ablation study results among different variants regarding hyperparameter value. For all metrics, the larger value indicates the model is better. Hyperparameter Value IR PS k1 ρ 4 10 12 10 0.38 0.43 0.42 0.39 0.43 21.36 21.35 21.33 21.39 21.35 Aes 5.44 5.44 5. 5.47 5.44 HPS 26.75 26.76 26.73 26.72 26.76 ing from 4 to 10, the ImageReward increases by 0.05 while PickScore gets lower. When k1 is larger than 10, the performance generally saturates, which may be attributed to the overfitting problem. Apart from the above parameter, ρ in Eq. 12 also has an impact on the results. Larger ρ can make the difference in the given implicit label between the minority and majority samples greater, resulting in more obvious improvement in training. Application of our method to other baseline. To show the generalization ability of our method, we select strong baseline IPO [1] and extend our method to its formulation, named as Adaptive-IPO. The results are shown in Tab. 7. It is obvious that with such strong baseline, adopting our method can still lead to significant improvement. This further indicates that our method is generalizable remedy for Table 7. Ablation study results using IPO as baseline. For all metrics, the larger value indicates the model is better."
        },
        {
            "title": "Method",
            "content": "IR () PS () Aes () HPS () IPO IPO+Ours 0.16 0.33 21.09 21.17 5.31 5. 26.48 26.58 DPO and its followers to solve the problem of minority data in the training set. 6. Related Work Due to space limitations, we refer the reader to the supplementary materials for full related works. Aligning large-scale models. Recent advancements in aligning large-scale models with human preferences have introduced efficient alternatives to RLHF. DPO [20] bypasses the need for reward models, while IPO [1], KTO [7], and RPO [30] extend preference optimization to various data settings. Robust-DPO [5] addresses noisy data but struggles with real-world applications. Our method effectively handles real-world preference data. Aligning diffusion models. The alignment of diffusion models has seen significant progress, with DDPO [2] modeling image denoising as Markov decision process. DPOK [8], D3PO [29], and Diffusion-DPO [22] further optimize these approaches. Diffusion-KTO [15] and Diffusion-RPO [13] extend these methods to unpaired data. However, addressing subjectivity and uncertainty in preference annotations remains an open challenge, which our work tackles. 7. Conclusion This paper discusses the minority preference problem in preference data for the first time, and gives an experimental test to verify its existence and its negative influence on the model. We hence propose Adaptive-DPO as solution to this problem. Adaptive-DPO not only demonstrates strong resistance to artificially introduced minority samples, achieving reasonably normal fine-tuning performance even at flipped-label rate as high as 30%, but also shows significant improvement when applied to real-world data, effectively addressing the issue of minority preference in naturally annotated datasets. Furthermore, AdaptiveDPO can be successfully applied across various models and derivative methods, such as IPO, showing versatility in both SD1.5 and SDXL. The improvements observed in these areas highlight its potential to enhance performance in wide range of applications."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. 2, 8, 11 [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 8, 11 [3] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudolabeling for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, pages 69126920, 2021. 4 [4] Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 14311439, 2015. 11 [5] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv:2403.00409, 2024. 8, 11, 12 [6] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, and Mubarak Shah. Curriculum direct preference optimization for diffusion and consistency models. arXiv preprint arXiv:2405.13637, 2024. [7] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. 8, 11 [8] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 8, 11 [9] Yanwei Fu, Timothy Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, and Yuan Yao. Robust subjective visual property prediction from crowdsourced pairwise labels. IEEE transactions on pattern analysis and machine intelligence, 38(3):563577, 2015. 11 [10] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models. arXiv preprint arXiv:2404.09824, 2024. 11 [11] Aritra Ghosh, Himanshu Kumar, and Shanti Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2017. 12 [12] Arushi Goel, Yunlong Jiao, and Jordan Massiah. Pars: Pseudo-label aware robust sample selection for learning with noisy labels. arXiv preprint arXiv:2201.10836, 2022. [13] Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. 8, 11 [14] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 3, 4, 5, 6, 13 [15] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. Advances in Neural Information Processing Systems, 37:2489724925, 2025. 8, 11 [16] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in neural information processing systems, 33:2033120342, 2020. 4 [17] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. 5 [18] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 1 [19] Devi Parikh and Kristen Grauman. Relative attributes. In 2011 International conference on computer vision, pages 503510. IEEE, 2011. 11 [20] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 8, 11 [21] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 3, 5, 6, 13 [22] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 1, 8, 11 [23] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. 12 [24] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 9 [25] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 5 [26] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference. arXiv preprint arXiv:2303.14420, 1(3), 2023. 3, 5, 6, 13 [27] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 26912699, 2015. 11 [28] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 3, 5, 6, 13 [29] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 8941 8951, 2024. 8, [30] Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou. Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts. arXiv preprint arXiv:2402.10958, 2024. 8,"
        },
        {
            "title": "Supplementary Materials",
            "content": "A. Discussion Relationship with previous works. One would note that there are several papers working on noisy data during finetuning LLMs with DPO, such as Robust-DPO [5] and Gao et. al. [10]. We argue that their scopes are different from ours. For text data, the preference criteria is relatively simple, e.g. length or accuracy. Therefore, the noisy samples studied in the previous works mainly belong to the erroneous annotations. Consequently, [10] proposed detecting the noisy sample with only confidence-based metric. Different with that, for image data we further consider the minority samples caused by subjectivity divergence, which however cannot be reasonably detected by the previous confidence-based metric. To this end we propose the Adaptive-DPO with the novel minority-aware metric. Moreover, our method is self-adaptive to different proportion of minority samples in the training data, which is different from Robust-DPO requiring noisy levels. Future works. Based on our proposed Adaptive-DPO, future work can focus on exploring more comprehensive analyses about the minority data in the preference data. Investigating the underlying principles of this approach could offer deeper insights into its mechanics, which may lead to more targeted enhancements and wider applicability across different domains. Understanding the theoretical foundations will also contribute to fine-tuning the parameter space for even better performance under different conditions, further reinforcing the methods adaptability and robustness. B. Related Work B.1. Aligning large scale models Due to the storage and computational limitation in RLHF, several alternative approaches have been proposed to effectively learn information from human preferences. First, the emergence of DPO [20] allows for RLHF to bypass the need for training reward model, enabling the model to directly learn from preference data, which makes the training process more convenient and efficient. Next, IPO [1] introduced new optimization approach that utilizes squared losses. Subsequently, KTO [7] expanded the application of preference optimization to unpaired data. Building on this foundation, RPO [30] made improvements that can be applied to both paired and unpaired data. In response to the presence of noisy data, Robust-DPO [5] was proposed. However, we found that while Robust-DPO performs well on synthetic noisy data in the image generation domain, it does not yield improvements when applied to real-world data. Our method is equally effective for real-world data. B.2. Aligning Diffusion Models Following the rapid development of large language models, the alignment of diffusion models with human preferences has also advanced significantly. DDPO [2] was the first to model the image denoising process as multi-step Markov decision process, effectively applying reinforcement learning to the alignment of diffusion models. DPOK [8] introduced KL loss and value function learning, further optimizing the methods of DDPO. D3PO [29] applied DPO to diffusion models, making the alignment with human preferences more convenient. Meanwhile, Diffusion-DPO [22] further derived the loss for D3PO, directly optimizing the models predicted noise within the loss, establishing it as the state-of-the-art method for applying DPO in diffusion models. After that, Diffusion-KTO [15] applied KTO to diffusion model, and Diffusion-RPO [13] applied RPO to diffusion model, making better use of the unpaired preference data in field of image generation. Also, Curriculum-DPO [6] aims to enhance the effectiveness of DPO by gradually introducing pairs that differ by increasingly subtle (finelevel) details during the training process. However, there has yet to be proposed solution addressing the subjectivity and inherent uncertainty in the preference data annotation process, which is the issue this paper seeks to address. B.3. Relative Attribute Relative attribute [19] is concept that has been mentioned in the field of image classification. Distinguishing from binary attribute, it refers to image features that are subjective or difficult to assess, necessitating richer representation beyond binary numerical values. In this era of large models, we find the learning of human preferences remains strongly correlated with the earlier proposed learning of relative attributes. Initially, relative attributes [19] focused on classlevel attribute comparisons, also there are some studies like [9] addressing outliers at the instance level. This paper aims to tackle the issue of minority preferences that inherently exist in crowd-sourced data. We focus on instance-level comparisons and propose novel method for DPO regarding preference as kind of relative attribute. B.4. Learning from noisy paired crowd-sourced data Training more robust model using dataset with noisy labels is the target of learning with noisy labels. Methods employed are mostly in the field of image classification, including robust algorithm and noisy label detection. Robust algorithm designs specific modules to ensure that the network can be well trained from the noise data set which includes the construction of robust networks such as [4, 27], 11 robust loss functions like [11].To give solution to noisy data in DPO, Robust-DPO [5] uses robust loss function to improve the models resistance to the noisy label; [23] provided an insight of why noisy label influence reward model, and give their approaches to solve it. However, the distribution of paired crowd-sourced data is significantly different from image classification tasks, correct or wrong is hard to define sometimes as stated in Sec.1; the difference between preference annotations can be caused by either pure mistakes or annotators subjectivity understanding. The former shows us noisy labels is indeed encompassed within the realm of minority preferences, while the latter factor makes it more complex than LNL. In this paper, we focus on real-world preference data and proposes an interpretable and effective method to clarify which label is more consistent with general public, rectifying the negative influence of minority preferences. C. Additional Results of Different Flippedlabel Rate The experiments in the following Tab.8 complement the synthetic minority experiment in Sec.5.2 of main paper, adding evaluation on HPDv2 and Pickapic v2 test dataset. We also train SD1.5 using Robust-DPO on different flippedlabel level, which can be used as further comparison. As can be seen from the comparison, our approach is much better than Robust DPO at different flip rates. D. Formualtion of different variants of reweight and margin We give below the formulas for the various vriants in Sec.5.4. Linear re-weight is as Eq. 18; quardratic re-weight is as Eq. 19; sqrt re-weight is as Eq. 20; sigmoid re-weight is as Eq. 21. Linear margin is as Eq. 22, quardratic margin is as Eq. 23. Wθ(x) = Wθ(x) = Wθ(x) = 1 1 + k1uθ(x) 1 1 + k1uθ(x)2 1 (cid:112)uθ(x) 1 + k1 1 1 + ek1uθ(x) Γθ(x) = k2uθ(x) + c2 Γθ(x) = k2uθ(x)2 + c2 Wθ(x) = (18) (19) (20) (21) (22) (23) 12 E. Details for Adaptive-IPO Based on the ℓθ(x) in Eq.10, Adaptive-IPO loss can be written as: LAdaptiveIP O(πθ; πref ) = Ec,xw,xl (cid:2)Wθ(x) (cid:0)ℓθ(x) Γθ(x) (cid:1)2(cid:3) 1 2β (24) The qualitative results of using IPO as baseline are shown in Fig. 6, which are consistent with those of using DPO as baseline. Figure 6. Qualitative comparison between using IPO and using Adaptive-IPO with SD1.5 as backbone. F. Prompts used for qualitative results SD1.5: fashion photograph of Harley Quinn standing in the middle of busy street, surrounded by crowd of paparazzi, confident and poised, fashionable clothing, vibrant color, sharp lines and high contrast, 12k resolution, Canon EOS R5, natural lighting, 50mm lens. goofy owl. cute cartoon anthropomorphic african american insta baddie dog fursona wearing hip hop fashion and heels, trending on Artstation, gangster, vector drawing style, character design, style hybrid mix of patrick brown and kasey golden, dribbble 8k, airbrush concept art, full body, furry art. digital painting of satyr archer. needle-felted robot. photograph of mountain. The joker holding pistols, vray, fantasy art, art by Russ Mills, blending, smooth, serious, detailed expressions, artstyle, detailed eyes, HDR, UHD, 64k, RTX, sharp, sharp focus, highly detailed, intricate detail, professional, artistic flow, ultra detailed, high resolution illustration The gate to the eternal kingdom of angels, fantasy, digital painting, HD, detailed. SDXL: 80s retrofuturism space-age, man as zoo keeper care about alien animal, very interesting movie set, beautiful clothes, insane details, ultra-detailed, extremely expressive body, photo portfolio reference, retrospective cinema, KODAK VISION3 500T, interesting color palette, cinematic lighting, DTM, Ultra HD, HDR, 8K. Table 8. Additional quantitative results of Diffusion-DPO, Robust-DPO and our method on different flipped-label level with SD1.5. The larger metric indicates the model is better. Pick-a-Pic Valid Pick-a-Pic Test HPDv2 Methods Flip Rate(%) IR PS Diffusion-DPO Robust-DPO Ours 10 20 10 20 30 10 20 30 0.05 0.00 -0.05 0.11 0.03 -0.02 0.39 0.34 0.31 20.91 20.83 20. 21.01 20.88 20.77 21.40 21.31 21.26 Aes 5.27 5.24 5.21 5.45 5.41 5.27 5.43 5.44 5. HPS IR PS 26.32 26.24 26.14 26.56 26.44 26.33 26.77 26.66 26. 0.09 0.05 -0.05 0.10 0.06 -0.01 0.39 0.36 0.26 20.99 20.92 20.81 21.02 20.94 20.83 21.44 21.34 21. Aes 5.44 5.41 5.38 5.45 5.43 5.38 5.59 5.59 5.58 HPS IR PS 26.40 26.38 26.26 26.45 26.40 26.27 26.85 26.77 26.64 0.12 0.04 -0.03 0.17 0.09 -0. 0.49 0.47 0.31 21.29 21.20 21.08 21.37 21.25 21.10 21.84 21.79 21.60 Aes 5.38 5.34 5. 5.59 5.56 5.50 5.59 5.60 5.54 HPS 27.05 26.99 26.87 27.14 27.05 26.89 27.51 27.43 27. anime girl wearing white thighhighs. doctor wearing scrubs, holding needle, staring at the camera. cute cartoon anthropomorphic african american insta baddie dog fursona wearing hip hop fashion and heels, trending on Artstation, gangster, vector drawing style, character design, style hybrid mix of patrick brown and kasey golden, dribbble 8k, airbrush concept art, full body, furry art. beautiful woman. photo of woman. analog style, face elon musk as like spiderman, 1080p, 16k Resolution, High Quality Rendering, RTX Quality, Realistic, Life Like. white background. G. Computation Cost of Adaptive-DPO Diffusion-DPO trained 12000 iterations on 8 V100 with total batch size of 128 takes about 25 hours. The time taken by Adaptive-DPO is about 32 hours; For Majority Voting+DPO training, it takes about 10 hours to re-label the data with PickScore [14], ImageReward [28], Aesthetic score [21] , and HPS [26] on 8 V100, so the total time of data preprocessing and training is about 35 hours. In general, our method can achieve significnatly better results than DPO with majority voting with less time required. H. More Qualitative Results for Synthetic Data In Fig. 7 we show some qualitative results for synthetic minority experiment. One can find that while the original DPO is vulnerable to the synthetic noise, our method is more robust against these data. I. More Qualitative Results for Real Data In Fig. 8 we visualize more results including pretrained SD1.5/SDXL, models finetuned with DPO and models finetuned with the proposed Adapter-DPO. Our method enjoys generally better image quality. 13 Figure 7. More qualitative comparison for Synthetic Data. The upper image grid denotes results generated by DPO under different noise rate, and the bottom one denotes ours. Note that the noise rate only represent the synthetic noise, but not the original noisy data contained in the training set. J. Additional Results for Portrait images Additionally, we train SD1.5 with self-collected portrait images using both Diffusion-DPO and Adaptive-DPO, the above pictures in Fig.9 shows us the results. Our methods achieves fewer deformities, clearer facial features, and high-quality portrait results. Figure 8. More qualitative comparison with SD1.5 and SDXL as backbone."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. 2, 8, 11 [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 8, 11 [3] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudolabeling for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, pages 69126920, 14 Figure 9. Additional results with SD1.5 as backbone training with portrait images. 2021. [4] Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 14311439, 2015. 11 [5] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv:2403.00409, 2024. 8, 11, 12 [6] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Nicu Sebe, and Mubarak Shah. Curriculum direct preference optimization for diffusion and consistency models. arXiv preprint arXiv:2405.13637, 2024. 11 [7] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. 8, 11 [8] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 8, 11 [9] Yanwei Fu, Timothy Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, and Yuan Yao. Robust subjective visual property prediction from crowdsourced pairwise labels. IEEE transactions on pattern analysis and machine intelligence, 38(3):563577, 2015. 11 [10] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models. arXiv preprint arXiv:2404.09824, 2024. [11] Aritra Ghosh, Himanshu Kumar, and Shanti Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2017. 12 [12] Arushi Goel, Yunlong Jiao, and Jordan Massiah. Pars: Pseudo-label aware robust sample selection for learning with noisy labels. arXiv preprint arXiv:2201.10836, 2022. 4 [13] Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. 8, 11 [14] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 3, 4, 5, 6, 13 [15] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. Advances in Neural Information Processing Systems, 37:2489724925, 2025. 8, [16] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in neural information processing systems, 33:2033120342, 2020. 4 [17] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. 5 [18] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 1 [19] Devi Parikh and Kristen Grauman. Relative attributes. In 2011 International conference on computer vision, pages 503510. IEEE, 2011. 11 [20] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 8, 11 [21] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 3, 5, 6, 13 [22] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 1, 8, 11 [23] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. 12 [24] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2 [25] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 5 [26] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference. arXiv preprint arXiv:2303.14420, 1(3), 2023. 3, 5, 6, [27] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 26912699, 2015. 11 [28] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 3, 5, 6, 13 [29] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 8941 8951, 2024. 8, 11 [30] Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou. Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts. arXiv preprint arXiv:2402.10958, 2024. 8,"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Fudan University",
        "Tencent",
        "The Hong Kong University of Science and Technology"
    ]
}