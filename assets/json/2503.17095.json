{
    "paper_title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
    "authors": [
        "Kwan Yun",
        "Chaelin Kim",
        "Hangyeul Shin",
        "Junyong Noh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}."
        },
        {
            "title": "Start",
            "content": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields Kwan Yun1 Chaelin Kim1 Hangyeul Shin2 Junyong Noh1 1KAIST, Visual Media Lab 2Handong Global University 5 2 0 M 1 2 ] . [ 1 5 9 0 7 1 . 3 0 5 2 : r Figure 1. Results of FFaceNeRF. With few-shot training, our method can edit 3D-aware images from desired layouts."
        },
        {
            "title": "Abstract",
            "content": "Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the project-page. 1. Introduction In the growing field of digital image synthesis, the ability to create realistic and controllable facial images is crucial, especially for applications in VR/AR, personalized avatar creation, and medical purposes. To deliver high-fidelity facial images and support their extensive customization, precise manipulation of facial attributes and intrinsic features is essential. To achieve this, image-based editing methods have been proposed [2, 6, 39, 46]. Although these methods successfully achieve high performance in generating edited images by controlled features, they suffer from inconsistency when the camera view is changed. Recent advancements in 3D face editing, driven by generative Neural Radiance Fields (NeRF) [4, 34], have significantly improved the resolution and quality of the edited images. These methods enable 3D-aware face generation and editing using segmentation masks. Early studies [5, 43] required large training datasets to build 3D segmentation fields, while recent studies [18, 42] have reduced the burden of data collection by utilizing pre-trained segmentation networks. Unfortunately, the use of pretrained segmentation network necessitates mask with fixed layout, making it impossible to edit the regions that are not included in the segment labels of the layout. The purposes of editing may differ from session to session, requiring different mask layout each time. For example, makeup artist would need eyelid control for visualizing makeup effects, while plastic surgeon would need detailed nasal ala editing for the previsualization of surgery. Achieving this with the previous methods either requires an extensive dataset segmented with various layouts or, by chance, finding pretrained segmentation network with the intended layout. Neither of these approaches is an attractive direction. As viable alternative, we introduce FFaceNeRF, new NeRF-based face editing method that can significantly enhance the versatility in 3D-aware face editing by fewshot training. FFaceNeRF utilizes pretrained segmentation network with fixed mask at the pretraining process while adding geometry adapter with feature injection to adapt the geometry decoder to the desired mask structure. With the proposed tri-plane features injection and augmentation strategy, Latent Mixing for Triplane Augmentation (LMTA), FFaceNeRF enables effective manipulation of geometric attributes with as few as 10 training samples, based on the adapted mask layouts. Additionally, during inference, our new overlap-based optimization method enables precise and effective editing of small regions. To determine which layers from the latent space are most effective for LMTA while preserving semantic information, we evaluated the capability of each layer and assessed its impact. Through comparison with existing NeRF-based facial editing methods, we demonstrate that our method outperforms others in terms of flexibility, control, and resulting image quality. The main contribution of this paper can be summarized as follows: We introduce FFaceNeRF, few-shot 3D face editing method that employs newly proposed geometry adapter with feature injection for efficient few-shot training. We propose an overlap-based optimization process to effectively manage customized masks, even in small regions. 2. Related work 2.1. 3D-aware Face Image Synthesis With advancements in style-based generators [22, 23], NeRF [34], and 3D Gaussian Splatting [24], 3D-aware face generative models have become increasingly popular. These models typically employ NeRF [4, 11, 14] or 3D Gaussian Splatting [26, 31, 35, 48] representations to generate high-quality 3D faces, producing realistic facial images with fine geometric consistency. Although the quality of the generated images is high in terms of realism and resolution, the conditional generation capacity and controllability of the methods are limited. Therefore, several methods have been proposed to provide way to use diverse inputs, including sketches [20], segmentation masks [7, 54], and facial attribute parameters [29, 44], for controllable 3D-aware face generation. Because the focus of these methods is on the conditional generation of diverse faces and not on editing of the input identity, they are not suitable for 3D-aware face editing. 2.2. 3D-aware Face Image Editing To edit face images in 3D-aware manner, several methods has been proposed [16, 17, 27, 55]. Unlike point-, sketch-, or text-based methods [8, 12, 30], mask based methods offer precise control by explicitly defining areas for modification while preserving unedited regions. SofGAN [5] uses semantic volumes trained on 3D segmentation dataset for 3D editable image synthesis while FENeRF [43] attempts to edit the local shape and texture in facial volume via GAN inversion. More related to our work, IDE-3D [42] and NerRFFaceEditing [18] utilize disentangled representations for efficient tri-plane-based 3D generative models [4] and pretrained segmentation networks [49] for high-resolution face editing. While these methods have been successful in global editing, their editing capacities are limited to an employed fixed segmentation layout. In contrast, our method utilizes the disentangled representation and segmentation network during the pretraining stage while enabling segmentation mask adaptation through our geometry adapter, trained with only 10 images. This few-shot training process dramatically improves versatility in face editing, allowing fine control over details such as the pupils, nasal area, and more. 2.3. Utilizing Features from Generative Models Generative models [4, 1315, 22, 23, 37, 38] have produced state-of-the-art results in tasks related to unconditional and conditional image generation. Inspired by their powerful image generation capabilities, many attempts have been made to utilize features from pretrained generative models for downstream tasks, as these features are known to contain richer information than original RGB images [1, 3, 9, 10, 28, 32, 33, 45, 47, 50, 51, 53]. However, both utilizing tri-plane features and utilizing features for 3D-aware face editing have not been explored. We first conduct experiments on utilizing tri-plane features to confirm if injecting these features helps training the geometry adapter. Then, we further analyze and demonstrate whether augmenting these features enhances the robustness and effectiveness of the geometry adapter. These approaches collectively enhance our method to achieve versatile few-shot face editing. 3. Methods 3.1. Pretraining FFaceNeRF is built upon EG3D [4] and follows the pretraining process of NeRFFaceEditing [18] for disentangled appearance and geometry representation as shown in the Figure 2. This pre-training starts with training appearance decoder Ψapp that outputs face volume, which becomes images when rendered. Then, using pretrained face segmentation network [49], geometry decoder Ψgeo is trained, which outputs segmentation mask volume that corresponds to the segmentation of the face volume. Here, following NeRFFaceEditing, Ψapp accepts original tri-plane tri as input while Ψgeo accepts normalized tri-plane ˆF tri. 2 By combining these two trained networks, we can generate face image and the corresponding segmentation mask from latent code. Unfortunately, because Ψgeo is trained using pretrained segmentation network, it can produce fixed segmentation volume only. viewing direction with semantics, most generative NeRFs include data processing steps that align facial features like eyes, nose, and lips. EG3D is one such model where the viewing direction is indeed related to semantics, thus we inject vd in addition to ˆF tri. This information injection and its fusion with the previous output of Ψgeo help Φgeo to be trained effectively, preserving coarse information while incorporating fine geometric details. 3.4. Latent Mixing for Triplane Augmentation Figure 2. Pretraining stage of FFaceNeRF following EG3D [4] and NeRFFaceEditing [18] for disentangled representation. 3.2. FFaceNeRF Our core idea is to add geometry adapter Φgeo that modulates the output of Ψgeo, producing the desired mask layout. FFaceNeRF can be trained with small number of data using the process described below. 1. First, random latent code w+ and corresponding face image Isource are generated using pretrained appearance decoder Ψapp (Top of the Figure 3). With this image, the user makes few customized segmentation masks in desired layout. 2. Latent mixing is applied between this w+ and another randomly generated latent w+ for LMTA, and the geometry adapter Φgeo is trained with all other parameters frozen. In the following subsections, we will describe the structure of geometry adapter Φgeo with feature injection (Sec. 3.3), LMTA (Sec. 3.4), training process (Sec. 3.5), and finally the proposed editing method (Sec. 3.6). 3.3. Geometry Adapter The geometry adapter Φgeo is network that modulates the output of the geometry decoder Ψgeo using few training data. Because the output dimensions may vary depending on the layout of the mask, Φgeo is added at the end of Ψgeo to produce mask volume whose dimensions correspond to the number of layout classes. Φgeo consists of lightweight MLP, enabling fast training and inference. Φgeo enables to output the segmentation in desired layout. While Φgeo would work by itself, the pre-training process for Ψgeo focuses on maintaining geometry information for its original classes, causing other information to be discarded. Therefore, we directly inject the tri-plane feature ˆF tri and view direction vd, as shown in the lower right corner of Figure 3. The tri-plane feature, rich in information necessary for face generation, is incorporated before any details are lost. The view direction also guides the geometry information. While NeRF generally does not associate the The geometry adapter, Φgeo, is trained with small number of data (e.g., 10 samples) to enable FFaceNeRF to handle diverse segmentation masks. Consequently, avoiding overfitting is crucial consideration. Therefore, we employ LMTA, an augmentation which maintains the semantic information while increasing the diversity of the original triplane. Specifically, our back-bone architecture inherits from style-based generator [4, 23] which is known to contain coarse-to-fine information through the early to later layers of the generator. This means that earlier latent code contains more geometric and coarse information while later code contains more color and fine details [19]. When training Φgeo to adapt the semantic mask volume to desired layout, details that do not affect the semantic information (such as hue or saturation) can be changed for data augmentation to ensure diverse inputs."
        },
        {
            "title": "Additional",
            "content": "to the ground-truth latent code w+ R14512, we randomly sample and pass it through the pretrained mapping network of EG3D [4] to generate random latent code w+. This latent code w+ is blended with the w+ using mixing weight α, and the result is passed through the tri-plane generator to output modified triplane feature tri is directly input to Ψapp, while normalized version, denoted as ˆF tri, is used for both Ψapp and injection to Φapp. For the mixing, only the latent code at certain selected layer Sel {0, 1}14 is used. This Sel is for the latent code whose semantic information is not changed. This process can be written as follows: tri. w+ = Mapping(z), where (0, I) (1) tri = G(α Sel w+ + (1 α) Sel w+ + (1 Sel) w+) The experiments to find the best Sel to choose will be discussed in Sec. 4.3. 3.5. Training FFaceNeRF is designed to facilitate editing of face images after training with small number of data, as we mentioned in Sec. 3.3. During training, only the geometry adapter Φgeo is updated, while other components such as G, Ψapp, and Ψgeo remain frozen. We train Φgeo with an augmented tri-plane feature ˆFtri to ensure that the geometry adapter is trained on varied data, enhancing its robustness and generalization. Additionally, we employ regularization to en3 Figure 3. Overview of FFaceNeRF. LMTA is conducted during the training of Φgeo. Φgeo takes as input the concatenation of normalized tri-plane feature ˆF tri (yellow box), view direction vd (white box), outputs of Ψgeo, which are segmentation labels Seg (blue box), and density σ (red box). Density σ is directly used from the output of Ψgeo, without further training using Φgeo. sure that the ground-truth mask can be generated from nonaugmented tri-plane features Ftri. The training objectives consist of two parts, LCE and Lovlp. LCE represents cross-entropy loss, which measures the dissimilarity between the predicted probability p(yi) and the ground truth class value yi for pixel i. This loss is crucial for ensuring that Φgeo learns to generate masks that closely match the ground truth. However, LCE calculates the pixel-wise correct rate which are prone to ignoring classes with small regions. To address this, we adopt an overlap loss Lovlp using DICE coefficient [41]. Because LCE is faster to optimize, only LCE is trained first and then Lovlp is added later. The objective is defined as follows: Ltotal = LCE + λLovlp, where LCE = (cid:88) (cid:18) Lovlp = 1 [yi log(p(yi)) + (1 yi) log(1 p(yi))] p(yi)yi + ϵ 2 (cid:80) p(yi) + (cid:80) yi + ϵ (cid:80) (cid:19) (2) (3) (4) Here, Lovlp quantifies the negative portion of the correct overlaps for each labels and ϵ is smoothing factor used to prevent division by zero. λ is the weight for the overlap loss using DICE coefficient, which is set to zero during the initial stage. 3.6. Inference To edit real face image I, we first invert the image into the latent space using pivotal tuning inversion [36]. This process yields projected latent code and finetuned generator G. With this w, FFaceNeRF predicts segmentation mask based on the trained data. The user then edits the mask to produce an edited mask S. To generate an edited face image , we optimize an editing vector δw+ so that the predicted segmentation mask 4 RCW matches the target mask RCW . Here, C, , and indicate the number of segmentation channel, width, and height, respectively. From this optimization, we obtain tri-plane feature tri that can generate the edited image . The process is defined as follows: (S, ) = (Φgeo(Ψgeo( ˆF where tri = G(w + δw+) tri)), Ψapp(F tri)), (5) The optimization employs the following losses as the minimization targets: Ledit = LLP IP S(I (1 r), (1 r))+ (6) λCELCE(S, S) + λovlpLovlp(S, S), where LLP IP indicates an LPIPS [52] loss whose purpose is to retain the unchanged region (1 r) R1W . Both LCE and Lovlp ensure editing fidelity. This optimization process differs from the conventional probability-based approaches [18, 32]. The conventional approaches compare the cross-entropy between and S, and therefore often fail to handle changes of labels in small regions. Our overlapbased optimization process ensures that is aligned with even for small regions while preserving the rest of the image unchanged. 4. Experiments 4.1. Implementation details We trained and tested our FFaceNeRF on computer with an NVIDIA RTX A6000 GPU. The training of Φgeo was conducted for 5,000 steps with the batch size of 4. The λ in Eq. (2) was set to 0 for the first 4,000 steps and set to 0.1 for the last 1,000 steps, and λCE and λovlp used in Eq. (6) Figure 4. Examples of dataset with different segmentation layouts. Green boxes are close-up views of eye regions while red boxes are close-up views of nose regions. Figure 6. Visualization of tri-plane mixing layers with the top L1 metrics. The geometry and the semantic information such as the outline of chin and beard are changed even when only 2 layers are mixed. Figure 5. Semantics-augmentation tradeoff: When mixing earlier layers, semantics and tri-plane feature information change largely (high L1, low mIoU). On the other hand, when mixing later layers, semantics and augmentation change little (low L1, high mIoU). were set to 0.5 and 1, respectively. The mixing rate α used in Eq. (1) was set to 0.5 in all of our experiments. The learning rate was increased until 0.03 and then decreased until 0 at the end of the training using OnecycleLR [40]. The total training required around 40 minutes. 4.2. Dataset For experiments, we prepared three different mask datasets with different layouts: Base segmentation layout (Base), Eyes-specialized segmentation layout (Eyes), and Nosespecialized segmentation layout (Nose). Base consisted of 17 classes, while Eyes and Nose were composed of 19 classes each. The Eyes and Nose layouts were built upon the Base layout by adding details for each layout as shown in Figure 4. For training, we annotated 10 segmentation masks from randomly generated images, sharing the source identity across all three datasets. For quantitative evaluation, we annotated 22 segmentation masks in the Base layout. Another 30 segmentation masks in the Base layout were annotated for an additional dataset scaling experiment. For qualitative evaluation, we randomly sampled 40 images from CelebA-HQ [21] and edited the mask using Eyes and Nose layouts. 4.3. Latent Mixing for Triplane Augmentation The style-based generator is known to contain coarse to fine details as the layers progress outwards. We conducted an experiment on mixing single layer with the latent code of w+ , where [1, 14] indicates the index of layer. This experiment was conducted to determine in which layer the 5 Figure 7. Visualization of tri-plane mixing layers based on the top mIoU metrics. The geometry and the semantic information are not changed while colors are changed. semantic and geometric information changes or remains unchanged. We randomly sampled 1,000 latent codes w+ from random passed through the mapping network and generated corresponding images I. In addition, we sampled another w+ from random z. We mixed this latent code w+ with w+ only in the i-th layer and generated an image I. The mean Intersection over Union (mIoU) was calculated between and using their segmentation masks obtained from the pretrained segmentation network [49], to determine until which layer we can utilize LMTA. high mIoU indicates the retention of semantic information after the mixing. In addition to mIoU, we calculated the L1 distance between the tri-plane G(Sel w+ +(1Sel)w+) and G(w+) to check the augmentation amount. higher L1 value indicates larger difference between the augmented tri-plane and the original tri-plane, allowing the network to be trained with more diverse data. As shown in Figure 5, we observed trade-off between semantics preservation and augmentation effectiveness depending on the selection of mixing layers. When earlier layers are selected, semantics change, failing to maintain the geometric structure of the original image, but diversity increases. Conversely, selecting later layers results in higher retention but modest diversity. Additionally, we conducted experiments on mixing multiple layers to find better augmentation effect. Specifically, we tested with mixing the top layers and measured mIoU and L1 within the range of N=[2, 6]. For example, for N=2 with top mIoU, we chose layers 13 and 14, producing the highest and the second highest mIoU values, as shown in Figure 5. For N=3 with top L1, we mixed layers 5, 1, and 2. We listed the top 6 layers in mIoU and L1 in Table 1 of supplementary material. We measured mIoU and L1 for these configurations, and the results are shown in Figure 8. 4.4.2. Baseline Comparisons To evaluate the editing performance of our method, we compared ours with other 3D-aware face editing methods. We chose two state of the art mask based 3D-aware face editing methods: NeRFFaceEditing, which serves as our backbone model, and IDE-3D, which performs similar editing tasks using tri-plane representation. Unlike our method, those methods typically require more than few thousand samples along with segmentation network to train other segmentation masks. Therefore, for fair comparison, we conducted transfer learning by training only the geometry decoder of each model with our same dataset while keeping all other components frozen. This ensured that the comparison focused solely on the effectiveness of the geometry decoder in the context of few-shot learning. We present qualitative comparison results in Figure 10, visualizing the editing results of our method, NeRFFaceEditing, and IDE-3D. As shown in the figure, our results reflected the target mask most faithfully, while NeRFFaceEditing reflected it to only small extent. IDE-3D did not produce correctly edited results and often failed to preserve the original identity of the person. This indicates that our method can perform editing better than the comparative methods in terms of following the edited target mask. To compare visual quality in human perception, we conducted perceptual study comparing the results of each method given edited target segmentation masks. We evaluated the editing performance of our method compared to NeRFFaceEditing and IDE-3D for editing using an A/B questionnaire. Participants were asked to choose the method that better satisfied the following three criteria: faithfulness to the changed regions, retention of the unchanged regions, and overall visual quality. For comparison, we presented the original image, semantic mask, modified semantic mask, and two results in random order. Each participant was asked to answer 90 questions from 30 comparisons (30 ours vs. 15 others each), with no difference option to avoid random guessing. total of 21 participants (aged from 23 to 32, all with normal vision) were recruited for the study. The percentages of A/B testing results are shown in TaThe percentage indicates the proportion of ble 1. participants choosing our method over the competitor ours+competitor 100(%)). In all questions, ours achieved ( more than 50%, indicating that participants rated our method higher in faithfulness, source retention, and visual quality compared to both competitors. ours We additionally performed quantitative comparison with NeRFFaceEditing on mask generation task. Faithfully generating mask is the first step toward effective editing, process that our editing method shares with NeRFFaceEditing. The mask generation task was conducted 5 times to reduce the effect of randomness, and the average, Figure 8. Results of mixing multiple tri-planes. Left: Mixing top layers based on mIoU, Right: Mixing top layers based on L1. Figure 9. Examples of multi-view images edited using our method. Edited regions are indicated with red arrows in target masks. In both cases, we observed that as the number of mixed layers increased, mIoU decreased while L1 increased. However, in the case of mixing the top layers based on L1 (Figure 8, right), the mIoU values were consistently very low, less than 0.8 for all N. This indicates that mixing layers based on high L1 value is not suitable for local editing tasks. As shown in Figure 6, even mixing just two layers (N=2) resulted in changes to the outline of the chin. On the other hand, for mixing the top layers based on mIoU (Figure 8, left), as increased, the L1 value consistently increased, while the mIoU value did not show significant changes until N=5. Additionally, the L1 and mIoU values were relatively higher overall compared to mixing the top layers based on L1. Based on these results, we opted to mix the latent codes of the top 5 layers based on mIoU (layers [10, 14]) for our method. As shown in Figure 7, the geometry or semantic information of the source does not change even when 5 layers are mixed, while only the hue of the clothes and the brightness of the face change. 4.4. Evaluations 4.4.1. Qualitative Results We evaluated our method based on two main features: editing capability and multi-view functionality. The results are displayed in Figure 9, which were edited from testset identities unseen during training. The results demonstrate that our method can produce intended outcomes following the edited segmentation mask. Additionally, when rendered from different views, the edited face regions are well-blended with the original face structure. This indicates that our adaptation method effectively learns to follow the mask layout when editing face image while preserving 3D information from few-shot training. 6 Figure 10. Examples of our results and those of baseline methods in editing tasks. The results of our method faithfully reflect the edited regions. Table 1. Perceptual study results on face editing. The number indicates the percentage of selections for our methods over competitors. Comparison vs NeRFFaceEditing vs IDE-3D Faithfulness(%) Retention(%) Quality(%) 67.83 80.17 72.29 79.65 68.68 81.22 Table 2. Quantitative results of our method and NeRFFaceEditing on mask generation. Highest scores are denoted in bold. Methods Ours NeRFFaceEditing Average mIoU [min, max] (%) 85.33 [84.8, 85.7] 81.37 [81.2, 81.5] minimum, and maximum values of the 5 trials were noted in Table 2. Our method performed better than the comparative method in generating correct masks using 22 Base layout test sets. We attribute this superior performance to our adaptation method. 4.4.3. Ablation Study To perform an ablation study, we trained our model using different settings to identify the effectiveness achieved by each of our components: 1) Without feature injection; 2) Without augmentation (LMTA); 3) Using all layers for mixing; 4) our full model. We tested with different numbers of data to evaluate the effects of each component with varying data sizes. For this evaluation, mIoU was used for 22 test sets. As shown in the Table 3, method without injection resulted poor even with larger training data such as 30 sets, method without LMTA suffered when small number of training data was used due to overfitting, and mixing all layers resulted worst due to semantic changes during its strong augmentation. On the other hand, our full method produced the best results across 5 to 30 training data. For training with one data, ours record the second-best results because it is too small to adapt to new mask layout. This is also shown in the visualization presented in Figure 11 which was produced by all methods trained with five data. The results from our methods did not show color shift while following the target mask. Figure 11. Ablation study results. Our full model best followed the target masks while preserving the original identity. The color shifted when trained without feature injection, hair was not faithfully generated when trained without LMTA, and the source identity changed when trained with mixing all layers. Figure 12. Comparisons with percentage-based optimization on enlarging eyes show that our method more faithfully follows the desired eye size modifications. Table 3. Quantitative results of ablation study on mask generation with different number of training data. The highest scores are denoted in bold. Number of Data 1 Ours w/o injection w/o LMTA Mixing all 0.711 0.741 0.695 0.603 5 0.832 0.806 0.829 0.654 0.850 0.835 0.845 0.743 20 0.855 0.844 0.855 0.785 30 0.860 0.847 0.859 0.780 We further conducted visual comparison between the results of our overlap-based optimization and the previous percentage-based optimization [18, 32]. As shown in Figure 12, our overlap-based optimization produced bigger eyes, following the given layout. This indicates that overlap based optimization is crucial for small segments, such as eye editing. 7 Figure 13. Results of full and partial style transfer application. 5. Application Partial Style Transfer Tri-plane-based NeRF offers powerful framework for facial representation, and style transfer is one of the key applications for face editing. FFaceNeRF leverages disentangled geometry and appearance representations to modify facial style while preserving geometry. Additionally, use of customized mask makes this editing possible with desired partial labels. To produce the results, the mean and variance of the tri-plane from the target style images were extracted, and the normalized source tri-plane was denormalized using these values. This denormalized tri-plane feature was then passed through the appearance decoder, Ψapp, for full style transfer. Afterward, using the generated masks in the desired layout, only the selected label was blended with the original face. To ensure seamless transition without visible boundaries, linear blending was applied at the edges. As shown in Figure 13, our method successfully transfer the style for the desired regions. FFaceGAN DatasetGAN [53] is powerful framework that generates segmentation using limited data with pretrained StyleGAN [22]. To demonstrate the effectiveness of our adapter and LMTA, we integrated these into DatasetGAN, resulting in FFaceGAN. comparison between DatasetGAN and our FFaceGAN is presented in Figure 14. The results of FFaceGAN show dramatic improvement of the quality over those of DatasetGAN. For example, artifacts such as segmentation holes and unlabeled regions were reduced, illustrating the effectiveness of our proposed strategies. FFaceGAN demonstrates that the proposed techniques can be applied to architectures beyond NeRFFaceEditing and EG3D. Additional details and explanations for both applications are provided in the supplementary material. 6. Limitation and Conclusion In this paper, we introduced FFaceNeRF, novel NeRFbased face editing technique that enhances mask-based 3Daware face editing through few-shot training. FFaceNeRF overcomes limitations of existing methods that often deFigure 14. Comparison between our FFaceGAN and DatasetGAN in generating segmentation masks based on the target labels. pend on extensive datasets or pretrained segmentation networks, by significantly reducing the number of required training samples. This approach offers greater user control and customization. To achieve this, we proposed geometry adaptor with feature injection, which enables effective manipulation of geometric attributes using small number of training data. Additionally, we adopted LMTA that preserves semantic information while improving the efficiency of data augmentation. To further refine editing capabilities on small labels, we introduced an overlap-based optimization technique. Finally, we demonstrated the potential for broader applications beyond existing frameworks like EG3D and NeRFFaceEditing. While FFaceNeRF demonstrated strong performance in few-shot segmentation-based editing, achieving real-time performance remains challenging. Our inferences are conducted with iterative optimization, requiring around 31 seconds per edit. One potential way to overcome this limitation is to train an encoder for customized masks. However, training an encoder with as few as 10 data samples has not been well-researched. Therefore, utilizing pretrained mask encoder to develop customized encoder for interactive face editing could be meaningful future research direction. In addition, performance in one-shot setting is limited. Because FFaceNeRF requires to train geometry adapter, it cannot generalize to all faces when only one data is provided even with our augmentation. Incorporating SAM-like models [25] could help facilitate one-shot approach due to their general knowledge of segmentation."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS2024-00439499)."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Peihao Zhu, Niloy Mitra, and Peter Wonka. Labels4free: Unsupervised segmentation using stylegan. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1397013979, 2021. 2 [2] Rameen Abdal, Peihao Zhu, Niloy J. Mitra, and Peter Styleflow: Attribute-conditioned exploration of Wonka. stylegan-generated images using conditional continuous normalizing flows. ACM Trans. Graph., 40(3), 2021. 1 [3] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 2 [4] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1612316133, 2022. 1, 2, 3 [5] Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. Sofgan: portrait image generator with dynamic styling. ACM Transactions on Graphics (TOG), 41 (1):126, 2022. 1, [6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29, 2016. 1 [7] Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Sem2nerf: Converting single-view semantic masks to neural radiance fields. In European Conference on Computer Vision, pages 730748. Springer, 2022. 2 [8] Yuhao Cheng, Zhuo Chen, Xingyu Ren, Wenhan Zhu, Zhengqin Xu, Di Xu, Changpeng Yang, and Yichao Yan. 3daware face editing via warping-guided latent direction learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 916926, 2024. 2 [9] Yu Chi, Fangneng Zhan, Sibo Wu, Christian Theobalt, and Adam Kortylewski. Datasetnerf: Efficient 3d-aware data arXiv preprint factory with generative radiance fields. arXiv:2311.12063, 2023. 2 [10] Min Jin Chong and David Forsyth. Jojogan: One shot face stylization. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVI, pages 128152. Springer, 2022. 2 on Computer Vision and Pattern Recognition, pages 10673 10683, 2022. [12] Lin Gao, Feng-Lin Liu, Shu-Yu Chen, Kaiwen Jiang, Chunpeng Li, Yukun Lai, and Hongbo Fu. Sketchfacenerf: Sketch-based facial generation and editing in neural radiance fields. ACM Transactions on Graphics, 42(4), 2023. 2 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [14] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Stylenerf: style-based 3d-aware generaarXiv preprint Theobalt. tor for high-resolution image synthesis. arXiv:2110.08985, 2021. 2 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [16] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: real-time nerf-based parametric In Proceedings of the IEEE/CVF Conference head model. on Computer Vision and Pattern Recognition, pages 20374 20384, 2022. [17] Andrew Hou, Feng Liu, Zhiyuan Ren, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Infamousnerf: Improving face modeling using semantically-aligned hypernetworks with neural radiance fields. arXiv preprint arXiv:2312.16197, 2023. 2 [18] Kaiwen Jiang, Shu-Yu Chen, Feng-Lin Liu, Hongbo Fu, and Lin Gao. Nerffaceediting: Disentangled face editing in neural radiance fields. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 1, 2, 3, 4, 7 [19] Kaiwen Jiang, Shu-Yu Chen, Hongbo Fu, and Lin Gao. Nerffacelighting: Implicit and disentangled face lighting representation leveraging generative prior in neural radiance fields. ACM Transactions on Graphics, 42(3):118, 2023. 3 [20] Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang, and Jaegul Choo. Cg-nerf: Conditional generative neural In Proceedradiance fields for 3d-aware image synthesis. ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 724733, 2023. 2 [21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 5 [22] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 2, 8 [23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020. 2, 3 [24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [11] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for 3d-aware image In Proceedings of the IEEE/CVF Conference generation. [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment any9 thing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 8 [26] Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, and Matthias Nießner. Gghead: Fast In SIGGRAPH Asia and generalizable 3d gaussian heads. 2024 Conference Papers, pages 111, 2024. [27] Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, and Bo Dai. Self-supervised geometry-aware encoder for styleIn Proceedings of the IEEE/CVF based 3d gan inversion. Conference on Computer Vision and Pattern Recognition, pages 2094020949, 2023. 2 [28] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2133021340, 2022. 2 [29] Jianhui Li, Jianmin Li, Haoji Zhang, Shilong Liu, Zhengyi Wang, Zihao Xiao, Kaiwen Zheng, and Jun Zhu. Preim3d: 3d consistent precise image attribute editing from sinIn Proceedings of the IEEE/CVF Conference gle image. on Computer Vision and Pattern Recognition, pages 8549 8558, 2023. 2 [30] Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen InstructZheng, Jinghui Xu, Jianmin Li, and Jun Zhu. pix2neRF: Instructed 3d portrait editing from single image. In The Twelfth International Conference on Learning Representations, 2024. 2 [31] Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, and Yu Pan. Ggavatar: Geometric adjustment of gaussian head avatar. arXiv preprint arXiv:2405.11993, 2024. 2 [32] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. Advances in Neural Information Processing Systems, 34:1633116345, 2021. 2, 4, [33] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. arXiv preprint arXiv:2305.14334, 2023. 2 [34] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 2 [35] Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias Nießner. Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2029920309, 2024. 2 [36] Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on Graphics (TOG), 42(1):113, 2022. 4 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [39] Yujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence, 44(4):20042018, 2020. 1 [40] Leslie Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learnIn Artificial intelligence and machine learning ing rates. for multi-domain operations applications, pages 369386. SPIE, 2019. 5 [41] Carole Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and Jorge Cardoso. Generalised dice overlap as deep learning loss function for highly unbalanced segmenIn Deep Learning in Medical Image Analysis and tations. Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 14, Proceedings 3, pages 240248. Springer, 2017. 4 [42] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. Ide-3d: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis. ACM Transactions on Graphics (ToG), 41(6):110, 2022. 1, 2 [43] Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, and Jue Wang. Fenerf: Face editing in neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 76727682, 2022. 1, 2 [44] Junshu Tang, Bo Zhang, Binxin Yang, Ting Zhang, Dong Chen, Lizhuang Ma, and Fang Wen. 3dfaceshop: Explicitly controllable 3d-aware portrait generation. IEEE Transactions on Visualization and Computer Graphics, 2023. 2 [45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence arXiv preprint arXiv:2306.03881, from image diffusion. 2023. 2 [46] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick Perez, Michael Zollhofer, and Christian Theobalt. Stylerig: Rigging stylegan for 3d control over portrait images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61426151, 2020. [47] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panopIn tic segmentation with text-to-image diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 2 [48] Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, and Yebin Liu. Gaussian head avatar: Ultra high-fidelity head avatar via dynamic gaussians. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2024. 2 10 [49] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation In Proceednetwork for real-time semantic segmentation. ings of the European conference on computer vision (ECCV), pages 325341, 2018. 2, 5 [50] Kwan Yun, Youngseo Kim, Kwanggyoon Seo, Chang Wook Seo, and Junyong Noh. Representative feature extraction during diffusion process for sketch extraction with one example, 2024. 2 [51] Kwan Yun, Kwanggyoon Seo, Chang Wook Seo, Soyeon Yoon, Seongcheol Kim, Soohyun Ji, Amirsaman Ashtari, and Junyong Noh. Stylized face sketch extraction via generative prior with limited data. In Computer Graphics Forum, page e15045. Wiley Online Library, 2024. 2 [52] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [53] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, JeanFrancois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with In Proceedings of the IEEE/CVF minimal human effort. Conference on Computer Vision and Pattern Recognition, pages 1014510155, 2021. 2, 8 [54] Wen-Yang Zhou, Lu Yuan, Shu-Yu Chen, Lin Gao, and ShiMin Hu. Lc-nerf: Local controllable face generation in neural radiance field. IEEE Transactions on Visualization and Computer Graphics, 2023. 2 [55] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf: Morphable facial neural radiance field. In European conference on computer vision, pages 268285. Springer, 2022."
        }
    ],
    "affiliations": [
        "Handong Global University",
        "KAIST, Visual Media Lab"
    ]
}