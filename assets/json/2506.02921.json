{
    "paper_title": "A Controllable Examination for Long-Context Language Models",
    "authors": [
        "Yijun Yang",
        "Zeyu Huang",
        "Wenhao Zhu",
        "Zihan Qiu",
        "Fei Yuan",
        "Jeff Z. Pan",
        "Ivan Titov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the \"needle\" and the \"haystack\" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: $\\textit{seamless context}$, $\\textit{controllable setting}$, and $\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of $\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$. Our experimental evaluation, which includes $\\textbf{18}$ LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 2 9 2 0 . 6 0 5 2 : r Controllable Examination for Long-Context Language Models Yijun Yang1,2, Zeyu Huang1, Wenhao Zhu3, Zihan Qiu4, Fei Yuan2, Jeff Z.Pan1, Ivan Titov1,5 1University of Edinburgh 2Shanghai Artificial Intelligence Laboratory 3Nanjing University 4Qwen Team, Alibaba Group 5University of Amsterdam"
        },
        {
            "title": "Abstract",
            "content": "Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. For example, real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-thehaystack (NIAH) format, wherein lack of coherence between the needle and the haystack compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, novel benchmark that utilizes artificially generated biographies as controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the models long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths, which in turn yields only marginal improvements in the models true capabilities. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable."
        },
        {
            "title": "Introduction",
            "content": "Long-context language models (LCLMs) have become increasingly important in recent years. They not only enhance pure text-based applications with lengthy documents [1, 2, 3, 4, 5] but also lay the vital groundwork for developing stronger multimodal language models that can integrate and process diverse data types [6, 7, 8]. Nevertheless, evaluating long-context capacity remains challenging problem that hinders consistent progress in the field. Existing long-context evaluation benchmarks are primarily of two types: natural and synthetic. Regarding benchmarks with natural datapoints, Karpinska et al. [11] collects different novels read by annotators and asks them to annotate true/false of the narrative phenomenon. Li et al. [17] collects *Equal contribution. 1The code and data are available at https://github.com/Thomasyyj/LongBio-Benchmark. Preprint. Table 1: Comparison of long-context Benchmarks. Here cheap means that the benchmark is cheap to construct. *: We only consider the non-synthetic task within the benchmark. Seamlessness Controllability Soundness Benchmark Cheap Fluent Coherent Configurable Extendable Leakage Prev. Reliable Metric L-Eval [9] LongBench-v2 [10] NoCha [11] -Bench* Helmet* [12] BABILong [13] RULER [14] Michelangelo [15] NoLiMa [16] LongBioBench the latest documents post year 2022 and crafts questions for them. Though they provide authentic language samples, they are expensive to collect and annotate and susceptible to data contamination. Moreover, their inherent complexity makes them hard to characterize for controlled studies. Thus, the models performance on such benchmarks often fails to shed sufficient light on the models underlying bottlenecks. In other words, while the data is genuine, it does little to pinpoint precisely how and why model might struggle, offering limited insights into how its long-context capabilities can be improved. Moreover, since their questions are crafted by human annotators, the task is frozen after annotation and thus unextensible to new evaluation scenarios. On the contrary, synthetic benchmarks are highly controllable. One can isolate specific aspects they want to test with carefully designed synthetic tasks. Benchmarks like RULER [14] allow practitioners to systematically adjust variables and examine their targeted hypotheses about the models potential behaviors. However, existing synthetic benchmarks mostly follow the Needle-In-The-Haystack (NIAH) [18] format, where the context is usually non-coherent because the needle (the information to be recalled) is semantically unrelated to the continuous context. We show in Sec. 4.2 that this makes the benchmark less challenging when the tasks become more difficult, as it potentially presents shortcuts for the model to retrieve the target information. Meanwhile, the complexity of the needle itself is another limitation. As we will show in Sec. 4.1, the numerical needle, which is employed by NIAH [18] and RULER [14] benchmarks, is much easier to retrieve than other types of information. This implicit bias makes them worse proxies of real-world applications, hindering the development of stronger models. These observations underscore critical need for more comprehensive evaluation framework that provides better trade-off between the authenticity of natural data and the controllability of synthetic tasks. Building upon discussions above, we summarize three features for ideal LCLM evaluation benchmarks and show the comparison between benchmarks in Table 1: (1) Seamless context: Unlike most existing synthetic benchmarks, we argue that the target information (the needle) should be seamlessly embedded into the long context to prevent potential shortcuts that could hack the benchmark. That means the needle should be better in fluent natural language, and the needle should be semantically coherent with the context haystack. (2) Controllability The benchmark should be configurable to enable controllable experiments and extensible to simulate newly emerging tasks. (3) Soundness: The task should be free from parametric knowledge and generated on the fly to prevent data contamination. The metric should be accurate for sound evaluation instead of undeterministic evaluators such as LLM-as-Judge. Figure 1: Our synthetic LongBioBench has high correlation with the real task HELMET [12]. The performance is tested on 128K context length from 12 overlapped models. Inspired by Allen-Zhu and Li [19], this paper proposes LongBioBench, employing fictional biographies as playground to examine existing long-context language models. In our framework, single configurable biography (the needle) is seamlessly embedded within set of other configurable biographies (the haystack), enabling diverse and scalable task design grounded in each bios rich factual content. Notably, we show in Fig. 1 that the evaluation scores on our purely synthetic benchmark exhibit stronger correlation (0.853) with the scores of HELMET [12], which employs real-world tasks, than with other existing purely synthetic dataset RULER [14] (0.559). Furthermore, after evaluating 18 LCLMs, we reveal that (1) long context modeling faces some unique challenges. They usually struggle with numerical reasoning, constrained planning, or trustworthy generation, even though they are capable of retrieving relevant information (Sec. 3.3). (2) Using non-coherent context or numerical needles could prohibit the benchmark from revealing the true capability of LCLM, especially when tasks become more challenging (Sec. 4.2). (3) The density of distractors is another bottleneck for the performance of LCLMs (Sec. 4.4). (4) Finally, by testing our benchmark during the pretraining process, we show that the performance saturates at the early stage and challenge the current routine of continuing pretraining on many long context data (Sec. 4.3). In summary, though LongBioBench is not perfect proxy for real-world tasks, we hope it can help the community deepen the understanding of LCLM behaviors to develop stronger models."
        },
        {
            "title": "2.1 Desiderata: What is an ideal benchmark for evaluating LCLM?",
            "content": "Before formally introducing LongBioBench, we first posit that an ideal synthetic benchmark for LCLM should satisfy the following three properties to address the flaws existing in most benchmarks. (1) Seamless context The needle should be fluent in natural language and coherent within the context to prevent potential shortcuts that ease the task. Though most existing synthetic long-context benchmarks insert needle into an irrelevant context (e.g RULER [14], BABILong [13]), we argue that such construction method may destroy the harmony of the original context, thus causing some implicit shortcut for LCLMs to locate the needle and making the evaluation biased. Specifically, we show in Sec. 4.1 that LCLMs may be sensitive to retrieving numerical needles and in Sec. 4.2 that performance on incoherent needles is easier to retrieve compared with coherent needles as the difficulty of the tasks increases. Therefore, we propose that the inserted needle should be in fluent language, the same as the haystack, and should be logically coherent with the context. (2) Controllable settings The benchmark should be configurable to perform controllable ablation and extensible to simulate diverse tasks, allowing researchers to systematically investigate the internal dynamics of language models. Ideally, this extensibility should enable researchers to isolate and examine different prerequisite capabilities (e.g., arithmetic reasoning vs. retrieval skills). Despite the importance of these properties, we found that few existing synthetic benchmarks emphasize both configurability and extensibility. (3) Sound evaluation The evaluation should be unconfounded by parametric factual knowledge, and the metric should be reliable. To ensure reliable evaluation, we propose that the task should be free from reliance on the models parametric factual knowledge to prevent contamination (e.g., model may find it easier to identify an inserted needle if it has already memorized part of the haystack). In addition, the evaluation metric should be objective and reliable, avoiding using non-deterministic measures such as LLM-as-Judge and unexplainable metrics such as perplexity [20]. 2.2 Data Construction data point for long-context evaluation could be composed of the following components: (1) long context containing the needles (the information to answer the query) and the haystack (all other irrelevant information); (2) one or few questions asking the model to understand, retrieve, or reason according to the needle in the context; and (3) the ground truth answer to the question. Overall, LongBioBenchs context and needle are both artificial biographies. And the question in LongBioBench could vary, from naive retrieval (e.g., retrieve specific information from the needle bio) to elementary numerical reasoning (e.g., find two people with given age difference). We use the simplest version, referred to as Standard version, as an example to show how single data point is constructed. An illustration of our proposed context is shown on the left of Fig. 2, and the detailed construction progress is explained in App. A. Specifically, given task, we first generate needle and haystack biographies using biography generator. The needle biography is inserted into the haystack to form the context. The corresponding questions and answers are generated alongside Figure 2: The example of our data (left), the supported configs and the extensible tasks (right). The underlined text shows the inserted attributes. The color of different tasks marks the category of the task. the needle biographies, yielding complete data point. For the biography generator, it typically samples six attributes from predefined attribute pools and fills them into human-written templates to produce coherent biographies for each individual. We also provide flexible configuration options at each step of the construction process to ensure controllability. For instance, we can adjust the distractor density when generating the haystack bios or define the number of required needles when constructing the needle bios. We report the statistics of the dataset in App. B. The proposed benchmark closely matches the desiderata aforementioned. For seamless context, our generated biographies are ensured to be naturally fluent and maintain the coherence of the entire context. Regarding controllability, the benchmark is highly modular and configurable, allowing us to do isolated ablation studies to probe the models behavior. The benchmark is also extensible, as the richly factual nature of the context enables wide range of downstream tasks, depending on how the embedded knowledge is manipulated (further discussed in the next section). Finally, we ensure sound evaluation because the final answer in our bench can be easily verified. We also prevent contamination because all biographies are artificial and can be generated on-the-fly, thus avoiding reliance on any parametric knowledge memorized by the model. 2.3 Task Description This subsection will explain how we extend to have all the tasks in LongBioBench. The tasks are split into three categories: understanding, reasoning, and trustworthiness, representing the core capabilities required to solve the tasks. An overview of how each task is extended is shown on the right of Fig. 2 and the detailed description for each task is in App. . Long Context Understanding One basic skill for long-context language models is understanding the users query and retrieving relevant information from the long context. To examine this basic capability, we propose five subtasks with the difficulty gradually increasing: (1) Standard: In this setting, we simplify all settings to form the most basic and standard version of the task, which serves as foundation for extending to more complex variants. Following this principle, we design the attribute description templates to be as straightforward and uniform as possible across all biographies, and make each sentence in bio include the persons full name, e.g., The hobby of Andrew Xavier Jimenez is radiocontrolled model collecting.. Each biography consists of six sampled attributes per individual. The task requires the model to retrieve specific attribute from the context. The Standard setting could be regarded as an ameliorated version of NIAH, where the needle is the bio containing the answer, and the haystack is other bios. However, our setting could be more challenging because of improved pertinence between the needle and the haystack. Based upon the Standard setting, we gradually add more confounders to increase the difficulty. Specifically, we propose (2) Multi Standard to ask the model to simultaneously retrieve different attributes from different persons.The needles are irrelevant to each other and can be located at very different locations in the context. (3) Paraphrase provides more diverse templates for attribute description, examining how models capture information with paraphrased templates. (4) Pronoun is an updated version of Paraphrase by converting the original third-person description to self-introduction, thus requiring models to understand the pronoun reference better. Long Context Reasoning Reasoning is another critical skill for models to solve real-world tasks. We design four subtasks to test the models reasoning ability with long context. All of them are 4 Figure 3: The average performance of all models on Understanding, Reasoning, and Trustworthiness categories. based on the Multi Standard setting. (1) Rank asks the model to rank people according to their age. The task is quite simple if is small. So we extend it to (2) Calculation to ask the model to calculate the age difference of two people, and also (3) Two-diff, which requires finding two people with specific given age difference. Note that the models must plan on what bios to retrieve in the context to solve Two-diff, which is also different from previous synthetic benchmarks. As those tasks mainly focus on numerical reasoning, we also present (4) Multi-hop tasks where some bios are dependent on each other, and the model needs to figure out the answer by looking through all related bios. Trustworthiness Beyond understanding and reasoning, we also test the models trustworthiness in the long-context setting. We propose the following two subtasks: (1) Citation: Built upon the Standard setting, we index the bios and ask the model to retrieve answers while referring to the bio presenting the target attribute with its index. Therefore, for this task, we not only evaluate the final accuracy but also the precision of the models citation. (2) IDK: Another desirable feature in real-world applications is that the model should refuse to answer the question if the target information is not provided in the context. Therefore, we reuse the same data points from the Standard setting and deliberately remove the target information. Considering that weaker models tend to refuse all questions when the task becomes more difficult (e.g, longer context or harder tasks), we evaluate models with combination of standard and needle-removed context."
        },
        {
            "title": "3 Main Evaluation Results",
            "content": "3.1 Evaluation setup We evaluate 15 open-source LCLMs supporting more than 128k context lengths, such as Llama [21], Phi [22], Qwen2.5 [23], Mistral [24]. Details are summerized in App. E. We also include three closed-source models in the GPT family [25]. Each model is evaluated at input lengths: {2K, 8K, 16K, 32K, 64K, 128K} where is the number of tokens counted with the tokenizer of each model. We use zero-shot prompts in all understanding and IDK tasks, and use 2-shot prompting for all reasoning and citation tasks to ensure that the model follows the answer format. We list the prompts for all tasks in App. G. During initial studies, we observed that the models performance on our proposed tasks nearly converges 2 at around 800 data points, so each test set contains 800 data points. We use the vLLM [26] framework for inference with 8H800 GPUs. We use greedy decoding for all models. Regarding evaluation metrics, we use accuracy with exact match for all understanding and reasoning tasks (all-or-nothing accuracy is applied in the multiple retrieval case). We measure the accuracy of the citations only in the citation task. For the IDK task, the metric is the proportion of questions where the model answers correctly when information is present and refuses to answer when it is absent. The overall performance is shown in Fig. 3, and the individual scores for each task are shown in Fig. 4. We draw the following key observations based on the figures. 3.2 Validating all proposed tasks One ideal benchmark for evaluating models long-context capability should first fulfill the following two criteria: (1) The tasks should be solvable by the model in short context. ensuring that model performance decreases mainly because of the change in context length. As indicated in Fig. 5, almost all models achieve near-perfect performance at 2k-token context length on our proposed tasks, except 2The accuracy fluctuates within 1% for 32 data points 5 Figure 4: The performance of 8 different models by tasks. Some results are blank since the length of target biographies exceed the specified context length. twodiff, which was intentionally designed as an example to show how our framework can be extended to more challenging tasks. (2) The tasks ought to be unsolvable if the context is unprovided. In certain natural long-context tasks, the model can even achieve reasonable performance without relying on context by just using its parametric knowledge [27]. To validate this point, we test LLaMA-3.1-8B and Qwen2.5-7B with only questions from the standard setting without context. Both models fail the task, ensuring that LongBioBench is not contaminated by memorized knowledge. 3.3 Challenges for current long context modeling This section summarizes six key results observed when evaluating LCLMs on LongBioBench. We first analyze the overall performance across all LCLMs from Fig. 3 (R1 and R2) and look deeper into the performance of different tasks in Fig. 4 (R3-R6). The full results are shown in App. F. R1: Open-sourced LCLMs struggle at elementary numerical reasoning and trustworthy tasks, even though they can retrieve. While some LCLMs demonstrate strong performance on understanding tasks, they tend to struggle on reasoning and trustworthiness ones. shown in Fig. 3, GPT-4o, Qwen2.5-14B-1M, and Qwen2.5-7B-1M achieve over 85% accuracy on understanding, but the highest accuracy on reasoning is only 66.5%, and no model exceeds 90% on trustworthy behavior tasks. Notably, most of our reasoning tasks only involve elementary numerical reasoning. So we expect that the model may totally fail the more complex real-world tasks, suggesting substantial room for improvement in these two areas. If we dive deeper into how models fail the reasoning task, we find that the models are capable of retrieving the relevant information. To disentangle retrieval ability from reasoning, we compare performance on reasoning tasks with that on multi-retrieval tasks in Fig. 4, as the former are deliberately constructed by extending the latter. Across all tasks, we consistently observe substantial performance gap between multi-retrieval and multi-hop reasoning, indicating that while LCLMs can successfully recall relevant information, they struggle to reason over it effectively. Besides, it is worth noting that for some models, the extended calculation task score is not bounded but exceeds that of the multi-retrieval task. This is because these models are 6 particularly sensitive to retrieving numerical information, allowing them to surpass the expected bounded performance (Discussed in Sec. 4.1). R2: Poor Correlation between trustworthiness and task-solving performance. We cannot find clear correlation when comparing trustworthiness scores with performance in understanding and reasoning tasks. For example, although GPT-4o achieves the highest scores in both understanding and reasoning, it ranks lower on the citation and IDK. Furthermore, all models exhibit similar trustworthiness performance under the 2k context setting, underscoring the distinct challenge of ensuring safety alignment in long-context scenarios. R3: Context length is still the main bottleneck. As shown in Fig. 4, the performance of all models consistently declines on almost all tasks as the context length grows. Notably, certain models, such as Llama-3.1-8B-Instruct, experience sharp drop in performance when the context is extended from 64k to 128k, suggesting that the models effective context length may be shorter than its advertised capacity [14], underscoring that the long-context problem remains an open challenge. R4: Poor correlation between calculation and other reasoning tasks Fig. 4 indicates that most models perform well on simple arithmetic calculations involving the ages of different individuals. However, their performance drops significantly when the task shifts to ranking these ages , even though ranking requires similar level of numerical reasoning. (Note that the baseline random guess for 2-ranking is 50%) The performance further declines when transitioning from numeric operations to textual comprehension in multi-hop reasoning. These results suggest that while some LCLMs are proficient at numerical calculation, this capability does not generalize to other forms of reasoning. R5: LCLMs struggle on constrained planning problem We construct twodiff as an example of hard task for LCLMs. The answer to this task is not determined, and all bios in the context could serve as the needle. As shown in Fig. 4, even models with strong multihop and arithmetic reasoning abilitiessuch as Qwen-2.5-14B-1Mstruggle with constrained retrieval, failing to perform well even at 2k context length. Besides, none of the models perform over 30% accuracy at 128k context level, which could be easier with bigger selection pool. This aligns with the findings from [13] that LCLMs only use small part of context when doing the reasoning task. This highlights fundamental limitation: current LCLMs remain far from being able to reason effectively over long contexts."
        },
        {
            "title": "4 Analysis",
            "content": "4.1 Some LCLMs are more sensitive to retrieving numerical than textual information Observing Fig. 4, we find that certain LCLMs, such as InternLM3-Instruct and Qwen2.5-7B-Instruct, counterintuitively perform better on the calculation task an upgraded variant of the 2-retrieval task than on the 2-retrieval task itself. We hypothesize that this counterintuitive result stems from these models stronger ability to retrieve and manipulate numerical values than textual information. Since the 2-retrieval task requires extracting broader range of attribute types, it poses greater challenge. To investigate this, we cluster each question from the standard setting by attribute types and report the corresponding accuracies in Fig. 5. The figure reveals that InternLM-8B, Prolong-8B, and Qwen2.5-7B achieve their highest scores when retrieving numerical birthdate attributes, and all models exhibit stronger performance on the calculation task than the 2-retrieval task. In contrast, Qwen2.5-7B-Instruct-1M demonstrates more balanced performance across all attribute types, and its accuracy on the calculation task is bounded by the 2-retrieval task. These findings support our hypothesis: certain LCLMs appear to be particularly effective at extracting numeric information rather than textual attributes, and this feature appears when their calculation tasks have higher scores than the 2-retrieval tasks. Figure 5: The bar chart of performance by different attributes on the 2-retrieval task. For simplicity, we abbreviate the names of all instruct models. 7 Figure 6: Performance of Qwen-7b-Instruct-1M and InternLM-7B-Instruct on both our LongBioBench and BiaH. The task is controlled as standard retrieval on the left figure,s and the retrieval number is fixed to be 2 on the right figures. bigger gap is observed on both models as the task difficulty increases. 4.2 Coherent context is important To highlight the importance of contextual coherence and to draw comparison with NiaH-style tasks, we introduce bio-in-a-haystack (BiaH) setting. For each task in our benchmark, we construct BiaH by replacing the original context with haystack in NiaH (we use the Paul Graham essay 3 as the haystack) while preserving the key information relevant to the question. We evaluate the best-performing model, Qwen2.5-Instruct-7B-1M, on both BiaH and our benchmark. The results are presented in Fig. 6. The results reveal clear performance gap between BiaH and BioBench. While the gap is modest in simpler settings such as standard retrieval (-7.9%), it widens substantially as task difficulty increases, reaching -28.3% and -88.9% in more complex scenarios with larger retrieval scopes. This trend indicates that LLMs exploit incoherent cues as shortcuts when faced with harder tasks. These findings underscore the importance of ensuring context coherence in synthetic contexts. 4.3 Performance trend during long-context continual pre-training Setting To analyze how different capabilities evolve during long-context continual pretraining, we evaluate our benchmark on checkpoints of long-context continual pretraining for Qwen2.5-7B, where the context length is extended from 4,096 to 32,768 tokens [23]. We use the Qwen2.5 checkpoints from 2k to 20k training steps. All tasks are conducted in 2-shot setting to ensure the model adheres to task instructions, with the reasoning task using chain-of-thought prompts. We only test on 32k context lengths on our benchmark to adapt the max context window of the model. The results across all tasks are presented in Fig. 7. Our findings are as follows. Performance saturates at the early stage. Based on the left figure in Fig 7, we observe significant performance improvement across all tasks in the early training stages. After peaking, the performance slightly declines and then stabilizes with minor fluctuations. This suggests that during the initial 4K training steps, the model rapidly adapts to the previously unseen RoPE embeddings. Notably, accuracy on the retrieval task peaks at the 4K step checkpoint, indicating that relatively small amount of data may be sufficient to unlock long-context capabilities in LLMs, with additional training yielding marginal gains. Long context pretraining boosts retrieving, but does not help reasoning Comparing the middle figure with the left one, we observe that performance improves consistently across all retrieval tasks, whereas the reasoning task shows only slight improvement, with accuracy remaining extremely low at around 10% (note that the random guess accuracy for the ranking task is 50% since it involves ranking two individuals). This indicates that pretraining on longer contexts primarily enhances retrieval capabilities but not reasoning abilities. Interestingly, the calculation task follows performance trajectory similar to all retrieval tasks and already achieves high accuracy before long-context pretraining. This suggests that Qwen2.5 already possesses the capability to perform calculations over relatively long contexts, and that the main bottleneck on this task lies in retrieval rather than reasoning. Therefore, we categorize the calculation task alongside the retrieval tasks. The model becomes less trustworthy as the training proceeds The right figure in Fig. 7 shows consistent decline in performance as pretraining progresses. This suggests that while the models ability to locate exact information improves with more data, its capability to accurately cite sources and appropriately refuse to answer when information is missing deteriorates. This highlights the 3https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main/needlehaystack/ PaulGrahamEssays 8 Figure 7: The performance of Qwen2.5 at the long-context pretraining stage on our bench with 32K context length. The x-axis represents the number of training steps. The y-axis shows the accuracy over different tasks. necessity of post-training techniques, such as reinforcement learning with human feedback (RLHF), to enhance the models alignment and reliability in handling uncertain or incomplete information. 4.4 Distractor density is another bottleneck for long context tasks To further investigate the factors influencing the performance of LCLMs, we conduct stress test on Qwen2.5-Instruct-7B-1M by controlling the position of the answer information and the density of distractors as the variables while keeping the context length and task fixed. Detailed analysis is shown in App. and we summarize the results as follows: (1) We observe strong negative correlation between distractor density and model performance, suggesting that beyond context length, higher distractor density is key factor contributing to the difficulty LCLMs face with long-context tasks. (2) We observe the lost-in-the-middle [28] but it is less evident on relatively easier tasks."
        },
        {
            "title": "5 Related Work",
            "content": "With the growing interest in long-context language modeling, various benchmarks have emerged. Some focus on real-world tasks across diverse domains, such as document understanding [11, 29, 27], safety [30], and medical question answering [31]. Others try to collect collection of real-world tasks aimed at comprehensive evaluation [10, 32, 12]. However, these benchmarks are often costly to construct and suffer from limited interpretability, as it is challenging to control task complexity systematically. On the other hand, many synthetic tasks are proposed due to their low construction cost and high flexibility [14, 33, 34, 13, 15, 35]. Among them, Needle-in-a-Haystack (NIAH) [18] is the most popular setting, where needle is inserted at long essay (i.e., the haystack) and the model is tasked with retrieving it. There are also variants of NIAH such as RULER [14]. Nonetheless, several studies have raised concerns about the limitations of this approach [13, 16]. In this work, we argue that the semantic irrelevance between the needle and the surrounding context may allow models to exploit cues or shortcuts, ultimately reducing the challenge and bringing bias into evaluations."
        },
        {
            "title": "6 Conclusion and Limitations",
            "content": "Conclusion In this work, we first highlight the limitations of existing long-context evaluation benchmarks: Real-world tasks often lack controllability and are costly to construct, while current synthetic benchmarks frequently overlook the coherence between the inserted information (needles) and the surrounding context (haystacks). We argue that an ideal synthetic benchmark should meet three key criteria: seamlessness, controllability, and soundness. To this end, we introduce LongBioBench, synthetic benchmark composed of artificial biographies that satisfy all three principles and demonstrate high correlation with existing real-world task benchmarks. Testing 18 LCLMs on these benchmarks in controllable setting, we found that although current models can retrieve relevant information, they struggle when we extend the task into the reasoning or more complex scenarios. We hope LongBioBench will facilitate more controllable and diagnostic evaluation of LCLMs and serve as valuable framework for the research community. Limitation We only focus on the most straightforward extension for each task for the controlled study. There will be broad space for extending more challenging tasks, such as in-context learning [34] or passage reranking tasks [2]. We do not focus on more closed-source LCLMs such as Gemini [36], Claude [37] and models using linear attention [38] due to the funding and computation budget. We leave these evaluations as future works."
        },
        {
            "title": "References",
            "content": "[1] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465 6488, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-main.398. URL https://aclanthology.org/2023.emnlp-main.398/. [2] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1491814937, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.923. URL https://aclanthology.org/2023.emnlp-main.923/. [3] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In Proceedings of the 41st International Conference on Machine Learning, pages 1412514134, 2024. URL https://dl.acm.org/doi/10.5555/3692070.3692634. [4] Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay Cohen, and Benjamin Han. Eliciting in-context retrieval and reasoning for long-context large language models. arXiv preprint arXiv:2501.08248, 2025. URL https://arxiv.org/abs/2501.08248. [5] Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, and Alexandra Birch. Generalizing from short to long: Effective data synthesis for long-context instruction tuning. arXiv preprint arXiv:2502.15592, 2025. URL https://arxiv.org/abs/2502. 15592. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. URL https://openreview.net/forum?id=EbMuimAbPbs. [7] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. Vl-gpt: generative pre-trained transformer for vision and language understanding and generation. CoRR, 2023. URL https://arxiv.org/abs/2312.09251. [8] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. CoRR, 2024. URL https://arxiv.org/ abs/2401.10208. [9] Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1438814411, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.776. URL https://aclanthology.org/2024.acl-long. 776/. [10] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. URL https://arxiv.org/abs/2412.15204. [11] Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: novel challenge for long-context language models. ArXiv, abs/2406.16264, 2024. URL https://arxiv.org/html/2406.16264v1. [12] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. In International Conference on Learning Representations (ICLR), 2025. URL https://arxiv.org/abs/2410.02694. 10 [13] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 106519106554. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/c0d62e70dbc659cc9bd44cbcf1cb652f-Paper-Datasets_and_ Benchmarks_Track.pdf. [14] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? In First Conference on Language Modeling. URL https://openreview.net/forum?id= kIoBbc76Sy#discussion. [15] Kiran Vodrahalli, Santiago Ontannon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, Rohan Anil, Ethan Dyer, Siamak Shakeri, Roopali Vij, Harsh Mehta, Vinay Venkatesh Ramasesh, Quoc Le, Ed Huai hsin Chi, Yifeng Lu, Orhan Firat, Angeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri, and Kate Olszewska. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. ArXiv, abs/2409.12640, 2024. URL https://arxiv.org/abs/2409. 12640. [16] Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, and Hinrich Sch√ºtze. Nolima: Long-context evaluation beyond literal In Forty-second International Conference on Machine Learning, 2025. URL matching. https://arxiv.org/abs/2502.05167. [17] Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. LooGLE: Can long-context language models understand long contexts? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1630416333, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.859. URL https: //aclanthology.org/2024.acl-long.859/. [18] Gregory Kamradt. Needle in haystack - pressure testing llms, 2023. URL https://github. com/gkamradt/LLMTest_NeedleInAHaystack. [19] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023. URL https://arxiv.org/abs/ 2309.14316. [20] Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. URL https://arxiv.org/abs/2410.23771. [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/ 2407.21783. [22] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. URL https://arxiv.org/abs/2404.14219. [23] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. [24] Fengqing Jiang. Identifying and mitigating vulnerabilities in llm-integrated applications. Masters thesis, University of Washington, 2024. URL https://arxiv.org/abs/2311.16153. 11 [25] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/abs/ 2303.08774. [26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. URL https://doi.org/10.1145/3600006. 3613165. [27] Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. Detectiveqa: Evaluating long-context reasoning on detective novels. CoRR, 2024. URL https://arxiv.org/abs/2409.02465. [28] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. doi: 10.1162/tacl_a_00638. URL https://aclanthology.org/2024.tacl-1.9/. [29] Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, et al. Novelqa: Benchmarking question answering on documents exceeding 200k tokens. arXiv preprint arXiv:2403.12766, 2024. URL https://arxiv.org/abs/2403.12766. [30] Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, et al. Longsafetybench: Long-context llms struggle with safety issues. arXiv preprint arXiv:2411.06899, 2024. URL https://arxiv. org/abs/2411.06899. [31] Pedram Hosseini, Jessica Sin, Bing Ren, Bryceton Thomas, Elnaz Nouri, Ali Farahanchi, and Saeed Hassanpour. benchmark for long-form medical question answering. In Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond. URL https://openreview.net/forum?id=8Qba6OeW9a. [32] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Bench: Extending long context evaluation beyond 100K tokens. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1526215277, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.814. [33] Xiaoyue Xu, Qinyuan Ye, and Xiang Ren. lanIn The Thirty-eight Conferguage models with lifelong icl and task haystack. Information Processing Systems Datasets and Benchmarks Track. ence on Neural URL https://proceedings.neurips.cc/paper_files/paper/2024/hash/ 1cc8db5884a7474b4771762b6f0c8ee1-Abstract-Datasets_and_Benchmarks_Track. html. Stress-testing long-context [34] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024. URL https://arxiv. org/abs/2404.02060. [35] Amey Hengle, Prasoon Bajpai, Soham Dan, and Tanmoy Chakraborty. Multilingual needle in haystack: Investigating long-context behavior of multilingual large language models. arXiv preprint arXiv:2408.10151, 2024. URL https://arxiv.org/abs/2408.10151. [36] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. URL https: //arxiv.org/abs/2312.11805. 12 [37] Anthropic. Claude 3.7 sonnet: Hybrid reasoning ai model. https://www.anthropic.com/ news/introducing-citations-api, 2025. Accessed: 2025-05-16. [38] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. URL https: //arxiv.org/abs/2403.19887. [39] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022. URL https://arxiv.org/abs/2205. 14334."
        },
        {
            "title": "A Dataset Construction Details",
            "content": "We introduce the details of constructing the data in this section: The overall idea is: Firstly, we sample attributes for each person, allowing the benchmark to be generated on the fly and remain independent of any parametric knowledge. Second, we use these attributes to generate coherent biographical text for each individual. Finally, we concatenate multiple bios to construct full context, where the configuration can be freely adjusted to control the task setup, thus making the framework highly extensible and interpretable. Attribute Sampling To separate the parametric knowledge within LLMs and enable on-the-fly generation and inspired by the bioS dataset [19], we sample attribute values and corresponding sentence templates uniformly from collected pool. Specifically, each biography includes seven attributes: full name, birthdate, birthplace, hobby, graduated university, major, and working city. We generate 100 unique first, middle, and last names independently using LLaMA-3.1-8B-Instruct and ensure that the resulting full names are unique. Birthdates are sampled uniformly from 1950-01-01 to 2001-12-31. For the other attributes, we extract values from datasets on Kaggle4, selecting the top 500 most common universities and 300 most common working cities. Bio construction To generate coherent bio for each individual, we manually write clear and straightforward description template for each attribute. These templates are used to construct the biography as sequence of six sentences (excluding the full name) in the bios construction stage. In the standard setting, all biographies use the same sentence templates, and the attribute order is fixed for consistency. To support evaluation under more semantically diverse conditions, we also provide various paraphrases for each template generated from LLama-3.1-8b-Instruct. Each paraphrase is manually reviewed to ensure clarity and eliminate ambiguity. To maintain control over content structure and quality, paraphrasing is applied at the sentence level only, and we retain the original attribute order since variations in order showed negligible effect on the model performance. Context Synthesis We use controllable context construction based on three key configurations: key information number, key information position, and distractor density. Key information number refers to the number of information required to answer the question, and key information position means the position of the question information. Moreover, we introduce an exclusive feature distractor density, which represents the density of the same attribution appears within the context. Our experiments show that the knowledge density can be another strong bottleneck for the long-context tasks. Given those configs, we construct the context in needle-insertion manner. Specifically, we first construct the haystack by keep concatenating the bios until it reaches context threshold and then inserting the questioned bios. We use specific config to control the position where the question bios are inserted. Then we got sample context and its corresponding question-answer pair."
        },
        {
            "title": "B Data Statistics",
            "content": "We provide the statistics across the average number of biographies in each task in Table 2 and the average token length for all biographies in Table 3."
        },
        {
            "title": "C Task Description",
            "content": "This subsection will outline our motivation and explain how we developed all the current tasks in the proposed bench. The tasks are split into three categories: understanding, reasoning, and trustworthiness, representing the core capabilities required to solve the tasks. An overview of the benchmark is presented in Table 4. 4https://www.kaggle.com/datasets 14 Table 2: The average number of biographies in randomly generated Longbiobench dataset. Task/Length 2 8 16 32 64 standard paraphrase pronoun multi_standard calculation rank multihop twodiff 14.93 12.88 15.07 12.05 12.84 12.86 12.06 12.85 73.77 62.36 75.21 70.87 76.34 75.77 70.88 76.31 152.19 128.01 156.65 149.28 161.12 160.62 149.27 161.42 308.83 263.86 316.81 305.35 330.70 329.90 305.82 330.84 622.18 528.61 636.10 617.94 668.71 667.42 619.39 670. 1248.75 1060.80 1276.81 1246.20 1348.31 1345.70 1246.22 1347.75 Table 3: The average number and standard deviation of tokens within each biography tokenized by Qwen2.57b-Instruct. Task/Length 128 32 16 2 8 105.428.40 105.458.41 105.658.35 105.578.31 standard 126.8512.68 125.1612.15 125.5911.60 123.4811.45 124.1411.23 124.1411.21 paraphrase 103.287.46 103.135.89 pronoun 97.618.41 97.698.41 calculation 105.568.32 106.328.56 multihop 105.568.31 multi_standard 105.728.31 97.648.41 97.638.29 twodiff 97.768.43 97.458.46 rank 102.377.84 97.618.43 105.658.44 105.568.52 97.448.33 97.608. 102.907.73 97.628.42 105.658.36 105.778.38 97.578.39 97.708.46 103.257.60 97.788.46 105.578.45 105.808.43 97.558.36 97.908.60 103.257.33 97.588.32 105.748.47 105.668.52 97.638.32 97.658.41 105.528.39 105.548.26 Standard Information Retrieval (Standard). We start with the simplest retrieval settings as the Standard version. To allow for increments in task difficulty, we ensure that all statements are expressed using the simplest and most direct sentences, such as The hobby of {person} is . This also avoids ambiguity for models, which establishes robust baseline for subsequent, more challenging tasks. The model will be asked to retrieve specific attribute for person. Multi Information Retrieval (Multi_standard). To further challenge the model to simultaneously retrieve information across different context locations, we upgrade the single retrieval task to multi-retrieval task by asking models to retrieve attributes from people instead of one, where can be modified when constructing the dataset. Here we let equal 2, 5, 10 by default. Retrieval on Paraphrased Bios (Paraphrase). To demand stronger contextual understanding, we paraphrase the expression of attributes within the bios. This prevents models from relying on exact matches between questions and sentences to locate answers. As result, we can control for other confounding factors and more accurately assess the models true comprehension capabilities by examining the performance gap relative to the Standard version. Retrieval on Bios stated with Pronoun (Pronoun). This task is an extension of the paraphrasing task. Based on the paraphrase setting, each bio is rewritten as self-introduction. All sentences that describe persons attributes are expressed in the first person, with the individuals name appearing only at the beginning of the bio. This design builds upon sentence-level understanding in paraphrasing and further challenges the LLMs ability to understand the paragraph-level semantics, which is the hardest task in the understanding category. Calcuating the Ages (Calculation). For the reasoning level, we require the LLM to reason on the retrieved information. The calculation task asked LLM to calculate the subtraction of the ages of two people. We use subtraction here instead of the summation to make this task expandable to the TwoDiff task later. Besides, to prevent the ages of people from changing over time, we note that all birthdate attributes are replaced by the specific ages under this setting. Ranking the Ages (Rank). We extend the Calculation setting to ranking the ages of different people so that we can freely define the number of retrieved information by specifying the number of Task Description Metric Example Table 4: Task Overview for the LongBioBench Understanding Standard Multi_standard Paraphrase Pronoun Reasoning Calculation Rank Multihop Twodiff Trustworthy Citation Retrieve specific attribute of one person. Retrieve multiple attributes of different people. Attribute expressions are paraphrased. Bio written from first-person view. Compute age difference between two people. Rank people by age. Retrieve an attribute via crossperson reference. Identify two people with specific age difference. Answer plus source citation. IDK No-answer case detection. Acc All-orNothing Acc Acc Acc Acc Acc Acc Acc Citation Acc Refuse while Answer Acc Attribute: The hobby of {P1} is dandyism. Question: Whats the hobby of {P1}? Attribute: The hobby of {P1} is dandyism. {P2} is mycology. Question: Whats the hobby of {P1} and {P2}? Attribute: {P1} worked in Dhaka. Question: Which city did {P1} work in? Attribute: was born on 1993-06-26. Question: What is the birthday of {P1}? Attribute: {P1} is 61, {P2} is 43. Question: Whats their age difference? Attribute: {P1} is 61, {P2} is 43. Question: Rank from youngest to oldest. Attribute: {P1} born in Santa Paula. {P2} born same place as {P1}. Question: Birthplace of {P2}? Attribute: {P1} is 61, {P2} is 43. Question: Who has 18 years age difference? Attribute: Bio [1]: {P1} born in Santa Paula. Question: Which university did Isabel graduate from? Attribute: Attribute removed. Question: Whats the hobby of {P1}? people to be ranked. Here we let equal 2 and 5 by default since we observe that 5 retrievel ranking task is challenging enough for most models. Retrieve Two People Satisfying the Age Difference (Twodiff). In this task, we give LLM an age difference and ask LLM to retrieve two people whose age difference satisfies the age difference. This demands that LLMs plan on retrieving the target instead of directly retrieving it based on the given information. We design this task as naive simulation of the scenario where LLMs are asked to do some constrained retrieval (e.g in pairs trading, traders look for two stocks whose price difference equals predetermined target). Multi-hop Retrieval (Multihop). Multi-hop question answering is popular setting in document question answering. In our benchmark, we replicate this setting by randomly changing the expression of an attribute into The {attribute} of {person 1 name} is the same as {person 2 name} where we ensure that person 2 appeared after person 1 in the context. This forces LLM to understand the expression and retrieve sequentially across different positions in the context, which is an extended, harder version of multi-retrieval. Citation (Cite). Built upon the Standard setting, we index the bios and ask the model to retrieve answers while referring to the bio presenting the target attribute with its index. Therefore, for this task, we not only evaluate the final accuracy but also the precision of the models citation. Generating with citations has long been an essential ability of trustworthy LLMs. We test their capabilities on citing the correct bios after their answer in this task. To make the citation trackable, we add number before each bio and ask LCLM to generate both the answer and its corresponding number and measure the accuracy of the citation in the end. As an extended version of Standard/Multi_standard, we set the number of information pieces to 1 and 2 by default. dont know (IDK). Expressing uncertainty is critical aspect of trustworthy behavior in LLMs [39]. To evaluate this, we simulate controlled setting in which the target information is deliberately removed, and the LLM is prompted to respond with The answer is not explicitly stated. Observing that weaker LLMs tend to refuse all questions when the task becomes more difficult (e.g, with longer context or harder version), we evaluate models based on combination of standard retrieval and uncertainty expression. Specifically, model is considered to have successfully passed question only 16 if it (1) correctly retrieves the attribute when the relevant information is present, and (2) appropriately refuses to answer when the attribute sentence is removed. Analysis: Density and Needle position v.s Performance To further investigate the factors influencing the performance of LCLMs, we conduct stress test on Qwen2.5-Instruct-7B-1M by controlling the position of the answer information and the density of distractors as the variables while keeping the context length and task fixed. Specifically, we adjust the percentage of the haystack depth to insert the needle for controlling the position and set the probability of generating the same attribute as the needle attribute to control the distractor density. We evaluate the model on two representative tasks: the simplest reasoning task, calculation, and the most challenging understanding task, pronoun, as their baseline performances lie in moderate rangeneither too high nor too low. The results are visualized in the heatmap shown in Fig. 8. Our key observations from the figure are as follows. We first observe strong negative correlation between distractor density and model performance, suggesting that beyond context length, higher distractor density is key factor contributing to the difficulty LCLMs face with long-context tasks. Second, we observe the lost-in-the-middle [28] phenomenon with our proposed synthetic task calculation, where performance declines when the needle appears in the middle of the context. Interestingly, this trend is less evident in the pronoun task. We conjecture that this is because the model already performs relatively well on the pronoun. Finally, both of these effectsperformance decay with density and positional sensitivityare more pronounced in the reasoning task than in the understanding task. This suggests that certain failure patterns emerge only under sufficiently challenging conditions, reinforcing the need to continue developing more difficult long-context benchmarks. Figure 8: The performance on calculation (left) task and pronoun retrieval (right) task corresponding with the answer depth and distractor density. Y-axis shows the percentage of insertion depth in the context and xaxis shows the percentage of the distracted information appeared in the context."
        },
        {
            "title": "E Model Details",
            "content": "We provide the details of all models evaluated in Table 5."
        },
        {
            "title": "F Full Results",
            "content": "The full results are shown in Fig. 9."
        },
        {
            "title": "G Prompts",
            "content": "The prompt we used for each tasks are shown as follows: Standard/Paraphrase/Pronoun: (System): Your task is to answer the users question based on long context, which consists of many bios. Output the answer only. Dont explain or output other things. (User):\"Context: {given_context} Question: {question} (Assistant): Based on the provided context, {question_prefix} 17 Table 5: Details of all evaluated Long-Context Language Models Release Date Size Support Context Length Models gpt-4.1-nano-2025-04-14 gpt-4o-2024-11-20 gpt-4o-mini-2024-07-18 internlm3-8b-instruct Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct-1M Llama-3.3-70B-Instruct Llama-3-8B-ProLong-512k-Instruct Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-3.5-mini-instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Mistral-Nemo-Instruct-2407 glm-4-9b-chat-1m Phi-3-medium-128k-instruct 2025-04 2024-11 2024-07 2025-01 2025-01 2025-01 2024-12 2024-10 2024-09 2024-09 2024-09 2024-09 2024-08 2024-07 2024-07 2024-07 2024-06 2024-05 - - - 8B 7B 14B 70B 8B 7B 72B 1B 3B 4B 8B 70B 12B 9B 14B 128,000 128,000 128,000 131,072 1,010,000 1,010,000 131,072 524,288 131,072 131,072 131,072 131,072 131,072 131,072 131,072 131,072 1,048,576 131,072 Multi_Standard: (System): Your task is to answer all the users questions based on long context, which consists of many bios. Output only the answers for each question sequentially. Dont explain or output other things. (User): Context: {given_context} The Questions are as follows: question Answer each question in sequence. (Assistant): Based on the provided context, the answer is Rank: (System): Following the format of the examples, your task is to rank the users based on their bios in long context. (User): Context: {given_context} examples_with_cot Question: {question} (Assistant): Based on the provided context, Calculation: (System): Your task is to calculate the age difference of the given people based on the given instruction from long context containing multiple bios. (User): Context: {given_context} examples_with_cot Question: {question} (Assistant): Answer: Based on the provided context, 18 Figure 9: The performance of 16 models by tasks. Multihop: (System): Following the format of the examples, your task is to answer the users question based on long context, which consists of many bios. (User): Context: {given_context} examples Question: {question} (Assistant): Answer: Based on the provided context, 19 Twodiff: (System): Your task is to find the names of people based on the given instruction from long context containing multiple bios. Follow the format provided in the examples closely and give the final answer. (User): Context: {given_context} examples Question: {question} (Assistant): Answer: Cite (Standard): (System): Your task is to answer the users question with citation based on long context, which consists of many bios. You must output the answer following with the citation number of the relevant bios strictly surrounded by square brackets such as [1]. Dont explain or output other things. (User): Context: {given_context} examples Question: {question} Answer: (Assistant): Based on the provided context, {question_prefix} Cite (Multi-Standard): (System): Your task is to answer all the users questions with citation based on long context, which consists of many bios. Following the format of the examples, You must output the answer ending with the citation number of the relevant bios strictly surrounded by square brackets such as [1]. You should give the answer and citation for each question sequentially. Dont explain or output other things. (User): Context: {given_context} examples question Answers: (Assistant): Based on the provided context, IDK: (System): Your task is to answer the users question based on long context, which consists of many bios. Output the answer only. If you dont know the answer or the answer is not explicitly stated, you should strictly output The answer is not explicitly stated. Dont explain or output other things. (User): Context: {given_context} Question: {question} (Assistant): Based on the provided context, {question_prefix}"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Qwen Team, Alibaba Group",
        "Shanghai Artificial Intelligence Laboratory",
        "University of Amsterdam",
        "University of Edinburgh"
    ]
}