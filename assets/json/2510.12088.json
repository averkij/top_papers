{
    "paper_title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration",
    "authors": [
        "Zaid Khan",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Jaemin Cho",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only \"one life\" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments."
        },
        {
            "title": "Start",
            "content": "ONE LIFE TO LEARN: INFERRING SYMBOLIC WORLD MODELS FOR STOCHASTIC ENVIRONMENTS FROM UNGUIDED EXPLORATION Zaid Khan UNC Chapel Hill zaidkhan, archiki, esteng, jmincho, mbansal"
        },
        {
            "title": "Archiki Prasad",
            "content": "Elias Stengel-Eskin { @cs.unc.edu }"
        },
        {
            "title": "Mohit Bansal",
            "content": "5 2 0 2 4 1 ] . [ 1 8 8 0 2 1 . 0 1 5 2 : r onelife-worldmodel.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Symbolic world modeling is the task of inferring and representing the transitional dynamics of an environment as an executable program. Previous research on symbolic world modeling has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human-provided guidance. We address the more realistic and challenging problem of learning symbolic world model in complex, stochastic environment with severe constraints: limited interaction budget where the agent has only one life to explore hostile environment and no external guidance in the form of human-provided, environmentspecific rewards or goals. We introduce ONELIFE, framework that models world dynamics through conditionally-activated programmatic laws within probabilistic programming framework. Each law operates through precondition-effect structure, allowing it to remain silent on irrelevant aspects of the world state and predict only the attributes it directly governs. This creates dynamic computation graph that routes both inference and optimization only through relevant laws for each transition, avoiding the scaling challenges that arise when all laws must contribute to predictions about complex, hierarchical state space, and enabling accurate learning of stochastic dynamics even when most rules are inactive at any given moment. To evaluate our approach under these demanding constraints, we introduce new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the popular Crafter environment that exposes structured, object-oriented symbolic state and pure transition function that operates on that state alone. ONELIFE can successfully learn key environment dynamics from minimal, unguided interaction, outperforming strong baseline on 16 out of 23 scenarios tested. We also demonstrate the world models utility for planning, where rollouts simulated within the world model successfully identify superior strategies in goal-oriented tasks. Our work establishes foundation for autonomously constructing programmatic world models of unknown, complex environments."
        },
        {
            "title": "INTRODUCTION",
            "content": "World modeling is critical task in artificial intelligence, providing an agent with functional understanding of its environments underlying dynamics. By learning world model, an agent can predict the outcomes of its actions without having to actually interact with the real world. One line of research in world modeling aims to learn symbolic world models via program synthesis (i.e., representing worlds models with code) with view towards building representations that are interpretable, editable, and verifiable by humans. While such approaches have been successful in environments with limited number of discoverable mechanics and low stochasticity (Piriyakulkij et al., 2025; Tang et al., 2024; Dainese et al., 2024) 1 Figure 1: ONELIFE synthesizes world laws from single unguided (no environment-specific rewards / goals) episode in hostile, stochastic environment. ONELIFE models the world as mixture of laws written in code with precondition-effect structure, each governing an aspect of the world, and infers parameters for the mixture that best explain the observed dynamics of the world. The resulting world model (WM) provides probability distribution over attributes of an object-oriented world state, such as the position of particular zombie. ONELIFE outperforms strong baseline in modeling 16/23 core game mechanics tested, measured by MRR (Mean Reciprocal Rank) of the true next state (Sec. 4) under the WMs likelihood. See Fig. A.3 for synthesized zombie law. these assumptions are often violated in more complex environments. Examples of such environments are popular open-world sandbox games (e.g. MineCraft, RuneScape) containing numerous, diverse mechanics spanning crafting, combat, and physics. These more realistic environments have irreducible stochasticity (e.g., outcomes of actions are subject to random chance, non-player characters taking unpredictable actions), lack of extrinsic rewards (e.g., players set their own goals and there is no well-defined criteria for winning), and high cost of exploration (e.g., entering dangerous areas without preparation can result in death), making it crucial to learn from minimal interaction. This leads to our central research question: How can an agent reverse engineer the laws of complex, dangerous stochastic world, given limited interaction budget and without environment-specific human-specified goals or rewards? We introduce framework for symbolic world modeling, ONELIFE, name that reflects our focus on learning symbolic world model from single episode with unguided exploration. As illustrated in Fig. 1 (top-right), ONELIFE learns from just single, unguided run in the environment, contrast to previous work (Piriyakulkij et al., 2025; Tang et al., 2024; Dainese et al., 2024) that assumes access to large number of interactions as well as environment specific guidance provided by humans (e.g., goals / rewards designed for the environment). ONELIFE recovers program that st, at) which models the probadescribes the environments underlying transition dynamics p(st+1 bility distribution over next states st+1 given current state st and action at. The agent performs this inference using only observations, without access to rewards or other domain-specific guidance. ONELIFE has two key components: law synthesizer (Sec. 3.3) that proposes new laws and an inference algorithm (Sec. 3.4) that re-weights laws based on their predictive ability over observations. Crucially, the inference algorithm is gradient-based and only updates the laws that alter the observed variables between current state st and predicted next state st+1, allowing for efficient and targeted learning. These components work together in probabilistic programming approach (Sec. 3.2) that proposes and re-weights rules based on whether the preconditions for the laws to be applicable are met and the effect of the predictions w.r.t. the observed environment transitions. This approach enables our model to infer distributions over complex, stochastic events, as shown in the Fig. 1 (bottom-right), where learned world model outputs distribution over zombies next move. Crucially, ONELIFE not only produces distribution over states but learns from stochastic observations; the true movement of the zombie in Fig. 1 also follows distribution, which ONELIFE seeks to approximate. 2 To evaluate our approach, we first created suitable testbed Crafter-OO by re-engineering the of structured, complex Crafter (Hafner, 2022) environment to be pure function (s, a) text-based hierarchical object-oriented world state. In other words, all the information needed to compute the next state is represented in single structured, object-oriented representation, and there is ground-truth program for the transition function that computes the next environment state purely from the state representation, without any hidden variables. This text-based, object-oriented representation is natively readable by LLMs and thus allows them to try reconstructing the transition function by writing code that programatically modifies the structured state. We introduce new evaluation protocol that uses two axes (Sec. 4): state ranking, the ability to distinguish valid outcomes from invalid ones according to the worlds laws, and state fidelity, the ability to produce plausible future states for planning. Our experiments show that ONELIFE better captures the environments dynamics compared to several baselines, including PoE-World (Piriyakulkij et al., 2025), showing improved ability to simulate future states given state and candidate action, and to distinguish between likely and unlikely outcomes of an action. We further show that the learned model supports planning in imagination; by simulating rollouts of different policies entirely within the model, we can evaluate and distinguish between effective and ineffective strategies for goal-oriented tasks. In summary, our contributions include: ONELIFE, probabilistic symbolic world model that can learn from stochastic and hostile environments with minimal interactions and without access to human-defined rewards. ONELIFE outperforms prior work, learning world model that better predicts true environment dynamics. Crafter-OO, reimplementation of Crafter (Hafner, 2022) that exposes structured, objectoriented symbolic state and pure transition function that operates on that state alone. This enables us to test ONELIFE in complex, stochastic environment and lays the groundwork for future work in symbolic world modeling and programmatic reinforcement learning. An evaluation suite for world modeling within Crafter / Crafter-OO with 30+ executable scenarios that test knowledge of all core mechanics in Crafter and pool of mutators that can programatically generate illegal distractor states to probe world model understanding alongside, new state fidelity and state ranking metrics for evaluating world models in complex, stochastic, environments."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Symbolic World Models. Symbolic world models represent an environments transition dynamics as executable code, producing interpretable, editable, and generalizable models from limited data. Prior work has used LLMs to synthesize single, monolithic program that functions as world model (Tang et al., 2024; Dainese et al., 2024). Piriyakulkij et al. (2025) introduced compositional approach by representing the world model as product of programmatic experts, enabling modeling of more complex dynamics. Other methods have synthesized programs for planning (Ahmed et al., 2025) or combined functional and automata synthesis to capture latent state dynamics (Das et al., 2023). LLMs have also been used to construct formal planning representations like PDDL from environment interactions or text for symbolic planners (Guan et al., 2023; Deng et al., 2024). Our work differs from these methods in three aspects. First, we operate in complex, open-world environment based on Crafter (Hafner, 2022) with stochasticity and many interacting mechanics, whereas prior work has operated in simpler, often deterministic domains (e.g., grid-worlds or Atari games). Second, we do not assume abundant interaction data: our agent learns from limited budget obtained in single episode or life. Third, ONELIFE learns without external rewards or human-specified goals, framing the task as unguided reverse engineering of the environments laws. Programmatic Representations for Decision-Making. Program synthesis has been used to represent other components of intelligent agents. Programmatic policies have been shown to offer greater interpretability and generalization compared to neural networks (Trivedi et al., 2021; Liang et al., 2022). LLMs have been used to generate programmatic reward functions from natural language instructions, enabling agents to pursue complex, user-specified objectives (Ma et al., 2024; Yu et al., 2023; Klissarov et al., 2025). Programs have been used to build libraries of composable, temporally extended skills, allowing agents to solve long-horizon tasks by combining previously learned behaviors (Wang et al., 2025; Stengel-Eskin et al., 2024). These methods focus on representing components of the agents internal decision-making process: how it should act (policies), 3 what it should value (rewards), or what it is capable of doing (skills). In contrast, our work learns model of how the external world behaves; this task-agnostic model of environment dynamics is complementary to policies, rewards, and skills, and supports planning and decision-making for any downstream goals. World Modeling for Open-Ended Exploration and Discovery. Agents that explore and learn in complex, open-world environments without extrinsic rewards typically learn non-symbolic, latent world models and use them to drive exploration through intrinsic motivation (Hafner et al., 2023; Micheli et al., 2023; Dedieu et al., 2025; Schwarzer et al., 2021). These agents plan using their world models to find novelty or surprise in their environments, discovering useful skills without task-specific supervision (Sekar et al., 2020). This connects to automated scientific discovery, which requires autonomously forming hypotheses and performing experiments to understand unknown systems (Jansen et al., 2024; Chen et al., 2025; Geng et al., 2025). New evaluation frameworks have been proposed to assess an agents ability to rapidly induce world models in novel contexts (Ying et al., 2025; Vafa et al., 2024). Unlike methods that learn implicit, latent world models, our work learns an explicit, symbolic representation of the worlds laws. We frame learning as reverse engineering complex systems rules from unguided, limited interaction."
        },
        {
            "title": "3 OVERVIEW OF ONELIFE",
            "content": "Our framework, ONELIFE is designed to learn symbolic world models from single, unguided episode of exploration. It is built on two key abstractions, programmatic representation of world dynamics as mixture of modular laws with learnable weights and an observable extractor that decouples the environments state from the learning process. The framework consists: world model as program (Sec. 3.2), law synthesizer that proposes new laws using offline data from an unguided exploration policy (Sec. 3.3), an inference algorithm that re-weights laws based on observations (Sec. 3.4), and forward simulation process that uses the learned model for predicting future states (Sec. 3.5). ( ), where ( We model the environment as having pure, but potentially stochastic, transition function : . This functional view aligns with modern reinforcement learning environment frameworks (Freeman et al., 2021; Matthews et al., 2024) and physical models, where the future state of system is pure function of an explicit state and any interventions. ) is the space of probability distributions over the state space S"
        },
        {
            "title": "3.1 CRAFTER-OO: A TESTBED FOR SYMBOLIC WORLD MODELING",
            "content": "A common design assumption in previous work on symbolic world modeling (Tang et al., 2024; Piriyakulkij et al., 2025; Dainese et al., 2024) is that we have access to an object-oriented world state to use as input to the symbolic world model under construction. In practice, this state is only easily accessible for simple environments such as Minigrid (Chevalier-Boisvert et al., 2023) or BabyAI (Chevalier-Boisvert et al., 2018). Programmatic access to the state of more complex environments such as Atari games as used by Piriyakulkij et al. (2025) is only possible due to standalone development efforts such as OCAtari (Delfosse et al., 2024) which makes the internal object-oriented state of these environments accessible to researchers. The lack of an environment with an exposed, object-oriented state that is more complex than gridworlds or with mechanics more diverse than Atari games has thus far prevented evaluation and development of symbolic world modeling approaches for more complex environments. To close this gap, we implement Crafter-OO (Sec. B), which emulates the Crafter (Hafner, 2022) environment by operating purely on an explicit, objectoriented game (Section B.2). Additionally, we contribute utilities for programmatically modifying the game state to create evaluation scenarios (Sec. D, Sec. 4.1). Our target environment Crafter-OO features significant stochasticity, diverse forms of mechanics, and active non-player characters. This includes elements such as hostile and friendly agents with diverse, inherently random behaviors. Our framework is designed to infer the rules governing these interactions from observation alone, without access to rewards or human-specified goals. For instance, in Fig. 2, the scenario contains zombie character chasing the player via stochastic movements. While one cannot perfectly predict the future position of zombie due to inherent randomness built 4 Figure 2: Illustration of the inference process. The active laws for each observable (defined by Ik(st, a)) determine the structure of the computation graph, i.e., which laws and their corresponding parameters θi are related to which observables. This structure in turn informs the parameter updates. Shown here is dataset with single transition instance, in which the player (P) moves right; at the same time, zombie (Z) independently moves left. this implicates two laws, PlayerMovementLaw and ZombieMovementLaw, while not implicating the InventoryUpdateLaw. As result, the loss computation is only function of θ1 and θ2. Note we use here to denote the normalizing factor. Examples of synthesized laws can be seen in Sec. A. into the environment, our world model is able to capture this chasing the player behavior without any explicit supervision by predicting discrete distribution for the zombie.position attributes."
        },
        {
            "title": "3.2 ONELIFE: WORLD MODEL AS A MIXTURE OF LAWS",
            "content": "S where the full state may We consider environments with complex, structured state spaces be hierarchical and contain mixture of entity types and attributes. An agent interacts with the environment by taking an action and observing transition from state st to st+1, as illustrated in Fig. 2. We model an environments transition function as composition of programmatic laws. is precondition law, Li, is program defined by pair (ci, ei), where ci(s, a) is an effect. The precondition determines whether the law is applicable to stateand ei(s, a) action pair (s, a). The effect function makes prediction by modifying attributes on copy of the state. For example, the PlayerMovementLaw in Fig. 2 applies to state-action pairs with player and move action, and has an effect on the player positions (x) observable. This precondition-effect structure is inspired by classical planning and provides natural way to specify the scope of each law, ensuring modularity (McDermott et al., 1998). During any given transition, multiple or no laws may be applicable. true, false { } To create tractable interface to compare states predicted by world model and the true state of the environment, we introduce an observable extractor, . This function maps complex state into vector of primitive-valued observables . In the scenario sketched in Fig. 2, the next state st+1 can be complex, with additional entities and objects (e.g., trees, inventory items, etc.). Nevertheless, one can tractably compare states via observations, i.e., changes between st and st+1 such as player.position, player.inventory, zombie.position, etc. Note that any given law Li only makes predictions about subset of all possible observables. For instance, in Fig. 2, the PlayerMovementLaw only makes predictions about player.position observables and does not predict the zombie.position observables. : Our world model can be viewed as probabilistic program (van de Meent et al., 2021) that generates the next states observables conditioned on the current state and action a. The set of laws Li} defines the components of this program. The effect ei of each law specifies set of conditional probability distributions ϕi,o(o = s, a) for an observable o, where denotes specific outcome in the discrete support of the observable supp(o). For given state-action pair (s, a), the set of (e.g., PlayerMovementLaw and ZombieMovementLaw active laws is ci(s, a) is true (s, a) = { i { } 5 in Fig. 2). The model assumes that all observables are conditionally independent given the current state and action. The predictive distribution for single observable is formed by combining the predictions from all active laws that have an opinion on it. Let O} be the set of active laws relevant to observable o. The probability of observing an outcome for this observable is given by weighted-product of conditional probability distribution from each law, parameterized by θ: Io(s, a) = (s, a) { p(o = s, a; θ) ϕi(o = s, a)θi (1) (cid:89)iIo(s,a) The complete predictive distribution over the next state is the product of the individual observable distributions: p(s s, a; θ) = (cid:89)oO s, a; θ) p(o (2)"
        },
        {
            "title": "3.3 ONELIFE: UNGUIDED ENVIRONMENT EXPLORATION AND LAW SYNTHESIS",
            "content": "The set of candidate laws Li is generated from unguided agent-environment interactions through two-stage process. First, an autonomous exploration policy gathers corpus of interaction data. Second, synthesizer proposes candidate laws that explain the state transitions observed in this data. Exploration Policy. Previous work in symbolic world modeling often assumes access to curated offline datasets or utilizes online interaction guided by human-provided goals or environment rewards. In our unsupervised setting, such guidance is unavailable. Furthermore, in hostile environment such as Crafter-OO, simple random policy fails to survive long enough to experience the diverse mechanics necessary for comprehensive world modeling. Therefore, we employ an exploration policy driven by large language model. The policy is not provided with specific knowledge of the environment; instead, it is given the high-level objective to discover as many underlying mechanics as possible, treating exploration as reverse-engineering task. We use the agent scaffolding from Balrog (Paglieri et al., 2025) to implement the agent. The agents architecture maintains rolling window of its recent state-action history to provide context for decisions. The prompt (see Sec. F) also instructs the agent to maintain transient summary of its current understanding of the worlds rules, refining its hypotheses as it interacts with the environment. Law Synthesizer. The synthesizers task is to propose laws explaining the experienced transitions. Prior approaches (Piriyakulkij et al., 2025) have often relied on large suite of hand-designed synthesizers, each tailored to specific types of interactions. This method embeds significant domain knowledge, which runs contrary to our goal of unsupervised discovery. We instead adopt more general approach where synthesizer is prompted to propose large set of simple, atomic laws for each observed transition. An atomic law is one that describes change to minimal number of state attributes. For instance, complex combat event involving the player and zombie resulting in changes to both entities positions and health is not modeled by single monolithic law. Our synthesizer decomposes the event into multiple atomic laws: one for the players health decrease, another for the zombies movement, and so on. This decomposition into fine-grained, modular laws allows the subsequent weight-fitting stage Sec. 3.4 to perform more precise credit assignment, isolating and down-weighting incorrect hypotheses without discarding entire complex rules that may be partially correct. We provide examples of laws in Sec. A."
        },
        {
            "title": "3.4 ONELIFE: INFERENCE ON LAW PARAMETERS",
            "content": "We learn the weight vector θ by maximizing the log-likelihood of dataset of observed transitions t=1. For clarity, we first define the loss for single transition (s, a, s); the total (st, at, st+1) = loss is the sum over all transitions in the dataset. { } Based on the conditional independence of observables, the negative log-likelihood for single transition decomposes into sum over each observable (θ; s, a, s) = : log p(v s, a; θ) (3) = where probability term is derived from the combined predictions of the active laws. Let (s)o is the ground truth value of observable extracted from the next state s. The logIo(s, a) be the set (cid:88)oO 6 Figure 3: Two evaluation metric categories described in Sec. 4. world state of an environment usually has more than two keys (i.e. Crafter-OOs state (Section B.2) when populated has 100+ keyvalue pairs,) and often has nested values, but here we show simplest case to explain the calculation of (normalized) edit distance. We create distractors for state ranking using mutators (Sec. C), which programatically modify the next state in transition (s, a, s) to be illegal under the true transition function. For example, one of our mutators allows crafting action (e.g. making stone pickaxe) to succeed even when the prequisites for the crafting are not met. of active laws that make prediction for observable o. We first define the combined, unnormalized log-score for any potential value as the weighted sum of log-scores from these laws. The weights θi are the only learnable parameters: ℓo(v s, a; θ) = θi ϕi,o(v s, a) (4) (cid:88)iIo(s,a) Normalized log-probability of observing the specific outcome function. Let supp(o) be the discrete support (set of all possible values) for observable o: is then given by the log-softmax log p(v s, a; θ) = ℓo(v s, a; θ) log exp (ℓo(v s, a; θ)) (5) (cid:88)vsupp(o) The optimization process leverages the dynamic computation graph induced by our law structure. For each transition and each observable, the loss gradient is calculated with respect to the weights Io(st, at). This effectively routes credit for an outcome excluθi only for the active laws sively to the laws that made prediction about it. This sparse, targeted update mechanism provides more precise credit assignment than methods that update global set of weights based on aggregate outcomes. We use L-BFGS for optimization (Nocedal & Wright, 2006)."
        },
        {
            "title": "3.5 ONELIFE: FORWARD SIMULATION AND LIKELIHOOD",
            "content": "Forward simulation is the process of using the learned world model generatively to predict future state ˆst+1 given current state st and an action at. By generating rollouts of future trajectories, an agent can evaluate action sequences against specific goal or reward function without costly or irreversible real-world interaction. st, at; θ). This distribution is constructed by identifying the set of active laws The simulation of single timestep from (st, at) involves multi-step sampling and reconstruc- , the model forms predictive probability distribution tion process. First, for each observable Io(st, at) relevant p(o to that observable and combining their predictions according to their learned weights θi, as specified in Equation 1. This distribution can be used to evaluate the likelihood of an observable conditioned on (s, a) pair. Second, concrete outcome ˆvo can be sampled from this distribution for each observˆvo}oO is used to construct the able: ˆvo full symbolic next state ˆst+1. reconstruction function, which mirrors the observable extraction process, assembles these values back into the environments structured state representation. st, at; θ). This the collection of sampled outcomes p(o {"
        },
        {
            "title": "4 EVALUATION PROTOCOLS AND METRICS",
            "content": "The evaluation of world models for stochastic environment is non-trivial. An useful world model fulfills two criteria: (a) state ranking, the ability to distinguish plausible future states from implau7 Table 1: Performance comparison of world modeling methods on the Crafter-OO environment, averaged over ten trials. We evaluate models on two criteria: state fidelity and state ranking All methods use the ONELIFE exploration policy and law synthesizer but differ in their parameter inference method. ONELIFE shows significant improvements over the PoE-World inference algorithm and ONELIFE variant without parameter inference. The random baseline is shaded in gray. Law Synthesis Law Param. Inference"
        },
        {
            "title": "State Fidelity",
            "content": "(Sec. 3.3) (Sec. 3.4) Rank @"
        },
        {
            "title": "ONELIFE\nONELIFE\nONELIFE",
            "content": "PoE-World None ONELIFE over PoE-World 8.5% 10.8% 13.0% 18.7% (+7.9%)"
        },
        {
            "title": "MRR",
            "content": "0.322 0.351 0.429 0.479 (+0.128) Raw Edit Dist. 121.538 10.634 8.540 8.764 (-1.870) Norm. Edit Dist. 0.809 0.071 0.057 0.058 (-0.013) sible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. Both are illustrated in Fig. 3. State Ranking (Fig. 3 (a)). These metrics assess the models ability to rank the true next state higher than the distractors. To create the distractor states, we use mutators, which are programmatic functions that apply semantically meaningful, rule-breaking changes to the true next state. For example, mutator could change characters position to location they cannot physically reach. We include details on mutators in Sec. C. Rank @ 1 (R@1): binary metric that measures whether the model correctly assigns the highest probability (rank 1) to the true next state among all candidates. Mean Reciprocal Rank (MRR): This metric averages the reciprocal rank of the correct answer across all test instances. higher MRR indicates that the model consistently ranks the correct 1 state higher. The formula is: MRR = 1 , where ri is the rank of the ground truth state ri for the i-th transition, with rank 1 being the highest probability. i=1 (cid:80) State Fidelity (Fig. 3 (b)). These measure the error between predicted and ground truth states. Raw Edit Distance: The total number of atomic JSON Patch operations required to transform the predicted state, t+1, into the ground truth state, st+1. Normalized Edit Distance: The raw edit distance divided by the total number of elements in the state representation."
        },
        {
            "title": "4.1 EVALUATION FRAMEWORK IMPLEMENTATION ON CRAFTER-OO",
            "content": "Evaluating world model on random rollouts may not provide sufficient coverage of rare or important events in an environment. To ensure our evaluation is comprehensive, we create evaluation trajectories from suite of scenarios. Each scenario runs short, scripted policy from an initial state designed to reliably exercise specific game mechanic or achieve particular goal, ensuring that our evaluation thoroughly covers the environments dynamics. Our scenarios cover every achievement in the achievement tree of Crafter-OO/Crafter, and can be seen in Fig. 4. We generate comprehensive evaluation dataset by implementing scenarios that cover every achievement in the games achievement tree. This ranges from basic actions like collecting wood to complex, multi-step tasks like crafting an iron sword, ensuring all of the games core mechanics are tested. More details on scenarios are provided in Sec. D. We generate distractors for each transition in the evaluation dataset using bank of 8 mutators which each produce subtle, but illegal transformation of the game state in response to an action. Some examples are causing an incorrect item to be produced when taking crafting action, or allowing an item to be produced without the correct requirements, or illegal entity behavior such as teleporting. More details on mutators are provided in Sec. and general implementation details are in Sec. E. 8 Figure 4: Per-scenario state ranking performance of ONELIFE (Ours) versus PoE-World, measured by Mean Reciprocal Rank (MRR ). Scenarios are grouped by the core game mechanic they test. Horizontal lines show the average MRR across all scenarios in group for ONELIFE and PoEWorld. ONELIFE demonstrates more accurate understanding of the environments laws, achieving higher average MRR and outperforming the baseline on the majority of individual scenarios."
        },
        {
            "title": "5 EXPERIMENTAL SETUP AND RESULTS",
            "content": "We conduct series of experiments to evaluate ONELIFE. First, we quantitatively assess the models predictive accuracy using our state ranking and fidelity metrics across comprehensive suite of scenarios. Second, we test the models ability to support planning in imagination. We use the model to perform simulated rollouts of different policies, evaluating whether it can predict the outcomes of these plans well enough to distinguish effective strategies from ineffective ones."
        },
        {
            "title": "5.1 BASELINE MODELS",
            "content": "To contextualize the results of our proposed model, we compare against several two baselines: Random World Model: model that assigns uniform probability to all candidate states in the discriminative task. Its performance is equivalent to random guessing and serves as sanity check for discriminative accuracy. PoE-World (Piriyakulkij et al., 2025): state-of-the-art symbolic world model that scaled symbolic world modeling to domains like Atari. Both PoE-World and ONELIFE represent the transition function as weighted product of programs, though the structure of the programs and inference algorithms differ. Because PoE-Worlds law synthesis component is Atari-specific and relies on online interaction using human-provided goals, we reimplement this baseline with our exploration policy and law synthesizer, noting that this makes it stronger baseline (without these changes, PoE-Worlds Atari-specific implementation would be fundamentally incompatible with Crafters state)."
        },
        {
            "title": "5.2 RESULTS",
            "content": "State Fidelity and Ranking. ONELIFE learns world model with significantly higher predictive judgment than baseline methods while maintaining competitive generative fidelity. Table 1 compares our full method against baselines and key ablations across all evaluation metrics. ONELIFEs primary advantage appears in the predictive judgment metrics. We achieve discriminative accuracy of 18.7% and an MRR of 0.479, outperforming the PoE-World optimization baseline by 7.9 percentage points and 0.128, respectively. While precisely generating complex future state remains challenging, our model has learned an accurate understanding of the environments underlying laws. This enables it to assign high probability to valid transitions and low probability to invalid ones. The comparison to the random world model shows that (i) high edit distance can quickly be amassed if the world models updates observables that are unchanged in the ground truth state, thus, reinforcing why such simulation is challenging; (ii) optimizing for generative metrics like state fidelity alone does not yield better world model to guide an agent, e.g., while the PoE-world model (row 2 in 9 Table 2: Planning via forward simulation. Our learned world model is used to compare alternative plans in three scenarios. This is done by executing the plans in the world model, and measuring the reward obtained by each plan. In each case, ONELIFE produces the same ranking over plans as the ground-truth environment, demonstrating its ability to capture causally relevant dynamics for goal-directed decision-making and accurately simulate long action sequences of > 30 steps. Each plan was executed 10 times."
        },
        {
            "title": "Reward Function",
            "content": "Avg. Steps True Env. ONELIFEs WM"
        },
        {
            "title": "Sword Maker",
            "content": "Harvest Wood Fight Immediately"
        },
        {
            "title": "Craft Table",
            "content": "Harvest Wood Mine Immediately"
        },
        {
            "title": "Craft Table",
            "content": ""
        },
        {
            "title": "Fight",
            "content": ""
        },
        {
            "title": "Mine",
            "content": ""
        },
        {
            "title": "Swords Crafted",
            "content": "33 17 31 13 5 10 2.0 1.0 3.0 0.0 4.0 2. 2.03 1.67 3.0 0.0 4.0 2. Tab. 1) dramatically improves the state fidelity by reducing the edit distance factor of 10, it only marginally improves the ability to rank multiple states by 2% over random (Rank@1) reiterating the need for state ranking metrics. Fine-grained Evaluation. Figure 4 breaks down Mean Reciprocal Rank performance across individual scenarios spanning mechanics from resource collection to combat. ONELIFE consistently outperforms the PoE-World baseline on the majority (16/23) of scenarios. These improvements stem from robust understanding of the environments diverse rules rather than strong performance on only few simple mechanics."
        },
        {
            "title": "5.3 PLANNING WITH THE LEARNED WORLD MODEL",
            "content": "To assess the practical utility of the learned world model, we evaluate its effectiveness in planning context. Our protocol tests the models ability to distinguish between effective and ineffective plans through forward simulation. For set of scenarios, we define reward function and two distinct, programmatic policies (plans) to achieve goal within the scenario. Each plan is represented as hierarchical policy (in code) that composes subroutines for navigation, interaction, and crafting. We give an example in Fig. F.3 for the Zombie Fighter scenario. Each reward function is likewise written in code and calculates rewards from the rollout of plan. We execute rollouts of both plans within our learned world model and, separately, within the ground-truth environment. The measure of success is whether the world models simulation yields the same preference ranking over the two plans as the true environment, based on the final reward. This assesses if the model has captured the causal dynamics necessary for goal-directed reasoning. Setup. We design three scenarios that test distinct aspects of the environments mechanics: combat, tool-use and resource consumption, as shown in Table 2. In the Zombie Fighter scenario, an agent with low health must defeat two zombies. The superior plan involves multi-step process: pathfinding to locate and harvest trees, crafting table and then sword, and only then engaging in combat. The alternative is to fight immediately. The Stone Miner scenario tests the models understanding of resource collection. The effective plan is to first harvest wood, craft pickaxe, pathfind to stone, and then mine. Attempting to mine stone directly is ineffective. Finally, the Sword Maker scenario evaluates knowledge of resource consumption. The goal is to craft multiple swords. The efficient plan places single crafting table and reuses it, whereas the inefficient plan wastes wood by placing new table for each sword. On average, plan requires 18 steps to execute, with the longest plans taking > 30 steps. Thus, simulating the results of these plans tests the ability of the world model to accurately model the consequences of long sequences of actions upon the world. We show an example of plan execution in imagination for the Stone Miner scenario in Fig. 5. Results. Table 2 shows that across all three scenarios, our learned world model correctly predicts the more effective plan. The ranking of plans generated by simulating rollouts in ONELIFE matches the ranking from the ground-truth environment. For instance, in the Zombie Fighter scenario, the model correctly simulates that the multi-step plan of crafting sword leads to higher Damage Per Second, identifying it as the superior strategy. This demonstrates that ONELIFE captures sufficiently accurate causal model of the world to support basic, goal-oriented planning. Figure 5: We show an example of plan execution within ONELIFEs world model for the Stone Miner scenario. The task is to mine stone, and can only be successfully completed if wooden pickaxe is obtained before attempting to mine stone. We simulate two plans within the world model. The effective plan carries out multi-step sequence of gathering wood, crafting wooden pickaxe, and then attempting to mine. The ineffective plan attempts to mine the stone directly. The world learned by ONELIFE correctly simulates causal game mechanics that cause the effective plan to succeed and the ineffective plan to fail. The frames are generated by rendering the structured states constructed by ONELIFEs learned transition function."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We address the problem of learning symbolic world model from limited, unguided interaction in complex, stochastic environment. We introduced ONELIFE, framework that represents world dynamics as probabilistic mixture of modular, programmatic laws. Its core learning mechanism routes credit for observed state changes exclusively to the laws responsible for predicting them, enabling effective learning even when many rules are inactive during given transition. Evaluated on Crafter-OO, our variant of the complex Crafter environment with object-centric state, ONELIFE learns world model with superior predictive judgment compared to strong baseline, more accurately distinguishing plausible future states from implausible ones. This improvement is consistent across wide range of game mechanics. Our work provides foundation for building agents that can autonomously reverse engineer the rules of an unknown environment."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was Institute DRL-2112635, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, DARPA ECOLE Program No. HR00112390060, Capital One Research Award, Apple PhD Fellowship, Bloomberg PhD supported by NSF-AI Engage 11 Fellowship, and NDSEG PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "REFERENCES",
            "content": "Zergham Ahmed, Joshua Tenenbaum, Christopher Bates, and Samuel Gershman. Synthesizing world models for bilevel planning. TMLR, 2025. P. (Ed.) Bryan and M. (Ed.) Nottingham. JavaScript Object Notation (JSON) Patch. RFC 6902, IETF, April 2013. URL https://datatracker.ietf.org/doc/html/rfc6902. Yimeng Chen, Piotr Piekos, Mateusz Ostaszewski, Firas Laakom, and Juergen Schmidhuber. Physgym: Benchmarking llms in interactive physics discovery with controlled priors. arXiv preprint, 2025. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018. Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. In Advances in Neural Information Processing Systems 36, New Orleans, LA, USA, December 2023. Nicola Dainese, Matteo Merler, Minttu Alakuijala, and Pekka Marttinen. Generating code world models with large language models guided by monte carlo tree search. In NeurIPS, 2024. Ria Das, Joshua B. Tenenbaum, Armando Solar-Lezama, and Zenna Tavares. Combining functional and automata synthesis to discover causal reactive programs. Proc. ACM Program. Lang., 7 (POPL), January 2023. doi: 10.1145/3571249. URL https://doi.org/10.1145/3571249. Antoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, Swaroop Guntupalli, Miguel Lazaro-Gredilla, and Kevin Patrick Murphy. Improving transformer world models for data-efficient rl. In ICML, 2025. Quentin Delfosse, Jannis Bluml, Bjarne Gregori, Sebastian Sztwiertnia, and Kristian Kersting. OCAtari: Object-centric Atari 2600 reinforcement learning environments. Reinforcement Learning Journal, 1:400449, 2024. Boyuan Deng, Qiaochu Xu, Hao Li, Ling Li, Rui Xu, Yunlong Lu, Jie Lin, and Ziqi Zhang. Pddlego: Iterative planning in textual environments. arXiv preprint, 2024. C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax - differentiable physics engine for large scale rigid body simulation, 2021. URL http: //github.com/google/brax. Jiayi Geng, Howard Chen, Dilip Arumugam, and Thomas Griffiths. Are large language models reliable ai scientists? assessing reverse-engineering of black-box systems. arXiv preprint, 2025. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pretrained large language models to construct and utilize world models for model-based task planning. In NeurIPS, 2023. Danijar Hafner. Benchmarking the spectrum of agent capabilities. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=1W0z96MFEoH. Danijar Hafner, Jurgis Paˇsukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint, 2023. Peter Jansen, Marc-Alexandre Cˆote, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents. In NeurIPS, 2024. 12 Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos Machado, and Pierluca DOro. Maestromotif: Skill design from artificial intelligence feedback. In ICLR, 2025. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint, 2022. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. In ICLR, 2024. Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel Coward, and Jakob Foerster. Craftax: lightning-fast benchmark for open-ended reinforcement learning. In International Conference on Machine Learning (ICML), 2024. Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. PDDL the planning domain definition language. Technical report, Yale Center for Computational Vision and Control, 1998. Vincent Micheli, Eloi Alonso, and Francois Fleuret. Transformers are sample-efficient world models. In ICLR, 2023. Oral Presentation. Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, second edition, 2006. Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. In The Thirteenth International Conference on Learning Representations, 2025. Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, and Kevin Ellis. Poe-world: Compositional world modeling with products of programmatic experts. arXiv preprint, 2025. Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bachman, and Aaron Courville. Pretraining representations for data-efficient reinforcement learning. In NeurIPS, 2021. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In ICML, 2020. Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal. Regal: Refactoring programs to discover generalizable abstractions. In International Conference on Machine Learning, pp. 4660546624. PMLR, 2024. Hao Tang, Darren Key, and Kevin Ellis. Worldcoder, model-based llm agent: Building world models by writing code and interacting with the environment. arXiv preprint, 2024. Dweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph Lim. Learning to synthesize programs as interpretable and generalizable policies. In NeurIPS, 2021. Keyon Vafa, Joshua Tenenbaum, Jacob Andreas, Yoshua Bengio, and Brenden Lake. Evaluating the world model implicit in generative model. In NeurIPS, 2024. Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to probabilistic programming, 2021. URL https://arxiv.org/abs/1809.10756. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. TMLR, 2025. Lance Ying, Katherine Collins, Prafull Sharma, Cedric Colas, Kaiya Ivy Zhao, Adrian Weller, Zenna Tavares, Phillip Isola, Samuel Gershman, Jacob Andreas, Thomas Griffiths, Francois Chollet, Kelsey Allen, and Joshua Tenenbaum. Assessing adaptive world models in machines with novel games. In ICLR, 2025. 13 Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, et al. Language to rewards for robotic skill synthesis. In CoRL, 2023."
        },
        {
            "title": "A LAW EXAMPLES",
            "content": "Below, we give examples of various laws synthesized by ONELIFE. In Fig. A.1 and Fig. A.2, we show examples of how ONELIFE has learned the hierarchical structure of Crafter-OO/Crafters tech-tree. In this case, one must mine stone before stone pickaxe can be produced. These laws are deterministic. In Fig. A.3, we give an example of law synthesized by ONELIFE for stochastic mechanic, in this case, the chase behavior of zombies when they are within certain range of player. The idle skeleton law in Fig. A.6 and moving skeleton law in Fig. A.4 make conflicting predictions; these are aggregated by the weight inference process in Sec. 3.4 to produce distributional prediction that takes into account the predictive accuracy of both laws. Figure A."
        },
        {
            "title": "Mine Stone Law",
            "content": "1 class MineStoneLaw: 2 def __init__(self): \"\"\"Initialize with configurable parameters.\"\"\" pass 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 3 4 6 7 def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" if action != \"Do\": return False target_material, _ = current_state.get_target_tile() if target_material == \"stone\": # Check if the player has any pickaxe has_pickaxe = ( current_state.player.inventory.wood_pickaxe > 0 or current_state.player.inventory.stone_pickaxe > 0 or current_state.player.inventory.iron_pickaxe > 0 ) return has_pickaxe return False def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" # Increment stone in inventory current_state.player.inventory.stone = DiscreteDistribution( support=[current_state.player.inventory.stone + 1] ) # Replace the mined stone material with grass current_state.set_facing_material(\"grass\") Figure A."
        },
        {
            "title": "Craft Stone Pickaxe",
            "content": "1 class CraftStonePickaxe: def __init__(self): 2 \"\"\"Initialize with configurable parameters.\"\"\" # No specific parameters needed for this crafting recipe. pass def precondition(self, current_state: WorldState, action: str) -> bool: 14 8 10 11 12 13 14 16 17 18 19 20 22 23 24 \"\"\"Return True if this law should apply to the given state and action.\"\"\" # Check if the action is \"Make Stone Pickaxe\" if action == \"Make Stone Pickaxe\": # Check if player has required materials has_wood = current_state.player.inventory.wood >= 1 has_stone = current_state.player.inventory.stone >= 1 return has_wood and has_stone return False def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" # Decrease wood by 1 current_state.player.inventory.wood = DiscreteDistribution(support=[ current_state.player.inventory.wood - 1]) # Decrease stone by 1 current_state.player.inventory.stone = DiscreteDistribution(support=[ current_state.player.inventory.stone - 1]) # Increase stone_pickaxe by 1 current_state.player.inventory.stone_pickaxe = DiscreteDistribution( support=[current_state.player.inventory.stone_pickaxe + 1]) Figure A."
        },
        {
            "title": "Zombie Chase",
            "content": "1 class ZombieAggroMovement: 2 def __init__(self): \"\"\"Initialize with configurable parameters.\"\"\" pass # No specific parameters are needed for this observed law. def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" # This law applies if there are any ZombieState entities within the player's # update range, as their movement is an autonomous process. zombies_in_range = current_state.get_object_of_type_in_update_range( ZombieState) return len(zombies_in_range) > 0 def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" player_pos = current_state.player.position # Retrieve all ZombieState objects that are within the update range. # This implicitly filters for zombies close enough to be active/ observable. zombies_to_update = current_state.get_object_of_type_in_update_range( ZombieState) for zombie in zombies_to_update: # Calculate the differences in coordinates between the player and the zombie. dx = player_pos.x - zombie.position.x dy = player_pos.y - zombie.position.y # Initialize new positions to current positions (no movement by default) new_x = zombie.position.x new_y = zombie.position.y # Prioritize movement along the X-axis if dx != 0: 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 # Move one step towards the player along the X-axis. new_x = zombie.position.x + (1 if dx > 0 else -1) elif dy != 0: # If X-axis is already aligned, move one step towards the player along the Y-axis. new_y = zombie.position.y + (1 if dy > 0 else -1) # Update the zombie's position in the state using DiscreteDistribution. zombie.position.x = DiscreteDistribution(support=[new_x]) zombie.position.y = DiscreteDistribution(support=[new_y]) Figure A."
        },
        {
            "title": "Skeleton Movement",
            "content": "1 class SkeletonRandomMovementLaw: 2 def __init__(self): \"\"\"Initialize with configurable parameters.\"\"\" pass def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" # This law applies generally to all skeletons, independent of player action for movement return True def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" skeletons = [obj for obj in current_state.objects if isinstance(obj, SkeletonState)] for skeleton in skeletons: current_x = skeleton.position.x current_y = skeleton.position.y # Possible next positions: current_x, current_x + 1, current_x - 1 skeleton.position.x = DiscreteDistribution(support=[ current_x, current_x + 1, current_x - 1 ]) # Possible next positions: current_y, current_y + 1, current_y - 1 skeleton.position.y = DiscreteDistribution(support=[ current_y, current_y + 1, current_y - 1 ]) Figure A."
        },
        {
            "title": "Health Regeneration Law",
            "content": "1 class PlayerInventoryHealthRegeneration: 2 def __init__(self, max_health: int = 20, recover_threshold: float = 1.0): \"\"\"Initialize with configurable parameters for health regeneration.\"\"\" self.max_health = max_health self.recover_threshold = recover_threshold def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\" Return True if the player's inventory health should regenerate. 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 This law applies if the player is not at max health, has sufficient recover points, and is not sleeping. \"\"\" player = current_state.player # Check if player's current inventory health is less than the defined maximum has_space_for_health = player.inventory.health < self.max_health # Check if player has sufficient recover points to enable regeneration has_recover_points = player.recover >= self.recover_threshold # Check if the player is not currently sleeping not_sleeping = not player.sleeping # This is passive regeneration effect, so the specific action taken (e. g., \"Move North\") # is not direct precondition, but the effect occurs during the state transition. return has_space_for_health and has_recover_points and not_sleeping def effect(self, current_state: WorldState, action: str) -> None: \"\"\" Apply the law by increasing the player's inventory health by 1. \"\"\" # Increment the player's inventory health by 1. current_state.player.inventory.health = DiscreteDistribution(support=[ current_state.player.inventory.health + 1]) Figure A."
        },
        {
            "title": "Skeleton Idle",
            "content": "1 class SkeletonIdleLaw: def __init__(self): 2 \"\"\"Initialize with configurable parameters.\"\"\" pass def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" # This law applies if there are any skeletons in the world that aren't otherwise engaged. # Since no changes were observed, we assume this is their default passive behavior. return True # Applies universally as default behavior for skeletons def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" for skeleton in current_state.get_object_of_type_in_update_range( SkeletonState): # Based on observation, skeletons remain unchanged. # We predict their attributes will stay the same. skeleton.health = DiscreteDistribution(support=[skeleton.health]) skeleton.position.x = DiscreteDistribution(support=[skeleton.position .x]) .y]) skeleton.position.y = DiscreteDistribution(support=[skeleton.position skeleton.reload = DiscreteDistribution(support=[skeleton.reload]) 17 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 THE CRAFTER-OO ENVIRONMENT This appendix details Crafter-OO, our reimplementation of the Crafter environment that exposes structured, object-oriented symbolic state and operates through pure transition function. We developed Crafter-OO as testbed for symbolic world modeling approaches in complex, stochastic domain. B.1 MOTIVATION AND DESIGN PRINCIPLES Symbolic world modeling benefits from environments where the complete state is accessible as structured representation. Simple grid worlds provide this but lack complexity, while more complex environments typically require additional engineering to expose their internal state. More fundamentally, existing testbeds for symbolic world modeling have focused on environments that are either deterministic or have limited stochasticity and narrow range of mechanics. Atari games, for instance, while complex in visual processing demands, have relatively predictable dynamics and constrained set of interactions compared to open-world environments. We developed Crafter-OO to address this gap. The environment features significant stochasticity in entity behaviors, diverse mechanics spanning resource collection to combat, and multi-step causal chains. Our design follows three principles: 1. Explicit Object-Oriented State: The entire game state is captured in single, hierarchical data model that serves as input and output for world models. 2. Functional Purity: The environments dynamics are exposed as pure transition function, (state, action) next state, with no hidden variables. 3. Programmatic Modification: The state representation can be precisely manipulated with code, enabling controlled experimental setups. B.2 THE WorldState DATA MODEL The core of Crafter-OO is the WorldState data model, which captures the environment at single timestep. This model is defined using Pydantic for structure and validation. Its components include: player: PlayerState object containing position, inventory, health, and current action. objects: list of non-player entities (CowState, ZombieState, PlantState, etc.) with type discrimination via name field. materials: 2D array representing the terrain map. Global Properties: World-level attributes including daylight, size, and serialized random state. Listing 1 shows the structure of this model. This representation provides the interface between the environment and symbolic world models. 1 from typing import TypeAlias, Literal 2 3 # --- Basic Data Structures --- 4 5 class Position: 6 \"\"\"Represents 2D position (x, y) in the game world.\"\"\" x: int y: int 9 10 class Inventory: 11 \"\"\"Represents the player's inventory counts for each item type.\"\"\" health: int food: int drink: int energy: int sapling: int wood: int stone: int coal: int iron: int 7 12 13 14 15 16 18 19 20 18 21 23 24 25 26 27 diamond: int wood_pickaxe: int stone_pickaxe: int iron_pickaxe: int wood_sword: int stone_sword: int iron_sword: int 28 29 class Achievements: 30 \"\"\"Represents the player's unlocked achievements.\"\"\" collect_coal: int collect_diamond: int collect_drink: int collect_iron: int collect_sapling: int collect_stone: int collect_wood: int defeat_skeleton: int defeat_zombie: int eat_cow: int eat_plant: int make_iron_pickaxe: int make_iron_sword: int make_stone_pickaxe: int make_stone_sword: int make_wood_pickaxe: int make_wood_sword: int place_furnace: int place_plant: int place_stone: int place_table: int wake_up: int 54 55 # --- Game World Entities --- 56 57 class BaseObject: 58 \"\"\"The base class for all dynamic objects in the game world.\"\"\" entity_id: int position: Position health: int removed: bool 63 64 class Player(BaseObject): 65 \"\"\"The state of the player character.\"\"\" name: Literal[\"player\"] = \"player\" facing: Position action: str sleeping: bool inventory: Inventory achievements: Achievements thirst: float hunger: float fatigue: float recover: float last_health: int 77 78 class Cow(BaseObject): 79 \"\"\"The state of cow.\"\"\" name: Literal[\"cow\"] = \"cow\" 80 81 82 class Zombie(BaseObject): 83 \"\"\"The state of zombie.\"\"\" name: Literal[\"zombie\"] = \"zombie\" cooldown: int 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 60 61 62 66 67 69 70 71 72 73 75 76 84 85 89 94 95 99 100 101 117 118 122 123 124 126 127 128 129 130 132 133 134 135 136 138 139 140 141 142 144 145 146 86 87 class Skeleton(BaseObject): 88 \"\"\"The state of skeleton.\"\"\" name: Literal[\"skeleton\"] = \"skeleton\" reload: int 91 92 class Arrow(BaseObject): \"\"\"The state of an arrow projectile.\"\"\" name: Literal[\"arrow\"] = \"arrow\" facing: Position 96 97 class Plant(BaseObject): 98 \"\"\"The state of plant, which can be eaten.\"\"\" name: Literal[\"plant\"] = \"plant\" grown: int ripe: bool 102 103 class Fence(BaseObject): 104 \"\"\"The state of fence object.\"\"\" name: Literal[\"fence\"] = \"fence\" 106 107 # union of all possible entity types in the world. 108 Entity: TypeAlias = Player Cow Zombie Skeleton Arrow Plant Fence 110 111 # --- World and Spatial Structures --- 112 113 MaterialT: TypeAlias = str 114 115 class Chunk: 116 \"\"\"Represents spatial region of the world for efficient updates.\"\"\" chunk_key: tuple[int, int, int, int] object_ids: list[int] 119 120 class WorldState: 121 \"\"\"Represents the complete, hierarchical state of the game world at single timestep.\"\"\" # World dimensions and configuration size: tuple[int, int] chunk_size: tuple[int, int] view: tuple[int, int] # World status daylight: float step_count: int # The grid of static materials (e.g., grass, stone, water) materials: list[list[MaterialT None]] # list of all dynamic entities currently in the world. objects: list[Entity] # direct reference to the player object for easy access. player: Player # Spatial partitioning data. chunks: list[Chunk] # Internal simulation state entity_id_counter_state: int serialized_random_state: str event_bus: list[str] Listing 1: Simplified structure of the WorldState data structure. B.3 EXTRACTING STATE FROM CRAFTERS GAME ENGINE"
        },
        {
            "title": "Declarative State\nst",
            "content": "Export (produces st+1) Reconstruction ( reconstruct)"
        },
        {
            "title": "Imperative World\nInstance",
            "content": "Imperative Step world.step(a) Figure 6: The functional cycle for state transition. declarative state snapshot is reconstructed into live, imperative world instance. The engine simulates single step, and the resulting world is exported back into new declarative state snapshot for the next timestep. This ensures we match Crafters mechanics exactly. The simulation state in the original engine is not single data structure but is distributed across graph of live Python objects, each with its own internal state and complex inter-dependencies, such as non-player characters holding direct references to the player object. Furthermore, the engines behavior relies on implicit state, including the internal state of its pseudo-random number generator, which governs all stochastic events. Achieving pure functional interface required developing robust mechanism to first serialize this entire, complex state into self-contained, declarative representation and then perfectly reconstruct the live object graph from that representation for each step of the simulation. The state export process transforms the live simulation into serializable snapshot. This procedure performs deep traversal of the game engines internal state, capturing all information required to reproduce the exact game moment. This includes the grid of world materials, the positions of all entities, and the type-specific attributes of each entity, such as zombies attack cooldown or plants growth progress. Crucially, the process also serializes the state of the engines pseudorandom number generator, ensuring that the sequence of random numbers for subsequent stochastic events is preserved. To maintain the spatial partitioning data used for efficient queries, the set of entities within each world chunk is recorded by storing their unique identifiers. The final output is complete, declarative data structure that represents the world at single point in time, free from any live object references or other runtime-specific information. State reconstruction reverses this process, rebuilding the live simulation from the declarative snapshot. This is more complex than simply loading data. It involves re-instantiating the entire graph of game objects and correctly re-establishing their inter-dependencies. key complexity arises from object relationships; for instance, hostile entities require direct reference to the live player object to guide their behavior. To resolve this, we employ multi-pass reconstruction algorithm. First, entities with no external dependencies, such as the player, are instantiated. Then, dependent entities are instantiated in second pass, receiving references to the already-created objects they require. Once all objects are created, the spatial partitioning system is rebuilt by mapping the stored entity identifiers back to the newly created live object instances. Finally, the deserialized state of the pseudo-random number generator is loaded, ensuring that the reconstructed world will produce the exact same stochastic outcomes as the original. The overall process is described in Box 1 and illustrated in Figure 1. Figure B."
        },
        {
            "title": "Pseudocode for the Functional Transition Cycle",
            "content": "function FunctionalTransition(declarative_state_t, action_t): // 1. Reconstruct the imperative world from the declarative state snapshot. world_instance <- ReconstructWorldFromState(declarative_state_t) // 2. Emulate single step in the imperative engine. player <- FindPlayerObject(world_instance) ApplyActionToPlayer(player, action_t) for object in world_instance.get_all_objects(): object.update() // 3. Export the new world state into declarative representation. declarative_state_t+1 <- ExportStateFromWorld(world_instance) return declarative_state_t+ function ExportStateFromWorld(world_instance): snapshot <- new DeclarativeState snapshot.materials <- CopyGrid(world_instance.material_grid) snapshot.rng_state <- Serialize(world_instance.random_generator) for object in world_instance.get_all_objects(): AddObjectState(snapshot, object.type, object.attributes, object.id) return snapshot function ReconstructWorldFromState(snapshot): world_instance <- new ImperativeWorld world_instance.material_grid <- CopyGrid(snapshot.materials) world_instance.random_generator <- Deserialize(snapshot.rng_state) // Multi-pass object instantiation to handle dependencies. player_state <- FindPlayerStateInSnapshot(snapshot) player_object <- InstantiateObject( player_state.type, player_state.attributes ) AddObjectToWorld(world_instance, player_object) for object_state in snapshot.get_all_object_states(): if not is_player(object_state): // Pass player reference to dependent objects (e.g., Zombie). dependencies <- {player: player_object} new_object <- InstantiateObject( object_state.type, object_state.attributes, dependencies ) AddObjectToWorld(world_instance, new_object) RebuildSpatialIndex(world_instance) return world_instance B.4 THE FUNCTIONAL ENVIRONMENT INTERFACE We provide transition function that implements stateless API for environment steps: 1. Input: WorldState object st 2. Reconstruct live game engine instance 3. Execute single update tick with given action 4. Export resulting state as st+1 5. Return new WorldState object 22 This ensures every transition is pure function of the explicit state, making the environment suitable for symbolic reasoning and program synthesis. B.5 UTILITIES FOR PROGRAMMATIC STATE INTERACTION key contribution of Crafter-OO is rich set of utilities that enable programmatic interaction with the world state. These functions are essential for two purposes: first, they allow for the precise, reproducible setup of the evaluation scenarios discussed in Section D; second, they provide highlevel API that simplifies the authoring of programmatic world model laws. To provide clear overview of this toolkit, Table 3 catalogues the key functions, which are grouped into three main categories: World Setup, Player State, and High-Level State Queries & Modifications. Table 3: catalogue of key utilities for programmatic state manipulation in Crafter-OO. These functions provide the building blocks for creating controlled experimental scenarios and for writing concise, high-level world model laws."
        },
        {
            "title": "Player State Utilities",
            "content": "High-Level State Queries & Modifications Function Signature (Simplified) set tile material(pos, material) add object to world(cls, pos, ...) remove object from world(obj) set daylight(level) set player position(pos) set player facing(direction) set player inventory item(item, qty) set player internal stat(stat, val) Description Modifies the terrain at specific coordinate (e.g., changes grass to stone). Adds an entity instance (e.g., Cow or Zombie) to the world. Removes specific entity instance from the world. Sets the global daylight level, affecting visibility and mob spawning. Sets the players exact (x, y) coordinates. Sets the players facing direction (e.g., up, down, left, right). Sets the quantity of specific item in the players inventory. Adjusts internal player stats like health, hunger, or energy. get target tile() get object of type in update range(cls) Returns all entities of specific type near the player. move object(obj, dir, walkable) set facing material(material) Moves an entity one step if the target tile is valid and unoccupied. Changes the material of the tile the player is facing. Returns the material and any object at the tile the player is facing. These utilities are composed to construct the specific initial conditions for our evaluation scenarios. Listing 2 demonstrates how they work in concert to create test case for resource collection mechanic. World setup utilities are first used to clear an area and place specific resource (coal). Then, player state utilities are used to position the player correctly and provide the necessary tool (wood pickaxe) in their inventory. This level of programmatic control, enabled by the functions detailed in Table 3, is what makes our targeted evaluation methodology possible. 1 def get_initial_state_for_coal_collection(): 2 # Create base world and get references to the world and player objects world = reconstruct_world_from_state(initial_state()) player = find_player(world) # --- World Setup Utilities --- # Clear 3x3 area around the player to be grass for in range(4, 7): for in range(4, 7): world_utils.set_tile_material(world, (x, y), \"grass\") # Place the target resource in specific location world_utils.set_tile_material(world, (6, 5), \"coal\") # --- Player State Utilities --- # Set the player's starting position player_utils.set_player_position(player, (5, 5)) # Make the player face the target resource player_utils.set_player_facing(player, (1, 0)) # Add the required tool to the player's inventory player_utils.set_player_inventory_item(player, \"wood_pickaxe\", 1) # Convert the configured world back to serializable WorldState return export_world_state(world, view=(9, 9)) Listing 2: Example of programmatic state manipulation to create an initial state for scenario. World setup utilities create the environment, while player state utilities configure the agent. 23 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24"
        },
        {
            "title": "C MUTATORS",
            "content": "Mutators are core component of our evaluation framework, designed to test world models ability to distinguish between plausible and implausible future states, as described in Sec. 4. mutator is deterministic function that takes state-action pair (st, at) and produces an alternative, incorrect next state st+1. These generated states, called distractors, represent violations of the environments true dynamics. For example, distractor might show the agent crafting an item without the necessary resources or moving through solid obstacle. By creating candidate set containing the true next state st+1 and several such distractors , st+1} we construct discriminative task for the world model. model with robust understanding of the environments laws should assign significantly higher probability to the true outcome than to any of the distractors. This allows us to quantitatively measure the models predictive judgment using the state ranking metrics from Sec. 4. { All mutators adhere to common interface, shown in Listing 3. Each mutator implements precondition method that checks if the mutation is applicable to given state and action. If the precondition is met, the effect method is called to generate the mutated state. This design allows for the creation of targeted mutators that only apply under specific circumstances, leading to more subtle and challenging distractors. 1 class Mutator: 2 \"\"\"A protocol for functions that generate distractor states.\"\"\" 3 4 5 6 7 9 10 11 12 13 15 16 def precondition(self, state: WorldState, action: Action) -> bool: \"\"\" Returns True if the mutator can be applied to the given state-action pair, False otherwise. \"\"\" ... def __call__(self, state: WorldState, action: Action) -> WorldState: \"\"\" Applies mutation to copy of the state and returns the modified state, representing an illegal transition outcome. \"\"\" ... Listing 3: The general interface for mutator. Each mutator is callable object with method to check for applicability. We have implemented suite of mutators for the Crafter-OO environment, categorized by the type of game mechanic they target. Tab. 4 provides comprehensive list of these mutators and the specific rule violations they introduce. Table 4: Catalogue of mutators implemented for the Crafter-OO environment."
        },
        {
            "title": "Description of Rule Violation",
            "content": "Causes the player to move when non-movement action is taken. Teleports non-player entities to random distant locations. Arbitrarily adds or subtracts small amount of health from the player. Sets the health of non-player entities to random, incorrect value. CraftIllegalItemMutator Produces different item than the one specified by the crafting action. CollectIllegalMaterialMutator Adds an incorrect resource to the players inventory when collecting. PlaceIllegalItemMutator Places different object or tile than the one specified by the action."
        },
        {
            "title": "InventoryMutator",
            "content": "Randomizes all quantities in the players inventory. Below we provide detailed descriptions and simplified implementations for three representative mutators from different categories. 24 6 7 9 10 11 12 13 15 16 17 5 6 8 9 10 11 12 14 15 16 17"
        },
        {
            "title": "ILLEGAL MOVEMENT MUTATOR",
            "content": "This mutator tests the models understanding of which actions cause player movement. It activates when the agent takes an action that should not result in change of position, such as noop or do. The effect is to move the player one step in random direction, creating state that would be valid for movement action but is invalid for the action actually taken. Listing 4 shows its logic."
        },
        {
            "title": "1 NON_MOVEMENT_ACTIONS = {\"noop\", \"do\", \"sleep\", \"make_wood_pickaxe\", ...}\n2 DIRECTIONS = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n3\n4 class IllegalMovementMutator:\n5",
            "content": "def precondition(self, state: WorldState, action: Action) -> bool: # This mutator applies only to actions that should not cause movement. return action in NON_MOVEMENT_ACTIONS def __call__(self, state: WorldState, action: Action) -> WorldState: mutated_state = state.model_copy(deep=True) # Choose random direction and update the player's position. random_direction = random.choice(DIRECTIONS) mutated_state.player.position.x += random_direction[0] mutated_state.player.position.y += random_direction[1] return mutated_state Listing 4: Simplified logic for the IllegalMovementMutator."
        },
        {
            "title": "CRAFT ILLEGAL ITEM MUTATOR",
            "content": "This mutator targets the logic of crafting recipes. It checks if the agent is attempting to craft an item. If so, it alters the outcome by giving the player different, randomly selected craftable item. This tests whether the world model has correctly associated specific crafting actions with their unique outcomes. For example, if the action is make wood pickaxe, this mutator might instead add stone sword to the players inventory. Listing 5 illustrates this process. 1 CRAFTING_ACTIONS = {\"make_wood_pickaxe\", \"make_stone_sword\", ...} 2 3 class CraftIllegalItemMutator: 4 def precondition(self, state: WorldState, action: Action) -> bool: # This mutator applies only to crafting actions. return action in CRAFTING_ACTIONS def __call__(self, state: WorldState, action: Action) -> WorldState: mutated_state = state.model_copy(deep=True) # Select different crafting action to determine the illegal outcome. other_crafting_actions = CRAFTING_ACTIONS - {action} illegal_action = random.choice(list(other_crafting_actions)) # Add the item corresponding to the illegal action to the inventory. if illegal_action == \"make_stone_sword\": mutated_state.player.inventory.stone_sword += 1 # ... logic for other craftable items return mutated_state Listing 5: Simplified logic for the CraftIllegalItemMutator."
        },
        {
            "title": "ENTITY HEALTH MUTATOR",
            "content": "This mutator introduces arbitrary changes to the health of non-player characters (NPCs), violating the rules of combat, regeneration, and damage. It is an always on mutator, meaning its precondition is always true, as health can be dynamic property in any state. Its effect is to iterate through all 25 non-player entities and set their health to random value that is not close to their current health. This prevents generating trivial changes that might occur naturally (e.g., from regeneration) and creates more distinctively incorrect state. Listing 6 shows the implementation. 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 1 class EntityHealthMutator: 2 def precondition(self, state: WorldState, action: Action) -> bool: # This mutator is always applicable. return True def __call__(self, state: WorldState, action: Action) -> WorldState: mutated_state = state.model_copy(deep=True) for entity in mutated_state.objects: # Skip the player entity. if entity.entity_id == mutated_state.player.entity_id: continue # Generate new health value that is not the same as the current # health, nor immediately adjacent to it. possible_health_values = set(range(11)) # Health is 0-10 excluded_values = {entity.health, entity.health - 1, entity.health + 1} valid_new_values = list(possible_health_values - excluded_values) if valid_new_values: entity.health = random.choice(valid_new_values) return mutated_state Listing 6: Simplified logic for the EntityHealthMutator."
        },
        {
            "title": "D SCENARIOS",
            "content": "An evaluation framework that relies on data from unguided exploration may not sufficiently cover all of an environments mechanics, especially those that are rare or require specific preconditions. To ensure comprehensive and targeted assessment of world models understanding, we generate evaluation data from suite of scenarios. Each scenario is short, programmatic interaction sequence designed to isolate and test single game mechanic under controlled conditions. This approach produces dataset of transitions that robustly covers the environments dynamics, from basic resource collection to complex combat encounters. The transitions generated by these scenarios form the basis for the evaluation metrics described in Sec. 4. D.1 SCENARIO STRUCTURE AND EXECUTION scenario is defined by common programmatic interface, as outlined in listing 7. It specifies an initial state, scripted policy to guide the agents actions, and termination condition based on either achieving specific goal or reaching maximum number of steps. The execution of scenario, shown in listing 8, produces sequence of (state, action, next state) transitions that serve as ground truth test cases for the world model. 26 1 class Scenario: 2 @property def name(self) -> str: ... 3 4 5 6 7 9 10 11 12 def get_initial_state(self) -> WorldState: ... def policy(self, state: WorldState) -> Action: ... def goal_test(self, transitions: list) -> bool: ... @property def max_steps(self) -> int: ... Listing 7: Structure of an evaluation scenario. D."
        },
        {
            "title": "IMPLEMENTED SCENARIOS",
            "content": "3 1 def run_scenario(scenario): transitions = [] 2 state = scenario.get_initial_state() for _ in range(scenario.max_steps): action = scenario.policy(state) next_state = env.transition(state, 4 5 action) transitions.append((state, action, next_state)) state = next_state if scenario.goal_test(transitions) : break return transitions Listing 8: transitions."
        },
        {
            "title": "Execution loop for generating",
            "content": "6 7 8 9 10 We developed over 40 scenarios for Crafter-OO, covering every core game mechanic present in the original Crafter environment. These scenarios are categorized by the type of mechanic they test, as detailed in Tab. 5. For many mechanics, we include both successful and an unsuccessful variant. The successful version sets up the preconditions for an action to succeed (e.g., having enough resources to craft an item), while the unsuccessful version deliberately violates precondition. This allows us to test whether world model understands not only what should happen, but also what should not happen."
        },
        {
            "title": "E EVALUATION IMPLEMENTATION DETAILS",
            "content": "This section provides procedural specification of our evaluation framework. We begin by defining general-purpose interface that any world model must satisfy to be evaluated. We then detail the computational steps that transform the raw outputs of model satisfying this interface into the final State Fidelity and State Ranking metrics presented in Sec. 4. The process relies on the evaluation trajectories generated from Scenarios (Sec. D) and the distractor states generated by Mutators (Sec. C). Our evaluation framework is designed to be model-agnostic. Any world model can be benchmarked, provided it adheres to the simple, two-method interface shown in listing 9. This interface cleanly separates the two core capabilities required for our metrics: the ability to generate likely future state (for fidelity) and the ability to score given future state (for ranking). 1 class EvaluatableWorldModel(Protocol): 2 \"\"\"A protocol for world models that can be evaluated by our framework.\"\"\" 3 5 6 7 8 9 11 12 13 14 15 17 def sample_next_state(self, current_state: WorldState, action: Action) -> WorldState : \"\"\" Generative function: Samples single predicted next state s_hat_{t+1} from the model's posterior distribution P(s_{t+1} s_t, a_t). \"\"\" ... def evaluate_log_probability( self, state: WorldState, action: Action, next_state: WorldState ) -> float: \"\"\" Discriminative function: Computes the log-probability of specific next_state given the current state and action. \"\"\" 27 Table 5: Complete list of evaluation scenarios used to test world models in Crafter-OO."
        },
        {
            "title": "Scenario Name\nrandom movement",
            "content": "collect wood collect drink collect stone unsuccessful collect stone collect coal unsuccessful collect coal collect iron unsuccessful collect iron collect diamond unsuccessful collect diamond eat plant unsuccessful eat plant"
        },
        {
            "title": "Description",
            "content": "Tests basic player movement in the cardinal directions. Player faces tree and collects wood. Player faces water and collects it. Player collects stone with the required pickaxe. Player attempts to collect stone without the required pickaxe. Player collects coal with the required pickaxe. Player attempts to collect coal without the required pickaxe. Player collects iron with the required pickaxe. Player attempts to collect iron without the required pickaxe. Player collects diamond with the required pickaxe. Player attempts to collect diamond without the required pickaxe. Player eats ripe plant to gain food. Player attempts to eat an unripe plant. craft wooden pickaxe unsuccessful craft wooden pickaxe craft wooden sword unsuccessful craft wooden sword craft stone pickaxe unsuccessful craft stone pickaxe craft stone sword unsuccessful craft stone sword craft iron pickaxe unsuccessful craft iron pickaxe craft iron sword unsuccessful craft iron sword Player crafts wooden pickaxe with sufficient wood. Player attempts to craft without sufficient wood. Player crafts wooden sword with sufficient wood. Player attempts to craft without sufficient wood. Player crafts stone pickaxe with required resources. Player attempts to craft without required resources. Player crafts stone sword with required resources. Player attempts to craft without required resources. Player crafts an iron pickaxe with required resources. Player attempts to craft without required resources. Player crafts an iron sword with required resources. Player attempts to craft without required resources."
        },
        {
            "title": "Placement",
            "content": "Player places crafting table with sufficient wood. Player attempts to place table without sufficient wood. Player places stone with sufficient inventory. Player attempts to place stone without sufficient inventory. Player places furnace with sufficient stone. Player attempts to place furnace without sufficient stone. Player places sapling on grass tile. Player attempts to place sapling without one in inventory. Player, equipped with sword, defeats zombie. Player defeats skeleton. Player defeats cow to obtain food. Player with low health is defeated by zombie. Tests the stochastic movement of cow over several steps. Player goes to sleep and wakes up after their energy is restored. place table unsuccessful place table place stone unsuccessful place stone place furnace unsuccessful place furnace place plant unsuccessful place plant"
        },
        {
            "title": "NPC Behavior",
            "content": "zombie defeat defeat skeleton eat cow player death cow movement wake up 18 ... Listing 9: The interface any world model must implement to be compatible with our evaluation framework. E.1 STATE COMPARISON VIA CANONICAL REPRESENTATION All metrics that involve comparing two world states, such as edit distance or checking for equality, require deterministic and canonical representation of the state. direct object-to-object comparison can be unreliable due to factors like in-memory object identifiers or the ordering of elements in lists. To address this, we serialize each WorldState object to canonical JSON format before any comparison is performed. This process, outlined in listing 10, ensures that two states are considered identical if and only if they represent the same game-world configuration. 1 def to_canonical_json(state: WorldState) -> dict: 2 \"\"\" Serializes WorldState object to deterministic JSON representation. \"\"\" # 1. Exclude non-semantic or non-deterministic fields from serialization. excluded_fields = {\"event_bus\", \"serialized_random_state\"} serialized_state = state.model_dump(exclude=excluded_fields, mode=\"json\") 3 4 6 7 8 28 9 11 12 13 14 15 17 18 19 20 # 2. Sort lists of objects by stable, unique key to ensure order invariance. # The player object is handled separately and removed from the main list. serialized_state[\"objects\"] = [ obj for obj in serialized_state[\"objects\"] if obj[\"name\"] != \"player\" ] serialized_state[\"objects\"].sort(key=lambda obj: obj[\"entity_id\"]) # Chunks are also sorted to ensure map representation is stable. if \"chunks\" in serialized_state: serialized_state[\"chunks\"].sort(key=lambda chunk: chunk[\"chunk_key\"]) return serialized_state Listing 10: Canonical serialization of WorldState object. E.2 STATE FIDELITY METRIC CALCULATION The state fidelity metrics measure the difference between world models predicted next state and the ground truth. We use JSON Patch (Bryan & Nottingham, 2013), standard for describing changes in JSON document, to provide precise, interpretable measure of this difference. The calculation for single transition (st, at, st+1) proceeds as described in listing 11. 1 def calculate_state_fidelity(world_model, s_t, a_t, s_t_plus_1): 2 \"\"\" Computes Raw and Normalized Edit Distance for world model's prediction. \"\"\" # 1. Generate predicted next state from the world model. s_hat_t_plus_1 = world_model.sample_next_state(s_t, a_t) # 2. Convert both true and predicted next states to canonical JSON. json_true = to_canonical_json(s_t_plus_1) json_predicted = to_canonical_json(s_hat_t_plus_1) # 3. Compute the JSON Patch from the predicted state to the true state. patch = jsonpatch.make_patch(json_predicted, json_true) # 4. Raw Edit Distance is the number of operations in the patch. raw_edit_distance = len(list(patch)) # 5. Normalized Edit Distance is the raw distance divided by the total number # of elements in the true state, providing scale-invariant measure. total_elements = count_elements(json_true) normalized_edit_distance = raw_edit_distance / total_elements if total_elements > 0 else 0 return raw_edit_distance, normalized_edit_distance Listing 11: Calculation of State Fidelity metrics for single transition. 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 Example. Consider transition where the player, at position (x = 5, = 5) with health = 9, takes the action move right. The true next state, st+1, has the player at (x = 6, = 5) with health = 9. Suppose world model predicts state, ˆst+1, where the player correctly moves to (x = 6, = 5) but their health incorrectly drops to 8. The simplified canonical JSON representations for the player object in each state would be: 1 { 2 3 4 5 6 } \"player\": { \"position\": {\"x\": 6, \"y\": 5}, \"health\": 9 } 1 { 2 3 5 6 } \"player\": { \"position\": {\"x\": 6, \"y\": 5}, \"health\": 8 } Listing 12: Canonical JSON for the true next state. Listing 13: Canonical JSON for the predicted next state. 29 The JSON Patch required to transform the predicted JSON into the true JSON is single replace operation: [ ]. The Raw Edit Distance is the number of operations in this patch, which is 1. The Normalized Edit Distance would be this value divided by the total number of elements in the true states full JSON representation. op: replace, path: /player/health, value: 9 { } E.3 STATE RANKING METRIC CALCULATION State ranking metrics evaluate models ability to distinguish the true outcome of an action from set of plausible but incorrect alternatives. This process involves generating set of candidate states and using the world model to score them, as detailed in listing 14. 1 def calculate_state_ranking(world_model, s_t, a_t, s_t_plus_1, mutators, num_distractors 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 ): \"\"\" Computes Rank@1 and Mean Reciprocal Rank for world model. \"\"\" # 1. Generate set of distractor states using the mutator bank. distractors = [] applicable_mutators = [m for in mutators if m.precondition(s_t, a_t)] random.shuffle(applicable_mutators) # Ensure variety in distractors for mutator in applicable_mutators: if len(distractors) >= num_distractors: break distractors.append(mutator(s_t, a_t)) # 2. Form the candidate set, including the ground truth and distractors. candidate_set = [s_t_plus_1] + distractors random.shuffle(candidate_set) # Avoid biasing models that may be sensitive to order # 3. Score each candidate state using the world model's log-probability function. scores = [] for s_candidate in candidate_set: log_prob = world_model.evaluate_log_probability(s_t, a_t, s_candidate) scores.append(log_prob) # 4. Determine the rank of the true next state. # Ranks are 1-indexed, with rank 1 being the highest score. ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True) true_state_index = candidate_set.index(s_t_plus_1) rank_of_true_state = ranked_indices.index(true_state_index) + # 5. Calculate metrics from the rank. rank_at_1 = 1.0 if rank_of_true_state == 1 else 0.0 reciprocal_rank = 1.0 / rank_of_true_state return rank_at_1, reciprocal_rank Listing 14: Calculation of State Ranking metrics for single transition. Example. Continuing the previous example, the true state st+1 is the player moving right. mutator might generate distractor state sdistractor where the player illegally teleports to (x = 20, = . good world model should assign much higher 20). The candidate set becomes } probability to the true outcome. For instance, it might yield log-probabilities of log p(st+1 . . . ) = 15.4, the true state is ranked first. This 15.4. Since 0.7 and log p(sdistractor yields Rank@1 of 1.0 and Mean Reciprocal Rank of 1/1 = 1.0 for this transition. { . . . ) = st+1, sdistractor 0.7 > E.4 AGGREGATION ACROSS SCENARIOS The final metrics reported in Tab. 1 are aggregated from the per-transition results. To ensure that each distinct game mechanic contributes equally to the final score, we employ two-level aggregation strategy. First, we compute the mean metric values across all transitions within single scenario. Second, we compute the final reported metric by taking the mean of these per-scenario means. This 30 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 prevents scenarios with more transitions (e.g., long movement sequence) from dominating the overall results compared to scenarios with fewer, more critical transitions (e.g., single crafting action). listing 15 formalizes this entire pipeline. 1 def evaluate_world_model(world_model, scenarios, mutators, config): 2 \"\"\" Runs the full evaluation pipeline and returns aggregated metrics. \"\"\" per_scenario_metrics = {} # 1. Evaluate each scenario independently. for scenario in scenarios: transitions = run_scenario(scenario) # See Sec. C.1 for run_scenario scenario_results = [] for (s_t, a_t, s_t_plus_1) in transitions: # Calculate metrics for each transition in the scenario. r_at_1, mrr = calculate_state_ranking( world_model, s_t, a_t, s_t_plus_1, mutators, config.num_distractors ) raw_ed, norm_ed = calculate_state_fidelity( world_model, s_t, a_t, s_t_plus_1 ) scenario_results.append({ \"R@1\": r_at_1, \"MRR\": mrr, \"RawEditDist\": raw_ed, \"NormEditDist\": norm_ed }) # 2. First level of aggregation: average metrics within the scenario. if not scenario_results: continue per_scenario_metrics[scenario.name] = { key: sum(res[key] for res in scenario_results) / len(scenario_results) for key in scenario_results[0] } # 3. Second level of aggregation: average the per-scenario means. final_metrics = { key: sum(metrics[key] for metrics in per_scenario_metrics.values()) / len( per_scenario_metrics) for key in list(per_scenario_metrics.values())[0] } return final_metrics Listing 15: Overall evaluation pipeline and metric aggregation."
        },
        {
            "title": "F SYNTHESIS AND EXPLORATION IMPLEMENTATION DETAILS",
            "content": "The process of generating candidate world laws is divided into two main stages: unguided exploration to collect dataset of interactions, and law synthesis to propose programmatic laws from that dataset. F.1 EXPLORATION POLICY = t=1, we employ an autonomous exploration To gather the interaction dataset } policy driven by large language model. This policy operates without access to environmentspecific rewards or human-provided goals. Instead, it is given high-level instruction to explore the environment and discover as many of its underlying mechanics as possible, treating the task as reverse-engineering problem. The full prompt provided to the exploration policy is detailed in Fig. F.1. (st, at, st+1) { 31 Figure F."
        },
        {
            "title": "1 You are an explorer in an unknown digital world. Your mission is to experience as\nmany of the world's hidden mechanics as possible. Your recorded experiences\nwill be analyzed later to create a complete map of the world's physical\nlaws.",
            "content": "2 3 The laws of any world can be thought of as IF-THEN hypotheses: `IF (a specific situation occurs) AND (you take an ACTION), THEN (a certain outcome happens) .` 4 5 To succeed, you must trigger as many different `IF-THEN` scenarios as you can. 6 7 **What to Expect in the World:** 8 This world is complex and may be dangerous. 9 - **Hostile Entities:** You may encounter creatures that are hostile and will attack you. 10 - **Resource Collection:** The world contains raw materials that can be gathered, though there may be preconditions for collection. 11 - **Item Production:** You have the ability to craft useful items from raw materials, though there may be preconditions for production. 12 - **Combat:** You can engage in combat with the entities you encounter. 13 14 Your primary goal is to discover the rules governing these activities. 15 You will need to explore the game world by moving around and interacting with the entities and materials in the world. 16 If an action has no effect, you may not have fulfilled the preconditions for the action to have an effect. 17 Try out variety of actions from each category: movement, interaction, placement , production. 18 If an action seems to have no effect, you may not have fulfilled the preconditions for the action to have an effect. 19 Try to acquire additional resources or change something about the world and try again. 20 Before taking actions, set goals for yourself in an IF-THEN format, and let the results invalidate those actions. 21 If an entity is hostile, you can attempt to defend yourself from it. 22 If an entity seems passive or beneficial, you can attempt to interact with it. 23 You will likely need to progress through the \"tech tree\" of the game in specific order. 24 This will require interleaving resource collection with placement of crafting stations and production of better tools. 25 In the meantime, you will need to survive hostile enemies and find ways to heal from damage you've taken. 26 Some resources likely cannot be acquired without first producing tool to acquire them. 27 Tools may require mix of materials and crafting stations to produce. 28 29 The following are the only valid actions you can take: 30 31 {action_strings}. 32 33 You will now receive observations from the world. Begin your exploration. This LLM-based policy is crucial for gathering sufficiently diverse data in hostile environment like Crafter-OO. purely random policy survives for an average of 100 steps before the agent perishes. In contrast, our LLM-based policy navigates the environment for an average of 400 steps. Despite this improvement, exploration remains significant bottleneck. The policy often struggles to progress through the environments technology tree, frequently failing to discover the necessary preconditions for crafting advanced items. It also exhibits tendency to forget previously learned information, which prevents it from effectively building upon past successes within single trajectory. F.2 LAW SYNTHESIS FROM TRAJECTORIES The law synthesis pipeline processes the trajectory data from the exploration phase to generate set of candidate laws . The core idea is to identify state transitions where meaningful changes occur, and then prompt large language model to propose atomic, programmatic laws that explain those specific changes. This process is outlined in Algorithm 17. Li} { Change Detection for Tractable Synthesis. In an environment with complex, structured state like Crafter-OO, changes between timesteps are often sparse and localized to specific subcomponents. To make law synthesis tractable, we first isolate these localized changes to provide focused context for the synthesizer. This is achieved through set of detectors that monitor different aspects of the world state. An aspect is semantically-cohesive subset of the state, typically corresponding to top-level attribute (e.g., player.inventory) or collection of entities of the same type (e.g., all ZombieState objects). For each transition (st, at, st+1), we check for changes across all aspects. If detector identifies change, synthesis task is created for that specific transition and aspect. 1 class ChangeDetector: def aspect_name(self) -> str: ... def has_changes(self, s_t: WorldState, s_t_plus_1: WorldState) -> bool: ... 3 7 8 12 14 4 5 class PlayerInventoryChangeDetector(ChangeDetector): def aspect_name(self): return \"player_inventory\" 6 def has_changes(self, s_t, s_t_plus_1): return s_t.player.inventory != s_t_plus_1.player.inventory 9 10 class ZombieStateChangeDetector(ChangeDetector): def aspect_name(self): return \"zombies\" 11 def has_changes(self, s_t, s_t_plus_1): # Logic to compare zombie states between s_t and s_t_plus_1 ... 15 16 # list of all detectors is used to check each transition 17 ALL_DETECTORS = [ PlayerInventoryChangeDetector(), ZombieStateChangeDetector(), ... # Other detectors for map tiles, cows, etc. 19 20 21 ] Listing 16: Simplified change detection logic. Each detector checks for changes in specific part of the world state between st and st+1. This decomposition is not form of environment-specific guidance but rather generic mechanism derived directly from the structure of the state representation itself. The Crafter-OO environment exposes an object-oriented state, defined by schema of classes and attributes. Our change detectors mirror this schema, creating one detector for each top-level attribute and for each object type. This approach provides structural inductive biasthat the environments causal mechanisms are likely aligned with its object-oriented structurewithout embedding knowledge of the environments actual dynamics. The process could be fully automated for any environment that exposes typed, structured state; the detectors can be generated programmatically by reflecting on the state schema. This is analogous to how computer vision model might process distinct objects in scene separately; we partition the state space based on its given structure, but the rules governing the interactions between these partitions must still be learned from scratch. Prompt Generation. For each transition-aspect pair that triggers synthesis task, we generate detailed prompt for the LLM. The goal is to provide all necessary context for the model to infer the underlying game mechanic. The prompt contains several key components: 1. The initial state st and resulting state st+1, serialized to structured format (JSON). 2. The action at that caused the transition. 3. textual diff that highlights the exact changes between st and st+1. 33 4. human-readable 2D ASCII rendering of the local environment around the player for both states, providing spatial context. 5. The name of the aspect (e.g., player inventory) that changed, which instructs the LLM to focus its analysis. This structured presentation of the transition allows the LLM to ground its reasoning in the specific, observed changes. The full prompt template is provided in Fig. F.2. Figure F."
        },
        {
            "title": "Synthesis Prompt",
            "content": "1 ## Role 2 You are **World Law Synthesizer** - an expert at analyzing game state transitions and extracting the underlying rules that govern virtual worlds. Your job is to observe how actions transform game states and codify these transformations into precise, executable laws that can model game mechanics, as well as try to model aspects of the underlying transition dynamics as functions. 3 4 ## Task Description 5 Given world state, an action taken, an aspect of the state we are interested in modeling, and the resulting next world state (plus diff highlighting the changes), you must: 6 - Identify how the aspect of the state we are interested in modeling changed between the observations 7 - Determine the underlying rules or laws that caused these changes 8 - Implement these laws as executable Python code using the provided WorldState interface and DiscreteDistribution for predictions 9 10 **IMPORTANT: You should write MULTIPLE laws when you observe multiple distinct changes.** Each law you write should be modular, minimalistic, focused on single game mechanic, and capable of being combined with other laws to model complex game behavior. 11 12 In particular, you should strive to write laws that are responsible for as little of the state as possible. In any given transition, you may see many changes . Each of these changes could be caused by different law. Think about what changes could be grouped together into single law, and write separate laws for different types of changes. 13 14 - Break up the laws to each account for single precondition and effect. For example, if an entity moves, write law for the movement of entities of that type. If player takes particular action, write law for that action specifically. 15 - Certain attributes cannot have `DiscreteDistribution` applied to them. For example, the `materials` field should just be modified directly, not wrapped in `DiscreteDistribution`. Alternatively, use `set_material` or ` set_facing_material` to modify the materials field. Either way, they cannot be wrapped in `DiscreteDistribution`. 16 - Use the `DiscreteDistribution` class to indicate probabilistic predictions, for example when trying to write general law governing all entities of type when you cannot reconcile all changes visible to that entity type into deterministic law. 17 - You DO NOT need to use imports. Everything you need can be coded without the use of imports, and all classes defined below are already imported. 18 19 ## Aspect of the State 20 You will be given an aspect of the state we are interested in modeling. The laws you write should be focused on modeling changes to this aspect of the state. 21 However, you can use _all_ of the state to help you write the laws, as the aspect of the state may be influenced by other aspects of the state."
        },
        {
            "title": "23 If told to focus on the player, you should write laws that model how the player's",
            "content": "state changes. Again, these effects may be influenced by the entities that the player is interacting with. 24 25 ## Guidelines for Writing Laws 26 - Some laws may be dependent on an action being taken, or particular state of the world, while others may always apply. For these, the precondition can always be `True`. 27 - Make use of `adjacent_to_player` and `get_target_tile` to help you write laws about interactions between the player and other entities. 28 - Do NOT use `entity_id` when writing laws. You should instead write laws that apply to type of entity, e.g. `ZombieState` or `CowState`. 29 - When modifying attributes, use RELATIVE assignments rather than absolute assignments. For example, instead of changing entity's position via ` entity.position.x = DiscreteDistribution(support=[7])`, use `entity.position .x = DiscreteDistribution(support=[entity.position.x + delta])`. The only exception to this is when modifying the materials field. 30 - Use the helper functions `get_object_of_type_in_update_range`, and ` get_objects_in_update_range` rather than writing your own iteration logic. 31 - You DO NOT need to use the `entity_id` attribute. Use `get_target_tile` to get the tile or entity targeted by the player. Use `adjacent_to_player` to check if an entity is adjacent to the player for interactions between the player and other entities. 32 - Consider writing laws that make \"soft\" predictions. For example, if you see an entity moving but are unsure if it is general principle, you can assign discrete distribution to the entity's position to represent your uncertainty . Example: `entity.position.x = DiscreteDistribution(support=[entity. position.x + delta_a, entity.position.x - delta_b, ...])`. 33 - You can speculatively pose laws, but these should go last. Speculative laws are those that were not directly observed in the transition, but those that you believe might exist. For example, given that you have identified law about certain crafting recipe, you can speculatively pose law about _other_ crafting recipes that you believe might exist. 34 35 36 37 ## Formatting Instructions 38 Structure your response exactly as follows. **You can write multiple laws by repeating the pattern below for each law:** 39 40 ```xml 41 <keyChanges> 42 List the specific, concrete changes that occurred between the observations: 43 - What entities appeared, disappeared, or moved 44 - What stats/values changed and by how much 45 - What items were added/removed from inventory 46 - Any other measurable state differences 47 </keyChanges> 48 <naturalLanguageLaw> 49 Write clear, concise description of the game rule that explains these changes: 50 - What triggers this law (the preconditions) 51 - What the law does (the effects/transformations) 52 - Any important parameters or variations 53 - Give the law descriptive name 54 </naturalLanguageLaw> 55 <lawCode> 56 ```python 57 class YourLawNameHere: 58 def __init__(self, param1: type = default_value, param2: type = default_value ): 35 59 60 62 63 64 65 66 68 69 70 71 72 74 75 \"\"\"Initialize with configurable parameters.\"\"\" self.param1 = param1 self.param2 = param2 # Add any lookup tables or constants here def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" # Implement your precondition logic here # Check action type, entity presence, player state, etc. # Replace with actual logic return False def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" # Implement the state transformation here # Modify entities, player stats, inventory, etc. # Use DiscreteDistribution(support=[value]) to set deterministic predictions # Example: current_state.player.health = DiscreteDistribution(support=[ new_health]) pass # Replace with actual implementation 76 77 ``` 78 </lawCode> 79 80 <keyChanges> 81 [Changes for second law...] 82 </keyChanges> 83 <naturalLanguageLaw> 84 [Description of second law...] 85 </naturalLanguageLaw> 86 <lawCode> 87 ```python 88 class YourSecondLawNameHere: 89 90 ``` 91 </lawCode> 92 ``` 93 94 **Critical Formatting Notes**: 95 - **Write multiple laws when you observe multiple distinct changes** - each law # [Implementation of second law...] should focus on single type of change 96 - Use exactly these XML-style tags: `<keyChanges>`, `<naturalLanguageLaw>`, `< lawCode>` 97 - Close each tag properly: `</keyChanges>`, `</naturalLanguageLaw>`, `</lawCode>` 98 - Put all Python code inside triple backticks within the `<lawCode>` section 99 - Be precise and specific in the key changes - use exact numbers and entity names from the observations 100 - Make the natural language law description clear enough that another programmer could implement it independently 101 - Only output the code for the law, not the entire file. Assume the `WorldState` class as well as its components are already defined. 102 - Format your response well, with newlines between the tags and code blocks. 103 - **Each law should be completely self-contained** - repeat the full XML structure for each law you write. 104 105 ## WorldState 106 The world state is Pydantic model that represents the complete game world state . The world laws you write will operate on this state. 107 108 ```python 109 {{ world_state_schema }} 110 ``` 111 112 # World Laws 36 118 120 121 122"
        },
        {
            "title": "113 Each world law must conform to the following interface:\n114\n115 ```python\n116 class WorldLaw:\n117",
            "content": "def precondition(self, current_state: WorldState, action: str) -> bool: \"\"\"Return True if this law should apply to the given state and action.\"\"\" ... def effect(self, current_state: WorldState, action: str) -> None: \"\"\"Apply the law by modifying the world state.\"\"\" # Use DiscreteDistribution(support=[value]) to set deterministic predictions # Example: current_state.player.health = DiscreteDistribution(support=[ new_health]) ... 125 126 ``` 127 You may add any additional fields or methods to the class as needed. 128 129 ## DiscreteDistribution Usage 130 When modifying state values in your law's `effect` method, you must wrap the new values with `DiscreteDistribution`: 131 132 ```python 133 # For deterministic predictions: 134 current_state.some.value = DiscreteDistribution(support=[new_health]) 135 136 # For stochastic predictions (if needed): 137 current_state.some_value = DiscreteDistribution(support=[value1, value2, value3]) 138 ``` 139 140 The `DiscreteDistribution` class represents probabilistic predictions over discrete values. For deterministic laws, you typically provide single value in the support list. For stochastic laws, you provide multiple values in the support list to represent the possible outcomes. 141 142 When accessing the materials field, pay attention to the `MaterialT` type. Everything in the `materials` field is `MaterialT`. Do not use the emojis in the world map, they are only there for your convenience. 143 144 # Your Turn 145 ## Aspect of the State 146 Focus on modeling changes to the following aspect of the state: 147 {{ aspect_of_state }} 148 149 ## Focused Changes for {{ aspect_of_state }} 150 {{ aspect_changes }} 151 152 ## View Legend 153 {{ view_legend }} 154 155 ## State 156 ```json 157 {{ state }} 158 ``` 159 ### Local View 160 ``` 161 {{ local_view }} 162 ``` 163 164 ## Action 165 The action taken was: \"{{ action }}\" 37 Law Generation and Parsing. The generated prompt is sent to an LLM, which is instructed to return one or more atomic laws that explain the observed changes for the specified aspect. An atomic law is simple, modular rule focused on single game mechanic. The LLMs response is formatted using XML-style tags to clearly delineate the key components of each proposed law. The expected format for single law is: <keyChanges>...</keyChanges> <naturalLanguageLaw>...</naturalLanguageLaw> <lawCode> ```python class LawName: def precondition(self, state, action): ... def effect(self, state, action): ... ``` </lawCode> We parse this semi-structured text to extract the natural language description and the executable Python code for each proposed law. This is done by searching for the corresponding tags and extracting their content. The Python code is then loaded as candidate law for the subsequent parameter inference stage. 1 def synthesize_laws_from_trajectory(trajectory: list[Transition]) -> list[Law]: 2 candidate_laws = [] 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 # Iterate over all transitions from the exploration data for transition in trajectory: s_t, action, s_t_plus_1 = transition # 1. Detect which aspects of the state have changed changed_aspects = [] for detector in ALL_DETECTORS: if detector.has_changes(s_t, s_t_plus_1): changed_aspects.append(detector.aspect_name()) # 2. For each detected change, generate laws for aspect in changed_aspects: # 2a. Render detailed prompt for the LLM prompt = render_synthesis_prompt( state=s_t, action=action, next_state=s_t_plus_1, aspect_of_state=aspect ) # 2b. Query the LLM to synthesize laws llm_response_text = call_llm(prompt) # 2c. Parse the response to extract structured laws parsed_laws = parse_laws_from_response(llm_response_text) candidate_laws.extend(parsed_laws) return candidate_laws Listing 17: High-level overview of the law synthesis pipeline. Figure F."
        },
        {
            "title": "Zombie Fighter Plan",
            "content": "1 def craft_wooden_sword_plan( state: WorldState, 2 transition_fn: Callable[[WorldState, CrafterAction], WorldState], num_trees: int = 3 3 4 38 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 33 34 39 40 41 42 43 45 46 47 48 56 58 59 60 61 62 64 65 5 ) -> WorldState: 6 trees_chopped = 0 pathfind_option = PlayerPathfindOption( lambda s: find_closest_material_of_type(s, \"tree\")[1] ) interact_option = PlayerInteractAdjacentOption( lambda s: find_closest_material_of_type(s, \"tree\")[1] ) # Gather wood by iterating between pathfinding and interaction while trees_chopped < num_trees: try: action = pathfind_option.action(state) except TerminationCondition: action = interact_option.action(state) if action == \"do\": trees_chopped += 1 state = transition_fn(state, action) # Place crafting table and craft sword state = transition_fn(state, \"place_table\") state = transition_fn(state, \"make_wood_sword\") return state 30 31 def defeat_zombies_plan( state: WorldState, transition_fn: Callable[[WorldState, CrafterAction], WorldState], zombie_ids: list[int], max_steps_per_zombie: int = 10 35 36 ) -> WorldState: 37 for zombie_id in zombie_ids: combat_option = CombatFixedEntityOption(entity_id=zombie_id) for _ in range(max_steps_per_zombie): try: action = combat_option.action(state) state = transition_fn(state, action) except TerminationCondition: break # Zombie defeated return state 49 50 def sword_then_zombies_plan( state: WorldState, 51 transition_fn: Callable[[WorldState, CrafterAction], WorldState], zombie_ids: list[int] 52 53 54 ) -> WorldState: \"\"\" High-level plan: Craft weapon before engaging in combat. Composes two sub-plans into complete strategy. \"\"\" # Sub-plan 1: Obtain weapon state = craft_wooden_sword_plan(state, transition_fn, num_trees=3) # Sub-plan 2: Defeat enemies state = defeat_zombies_plan(state, transition_fn, zombie_ids) return state"
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}