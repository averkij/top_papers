{
    "paper_title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Qiushi Sun",
        "Jinyang Wu",
        "Zixian Huang",
        "Muye Huang",
        "Jingyang Gong",
        "Zichen Ding",
        "Kanzhi Cheng",
        "Yian Wang",
        "Xinyu Che",
        "Zeyi Sun",
        "Jian Zhang",
        "Zhangyue Yin",
        "Haoran Luo",
        "Xuanjing Huang",
        "Ben Kao",
        "Jun Liu",
        "Qika Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena"
        },
        {
            "title": "Start",
            "content": "ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Fangzhi Xu 1 * Hang Yan 1 * Qiushi Sun 2 * Jinyang Wu 3 Zixian Huang 4 Muye Huang 1 Jingyang Gong 2 Zichen Ding 4 Kanzhi Cheng 5 Yian Wang 6 Xinyu Che 1 Zeyi Sun 4 Jian Zhang 1 Zhangyue Yin 7 Haoran Luo 8 Xuanjing Huang 7 Ben Kao 2 Jun Liu 1 Qika Lin 6 6 2 0 2 5 ] . [ 1 3 4 8 5 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce ODYSSEYARENA, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish ODYSSEYARENALITE for standardized benchmarking, providing set of 120 tasks to measure an agents inductive efficiency and long-horizon discovery. Pushing further, we introduce ODYSSEYARENACHALLENGE to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit deficiency in inductive scenarios, identifying critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/ xufangzhi/Odyssey-Arena *Equal contribution 1Xian Jiaotong University 2The University of Hong Kong 3Tsinghua University 4Shanghai AI Laboratory 5Nanjing University 6National University of Singapore 7Fudan University 8Nanyang Techonological University. Correspondence to: Qika Lin <linqika@nus.edu.sg>. Preprint. Copyright 2026 by the author(s). 1 Figure 1. Comparison between deductive and inductive settings in multi-turn agentic tasks. 1. Introduction The emergence of Large Language Models (LLMs; Gemini Team, 2025; Anthropic AI, 2024) has sparked unprecedented interest in autonomous agents that can perceive environments, make decisions, and take actions to accomplish complex tasks. These AI agents are increasingly deployed across diverse domainsfrom robotics (Wang et al., 2025a) and game playing (Akata et al., 2025) to scientific discovery (Sun et al., 2025b) and business automation (Xu et al., 2025c). As the capabilities of LLMs expand, so does the demand for evaluation benchmarks that can reliably assess agent performance in realistic, dynamic settings. While recent benchmarks have evolved from static question answering toward interactive decision-making (Xie et al., 2024; Vodrahalli et al., 2024; Chung et al., 2025), they predominantly assess deductive mode of intelligence where agents rely on extensive prior knowledge to complete tasks. As illustrated in Figure 1, such evaluation overlooks the essential inductive mode, in which agents are required to actively explore and induce the hidden rules underlying the environment. This omission restricts the evaluation of an agents proficiency in complex environments where rules are not pre-specified. We identify three critical capability dimensions that remain largely unaddressed in the current landscape. First, extremely long-horizon interaction is often neglected by existing benchmarks that restrict episodes to fewer than 50 steps, which fails to capture the strategic coherence and error accumulation challenges inherent in sequences of thousands of steps. Second, active exploration ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions and trial-and-error are frequently bypassed by environments that provide fully specified success criteria, whereas realistic deployment requires agents to actively probe, react to feedback, and iteratively adjust their actions through self-improvement (Yan et al., 2025). Finally, inductive reasoning from interaction represents critical evaluation gap in current benchmarks, as most protocols assess deductive compliance to provided instructions rather than the capacity to infer latent rules and transition dynamics from experience. In order to bridge this gap, we introduce ODYSSEYARENA, suite of interactive environments that re-centers evaluation on long-horizon, active, and inductive reasoning, which entails inferring latent transition laws from empirical interactions. We formalize environments as generative state transition functions: (st+1, rt) = (st, at), where st is the latent state and at is the agent action. The transition function implicitly encodes the environments rules and regularities, which agents must actively induce from interaction in order to anticipate outcomes, plan effectively, and optimize behavior over long horizons. This makes the central object of agent evaluation. To systematically study an agents inductive capacity, we decompose into taxonomy of four representative structural primitives: discrete latent rules, continuous stochastic dynamics, multi-objective periodic patterns, and relational dependencies. To facilitate empirical investigation into these abstract dynamics, we materialize these primitives into four diverse ODYSSEYARENA environments. These environments are curated to be computationally efficient and lightweight while remaining functionally representative of real-world systems, providing tractable and scalable testbed for the community. Specifically, Turn On Lights grounds discrete Boolean logic in interdependent bulb configurations, while AI Trading presents continuous stochastic dynamics through multivariate stock-factor relationships. Energy Dispatch requires agents to uncover periodic efficiency patterns under multi-objective constraints, and Repo System engages agents in deducing the topological dependencies among software package versions in virtual environment. For practical evaluation, we provide ODYSSEYARENA-LITE as standardized benchmark of 120 curated tasks. This suite serves as representative of scalable task distribution, optimized for high evaluation throughput while preserving the core challenges of active and inductive discovery. Each task maintains interaction horizons that are computationally tractable yet sufficiently non-trivial to necessitate longhorizon planning and the active induction of latent rules. In addition, we release ODYSSEYARENA-CHALLENGE, stress-test suite with 1,000+ steps per task and 10 tasks per environment, intended to probe the limits of agent persistence, reasoning stability, and the ability to maintain coherent strategies over extremely long horizons. Together, these two settings balance accessibility, efficiency, and scalability, supporting both rapid iteration on current agents and rigorous evaluation of next-generation capabilities. We evaluated over 15 trending LLMs, spanning proprietary models and open-source models across different scales. Overall, commercial models consistently outperform opensource alternatives, with Gemini 3 Pro Preview achieving the highest success rate of 44.17. Despite this, even the strongest commercial models broadly remain far below human-level performance across four environments in ODYSSEYARENA-LITE, highlighting substantial gaps in long-horizon reasoning, active exploration, and inductive generalization. Beyond these aggregate results, we conducted detailed, fine-grained analysis of agent behavior across different environments and tasks, revealing patterns and failure modes that provide actionable insights for designing more capable and robust autonomous agents. Our primary contributions are as follows: (1) Novel Perspectives For Agent Evaluation: We propose novel evaluation paradigm centered on the capacity for autonomous discovery. This shift refocuses agentic intelligence on the induction of latent world dynamics through long-horizon and active interaction. (2) Reliable and Scalable Evaluation : We instantiate the inductive paradigm evaluation into ODYSSEYARENA environments, from which we establish ODYSSEYARENALITE as standardized suite for efficient evaluation. Our environments further provide foundation that scales to probe the limits of agentic intelligence. (3) Extensive Evaluations and Insights: Through an extensive evaluation of 15+ top-tier LLMs, we characterize the inductive bottleneck as fundamental barrier to autonomous discovery while establishing rigorous benchmarking results for future research. 2. Related Work Interactive Benchmarks. Interactive benchmarks for LLM agents have evolved from grounded language understanding in simplified grid-worlds (Shridhar et al., 2021; Chevalier-Boisvert et al., 2019) to sophisticated digital (Deng et al., 2023; Zhou et al., 2024; Xie et al., 2024) and real-world systems (Xu et al., 2025c; Yao et al., 2025). Despite this progress, critical bottleneck remains in temporal depth: most environments favor short horizons or trajectories (typically dozens of steps; Rawles et al., 2025; Sun et al., 2025a), failing to capture the butterfly effect of error accumulation and the decay of long-term planning consistency (Liu et al., 2024). Furthermore, many existing protocols (Wang et al., 2025b; Patil et al., 2025) bypass exploratory requirements by providing gold instructions or 2 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 1. Comparison of representative multi-turn agentic benchmarks. Ind. indicates whether inductive reasoning is required. Horizon denotes the number of steps required to complete task, categorized as short (<50), long (50100), or X-Long (>100). Deploy describes the evaluation setup required to run the environment, with API-based deployment being the most lightweight. ten focus on policy optimization within known or fixed MDPs, ODYSSEYARENA emphasizes latent structure induction. To achieve long-horizon planning and optimal decision-making, agents must autonomously discover the functional form of through strategic interaction."
        },
        {
            "title": "Benchmark",
            "content": "BabyAI (2019) ALFWorld (2021) GAIA (2023) WebArena (2024) OSWorld (2024) AndroidWorld (2025) BrowseComp (2025) ODYSSEYARENA Ind. Horizon Long Short Long Short Long Short Long X-Long"
        },
        {
            "title": "API",
            "content": "detailed API docs. ODYSSEYARENA and derived benchmarks bridge these gaps by introducing long-horizon tasks that demand coherent internal states and robust recovery strategies over extended interaction sequences. Inductive Reasoning. Current agentic frameworks, such as ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2023), primarily rely on deductive reasoning or test-time interactions (Sun et al., 2023; Xu et al., 2025a;b) to apply internal knowledge or provided rules. However, the nature of intelligence necessitates inductive reasoning, which refers to the capacity to infer latent rules and transition dynamics from raw observations (Lake et al., 2017). While static benchmarks like ARC (Chollet, 2019) and Zebra-Logic (Lin et al., 2025) evaluate rule synthesis, they remain passive and fail to capture the active discovery loop essential for autonomous agents (Lin et al., 2025). While interactive environments like Mars (Tang et al., 2024) facilitate exploratory interaction, they often struggle to decouple pure induction from pre-trained knowledge priors. In contrast with typical agent benchmarks shown in Table 1, this work necessitates inducing latent world structures through extremely longhorizon interactions, aligning with the concept of world models (Ha & Schmidhuber, 2018) while posing higherorder symbolic challenges for future agentic intelligence. 3. ODYSSEYARENA 3.1. Preliminaries Interactive environments can be characterized as generative processes where the environments response to an action at at latent state st is governed by transition function : (st+1, rt) = (st, at). (1) In this framework, encapsulates the unobservable regularities and constraints that dictate the systems evolution. Unlike standard reinforcement learning paradigms that ofTo systematically characterize environment dynamics, we decompose the landscape of environment dynamics into taxonomy of four orthogonal structural primitives. These primitives represent the fundamental mathematical motifs that lead to complex real-world systems (Clark, 1997; Li et al., 2024). By isolating these irreducible structures, we define comprehensive set of world-modeling challenges that an autonomous agent must navigate: Discrete Symbolic Rules: The transition is governed by Boolean logic over bits, where {0, 1}N . This requires the agent to perform symbolic hypothesis testing to uncover the latent causal dependencies and logical couplings. Continuous Stochastic Dynamics: The system evolves through continuous state space Rd according to st+1 = (st, at) + ϵ, where incorporates latent functional signal and exogenous noise ϵ. This necessitates statistical inference to disentangle underlying regularities from stochastic fluctuations. Periodic Temporal Patterns: The transition function exhibits cyclic regularities defined by period , such that (s, a, t) (s, a, + ). This necessitates identifying long-range temporal dependencies to optimize multiobjective trade-offs. Relational Graph Structures: The environment is defined by graph = (V, E), where transitions involve non-local interactions between entities. Success requires relational reasoning over the topological constraints that govern global state changes. This taxonomy ensures comprehensive evaluation, as each structure induces distinct cognitive requirementranging from logical deduction to relational abstractionthat is irreducible to the others. ODYSSEYARENA integrates these primitives into diverse suite of environments designed to assess an agents fundamental capacity for world-structure induction, as illustrated in Figure 2. We will introduce the respective environments in detail as follows. 3.2. Env I: Turn On Lights Overview. This environment instantiates the discrete symbolic rules primitive by simulating network of interdependent lights. The agent aims to reach target configuration = 1, representing the state where all lights are illuminated, through sequence of toggling actions. Dynamics are governed by latent Boolean couplings that remain fixed within one trial but vary across episodes, where an 3 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 2. Demonstrations of four ODYSSEYARENA environments: Turn On Lights, AI Trading, Energy Dispatch, and Repo System. For clarity, we omit the task prompts here and present only the interaction trajectories. Full prompts are provided in Appendix C. intervention on single light may trigger deterministic cascade of state changes across the network. Consequently, success requires active exploration to infer the underlying logic and the correct activation sequence. Hidden Rules. State transitions are governed by latent discrete rules that define how each action influences the configuration of lights. For each episode, these rules are instantiated by randomly combining Boolean operators to create unique logical network, while ensuring that the resulting dependencies are solvable. As result, an action targeting particular light may deterministically affect multiple lights through indirect toggling or conditional activation. The dependencies remain fixed within an episode but vary across episodes, and are not directly observable to the agent. Consequently, the agent must infer the underlying logical structure through deliberate intervention and observation of state changes. Action and Observation Spaces. At each step, the agent takes discrete action to activate/toggle single light. Although actions are defined over individual lights and remain fixed across episodes, their effects are not necessarily localized due to hidden logical couplings. After each interaction, the agent observes the on/off status of all lights. No information about the underlying logical dependencies or transition rules is revealed. As result, while the environment is fully observable with respect to the surface state, it is partially observable in terms of the latent transition dynamics induced by the hidden rule configuration. Task Evaluation. An episode is considered successful if all lights are turned on before the interaction budget is exhausted. Performance is measured by the task success rate. Solving the task efficiently requires the agent to identify and exploit the latent logical rules, rather than relying on myopic or brute-force exploration. 3.3. Env II: AI Trading Overview. This environment instantiates the continuous stochastic dynamics primitive, where the agent manages multi-asset portfolio to maximize cumulative returns (cid:80) rt. Market transitions are driven by latent factors obscured by stochastic fluctuations, with daily news serving as indirect hints for future price movements. Unlike reactive tasks, success here depends on the agents ability to induce the underlying market regularities, requiring it to disentangle the meaningful signal from noise to execute optimal multistep, long-horizon trading strategies. Hidden Rules. Price transitions are governed by the function st+1 = Wzt + ϵ, where Rd denotes asset returns. In this formulation, represents latent factor loading matrix that maps vector of unobserved market factors zt to the price changes of assets. While zt and the noise ϵ fluctuate at each timestep, the structural relationships defined by remain invariant within an episode but vary across tasks. Consequently, agents must treat the sequence of interactions as noisy observation process to identify the functional form of for effective long-horizon planning. 4 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Action and Observation Spaces At each step t, the agent perceives an observation ot comprising historical prices and news-derived indicators. Since ot serves as noisy proxy for zt, the agent must utilize the information within ot to estimate the underlying market state. Based on the perceived ot, the agent issues combinatorial action at specifying buy and sell quantities across all assets. To enable portfolio reallocation within single decision cycle, the environment enforces sequential execution, processing sell orders before buy orders, requiring the agent to jointly reason about asset selection, trade timing, and capital allocation under the transition logic . Task Evaluation. Performance is measured by cumulative return over the trading horizon, adjusted for transaction costs and risk constraints. Successful performance requires learning and exploiting the latent stochastic dynamics over time, as opposed to reactive or single-step trading strategies. 3.4. Env III: Energy Dispatch Overview. This environment instantiates periodic temporal patterns by modeling long-horizon energy dispatch problem in dynamic power grid. The agent acts as dispatcher, allocating thermal, wind, solar, and battery resources to meet daily demand under budget constraints. Unlike deductive optimization tasks, the environment enforces strict constraints: repeated budget or demand violations trigger immediate termination, simulating irreversible system failure. Achieving stable, low-carbon supply therefore requires anticipating latent efficiency cycles and planning over extended horizons to avoid systemic collapse. Hidden Rules. The core challenge lies in the discrepancy between the agents planned dispatch and the actual power generation. The system evolves according to transition logic where the realized output Preal is determined by the agents rated action at modulated by latent efficiency vector Et, formally Preal at Et. Here, Et represents the time-varying efficiency for each power source. Crucially, the efficiency factors for wind and solar are governed by distinct, unobserved periodic functions Et Et+T with unique periods . Since these fluctuations are not directly observable, the agent must infer the underlying periodic structures from historical output gaps. simulating an irreversible grid collapse. Task Evaluation. Success is defined by rigorous multiobjective criterion. First, the agent must ensure survival by maintaining system stability without triggering early termination over the full horizon (e.g., 120 days). Second, upon completion, the aggregate performance must satisfy predefined thresholds for both Carbon Intensity (C < τc) and Grid Stability (S > τs). 3.5. Env IV: Repo System Overview. This environment models realistic software repository management scenario, where the agent must configure Python project to execute successfully. The agent interacts with partially observable dependency ecosystem, in which resolving local failures may introduce new global inconsistencies. Success requires systematic diagnosis, relational reasoning over dependencies, and careful planning under non-monotonic side effects. Hidden Rules. The transition logic is defined by latent dependency graph = (V, E). Here, nodes represent software packages with specific versions, and directed edges represent compatibility constraints (e.g., Pkg v1.0 requires Pkg v2.0). The system state st represents the currently installed environment configuration. Crucially, the graph is hidden from the agent. transition st+1 = (st, at) triggered by an installation command involves rigorous resolution process: the system automatically installs required ancestors and uninstalls conflicting nodes to maintain local consistency, potentially altering parts of st not targeted by at (side effects). Action and Observation Space. The agent perceives the environment through ot, consisting of terminal outputs, file structures, and execution logs. Since is latent, ot serves as sparse signal revealing only the graph broken edges (e.g., ImportError). Based on these error traces, the agent issues discrete symbolic actions at Ashell (e.g., pip install). Since actions act as high-level graph mutation operators, the agent must sequence them to navigate the combinatorial state space, deducing the topology of to resolve conflicts without explicitly observing the full picture. Action and Observation Space. At each timestep, the agent receives ot containing the current electricity demand Dt and operating budget Bt. Based on these requirements, the agent issues continuous action at R4 specifying the rated output for each generation type and the net charge/discharge command for the battery. The battery introduces inter-temporal dependencies, allowing the agent to buffer energy across time steps. Crucially, the environment enforces strict safety protocol: repeated violations of demand satisfaction or budget limits trigger an early termination, Task Evaluation. The task is evaluated by whether the agent achieves globally consistent environment in which the full project executes successfully. Partial or intermediate fixes (e.g., running individual sub-scripts) receive no reward if the global entry point fails, as local improvements may conflict with overall correctness. This strictly binary protocol enforces global consistency, emphasizing long-horizon planning, relational reasoning over interdependent components, and robustness to delayed and indirect effects over myopic error patching. 5 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 3. Overview of the benchmark architecture, illustrating the environment configuration initialization (left) and the interaction loop between the LLM agent and the environment step logic (right). 4. ODYSSEYARENA-LITE and ODYSSEYARENA-CHALLENGE Building upon the infinite task space provided by ODYSSEYARENA, we derive two distinct benchmarking protocols to serve as standardized instantiations designed to evaluate agent performance across different difficulties: We primarily introduce ODYSSEYARENA-LITE as suite tailored for efficient and reproducible performance assessment. Additionally, we provide ODYSSEYARENA-CHALLENGE for stress-testing agent stability and inductive resilience over extreme interaction horizons. The benchmark construction process and sample details are described below. 4.1. Task Configuration and Curation To derive verifiable benchmarks from ODYSSEYARENA, we formalize each task as deterministic instance sampled from bounded parameter distribution (see Appendix for details). This transition from abstract primitives to concrete evaluation is achieved by decomposing each instance into its structural configuration and temporal trajectory. Structural Configuration. The configuration initializes latent rules that remain invariant throughout an episode. In Turn On Lights, this corresponds to stochastically generating DAG with specified density and logical operator ratios; in AI Trading and Repo System, it instantiates linear coefficient matrices and package dependency graphs, respectively. To ensure solvability, constraint satisfaction algorithms are applied during curation to guarantee valid solution path for every rule set. Temporal Trajectory. To eliminate stochasticity, we predetermine all time-varying factors in ODYSSEYARENA as fixed sequences within the task metadata. For environments with dynamic states such as AI Trading and Energy Dispatch, components including daily factor fluctuations, efficiency curves, and resource budgets are pre-calculated. This ensures that the environments evolution is not subject to runtime randomness, allowing for an identical and fair comparison across different agents and experimental trials. Task Sampling Strategy. We sample tasks by defining valid ranges for key environment parameters, such as the number of entities or the depth of logical dependencies. This sampling process is specifically calibrated to modulate task difficulty across two protocols. ODYSSEYARENA-LITE occupies tractable parameter set for efficient evaluation, while ODYSSEYARENA-CHALLENGE targets the empirical limits of these ranges to stress-test agent stability over extreme horizons. This approach enables characterization of the inductive bottleneck across varying environmental challenges. Detailed human anno processes are in Appendix D. 4.2. Task Statistics Under the ODYSSEYARENA-LITE setting, we construct fixed evaluation suite by sampling 30 tasks for each environment. The maximum allowed number of interaction steps is environment-specific, reflecting the inherent complexity of the underlying dynamics. Specifically, the step limits are set to 200 for TurnOnLights, and 120 for AI Trading, Energy Dispatch, and Repo Management, respectively. This configuration yields balanced benchmark that supports reliable comparison across environments while remaining computationally efficient for online evaluation. While ODYSSEYARENA-LITE serves as our primary evaluation setting and the basis for all main experiments in this paper, we additionally introduce ODYSSEYARENACHALLENGE as more demanding variant designed for stress-testing advanced agents, which substantially extends the required reasoning horizon, with tasks often exceeding 1,000 interaction steps. This setting targets failure modes that do not manifest under standard budgets, such as longterm credit assignment breakdown and compounding planning errors. Due to its significantly higher computational cost, ODYSSEYARENA-CHALLENGE is not used in our 6 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 4. Success rate comparison of w/ and w/o rules in Turn On Lights. We select Llama 3.3 70B Instruct, GLM-4-32B-0414, Qwen3-235B-A22B-Instruct, DeepSeek-V3.2, Grok 4 Fast, GPT5, Gemini 3 Pro Preview for illustration. main evaluations, but is provided as an optional benchmark for future research. 5. Experiments 5.1. Experimental Settings We evaluate over 15 trending LLMs on ODYSSEYARENALITE, encompassing proprietary frontiers such as Gemini 3 Pro Preview, Gemini 2.5 Pro (Gemini Team, 2025), GPT5, and Grok 4 Fast, alongside open-source series including DeepSeek-V3.2 (Liu et al., 2025), gpt-oss-120b (OpenAI, 2025), Qwen3 series (Yang et al., 2025), Llama 3 series (Grattafiori et al., 2024), and GLM-4 series (GLM et al., 2024). Each test case is executed four times to report both Avg.@4 and Pass@4 success rates. To manage the extreme interaction horizons, our standard prompts retain only the history of actions and environment feedback, omitting intermediate reasoning traces from previous steps to ensure context efficiency. Additional details regarding model specifications, reasoning effort, and prompt templates are provided in Appendix C.1 and C.2. 5.2. Main Results. The General Performance Gap. Table 2 illustrates performance disparity between SOTA LLMs and human. Unlike human participants who successfully resolve tasks by isolating causal variables and distilling latent rules through instinctive trial-and-error, LLMs exhibit marked deficiency in autonomous rule induction. This gap highlights fundamental failure in current agents to internalize latent world dynamics from experience, leading to significant performance degradation as interaction horizons extend. Proprietary Model and the Scaling Limit. Frontier proprietary models, led by Gemini 3 Pro Preview, consistently provides the current performance ceiling and substantially outperform other counterparts across most environments. Despite this advantage, the ubiquitous failure in Energy Dispatch underscores critical architectural limitation shared across the spectrum: an inability to synthesize periodic patterns over extended observation windows (20 steps). This Figure 5. Task success status (based on pass@4) of different tasks in Turn On Lights. Each row represents: (a) Human, (b) Gemini3 Pro Preview, (c) GPT-5, (d) Gemini 2.5 Pro, (e) gpt-oss-120b (high), (f) DeepSeek-V3.2, (g) Grok 4 Fast, (h) Qwen3-235BA22B-Instruct, (i) gpt-oss-120b (medium), (j) Qwen3-30B-A3BInstruct, (k) GLM-4-32B-0414, (l) gpt-oss-120b (low), (m) Llama 3.3 70B Instruct, (n) Qwen3-4B-Instruct, (o) Llama 3.1 8B Instruct, (p) GLM-4-9B-Chat. Dark green cells indicate tasks solved by Human. Green cells indicate tasks solved by LLM agents. Gray cells indicate unsolved tasks. for each subset (Easy, Medium and Hard), we report the average success rate across all LLMs. suggests that while increased scale enhances deductive compliance, it remains insufficient to overcome the inductive bottleneck for robust world-structure modeling. Deductive Proficiency vs. Inductive Deficiency. To disentangle the root of failure, we evaluate agents with explicit access to latent transition rules. Figure 4 shows that frontier models achieve near-perfect success when the underlying logic is provided, yet falter significantly without it. This contrast identifies fundamental asymmetry: LLMs excel at deductive reasoning but lack the inductive capacity to autonomously synthesize environment mechanics from experience. These findings confirm that the primary bottleneck in ODYSSEYARENA is the discovery of world dynamics rather than the complexity of the task logic itself. 6. Analysis 6.1. Complexity Scaling and the Inductive Ceiling To probe agentic limits, we analyze performance across Easy, Medium, and Hard task subsets. Figure 5 reveals distinct inductive ceiling: while humans achieve perfect success (30/30), the leading Gemini 3 Pro Preview resolves only 23 instances. Notably, 6 high-complexity tasks remain unsolved by any tested model, identifying rigid inductive barrier. This stagnation suggests that as environmental dimensionality scales, the requirements for world-modeling exceed the capacity of current LLMs, highlighting persistent gap between deductive compliance and autonomous discovery. More results on other environments can be found in Appendix B.4. ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 2. Performance comparison on four environments. We provide three different reasoning effort of gpt-oss-120b. For AI Trading environment, we report the profit rate and pass@4 is calculated based on the highest profit of each task. For other three environments, we report the success rate. Colored Rows represent proprietary models. The best results are in bold. Model Human Gemini 3 Pro Preview GPT-5 Gemini 2.5 Pro gpt-oss-120b (high) DeepSeek-V3.2 Grok 4 Fast Qwen3-235B-A22B-Instruct Qwen3-30B-A3B-Instruct gpt-oss-120b (medium) GLM-4-32B-0414 gpt-oss-120b (low) Llama 3.3 70B Instruct Qwen3-4B-Instruct Llama 3.1 8B Instruct GLM-4-9B-Chat Turn On Lights AI Trading Energy Dispatch Repo System Avg@4 Pass@4 Avg@ Pass@4 Avg@4 Pass@4 Avg@4 Pass@4 81.67 44.17 28.33 29.17 27.50 18.33 14.17 15.00 11.67 16.67 14.17 7.50 6.67 0.00 6.67 0. 100.00 +92.55% +197.23% 25.00 76.67 40.00 50.00 40.00 36.67 40.00 43.33 26.67 40.00 33.33 13.33 16.67 0.00 20.00 0.00 +67.71% +76.94% 30.00 23.33 +17.32% +20.47% 10.83 +33.02% +40.12% 0.00 +23.27% +27.47% 0.00 +12.88% +8.62% 0.00 +5.70% +11.52% 0.00 +11.26% +17.67% 0.00 +8.94% +4.76% 0.00 +7.09% +3.21% 0.00 +7.24% +3.14% 0.00 +5.70% +2.02% 0.00 +2.01% +0.77% 0.00 +6.95% +1.67% 0.00 +3.07% +0.55% 0.00 +0.41% -0.18% 60.00 36.67 40.00 26.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 77.50 65.83 62.50 50.00 18.33 48.33 38.33 15.83 26.67 2.50 9.17 9.17 19.17 13.33 0.00 0.00 100.00 80.00 83.33 66.67 33.33 76.67 60.00 36.67 50.00 6.67 30.00 26.67 40.00 26.67 0.00 0.00 Figure 6. Success Rate against Step in two environments. Figure 7. Model performance is significantly related to loop ratio. Infeasible region indicates that high Loop Ratio results in an inability to solve long-horizon inductive reasoning tasks. 6.2. Performance Saturation in Long-Horizon 6.3. Action Loops and Inductive Stagnation Analysis of success rates across interaction sequences reveals distinct performance saturation effect. As illustrated in Figure 6, increasing the interaction budget beyond an initial exploratory phase yields negligible marginal gains for most models. This plateau suggests fundamental inductive bottleneck in long-horizon scenarios, where extended interaction fails to rectify the absence of coherent internal world model. Furthermore, weaker models frequently underperform the random baseline, underscoring an inherent inability to extract latent regularities from environmental feedback. These findings indicate that the primary barrier is not interaction volume but the underlying capacity for inductive discovery. More details are in Appendix B.3. Analysis of agent trajectories reveals prevalent failure mode characterized by persistent action loops, where models repeat invalid operations despite receiving negative environmental feedback. As shown in Figure 7, higher loop ratio directly correlates with diminished success rates, identifying critical inability to capture hidden rules during interaction. This repetitive behavior signifies inductive stagnation, as agents fail to synthesize latent world laws from unsuccessful trials to refine their long-term strategy. Consequently, these cycles highlight the failure to transform trialand-error into active discovery, underscoring the gap between deductive compliance and inductive world-modeling. More loop ratio details can be found in Appendix B.5. 8 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions 7. Conclusion This work introduces paradigm shift in agentic evaluation by transitioning from deductive instruction-following to long-horizon, inductive modeling. We formalize abstract environment dynamics into four structural primitives and instantiate them via ODYSSEYARENA, and then establish standardized framework for the reproducible evaluation of an agents ability to discover latent transition laws. The observed low-performance plateau across multiple flagships reveals fundamental inductive bottleneck that scaling alone cannot resolve, underscoring the necessity of moving beyond deductive compliance. Future research toward more agentic intelligence should prioritize architectures capable of distilling latent transition laws from raw experience, thereby bridging the gap between passive rule-following and active discovery in complex, dynamic worlds."
        },
        {
            "title": "Impact Statement",
            "content": "We introduces ODYSSEYARENA, an environment for evaluating agents through long-horizon, active, and inductive interactions. By prioritizing the autonomous discovery of latent world dynamics, we facilitate the development of agents with enhanced strategic foresight in complex domains such as energy management and financial trading. We advocate for paradigm of responsible AI where agent reasoning remains interpretable and subject to robust human oversight. Agents that learn through trial and error pose risks of triggering failures if they lack sufficient safety boundaries during the discovery process. Furthermore, extreme interaction horizons present challenges in maintaining strategic coherence and preventing the accumulation of errors. We are committed to fostering reliable autonomous systems through rigorous benchmarking that emphasizes safety and ethical alignment."
        },
        {
            "title": "References",
            "content": "Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. Playing repeated games with large language models. Nature Human Behaviour, 9 (7):13801390, May 2025. doi: 10.1038/s41562-025-02172-y. URL http://dx.doi. org/10.1038/s41562-025-02172-y. ISSN 2397-3374. haiku. The claude 3 model family: Opus, Claude-3 Model Card, 1:1, URL https://assets.anthropic. Anthropic AI. sonnet, 2024. com/m/61e7d27f8c8f5919/original/ Claude-3-Model-Card.pdf. language learning. In International Conference on Learning Representations, 2019. URL https://iclr.cc/ virtual/2019/poster/733. Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. URL https://arxiv. org/abs/1911.01547. Chung, A., Zhang, Y., Lin, K., Rawal, A., Gao, Q., and Chai, J. Evaluating long-context reasoning in llm-based In NeurIPS 2025 Workshop on Bridging webagents. Language, Agent, and World Models for Reasoning and Planning, 2025. URL https://openreview.net/ forum?id=oxj422wRvO. The dynamical challenge. Clark, A. science, //www.sciencedirect.com/science/ article/abs/pii/S0364021399800305. 21(4):461481, 1997. Cognitive URL https: Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. URL https: //openreview.net/forum?id=kiYqbO3wqw. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and arXiv preprint next generation agentic capabilities. arXiv:2507.06261, 2025. URL https://arxiv. org/abs/2507.06261. GLM, T., Zeng, A., Xu, B., Wang, B., Zhang, C., Yin, D., Zhang, D., Rojas, D., Feng, G., Zhao, H., et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. URL https://arxiv.org/abs/2406.12793. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of modarXiv preprint arXiv:2407.21783, 2024. URL els. https://arxiv.org/abs/2407.21783. Ha, D. and Schmidhuber, J. information processing systems, Recurrent world modAdvances in neu2018. 31, https://proceedings.neurips. els facilitate policy evolution. ral URL cc/paper_files/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. Babyai: platform to study the sample efficiency of grounded Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions SIGOPS 29th Symposium on Operating Systems Principles, 2023. URL https://dl.acm.org/doi/ abs/10.1145/3600006.3613165. Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. URL https://pubmed.ncbi.nlm.nih. gov/27881212/. Li, J., Wu, R., Jin, X., Ma, B., Chen, L., and Zheng, Z. State space models on temporal graphs: first-principles study. Advances in Neural Information Processing Systems, 37: 127030127058, 2024. URL https://openreview. net/forum?id=UaJErAOssN. Lin, B. Y., Le Bras, R., Richardson, K., Sabharwal, A., Poovendran, R., Clark, P., and Choi, Y. Zebralogic: On the scaling limits of llms for logical reasoning. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=sTAJ9QyA6l. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. URL https: //arxiv.org/abs/2512.02556. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. In ICLR, 2024. URL https: //openreview.net/forum?id=zAdUB0aCTQ. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. OpenAI. gpt-oss-120b & gpt-oss-20b model card. gpt-oss model card, 1:1, 2025. URL https://arxiv.org/ abs/2508.10925. Patil, S. G., Mao, H., Yan, F., Ji, C. C.-J., Suresh, V., Stoica, I., and Gonzalez, J. E. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/pdf?id=2GmDdhBdDk. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W. E., Li, W., CampbellAjala, F., Toyama, D. K., Berry, R. J., Tyamagundlu, D., Lillicrap, T. P., and Riva, O. Androidworld: dynamic benchmarking environment for autonomous agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=il5yUQsrjC. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. URL https: //openreview.net/pdf?id=vAElhFcKW6. Shridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/ pdf?id=0IOX0YcCdTn. Sun, Q., Yin, Z., Li, X., Wu, Z., Qiu, X., and Kong, L. Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. arXiv preprint arXiv:2310.00280, 2023. Sun, Q., Cheng, K., Ding, Z., Jin, C., Wang, Y., Xu, F., Wu, Z., Jia, C., Chen, L., Liu, Z., et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 55555579, 2025a. Sun, Q., Liu, Z., Ma, C., Ding, Z., Xu, F., Yin, Z., Zhao, H., Wu, Z., Cheng, K., Liu, Z., et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows. arXiv preprint arXiv:2505.19897, 2025b. Tang, X., Li, J., Liang, Y., Zhu, S.-C., Zhang, M., and Zheng, Z. Mars: Situated inductive reasoning in an open-world environment. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview. net/forum?id=3qoQ6AolAz. Vodrahalli, K., Ontanon, S., Tripuraneni, N., Xu, K., Jain, S., Shivanna, R., Hui, J., Dikkala, N., Kazemi, M., Fatemi, B., et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. CoRR, 2024. URL https://openreview.net/forum? id=jdc57bqY3u. Wang, J., Shi, E., Hu, H., Ma, C., Liu, Y., Wang, X., Yao, Y., Liu, X., Ge, B., and Zhang, S. Large language models OpportuJournal of nities, challenges, and perspectives. Automation and Intelligence, 2025a. https://www.sciencedirect.com/ URL science/article/pii/S2949855424000613. 4(1):5264, robotics: for Wang, W., Han, D., Diaz, D. M., Xu, J., Ruhle, V., and Rajmohan, S. Odysseybench: Evaluating llm agents on long-horizon complex office application workflows. arXiv preprint arXiv:2508.09124, 2025b. URL https: //arxiv.org/abs/2508.09124. ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. URL https://arxiv.org/pdf/2504.12516. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2023. URL https: //openreview.net/forum?id=WE_vluYUL-X. Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. R. {$tau$}-bench: benchmark for underline{T}oolunderline{A}gent-underline{U}ser interaction in realworld domains. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=roNSXZpUDN. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=oKn9c6ytLx. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. URL https://openreview.net/forum? id=tN61DTr4Ed#discussion. Xu, F., Yan, H., Ma, C., Zhao, H., Liu, J., Lin, Q., and Wu, Z. ϕ-decoding: Adaptive foresight sampling for balanced inference-time exploration and exploitation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1321413227, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.647. URL https: //aclanthology.org/2025.acl-long.647/. Xu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: generalizable and purely unsupervised self-training frameIn Proceedings of the work for advanced reasoning. 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13153 13167, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.644. URL https: //aclanthology.org/2025.acl-long.644/. Xu, F. F., Song, Y., Li, B., Tang, Y., Jain, K., Bao, M., Wang, Z. Z., Zhou, X., Guo, Z., Cao, M., Yang, M., Lu, H. Y., Martin, A., Su, Z., Maben, L. M., Mehta, R., Chi, W., Jang, L. K., Xie, Y., Zhou, S., and Neubig, G. Theagentcompany: Benchmarking LLM agents on consequential real world tasks. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025c. URL https: //openreview.net/forum?id=LZnKNApvhG. Yan, H., Che, X., Xu, F., Sun, Q., Ding, Z., Cheng, K., Zhang, J., Qin, T., Liu, J., and Lin, Q. Tide: Trajectorybased diagnostic evaluation of test-time improvement in llm agents. arXiv preprint arXiv:2602.02196, 2025. URL https://arxiv.org/abs/2602.02196. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions A. Task Curation A.1. Turn On Lights Curation This task instantiates the discrete symbolic rules primitive by simulating network of interdependent lights. The agent aims to reach the target configuration = 1 (where all lights are illuminated) from an initial state s0 = 0. The environments dynamics are governed by transition function (st+1, rt) = (st, at) that encapsulates unobservable regularities in the form of latent boolean logic. Success necessitates the systematic discovery of these hidden logical dependencies through strategic interaction and hypothesis testing. A.1.1. STRUCTURAL CONFIGURATION Dependency Structure Construction. Each light Li = {L0, . . . , LN 1} is associated with an activation condition ϕi, propositional formula over other light states. We define strict partial order in such that if Lj appears in ϕi, then Lj Li. This ensures that each lights condition only references lights with smaller indices: ϕi = (Lj1 , Lj2 , . . . , Ljk ), where j1, . . . , jk < i. (2) The minimal element L0 is defined with constant activation condition (ϕ0 True) and its role as the entry point of the causal chain is hidden. Specifically, we apply random mapping σ : L. Consequently, the agent cannot rely on numerical preference or ordering of IDs to infer the dependency graph; instead, it must perform systematic interactions to inductively reason and subsequently bootstrap its knowledge of the transition function . Toggle Mechanism and State Transitions. Let {0, 1}N denote the state vector. When an agent performs an action at to toggle light Li, the transition function updates the state as follows: st+1,i = (cid:40) st,i st,i if ϕi(st) = True if ϕi(st) = False (3) If ϕi(st) = False, the agent receives only generic failure message. The presence of negation () introduces nonmonotonic dynamics: turning light on may satisfy one condition while violating another, necessitating complex inductive reasoning to navigate the state space. A.1.2. RESOLVABILITY AND DIVERSITY Guaranteed Resolvability. Every generated instance is verified to have at least one valid solution through an exhaustive search. An instance is accepted only if: (1) there exists path to the goal state = 1, and (2) the minimum steps required to finish the task is larger than predefined threshold. The partial order structure provides constructive guarantee for latent structure induction: since L0 is always accessible and each subsequent ϕi depends only on its predecessors, solution path always exists. Task Diversity. To ensure robust evaluation, we leverage the following parameters to generate diverse suite of environment instances: State Space Scaling (N ): By modulating the number of lights , we control the exponential growth of the state space {0, 1}N . This allows for the creation of tasks ranging from localized logic puzzles to complex systems with 2N possible configurations. Logical Combination: By varying the density of the predecessors Lj Li and the specific mixture of {, , } operators, each instance presents unique transition regularities. A.2. AI Trading Curation This task instantiates the continuous stochastic dynamics primitive by simulating multi-stock management scenario. Agents must autonomously infer the latent dependency matrix to maximize cumulative reward (cid:80) rt. The challenge lies in performing statistical inference to unravel the underlying market signal from stochastic fluctuations ϵ over fixed trading horizon. A.2.1. STRUCTURAL CONFIGURATION Dependency Matrix Construction. The relationship between latent market factors and asset returns is modeled via transition matrix RdK, where denotes the number of stocks and the number of unobserved market factors. At each time step t, the environment generates factor change vector zt RK. The resulting stock returns st+1 are governed by: st+1 = Wzt + ϵ, ϵ (0, σ2) (4) where Rd represents the vector of price returns. The price of the stock at time + 1 is updated to pt+1,i = pt,i + st+1,i. The matrix remains invariant within an episode to represent the unobservable regularities of the specific market task. Trading Mechanism and State Transitions. The action at consists of set of buy/sell operations. Specifically, the agents action at specifies the buy/sell and quantity for each stock at every step. The transition function then updates the agents portfolio, which consists of available cash and vector of stock holdings. The reward rt is determined by the sum of cash and the current market value of all held stocks. The agent must inductively reason about the hidden 12 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions matrix by observing how price changes correlate with market factors over time, enabling them to execute strategic, long-horizon trading decisions. A.2.2. TEMPORAL TRAJECTORY To ensure reproducible and fair comparisons across different agents, the environments stochastic elements are predetermined within the task metadata. The complete timeline of the changes of and ϵ is generated at the beginning of each task and stored in the configuration. This approach ensures that while the market factors fluctuate, the environments evolution remains deterministic for any given task instance. Consequently, the optimal trading strategy is theoretically computable with complete knowledge of the dependency matrix and the pre-generated factor timeline, providing consistent benchmark for evaluating agent performance. A.2.3. RESOLVABILITY AND DIVERSITY. Guaranteed Resolvability There is no failure in this environment, and every trajectory finishes each task with different profit. So all the tasks are resolvable. Task Diversity. We generate diverse tasks by modulating the following parameters: Dimensionality Scalability (d, K): By scaling the number of stocks and market factors K, we control the complexity of the inference problem. larger expands the action space, while higher increases the difficulty of disentangling the latent factors from the observations. Dependency Sparsity: The density of non-zero entries in determines the complexity of the structural relationships. Sparse matrices require the agent to identify few factor-stock couplings, whereas dense matrices test the agents ability to model global market correlations. Signal-to-Noise Ratio: By adjusting the variance of ϵ, we modulate the information scarcity. This forces agents to distinguish between persistent structural signals defined by and stochastic fluctuations. A.3. Energy Dispatch Curation This task simulates an energy grid dispatch scenario where agents must allocate power generation resources to satisfy electricity demand while maintaining grid stability, budget constraints, and carbon emission targets. Given 4 generation sources (thermal, wind, solar, and battery) with respective capacities, the agent specifies daily rated power allocations at RK over horizon H. The challenge lies in adapting to time-varying renewable efficiency while balancing multiple competing objectives. A.3.1. STRUCTURAL CONFIGURATION To evaluate the agents capacity for world-structure induction, we model the discrepancy between planned and realized power through latent efficiency vector Et and real generated energy Preal at Et. Multi-Objective Constraints and State Transitions. Each day, the environment evaluates the following constraint satisfaction criteria: Demand: Total realized supply (cid:80) Preal,t must meet demand Dt. Budget: Total cost must not exceed budget Bt. Carbon: The ratio of cumulative generated thermal energy τ =1(Pthermal,τ ,Pwind,τ ,Psolar,τ ) must remain below target τc. τ =1 Pthermal,τ (cid:80)H (cid:80)H Stability: Grid stability penalizes large allocation changes between consecutive days. Additionally, violation of either demand or budget could significantly influence the grid stability. Consecutive violations (3 days in ODYSSEYARENA-LITE) of demand or budget constraints trigger early termination, simulating an irreversible grid collapse. A.3.2. TEMPORAL TRAJECTORY For renewable sources wind and solar, Et is governed by five-level hierarchical generative process that simulates multi-scale temporal dependencies: Ewind/solar,t = Clip (cid:0)Ebase(t mod ) + δt/T + ϵt (cid:1) (5) where [15, 25] denotes the hidden period length randomly sampled. The generative logic is structured as follows: Base Pattern (Ebase): It is constructed as piecewise-linear sequence where each segment (25 days) is assigned random efficiency baseline to simulate the consistency of weather. To introduce intra-period complexity, we add stochastic spikes (with probability of 5%) representing extreme weather. Cyclic Variation (δt/T ): To prevent the agent from relying on the memorization of fixed efficiency curve, each full cycle incorporates unique random offset δ. This mimics the changes between different months or seasons, requiring the agent to continuously recalibrate its internal model. 13 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Micro-Fluctuations (ϵt): high-frequency Gaussian noise ϵt (0, 0.012) is added to the daily output, simulating real-world stochastic. Value Clipping: Finally, the efficiency is clipped to domain-specific ranges ([0.6, 1.05] for wind and [0.65, 1.1] for solar) to ensure physical realism and prevent extreme outliers. Additionally, the efficiency of thermal is near to 1 with minimum fluctuation and the battery efficiency is constant 1 without any fluctuation. positive action abattery,t > 0 denotes discharging stored energy to the grid, while abattery,t < 0 denotes charging from excess generation. The efficiency curves are unobservable to the agent. Consequently, the agent must inductively reason for the stable periodic signal from transient fluctuations by comparing its historical rated actions at with the real power outputs Preal,t. To ensure reproducibility and fair comparison, all timevarying factors (Dt, Bt, Et) are pre-determined as fixed sequences within the task metadata. This ensures that the environments evolution is fully deterministic given the agents sequence of actions, allowing for identical experimental trials across different models. A.3.3. RESOLVABILITY AND DIVERSITY Guaranteed Resolvability. Each instance is guaranteed to be feasible through careful parameter design. Total capacity exceeds peak demand, and budgets are set as Bt = 4.2 Dt to provide adequate financial space. Efficiency values and objective thresholds are tuned to ensure that foresighted dispatch policy can successfully complete the horizon without triggering early termination, while leaving substantial space for diverse strategies. Task Diversity. Our generation framework provides finegrained control over task difficulty through several orthogonal parameters: Temporal Dynamics: Varying period lengths for wind and solar creates complex interference patterns that the agent must disentangle. Constraint Tightness: Adjusting targets τc and τs modulates the precision required to balance competing carbon and stability goals. A.4. Repo System Curation This task instantiates the relational graph structures primitive by simulating software repo dependency resolution scenario. The agent must discover valid configuration of packages that satisfies all latent constraints in dependency graph = (V, E). A.4.1. STRUCTURAL CONFIGURATION Dependency Graph Construction. The environment is defined by latent graph = (V, E), where each node represents specific version of software package. Edges represent directed compatibility constraints; for instance, an edge (vi, vj) may indicate that version vi of Package requires version vj of Package B. To ensure structured challenge, we generate random topological ordering over packages to prevent circular dependencies, while allowing version-level constraints to create complex requirements. To simulate ubiquitous dependencies on foundational libraries (e.g., NumPy, PyTorch), we designate 1-2 packages as base libraries at the root of the topological order. proportion of other packages depend on these base libraries with varying version constraints. There are two types of constraints: (1) the base library and the dependent library should be the same version. For example, the version of base library is 1.1, then the version of dependent package should be 1.1 as well. (2) the base library and the dependent library should be the same main version. For example, the version of base library is 1.1, then the version of dependent package should be 1.X. Transition Logic and Side Effects. The state represents the set of currently installed package versions. When an agent issues an installation action at, the transition function st+1 = (st, at) simulates rigorous resolution process. Unlike simple state updates, may induce side effects, such as automatically upgrading, downgrading, or uninstalling conflicting packages to satisfy the constraints in G. We implement four types of resolution behaviors: (1) Ensure: automatically install missing dependencies; (2) Force-high/low: coercing dependencies to extreme compatible versions; and (3) Pin: locking package to specific version. These behaviors introduce non-monotonic dynamics, where the sequence of actions a1, a2 may result in different final state than a2, a1, forcing the agent to reason about the order of interventions. A.4.2. RESOLVABILITY AND DIVERSITY Guaranteed Resolvability. Every instance is verified to have at least one valid goal state through solution-first generation strategy. We first sample ground-truth configuration and then construct the edges such that they provably contain this solution. All dependencies and constraints are then generated to provably include the ground-truth packages version. A.5. Action Space for ODYSSEYARENA For the four environments in ODYSSEYARENA: Turn On Lights, AI Trading, Energy Dispatch, and Repo System, we 14 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 3. Action Space of and description for ODYSSEYARENA. All parameters are wrapped in <>. Environment Action Description"
        },
        {
            "title": "Turn On Lights",
            "content": "Toggle <light>"
        },
        {
            "title": "Energy Dispatch",
            "content": "{ } { } \"Buy\":<stocks, shares>, \"Sell\":<stocks, shares> \"Thermal\":<p1>, \"Wind\":<p2>, \"Solar\":<p3>, \"Battery\":<p4> repo tree/ls pip install <package> Repo System The parameter light is the specific index of the light you want to toggle. This action is successfully executed only if its latent logical condition is satisfied. Purchase specific shares of stock and sell specific shares of stock. Multiple stocks can be acquired or sold simultaneously. The buy action is successful only if the cash is enough. If the selling shares exceed the current holdings, the environment defaults to selling the entire existing position. Dispatch p1, p2, p3 units of thermal, wind, and solar power respectively. For p4, negative value represents charging and positive represents discharging. Notably, charging is truncated upon reaching the batterys max capacity, and the discharging is limited to the available capacity if it exceeds the current state of charge. The cost of the four energies should be limited within the total budget. Inspect the repositorys directory structure to identify script paths. Install the package. Supports specific version (==) and range constraints (>, >=, <, <=). pip uninstall <package> Remove the specified package from the configuration. pip list List all installed packages with version identifiers. python <script> Execute specified script to verify configuration or trigger entry points. list detailed action space and their description in Table 3. B.2. Reasoning Boosts Inductive Reasoning Notably, the agent can only execute one action in Turn On Lights and Repo System environments. For AI Trading environments, the agent should first sell and then buy stocks within one step. For Energy Dispatch environment, the agent can plan for thermal, wind, solar, and battery together. However, charging and discharging cannot be executed simultaneously in one step. B. Details of Analysis B.1. Detailed Comparison Between w/ and w/o Rules To additionally demonstrate the performance between w/ and w/o rules for SOTA models, we provide detailed results in Table 4. From the extended results, we argue the same conclusion that SOTA models are strong deductive reasoners rather than good inductive reasoners. In Table 2, we further compare gpt-oss-120b across varying reasoning budgets (low, medium, and high). The results reveal that LLMs demonstrate better inductive performance with more reasoning budget. For example, in Turn On Lights, gpt-oss-120b (high) achieves average success rate of 27.50% , while gpt-oss-120b (medium) achieves 16.67% and gpt-oss-120b (low) achieves 7.50%. B.3. More Results and Implementation Details of Evolution Progress More Results of Evolution Progress. We provide detailed results for 15 models and random baseline in Figure 8. We can observe that: (1) In Turn On Lights and Repo System environments, most models demonstrate saturation as the interaction step increases, indicating that long-horizon remains bottleneck for SOTA LLM models. (2) Some models, e.g. GLM-4-9B-Chat, perform equivalent or even worse than random baseline, indicating their inability of discovering hidden rules of the environment. (3) Models 15 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 4. Performance comparison between w/o rules and w/ rules settings. We provide three different reasoning effort of gpt-oss-120b. For AI Trading environment, we report the profit rate and pass@4 is calculated based on the highest profit of each task. For other three environments, we report the success rate. Colored Rows represent proprietary models. Model Turn On Lights AI Trading Energy Dispatch Repo Management w/o rules w/ rules w/o rules w/ rules w/o rules w/ rules w/o rules w/ rules Gemini 3 Pro Preview GPT-5 Gemini 2.5 Pro gpt-oss-120b (high) DeepSeek-V3.2 Grok 4 Fast Qwen3-235B-A22B-Instruct Qwen3-30B-A3B-Instruct gpt-oss-120b (medium) GLM-4-32B-0414 gpt-oss-120b (low) Llama 3.3 70B Instruct Qwen3-4B-Instruct Llama 3.1 8B Instruct GLM-4-9B-Chat 44.17 28.33 29.17 27.50 18.33 14.17 15.00 11.67 16.67 14.17 7.50 6.67 0.00 6.67 0.00 100.00 100.00 100.00 100.00 96.67 100.00 90.00 50.00 100.00 60.00 36.67 33.33 10.00 10.00 0.00 +67.71% +135.48% +17.32% +132.02% +33.02% +126.24% +23.27% +141.63% +8.62% +118.88% +5.70% +62.96% +11.26% +123.66% +4.76% +94.00% +3.21% +100.27% +18.50% +3.14% +26.40% +2.02% -0.93% +0.77% +60.52% +1.67% +0.18% +0.55% +0.34% -0.18% 30.00 23.33 10.83 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 16.67 13.33 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 65.83 62.50 50.00 18.33 48.33 38.33 15.83 26.67 2.50 9.17 9.17 19.17 13.33 0.00 0.00 96.67 96.67 96.67 50.00 56.67 36.67 33.33 43.33 30.00 6.67 26.67 50.00 16.67 3.33 0.00 demonstrate different performance curves, suggesting that we should evaluate each model from multiple timestamps. there is significant gap between human and SOTA LLMs. Notably, most LLMs fail all tasks in Energy Dispatch, indicating an inability of long-horizon inductive reasoning. Implementation Details. For the Turn On Lights and Repo Management environments, we plot evolution curves with the interaction step index on the x-axis and the cumulative task success rate on the y-axis, defined as the proportion of successfully completed tasks from the beginning to the current step. This formulation captures the progression of task completion over time and reflects the agents inductive reasoning capability as the interaction horizon increases. At any given step, higher y-axis value indicates stronger inductive reasoning performance in the current step. For the AI Trading environment, the y-axis represents the profit rate relative to the initial capital, which is then averaged across all tasks in this environment. We do not report evolution curves for the Energy Dispatch environment, as task success is determined by complex, multi-constraint conditions that do not admit well-defined step-wise success metric. Notably, as shown in the results above, model performance varies across different interaction steps, indicating that evaluation at single step is insufficient. Therefore, it is necessary to assess model performance across multiple steps to obtain more comprehensive and reliable understanding of their inductive reasoning capabilities. B.4. More Results and Implementation Details of Task Success Status More Results of Task Success Status. We provide detailed results of task success status for 14 models in Energy Dispatch and Repo System in Figure 10. We can observe that Implementation Details of Task Success Status. The difficulty of Turn On Lights task is decided by multiple conditions and we simplify it using the number of lights in Figure 5. For Repo System, we also simplify the difficulty representation using the number of required packages. Notably, the we do not simplify or provide the difficulty level of Energy Dispatch due to its complex multi-object goal. B.5. Loop Implementation Details Given trajectory dataset, global measure of interaction volume is obtained by aggregating the total number of executed actions across all trajectories. For Turn On Lights and Repo System environments, we additionally analyses the Loop Ratio for each model. To characterize the inability of inductive reasoning, we focus on repetitive execution of previously taken action that yields no effective task progress. Concretely, we define the state as the on/off of each light for Turn On Lights environment and the version of each package for Repo System environment. Within each trajectory, we then detect whether an agent replays the same pair (state, action) immediately after completing it and the two interactions make no task progress. We then sum the action counts of all such immediately repeated pairs and normalize this quantity by the total number of actions. This normalized proportion defines the Loop Ratio, which measures how much of the agents interaction is spent on consecutive, unproductive repetition. 16 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 8. All data for Success Rate against Step. We do not plot for Energy Dispatch environment due to its complex success conditions. Under this formulation, smaller Loop Ratio indicates that the agent is more capable to find hidden rules through inductive reasoning. B.6. Step Distribution Analysis We plot the distribution of the total steps number required to complete the task for each trajectory, including both successful and failed trajectories. In the Turn On Lights and Repo System environments, step usage serves as proxy for inductive reasoning efficiency, where fewer steps indicate stronger inductive reasoning capability. In the Energy Dispatch environment, step usage corresponds to the number of days during which energy is supplied without violating the three-day consistency constraint. higher step usage represents better inductive reasoning capability. Notably, every trajectory in AI Trading environment has 120 steps in ODYSSEYARENA-LITE, so we do not plot step distribution figure for this environment. The results are in Figure 9. We can observe sharp concentration around max step limit (200 steps for Turn On Lights and 120 steps for AI Trading), indicating that most SOTA models demonstrate limited inductive reasoning capability in this environment and can not solve the task within pre-defined steps limit. Additionally, in Energy Dispatch environment, most models exhibit step distributions that are sharply concentrated at relatively small values, indicating an inability to satisfy the demand and budget constraints for three consecutive days. Conversely, for Gemini 3 Pro Preview, Gemini 2.5 Pro, and GPT-5, the step distribution is sharply concentrated around 120 steps, suggesting that these models can satisfy the demand and budget constraints over the evaluated horizon. Notably, task success is additionally governed by the carbon and stability constraints and sustaining 120 days does not necessarily imply success. B.7. Token Efficiency Analysis We analyze token efficiency, defined as the contribution of each token to the final success rate (or trading profit), and calculate it as the success rate (or trading profit) divided by the total token usage. We provide detailed analysis of GLM series, Llama series and Qwen series in Figure 11. Results in Figure 11 demonstrate that GLM-4-32B-0414 is the most token-efficient in Turn On Lights and AI Trading environment and Llama 3.3 70B Instruct is the most tokenefficient in Repo System environment. Interestingly, model GLM-4-9B-Chat generates negative profit in AI Trading environment, so its token efficiency is negative. Moreover, models from the three series fail all the tasks in Energy Dispatch environment. 17 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 9. Step density distribution for LLM models: (a) Gemini 3 Pro Preview, (b) GPT-5, (c) Gemini 2.5 Pro, (d) gpt-oss-120b (high), (e) DeepSeek-V3.2, (f) Grok 4 Fast, (g) Qwen3-235B-A22B-Instruct, (h) gpt-oss-120b (medium), (i) Qwen3-30B-A3B-Instruct, (j) GLM-432B-0414, (k) gpt-oss-120b (low), (l) Llama 3.3 70B Instruct, (m) Qwen3-4B-Instruct (n) Llama 3.1 8B Instruct, (o) GLM-4-9B-Chat. Figure 10. Task success status (based on pass@4). Each row represents: (a) Human, (b) Gemini3 Pro Preview, (c) GPT-5, (d) Gemini 2.5 Pro, (e) gpt-oss-120b (high), (f) DeepSeek-V3.2, (g) Grok 4 Fast, (h) Qwen3-235B-A22B-Instruct, (i) gpt-oss-120b (medium), (j) Qwen3-30B-A3B-Instruct, (k) GLM-4-32B-0414, (l) gpt-oss-120b (low), (m) Llama 3.3 70B Instruct, (n) Qwen3-4B-Instruct, (o) Llama 3.1 8B Instruct, (p) GLM-4-9B-Chat. Dark green cells indicate tasks solved by Human. Green cells indicate tasks solved by LLM agents. Gray cells indicate unsolved tasks. We report the average success rate in Repo System across all LLMs for each subset (Easy, Medium and Hard). Notably, although Qwen achieves best success rate (or trading profit) among the three series, their consistent high token usage makes their token efficiency extremely low. B.8. Error Case Analysis As shown in Figure 12, we analyze the failed trajectories from Gemini 3 Pro Preview in all the four environments. We divide all errors into four kinds of inability of inductive reasoning: Behavior Stagnation, Error Credit Assignment, Long-Horizon Dependence Decay, Local Optima. Behavior Stagnation refers to the agent repeat the same error action despite the explicit negative environment feedback. For example, the agent continuously toggle the same light in Turn On Lights without any state change. Error Credit Assignment refers to the inability to correlate error signals with specific hidden constraints in the transition function. For example, in the Energy Dispatch environment, the agent fails to disentangle the latent periodic signal from noise, misinterpreting stochastic fluctuations as structural regularities and ultimately resulting in an inaccurate induction of the efficiency cycles period. Long-Horizon Dependence Decay refers to breakdown in maintaining and utilizing the global state representation over extended interaction sequences. For example, although the period of efficiency in Energy Dispatch is constant, the agent fails to recognize it and fail to adaptively adjust its actions based on the period. Local Optima refer to the focus on immediate state changes that ignores the broader relational or periodic regularities of 18 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 11. The line chart represents token usage and the bar chart represents token efficiency. Token Usage is measured in units of 106, while Token Efficiency is reported in units of 106. through only python scripts. Optimal Strategy. We introduce an optimal strategy that operates under the assumption of perfect information regarding future market dynamics. Specifically, with full access to the price trajectory of all stocks, this method adopts greedy strategy: at each time step, it allocates the entire portfolio to the stock with the highest single-day return ratio. In scenarios where all stocks exhibit downward trend, keep all assets as cash. Conservative Strategy. The fundamental principle of this strategy is to delay trading until sufficient historical data is collected. Specifically, wait and do nothing for at least num factors + 2 days. This threshold ensures that the linear system of equations is overdetermined or has unique solution before any parameter estimation occurs, thereby avoiding instability in the early stages. We first estimate the dependency matrix , which maps factor changes to price changes. We then constructs the price and factor matrices to solve for using linear algebra solvers and buy/sell stocks based on the hidden rules we find. Figure 12. Error types of Gemini 3 Pro Preview. (a) Behavior Stagnation, (b) Error Credit Assignment, (c) Long-Horizon Dependence Decay, (d) Local Optima. the system. For example, in Repo System, the agent attempts to resolve local compatibility between specific package pair while failing to account for other package constraints of the entire dependency graph. B.9. Different Strategies for AI Trading In this subsection, we provide more methods for AI Trading environment and analyze their profit. Notably, the strategies in this subsection do not require LLMs and is implemented Progressive Strategy. This strategy adopts an aggressive mechanism, starting trading from the third day rather wait for more information. 19 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 5. Comparison of different strategies, human and models. For LLM models, we provide the best results from both proprietary models and open-source models. represents the delta of average profit ratio against optimal strategy. Colored Rows represent proprietary models. Table 6. Performance of three LLMs in Repo System of ODYSSEYARENA-CHALLENGE and ODYSSEYARENA-LITE. Colored Row represents proprietary models. We additionally provide the performance gap. Method Avg. Profit Ratio Optimal Strategy Conservative Strategy Progressive Strategy Correlation Strategy Rolling Window Strategy Ridge Regression Strategy Human Annotation Gemini 3 Pro Preview Qwen3-235B-A22B-Instruct +211.13% +192.23% +197.33% +181.51% +197.31% +192.63% 92.55% +67.71% +11.26% -18.90% -13.80% -29.62% -13.82% -18.50% -118.58% -143.42% -199.87% We employ an incremental learning framework based on the least square method. At any given time step, we utilize the entire history of available price and factor changes accumulated up to that point to estimate the dependency matrix. Unlike approaches that rely on fixed data windows or static thresholds, this strategy dynamically updates its understanding of the market. As new data become available each day, the linear regression model is re-calculated using the full dataset, allowing the estimated parameters to evolve continuously. This approach fundamentally represents strategic trade-off between estimation precision and market timing. In the initial stages, the model operates with higher risk because the dataset is sparse, which may lead to significant estimation errors and suboptimal trading decisions. The estimator naturally converges towards the true parameters as more information is gathered. Correlation Strategy. While progressive strategy attempt to solve for the entire dependency matrix simultaneously by considering the joint influence of all factors, the correlation strategy operates on the assumption that these factors are statistically independent. For every individual pair of stocks and factors, we utilize the full historical dataset available to compute this simple regression coefficient. By iterating through all combinations, the model updates the dependency matrix element by element in each step. Model Lete Challenge Gemini3 Pro Preview Qwen3-235B-A22B-Instruct Qwen3-30B-A3B-Instruct 65.83 15.83 26.67 10.00 0.00 0.00 -55.83 -15.83 -26. matrix using only data pairs from the most recent days through the least squares method. Old data points that fall outside this window are systematically ignored in the calculation. Ridge Regression Strategy. This strategy modifies the core learning logic of the progressive strategy by introducing regularization mechanism. Instead of simply minimizing the prediction error through the least squares method, we apply strict penalty to the magnitude of the regression coefficients. By artificially shrinking the coefficients, the algorithm effectively suppresses the noise that arises when factors are too similar, preventing any single factor from exerting an unrealistically large influence on the trading decision. Comparison of Strategies We provide results of the above mentioned strategies, human annotation and two SOTA models in Table 5. We can observe that various strategies achieve different performance. Notably, SOTA models demonstrate significant performance gap against both human annotation and the strategies proposed in this subsection, indicating huge space for inductive reasoning ability optimization. B.10. ODYSSEYARENA-CHALLENGE Results For our proposed extremely long-horizon and complex dataset ODYSSEYARENA-CHALLENGE, we test Repo System and report the results in Table 6, indicating that the longhorizon scenario is still the bottleneck of current LLMs inductive reasoning. C. Main Results Details C.1. Baseline Settings Rolling Window Strategy. The Rolling Window strategy posits that recent market behaviors are more predictive of immediate future trends. Consequently, the strategy limits its estimate scope to fixed-size trailing window (specifically set to 15 days), ensuring that the model remains sensitive to structural shifts in the market environment. On any given trading day t, we construct the dependency For Qwen3-4B-Instruct and Qwen3-235B-A22B-Instruct, we use the version Qwen3-4B-Instruct-2507 and Qwen3235B-A22B-Instruct-2507. For Llama3.1-8B-Instruct and Llama3.3-70B-Instruct, we use the checkpoint updated in February, 2025. For GLM-4-9B-Chat, we use the checkpoint updated in January, 2025. For DeepSeek-V3.2, we use the non-thinking version. The reasoning effort of GPT-5 is ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions medium. We additionally report different reasoning effort of gpt-oss-120b: low, medium, high. Experiments of open-source models are conducted on NVIDIA H200 GPUs using VLLM engine (Kwon et al., 2023). For all experiments, we set the temperature to 0.6. For generation, we use </action> and </finish> as stop tokens. We then parse the generated action within each <action></action> <finish></finish> pair. For AI Trading environment, we keep the 50 most recent interactions as the memory. For Energy Dispatch environment, we keep the 40 most recent interactions as the memory. For other two environments, we keep all interactions as memory. C.2. Evaluation Prompts Each environment provides the model with three components: system prompt, history records, and current state observation. History: Interaction history formatted as state + + action, containing daily status metAction: rics, previous generation results, supply/demand balance, and financial status. Current State: Day number, status indicators (stability, carbon, battery), previous step results, current demand, and the next days budget. Repo System Environment The Repo environment provides the following state information: System Prompt: Detailed environment description, debugging strategies, error interpretation guidelines, action format (command strings). History: Interaction history in structured format: Feedback:{result}nn=== Step {n} ===n>>> Command: separated by double newlines. {command}, Turn On Lights Environment The Lights environment provides the following information: Current State: Execution result of the last command (success message or error details). System Prompt: Concise goal description (light all bulbs), rule explanation, action format (integer index). History: back: {feedback}, State: {obs}. Interaction history with explicit feedAction: {action}, Feedback: D. Human Annotation Details In this section, we provide detailed information regarding the human annotation process conducted to evaluate ODYSSEYARENA-LITE, in accordance with responsible NLP research practices. Current State: Visual representation using symbols. D.1. Instructions Given to Participants AI Trading Environment The Trade environment provides the following state information: System Prompt: Trading objectives, market dynamics explanation, action format (JSON buy/sell combinations). History: Interaction history formatted as market info + Action: + action, containing stock prices, holdings, cash, total value, and news hints. Current State: Current day, stock prices and holdings, cash, total value, and next days news (predictive price change hints). Energy Dispatch Environment The Energy environment provides the following state information: We provide annotators with the full text of instructions and comprehensive guide prior to the task. To facilitate clear understanding of the evaluation criteria, we designed an intuitive annotation interface. screenshot of this interface is presented in Figure 13. Moreover, we provide the user instructions for all environments. We detail the System Prompts of the four above mentioned environments in the following part. D.2. Recruitment and Compensation This approach ensures high level of expertise, as all recruited annotators are students with background in Artificial Intelligence and possess the necessary domain knowledge to accurately assess model performance against the specific criteria defined for ODYSSEYARENA. Annotators are compensated at rate of about $ 15 per hour. D.3. Consent System Prompt: Environment description (four generation types), constraints (demand, budget, stability, carbon), dynamic target thresholds, action format. Informed consent was obtained from all annotators, with the mutual understanding that the collected data would be utilized for research purposes and released as public dataset. ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Table 7. Inter-rater reliability analysis across four experimental environments. For tasks with discrete success metrics (Turn On Lights, Energy Dispatch, and Repo System), we report Fleiss Kappa (κ). For the continuous reasoning task (AI Trading), the Intraclass Correlation Coefficient (ICC) is employed. Environment Metric Turn On Lights Energy Dispatch AI Trading Repo System Fleiss κ Fleiss κ Intraclass Correlation Coefficient Fleiss κ Score 0.42 0.40 0.12 0.18 D.4. Annotation Process To mitigate individual bias, each example in our dataset was annotated by four independent annotators. The annotation workflow consisted of two stages: 1. One-shot Demonstration: Participants were first presented with simplified examples for each environment. These examples served as tutorials and did not overlap with the test dataset. 2. Main Annotation: Annotators labeled the generated outputs strictly adhering to the specific rules defined within each environment. D.5. Inter-Annotator Agreement To ensure the reliability of the human annotated data, we conducted inner agreement analysis across four experimental environments. For tasks with discrete outcomes (Turn On Lights, Energy Dispatch, and Repo System), where performance can be explicitly judged as binary success metric, Fleiss Kappa was employed to account for multiple annotators. The resulting coefficients for these environments indicate moderate level of consensus consistent. Furthermore, for the AI Trading environment, which yields continuous performance metrics (profit rate), we use the Intraclass Correlation Coefficient (ICC). And the ICC score quantitatively confirm that annotators employed significantly divergent inductive reasoning strategies in this environment. 22 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions"
        },
        {
            "title": "Turn On Lights Environment System Prompt",
            "content": "You are an intelligent agent. ### Goal: Your mission is to light on all the bulbs. However, the accessibility of the bulbs is based on the current condition of other bulbs. You need to learn the hidden rule behind the environment and complete the task. ### Action Space: The action space is based on the index of bulbs. For example, you would like to light on / off the first bulb, you should output <action>0</action> to toggle the state of the bulb. ### History Action and Feedback: {history trajectories} ### Current State: {the state of each light} Now think step by step and choose the next action to act in the environment. You are encouraged to act actively to derive the environment dynamics. Output ONLY one action in the format: <action>n</action> AI Trading Environment System Prompt ### Goal: Your mission is to maximize your total portfolio value by buying and selling stocks. The market prices are influenced by underlying variables F, and each days news provides hints about future price changes. You need to learn the hidden dynamics of the simulated market and make decisions accordingly. Please note that the underlying meaning of variables may differ from the real stock. ### Action Space: You can take actions in the form of buying or selling multiple stocks each day. You can combine buy and sell in one action. The environment will first execute all sell actions, then all buy actions. You cannot spend more cash than you have or sell stocks you dont own. **Action Format Examples:** - To buy 10 shares of S0 and 20 shares of S2, and sell 10 shares of S1: <action>{{buy: {{S0: 10, S2: 20}}, sell: {{S1: 10}}}}</action> - To only buy: <action>{{buy: {{S0: 5}}, sell: {{}}}}</action> - To do nothing: <action>{{buy: {{}}, sell: {{}}}}</action> **Important:** - Stock symbols and numbers should NOT have quotes - Use valid JSON format inside <action></action> tags - If you cannot afford purchase or dont own enough shares to sell, that part of the action will be ignored ### History Actions and Feedback: {history trajectories} ### Current State: {cash and the price of each stock} Think carefully step by step and decide your next action. You are encouraged to act proactively, using the news to predict future price changes, and to improve your strategy over time. Provide your action in the format: <action>...</action> ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Energy Dispatch Environment System Prompt (Part I) You are an intelligent energy system operator managing Dynamic Energy Grid. Your goal is to achieve safe, stable, and low-carbon electricity supply across long planning horizon. Each day, you adjust the composition of generation resources within strict physical and economic limits. To perform well, you must learn and exploit hidden temporal patterns from the history. # ENVIRONMENT OVERVIEW This environment simulates long-horizon national power grid with four generation types: Thermal highly reliable, carbon-intensive, lowest cost. Wind highly variable, driven by seasonal cycles. Solar variable, driven by seasonal cycles. Battery (Storage) storage buffer that can charge or discharge based on the capacity. Its carbon footprint is determined by the source of energy used for charging. Each day t, the system evolves according to underlying temporal dynamics. The agent must design the next days rated generation scheme while anticipating these dynamics. ## Demand & Budget The allocation scheme must strictly satisfy both demand and budget constraints. current demand (MW) electricity required today. current budget tomorrows maximum allowable total generation cost. ## Generation Cost Model (Unit Prices) Each generation type has fixed unit cost per MW of rated output: Thermal: cheapest (e.g., 3.0 cost/unit) Wind: moderate cost (e.g., 5.0 cost/unit) Solar: more expensive (e.g., 6.0 cost/unit) Battery: operational cost (Charge/Discharge), very low (e.g., 0.1 cost/unit) ## Grid Stability To maintain stable grid, the agent must avoid large day-to-day changes in the rated outputs. Sudden increases or decreases (ramping) reduce stability, which affects overall performance. good strategy adjusts gradually, anticipating future needs rather than reacting abruptly. Violating the daily budget or failing to meet the demand would largely damage system stability. ## Carbon Intensity Thermal generation emits carbon. To maintain clean and sustainable city, the agent should limit the proportion of thermal power while still meeting demand and respecting budget constraints. This creates non-trivial trade-off between cost, stability, and carbon impact. ## Season & Efficiency Actual generation is not equal to rated generation. It depends on time-varying efficiency term: actual output = rated output efficiency (t) Efficiency changes periodically over time. Solar and Wind share different periods. Agent is required to derive the hidden temporal rules from the history observation. Because actual output fluctuates around rated output, the agent must leave safety margins and learn the temporal structure. # Objective The agent needs to simulate across long planning horizon (120 Turns). The task is successful only if the final metric **Stability > {target stability}, Carbon < {target carbon} **. Notably, violation of daily cost or demand constraints for 3 consecutive steps would lead to termination. 24 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Energy Dispatch Environment System Prompt (Part II) # Action Space Each day, the agent must decide the rated generation for the next day within the capacity limit: Thermal (MW), Rated Power Command, [0,600], Must be non-negative. Wind (MW), Rated Power Command, [0,350], must be non-negative. Solar (MW), Rated Power Command, [0,250], must be non-negative. Battery (MW), Net Flow Command, battery capacity=80, Bidirectional: Negative = Charge (Consumption), Positive = Discharge (Supply). **Action Format Example 1**: <action>{{thermal: 400.0, wind: 10.0, solar: 35.0, battery: -15.0}}</action> Interpretation: The agent sets the Rated Power for Thermal, Wind, and Solar to 400 MW, 10 MW, and 35 MW, respectively. Additionally, the agent commands the battery to consume 15 MW from the grid for charging. This 15 MW consumption will be drawn from the total supply available from the three generation units. **Action Format Example 2**: <action>{{thermal: 350.0, wind: 25.0, solar: 15.0, battery: 10.0}}</action> Interpretation: The agent sets the Rated Power for Thermal, Wind, and Solar to 350 MW, 25 MW, and 15 MW, respectively. Additionally, the agent commands the battery to supply 10 MW of power to the grid (discharging). This 10 MW is added to the total supply from the three generation units. # History Action and Feedback: {history trajectories} # Current State: {last day info and today info} **Important Note:** - Set Rated Capacity above Actual Demand to save room for the efficiency gap (Rated vs. Actual output) and forecast uncertainty. - Keep daily cost within the budget and meet the daily demand, violation of either cost and supply for three consecutive steps would lead to immediate, irreversible grid collapse. - Stability and Carbon are long-term average metric. After 120-turn, stability must be > {target stability}, Carbon must be < {target carbon}. Now think step by step and choose the next action to act in the environment. You are encouraged to act actively to derive the environment dynamics. Output ONLY one action within the tag of <action></action>. Repo Management Environment System Prompt (Part I) You are an intelligent computer-using agent. # Environment Overview You are interacting with simulated Python project setup environment. This environment mimics real-world difficulties of configuring repo: - Partial information (no full dependency graph) - Object-level runtime failures (module/symbol/kwarg), not explicit version instructions - Non-monotonic side-effects: installing one package may upgrade/downgrade other packages - Hidden rules that may only trigger in specific sub-modules or late-stage scripts 25 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Repo System Environment System Prompt (Part II) # Repo Hierarchy & Debugging The repo is hierarchical: it contains multiple runnable scripts under subdirectories. You can debug incrementally by running sub-scripts (to locate which subsystem fails), but the final goal is to make the entire project pass. Use: - repo tree (or repo ls) to list available scripts in the repo. - python <script path> to run specific sub-script and fix it step by step. - python run.py to run the whole project (a sequence of entry points). This is the only command that ends the episode with success. # Goal Your ultimate goal is to make: python run.py execute successfully. # Action Space (ONE command per step) - Install Python: - pip install python==3.10 - Install packages: - pip install pkgX - pip install pkgX==1.2 (note: if you output x.y.z, it will be interpreted as x.y) - pip install pkgX>=1.1,<2.0 - Uninstall packages: - pip uninstall pkgX - Inspect environment: - pip list - Inspect repo structure: - repo tree / repo ls - Execute scripts: - python run.py - python core/smoke.py (example; use repo tree to discover actual paths) Other commands (e.g., upgrade) are not supported. # How to Interpret Errors (Important) Errors are meant as clues without directly stating version ranges: - ModuleNotFoundError: No module named pkgX usually means pkgX is missing. - ImportError: cannot import name from pkgX.mod often means pkgX version does not export that symbol. - TypeError: ... got an unexpected keyword argument kw indicates signature/API mismatch. If the message says during project entry, adjust the provider package used by the project. If it says while importing caller pkg, it indicates caller->provider incompatibility. Because installations can trigger side effects, later fix may break an earlier sub-script. Use sub-scripts to localize failures, but always re-run python run.py to confirm global consistency. # History Action and Feedback: {history trajectories} # Current Environment Feedback: {specific feed back of command} Now think step by step and choose the next action. Output exactly ONE action inside <action></action>, e.g. <action>pip install pkg0==2.1</action>. 26 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Figure 13. Screenshot of the user interface and instructions provided to the human annotators. 27 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions"
        },
        {
            "title": "Turn On Lights Environment User Instruction",
            "content": "Assume there are 3 bulbs (indices 0, 1, 2), all initially off. Example Logic (only shown in examples, these rules are hidden in actual tasks, users need to infer): B0: True # Represents B0 can be turned on under any circumstances B1: B0 # Represents B1 can only be turned on when B0 is on B2: not B1 and B0 # Represents B2 can only be turned on when B1 is off and B0 is on Example Steps: 1. Step 1: Input action 1, click Execute Action Environment state after execution: Environment feedback: B1 remains inactive... remaining bulbs should be in specific mode. Reason: B1 can only be turned on when B0 is on, but B0 is off, so B1 cannot be turned on. 2. Step 2: Input action 0, click Execute Action Environment state after execution: Environment feedback: Toggled B1 to True Reason: B0 can be turned on at any time. 3. Step 3: Input action 2, click Execute Action Environment state after execution: Environment feedback: Toggled B2 to True Reason: B2 can only be turned on when B1 is off and B0 is on, so B2 was turned on. 4. Step 4: Input action 1, click Execute Action Environment state after execution: (Task completed) Environment feedback: Toggled B1 to True Reason: B1 can only be turned on when B0 is on, so B1 was turned on. Tips: indicates bulb is lit indicates bulb is not lit Each bulbs availability may depend on other bulbs states You need to discover hidden rules through experimentation Maximum 200 steps allowed Goal: Light all bulbs (all bulbs display as ) 28 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions AI Trading Environment User Instruction (Part I) Scenario Description You are stock trader who needs to perform buy and sell operations across multiple trading days, achieving maximum returns within 120 days. Important Concepts: S0, S1: Stock codes (Stocks), representing 2 different stocks that can be bought and sold F0, F1: Market factors (Factors), representing market factors that affect stock prices News will report changes in these factors (e.g., F0 rose slightly (+0.03)) Factor changes affect stock prices through dependency matrix You need to predict stock price changes based on news, then trade Please check news, e.g., F0 rose slightly (+0.03) F1 decreased significantly (-0.10) predict which stocks will rise/fall based on factor changes Buying is limited by cash Selling is limited by holdings Available Actions: Buy Stock: Input positive number to buy (e.g., S0 input 100 means buy 100 shares of S0) Sell Stock: Input negative number to sell (e.g., S0 input -50 means sell 50 shares of S0) Buying is limited by cash, selling is limited by holdings Example: Example Logic (only shown in examples, these rules are hidden in actual tasks, users need to infer): Matrix corresponding to S0, S1, F0, F1 is: (cid:20) 0.1 0.3 (cid:21) 0.2 0. Represents: F0 rises 1 point, S0 rises 0.1 points; F0 rises 1 point, S1 falls 0.3 points F1 rises 1 point, S0 rises 0.2 points; F1 rises 1 point, S1 rises 0.4 points Initial Environment in This Example: You have 100 cash S0 initial price is 1, S1 initial price is 2 This example is simple demonstration, only keeps 2 days (actual task is 120 days) 29 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions AI Trading Environment User Instruction (Part II) Example Steps: Step 1 (Day 1): Environment state before execution: Tomorrow F0 rose significantly (+0.10) F1 rose slightly (+0.05) Stock prices before execution: S0 1.00, S1 2.00, Cash Action: Buy 100 shares of S0 Reason: S0 tomorrows price = 1.00 + (0.10.10) + (0.20.05) = 1.00 + 0.01 + 0.01 = 1.02 (up 2%), while S1 tomorrows price is S1 = 2.00 + ((-0.3)0.10) + (0.40.05) = 2.00 - 0.03 + 0.02 = 1.99 (down 0.5%). S0 rises while S1 falls, so buy S0. Buying 100 shares of S0 costs 100, cash becomes 0. Step 2 (Day 2): Environment state before execution: Tomorrow F0 decreased significantly (-0.15) F1 rose significantly (+0.10) Stock prices before execution: S0 1.02, S1 1.99, Cash 0, Holdings 100 shares of Sell 100 shares of S0, buy about 51 shares of S1 Reason: S0 tomorrows price = 1.02 + (0.1(-0.15)) + (0.20.10) = 1.02 - 0.015 + 0.02 = 1.025 (slight rise 0.5%), while S1 tomorrows price is S1 = 1.99 + ((-0.3)(-0.15)) + (0.40.10) = 1.99 + 0.045 + 0.04 = 2.075 (up 4.3%). S1 rise is much greater than S0, so sell S0 and buy S1. Selling 100 shares of S0 gets 102, can buy about 51 shares of S1 (102/1.99 = 51.26, rounded to 51 shares, costs about 101.49). Step 3 (Day 3): Environment state before execution: Tomorrow F0 stable (0.00) F1 rose significantly (+0.20) Stock prices before execution: S0 1.025, S1 2.075, Cash 0.51, Holding 51 shares of S1 Action: No operation (or use remaining cash to buy small amount of S1) Reason: S0 tomorrows price = 1.025 + (0.10) + (0.20.20) = 1.025 + 0.04 = 1.065 (up 3.9%), while S1 tomorrows price is S1 = 2.075 + ((-0.3)0) + (0.40.20) = 2.075 + 0.08 = 2.155 (up 3.9%). Both stocks have similar rises, but S1 absolute rise is larger (0.08 vs 0.04), and already holding S1, so maintain position. Final State: 51 shares of S1, price 2.155 per share, total value about 109.91 (512.155), plus remaining cash about 0.51, total value about 110.42, return rate about 10.42% 30 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Energy Dispatch Environment User Instruction (Part I) Scenario Description You need to manage an energy grid, balancing generation, demand, and budget while meeting stability and carbon emission targets, completing at least 120 days of tasks. If demand violations or budget violations occur for three consecutive days, the task will fail immediately. Task Objectives: Completion Days: Complete at least 120 days Stability Target: Final average stability must be target value (shown in status) Carbon Emission Target: Final carbon emission ratio must be target value (shown in status) Violation Limit: 3 consecutive days of demand violations or budget violations will cause task failure Available Actions: Thermal: Input thermal power generation (0) Wind: Input wind power generation (0) Solar: Input solar power generation (0) Battery: Input battery operation Negative value = charging (e.g., -20) Positive value = discharging (e.g., 20) 0 = no battery usage Battery has maximum capacity limit of 80 Actual Generation Calculation: Actual generation = Input generation efficiency coefficient After actual generation, store to battery, this stage has no loss For example, input thermal 10, wind 20, solar 30, battery storage 10. Thermal efficiency 0.9, wind efficiency 1.1, solar efficiency 1 Then actual generation: 10 0.9 + 20 1.1 + 30 1 = 61 Applied to grid (subtract battery storage): 61 10 = 51 Stability Requirements: Daily generation configuration changes cannot be too large, otherwise it will cause grid instability Stability calculation considers: generation configuration change magnitude (ramping), budget violations, demand violations If budget violation or demand violation occurs, stability will decrease significantly Important: Insufficient stability will not directly terminate the task, but will be used to judge task success after completion. So you need to adjust strategy timely to improve stability 31 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Energy Dispatch Environment User Instruction (Part II) Carbon Emission Requirements: Carbon emission ratio = Historical cumulative thermal actual generation / Historical cumulative total actual generation When task is completed, carbon emission ratio must be target value Need to control thermal power proportion of total generation throughout the task Important: Excessive carbon emissions will not directly terminate the task, but will be used to judge task success after completion. So you need to adjust strategy timely to reduce carbon emissions Violation Explanation: Demand Violation: Actual supply < demand Budget Violation: Actual cost > budget Insufficient stability or excessive carbon emissions do not count as violations Three consecutive days of violations will directly terminate and fail the task Important: Only demand violations and budget violations will increase consecutive violation days. Insufficient stability and excessive carbon emissions are not violations but affect final results Example: Scenario Description: Thermal, wind, solar unit prices are 2, 4, 6 per unit respectively, battery operation cost 0.1 yuan/unit Carbon emission ratio target 0.81 (i.e., thermal proportion 0.19) Stability target 0.5 This example demonstrates 6 days, actual task requires completing 120 days Example Logic (only shown in examples, these rules are hidden in actual tasks, users need to infer): Thermal efficiency sequence: [1.0, 1.0, 1.0, 0.9, 1.1, 1.0] (randomly fluctuates around 1) Wind efficiency sequence: [1.1, 1.0, 1.1, 1.0, 1.1, 1.0] (cycle every 2 days) Solar efficiency sequence: [0.9, 1.0, 1.1, 0.9, 1.0, 1.1] (cycle every 3 days) Important Notes: In actual tasks, efficiency coefficients are hidden and need to be inferred from historical data Need to balance cost, stability, carbon emissions, and demand satisfaction Insufficient stability and excessive carbon emissions will not directly terminate the task but will affect final task completion conditions Only demand violations and budget violations will increase consecutive violation days, 3 consecutive days of violations will cause task failure When violations occur, need to adjust strategy timely to avoid consecutive violations In actual problems, you cannot see the specific calculation process of stability coefficient, you only see result, please adjust strategy based on this result 32 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Repo System Environment User Instruction (Part I) Scenario Description You need to configure Python environment and install correct package versions so that the project can run successfully: python run.py Available Commands: pip install python==3.10 - Install Python version pip install pkg0==1.2 - Install package (supports version constraints) pip uninstall pkg0 - Uninstall package pip list - View current environment status repo tree - View repository structure python run.py - Run project (success means task completed) Example Hidden Rules (users need to discover in actual tasks): Requires python>=3. Requires pkg1==1.0 Requires pkg2>=1.2,<=2.0 Requires pkg3<=1.0 All version numbers of pkg3 must match pkg1 (including integer and decimal parts) Major version number of pkg2 must match pkg1 (integer part) Example Steps: Step 1: Input pip install python==3.10, click Execute Action Environment feedback: Successfully installed python==3.10 Reason: Successfully installed Step 2: Input python run.py, click Execute Action Environment feedback: ModuleNotFoundError: Reason: pkg1 not installed No module named pkg1. Step 3: Input pip install pkg1==1.0, click Execute Action Environment feedback: Successfully installed pkg1==1.0 Reason: Successfully installed pkg1==1.0 Step 4: Input python run.py, click Execute Action Environment feedback: ModuleNotFoundError: Reason: pkg2 not installed No module named pkg2. Step 5: Input pip install pkg2==2.0, click Execute Action Environment feedback: Successfully installed pkg2==2.0 Reason: Successfully installed pkg2==2. Step 6: Input python run.py, click Execute Action Environment dependent packages. Reason: Major version number of pkg2 does not match pkg1 RuntimeError: feedback: ABI mismatch detected between pkg6 and 33 ODYSSEYARENA: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions Repo Syetem Environment User Instruction (Part II) Step 7: Input pip install pkg2==1.0, click Execute Action Environment feedback: Successfully installed pkg3==1.0 Reason: Successfully installed pkg3==1.2 Step 8: Input python run.py, click Execute Action Environment feedback: ModuleNotFoundError: Reason: pkg2 not installed No module named pkg3. Step 9: Input pip install pkg3==1.0, click Execute Action Environment feedback: Successfully installed pkg3==1.0 Reason: Successfully installed pkg3==0.1 Step 10: Input python run.py, click Execute Action Environment with pkg1. Reason: All version numbers of pkg3 must match pkg1 (including integer and decimal parts) RuntimeError: feedback: tightly-coupled components are out of sync Step 11: Input pip install pkg3==1.0, click Execute Action Environment feedback: Successfully installed pkg3==1.0 Reason: Successfully installed pkg3==1.0 Step 12: Input python run.py, click Execute Action Environment feedback: Task completed! Project ran successfully! Reason: All conditions met Tips: Packages may have dependencies and version conflicts Need to carefully handle version constraints Maximum 120 steps allowed Goal: Successfully run python run.py so that the project can execute normally"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "Nanyang Technological University",
        "National University of Singapore",
        "Shanghai AI Laboratory",
        "The University of Hong Kong",
        "Tsinghua University",
        "Xian Jiaotong University"
    ]
}