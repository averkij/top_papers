{
    "paper_title": "Image Diffusion Preview with Consistency Solver",
    "authors": [
        "Fu-Yun Wang",
        "Hao Zhou",
        "Liangzhe Yuan",
        "Sanghyun Woo",
        "Boqing Gong",
        "Bohyung Han",
        "Ming-Hsuan Yang",
        "Han Zhang",
        "Yukun Zhu",
        "Ting Liu",
        "Long Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver."
        },
        {
            "title": "Start",
            "content": "Fu-Yun Wang1,2* Hao Zhou1 Liangzhe Yuan1 Sanghyun Woo1 Boqing Gong1 Bohyung Han1 Ming-Hsuan Yang1 Han Zhang1 Yukun Zhu1 Ting Liu1 Long Zhao 1Google DeepMind 2The Chinese University of Hong Kong 5 2 0 2 5 1 ] . [ 1 2 9 5 3 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/GUN/consolver. 1. Introduction Diffusion models [14] have significantly advanced generative artificial intelligence, particularly in high-fidelity visual data synthesis [6, 19, 31] and multimodal content creation [8, 29]. Their ability to generate diverse, high-quality outputs has driven progress in various generative tasks. However, their computationally intensive inference process, requiring numerically solving the reverse differential equations, limits their practicality in resource-constrained settings (e.g., mobile devices). To tackle this issue, we propose *Work done while the author was student researcher at Google DeepMind. Correspondence to Fu-Yun Wang (fywang0126@gmail.com) and Long Zhao (longzh@google.com). Figure 1. Overview of our Diffusion Preview framework for efficient image generation using diffusion models. Given text prompt and noise map, we first perform faster diffusion sampling to quickly generate preview image. The user then decides whether the result is satisfactory. If not, they may refine the prompt or change the random seed. Once satisfied, full-step diffusion sampling is applied to generate the final high-quality image. This iterative workflow improves sampling efficiency and reduces unnecessary computational cost. preview-and-refine framework, namely Diffusion Preview, illustrated in Fig. 1, which splits the users generation trials into two stages: (i) rapid preview stage for generating and evaluating preliminary outputs and (ii) refinement stage for resource-intensive high-quality sampling. Specifically, in the preview stage, fast, low-step sampling process generates preliminary output that closely approximates the final high-quality result. This enables users to iterate quickly, experimenting with prompts or random seeds with miniIn the refine stage, when premal computational cost. view meets expectations, the same iterated parameters will 1 be used in full-step sampling process to produce highfidelity output, fully leveraging the models capabilities. This workflow is particularly valuable in interactive settings, such as design prototyping, where rapid feedback is critical. For instance, designer can preview multiple image variations in seconds, select promising candidate, and refine it into polished result, saving significant time and resources. We argue that robust Diffusion Preview framework should exhibit the following characteristics: Fidelity. Previews should closely resemble the final output in visual and structural quality, providing reliable representations that enable informed user decisions while maintaining sufficient quality for effective evaluation. Efficiency. To support rapid iteration, the preview stage should minimize computational overhead, enabling users to quickly generate and explore multiple variations. Consistency. Previews should ensure predictable and stable mapping between initial parameters (e.g., random seeds) and the final output, guaranteeing that refining satisfactory preview produces high-quality result aligned with user expectations. We consider the diffusion sampling process based on the Probability Flow ODE (PF-ODE) of diffusion models, as PF-ODE is deterministic sampling algorithm [38]. When all initial parameters are fixed (e.g., prompts, initial noise), executing the exact PF-ODE sampling yields consistent results. This distinguishes PF-ODE from general SDE algorithms, as the sampling process does not introduce any additional random noise. We treat the exact PF-ODE sampling (termed full-step sampling) as the target for our refined results, aiming to achieve accurate previews of the final target through low-step sampling. However, achieving effective Diffusion Preview poses significant challenges for existing diffusion acceleration techniques. Training-free methods, such as zero-shot ODE solvers [15, 2224, 36], rely on theoretical assumptions that may not align with the models actual behavior. It frequently produces low-quality previews that fail to capture the essential characteristics of the final output. Posttraining approaches present different limitations. ODE distillation methods [25, 39] and score distillation techniques [26, 49, 52] bake acceleration directly into model weights, enabling high-quality outputs in few steps but at substantial cost. These methods require expensive retraining and often disrupt the deterministic correspondence between noise space and data space induced by the PF-ODE. Moreover, ODE distillation methods suffer from accumulated distillation errors, causing degradation of the original ODE path and deterioration in generation quality. Score distillation methods fundamentally alter the models learned trajectory due to their GAN-like training objectives [13, 49]. Furthermore, distilled models typically lose key properties of the original diffusion models, such as flexible inference step selection and score estimation. To this end, we introduce ConsistencySolver, novel solution tailored for the Diffusion Preview paradigm. ConsistencySolver is trainable, high-order solver that optimizes the sampling dynamics of pre-trained diffusion models using Reinforcement Learning (RL) [40]. By adapting to the models sampling dynamics rather than modifying the model itself, ConsistencySolver produces high-quality previews in low-step regimes while preserving the deterministic PF-ODE mapping essential for consistent refinement. ConsistencySolver synergizes the strengths of efficient ODE solving and distillation learning, learning an improved sampling strategy directly from data while maintaining the base models integrity and flexibility. In summary, our main contributions are: (i) flexible, trainable solver framework that improves preview fidelity in low-step sampling scenarios; (ii) An RL-based optimization strategy for diffusion model sampling dynamics, offering robust alternative to existing acceleration techniques; (iii) Comprehensive empirical experiments demonstrating that ConsistencySolver achieves superior balance among preview fidelity, efficiency, and consistency, enabling seamless Diffusion Preview workflows. 2. Related works Despite the superior generative quality of diffusion models since their inception [14, 37], sampling latency remains critical bottleneck relative to alternatives such as GANs [11] and VAEs [17]. Training-free ODE solvers. Training-free acceleration hinges on optimized ODE solvers for the probabilityflow ODE (PF-ODE) [38]. Early strides reduced NFE from 1000 to under 50 via deterministic [27] or quadratic timestep schedules [36], with Analytic-DPM [2] deriving closed-form optimal variance. Leveraging PF-ODEs semilinear structure, subsequent solvers approximate analytic integrals: DPM-Solver [23] employs Taylor expansion, DEIS [50] polynomial extrapolation, and iPNDM lowerorder multistep warm-starts. Extensions include DPMSolver++ [24] (singleand multi-step variants), EDM [15] (Heuns method), PDNM [22] (linear multistep with RungeKutta initialization), and UniPC [51] (unified predictorcorrector), collectively pushing NFE toward 10. Distilling ODE sampling dynamics. Distillation-based solvers, by contrast, train auxiliary networks to emulate multi-step trajectories in single-step predictions. Representative approaches encompass reparameterized DDPMs with KID loss [44], higher-order gradient prediction via truncated Taylor terms (GENIE [7]), intermediate timestep regression (AMED-Solver [53]), and stepwise residual coefficients (D-ODE [16]). Although differing in formulation, these methods converge on segment-wise trajectory 2 matching (i.e., supervising single-step high-order inference with multi-step outputs), which yields locally consistent but globally suboptimal alignment. In opposition, our proposed framework introduces generalized functional form, empirically validated via reinforcement learning to achieve superior efficiency, efficacy, and final-sample consistency. 3. Preliminaries on ODE solvers Diffusion models [14] generate samples by numerically integrating PF-ODE [38]. We start by reviewing the mathematical foundations of the PF-ODE and common solver approximations, and then discuss general linear multistep methods that leverage multiple prior states to improve convergence and accuracy. 3.1. PF-ODE Diffusion models define series of intermediate distributions Pt(xx0) = (αtx0, σ2 I), where x0 is the data. The noise adding process is formulated as the Stochastic Differential Equation (SDE) [37, 38]: dxt = ftxtdt + gtdwt, where dwt denotes the Wiener process, and the functions ft and gt are defined as: dft = log αt dt 2 log αt . The deterministic reversal of the SDE (i.e., PFODE) is given by [38]: = dσ2 dt σ2 , g2 dt dxt = (cid:20) ftxt g2 2 (cid:21) xt log Pt(xt) dt . (1) Adopting ϵ(xt, t) = σtxt log Pt(xt, t), we can re-write Eq. (1) into simplified form: (cid:19) (cid:18) xt αt = (cid:19) (cid:18) σt αt ϵ(xt, t) . (2) 3.2. Diffusion ODE solvers , ys = xs Denote yt = xt αs αt we can give the exact solution of the above PF-ODE: , ns = σs αs , nt = σt αt in Eq. (2), ys = yt + (cid:90) ns nt ϵ(xtn , tn)dn , (3) where tn is the inverse function of nt. The key to obtaining the exact solution for Eq. (3) lies in how we approximate the integration from nt to ns. Common techniques include: (i) naive approximation, where assuming constant ϵ(xt, t) over [s, t] yields ys = yt + (ns nt)ϵ(xt, t), equivalent to DDIM [36]; (ii) middle point approximation, where midpoint with nr = nt ns gives ys = yt + (ns nt)ϵ(xr, r), equivalent to DPM-Solver-2 [23]. These approximations can also be derived via Taylor expansion analysis (see the supplementary material). 3.3. Linear Multistep Method In addition to the above naive approximations, Linear Multistep Methods (LMMs) [4, 12, 32] are known to be effective for solving ODEs by utilizing multiple prior states to improve accuracy and speed up the convergence. Given an ODE of the form dxt dt = (xt, t), an m-step LMM approximates the solution xti+1 using the recurrence: xti+1 = m1 (cid:88) j= µjxtij + (ti+1j tij) (cid:88) j=0 wjf (ti+1j, xti+1j ) , (4) for = m1, m, . . . , 1, where xti, xti1 , . . . , xtim+1 are the state vectors stored for the last steps, represents the ODEs derivative function, and µj and wj are approachspecific coefficients. The method is explicit if w0 = 0, using only past states for the update, or implicit if w0 = 0, requiring nonlinear solve at each step. Typically, explicit methods are favored for computational efficiency, while implicit methods enhance stability for stiff ODEs. 4. ConsistencySolver 4.1. Adaptive ODE solvers for faithful previews To achieve high-fidelity, consistent previews in few-step diffusion sampling, we introduce ConsistencySolvera learnable, multistep ODE solver that dynamically adapts its integration strategy to maximize alignment between lowstep previews and high-step reference generations. Unlike fixed solvers that apply rigid numerical schemes across all timesteps, ConsistencySolver treats the choice of integration coefficients as policy to be optimized, conditioned on the local dynamics of the sampling trajectory. Given pretrained diffusion model ϵϕ(xt, t, c) where xt is the noisy input at time t, and is the conditioning signal (e.g., text prompt), we perform -step sampling over discretized timesteps {ti}N i=0 [0, 1]. For clarity, we denote ϵi ϵϕ(xti , ti, c). At each transition from ti to ti+1, ConsistencySolver computes the update via weighted combination of past noise predictions, followed by deterministic ODE step. Specifically, Ψθ is formulated as: yti+1 = yti + (nti+1 nti) (cid:88) wj(ti, ti+1) ϵi+1j , j=1 (5) where yti = xti , xti+1 can be obtained by αti+1 yti+1 αti , nt = σt/αt, is the solver order (number of historical steps used), and the adaptive coefficients wj(ti, ti+1) are generated by lightweight neural policy network: (cid:2)w1 w2 wm (cid:3) = θ(ti, ti+1) . (6) 3 Figure 2. Overview of our RL framework for optimizing learnable ODE solver in diffusion sampling. Given prompt and noise map, the diffusion model ϵϕ predicts denoising directions conditioned on the prompt. learnable ODE solver Ψθ generates preview image xp via few-step sampling, while training-free solver Ψ produces target image xgt using full-step sampling. The similarity reward based on depth maps, segmentation masks, DINO features etc. guides the update of θ via Proximal Policy Optimization (PPO). The network θ which implemented as an MLP with inputs (ti, ti+1) learns to predict context-aware integration weights that best preserve semantic and structural fidelity across step budgets. We provide diagram illustrating the workflow of the generalized learnable ODE solver Ψθ in the supplementary material. Training objective. The training objective is to maximize previewtarget consistency. To be specific, let xgt be the output of full-step sampling from initial noise (0, I) under prompt c; let xp be the output of few-step sampling using ConsistencySolver with the same and c. Our goal is to find the optimal solver policy that achieves the highest similarity reward = Sim(xgt, xp): Ψθ = arg max Ψθ Ez,c [Sim(xgt, xp)] , (7) where Sim(, ) is perceptual similarity metric (e.g., depth maps, segmentation masks, DINO, etc.). This objective directly incentivizes the solver to produce previews that serve as reliable proxies for the final generation. Solver searching via RL. To discover an optimal adaptive multistep ODE solver, we cast the training of the policy network θ as sequential decision-making problem and optimize it with Proximal Policy Optimization (PPO) [34]. gt )}M Offline dataset preparation. Prior to training, we generate an offline dataset consisting of promptnoisereference triples {(c(k), z(k), x(k) k=1. For each entry, c(k) is sampled from the training prompt distribution, z(k) (0, I), and x(k) is generated via full-step sampling using the pregt trained diffusion model. This dataset is fixed and reused across all experiments, enabling reproducible reward computation and eliminating the overhead of on-the-fly reference target generation during policy optimization. Training episode rollout. At each PPO episode, we uniformly sample batch of triples from the offline dataset. For each selected (c, z, xgt), we unroll K-step preview trajectory using Ψθ of Eq. (5). At every transition (ti ti+1) within predefined K-step schedule {t0 > t1 > > tK}, the policy processes inputs (ti, ti+1) through lightweight MLP to output the coefficients sampling w(ti, ti+1) = [w1, . . . , wm] and corresponding probabilities. Reward and policy update. Upon completing the Kstep rollout, the preview xp is compared against the precomputed xgt, yielding scalar similarity reward = Sim(xgt, xp). The policy is optimized via the standard PPO clipped surrogate objective: JPPO = (cid:104) min(cid:0)r(θ) ˆA, clip(r(θ), 1 ϵ, 1 + ϵ) ˆA(cid:1)(cid:105) , (8) where θ denotes policy parameters, r(θ) = πθ(as) πθold (as) is the probability ratio between current and old policies, ˆA is the estimated advantage, ϵ (0, 1) is the clipping parameter, and clip(, 1ϵ, 1+ϵ) restricts r(θ) to [1ϵ, 1+ϵ] to ensure stable updates. The advantage is computed with batch selfnormalization: ˆA = E[R] σ[R] + δ , (9) with E[R] and σ[R] being the mean and standard deviation of rewards in the current minibatch, and δ > 0 small constant to prevent division by zero. This follows common RL practice in generative modeling [1, 3, 9, 20, 35]. 4.2. Theoretical grounding While ConsistencySolver is trained end-to-end via RL, its architectural form is rigorously derived from classical LMMs [4, 12, 32], adapted to PF-ODEs. Recall the general m-step LMM for dxt dt = (t, xt) in Eq. (4). We adapt LMMs to PF-ODE sampling through three principled modifications: 4 1. Explicit-only design: w0 = 0. Empirical analyses show that PF-ODE trajectories are smooth and nonstiff [5, 53]. Implicit solves are unnecessary and computationally prohibitive. Therefore, we only consider the explicit design by setting w0 = 0. 2. Anchor to current state: µ0 = 1, µj = 0 for 1. We retain only the most recent state yti as the integration base, eliminating redundant history storage while preserving high-order accuracy via derivative blending. 3. Timestep-conditioned coefficients. Classical LMMs use fixed wj in Eq. (4). We relax this to wj(ti, ti+1), allowing the solver to adapt its integration paradigm as the denoising timesteps. Notably, rather than deriving the coefficients in Eq. (5) through theoretical assumptions or approximations, we treat them as learnable unknowns, which endows the ConsistencySolver with exceptional flexibility and broad applicability. We further demonstrate that several widely used diffusion solvers [2224, 36] can be recast within the ConsistencySolver framework defined in Eq. (5). See the supplementary material for additional details. 4.3. RL vs. distillation ConsistencySolver is flexible in training, supporting either RL or distillation. We choose to use RL due to its three key advantages compared with distillation methods: (i) Compatibility with non-differentiable rewards. RL eliminates the need for differentiable reward and avoids backpropagating through the diffusion trajectory, thereby removing primary cause of instability and overhead in distillation. (ii) Superior generalization and quality. The RL-trained ConsistencySolver better generalizes to novel prompt-noise pairs, yielding higher fidelity and elevated average consistency scores across CLIP, DINO, Depth and additional metrics (see Tab. 2). (iii) Reduced training overhead. Relying solely on sparse rewards from the final clean output, RL forgoes intermediate gradient storage. Furthermore, only the compact MLP participates in loss computation, substantially lowering memory usage and facilitating efficient training. In Sec. 5.2, we compare the proposed RL based ConsistencySolver with distillation baselines (AMED [53] and Ours-Distill). The experimental results empirically demonstrate the advantages of the proposed RL based method to distillation methods. 5. Experiments 5.1. Experimental setup We evaluate ConsistencySolver using Stable Diffusion [31] for text-to-image generation and FLUX.1-Kontext [18] for instructional image editing. For each model, we sample 2,000 caption-noise-sample pairs from evaluation datasets, with ground truth samples (xgt) obtained using 40-step multistep DPM-Solver. Without otherwise specified, we use depth maps as the reward function in RL. To evaluate Diffusion Preview, we assess three core aspects: fidelity, efficiency, and consistency. These metrics ensure previews are accurate, efficient, and well-aligned with refined outputs, meeting the demands of high-quality image generation. For text-to-image generation, the fidelity is measured using the Frechet Inception Distance (FID) [13], which compares feature distributions between generated previews and real images. For instructional image editing, we adopt Edit Reward [46] and Edit Score [45] to measure the editing fidelity and the instruction alignment. The efficiency is quantified as inference time per image, reflecting the efficiency of preview generation. Tab. 1 summarizes the six dimensions we utilized for measuring consistency. Evaluation datasets. For text-to-image generation with Stable Diffusion, we use the prompts from the validation set of COCO 2017 [21] as the prompts for evaluation, which is common dataset adopted to assess the generation capacity of text-to-image diffusion models. For instructional image editing, we use KontextBench [18] as the reference images and editing instructions to reflect the models performance regarding aspects such as character reference, global editing, local editing, etc. Distillation baselines. We use trajectory based distillation methods as our distillation baselines. Two methods are selected: AMED [53] and Ours-Distill. Ours-Distill distills the full sampling trajectory by aligning intermediate states in segment-wise fashion, sharing similar principles with AMED [53] and D-ODE [16]. More details are discussed in the supplementary material. 5.2. Quantitative comparison Stable Diffusion. Tab. 2 presents comprehensive quantitative comparison of ConsistencySolver against various baselines on Stable Diffusion for text-to-image generation across multiple measures including FID and consistency metrics. Among training-free ODE solvers such as DDIM, iPNDM, and multistep DPM-Solver, ConsistencySolver consistently outperforms at equivalent step counts. It achieves lower FID values (e.g., 20.39 at 5 steps vs. multistep DPM-Solvers 25.87) and higher consistency scores across all dimensions, demonstrating superior alignment with refined outputs. Compared with distillation-based methods such as DMD2, Rectified Diffusion, LCM, and PCM, which often require fewer steps but sacrifice quality, ConsistencySolver delivers competitive or better performance. For instance, at 4 to 8 steps, it surpasses LCM and PCM in FID and most consistency metrics, highlighting its efficiency in balancing speed and quality without distillation overhead. As the number of steps increases (e.g., up to 12), ConsistencySolver further refines its outputs, yielding the best overall results with FID as low as 18.53 and peak 5 Dimension Description Model Metric Table 1. Metrics employed for consistency evaluation. Image semantic measured by vision embeddings Semantic alignment (CLIP) Structural consistency (DINO) Alignment in image structure and layout Perceptual similarity (Inc.) Segmentation accuracy (Seg.) Pixel-level similarity (Img.) Depth consistency (Dep.) Visual resemblance through perception models Overlaps between segmentation masks Pixel-wise differences between raw images Differences between depth maps CLIP ViT-L/14 [30] DINOv2-L/14 [28] Inception V3 [41] SegFormer [47] Depth Anything V2 [48] Cosine similarity Cosine similarity Cosine similarity Mean Dice coefficient PSNR PSNR Table 2. Comparison of ConsistencySolver with baselines at various steps. Best results per step in bold. Ours-Distill is the proposed ConsistencySolver with coefficients trained with trajectory distillation. AMED is only applicable to even steps. Method Steps FID CLIP Seg. Dep. Inc. Img. DINO Training-Free ODE Solvers DDIM [36] iPNDM [22] UniPC [51] DEIS [50] Multistep DPM [23] DDIM iPNDM UniPC DEIS Multistep DPM DDIM iPNDM UniPC DEIS Multistep DPM DDIM iPNDM UniPC DEIS Multistep DPM DMD2 [49] LCM [25] LCM LCM PCM [42] Rectified Diff. [43] AMED [53] AMED AMED AMED AMED Ours-Distill Ours-Distill Ours-Distill Ours-Distill ConsistencySolver ConsistencySolver ConsistencySolver ConsistencySolver 5 5 5 5 5 8 8 8 8 8 10 10 10 10 10 12 12 12 12 1 2 4 8 4 4 4 6 8 10 14 5 8 10 12 5 8 10 12 52.59 37.44 23.15 25.78 25.87 29.46 25.88 19.68 20.14 19.53 24.88 22.65 19.38 19.42 19.29 22.81 21.23 19.32 19.24 18.95 87.8 89.3 93.2 92.2 93.1 91.1 91.7 95.5 94.9 95.9 92.4 92.9 96.5 95.9 97.0 93.1 93.5 97.2 96.6 97.7 41.9 44.1 67.2 65.4 66.6 54.2 54.9 75.0 73.6 76.3 59.0 59.1 79.7 77.2 80.5 61.6 62.7 82.5 80.2 84. 14.2 14.3 18.7 18.4 19.1 16.2 16.4 21.4 20.7 21.8 17.1 17.4 23.2 21.9 24.1 17.7 17.9 24.7 23.2 25.9 Distillation-Based Methods 19.88 22.20 22.00 21.50 21.67 20.64 89.3 89.8 90.0 89.2 92.6 94.4 42.1 51.9 50.8 47.3 63.0 67.6 12.6 14.9 14.3 13.6 17.2 18. Distillation-Based Solvers 31.09 20.42 19.22 18.95 19.08 22.91 19.65 19.29 19.06 20.39 18.82 18.66 18.53 90.4 93.3 94.9 96.2 97.2 92.9 95.1 95.9 96.6 58.6 66.2 72.4 78.3 83.3 66.7 74.0 77.5 80.4 Proposed Method 69.4 78.5 83.2 85.6 94.2 96.4 97.2 97. 16.9 18.1 20.0 22.2 24.4 18.5 20.8 22.0 23.0 19.3 22.2 24.9 26.7 74.1 75.7 85.0 83.8 85.6 81.5 81.7 90.0 89.1 90.8 83.6 84.0 91.9 90.8 93.1 85.1 85.3 93.4 92.3 94.7 70.5 77.9 78.1 76.5 83.9 87.0 80.4 85.5 88.3 91.4 93.6 84.8 89.3 91.0 92.3 87.1 91.6 93.9 95. 16.4 15.0 19.6 19.2 20.6 17.9 17.0 21.6 21.0 23.2 18.7 18.0 23.0 22.1 25.1 19.2 18.6 24.4 23.2 26.8 12.1 14.3 12.6 10.7 17.9 19.7 17.9 18.8 20.5 22.3 24.3 19.3 21.1 22.2 23.2 20.8 23.4 25.3 26.7 73.2 73.9 85.5 84.3 85.5 79.9 79.8 90.5 89.3 90.6 82.1 82.6 92.5 91.4 93.0 83.4 83.9 94.1 92.9 94.5 73.8 75.4 75.1 72.9 82.3 85. 80.8 85.4 88.8 91.8 94.3 85.1 89.5 91.5 92.8 86.5 91.2 93.5 95.0 consistency scores like 97.9 in CLIP and 95.1 in Inception. FLUX.1-Kontext. In Tab. 3, we compare ConsistencySolver with baselines in terms of Edit Reward (E. R.) and Edit Score (E. S.) for fidelity and instruction alignment, alongside consistency metrics (DINO, Inception, CLIP, and Depth). At lower steps (3 to 4), ConsistencySolver shows marked improvements over FLUX.1-Kontext, with higher Edit Reward (e.g., 0.73 at 4 steps vs. 0.61) and Edit Score (5.67 vs. 5.45), indicating better editing accuracy and adherence to instructions. By 5 steps, it achieves the best results across all metrics, including superior Edit Reward of 0.86 and Depth consistency of 25.18, underscoring its 6 Table 3. Comparison of ConsistencySolver with FLUX.1-Kontext at various steps. Best results per step in bold. Method Euler [15] Heun [15] DPM Multistep DPM ConsistencySolver Steps E. R. E. S. DINO 88.47 91.31 93.09 82.13 77.83 87.69 89.36 92.10 93.68 89.20 91.83 93.44 89.38 92.39 93.90 0.32 0.61 0.79 -0.23 -0.40 0.18 0.39 0.69 0.80 0.41 0.72 0.83 0.45 0.73 0.86 4.66 5.45 5.80 3.08 2.21 4.31 4.90 5.60 5.88 5.04 5.57 5.92 5.13 5.67 6.02 3 4 5 3 4 5 3 4 5 3 4 5 3 4 5 Inc. CLIP Dep. 22.90 92.68 83.56 23.99 93.95 86.75 24.76 95.25 89.16 20.25 89.70 77.28 17.85 86.95 72.39 21.71 92.28 81.73 23.20 93.14 84.68 24.30 94.64 88.22 25.05 95.71 90.33 22.96 93.11 84.87 23.70 94.49 88.12 24.59 95.53 90.17 23.08 93.10 85.01 24.27 94.86 88.71 25.18 95.87 90.76 ability to produce high-fidelity previews that closely match refined edits while maintaining computational efficiency. 5.3. Qualitative comparison Fig. 3 presents visual comparisons of previews generated by Stable Diffusion for text-to-image tasks, while Fig. 4 shows visual comparisons of previews produced by FLUX.1Kontext for instructional image editing. We demonstrate five representative examples across diverse editing tasks, including character reference, text editing, style reference, global editing, and local editing. Compared to training-free ODE solvers and distillation-based methods, ConsistencySolver yields previews with sharper details and superior alignment to the refined outputs. 5.4. Studies on Diffusion Preview In addition to the aforementioned evaluations on generation quality and consistency, we further validate the practical effectiveness of our proposed preview-and-refine paradigm through user study. Specifically, we fix the prompt and repeatedly sample images with different random noise until the users are satisfied or the attempt limit is reached. We then compare the average time and attempts used by different methods to generate the user satisfactory images. Besides real human user, we also use Claude Sonnet 4 as proxy for discerning users to avoid any potential bias from human. To demonstrate the efficiency gains of our preview mechanism, we conduct comparisons with two modes. In the high-quality mode, for given prompt, we generate the image using 40-step multistep DPM-Solver. The Figure 3. Visual comparison on Stable Diffusion for text-to-image generation. output is evaluated using both Claude Sonnet 4 and human judgment to determine whether it meets expectations. In the preview mode, we first generate fast preview using an 8-step ConsistencySolver and assess it via the same judgment mechanism. If the preview fails to meet requirements, new preview is generated; otherwise, we perform one 40-step DPM-Solver refinement (i.e., full-step sampling is triggered only after confirming satisfied preview). We report the average end-to-end inference time (including denoising and VAE decoding) for both paradigms. To prevent cases where Stable Diffusion fundamentally fails to satisfy certain prompts from skewing the results, we impose maximum of 10 attempts per prompt. Prompts that remain unsatisfactory after 10 trials are discarded, ensuring that timing statistics accurately reflect the efficiency of the preview mechanism under normal conditions. To evaluate generalizability across diverse user needs, we use three validation prompt sets: GenEval prompts [10], COCO 2017 validation [21], and LAION [33]. Detailed experimental protocols, including LLM prompts and human evaluation guidelines, are provided in the supplementary material. As shown in Tab. 4, Diffusion Preview reduces average inference time by up to 55% on LAION with only minor increase in attempts (i.e., 6.00 6.35). Comparison to distillation. As distillation-based models continue to improve, particularly the emergening of stateof-the-art single-step models like DMD2 [49], natural question arises: do we still need the preview-and-refine 7 Figure 4. Visual comparison on FLUX.1-Kontext for instructional image editing. Previews are generated with 5 inference steps. Table 4. Average attempts and end-to-end H100 inference time (in seconds) on three prompt sets. Lower is better. Table 6. Ablation study on solver order at 5, 8, and 10 steps. Best results per metric in bold. High-quality Preview Orders Steps Dep. Inc. Seg. Img. CLIP DINO Datasets GenEval COCO 2017 LAION GenEval COCO 2017 LAION Avg. attempts Avg. time (s) Avg. attempts Avg. time (s) Claude-Sonnet 2.88 3.64 6.35 Human Evaluation 3.82 3.52 5.18 3.12 3.60 6. 3.80 3.42 5.45 3.00 3.71 6.00 3.55 3.30 5.17 1.74 1.85 2.87 2.16 2.03 2.58 Table 5. User satisfaction within 10 attempts. Despite competitive FID, distillation methods show significant satisfaction drops, highlighting the practical importance of consistency. Method COCO 2017 GenEval Satisfied % of Base Satisfied % of Base Base model (40-step) DMD2 w/ GAN DMD2 w/o GAN ConsistencySolver (8-step) 2,143 1,389 1,267 2,057 100.0% 64.8% 59.1% 96.0% 69 57 114 100.0% 57.0% 47.1% 94.2% paradigm? If the generation quality is sufficiently high, one might argue that the Diffusion Preview paradigm and consistency property become less critical. To investigate this, we use Claude Sonnet 4 to conduct user-centric evaluation. We record the number of prompts satisfied within 10 attempts. As shown in Tab. 5, though DMD2 achieves competitive FID scores, it satisfies significantly fewer prompts compared to both the base model and our ConsistencySolver. On the GenEval prompts, DMD2 with and without GAN satisfy only 57.0% and 47.1% of the prompts compared with the base model, while our method maintains 94.2%. This disparity reveals critical insight: despite the competitive FID scores achieved by distillation-based methods, the loss of consistency fundamentally undermines generation quality in ways not captured by distribution-level metrics. For the proposed preview-and-refine workflows, where users rely on previews to guide iterative refinement, maintaining consistency is essential. 5.5. Ablation study Solver orders. We assess the effect of solver order, i.e., in Eq. (5), on ConsistencySolvers preview consistency at 5, 8, and 10 steps. As shown in Tab. 6, Order 4 consistently achieves the best overall performance across step counts, leading in key structural and perceptual metrics while maintaining strong semantic alignment. Lower-order solvers (e.g., Order 2 or 3) show reduced fidelity in layout and depth consistency, whereas Order 5 yields only marginal improvements in minor dimensions likely due to the increased RL search space complexity. Overall, Order 4 strikes better 8 Order 2 Order 3 Order 4 Order 5 Order 2 Order 3 Order 4 Order 5 Order 2 Order 3 Order 4 Order 5 5 5 5 8 8 8 8 10 10 10 10 5 Steps 87.30 86.46 87.07 87.16 69.36 68.93 69.42 69.38 8 Steps 91.59 91.57 91.65 91.65 78.56 77.92 78.52 78.19 10 Steps 93.74 93.74 93.88 93.79 82.86 82.68 83.22 82.78 20.84 20.26 20.75 20. 23.34 23.20 23.43 23.15 25.16 25.23 25.32 24.88 19.33 19.15 19.29 19.33 22.12 22.14 22.15 22.12 24.72 24.66 24.94 24.72 94.40 93.80 94.22 94. 96.31 96.26 96.35 96.33 97.25 97.23 97.25 97.18 86.39 85.83 86.35 86.44 91.03 90.81 91.09 90.97 93.45 93.29 93.48 93.36 Table 7. Ablation study on reward model choice at 5, 8, and 10 steps. Best results per metric in bold. Rewards Steps Dep. Inc. Seg. Img. CLIP DINO Dep. Inc. CLIP Img. DINO Seg. Dep. Inc. CLIP Img. DINO Seg. Dep. Inc. CLIP Img. DINO Seg. 5 Steps 87.07 87.05 87.30 87.22 87.19 86.81 69.42 69.49 69.73 69.44 69.64 69.28 8 Steps 91.65 91.51 91.45 91.75 91.62 91.36 78.52 77.33 77.54 78.17 77.84 77.05 10 Steps 93.88 93.35 93.44 93.87 93.60 93.15 83.22 82.01 81.84 82.74 82.55 81.37 20.75 20.18 20.30 20.69 20.43 19. 23.43 22.67 22.56 23.39 22.99 22.41 25.32 24.33 24.14 25.37 24.81 24.04 19.29 19.20 19.32 19.32 19.29 19.16 22.15 22.00 21.94 22.11 22.03 21.82 24.94 24.17 24.25 24.80 24.49 23.73 94.22 94.29 94.46 94.40 94.39 94. 96.35 96.17 96.15 96.34 96.28 96.05 97.25 97.05 96.99 97.28 97.15 96.96 5 5 5 5 5 5 8 8 8 8 8 8 10 10 10 10 10 10 86.35 86.30 86.50 86.53 86.43 86. 91.09 90.61 90.75 90.97 90.87 90.39 93.48 92.68 92.76 93.39 93.01 92.44 balance between efficiency and complexity. Reward models. We investigate the impact of different reward models on the RL training of ConsistencySolver. As shown in Tab. 7, the Depth reward provides strong structural fidelity, consistently achieving good performance across all steps. Meanwhile, the Img. reward performs well in pixellevel fidelity, particularly at higher steps. Although CLIP and DINO show competitive results in semantic alignment, Depth offers more balanced trade-off between structural consistency and overall robustness. We therefore adopt Depth as the default reward for its reliable generalization across diverse evaluation scenarios. 6. Conclusion This paper proposes Diffusion Preview, novel paradigm aimed at generating fast and consistent approximations of diffusion model outputs to enable efficient previewing in generative modeling. To address this task, we introduce ConsistencySolver, method that delivers reliable previews with few steps, outperforming existing training-free and distillation-based approaches in consistency, paving the way for more practical generative modeling workflows."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. 4 [2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytican analytic estimate of the optimal reverse variarXiv preprint dpm: ance in diffusion probabilistic models. arXiv:2201.06503, 2022. 2 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 4 [4] John Charles Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016. 3, 4 [5] Defang Chen, Zhenyu Zhou, Can Wang, Chunhua Shen, and Siwei Lyu. On the trajectory regularity of ode-based diffusion sampling. arXiv preprint arXiv:2405.11326, 2024. 5 [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. 1 [7] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. In NeurIPS, 2022. 2 [8] Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. 1 [9] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2024. 4 [10] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. In NeurIPS, 2023. 7, 2 [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In NeurIPS, Yoshua Bengio. Generative adversarial nets. 2014. [12] Ernst Hairer, Syvert P. Nørsett, and Gerhard Wanner. Solving Ordinary Differential Equations I: Nonstiff Problems. Springer-Verlag, Berlin, 2nd edition, 1993. Chapter III: Multistep methods. 3, 4 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In NeurIPS, 2017. 2, 5 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2, 3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 2, 6 [16] Sanghwan Kim, Hao Tang, and Fisher Yu. Distilling ode In CVPR, solvers of diffusion models into smaller steps. 2024. 2, 5 [17] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [18] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 5 [19] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 1 [20] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. 4 [21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 5, 7, 2 [22] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo In numerical methods for diffusion models on manifolds. ICLR, 2022. 2, 5, 6, [23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 2, 3, 6, 1 [24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 2, 5 [25] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2, 6 [26] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. In NeurIPS, 2023. 2 [27] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In ICML, pages denoising diffusion probabilistic models. 81628171. PMLR, 2021. 9 image editing via high-fidelity reward modeling. CoRR, abs/2509.23909, 2025. 5 [46] Keming Wu, Yifan Zhang, Tianyu Zhang, Qihang Yu, Junda Lu, and Xiaojuan Qi. Editreward: human-aligned reward model for instruction-guided image editing. CoRR, abs/2509.26346, 2025. 5 [47] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021. 6 [48] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. 6 [49] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 2, 6, [50] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In ICLR, 2023. 2, 6 [51] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867, 2023. 2, 6 [52] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024. 2 [53] Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in around 5 steps. In CVPR, 2024. 2, 5, 6 [28] Maxime Oquab et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6 [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [30] Alec Radford et al. Learning transferable visual models from natural language supervision. ICML, 2021. 6 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 5 [32] Timothy Sauer. Numerical Analysis. Pearson, 3rd edition, 2018. 3, 4, 1 [33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 7, 2 [34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4 [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn ICLR, 2021. 2, 3, 5, 6, ing diffusion implicit models. 1 [37] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. 2, 3 [38] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2, 3 [39] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. 2 [40] Richard Sutton. Reinforcement learning: An introduction. Bradford Book, 2018. 2 [41] Christian Szegedy et al. Rethinking the inception architecture for computer vision. In CVPR, 2016. 6 [42] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. In NeurIPS, 2024. 6, [43] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not In ICLR, pages 3142031445, your need in rectified flow. 2025. 6 [44] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In ICLR, 2021. 2 [45] Yifan Wei, Xiaoguang Hu, Xiaojuan Qi, Stephen Lin, and Hengshuang Zhao. Editscore: Unlocking online RL for"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Common Diffusion ODE Solvers via Taylor B. Common diffusion ODE solvers interpreted"
        },
        {
            "title": "Expansion",
            "content": "using ConsistencySolver The exact solution of Eq. (3) requires numerical approximation of yts = ϵ(xtn , tn) dn. (10) (cid:90) ns nt Let = ns nt. The Taylor expansion of the integrand around nt yields (cid:90) ns nt ϵ(xtn , tn) dn = ϵ(xt, t) + h2 dn d2 (cid:12) (cid:12) dn2 ϵ(xtn, tn) (cid:12)nt (cid:12) (cid:12) ϵ(xtn , tn) (cid:12)nt + . (11) + h3 For brevity, we denote ϵt ϵ(xt, t), (12) and similarly for other time points (e.g., s). A.1. First-order: DDIM / Euler (naıve) yts ϵt . (13) Retains only the zeroth-order term in Eq. (11). A.2. Second-order: DPM-Solver-2 / Midpoint The midpoint method uses one evaluation near the interval center: yts ϵr, nr nt + . (14) To see second-order accuracy, approximate the missing derivative with centered finite difference: dn ϵ (cid:12) (cid:12) (cid:12)nt ϵr ϵt h/2 . (15) Insert into the desired second-order truncation: ϵt + h2 2 ϵr ϵt h/2 = ϵt + (cid:0)ϵr ϵt (cid:1) = ϵr. (16) Thus ϵr exactly matches the second-order Taylor integral when the first derivative is estimated by midpoint difference. DPM-Solver-2 exploits this insight, typically choosing nr = ntns (geometric midpoint in noise-scale space). ConsistencySolver treats the coefficients in Eq. (5) as learnable unknowns. Here we show that several widely adopted diffusion solvers [22, 23, 36] can be easily interpreted using the form of ConsistencySolver. For notational simplicity, we denote ϵϕ(xti, ti) simply as ϵi throughout this section. DDIM (naive approximation) performs the update: yti+1 = yti + (nti+1 nti)ϵi . (17) Comparing with Eq. (5), we can have the naive approximation corresponds to one-step method (m = 1) with the coefficient w1 = 1. PNDM utilizes the explicit 4-step Adams-Bashforth method [32]. For the Ininial Value Problem (IVP) dy/dn = ϵ, the update is: ni 24 yti+1 = yti + [55ϵi 59ϵi1 + 37ϵi2 9ϵi3] , (18) where ni = nti+1 nti. This corresponds to = 4 with coefficients: w1 = 55 24 , w2 = 59 24 , w3 = 37 24 , w4 = 9 24 , (19) of the proposed the ConsistencySolver defined in Eq. (5). DPM-Solver-2 (midpoint approximation) uses an evaluation at an intermediate point ti (corresponding to nti = nti1nti+1): yti = yti1 + (nti nti1)ϵi1 , yti+1 = yti1 + (nti+1 nti1 )ϵi = yti + (nti+1 nti1 )ϵi (nti nti1 )ϵi1 (nti+1 nti1 ) (nti+1 nti ) ϵi = yti + (nti+1 nti)[ (nti nti1) (nti+1 nti) ϵi1] (20) Comparing with Eq. (5), we can have DPM-Solver-2 corresponds to two-stages computation. When is even (i.e., 0, 2, 4, . . . ), the approximation corresponds to one-step method (m = 1) with the coefficient w1 = 1. When is odd, the approximation corresponds to two-step method (nti+1 nti1 ) (nti+1 nti ) , w2 = (m = 2) with the coefficient w1 = (nti nti1 ) (nti+1 nti ) . Figure 5. Workflow of the generalized learnable ODE solver Ψθ with Order 4 (m = 4). At each sampling step, the diffusion model predicts noise ϵi conditioned on the input prompt and timestep. learnable neural network θ generates adaptive coefficients wj, = 1, 2, 3, 4 from current timestep ti, and target timestep ti+1 , which are used to form multi-step noise estimate ϵ = (cid:80)4 j=1 wj ϵi+1j. The ODE solver Ψθ then updates the sample from xti to xti+1 . This approach enables more accurate and stable integration in the generative sampling process. C. Visualization of ConsistencySolver We visualize the computation paradigm of the proposed ConsistencySolver in Fig. 5, taking Order 4 (m = 4) as an example. D. Implementation Details D.1. ConsistencySolver training Training dataset. We randomly sample 2,000 prompts from the LAION dataset [33] and generate corresponding images using 40-step multistep DPM-Solver, forming noise-prompt-target image triplets as our training data. Training procedure. All experiments are conducted on single H100 GPU. For each training iteration, we select one prompt-noise pair and replicate it 80 times. We then apply the trainable ConsistencySolver to generate 80 different sampling trajectories with random perturbations. Following the PPO algorithm, we increase the probability of highreward trajectories while suppressing low-reward ones. By default, we use Order-4 solver configurations. The MLP network in ConsistencySolver is trained from scratch using learning rate of 1 104 for 3,000 iterations, requiring approximately 12 H100 GPU hours in total. D.2. Distillation baseline training Beyond the proposed RL-based training approach, we explore distillation-based alternatives to optimize the dynamic coefficients in ConsistencySolver. We investigate two distillation schemes: Final-state distillation. This approach treats the entire few-step diffusion sampling chain as differentiable and directly uses the negative reward at the final state as the loss function. Gradients are backpropagated through the complete inference chain to optimize the parameters. While conceptually straightforward, this method exhibits significant drawbacks. First, backpropagating through the entire chain requires computing gradients not only for the ConsistencySolver MLP but also for the underlying diffusion model (typically containing billions of parameters), substantially increasing computational cost. Second, we observe severe training instability, with the MLP failing to converge effectively in practice. Trajectory distillation. Inspired by prior work [42, 53], we propose trajectory-based distillation method, referred to as Ours-Distill in the main text. This approach requires storing the complete 40-step trajectory from the multistep DPM-Solver (introducing additional storage overhead). The objective is to match each intermediate state in the few-step ConsistencySolver sampling to corresponding states in the 40-step reference trajectory. For example, when performing 5-step sampling, each ConsistencySolver step should align with 8 steps of the reference solver. We use the negative similarity between these states as the loss function for backpropagation. This method significantly outperforms final-state distillation but still falls short of the RLbased approach, as demonstrated in our quantitative comparisons in Tab. 2. Training dataset. We use the same 2,000 training samples as for ConsistencySolver training to ensure fair comparison. D.3. Preview study experimental protocol Evaluation datasets. For the preview study, we evaluate on three datasets: (1) GenEval evaluation set containing 553 prompts [10], (2) COCO 2017 validation set with 5,000 prompts [21], and (3) 5,000 randomly sampled prompts from LAION [33]. Evaluation with LLM. We use Claude Sonnet 4 as an automated judge to simulate discerning user. The system 2 Table 8. Ablation study on model structure at 8 and 10 steps. Best results per metric in bold. Model Steps Dep. Inc. Img. CLIP DINO 8 Steps Hidden Dim 32 Hidden Dim 256 Hidden Dim 1024 Deep (12-Layer MLP) Hidden Dim 32 Hidden Dim 256 Hidden Dim 1024 Deep (12-Layer MLP) 8 8 8 8 10 10 10 10 22.08 22.22 21.82 22.00 10 Steps 24.68 25.01 24.12 24.38 91.55 91.68 91.30 91.20 93.67 93.85 93.23 93. 23.07 23.56 22.36 22.60 24.80 25.57 23.96 24.22 96.24 96.36 96.04 96.14 97.16 97.30 96.92 96.99 90.96 91.14 90.57 90.68 93.31 93.67 92.67 93. prompt is designed to enforce strict evaluation criteria: You are very picky user evaluating an AIgenerated image for the prompt {prompt}. Be extremely criticalonly approve if it perfectly matches the description in composition, quality, details, and realism. Respond with ONLY SATISFIED if its perfect, or NOT SATISFIED: [brief reason] otherwise. Keep the reason under 50 words. This ensures the LLM judges each generated image with high standards, accepting only those that closely align with the prompt requirements. Human evaluation. To complement LLM evaluation, we conduct human studies with real users. For each prompt, we pre-generate 10 images and record their generation times. These images are organized into questionnaires where participants sequentially evaluate whether each image satisfies the prompt. Participants stop at the first satisfactory image; if all images are unsatisfactory, the trial is discarded as discussed in the main text. We recruit 20 volunteers, each responsible for evaluating 100 prompts uniformly sampled across all test datasets, resulting in comprehensive human feedback on the practical effectiveness of our preview mechanism. D.4. Ablation study on model structures We analyze architectural variants of ConsistencySolver, varying hidden dimension size and testing deep 12-layer MLP with residual LayerNorm, evaluated at 8 and 10 steps. According to Tab. 8, the 256-dimensional model consistently outperforms others, delivering superior results in image similarity, semantic alignment, and overall consistency. Larger dimensions (e.g., 1024) slightly enhance depth estimation but compromise balance and efficiency. The deep MLP variant shows no meaningful advantage over the standard 256-dim architecture, suggesting that moderate capacity is sufficient for the task."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "The Chinese University of Hong Kong"
    ]
}