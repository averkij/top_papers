{
    "paper_title": "Context Compression via Explicit Information Transmission",
    "authors": [
        "Jiangnan Ye",
        "Hanqi Yan",
        "Zhenyi Shen",
        "Heng Chang",
        "Ye Mao",
        "Yulan He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression."
        },
        {
            "title": "Start",
            "content": "Jiangnan Ye 1 Hanqi Yan 1 Zhenyi Shen 1 Heng Chang 2 Ye Mao 3 Yulan He 1 4 Abstract Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing keyvalue caches, motivating In this work, we study context compression. soft context compression, where long context is condensed into small set of continuous representations. Existing methods typically repurpose the LLM itself as trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), lightweight framework that formulates soft compression into new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the models internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into small number of slots via globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only 1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust longcontext compression. 6 2 0 2 3 ] . [ 1 4 8 7 3 0 . 2 0 6 2 : r 1. Introduction As Large Language Models (LLMs) continue to scale, the computational and memory costs associated with long1Kings College London, UK 2Tsinghua University, China 3Imperial College London, UK 4The Alan Turing Institute, UK. Correspondence to: Yulan He <yulan.he@kcl.ac.uk>. Preprint. February 4, 2026. 1 Figure 1. Visualization of two structural weaknesses of existing LLM-as-a-compressor methods. (i) Representation overwriting: The information carried by the compression token that captures Year 2012 is gradually overwritten into highly abstract features during the layer-by-layer encoding, leading to information loss for the decoder. (ii) Lack of global coordination: The key token Paris is not attended by the compression tokens due to the lack of global coordination of compression capacity allocation. context processing have become major bottleneck for practical deployment (Liu et al., 2025a; 2024). Context compression has therefore emerged as an effective technique to reduce the length of input context, lowering inference cost and alleviating the memory footprint of keyvalue caches during generation (Li et al., 2025a). Existing compression methods can be broadly categorized into hard and soft approaches. Hard compression methods select or prune discrete tokens based on estimated salience (Li et al., 2023; Jiang et al., 2023; Pan et al., 2024), which is efficient but lossy under high compression rate. Soft compression instead compress information from many tokens into small set of continuous vectors (Mu et al., 2023; Ge et al., 2024; Li et al., 2025b; Zhang et al., 2025; Deng et al., 2025a). We focus on soft compression in this work for its greater expressive capacity and fine-grained aggregation beyond discrete token boundaries. Most existing soft compression methods follow an LLM-as-"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "a-compressor paradigm, where special compression tokens (e.g. gist/memory tokens) are introduced and iteratively updated through self-attention during LLM training in order to absorb information from context into these tokens across layers. After forward pass, the final-layer hidden states of these compression tokens serve as the compressed context for decoding (Ge et al., 2024; Li et al., 2025b; Zhang et al., 2025). While effective, this design has two structural weaknesses as illustrated in Figure 1. First, this layer-by-layer compression is intertwined with the deep Transformer computation and can induce fundamental representation overwriting issue: as compression tokens are repeatedly updated, information aggregated in earlier layers may be overwritten, while the states in late layers may inherently drift toward highly abstract, generationspecialized features. Consequently, this increases the distribution mismatch between the layer space of the compressor and the decoder, making compression harder to optimize and less robust. Second, compression tokens lack coordinated allocation. Self-attention aggregation in existing LLM-as-a-compressor methods is performed independently across compression tokens, with no mechanism to allocate capacity across tokens. As result, some regions of context may be redundantly covered while others are underrepresented, leading to inefficient use of the limited compression budget. These limitations motivate us to view compression from different perspective. We consider that forward pass of an LLM already produces informative hidden representations across layers at multiple levels of abstraction, ranging from low-level lexical features to high-level semantic abstractions (Zhang et al., 2024). Therefore, instead of training the LLM to learn how to compress, we propose to keep the LLM frozen and perform compression directly over these cached hidden states. This decouples compression from the LLMs internal self-attention dynamics, avoiding progressive representation drift and uncoordinated allocation. To realize this new paradigm, we propose ComprExIT, which formulates soft context compression as explicit information transmission over frozen LLM hidden states. The method performs compression along two orthogonal dimensions. First, depth-wise (inter-layer) transmission selectively aggregates layer-wise information into token anchors, mitigating progressive overwriting and reducing the mismatch induced by late-layer outputs. Second, width-wise (token-to-slot) transmission aggregates token anchors into slots under globally optimized transmission plan, explicitly coordinating how information is distributed across slots, resulting in the final compression tokens. ComprExIT is lightweight yet highly effective, adding only about 1% additional parameters. It achieves state-of-theart performance across both in-domain and out-of-domain evaluation benchmarks, matching (or even surpassing) an uncompressed fine-tuned model. In summary, we make the following contributions: We identify two key structural limitations in existing soft context compression paradigms: (i) the representation overwriting and distribution mismatch induced by the layer-by-layer compression process (ii) the lack of global coordination in the information aggregation of compression tokens. We propose new formulation of soft compression as explicit information transmission over frozen LLM hidden states, decoupling compression from internal self-attention and enabling stable, controllable aggregation. Based on the new paradigm, we introduce ComprExIT, lightweight framework that performs coordinated depth-wise and width-wise transmission. Extensive experiments on six QA benchmarks demonstrate that ComprExIT consistently outperforms prior state-of-the-art compression methods. We provide comprehensive analyses and ablation studies to understand ComprExIT, offering insights into its behavior, and key design choices. 2. Related Work Hard compression. methods reduce long-context cost by pruning input tokens according to estimated importance. Representative approaches include SelectiveContext (Li et al., 2023) and LLMLingua-style methods (Jiang et al., 2023; 2024), which score tokens/spans using language model (e.g., via self-information or related salience measures) and retain only the most informative parts of the context. LLMLingua-2 (Pan et al., 2024) instead distills lightweight classifier from stronger LLM (e.g., GPT-4) to decide which tokens to keep. EFPC (Cao et al., 2025) further unifies task-aware and task-agnostic compression within single framework. Despite their efficiency gains, hard methods remain discrete and lossy, often facing bounded compression ratios and limited expressiveness compared with soft compression in continuous space. Soft context compression. provides greater flexibility than discrete token removal under high compression ratios. Wingate et al. presented an early attempt to compress context tokens into single vector, focusing on representation learning. AutoCompressor (Chevalier et al., 2023) builds LLMs into compressors inspired by the Recurrent Memory Transformer (Bulatov et al., 2022), accumulatively compressing contexts into summary tokens. Mu et al. introduces gist tokens as an information bottleneck by modifying attention masks, establishing the LLM-as-a-compressor paradigm. ICAE (Ge et al., 2024) further simplifies this paradigm into an encoderdecoder framework, which has"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Figure 2. comparison between existing LLM-as-a-compressor methods (left) and ComprExIT (right). Existing methods introduce gist tokens that are iteratively encoded by the self-attention layers in the LLMs, which are trained to aggregate information from context tokens and align the representations to the decoders input space. ComprExIT instead leverages the hidden states of the context tokens encoded in forward pass. The hidden states across layers are selectively aggregated into token anchors, which are then transmitted to the compression tokens through coordinated transmission plan. since become widely adopted paradigm. Building upon ICAE, 500 (Li et al., 2025b), Activation Beacon (Zhang et al., 2025), and UniGist (Deng et al., 2025a) pass cached keyvalue states to the decoder, together with carefully designed gist-token attention masks, to obtain more informative compressed representations. Deng et al. systematically study the effects of different design components within the encoderdecoder compression framework. EPL (Zhao et al., 2025) further improves compression performance by adjusting the positional encodings of gist tokens. SAC (Liu et al., 2025b) examines the necessity of the auto-encoding training objective, demonstrating that effective compression can be achieved using only text completion objective. xRAG (Cheng et al., 2024) explores converting representations from text embedding models into compression tokens. Different from the existing methods that use the internal self-attention mechanisms of LLMs to perform compression, our method adopts fundamentally different paradigm by decoupling compression from the LLM architecture and formulating it as an explicit information transmission problem over frozen hidden states. 3. Method 3.1. Problem Formulation and Motivation Let = (x1, . . . , xt, . . . , xN ) be an input sequence and fθ an LLM with layers. forward pass of fθ produces hidden states (ℓ) RN d, ℓ {1, . . . , L}, where h(ℓ) denotes the representation of token xt at layer ℓ. Given compression budget (K is the number of compression tokens), prompt compression generates sequence of compact tokens. We denote by = (z1, . . . , zK) the intermediate representations of the compression tokens during the compression process. The final compressed representation is taken at the last layer as = (L). Self-attention in LLM-as-a-compressor. Most existing prompt compression methods convert the LLM itself into compressor by modifying its internal computation during training. Concretely, as shown in Figure 2 (right), they introduce set of compression tokens, denoted as = (z1, . . . , zK), and rely on the self-attention to iteratively aggregate information from context tokens to the compression tokens: (ℓ+1) = Attn(cid:0)Z (ℓ), (ℓ)(cid:1), ℓ = 1, . . . , L, (1) where (0) are initialized either as appended special tokens, interleaved tokens, or selected input tokens. While effective in practice, this formulation inherits fundamental limitations: Challenge I: Layer-wise distribution mismatch. This iterative update of self-attention can induce representation drift across layers, yielding mismatch to the decoder input space. Let p(Z (ℓ)) denote the empirical distribution of compression token representations {z(ℓ) k=1 at layer ℓ. Using Wasserstein distance as distribution mismatch measure M, by its triangle inequality (Clement & Desch, 2008): }K (cid:16) := (cid:16) p(Z (0)), pdec p(Z (L)), pdec (cid:17) (cid:17) L1 (cid:88) (cid:16) + ℓ=0 p(Z (ℓ+1)), p(Z (ℓ)) (cid:17) , (2) where pdec denotes the representation distribution expected by the decoder. This states that the final gap to the decoder input distribution is bounded by the initial gap plus the cumulative layer-wise drift. Thus, even modest per-layer drift can accumulate with depth, enlarging and making compression harder to optimize."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Challenge II: Lack of global allocation. Self-attention adopts independent aggregation strategy between compression tokens: z(ℓ+1) = k,th(ℓ) α(ℓ) , (cid:88) (cid:88) α(ℓ) k,t = 1. (3) In an unrestricted manner, multiple compressed tokens zk may favor the same context positions, which collectivley ignores other places, leading to information loss. Moreover, the attention weights α(ℓ) k,t are allowed to perceive the whole sequence, which often corrupts the semantic order of the original sequence, yielding non-monotonic content that degrades decoder behavior. 3.2. ComprExIT Overview. To address the limitation introduced by selfattention, we propose ComprExIT, framed by new compression paradigm that operates on the hidden states encoded by frozen LLM. We formulate context compression as explicit information transmission over frozen LLM hidden states. The encoded information is first transmitted along the depth direction, selectively propagating hidden states across layers to an intermediate set of token anchors via gating, which directly access representations at corresponding layers. The information then flows along the width direction, from token representations to compression slots, with each slot attending to certain field of tokens. This process is coordinated by global transmission plan that jointly considers all communication paths, preventing redundancy and information loss due to improper allocation. We consider simplicity and light-weight as our design principle, based on the intuition that the hidden states encoded by the LLM already contain high-quality compressed features. Next, we elaborate on our design of the information transmission process. 3.2.1. TOKEN ANCHORS In the depth dimension, we construct token anchors by explicitly gating the flow of information across layers at each token position {1, . . . , }. Specifically, We first perform structural mixture of layer states: coefficients are obtained by normalizing the scores across layers: αt,ℓ = (cid:17) exp (cid:16) st,ℓ τ (cid:16) st,j j=1 exp τ (cid:80)L (cid:17) , (cid:88) ℓ=1 αt,ℓ = 1, (6) where αt,ℓ controls how much information is allowed to flow from layer ℓ at position t. Finally, layer representations are unified and aggregated to form the token anchor: ht = (cid:88) ℓ=1 αt,ℓ Wah(ℓ) , (7) where Wa projects layer-wise representations into shared anchor space. 3.2.2. COMPRESSION SLOTS Given sequence of token anchors aggregated across layers, we next describe how information is transmitted from dense token anchors to coarse compression slots to achieve compression. In specific, we seek transmission plan Π RN , where Πt,k represents the amount of information transmitted from token anchor to compression slot k. + The utility matrix. To obtain transmission plan that globally coordinates all possible information flow paths, we first construct utility matrix Ut,k that quantifies how useful each transmission path (from the uth anchor to the kth slot) is to retain contextual information. We consider each token anchor as sender. We build receiver for each compression slot and assign each receiver to cover local set of senders, thereby preserving the semantic order of the context. We uniformly partition the token anchor sequence into local fields Fk and perform mean aggregation over its field to obtain the representation of each receiver, imposing locality bias when building the receiver: rk = 1 Fk (cid:88) ht. tFk (8) ht = (cid:88) ℓ=1 wℓ h(ℓ) , (cid:88) ℓ=1 wℓ = 1, (4) We then calculate the utility matrix Ut,k by the cosine similarity between the sender and receiver representations transformed with shared linear layer, where wℓ are learned structure priors over layers inspired by Denseformer (Pagliardini et al., 2024). Conditioned on this context, we compute layer-wise gating scores in shared space through linear projections: st,ℓ = (cid:68) Wcht, Wℓh(ℓ) + eℓ (cid:69) , (5) where eℓ is learnable layer embedding to provide signal of the layer index, and indicates dot product. The gating (cid:16) Ut,k = cos Wuht, Wurk (cid:17) . (9) Information capacity. We next consider that tokens can exhibit varying levels of importance relative to their context. Therefore, we learn an information capacity for each sender (at each token anchor), enabling the sender to selectively discard redundant information. This capacity is predicted"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "by simple linear layer followed by normalization: ρt = (cid:16) (cid:17) exp Wρht (cid:16) j=1 exp Wρhj (cid:80)N (cid:17) , = 1, . . . , N. (10) We assign equal capacity to receivers ρk = 1 . This allows certain receivers to connect to important tokens in distance, capturing long-term dependencies that key tokens often involve in. The transmission plan. Given the utility matrix and information capacities defined above, we derive the transmission plan by solving the following optimization problem: min Π0 s.t. (cid:88) (cid:88) t= k=1 Πt,k Ct,k (cid:88) k=1 (cid:88) t= Πt,k = ρt, t, (11) Πt,k = ρk, k, where the cost is defined as Ct,k = 1 Ut,k. The solution of this problem is equivalent to the optimal transport (Villani, 2008) between the senders and the receivers. We solve this problem under an entropy-regularized formulation, which introduces strictly convex entropy term and enables efficient optimization via the Sinkhorn algorithm (Sinkhorn & Knopp, 1967; Cuturi, 2013). As common design choice for stability and efficiency on long sequences, we apply Sinkhorn over fixed-size segments of length . We adopt relatively large segment (e.g., = 128) to preserve broad range of information allocation, while preventing overly aggressive long-range assignments that may disrupt local semantic order. This results in globally coordinated transmission plan that balances comprehensive coverage with ordered information flow. 3.2.3. THE COMPRESSED REPRESENTATIONS Finally, each compression slot aggregates information from token anchors according to the transmission plan, yielding compact sequence of representations that preserves the most informative content of the original context. Specifically, token anchor representations are projected and aggregated as (cid:88) zk = Πt,k Wght. (12) t=1 Before feeding the compression tokens to the decoder, we apply lightweight two-layer MLP for representation alignment, obtaining the final compressed representations: ck = MLP(zk). (13) 5 Overcoming the limitations. We summarize how organized information transmission addresses the limitations of prior compression methods. First, depth-wise transmission aggregates representations from different layers at the same semantic level, mitigating distribution mismatch and preventing progressive overwriting of information. Second, each compression slot is softly allocated to localized field of tokens, which preserves the semantic order of the original sequence. Critically, by solving the optimal transport problem, information allocation during compression is jointly determined under global constraints, avoiding both information loss and redundant compression. Together, these properties enable our method to perform effective and robust compression. 3.3. Training Method We adopt two-phase training procedure under questionIn the first phase, we train the model unaware setting. with next-token prediction (NTP) objective on the pretraining corpus, encouraging the compression module to retain general contextual information that supports fluent and coherent generation. In the second phase, we perform supervised fine-tuning (SFT) on downstream tasks, allowing the model to adapt its information transmission strategy for domain-specific and task-relevant selective compression. 4. Experiments and Analysis 4.1. Setup Models. In the experiemnt, we use Llama-3.2-1B-Base and Llama-3.2-3B-Base (Grattafiori et al., 2024) as the base model. Besides the projection to the decoders input space, we use 256 as the projection hidden size. We use BF16 precision for the base model. We use the Sinkhorn algorithm with 30 iterations to approximately solve the entropy-regularized optimal transport problem. Baselines. We compare with several soft state-of-the-art compression baselines (1) ICAE (Ge et al., 2024): seminal LLM-as-a-compressor method that trains an LLM to encode compressed information into compression tokens. (2) 500 (Li et al., 2025b): introduces an extra design that passes the KV states of the compression tokens to the decoder (3) Activation Beacon (Zhang et al., 2025): introduces the interleaving placement of compression tokens. Except for the base models evaluated under the inference-only setup, all baselines are trained under the same two-phase procedure. We also include three non-compression baselines: (4) Zero-shot [w/ context] prompts the LLM with context, serving as an untrained baseline with zero information loss; (5)Zero-shot [w/o context] prompts the LLM without the context, serving as an untrained baseline with complete information loss; and (6) Prompt-tuning (Lester"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Table 1. Experimental results on six QA benchmark datasets. Best among compression baselines is bold. Our method is colored in The prompt-tuned uncompressed baseline is colored in . . Methods SQuAD NewsQA TriviaQA SearchQA HotpotQA NQ Average EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 Prompt tuning [w/ context] Zero-shot [w/ context] Zero-shot [w/o context] ICAE (Ge et al., 2024) 500x (Li et al., 2025b) Beacon (Zhang et al., 2025) ComprExIT Prompt tuning [w/ context] Zero-shot [w/ context] Zero-shot [w/o context] ICAE (Ge et al., 2024) Beacon (Zhang et al., 2025) 500x (Li et al., 2025b) ComprExIT 71.89 16.59 2. 36.84 4.65 24.88 51.38 80.39 35.85 9.14 48.51 59.81 52.08 59.26 81.09 38.76 8.83 50.21 14.02 39.50 68.08 88.41 55.05 17. 62.53 72.65 65.56 75.68 30.72 9.97 0.97 20.20 1.85 9.12 30.39 35.23 16.33 2.40 27.02 15.46 25.45 35.73 Llama-3.2-1B-Base 61.04 35.50 15.63 57.59 26.09 0.51 64.62 66.75 49.31 21.60 63.54 32.66 2.15 70.93 64.03 5.83 4.04 67.40 17.44 1.90 71. Llama-3.2-3B-Base 71.23 57.52 46.20 68.62 1.36 65.77 72.97 76.30 67.83 52.06 74.13 4.72 71.59 78.86 72.04 29.40 25. 76.43 5.24 69.26 78.00 45.76 26.99 3.11 33.73 5.27 15.74 49.12 50.98 34.89 5.78 43.04 24.39 40.99 55.23 70.64 12.39 6. 74.43 25.59 9.86 78.93 78.42 37.26 30.35 82.89 9.13 76.28 84.40 53.60 22.51 2.76 40.67 5.56 31.23 49.84 55.31 38.94 8. 47.92 51.55 47.28 55.92 69.49 39.24 7.17 57.04 13.44 46.35 68.40 72.65 57.24 16.03 66.16 67.41 65.71 74.15 53.03 18.14 3. 42.26 5.58 21.04 45.93 59.46 32.74 10.31 50.87 45.05 51.32 52.23 66.59 38.90 8.43 58.00 13.89 37.05 63.88 73.32 52.09 18. 66.75 59.56 66.60 68.95 55.72 18.09 4.93 44.16 10.20 14.78 52.34 62.28 35.13 17.02 53.23 29.75 51.86 59.02 66.72 34.27 9. 56.16 17.48 25.11 66.55 73.35 50.73 23.32 65.92 39.64 64.46 72.88 et al., 2021) prepends learnable tokens while retaining full context access. Since compression methods are fine-tuned on downstream tasks to implicitly encode task knowledge into compression slots, prompt-tuning trained in similar way can serve as an upper bound for compression methods. Training and evaluation. We use 1B tokens sampled from the SlimPajama dataset (Shen et al., 2023) for NTP. For SFT, we use MRQA (Fisch et al., 2019) with six indomain and six out-of-domain question-answering datasets spanning diverse categories . We set the compression ratio to 4. All models use context length of 512 tokens in both NTP and SFT. We use F1 and Exact Match (EM) as the metrics. 4.2. Main Results Overall performance. As shown in Table 1, ComprExIT consistently outperforms the compression baselines across diverse QA benchmarks, including extractive, open-domain, and multi-hop question answering datasets, indicating strong generalization across task structures. Remarkably, ComprExIT matches the performance of uncompressed prompt tuning and even surpasses the uncompressed baseline (TriviaQA and SearchQA for the 1B model; NewsQA, TriviaQA, SearchQA, and HotpotQA for the 3B model). This may be because the selection mechanism in ComprExIT filters out distracting context, which is particularly beneficial for these information-retrievaloriented datasets. Moreover, we observe that on datasets with less noise (SQuAD, NewsQA), the performance gains are larger, suggesting that ComprExIT better preserves fine-grained information by mitigating distribution shift in the feature representations. Larger performance gain also occurs on the multi-hop QA (HotpotQA), where global allocation jointly mitigates duplicated compression capacity usage and enables the retention of multiple supporting facts. Performance on smaller models. When scaling down from 3B to 1B backbones, 500X and Beacon exhibit severe degradation, whereas ComprExIT remains stable across datasets. We attribute this gap to stronger distribution mismatch in prior methods. Specifically, 500x and Beacon directly inject the compression tokens cached keyvalue (KV) states into the decoder. While KV injection preserves more internal features, it also more intrusively couples the layer representation space of the compressor and the decoder, leaving the decoder less flexibility to absorb representation drift. As result, these methods become more dependent on the size of the decoder. In contrast, ComprExIT provides cleaner and better-aligned representations to the input space of the decoder in non-intrusive manner, allowing more room for the decoder to digest the distribution shift, yielding stable performance even with smaller backbones. Out-of-domain performance. We evaluate the generalization ability of different compression methods under two complementary out-of-domain (OOD) settings. First, we report results on six OOD QA datasets from MRQA in Table 5 (in the Appendix due to page limit), where the model is evaluated on unseen data distributions. We observe that ComprExIT consistently outperforms other compression baselines across all datasets in the OOD setup, except for marginal gap to ICAE on RelationExtraction. Since the average context length of RelationExtraction is only 30 and ICAE and 500x have fixed number (128) of compression tokens, they share overly sufficient compression bandwidth on this dataset, which may result in the performance gap."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Table 2. Experimental results with NTP pretraining only, without finetuning with in-domain data."
        },
        {
            "title": "Methods",
            "content": "Llama-3.2-1B Llama-3.2-3B Avg. EM Avg. F1 Avg. EM Avg. F1 ICAE Beacon 500x ComprExIT 7.59 0.90 2.36 13.22 16.00 2.72 8.17 25. 15.71 1.70 17.83 25.18 25.93 4.36 29.52 38.09 Table 3. Ablation study of the key design choices in ComprExIT. We train the ablation models in the same procedure with Llama3.2-1B. Red arrows indicate performance drops compared to the full model."
        },
        {
            "title": "Ablations",
            "content": "Avg. EM Avg. F1 w/o Coordinated Allocation 47.69 4.65 w/o Layer Aggregation 61.94 4.61 36.12 16.22 49.35 17."
        },
        {
            "title": "ComprExIT",
            "content": "52.34 66.55 Figure 3. Pearson correlation between compression slots/tokens aggregation distributions. For each compression slot, we treat its normalized aggregation weights over input tokens (i.e., the attention weights in ICAE (left) or the transmission plan in ComprExIT (right)) as vector, and compute pairwise Pearson correlation between these vectors across slots. Large off-diagonal values indicate that different slots aggregate highly overlapping subsets of tokens, reflecting duplicated allocation behavior. As shown, ICAE exhibits substantial off-diagonal correlations with large high correlation areas (marked out in the figure), whereas ComprExIT maintains low inter-slot correlation, indicating more coordinated information allocation. Compared to in-domain results, the performance gains of ComprExIT over prior methods become larger, indicating stronger robustness and generalizability. Second, we consider more challenging OOD setup in terms of training signals, where all methods are trained solely with the nexttoken prediction (NTP) objective and evaluated on downstream QA tasks without any supervised fine-tuning. As shown in Table 2, ComprExIT consistently outperforms all baselines across both model scales. Notably, Beacon and 500x exhibit near-random performance with the 1B backbone, suggesting severe optimization difficulty under weak supervision. In contrast, ComprExIT maintains strong performance, demonstrating effective zero-shot compression capability. These results indicate that ComprExIT does not rely on task-specific supervision, but instead learns transferable and well-aligned compression representations directly from generic language modeling objectives. 4.3. Ablation Studies Performances by replacing the current design choice with its ablations are shown in Table 3. To isolate the effect of coordinated allocation, we replace width-wise transmission with window attention module that preserves locality but removes global allocation. We can observe from the table that performance drops consistently across all datasets, indicating that global allocation is critical for effective compression. Next, we remove depth-wise transmission and directly use the final-layer LLM representations, matching prior compression methods. This leads to substantial performance drop, confirming the importance of explicit layer-wise aggregation for preserving diverse semantic features and avoiding distribution shift from late-layer representations. 4.4. Further Analysis 4.4.1. COORDINATED ALLOCATION To further understand the impact cast by coordinated allocation, we analyze the aggregation behaviors of compression tokens. Figure 3 visualizes the Pearson correlation between the aggregation distributions of the compression tokens. In the absence of globally coordinated allocation mechanism, ICAE exhibits substantial off-diagonal correlations, indicating that many gist tokens attend to highly overlapping subsets of input tokens. This suggests duplicated aggregation behaviors and inefficient usage of the limited compression bandwidth. In contrast, ComprExIT maintains consistently low inter-slot correlations, implying that different compression tokens absorb complementary information rather than redundantly focusing on the same content. We further examine the allocation matrix via singular value decomposition in Figure 4. Compared with ComprExIT, ICAE shows much steeper singular value decay, resulting in significantly lower effective rank (27.16 vs. 38.37). This indicates that ICAEs aggregation patterns lie in lower-dimensional subspace, despite using more compression tokens for short contexts. Together, these results suggest that without global allocation, compression tokens tend to collapse into correlated and low-rank behaviors, whereas ComprExIT promotes more coordinated, and information-efficient allocation across slots. 4.4.2. IMPACTS OF LAYER SELECTION Figure 5 illustrates the gating scores controlling the depthwise information flow across layers. First, we observe that most gating mass is allocated to early and middle layers"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "ferent compression methods. It is observed that ComprExIT consistently performs faster convergence and reaches lower loss plateau than baselines methods. Moreover, ComprExIT also shows the lowest initial loss. These suggest that our method yields representations that are better aligned with the decoder even at the beginning of training, and is much easier to optimize than the existing LLM-as-acompressor methods. We attribute this to the proposed paradigm that narrows the distribution shift and the coordinated allocation mechanism that provides high-quality compression representations for the decoder. We also find that 500x reaches lower loss plateau on the 3B model, probably due to the improved decoding capability of the larger model, which can better mitigate the distribution gap from the compressor. (a) Llama-3.2-1B (b) Llama-3.2-3B Figure 6. The training curves of baseline methods and ComprExIT under the next-token prediction task. 5. Conclusion and Future Work This work presents ComprExIT, new paradigm for soft context compression by formulating compression as explicit information transmission over frozen LLM hidden states. ComprExIT performs coordinated depth-wise and width-wise information transmission for compression. By decoupling compression from the iterative self-attention dynamics used in prior LLM-as-a-compressor methods, our approach mitigates distribution shift caused by progressive representation overwriting and enables more controllable aggregation of contextual information. Experiments show that our method consistently outperforms prior compression methods. While ComprExIT shows performance gains over existing approaches, our study is limited to relatively smaller LLMs with 1B to 3B parameters, relatively short context length (512) and fixed compression ratio (x4) due to our limited computational resources. Future work could explore scaling to larger models with various compression ratios and context lengths, and investigate our approach on other model families. More broadly, the proposed paradigm provides flexible design space for context compression beyond standard attention-based formulations. Therefore, we hope that this work can inspire future research to explore more design choices in this new paradigm to facilitate the development of the context compression field. Figure 4. Singular value spectrum (normalized) and effective rank (erank in the figure) of the aggregation matrix of compression tokens, where compression slot has vector of aggregation weights over input tokens. The spectrum shows much lower effective rank of ICAEs aggregation matrix compared with ComprExIT, indicating much higher allocation redundancies in compression. (before layer 10), while later layers are consistently suppressed. This suggests that later layers may be less suitable for constructing compact, decoder-friendly representations, as they mainly encode highly abstract features specialized for generation and thus contain less useful information to retain. Moreover, we observe clear layer preference across tokens: important entity tokens tend to select representations from middle layers, whereas less informative tokens more often favor earlier layers. plausible explanation is that middlelayer hidden states encode richer contextual and relational information, which is particularly salient for entity tokens that participate in long-range dependencies, while earlylayer representations mainly capture local or lexical features sufficient for less important tokens. This indicates that ComprExIT can selectively leverage contextual features at different depths, leading to higher-quality compression representations. Figure 5. Depth-wise gating weights across layers in ComprExIT. The weights represent ComprExITs preference for layers at the position of each input token. 4.5. Optimization Behavior We plot next-token prediction (NTP) training curves in Figure 6 to better understand the optimization behavior of dif-"
        },
        {
            "title": "Impact Statement",
            "content": "Our work focuses on improving LLMs efficiency in processing long-context input through explicit and coordinated soft compression. By reducing the computational and memory cost of handling long documents, ComprExIT has the potential to make LLMs more accessible in low-resource settings. However, as with all compression methods, there is risk that important information may be omitted, which could introduce bias or misinterpretation for downstream tasks. As such, it is important to explore ways of improving the faithfulness of compressed representations, ensuring that efficiency gains do not come at the cost of reliability."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the UK Engineering and Physical Sciences Research Council through Turing AI Fellowship (grant no. EP/V020579/1, EP/V020579/2) and Prosperity Partnership scheme (grant no. UKRI566)."
        },
        {
            "title": "References",
            "content": "Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:1107911091, 2022. Cao, Y.-H., Wang, Y., Hao, S., Li, Z., Zhan, C., Liu, S., and Hu, Y.-Q. Efpc: Towards efficient and flexible prompt compression. arXiv preprint arXiv:2503.07956, 2025. Cheng, X., Wang, X., Zhang, X., Ge, T., Chen, S.-Q., Wei, F., Zhang, H., and Zhao, D. xRAG: Extreme context compression for retrieval-augmented generation with one token. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=6pTlXqrO0p. Chevalier, A., Wettig, A., Ajith, A., and Chen, D. In Adapting language models to compress contexts. Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 232. URL https://aclanthology.org/2023. emnlp-main.232/. Clement, P. and Desch, W. An elementary proof of the triangle inequality for the wasserstein metric. Proceedings of the American Mathematical Society, 136(1):333339, 2008. Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. Deng, C., Zhang, Z., Mao, K., Li, S., Fang, T., Zhang, H., Mi, H., Yu, D., and Dou, Z. Unigist: Towards general and hardware-aligned sequence-level long context compression. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. URL https: //openreview.net/forum?id=1C4mXyh31p. Deng, C., Zhang, Z., Mao, K., Li, S., Huang, X., Yu, D., and Dou, Z. silver bullet or compromise for full attention? comprehensive study of gist tokenbased context compression. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 48614879, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.241. URL https: //aclanthology.org/2025.acl-long.241/. Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., and Chen, D. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., and Chen, D. (eds.), Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 113, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. URL https: //aclanthology.org/D19-5801/. Ge, T., Jing, H., Wang, L., Wang, X., Chen, S.-Q., and Wei, F. In-context autoencoder for context compresIn The Twelfth Insion in large language model. ternational Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=uREj4ZuGJE. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., and et al. The llama 3 herd of models, 2024. URL https:// arxiv.org/abs/2407.21783. Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. LLMLingua: Compressing prompts for accelerated inference of large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1335813376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.825. URL https://aclanthology. org/2023.emnlp-main.825/. Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "1: Long Papers), pp. 16581677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.91. URL https: //aclanthology.org/2024.acl-long.91/. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 243. URL https://aclanthology.org/2021. emnlp-main.243/. Li, Y., Dong, B., Guerin, F., and Lin, C. Compressing context to enhance inference efficiency of large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 63426353, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 391. URL https://aclanthology.org/2023. emnlp-main.391/. Li, Z., Liu, Y., Su, Y., and Collier, N. Prompt compression for large language models: survey. In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7182 7195, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. ISBN 979-8-89176189-6. URL https://aclanthology.org/2025. naacl-long.368/. Li, Z., Su, Y., and Collier, N. 500xCompressor: Generalized prompt compression for large language models. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2508125091, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long. 1219. URL https://aclanthology.org/2025. acl-long.1219/. Liu, J., Zhu, D., Bai, Z., He, Y., Liao, H., Que, H., Wang, Z., Zhang, C., Zhang, G., Zhang, J., et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025a. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of 10 the Association for Computational Linguistics, 12:157 173, 2024. doi: 10.1162/tacl 00638. URL https: //aclanthology.org/2024.tacl-1.9/. Liu, X., Zhao, R., Huang, P., Liu, X., Xiao, J., Xiao, C., Xiao, T., Gao, S., Yu, Z., and Zhu, J. Autoencodingfree context compression for llms via contextual semantic anchors. arXiv preprint arXiv:2510.08907, 2025b. Mu, J., Li, X. L., and Goodman, N. Learning to comIn Thirty-seventh press prompts with gist tokens. Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=2DtxPCL3T5. Pagliardini, M., Mohtashami, A., Fleuret, F., and Jaggi, M. Denseformer: Enhancing information flow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=kMnoh7CXrq. Pan, Z., Wu, Q., Jiang, H., Xia, M., Luo, X., Zhang, J., Lin, Q., Ruhle, V., Yang, Y., Lin, C.-Y., Zhao, H. V., Qiu, L., and Zhang, D. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 963981, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 57. URL https://aclanthology.org/2024. findings-acl.57/. Shen, Z., Tao, T., Ma, L., Neiswanger, W., Liu, Z., Wang, H., Tan, B., Hestness, J., Vassilieva, N., Soboleva, D., et al. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. Sinkhorn, R. and Knopp, P. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of Mathematics, 21(2):343348, 1967. Villani, C."
        },
        {
            "title": "Optimal",
            "content": "Old and new. 2008. URL https://api.semanticscholar. org/CorpusID:118347220. transport: Wingate, D., Shoeybi, M., and Sorensen, T. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 56215634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp. 412. URL https://aclanthology.org/2022. findings-emnlp.412/."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Zhang, N., Yao, Y., Tian, B., Wang, P., Deng, S., Wang, M., Xi, Z., Mao, S., Zhang, J., Ni, Y., et al. comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024. Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Long context compression with activation beacon. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=1eQT9OzfNQ. Zhao, R., Liu, X., Liu, X., Huang, P., Xiao, C., Xiao, T., and Zhu, J. Position IDs matter: An enhanced position layout for efficient context compression in large language models. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2025, pp. 1771517734, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-335-7. doi: 10.18653/v1/2025.findings-emnlp. 962. URL https://aclanthology.org/2025. findings-emnlp.962/."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "A. Appendix A.1. Datasets Table 4 shows the statistics of the datasets we use in the experiments. Table 4. Statistics of the training and evaluation datasets used, including in-domain and out-of-domain datasets. #Train represents the number of training samples and #Validation represents the number of validation samples."
        },
        {
            "title": "Average Context Length",
            "content": "#Train #Validation"
        },
        {
            "title": "BioASQ\nDROP\nDuoRC\nRACE\nRelationExtraction\nTextbookQA",
            "content": "11 8 16 17 22 9 11 11 9 12 9 11 137 599 784 749 232 153 248 243 681 349 30 657 86,588 74,160 61,688 117,384 72,928 104,071 10,507 4,212 7,785 16,980 5,904 12,836 1,518 1,501 1,503 1,502 1,500 1,508 A.2. Model Inference Implementation We carefully deal with the position of the padding tokens, by which we find bring improvements to all methods. We keep the no-padding setup during NTP to fully make use of the utility of the GPUs. In SFT, as we stick to the question independent setup, we have to encode the contexts first and feed the context compression tokens along with the question tokens to the decoder. For efficient batch compression, we first apply left-padding to the context tokens first for the compressors. We then concatenate the compression tokens with padded question and answer tokens, and perform an operation to move all the padding tokens to the left most side according to the attention mask. We also create the shifted new attention mask and labels accordingly. This ensure that the generation start from non-padding token and there is no position gap between the compression tokens and the questions. A.3. More Experimental Results A.3.1. MORE OUT-OF-DOMAIN RESULTS A.3.2. FULL NTP RESULTS The full results of the methods trained with only the NTP objective (without SFT) are presented in Table 6. ComprExIT outperforms the baseline methods on all datasets, showing that ComprExIT is capable of learning generalizable compression representations using the general training objective. A.3.3. THE ALLOCATION MATRIX Figure 7b visualizes the learned width-wise transmission plan in ComprExIT. Compared with existing methods (e.g., ICAE in Figure 7a), ComprExIT exhibits more structured allocation pattern: each compression slot is softly anchored to contiguous local region of input tokens, while still retaining non-negligible connections to distant tokens. This enables ComprExIT to preserve the semantic order of the context without sacrificing the ability to capture long-range dependencies. In contrast, although ICAE tries to maintain an order bias (there is diagonal pattern of high attention values in Figure fig:width-icae), its compression-token attention patterns are weakly localized. Moreover, it can be observed that some important tokens (e.g. entity Marcus, number 4.) are ignored collectively by most of the compression tokens, but some entities (e.g. Broncos) are overly focused. We attribute these observations to the lack of coordinated allocation."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "(a) ICAE (b) ComprExIT Figure 7. Last-layer attention heatmap of gist tokens produced by ICAE (left) and width-wise information transmission plan in ComprExIT (right). Figure 8. F1 scores on single-hop and multi-hop QA datasets (SQuAD and HotpotQA) using different layers from Llama-3.2-1B (16 layers) for compression. We use simple mean-pooling to obtain compression representations."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Table 5. Experimental results on six out-of-domain MRQA benchmark datasets. Out-of-domain: these datasets are not included in the training data. Best among compression baselines is bold. Our method is colored in . The prompt-tuned uncompressed baseline is colored in . Note that ICAE and 500x have smaller compression ratio on samples of context length shorter than 512 because they are trained with fixed set (128) of compression tokens. Particularly, on RelationExtraction dataset whose average context length is only 30, they share overly sufficient compression bandwith. Methods RelationExtraction BioASQ TextbookQA DuoRC DROP RACE Average EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 Prompt tuning [w/ context] Zero-shot [w/ context] Zero-shot [w/o context] ICAE (Ge et al., 2024) 500x (Li et al., 2025b) Beacon (Zhang et al., 2025) ComprExIT Prompt tuning [w/ context] Zero-shot [w/ context] Zero-shot [w/o context] ICAE (Ge et al., 2024) 500x (Li et al., 2025b) Beacon (Zhang et al., 2025) ComprExIT 69.91 27.48 5.09 62.28 6.31 33.18 59.46 77.37 54.24 14. 70.66 65.13 58.92 65.31 81.15 47.87 10.33 75.97 16.30 48.82 74.96 86.70 68.46 21.03 82.10 79.67 71.62 77.25 Llama-3.2-1B-Base 78.80 28.84 20.56 54.09 35.52 47.84 63.74 52.10 18.16 11.11 28.21 16.77 5.92 41.65 59.61 29.97 16.03 34.60 24.40 10.34 50. Llama-3.2-3B-Base 81.50 64.31 25.76 67.21 60.82 66.86 73.87 60.48 36.46 23.29 42.25 29.41 10.98 56.09 68.75 46.66 28. 50.70 38.10 15.68 64.50 38.51 17.46 0.60 12.59 1.07 2.40 28.78 45.50 28.31 2.07 23.65 22.25 4.73 34.31 65.96 6.98 14. 42.62 22.54 32.25 48.40 67.62 43.22 20.94 55.85 47.67 54.52 59.64 47.40 30.54 2.55 18.80 3.91 4.90 38.77 55.63 42.36 4. 32.81 30.91 7.95 44.98 32.54 18.43 14.84 24.15 8.18 21.82 29.01 48.84 23.69 16.17 35.33 29.74 39.39 43.58 43.14 32.30 19. 33.23 18.32 29.24 39.86 57.18 37.45 21.26 44.89 39.12 50.55 53.16 33.68 10.53 0.45 9.50 1.93 15.88 23.00 43.62 22.70 1. 21.51 19.14 30.71 31.60 46.30 22.71 2.88 16.95 5.70 25.35 37.95 56.35 38.16 5.65 33.85 30.58 43.95 47.69 48.78 16.51 7. 29.89 9.47 18.57 38.38 57.24 34.77 13.09 41.54 35.56 33.21 47.19 59.40 32.71 12.01 38.94 17.36 27.75 51.00 67.68 49.57 17. 51.93 46.53 42.77 59.34 Table 6. Experimental results (in-domain) for methods trained only with the next token prediction (NTP) objective."
        },
        {
            "title": "HotpotQA",
            "content": "NQ"
        },
        {
            "title": "Average",
            "content": "EM F1 EM F1 EM EM F1 EM F1 EM EM F1 Llama-3.2-1B ICAE 500x Beacon 3.89 1.69 1.53 13.90 9.67 5. ComprExIT 11.25 26.55 ICAE 500x Beacon 8.97 12.81 3.01 20.63 25.32 8.30 1.80 0.71 0. 6.43 4.27 4.80 0.59 6.84 3.63 1.10 21.40 7.44 0.00 32.30 17.02 0.22 18. 34.99 47.50 8.95 0.62 0.01 3.51 16.68 2.86 0.11 5.34 1.51 1. 13.55 7.21 3.23 4.18 2.22 2.55 12.74 8.60 6.26 7.59 2.37 0.90 16.00 8.17 2.72 8. 12.86 27.03 10.27 24.61 13.22 25. Llama-3.2-3B 12.19 13.22 2.30 44.23 42.51 0.12 54.53 53.66 0.78 10.81 9.24 0.13 16.53 15.54 0. 14.42 19.12 2.68 27.39 34.99 5.85 11.55 18.53 3.68 24.31 34.38 8.33 15.71 17.83 1.70 25.93 29.52 4. ComprExIT 18.25 34.32 10.57 23.55 52.95 63. 28.75 36.31 23.74 39.61 16.81 31. 25.18 38.09 A.3.4. PRELIMINARY STUDY OF LAYER IMPACTS Besides, we also conduct preliminary study to examine how representations from different LLM layers affect compression quality. At this stage, we adopt mean pooling for aggregation and evaluate on SQuAD and HotpotQA, using two-layer MLP to map compressed representations to the decoder input space. As shown in Figure 8, we observe clear layer-dependent behavior: middle-layer representations perform better on single-hop QA, while early and late layers are more beneficial for multi-hop QA. We also observe that replacing mean pooling with attention-based pooling yields substantial performance gain for middle layers, suggesting that mean pooling introduces an over-smoothing effect that flattens semantically salient anchor states in middle layers. These findings inspired us to design the selective depth-wise transmission mechanism and dynamically assign transmission budget to the tokens. Besides, we also find that the representations from the last layer consistently yields poor results, which may degrade compression representations generated by the previous layers in the existing LLM-as-a-compressor methods."
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "A.4. Training A.4.1. TRAINING DETAILS Tables 7 to 9 present the hyperparameters we use when training ComprExIT and the baseline methods. Table 7. NTP Training configuration for ComprExIT. Item Run & setup Dataset Number of Tokens Device Precision Sequence & compression Context length Generation length Compression ratio Optimal-Transport (OT) OT window size OT iterations OT projection dimension Layerwise gate hidden size Optimization Learning rate Warmup ratio Max grad norm Batch size (per device) Gradient accumulation steps Actual Batch Size Epochs ComprExIT SlimPajama-6B 1 Billion 4 NVIDIA-A100 Bfloat16 512 128 128 30 256 256 1 104 0.05 20.0 16 32 2048"
        },
        {
            "title": "Context Compression via Explicit Information Transmission",
            "content": "Table 8. NTP Training configuration for ICAE and 500x. Item ICAE 500x Run & setup Dataset Number of Tokens Device Precision SlimPajama-6B 1 Billion 4 NVIDIA-A100 Bfloat Sequence & compression Context length Generation length Compression ratio Memory Number of memory tokens Optimization Learning rate Warmup ratio Max grad norm Batch size (per device) Gradient accumulation steps Actual Batch Size Epochs 512 128 4 128 3 105 0.05 20.0 16 16 2048 LoRA (compressor) Enabled Rank Alpha α Dropout Bias Task type True 128 32 0.05 none CAUSAL LM SlimPajama-6B 1 Billion 4 NVIDIA-A100 Bfloat16 512 128 4 128 3 105 0.05 20.0 32 32 2048 True 128 32 0.05 none CAUSAL LM Table 9. NTP Training configuration for Activation Beacon. Item Run & setup Dataset Number of Tokens Device Precision Sequence Context length Generation length Activation Beacon configuration Beacon enabled Beacon window / stride Beacon ratio Beacon attention Attend previous beacons Trainable beacon params Beacon position Grouping by stride Optimization Learning rate Max grad norm Batch size (per device) Gradient accumulation steps Actual Batch Size Epochs Activation Beacon SlimPajama-6B 1 Billion 4 NVIDIA-A100 Bfloat16 512 128 True 64 / 64 4 full-coverage True q, k, interleave strict 1 104 20.0 16 32"
        }
    ],
    "affiliations": [
        "Imperial College London, UK",
        "Kings College London, UK",
        "The Alan Turing Institute, UK",
        "Tsinghua University, China"
    ]
}