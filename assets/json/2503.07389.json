{
    "paper_title": "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models",
    "authors": [
        "Ruidong Chen",
        "Honglin Guo",
        "Lanjun Wang",
        "Chenyu Zhang",
        "Weizhi Nie",
        "An-An Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 8 3 7 0 . 3 0 5 2 : r TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models Ruidong Chen1, Honglin Guo1, Lanjun Wang2, Chenyu Zhang2, Weizhi Nie1, An-An Liu1 1The School of Electrical and Information Engineering, Tianjin University 2The School of New Media and Communication, Tianjin University Figure 1. We propose TRCE to achieve reliable malicious concept erasure in text-to-image diffusion models. Left ) TRCE effectively erases malicious concepts (e.g., sexual) , even under adversarial prompts [7, 42, 44, 48]. Right ) Compared to previous methods [10 12, 19, 21, 22, 38, 45], TRCE reaches an outperform trade-off between erasure effectiveness and knowledge preservation, effectively removing malicious concepts while minimizing the impact on the irrelevant content."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the models normal generation capability. To address this challenge, our study proposes TRCE, using two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying critical mapping objective(i.e., the [EoT] embedding), we optimize the crossattention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of Corresponding author: Lanjun Wang{wang.lanjun}@outlook.com, An-An Liu{anan0422}@gmail.com the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the models original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material. 1. Introduction Recently, large-scale text-to-image (T2I) diffusion models (DMs) [9, 28, 34, 36] have attracted widespread attention due to their capabilities to generate highly realistic images. Trained on extensive internet-sourced data, these models acquire the ability to produce diverse visual concepts from textual prompts. However, due to the existence of toxic training data, these models also learn to generate inappropriate content. Consequently, they may be misused to produce NSFW (Not Safe For Work) images with prompts containing inappropriate concepts. To address these safety 1 concerns, researchers have proposed various mechanisms for enhancing safety, such as filtering out toxic data and retraining the models [33], employing safety checkers to filter outputs [30], and applying safeguard strategies to guide the generation process [38, 45]. To further enhance safety, concept erasure (CE) [1012, 17, 19, 21, 22, 46] has been proposed to limit the ability of DMs to generate specific concepts. For example, if the concept sexual were erased by CE, when the prompt includes keywords such as nudity, erotic, porn, etc., the model will generate images of clothed people instead. However, due to insufficient erasure, current CE methods are still not effective enough in preventing the generation of NSFW content [7, 42, 44, 48]. Existing methods are typically designed to eliminate specific keywords by finetuning [10, 17, 19, 21, 22, 46] or editing [11, 12, 21] model parameters. As result, they struggle to erase malicious concepts, as they are often expressed metaphorically or associatively, without using keywords (e.g., the prompt in the second row of Fig. 2). In most scenarios, the semantics of malicious concepts are implicitly embedded in the prompts, which poses challenges for erasing malicious concepts. To eliminate such implicit semantics, existing methods often degrade the models original generation ability on irrelevant content to achieve stronger erasure reliability (Fig. 1 Right). Overall, the inability to balance reliable concept erasure with knowledge preservation constitutes the main limitation that prevents current CE methods from being effectively applied to eliminate malicious concepts. To address these limitations, this study proposes TRCE, which is based on two-stage erasure design, aiming to ensure both reliable erasure and knowledge preservation through the cooperation of the two stages. Firstly, the TRCE begins with the Textual Semantic Erasure (Sec. 4.1), mitigating the influence of malicious semantics from the input prompt on the generation process. Recognizing the influence of special embeddings [8, 29] on generation (as illustrated in Fig. 2), we note that the [EoT] (End of Text) embeddings capture semantics from the entire prompt and significantly contribute to the generation of salient regions [6]. By leveraging this insight, we can move away from existing methods [11, 12, 21] that directly map keyword embeddings Instead, we select [EoT] as the mapto unrelated ones. ping objective and employ closed-form solution [11, 12] to modify the cross-attention matrices. Mapping prompts with malicious concepts to contextually similar prompts but with safe concepts, thus achieving the effective erasure of implicitly embedded malicious semantics. On the basis of the first stage, TRCE further proposes Denoising Trajectory Steering (Sec. 4.2) to avoid the final generation of malicious visual content. Typically, during the denoising process, diffusion model first forms outlines of prominent regions and then produces specific visual Figure 2. Due to the attention mechanism, special embeddings [8, 29] of input prompts (e.g. [EoT]) carry rich semantics about concepts and pay attention to the semantics of salient regions [18]. By eliminating the influence of the [EoT] token on generation (i.e., setting the attention map corresponding to [EoT] to 0), the image content will be dramatically affected. details at the turning point [31]. Based on this behavior, at an early sampling stage of generation, we propose using reference model to provide both safe and unsafe predictions before the turning point. Specifically, contrastive loss is employed to steer the denoising prediction toward the safe direction and away from the unsafe one. As shown in Fig. 4, take advantage of the deterministic properties [41] of diffusion models sampling trajectory, since the malicious semantics of generation guidance have been mostly eliminated in the first stage, by just adjusting the models early denoising, the models sampling trajectory can deviate from unsafe visual patterns without significantly affecting the image content, thus achieving better knowledge preservation ability for the concept erasure. We summarize the contribution of TRCE as follows: TRCE proposes collaborated two-stage erasure strategy, achieving reliable malicious concept erasure while taking minimalist effects on the models generation ability. TRCE identifies critical mapping objective for textual semantic erasure, specifically the [EoT] embeddings, which achieves effective erasure of the implicitly embedded malicious semantics. TRCE proposes an effective denoising trajectory steering strategy, which optimizes the models early denoising through contrastive fine-tuning. We conduct comprehensive evaluations of TRCE across various malicious content generation scenarios, including network prompts [38], adversarial prompts [7, 42, 44, 48], and multi-concept erasure [38], along with ablation studies. The results demonstrate the effectiveness of TRCE in conducting reliable malicious concept erasure. 2. Related Work 2.1. Concept Erasure To prevent diffusion models from being misused for generating unsafe content, recent research has begun exploring 2 methods to remove unsafe concepts from the model. These methods can be categorized into two types: inference-time guidance [38, 45], and concept erasure [1012, 17, 19, 21, 22, 46]. Compared to inference-time guidance, concept erasure typically involves optimizing model parameters to mitigate the models ability to generate specific concepts, providing better performance in preventing unsafe content. In early research related to this task, ESD [10] and CA [17] first proposed using the models knowledge to erase their concepts, i.e., fine-tuning the model with specific concept prompts to align its noise predictions with those of unrelated concepts. To improve erasure efficiency, SPM [22] and MACE [21] suggest fine-tuning only small adapter [16] to perform concept erasure. Considering the critical role of cross-attention [2, 43] in concept generation within denoising networks [15]. UCE [11] first introduced closedform solution to directly optimize the attention matrix of the cross-attention layers, mapping concept keywords to unrelated concepts, which achieves fast erasure with only seconds per concept. Building upon UCE, RECE [12] introduces an iterative erasure process based on UCE [11] to enhance the robustness of concept erasure, which is the first study to discuss enhancing the robustness of concept erasure facing adversarial prompts [7, 42, 44, 48], however, it only improves the erasure robustness to limited extent. In this work, we propose TRCE, which aims to further improve the reliable of concept erasure while better preserving the models existing knowledge. 2.2. Red-Teaming Tools With the rapid development of generative models, recent studies propose red-teaming tools [5, 27, 39, 40] for discovering security vulnerabilities of models. Based on whether the attack accesses the internal mechanisms of the diffusion model, existing attack methods can be categorized into model-aware [7, 48] and model-agnostic [42, 44] approaches. Representative model-aware attack methods include P4D [7] and UnlearnDiff [48]. These approaches typically target malicious image by leveraging an alignment loss to optimize multiple token embeddings, which are subsequently combined into an adversarial prompt. In contrast, MMA [44] and Ring-a-Bell [42] are model-agnostic methods. Instead of directly interacting with T2I models, they rely solely on an open-source CLIP [29] text encoder to optimize adversarial prompts by an alignment loss with malicious prompts within the CLIP feature space. By mining the relevant adversarial prompts that lead to the generation of inappropriate content, these methods can identify vulnerabilities overlooked by concept erasure models, thereby guiding these models to regenerate malicious content. To address this challenge, TRCE aims to further enhance the reliability of the concept erasure, ensuring sufficient robustness even with adversarial prompts. 3. Preliminaries This work focuses on erasing concepts in latent diffusion models (LDMs) [32, 34], which have emerged as powerful tools for generating high-quality images. In this section, we introduce the preliminaries of LDMs, and relevant components we used to conduct erasure. Cross-attention Layers: In LDMs, an image x0 is first mapped into latent representation z0 using an encoder E. The model then utilizes denoising U-Net [35], denoted as ϵθ, to learn the denoising process [15] in the latent space. To generate specific visual concepts, pivotal aspect of LDMs is their ability to generate images conditioned on textual information. This is achieved through CLIP [29] text encoder τθ, which transforms text prompt into the embedding = τθ(y) = {eSoT , ep l2}, where SoT and EoT are special tokens of start and end of the text [8, 29] respectively, is the maximum size processed by τθ, and is the size of words in the prompt. is then incorporated into the generation via cross-attention mechanism: l1, eEoT 0 0, ..., ep , ..., eEoT Attn(Q, K, ) = Softmax (cid:18) QK (cid:19) V, (1) where = WQφ(zt), = WK e, = WV e, and φ(zt) represent the hidden states in the U-Net at the diffusion step with the latent variable zt. Classifier-free Guidance: To enhance the fidelity and controllability of generated images, LDMs employ the classifier-free guidance (c.f.g) mechanism [14, 23]. This approach involves training the model to predict noise in both conditioned (on the text prompt y) and unconditioned (on the null text ) scenarios, and the training objective becomes: Lc.f.g. = Ez0,y,ϵ,t Ez0,y,ϵ,t (cid:104) (cid:104) ϵθ (zt, y, t) ϵ2(cid:105) ϵθ (zt, , t) ϵ2(cid:105) , with (1 p), , with p. (2) where zt represents the latent feature at the time step obtained from z0 by applying the noise ϵ. The model is trained with probability to discard condition y, but trained unconditionally. Through training in this strategy, c.f.g can direct the model inference by adjusting the noise prediction to strengthen the influence of the text prompt y, this process can be formulated as follows: ϵθ(zt, y, t) = ϵθ(zt, t) + α (ϵθ(zt, y, t) ϵθ(zt, t)) (3) where α indicates the guidance strength. Finally, with this guidance, the inference process begins with random Gaussian noise zT and iteratively denoises it over steps using the adjusted noise prediction ϵθ to approximate the latent code ˆz0. The final image ˆx is then reconstructed by passing ˆz0 through the image decoder D(). 3 Figure 3. The overall framework of proposed TRCE, which involves two-stage model refinement to conduct reliable malicious concept erasure. a) Textual Semantic Erasure (Sec. 4.1): In the first stage, we refine the Key and Value matrices = {WK , WV } of crossattention layers to eliminate the textual semantics of specific concepts embedded in input prompts via closed-form solution. b) Denoising Trajectory Steering (Sec. 4.2): In the second stage, the first-stage refined U-Net is then fine-tuned to steer the early denoising prediction toward the safe direction while away from the unsafe one, thereby further avoiding the generation of malicious visual content. 4. Method As shown in Fig. 3, TRCE consists of two stages, Textual Semantic Erasure (Sec. 4.1) and Denoising Trajectory Steering (Sec. 4.2), to reliably erase malicious concepts in T2I diffusion models. In this section, we introduce the detailed methodologies of these two stages. 4.1. Textual Semantic Erasure In this stage, TRCE starts by eliminating the influence of malicious semantics from input prompts. We apply closed-form cross-attention refinement [11], which is widely used in editing knowledge in attention-based networks [1, 4, 11, 12, 21, 26]. In these studies, the Key and Value projection matrices WK and WV of the crossattention layers are adjusted to map the concept embeddings {ef i=1 (e.g. map the word nudity to the null text ). During this process, unrelated embeddings {ep j=1 have to remain unaffected for knowledge preservation. The objective function is formulated as: i=1 into the target embeddings {et }m }n i}n min (cid:88) i=1 (cid:13) (cid:13)W ef (cid:13) et (cid:13) 2 (cid:13) (cid:13) +η (cid:88) j=1 (cid:13) (cid:13)W ep ep (cid:13) 2 (cid:13) (4) where = {W } indicates the refined projection matrices, and η controls the balance between erasure ability and prior preservation. K, In this study, we enhance the strategy of applying Eq. 4 to improve the erasure effect while maintaining prior preservation by figuring out more efficient mapping objective. The detailed analysis is as follows. 4 Role of special embeddings: As shown in previous studies related to T2I diffusion models [6, 18, 21], the role of special embeddings in image generation can be summarized as: The embedding of [SoT] contributes most significantly to visual content generation, particularly influencing the overall composition [6]. Modifying the embedding of [SoT] leads to rapid changes in generated content [18]. The embedding of [EoT] focuses on salient regions and carries the semantics of the overall prompt [6, 18]. Modifying [EoT] leads to changes in image content while preserving the general context of the prompt [18]. The keyword embeddings [KEY] of specific concepts (e.g., those represent nudity) carry less semantics from prompt context and usually exhibit distinct semantics of the concepts. Consequently, extensive remapping of these embeddings results in rapid knowledge forgetting [11]. To sum up, based on the above analysis, TRCE optimizes only [EoT] embeddings to enhance the effectiveness of erasure, while avoiding the image quality degradation that results from optimizing [SoT] or keyword embeddings. This approach allows TRCE to target the erasure of specific concept-related semantics while preserving the context of the entire prompt. As result, it improves both the erasure effectiveness and the knowledge preservation. Closed-form refinement: Given pre-trained diffusion UNet ϵθ, we aim to eliminate the semantics of the malicious concept cm to obtain refined U-Net ˆϵθ. To achieve this, firstly, as shown in Fig. 3 (a), we conduct concept augmentation [17, 21] via Large Language Models (LLMs) [25] to list synonyms of cm and their opposite safe concept cs, and apply them to various prompt templates, which 1, ps 1 , pm 2, . . . , ps 2 , . . . , pm are to present the concept in diverse visual contexts. Following this step, we obtain an erasure prompt set = } with malicious concept cm, and obtain {pm the target prompt set = {ps } with opposite safe concept cs (e.g., person wearing clothes is an opposite concept of nudity). Following the same way, we obtain preservation prompt set for preserving the knowledge, which follows the same settings as in previous work [21]. Using the text encoder τθ to extract the embeddings for prompts in m, s, and k, since multiple [EoT] embeddings carry similar information [6, 18], we can optimize only the first [EoT] token in each prompt. Finally, the embeddings for optimizing are denoted as }q }n {em j=0. According to Eq. 4, we refine the related attention matrices in ϵθ, and this optimization objective yields closed-form solution: i=0, and {ek i=0, {es }n (cid:88) = es (em ) + η ek (cid:0)ek (cid:1) (cid:88) j=1 i=1 em (em ) + η 1 (cid:0)ek ek (cid:1) . (cid:88) j=1 (cid:88) i=1 (5) After conducting this refinement, we obtain refined matrices = {W } to update the pre-trained ϵθ into our aimed ˆϵθ, which gains the ability to eliminate malicious semantics from the textual input. K, 4.2. Denoising Trajectory Steering To further enhance erasure while avoid degrading the generation ability, in the second stage, TRCE further fine-tunes the models early denoising prediction, steering the diffusion sampling trajectory toward safer content generation. Fig. 4 illustrates the main idea of steering the denoising trajectory for eliminating malicious visual content. Typically, diffusion sampling initially generates the general outline of the image. At turning point in the mid-sampling, the model begins to generate details of specific visual concepts [31]. Building on this observation, we fine-tune the first-stage refined model ˆϵθ, steering its denoising prediction before this turning point. In this stage, to ignore the dependency of textual input, we only fine-tune the visual layers (self-attention layers and matrices of cross-attention layers) of ˆϵθ, aiming to obtain the final safe model ˆϵ θ Trajectory preparation: Leveraging the original U-Net ϵθ with the malicious prompts used in Sec. 4.1, we cache the early sampling trajectories of model inference. Each trajectory is represented as {zm 1, . . . } , where is the maximum timestep, we empirically set the initial 50% steps to be randomly selected for fine-tuning. Additionally, we generate set of unconditional sampling trajectories {zu t=0 (with null text ) for the regularization term. } = {zm , zm }T Figure 4. Based on the deterministic property of ODE trajectories in diffusion model sampling [41], the denoising trajectory can be simply steered by modifying single denoising prediction in the early denoising stage. Guidance Enhancement: Given zm sampled from cached trajectory, we aim to steer its both conditional and unconditional denoising predictions ˆϵθ(zp , c, t), = cm or to safe direction while away from unsafe one. To model these directions, using reference U-Net ϵθ (original version), we leverage Eq. 3 to construct semantically-enhanced denoising prediction of cm and cs: funsaf = ϵθ(zm , , t) + β(ϵθ(zm , cm, t) ϵθ(zm fsaf = ϵθ(zm , , t) + β(ϵθ(zm , cs, t) ϵθ(zm , , t)), (6) , , t)), (7) the β indicates guidance scale, the funsaf and fsaf indicates strengthened unsafe and safe predictions respectively. Fine-tuning objectives: We use standard triplet margin loss [3] as the contrastive function, which is written as: Lerase =E[max(ˆϵθ(zm , c, t) fsafe2 ˆϵθ(zm , c, t) funsafe2 2 + margin, 0)]. (8) This objective function makes the current denoising predictions more biased towards the safe direction while steering away from the unsafe direction, with margin being the margin value, constraining the optimization direction to be more inclined towards fsaf e. Additionally, an alignment of unconditional predictions is used as regularization term to ensure the models original generation abilities remain unaffected. This optimization objective is formulated as: Lpreserve = ˆϵθ(zu , , t) ϵθ(zu , , t)2 2. (9) To preserve the models prediction throughout the entire sampling process, we apply this regularization term using uniformly sampled from [0, ]. Finally, the overall finetuning objective is optimized as Lerase + λLpreserve, and the λ is used for balancing the erasure and prior preservation ability. After this fine-tuning, we obtain the final model ˆϵ θ, which obtains more reliable erasure performance to mitigate the generation of malicious visual content. 5 Method SD1.4 [32] SLD [38] Safree [45] ESD [10] SPM [22] UCE [11] SafeGEN [19] RECE [12] MACE [21] TRCE(T) TRCE(V) TRCE(T+V) User Prompt I2P [38] Adversairal Prompt Knowledge Preservation MMA [44] P4D [7] Ring [42] UnDiff [47] FID CLIP-S 34.69% 17.29% 10.20% 31.15% 12.57% 8.16% 11.39% 6.34% 7.09% 5.05% 13.86% 1.29% 79.00% 64.30% 44.60% 58.50% 67.40% 30.80% 0.40% 23.10% 10.60% 7.80% 35.00% 1.40% 84.00% 64.00% 50.00% 82.67% 64.00% 44.00% 8.70% 32.00% 8.00% 8.00% 48.00% 2.00% 59.49% 34.18% 22.38% 50.63% 31.65% 13.92% 13.75% 6.33% 10.13% 11.39% 26.76% 1.27% 57.75% 47.18% 30.00% 77.46% 35.21% 19.72% 21.98% 15.49% 11.27% 11.97% 35.00% 0.70% - 16.04 13.26 12.18 10.50 13.64 11.77 14.21 17.44 11.94 11.03 12.08 30.97 29.78 26.79 31.21 30.85 30.92 31.06 30.79 28.84 30.69 31.04 30.71 Table 1. The Attack Success Rate (ASR) and knowledge preserving ability with current concept erasure methods in erasing unsafe concept sexual. The ASR is measured by NudeNet [24] with threshold of 0.45 [48]. The FID [13] scores are measured by comparing generated images with the default SD V1.4 output to verify the knowledge preservation ability. TRCE (T) and TRCE (V) refer to the application of the first and second stages, respectively, while TRCE (T+V) indicates the application of both. 5. Experiment In this section, we evaluate the effectiveness of TRCE through series of experiments. Following the setup of the previous works, we use SD V1.4 [32] as the base model, evaluate the malicious concept erasure ability on sexual content erasure (Sec. 5.2) and multi malicious concept erasure (Sec. 5.3) tasks. For comparison methods, we use their officially provided implementations for evaluation. 5.1. Implementation Details We implement all experiments with the Diffusers library and generate images with DDIM scheduler [41] with 30 steps. For the first stage, we use GPT-4-o [25] to enhance the concept keyword into 20 synonyms and apply them to 15 prompt templates, counting to 300 prompts for closedform refinement. The η in Eq. 4 is set to 0.01 by default. For the second stage, guidance scale β and preservation scale λ are set to 15 and 100. For sexual/multi-concept erasure tasks, we generate 100 and 300 trajectory samples for fine-tuning over 3 epochs, using the Adam optimizer with learning rate of 1e-6. The fine-tuning process takes approximately 300 seconds on single RTX 4090 GPU. Please refer to Appendix A.3 for more detailed implementation. 5.2. Sexual Content Erasure In this section, we evaluate the erasure ability of TRCE and baselines with the unsafe concept sexual, which has been widely studied with red-team attack methods [7, 42, 44, 48]. Evaluation benchmark. Following previous work [45], we evaluate the safeguard ability of malicious concept erasure methods against both network-sourced user prompts and adversarial prompts. For user prompts, we use the I2P [38] (Inappropriate Image Prompts) dataset and evaluate 931 prompts tagged with sexual. For adversarial prompts, we use four adversarial prompts datasets generated by red-teaming tools: MMA-diffusion (MMA) [44], Prompt4Debugging (P4D) [7], Ring-A-Bell (Ring) [42], UnlearnDiff (UnDiff) [48]. Evaluation metrics. We adopt Attack Success Rate (ASR) to measure the safeguard ability on malicious content. For the sexual erasure task, we utilize the NudeNet [24] detector to identify whether images contain nude body parts. We set the threshold of the detector to 0.45 following [48] to obtain higher sensor sensitivity. For the multi-concept erasure task, we utilize the Q16 detector [38] to identify whether images contain malicious content. To verify the knowledge preservation ability, we use prompts in COCO [20] to generate models using default SD V1.4 and evaluate FID [13] with 3000 samples generated by each method to measure whether concept erasure maintains the generation ability of the original model. Additionally, we use CLIP-Score [29] to measure the text-image consistency. Result analysis. Quantitative experimental results are shown in Table 1. In terms of erasure effectiveness, the proposed TRCE achieves the best performance. When eliminating only textual semantics, TRCE (T) already demonstrates strong erasure ability by identifying more appropriate optimization target (semantics in the [EoT]). When the denoising trajectory offset strategy is applied alone, the performance TRCE (V) is relatively poor. This is because even if the early denoising trajectory is steered, the malicious semantic from the prompt still leads to sexual content in the later stages of denoising. Therefore, by combining the two stages, TRCE (T+V) can achieve significantly stronger erasure robustness Additionally, we present some 6 Method SD1.4 [32] SLD* [38] ESD* [10] UCE* [11] RECE* [12] Safree [45] MACE [21] TRCE (T) TRCE (V) TRCE (T+V) Hate Harassment Violence 21.2% 41.1% 3.5% 10.8% 4.3% 4.8% 1.7% 3.9% 10.4% 0.9% 40.1% 19.7% 16.7% 23.3% 14.2% 21.0% 6.9% 5.2% 12.2% 3.0% 19.7% 20.1% 6.4% 12.1% 6.1% 9.8% 4.5% 3.4% 7.3% 2.2% Self-harm Sexual 54.5% 22.9% 16.4% 16.2% 8.6% 5.3% 4.9% 1.7% 18.6% 1.5% 35.5% 19.2% 11.1% 12.9% 8.5% 15.4% 4.5% 3.8% 9.8% 2.6% Shocking 42.1% 16.0% 16.1% 19.2% 9.7% 33.0% 4.2% 4.0% 6.7% 3.6% Illegal Activity Overall - 35.6% - 35.6% - 12.2% - 15.6 % - 8.5% 13.72 8.8% 56.33 5.8% 12.07 3.6% 12.0% 10.72 2.0% 12.11 19.4% 19.4% 6.3% 9.8% 6.1% 10.9% 7.1% 3.6% 3.6% 2.0% 30.97 - - - - 26.57 21.66 30.43 31.02 30. FID CLIP-S Table 2. The evaluation results of erasing multiple malicious concept in I2P benchmark [38]. We report the inappropriate rate detected by the Q16 detector [37]. The results of * tagged works are sourced from [12]. Figure 5. The visualization of current methods against adversarial prompt [7]. For more visualization results, please refer to Appendix of the generated cases for TRCE and comparison methods in Fig. 5. It can be observed that TRCE effectively erases unsafe semantics while preserving the overall context of the input prompt, without significantly affecting unrelated areas, This indicates that TRCE more finely erase malicious concepts, thus better preserving the generation ability. 5.3. Multi Malicious Concept Erasure To further demonstrate the effectiveness of TRCE erasing larger categories of explicit content, following the experiment settings with [12], we experiment erasing all toxic concepts in the I2P dataset [38], which includes: hate, harassment, violence, self-harm, sexual, shocking and illegal activity. The results are reported in Table 2, since we cannot access the experimental settings of previous works erasing multiple explicit concepts, the results of some previous works are sourced from [12] for comparison. The experimental results demonstrate that TRCE achieves optimal erasure performance even when only the first stage is applied. Furthermore, the second stage enables more sufficient erasure of multiple malicious concepts, resulting in better erasure performance. Moreover, it is worth noting that we observe that existing methods [21, 45] lead to significant impact on the general generation ability when simultaneously erase multiple malicious concepts. In conFigure 6. The visualization demonstrates the erasure ability of TRCE to erase multiple malicious concepts from I2P [38]. We blur images that contain offensive content for safety concerns. trast, TRCE demonstrates good knowledge preservation even with multi-concept erasure, which greatly enhances TRCEs practical applicability to serve as safeguard to preventing malicious content. 5.4. Module Analysis In this section, we conduct ablation studies to verify the effectiveness of our proposed key components using the same experimental settings introduced in Sec. 5.2. We report the average ASR within four adversarial prompt benchmarks as the metric Adv in this part. 7 η Method I2P Adv FID CLIP-S Refined Embeddings I2P Adv FID CLIP-S 0.002 0.005 0. 0.02 TRCE (T) TRCE (T+V) 3.54% 4.91% 13.87 0.64% 0.50% 14.16 TRCE (T) TRCE (T+V) 4.73% 6.92% 12.82 1.29% 1.20% 12.99 TRCE (T) TRCE (T+V) 5.05% 9.79% 11.94 1.29% 1.33% 12.08 TRCE (T) TRCE (T+V) 7.20% 13.52% 11.57 1.50% 2.36% 11.67 30.24 30.34 30.40 30.49 30.69 30. 30.78 30.90 [EoT] [KEY] [EoT]+[KEY] [EoT]+[SoT] [EoT]+[SoT]+[KEY] 5.05% 9.79% 11.94 22.8% 53.09% 11.32 6.34% 10.27% 12.34 6.56% 11.22% 12.31 5.38% 11.34% 12.29 30.69 30.71 30.35 30.36 30.43 Table 4. The quantitative ablation results in optimizing different tokens for textual semantic erasure (Sec. 4.1). The knowledge preservation rate is set to η = 0.001 by default. Table 3. The quantitative ablation in the performance of textual semantic erasure and further applying denoising trajectory steering in different knowledge preservation rate η. Effectiveness of two-stage design. The key idea of TRCE is to collaborate textual semantic erasure (TRCE (T)) and denoising trajectory steering (TRCE (V)) to achieve reliable malicious concept erasure. As shown in Table 3, we analyze the effect of applying the TRCE (V) with the TRCE (T) under the different knowledge preservation rates η. As discussed in Sec. 4.2, it can be observed that the performance of TRCE (T) is highly sensitive to the η, whereas TRCE (V) consistently improves concept erasure capability with small sacrifice in knowledge preservation. Notably, at higher η values, TRCE (V) demonstrates more substantial enhancement in defending adversarial prompt robustness. Overall, the role of TRCE (V) is to enhance the erasure by modifying the denoising predictions when TRCE (T) struggles to balance the erasure intensity. At the same time, since TRCE (T) can eliminate the influence of most malicious semantics on the generation process, this makes it necessary factor for the effectiveness of TRCE (V). Finally, the collaboration of the two stages enables more effective concept erasure while achieving better preservation of model knowledge. Effectiveness of optimizing [EoT]. Table 4 shows the results on optimizing different tokens in textual semantic erasure. As shown in the table, the model struggles to prevent unsafe content when only optimizing [KEY]. This is because [KEY] carries less semantic information than [EoT] due to the attention mechanism. Among all the evaluation results, only optimizing [EoT] embedding achieves the best erasure performance while achieving favorable knowledge preservation ability. However, fine-tuning [SoT] and [KEY] together decreases both erasure and knowledge preservation abilities, which demonstrates the effectiveness of the proposed strategy of only optimizing [EoT]. For more analysis of [EoT], please refer to Appendix B. Effectiveness of proposed loss functions. Table 5 shows the ablation results on applying different loss functions for the second-stage fine-tuning. The table clearly shows that Lpreserve plays crucial role in maintaining the models Lpreserve Lerase I2P Adv FID CLIP-S 1.93% 2.29% 12.09 0.21% 0.27% 14.64 1.29% 1.33% 12. 30.70 30.44 30.71 Table 5. The quantitative ablation results on applying different fine-tuning objectives for denoising trajectory steering (Sec. 4.2). The setting without Lerase indicates that we only apply safe guidance alignment rather than contrastive learning. original generation abilities. While removing Lpreserve can accelerate the convergence of fine-tuning, it significantly compromises the quality of the generated images. For contrastive loss Lerase applied for concept erasure, compared with just aligning using safe prediction, the contrastive learning strategy enhances the identification of malicious semantics in the denoising predictions. This provides clearer guidance for steering the denoising trajectory, thereby effectively enhancing the effectiveness of concept erasure. In the supplementary materials, we provide ablation studies on concept augmentation (in Sec. 4.1) and guidance enhancement (in Sec. 4.2). Please refer to Appendix for more detailed information. 6. Conclusion This paper proposes TRCE, which leverages the cooperation of two-stage erasing to achieve reliable malicious concept erasure. In the first stage, by identifying the [EoT] embedding as critical mapping objective, TRCE eliminates the textual semantics by refining the model parameter via closed-form solution, which effectively eliminates the influence of malicious semantics from the generation process. In the second stage, TRCE further fine-tunes the early denoising prediction of diffusion models. Steering the sampling trajectory towards safe directions through contrastive learning. Finally, we conduct comprehensive evaluations of TRCE on multiple benchmarks. Results confirm that TRCE exhibits reliable malicious concept erasure while better preserving the models original generation ability."
        },
        {
            "title": "References",
            "content": "[1] Dana Arad, Hadas Orgad, and Yonatan Belinkov. Refact: Updating text-to-image models by editing the text encoder. arXiv preprint arXiv:2306.00738, 2023. 4 [2] Dzmitry Bahdanau. jointly learning to align and translate. arXiv:1409.0473, 2014. 3 Neural machine translation by arXiv preprint [3] Vassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Learning local feature descriptors with triplets and shallow convolutional neural networks. In Bmvc, page 3, 2016. 5 [4] Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, and Varun Manjunatha. Localizing and editing knowledge in text-to-image generative models. In The Twelfth International Conference on Learning Representations, 2023. [5] Kai Chen, Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang. Gcma: Generative cross-modal transferable In Proceedings adversarial attacks from images to videos. of the 31st ACM International Conference on Multimedia, pages 698708, 2023. 3 [6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free In Proceedlayout control with cross-attention guidance. ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 53435353, 2024. 2, 4, 5 [7] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, PinYu Chen, and Wei-Chen Chiu. Prompting4debugging: Redteaming text-to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135, 2023. 1, 2, 3, 6, 7 [8] Jacob Devlin. Bert: Pre-training of deep bidirectional arXiv preprint transformers for language understanding. arXiv:1810.04805, 2018. 2, 3 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 [10] Rohit Gandikota, Jaden FiottoKaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 24262436, 2023. 1, 2, 3, 6, 7 Joanna Materzynska, [11] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 51115120, 2024. 2, 3, 4, 6, 7 [12] Chao Gong, Kai Chen, Zhipeng Wei, Jingjing Chen, and YuGang Jiang. Reliable and efficient concept erasure of text-toimage diffusion models. arXiv preprint arXiv:2407.12383, 2024. 1, 2, 3, 4, 6, 7 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, 1 [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [17] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2269122702, 2023. 2, 3, 4 [18] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Get Image content supwhat you want, not what you dont: pression for text-to-image diffusion models. arXiv preprint arXiv:2402.05375, 2024. 2, 4, 5 [19] Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, and Wenyuan Xu. Safegen: Mitigating unsafe content generation in text-to-image models. arXiv preprint arXiv:2404.06666, 2024. 1, 2, 3, [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6, 5 [21] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6430 6440, 2024. 1, 2, 3, 4, 5, 6, 7 [22] Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75597568, 2024. 1, 2, 3, 6 [23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [24] notAI tech. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019. 6, [25] Openai. Gpt-4o system card, 2024. 4, 6, 1 [26] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 70537061, 2023. 4 [27] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks. arXiv preprint arXiv:2309.17410, 2023. 3 [42] Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, JiaYou Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept removal methods for diffusion models? arXiv preprint arXiv:2310.10012, 2023. 1, 2, 3, 6, 4 [43] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [44] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Tsung-Yi Ho, Nan Xu, and Qiang Xu. MMA-Diffusion: MultiModal In Proceedings of the IEEE Attack on Diffusion Models. Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 2, 3, 6, [45] Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, and Mohit Bansal. Safree: Training-free and adaptive guard for safe text-to-image and video generation. arXiv preprint arXiv:2410.12761, 2024. 1, 2, 3, 6, 7 [46] Gong Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Forget-me-not: Learning to forget and Humphrey Shi. In Proceedings of the in text-to-image diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17551764, 2024. 2, 3 [47] Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, and Sijia Liu. Defensive unlearning with adversarial training for robust concept erasure in diffusion models. arXiv preprint arXiv:2405.15234, 2024. 6 [48] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. European Conference on Computer Vision (ECCV), 2024. 1, 2, 3, 6, 4 [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 6, [30] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram`er. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. 2 [31] Gabriel Raya and Luca Ambrogioni. Spontaneous symmetry breaking in generative diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 5 [32] R. Rombach. Stable diffusion v1-4 model card. model card, 2022. 3, 6, 7 [33] R. Rombach. Stable diffusion 2.0 release, 2022. 2 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3 [35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [37] Patrick Schramowski, Christopher Tauchmann, and Kristian Kersting. Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content? In Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022. 7 [38] Patrick Schramowski, Manuel Brack, Bjorn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2252222531, 2023. 1, 2, 3, 6, 7, 4 [39] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multimodal language models. In The Twelfth International Conference on Learning Representations, 2023. 3 [40] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844, 2023. 3 [41] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 5, 6, and Stefano Ermon. arXiv preprint 10 TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is organized as follows: In Sec. A, we provide more detailed implementation of experiments. Sec. offers additional discussions about the rationale for optimizing the [EoT] embedding in the first-stage TRCE. Sec. presents extended ablation studies to verify the effectiveness of the components in both stages of TRCE. In Sec. D, we test the copy-right protected ability of TRCE through an art-style erasure task Finally, Sec. showcases additional visualization results. A. Implementation Details A.1. Evaluation Benchmarks. For evaluating sexual content erasure 5.2, we adopt network-sourced I2P benchmark and four adversarial prompts benchmarks generated by read-teaming tools: I2P [38]: contains 4703 unsafe prompts related to multiple toxic concepts: hate, harassment, violence, self-harm, sexual, shocking, illegal activity. In the main text, we follow the settings of most previous works to evaluate the sexual content. We use the 931 prompts tagged with sexual in the dataset for evaluation. MMA-Diffusion [44]: This red-teaming framework uses both textual and visual information to bypass the security mechanisms of the T2I model. We use the officially released 1,000 adversarial prompts related to nudity. P4D [7]: This work employs prompt engineering to generate problematic prompts with T2I models. We use their officially released P4D-N-16 dataset that contains 151 adversarial prompts. Ring-A-Bell [42]: This is model-agnostic framework that uses the text encoder to generate adversarial prompts. Adopting the experiment setting of previous work [12, 45], we use the dataset version of 79 prompts produced with the unsafe concept of nudity in this work UnlearnDiffAtk [48]: This method leverages the classification ability of diffusion models to generate adversarial prompts that lead to images being classified under the nudity concept. We use their officially provided 142 prompts for evaluation. A.2. Evaluation Metrics. In Sec. 5.2, we evaluate the ability of methods to remove the nudity concept using the Attack Success Rate (ASR), while FID [13] and CLIP-Score [29] are used to assess the models capability to preserve knowledge. The detailed calculation methods are as follows: ASR: For generated images, we use the NudeNet [24] detector to identify whether they contain exposed body If any detected region exceeds the probability parts. threshold of 0.45, the image is considered failure case. FID [13]: It measures the distribution difference of generated images between original models and concept-erased models, which is formulated as: (cid:17) (cid:16) Σx + Σg 2(cid:112)ΣxΣg FID(x, g) = µx µg + Tr , (10) where indicates the feature distribution of images generated by the original model, and indicates that of the concept-erased models. Using prompts from the COCO30k dataset, we compute the FID between 3k images generated by the concept-erased models on the validation set and the full set of images generated by the original model. CLIP-Score [29]: This metric evaluates the models ability to generate images matching text descriptions based on the similarity between CLIP embeddings of generated images and input text. Same as FID, we evaluate this metric using prompts from the COCO-30k dataset. A.3. Implementation Details of TRCE In this part, we detail the implementations of the proposed two stages of TRCE: Textual Semantic Erasure (Sec. 4.1) and Denoising Trajectory Steering (Sec. 4.2) Textual Semantic Erasure. In this stage, the knowledge preservation rate η is set to 0.01 by default to achieve the optimal balance between erasure ability and knowledge preservation. To conduct concept augmentation. We use GPT-4-o [25] to list synonyms of specific concept(s) using the following prompt: am currently building filter word/phrase sets for inappropriate/copyrighted content for image generation. For the concepts {KEYWORDS}, provide me with list of {NUMBER} filter words/phrases. For erasing sexual and multiple malicious concepts, we list 20 and 40 synonyms, respectively. We apply those synonyms to 15 prompt templates (same templates as [21]) to build prompts for closed-form refinement. Denoising Trajectory Steering. In this stage, for preparing the early sampling steps of concepts, we use the original SD v1.4 model to generate 100 samples with the DDIM scheduler [41] (300 samples for erasing multiple malicious concepts). Each sample contains 50 intermediate latent representations of each time step. For the regularization term, we Figure 7. a) The visualization of how [EoT] embeddings carry the implicit concept nudity embedded in prompt photo of woman/man without clothes, zeroing all attention maps of [EoT] embeddings lead to the erasure of nudity while maintaining woman/man. b) The visualization of similarities between all embeddings in single prompt, which illustrates all [EoT] embeddings carry similar semantics. prepare 2000 samples generated with the null text , and they are applied to all experiments. The guidance strength β and prior preservation weight λ are set to 15 and 100 by default. During fine-tuning, we use uniformly sampled timesteps from 0 to 25 of 50 DDIM steps. The margin in Lerase is set to 0.01. We use the Adam optimizer to finetune the visual layers (e.g. self-attention layers and query matrices in cross-attention layers) at learning rate of 1e-6, with 3 epochs. This process costs about 300 seconds using single RTX 4090 GPU. I2P Adv FID CLIP-S 1 2 5 10 20 50 19.87% 57.38% 10.90 19.76% 48.05% 11.06 14.61% 30.87% 11.17 10.85% 21.72% 11.58 9.79% 11.94 5.05% 7.46% 12.74 5.80% 30.99 30.97 30.87 30.75 30.69 30.06 Table 6. The ablation results in number of synonyms. B. Extended Discussion on Optimizing [EoT] I2P Adv FID CLIP-S In the Sec. 4.1, we introduce the [EoT] embedding as an effective optimization object for the textual semantic erasure. This is motivated by the observation of Fig. 2 that the [EoT] embeddings carry rich information and contribute most to image generation. [EoT] embeddings carry implicit concepts embedded in prompts. As shown in Fig. 7 (a), we use simple case photo of woman/man without clothes to present how the concept nudity is implicitly embedded in the prompt. For the prompt embeddings, we denote the number of its [EoT] embeddings as K. Following the same approach as in Fig. 2, we gradually zero the cross-attention maps corresponding to different numbers of [EoT] embeddings to observe their effects on image generation. The results show that when all [EoT] maps are eliminated, the prompts semantics primarily retain the woman/man while excluding the implicit embedded nudity, finally generating woman/man in clothes. However, leaving just 12 [EoT] embeddings is sufficient to reintroduce nudity into the generated results. This indicates that, under the attention mechanism, the [EoT] tokens obtain implicit semantics representing key attributes of an image from the prompt words. Erasing them can effectively avoid insufficiency erasure caused 1 2 5 10 15 30 25.03% 50.91% 10.57 22.66% 45.07% 10.73 16.65% 32.33% 11.18 7.30% 16.97% 11.49 9.79% 11.94 5.05% 9.82% 12.34 5.48% 30.94 30.92 30.92 30.68 30.69 30.45 Table 7. The ablation results in number of prompt templates. by only erasing concept keywords. Rationale of only optimizing the first [EoT]. As illustrated in Fig. 7 (b), all [EoT] embeddings exhibit similar semantics within prompt. Therefore, to improve computational efficiency, we can use only the first [EoT] embedding of each prompt as the optimized item for closed-form refinement. C. Extended Ablation Studies In this section, we conduct ablation studies on the key components in the two stages of TRCE. The experimental settings and evaluation metrics are consistent with the module analysis part in the main text. (Sec. 5.4) β 3 5 10 15 20 I2P Adv FID CLIP-S 3.11% 4.94% 12.05 2.32% 3.16% 12.04 1.93% 2.81% 12.13 1.29% 1.33% 12.08 1.49% 2.32% 12.17 30.73 30.74 30.72 30.71 30.73 Table 8. The effectiveness of the guidance scale for guidance enhancement applied in Sec. 4.2. C.1. Effectiveness of Concept Augmentation As described in Sec. 4.1 of the main text, we perform concept augmentation by listing synonyms of concept keywords and applying them to diverse prompt templates to create varied visual contexts. To examine how the number of synonyms and prompt templates impacts erasure performance, we conduct ablation studies. The results are presented in Table 6 and Table 7. It is important to note that the optimization term for closed-form refinement has been normalized, thereby eliminating the effect of the number of prompts on optimization. The results indicate that an appropriate number of concept augmentations can simultaneously enhance both concept erasure and knowledge preservation. However, an excessively high number can adversely affect these abilities. Based on these findings, we select 20 synonyms and 15 prompt templates to achieve the best performance. C.2. Effectiveness of Guidance Enhancement As introduced in Sec. 4.2, when fine-tuning the early denoising prediction, we apply guidance enhancement to leverage the paradigm of classifier-free guidance [14] to provide discriminative training objectives. The effect of the selection of guidance scale is shown in Table. 8. It can be seen that selecting reasonable guidance intensity has an important role in learning the distinctive semantic features of malicious features. We ultimately choose β = 15 as the optimal setting for guidance intensity. D. Artistic Style Removal For evaluating the artistic style erasure ability, we follow the experiment settings from previous works [1012] using two benchmarks: Famous Artists (Erase Van Gogh) and Modern Artists (Erase Kelly Mckernan). Each dataset contains 20 prompts per artist style. We follow the previous works to measure the erasure ability of target artists (Van Gogh and Kelly Mckernan) while measuring the preservation ability of unrelated styles. Evaluation benchmark. We follow the setting from previous works [1012] to erase the style of artists Van Gogh and Kelly McKernan, evaluating whether the methods can 3 Figure 8. The visualization of artistic style erasure comparison. TRCE is able to effectively remove target styles while better preserving the details of the original image and prompt. Method SD V1.4 [32] ESD [10] UCE [11] RECE [12] TRCE (Ours) Van Goah Kelly Mckernan Acce Accu Acce Accu 0.95 0.15 0.90 0.35 0.20 0.95 0.67 0.88 0.83 0.85 0.90 0.25 0.75 0.30 0. 0.93 0.70 0.85 0.80 0.85 Table 9. Comparison of artist style erasure task. The metric with SD V1.4 are reported for performance reference. erase the target artistic style while retaining others. GPT as style judger: Given the subjective nature of artistic styles, followed by [45], we employ an advanced multimodal large language model, GPT-4-o, as the judger to determine whether an image belongs to specific artistic style. Result analysis. From the quantitative results illustrated in Table. 9, TRCE achieves favorable Acce while effectively preserving un-related art styles (as the evaluation performance of Accu). In Fig. 8, we demonstrate the effect of erasing Van Gogh. We find that the advantage of TRCE in style erasure task is that it can erase the targeted style while preserving the original content and composition of the image, while better refer the prompts instructions for generating the image content. This also indicates that TRCE better preserves the models original ability to prevent unrelated content from being influenced. E. More Visualization Results In this part, we showcase more visualization results. In Fig. 9, we display erasure comparison through different benchmarks [38, 42, 44, 48]. And Fig. 10 provides visualization results of knowledge preservation comparison. Figure 9. The visualization of the erasure ability of current methods on I2P [38], Ring-A-Bell [42], MMA-Diffusion [44] and UnlearnDiff [48] datasets. TRCE achieves reliable sexual concept erasing while maintaining the overall visual context of generated images. 4 Figure 10. The visualization of knowledge preservation ability to generate general images [20]. TRCE better preserves the generation of general images and exhibits strong knowledge preservation ability."
        }
    ],
    "affiliations": [
        "The School of Electrical and Information Engineering, Tianjin University",
        "The School of New Media and Communication, Tianjin University"
    ]
}