{
    "paper_title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "authors": [
        "Haolei Xu",
        "Yuchen Yan",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Guiyang Hou",
        "Shengpei Jiang",
        "Kaitao Song",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 2 4 8 6 4 1 . 5 0 5 2 : r Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning Haolei Xu1 Yuchen Yan1 Yongliang Shen1 Wenqi Zhang1 Guiyang Hou1 Shengpei Jiang2 Kaitao Song3 Weiming Lu1 Jun Xiao1 Yueting Zhuang 1 Zhejiang University 2 The Chinese University of Hong Kong 3 Microsoft Research Asia {xuhaolei,syl,luwm}@zju.edu.cn Project: https://zju-real.github.io/CoT-Bridge"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [13] have demonstrated significant performance improvements through Chain-of-Thought (CoT) reasoning [4], particularly on complex tasks such as mathematics and coding [5]. CoT guides models to generate intermediate reasoning steps, simulating the human process of step-by-step problem solving, thereby enhancing both solution accuracy and interpretability [6]. The quality of these reasoning chains directly impacts model performance, serving as critical foundation for advanced reasoning capabilities. Despite substantial progress, we identify prevalent but understudied phenomenon in CoT datasets[7 11]: Thought Leap. This refers to instances where one or more intermediate reasoning steps are omitted between adjacent steps, creating cognitive gaps in the reasoning chain. Unlike factual errors or answer inaccuracies that have been extensively studied, Thought Leap specifically concerns the completeness of reasoning structures. Figure 1 (a) illustrates this phenomenon through mathematical problem where critical bridging steps (highlighted in gold) are missing in the original CoT. Without Equal contribution. Corresponding Author. Preprint. Under review. Figure 1: Overview of the Thought Leap phenomenon and our bridging approach. (a) Thought Leaps in CoT; (b) Negative impact on training; (c) Bridging leaps improves reasoning performance. these bridges, models struggle to follow how the pigeonhole principle applies to dice probability and where the value \"15\" comes fromgaps that require implicit knowledge that may be obvious to experts but create significant barriers for models during learning. Thought Leaps arise naturally in CoT datasets sourced from educational materials and expert demonstrations. Human experts often omit steps they consider trivial based on their extensive background knowledge, unconsciously creating gaps that can impede effective learning. This phenomenon is particularly problematic for both human learners and LLMs that lack the implicit knowledge necessary to bridge these gaps. Our preliminary investigations reveal the detrimental effects of Thought Leap on model performance. By systematically introducing varying degrees of step omissions in the MetaMathQA [7] dataset, from mild to severe, we demonstrate that Thought Leaps significantly undermine training effectiveness. As shown in Figure 1 (b), models trained on datasets with Thought Leaps exhibit substantially lower performance ceilings (up to 27.83% reduction in accuracy for severe leaps, see details in Appendix C) and slower convergence rates compared to those trained on complete reasoning chains. These findings align with recent research suggesting that structural disruptions in reasoning chains can be more harmful than factual inaccuracies [12, 13]. To address this challenge, we propose the CoT Thought Leap Bridge Task, which aims to automatically detect Thought Leaps and generate the necessary intermediate reasoning steps to restore coherence and completeness in CoT demonstrations. As illustrated in Figure 1 (a), our approach effectively transforms incomplete reasoning chains into coherent step-by-step solutions by inserting appropriate bridging content, allowing learners to follow the complete reasoning process. Our approach involves creating specialized dataset (ScaleQM+) for the Thought Leap Bridge Task by systematically removing intermediate steps from the structured ScaleQuestMath [14] dataset and pairing incomplete reasoning chains with their complete counterparts. We then develop CoT-Bridge, fine-tuned model based on Qwen2.5-Math-7B [15] specifically designed to identify and bridge Thought Leaps in mathematical reasoning. Finally, we apply CoT-Bridge to existing mathematical reasoning datasets to enhance their completeness and coherence, thereby improving the quality of training data for downstream models. Our experimental results demonstrate that models fine-tuned on bridged datasets achieve significant performance improvements compared to those trained on the original datasets with Thought Leaps. As illustrated in Figure 1 (c), addressing the Thought Leap phenomenon leads to consistent improvements across different model architectures and datasets, with performance gains of up to +5.87% on NuminaMath and +3.36% on MetaMathQA. These results highlight the broad applicability of our method and its ability to enhance reasoning capabilities across various mathematical and logical reasoning benchmarks. Furthermore, our approach functions as plug-and-play enhancement module that can be integrated with other advanced techniques. As shown in Figure 1 (c), it can improve the quality of distilled data (+3.02%) or provide better cold start models for Reinforcement Learning (+3.1%), thereby amplifying the effectiveness of existing methods for enhancing reasoning capabilities. In summary, our contributions are: To the best of our knowledge, we are the first to systematically identify and formalize the Thought Leap phenomenon in CoT reasoning. We introduce the CoT Thought Leap Bridge Task along with an evaluation framework for addressing this issue. We develop specialized dataset (ScaleQM+) and model (CoT-Bridge) for identifying and bridging Thought Leaps, demonstrating their effectiveness through comprehensive experiments. We apply CoT-Bridge to existing mathematical reasoning datasets, achieving significant performance improvements and demonstrating good generalization capabilities on out-ofdomain logical reasoning benchmarks (+2.99%). We validate that our approach can function as plug-and-play enhancement module, compatible with methods such as knowledge distillation and reinforcement learning, to further amplify model performance."
        },
        {
            "title": "2 Method",
            "content": "In this section, we provide detailed discussion of our approach. Section 2.1 presents rigorous formalization of the Thought Leap phenomenon and CoT Thought Leap Bridge task. Section 2.2 describes the construction of the Thought Leap dataset and the training of the bridge model, along with brief introduction to variant method. Section 2.3 outlines the process of applying the bridge model to existing step-by-step datasets."
        },
        {
            "title": "2.1 Task Formalization",
            "content": "1, Let = (Q, 2, . . . , the transition from to , explicit and coherent. Defining completeness function , for we have (Q, i [1, m1], (s adjacent steps is sufficiently detailed and logically sound. m) represent an ideal complete CoT starting from question Q. In , 1, as well as transitions between any adjacent steps (s i+1), are both 1) = True and i+1) = True. This function captures whether the reasoning transition between , Now consider CoT = (s0, s1, s2, . . . , sn) that successfully derives an answer (for convenience, we denote question as s0). If there exists at least one pair of adjacent steps (sk, sk+1) within that fails the completeness criterion, i.e., (sk, sk+1) = False, then Thought Leap exists between sk and sk+1. Such incompleteness indicates that necessary intermediate reasoning steps miss = (s k.j), where 1, have been omitted between sk and sk+1. These omitted steps must satisfy three conditions: (1) completeness from sk to the first missing step: (sk, k.1) = True; (2) internal completeness within the missing sequence: [1, 1], (s k.i+1) = True; and (3) completeness from the last missing step to sk+1: (s k.j, sk+1) = True. k.2, . . . , k.1, k.i, Based on this formalization, we define the CoT Thought Leap Bridge Task as two-stage process: first identifying all adjacent step pairs (sk, sk+1) within that satisfy (sk, sk+1) = False, and then for each identified leap, generating the missing intermediate step sequence miss to bridge the gap as sk miss sk+1. This task directly addresses the completeness aspect of reasoning, which is complementary to the more commonly studied factual accuracy dimension."
        },
        {
            "title": "2.2 Thought Leap Bridging Dataset Construction and Bridge Model Training",
            "content": "To implement the CoT Thought Leap Bridge task, we created specialized training dataset named ScaleQM+ based on the structurally comprehensive ScaleQuestMath dataset. As illustrated in Figure x, we start with complete reasoning chains = (s0, m) where s0 = is the question, and strategically remove intermediate steps to produce incomplete chains = (s0, s1, . . . , sn) containing Thought Leaps. The removed steps form the reference set of missing intermediate steps needed to bridge these leaps. 1, . . . , Our step removal strategy follows several principles to create effective training examples. We always retain the final step to preserve the completeness of the answer, while allowing the initial reasoning 3 Figure 2: Illustration of our work. The left panel shows data construction for training, where we strategically remove intermediate steps (e.g., between Step 0 and Step 1, or Step 2 and Step 3) from complete reasoning chains in ScaleQuestMath to create ScaleQM+ with Thought Leaps. The right panel demonstrates inference, where CoT-Bridge identifies gaps and generates appropriate intermediate steps to restore coherence in reasoning. step 1 to be removed to help the model learn macro-planning skills. The number of steps to remove, kdel, scales with the length of the original chain: for shorter chains (m 10), we remove 1-2 steps; for longer chains (m > 10), we remove 1-3 steps. Additionally, with probability of 0.2, we retain the complete chain unchanged (C = ) to train the model to recognize when no bridging is needed. For each modified chain C, we generate two essential training components: the ground-truth Thought Leap positions Lgt identifying where step deletion resulted in reasoning gaps, and the corresponding ground-truth missing steps Mgt consisting of the original removed steps. Our primary model, CoTBridge, learns the mapping : ( ˆL, ˆM), taking an incomplete chain as input and producing both predicted leap positions and the corresponding missing steps. As baseline for comparison, we also implement CoT-Bridge-Random, which learns : (C, Lgt) ˆM, focusing solely on generating missing steps given the ground-truth leap positions. In practice, we process the ScaleQuestMath dataset by segmenting reasoning chains using \"nn\" as delimiters and selecting examples with at least 6 steps (m 6) to ensure sufficient complexity. This process yields 588k training samples with 10k examples held out for testing. We fine-tune CoT-Bridge from the Qwen2.5-Math-7B base model using standard instruction tuning techniques for one epoch. We provide details such as instruction templates and training parameters in Appendix E."
        },
        {
            "title": "2.3 Data Augmentation with Bridge Model",
            "content": "After training, we apply CoT-Bridge to enhance existing mathematical reasoning datasets, specifically MetaMathQA and NuminaMath-CoT, creating improved versions named MetaMath-Bridge and NuminaMath-Bridge respectively. As shown in Figure x, CoT-Bridge processes reasoning chains to identify gaps and generate appropriate bridging content. )}Nleap Given an input reasoning chain = (s0, s1, . . . , sn), CoT-Bridge produces set of predicted Thought Leap repairs {(ki, ˆS i=1 , where each ki identifies leap between steps ski and miss,ki ski+1, Nleap represents the total number of leaps detected, and ˆS is the sequence of generated intermediate steps. For each identified leap, we insert the generated steps between the corresponding original steps, performing ski ˆS To accommodate the format differences across datasets, we adapt our approach to the specific step delimiters used: \"n\" for MetaMathQA and \"nn\" for NuminaMath-CoT. For the CoT-BridgeRandom variant, we provide randomly selected positions krandom instead of model-identified leaps, allowing us to evaluate the importance of accurate gap identification in improving reasoning quality. ski+1 to create the bridged reasoning chain Cbridged. miss,ki miss,ki 4 This approach enables us to assess both the leap detection and step generation aspects of the bridge task separately."
        },
        {
            "title": "3.1 Setup",
            "content": "To evaluate the generality and effectiveness of our proposed approach, we performed supervised finetuning (SFT) experiments using MetaMathQA, NuminaMath-CoT datasets and their bridged versions on representative base models. Specifically, we selected Meta-Llama3.1-8B[16] as representative general-purpose model, and Qwen2.5-Math-1.5B[15] as representative math-specialized model. To ensure comparability, we maintained unified experimental configuration across all SFT experiments. The detailed training settings are provided in Appendix D.1."
        },
        {
            "title": "3.2 Evaluation",
            "content": "We employed six benchmarks: GSM8K [10], MATH500 [17], and GaoKao2023EN [18] as basiclevel benchmarks, and MathOdyssey [19], OlympiadBenchEN [20], and AMC23 [21] as advanced competition-level benchmarks. The evaluation was carried out using the vLLM [22] inference library. To ensure consistency, all models adopted identical generation parameters, employing greedy decoding with zero-shot prompting, without external tools. To mitigate evaluation variance, we sampled four outputs for each problem and computed average accuracy as the final metric. The detailed evaluation settings are provided in Appendix D.2."
        },
        {
            "title": "3.3 Baselines",
            "content": "We establish series of baselines for comparison. The most important baseline is standard SFT performed directly on the original datasets. In addition, we use zero-shot briding which method leveraging general-purpose LLMs (in this paper, Qwen2.5-Instruct-7B/72B [23]) to explore Thought Leap bridging without direct training. This baseline aims to assess the effectiveness of pure prompt engineering in bridging Thought Leaps, determining whether specialized detection and repair mechanisms are necessary. Furthermore, we use CoT-Bridge-Random to generated intermediate steps at randomly chosen positions, to evaluate the importance of accurate Thought Leap localization for effective bridging. As reference, we consider the performance of base models in 4-shot setting. Additionally, we present the results of SFT using GSM8K+MATH and MathInstruct."
        },
        {
            "title": "3.4 Main Results",
            "content": "Table 1 presents our main experimental results across all benchmarks, base models, and training datasets. We observe several consistent patterns that highlight the effectiveness of our CoT Thought Leap Bridge approach. Bridging Thought Leaps consistently improves reasoning performance. When comparing CoTBridge with Direct SFT, we observe substantial improvements across almost all configurations. The most significant gains are seen with Meta-Llama3.1-8B tuned on NuminaMath, where CoT-Bridge achieves an average improvement of +5.87%, with particularly impressive gains on competition-level benchmarks (+15.63% on AMC23). Similarly, for Qwen2.5-Math-1.5B on MetaMathQA, CoTBridge yields +3.36% average improvement, with remarkable +7% on MATH500. These results indicate that addressing reasoning completeness through bridging enhances model performance, especially on more challenging problems that require rigorous step-by-step reasoning. Accurate leap identification is crucial for effective bridging. Comparing CoT-Bridge with CoTBridge-Random reveals the importance of precise gap detection. While CoT-Bridge consistently improves performance, CoT-Bridge-Random shows highly variable results, degrading performance on certain benchmarks. For instance, with Qwen2.5-Math-1.5B on NuminaMath, CoT-Bridge-Random decreases accuracy on GSM8K (-0.56%), GaoKao2023EN (-1.56%), and MathOdyssey (-3.68%), with only marginal average improvement (+0.64%). In contrast, CoT-Bridge achieves substantial +3.39% average gain under the same setting. This stark difference highlights that merely inserting 5 Dataset Size Method Basic Level Competition Level Average GSM8K MATH GaoKao Odyssey Olympiad AMC23 Meta-Llama3.1-8B / / GSM8K+MATH 15k 262k MathInstruct 4-shot Direct SFT Direct SFT 54.15 65.09 68. 18.30 19.25 23.60 20.58 21.69 25.52 16.54 18.48 25.06 4.85 5.07 5.89 11.25 12.50 7.50 20.95 23.68 25. MetaMathQA 395k NuminaMath 859k Direct SFT QwenBridger-S QwenBridger-L CoT-Bridge-R CoT-Bridge Direct SFT QwenBridger-S QwenBridger-L CoT-Bridge-R CoT-Bridge 24.68 36.10 32.86 78.90 81.10+2.20 34.851.25 30.522.34 22.672.01 80.80+1.90 38.05+1.95 31.431.43 24.480.20 80.46+1.56 38.05+1.95 33.57+0.71 24.420.26 81.14+2.24 38.15+2.05 33.12+0.26 25.97+1.29 33.09 17.50 8.48 7.5010.00 30.982.11 9.26+0.78 9.37+0.89 2.5015.00 31.111.98 9.37+0.89 12.505.00 33.060.03 9.48+1.00 18.75+1.25 34.44+1. 36.56 51.45 49.03 84.86 84.230.63 52.40+0.95 51.95+2.92 39.73+3.17 24.70+3.40 27.50+7.50 46.75+2.88 85.25+0.39 54.20+2.75 51.62+2.59 39.08+2.52 25.33+4.03 35.00+15.00 48.41+4.54 84.820.04 54.20+2.75 51.88+2.85 40.12+3.56 26.15+4.85 33.75+13.75 48.50+4.63 85.97+1.11 56.80+5.35 54.42+5.39 40.76+4.20 24.85+3.55 35.63+15.63 49.74+5.87 43.87 21. 20.00 / / GSM8K+MATH 15k 262k MathInstruct 4-shot Direct SFT Direct SFT 79.00 74.45 70.96 48.05 51.40 48.90 45.52 47.66 46.49 38.18 38.50 40.89 19.07 17.44 16.22 22.50 27.50 20. 42.05 42.83 40.58 Qwen2.5-Math-1.5B MetaMathQA 395k NuminaMath 859k Direct SFT QwenBridger-S QwenBridger-L CoT-Bridge-R CoT-Bridge Direct SFT QwenBridger-S QwenBridger-L CoT-Bridge-R CoT-Bridge 38.63 49.60 46.62 81.01 81.58+0.57 51.30+1.70 46.69+0.07 38.630.00 18.52+0.33 25.63+4.38 44.04+1.49 81.010.00 53.63+4.03 49.22+2.60 38.70+0.07 18.85+0.66 27.50+6.25 44.82+2.27 81.58+0.57 53.65+4.05 48.12+1.50 39.02+0.39 18.22+0.03 25.63+4.38 44.37+1.82 81.39+0.38 56.60+7.00 49.61+2.99 39.66+1.03 19.44+1.25 28.75+7.50 45.91+3. 42.55 18.19 21.25 46.77 63.90 57. 83.62 83.230.39 64.20+0.30 58.25+0.85 46.96+0.19 32.041.00 35.00+2.50 53.28+0.41 82.810.81 66.25+2.35 57.79+0.39 46.640.13 32.700.34 40.00+7.50 54.37+1.50 83.060.56 65.20+1.30 55.841.56 43.093.68 33.89+0.85 40.00+7.50 53.51+0.64 84.61+0.99 68.05+4.15 59.29+1.89 47.16+0.39 34.11+1.07 45.00+12.50 56.26+3.39 52.87 33.04 32.5 Table 1: Main results (%) on mathematical benchmarks. MATH, GaoKao, Odyssey, and Olympiad correspond to the MATH500, GaoKao2023EN, MathOdyssey, and OlympiadBenchEN benchmarks, respectively. QwenBridger-S and QwenBridger-L represent zero-shot bridging based on Qwen2.5Instruct-7B and Qwen2.5-Instruct-72B, respectively. CoT-Bridge-R stands for CoT-Bridge-Random. additional steps without strategic placement can disrupt reasoning coherence, whereas targeted bridging at identified leaps enhances logical flow and ultimately improves reasoning quality. Zero-shot bridging shows promise but lacks consistency. Although zero-shot bridging demonstrates certain effectiveness on specific tasks, its overall performance still falls short compared to CoT-Bridge. Taking the LLaMA + NuminaMath setup as an example, the 72B bridge model improves the average accuracy by 4.54% over standard SFT, whereas CoT-Bridge further boosts it to 5.87%. Notably, the noise introduced by zero-shot bridge can degrade model performance in some scenarios. Under the LLaMA + MetaMathQA configuration, the 72B model shows decreased performance on GaoKao2023EN, MathOdyssey, and AMC23 by 1.43%, 0.20%, and 15.00%, respectively, leading to an overall average drop of 1.98%. This effect is more pronounced when using the weaker 7B model. In contrast, CoT-Bridge achieves more robust improvements under the same configuration, raising the average accuracy by 1.35%, highlighting its superior quality control and structural adaptability."
        },
        {
            "title": "4.1 Plug-and-Play Integration",
            "content": "CoT-Bridge can serve as plug-and-play enhancement module that seamlessly integrates into existing training pipelines while delivering consistent performance improvements. To evaluate its adaptability and benefits across different training paradigms, we applied it to two representative scenarios: (1) improving the quality of generated data in knowledge distillation and rejection sampling, and (2) 6 Dataset Method GSM8K MATH GaoKao Odyssey Olympiad AMC23 Average Distill Reject Sampling Direct SFT CoT-Bridge Direct SFT CoT-Bridge 81.86 82.52+0.66 71.50+3.35 66.43+5.59 49.16+1.03 34.89+1.89 45.00+5.63 58.25+3.02 48.13 39. 55.23 33.00 60.84 68.15 83.36 83.74+0.38 75.25+0.35 67.47+2.53 51.87+0.06 39.41+1.78 53.13+3.13 61.81+1.37 51. 50.00 60.44 64.94 74.90 37.63 Table 2: Model performance after bridging distilled and rejection-sampled data. assessing whether models fine-tuned on CoT-Bridge-enhanced data perform better in subsequent RL stages compared to models trained on original data. We used Qwen2.5-Instruct-72B to generate data via distillation and rejection sampling on GSM8K and MATH training sets. For rejection sampling, we sampled 4 responses per prompt (temperature = 1.0, top-p = 1.0) and retained the correct answers, and then bridged them using CoT-Bridge. As shown in Table 2, models fine-tuned on the bridged data achieve superior performance: average accuracy improved from 55.23% (distill) / 60.44% (rejection sampling) to 58.25% / 61.81%, respectively. Figure 3: Model accuracy over training steps on MATH500. Figure 4: PRM scores of Qwen2.5-Instruct-7B/72B on CoTBridge for MetaMathQA and NuminaMath. In the reinforcement learning setup, we continued training the Qwen2.5-Math-1.5B models that were fine-tuned on NuminaMath and NuminaMath-Bridge in the main experiments, using the GRPO [24] algorithm and the DAPO-Math-17K [25] dataset. Additional training details are provided in the Appendix G. Figure 3 shows the training curves of models, demonstrating that our method provides both higher starting point and better final performance. As shown in Table 9 from Appendix G.2, the model trained with NuminaMath-Bridge achieved an RL accuracy of 63.98%, outperforming that trained with NuminaMath at 60.88%. This result also surpasses the officially released Qwen2.5Math-Instruct-1.5B [15] model, which was also trained with GRPO algorithm."
        },
        {
            "title": "4.2 Evaluation of Thought Leap Bridge Task",
            "content": "/ / / Pre Method Position Similarity Overall Rec Red 14.15 33.99 78.02 12.04 33.64 78.37 Qwen2.5-Instruct-7B Qwen2.5-Instruct-72B CoT-Bridge To evaluate the capability of various methods in CoT Thought Leap Bridge Task, we constructed standardized evaluation framework on ScaleQM+ test set, covering leap identification and generation quality. Calculation methods for these metrics are detailed in Appendix H. As shown in Table 3 CoT-Bridge significantly outperforms other methods in localization precision (78.02%), redundancy rate (1.61%) and overall metric (76.15%). Qwen2.5-Instruct-72B notably outperforms the 7B model, indicating that stronger generative models possess certain zero-shot localization and restoration capabilities. We also evaluated an exhaustive approach based on full-position generation and similarity filtering. This method uses CoT-Bridge-Random to generate candidate content between all adjacent steps, filtering them using similarity thresholds. While this approach demonstrates high recall capability, it exhibits an extremely high redundancy rate. Table 3: Performance of different bridging methods on ScaleQM+ test set. Pre stands for Precision, Rec stands for Recall and Red stands for Redundancy. 79.64 75.07 59.21 30.17 8.65 75.72 71.22 55.9 28.26 7.84 79.04 76.57 74.37 63.87 32.51 20.96 23.42 24.81 19.34 7.47 1 0.95 0.90 0.85 0.80 10.54 31.12 76. 34.13 33.73 1.61 Full-position 7 The substantial redundant content may cause LLMs to learn low-quality repetitive patterns during fine-tuning, thereby compromising reasoning ability. Additionally, this strategy is highly sensitive to similarity thresholds, incurs significant computational costs, and is difficult to scale in practice."
        },
        {
            "title": "4.3 Evaluation of Out-of-Domain Reasoning Capability",
            "content": "To verify the generalization capability of our method in Out-of-Domain(OOD) reasoning, we conducted evaluations on five out-of-domain logical reasoning datasets (FOLIO[26], LogicQA[27], ProofWriter[28], ReClor[29], RuleTaker[30]) that were not used in training. These datasets cover formal logic, factual deduction, and textual entailment reasoning types. In the evaluation, we used XFinder[31] to automatically extract answers from model responses, with outputs that failed to yield valid answers (such as those deviating from the prompt or exhibiting logical loops) counted as errors and separately tallied. Bridge Method Mertic FOLIO LogicQA PW ReClor RuleTaker Average NuminaMath+Meta-Llama3.1-8B No Bridge GapBridge No Bridge GapBridge Accuracy Invalid Accuracy Invalid Accuracy Invalid Accuracy Invalid 68.15 1.48 74.07+5.92 0.74 34.33 6.3 35.64+1.31 4.92 59.09 0.51 61.52+2.43 0.2 47 2 50.20+3.20 2. 54.5 0.31 56.57+2.07 0.1 52.61 2.12 55.60+2.99 1.63 NuminaMath+Qwen2.5-Math-1.5B 74.07 1.48 71.112.96 1.48 29.72 4.53 33.10+3.38 3.46 55.84 0.3 58.38+2.54 0. 37.6 1.2 39.00+1.40 1.4 53.05 0.72 53.67+0.62 0 50.06 1.65 51.05+0.99 1.33 Table 4: Performance of NuminaMath and its bridged version on logical reasoning benchmarks. As show in Table 4, we compared the performance of models trained on NuminaMath and NuminaMath-Bridge. Results indicate that CoT-Bridge improved OOD reasoning performance (Meta-Llama3.1-8B: 2.99%, Qwen2.5-Math-1.5B: 0.99%) while reducing invalid response rates. We attribute the performance improvement to the refined intermediate steps introduced by CoT-Bridge, which help models master general reasoning structures and enhance generalization capabilities."
        },
        {
            "title": "4.4 Analysis of Bridging Positions",
            "content": "Leap bridging can enhance reasoning capabilities of LLMs, but whether complementary content bridged at different structural positions contributes substantially still lacks systematic analysis. Therefore, we categorized the bridging content in CoT-Bridge by its position in the reasoning chain as begin (reasoning starting point, planning strategy), middle (key calculations and logical progression), and end (result verification and summary). Statistics show that middle type bridgings dominate (MetaMathQA: 66.34%; NuminaMath: 55.52%), followed by begin, with end accounting for relatively small proportion (approximately 2%). To evaluate the specific role of each type of content, we designed position component ablation experiments: removing one type of bridging (begin / middle / end) from the CoT-Bridge data while keeping the rest unchanged, and fine-tuning Qwen2.5-Math-1.5B under identical training settings. As show in Table 5, experimental results indicate that removal leads to performance degradation. This suggests that all three types of completion have positive effect on model performance. We believe that begin bridgings help clarify problem objectives and establish solution frameworks; middle bridgings support critical links in reasoning paths; and end bridgings, although proportionally smaller, help strengthen logical closure and result reasonability in certain tasks."
        },
        {
            "title": "4.5 Process Supervision Scoring and Noise Impact Analysis",
            "content": "To measure the bridging quality of various methods, we introduced the process supervision model Qwen2.5-Math-PRM-7B to score the intermediate steps (range 01) and analyzed the score distribution. Table 11 in Appendix and Figure 4 present the PRM score distribution of the bridged steps. 8 Delete Pos Num Ratio GSM8K MATH GaoKao Odyssey Olympiad AMC23 Average Qwen2.5-Math-1.5B+MetaMath / - begin - middle - end / - begin - middle - end / 225873 470541 12908 / 694887 906168 31115 / 31.84 66.34 1.82 / 42.57 55.52 1.91 56. 49.61 81.39 82.71+1.32 55.850.75 49.550.06 41.09+1.43 19.220.22 22.506.25 45.150.76 80.970.42 52.454.15 49.350.26 37.142.52 17.262.18 27.501.25 44.111.80 81.140.25 55.750.85 49.94+0.33 37.022.64 18.780.66 28.120.63 45.130.78 39.66 19.44 45.91 28. Qwen2.5-Math-1.5B+NuminaMath 59.29 68.05 84.61 82.602.01 64.853.20 58.670.62 45.481.68 32.960.48 38.756.25 53.892.37 83.511.10 63.554.50 56.432.86 47.160.00 28.305.14 38.136.87 52.853.41 82.791.82 64.054.00 55.973.32 46.510.65 27.485.96 42.502.50 53.223.04 47.16 33. 45.00 56.26 Table 5: Performance variation after removing each bridging component. Threshold GSM8K MATH GaoKao Odyssey Olympiad AMC23 Average MetaMath+Qwen2.5-Math-1.5B / prm < 0.1 prm < 0.3 prm < 0. / prm < 0.1 prm < 0.3 prm < 0.5 56.60 49.61 81.39 81.220.17 57.00+0.40 50.45+0.84 37.402.26 19.70+0.26 29.38+0.63 45.860.05 80.990.40 56.90+0.30 51.88+2.27 37.791.87 18.331.11 32.50+3.75 46.40+0.49 80.530.86 54.751.85 49.090.52 37.861.80 18.780.66 25.003.75 44.341.57 45.91 19. 28.75 39.66 NuminaMath+Qwen2.5-Math-1.5B 68.05 59.29 84.61 83.281.33 65.502.55 59.090.20 46.190.97 34.33+0.89 41.883.12 55.041.22 83.850.76 63.005.05 59.35+0.06 46.380.78 33.070.37 45.000.00 55.111.15 83.261.35 65.952.10 58.051.24 46.061.10 33.56+0.12 41.883.12 54.791. 33.44 47.16 45.00 56.26 Table 6: Performance after removing low-scoring steps (evaluated by Qwen2.5-Math-PRM-7B). Results show that steps generated by CoT-Bridge have higher quality compared to other methods. On the NuminaMath dataset, CoT-Bridges proportion in the high-quality (range 0.91) reaches 83.51%, significantly higher than 72B Fill (53.45%) and 7B Fill (56.8%). Furthermore, CoT-Bridges proportion in the low-quality (range 00.1) is only 2.08%, compared to 14.81% and 13.12% for 72B and 7B Fill, respectively. We also used DeepSeek-R1 [3] to score the CoT comprehensively, CoT bridged by CoT-Bridge scoring higher than the original chains. The scoring prompt, results and examples are provided in the Appendix I. To further verify whether low-quality steps generated by CoT-Bridge affect model training effectiveness, we set different thresholds based on PRM scores to denoise the bridged data and conducted SFT training on Qwen2.5-Math-1.5B accordingly. As shown in Table 6, results indicate that removing low-scoring steps has limited impact on model performance, suggesting that noise introduced by bridging has minimal effect on training. On the NuminaMath dataset, after removing steps with PRM scores below 0.1, accuracy decreased from 56.26% to 55.04%; while on MetaMathQA, removing steps with scores below 0.3 slightly improved performance from 45.91% to 46.40%. We attribute this phenomenon to two factors: first, the proportion of noise introduced by CoT-Bridge itself is already very low; second, the PRM model may misjudge in complex reasoning scenarios, with certain steps marked as low-scoring still possessing heuristic and training value."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we addressed the critical issue of Thought Leaps in CoT reasoning by introducing the CoT Thought Leap Bridge Task and developing the CoT-Bridge model trained on ScaleQM+ dataset. Our approach automatically detects and fills these reasoning gaps, demonstrably enhancing the completeness and coherence of reasoning chains. Comprehensive experiments revealed that models tuned on these bridged datasets leads to significant performance improvements and better generalization to out-of-domain reasoning tasks. Furthermore, CoT-Bridge acts as an effective plugand-play module, improving outcomes in knowledge distillation and reinforcement learning, thereby 9 highlighting the substantial benefits of rectifying thought leaps for more robust and capable large language models."
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [2] Qwen Team. QwQ-32B Preview. https://qwenlm.github.io/blog/qwq-32b-preview/, March 2025. Blog Post. [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [5] Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. [6] Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, and Xiang Ren. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. arXiv preprint arXiv:2305.07095, 2023. [7] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [8] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [9] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [12] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. [13] Xinhao Yao, Ruifeng Ren, Yun Liao, and Yong Liu. Unveiling the mechanisms of explicit cot training: How chain-of-thought enhances reasoning generalization. arXiv preprint arXiv:2502.04667, 2025. [14] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024. [15] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 10 [18] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. [19] Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. arXiv preprint arXiv:2406.18321, 2024. [20] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [21] Mathematical Association of America. American mathematics competitions (amc) 2023 problems. Official AMC Administration / Relevant LLM Benchmark Collection, 2023. [22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [23] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [25] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [26] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. [27] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. [28] Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020. [29] Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: reading comprehension dataset requiring logical reasoning. arXiv preprint arXiv:2002.04326, 2020. [30] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020. [31] Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, and Ding Chen. xfinder: Robust and pinpoint answer extraction for large language models. arXiv preprint arXiv:2405.11874, 2024. [32] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [33] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [34] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [35] Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yunjie Ji, Yiping Peng, Han Zhao, and Xiangang Li. Think twice: Enhancing llm reasoning by scaling multi-round test-time thinking. arXiv preprint arXiv:2503.19855, 2025. [36] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 11 [37] Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. Enhancing mathematical reasoning in llms by stepwise correction. arXiv preprint arXiv:2410.12934, 2024. [38] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. [39] Hyunseok Lee, Seunghyuk Oh, Jaehyung Kim, Jinwoo Shin, and Jihoon Tack. Revise: Learning to refine at test-time via intrinsic self-verification. arXiv preprint arXiv:2502.14565, 2025. [40] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [41] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. Mathfusion: Enhancing mathematic problem-solving of llm through instruction fusion. arXiv preprint arXiv:2503.16212, 2025. [42] Jing Luo, Run Luo, Longze Chen, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, et al. Personamath: Enhancing math reasoning through persona-driven data augmentation. arXiv preprint arXiv:2410.01504, 2024. [43] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, and Yueting Zhuang. Mathfimer: Enhancing mathematical reasoning by expanding reasoning steps through fill-in-the-middle task. arXiv preprint arXiv:2502.11684, 2025. [44] Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning. arXiv preprint arXiv:2404.01067, 2024. [45] Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, and Marina Danilevksy. Delift: Data efficient language model instruction fine tuning. arXiv preprint arXiv:2411.04425, 2024. [46] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. [47] Hynek Kydlíˇcek. Math-Verify: Math Verification Library, 2025. [48] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. [49] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. [50] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv. org/abs/2503.20783."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Methods for Enhancing Mathematical Reasoning Researchers have explored various reasoning enhancement methods based on CoT, broadly categorized into inference-time methods and training-based methods. Inference-time approaches improve reasoning by adopting advanced decoding strategies or utilizing extra computational resources. Self-Consistency (SC)[32] enhances robustness by sampling multiple reasoning paths and applying majority voting. Search-based methods, including Tree-of-Thought (ToT)[33] and Monte Carlo Tree Search (MCTS)[34], systematically explore reasoning spaces for optimal solutions. Recent testtime expansion techniques, such as multi-round thinking[35] and s1 budget forcing[36], dynamically increase computation to boost performance. Training-based approaches internalize enhanced reasoning abilities through parameter updates during model training. common strategy involves fine-tuning models on datasets with explicitly detailed intermediate reasoning steps. Additionally, training paradigms incorporating self-reflection, self-verification, and selfcorrection have gained attention, teaching models to recognize and rectify errors autonomously. Frameworks like StepCo[37], S2R[38], and ReVISE[39] integrate these metacognitive capabilities through supervised, reinforcement, or preference learning. We leverage CoT-Bridge to enhance the structural completeness of existing datasets, enabling models to acquire finer-grained reasoning patterns during training. A.2 High-Quality Mathematical Datasets for Training Developing high-quality datasets is essential for enhancing mathematical reasoning capabilities in LLMs. MetaMathQA[7] consists of 395k problem-solution pairs derived by reformulating and reverse-engineering existing datasets like GSM8K[10] and MATH[11]. MathInstruct[8] aggregates 13 existing math datasets and employs GPT-4[40] to synthesize 260k examples containing both Chain-of-Thought and Program-of-Thought solutions. NuminaMath-CoT[9] compiles data from examinations, competitions, and Q&A communities, resulting in dataset of 860k problem-solution pairs. ScaleQuestMath[14] introduces zero-shot data generation framework for mathematical questions using small open-source models, resulting in scalable dataset of about 1 million samples. DAPO[25] developed DAPO-Math-17K, comprising 17k standardized integer-formatted math problems sourced primarily from platforms such as AoPS for effective reward model evaluation. Beyond creating new datasets, extensive research has focused on data augmentation (e.g., MathFusion[41], PersonaMathQA[42], MathFimer[43]) and selective strategies (e.g., QaDS[44], DELIFT[45]) to enhance data efficiency and training effectiveness. CoT-Bridge contributes by bridging Thought Leap issues in existing mathematical reasoning datasets, thus improving their overall data quality."
        },
        {
            "title": "B Limitations",
            "content": "Potential Noise in Reasoning Steps: CoT-Bridge cant guarantee that all bridged reasoning steps are entirely correct, which may inevitably introduce certain level of noise during automated data construction. Although our analysis in Section 4.5 suggests that this noise has limited impact on overall model performance, its presence remains non-negligible concern. Lack of Validation on Larger-Scale Models: Due to computational resource constraints, our experiments focus primarily on small to medium-sized models, and we have not yet conducted evaluations on larger models such as 32B or 72B-scale LLMs. This may limit the verification of our methods generalization capability on very large-scale pretrained models. Future work could explore its performance and adaptability on larger architectures. Training Limited to Mathematical Data: CoT-Bridge is trained exclusively on ScaleQuestMath in mathematical domain. While we demonstrate promising OOD generalization to logical reasoning tasks that are relatively close to mathematics, the method remains largely domain-specific. It is worth noting, however, that CoT-Bridge is theoretically generalizable and can potentially be applied to other multi-step reasoning tasks in domains such as law, medicine, and scientific QA. Future research could further investigate its adaptability across diverse task types."
        },
        {
            "title": "C Preliminary Experiment",
            "content": "C.1 Settings To simulate varying degrees of Thought Leap, we created modified versions of the MetaMathQA dataset by removing intermediate reasoning steps. Considering the structure of MetaMathQA, we use \"n\" to segment individual steps within CoT. The specific configurations are as follows: The final step which contains the answer explanation is always retained. Mild, Moderate, Severe Thought Leap: We randomly remove 1, 2, or 3 intermediate steps, respectively, if sufficient steps are available. Extreme Thought Leap: All intermediate reasoning steps are removed and only the final answer explanation is preserved. We adopt the same training configuration as described in Section 3.1 using Meta-Llama3.1-8B. Each model is trained for 200 steps, with evaluation performed on the GSM8K every 10 steps. For convenience, answer verification is conducted using Math-Verify only. C.2 Results We present the detailed results from Figure 1(b) in Table 7. Training Step Original Data Mild Thought Leap Moderate Thought Leap Severe Thought Leap Answer Only 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 8.64 43.67 62.32 60.96 62.77 64.9 68.08 66.94 69.9 69.06 70.51 69.74 69.98 68.92 69.6 69.83 68.61 70.28 68.61 67.63 8.79 38.71 56.03 55.34 55.42 60.88 63.61 64.67 66.03 64.52 63.68 66.41 65.28 63.91 62.47 66.64 64.29 65.5 66.03 62.77 8.64 33.2 40.33 46.4 50.87 50.57 52.69 55.57 56.94 60.5 57.16 61.03 61.79 57.77 56.1 56.03 54.66 48.75 50.49 60.35 8.57 25.78 21.53 29.95 40.33 35.71 35.18 37.98 40.03 37.76 48.37 37.15 36.39 37.83 29.04 38.29 29.72 32.9 27.45 39.8 7.35 8.04 10.77 15.09 14.4 16.37 14.86 16.22 16.22 16.38 17.29 15.62 17.66 16.83 16.22 17.36 15.01 15.39 16 13. Table 7: Accuracy of Meta-Llama3.1-8B on GSM8K over training steps using data with varying degrees of Thought Leaps."
        },
        {
            "title": "D Experimental Details",
            "content": "D.1 Model Training Settings We utilized LlamaFactory[46] as SFT training framework. The initial learning rate was set to 1 105 with warm-up ratio of 0.1, and cosine scheduling was used to gradually reduce the learning rate to zero. The maximum sequence length was set to 8192 tokens, with global batch size of 128. We trained models for 3 epochs on MetaMathQA[7] and for 2 epochs on NuminaMath-CoT[9], as preliminary experiments indicated no further improvement from additional epochs. All SFT experiments were conducted using 8 Ascend H910B-64G. For SFT training template, we use the default style template in LlamaFactory: Prompt Template System: You are math problem solver. You should think step by step. Human: <Question> Assistant: <Answer> D.2 Evaluation Settings The maximum token limit for generation was set to 2048, and prompt templates remained consistent with training ones. It is noteworthy that, although we set the decoding temperature parameter to zero, vLLM outputs still exhibited some randomness. For answer extraction and comparison, we employed the Math-Verify[47] tool. Given limitations of math-verify in handling complex expressions, responses failing initial verification were subsequently validated using DeepSeek R1[3]. All model evaluations were performed using 4 NVIDIA A100-40G GPUs. Referring to OpenR1[48], we use the following prompt template: Prompt Template You are mathematical answer validator. You will be provided with mathematical problem and you need to compare the answer in the reference solution, and the final answer in models solution to determine if they are equivalent, even if formatted differently. PROBLEM: <problem> REFERENCE SOLUTION: <answer> MODELS SOLUTION: <generation> Focus ONLY on comparing the final mathematical answer provided by the model while ignoring differences in: - Formatting (e.g., boxed{} vs plain text) - Multiple choice formatting (e.g., \"A\" vs full solution) - Order of coordinate pairs or solutions - Equivalent mathematical expressions or notation variations - If the models answer is nonsense, return \"Verdict: AMBIGUOUS\" Start with brief explanation of your comparison (2-3 sentences). Then output your final answer in one of the following formats: - \"Verdict: EQUIVALENT\" - \"Verdict: DIFFERENT\" - \"Verdict: AMBIGUOUS\" ScaleQM+ Construction and CoT-Bridge training E.1 ScaleQM+ Template Prompt Template System: You are mathematics teacher reviewing solution that may be missing one or more steps. Your task is to: 1. Identify all points in the logical flow where step is missing. For each missing step, specify exactly between which two consecutive steps it should be placed. 2. Provide the complete missing step(s) with necessary explanations and equations. The solution may be missing multiple steps or might be complete. The steps in the solution are labeled from Step 0 (problem statement) to Step N. For each missing step, please format your response as follows:: Missing Step X: The missing step should be placed between Step and Step Y+1. The missing step is: [Write the complete missing step here with necessary explanations and equations] If there are no missing steps, please output: No missing steps. Human: <incomplete_solution> Step 0: <question statement> Step 1: ... Step N: ... </incomplete_solution> Assistant: <Answer> E.2 Variant Template Prompt Template System: You are mathematics teacher reviewing solution that appears to be missing one step. Given the position of the missing step, your task is to fill in the missing step. The steps in the solution are labeled from Step 0 (problem statement) to Step N. Please format your response as: The missing step is: [Write the complete missing step here with necessary explanations and equations] Human: There is missing step between Step and Step X+1. <incomplete_solution> Step 0: <question statement> Step 1: ... Step N: ... </incomplete_solution> Assistant: <Answer> E.3 Discussion of Hyperparameter Settings We set the minimum CoT length to 6, based on the following considerations. CoTs with too few steps should be avoided, as identifying Thought Leaps in such cases becomes trivially easy. For example, in two-step CoT, 16 the final step (the answer) cannot be removed, so only the first step is eligible for deletion. This makes it very easy for the model to identify the location of the Thought Leap, which lies directly between the question and the answer. Regarding the number of steps to delete, we avoid removing too many steps. For instance, in 6-step CoT, deleting 4 steps would leave too little context, making the task resemble generating new solution based on the final answer rather than bridging partial reasoning chain. As for the probability of retaining the original CoT without modification, this parameter is introduced to avoid introducing redundancy into already complete reasoning chains. E.4 CoT-Bridge Training Settings We broadly follow the setup in D.1, but train for only 1 epoch with global batch size of 1024, considering the large amount of data and the primary goal of learning the structural integrity of CoT rather than introducing new knowledge."
        },
        {
            "title": "F Evaluation Benchmarks",
            "content": "F.1 Mathematics Benchmarks GSM8K: This benchmark consists of 1,319 grade-school math word problems, primarily used to evaluate models ability to perform multi-step arithmetic reasoning. MATH500: curated subset of 500 representative problems from the larger MATH dataset, designed to reflect the original datasets coverage across diverse mathematical topics and difficulty levels. Gaokao2023EN: This benchmark includes 385 English-translated questions from the 2023 Chinese Gaokao (college entrance exam) mathematics section. It tests models ability to solve complex problems at the advanced high school level. MathOdyssey: benchmark of 387 problems spanning difficulty levels from high school to early undergraduate mathematics. It emphasizes evaluating models performance on problems requiring deeper understanding and more complex reasoning steps. OlympiadBenchEN: collection of 675 problems at the difficulty level of international mathematical olympiads. Known for their non-standard formats and high demands on creative problem-solving, these problems serve as rigorous test of advanced mathematical reasoning capabilities. AMC23: This benchmark contains 40 problems selected from the 2023 American Mathematics Competitions (AMC), offering challenging testbed for evaluating models in standardized competition setting. F.2 Logical Reasoning Benchmarks FOLIO: This benchmark consists of 1,430 expert-crafted natural language inference instances, each annotated with first-order logic (FOL) expressions. It is designed to assess models ability to translate between natural language and formal logic, as well as perform multi-step reasoning over complex logical structures. LogicQA: Collected from the Chinese National Civil Servant Examination, this benchmark targets logical reasoning in reading comprehension tasks. ProofWriter: Composed of multiple small rulebases, each containing natural language descriptions of facts and rules, along with questions labeled as true, false, or unknown. This benchmark supports evaluating models ability to generate reasoning chains and perform multi-step logical inference. ReClor: Derived from standardized graduate-level entrance exams such as the LSAT and GMAT, ReClor evaluates models performance on complex logical reasoning questions. RuleTaker: Built from synthetically generated rules and facts, this benchmark tests whether model can simulate deductive reasoning. It evaluates the models ability to draw logical conclusions from natural language rules."
        },
        {
            "title": "G RL Training",
            "content": "G.1 Settings All RL experiments were conducted using the veRL[49] training framework. We employed Math-Verify for answer verification. We set the initial learning rate to 1 106 and used global batch size of 512. The maximum response length was limited to 4096 tokens. During rollout, 4 samples were generated per input. KL regularization was disabled, and evaluation was performed with zero temperature every 10 epochs on MATH500. We use the same template in Appendix D.1. G.2 Results We present the detailed results from Figure 3(b) in Table 8. For simplicity, only Math-Verify is used for answer verification without further applying DeepSeek R1. Training Step Qwen2.5-Math-Numina Qwen2.5-Math-NuminaBridge 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 62.2 64.4 65.4 64.6 66.8 65.6 67 67.8 68 69.6 69 67.6 69.6 70.8 71.4 72 72 73.4 72 72.8 71.6 71.6 72.6 72.8 73.6 71.8 73.6 73.4 67.4 67.8 65.6 66.4 67.6 67.6 70.6 70.4 71.4 73.6 72.6 71.8 71.8 72.8 74 74 74.8 73.6 74.8 76 75.4 75.2 76.8 78 76.4 77.4 77.4 77.6 Table 8: Model accuracy on MATH500 over GRPO training steps. Model Method GSM8K MATH GaoKao Odyssey Olympiad AMC23 Average Qwen2.5-Math-Instruct-1.5B Oat-Zero-1.5B Qwen2.5-Math-1.5B / 84.80 Dr. GRPO 83.62 82.71 GRPO 75.80 74.20 74.60 65.50 69.61 64.94 54.52 52.71 49.10 38.10 37.60 35. 60.00 53.00 50.00 63.12 61.79 59.33 Qwen2.5-Math-Numina Qwen2.5-Math-NuminaBridge GRPO GRPO 84.31 84.080.23 78.20+3.40 67.01+4.67 54.26+2.32 40.30+0.89 60.00+7.50 63.98+3.10 74. 52.50 51.94 39.41 60.88 62.34 Table 9: Reinforcement learning results. Qwen2.5-Math-Numina and Qwen2.5-Math-NuminaBridge refer to the Qwen2.5-Math-1.5B models fine-tuned on NuminaMath and NuminaMath-Bridge respectively in the main experiments. We also present the results of Qwen2.5-Math-1.5B trained directly on DAPO-Math-17K using GRPO, along with the results of Qwen2.5-Math-Instruct-1.5B[15] and Oat-Zero-1.5B [50] for reference."
        },
        {
            "title": "H Metrics for CoT Thought Leap Bridge Task",
            "content": "H.1 Leap Position Identification Metrics Let the ground-truth set of leap positions be = {g1, . . . , gG}, and the predicted set be = {p1, . . . , pP }. If predicted position pj exactly matches ground-truth position gi, it is counted as True Positive (TP). The metrics are defined as: Precision = TP , Recall = TP To measure the proportion of redundant predictions, we define the redundancy rate as: Redundancy = TP H.2 Overall Metric To evaluate both leap positions locating ability and the quality of the generated step, we introduce the Positionaware BERTScore. This metric only compares text at successfully matched positions. Let the reference text at matched position be ti and the generated text be ˆti, the Position-aware BERTScore is defined as: BERTScorepos-aware = 1 TP (cid:88) i=1 BERTScore(ˆti, ti) Here, BERTScore is computed using the RoBERTa-large model and measures the semantic similarity between each pair of texts (ˆti, ti)."
        },
        {
            "title": "I CoT Score",
            "content": "I.1 Score Template Since scoring single response with DeepSeek R1 can result in high variance, we instead compare the original CoT and its bridged version for the same question side-by-side. DeepSeek R1 Score Template You are math reasoning expert. Please evaluate the coherence of the given two Chain of Thoughts (CoTs) based on the provided question. Perform an overall assessment of the CoTs based on the following five dimensions: 1. Problem Understanding: Does the CoT accurately identify the core of the question and interpret its requirements appropriately? 2. Planning: Does it show evidence of having structured plan or certain degree of foresight in approaching the problem? 3. Logical Coherence: Is the reasoning step-by-step, consistent, and free of major logical leaps? 4. Detail Elaboration: Does it include detailed intermediate calculations, formula derivations, or other necessary expansions? 5. Conclusion Support: Does it provide clear summary or verification to support the final conclusion? - Question <problem> - CoT 1 <answer1> - CoT 2 <answer2> [Scoring Criteria Description] - 10 points: Outstanding across all dimensions. Demonstrates deep understanding, clear planning, rigorous step-by-step reasoning, detailed intermediate calculations or derivations, and strong conclusion verification. Rarely achieved; reflects expert-level CoT. - 8 points: Strong performance with only minor flaws. Reasoning is coherent and mostly complete, with evident planning and elaboration. May miss slight details or final verification, but overall very solid. - 6 points: Adequate but uneven. Shows basic understanding and partial planning. Some steps are under-explained or loosely connected; may lack full elaboration or support for conclusion. - 4 points: Weak overall. Reasoning is fragmented or inconsistent, with limited detail and minimal evidence of structured planning. Key steps are missing or poorly justified. - 2 points: Very poor. Little to no logical structure or elaboration; response lacks meaningful connection to the question. - 0 points: No relevant reasoning. Off-topic or fundamentally flawed throughout. Note: Scores may also be assigned in odd numbers (e.g., 5, 7, 9) to reflect responses that fall between two defined levels. For instance, score of 7 indicates performance stronger than the 6-point criteria but not fully reaching the 8-point standard. [Output Format] - Justification: Logical Coherence Analysis: <Your brief analysis> - Score: CoT 1 overall score: <Integer between 0-10> CoT 2 overall score: <Integer between 0-10> brief analysis is sufficient. 20 I.2 Score Example Example Okay, lets tackle this problem. The user wants me to evaluate two Chain of Thoughts (CoTs) for the question about the minimum number of throws needed to ensure repeated sum with two eight-sided dice. The scoring is based on five dimensions: Problem Understanding, Planning, Logical Coherence, Detail Elaboration, and Conclusion Support. First, need to understand the question thoroughly. The key here is the application of the pigeonhole principle. The possible sums when rolling two 8-sided dice range from 2 (1+1) to 16 (8+8). That gives 15 possible sums. According to the pigeonhole principle, to guarantee repeat, you need one more than the number of possible sums, so 15 + 1 = 16 throws. Both CoTs arrive at this answer, but need to assess how well each explains the reasoning. Looking at CoT 1: It starts by identifying the minimum and maximum sums correctly. Then it directly applies the pigeonhole principle, stating 15 +1 =16. The conclusion is there, but the explanation is very brief. It doesnt mention how many possible sums there are, just states the range. The step from the range to 15 possible sums is implicit. reader might wonder why 15 instead of 16-2=14. So, CoT 1 skips explaining the count of possible sums, which is crucial step. The logical steps are there but not fully elaborated. CoT 2 is more detailed. It explicitly states the pigeonhole principle, defines the range of sums, calculates the number of possible sums as 15 (16-2+1), then adds 1. This makes the reasoning clearer. The explanation of why there are 15 possible sums is important because its common point of confusion (inclusive counting). CoT 2 addresses that, making it more thorough. Now, evaluating each dimension for both CoTs: For Problem Understanding: Both correctly identify the core of the question, which is applying the pigeonhole principle. CoT 2 does better job by explicitly stating the principle and the count of sums. Planning: CoT 2 structures the answer step-by-step, starting with the principle, then calculating possible sums, then applying the formula. CoT 1 jumps to the answer without explaining the count of sums, which is gap in planning. Logical Coherence: Both are logically correct, but CoT 1 has minor leap in assuming the number of possible sums. CoT 2 connects each step more clearly. Detail Elaboration: CoT 2 elaborates on the number of sums, which CoT 1 omits. This makes CoT 2s explanation more complete. Conclusion Support: Both reach the correct conclusion, but CoT 2s conclusion is better supported by the preceding steps. Scoring: CoT 1 might get 6 or 7 because its correct but lacks some details. CoT 2 is more thorough, so probably an 8 or 9. Since CoT 2 explains each step clearly without missing key points, its closer to the 8-9 range. CoT 1 is adequate but not as detailed, so maybe 7. However, according to the criteria, 8 is for strong performance with minor lapses. CoT 2 doesnt have lapses, so maybe 9? Wait, the scoring criteria say 10 is outstanding. CoT 2 is thorough but maybe not \"outstanding\" because its straightforward application. Alternatively, since its complete, maybe 10. But the example given in the criteria for 10 is \"precise, step-by-step reasoning\" with \"careful attention to clarity\". CoT 2 does that. CoT 1 is missing the explanation of the number of sums, so maybe 6 or 7. Let me check again. CoT 1: Problem Understanding: Correct. Planning: Minimal, skips the count of sums. Logical Coherence: Correct but with leap. Detail Elaboration: Missing the count explanation. Conclusion: Correct. So maybe 6 or 7. The criteria for 6 say \"intermediate steps present but not fully explained\". CoT 1 has the steps but skips the count. So 6. CoT 2: All steps explained, so 10? But the example for 10 says \"smooth and complete\". CoT 2 is complete. Alternatively, maybe 9 if theres minor point missing. But dont see any. It explains the pigeonhole principle, calculates the sums correctly, adds 1. So 10. But the users example answer might have given 8 and 10. Wait, the users example in the criteria for 10 is \"outstanding across all dimensions\". CoT 2 is thorough, but perhaps not outstanding. Hmm. Alternatively, maybe 9. But given that its correct and explains all steps, 10. CoT 1 gets 6 because its adequate but missing the count explanation. </think> - Justification: Logical Coherence Analysis: CoT 1 correctly applies the pigeonhole principle but skips explaining the count of possible sums (15), leaving minor gap. CoT 2 explicitly calculates the number of sums (16 - 2 + 1 = 15) and links each step clearly, ensuring full logical continuity. - Score: CoT 1 overall score: 7 CoT 2 overall score: 10 **Note:** CoT 1 earns 7 for adequate reasoning but lacks explicit calculation of the sum count. CoT 2 achieves 10 by thoroughly addressing all dimensions, including precise step-by-step elaboration and seamless logical flow. 21 The question, CoT1, and CoT2 here are the same as those shown in the example in Figure 1(a). I.3 Score Result We randomly sampled 1,000 CoT pairs from both MetaMath and NuminaMath for scoring, and the results are shown in Table 10."
        },
        {
            "title": "Metamath\nNuminamath",
            "content": "7.85 7.49 8.44 8.22 Table 10: DeepSeek R1 scoring results for the original CoT and the bridged CoT."
        },
        {
            "title": "J PRM Score",
            "content": "We present the data from Figure 4 in Table 11. Method Metric 0-0.1 0.1-0.2 0.2-0. 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0. 0.9-1 GapBridge Qwen2.5-Instruct-72B Qwen2.5-Instruct-7B GapBridge Qwen2.5-Instruct-72B Qwen2.5-Instruct-7B num percent num percent num percent num percent num percent num percent 5021 0.71 9528 2.39 19349 5.84 33923 2.08 219586 14. 177687 13.12 2516 0.35 4946 1.24 5557 1.68 20643 1.26 38135 2. 38694 2.86 MetaMathQA 2149 0.3 4410 1.11 4706 1.42 2228 0. 4516 1.13 4372 1.32 NuminaMath 18397 1.13 34371 2.32 32492 2. 17857 1.09 34360 2.32 31388 2.32 2401 0.34 4891 1.23 4714 1. 18783 1.15 36910 2.49 32955 2.43 2765 0.39 5990 1.5 5287 1. 21589 1.32 43698 2.95 37837 2.79 3560 0.5 8172 2.05 7023 2. 27041 1.66 56699 3.82 47592 3.51 5045 0.71 11505 2.89 9568 2. 37375 2.29 78320 5.28 65059 4.8 10812 1.52 22577 5.67 18096 5. 672825 94.85 321582 80.78 252466 76.24 73499 4.5 147988 9.98 121298 8. 1363063 83.51 792281 53.45 769034 56.8 Table 11: The count and proportion of bridge steps falling into different PRM score intervals for Qwen2.5-Instruct-7B/72B and CoT-Bridge on MetaMath and NuminaMath."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}