{
    "paper_title": "Lynx: Towards High-Fidelity Personalized Video Generation",
    "authors": [
        "Shen Sang",
        "Tiancheng Zhi",
        "Tianpei Gu",
        "Jing Liu",
        "Linjie Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation."
        },
        {
            "title": "Start",
            "content": "Lynx: Towards High-Fidelity Personalized Video Generation Shen Sang Tiancheng Zhi"
        },
        {
            "title": "Linjie Luo",
            "content": "Intelligent Creation, ByteDance Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "We present Lynx, high-fidelity model for personalized video synthesis from single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation. Date: September 22, 2025 Project Page: https://byteaigc.github.io/Lynx/ 5 2 0 2 9 1 ] . [ 1 6 9 4 5 1 . 9 0 5 2 : r Figure 1 Left: Lynx consistently preserves facial identity with high fidelity, while producing natural motion, coherent lighting, and flexible scene adaptation (input shown at top-left). Right: Lynx demonstrates clear superiority in identity resemblance and perceptual quality, while remaining competitive in motion naturalness compared to other methods."
        },
        {
            "title": "Introduction",
            "content": "The field of visual content generation has witnessed rapid progress, largely propelled by the emergence of diffusion models [17, 34, 37], which offer scalable and effective framework for high-fidelity synthesis across diverse modalities. Building upon early breakthroughs in text-to-image generation [3, 32, 35, 36, 39], the community has extended diffusion-based methods into the temporal domain, giving rise to text-to-video models [4, 15, 26, 27, 33, 40, 42, 46] capable of synthesizing dynamic visual content from natural language prompts. Recent advancements in backbone architecturessuch as Diffusion Transformers (DiT) [34]have further improved generation quality and scalability. Beyond foundational generation, there is growing interest in downstream tasks including video editing [28, 44, 51], multi-shot storytelling [24], and controllable motion synthesis [14, 23], reflecting the fields increasing demand for controllability, reusability, and efficiency. key direction emerging from this trend is personalized video generation, which aims to synthesize videos that faithfully preserve subject identity. Personalized content creation has been extensively explored in the image domain, where foundation generative models are adapted to user-provided references to enable the synthesis of diverse yet identity-consistent outputs. Early methods [12, 18, 38] achieve strong identity preservation via either full-model or parameter-efficient fine-tuning such as LoRA [18]. More recent approaches [43, 47, 50] employ lightweight conditioning mechanisms based on identity embeddings or reference features, enabling efficient personalization without retraining the entire model. Building upon these advances, as well as the progress in general-purpose video foundation models, recent efforts have begun to explore identity preservation in the temporal domain [9, 11, 22, 31, 45, 48], aiming to generate coherent and realistic personalized videos over time. In this report, we present Lynx, high-fidelity personalized video generation framework designed to preserve identity from single input image. Instead of restructuring or fine-tuning the full base model, Lynx adopts an adapter-based design with two specialized components: the ID-adapter and the Ref-adapter. The ID-adapter leverages cross-attention to inject identity features extracted from single facial image. Specifically, facial embeddings are obtained using face recognition model and transformed into compact set of identity tokens via Perceiver Resampler, enabling rich and efficient representation learning. To further enhance detail preservation, the Ref-adapter incorporates reference features extracted from pretrained VAE encoder (inherited from the base model). These features are passed through frozen copy of the diffusion backbone to obtain intermediate activations across all DiT blocks, which are then fused into the generation process via cross-attention. For training, we adopt multi-stage progressive strategy with spatio-temporal frame packing design to effectively handle both image and video data of varying aspect ratios and temporal lengths. We evaluate Lynx on curated benchmark of 40 diverse subjects and 20 human-centric unbiased prompts, resulting in 800 test cases. Face resemblance is assessed using three expert face recognition models. To evaluate prompt following and video quality, we construct an automated pipeline with the Gemini-2.5-Pro API 1, instructing the model to score aesthetic quality, motion naturalness, prompt alignment, and overall video quality. As reported in Table 1 and Table 2, Lynx consistently outperforms recent state-of-the-art personalized video generation methods in identity preservation, while also delivering stronger prompt alignment and superior overall video quality."
        },
        {
            "title": "2 Related Works",
            "content": "Video Foundation Models. Recent video foundation models are predominantly built on the diffusion framework, where variational autoencoders (VAEs) [25] compress raw videos into compact latent representations, enabling efficient training and generation. Early latent diffusion methods extended image foundation models with U-Net architectures to the video domain by incorporating temporal modules such as 3D convolutions and temporal attention [4, 16, 40]. As the demand for scalability and long-range temporal coherence increased, research shifted toward transformer-based architectures. Diffusion Transformers (DiT) [34] and their dualstream variant MMDiT [10] demonstrated more expressive spatio-temporal modeling, leading to improved temporal consistency. These architectures now underpin state-of-the-art video foundation models, including 1https://ai.google.dev/gemini-api/docs 2 Figure 2 Videos generated from single input image, showing strong identity preservation across expressive facial expressions (rows 3), diverse lighting (rows 1, 4, 5), pose variations (rows 2, 6, 7), and object interactions (rows 8). 3 Figure 3 Architecture of Lynx. Built on DiT-based video foundation model, Lynx introduces two adapter modules that inject identity features through cross-attention. CogVideoX [46], HunyuanVideo [27], Wan2.1 [42], Seedance 1.0 [13], etc., which achieve strong generalization through large-scale training data, substantial computational resources, and extended context length. Identity-Preserving Content Creation. Identity-preserving generation is central topic in content creation and has been extensively studied in the image domain. Early approaches [12, 18, 38] typically rely on model fine-tuning or optimization to obtain subject-specific models. However, such tuning-based methods are often impractical for real-world applications because of their computational cost and lack of scalability. For example, DreamBooth [38] and LoRA-based variants [18] require fine-tuning either the full base model or additional low-rank adapters. To overcome these limitations, tuning-free methods [43, 47] introduce lightweight ID-injection modules that avoid per-subject training. IP-Adapter [47] represents identity features with face recognition encoder and injects them into the base model through adapters. Building on this idea, InstantID [43] incorporates ControlNet [49] module for input decoupling and finer-grained control. With the advent of large video foundation models, research attention has shifted toward personalized video generation. For instance, ConsistID [48] enforces facial identity consistency via frequency decomposition. ConceptMaster [21] employs CLIP image encoder and learnable Q-Former to fuse visual representations with corresponding text embeddings for each concept. HunyuanCustom [20] extends HunyuanVideo [27] with multi-modal customization framework that integrates image, audio, video, and text conditions through modality-specific modules, achieving stronger identity consistency and controllable video generation. Another line of work (e.g., SkyReels-A2 [11], VACE [22], Phantom [31]) concatenates reference conditions with noisy latents and processes the full sequence during denoising. However, balancing identity resemblance and editability has long been persistent challenge. Our method significantly improves resemblance while maintaining strong prompt following and high video quality."
        },
        {
            "title": "3.1 Model Architecture",
            "content": "We adopt Wan2.1 [42], one of the latest open-sourced video foundation models, as our base model. Wan is built upon the DiT architecture [34], combined with the Flow Matching [30] framework. Each DiT block first applies spatio-temporal self-attention over visual tokens, enabling joint modeling of spatial details and temporal dynamics, followed by cross-attention to incorporate text conditions. Instead of restructuring and fine-tuning the full model, we introduce two adapter modules, i.e., ID-adapter and Ref-adapter, to inject identity features and enable personalized video generation on top of the base model. The overall architecture and adapter design are illustrated in Figure 3. 4 ID-adapter. Prior works [43, 47] incorporated face recognition features [8] to achieve personalized generation in text-to-image models such as Stable Diffusion [36]. These methods typically attach additional adapter layers and introduce extra cross-attention modules to condition generation on identity features. Specifically, face image is passed through face feature extractor to obtain feature vector. To convert this vector into sequence suitable for cross-attention, Perceiver Resampler [1] (also known as the Q-Former [29]) is trained to map it into fixed-length token embedding representation. We adopt the same paradigm. Given face feature vector of dimension 512, the Resampler produces sequence of 16 token embeddings of dimension 5120. The token embedding is concatenated with 16 additional register tokens [6] and cross-attended with the input visual tokens. The resulting representation is then added back to the main branch. Ref-adapter. Several recent approaches [11, 31] use VAE features to enhance detail preservation during reference injection, taking advantage of the spatially dense representations produced by VAE encoders. Complementing the ID-adapter, our design also incorporates VAE dense features to enhance identity fidelity. Unlike prior approaches that directly place the feature map in front of noisy latents in an image-to-image-like generation fashion, we instead process the reference image through frozen copy of the base model (with noise level as 0 and text prompt as \"image of face\"), similar to the design of ReferenceNet [19]. This allows spatial details from the reference image to be captured across all layers. As with ID-adapter, we apply separate cross-attention at each layer to integrate the corresponding reference tokens."
        },
        {
            "title": "3.2 Training Strategy",
            "content": "We describe here the strategies employed for large-scale training. Since training videos (and images) vary in both spatial resolution and temporal duration, we adopt the NaViT approach [7] to efficiently batch heterogeneous inputs. Multiple videos or images are packed into single long sequence, with attention masks applied to separate samples. Training follows progressive curriculum beginning with image pretraining, which leverages the abundance of large-scale image data, and is then extended to video training to restore temporal dynamics."
        },
        {
            "title": "3.2.1 Spatio-Temporal Frame Pack",
            "content": "Traditional training in the image domain often relies on bucketing to handle multi-resolution inputs. Images are cropped and resized into set of predefined aspect ratios and resolutions, and during training the data loader samples from single bucket so that images within batch share the same dimensions. While effective for images, this strategy does not generalize well to video, as the additional temporal dimension (frame length) introduces significant complexity. Bucketing by both resolution and duration reduces flexibility and limits the models ability to generalize to arbitrary aspect ratios and video lengths. To overcome this limitation, inspired by Patch Pack [7], we concatenate the patchified tokens of each video into single long sequence, treating the collection unified batch. An attention mask ensures that tokens only attend within their own video, preventing cross-sample interference. For positional encoding, we apply 3D Rotary Position Embeddings (3D-RoPE) [41] independently to each video. This design enables efficient batching of heterogeneous images and videos while preserving both spatial and temporal consistency."
        },
        {
            "title": "3.2.2 Progressive Training",
            "content": "Image Pretraining. We begin with image pretraining given the large amount of available image data. To ensure consistency across training stages, each image is treated as single-frame video and the same frame pack strategy described above is applied. In our experiments, training the Perceiver Resampler from scratch yielded unsatisfactory results: no facial resemblance was observed even after substantial training, suggesting that the model either fails to converge or requires prohibitively longer training. Instead, we found that initializing the Resampler from an image-domain pretrained checkpoint (e.g., InstantID [43]) leads to much faster convergence. With this initialization, recognizable facial resemblance emerges after only 10k iterations, while the complete first stage runs for 40k iterations. Video Training. Image pretraining alone tends to produce videos that are largely static, as the model primarily learns to preserve appearance rather than capture motion. To restore temporal dynamics, second stage 5 (a) Expression Augmentation (b) Portrait Relighting Figure 4 Examples of our augmentation strategies: (a) expression augmentation via X-Nemo [52], and (b) portrait relighting via LBM [5]. that exposes the model to large-scale video data is necessary. This stage enables the network to learn motion patterns, scene transitions, and temporal consistency while retaining and enhancing the strong identity conditioning established during image pretraining. Training proceeds for 60k iterations."
        },
        {
            "title": "4 Data Pipeline",
            "content": "The goal of our data pipeline is to construct high-quality persontextvideo triplets. While text prompts can be readily obtained through captioning models (e.g., Qwen 2.5-VL [2]), the main challenge lies in establishing reliable personvideo pairs, i.e., pairing an image of person as the identity (ID) condition with target video of the same individual. Our raw data consist of images and videos collected from both publicly available datasets and in-house sources. These data can be categorized into four types: (1) single images; (2) single videos; (3) multi-scene image collections of the same person; and (4) multi-scene video collections of the same person. To construct imageimage and imagevideo pairswhere one image serves as the ID condition and the other image or video serves as the generation targeta straightforward approach is to crop faces directly from images or videos. However, this often leads to overfitting of expression and lighting. Meanwhile, multi-scene data, which are essential for robust training, are inherently scarce. To address these limitations, we adopt two augmentation strategies, illustrated in Figure 4: Expression Augmentation. We employ X-Nemo [52] to edit source face so that it matches the target expression, thereby enriching expression diversity (Figure 4a). Portrait Relighting. We apply LBM [5] to relight faces and replace backgrounds under varying illumination conditions, enhancing robustness to lighting variation (Figure 4b). After augmentation, we perform identity verification using face recognition model and discard pairs with low resemblance to ensure high-quality ID consistency. Resemblance filter is also applied to raw multi-scene data without augmentation. Finally, our pipeline constructs total of 50.2M pairs, consisting of 21.5M single-scene pairs, 7.7M multi-scene pairs, and 21.0M augmented single-scene pairs. For single-scene pairs where the condition image is directly cropped from the target, we additionally apply background augmentation by segmenting the human subject and replacing the background. During training, these different types of pairs are retrieved through weighted sampling to balance data diversity."
        },
        {
            "title": "5.1 Benchmark and Metrics",
            "content": "We construct an evaluation benchmark comprising 40 subjects and 20 unbiased text prompts, resulting in total of 800 test videos. The subject set consists of (1) 10 celebrity photos, (2) 10 AI-synthesized portraits, 6 In bustling open-air market filled with vibrant colors and aromatic spices, an animated person of mixed cultural background engages enthusiastically... person sits at wooden table in warmly lit kitchen, joyfully eating plate of steaming dumplings... lift each dumpling with chopsticks... 2 - e S V t P G I - t u Figure 5 Qualitative comparison with baseline methods. Competing methods often exhibit issues such as unrealistic actions (row 1 example 2), copy-pasting effects of background (row 4 example 2) or lighting (row 5 example 2), or poor identity resemblance (row 1 example 1, row 3 example 2). In contrast, Lynx consistently preserves facial identity with high fidelity, while producing natural motion, coherent lighting, and flexible scene adaptation. and (3) 20 in-house licensed photos spanning diverse demographic groups to capture racial and ethnic diversity. The text prompts are generated using ChatGPT-4o, guided by carefully designed in-context examples, and explicitly crafted to avoid bias with respect to race, age, gender, motion, and other attributes. We evaluate Lynx along three key dimensions: face resemblance, prompt following, and video quality. Face resemblance. To measure identity fidelity, we compute cosine similarity using three independent feature extractors. These include two publicly available ArcFace implementations, facexlib2 and insightface3, together with our in-house face recognition model. Employing multiple extractors reduces reliance on single feature space and yields more reliable assessment of identity preservation. Prompt following and video quality. To assess semantic alignment and perceptual quality, we construct an automated evaluation pipeline based on the Gemini-2.5-Pro API. In this pipeline, Gemini is instructed with task-specific prompts to assign scores across four dimensions: (1) prompt alignment, which evaluates consistency between the generated video and the input text description, (2) aesthetic quality, which measures visual appeal and composition, (3) motion naturalness, which captures the smoothness and realism of temporal 2https://github.com/xinntao/facexlib 3https://github.com/deepinsight/insightface 7 dynamics, and (4) general video quality, which provides an overall judgment that integrates multiple aspects of perceptual fidelity. This evaluation framework allows scalable and multi-faceted assessment of generated videos beyond traditional expert-model-based metrics."
        },
        {
            "title": "5.2 Qualitative Results",
            "content": "Figure 5 presents qualitative comparisons between Lynx and state-of-the-art baselines. As shown, existing methods frequently struggle with identity preservation, producing faces that drift away from the reference subject or lose fine-grained details (row 1 example 1, row 3 example 2). Moreover, they often generate unrealistic behaviors (row 1 example 2), copy-pasting effects of background (row 4 example 2) or lighting (row 5 example 2). In contrast, Lynx successfully maintains strong identity consistency across diverse prompts, while achieving natural motion, coherent visual details, and high-quality scene integration. These results demonstrate that our model effectively balances identity preservation, prompt alignment, and video realism, outperforming existing approaches both in terms of fidelity and controllability."
        },
        {
            "title": "Face Resemblance",
            "content": "facexlib insightface in-house SkyReels-A2 [11] VACE [22] Phantom [31] MAGREF [9] Stand-In [45] Lynx (ours) 0.715 0.594 0.664 0.575 0. 0.779 0.678 0.548 0.659 0.510 0.576 0.699 0.725 0.615 0.689 0.591 0.634 0.781 Table 1 Quantitative comparison of Lynx with recent personalized video generation models on face resemblance. Scores are computed with three independent evaluators: facexlib, insightface, and our in-house face recognition model. Lynx achieves the best overall identity consistency across all evaluators, while SkyReels-A2 ranks second but shows weak prompt following due to reliance on copypaste mechanisms, as shown in Table 2."
        },
        {
            "title": "Prompt Following Aesthetic Motion Naturalness Video Quality",
            "content": "SkyReels-A2 [11] VACE [22] Phantom [31] MAGREF [9] Stand-In [45] Lynx (ours) 0.471 0.691 0.690 0.612 0.582 0.722 0.704 0.846 0.825 0.787 0.807 0. 0.824 0.851 0.828 0.812 0.823 0.837 0.870 0.935 0.888 0.886 0.926 0.956 Table 2 Quantitative comparison of Lynx with competing methods on prompt following, aesthetic quality, motion naturalness, and overall video quality, evaluated using the Gemini-2.5-Pro pipeline. Lynx achieves the highest performance in three out of four metrics, with particularly strong results in prompt alignment and overall quality. Table 1 reports quantitative comparisons across face resemblance, prompt following, and video quality. On identity preservation, Lynx consistently outperforms all baselines, achieving the highest resemblance scores under facexlib, insightface, and our in-house face recognition model. SkyReels-A2 ranks second on identity resemblance, but its reliance on copypaste generation introduces visual artifacts and leads to weak semantic alignment, as reflected in its poor prompt following performance as shown in Table 2. Phantom demonstrates strong prompt alignment but does so at the expense of identity fidelity, suggesting trade-off between semantic consistency and subject preservation. In contrast, Lynx achieves the best balance, combining superior identity fidelity with competitive prompt alignment, highlighting the advantage of our adapter-based design. Table 2 further evaluates prompt following, aesthetic quality, motion naturalness, and overall video quality 8 using the Gemini-2.5-Pro evaluation pipeline. Lynx delivers the best performance in four out of five metrics, including prompt alignment, aesthetics, and overall video quality, which demonstrates the perceptual quality of our outputs. VACE attains the highest score in motion naturalness, reflecting its strong temporal modeling capability, while Phantom and Stand-In perform competitively across most dimensions but lag behind in overall video quality. These results show that Lynx not only preserves identity more effectively but also produces videos that are semantically accurate, visually appealing, and of high perceptual quality. Figure 1 provides visual summary of these comparisons, where Lynx demonstrates consistent superiority across identity resemblance and perceptual quality dimensions, while remaining competitive in motion naturalness. The combined evidence from multiple evaluators underscores the robustness of our approach and establishes Lynx as new state of the art in personalized video generation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced Lynx, high-fidelity framework for personalized video generation that preserves subject identity from single reference image. The model incorporates two lightweight adapters: the IDadapter, which encodes ArcFace-derived identity tokens, and the Ref-adapter, which integrates VAE-based dense features through frozen reference pathway. Together, these components enable robust identity fidelity while maintaining motion naturalness and visual coherence. We evaluated Lynx on curated benchmark of 40 subjects and 20 unbiased prompts, totaling 800 test cases across diverse identities and scenarios, and found that it achieves state-of-the-art performance in face resemblance while also delivering competitive prompt following and strong video quality. Overall, Lynx advances personalized video generation with scalable adapter-based framework that balances identity preservation, controllability, and realism, and lays the groundwork for future extensions toward multi-modal and multi-subject personalization."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [5] Clément Chadebec, Onur Tasar, Sanjeev Sreetharan, and Benjamin Aubin. Lbm: Latent bridge matching for fast image-to-image translation. arXiv preprint arXiv:2503.07535, 2025. [6] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. [7] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:2252 2274, 2023. [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [9] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [11] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [13] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [14] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 112, 2025. [15] Google DeepMind. Veo: Advanced text-to-video generation. https://deepmind.google/technologies/veo/, 2025. Accessed: 2025-08-30. [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 10 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [19] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. [20] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. [21] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [22] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [23] Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, and Sunghyun Cho. Flovd: Optical flow meets video diffusion model for enhanced camera-controlled video synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20402049, 2025. [24] Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James M. Rehg, and Tobias Hinz. Shotadapter: Text-to-multi-shot video generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [25] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [26] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [28] Guangzhao Li, Yanming Yang, Chenxi Song, and Chi Zhang. Flowdirector: Training-free flow steering for precise text-to-video editing. arXiv preprint arXiv: 2506.05046, 2025. [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [30] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [31] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [33] OpenAI. Sora: text-to-video diffusion model. https://openai.com/sora, 2024. Accessed: 2025-08-30. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 11 [38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [40] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [41] Su, Lu, Pan, Murtadha, Wen, and YL Roformer. Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2023. [42] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [43] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [44] Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, and Yulan Guo. Videodirector: Precise video editing via text-to-video models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25892598, 2025. [45] Bowen Xue, Qixin Yan, Wenjing Wang, Hao Liu, and Chen Li. Stand-in: lightweight and plug-and-play identity control for video generation. arXiv preprint arXiv:2508.07901, 2025. [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [47] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arxiv:2308.06721, 2023. [48] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. [49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [50] Yimeng Zhang, Tiancheng Zhi, Jing Liu, Shen Sang, Liming Jiang, Qing Yan, Sijia Liu, and Linjie Luo. Id-patch: Robust id association for group photo personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025. [51] Zhenghao Zhang, Zuozhuo Dai, Long Qin, and Weizhi Wang. Effived: Efficient video editing via text-instruction diffusion models. arXiv preprint arXiv:2403.11568, 2024. [52] Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, and Yebin Liu. X-nemo: Expressive neural motion reenactment via disentangled latent attention. arXiv preprint arXiv:2507.23143, 2025."
        }
    ],
    "affiliations": [
        "Intelligent Creation, ByteDance"
    ]
}