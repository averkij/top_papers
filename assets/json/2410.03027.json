{
    "paper_title": "MLP-KAN: Unifying Deep Representation and Function Learning",
    "authors": [
        "Yunhong He",
        "Yifeng Xie",
        "Zhengqing Yuan",
        "Lichao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in both representation learning and function learning have demonstrated substantial promise across diverse domains of artificial intelligence. However, the effective integration of these paradigms poses a significant challenge, particularly in cases where users must manually decide whether to apply a representation learning or function learning model based on dataset characteristics. To address this issue, we introduce MLP-KAN, a unified method designed to eliminate the need for manual model selection. By integrating Multi-Layer Perceptrons (MLPs) for representation learning and Kolmogorov-Arnold Networks (KANs) for function learning within a Mixture-of-Experts (MoE) architecture, MLP-KAN dynamically adapts to the specific characteristics of the task at hand, ensuring optimal performance. Embedded within a transformer-based framework, our work achieves remarkable results on four widely-used datasets across diverse domains. Extensive experimental evaluation demonstrates its superior versatility, delivering competitive performance across both deep representation and function learning tasks. These findings highlight the potential of MLP-KAN to simplify the model selection process, offering a comprehensive, adaptable solution across various domains. Our code and weights are available at \\url{https://github.com/DLYuanGod/MLP-KAN}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 7 2 0 3 0 . 0 1 4 2 : r Preprint. MLP-KAN: UNIFYING DEEP REPRESENTATION AND FUNCTION LEARNING Yunhong He Yifeng Xie Zhengqing Yuan2 Lichao Sun1 1Lehigh University 2University of Notre Dame"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in both representation learning and function learning have demonstrated substantial promise across diverse domains of artificial intelligence. However, the effective integration of these paradigms poses significant challenge, particularly in cases where users must manually decide whether to apply representation learning or function learning model based on dataset characteristics. To address this issue, we introduce MLP-KAN, unified method designed to eliminate the need for manual model selection. By integrating MultiLayer Perceptrons (MLPs) for representation learning and Kolmogorov-Arnold Networks (KANs) for function learning within Mixture-of-Experts (MoE) architecture, MLP-KAN dynamically adapts to the specific characteristics of the task at hand, ensuring optimal performance. Embedded within transformer-based framework, our work achieves remarkable results on four widely-used datasets across diverse domains. Extensive experimental evaluation demonstrates its superior versatility, delivering competitive performance across both deep representation and function learning tasks. These findings highlight the potential of MLPKAN to simplify the model selection process, offering comprehensive, adaptable solution across various domains. Our code and weights are available at https://github.com/DLYuanGod/MLP-KAN."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, deep learning has evolved from early neural network concepts to sophisticated architectures, such as transformer networks (Vaswani, 2017), driven by advancements in computational resources and the availability of large datasets, thereby achieving remarkable performance across diverse applications. Along with important technological breakthroughs, representation learning (OpenAI, 2023a; Anthropic, 2024; OpenAI, 2023b; Touvron et al., 2023) and function learning (Narayan et al., 1996; Zhang et al., 2022; Wu et al., 2005) moments of prominence and have been extensively explored and utilized in various research and application tasks related to data and learning nowadays. At the same time, the focus of function learning research has shifted from simple function fitting to deep learning (Cuomo et al., 2022; Cai et al., 2021), which excels in tasks requiring precise function approximation and has seen new advancements, particularly in its applicability to univariate function tasks. The key difference between representation learning and function learning lies in their objectives: representation learning aims to extract features from data to understand its underlying structure (Bengio et al., 2013), while function learning focuses on creating direct mappings between inputs and outputs, making it more suited for tasks requiring precise functional relationships (Zupan et al., 1997). In this paper, we introduce MLP-KAN, novel framework that unifies two distinct learning approaches into cohesive system, utilizing the Mixture of Experts (MoE) methodology Jiang et al. (2023). Within the architecture of MLP-KAN, Multi-Layer Perceptrons (MLP) (Rumelhart et al., 1986) function as representation experts, while Kernel Attention Networks (KAN) (Liu et al., 2024) are designated as function experts. The MoE mechanism efficiently routes inputs to the appropriate expert, significantly enhancing both efficiency and performance across diverse range of tasks. *Yunhong and Yifeng are independent undergraduate students, remotely work with Lichao Sun. Lichao Sun is corresponding author: lis221@lehigh.edu 1 Preprint. Figure 1: The comparison between the MLP, KAN, and our proposed MLP-KAN. In the domains of Computer Vision and Natural Language Processing, the goal is to achieve the highest accuracy possible. In contrast, for the Symbolic Formula Representation task, the objective is to minimize the root mean square error (RMSE). The numbers are the average values of the experimental results. MLP-KAN effectively combines the strengths of both, ensuring strong performance in representation and function learning, and eliminating the need for task-specific model selection. MLP-KAN was developed to address the problem users encounter when determining whether to apply representation learning or function learning models across diverse datasets. By integrating MLPs and KANs within mixture-of-experts framework, this architecture dynamically adapts to the specific characteristics of the task, ensuring optimal performance without requiring manual model selection. The main challenge in our method is effectively integrating MLPs and KANs, ensuring the right model is selected for each task without compromising performance. In additional, balancing the differing training needs of representation and function learning while maintaining efficiency across diverse datasets is complex. The main challenge in our method is effectively integrating MLPs and KANs, ensuring the right model is selected for each task without compromising performance, as shown in Figure 1. In additional, balancing the differing training needs of representation and function learning while maintaining efficiency across diverse datasets is complex. To address the challenge of effectively integrating MLPs and KANs within the MoE framework, we utilized soft MoE approach. This method enables dynamic and flexible routing between MLPs for representation learning and KANs for function learning. By incorporating this MoE system within transformer framework, the model can seamlessly perform deep representation learning or deep function learning, adapting to the specific nature of the task at hand while maintaining efficiency across diverse datasets. The main contributions of this work are as follows: We present MLP-KAN, unified framework that synergizes MLP for representation learning with KAN for function learning. This novel architecture leverages MoE mechanism to dynamically route tasks between representation and function experts, addressing the challenge of selecting the appropriate learning paradigm for diverse datasets. We propose flexible and versatile model by integrating MLP-KAN within the transformer architecture, enabling efficient performance across both representation and function learning tasks. This integration enhances model capability and improves performance across broad range of tasks, including computer vision, natural language processing, and symbolic formula representation. We perform extensive experimental evaluations, demonstrating that MLP-KAN consistently outperforms or matches state-of-the-art models such as MLP and KAN on widely recognized benchmarks, including computer vision,nature language processing, and functional dataset. Our approach achieves superior accuracy in representation learning tasks and lower RMSE in function learning tasks, underscoring its universal applicability across diverse domains."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Deep Representation Learning. Deep representation learning has gained significant attention due to its ability to automatically discover hierarchical feature representations from raw data (Butepage et al., 2017; Zhong et al., 2016; Long et al., 2018), outperforming traditional hand-crafted feature 2 Preprint. extraction techniques. The introduction of deep learning methods, such as MLP based convolutional neural networks (Li et al., 2021) and recurrent neural networks, enabled breakthroughs in areas like image recognition (Zoph et al., 2018; He et al., 2016), object detection (Zhao et al., 2019; Yu et al., 2016; Liu et al., 2020), and natural language processing (Chowdhary & Chowdhary, 2020; Khurana et al., 2023) by capturing more abstract and high-level features. Recent advancements in deep architectures, including transformer-based models (Gillioz et al., 2020), have further pushed the boundaries of representation learning, proving highly effective across diverse domains. For example, generative AI, such as large language models (LLMs) (Yao et al., 2024; Zhao et al., 2023), has garnered significant attention for its ability to generate coherent, contextually relevant text and learn deep representations from vast amounts of unstructured data. LLMs like GPT-4o (OpenAI, 2024) and LLaMA (Touvron et al., 2023) utilize MLP based transformer architectures, which excel at capturing long-range dependencies in sequential data, allowing them to perform tasks such as text generation, summarization, and translation with remarkable accuracy. Beyond natural language processing, LLMs have also influenced other fields, including code generation (Chung et al., 2024; Li et al., 2022), medical diagnosis (Kononenko, 2001; Amato et al., 2013), and drug discovery (Drews, 2000; Sliwoski et al., 2014), by leveraging their deep learning capabilities to model complex relationships in data. These advancements highlight the growing importance of deep representation learning in not only understanding and generating human-like text but also in solving wide range of interdisciplinary challenges (Newell et al., 2001). In these models, MLP play crucial role as fundamental building blocks, serving as dense layers that transform and learn high-dimensional representations by mapping inputs to deeper abstract features (Donoho et al., 2000). Deep Function Learning. Deep function learning focuses on capturing complex mathematical relationships and patterns within data, particularly in scientific and engineering domains (Sarker, 2021; Shen, 2018; Karpatne et al., 2017). Techniques such as Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) have emerged as powerful tools for solving partial differential equations (PDEs) (Evans, 2022) by embedding physical laws into neural network architectures, allowing for accurate modeling of phenomena governed by underlying physical principles (Raissi et al., 2019; Cuomo et al., 2022). Beyond traditional neural networks, deep function learning leverages overparameterized models, which enable the precise interpolation of data, even in the presence of noise, enhancing both generalization and optimization performance (Karniadakis et al., 2021; Advani et al., 2020; Chen et al., 2022). Recent advancements have demonstrated the potential of these methods for tasks such as surrogate modeling (Razavi et al., 2012), sensitivity analysis (Christopher Frey & Patil, 2002; Lenhart et al., 2002), and discovery of new scientific relationships (Wren et al., 2004; Klahr & Simon, 1999). KAN are highly effective for function learning due to their ability to capture complex non-linear relationships through learnable spline-based univariate functions, offering superior approximation capabilities and scaling compared to traditional MLP (Yu et al., 2024; Liu et al., 2024; Zhang, 2024; Vaca-Rubio et al., 2024)."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Table 1: Comparison between MLP and KAN. Feature Activation Functions Weight Structure Layer Architecture Error Scaling Scaling Law Expressiveness MLPs Fixed functions (e.g., ReLU, SiLU) Scalar weights Standard fixed depth Limited by dimensionality ℓ α with lower α Suited for general representation learning (cid:16)(cid:80)n i=1 ciBi(x) KANs φ(x) = (cid:80)k Spline-based weights φ(x) (cid:17) p=1 φq,p(xp) Φq (KAN )Cm CGk1+m ℓ α with higher α = 4 Suited for functional learning KAN are inspired by the Kolmogorov-Arnold Representation Theorem (Liu et al., 2024), which asserts that any multivariate continuous function (x) can be decomposed into sum of univariate functions. This is formally stated as: 3 Preprint. (x) = 2n+1 (cid:88) Φq (cid:32) (cid:88) q=1 p=1 (cid:33) φq,p(xp) (1) where φq,p(xp) and Φq are univariate functions, summing over and p. Unlike traditional MultiLayer Perceptrons (MLPs), which use fixed activation functions at each neuron, KANs introduce learnable univariate activation functions on the edges between layers (Vaca-Rubio et al., 2024; Aghaei, 2024). Each weight in KANs is replaced by learnable spline function: φ(x) = (cid:88) i=1 ciBi(x) (2) where Bi(x) are basis functions (such as B-splines) and ci are trainable coefficients (Eilers & Marx, 1996). This spline-based approach allows KANs to better capture non-linear relationships, particularly in high-dimensional tasks where MLPs tend to struggle. KANs also generalize the original two-layer architecture of the theorem by stacking multiple layers of univariate functions, expressed as: KAN (x) = (ΦL1 ΦL2 Φ1 Φ0)(x) (3) The approximation capabilities of KANs scale better compared to MLPs, as shown in Table 1. The error bound for KANs with splines of order and grid size is (KAN )Cm CGk1+m where is constant, and represents the order of derivatives considered. Furthermore, KANs exhibit superior neural scaling laws, with the test loss decreasing as ℓ α where is the number of parameters and α depends on the spline order k. For cubic splines (k = 3), KANs achieve α = 4, outperforming MLPs, which often cannot reach these scaling efficiencies. This makes KANs particularly effective for high-dimensional function approximation (Sprecher & Draghici, 2002; Koppen, 2002)."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "4.1 MLP-KAN As shown in Figure 2, our proposed MLP-KAN is composed of experts, which can be classified into two types: representation experts and function experts. Representation experts, based on MLP architectures, focus on learning rich feature representations, while function experts, utilizing FasterKAN architectures, specialize in tasks requiring smooth and precise interpolation over continuous data points. The experts are dynamically selected and routed using gating mechanism to improve computational efficiency and maintain high performance. Representation Expert. In the context of MLP-KAN models, half of the experts are designed as representation experts, utilizing multi-layer perceptrons (MLPs). These experts excel in tasks requiring the learning of rich feature representations, such as image classification. Specifically, the architecture of single MLP-based expert is defined as follows: Experti = MLP(X) for = 1, . . . , (4) In this configuration, each expert processes the input through multiple fully connected layers that employ the SiLU (Sigmoid Linear Unit) activation function. Unlike ReLU (Rectified Linear Unit) (Hahnloser et al., 2000), SiLU provides smooth gradients and mitigates the issue of dying neurons, enhancing the robustness and efficiency of learning. The process of forward propagation within each expert is executed as follows: Given an input RBN D, where is the batch size, is the sequence length, and is the feature dimension, the transformation through the MLP involves applying linear transformation followed by the SiLU activation function: 4 Preprint. Figure 2: The framework combines soft mixture of experts (MoE) with unification of MLPs and KANs, denoted as the MLP-KAN module, to dynamically select experts for each token. The input tokens are passed through multi-headed self-attention mechanism followed by layer normalization. The routing process involves soft weighting of experts for each slot and token via linear combinations and softmax layer per slot and token. MLP and KAN experts are arranged in parallel, and based on the inputs characteristics, either MLP or KAN is selected for computation, enhancing the models ability to handle diverse representations efficiently. The gating mechanism determines the most relevant expert for each token, improving overall computational efficiency. This architecture retains the residual connections of the traditional Transformer while expanding its capacity to model complex functional and representational data. h(1) = SiLU(W(1)X + b(1)), h(2) = W(2)h(1) + b(2) (5) are the weight matrices, and b(1) RH and b(2) RD where W(1) RDH and W(2) RHD are the bias vectors of the corresponding layers. The output h(2) is passed on for further processing. Function Expert. The other half of the experts in MLP-KAN are defined as function experts to handle specialized data, particularly in functional datasets. These experts are based on the FasterKAN (Delis, 2024) architecture, which is known for its strong performance in tasks requiring smooth interpolation over continuous data points. We define the function expert based on the FasterKAN architecture as follows: Experti = FasterKAN(X) for = 2 + 1, . . . , (6) This architecture enables the function expert to capture non-linear transformations effectively by utilizing grid-based mechanism. Each FasterKAN maps input features through learned reflection switch functions that operate on structured grid over the input space. The transformation of an input RBN through the experts layers follows these steps: First, each input feature vector is normalized using LayerNorm to stabilize the distribution during training: Xnorm = LayerNorm(X) (7) Subsequently, the reflectional switch function ϕ(x) computes the differences between the normalized input, predefined grid points and hyper-parameter denominator, followed by non-linear transformation to approximate smooth basis functions: ϕ(X) = 1 tanh (cid:18) grid (cid:19) denominator (8) 5 Preprint. Lastly, the computed basis values are passed through spline transformation Wspline to map the input to the output dimension: = Wspline ϕ(X) (9) By integrating FasterKAN for half of the experts, MLP-KAN is well-equipped to process functional data, leveraging FasterKANs interpolation across smooth grid representation. The remaining experts can follow alternative architectures, allowing MLP-KAN to dynamically select the optimal model based on the inputs characteristics. Gating Mechanism. In MLP-KAN, the gating mechanism serves pivotal function in dynamically routing input tokens to the most relevant experts. This mechanism efficiently selects subset of experts for each input sequence, reducing computational overhead while maintaining robust model performance. Given an input sequence RBN D, the gating mechanism computes the similarity between the input tokens and set of learnable slot embeddings RN ESD, where is the number of experts and is the number of slots per expert. This similarity is calculated as follows: logitsb,n,e,s = Xb,n,:, Ee,s,:, for [1, B], [1, ], [1, E], [1, S] (10) where , denotes the dot product, and the resulting logits logits RBN ES represent the unnormalized attention scores between each token and the expert slots. Next, softmax function is applied along the expert and slot dimensions to compute the dispatch weights α RBN ES, determining the contribution of each token to each expert: αb,n,e,s = exp(logitsb,n,e,s) e,s exp(logitsb,n,e,s) (cid:80) (11) These dispatch weights α are then used to aggregate the input tokens across the sequence for each expert, resulting in routed inputs RBESD: zb,e,s,: = (cid:88) n=1 αb,n,e,sXb,n,: (12) Finally, each expert processes its routed inputs, and the outputs from all experts are aggregated using softmax-normalized combination weights. This ensures that the final output F(X) is unified combination of contributions from all experts, based on the initial input X. 4.2 ARCHITECTURE While the traditional Transformer architecture has shown remarkable success in various tasks, it still encounters limitations in scaling efficiently, particularly when dealing with diverse and complex input distributions. To address these challenges, we draw inspiration from two primary sources: the MLP-KAN paradigm, which allows dynamic routing of tokens to different experts, and the blocksparse operations that enable efficient expert utilization. As depicted in Figure 3 and Equation (13), we replaced the standard MLP layer in the Transformer block with an MLP-KAN-based module to improve the models capacity in handling diverse token representations. This modification helps the model better capture complex dependencies while maintaining computational efficiency by selecting only subset of experts for each token. = + MHA(LN(X)) + F(LN(X + MHA(LN(X)))) (13) Where LN denotes the layer normalization (Ba et al., 2016) applied to the input and intermediate states, respectively, MHA represents the multi-head self-attention mechanism that captures contextual information across the token sequence. Our proposed MLP-KAN replaces the traditional MLP, 6 Preprint. where experts are dynamically selected based on the input through gating mechanism, ensuring efficient routing of tokens to the most relevant experts. represents the input data after passing through the attention mechanism, and represents the output data after the combined processing of the MoE module and residual connections. This modification allows for more flexible token-wise computations while maintaining the overall structure of the Transformer block."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Datasets. We have validated the effectiveness of our method on several public datasets. In representation learning, we have validated the CIFAR-10, CIFAR-100, and mini-ImageNet datasets (Krizhevsky et al., 2010; Vinyals et al., 2016) in the field of computer vision, and the SST2 dataset (Socher et al., 2013) in the field of natural language processing. In function learning, we have validated thirty functions on the Feynman dataset (Udrescu & Tegmark, 2020). The CIFAR-10 and CIFAR-100 datasets are the tasks of image classification, both consisting of 50,000 images for the training set and 10,000 images for the test set. However, the former has only 10 categories, while the latter has 100 categories. miniImageNet is widely-used benchmark dataset for few-shot learning tasks, consisting of 60,000 color images divided into 100 classes, with 600 images per class. Both CV datasets use top-1 accuracy (top1-acc.) and top-5 accuracy (top5-acc.) as metrics to judge the models prediction accuracy for single category and the top five categories, respectively. SST-2 is dataset for sentiment analysis derived from movie reviews, containing sentences labeled as positive or negative, used to train models to understand textual emotional content. Specifically, we use the F1 score (F1) and the accuracy score (Acc) to measure performance. The Feynman dataset is commonly used for symbolic regression tasks, which involve finding mathematical equation that describes the output variable from set of input variables. The root-mean-square error (RMSE) can quantitatively assess the models prediction accuracy and performance, and here we use the lowest test RMSE from the validation to demonstrate this, where smaller value indicates the higher prediction accuracy of the model. Figure 3: Architecture of the transformer encoder with MLP-KAN Integration. Training and Evaluation Details. To comprehensively demonstrate the superiority of MLPKAN, our experimental setup involved comparisons with MLP and KAN. These extensive experiments demonstrate that our method can be universally applied across various domains and consistently achieves excellent results. All experiments were conducted using four A100 GPUs. During the training phase, we meticulously tuned parameters to optimize the learning process. For datasets related to representation learning, we use batch size of 128, whereas for datasets related to functional learning, we set the batch size to 4. The learning rate was initially set at 5e-5, and the training continues until convergence. We applied dropout to the output of each MLP-KAN using dropout rate of 0.1. Regarding the hyperparameters of MLP-KAN, we configured = 8 (i.e., 8 experts) and = 2 (i.e., top2 experts). 5.2 FUNCTION LEARNING The results from Table 2 demonstrate that MLP-KAN significantly outperforms both MLP and KAN across variety of equations. or simpler equations like I.6.20a, MLP-KAN achieves an RMSE of 3.87 104, which is much lower than KANs 8.82 104 and MLPs 1.37 101. This illustrates our methods ability to accurately capture basic functional relationships with far fewer errors than MLP, which often over-parameterizes for simple tasks. For more complex equations involving multiple variables, such as I.9.18, MLP-KAN maintains strong advantage, achieving an RMSE of 3.13 103 compared to KANs 4.87 103 and MLPs much higher 1.40 102. This shows that our MLP-KAN scales effectively and can manage the intricacies of complex interactions that MLP struggles to capture without excessive parameters. Our proposed MLP-KAN demonstrates versatility across different types of equations, such as in I.12.5, where it achieves lower RMSE Preprint. Table 2: Comparison of losses for Feynman Equations. Results highlighted in bold represent the best performance in the comparison, while those underlined represent the second-best results. Feynman Eq. I.6.20a I.6.20 I.6.20b I.8.4 I.9.18 I.10. I.11.19 I.12.1 I.12.2 I.12.4 I.12.5 I.12.11 I.13.4 I.13.12 I.14.3 I.14.4 I.15.3x I.15.3t I.15.10 I.16.6 I.18.4 I.18.5 I.18.16 I.24.6 I.25.13 I.26.2 I.27.6 I.29.4 I.29.16 I.30.3 (cid:113) Original Formula eθ2/2 2π eθ2/2σ2 2πσ2 e(θθ1)2/2σ2 2πσ2 (cid:112)(x2 x1)2 + (y2 y1)2 Gm1m2 (x2x1)2+(y2y1)2+(z2z1)2 m0 1 v2 c2 x1y1 + x2y2 + x3y3 µNn q1q2 4πϵr2 q1 4πϵr2 q2Ef q(Ef + Bv sin(θ)) 1 2 m(v2 + u2 + w2) (cid:17) (cid:16) 1 Gm1m2 r2 mgz 1 2 ksx2 xut 1 u2 c2 tux/c2 (cid:113) 1 u2 c2 m0v 1 v2 c2 u+v 1+ uv c2 m1r1+m2r2 m1+m2 rF sin(θ) mrv sin(θ) 4 m(ω2 + ω2 1 r1 0)x2 (cid:113) (cid:113) 1 arcsin(n sin(θ2)) 1 1/d1+n/d2 ω (cid:112)x2 1 + x2 2 2x1x2 cos(θ1 θ2) I0 sin2(nθ/2) sin2(θ/2) Variables θ θ, σ KAN loss 8.82 104 MLP loss MLP-KAN loss 1.37 101 3.87 10 1.42 102 1.20 101 8.44 103 θ, θ1, σ x1, x2, y1, y2 G, m1, m2, x1, x2, y1, y2, z1, z2 m0, v, 1.59 102 1.16 101 4.58 103 1.91 101 4.87 103 1.40 102 2.04 102 3.22 101 x1, y1, x2, y2, x3, y3 µ, Nn q1, q2, ϵ, q1, ϵ, q2, Ef q, Ef , B, v, θ m, v, u, G, m1, m2, r1, r2 m, g, ks, x, u, t, 9.89 102 3.37 102 3.34 101 9.22 103 4.75 102 6.75 103 5.62 103 4.87 102 2.93 103 3.25 101 1.85 101 6.38 102 2.10 102 1.26 101 8.69 103 3.87 102 8.98 103 1.64 101 5.13 103 1.11 101 3.50 102 3.48 101 4.99 103 1.23 102 3.13 103 1.46 101 2.65 102 7.17 103 3.06 103 3.86 103 3.61 103 3.56 102 9.68 103 9.78 103 2.80 103 6.79 103 8.52 102 t, u, x, m0, v, u, v, m1, r1, m2, r2 r, F, θ m, r, v, θ m, ω, ω0, q, n, θ2 d1, d2, ω, x1, x2, θ1, θ2 I0, n, θ 3.69 102 3.44 101 7.18 102 2.36 102 2.27 101 1.47 102 8.73 103 1.45 101 6.18 103 2.33 101 2.03 101 5.67 102 1.02 101 6.88 102 6.20 102 7.99 103 1.07 102 5.17 101 4.45 101 2.74 102 5.97 103 1.42 101 2.26 101 5.27 103 2.91 101 8.48 102 4.07 101 2.24 10 1.06 102 2.26 102 4.93 102 3.40 102 5.87 103 8.33 103 1.15 102 6.18 103 3.45 103 5.31 102 1.99 101 Table 3: Comparison of results in representation learning. Results highlighted in bold represent the best performance in the comparison, while those underlined represent the second-best results. Method Dataset: CIFAR-10 Dataset: CIFAR-100 Dataset: mini-ImageNet Dataset: SST2 Acc1 0.904 KAN 0.922 MLP MLP-KAN 0.920 Acc5 0.989 0.997 0.996 Acc1 0.731 0.752 0.750 Acc5 0.933 0.958 0.952 Acc1 0.623 0.680 0. Acc5 0.803 0.845 0.843 Acc 0.925 0.931 0.935 F1 0.925 0.930 0.933 (3.61 103) than both KAN and MLP. The results reflect its ability to adapt dynamically to different functional forms, from basic algebraic equations to those involving physical constants and nonlinearities. physics-based equations like I.15.3t, which involves relativistic transformations, MLP-KAN outperforms both KAN and MLP with an RMSE of 7.18 102 compared to KANs 3.69 102 and MLPs 3.44 101. This indicates the superior ability of our method to generalize across equations that require deep understanding of physical laws. Our proposed achieves superior performance without the excessive parameter overhead required by MLPs, making it computationally efficient. For example, in I.14.4, MLP-KAN achieves an RMSE of 6.79 103, far outperforming MLPs 1.11 101, demonstrating that MLP-KAN can achieve better accuracy with fewer resources. Across almost all equations, MLP-KAN consistently outperforms both KAN and MLP, often achieving RMSEs that are orders of magnitude smaller. This consistent superiority highlights MLP-KAN versatility and adaptability to both simple and complex mathematical forms, making it the most robust and efficient solution for function learning across diverse domains. 8 Preprint."
        },
        {
            "title": "5.3 REPRESENTATION LEARNING",
            "content": "As shown in Table 3, our proposed MLP-KAN shows consistent high performance, demonstrating particular strengths across diverse datasets. Notably, MLP-KAN achieves the second-best results for both top-1 and top-5 accuracy metrics on CIFAR-10, with scores of 0.920 and 0.996, respectively, closely trailing the MLP method. It also performs competitively on CIFAR-100, with only negligible 1% gap from the best method in both top-1 and top-5 accuracy metrics. Furthermore, MLP-KAN consistently outperforms KAN, which achieves an Acc1 of 0.904 for CIFAR-10 and 0.731 for CIFAR-100. On the mini-ImageNet dataset, which also focuses on image classification, similar trend is observed. In addition, MLP-KAN excels in the NLP task on the SST2 dataset, achieving the best results with an accuracy of 0.935 and an F1 score of 0.933. This superior performance highlights MLP-KANs versatility and robustness in handling not only image data but also text data, making it an excellent choice for representation learning."
        },
        {
            "title": "5.4 ABLATION AND ANALYSIS",
            "content": "Number of Experts. In this ablation study, we investigate the impact of the number of experts in the MoE component of MLP-KAN on the performance of CIFAR-10 and CIFAR-100. As observed in Table 4, increasing the number of experts from 4 to 10 yields steady improvements in both top-1 and top-5 accuracy across both datasets. Notably, the top-1 accuracy for CIFAR-10 increases from 0.908 to 0.928, while CIFAR-100 improves from 0.742 to 0.755 when the number of experts increases from 4 to 10. However, performance gains begin to diminish after using 8 experts. The difference between using 8 and 10 experts is marginal: The accuracy of the top-1 of CIFAR-10 only increases by 0.8%, and CIFAR-100 sees mere 0.5% improvement. While the model with 10 experts delivers slightly better results, the computational cost associated with using more experts becomes significant. Increasing the number of experts beyond 8 leads to higher demand for computational resources, memory usage, and training time, making the trade-off between performance and efficiency unfavorable. Table 4: Results of CIFAR-10 and CIFAR-100 accuracy with different numbers of experts. Expert CIFAR-10 (Acc1) CIFAR-10 (Acc5) CIFAR-100 (Acc1) CIFAR-100 (Acc5) 8 4 6 10 0.920 0.908 0.914 0.928 0.996 0.990 0.996 0. 0.750 0.742 0.740 0.755 0.953 0.950 0.952 0.958 Number of Top-K. In this ablation study, we examine the impact of varying the Top-K value on the accuracy of CIFAR-10 and CIFAR-100. As shown in Table 5, we experiment with Top-K values of 1, 2, and 3, measuring their impact on both top-1 and top-5 accuracy across both datasets. Interestingly, we observe that setting Top-K to 2 yields the best performance. For CIFAR-10, both top-1 and top-5 accuracies improve slightly compared to K=1. Specifically, the top-5 accuracy increases from 0.990 to 0.996, while top-1 remains constant at 0.920. similar trend is observed for CIFAR-100, where the top-1 accuracy remains stable at 0.750, but top-5 accuracy improves slightly from 0.952 to 0.953. On the other hand, when Top-K is set to 3, we notice decline in performance. Both CIFAR-10 and CIFAR-100 exhibit reduced accuracy, with CIFAR-10 top-1 accuracy dropping to 0.908 and CIFAR-100 top-1 accuracy falling to 0.742. This indicates that increasing Top-K beyond 2 leads to diminished returns, as the additional experts likely introduce more noise or less relevant expertise. Table 5: Results of CIFAR-10 and CIFAR-100 accuracy with different Top-k values. Top-k CIFAR-10 (Acc1) CIFAR-10 (Acc5) CIFAR-100 (Acc1) CIFAR-100 (Acc5) 2 1 0.920 0.920 0.908 0.996 0.990 0.991 0.750 0.750 0.742 0.953 0.952 0.949 9 Preprint."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose novel approach that effectively enhances both representation learning and function learning. This approach demonstrates excellent performance when integrated with MLP and KAN experts. Additionally, our proposed MLP-KAN can seamlessly replace the existing MLP layers in the transformer architecture. Furthermore, our extensive evaluations confirm that MLP-KAN significantly improves performance in each area. 10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Madhu Advani, Andrew Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428446, 2020. Alireza Afzal Aghaei. fkan: Fractional kolmogorov-arnold networks with trainable jacobi basis functions. arXiv preprint arXiv:2406.07456, 2024. Filippo Amato, Alberto Lopez, Eladia Marıa Pena-Mendez, Petr Vaˇnhara, Aleˇs Hampl, and Josef Havel. Artificial neural networks in medical diagnosis, 2013. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https://arxiv.org/abs/1607.06450. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. Judith Butepage, Michael Black, Danica Kragic, and Hedvig Kjellstrom. Deep representation learning for human motion prediction and classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 61586166, 2017. Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physicsinformed neural networks (pinns) for fluid mechanics: review. Acta Mechanica Sinica, 37(12): 17271738, 2021. Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin. Learning to optimize: primer and benchmark. Journal of Machine Learning Research, 23(189):159, 2022. KR1442 Chowdhary and KR Chowdhary. Natural language processing. Fundamentals of artificial intelligence, pp. 603649, 2020. Christopher Frey and Sumeet Patil. Identification and review of sensitivity analysis methods. Risk analysis, 22(3):553578, 2002. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco Piccialli. Scientific machine learning through physicsinformed neural networks: Where we are and whats next. Journal of Scientific Computing, 92(3):88, 2022. Athanasios Delis. Fasterkan. https://github.com/AthanasiosDelis/faster-kan/, 2024. David Donoho et al. High-dimensional data analysis: The curses and blessings of dimensionality. AMS math challenges lecture, 1(2000):32, 2000. Jurgen Drews. Drug discovery: historical perspective. science, 287(5460):19601964, 2000. Paul HC Eilers and Brian Marx. Flexible smoothing with b-splines and penalties. Statistical science, 11(2):89121, 1996. Lawrence Evans. Partial differential equations, volume 19. American Mathematical Society, 2022. Richard Phillips Feynman. Feynman Lectures on Physics: Electrical and Magnetic Behavior. Volume 4. Perseus Books, 1999. 11 Preprint. Anthony Gillioz, Jacky Casas, Elena Mugellini, and Omar Abou Khaled. Overview of the transformer-based models for nlp tasks. In 2020 15th Conference on computer science and information systems (FedCSIS), pp. 179183. IEEE, 2020. Richard HR Hahnloser, Rahul Sarpeshkar, Misha Mahowald, Rodney Douglas, and Sebastian Seung. Digital selection and analogue amplification coexist in cortex-inspired silicon circuit. nature, 405(6789):947951, 2000. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016. URL https://arxiv.org/abs/1603.09382. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. George Em Karniadakis, Ioannis Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422440, 2021. Anuj Karpatne, Gowtham Atluri, James Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin Kumar. Theory-guided data science: new paradigm for scientific discovery from data. IEEE Transactions on knowledge and data engineering, 29(10):23182331, 2017. Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. Natural language processing: state of the art, current trends and challenges. Multimedia tools and applications, 82(3):37133744, 2023. David Klahr and Herbert Simon. Studies of scientific discovery: Complementary approaches and convergent findings. Psychological Bulletin, 125(5):524, 1999. Igor Kononenko. Machine learning for medical diagnosis: history, state of the art and perspective. Artificial Intelligence in medicine, 23(1):89109, 2001. Mario Koppen. On the training of kolmogorov network. In Artificial Neural NetworksICANN 2002: International Conference Madrid, Spain, August 2830, 2002 Proceedings 12, pp. 474 479. Springer, 2002. Alex Krizhevsky, Geoff Hinton, et al. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 40(7):19, 2010. Lenhart, Eckhardt, Fohrer, and H-G Frede. Comparison of two different approaches of sensitivity analysis. Physics and Chemistry of the Earth, Parts A/B/C, 27(9-10):645654, 2002. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. survey of convolutional neural networks: analysis, applications, and prospects. IEEE transactions on neural networks and learning systems, 33(12):69997019, 2021. Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikainen. Deep learning for generic object detection: survey. International journal of computer vision, 128:261318, 2020. Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljaˇcic, arXiv preprint Thomas Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv:2404.19756, 2024. 12 Preprint. Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael Jordan. Transferable IEEE transactions on pattern analysis representation learning with deep adaptation networks. and machine intelligence, 41(12):30713085, 2018. Sridhar Narayan, Gene Tagliarini, and Edward Page. Enhancing mlp networks using distributed data representation. IEEE Transactions on Systems, Man, and Cybernetics, Part (Cybernetics), 26(1):143149, 1996. William Newell, Jay Wentworth, and David Sebberson. theory of interdisciplinary studies. Issues in Interdisciplinary Studies, 2001. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023a. OpenAI. Introducing chatgpt, 2023b. URL https://openai.com/blog/chatgpt. OpenAI. Gpt-4o: Multimodal intelligence for text, audio, and vision in real time. OpenAI Research Announcements, 2024. URL https://www.openai.com/gpt4o. Accessed: 2024-05-13. Maziar Raissi, Paris Perdikaris, and George Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686707, 2019. Saman Razavi, Bryan Tolson, and Donald Burn. Review of surrogate modeling in water resources. Water Resources Research, 48(7), 2012. David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by backpropagating errors. nature, 323(6088):533536, 1986. Iqbal Sarker. Deep learning: comprehensive overview on techniques, taxonomy, applications and research directions. SN computer science, 2(6):420, 2021. Chaopeng Shen. transdisciplinary review of deep learning research and its relevance for water resources scientists. Water Resources Research, 54(11):85588593, 2018. Gregory Sliwoski, Sandeepkumar Kothiwale, Jens Meiler, and Edward Lowe. Computational methods in drug discovery. Pharmacological reviews, 66(1):334395, 2014. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 16311642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. David Sprecher and Sorin Draghici. Space-filling curves and kolmogorov superposition-based neural networks. Neural Networks, 15(1):5767, 2002. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Silviu-Marian Udrescu and Max Tegmark. Ai feynman: physics-inspired method for symbolic regression. Science Advances, 6(16):eaay2631, 2020. 13 Preprint. Cristian Vaca-Rubio, Luis Blanco, Roberto Pereira, and M`arius Caus. Kolmogorov-arnold networks (kans) for time series analysis. arXiv preprint arXiv:2405.08790, 2024. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. MatchIn Advances in Neural Information Processing Systems ing networks for one shot learning. 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 36303638, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html. Jonathan Wren, Raffi Bekeredjian, Jelena Stewart, Ralph Shohet, and Harold Garner. Knowledge discovery by automated identification and ranking of implicit relationships. Bioinformatics, 20(3):389398, 2004. Dalei Wu, Andrew Morris, and Jacques Koreman. Mlp internal representation as discriminative features for improved speaker recognition. In International Conference on Nonlinear Analyses and Algorithms for Speech Processing, pp. 7280. Springer, 2005. Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, pp. 100211, 2024. Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. Unitbox: An advanced object detection network. In Proceedings of the 24th ACM international conference on Multimedia, pp. 516520, 2016. Runpeng Yu, Weihao Yu, and Xinchao Wang. Kan or mlp: fairer comparison. arXiv preprint arXiv:2407.16674, 2024. David Junhao Zhang, Kunchang Li, Yali Wang, Yunpeng Chen, Shashwat Chandra, Yu Qiao, Luoqi Liu, and Mike Zheng Shou. Morphmlp: An efficient mlp-like backbone for spatial-temporal In European Conference on Computer Vision, pp. 230248. Springer, representation learning. 2022. Jiawei Zhang. Rpn: Reconciled polynomial network towards unifying pgms, kernel svms, mlp and kan. arXiv preprint arXiv:2407.04819, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: review. IEEE transactions on neural networks and learning systems, 30(11):32123232, 2019. Guoqiang Zhong, Li-Na Wang, Xiao Ling, and Junyu Dong. An overview on data representation learning: From traditional feature learning to recent deep learning. The Journal of Finance and Data Science, 2(4):265278, 2016. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 86978710, 2018. Blaz Zupan, Marko Bohanec, Ivan Bratko, and Janez Demsar. Machine learning by function decomposition. In ICML, pp. 421429. Citeseer, 1997."
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "Building on the transformer architecture, the input initially passes through the attention layer, where the number of attention heads is set to 8. Furthermore, our proposed MLP-KAN replaces the original MLP layer and consists of 8 experts (4 MLP experts and 4 KAN experts), with 2 experts dynamically selected for computation in each forward pass. Subsequently, an additive residual connection is 14 Preprint. applied before the attention and MLP-KAN layers. We also use the normalization layer to ensure consistent numerical distribution across different feature dimensions. This improves both the stability during training and the overall performance of the model. We utilized structure with 12 identical layers. To enhance model generalization, we employ Stochastic Depth (Huang et al., 2016), which randomly drops certain layers during training. The process is as follows: Step 1: Tokenize the input into tokens Xi: = [X1, X2, . . . , Xm]; Step 2: Apply the multi-head self-attention mechanism (MHA) and layer normalization (LN), obtaining: = MHA (LN(X)) + Step 3: Continue processing with MLP-KAN to obtain the following results: = F(LN(X)) + Typically, MLP-KAN, denoted as F(), incorporates Mixture of Experts (MoE) layer comprising multiple feed-forward networks (FFNs). These FFNs form pool of experts [e1, e2, . . . ]. In this work, the MLP and KAN experts represent two distinct implementations within the FFN ensemble, together constituting the complete pool of experts. The gating mechanism, functioning as linear layer, calculates the probability of each input token being assigned to particular expert. Based on the routers output, the Top-K mechanism most probable experts are selected to process the input, and the outputs of these experts are weighted and summed to form the final result. The final representation is expressed as follows: αi(X) = egi(X) egj (X) (cid:80)E , where g(X) = represents the logit produced by the gate, and the weights are normalized via softmax function to yield the assignment probabilities for each input token across the experts. Through the Top-K operation, experts with the highest probabilities are selected to process each input token. Each selected expert processes the input, and the outputs are weighted according to softmax probabilities. These are then aggregated into weighted sum to produce the final output, which can be described as follows: F(X) = (cid:88) i=1 αi(X) ei(X). This mechanism allows each token to be effectively processed by only few relevant experts, thereby achieving efficient computation and expanding the models capacity."
        },
        {
            "title": "B DATASETS",
            "content": "B.1 CIFAR-10 DATASET The CIFAR-10 dataset is labeled subset of the 80 million tiny images dataset, containing 60,000 32x32 color images distributed across 10 mutually exclusive classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Each class contains 6,000 images, and the dataset is divided into 50,000 training images and 10,000 test images. The training images are split into five batches, each consisting of 10,000 images, while the test batch contains 10,000 randomly selected images. The dataset provides diverse representation of objects, and the classes are non-overlapping; for instance, automobile includes small vehicles like sedans and SUVs, while truck includes only larger vehicles like big trucks. Each image is represented by 1x3072 array of pixel values, where the first 1024 entries correspond to the red channel, the second 1024 to the green channel, and the last 1024 to the blue channel, stored 15 Preprint. in row-major order. The dataset is widely used for image classification benchmarks, and baseline results using convolutional neural networks have achieved test error rates of 18% without data augmentation and 11% with augmentation. The dataset is commonly accessed in Python, Matlab, or binary formats, with convenient tools for loading and processing the images for machine learning tasks. The structure of the CIFAR10 dataset as shown in Table 6. Table 6: CIFAR-10 Dataset Structure"
        },
        {
            "title": "Description",
            "content": "(50000, 32, 32, 3) Training Samples Training Labels Testing Samples Testing Labels (50000, 1) (10000, 32, 32, 3) (10000, 1) B.2 CIFAR-100 DATASET The CIFAR-100 dataset shares the same general structure as CIFAR-10 but is more granular, containing 100 classes of objects, each represented by 600 images, with 500 training images and 100 test images per class. The dataset introduces hierarchical structure where the 100 fine-grained classes are grouped into 20 superclasses (coarse labels). For example, the superclass aquatic mammal includes beaver, dolphin, otter, seal, and whale, while the superclass vehicles 1 contains bicycle, bus, motorcycle, pickup truck, and train. Similar to CIFAR-10, CIFAR-100 images are stored as 1x3072 arrays, with two label bytes for each image: one for the coarse label and one for the fine label. This dataset is often used for fine-grained classification tasks, presenting more challenging problem due to its increased number of classes and hierarchical structure. Both the CIFAR-10 and CIFAR-100 datasets have been extensively used in the computer vision community for benchmarking the performance of image classification algorithms. The structure of CIFAR-100 as shown in Table 7. Table 7: Classification Table Category Aquatic Mammals Fish Flowers Food Containers Fruits and Vegetables Household Appliances Household Furniture Insects Large Carnivores Large Man-made Outdoor Things Large Natural Outdoor Scenes Subcategory Beaver, Dolphin, Otter, Seal, Whale Aquarium Fish, Flounder, Ray, Shark, Trout Orchid, Poppy, Rose, Sunflower, Tulip Bottle, Bowl, Can, Cup, Plate Apple, Mushroom, Orange, Pear, Bell Pepper Clock, Computer Keyboard, Lamp, Phone, TV Bed, Chair, Sofa, Table, Wardrobe Bee, Beetle, Butterfly, Caterpillar, Cockroach Bear, Leopard, Lion, Tiger, Wolf Bridge, Castle, House, Road, Skyscraper Cloud, Forest, Mountain, Plain, Sea Large Omnivores and Herbivores Camel, Cow, Chimpanzee, Elephant, Kangaroo Medium-sized Mammals Non-insect Invertebrates People Reptiles Small Mammals Trees Vehicles Fox, Porcupine, Opossum, Raccoon, Skunk Crab, Lobster, Snail, Spider, Worm Baby, Boy, Girl, Man, Woman Crocodile, Dinosaur, Lizard, Snake, Turtle Hamster, Mouse, Rabbit, Shrew, Squirrel Maple, Oak, Palm, Pine, Willow Bicycle, Bus, Motorcycle, Van, Train B.3 FEYNMAN DATASET The Feynman dataset is collection of physics equations sourced from the Feynman Lectures on Physics (Feynman, 1999), designed as benchmark for symbolic regression tasks. It comprises 120 16 Preprint. formulas, primarily drawn from classical physics, including key concepts from mechanics, electromagnetism, and thermodynamics. For our purposes, we focus on the Feynman no units subset, specifically equations involving at least two variables, which reduce to one-dimensional splines. An example is the relativistic velocity addition formula, (u, v) = u+v 1+uv , where and are sampled from the range (-1, 1), and the network is trained to predict based on these inputs. The dataset serves to evaluate the ability of neural networks and other symbolic regression methods to model and predict underlying physical laws from empirical data. B.4 MINI-INMAGENET DATASET Mini-Imagenet is small-scale dataset extracted from the ImageNet dataset by the Google DeepMind team in 2016, primarily used for research in the field of few-shot learning. The total size of the dataset is approximately 3GB and contains 60,000 images divided into 100 classes, with 600 images per class. These images are of varying sizes and are saved in .jpg format. Compared to the full ImageNet dataset, Mini-Imagenet significantly reduces the data volume, making it more accessible for researchers with limited hardware resources. It is suitable for rapid prototyping and evaluating models classification performance, especially in few-shot learning scenarios. The dataset is structured as follows: Table 8: Mini-Imagenet Dataset Structure Directory mini-imagenet/ images/ train.csv val.csv test.csv Description Root directory of the dataset Folder containing all the images Label file for the training set Label file for the validation set Label file for the test set It is important to note that when this dataset was created, the labels were not evenly sampled from each class, which adds an additional challenge for models designed for few-shot learning. Researchers can use these CSV files to obtain image labels and perform training, validation, and testing. B.5 SST-2 DATASET The Stanford Sentiment Treebank (SST) is linguistically annotated dataset designed to enable detailed analysis of sentiment composition in natural language. Derived from movie reviews, this dataset includes 11,855 individual sentences, which were parsed into syntactic structures using the Stanford parser. The resulting parse trees consist of 215,154 unique phrases, all annotated by human judges to capture nuanced sentiment at various granularities. distinctive feature of the SST dataset is its ability to support research on compositional sentiment analysis, as each sub-phrase in sentence is independently labeled for sentiment. This allows for deeper understanding of how sentiment is constructed and expressed through the combination of linguistic elements. In the context of binary sentiment classification tasks, simplified version of the dataset, known as SST-2, is often used. In SST-2, neutral sentences are excluded, and the remaining sentences are categorized into either negative or positive classes. This binary classification setup has become widely adopted benchmark for evaluating sentiment analysis models."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "University of Notre Dame"
    ]
}