{
    "paper_title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
    "authors": [
        "Hyungjoo Chae",
        "Namyoung Kim",
        "Kai Tzu-iunn Ong",
        "Minju Gwak",
        "Gwanwoo Song",
        "Jihoon Kim",
        "Sunghwan Kim",
        "Dongha Lee",
        "Jinyoung Yeo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the \"world model\". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents."
        },
        {
            "title": "Start",
            "content": "WEB AGENTS WITH WORLD MODELS: LEARNING AND LEVERAGING ENVIRONMENT DYNAMICS IN WEB NAVIGATION Hyungjoo Chae 1 Namyoung Kim1 Kai Tzu-iunn Ong1 Minju Gwak1 Gwanwoo Song1 Sunghwan Kim1 Dongha Lee1 Jihoon Kim1 Jinyoung Yeo1 4 2 0 O 7 1 ] . [ 1 2 3 2 3 1 . 0 1 4 2 : r 1Yonsei University {mapoout, namyoung.kim, donalee, jinyeo}@yonsei.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have recently gained much attention in building autonomous agents. However, performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the world model. Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents policy selection without training and demonstrate superior costand timeefficiency compared to recent tree-search-based agents. The codes are publicly available at https://github.com/kyle8581/WMA-Agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have been widely applied to solve tasks in diverse domains, including web navigation, where LLMs generate action sequences (e.g., click) to accomplish user goals on websites (Shi et al., 2017; Kim et al., 2024). Despite some success (Yao et al., 2022), LLMbased web agents performance remains significantly poor in long-horizon environments such as WebArena (Zhou et al., 2023), where GPT-4 yields task success rate of 14.41% whereas humans have success rate of 78.24%. This raises question: Why do LLMs, despite their advancements, perform much worse than humans in web navigation? Humans avoid unwanted situations by considering the possible outcomes of our actions beforehand (Edwards, 1954). Such awareness of actions and outcomes is referred to as the world model (Forrester, 1995). Meanwhile, existing LLM-based web agents rely heavily on trial and error to make decisions, as they lack world models to help them foresee the outcome of an action without actually performing it (LeCun, 2022), leading to sub-optimal decision-making that is irreversible (e.g., repeatedly buying non-refundable item). Acknowledging the importance of world models, studies in robotics and reinforcement learning (RL) have proposed to incorporate world models for agents in navigation tasks. For instance, Du et al. (2023) and Yang et al. (2024) apply world models to simulate visual outcomes/observations of input texts or robot control. The Dreamer 1 series use world models to predict latent state of images and use them to optimize policies, reducing the need for actual interactions in game environments (Hafner et al., 2019a; 2020; 2024). Motivated by these, this paper begins by investigating SOTA LLMs understanding of environment dynamics, i.e., the association between actions and environment states. We reveal that (i) current LLMs (e.g., GPT-4o and Claude-3.5-Sonnet) struggle with predicting the outcomes of their actions and (ii) the awareness of potential outcomes helps them make decisions aligning with user goals. Upon these findings, we present World-Model-Augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. However, naively training world model to predict the next observation state (i.e., the entire webpage) can lead to large amount of repeated elements across observations and long HTML inputs, negatively affecting model performance. Thus, we propose novel transition-focused observation abstraction, where the world model is trained to generate free-form natural language descriptions exclusively highlighting important state differences between time steps (e.g., an updated price on the website). During inference, our agent first simulates the outcome (i.e., next observation) of each action candidate (from the policy model) using the world model. Then, value function estimates the rewards of all simulated observations, helping the agent select final action with the highest estimated reward. Our contributions are two-fold: We are the first to pioneer world models in LLM-based web agents, laying the groundwork for policy adaptation through simulated environment feedback in web navigation. We present novel transition-focused observation abstraction for training LLMs as world models. We show that using world models trained with this method can improve action selection by simulating the action candidates without training the policy models. Also, we demonstrate our agents costand time-efficiency compared to recent tree-search-based agents (Koh et al., 2024), by 6.8x and 5.3x, respectively."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Benchmarks for web agents. Many benchmarks have been introduced to evaluate LLM-based agents ability in web navigation (Kim et al., 2024). MiniWoB (Shi et al., 2017) and MiniWoB++ (Liu et al., 2018) are among the first widely adopted benchmarks. More recently, WebShop (Yao et al., 2022) simulates e-commerce environments where agents are tested to execute tasks based on text instructions on the web. These early benchmarks are limited to specific and constrained domains. Mind2Web (Deng et al., 2024) curates web tasks across more diverse domains, and WebArena (Zhou et al., 2023) emphasizes functional correctness and more realistic scenarios (e.g., posting articles on Reddit) in simulated environment. We adopt Mind2Web and WebArena for evaluation for their generalizability and resemblance of real-world web interactions. LLM-based web agents. In recent years, LLM-based agents have become popular in the web navigation domain. However, since many powerful proprietary LLMs do not provide access to model parameters, many studies of web navigation have been focusing on training-free methods where LLMs directly learn from user inputs (i.e., prompts) without task-specific training (Sodhi et al., 2023; Zheng et al., 2023). For instance, Wilbur (Lutz et al., 2024) and Agent Workflow Memory (Wang et al., 2024b) leverage verification model (Pan et al., 2024) with prompt-based methods to collect successful trajectory data for guiding the agents policy at inference time. AutoEval (Pan et al., 2024) and Tree search agent (Koh et al., 2024) increase the number of trials and reasoning paths, further improving system performance. However, due to their trial-and-error nature, these approaches can not only be computationally inefficient in gathering trajectories as tasks become more complex but also are more prone to undesired results (e.g., booking non-refundable ticket). Our WMA web agent reduces such risks via world model, which predicts future observations and the rewards of their corresponding action candidates before actually making an action. Furthermore, our approach can be orthogonally applied to many of the existing methods. World models in autonomous agents. World models refer to systems that generate internal representations of the world, predicting the effects of their actions on environments (LeCun, 2022). In RL, simulating observations and environmental feedback using world models allow the policy model to learn (Sutton, 1990) or plan (Ha & Schmidhuber, 2018; Hafner et al., 2019b) without actually interacting with the environment. While some world models are trained with raw observa2 tions (Oh et al., 2015; Chiappa et al., 2017), others are built on latent representations (Hafner et al., 2019a; 2020; Kipf et al., 2020). For instance, in the image domain, Hafner et al. (2020) train world model by training it to first compute posterior stochastic state based on the current image and then prior stochastic state that tries to predict the posterior without access to the image. Within the field of LLMs, Zhang et al. (2024) convert visual observations into natural language and employs an LLM-based world model for text-based games, and Wang et al. (2024a) further transform observations into structural format (e.g., JSON), improving LLMs reasoning over state transition functions. In web navigation, environments are built upon not only natural language but on more complex text modalities such as HTML and DOM trees. We address this by transforming them to novel free-form description, highlighting the state difference between each time step."
        },
        {
            "title": "3 PRELIMINARY ANALYSES: ARE CURRENT LLMS AWARE OF",
            "content": "ENVIRONMENT DYNAMICS IN WEB NAVIGATION? We first start with investigating whether LLMs can understand the association between actions and their effects on the environment, i.e., understand the environment dynamics. We conduct analyses addressing these two questions: Preliminary question I: Are LLMs aware of the outcomes of their actions? Preliminary question II: When having access to the outcome of each action candidate, can LLMs select an optimal action aligning with the user objective? For the analyses, we sample 100 user instructions from WebArena and annotate human trajectories within the environment. Each instance has user instruction, the current state, human-annotated golden action, and the corresponding next state resulting from the golden action. We analyze 4 popular closed-source SOTA LLMs: GPT-4o-mini (Zhu et al., 2023), GPT-4o, GPT-4-Turbo (OpenAI, 2023), and Claude-3.5-Sonnet (Anthropic, 2024). More details are in Appendix B. 3.1 PRELIMINARY ANALYSIS - LLMS STRUGGLE WITH PREDICTING THE NEXT STATES CAUSED BY THEIR ACTIONS Setups. We test LLMs ability to predict the outcomes of actions on the web via binary classification task. Given the current state and the golden action, the LLM is prompted to select the correct next state from (i) the golden next state and (ii) lexically similar yet incorrect next state retrieved from the same trajectory. We calculate the lexical similarity with difflib (Python, 2024). We assess classification accuracy. Figure 1: LLMs performance in next state prediction. Results. Figure 1 reveals that under vanilla settings, current LLMs cannot effectively predict the next states caused by their actions. First, all adopted LLMs (54.75% on average) lose significantly to humans. Also, Claude-3.5-Sonnet performs almost as badly as random guessing. These suggest that the world model, the ability to foresee the potential outcomes of actions taken, is absent in LLMs. 3.2 PRELIMINARY ANALYSIS II - LLMS MAKE BETTER ACTION SELECTION WHEN ACCESSING THE OUTCOME OF EACH ACTION CANDIDATE Setups. We assess whether LLMs can select correct action that aligns with the user goal when they are provided with the outcome of each action candidate. Given the current state, 10 action candidates, and their corresponding outcomes/next states, the LLM is prompted to differentiate the golden action from other 9 negative actions. Results. Figure 2 compares LLMs performance in differentiating the golden action from negative actions when Figure 2: LLMs performance in action selection (w/ and w/o next states). 3 Figure 3: Framework overview. We first collect training data for world models (top). After training, we perform policy optimization by selecting the action leading to an optimal next state (bottom). they are/are not provided with the resulting next state of each candidate action. We find that current SOTA LLMs have difficulty in selecting correct actions when they can only rely on the current observations/states (striped bars), yielding an average accuracy of only 49%. However, when augmented with the corresponding next state of each action candidate, they demonstrate huge performance gains (up to 38% improvement) in selecting correct actions. When only the current state and the user objective are provided, GPT-4o yields an accuracy of 53%. In contrast, when the next state is given, performance rises to 73%. 3.3 INSIGHTS FROM PRELIMINARY ANALYSES Through our preliminary analyses, we have demonstrated that: (i) Web agents built with SOTA LLMs are bad at predicting how their actions affect next states; (ii) When being aware of how an action affects the next state, LLMs can make better decisions. These findings highlight the necessity of world models in LLM-based web agents, pointing out promising direction for facilitating better web agents in complex, long-horizon navigation tasks."
        },
        {
            "title": "4 WORLD-MODEL-AUGMENTED WEB AGENTS",
            "content": "Motivated by the above insights, we present novel framework for World-Model-Augmented (WMA) web agents, LLM-based web agents equipped with world models. The world models learn/leverage environment dynamics (i.e., association of actions and outcomes) to simulate plausible next observations of agents actions, facilitating better decisions (i.e., polices) in web navigation. Formulations. Since web agents access only information in the viewport (i.e., users visible area), we model web navigation as partially observable Markov decision process (POMDP). We consider web environment with: (i) hidden state space S; (ii) an action space A, including languageguided actions (e.g., CLICK, TYPE, HOVER, etc.) and their descriptions; (iii) an observation space representing an accessibility tree of the page, which is simplified DOM tree (Zhou et al., 2023). In the POMDP, the agent receives new partial observation ot+1 from after performing an action at based on ot. Such state transition st st+1 is managed by golden transition function : provided in the environment. 4.1 WORLD MODEL TRAINING We hereby introduce the training process of our world models. As shown in Figure 3 (top), our training consists of three main steps:"
        },
        {
            "title": "4.1.2 STEP II: TRANSITION-FOCUSED OBSERVATION ABSTRACTION\nWith the collected data D = (cid:80)n\nt=1{I, ot, at, ot+1}, it is intuitive to train LLM-based world models\nto predict ot+1, which is expressed with texts (e.g., HTML and accessibility tree) (Deng et al., 2024;\nZhou et al., 2023). However, simply using textual observations to represent environment states and\nuse them as training objectives may introduce the following downsides:",
            "content": "Low information gain during training: State transitions in websitesoften involve altering only part of the previous observation (e.g., drop-down menu is clicked). As result, most information in ot+1 remains the same as it is in ot. Therefore, predicting the entire textual observation from scratch may result in low information gain during training. Excessively long sequence length: Processing the whole text-based observations can lead to excessively long sequence length and consequently high computational costs. Indeed, this can be partially mitigated by replacing raw HTML with an accessibility tree (relatively simple), using it as LLMs training objectives still introduce long sequence length (4K tokens on average, see Figure 4). To address the above bottleneck in training text-based models (i.e., LLMs) as world models, we draw inspiration from how the RL community conventionally implements world models: using estimated latent vectors as summaries of raw visual observations, reducing memory footprints for effectively learning environment dynamics (Doerr et al., 2018; Hafner et al., 2019a) We thus propose to abstract raw text observations, with focus on state transition between consecutive observations, for obtaining better training objectives. Figure 4: Sequence length distribution of different observation representations. To collect abstracted next observations for training world models, one may simply run an off-the-shelf summarizer on ot+1 collected in Step I. However, while reducing sequence length, this does not address the low information gain caused by repeated elements between ot and ot+1. Thus, instead of such naive approach, as shown in Figure 5, we first (i) apply the Hungarian algorithm (Kuhn, 1995) to calculate cost matrix for matching elements between ot and ot+1 and (ii) mechanically transform the results into list of state transition (ot, ot+1), pointing out UPDATED, DELETED, and ADDED elements on the web. After that, we prompt an LLM to convert the extracted (ot, ot+1) into free-from natural language description ot+1, which highlights the difference between the new observation ot+1 and ot. Replacing ot+1 in = {I, ot, at, ot+1} collected in Step with ot+1 we just acquired here, we get final dataset = (cid:80)n t=1{I, ot, at, ot+1} for training world models. 4.1.3 STEP III: LEARNING ENVIRONMENT DYNAMICS Lastly, using D, we proceed to train the internal world model ϕ of the web agent to learn the environment dynamics. Formally, an LLM working as the world model is trained to predict the abstracted observation of the next state st+1, given three inputs: the user instruction I, the current observation ot, and the current action at. This LLM is trained to minimize the following loss term via the next-token prediction objective: Lϕ = log (cid:88) p(ot+1ot, at, I) (o,o,a,I) (1) Through this process, this LLM learns the environment dynamics, working as world model that helps the web agent to foresee the potential outcome (i.e., predict the next observation) of its action. 5 Figure 5: The overview of transition-focused observation abstraction. 4.2 INFERENCE-TIME POLICY OPTIMIZATION WITH THE WORLD MODEL In this section, we explain how we use the developed world model ϕ to improve LLM-based agents performance in web navigation. As illustrated in Figure 3 (bottom), the web agent consists of three main components: (i) policy model θ; (ii) our world model ϕ; (iii) value function . Note that the policy model θ is frozen, i.e., we do not update its parameters. During inference at time with current observation ot, WMA web agent ought to utilize the world model ϕ to foresee how an action can affect the state (i.e., predict oi t+1), and accordingly find an optimal action/policy at from the policy model θ that can lead to the target goal defined in I. , a2 We begin by sampling action candidates {a1 } from θ via top-p decoding (Holtzman et al., 2019), explore diverse next states st+1 (Wang et al., 2022). Next, we use the world model ϕ to simulate the potential next observation oi t+1 caused by each action candidate at: , ..., ak {oi t+1}k i=1 = {ϕ(ot, ai t, I)}k i=1 (2) Note that each oi t+1 is free-form description of the next observation, as shown in Figure 5 (right). Lastly, we decide the agents action for actual operation by selecting the action leading to the most optimal future state st+1 from all action candidates. Following Koh et al. (2024), we use an offthe-shelf LLM as value function () to estimate the reward yielded by each action candidate and select the action ˆat with the highest reward: ˆat = argmax ,...,ak at{a1 } (I, ot, at, oi t+1) (3) With this process, we are able to optimize the policy selection of web agents in inference time without training the policy models. This training-free augmentation of world models allows us to easily adapt our world model ϕ to existing web agents, including prompt-based (Pan et al., 2024; Wang et al., 2024b) and fine-tuned LLMs (Gur et al., 2023; Lai et al., 2024)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 SETUPS AND IMPLEMENTATION DETAILS Benchmarks and evaluation metrics. For evaluation, we use the official WebArena and Mind2Web benchmarks (Zhou et al., 2023; Deng et al., 2024). WebArena includes 812 real-life tasks in simulated environments across five different websites, spanning four key domains - e-commerce (Shopping), social forums (Reddit), collaborative software development (Gitlab), content management (CMS), and Map. Details of each domain are further explained in Appendix C.3. The main metric, Success Rate (SR), is calculated as the percentage of the user instructions that are successfully accomplished by the generated agent trajectory. On the other hand, Mind2Web (Deng et al., 2024) covers over 2,000 open-ended tasks, collected from 137 websites of 31 domains and crowdsourced action sequences for the tasks. Along with the SR, Mind2Web also uses Step SR, which measures whether the predicted action selects both the correct action type (action F1) and element ID (element accuracy). When the agent succeeds in all steps in trajectory, it is evaluated as success. Training data for world models. (i) For evaluation in WebArena: To facilitate applications in the real world, the training data for world models needs to cover wide range of tasks/goals. Since 6 Table 1: Agent performance in WebArena. : relative performance gains from policy optimization. Policy LLMs Methods Max Actions GPT-4 GPT-4o AutoEval (Pan et al., 2024) BrowserGym (Drouin et al., 2024) SteP (Sodhi et al., 2023) AWM (Wang et al., 2024b) Vanilla CoT (Zhou et al., 2023) Tree search agent (Koh et al., 2024) WMA web agent (ours) GPT-4o-mini WMA web agent (ours) 30 30 5 5 5 Success Rate (SR) No Policy Opt. + Policy Opt. 20.2% 23.5% 35.8% 35.5% 13.1% 15.0% 12.8% 9.4% - - - - - 19.2% 16.6% 13.5% - - - - - +28.0% +29.7% +43.6% Table 2: Domain-specific performance of agents using GPT-4o-mini as policy models Methods / Domains Shopping CMS Reddit Gitlab Map Overall Vanilla CoT (max actions = 5) WMA web agent (ours) 18.8% 19.3% +3% 8.2% 5.3% 11.5% 7.9% 3.1% 11.6% 9.4% 8.7% 22.3% 13.5% +40% +49% +181% +92% +44% diverse and large-scale user instructions set is not available,1 we synthesize user instructions using an LLM. With these synthesized instructions of various goals, we are able to collect rich trajectories as training data, improving world models generalization to diverse real-world situations. In practice, we sample trajectories for each I.2 We generate 870 synthetic user instructions and gather 14K instances from WebArena using GPT-4o-mini as the policy model. To avoid redundant learning, we filter out repeated state-action pairs. (ii) For evaluation in Mind2Web: we adopt the offline trajectory data from Mind2Web, following the setting of Wang et al. (2024b). Baselines. For baselines, we adopt: (1) prompting-based LLM (Zhou et al., 2023) powered by chain-of-thought prompting (Wei et al., 2022); (2) AutoEval (Pan et al., 2024). It refines agents trajectories based on the feedback on the final state of the trajectory (i.e., succeed or fail) from VLM evaluator (Shinn et al., 2024); (3) BrowserGym (Drouin et al., 2024) trains web agents with multi-modal observations, including HTML contents and the screenshot image of the browser; (4) SteP (Sodhi et al., 2023), framework based on human-authored hierarchical policies injected to the agent; (5) HTML-T5 (Gur et al., 2023), the previous SOTA method on Mind2Web, uses LLMs pre-trained LLMs on HTML corpus. (6) Agent workflow memory (AWM;Wang et al. (2024b)) leverages self-discovered workflow memory to guides its policy; (7) Tree search agent (Koh et al., 2024), the most competitive baseline that explores multiple trajectories and selects an optimal path via tree search algorithm. The main difference between ours and Tree search agents is that ours only uses the predicted future states via simulation and does not actually explore diverse states. World model. We use Llama-3.1-8B-Instruct (Dubey et al., 2024) as the backbone LLM for our world models.3 For WebArena, we construct our dataset in online setting using the provided web environment. In Mind2Web, we use the offline trajectory data (i.e., the train set) following Wang et al. (2024b). For prompt-based world models (baselines) in our experiments, we use 2-shot demonstrations to instruct LLMs to predict the next state. More details are provided in Appendix C.1. Policy model. Following Koh et al. (2024), we adopt GPT-4o (gpt-4o-0513) as the agent backbone for evaluation in WebArena. Additionally, we test with GPT-4o-mini (gpt-4o-mini-0718) to test our framework in relatively more resource-restricted scenarios. 1In WebArena, only test data (i.e., instructions) is provided. 2We empirically set = 5 in our work. Further details on the whole data collection process are in Appendix. 3https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 7 Table 3: Success rate on Mind2Web tests using GPT-3.5-Turbo as policy models. EA = element accuracy; EF = element filtering; AF1 = action F1; * = results from the original paper. Methods Synapse* HTML-T5-XL* MindAct* Cross-Task EA AF1 Step SR SR EA Cross-Website AF1 Step SR SR EA Cross-Domain AF1 Step SR - 34.4% 60.6% 81.7% 60.6% 41.6% 30.6% 57.8% 36.2% 2.0% 28.8% 10.3% 47.6% 71.9% 51.1% 35.8% 2.0% - 23.4% 42.9% 30.1% 1.1% 29.4% 5.6% 50.2% 74.9% 52.8% 2.0% 21.6% - 25.9% 48.3% 18.6% SR 1.6% 5.1% 1.0% AWM (w/ EF)* AWM (w/o EF) AWM+WMA (ours) 50.6% 57.3% 78.3% 74.1% 79.9% 75.8% 2.3% 36.4% 41.6% 45.1% 62.8% 6.2% 74.8% 71.2% 67.0% 25.4% 75.7% 72.1% 61.3% 8.5% 75.9% 72.6% 4.8% 41.4% 46.2% 15.3% 74.7% 70.1% 33.7% 58.6% 0.7% 32.6% 60.7% 9.5% 63.4% 10.1% Value function. We fine-tune Llama-3.1-8B-Instruct rewards using data from Mind2Web, where rewards (as training objective) are calculated based on the progress toward the goal, i.e., t/(len(τ )) when at is taken. to predict"
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Agent performance in WebArena. In Table 1 (middle), we first compare our WMA web agent (16.6%) with vanilla CoT (13.1%) and observe significant improvements over almost all domains in WebArena as detailed in Table 2. Interestingly, when using GPT-4o-mini as the policy model, our agent achieve 181% and 92% performance gains over CoT in Gitlab and Map, respectively. The relatively small improvement in Shopping might be due to the large-scale state space in the domain, such as the diversity of searched item lists from different user queries, which makes it harder for the world model to properly learn environment dynamics. Regardless, the overall improvement suggests the effectiveness of leveraging learnt environment dynamics during inference time. Next, we compare our approach with Tree search agent (Koh et al., 2024), which uses the oracle next state observation (i.e., resulted by the gold transition function from the environment) for policy selection instead of estimated observation via the world model. While the absolute SR of our WMA agent (16.6%) is slightly below Tree search agent (19.2%) when using GPT-4o as policy models, our policy optimization with the world model brings larger performance gain to vanilla CoT than tree search (+29.7% vs. +28.0%). Also, in the later section (5.3), we present ours superior cost and time efficiency over Tree search agent. Agent performance in Mind2Web. We compare WMA web agent with MindAct (Deng et al., 2024) and AWM (Wang et al., 2024b), which are previous and current SOTAs on Mind2Web.4 Table 3, demonstrates that WMA web agent significantly outperforms AWM,5 achieving new SOTA performance. Furthermore, the results indicate that WMA web agent trained on Mind2Web data has strong generalization capability. This makes our approach much more valuable in scenarios where collecting data for new web environments is non-trivial. Our advantages besides performance gains. Based on the performance reported, we can conclude that our strategy of building world models (i.e., observation abstraction) is effective not only for accessibility tree format (WebArena) but also for HTML format (Mind2Web), underscoring the applicability of our approach across different representations of web data. Another advantage of our approach over others is that the developed world models can be incorporated into existing or future web agents without any additional training of policy models, enabling easy implementation. 5.3 ANALYSES OF TIME AND COST EFFICIENCY We compare our WMA web agent with Tree search agent in terms of time and API cost efficiency. Results are shown in Table 4. To run one user instruction, Tree search agent spends about 748.3 seconds on average, as it involves the exploration of diverse future states while actually interacting with the environment. When it conducts backtracing to revert to the previous state, the whole sequence of previous actions has to be executed again. By contrast, WMA web agent only takes 140.3 seconds per instance by simulating the possible action candidates rather than actually executing them, which 4Tree search agent is not applicable to this benchmark as the environment is not available. 5Surprisingly, we find element filtering (EF) of MindAct, applied to AWM in default, largely hindering its performance. Thus, in Table 3, we include the results without EF. detailed discussion is in Appendix C.6 8 Table 4: Head-to-head comparison of Tree search agent (results are from Koh et al. (2024)) and ours regarding (i) SR and (ii) API cost, and (iii) inference time. We use GPT-4o for policy models. Methods Shopping CMS Reddit Gitlab Map API cost Inference time (sec) Tree search agent WMA (ours) 28.1% 20.8% 16.5% 10.5% 13.3% 25.8% 14.3% 10.5% 13.3% 26.8% $2.7 $0. 748.3 140.3 Table 5: Results of the ablation study in WebArena. Settings w/o next states in reward estimation (4.2) w/o training world models (4.1) w/o abstracting observations (4.1.2) WMA (ours) World Model Use Training Success Rate (SR) Shopping Gitlab Map Overall 28.0% 30.0% 22.0% 32.0% 6.0% 19.0% 18.0% 10.0% 15.0% 17.5% 6.0% 15.0% 14.5% 14.0% 21.0% 22.0% is 5.3 times faster than Tree search agent. Tree search agent requires 6.8 times more API cost due to its multi-modal inputs. To sum up, while showing comparable performance to Tree search agent in CMS, Reddit, Gitlab, and Map, our WMA web agent demonstrates superior cost and time efficiency. 5.4 ABLATION STUDIES We conduct several ablation studies on our WMA web agent with 200 randomly sampled instances from WebArena (Shopping: 50; Gitlab: 50; Map: 100). We use GPT-4o-mini as policy models. Accessing simulated next states in reward estimation improves agent performance. To assess the impact of incorporating the simulated next state when calculating the value score, we compare our reward estimation strategy to Q-value function (Haarnoja et al., 2017) that predicts the reward based on only (ot, at). The results in Table 5 show that the information of the resulting next state helps the value function to predict rewards more accurately, resulting better task performance. Fine-tuning facilitates better world models than promptbased approaches. To assess the effectiveness of our training approach for world models, we compare our framework with variant, where we replace the trained world model (i.e., finetuned Llama-3.1-8B-Instruct) with GPT-4o-mini prompted to predict the next observation solely based on 2-shot demonstrations (i.e., in-context learning) without training. The sub-optimal performance of this variant, as shown in Table 5 (2nd row), suggests that SOTA LLMs do not have sufficient knowledge of environment dynamics, which is consistent with our findings in 3.1. Table 6: Performance with different value models. GPT-4o-mini Llama-3.1-8B Value Function Training 12.7% 13.5% SR Abstracting observation elicits better next state prediction. We evaluate the effectiveness of our observation abstraction (4.1.2), which focuses on state transition. For that, we train world model that learns to predict the full accessibility tree, i.e., ot+1 instead of our transition-focused abstraction ot+1. As we expected, Table 5 (3rd row) reveals that generating the whole next observations (i.e., all elements in the viewport) results indeed hinder agent performance, yielding the worst SR among all ablations. This shows that processing redundant and repeated information across observations negatively affects the world model in capturing critical state changes compared to abstracted observations that exclusively highlight state transition. Figure 6: Ablation on the number of sampled actions (k). Choice of value functions. We compare the fine-tuned value model (i.e., Llama-3.1-8B-Instruct) used for implementing WMA web agents with prompted GPT-4o-mini in Table 6. Ours lead to slightly better agent performance compared to GPT-4o-mini. This suggests fine-tuning the value function is reasonable alternative in scenarios where API budgets are limited. 9 Budget for exploration. Figure 6 shows that there is positive trend between the number of sampled actions (k) during inference-time policy optimization in 4.2) and the agents task performance (SR). These results suggest that our WMA web agent may benefit from more exploration of the future states when the budget is allowed."
        },
        {
            "title": "6.1 COMBINING SELF-REFINE WITH OUR WORLD MODELS",
            "content": "Besides our inference-time policy optimization, another way of using world models to improve the policy model is prompting it to refine its predicted action (Madaan et al., 2024), based on the outcome simulated by the world model. Such self-refinement has been showing promising performance in diverse LLM applications (Shinn et al., 2024; Chae et al., 2024). Table 7: Results of applying self-refine to GPT-4o-mini using simulated environment feedback. Methods SR Vanilla CoT Self-refine w/ our world model WMA (ours; Fig 3) 11.6% 13.4% 22.3% Here, we conduct demonstrative experiment of combining self-refine with our world model in the Map domain from WebArena. Since tasks in this domain involve complex set of utility tools, such as sorting and zoom-in, we consider it suitable for testing self-refine. In this experiment, after the policy model θ produces an action at based on the current observation ot, we use our world model to simulate the next observation ot and prompt θ to refine the action based on ot. Simply put, this setting allows θ to make adjustments to its output action when the predicted next observation is not optimal. Table 7 shows that refining with simulated environment feedback improves the agents policy selection by 1.8% percentage point in terms of accuracy compared to CoT. While this provides plausible direction for future work, our simulate-score-select paradigm (Figure 3 (bottom)) yields almost 2x higher accuracy (13.4% vs. 22.3%), making it our choice of the policy optimization method.6 6.2 TYPES OF ERRORS IN WORLD MODELS PREDICTIONS To gain deeper insights into WMA web agents, we sample 50 erroneous predicted states (i.e., ot+1) from world models in WebArena, and manually categorize the type of errors. Whether predicted state is erroneous is judged by CS major who manually compares the viewport and the predicted observation. Examples of each type and details on the sampled states are provided in Appendix D.2. Figure 7 shows the statistics of the following error types: (i) Correct yet overly generic statements (24%) - Statements such as The user will see comprehensive layout of various order-related functionalities, where the structure of the layout and what functionalities will be seen are not specified; (ii) Low competence in web elements/functions (26%) - Cases where the world model does not know how to use components on the web, e.g., expecting the search engine to show the desired items when the agent does not delete old texts on the search bar before entering new keyword; (iii) Counterfactual imagination (42%) - Cases where the next observation predicted by the world model includes elements that are not supposed to occur/exist, e.g., making up products that are not sold in the store; (iv) others (8%) - other errors, such as skipping the next observation and predicting an observation that is further from the current time step. Figure 7: Statistics of error types in erroneous observations predicted by ϕ."
        },
        {
            "title": "7 CONCLUSIONS",
            "content": "We are the first study that incorporates world models in LLM-based web agents, addressing the limitation of current SOTA LLMs in understanding environment dynamics. Through extensive experiments in WebArena and Mind2Web, we show that (i) our WMA web agent can demonstrate great efficacy in policy selection by simulating outcomes of its actions via world models trained 6As pilot/demonstrative trial, we do not consider sampling multiple actions for self-refinement. 10 using our approach (i.e., transition-focused observation abstraction). Moreover, (ii) our WMA web agent outperforms strong baselines (i.e., Tree search agent) with reduced cost and time for the exploration and (iii) achieves new SOTA performance in Mind2Web. By augmenting LLM-based web agents with world models, we establish strong foundation for future research in web navigation."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 3.5 sonnet, June 21 2024. URL https://www.anthropic. com/news/claude-3-5-sonnet. Accessed: 2024-09-30. Hyungjoo Chae, Taeyoon Kwon, Seungjun Moon, Yongho Song, Dongjin Kang, Kai Tzu-iunn Ong, Beong-woo Kwak, Seonghyeon Bae, Seung-won Hwang, and Jinyoung Yeo. Coffee-gym: An environment for evaluating and improving natural language feedback on erroneous code. arXiv preprint arXiv:2409.19715, 2024. Silvia Chiappa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc TouIn International ssaint, and Sebastian Trimpe. Probabilistic recurrent state-space models. Conference on Machine Learning, 2018. URL https://api.semanticscholar.org/ CorpusID:45425492. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Leo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: arXiv preprint How capable are web agents at solving common knowledge work tasks? arXiv:2403.07718, 2024. Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation, 2023. URL https://arxiv.org/abs/2302.00111. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ward Edwards. The theory of decision making. Psychological bulletin, 51(4):380, 1954. Jay Forrester. Counterintuitive behavior of social systems. The System Dynamics Road Maps, 1995. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Tuomas Haarnoja, Haoran Tang, P. Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning, 2017. URL https: //api.semanticscholar.org/CorpusID:11227891. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pp. 25552565. PMLR, 2019b. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. 11 Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models, 2024. URL https://arxiv.org/abs/2301.04104. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36, 2024. Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models, 2020. URL https://arxiv.org/abs/1911.12247. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024. Harold Kuhn. The hungarian method for the assignment problem. 50 Years of Integer Programming 19582008, pp. 29, 1995. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. arXiv preprint arXiv:2404.03648, 2024. Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, and Giovanni Campagna. arXiv preprint Wilbur: Adaptive in-context learning for robust and accurate web agents. arXiv:2404.05902, 2024. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. Advances in neural information processing systems, 28, 2015. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. Python. difflib helpers for computing deltas, 2024. URL https://docs.python.org/3/ library/difflib.html. Python Documentation. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pp. 31353144. PMLR, 2017. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Paloma Sodhi, SRK Branavan, and Ryan McDonald. Heap: Hierarchical policies for web actions using llms. arXiv preprint arXiv:2310.03720, 2023. Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bull., 2:160163, 1990. URL https://api.semanticscholar.org/CorpusID: 207162288. Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Cˆote, Peter Clark, and arXiv preprint Peter Jansen. Can language models serve as text-based world simulators? arXiv:2406.06485, 2024a. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators, 2024. URL https: //arxiv.org/abs/2310.06114. Shunyu Yao, Howard Yang Chen, John, and Karthik Narasimhan. Webshop: Towards scalable realworld web interaction with grounded language agents. 2022. doi: 10.48550/arXiv.2207.01206. Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, and Karthik Narasimhan. Language-guided world models: model-based approach to ai control. arXiv preprint arXiv:2402.01695, 2024. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv e-prints, pp. arXiv 2304, 2023."
        },
        {
            "title": "A LIMITATIONS",
            "content": "This paper has the following limitations: Single modality. This study focuses on building text-based world models and web agents. In web navigation, however, visual information can also play critical role (Liu et al., 2024; Zheng et al., 2024). Although HTML and accessibility tree do represent the visual structure of webpage to some degree, it is recommended for future work to incorporate visual information in addition to textual information for improving the learning of dynamics in the environment (Koh et al., 2024). Application of multi-step planning. Our current approach demonstrates significant performance gains in selecting actions within single time step, i.e., from time to time + 1. However, we believe that our idea can be extended to multi-step planning (i.e., predicting ot+n where > 1). This can be achieved by recursively feeding the predicted next state back into the world model as the new observation, along with action candidates generated at each time step. Incorporation with techniques such as Monte Carlo Tree Search (Koh et al., 2024) or other planning algorithms is also feasible. Future work can explore such multi-step planning to enable web agents to understand longer-term consequences of their actions and make more informed decisions in scenarios where planning multi-step ahead is necessary."
        },
        {
            "title": "B EXPERIMENTAL DETAILS OF PRELIMINARY ANALYSES",
            "content": "B.1 PRELIMINARY ANALYSIS We formulate next state prediction as binary classification task rather than generation task for an easier and more accurate evaluation (it is non-trivial to evaluate machine-generated accessibility tree or HTML). Measuring the next state prediction capability as generation task requires an additional human evaluation or off-the-shelf LLM judges, but it might introduce evaluation bias and there is no consensus that LLMs can judge this capability. To collect training objectives for next state prediction, we use difflib python library7 to calculate the lexical similarity between the golden next state and similar yet incorrect next state. Then, we select the top-1 similar yet wrong state as the negative next state and randomly shuffle the answer choices. The prompt used for next state prediction is shown in Figure 15. The interface for human annotation is shown in Figure 8. B.2 PRELIMINARY ANALYSIS II We use greedy decoding for sampling sequence of 9 negative actions from GPT-4o-mini. Specifically, the LLM is instructed to generate 9 negative action candidates with the 2-shot demonstration. Prompts used for action selection in preliminary analysis II are shown in Figure 16 and 17."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 WORLD MODEL C.1.1 DATASET CONSTRUCTION Instruction and trajectory collection from WebArena. As mentioned in 5.1, WebArena does not provide anything other than the test set. We thus synthesize user instructions and accordingly collect trajectories. In total, we obtain 14,200 instances using GPT-4o-mini with CoT prompt provided in Zhou et al. (2023). These instances are used to collect training data for world models in WebArena. 7https://docs.python.org/3/library/difflib.html 14 Transition-focused observation abstraction. We implement the Hungarian algorithm (Kuhn, 1995) using munkres python package.8 Details of the algorithm are in Algorithm 1. TaO in Algorithm 1 stands for Transition-aware Observation, and denotes the direct observation output from the Hungarian algorithm used in 4.1.2. Then, using the output from the algorithm, we prompt an LLM to make free-form description that captures the state transitions. The prompt used for producing free-form description is shown in Figure 18 and Figure 19. Algorithm 1: Observation tree state matching for (ot, ot+1) in 4.1.2 Input n1], ot+1 = [et+1 , . . . , et+1 0, . . . , et : States ot = [et location li. Weights ωn, ωr, ωl. m1]. Each ei has name ni, role ri, Output: STaO t+1 if len(ot+1) τ len(ot) then # Construct cost matrix for Hungarian matching lt+1 + ωl lt + ωr 1rt Ci,j ωn 1nt # Apply Hungarian algorithm to find optimal matching argmin i=nt+1 =rt+1 (cid:80) i,j Ci,j Mi,j # Identify unmatched elements {jM i,j = 0, {0, . . . , 1}} end if len(U ) or = then STaO t+1 ot+1 else # Construct TaO state based on unmatched and nearby elements t+1 [et+1 STaO or (len(U ) and minuU j y)] end C.1.2 TRAINING For world models and value functions, we use learning rate of 1e-5 and spend around 3 GPU hours training them for 2 epochs on 8 RTX 4090 GPUs. C.2 INFERENCE We use top-p decoding with = 1.0 for sampling 20 actions from the model following (Koh et al., 2024). The three most frequent actions among the sampled actions are to be selected, and next state prediction is to be performed for these actions. The prompt used for the next state prediction of world models is shown in Figure 20. For each predicted next state, reward is calculated using the value function (the prompt is in Figure 21) , and the action with the highest reward is finally selected. We use vLLM (Kwon et al., 2023) to run inference of fine-tuned LLMs. C.3 DETAILS ON WEBARENA To ensure fair comparison and reproducibility, we conduct our experiments using the WebArena environment. Specifically, we utilize an Amazon Web Services (AWS) EC2 instance pre-configured with the Docker environment for WebArena.9 This setup is identical to the experimental configuration employed by Zhou et al. (2023) in their original study. By using this standardized environment, we maintain consistency with previous research and facilitate direct comparisons of our results with those reported in the literature. The WebArena Docker environment encapsulates all necessary dependencies, web interfaces, and evaluation metrics, ensuring that our experiments are conducted under controlled and replicable conditions. Details of each domain are explained below. 8https://pypi.org/project/munkres/ 9https://github.com/web-arena-x/webarena/blob/main/environment docker/README.md#pre-installedamazon-machine-image 15 Shopping: E-commerce platforms supporting online shopping activities (e.g., Amazon, and eBay). In this website, the agent can search and make an order for realistic items. CMS: Content Management Systems that manage the creation and revision of digital content (e.g., online store management). Reddit: Social forum platforms for opinion exchanges. Gitlab: Collaborative development platforms for software development. Map: Navigation and searching for information about points of interest such as institutions or locations. For Map domain, we use the online openstreetmap website10 since the button for searching route of the provided docker does not properly work. This issue is also raised in the official WebArena github. C.4 DETAILS ON MIND2WEB For running our experiments on Mind2Web, we obtain Mind2Web data from the official project page.12 We use the implementation of Wang et al. (2024b) to calculate the evaluation metrics, EA, AF1, Step SR, and SR. Each action in the sequence comprises (Target Element, Operation) pair, We measure Element Accuracy (EA) which compares the selected element with all ground-truth elements, and Action F1 (AF1) that calculates token-level F1 score for the predicted action. Each step of the task is evaluated independently with the ground-truth history provided. We then define Step Success Rate (Step SR) and Success Rate (for the whole task). For calculating Step Success Rate (Step SR) and Success Rate (SR), step is regarded as successful only if both the selected element and the predicted action is correct. task is regarded successful only if all steps have succeeded. For step-wise metrics, we report macro average across tasks. C.5 IMPLEMENTATION DETAILS OF BASELINES Vanilla CoT (Zhou et al., 2023) For fair comparison, we first sample 20 actions with top-p sampling similar to ours. We use the original CoT prompt from Zhou et al. (2023). Then we choose the most frequent action as the final action. We use the prompt in Figure 22. Tree Search Agent (Koh et al., 2024) We use the codes from the official Github repository for implementing Tree search agent.13 For time and cost analysis on this agent, we run Tree search agent on 10% of WebArena instances due to its excessive cost. Agent Workflow Memory (Wang et al., 2024b) We use the codes from the official github repository to implement Agent Workflow Memory (AWM). We use GPT-3.5-Turob to create workflow memory from the train data of Mind2Web dataset. During our experiments, we find that the candidate generation module of MindAct (Deng et al., 2024) significantly degrades the original performance. This module calculates the relevance score of each element to the query so that web agents can predict action with more shortened observation. We provide the results of both settings with and without the candidate generation module. For certain baselines, we obtain the performance from the original papers, which are marked with * in the result tables. C.6 ISSUE REGARDING THE ELEMENT FILTERING MODULE OF MINDACT The element selection module proposed by Deng et al. (2024) used for filtering out irrelevant elements in the extremely long HTML content to avoid confusion. This element selection module is adapted to the suggested baseline in Mind2Web paper, MindAct and widely applied to the following methods (Wang et al., 2024b; Zheng et al., 2024) including the AWM baseline. However, we find that this module introduces significant performance decrease, by removing not only the irrelevant 10https://www.openstreetmap.org/ 11https://github.com/web-arena-x/webarena/issues/159 12https://osu-nlp-group.github.io/Mind2Web/ 13https://github.com/kohjingyu/search-agents 16 items but also the relevant ones. Thus, we re-implemented AWM in both with and without the filtering module. C."
        },
        {
            "title": "INSTANCE IDS OF ADAPTED TASKS FOR WEBARENA",
            "content": "We randomly sampled 200 instances from WebArena (50, 50, and 100 instances from Shopping, Gitlab, and Map, respectively) We sample 100 instances from the Map domain as it is costand time-efficient due to its short inference time. We provide the full list of task ids below: Shopping: 49, 51, 96, 144, 146, 158, 162, 164, 165, 188, 189, 190, 226, 231, 235, 238, 263, 274, 278, 281, 300, 313, 319, 333, 337, 352, 355, 362, 376, 385, 386, 387, 432, 467, 468, 469, 506, 509, 511, 513, 515, 517, 518, 521, 528, 529, 530, 531, 587, 589 GitLab: 156, 174, 177, 178, 205, 207, 297, 305, 306, 311, 315, 317, 339, 341, 349, 357, 389, 395, 396, 416, 418, 422, 441, 452, 475, 482, 483, 523, 524, 535, 537, 552, 553, 563, 564, 566, 569, 658, 662, 664, 669, 670, 736, 751, 783, 787, 789, 800, 803, 810 Map: 7, 8, 9, 10, 16, 17, 18, 19, 20, 33, 34, 35, 36, 37, 38, 40, 52, 53, 54, 55, 56, 57, 58, 60, 61, 70, 71, 72, 73, 75, 76, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 97, 98, 99, 100, 101, 137, 138, 139, 140, 151, 153, 154, 218, 219, 220, 221, 222, 223, 224, 236, 248, 249, 250, 251, 252, 253, 254, 256, 257, 287, 356, 364, 365, 366, 367, 369, 371, 372, 373, 377, 378, 380, 381, 382, 383, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,"
        },
        {
            "title": "D DETAILS OF FURTHER ANALYSES",
            "content": "D.1 SELF-REFINE We implement self-refine using the prompt shown in Figure 23. Specifically, we first generate single action using the CoT prompt and we obtain the feedback from the value model used in our method. Then, we refine the action according to the feedback. D.2 ERROR TYPE ANALYSIS AND EXAMPLES We sample 50 errors from the inference results in WebArena for our error analyses. The numbers of selected samples by domains are Shopping: 8, CMS: 11, Gitlab: 11, Reddit: 10, and Map: 10. The examples of the four error types are mentioned in 6.2 and are respectively shown below. Low competence in web elements/functions: Figure 9. Counterfactual imagination: Figure 10. Correct yet overly generic statement: Figure 11. Others: Figure 12."
        },
        {
            "title": "E EXAMPLES OF SUCCESSFUL INFERENCE",
            "content": "We provide several successful examples of our WMA web agents: Inference on Mind2Web: Figure 13 Inference on WebArena: Figure"
        },
        {
            "title": "F PROMPTS",
            "content": "The following are prompts utilized in our study: Prompt for next state prediction in preliminary analysis in Figure 15. Prompts for action selection in preliminary analysis II in Figure 16 and Figure 17. Prompt for refining TaO output in Figure 18 17 Prompt for transition focused observation abstraction in Figure 19. Prompt used for the next state prediction of the world model in Figure 20. Prompt for reward calculation in value function in Figure 21. Prompt for baseline action prediction using accessibility tree with CoT in Figure 22. Prompt for self-refining in Figure 23. 18 Figure 8: Human annotation interface for preliminary analysis in 3.1. 19 Figure 9: Erroneous example (Low competence in web elements/functions). Although the agent does not delete old texts on the search bar before entering the new keyword restaurants near CMU Posner Hall, the world model still expects the next observation to show the desired search results. Figure 10: Erroneous example (Counterfactual imagination). The model predicts that specific products (96 TY CITY86 Bmw 740i Limited Collector Hoodie Mens Close; Toyota 86 Bad Institute Monkey Champagne Cup, Volkswagen A9 Bug Pick Dead Red) will appear in the next observation, while this specific page does not list them as the products for sell. 20 Figure 11: Erroneous example (Correct yet overly generic statements). Comprehensive layout and various order-related functionalities are ambiguous and unclear expressions. Figure 12: Erroneous example (Others). The predicted next state (i.e., contributions and activities) is actually several steps further away from the current time step. 21 Figure 13: Successful example (Mind2Web). WMA web agent successfully inferences on the Mind2Web benchmark (menards task #0). Using the policy model (i.e., GPT-4o), WMA web agent selects the most proper action click [208] by leveraging its learned environment dynamics. 22 Figure 14: Successful example (WebArena). WMA web agent successfully inferences on Gitlab domain in the WebArena benchmark (instance #175). Using the policy model (i.e., GPT-4o), WMA web agent selects the most proper action click [88] by leveraging its learned environment dynamics. Figure 15: The prompt for preliminary analysis in 3.1: Next state prediction Figure 16: The prompt for preliminary analysis II in 3.2: Action selection w/o next state 24 Figure 17: The prompt for preliminary analysis II in 3.2: Action selection w/ next state 25 Figure 18: The prompt for refining TaO output before generating final Transition-focused observation abstraction in 4.1.2 Figure 19: The prompt for transition-focused observation abstraction in 4.1.2 27 Figure 20: The prompt used for the next state prediction of the world model 4.2 28 Figure 21: The prompt for reward calculation using the value function in 4.2 Figure 22: The prompt used for baseline comparison with accessibility tree input using CoT in 5.4 30 Figure 23: The prompt for self-refine in 6."
        }
    ],
    "affiliations": [
        "Yonsei University"
    ]
}