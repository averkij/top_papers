{
    "paper_title": "GenEx: Generating an Explorable World",
    "authors": [
        "Taiming Lu",
        "Tianmin Shu",
        "Junfei Xiao",
        "Luoxin Ye",
        "Jiahao Wang",
        "Cheng Peng",
        "Chen Wei",
        "Daniel Khashabi",
        "Rama Chellappa",
        "Alan Yuille",
        "Jieneng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration."
        },
        {
            "title": "Start",
            "content": "GenEx: Generating an Explorable World Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan L. Yuille, and Jieneng Chen"
        },
        {
            "title": "Johns Hopkins University",
            "content": "4 2 0 2 2 1 ] . [ 1 4 2 6 9 0 . 2 1 4 2 : r Figure 1 GenEx explores an imaginative world, created from single RGB image and brought to life as generated video. See more examples in our website (genex.world). Understanding, navigating, and exploring the 3D physical real world has long been central challenge in the development of artificial intelligence. In this work, we take step toward this goal by introducing GenEx, system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is grounded in the physical world. It captures continuous 360 environment with little effort , offering boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation and robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by the generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectations regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration. Keywords: Generative AI, World Models, Embodied AI, World Explorer 1. Introduction Humans explore and interact with the 3D physical world by perceiving their surroundings, taking actions, and engaging with others. Through these interactions, they form mental models that simulate the complexities of their environment. With just glimpse, humans can construct an internal 3D representation of their surroundings in their minds, enabling reasoning, navigation, and problem-solving. This remarkable ability has long been central challenge in the development of artificial intelligence. In this work, we introduce GenEx, platform designed to push this boundary by Generating an Explorable world and facilitating explorations in this generated world. GenEx combines two interconnected components: an imaginative world, which dynamically generates 3D environments for exploration, and an embodied agent, which interacts with this environment to refine its understanding and decision-making. Together, these components form symbiotic system that enables AI to simulate, explore, and learn in ways similar to human cognitive processes. We begin by constructing an imaginative world that captures 360, 3D environment grounded in the physical world, leveraging recent advancements in Generative AI. Starting from single image, the model generates new environments expansively and dynamically while maintaining coherence and 3D consistency, even during longdistance exploration. This boundless landscape provides endless opportunities for AI agents to explore and interact. The environment is brought into life in the form of diffusion video generation, conditioned on moving angle, distance, and single initial view to serve as starting point. To address fieldof-view constraints, we utilize panoramic representations and train our video diffusion models with spherical-consistent learning techniques. This ensures the generated environments maintain coherence and 3D consistency, even during long-distance exploration. To anchor our video generation model in the physical world, we curate training data from physics engines like Unreal En2024-12-13 gine, enabling realistic and immersive outputs. Within this imaginative landscape, embodied agents play crucial role. Enhanced by GPTs, these agents can explore unseen parts of the physical world with imagined observations to refine their understanding of surroundings, simulate different outcomes based on potential decisions, and make more informed choices. Furthermore, GenEx supports multi-agent scenarios, allowing agents to mentally navigate others positions, share imagined beliefs, and collaboratively refine their strategies. In summary, GenEx represents transformative step forward in the development of AI, offering platform that bridges the generative and physically grounded world. By enabling AI to explore, learn, and interact in boundless, dynamically generated environments, GenEx opens the door to applications ranging from real-world navigation, interactive gaming, and VR/AR to embodied AI. 2. Generating an Explorable World We define the explorable generative world and the problem in 2.1, present the world initialization in 2.2 and world transition in 2.3. 2.1. Problem Formulation Defining an explorable generative world. We define an explorable generative world as an AIgenerated virtual environment, constrained to the agents immediate surroundings. The generative world is both physically plausible and visually coherent. This environment is represented by the agents egocentric panoramic observations, denoted as x. While is synthesized, it remains grounded in intuitive physical principles and realistic appearance, akin to high-fidelity, physically realistic video game environment. Crucially, the explorable nature of our generative world ensures the agents experience is not limited to static scene. Instead, the environment dynamically evolves in response to the agents movements and actions, simulating continuous and coherent exploration. Formally, let 洧녩洧노 be the agents action at step 洧노, encompassGenEx: Generating an Explorable World 洧노 , 洧논1 洧노 , . . . , 洧논洧녡 ing both view rotation 洧띺 and forward distance 洧녬. Let x洧노 = (洧논0 洧노 ) represent the sequence of panoramic observations encountered as the agent moves according to 洧녩洧노, where 洧녡 corresponds to sequence length in x洧노, or the traveled distance. Each 洧논 洧 洧노 in x洧노 is generated to reflect the environments currently perceivable state, ensuring that the agents evolving viewpoint remains coherent and physically meaningful. We train our models using data harvested from controlled, simulated setting. By employing physics-based data engine (2.2), we ensure realistic and diverse training scenarios that capture the intricate variations encountered in complex, virtual landscapes. Task formulation: We reformulate the task of exploring generative world as the problem of generating an initial panoramic world view 洧논0 and sequence of world views represented by panoramic videos x1:洧녢 , together represented as x0:洧녢 , given single initial image 洧녰0, description 洧녳0, and action 洧녩洧노 at each step 洧노, where 洧노 = 1, . . . , 洧녢. Formally, we have 洧녷(x0:洧녢 洧녰0, 洧녳0) = 洧녷洧랚1 (洧논 洧녰0, 洧녳0) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) world initialization (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 洧녢 (cid:214) 洧녷洧랚2 洧노=1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:0)x洧노 洧논洧녡 洧노1, 洧녩洧노(cid:1) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) world transition (cid:125) Algorithm 1 Generating an Explorable World 洧녷(x0:洧녢 洧녰0, 洧녳0) Require: initial single-view image 洧녰0. language description 洧녳0 specifying the desired panoramic world initialization. conditional distribution 洧녷洧랚1 (洧논 洧녰0, 洧녳0), parameterized by an image-to-panorama generation model 洧랚1 to initialize the 360 world. Action space defined in the physical engine, from which an action is sampled: 洧녩洧노 A. conditional distribution 洧녷洧랚2 (x 洧논洧녡 洧노1, 洧녩洧노), parameterized by panoramic video generation model 洧랚2. 1: Notation: Let x洧노 = (洧논 洧노 ) denote the generated panoramic video at exploration step 洧노. Here, 洧논洧녡 洧노 is the latest explored panoramic view. 洧노 , . . . , 洧논洧녡 洧노 , 洧논1 2: World initialization: Initialize 360 panoramic world from single image: 洧논0 洧녷洧랚1 (洧논 洧녰0, 洧녳0) 3: for 洧노 = 1 to 洧녢 do 4: World transition at step 洧노: Given 洧녩洧노 and 0 洧논0, where 洧논洧녡 the latest explored world 洧논洧녡 we sample the new panoramic video x洧노: 洧노1 x洧노 洧녷洧랚2 (x 洧논洧녡 洧노1, 洧녩洧노) 5: end for 6: return The initial 360 panoramic world view 洧논0 and sequence of generated panoramic states x1:洧녢 , which together represent one explorable generative world, denoted as x0:洧녢 . In this unified form, the core terms are: 2.2. World Initialization World initialization (2.2): Given the initial image 洧녰0 and language description 洧녳0, the anchor 360 world view 洧논0 is sampled from: 洧논0 洧녷洧랚1 (洧논 洧녰0, 洧녳0), where 洧랚1 is an image-to-panorama generator. World transition (2.3): Given the chosen action 洧녩洧노, the next world view x洧노 is sampled from: x洧노 = (洧논0 洧노 , 洧논1 洧노 , . . . , 洧논洧녡 洧노 ) 洧녷洧랚2 (x 洧논洧녡 洧노1, 洧녩洧노), where 洧랚2 is 360 panoramic video generator, 洧노 = 1, . . . , 洧녢, and 洧논洧녡 0 洧논0. Preliminary: data and representation. Collecting diverse world exploration data in the real world is challenging due to resource constraints and environmental variability. Thus, we utilize physics engines such as Unreal Engine 5 and Unity in Figure 2 for data curation. These engines allow for the creation of rich, diverse virtual environments where we can simulate exploration trajectories and collect corresponding data efficiently."
        },
        {
            "title": "We represent",
            "content": "the 360 world using the panoramic view of the agent. Panoramic images capture complete 360 180 view of scene from fixed viewpoint. One common panoramic representation is the cubemap, which projects 360 view onto the six faces of cube. Each face 3 GenEx: Generating an Explorable World World initialization model. Starting from single input image 洧녰0, we aim to construct full 360 panoramic representation 洧논0 of the agents environment. To achieve this, we condition pretrained text-to-image diffusion model on both the input image 洧녰0 and text description 洧녳0 of the desired 3D world, yielding high-dynamic-range panorama. Thus, 洧논0 is drawn from the conditional distribution 洧녷(洧논 洧녰0, 洧녳0). Figure 2 Our data curation leverages physical engines, utilizing realistic city assets from UE5 and animated world assets from Unity. captures 90 field of view, resulting in six perspective images that can be seamlessly stitched together. Due to its simplicity and compatibility with rendering engines, we directly collect cubemaps in the physics engine to represent the egocentric world. Notably, cubemaps, equirectangular panorama, and sphere are three representations of 360 panoramic world. The curated cubemaps will be projected to equirectangular panoramas for video generation in the world exploration stage, and projected to spherical space when changing the exploring angle. Given predefined exploration trajectories, we collect sequences of cubemaps to represent different exploration outcomes in the virtual world. By sampling large number of exploration directions uniformly, we curate an extensive dataset of world exploration scenarios, which serves as the training data for our models. Figure 3 Three panorama representations that can be transformed into one another. 4 Figure 4 From single view to 360 panorama. Our world initialization model is built up on state-of-the-art text-to-panorama model (Bilcke, 2024) tuned from the state-of-the-art textto-image model FLUX.1 (Labs, 2024). The textto-panorama model (Bilcke, 2024) generates panorama from text description 洧녳0: 洧논0 洧녷flux(洧논 洧녳0) . However, without being conditioned on the single image, this approach cannot guarantee the coherence of generated panorama 洧논0 with the provided reference image 洧녰0. We extend the model to condition on both textual input and single image. This adaptation allows the model to produce full 360-degree environment that aligns with the provided image: 洧논0 洧녷洧랚1 (洧논 洧녰0, 洧녳0) . Although this yields coherent, image-consistent panorama, the scene remains static and does not permit dynamic movement or exploration. To enable deeper interaction within the generative world, we introduce the world transition. GenEx: Generating an Explorable World 2.3. World Transition When the agent moves within the imaginative environment, its egocentric 360 view changes, prompting world transition. We model this transition as an action-driven panoramic video generation process, transforming the previously observed panorama into new, forward-looking view as the agent progresses. 洧노 , 洧논1 洧노 , ..., 洧논洧녡 Transition objective. The goal is to sample x洧노 = (洧논0 洧노 ), newly explored panoramic video, conditioned on the previous panorama 洧논洧녡 洧노1 and the action 洧녩洧노 = (洧띺洧노, 洧녬洧노). Here, 洧띺洧노 is the moving angle and 洧녬洧노 is the distance. Formally, we have the transition objective: 洧노1, 洧녩洧노) . x洧노 洧녷(x 洧논洧녡 The transition procedure has core modules: Action sampling: Consider an action sequence 洧녩1:洧녢 drawn from an infinitely large action set in the Unreal Engine and Unity. We can denote the action space as: A, where = . Each element of the sequence for 洧노 = 1, . . . , 洧녢 is sampled from A: 洧녩洧노 A, 洧노 = 1, . . . , 洧녢, As result, the entire action sequence 洧녩1:洧녢 = (洧녩1, . . . , 洧녩洧녢 ) lies in A洧녢 . Sphere rotation: The action 洧녩洧노 determines rotation angle 洧띺洧노, which we apply to the spherical representation of the equirectangular panorama 洧논洧녡 . This yields rotated equirectangular panorama 洧논洧녡 洧노1 : 洧노1 = (洧논洧녡 洧논洧녡 洧노1 洧노1, 洧띺洧노) , where is known rotation geometric transformation defined to Equation 3 in Appendix. Panoramic video generation: We next generate videos to travel in the imaginative space by distance 洧녬洧노. Our video generator is adapted from video diffusion model conditioned on and randomly the latest explored view 洧논洧녡 sampled noise 洧랬洧노 (0, 洧냪): x洧노 洧녷洧랚2 (x 洧논洧녡 , 洧랬洧노) . 洧노1 洧노1 This approach ensures that each generated panoramic video remains consistent with the prior view, while incorporating stochastic variations to represent an explorable world. 5 Figure 5 We model the world transition as panoramic video generation process. Given the last explored 360 panorama and an action that rotates the viewing sphere, the model produces sequence of newly generated panoramic views We aim to learn to produce panoramic videos that remain visually coherent on spherical surface. Without additional constraints, training on equirectangular panorama alone can result in discontinuities at the panorama edges. To address this, we adopt spherical-consistency learning, or SCL, detailed in (Lu et al., 2024), which promotes smooth and continuous imagery across all viewing directions on the sphere. In essence, the world transition step Summary. updates the agents observed 360 panorama into newly explored view sequence. Through actiondriven rotation, spherical adjustments, and diffusion-based video model, we achieve seamless transitions and maintain coherent, panoramic representations as the agent navigates the generative environment. GenEx: Generating an Explorable World 3. Exploration in the Generative World After generating the explorable world, human or embodied agents can explore the virtual world with an exploration policy, defined in 3.1. We then introduce three exploration modes in 3.2. 3.1. Exploration Policy The exploration action 洧녩洧노 is decided by policy: 洧녩洧노 = arg max 洧녩 洧랢洧뉧롐 洧녷洧녳洧녶洧洧(洧녩洧논洧녡 洧노1, I), where is the instruction that specifies the exploration mode to be either human interaction or assisted by GPT, detailed in 3.2. Note that 洧논洧녡 洧노1 denotes the latest explored view from the previous step 洧노 1. At 洧노 = 1, it corresponds to the initial panorama 洧논0. The action 洧녩洧노 = (洧띺洧노, 洧녬洧노) defines how the agent rotates its field of view with the rotation angle 洧띺洧노 and moves forward with 洧녬洧노 distance, shaping the direction and extent of exploration. 3.2. Exploration Modes The GenEx framework enables agents to explore within an imaginative world by streaming video generation, based on current single view image 洧녰0 and the given exploration action 洧녩. We support three modes for generative world exploration, including (a) interactive exploration, (b) GPT-assisted free exploration, and (c) goaldriven navigation, illustrated in Figure 6. Interactive exploration. GenEx enables the agent to freely explore the synthetic world with an unlimited range of orientations, enhancing its understanding of the surrounding environment. Users can control the agents movement directions and distances, allowing for continuous exploration of the virtual world. GPT-assisted free exploration. However, human-provided commands can sometimes lead the model to collapse. For example, if users instruct the agent to move excessively close to wall, the resulting viewpoint may reduce the quality of subsequent generated video frames. To mitigate this, we employ GPT-4o (Achiam et al., 2023) as pilot to determine exploFigure 6 Three exploration modes interactive, GPT-assisted, and goal-driven each defined by distinct exploration instructions. ration configurations, encompassing full 360 explorable directions and distances. Given that generation quality can compoundingly degrade over time, GPT-4o acts as policy that selects actions to maximize the fidelity of generative worlds and avoid model collapsing. Goal-driven navigation. The agent receives goal with navigation instruction I, such as, Move to the blue cars position and orientation.\" GPT performs high-level planning based on the instruction and initial image, generating low-level exploration configurations in an iterative manner. GenEx then processes these configurations stepby-step, updating images progressively throughout the imaginative exploration. This allows for greater control and targeted exploration. GenEx: Generating an Explorable World 4. Advancing Embodied AI In our generative world, we can explore previously unobserved regions of the physical environment, gather more comprehensive information, and refine our beliefs for more informed decision-making. We frame this process in form of human-like decision-makingan imaginationaugmented policythat could play crucial role in shaping the future of embodied AI. Preliminary. We first denote common embodied policy as 洧랢洧랚( 洧냢洧녶, 洧녮) where 洧랚 is GPT-based planner, 洧녶 is the agents observation, 洧녮 is the goal to answer questions such as Danger ahead. Stop or go ahead?. Here, 洧냢 denotes higher-level embodied actions (e.g., answering the questions or generating navigation plans), which differ from the exploration actions 洧녩 introduced earlier. However, if the observation is limited to single initial image 洧녰0, then executing arg max洧냢 洧랢洧랚( 洧냢洧녶 = 洧녰0, 洧녮) may fail because it provides no visibility into unseen parts of the environment. The decision can become more informed if the agent gains clearer understanding of its surroundings (Fan et al., 2024). By navigating through the physical space, the agent gathers additional information about its environment (Physical path in the cyan color in Figure 7), enabling more accurate assessments and better choices moving forward. Nevertheless, physically traversing the space is inefficient, expensive, and even impossible in dangerous scenarios. To streamline this process, we use imagination as pathway for the agent to simulate outcomes without physically traversing (Imaginative path in purple color in Figure 7). The key question is: How can an agent make more informed decisions through exploration in generative 360 world? 4.1. Imagination-Augmented Policy We propose new policy based on imagined observations in the generative world, described in Algorithm 2. The Imagination-Augmented Policy consists of the following two steps: Step 1: Gather imagined observations sampled Figure 7 GenEx-driven imaginative exploration can gather observations that are just as informed as those obtained through physical exploration. Algorithm 2 Imagination-Augmented Policy Require: Initial observation 洧녰0 and world initialization description 洧녳0 goal 洧녮 to answer embodied questions. E.g, Danger aheadstop or go ahead? navigation instruction I. E.g, Navigate to the unseen parts of the environment. GenEx 洧녷(x0:洧녢 洧녰0, 洧녳0, I) defined in 2.1 and Algorithm 1. An embodied policy 洧랢洧랚3 ( 洧냢洧녶, 洧녮) conditioned on observation variable 洧녶 and goal 洧녮. 1: Gather imagined observations with GenEx: x0:洧녢 洧녷(x0:洧녢 洧녰0, 洧녳0, I) 2: Select an action with imagined observations to maximize the policy: 洧냢 = arg max 洧냢 洧랢洧랚( 洧냢 洧녰0, x0:洧녢 , 洧녮) from GenEx (Algorithm 1): x0:洧녢 洧녷(x0:洧녢 洧녰0, 洧녳0, I) . Step 2: Select an action conditioned on the imagined observations to maximize the policy: 洧냢 = arg max 洧냢 洧랢洧랚3 ( 洧냢 洧녰0, x0:洧녢 , 洧녮) . In our work, we apply GenEx for imaginative exploration and an LMM as the policy model 洧랢洧랚3, with examples in Figure 8. Compared to arg max洧녩 洧랢洧랚3 ( 洧냢 洧녰0, 洧녮) the common policy which selects the action based solely on real observations 洧녰0, the ImaginationAugmented Policy selects actions using both actual and imagined observations (洧녰0, x0:洧녢 ), potentially leading to more informed decisions. 7 GenEx: Generating an Explorable World Figure 8 Single agent reasoning with imagination and multi-agent reasoning and planning with imagination. (a) The single agent can imagine previously unobserved views to better understand the environment. (b) In the multi-agent scenario, the agent infers the perspective of others to make decisions based on more complete understanding of the situation. Input and generated images are panoramic; cubes are extracted for visualization. Step 2: Repeat Step 1 total of 洧 times, then imaginatively explore the resulting positions of all 洧 agents in our generated explorable world: {x(洧녲) 1:洧녢 }洧 洧녲=1 = (x(1) 1:洧녢 , x(2) 1:洧녢 , ..., x( 洧 ) 1:洧녢 ) . Step 3: Select an embodied action 洧냢 with imagined observations to maximize the policy: 洧냢 = arg max 洧냢 洧랢洧랚3 ( 洧냢 洧녰0, {x(洧녲) 1:洧녢 }洧 洧녲=1, 洧녮) . When exploring another agents surrounding environment, we can predict what that agent sees, understands, and might do next, which in turn helps us adjust our own actions with more complete information. 4.2. Multi-Agent Imagination-Augmented Policy Our Imagination-Augmented Policy can be generalized to the multi-agent scenario. An agent can explore the position of other agents. This predicts other agents observations and infers their understanding of the surrounding environments. Technically, we can create multiple exploration paths by providing instructions like navigate to the position of agent-k. The agent can then explore the generated 360 environment to reach agent-ks location. By extending Algorithm 2, the Multi-Agent Imagination-Augmented Policy has three steps: Step 1: Gather imagined observations by exploring the position to agent-k using Algorithm 1, with instruction I洧녲 navigate to the position of agent-k: x(洧녲) 0:洧녢 洧녷(x0:洧녢 洧녰0, 洧녳0, I洧녲) . 8 GenEx: Generating an Explorable World 5. Applications 5.1. Generation Quality We evaluate the video generation quality using FVD (Unterthiner et al., 2019), SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018), and PSNR (Hor칠 and Ziou, 2010). Table 1 shows our earlier GenEx version (Lu et al., 2024) has high video quality in all metrics. Model Representation FVD MSE LPIPS PSNR SSIM Baseline 6-view cubemaps 196.7 0.10 0.09 GenEx w/o SCL panorama 81.9 0. 0.05 26.1 29.4 0.88 0.91 GenEx panorama 69.5 0.04 0.03 30.2 0. Table 1 GenEx with high generation quality. 5.2. Exploration Loop Consistency We propose Imaginative Exploration Loop Consistency (IELC) to measure long-range exploration fidelity. For each randomly sampled closedloop path, we compute the latent MSE between the initial real image and the final generated image, and then average these values over 1000 loops with varying rotations and distances, discarding blocked paths. As shown in Figure 9, the IELC remains high even for 20m loops and multiple consecutive videos, maintaining latent MSE below 0.1 and thus indicating minimal drift. This robustness stems from preserving spherical consistency, ensuring that rotations do not compromise image quality. 5.3. Generating Birds-Eye Worlds By exploring upward along the z-axis, our method generates top-down (birds-eye view) maps directly from single panoramic image. As shown in Figure 10, these overhead layouts give the agent an objective, third-person understanding of the scene, thereby improving reasoning. Figure 10 Through generative exploration in z-axis, we are able to generate the 2D bird-eye world view of the current scene. 5.4. 3D Consistency Our method enables the generation of multi-view videos of an object through imaginative exploration with path circling around it. Our model demonstrates superior performance compared with the SOTA open-source models. Importantly, it maintains near-perfect background consistency and effectively simulates scene lighting, object orientation, and 3D relationships as in Figure 11. Figure 9 Imaginative Exploration Loop Consistency (IELC) varying distance and rotations. Figure 11 Through exploration, our model achieves higher quality in novel view synthesis for objects and better consistency in background synthesis, compared to SOTA 3D reconstruction models (StabilityAI, 2023; Tochilkin et al., 2024; Voleti et al., 2024). 9 GenEx: Generating an Explorable World 5.5. Active 3D Mapping in Generated Worlds When the agent actively explores the generative world, it continuously gathers observations that can be leveraged to reconstruct 3D map using DUSt3R (Wang et al., 2024b), shown in Figure 12. Figure 12 Active 3D mapping from single image. 5.6. Embodied Decision Making We next evaluate the Imagination-Augmented Policy proposed in 4 and share two key findings. Evaluation. We evaluate our ImaginationAugmented Policy (4.1) in Table 2. We extend the Genex-EQA in (Lu et al., 2024) with controlled counterpart for each scenario. We use Unimodal to refer to agents receiving only text context, while Multimodal reasoning demonstrates LLM decision when prompted along with an egocentric visual view. GenEx shows the performance of models equipped as agents with generative world explorer. We evaluate our Multi-Agent Imagination-Augmented Policy (4.2) in Table 3. Method"
        },
        {
            "title": "Random",
            "content": "Acc. (%) Confidence (%) Logic Acc. (%) 25.00 25.00 - Human Text-only Human with Image Human with GenEx 44.82 91.50 94.00 Unimodal Gemini-1.5 30. Unimodal GPT-4o 27.71 Multimodal Gemini-1.5 46.73 Multimodal GPT-4o 46. 52.19 80.22 90.77 29.46 26.38 36. 44.10 GPT4-o with GenEx 85.22 77.68 46.82 70. 86.19 13.89 20.22 0.0 12.51 83. Table 2 Eval of Imagination-Augmented Policy. 10 Method Random Acc. (%) Confidence (%) Logic Acc. (%) 25. 25.00 - Human Text-only Human with Image Human with GenEx 21. 55.24 77.41 Unimodal Gemini-1.5 26.04 Unimodal GPT-4o 25. Multimodal Gemini-1.5 11.54 Multimodal GPT-4o 21.88 11.56 58. 71.54 24.37 26.99 15.35 21.16 13. 46.49 72.73 5.56 5.00 0.0 6. GPT4-o with GenEx 94.87 69.21 72.11 Table 3 Evaluation of Multi-Agent ImaginationAugmented Policy. Findings. We identified two findings based on the results from human policy ( grey row ) and GenEx-enhanced GPT policy ( blue row ). Vision without imagination can be misleading for GPTs. Interestingly, unimodal response that relies solely on the environments text description often outperforms its multimodal counterparts, which incorporate both text and egocentric visual inputs. This suggests that vision without imagination can be misleading, as it may lead to incorrect inferences due to the lack of spatial context and relying only on languagebased commonsense reasoning. This highlights the importance of integrating visual imagination to enhance the accuracy and reliability of the agents decision-making processes. GenEx has the potential to enhance cognitive abilities for humans. Human performance results reveal several key insights. First, individuals using both visual and textual information achieve significantly higher decision accuracy compared to those relying solely on text. This indicates that multimodal inputs enhance reasoning. Secondly, when provided with imagined videos generated by GenEx, humans make even more accurate and informed decisions than in the conventional image-only setting, especially in multi-agent scenarios that require advanced spatial reasoning. These findings demonstrate GenExs potential to enhance cognitive abilities for effective social collaboration and situational awareness. GenEx: Generating an Explorable World 6. Discussion 7. Conclusion Related works. Advances in single-image 3D modeling (Tewari et al., 2023; Yu et al., 2024) enable novel view synthesis but are limited by render distances or fields of view, relying heavily on depth estimator. Meanwhile, video generation methods (Blattmann et al., 2023; Kondratyuk et al., 2024; OpenAI, 2024) excel at producing diverse videos but often lack physical grounding, reducing their utility for exploration. Video generation models (Bu et al., 2024; Du et al., 2024a,b; Wang et al., 2024a; Yang et al., 2024) are capable of directly synthesizing visual plans for decision-making, but world exploration for imagined observations remains unexamined. Our approach unites these domains by drawing on physically grounded data to generate 3D-consistent, explorable worlds and advance embodied AI. Extension to our earlier work. Our earlier work (Lu et al., 2024), published on arXiv in November 2024, conceptualized world transitions, exploration, and applications in embodied AI, but it did not address the crucial aspect of world initialization from single image. Relation to concurrent industrial progress. WorldLabs (WorldLabs, 2024) recently released demos of anime-world generation from single image. DeepMind (DeepMind, 2024) released blog on interactive world models. Our work complements these ongoing industrial efforts, jointly contributing toward shared vision: creating rich, interactive, 3D-consistent generative worlds. Importantly, we offer our technical details. Beyond this, we also introduce the concept of an Imagination-Augmented Policy by exploring the generative world, further expanding the frontiers of embodied AI. Challenges. Bridging imaginative and realworld environments remains core challenge in AI. Current approaches rely on physical engines. Future work must address several key limitations, including sim-to-real adaptation, real sensor integration, dynamic conditions, and ethical safeguards, to ultimately enable reliable deployment of embodied AI in diverse physical settings. We introduce GenEx, platform that Generates an Explorable world and enables agents, either instructed by human users or GPT, to freely explore in this imaginative panoramic world. By generating 3D-consistent environments from single image, our approach enables the creation of immersive and interactive worlds offering boundless landscape, grounded in the physical world, and explored by agents. We demonstrate diverse applications of GenEx, showing that this generative explorable world technique can create diverse and consistent 3D environments, build active 3D mappings, and advance embodied decision-making by allowing agents to create more informed and effective plans. Furthermore, GenExs framework supports multi-agent interactions, paving the way for more advanced and cooperative AI systems. This work marks an advancement toward real-world navigation, interactive gaming, and achieving human-like intelligence in embodied AI."
        },
        {
            "title": "Author Contributions",
            "content": "We list author contributions here alphabetically by last name. Please direct all correspondence to the project lead Jieneng Chen (jchen293@jh.edu)."
        },
        {
            "title": "Core Contributors",
            "content": "Taiming Lu: project leadership, data engine, model research and pipeline, infrastructure Tianmin Shu: embodied policy research, writing, revising, technical advice Junfei Xiao: image-to-panorama data and model research, writing, editing"
        },
        {
            "title": "Contributors and Advisors",
            "content": "Rama Chellappa: device support, advice Daniel Khashabi: writing, technical advice Cheng Peng: data support, editing Jiahao Wang: math, postprocessing, editing Chen Wei: revising, editing, writing advice Luoxin Ye: model, postprocessing Alan L. Yuille: math revising, funding, editing, writing advice, technical advice 11 GenEx: Generating an Explorable World"
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. J. Bilcke. Flux.1-[dev] panorama lora (v2), 2024. URL https://huggingface.co/jbilcke-hf/ flux-dev-panorama-lora-2. Accessed: 202412-05. A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. URL https://arxiv.org/ abs/2311.15127. Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, and H. Li. Closed-loop visuomotor control with generative expectation for robotic manipulation. arXiv preprint arXiv:2409.09016, 2024. DeepMind. Genie 2: large-scale founURL 2024. dation world model, deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model. Accessed: 2024-12-10. Y. Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum, et al. Video language planning. ICLR, 2024a. Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel. Learning universal policies via text-guided video generation. In NeurIPS, 2024b. L. Fan, M. Liang, Y. Li, G. Hua, and Y. Wu. Evidential active recognition: Intelligent and prudent openworld embodied perception. In CVPR, 2024. A. Hor칠 and D. Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010. D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, V. Birodkar, et al. Videopoet: large language model for zero-shot video generation. ICML, 2024. B. F. Labs. Flux.1 [dev], 2024. URL https: //huggingface.co/black-forest-labs/ FLUX.1-dev. Accessed: 2024-12-05. OpenAI. Video generation models as world simulators, 2024. StabilityAI. Stable zero123, 2023. A. Tewari, T. Yin, G. Cazenavette, S. Rezchikov, J. Tenenbaum, F. Durand, B. Freeman, and V. Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In NeurIPS, 2023. D. Tochilkin, D. Pankratz, Z. Liu, Z. Huang, A. Letts, Y. Li, D. Liang, C. Laforte, V. Jampani, and Y.-P. Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate generative models of video: new metric and challenges, 2019. URL https://arxiv.org/ abs/1812.01717. V. Voleti, C.-H. Yao, M. Boss, A. Letts, D. Pankratz, D. Tochilkin, C. Laforte, R. Rombach, and V. Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. arXiv preprint arXiv:2403.12008, 2024. B. Wang, N. Sridhar, C. Feng, M. Van der Merwe, A. Fishman, N. Fazeli, and J. J. Park. This&that: Language-gesture controlled video generation for robot planning. arXiv preprint arXiv:2407.05530, 2024a. S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024b. Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. WorldLabs. Generating worlds, 2024. URL https:// www.worldlabs.ai/blog. Accessed: 2024-1210. S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. H.-X. Yu, H. Duan, C. Herrmann, W. T. Freeman, and J. Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv preprint arXiv:2406.09394, 2024. T. Lu, T. Shu, A. Yuille, D. Khashabi, and J. Chen. Generative world explorer. arXiv preprint arXiv:2411.11844, 2024. R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 12 GenEx: Generating an Explorable World"
        },
        {
            "title": "Appendix",
            "content": "A.1. Preliminary: Equirectangular Panorama Images Figure 13 Left: Pixel Grid coordinate and Spherical Polar coordinate systems; Middle: rotation in Spherical coordinates corresponds to rotation in 2D image; Right: expansion from panorama to cubemap or composition in reverse. A.1.1. Coordinate Systems An Equirectangular Panorama Image captures all perspectives from an egocentric viewpoint into 2D image. Essentially, it represents spherical coordinate system on 2D grid. Definition D.1 (Spherical polar coordinate system). S: Taking the origin as the central point, point in this system is represented by coordinates (洧랯, 洧랚, 洧) S, where 洧랯 denotes the longitude, 洧랚 the latitude, and 洧 the radial distance from the origin. The ranges for these coordinates are 洧랯 [洧랢, 洧랢), 洧랚 [洧랢/2, 洧랢/2], and 洧 > 0. Definition D.2 (Cartesian coordinate system for panoramic image). P: In this system, pixel is identified by the coordinates (洧녹, 洧녺) P, where 洧녹 and 洧녺 correspond to the column and row positions on the 2D panoramic image plane, respectively. Here, 洧녹 ranges from 0 to 洧녥 1 and 洧녺 ranges from 0 to 洧냩 1. Definition D.3 (Sphere-to-Cartesian Coordinate Transformation). The transformation between the spherical polar coordinates and the panoramic pixel grid coordinates can be defined by the following functions: 洧녭SP (洧랯, 洧랚) = 洧녭PS (洧녹, 洧녺) = (cid:18) 洧녥 2洧랢 (cid:18) 2洧랢洧녹 洧녥 (洧랯 + 洧랢), 洧랢, 洧랢 2 (cid:17) (cid:19) 洧랚 (cid:19) 洧냩 洧랢 (cid:16) 洧랢 2 洧랢洧녺 洧냩 (1) (2) Here, the function 洧녭SP maps the spherical coordinates (洧랯, 洧랚) to the pixel coordinates (洧녹, 洧녺), and the inverse function 洧녭PS maps the pixel coordinates (洧녹, 洧녺) back to the spherical coordinates (洧랯, 洧랚). This transformation ensures that the entire spherical surface is represented on the 2D panoramic image. Panorama effectively stores every perspective of the world from single location. In our work, due to the nature of panoramic images, we are able to preserve the global context during spatial navigation. This allows us to maintain consistency in world information from the conditional image, ensuring that the generated content aligns coherently with the surrounding environment. 13 GenEx: Generating an Explorable World A.1.2. Panorama Image transformations The spherical format allows various image processing tasks. For example, the image can be rotated by an arbitrary angle without any loss of information due to the spherical representation. Additionally, it can be broken down into cubemaps for 2D visualization, as shown in Figure 13. Definition D.4 (Rotation Transformation in Spherical Polar Coordinate System). Since panorama image is in spherical format, we can rotate the image to face different angle while preserving the original image quality. The rotation can be performed using the following formula: (洧녹, 洧녺, 풊洧랯, 풊洧랚) = 洧녭SP (R ( 洧녭PS (洧녹, 洧녺), 풊洧랯, 풊洧랚)) Where the rotation function is defined as: (洧랯, 洧랚, 풊洧랯, 풊洧랚) = (洧랯 + 풊洧랯 (mod 2洧랢), 洧랚 + 풊洧랚 (mod 洧랢)) If there is no explicit input, both 풊洧랯 and 풊洧랚 can be set to 0. (3) (4)"
        }
    ],
    "affiliations": []
}