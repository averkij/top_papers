{
    "paper_title": "Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning",
    "authors": [
        "Siyuan Huang",
        "Yue Liao",
        "Siyuan Feng",
        "Shu Jiang",
        "Si Liu",
        "Hongsheng Li",
        "Maoqing Yao",
        "Guanghui Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning."
        },
        {
            "title": "Start",
            "content": "Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning Siyuan Huang, Yue Liao, Siyuan Feng, Shu Jiang, Si Liu, Hongsheng Li, Maoqing Yao, Guanghui Ren 5 2 0 2 4 1 ] . [ 1 6 4 6 1 1 . 3 0 5 2 : r Abstract The pursuit of data efficiency, where quality outweighs quantity, has emerged as cornerstone in robotic manipulation, especially given the high costs associated with realworld data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection (ADC), Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts collaborative perturbation paradigm: during single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the teleoperator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning. More information can be found on our project page: https://sites.google.com/view/adc-robot. I. INTRODUCTION In real-world robotic manipulation, collecting real-scene imitation learning data requires humans to teleoperate robotic arms to perform tasks based on linguistic instructions, which is labor-intensive and costly process [1], [2]. Each demonstration demands significant human effort (e.g., expert operation, precise annotation), making it economically infeasible to scale data quantity for diverse environmental coverage. Real-world complexityvarying object configurations, lighting, and task constraintsfurther exacerbates the need for massive data diversity to ensure robust generalization [3]. While recent approaches attempt to address this by generating diversity through simulated environments [4], [5] or lowcost Universal Manipulation Interface (UMI) grippers [3], [6] (sampling one variation per trial, such as object poses), these methods face two fundamental limitations: indicates the equal contribution and indicates corresponding authors. Siyuan Huang is with Shanghai Jiao Tong University. Yue Liao and Hongsheng Li are with MMLab, CUHK. Siyuan Feng, Shu Jiang, Maoqing Yao and Guanghui Ren are with Agibot. Si Liu is with Beihang University. Comparative Analysis of the Real-Data Collection Loop in Fig. 1. Robotic Manipulation. (a) Traditional Approach: tele-operator executes tasks via fixed linguistic instructions in static visual environments. (b) Adversarial Data Collection (ADC) Framework: Employs Two-Humansin-the-Loop approach, where secondary operator intervenes to perturb the primarys execution dynamically when the tele-operator is executing task. (c) ADC Loop: The adversarial operator introduces visual (backgrounds, object positions/poses) and linguistic (task goals) perturbations, shifting environmental context and target objects within single episode. Synthetic data, despite its scalability, suffers from domain gap due to idealized physics/kinematics, hindering transfer to real-robot dynamics; Extending such diversity to tele-operated systems remains impractical, as even minor environmental changes (e.g., repositioning objects) require costly human-in-theloop resampling. To contextualize our approach, we first analyze the data requirements and structural constraints of state-of-the-art robotic imitation frameworks, particularly Vision-LanguageAction (VLA) models. VLAs integrate Multimodal Large Language Models (MLLMs) to unify reasoning over visual inputs, language instructions, and robotic actions. Most VLA [7], [8] frameworks inherit the Markov assumption, predicting actions based solely on the current visual observation. This design necessitates training data in the form of triplets: current visual scene, language instruction, and action supervision. As shown in Fig. 1(a), Conventional data collection, however, treats an entire task (e.g., grasping the orange) as single demonstration, slicing it into ineffisequential units. These units suffer from inherent cienciesvisually redundant frames, repetitive language instructions, and highly correlated actionseffectively diluting the informational value of each collected demonstration. For instance, 30-second pick-and-place task might yield hundreds of near-identical diversity in scenes or commands. training triplets, with minimal To overcome these limitations, we propose Adversarial Data Collection (ADC), collaborative Human-in-theLoop (HiL) framework that systematically injects controlled perturbations during data acquisition to maximize per-demonstration diversity and robustness. As illustrated in Fig. 1, the ADC framework operates through two synchronized human roles: tele-operator executes primary manipulation tasks via standard robotic interfaces, while an adversarial operator dynamically modulates task parameters through real-time perturbations across perceptual, linguistic, and physical dimensions. During single demonstration episode, the adversarial operator alters scene configurations (e.g., repositioning objects, rotating target objects mid-grasp and adjusting background) and paraphrases task instructions (e.g., grasping the orange grasping the watermelon). These perturbations force the tele-operator to adaptively re-plan actions, such as re-grasping strategies or trajectory corrections, under evolving constraints, thereby compressing recovery behaviors, instruction compositional generalization, and environmental variations into unified demonstration. By systematically coupling human-driven adversity with real-time adaptation, ADC not only elevates demonstration efficiency but also cultivates model robustness against perceptual ambiguities, linguistic variability, and physical uncertainties, which is critical for deploying VLAs in unstructured real-world environments To evaluate the effectiveness of ADC, we constructed dataset consisting of 200 demonstrations including balanced adversarially perturbed demonstrations (96K frames) from ADC and demonstrations (90K frames). Each demonstration from ADC incorporates multiple layers of real-time perturbations, spanning visual and linguistic dimensions, introduced dynamically during human tele-operation. While marginally more time-intensive per demonstration, ADCtrained models achieve superior generalization and robustness with significantly fewer demonstrationsoutperforming models trained on larger-scale conventional datasets in handling ambiguous instructions, environmental perturbations, and unseen object configurations. This efficiency stems from ADC capacity to encode real-world complexity through intentional adversity, transforming data quality into substitute for quantitya critical advancement for scalable robotic learning. Additionally, we are curating large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. Upon completion, this benchmark will be open-sourced to facilitate advancements in robotic imitation learning. II. RELATED WORK A. Data Collection in Robot Learning"
        },
        {
            "title": "Data collection methods in robot",
            "content": "learning have been extensively studied to improve generalization, generally with focus on scaling up robot datasets [9], [10], [2], [1]. These efforts have shown that greater object and task diversity can significantly improve policy robustness, highlighting the importance of collecting diverse datasets. For instance, Droid [2], one of the largest existing datasets, contains 76k demonstration trajectories collected across more than 500 scenes. More recently, studies [11], [3] have explored strategies for efficient data collection to enhance policy generalization; however, their discussions are largely constrained to compositional generalization with respect to visual factors. In this paper, we extend the scope of data collection by considering both visual and language dimensions, aiming to improve generalization across broader range of factors. Additionally, we introduce real-time human intervention during the data collection process, dynamically perturbing robot actions rather than merely resetting initial conditions, thereby implicitly accounting for human-robot interaction in deployment scenarios. B. Vision-based Imitation Learning"
        },
        {
            "title": "Traditional",
            "content": "imitation learning methods have relied on either state-based [12] or image-based [13] representations to describe environments and goals. While state-based methods struggle to handle unstructured environments, image-based methods offer richer representations but face challenges related to goal ambiguity and specification. To address these issues, natural language has emerged as an intuitive and flexible way to specify goals, with recent work integrating language goals and image observations to enable more adaptable policy learning [14], [15]. Additionally, finetuning VLMs into VLAs has further advanced this area [7], [16]. Furthermore, some studies have introduced diffusion models [17] into policy learning to better model multimodal action distributions [18], [19], [20]. Meanwhile, other work has explored leveraging large-scale, unlabelled action videos to enhance the temporal and spatial understanding of policies [21], [22]. As vision-based models scale up, they require increasingly diverse and high-quality robot data to avoid overfitting to shortcuts introduced by traditional data collection methods. To address this, we propose Adversarial Data Collection (ADC), which enhances information density through real-time perturbations. C. Generalization and Robustness in Robotic Policy Achieving both generalization and robustness in robotic policies is crucial, particularly in the context of dynamic human-robot interactions, where policies must not only generalize to unseen objects and environments but also remain robust to human interventions. Prior works have explored improving generalization through pre-trained visual representations [23] and diverse datasets [9], with some focusing on adapting to new environments [24], [25] or objects [26], [27]. However, these efforts often overlook robustness in scenarios involving human intervention, which is critical for real-world deployment. Additionally, while compositional generalizationreasoning about unseen combinations of environmental factorshas been studied [25], [28], prior work primarily examines small-scale policies. As robotic learning transitions to large Vision-Language Action (VLA) models [7], there is growing need to enhance both generalization and robustness Fig. 2. The overview of ADC. During training data collection, we introduce several adversarial perturbations, including dynamic visual perturbations and adaptive linguistic challenges. These perturbations increase information density, expand state space coverage, and provide more complete observations of target objects. The resulting high-quality dataset enables the trained policy model to achieve strong robustness and generalization, outperforming models trained with conventional data collection strategies. through fine-tuning on task-specific datasets. In this work, we address these challenges by systematically analyzing the interplay between generalization, robustness, and data collection, focusing on dynamic scenes involving human-robot interactions and scaling strategies for large VLA models. III. APPROACH To ground our approach, we first review the structure of training data in Vision-Language-Action (VLA) models (Sec. III-A), then detail our Adversarial Data Collection (ADC) framework in Sec. III-B, and finally outline its integration into robotic imitation policies (Sec. III-C) and VLA training (Sec. III-D). A. VLA Data Units Density Analysis 1) VLA Architecture and Training Data Structure: The Visual-Language-Action (VLA) architecture adapts pretrained multimodal Large Language Models (MLLMs) for robotic control through residual policy heads. Focused on mainstream memory-less pipelines (e.g., OpenVLA [7]), these temporal-independent models process multi-view observations (Vmulti-view ) and language instructions (Lt ) to predict actions at via: p(at Vmulti-view , Lt ) = VLA(Vt , Lt ) (1) Training requires demonstration units Ut (Vt , Lt , ), where denotes expert actions. Traditional datasets assemble these units into fixed episodes Ei = {U1, ...,Un} with static environments, causing two inefficiencies: Intra-Episode Redundancy: > 70% of consecutive Ut share redundant visual-language contexts Inter-Episode Fragmentation: Functionally equivalent units remain isolated across episodes 2) Density Optimization: To overcome these limitations, we reformulate data quality via information density per demonstration: ρ E[U(E)], U(E) {Ut Ut Ut } where Ut Ut indicates functional equivalence under task constraints, including spatial positions, prompt diversity, etc. Instead of maximizing episode counts (max {Ei}), our adversarial strategy maxperturb ρ enriches unit diversity within individual demonstrations through: (2) Multi-Order Variations: Perturbing object poses, lighting, and instructions mid-episode Compositional Coupling: Synergistic combinations of visual-linguistic perturbations As visualized in Fig. 2, this approach compresses hundreds of traditional trials into single demonstrations by forcing adaptive responses to dynamic perturbations. The resultant dataset structure aligns with VLAs Markovian requirementsmaximizing unique Ut coverage while eliminating temporal redundancy. B. ADC: Adversarial HiL Framework ADC transforms conventional teleoperation into an adversarial HiL process, where two collaborative roles, the tele-operator (executing tasks) and the adversarial operator (introducing perturbations), interact dynamically during data collection. This framework introduces controlled perturbations across visual and linguistic dimensions to maximize per-demonstration diversity while preserving physical plausibility. C. ADC in Conventional Robotic Policy To validate our proposed ADC strategy, we first conduct experiments on conventional robotic policies trained with single-task imitation learning. For data collection and deployment, we use the widely adopted Aloha [29] hardware system and the ACT model as our baseline, as shown in Figure 3(a). We evaluate the task Pick the plastic cup and place it on the plate, which requires the robot to grasp the cups handle while addressing the challenge of transparent material. Our adversarial data collection strategy follow the concepts of dynamic visual perturbations incorporating two key components: spatial diversity maximization and dynamic target perturbation. For controlled comparison, we collect equal volumes of adversarially generated and conventional demonstration data. Given the instability of the hardware and the fact that conventional robotic policy is not the primary focus of this paper, we provide only qualitative analysis. Policies trained on adversarially collected data demonstrate improved robustness and generalization performance. This improvement highlights the enhanced data density and state-action coverage in ADC. However, we observe oscillatory grasping behavior when the placement height of the target object is varied. We attribute this to two main factors: (1) the inherent difficulty of depth estimation under increased state variance, and (2) the capacity limitations of smaller models cannot cover the high state coverage and low action consistency brought by ADC, consistent with findings in [30]. These results provide empirical justification and motivation for scaling our methodology to VLA models. D. ADC in VLA To validate ADC at the VLA scale, we implement our framework on the AgiBot G1 robotic platform using AR teleoperation for data collection [1], as shown in Figure 3(b). We use π0 [8], state-of-the-art open-source VLA policy with demonstrated real-world manipulation capabilities, as our base model. Our evaluation focuses on the composite task Grasp the [FRUIT-TYPE], place into the [CONTAINER], where FRUIT-TYPE {orange, kiwi, orange} and CONTAINER { green plate, blue plate } . The pretrained model checkpoint [1] is obtained through pretraining on the AgiBot-World dataset and is then fine-tuned on this task to enhance its performance. This task requires both instruction grounding and visual target localization. To address these challenges, we design the following adversarial strategies: Visual Perturbation. Dynamically displace target objects before grasping and reposition containers after preplacement. Linguistic Perturbation. Modify instructions after successful grasping, e.g., changing Place the orange into the green plate to Place the orange into the purple plate. To manage annotation challenges from dynamic instructions, we employ task decomposition with subtask-level Fig. 3. Hardware setup used in ADC for both data collection and evaluation experiments. The Aloha robot is employed for conventional robotic policy experiments, which include various visual distractors. The AgiBot G1 robot is utilized for the VLA policy experiments, where different dynamic perturbations are applied. 1) Visual Perturbations: The adversarial operator introduces physics-grounded visual disruptions through two key mechanisms: Positional Dynamics: Target objects are initialized at positions sampled from task-specific Gaussian distributions (µtask, σ 2 task). If the end-effector (EEF) fails to enter the proximity threshold during grasping attempts, the adversarial operator randomly changes the objects positions within the worktable area. Grasp Pose Disruptions: Grasp Pose Disruptions: When the end-effector enters proximity threshold (set to 15 cm in practice), controlled perturbation impulses are applied to objects, altering their positions and orientations. This forces real-time grasp repositioning and reorientation, requiring the system to find the next-best grasp configuration. These mechanisms are also applied to target containers, further enhancing task complexity. 2) Linguistic Perturbations: Language instructions undergo contextual transformations to challenge static command assumptions: Mid-Execution Interruption: Task goals are dynamically altered during critical phases, including changes in the target object (e.g., place cup place bottle), modifications to the required action (e.g., pick up cup push cup), or simultaneous changes in both the object and the action (e.g., pick up cup push bottle). These transformations require immediate adaptation while maintaining task stability. Dynamic Spatial Redefinition: Spatial descriptors such as left or near are continuously redefined relative to changing object positions, preventing overfitting to fixed coordinate systems. In this paper, we primarily demonstrate the first mechanism, as spatial understanding for VLA is separate research focus. Through these coordinated perturbations, hundreds of static demonstration variations are compressed into unified episodes via real-time human adaptation, as illustrated in Fig. 2. The tele-operators responsive behaviorstrajectory replanning, grasp recovery, and compositional task switchinggenerate rich and diverse data units Ut = (Vmulti-view ) that conventional pipelines cannot achieve. , Lt , TABLE COMPARISON OF DATA COLLECTION STRATEGIES. Method Traditional ADC Vis Perturb. Lin. Perturb. Varied Height #Epis. 120 80 #Frame 90k 96k Collection Time 25s per episode 40s per episode Additional Label Time 10s per episode 15s per episode Avg Time 46.7ms/frame 45.8ms/frame labeling (grasping/placement phases) while maintaining temporal continuity. Leveraging the VLA models compositional generalization capability, we enable end-to-end policy learning from this structured input. Notably, we exclude kiwi placement from training data, leaving it as an unseen task for evaluation. IV. RESULTS AND ANALYSIS To evaluate ADC for VLA models, we follow the strategy outlined in Section III-D and fine-tune the open-source π0 model. To minimize modification effort, we adopt the default settings, using two wrist images and one head image as visual inputs. To assess the compositional generalization ability introduced by ADC, we exclude the data for the task Place the Kiwi into the [CONTAINER] from the training set, expecting the model to generalize both action execution and language understanding to this unseen task. Each task is executed 10 times in the real world, and we report the success rate(SR). The data collected with different strategies for training the VLA model is summarized in Table I. The Additional Label Time is mainly attributed to language-visual alignment verification, as language instructions can vary for single trajectory. Note that we define the Episode number as the scenario reset count, consistent with CALVIN [31]. A. Evaluating ADC in Static Environments We begin with the evaluation in the most common static environment. To assess the models performance more granularly, we report the SR for individual subtasks, such as Pick and Place, separately. The evaluation is conducted at three different worktable heights. Among these, Var. 1 represents the standard table height, while Var. 3 corresponds to an extreme height, pushing the robot to operate near its maximum reach limit. normal positions refer to cases where the target object is placed near the tables center, similar to the traditional data collection process, while varied positions involve sampling the target objects location across the entire work area. The evaluation results are summarized in Table II. The VLA trained with ADC outperforms the baseline VLA trained on the traditionalcollection dataset across all heights and positions, achieving an average success rate of 0.72 compared to 0.13 at the most challenging height (Var. 3). It demonstrates strong compositional generalization on the Place-C task, while the model trained with traditional data collection procedure struggles, particularly under varied positions (0.0). Notably, while both models perform well under the normal positions setting, the baseline model fails to handle varied positions, with significant performance drop to 0.0 for all tasks. TABLE II EVALUATION RESULTS IN STATIC ENVIRONMENT. THE COLUMN Place-C IS HIGHLIGHTED FOR COMPOSITIONAL GENERALIZATION EVALUATION. Method Height Normal Positions Varied Positions Avg. Traditional Var. 1 ADC Traditional Var. 2 ADC Traditional Var. 3 ADC Pick Place Place-C Pick Place Place-C 1.0 1.0 0.5 1.0 0.3 1.0 0.0 1.0 0.0 1.0 0.0 0.5 1.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.8 0.8 1.0 0.3 1.0 0.5 1.0 0.0 1.0 0.0 1.0 0.0 1. 0.47 1.0 0.3 1.0 0.13 0.72 B. Evaluating ADC in Dynamic Environments We further evaluate our method in dynamic environments. Specifically, we apply adversarial evaluation, similar to the approach used during data collection. For the visual aspect, human evaluator modifies the positions of the target object or container before the effective actions are executed. For the linguistic aspect, the human evaluator alters the language instructions in real time. The evaluation experiments are conducted at normal worktable height, with target objects placed across the entire workspace. The results are summarized in Tables III and IV. The results demonstrate that the model trained with ADC exhibits significant robustness to visual perturbations. Specifically, dynamic changes in spatial positions have minimal impact on performance, achieving SR of 0.88, comparable to the static environment evaluation. In contrast, the baseline model fails entirely under these conditions. For linguistic perturbations, we observe that performance is notably affected during the During Grasp phase. This is likely due to the fact that, once the gripper has already grasped or is very close to the object, changes in instructions require the policy to first halt the current action, re-evaluate the situation, and then decide on new target, introducing significant challenges. TABLE III EVALUATION RESULTS IN DYNAMIC ENVIRONMENT AGAINST THE VISUAL PERTURBATIONS. Pert. Method Varied Containers Pos. Varied Objects Pos. Avg. Pick Place Pick Place Place-C Place-C Traditional ADC 0.0 0.8 0.0 0.7 0.0 1. 0.0 0.8 0.0 1.0 0.0 1.0 0.0 0.88 C. Evaluating ADC in Sensor-Failure Scenarios We further evaluate the model trained with ADC under extreme conditions, simulating scenarios where the equipped camera hardware fails. To replicate such failure scenarios, TABLE IV EVALUATION RESULTS IN DYNAMIC ENVIRONMENT AGAINST THE LINGUISTIC PERTURBATIONS. Pert. Time Before Grasp. During Grasp. After Grasp. Methods Pick Place Pick Place Pick Place Traditional ADC 0.0 1.0 0.0 1. 0.0 0.6 0.0 0.7 0.0 1.0 0.0 1.0 we replace the input from the target camera with an all-zero matrix before passing it to the VLA model. The evaluation results are presented in Table V, demonstrating that the model trained with ADC maintains consistent robustness across various masking settings. To investigate the reasons behind this robustness, we visualize the attention maps between visual observations and action predictions extracted from the cross-attention layer. In this setup, the VLMs output tokens serve as inputs to the action expert header when specific camera is masked. As shown in Figure 4, the model trained with ADC dynamically shifts its attention to functional cameras when specific camera is masked. In contrast, the model trained with the traditional data collection process tends to focus more on irrelevant features, such as table edges, rather than the target objects when the wrist camera image is masked. The attention maps from the model trained with ADC exhibit more focused and meaningful response under masking conditions. Furthermore, ADC data includes more instances of occlusion and scenarios where target objects are observed from multiple viewpoints, resulting in more comprehensive visual coverage, as illustrated in Figure 5. This enriched data distribution significantly enhances the final policy models performance and robustness. TABLE EVALUATION RESULTS WITH MASKED CAMERAS IN STATIC ENVIRONMENT. Masked Cam. Right Wrist Head Avg. Methods Pick Place-AB Pick Place-AB Traditional ADC 0.0 0.6 0.0 0.5 0.0 0.7 0.0 0.4 0.0 0. Fig. 4. Comparison of attention maps when one camera is masked. Models trained with ADC focus more precisely on functional cameras, demonstrating superior attention concentration compared to models trained with traditional data collection pipelines. visual and linguistic perturbations (as detailed in Section IVB) on standard table height (Var.1). The results, summarized in Table VI, reveal that even with only 20% ADC data, the model demonstrates significantly greater robustness and positional generalization in both static and dynamic environments compared to the model trained with 100% traditionally collected data. Furthermore, as the proportion of ADC data increases, the policy models robustness improves further. TABLE VI PERFORMANCE COMPARISON OF DIFFERENT DATASETS IN STATIC AND DYNAMIC ENVIRONMENTS. Static Env. Dynamic Env. Dataset Receipt Pick Place Pick Place Avg. 100% Traditional 20% Ours 50% Ours 100% Ours 0.5 0.5 0.83 0.9 0.45 0.75 0.75 0.875 0.0 0.58 0.63 0. 0.0 0.75 0.75 0.94 0.24 0.65 0.74 0.89 D. Further Analysis Data Efficiency. As discussed in Section III-B, the data collected with ADC demonstrates higher information density. This raises the hypothesis that significantly less ADC to train the VLA model while data might be sufficient achieving comparable or even superior performance to model trained on 100% traditionally collected data. To test this hypothesis, we train additional VLA models under two conditions: (1) one model is trained with approximately 20% ADC data, and (2) another model is trained with 50% ADC data. These models are evaluated in static environments with both standard and varied object positions (as described in Section IV-A) and in dynamic environments involving both Complete Object Observation Coverage. We analyze the benefits of ADC by examining the visual observation coverage for the task Grasp the orange. As shown in Figure 5, compared to wrist observation images collected using the standard strategy, those collected with ADC exhibit substantially greater variation in the oranges visual appearance. This increased diversity in observation enhances the models generalization capabilities while reducing the need for extensive data collection. Dynamic HRI. Previous deployments and evaluations of VLA models have primarily focused on static environments, either in the real world or in simulators. However, the ultimate goal of robotics is seamless integration with humans, Fig. 5. Comparison of observation coverage for the task Grasp the orange. In the traditional data collection process, the target object (orange) is observed from similar viewpoints, resulting in limited visual diversity. In contrast, ADC introduces dynamic perturbations, allowing the orange to be observed from wider range of viewpoints. This leads to greater visual variation in the ADC dataset, improving model robustness and generalization. Fig. 6. Dynamic Human-Robot Interaction (HRI) scenarios. The robot is tasked with grasping the target fruit from the human hand, where the humans hand may move during the manipulation tasks. Evaluation experiments are conducted across different scenes. requiring robots to provide dynamic and adaptive responses during task execution, even under human intervention. To evaluate this capability, we design two task variations: (1) human holds and moves the target object during grasping, altering its pose and height; and (2) language perturbation, as described in Section IV-B. These scenarios require the robot to dynamically adjust its action predictions. The experimental setup is illustrated in Figure 6. Models trained with ADC demonstrate the ability for dynamic adaptation, critical prerequisite for future human-robot interaction (HRI). Shuffle Buffer. As discussed in [32], effective shuffling of frames during training is crucial for the final performance of VLA models, ensuring that each batch contains diverse information. Their approach required significant engineering effort to interleave images from different trajectories. In contrast, trajectory data collected via ADC inherently includes diverse motion and semantic information even within single trajectory. This significantly reduces the engineering effort needed to manage large shuffle buffers. Data Collection Cost Discussion. While both ADC and conventional methods exhibit comparable per-frame annotation latency (45.8ms vs. 46.7ms) as shown in Table I, their human cost profiles differ substantially. ADC requires two operators: tele-operator for task execution and an adversarial operator for perturbations. Though doubling the labor count, the adversarial operator demands significantly lower expertise and intermittent involvement compared to Fig. 7. Autonomous Failure Recovery in ADC-Trained Robotic Grasping: Real-time demonstration of failure recovery after empty grasp. Following initial contact the system autonomously recalibrates grip pose parameters and executes precision-aligned regrasp to complete the task. loss during peach acquisition, continuous tele-operation. Critically, ADC marginally higher episodic time cost (< 2) is counterbalanced by its > 5 improvement in data efficiency. This tradeoffminimal additional for exponential gains in sample qualityestablishes ADC as practical solution for scalable robotic learning in cost-constrained settings. labor Novel Scene Generalization. Although the ADC data is collected without tablecloths, using only the original white the final VLA policy demonstrates strong table surface, generalization capabilities in unseen, novel scenes. As shown in Figure 6, evaluation experiments are conducted in different scenes where tablecloths are introduced. We attribute this generalization ability to two key factors: (1) The strong pretrained visual encoders employed in the VLA model. (2) The ADC data collection procedure, which includes more visually adversarial images, making the trained model robust and generalizable. In contrast, the same model architecture trained with traditional data performs worse in zero-shot unseen scenarios. Failure Recovery Analysis.The VLA model trained on our ADC dataset demonstrates an autonomous failure recovery ability. As shown in Figure 7, the robotic arm automatically initiates secondary grasping attempt upon detecting an empty-gripper state. This capability is enabled by the enhanced robustness gained through systematically injected disturbances during ADC-based training. Two key factors brought by ADC contribute to this failure recovery ability: (1)The ADC data includes diverse grasp initialization poses, allowing the learned policy to generalize to broader range of scenarios. (2) During the adversarial data collection process, tele-operator failures are inevitable due to the injected disturbances. When failures occur, the tele-operator adjusts and reattempts the task. This retry data further enhances the models ability to recover from failure. [15] X. Jia, A. Donat, X. Huang, X. Zhao, D. Blessing, H. Zhou, H. Zhang, H. A. Wang, Q. Wang, R. Lioutikov et al., X-il: Exploring the design space of imitation learning policies, arXiv preprint arXiv:2502.12330, 2025. [16] X. Li, P. Li, M. Liu, D. Wang, J. Liu, B. Kang, X. Ma, T. Kong, H. Zhang, and H. Liu, Towards generalist robot policies: What matters in building vision-language-action models, arXiv preprint arXiv:2412.14058, 2024. [17] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, in International Conference on Learning Representations (ICLR), 2021, available at https://openreview.net/forum?id=St1giarCHLP. [18] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, p. 02783649241273668, 2023. [19] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu, Rdt-1b: diffusion foundation model for bimanual manipulation, arXiv preprint arXiv:2410.07864, 2024. [20] Z. Hou, T. Zhang, Y. Xiong, H. Pu, C. Zhao, R. Tong, Y. Qiao, J. Dai, and Y. Chen, Diffusion transformer policy: Scaling diffusion transformer for generalist vision-language-action learning, 2025. [Online]. Available: https://arxiv.org/abs/2410.15959 [21] C.-L. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang et al., Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation, arXiv preprint arXiv:2410.06158, 2024. [22] S. Huang, L. Chen, P. Zhou, S. Chen, Z. Jiang, Y. Hu, P. Gao, H. Li, M. Yao, and G. Ren, Enerverse: Envisioning embodied future space for robotics manipulation, 2025. [Online]. Available: https://arxiv.org/abs/2501.01895 [23] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, R3m: universal visual representation for robot manipulation, arXiv preprint arXiv:2203.12601, 2022. [24] E. Xing, A. Gupta, S. Powers, and V. Dean, Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts, in NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. [25] A. Xie, L. Lee, T. Xiao, and C. Finn, Decomposing the generalization gap in imitation learning for visual robotic manipulation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 31533160. [26] H. F. Hao-Shu Fang, Chenxi Wang and others., Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 2023. [27] Q. Yu, S. Huang, X. Yuan et al., Uniaff: unified representation of affordances for tool usage and articulation with vision-language models, arXiv preprint arXiv:2409.20551, 2024. [28] W. Pumacay, I. Singh, J. Duan, R. Krishna, J. Thomason, and D. Fox, The colosseum: benchmark for evaluating generalization for robotic manipulation, arXiv preprint arXiv:2402.08191, 2024. [29] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained bimanual manipulation with low-cost hardware, arXiv preprint arXiv:2304.13705, 2023. [30] S. Belkhale, Y. Cui, and D. Sadigh, Data quality in imitation learning, Advances in neural information processing systems, vol. 36, pp. 80 37580 395, 2023. [31] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks, IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 73277334, 2022. [32] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu et al., Octo: An open-source generalist robot policy, arXiv preprint arXiv:2405.12213, 2024. V. CONCLUSION In this paper, we demonstrate that strategic adversity during data acquisition is fundamental to efficient robotic learning. Our Adversarial Data Collection (ADC) framework proves that human-driven perturbations in visual and linguistic dimensions can compress hundreds of static variations into single demonstrations, directly resolving the data redundancy and domain gaps inherent to conventional pipelines. By integrating real-time human adaptation with physics-constrained perturbations, ADC-trained models inherently acquire robustness to environmental uncertainties and compositional language variationscapabilities unattainable through scale-driven data collection. This work establishes intentional adversity as new paradigm for robotic learning, where data quality, orchestrated through human-environment interplay, supersedes brute-force quantity. These insights compel re-examination of robotic data ecosystems: purposeful perturbation during acquisition, not just algorithmic innovation, may hold the key to generalizable embodied intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "[1] AgiBot, Agibot world, https://agibot-world.com, 2024. [2] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari et al., Droid: large-scale in-the-wild robot manipulation dataset, 2024. [3] F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao, Data scaling laws in imitation learning for robotic manipulation, arXiv preprint arXiv:2410.18647, 2024. [4] L. Wang, Y. Ling, Z. Yuan, M. Shridhar, C. Bao, Y. Qin, B. Wang, H. Xu, and X. Wang, Gensim: Generating robotic simulation tasks via large language models, arXiv preprint arXiv:2310.01361, 2023. [5] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, Rlbench: The robot learning benchmark & learning environment, IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 30193026, 2020. [6] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots, arXiv preprint arXiv:2402.10329, 2024. [7] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi et al., Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. [8] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter et al., pi 0: visionlanguage-action flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. [9] A. ONeill, A. Rehman, A. Gupta, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar et al., Open x-embodiment: Robotic learning datasets and rt-x models, arXiv preprint arXiv:2310.08864, 2023. [10] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine, Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning (CoRL), 2023. [11] J. Gao, A. Xie, T. Xiao, C. Finn, and D. Sadigh, Efficient data collection for robotic manipulation via compositional generalization, arXiv preprint arXiv:2403.05110, 2024. [12] J. Ho and S. Ermon, Generative adversarial imitation learning, Advances in neural information processing systems, vol. 29, 2016. [13] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto, imitation made easy, in Conference on Robot Learning. Visual PMLR, 2021, pp. 19922005. [14] M. Shridhar, L. Manuelli, and D. Fox, Cliport: What and where pathways for robotic manipulation, in Conference on robot learning. PMLR, 2022, pp. 894906."
        }
    ],
    "affiliations": [
        "Agibot",
        "Beihang University",
        "MMLab, CUHK",
        "Shanghai Jiao Tong University"
    ]
}