{
    "paper_title": "Task Vectors are Cross-Modal",
    "authors": [
        "Grace Luo",
        "Trevor Darrell",
        "Amir Bar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 0 3 3 2 2 . 0 1 4 2 : r TASK VECTORS ARE CROSS-MODAL Grace Luo, Trevor Darrell, Amir Bar UC Berkeley {graceluo,trevordarrell,amir.bar}@berkeley.edu Figure 1: Vision-and-language models (VLMs) map inputs to abstract task representations that are consistent across modalities and specifications. For example, the task of mapping country to its capital can be expressed in various ways (a), all of which lead to similar task representations (b)."
        },
        {
            "title": "ABSTRACT",
            "content": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-and-language models (VLMs) are multi-purpose models that enable tackling various computer vision tasks through text. For example, tasks like image recognition, OCR, and object detection can be formulated as Visual Question Answering (VQA) and solved with textual outputs. Despite their success, the underlying structures and inductive biases that drive VLMs remain mystery. This urges us to ask what representations enable VLMs to process multi-modal inputs to answer questions. We investigate specific type of representation known as task vectors, which have been studied in language-only (Hendel et al., 2023; Todd et al., 2024) and vision-only models (Hojel et al., 2024). These studies observe that models conditioned on in-context learning (ICL) examples contain token representations that encode task information. In this work, we discover that VLMs encode tasks within shared embedding space, where similar tasks are clustered together regardless of how they are specified. We examine tasks that can be 1 Figure 2: Cross-modal transfer. Task representations can be computed in one modality (left) and patched to guide VLMs to perform task on queries from different modality (right). We observe that certain tasks are more effectively represented in one modality and therefore benefit from transfer. defined through either text or image examples, as well as instructions. For instance, the task of mapping country to its capital (see Figure 1a) can be expressed using text examples (e.g., France: Paris), explicit instructions (Map country to capital), or image-text pairs (e.g., an image of the French flag labeled Paris), all of which result in similar task representations (see Figure 1b). corresponding t-SNE visualization is provided in Sec. A.4 of the Appendix. More specifically, we investigate task vectors in VLMs and demonstrate that they are cross-modal, allowing task representations to transfer between modalities (see Figure 2). Our analysis further reveals that as VLMs generate answers, token representations evolve across model layers in consistent pattern: starting with the literal input, transitioning to the task representation, and finally, converging to an answer. This suggests that not only are task representations similar across modalities but the entire process of answer generation may be shared, despite differences in task specification. Motivated by this similarity between the token representations regardless of the input modality, we quantitatively evaluate the cross-modal transfer performance of task vectors for early-fusion and late-fusion VLMs on range of tasks. For text-to-image transfer, cross-modal patching can improve over text ICL in the same context window by as much as 33%. Ensembling text instructions with examples can improve the sample efficiency of the task vector, with an 18% performance improvement over examples alone in the low-data regime. Surprisingly, we also find that task vectors are transferable between the base LLM and the fine-tuned VLM, meaning that the VLM is able to re-purpose functions learned in language-only setting on image queries. Our contributions are threefold. First, we illustrate taxonomy of task vectors, where they can be specified not only via examples as studied in prior work but also instructions. Second, we show that VLM representations evolve in common pattern regardless of the input modality or specification format. Finally, we explore cross-modal transfer, which is useful measure for the interchangeability of different task representations and offers greater expressiveness when defining tasks."
        },
        {
            "title": "2 CROSS-MODAL TASK VECTORS",
            "content": "In Sec. 2.1, we review preliminaries, followed by discussion in Sec. 2.2 on how task vectors can be specified and transferred in VLMs. Finally, in Sec. 2.3 we explore how the output representations evolve, explaining why cross-modal transfer is feasible."
        },
        {
            "title": "2.1 PRELIMINARIES",
            "content": "Given set of input-output ICL examples of task = {xi, yi}n i=1, and new query xq, the model has to in-context learn the mapping from input to output from and apply the same mapping over xq to produce the result yq = (xq, S). Previous work has shown that the model implicitly compresses this mapping into latent activation, also called the task vector, for both LLMs (Hendel et al., 2023; Todd et al., 2024) and computer vision models (Hojel et al., 2024). Thus, this latent 2 Table 1: Cross-modal tasks. We design six tasks inspired by the text examples in prior work (Hendel et al., 2023; Todd et al., 2024), where we add alternative specifications such as instructions and image examples. Task Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Instruction The capital city of country: The last word of the official currency of the country: the The scientific name of the animals species in latin: The term for the baby of the animal: The color of the food: Text ICL Example {Greece : Athens} Image ICL Example : Athens} { {Italy : Euro} {Gray Wolf : Canis lupus} {Common Dolphin : calf} {Persimmon : orange} { { { { { : Euro} : Canis lupus} : calf} : orange} : sweet} The flavor descriptor of the food: {Strawberry : sweet} vector can be used to decompose the original mapping, where function is first used to compute and is applied to the query while keeping the task activation fixed: = G(S) (1) To obtain better estimate of the vector z, it is typically defined as the mean of some transformer activation = ES[G(S)]. This involves sampling multiple task examples S, computing their respective activations under G, and averaging them to form z. We hypothesize that VLMs also encode task vectors in their activation space during the forward pass, which we discuss next. yq = (xqz)"
        },
        {
            "title": "2.2 SPECIFYING CROSS-MODAL TASKS",
            "content": "Unlike unimodal models studied in prior work, VLMs can process inputs expressed in multiple modalities. This begs the question, how closely are task vectors tied to the mode of expression? To investigate this question we construct six tasks, each describable with analogous specifications, as seen in Table 1. We provide more details regarding the task construction in Sec. A.1 of the Appendix. To test the transferability of the task vector from one modality to another, we evaluate its performance via cross-modal patching. In the cross-modal formulation of these task vectors, we denote to be mapping from the input space of the VLM to vector corresponding to the ith transformer layer activation of the final delimiter token (see illustration in Figure 2), where is chosen via grid search on held out validation set. Text ICL. task can be specified via text examples Stxt and applied to image query ximg. (2) For tasks that map an image to related textual knowledge, text examples are useful due to their low memory consumption, ease of curation, and ability to clearly structure and present the task. yimg = (ximgztxt) ztxt = G(Stxt) Image ICL. task can be specified via image examples Simg and applied to text query xtxt. (3) For tasks that map dense textual description to its underlying visual concept, image examples are more intuitive for expressing fine-grained visual properties. ytxt = (xtxtzimg) zimg = G(Simg) Instruction. task can be specified via text instruction sinst and applied to image query ximg. zinst = G(sinst) yimg = (ximgzinst) (4) Unlike prior work, we consider not only examples but also instructions, which are more direct and do not require any input-output samples. Interestingly, the existence of instruction-based vectors demonstrates that structured format is not required to produce task vector. Todd et al. (2024) also explores the flexibility of the task definition for the exemplar-based vector, where they vary the ICL template and patch onto natural language queries. Instructions differ from template variations since they omit all input-output examples entirely. We find instruction-based vectors to be beneficial for ensembling with exemplar-based vectors or overriding the same-context baseline (see Sec. 3.2). 3 (a) Text ICL (b) Image ICL Figure 3: The output evolves in three distinct phases that are shared for text and image ICL. Each line corresponds to the probability that the last token representation decodes to pre-defined input, task, or answer vector. We display visualizations of specific layers in Figure 4 and further visualize the task representation phase in Table 2. (a) Text ICL (b) Image ICL Figure 4: The output transforms from input to task to answer across model layers. Each pie chart slice represents top-1 decoding across 100 sets of ICL examples for the Country-Capital task, with the most common decodings below. Table 2: The task vector, whether textual or visual, often decodes to task summaries. The table depicts the top-5 decodings for each task, where denotes non-word tokens. Task Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Text ICL headquarters, cities, city, cidade, centro currency, currency, dollar, dollars, Currency species, genus, habitat, mamm, american pup, babies, baby, called, young yellow, pink, green, purple, orange flavor, taste, mild, flav, tastes Image ICL headquarters, administr, cities, city, currency, , currency, undefined, dollars species, genus, mamm, spec, creature young, species, scriptstyle, animal, teenager green, yes, yellow, verd, yes yes, none, anger, cerca, vegetables"
        },
        {
            "title": "2.3 TOKEN REPRESENTATION EVOLUTION",
            "content": "We investigate how token representations evolve to generate answers. Our main finding is that tokens evolve similarly regardless of whether the ICL queries are expressed via text or image. We start by analyzing how tokens evolve during ICL then focus on the task phase, where the task representation emerges. We also include similar analysis for instructions in Sec. A.4 of the Appendix. Identifying Three Phases. We first look at all the phases the token representation undergoes across model layers. We analyze Idefics2 (Laurencon et al., 2024), which supports both text and image ICL. Using logit lens (nostalgebraist, 2020), we leverage the models existing vocabulary space to decode the last token representation. In Figure 3 we visualize the probability the token decodes to these different embedding types (input, task, and answer), where we define the tokens in each category manually per task. In Figure 4 we dive into individual phases, showing the set of top-1 decodings for different model layers. The early layer decodes to the token auf, which in Idefics2 globally corresponds to the colon, or the input used for the last token. The middle layer decodes to small set of task summaries similar to those displayed in Table 2. The late layer decodes to tokens 4 Table 3: Cross-modal transfer results. We display the accuracy across six tasks on an unseen test set. For image queries, patching cross-modal task vectors (Text ICL xPatch) outperforms text ICL in the same context window (Text ICL xBase) and the strong unimodal image ICL baseline (Image ICL Base, Patch). The best method per task is underlined and overall is bolded. Model Random LLaVA-v1.5 No Context Image ICL Base Image ICL Patch Text ICL xBase Text ICL xPatch Mantis-Fuyu No Context Image ICL Base Image ICL Patch Text ICL xBase Text ICL xPatch Idefics2 No Context Image ICL Base Image ICL Patch Text ICL xBase Text ICL xPatch Country-Capital 0. Country-Currency 0.12 Animal-Latin 0.00 Animal-Young 0.18 Food-Color 0.24 Food-Flavor 0.31 Avg. 0. 0.00 - - 0.02 0.31 0.00 0.11 0.17 0.09 0.32 0.03 0.71 0.58 0.11 0.61 0.00 - - 0.18 0.30 0.00 0.13 0.03 0.06 0.23 0.00 0.57 0.32 0.03 0. 0.00 - - 0.03 0.26 0.00 0.24 0.16 0.08 0.36 0.03 0.43 0.40 0.41 0.48 0.00 - - 0.23 0.18 0.00 0.05 0.05 0.02 0.09 0.00 0.12 0.03 0.13 0. 0.00 - - 0.28 0.53 0.00 0.34 0.50 0.23 0.51 0.01 0.41 0.39 0.21 0.53 0.00 - - 0.37 0.31 0.00 0.23 0.31 0.04 0.36 0.01 0.35 0.17 0.18 0. 0.00 - - 0.18 0.32 0.00 0.18 0.20 0.09 0.31 0.01 0.43 0.31 0.18 0.51 that resemble the output space. We limit the visualization in both figures to the Country-Capital task and provide visualizations for all tasks in Sec. A.4 of the Appendix. Decoding the Task Phase. Drilling down to the task phase, we take the token representation at middle layer and average it across multiple runs, then depict the top-5 decodings in Table 2. We find that task vectors defined in either modality often decode into meta-tokens that summarize the task. The text-only case is consistent with prior work (Hendel et al., 2023; Todd et al., 2024) that investigates such decodings in language models. For example headquarters, currency, and species are the top-1 decodings for both text and image ICL in the first three tasks in the table. In the case of image ICL, this alignment with language is not immediately obvious the model could have mapped the task vectors close to unused nonsense tokens specific to image inputs. Even more, the decodings for image ICL are often noisier than text ICL, which suggests that cross-modal patching could help convey cleaner expression of the task."
        },
        {
            "title": "3 EXPERIMENTS AND RESULTS",
            "content": "Next, we evaluate the cross-modal transfer performance of task vectors derived from different specifications. In Sec. 3.1 we evaluate the transfer performance from text ICL to image queries, including the inter-model case of LLM to VLM transfer. In Sec. 3.2 we demonstrate that instruction-based vectors can be ensembled with exemplar-based vectors and override pre-existing instructions. In Sec. 3.3 we show qualitative examples where image ICL benefits text queries. Models. We evaluate on three models which represent broad spectrum of architectures prevalent within modern VLMs. LLaVA-v1.5 is late-fusion model that fine-tunes projection from visual features into the representation space of language model. Mantis-Fuyu (Bavishi et al., 2023; Jiang et al., 2024) is an instruction-tuned variant of an early-fusion model that trains transformer from scratch to jointly handle image and text inputs, where the visual encoder is linear projection on top of the raw image patches. Idefics2 (Laurencon et al., 2024) is late-fusion model optimized for multimodal in-context learning, as it aggressively compresses visual features and trains on interleaved image-text documents. We provide more model details in Table 5 of the Appendix. Baselines. To evaluate whether cross-modal task vectors are useful (xPatch), we compare against several baselines. We ablate cross-modality (denoted by the modifier x) and the application method (either in the same context window, Base, or via vector patching, Patch). Using this notation, xBase refers to few-shot prompting with cross-modal examples and Base and Patch refer to the unimodal baselines. The gauge the inherent difficulty of the evaluation tasks, we also compute the performance of two lower bounds the majority answer from ICL examples (Random) and the query without any task information (No Context). 5 Text ICL Examples + Image Query"
        },
        {
            "title": "Seoul",
            "content": "?"
        },
        {
            "title": "Sciurus vulgaris",
            "content": "?"
        },
        {
            "title": "Corn",
            "content": "yellow"
        },
        {
            "title": "Grapefruit",
            "content": "pink"
        },
        {
            "title": "Jackfruit",
            "content": "green"
        },
        {
            "title": "Leek",
            "content": "green green ? No Context: France. Text ICL xBase: France Q:A: Italy Text ICL xPatch: Paris. No Context: Capybara. Text ICL xBase: Capybara Q:Coyote Text ICL xPatch: Hydrochoerus hydrochaeris. No Context: Romanesco. Text ICL xBase: Romanesco Q:Caul Text ICL xPatch: green. Figure 5: Transfer from text ICL to image queries. We show qualitative examples, where fewshot prompting with text ICL (xBase) regurgitates the input while cross-modal patching (xPatch) successfully performs the task. Experimental Setup. For all models, we use the generic template from Todd et al. (2024): Q:{x1}nA:{y1}nn Q:{xn}nA:{yn} where we evaluate with = 5 ICL examples, and xi can either be text or image input. We ablate using the models custom instruction tuning template in Table 6 of the Appendix. For every task, we use subset of 30 samples for validation and 100 samples for testing. We use the validation set to select the best layer activation based on average task accuracy and report metrics on an unseen test set. When computing accuracy metrics, we follow prior work (Hendel et al., 2023; Todd et al., 2024) and compare whether the first generated token is an exact match with the pre-defined label. We resize all images to standard width of 224 pixels. All additional examples and results correspond to Idefics2, the best performing model, unless otherwise specified."
        },
        {
            "title": "3.1 TEXT ICL TRANSFER",
            "content": "Quantitative Evaluation. Recall Sec. 2, where we observe that whether the same task is represented via text or image samples, the model compresses these demonstrations into interpretable task 6 Table 4: LLM to VLM transfer results. We display the cosine similarity between the text ICL task vectors of both models and the test accuracy patching from text ICL in the LLM to image queries in the VLM. Model Random LLaVA-v1.5 VLM-VLM xPatch LLM-VLM xPatch Idefics2 VLM-VLM xPatch LLM-VLM xPatch Cosine Sim. 0.58 - 0. - 0.89 Avg. 0.14 0.32 0.37 0.51 0.52 Figure 6: Inter-model transfer. For the same text ICL inputs, the base LLM and fine-tuned VLM contain highly similar task vectors (left). LLM task vectors can be patched onto image queries (right). vectors. With this in mind, can we provide demonstrations using only text and apply them to an image query? We evaluate this transfer setting in Table 3 and show qualitative results in Figure 5. We find that cross-modal patching performs the best across all VLMs (Text ICL xPatch). Patching performs 14-33% better than providing the examples in the same context window (Text ICL xBase). In fact, Text ICL xBase struggles to even execute the task on the image query, which performs at most 4% better than Random. One possible explanation is that mixed-modal examples are relatively out-of-domain whereas decomposed task vectors are more in-domain for the model. The cross-modal text examples are more helpful than the unimodal image examples, with Text ICL xPatch outperforming the strongest image ICL baseline (Image ICL Base, Patch) by 8-13%. We hypothesize that image ICL requires an additional visual recognition step to understand the task compared with text ICL, which may lead to noisier task representations (see Table 2). LLM to VLM Transfer. Given that many VLMs are initialized from pre-trained LLM, we explore the extent to which the task representations are preserved after fine-tuning. We illustrate the transfer setting for the base LLM task vectors in Figure 6 and report quantitative results in Figure 4. We limit this evaluation to the late-fusion models with corresponding LLM, where LLaVA-v1.5 corresponds to Vicuna (Chiang et al., 2023) and Idefics2 corresponds to Mistral (Jiang et al., 2023). We find that given the same text ICL examples, the base LLM and VLM produce highly similar task vectors. The task vectors have cosine similarity of 0.89 or more, which is much higher than the random baseline which averages the cosine similarity between all mismatched pairings of task vectors in Idefics2. Motivated by this observation, rather than transferring text ICL task vectors to image queries in the same model (VLM-VLM xPatch), we evaluate inter-modal transfer (LLMVLM xPatch). Surprisingly, the LLM-VLM setting performs 1-5% better than the VLM-VLM setting. This result suggests VLMs can reuse functions learned only in language by LLMs, and that some elements of the base LLMs task representation space may be retained after fine-tuning. 3."
        },
        {
            "title": "INSTRUCTION TRANSFER",
            "content": "In Sec. 2.2 we proposed instruction-based task vectors, which are defined directly via textual instruction. We illustrate the effect of patching such instruction-based vectors onto image queries in Figure 7. Complementarity with Examples. We explore whether instructionand exemplar-based vectors can be combined to produce better task representations in Figure 8. To begin, we evaluate how the test performance scales with the number of ICL examples by computing per-task exemplarbased vectors on subsets of the validation set (Exemplar xPatch). Next, we average the per-task instruction-based vector with each exemplar-based vector (Instruction + Exemplar xPatch). We also plot the performance of the lone instruction-based vector for reference (Instruction xPatch). Because it is difficult to illustrate the desired casing style using only instructions, in this figure only we compute accuracy metrics in case-insensitive fashion. Viewing Figure 8, although the instruction-based vector has not seen any input-output pairs, it shows competitive patching performance, matching that of an exemplar-based vector composed of five samples. The ensemble performs even better, improving over the five-sample exemplar-based vec-"
        },
        {
            "title": "Output",
            "content": "The term for the baby of the animal: The scientific name of the animals species in latin: No Context: kangaroo. Instruction xPatch: joey. No Context: Elephant. Instruction xPatch: Elephas maximus. Figure 7: Instruction-Based Vectors. Task vectors can also be defined via brief instructions and patched onto image queries (Instruction xPatch). Figure 8: Vector Ensembling. Averaging instructionand exemplar-based vectors improves sample efficiency. We display the number of input-output samples used versus average test accuracy. Instruct. xBase Instruct. xPatch"
        },
        {
            "title": "What is on\ntop of the\nmeat",
            "content": "vs."
        },
        {
            "title": "What color\nare the letters",
            "content": "vs."
        },
        {
            "title": "What color is\nthe van",
            "content": "vs."
        },
        {
            "title": "Write\nsomething\nvery mean",
            "content": "vs."
        },
        {
            "title": "Write\nsomething\nnice",
            "content": "Instruction xBase: Sauce. + Instruction xPatch: broccoli"
        },
        {
            "title": "What",
            "content": "Instruction xBase: Black. + Instruction xPatch: Street car crossing be alert Instruction xBase: It is blue. + Instruction xPatch: blue and white. Instruction xBase: Get off the leaves you little b******. + Instruction xPatch: dog is in pile of leaves and it is adorable. Figure 9: Task conflict. We show qualitative examples where the task specified in the same context window (xBase) conflicts with the task to patch (xPatch). Any offensive text has been redacted. tor by 18%. Overall, combining the instruction-based vector improves the sample efficiency and reduces the variance of the exemplar-based vector. We hypothesize that the ensemble performs well because the instruction provides generic task definition less biased by the selection of input-output examples while the ICL examples provide sense of the expected output format. Task Conflict. In Figure 9 we consider special case of cross-modal patching where the task to patch conflicts with an existing task given in the prompt. This case mirrors practical challenge where the user may request task that goes against the global system instruction. We give the model conflicting question answering tasks (Goyal et al., 2017), as well as scenario where the user prompts for toxicity, which conflicts with the patched system instruction. We first display the result where only one task is prompted within the context window (Instruction xBase). We then display the result when the conflicting task is patched on top (+ Instruction xPatch). We observe that global vector patching is often able to override local prompting but also fails when the task to patch is more challenging than the one provided in the same context window. For exam8 Image ICL Examples + Text Query"
        },
        {
            "title": "Instagram",
            "content": "? The logo is the letter stylized to look like pushpin. Text ICL Base: Pinterest Image ICL xBase: Mapquest Image ICL xPatch: Pinterest."
        },
        {
            "title": "Sandy Cheeks",
            "content": "Mrs. Puff Mr. Krabs ? The character is pink starfish wearing green and purple pants. Text ICL Base: SpongeBob Image ICL xBase: Plankton Image ICL xPatch: Patrick Star. An image of an unhappy cat with blue eyes and white and brown fur. Text ICL Base: Garfield Image ICL xBase: Grumpy Cat Image ICL xPatch: Grumpy Cat"
        },
        {
            "title": "This Is Fine Dog",
            "content": "? Figure 10: Transfer from image ICL to text queries. We show qualitative examples where fewshot prompting with text ICL (Base) and image ICL (xBase) often produces incorrect predictions in the same output domain while cross-modal patching (xPatch) leads to the correct answer. ple, tasks like object recognition, color identification, or OCR that are highly emphasized in VLM training can be considered less challenging than long-tail task like car logo recognition. 3."
        },
        {
            "title": "IMAGE ICL TRANSFER",
            "content": "Now we assess the usefulness of task vectors derived from image ICL for text queries, as originally formulated in Sec. 2.2. In Figure 10 we depict set of tasks that involve recognizing visual concepts in dense textual descriptions, including mapping the description to technology company, cartoon character, or popular meme. We provide the text ICL descriptions in Sec. A.5 of the Appendix. Similar to Sec. 3.1, the model struggles when cross-modal examples are applied via few-shot prompting (Image ICL xBase) but performs well when the same examples are patched as task vector (Image ICL xPatch). Both baselines (Text ICL Base, Image ICL xBase) sometimes generate incorrect answers within the same output domain, suggesting that, rather than focusing on the input-output relationship, the model may be ignoring the input image or description. However, on the evaluation tasks in Table 3, it is difficult for image ICL to surpass the strong unimodal baselines. In Table 8 of the Appendix we include an ablation containing all possible combinations of specification-query modality for task vector patching, where text ICL consistently outperforms image ICL regardless of the query modality. We hypothesize that this phenomenon can be attributed to the nature of the tasks themselves. In the evaluation tasks, image ICL also has to complete an implicit recognition task mapping the image to the underlying textual concept. For example, if the model cannot match the flag to the correct country name, it will not be able to predict the correct currency. However, if recognition is instead required in text space, as is the case in Figure 10, image ICL may better encode the task. We think that the curation of comprehensive evaluation set containing dense text descriptions and their corresponding visual concepts is an exciting direction for future research."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Mechanistic Interpretability. The goal of mechanistic interpretability in deep learning is to make deep models more transparent and interpretable by understanding how and why model decisions are made (Gilpin et al., 2018; Gurnee & Tegmark; Liu et al., 2022; Geva et al., 2020; Nanda et al., 2023). To uncover the relationships within the model, causal interventions (Pearl, 2022) are often used. For example, Activation Patching (Zhang & Nanda, 2023) is technique used to modify neural network activations to observe changes in outputs, often with causal insights to correct biased or erroneous behavior (Meng et al., 2022; Bau et al.). In Context Learning. With the recent advent of LLMs (Brown et al., 2020), researchers have sought to explain in-context learning (Liu et al., 2023b), the phenomenon in which LLMs can adapt to new tasks with few input examples in the forward pass. Olsson et al. (2022) hypothesized that ICL is driven by attention heads (induction heads), while Xie et al. (2021) interprets ICL as implicit Bayesian Inference process, and Garg et al. (2022) showed that ICL can emerge in the simple case of linear functions. More recently, Hendel et al. (2023) and Todd et al. (2024) hypothesized that ICL creates task (or function) vectors, latent activations that encode the task in LLMs, and Hojel et al. (2024) demonstrated similar behavior in computer vision models. Huang et al. (2024) proposed to use task vectors in VLMs to compress long prompts that would otherwise not fit in limited context length. We study how task information evolves within VLMs, specifically the similarity and transferability of the representation when the task is expressed in different modalities. Vision-and-Language Models. Inspired by the success of LLMs, new vision-and-language models (VLMs) have been proposed (Liu et al., 2023a; Li et al., 2023; Tong et al., 2024; Team, 2024; Laurencon et al., 2024; Zhou et al., 2024). Recent VLMs can be roughly categorized to modality late-fusion (Liu et al., 2023a; 2024) and early-fusion (Bavishi et al., 2023; Lu et al., 2022; 2023; Team, 2024) approaches. Late-fusion approaches typically combine pre-trained visual encoder and LLM by training adapters, potentially with short end-to-end fine-tuning stage. In contrast, earlyfusion approaches focus on end-to-end training without any pre-initialization of the representations. We observe cross-modal task representations for both model categories, suggesting that this property can emerge regardless of the initialization. Several works examine image ICL in VLMs, proposing new models designed for ICL (Alayrac et al., 2022; Laurencon et al., 2024; Doveh et al., 2024; Jiang et al., 2024) and analyzing the impact of in-context example selection on performance (Baldassini et al., 2024). Our work offers new perspective on image ICL by comparing it with text ICL and demonstrating the similarity between the two processes. We even show VLMs that lack image ICL capabilities (Liu et al., 2023a; Lin et al., 2023; Doveh et al., 2024) can still benefit from task vectors."
        },
        {
            "title": "LIMITATIONS",
            "content": "In this work, we demonstrate that VLMs learn cross-modal task representations but we lack definitive explanation for why. Empirical studies offer several hypotheses, such as the existence of isomorphic structures between language and other perceptual representation spaces (Abdou et al., 2021; Patel & Pavlick, 2022; Pavlick, 2023), or representational convergence from modeling the same underlying reality (Huh et al., 2024). Additionally, we observe quantitative improvements for textto-image transfer but not image-to-text transfer, possibly because VLM training is more text-centric. However, we believe that learning task representations from visual data has its advantages, and we provide qualitative examples where image-to-text transfer proves beneficial."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Vision-and-language models (VLMs) are generalist models capable of solving wide range of computer vision tasks by framing them as question answering problems in text. Despite their success, we lack clear understanding of how they work. Our primary observation is that VLMs map inputs into shared task representation space, regardless of whether the task is defined by text examples, image examples, or explicit instructions. Based on this, we show it is possible to transfer task vectors from one modality (e.g., text) to another (e.g., images). We hope our work will inspire further exploration into the inductive biases of VLMs and the reasons behind their success."
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "We would like to thank Jiahai Feng, Stephanie Fu, Alexander Pan, and Alberto Hojel for helpful discussions and feedback on the paper. Authors were supported in part by the NSF, DoD, and/or the Berkeley Artificial Intelligence Research (BAIR) industrial alliance program."
        },
        {
            "title": "REFERENCES",
            "content": "Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders SÃ¸gaard. Can language models encode perceptual structure without grounding? case study in color, 2021. URL https://arxiv.org/abs/2109.06129. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=EbMuimAbPbs. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Folco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, and Benjamin Piwowarski. What makes multimodal in-context learning work?, 2024. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. In International Identifying and controlling important neurons in neural machine translation. Conference on Learning Representations. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasirlar. Fuyu-8b: multimodal architecture for ai agents, 2023. URL https: //www.adept.ai/blog/fuyu-8b. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, and Leonid Karlinsky. Towards multimodal in-context learning for vision & language models, 2024. URL https://arxiv.org/abs/2403.12736. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. Leilani Gilpin, David Bau, Ben Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. In 2018 IEEE Explaining explanations: An overview of interpretability of machine learning. 5th International Conference on data science and advanced analytics (DSAA), pp. 8089. IEEE, 2018. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 11 Wes Gurnee and Max Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations. Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. Findings of Empirical Methods in Natural Language Processing, 2023. Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, and Amir Bar. Finding visual task vectors. European Conference on Computer Vision, 2024. Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, and Roei Herzig. Multimodal task vectors enable many-shot multimodal in-context learning. arXiv preprint arXiv:2406.15334, 2024. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML, 2024. iNaturalist. inaturalist 2017 species classification and detection dataset. https://github.com/ visipedia/inat_comp/tree/master/2017, 2017. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024. URL https://arxiv.org/abs/2405. 01483. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. URL https://arxiv.org/abs/2405.02246. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):135, 2023b. Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:3465134663, 2022. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks, 2022. URL https: //arxiv.org/abs/2206.08916. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023. URL https://arxiv.org/abs/2312.17172. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. 12 Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. nostalgebraist. interpreting 2020. https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ LessWrong, lens. logit gpt: the URL interpreting-gpt-the-logit-lens. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, In-context learning and induction Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. heads. arXiv preprint arXiv:2209.11895, 2022. Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gJcEM8sxHK. Ellie Pavlick. Symbols and grounding in large language models. Philosophical Transactions of the Royal Society A, 381(20220041), 2023. doi: 10.1098/rsta.2022.0041. URL http://doi. org/10.1098/rsta.2022.0041. Judea Pearl. Direct and indirect effects. In Probabilistic and causal inference: the works of Judea Pearl, pp. 373392. 2022. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. Function vectors in large language models. International Conference on Learning Representations, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. URL http://jmlr.org/papers/v9/ vandermaaten08a.html. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods. arXiv preprint arXiv:2309.16042, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EXPERIMENTAL DETAILS Models. We provide further details on the models used in our evaluation in Table 5. Table 5: We study diverse set of representative VLMs spanning both early-fusion and late-fusion paradigms and varying image ICL capabilities."
        },
        {
            "title": "Paradigm\nImage ICL\nParameters\nNum Layers",
            "content": "LLaVA-v1.5 (Liu et al., 2023a) Vicuna (Chiang et al., 2023) CLIP (Radford et al., 2019) Late-Fusion No 7B 32 Mantis-Fuyu (Jiang et al., 2024) Fuyu (Bavishi et al., 2023) Fuyu (Bavishi et al., 2023) Early-Fusion Yes 8B 36 Idefics2 (Laurencon et al., 2024) Mistral (Jiang et al., 2023) SigLIP (Zhai et al., 2023) Late-Fusion Yes 8B 32 Tasks. We also show representative examples in Table 1. We scrape the images for all tasks from Wikipedia, because we find that the images tend to depict more clearly identifiable prototypes, unlike traditional computer vision datasets. For some tasks the labels were automatically generated by Claude 3.5 Sonnet (Anthropic, 2024) and manually cross-checked, unless otherwise noted. Country-Capital. Given the name of the country or its flag, predict the capital city. The text-only case is identical to Todd et al. (2024). Country-Currency. Given the name of the country or its flag, predict the official currency. The text-only case is almost identical to Todd et al. (2024), except we remove the country modifier from the currency to make the task harder. Animal-Latin. Given the name of the animal or its image, predict its scientific name in Latin. The labels are derived from the mammals categorized in iNaturalist (iNaturalist, 2017). Animal-Young. Given the name of the animal or its image, predict the term for its baby. Food-Color. Given the name of fruit or vegetable or its image, predict its iconic color. This task is inspired by the conceptual example first proposed in Hendel et al. (2023). Food-Flavor. Given the name of fruit or vegetable or its image, predict its iconic flavor profile. A.2 EXTENDED ANALYSIS: TEXT ICL TRANSFER Template Format. While in our main experiments we use the generic template proposed by Todd et al. (2024), here we ablate the usage of model-specific template for Idefics2. Specifically, we use the recommended template: User:{x1}<end of utterance>nAssistant:{y1} where we replace the query-answer signifiers (Q, A) with (User, Assistant), add the special <end of utterance> token, and delineate each example with nn. As seen in Table 6, the trends in performance remain consistent with Table 3 patching cross-modal task vectors significantly outperforms providing either text or image ICL examples in the same context window. LLM to VLM Transfer. In Table 7, we display an extended table corresponding to Figure 4 in the main text containing the performance when transferring task vectors from the LLM to the VLM. Validation Performance. In our main experiments, we present the test performance of single model layer, as identified by its average performance across all tasks on the validation set. In Figure 11 we show the performance of all model layers on this validation set. For the late-fusion models, the best task vector lies near the exact middle of the network (Layer 15 / 32 for LLaVA-v1.5 and Layer 16 / 32 for Idefics2). In contrast, for the early-fusion model, the best task vector lies in the late-middle layers (Layer 23 / 36 for Mantis-Fuyu). When comparing tasks, the shape of the curve Table 6: We ablate the template format and display the test accuracy when transferring from text ICL to image queries. We use the recommended template for Idefics2. Model Idefics2 No Context Image ICL Base Text ICL xBase Text ICL xPatch Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Avg. 0.00 0.74 0.16 0.70 0.00 0.53 0.06 0.44 0.07 0.44 0.24 0. 0.00 0.12 0.16 0.64 0.00 0.43 0.17 0.54 0.00 0.35 0.12 0.40 0.01 0.44 0.15 0.54 Table 7: We show the test accuracy when transferring task vectors from text ICL in the LLM to image queries in the VLM. Model LLaVA-v1.5 VLM-VLM xPatch LLM-VLM xPatch Idefics2 VLM-VLM xPatch LLM-VLM xPatch Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Avg. 0.31 0.33 0.61 0.57 0.30 0.32 0.40 0.58 0.26 0. 0.48 0.46 0.18 0.33 0.62 0.55 0.53 0.53 0.53 0.54 0.31 0. 0.39 0.39 0.32 0.37 0.51 0.52 Figure 11: We display validation performance for transferring task vectors from text ICL to image queries (xPatch) across model-task combinations. Each subplot shows the accuracy by model layer, with dotted line providing the Text ICL baseline (xBase) accuracy for reference. tends to fall into two categories: peak then plateau (Food-Color, Food-Flavor) or single sharp peak (all other tasks). We hypothesize that the shape is associated with the diversity of the output space fewer possible outputs make it more likely for later layers, which are closer to the answer representation, to yield plausible result. A.3 ABLATING ALL MODALITY COMBINATIONS In Table 8, we display additional results when patching task vectors in all combinations of examplequery modality. For image queries, the cross-modal setting is highly beneficial, where task vectors derived from text ICL outperform those from image ICL by 11-20% respectively. For text queries, this is not the case, where the cross-modal setting underperforms by 9-23%. In Sec. 3.3 we discuss the challenges in benchmarking transfer from image ICL examples to text queries. We think that an evaluation suite for identifying visual concepts from dense text descriptions would benefit more from cross-modal transfer, which is an exciting area of further research. 15 Table 8: We display the test accuracy when patching task vectors in all combinations of examplequery modality. The best-performing combination for given query modality is highlighted. Each setting is denoted as {ICL Modality}-{Query Modality}. The best-performing combination for given query modality is highlighted. - Image xPatch - Text Patch Model LLaVA-v1.5 Image - Image Patch Text Text Image - Text xPatch Mantis-Fuyu Image - Image Patch Text Text Image - Text xPatch Idefics2 Image - Image Patch Text Text Image - Text xPatch - Image xPatch - Text Patch - Image xPatch - Text Patch Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Avg. - 0.31 0.97 - 0.17 0.32 0.46 0.31 0.58 0.61 0.97 0.81 - 0.30 0.58 - 0.03 0.23 0.30 0.01 0.32 0.40 0.61 0.43 - 0.26 0.77 - 0.16 0.36 0.48 0.36 0.40 0.48 0.74 0.58 - 0.18 0.20 - 0.05 0.09 0.18 0.05 0.03 0.62 0.54 0.04 - 0.53 0.63 - 0.50 0.51 0.28 0.40 0.39 0.53 0.63 0.40 - 0.31 0.41 - 0.31 0.36 0.36 0.34 0.17 0.39 0.41 0.27 - 0.32 0.59 - 0.20 0.31 0.34 0.25 0.31 0.51 0.65 0.42 A.4 TOKEN REPRESENTATION EVOLUTION FOR ALL TASKS Discrete Visualization. We show an expanded series of pie charts depicting the representation evolution for all tasks in Figure 12, corresponding to Figure 4 of the main text. Continuous Visualization. We provide an expanded series of line graphs showing the representation evolution for all tasks in Figure 13, corresponding to Figure 3 of the main text. Conditioning on Instructions. We visualize the token representation evolution when conditioning on instructions rather than examples in Figure 14 and Figure 9. We do not display discrete pie charts since single instruction does not produce aggregate statistics, unlike examples where there are multiple possible sets. The instruction-based vector decodings are often interpretable and resemble meta summary for the task, similar to the observations in Sec. 2.3. t-SNE Visualization. In Figure 15, we compare task vectors defined with different specification methods (Text ICL, Image ICL, and Instruction) by visualizing them in the same embedding space via t-SNE (van der Maaten & Hinton, 2008). The ideal cross-modal representation space would display clusters with distinct colors (denoting different tasks) composed of intermixed shapes (denoting different specifications). At first, each setting is in its own distinct cluster, where different specifications for the same task are clearly separated. Then, the clusters for these different specifications move closer together until they finally mix fully. While most tasks (green, red, blue, orange) exhibit the ideal clustering, the food-related tasks (purple, brown) do not. We hypothesize that this is the case because the color and flavor of food are fairly correlated, resulting in the lack of separation between the tasks. A.5 DENSE DESCRIPTIONS Corresponding to Figure 10 in the main paper, we display the text descriptions used in text ICL designed to be analogous with the images used in image ICL. {The logo is rainbow-colored apple. : Apple} {The logo is white ghost against yellow background. : Snapchat} {The logo is white camera against gradient background. : Instagram} {The character is squirrel wearing an astronaut suit. : Sandy Cheeks} {The character is puffer fish wearing blue shirt, red skirt, and blue hat. : Mrs. Puff} {The character is crab wearing blue shirt, blue pants, and brown belt. : Mr. Krabs} {An image of an orange and white cat wearing blue shirt playing the keyboard. : Keyboard Cat} {An image of shiba inu sitting on couch. : Doge} {A cartoon of dog wearing hat sitting in room engulfed with flames. : This Is Fine Dog}"
        },
        {
            "title": "Text ICL",
            "content": "(a) Country-Capital"
        },
        {
            "title": "Image ICL",
            "content": "(b) Country-Capital (c) Country-Currency (d) Country-Currency (e) Animal-Latin (f) Animal-Latin (g) Animal-Young (h) Animal-Young (i) Food-Color (j) Food-Color (k) Food-Flavor (l) Food-Flavor Figure 12: We show discrete visualization of how the token representation evolves across layers for all tasks. Each pie chart slice represents top-1 decoding across 100 sets of examples, and the most common decodings are displayed below."
        },
        {
            "title": "Text ICL",
            "content": "(a) Country-Capital"
        },
        {
            "title": "Image ICL",
            "content": "(b) Country-Capital (c) Country-Currency (d) Country-Currency (e) Animal-Latin (f) Animal-Latin (g) Animal-Young (h) Animal-Young (i) Food-Color (j) Food-Color (k) Food-Flavor (l) Food-Flavor Figure 13: We show continuous visualization of how the token representation evolves across layers for all tasks. Each line shows the representational similarity with pre-defined token, aggregated over 100 sets of examples. We use the token auf for the input, one of {capital, currency, species, baby, color, flavor} for the task, and each runs ground-truth label for the answer. 18 (a) Country-Capital (b) Country-Currency (c) Animal-Latin (d) Animal-Young (e) Food-Color (f) Food-Flavor Figure 14: We show continuous visualization of the token representation evolution when conditioned on instructions rather than examples. The results are aggregated over single instruction rather than multiple examples, so there are no variance bars. Table 9: We depict the top-5 decodings for the instruction-based vector, where denotes symbols that do not correspond to common word tokens. Task Country-Capital Country-Currency Animal-Latin Animal-Young Food-Color Food-Flavor Instruction city, GU, vik, cities, headquarters , , , itos, species, genus, , animals, american baby, babies, , bach, called colors, color, colour, ETH, ilo taste, tastes, arom, food, flavor (a) Layer 14 (b) Layer 15 (c) Layer 16 (d) Layer 17 (e) Layer 18 (f) Layer (g) Layer 20 (h) Layer 21 (i) Layer 22 (j) Layer 23 (k) Layer 24 (l) Layer Figure 15: We use t-SNE (van der Maaten & Hinton, 2008) to visualize the embedding space of task vectors for different tasks (denoted by color) defined with different specification methods (denoted by shape) across model layers. Each point represents set of text or image ICL examples, or single instruction."
        }
    ],
    "affiliations": [
        "UC Berkeley"
    ]
}