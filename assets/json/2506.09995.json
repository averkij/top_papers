{
    "paper_title": "PlayerOne: Egocentric World Simulator",
    "authors": [
        "Yuanpeng Tu",
        "Hao Luo",
        "Xi Chen",
        "Xiang Bai",
        "Fan Wang",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 9 9 9 0 . 6 0 5 2 : r PlayerOne: Egocentric World Simulator Yuanpeng Tu1,2 Hao Luo2,3 Xi Chen1 Xiang Bai4 1HKU 2DAMO Academy, Alibaba Group Fan Wang2 Hengshuang Zhao1, 4HUST 3Hupan Lab https://playerone.github.io Figure 1: Simulated videos of our PlayerOne. Given an egocentric image as the scene to be explored, we can simulate egocentric immersive videos that are accurately aligned with the users motion sequence captured by an exocentric camera. All the users have been anonymized and action videos are shot with the front camera."
        },
        {
            "title": "Abstract",
            "content": "We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real-scene human motion of the user captured by an exocentric camera. PlayerOne is trained in coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications. Work during DAMO Academy internship. Corresponding author."
        },
        {
            "title": "Introduction",
            "content": "World models [36, 28, 35, 13, 1, 34] have undergone extensive research due to their ability to model environmental dynamics and predict long-term outcomes. Recent breakthroughs in video diffusion models [30, 12, 17] have revolutionized this domain, enabling the synthesis of high-fidelity, action-conditioned simulations that forecast intricate future states. These advancements empower applications ranging from autonomous navigation in dynamic real-world environments to the creation of immersive, responsive virtual worlds in AAA game development. By bridging the gap between predictive modeling and interactive realism, world simulators are emerging as critical infrastructure for next-generation autonomous systems and game engines, particularly in scenarios requiring real-time adaptation to complex, evolving interactions. Despite significant progress, this topic remains underexplored in existing research. Prior studies [9, 36, 7] predominantly focused on simulations within game-like environments, falling short of replicating realistic scenarios. Additionally, in their simulated environments, users are limited to performing predetermined actions (i.e., directional movements). Operating within the confines of constructed world restricts the execution of unrestricted movements as in real-world scenarios. While some initial efforts [19, 26, 1] have been made toward real-world simulation, they mainly contribute to world-consistent generation without human movement control. Consequently, users are reduced to passive spectators within the environment, rather than being active participants. This limitation significantly impacts the user experience, as it prevents the establishment of genuine connection between the user and the simulated environment. Faced with these challenges, we aim to design an egocentric world foundational framework that enables the user being freeform adventurer. Given user-provided egocentric image as the world to be explored, it can enable the user to perform unrestricted human movements real-time captured by an exocentric camera and consistent 4D scene modeling in the simulated world. Specifically, we propose the first realistic egocentric world simulator termed PlayerOne. Starting from diffusion transformer (DiT) model [23], we first extract the latent of an egocentric user-input image. Meanwhile, we select the real-world human motions (i.e., human pose or keypoints) as our motion representation. Considering the varying importance of different body parts in our task, the human motion sequence is partitioned into three groups (i.e., head, hands, feet and body) and fed into our part-disentangled motion injection to generate latents that can enable precise part-wise control. Additionally, we developed joint scene-frame reconstruction framework that can progressively complete scene point maps during the video generation process to enable scene-consistent generation. The DiT model takes the concatenation of the first frame latent, motion latent, video latent, and the point map latent as input and conducts noising and denoising on both the video and point map latent. Notably, the point map sequence is not required during inference, ensuring practical efficiency. Moreover, to overcome the absence of publicly available datasets, we curate required motion-video pairs from existing egocentric-exocentric datasets using an automated pipeline designed to filter and retain high-quality data. coarse-to-fine training strategy is also designed to compensate for the data scarcity. The base model is fine-tuned on large-scale egocentric text-video data for coarse-level generation, then refined on our curated dataset to achieve precise motion control and scene modeling. Finally, we distill our trained model [38] to achieve real-time generation. By integrating these innovations, PlayerOne advances the field of dynamic world modeling. Our contributions are summarized as follows: We introduce PlayerOne, the first egocentric foundational simulator for realistic worlds, capable of generating video streams with precise control of highly free human motions and world consistency in real-time and exhibiting strong generalization in diverse scenarios. We design novel part-disentangled motion injection scheme to enhance fine-grained motion alignment, where joint scene-frame reconstruction framework is introduced to guarantee world-consistent modeling in long-term video generation as well. We construct an effective automatic dataset construction pipeline to extract high-quality motion-video pairs from existing egocentric-exocentric datasets, where coarse-to-fine training scheme is also introduced to compensate for the data scarcity."
        },
        {
            "title": "2 Related Work",
            "content": "Video generation. The rapid development of diffusion models [27, 14, 41, 3] has driven substantial advancements in video generation. Early researchers [10, 5] adapted existing text-to-image models to 2 enable text-to-video generation to compensate for the limited availability of high-quality video-text datasets. Subsequently, diffusion transformers based frameworks [37, 23, 17, 12, 30, 29] are proposed. When scaling-up training, they enable more highly realistic and temporally coherent generation results. Among them, HunyuanVideo [17] substitutes T5 with Multimodal Large Language Model. LTXVideo [12] modifies the VAE decoder to handle the final denoising step and convert latents into pixels. Wan [30] introduces full spatial-temporal attention to ensure computational efficiency. World models. Existing world models [13, 35, 36, 7, 1, 34] can be roughly divided into two categories: 1) Agent learning targeted models, 2) World simulation models. For the former, they [13, 35, 34] aim at enhancing policy learning within simulated environments. Among them, Dreamer [13] and DayDreamer [35] tackle long-horizon tasks by leveraging latent imagination. Meanwhile, MuZero [34] employs self-play Monte Carlo Tree Search (MCTS). In contrast, world simulation approaches focus on modeling environments by explicitly predicting next states given the current state and action. These methods prioritize enabling human-neural network interaction through high-fidelity rendering, robust control mechanisms. Recent advancements in video generation have further enabled the development of high-quality, controllable world simulations, sparking significant progress in the field of world simulation [9, 26, 19, 36, 7, 1, 28, 6]. Among these works, WORLDMEM [36]. The Matrix [7] proposes the first world simulator capable of generating infinitely long real-scene video streams with real-time, responsive control. Matrix-Game [43] redefines video generation as an interactive process of exploration and creation. Cosmos [1] presents general-purpose world model and pre-training-then-post-training scheme. Aether [28] designs unified framework with synergistic knowledge sharing across reconstruction, prediction, and planning objectives. However, these methods primarily focus on virtual game scenarios and are limited to specific directional actions, rather than facilitating high-degree-of-freedom motion control in real-world environments. To address these limitations, we target at developing human motion driven realistic world simulator. Given an egocentric image, we can construct real-scene world that immerses users as freeform adventurers with precise and unrestricted human motion control."
        },
        {
            "title": "3 Method",
            "content": "In this section, we detail the methodology of PlayerOne. Sec. 3.1 introduces the relevant preliminaries and the overall pipeline. Sec. 3.2 presents the core of our proposed model, followed by Sec. 3.3 introducing our dataset construction and training strategy. 3.1 Overview Video diffusion models [23, 15] consist of two key processes: forward (noising) process and reverse (denoising) process. The forward process gradually adds Gaussian noise, denoted as ε G(0, I), to clean latent sample z0 Rkchw, where k, c, h, and represent the dimensions of the video latents. This transforms z0 into noisy latent zt. In the reverse process, learned denoising model ϵθ progressively removes the noise from zt to reconstruct the original latent representation. As shown in Fig. 2, our method comprises two core modules: Part-disentangled Motion Injection (PMI) and Scene-frame Reconstruction (SR). In PMI, we use real-scene human motion as the motion condition to enable free action control for the user. The first frame is converted into zf rame via 3D VAE encoder. The human motion sequence is split into three parts based on varying importance, and each part is fed into 3D motion encoder to obtain latents. These motion latents are concatenated into zmotion Rk3hw. To improve view alignment, we transform the head parameters of the human motion sequence into camera sequence, which is then fed into camera encoder. The output is added to the noised video latents zvideo to inject view-change signals. In SR, we jointly reconstruct video frames and 4D scenes to ensure world-consistent generation in the context of long video generation. We render point map sequence from the ground truth video and feed it into point map encoder with an adapter to obtain zpoint Rk64hw. Finally, all latents and conditions are concatenated channel-wise. The training objective of our method can be expressed as: (cid:104) = ε εθ (zt, t)2 2 (cid:105) , where zt = σtz0 + βtε, U(0, 1), ε G(0, I) (1) 2 = 1. Since we only add noise to point map latents and video latents, Where = 1, . . . , , σt thus z0 = zvideo zpoint. denotes the channel-wise concatenation operation, U() represents uniform distribution, and denotes the denoising steps. 2 + βt 3 Figure 2: Overall framework of our PlayerOne. It begins by converting the egocentric first frame into visual tokens. The human motion sequence is split into groups and fed into the motion encoders respectively to generate part-wise motion latents, with the head parameters converted into rotation-only camera sequence. This camera sequence is then encoded via camera encoder, and its output is injected into noised video latents to improve view-change alignment. Next, we render 4D scene point map sequence with the ground truth video, which is then processed by point map encoder with an adapter to produce scene latents. Then we input the concatenation of these latents into the DiT Model and perform noising and denoising on both the video and scene latents to ensure world-consistent generation. Finally, the denoised latents are decoded by VAE decoders to produce the final results. Note that only the first frame and the human motion sequence are needed for inference. 3.2 Model Components Part-disentangled motion injection. Prior studies [19, 26, 7, 43] typically utilize camera trajectories as motion conditions or are constrained to specific directional movements. These restrictions confine users to passive observer roles, preventing meaningful user interaction. In contrast, our approach empowers users to become active participants by adopting real-world human motion sequences (i.e., human pose or keypoints) as motion conditions, allowing for more natural and unrestricted movement. However, our empirical analysis reveals that extracting latent representations holistically from human motion parameters complicates precise motion alignment. To address this challenge, we introduce part-disentangled motion injection strategy that recognizes the distinct roles of various body parts. Specifically, hand movements are essential for interacting with objects in the environment, while the head plays crucial role in maintaining egocentric perspective alignment. Accordingly, we categorize the human motion parameters into three groups: body and feet, hands, and head. Each group is processed through its own dedicated motion encoder, comprising eight layers of 3D convolutional networks, to extract the relevant latent features. This specialized processing ensures accurate and synchronized motion alignment. These latents are subsequently concatenated along the channel dimension to form the final part-aware motion latent representation zmotion Rk3hw. To further enhance the egocentric view alignment, we solely transform the head parameters of the human motion sequence into sequence of camera extrinsics with only rotation values. We zero out the translation values in the camera extrinsics, assuming the head parameters are at the camera coordinate systems origin. Specifically, suppose the head parameter = (θx, θy, θz), we first normalize the rotation axis as follows: = , θ = Then we construct the rotation matrix as follows: = + sin θ [u] + (1 cos θ) [u]2 4 (2) (3) Figure 3: The overall pipeline of the dataset construction. By seamlessly integrating detection and human pose estimation models, we can extract motion-video pairs from existing egocentric-exocentric video datasets while retaining high-quality data through our automatic filtering scheme. Where is the cross product matrix of u, which can be denoted as follows: [u] = (cid:34) 0 uz uy uz 0 ux (cid:35) uy ux 0 (4) Then we use Plücker ray [40] to parameterize the camera extrinsics and then feed the output to an extra camera encoder, which shares similar structure with the motion encoder. Then the latents from this encoder are added to the noised video latents to inject the view-change information. Scene-frame reconstruction. While PMI enables precise control over egocentric perspective and motion, it does not guarantee scene consistency within the generated world. To address this limitation, we introduce joint reconstruction framework that simultaneously models the 4D scene and video frames, ensuring scene coherence and continuity throughout the video. Specifically, it begins by employing CUT3R [31] to generate point map for each frame based on ground truth video data, reconstructing the n-th frames point map using information from frames 1 through n. These point maps are then compressed into latent representations using specialized point map encoder [16]. To integrate these latents with video features, we implement an adapter composed of five 3D convolutional layers. This adapter aligns the point map latents with video latents and projects them into shared latent space, facilitating seamless integration of motion and environmental data. Finally, we concatenate the latent representations from the first frame, the human motion sequence, the noised video latents, and corresponding noised point map latents. This comprehensive input is then fed into diffusion transformer for denoising, resulting in coherent and visually consistent world. Importantly, point maps are only required during the training phase. During inference, the system simplifies the process by utilizing only the first frame and the corresponding human motion sequence to generate world-consistent videos. This streamlined approach enhances generation efficiency while ensuring that the resulting environment remains stable and realistic throughout the entire video. 3.3 Training Strategy Dataset preparation. The ideal training samples for our task are egocentric videos paired with corresponding motion sequences. However, no such dataset currently exists in publicly available repositories. As substitute, we derive these data pairs from existing egocentric-exocentric video datasets through an automatic pipeline. Specifically, for each synchronized egocentric-exocentric video pair, we first employ SAM2 [25] to detect the largest person in the exocentric view. The background-removed exocentric video is then processed using SMPLest-X [39] to extract the SMPL parameters of the identified individual as the human motion. To enhance optimization stability, an L2 regularization prior is incorporated. We then evaluate the 2D reprojection consistency to filter out low-quality SMPL data. This involves generating 3D mesh from the SMPL parameters using SMPLX [22], projecting the 3D joints onto the 2D image plane with the corresponding camera parameters, and extracting 2D key points via OpenPose [4]. The reprojection error is calculated by measuring the distance between the SMPL-projected 2D key points and those detected by OpenPose. Data pairs with reprojection errors in the top 10% are excluded, ensuring final dataset of high-quality motion-video pairs. The refined SMPL parameters are decomposed into body and feet (66 dimensions), head orientation (3 dimensions), and hand articulation (45 dimensions per hand) components for each frame. These components are fed into their respective motion encoders. The dataset construction pipeline is illustrated in Fig. 3. As detailed in Tab. 1, our training dataset combines multiple publicly available datasets to ensure comprehensive coverage of diverse environmental contexts, action types, and intensity levels, thereby enhancing model generalization. 5 Table 1: Statistics of datasets used for training our PlayerOne. quality particularly refer to the image resolution. Ego-Exo denotes whether the dataset contains egocentric-exocentric video pairs. Dataset Size Resolution Ego-Exo EgoExo-4D [8] Nymeria [20] FT-HID [11] EgoExo-Fitness [18] Egovid-5M [33] 740 1080p 1,200 1408p 38,364 1080p 1,276 1080p 5M 1080p Figure 4: Investigation on coarse-to-fine training. Joint-Train and No Pretrain denote training with both motion-video pairs and large-scale egocentric videos in one-stage manner and training with only motion-video pairs respectively. The Wanx2.1 1.3B is adopted as the baseline. Coarse-to-fine training. Though we can extract high-quality motion-video training data with our automatic pipeline, the limited scale of this dataset is insufficient for training video generation models to produce high-quality egocentric videos. To address this, we harness the extensive egocentric text-video datasets (i.e., Egovid-5M [33]). Specifically, we first fine-tune the baseline model using LoRA on large-scale egocentric text-video data pairs, enabling egocentric video generation with coarse-level motion alignment. Then we freeze the trained LoRA and fine-tune the last six blocks of the model with our constructed high-quality dataset to enhance fine-grained human motion alignment and view-invariant scene modeling, which can effectively address the scarcity of pair-wise data. Finally, we adopt an asymmetric distillation strategy that supervises causal student model with bidirectional teacher [38] to achieve real-time generation and long-duration video synthesis."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting Implementation details. We choose Wanx2.1 1.3B [30] as the base generator. We set the LoRA rank and the update weight of the matrices as 128 and 4 respectively and initialize its weight following [30]. The inference step and the learning rate are set as 50 and 1 105 respectively, where the Adam optimizer and mixed-precision bf16 are adopted. The cfg of 7.5 is used. We train our model for 100,000 steps on 8 NVIDIA A100 GPUs with batch size of 56 and sample resolution of 480480. The generated video runs at eight frames per second, and we utilize 49 video frames (6 seconds) for training. After distillation, our method can achieve 8 FPS to generate the desired results. All the action videos in this paper are shot with the front camera. Benchmark. Since there is no publicly available benchmark for our task, we construct benchmark with 100 videos collected from Nymeria [21] dataset, which is not included for training. It consists of coarse-level motion descriptions for each sample and covers diverse realistic scenarios. Considering the information gap between the human motion sequence and the text, we further use Qwen2.5-VL [2] to enrich the caption to generate videos for the competitors for more fair comparisons. Metrics. On our constructed benchmark, for evaluation of alignment with the given text descriptions, we calculate both CLIP-Score and DINO-Score, where PSNR, SSIM, and LPIPS [42] are employed to evaluate the video fidelity of the generated video. Besides, we calculate the frame consistency to evaluate the temporal coherence and consistency of the generated video frames over time. We further utilize 3D hand pose estimation model [24] to estimate the hand pose of the generated videos and 6 Figure 5: Investigation on part-disentangled motion injection. ControlNet denotes injecting motion latents with ControlNet [41]. Entangled and No Cam denote inputting the whole motion sequence into motion encoder without dividing into groups and removing the camera encoder respectively. Table 2: Quantitative evaluation on the components of PlayerOne. PlayerOne outperforms all these variants. No Camera/Filtering denote training without/with the camera encoder/data filtering. DINO-Score () CLIP-Score () MPJPE () MRRPE () PSNR() FVD () LPIPS() Baseline + Pretrain + Pretrain&ControlNet + Pretrain&Entangled + Pretrain&PMI (No Camera) + Pretrain&PMI + Pretrain&PMI&Filtering + Pretrain&PMI&Filtering&Recon(No Adapter) + Pretrain&PMI&Filtering&Recon(DUSt3R) PlayerOne(ours) 51.3 56.6 57.1 58.0 60.7 62.5 64.2 62.7 67.5 67.8 65.6 74.4 75.2 76.3 79.8 81.3 83.8 81.6 87.7 88.2 376.14 258.05 241.73 235.12 183.25 156.76 141.56 176.23 129.08 127.16 341.01 232.17 218.46 212.53 196.35 175.18 163.04 180.10 152.22 151. 35.6 41.2 42.8 43.9 45.6 48.3 49.1 47.3 52.2 52.6 394.16 301.32 287.52 279.41 257.04 245.72 230.50 240.17 228.20 226.12 0.1421 0.1146 0.1103 0.1060 0.0902 0.0839 0.0782 0.0919 0.0685 0.0663 use the results of the ground truth video as the labels. Afterward, we follow [24] to calculate two metrics: (1) Mean Per-Joint Position Error (MPJPE): the L2 distance between the predicted and ground truth joints for each hand after subtracting the root joint. (2) Mean Relative-Root Position Error (MRRPE): the metric distance between the root joints of the left hand and right hand. 4.2 Ablation Study Investigation on coarse-to-fine training. We first evaluate several variants of our coarse-to-fine training scheme, as depicted in Fig. 4. Specifically, when inputting action descriptions into the baseline model without fine-tuning, the generated results exhibit noticeable flaws. Similar issues can be observed when training with only motion-video pairs. We also explore jointly training with both large-scale egocentric videos and motion-video pairs. Specifically, when inputting egocentric videos, we set the motion latent values to zero and extract the latents of the text description to serve as the motion condition. Despite this variant being capable of generating egocentric videos, it fails to produce results accurately aligned with the given human motion conditions. In contrast, our coarse-to-fine training scheme delivers much better outcomes compared to these variants. Investigation on part-disentangled motion injection. Next, we conduct detailed analysis of our PMI module. Specifically, three variants are included: ControlNet-based [41] motion injection, inputting motion sequences as unified entity (the Entangled scheme), and removing our camera encoder. As shown in Fig. 5, the ControlNet-based scheme suffers from information loss, preventing it from producing results that accurately align with the specified motion conditions. Similarly, the entangled scheme demonstrates comparable shortcomings. Furthermore, removing the camera encoder leads to the models inability to generate view-accurate alignments. As depicted in Fig. 5, this variant fails to produce the corresponding perspective change associated with crouching. Ultimately, our PMI module successfully generates outcomes that are both view-aligned and action-aligned. Investigation on scene-frame reconstruction. Additionally, we conducted detailed analysis of the SR module, exploring three variants: omitting reconstruction, removing the adapter within the SR module, and substituting CUT3R [31] with DUStR [32] for point map rendering. As illustrated in 7 Figure 6: Investigation on scene-frame reconstruction. No Recon/No Adapter denote training without reconstruction/the adapter. DUStR is replacing CUT3R with DUStR for point map rendering. Figure 7: Qualitative evaluation on the motion alignment. We generate simulated videos based on the same first frame but different motion sequences. Results show that we can achieve accurate motion alignment. Fig. 6, the absence of reconstruction results in the models inability to generate consistently simulated results. Moreover, training without the adapter leads to noticeable distortions. Furthermore, after replacing CUT3R [31] with DUStR [32], our PlayerOne can also produce scene-consistent outputs, demonstrating its robustness to different point map rendering techniques. Motion alignment. To verify the alignment capability with the given motion condition, we conduct experiments by generating world-simulated videos with the same first frame but different human motion sequences. Fig. 7 shows that our PlayerOne can accurately generate corresponding results according to different conditions and produce reasonable interactive changes. Quantitative comparisons. We provide quantitative results on the core components of our PlayerOne in Tab. 2. All numerical results concur with the visualization outcomes. significant performance improvement is observed when the model undergoes pre-training on large-scale egocentric text-video datasets. The introduction of PMI yields an additional accuracy boost, and it outperforms all of its variants. In addition, our designed filtering strategy maximizes performance as well by filtering noisy motion-video pairs. By introducing our joint scene-frame reconstruction scheme, we achieve superior results across all metrics. 4.3 Comparison with State-of-the-arts Quantitative comparison. Since there is no method sharing the same setting as ours, we selected two potential competitors for comparison: Cosmos [1] and Aether [28]. As shown in Tab. 3, our PlayerOne outperforms all the baselines. Notably, Cosmos [1] exhibits better generalization ability than Aether [28] by explicitly capturing general knowledge of real-world physics and natural behaviors. Besides qualitative results, we provide visualization comparisons in Fig. 8 as well. 8 Figure 8: Qualitative comparisons between our method and other competitors. Our PlayerOne can achieve the best performance on both the motion alignment, video quality. Table 3: Quantitative comparison between our PlayerOne and other works. Seven metrics are employed for the evaluation. PlayerOne outperforms these methods across all the metrics. DINO-Score () CLIP-Score () MPJPE () MRRPE () FVD () LPIPS() PSNR() Aether [28] Cosmos(Diff-7B) [1] Cosmos(Diff-14B) [1] PlayerOne(ours) 38.0 45.3 51.6 67. 64.2 70.3 79.7 88.2 415.70 301.92 256.73 127.16 431.05 324.12 253.06 151.62 38.1 43.7 47.8 52.6 397.40 346.09 302.17 226.12 0.1856 0.1630 0.1351 0. Table 4: User study on our PlayerOne and existing alternatives. Quality, Fidelity, Smooth, and Alignment measure synthesis quality, object identity preservation, motion consistency, and alignment with the text descriptions, respectively. Each metric is rated from 1 (worst) to 4 (best). Quality () Fidelity () Smooth () Alignment () Aether [28] Cosmos(Diff-7B) [1] Cosmos(Diff-14B) [1] PlayerOne(ours) 1.32 2.07 3.02 3.59 1.30 2.13 2.94 3.63 1.31 2.05 2.98 3.65 1.34 2.09 2.71 3.86 User study. In Tab. 4, we report the comparison results of human preference rates. We let 20 annotators rate 25 groups of videos, where each group contains the generated video of each method and text description. And we provide detailed regulations to rate the results for scores of 1-4 from four views: Quality, Smooth, Fidelity, Alignment. Quality counts for whether the result is harmonized without considering fidelity. Smooth assesses the motion consistency across the video. Fidelity measures ID preservation and distortions within the video, while we use Alignment to measure the alignment with the given text descriptions. It can be noted that our model demonstrates significant superiority across all the metrics, especially for Alignment, and Smooth."
        },
        {
            "title": "5 Conclusion\nIn conclusion, PlayerOne represents a significant advancement in interactive and realistic world\nmodeling for video generation. Unlike conventional models that are restricted to particular game\nscenarios or actions, our PlayerOne can capture the complex dynamics of general-world environments\nand enable free motion control within the simulated world. By formulating world modeling as a joint\nprocess of videos and 4D scenes, our PlayerOne ensures coherent world generation and enhances\nmotion and view alignment with the given conditions through part-disentangled motion injection.\nExperimental results demonstrate our superior performance across diverse scenarios.\nLimitations. Despite the compelling outcomes, our performance in game scenarios is slightly inferior\nto realistic ones, likely due to the imbalanced distribution between realistic and game training data. It\ncan be addressed by incorporating more game-scenario datasets in future research.",
            "content": "Acknowledgements. This work was supported by DAMO Academy via DAMO Academy Research Intern Program."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv:2501.03575, 2025. 2, 3, 8, 9 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 6 [3] Andreas Blattmann, Robin Rombach, Kaan Oktay, and Björn Ommer. Retrieval-augmented diffusion models. In NeurIPS, 2022. 2 [4] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. TPAMI, 2019. 5 [5] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv:2305.13840, 2023. 2 [6] Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, and Ying Shan. Animegamer: Infinite anime life simulation with next game state prediction. arXiv:2504.01014, 2025. 3 [7] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv:2412.03568, 2024. 2, 3, 4 [8] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In CVPR, 2024. [9] Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv:2504.08388, 2025. 2, 3 [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. 2 [11] Zihui Guo, Yonghong Hou Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and Wanqing Li. Ft-hid: large scale rgb-d dataset for first and third person human interaction analysis. Neural Computing and Applications, 2022. 6 [12] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv:2501.00103, 2024. 2, 3 [13] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv:1912.01603, 2019. 2, [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arxiv:2006.11239, 2020. 2 [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. arXiv, 2022. 3 [16] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. arXiv:2504.07961, 2025. [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv:2412.03603, 2024. 2, 3 10 [18] Yuan-Ming Li, Wei-Jin Huang, An-Lan Wang, Ling-An Zeng, Jing-Ke Meng, and Wei-Shi Zheng. Egoexo-fitness: towards egocentric and exocentric full-body action understanding. In ECCV, 2024. 6 [19] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. arXiv:2412.12091, 2024. 2, 3, 4 [20] Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, Kevin Bailey, David Soriano Fosas, C. Karen Liu, Ziwei Liu, Jakob Engel, Renzo De Nardi, and Richard Newcombe. Nymeria: massive collection of multimodal egocentric daily motion in the wild. In ECCV, 2024. 6 [21] Lingni Ma, Yuting Ye, Fangzhou Hong, Vladimir Guzov, Yifeng Jiang, Rowan Postyeni, Luis Pesqueira, Alexander Gamino, Vijay Baiyya, Hyo Jin Kim, et al. Nymeria: massive collection of multimodal egocentric daily motion in the wild. In ECCV, 2024. [22] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 5 [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv:2212.09748, 2022. 2, 3 [24] Aditya Prakash, Ruisen Tu, Matthew Chang, and Saurabh Gupta. 3d hand pose estimation in everyday egocentric images. arXiv:2312.06583, 2024. 6, [25] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. In ICLR, 2025. 5 [26] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed worldconsistent video generation with precise camera control. arXiv:2503.03751, 2025. 2, 3, 4 [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. arXiv:2112.10752, 2021. 2 [28] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometric-aware unified world modeling. arXiv:2503.18945, 2025. 2, 3, 8, 9 [29] Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. 3 [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. 2, 3, [31] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv:2501.12387, 2025. 5, 7, 8 [32] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 7, 8 [33] Xiaofeng Wang, Kang Zhao, Feng Liu, Jiayu Wang, Guosheng Zhao, Xiaoyi Bao, Zheng Zhu, Yingya Zhang, and Xingang Wang. Egovid-5m: large-scale video-action dataset for egocentric video generation. arXiv:2411.08380, 2024. 6 [34] Aurèle Hainaut Werner Duvaud. Muzero general: Open reimplementation of muzero. https: //github.com/werner-duvaud/muzero-general, 2019. 2, [35] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, and Pieter Abbeel. Daydreamer: World models for physical robot learning. In CoRL, 2022. 2, 3 11 [36] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv:2504.12369, 2025. 2, 3 [37] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformer. arXiv:2410.10629, 2024. 3 [38] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 2, [39] Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Atsushi Yamashita, Lei Yang, and Ziwei Liu. Smplest-x: Ultimate scaling for expressive human pose and shape estimation. arXiv:2501.09782, 2025. 5 [40] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In ICLR, 2024. 5 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2, 7 [42] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. arXiv:1801.03924, 2018. [43] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Zedong Gao, Eric Li, Yang Liu, and Yahui Zhou. Matrix-game: Interactive world foundation model. arXiv, 2025. 3,"
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "HKU",
        "HUST",
        "Hupan Lab"
    ]
}