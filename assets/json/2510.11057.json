{
    "paper_title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
    "authors": [
        "Youngrok Park",
        "Hojung Jung",
        "Sangmin Bae",
        "Se-Young Yun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 5 0 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TEMPORAL ALIGNMENT GUIDANCE: ON-MANIFOLD SAMPLING IN DIFFUSION MODELS Youngrok Park Hojung Jung Sangmin Bae KAIST AI {yr-park, ghwjd7281, bsmn0223, yunseyoung}@kaist.ac.kr Se-Young Yun"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have achieved remarkable success as generative models. However, even well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages time predictor to estimate deviations from the desired data manifold at each timestep, identifying that larger time gap is associated with reduced generation quality. We then design novel guidance mechanism, Temporal Alignment Guidance (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have shown remarkable performance as generative models across various domains, including image (Dhariwal & Nichol, 2021; Rombach et al., 2022), video (Liu et al., 2024; Polyak et al., 2024), audio Kong et al. (2021); Popov et al. (2021), language Austin et al. (2021), and molecular generation (Hoogeboom et al., 2022). key factor in their success is the ability to perform guided generation, where conditions from different modalities can be effectively injected during the generative process (Dhariwal & Nichol, 2021; Ho & Salimans, 2021). Recently, diffusion models have been applied to variety of real-world use cases, such as blackbox optimization (Krishnamoorthy et al., 2023), personalization (Zhang et al., 2023), and inverse problems (Chung et al., 2023). These downstream applications often require modifications to the standard sampling procedure, incorporating an additional guidance term during the reverse process of the diffusion model. This guidance term steers the generated samples towards desired properties relevant to the specific downstream task (Graikos et al., 2022; Wang et al., 2024; Wei et al., 2024). Notably, several works have demonstrated the ability to guide samples even towards conditions unseen during training, technique often referred to as training-free guidance (Chung et al., 2023; Bansal et al., 2024). However, naively modifying the originally learned reverse process of diffusion models can catastrophically break other basic properties, as it may lead samples toward low density regions where the output of diffusion model is unreliable (Song & Ermon, 2019). This score approximation errors can accumulate over each timestep (Chen et al., 2023b; Oko et al., 2023) which contributes to the final generated samples deviate significantly from the true data manifold, resulting in unrealistic outputs (Shen et al., 2024; Guo et al., 2024). In this work, we refer to this problem as the off-manifold phenomenon in diffusion models and demonstrate that it can pose significant challenge to their practical applications. To address the off-manifold problem in diffusion models, we introduce Temporal Alignment Guidance (TAG), general solution designed to mitigate score approximation error induced by arbitrary modifications to the reverse process. Unlike traditional approaches that rely on fixed timesteps in Equal contribution"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of TAG algorithm. (Left) Without TAG, external guidance pushes samples off-manifold, causing the standard diffusion step log p(x) to miss the target manifold Mti1 . TAGs correction actively steers the sample back to the correct manifold Mti , ensuring the diffusion step accurately reaches the desired manifold Mti1 . (Right) Applying TAG can greatly improve the fidelity in conditional generation tasks with target conditions: worm for ImageNet, polarizability α for Molecule, female and black hair for CelebA. the reverse process, TAG leverages the inherent uncertainty of the time variable by representing it as probability distribution over range of possible values. This novel guidance term is designed to steer samples back to the higher density region, where learned score of the model becomes reliable, thereby improving sample quality while providing control in downstream tasks. Our approach introduces corrective step that steers samples back to the higher density region, where the models learned score becomes reliable. This mechanism is visually summarized in Figure 1 (Left). Through extensive experiments, we show that TAG significantly improves the quality of generated samples across multiple domains and tasks, as demonstrated in Figure 1 (Right). Promising results of TAG on these diverse scenarios implies that TAG could indeed serve as universal solution for mitigating the off-manifold phenomenon in diffusion models, common issue that arises in numerous downstream tasks but yet to be solved. We believe that this work represents an important stepping stone toward achieving reliable generation for real-world applications using diffusion models. Our main contributions can be summarized as follows: We identify off-manifold phenomena in diffusion models across multiple scenarios and demonstrate that these phenomena can be significantly amplified when the learned reverse process of the original diffusion model is arbitrarily adjusted. We design novel framework, Temporal Alignment Guidance (TAG), which pushes the samples toward the desired manifold at each timestep during generation and provide theoretical guarantees. We demonstrate that TAG significantly improves sample quality through extensive experiments in various domains and tasks, achieving state-of-the art results."
        },
        {
            "title": "2 OFF-MANIFOLD PHENOMENON IN DIFFUSION MODELS",
            "content": "Off-manifold phenomenon happens in each timestep if the sample is tilted towards the low density region of true marginal distribution pt(x), which represents the distribution of noisy sample at timestep t. Below, we list typical situations where off-manifold phenomenon can occur in diffusion models. Controlling by external guidance Anderson (1982) shows the forward process of diffusion model can be reversed once score function log qt(x) of marginal distribution qt is given for each by the following reverse SDE: dx = (cid:2)f (x) g2(t)x log qt(x)(cid:3) dt + g(t)d wt, (1) where wt denotes standard wiener process with backward time flows. In many practical scenarios, diffusion model sampling needs an extra guidance term v(x, c, t) to generate high-quality samples which modifies reverse diffusion process as follows: dx = (cid:2)f (x) g(t)2 (x log qt(x) + v(x, c, t))(cid:3) dt + g(t)d wt, (2)"
        },
        {
            "title": "Preprint",
            "content": "One notable approach is training-free guidance (Chung et al., 2023) where, v(xt, c, t) = xt log p(cˆx0), and ˆx0 is target estimate approximated with Tweedies formula (Efron, 2011) as follows: ˆx0 = xt + (1 αt)xt log p(xt) αt . (3) (4) Here, αt is function determined by the forward process (Appendix B.3 for further details). Although training-free guidance can approximate sampling from conditional distribution only with unconditional model (Chung et al., 2023; Ye et al., 2024), this extra guidance in each timestep make samples far from the original learned data manifold (Shen et al., 2024). Multi-conditional guidance Downstream applications with diffusion models often required fine-grained control such as multi-conditional guidance (Du et al., 2023) or constrained guidance (Schramowski et al., 2023), where linear combination of more than two score functions are used to satisfy target properties. However, as stated in (Du et al., 2023), naive combination of two independent conditional score functions does not equal to multi-conditional score function: log p(xc1, c2) = log p(xc1) + log p(xc2). (5) Few-step generation The probability flow ODE formulation of diffusion models (Song et al., 2021b) accelerates generation by reducing the number of function evaluation (NFE) for sampling. However, discretization errors accumulate during the reverse process, resulting in off-manifold problem. We provide further details in Appendix B.5. Degradation of sample quality in low-density regions The score function log pt(xt) of the diffusion model is trained to guide samples toward high density regions of the noisy data distribution pt(xt) at each timestep t. Ideally, in perfectly learned diffusion process, this ensures generated outputs remain close to the original data manifold, resulting in high-fidelity samples. However, if an external force drives sample to the low density region pt(xt) 0, the score function log pt(xt) estimated by the diffusion model becomes unreliable, as it is trained on noisy data that assumes the forward process is intact at the given timestep. This, often known as score approximation error (Oko et al., 2023; Chen et al., 2023a), accumulates over time as generation process goes on, causing compounding errors that degrade sample quality in the subsequent steps of the generation process (Li & van der Schaar, 2024). To illustrate how off-manifold phenomenon can become detrimental in diffusion sampling process, we construct toy example of two Gaussian mixtures where external drift term is added in every timestep of the reverse process (details in Appendix E.1). Figure 2a shows that applying this external drift term in every diffusion step results in samples far from the original distribution."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we introduce Temporal Alignment Guidance (TAG), novel framework designed to maintain sample fidelity during diffusion model generation by mitigating off-manifold deviations at each timestep. We first formally define TAG, introducing the core concept of the Time-Linked Score (TLS) (Sec. 3.1). Subsequently, we detail how TAG integrates with practical guidance techniques to enhance conditional generation (Sec. 3.2). Finally, we provide theoretical analysis on how TAG improves sample quality in the presence of off-manifold phenomenon (Sec. 3.3) along with illustrative example (Sec. 3.4). 3.1 TEMPORAL ALIGNMENT GUIDANCE (TAG) Projecting samples back to the On-Manifold We reinterpret timestep information as conditioning variable rather than fixed input in the reverse diffusion process. Fixed times scheduling suffices when samples remain on the original reverse path, it breaks down off-manifold because xt loses its temporal identity. To project xt back onto the correct manifold Mt (formal definition in Appendix B.4), we introduce the gradient term log pt(t x), analogous to the conditional score"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Temporal Alignment Guidance (TAG) Input: Diffusion model θ, time predictor ϕ, guidance strength schedule ωt, number of total diffusion steps xT (0, I) for = T, , 1 do xt xt + ωt log pϕ(t xt) Obtain log p(x) from diffusion model θ xt1 xt from reverse diffusion step following Eq. 1. end for Output: x0 (a) Original model score (b) Time score Figure 2: Generated samples with score field. (Left) Generated outputs from reverse diffusion process with external drift, with vector field of the diffusion model output at = 0. (Right) Generated outputs when applying TAG with external drift, with vector field of the TLS at = 0. in classifier guidance (Dhariwal & Nichol, 2021). Figure 2b illustrates that this vector field directs samples toward high-probability regions of the original distribution qt, whereas the conventional diffusion score struggles once off-manifold. Incorporating this term into each reverse step thus keeps generated samples aligned with the data distribution (Figure 2b). In the next subsection, we formally define and analyze this new gradient correction. Time-Linked Score (TLS) To further investigate the effect of this gradient term, we introduce the following definition: Definition 3.1. Time-Linked Score for data point and target time is defined as, TLS(x, t) := log p(t x). (6) Combining TLS with original score function of diffusion models, we now define Temporal Alignment Guidance: Definition 3.2. The Temporal Alignment Guidance (TAG) at time is defined as TAG(x, t) = log pt(x) + ω log pϕ (cid:0)t x(cid:1). (7) where ω is hyperparameter that controls the strength. Applying TAG in the reverse diffusion provides shortcut for sample to the original manifold by sending it to the tilted probability p(xt)p(tx)ω, just as in the classifier guidance Dhariwal & Nichol (2021). We provide pseudo-code of sampling with TAG in Algorithm 1. Time classification by time predictor Accurately identifying the correct manifold for each time is analytically impossible due to the complexity of the score function of real-world dataset Zhang & Chen (2023); Han et al. (2024b). Instead, we utilize time predictor Jung et al. (2024), which is an auxiliary neural network trained with one-hot embeddings of timestep labels with following objective function: Ltp(ϕ) = Et,x0 [log (ˆpϕ(xt)t)] , where ˆp denotes logit vector of the model output. Time predictor learns to classify which timestep random data with forward process should belong to. By calculating gradient of the time predictor, we can estimate TLS in Eq. 6. We use the simple cnn architecture that is substantially lightweight compared to the diffusion backbone. Details of the designing mechanism and performance of time predictor is in Appendix E.4. (8) 3.2 IMPROVING GUIDANCE WITH TAG We now present how TAG can be combined with standard zero-shot conditional sampling framework like training-free guidance (TFG) (Chung et al., 2023; Ye et al., 2024) to improve conditional generation of diffusion models."
        },
        {
            "title": "Preprint",
            "content": "Let be the target property and let : be off the shelf function that maps x0 to their predicted property values. Training-free guidance is applied as, (9) where ℓc : measures the discrepancy between the estimated property and target property, and ˆx0 is the denoised estimate from Eq. 4. xtlog pt(cxt) = xtlog Ep(x0xt) (cid:2)exp(cid:0)ℓc(A(ˆx0), c)(cid:3)"
        },
        {
            "title": "One can obtain TLS with similar approach by observing",
            "content": "p(t xt, c) exp(cid:0)ℓt(ϕ(xt, c), t)(cid:1), (10) where ℓt is penalty function for misalignment in time, and we set as cross-entropy loss. With the extended view of adding time information as another condition, we use Bayes rule to the conditional probability as: pt(xt c) pt(xt) p(c xt) p(t xt, c). Taking gradient respect to xt for both sides, one can obtain conditional score function as follows: xt log p(xtc) xt log p(xt) + σtxtℓc(A(ˆx0), c) + ωtxtℓt(ϕ(xt, c), t). (11) In essence, by treating time as an additional conditioning signal, TAG act as an on-manifold anchor at every reverse step: it pulls samples back onto the learned diffusion path, preventing off-manifold drift and markedly improving fidelity under arbitrary guidance. 3.3 THEORETICAL ANALYSIS OF TAG Here, we provide the theoretical justification of TAG. We rigorously show that TAG can effectively reduce the error bound between the distribution of generated samples and the target distribution. To start with, we first present the following theorem which states that TLS is linear combination of the score functions of different timesteps in the following way: Theorem 3.3. Assuming discrete diffusion timesteps [t1, t2, . . . , tn], Time-linked Score of random noisy sample to the target time ti can be represented as: log p(cid:0)ti x(cid:1) = (cid:88) k=i (cid:16) ptk (x) ptot(x) (cid:124) (cid:123)(cid:122) (cid:125) greater when off ti-manifold log pti(x) (cid:125) (cid:123)(cid:122) (cid:124) pull to ti manifold (cid:17) log ptk (x) (cid:125) (cid:123)(cid:122) (cid:124) repel other manifolds . (12) Here, ptis are marginal distributions at each timestep and ptot = (cid:80) rem 3.3 is in Appendix C.4. ptj (x). The proof of TheoTheorem 3.3 implies that TLS is particularly effective when pti (x) ptot(x). In this regime, log pti(x) attracts the sample toward original data manifold, while simultaneously repelling it from competing manifolds through the negative terms log ptk (x) for = i. Moreover, if ptj (x) dominates for some = i, the repulsive force log ptj (x) in equation 12 grows, aiding the sample to escape an incorrect manifold. The above result can be naturally extend to continuous time (Appendix C.5). Intuitively, at time t, score approximation errors tend to be larger in low-density regions of pt(x), since the model rarely encounters such regions during training. Consequently, corrector sampling (Song et al., 2021b) may become ineffective there, as the neural networks score estimates degrade. Moreover, even an accurate score estimate can struggle to guide samples out of inherently flat probability landscapes. Indeed, our empirical findings in Appendix D.2 show that corrector sampling becomes ineffective, sometimes degrade the sample quality under external guidance. Applying TAG can mitigate the aforementioned problems by increasing the chance of escape in this low density region. This can be formalized into the following proposition. Proposition 3.4. Applying TAG alters energy barrier map Uk(x) = log ptk (x) at timestep tk to Φk(x) for any by: Φk(x) = Uk(x) γi Ui(x), (13) (cid:88) where γi = pi(x) ptot(x) for = and γk = 1 (cid:80) i=k pi(x) ptot(x) ."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Effect of TAG across strength ω of TAG when reverse process is corrupted with noise level σ. σ = 0.1 σ = 0.2 σ = 0.3 ω TG FID IS TG FID IS TG FID IS 0.0 104.1 193.6 2.37 229.6 351.4 1.50 274.0 410.1 1.28 0.5 1.0 2.0 4.0 47.9 127.7 3.65 200.6 340.1 1.56 261.6 408.5 1. 41.8 120.9 3.69 175.5 323.7 1.61 250.9 406.7 1.28 39.0 132.6 3.33 140.2 285.1 1.64 232.6 390.3 1.27 44.4 159.8 3.00 103.4 246.3 1.70 197.8 361.2 1.31 Figure 3: FID values over different corruption levels for original diffusion process without TAG and with TAG. We defer the proof to Appendix C.6. Under mild assumptions, it shows that TAG sharpens the potential map via the negative repulsion of alternative timestep manifolds. Building on the Jordan KinderlehrerOtto (JKO) scheme (Jordan et al., 1998), one can show that the modified Langevin dynamics under this sharpened potential map accelerates correction with stronger gradient flows. In particular, applying single reverse diffusion step with TAG increases the chance of sample to move towards higher-density regions, thereby reducing expected score approximation errors. Building on prior analyses of diffusion models (Oko et al., 2023; Chen et al., 2023b), we show that TAG can improve the convergence guarantee by lowering the upper bound on the total variation distance dT between the sample distribution and the target distribution: Theorem 3.5. (Informal) Let pt and pt be the probability distribution at time in the original reverse process in equation 1 and in the reverse process apply with TAG (Algorithm 1). Then, under mild assumptions, the upper bound of dT (qdata, p0) can be reduced compared to dT (qdata, p0). Theorem 3.5 demonstrates TAGs ability to enhance sample quality, finding that aligns with our experimental observations. We provide formal statement of Theorem 3.5 with corresponding proof in Appendix C.7. 3.4 UNDERSTANDING TAG UNDER CORRUPTED REVERSE PROCESS To analyze TAGs corrective mechanism and evaluate its effectiveness under extreme perturbation, we conduct an experiment where artificial noise is applied at every reverse step. To quantify the temporal deviation during generation, we define the Time-Gap metric. Denoting the sample at timestep as xt and the time predictor as ϕ, the Time-Gap is defined as 1 (cid:12). lower Time-Gap indicates that samples remain closer to their expected temporal manifold and correlates with improved generation quality (see Appendix F.1 for formal definition and empirical validation). (cid:12)arg max ϕ(xt) t(cid:12) (cid:12) (cid:80)T t= Table 1 shows the effect of applying TAG under various noise levels (σ) and guidance strengths (ω). As ω increases, both FID and IS improve, while the Time-Gap decreases, indicating that samples are drawn closer to the correct manifold. Figure 3 further illustrates that TAG significantly alleviates the degradation caused by increasing σ. These findings empirically confirm that the TLS component indeed corrects deviations and steers samples back to the appropriate temporal manifold, even under extreme perturbations. Further details of the experiments with additional results are in Appendix E.2."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate TAG empirically across diverse scenarios including those prone to off-manifold errors and practical applications mentioned in Sec. 3. First, we show that TAG improves standard TFG benchmarks via extensive comparisons with related methods (Sec. 4.1). Next, we extend to multi-conditional guidance, demonstrating efficient conditioning on multiple attributes without combinatorial overhead (Sec. 4.2). Then, we assess its ability to mitigate errors in few-step generation (Sec. 4.3). Finally, we demonstrate its applicability and benefits in large-scale text-to-image generation tasks (Sec. 4.4)."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Quantitative results of TAG on TFG benchmark. Each cell presents the guidance validity / generation fidelity averaged across multiple targets in the task. The best result for each cell is reported in bold. Method DPS (Chung et al., 2023) DPS + TAG (ours) Rel. Improvement TFG (Ye et al., 2024) TFG + TAG (ours) Rel. Improvement Baseline Results TCS (Jung et al., 2024) Timestep Guidance (Sadat et al., 2024) Self-Guidance (Li et al., 2024b) 454.7 480.3 231.8 Deblur Super-resolution CIFAR ImageNet Audio declipping Audio inpainting FID LPIPS FID LPIPS FID Acc. FID Acc. FAD DTW FAD DTW 139.7 128.9 7.7% 64.2 62.7 2.3% 0.613 0.570 7.0% 0.154 0.151 1.9% 0.751 0.995 0.709 139.0 128. 7.7% 65.5 64.7 0.614 0.572 217.1 190.4 57.5 63.2 196.9 192. 24.5 22.9 2.41 2.33 191 189 2.26 2.25 176 157 6.8% 12.3% 9.9% 2.4% -6.5% 3.3% 1.0% 0.4% 10.8% 0.187 0. 114.1 102.7 55.8 61.5 231.0 219.4 14.3 17.8 1.42 0.74 256 0.52 0.42 74 51 1.2% 6.4% 10.0% 10.2% 5.0% 24.5% 47.9% 53.1% 19.3% 31.1% 465.1 480.3 231. 0.748 0.995 0.710 213.4 393.2 205.4 29.4 11.3 51.6 344.9 545.7 257.4 12.0 25.0 10.8 23.89 46.22 8. 567 492 521 21.41 45.94 6.99 558 491 463 Method DPS (Chung et al., 2023) DPS + TAG (ours) Rel. Improvement TFG (Ye et al., 2024) TFG + TAG (ours) Rel. Improvement Baseline Results Polarizability α Dipole µ MAE Stab. MAE Stab. MAE Heat capacity Cv ϵHOMO Stab. MAE Stab. MAE ϵLUMO Gap ϵ Stab. MAE Stab. 13.33 7.96 28.4 96.4 4779.92 1.48 34.4 97.2 3.47 3. 36.2 93.0 0.68 0.58 30.3 56.2 1.57 1.11 17.6 48.4 1.65 1. 10.6 93.5 40.3% 239.7% 99.9% 182.5% 13.1% 157.0% 6.1% 85.7% 29.6% 174.5% 21.4% 779.2% 8.91 4.46 19.2 43.6 2.41 1.28 26.3 94. 2.65 2.67 96.2 96.7 0.55 0.43 14.6 93.9 1.33 0.89 10.8 92. 1.40 0.78 16.1 82.8 49.9% 127.1% 46.9% 258.6% 0.3% 0.5% 21.8% 543.8% 33.1% 757.4% 44.3% 414.2% TCS (Jung et al., 2024) Timestep Guidance (Sadat et al., 2024) Self-Guidance (Li et al., 2024b) 11.44 25.07 16.33 15.3 70.2 65.3 1.60 N/A 62.86 6.3 N/A 70.9 3.17 4.18 3.89 19.6 82.9 79. 0.59 N/A N/A 50.2 N/A N/A 1.23 N/A 2.32 28.8 N/A 10.8 1.58 1.39 1.30 13.9 48.7 24. 4.1 TFG BENCHMARK Setup We follow the setup of TFG benchmark (Ye et al., 2024), standard zero-shot conditional sampling framework, applying TAG to DPS (Chung et al., 2023) and TFG with their reported optimal hyperparameters. This offers challenging comparison, since these carefully tuned baselines should exhibit less off-manifold drift than simpler methods. Experiments use 6 pretrained modelsCIFAR10-DDPM (Nichol & Dhariwal, 2021), ImageNet-DDPM (Dhariwal & Nichol, 2021), Cat-DDPM (Elson et al., 2007), CelebA-DDPM (Karras et al., 2018), Molecule-EDM (Hoogeboom et al., 2022), and Audio-Diffusion (Kong et al., 2021; Popov et al., 2021). The tasks include image restoration (deblurring, super-resolution), conditional generation (label-guided sampling, multi-attribute generation), molecular generation (molecular property control), and audio synthesis (clipping, inpainting). For all tasks, we report generation fidelity and validity, with further details provided in Appendix E.3. External guidance scenario We evaluate TAG in single-conditional guidance setting, where the objective is to sample from the target distribution p(x0 c) with DPS Chung et al. (2023) and (cid:112)(1 αt). The final results TFG (Ye et al., 2024). We set the guidance schedule of TAG as ωt = ω0 are averaged over the best-performing guidance strength w0 according to the grid search for all target values in each task. The results in Table 2 demonstrate that TAG significantly improves the fidelity while maintaining conditioning effect across most tasks. We observe that TAG is particularly effective when the adversarial effect of external guidance becomes larger (i.e, when training free guidance guidance degrades sample fidelity). To confirm this, we compare TAG against several recent approaches applied on top of DPS, including TCS (Jung et al., 2024), Timestep Guidance (Sadat et al., 2024), Self-Guidance (Li et al., 2024b), and exposure-bias methods (Ning et al., 2024; Li et al., 2024a; Ning et al., 2023). The result confirms that while these baselines degrade under external guidance drift, TAG remains robust (see further details in Appendix D). To further highlight this effectiveness, we conduct additional experiments by increasing the DPS strength from 1.0 to 5.0. Table 3 shows that TAG effectively mitigates the negative influence of stronger guidance strength, while applying only DPS results in generating mostly non-valid samples. In contrast, applying TAG with DPS show robust performance across all evaluation metrics. Qualitative results are in Appendix G."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Quantitative result of TAG for different values of DPS guidance strength. (DPS / DPS + TAG) Table 4: Quantitative evaluation of FID for few-step using DDPM sampling without external guidance. Heat cap. Cv Str. TAG FID Acc FID Acc MAE Stab MAE Stab ImageNet CIFAR10 Polar. α 1.0 1.5 2.5 5.0 217.1 57.5 196.9 24.5 190.4 63.2 192.2 22.9 269.5 51.4 219.3 27.0 231.9 62.3 204.1 32.7 334.1 41.9 230.2 28.5 289.7 51.9 212.7 30.2 384.8 29.4 246.7 24.3 347.8 41.0 233.1 27.2 103.7 48. 109.8 50.3 159.5 49.9 112.7 51.7 1.1 32.2 0.9 31.8 1.0 31. 1.1 30.4 13.7 9.9 16.2 11.1 18.4 12.2 N/A 14.7 1.9 5. 2.1 14.0 2.9 9.7 N/A 8.0 Inference Steps Dataset TAG 1 Step 3 Step 5 Step 10 Step 50 Step 100 Step CIFAR10 ImageNet Cat 460.0 234.1 158.6 271.1 160.5 118.8 430.3 297.6 295.2 352.8 265.1 265.0 433.7 313.5 243.9 314.8 178.8 199. 106.3 93.1 286.7 265.1 209.9 188.1 71.8 70.9 259.6 245.7 166.4 164. 67.6 66.5 251.1 244.7 154.9 152.2 Table 5: Quantitative results of TAG in Multi-Conditional generation on TFG benchmark. Each cell presents the guidance validity/generation fidelity averaged across multiple targets in the task. The best result for each cell is reported in bold. CelebA Molecule Gender + Age Gender + Hair α, µ Cv, µ α, µ, Cv, ϵ, ϵHOMO, ϵLUMO Method TAG KID Acc KID Acc MAE Stab MAE Stab MAE Baseline Multi. Single. Uncon. -2.75 -2.85 -2.86 -2.87 80.5 87.1 91.0 89.1 -3.16 -3.19 -3.27 -3. 92.1 94.9 96.1 96.0 13.7 1782.8 4.56 4.65 4.56 1.31 1.33 1.35 68. 84.7 83.9 84.9 4.97 1425.2 2.72 2.63 2.74 1.33 1.40 1.36 70.9 84.2 82.9 84. 10.1 31.9 4.33 0.635 1.14 1.18 4.52 1.45 2.94 0.610 1.13 1.15 4.58 1.39 3.05 0.577 1.05 1.11 4.48 1.44 2.82 0.530 1.07 1.15 Stab 56.0 91.2 85.9 85.9 4.2 MULTI-CONDITIONAL GUIDANCE We next evaluate TAG in multi-conditional settings, where naively combining multiple guidance terms can induce severe off-manifold errors. Extending to multiple conditions is nontrivial, as naive approaches demand combinatorial training or multiple specialized time predictors, motivating more efficient approach. Multi-condition reparametrization For multiple conditions ci with corresponding predictors Ai and losses ℓi, we write pt(xt c1, c2) pt(xt) p(c1 xt) p(c2 xt, c1) p(t xt, c1, c2). (14) Although multi-condition time predictor ϕ(xt, c1, c2) is possible, it is often impractical; instead, via single-condition reparameterization, we approximate p(t xt, c1, c2) p(t t, c2) by xt η2 (15) ) where ˆx0 = E[x0 xt]. detailed derivation is provided in Proposition B.1. For an unconditional time predictor, we iteratively incorporate each condition: xt, c1, c2] xt η2 xtℓ1(A1(xt), c1) η2 xtℓ1(A1(ˆx0), c1), xtℓ2(A2(x ), c2), E[x (16) where conditions while remaining efficient. The formal details are provided in Proposition B.2. reflects c1, leading to p(t xt, c1, c2) p(t t) and naturally extending to more Setup We consider molecule-generation tasks with (i) α, µ, (ii) Cv, µ, and (iii) all six molecular properties (α, µ, Cv, ϵHOMO, ϵLUMO, ϵ), along with CelebA (Gender+Age, Gender+Hair). We follow the TFG framework (Ye et al., 2024) to combine these conditions and compare three time-predictor variantsmulti, single, and unconditional as introduced in Sec. 3.2. (Refer to Appendix E.3 for setting details). Result As shown in Table 5, TAG significantly outperforms the baseline combination of independent guidance for all tasks. Notably, single and unconditional time predictors match or exceed multiconditional performance, indicating that explicit training of multi-conditional time predictor is not strictly necessary, and TAG can achieve effective multi-conditional guidance."
        },
        {
            "title": "4.3 FEW-STEP GENERATION",
            "content": "We evaluate TAG in widely-used accelerated sampling, where diffusion models skip timesteps to reduce computation but risk larger discretization errors. We compare standard DDIM sampler (Song et al., 2021a) with TAG for various step counts. As shown in Table 4, TAG consistently boosts sample quality, particularly under fewer steps. Notably, in an extreme single-step scenario on CIFAR10  (Table 17)  , TAG lowers FID by 41.1%. This aligns with our theoretical analysis indicating stronger negative guidance helps the sample escape incorrect manifolds. While one can analytically reduce discretization error (Karras et al., 2022), our focus is on treating it as external noise and demonstrating how TAG mitigates off-manifold drift in practice (see Appendix B.5 for further discussion)."
        },
        {
            "title": "4.4 LARGE-SCALE TEXT-TO-IMAGE GENERATION",
            "content": "We further evaluate TAG on large-scale text-to-image generations by integrating it into models based on Stable Diffusion v1.5 (Rombach et al., 2022), demonstrating its effectiveness on more practical generative tasks. Further details of the experimental setup are provided in Appendix E.5. Enhanced Reward Alignment We integrate TAG into DAS (Kim et al., 2025)a state-of-theart test-time sampler that optimizes text-to-image generation under explicit reward functions (e.g., Aesthetic score (Schuhmann et al., 2022) or CLIP score (Radford et al., 2021)). First, we follow Kim et al. (2025) to evaluate reward alignment using simple animal prompts and an Aesthetic target score. Next, we switch to CLIP-based reward and the HPSv2 prompt set (Wu et al., 2023). Finally, we evaluate multi-objective scenario where the target reward is linear combination of the Aesthetic and CLIP scores with HPSv2 prompt dataset. In each setting, we compare the original DAS sampler against DAS enhanced with TAG (DAS+TAG) on 256 randomly selected prompts. As shown in Table 6, adding TAG substantially increases the final reward while reducing the average Time-Gap (Def. F.1) which measures off-manifold deviation, confirming TAGs stabilization capability in practical, large-scale alignment scenario. Table 6: TAG enhances reward alignment with signle objective DAS, multi-objective DAS and Style Transfer on SD v1.5. Higher reward scores and lower Time-Gap (TG) are better. Method Single-objective DAS Multi-objective DAS Method Aesthetic TG CLIP TG Aesthetic CLIP TG Style Transfer Style Score TG DAS (Kim et al., 2025) DAS + TAG 7.948 9.087 90.04 28.84 0.389 0. 20.73 11.62 8.107 8.572 0.439 0.463 20.73 9.765 TFG (Ye et al., 2024) TFG + TAG 4.82 3. 80.6 23.6 Improved Style Transfer We also apply TAG to style transfer task building on TFG (Ye et al., 2024). Specifically, we combine text prompts (Partiprompts (Yu et al., 2022)) and reference style images (WikiArt (Yu et al., 2022)) via CLIP-based (Radford et al., 2021) Gram matrix alignment. Table 6 compares TFG alone with TFG+TAG, reporting Style Score and Time-Gap. Integrating TAG yields sizable drop in Style Score and substantially reduces the Time-Gap, indicating more faithful style adherence and fewer off-manifold deviations. 4.5 ABLATION STUDY We also probe how the time predictors training steps influence off-manifold correction, exploring the effect of different guidance strengths under added noise, verifying that TAGs gains persist when scaling to 50k samples, and analyzing how the Time-Gap metric correlates with standard image quality scores. Detailed analyses of predictor robustness, hyperparameter sensitivity, and additional baseline comparisons are in Appendix EF."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORKS",
            "content": "In this work, we identify when off-manifold phenomenon happen in diffusion models by measuring Time-Gap using time prediction mechanism. To reduce time gap, we introduce Temporal Alignment Guidance (TAG) as novel guidance mechanism to force the samples to desired manifold"
        },
        {
            "title": "Preprint",
            "content": "in each timestep. Our experimental results demonstrates TAG can significantly reduce this offmanifold phenomenon in multiple scenarios which shows the robustness of our method. We believe our method could be especially effective when applied to real-world downstream tasks where desired condition can vary in real-time. For future work, it would be promising to investigate the effect of TAG in another domains such as in reinforcement learning tasks Janner et al. (2022), discrete diffusion models Austin et al. (2021); Chen et al. (2023c)."
        },
        {
            "title": "REFERENCES",
            "content": "Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313326, 1982. 2 Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=h7-XixPCAL. 1, 10 Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Roni Sengupta, Micah Goldblum, Jonas Geiping, In The Twelfth International and Tom Goldstein. Universal guidance for diffusion models. Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=pzpWBbnwiJ. 1, 34 Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energyguided SDE for inverse molecular design. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=r0otLtOwYW. 41 Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. MultiDiffusion: Fusing diffusion paths for controlled image generation. In Proceedings of the 40th International Conference on Machine Learning, pp. 17371752, 2023. URL https://proceedings.mlr.press/v202/ bar-tal23a.html. 34, Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative modeling. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=oYIjw37pTP. 34 Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=r1lUOzWCW. 41 Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypotheISSN 2835-8856. URL https: sis. Transactions on Machine Learning Research, 2022. //openreview.net/forum?id=MhK5aXo3gB. Expert Certification. 24 Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pp. 46724712. PMLR, 2023a. URL https://proceedings.mlr. press/v202/chen23o.html. 3 Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: In The Eleventh International Conference on Learning Representations, 2023b. URL https: //openreview.net/forum?id=zyLVMgsZ0U_. 1, 6, 27, theory for diffusion models with minimal data assumptions. Ting Chen, Ruixiang ZHANG, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. In The Eleventh International Conference on Learning Representations, 2023c. URL https://openreview.net/forum?id=3itjR9QxFw. 10 Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=OnD9zGAGT0k. 1, 3, 4, 7, 23, 24, 34, 40 Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=1vmSEVL19f. 36 Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. 27 Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= AAWuCvzaVt. 1, 4, 7, 21,"
        },
        {
            "title": "Preprint",
            "content": "Yilun Du, Conor Durkan, Robin Strudel, Joshua Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, In Inrecycle: Compositional generation with energy-based diffusion models and mcmc. ternational conference on machine learning, pp. 84898510. PMLR, 2023. URL https: //proceedings.mlr.press/v202/du23a.html. 3 Yilun Du, Jiayuan Mao, and Joshua B. Tenenbaum. Learning iterative reasoning through energy diffusion. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=CduFAALvGe. 36 Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. 3, 22, 23, 24, 36 Jeremy Elson, John Douceur, Jon Howell, and Jared Saul. Asirra: captcha that exploits interestaligned manual image categorization. CCS, 7:366374, 2007. 7, Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=8OTPepXzeh. 34, 36 Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plugand-play priors. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=yhlMZ3iR7Pu. 1, 34 Yingqing Guo, Hui Yuan, Yukang Yang, Minshuo Chen, and Mengdi Wang. Gradient guidance for diffusion models: An optimization perspective. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=X1QeUYBXke. 1 Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, and Dongsheng Li. Trainingfree multi-objective diffusion model for 3d molecule generation. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=X41c4uB4k0. 28 Yinbin Han, Meisam Razaviyayn, and Renyuan Xu. Neural network-based score estimation in diffusion models: Optimization and generalization. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=h8GeqOxtd4. 4 Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, WeiHsiang Liao, Yuki Mitsufuji, Zico Kolter, Ruslan Salakhutdinov, and Stefano Ermon. Manifold preserving guided diffusion. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=o3BxOLoxm1. 24, Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibURL http://papers.nips.cc/paper/ rium. 7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium. 39 In NIPS, pp. 66296640, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview. net/forum?id=qw8AKxfYbI. 1, 21 Jonathan Ho, Ajay Jain, models. 2020. 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html. 21, 22 Denoising diffusion probabilistic in processing 33:68406851, https://proceedings.neurips.cc/paper/2020/hash/ and Pieter Abbeel. information Advances URL systems, neural Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. URL http://jmlr.org/papers/v23/21-0635.html."
        },
        {
            "title": "Preprint",
            "content": "Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion In International conference on machine learning, pp. 8867 for molecule generation in 3d. 8887. PMLR, 2022. URL https://proceedings.mlr.press/v162/hoogeboom22a. html. 1, 7, 41, 42 Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. 21 Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 99029915. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/janner22a.html. 10 Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 694711. Springer, 2016. 45 Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokkerplanck equation. SIAM journal on mathematical analysis, 29(1):117, 1998. 6, 30 Hojung Jung, Youngrok Park, Laura Schmid, Jaehyeong Jo, Dongkyu Lee, Bongsang Kim, Se-Young Yun, and Jinwoo Shin. Conditional synthesis of 3d molecules with time correction sampler. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=gipFTlvfF1. 4, 7, 34, 35, 36, 43 Khaled Kahouli, Stefaan Simon Pierre Hessmann, Klaus-Robert Müller, Shinichi Nakajima, Stefan Gugler, and Niklas Wolf Andreas Gebauer. Molecular relaxation by reverse diffusion with time step prediction. Machine Learning: Science and Technology, 5(3):035038, August 2024. ISSN 2632-2153. doi: 10.1088/2632-2153/ad652c. URL http://dx.doi.org/10.1088/ 2632-2153/ad652c. 34, 44 Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. Springer Science & Business Media, 1991. 27 Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hk99zCeAb. 7, 37, 38, Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=k7FuTOWMOc7. 9, 26 Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. Interspeech 2019, 2018. 42 Beomsu Kim and Jong Chul Ye. Denoising MCMC for accelerating diffusion-based generative models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1695516977. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/kim23z.html. 34 Sunwoo Kim, Minkyu Kim, and Dongmin Park. Test-time alignment of diffusion models without reward over-optimization. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=vi3DjUhFVm. 9, 36, 44 Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J. 1,"
        },
        {
            "title": "Preprint",
            "content": "Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion modIn International Conference on Machine Learning, els for black-box optimization. pp. 1784217857. PMLR, 2023. URL https://proceedings.mlr.press/v202/ krishnamoorthy23a.html. 1 Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 39 Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and Marie-Francine Moens. Alleviating exposure bias in diffusion models through sampling with shifted time steps. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=ZSD3MloKe6. 7, 34, 35 Tiancheng Li, Weijian Luo, Zhiyang Chen, Liyuan Ma, and Guo-Jun Qi. Self-guidance: Boosting flow and diffusion generation on their own. CoRR, abs/2412.05827, 2024b. URL https: //doi.org/10.48550/arXiv.2412.05827. 7, 34, 35 Yangming Li and Mihaela van der Schaar. On error propagation of diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=RtAct1E2zS. 3, Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. Sora: review on background, technology, limitations, and opportunities of large vision models. CoRR, abs/2402.17177, 2024. URL https://doi.org/10.48550/arXiv.2402.17177. 1 Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=2uAaGwlP_V. 21, 25, 26 Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. URL https:// openreview.net/forum?id=4vGwQqviud5. 26 Eloi Moliner and Vesa Välimäki. Diffusion-based audio inpainting. Journal of the Audio Engineering Society, 72(3):100113, March 2024. ISSN 1549-4950. doi: 10.17743/jaes.2022.0129. URL http://dx.doi.org/10.17743/jaes.2022.0129. 42 Eloi Moliner, Jaakko Lehtinen, and Vesa Välimäki. Solving audio inverse problems with diffusion model. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. 42 Meinard Müller. Information retrieval for music and motion. Information Retrieval for Music and Motion, 2007. 42 Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. URL https: //proceedings.mlr.press/v139/nichol21a.html. 7, 37, 39, 48 Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input perturbation reduces exposure bias in diffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2624526265. PMLR, 2329 Jul 2023. URL https://proceedings. mlr.press/v202/ning23a.html. 7, 34, 35 Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=xEJMoj1SpX. 7, 34,"
        },
        {
            "title": "Preprint",
            "content": "Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning, pp. 2651726582. PMLR, 2023. URL https://proceedings.mlr.press/v202/oko23a.html. 1, 3, 6, 27, 33 Bernt Øksendal. Stochastic differential equations. Springer, 2003. 21 Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam S. Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali K. Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dmitry Vengertsev, Edgar Schönfeld, Elliot Blanchard, Felix JuefeiXu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. CoRR, abs/2410.13720, 2024. URL https: //doi.org/10.48550/arXiv.2410.13720. 1 Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pp. 85998608. PMLR, 2021. URL https://proceedings.mlr.press/v139/ popov21a.html. 1, 7 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748 8763. PMLR, 2021. URL https://proceedings.mlr.press/v139/radford21a. html. 9, 44 Raghunathan Ramakrishnan, Pavlo Dral, Matthias Rupp, and Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):17, 2014. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. the High-resolution image synthesis with latent diffusion models. IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. URL https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_ High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_ CVPR_2022_paper.html. 1, 9, 44, 45 In Proceedings of Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. RB-modulation: Training-free stylization using reference-based modulation. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=bnINPG5A32. 36 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 39, 44 Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M. Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. CoRR, abs/2407.02687, 2024. URL https://doi.org/10.48550/arXiv.2407.02687. 7, 34, 35 Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):47134726, 2022."
        },
        {
            "title": "Preprint",
            "content": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. URL https://openreview.net/forum?id=WNzy9bRDvG. 37, 38 Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600, 2021. 34 Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. (n) equivariant graph neural networks. In International conference on machine learning, pp. 93239332. PMLR, 2021. URL https://proceedings.mlr.press/v139/satorras21a.html. 41, 43 Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In CVPR, pp. 2252222531, 2023. URL https://doi.org/10.1109/CVPR52729.2023.02157. 3 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY. 9, Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In ICASSP, pp. 47794783, 2018. URL https://doi.org/10.1109/ICASSP.2018. 8461368. 42 Yifei Shen, XINYANG JIANG, Yifan Yang, Yezhen Wang, Dongqi Han, and Dongsheng Li. In The ThirtyUnderstanding and improving training-free loss-based diffusion guidance. eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=Eu80DGuOcs. 1, 3 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. URL https://openreview.net/ forum?id=St1giarCHLP. 9, 20, 21 Yang Song and Prafulla Dhariwal. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=WNzy9bRDvG. 45 Improved techniques for training consistency models. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. URL https://openreview. net/forum?id=B1lcYrBgLH. 1, 21, 34, 36 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum? id=PxTIG12RRHS. 3, 5, 21, 22, 36 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3221132252. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/song23a.html. 21 Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control, 2025. URL https://openreview.net/ forum?id=pfS4D6RWC8. 34, 36 Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011."
        },
        {
            "title": "Preprint",
            "content": "Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. URL https://openaccess.thecvf. com/content/CVPR2024/papers/Wallace_Diffusion_Model_Alignment_ Using_Direct_Preference_Optimization_CVPR_2024_paper.pdf. 34 Wei Wang, Dongqi Han, Xufang Luo, Yifei Shen, Charles Ling, Boyu Wang, and Dongsheng Li. Toward open-ended embodied tasks solving, 2024. URL https://openreview.net/ forum?id=vsW5vJqBuv. 1 Chen Wei, Jiachen Zou, Dietmar Heinke, and Quanying Liu. Cocog-2: Controllable generation of visual stimuli for understanding human concept representation. CoRR, abs/2407.14949, 2024. URL https://doi.org/10.48550/arXiv.2407.14949. 1 Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of textto-image synthesis. CoRR, abs/2306.09341, 2023. URL https://doi.org/10.48550/ arXiv.2306.09341. 9, 44 Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Deli Zhao, Ran Yi, Wenping Wang, and Yong-Jin Liu. Towards more accurate diffusion model acceleration with timestep tuner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5736 5745, 2024. URL https://openaccess.thecvf.com/content/CVPR2024/html/ Xia_Towards_More_Accurate_Diffusion_Model_Acceleration_with_A_ Timestep_Tuner_CVPR_2024_paper.html. 34 Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pp. 3859238610. PMLR, 2023. URL https://proceedings.mlr.press/v202/xu23n. html. Shahar Yadin, Noam Elata, and Tomer Michaeli. Classification diffusion models: Revitalizing density ratio estimation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=d99yCfOnwK. 34, 35, 36 Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, and Stefano Ermon. TFG: Unified training-free guidance for diffusion models. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=N8YbGX98vc. 3, 4, 7, 8, 9, 34, 36, 39, 41, 42, 43, 45, 49 Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. 2023. 36 Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=AFDcYJKhND. Featured Certification. 9, 45 Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Trainingfree energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2317423184, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-toIn Proceedings of the IEEE/CVF international conference on image diffusion models. computer vision, pp. 38363847, 2023. URL https://openaccess.thecvf.com/ content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_ Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf. 1,"
        },
        {
            "title": "Preprint",
            "content": "Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=Loek7hfb46P. 4 Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The In Proceedings of unreasonable effectiveness of deep features as perceptual metric. the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. URL https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf."
        },
        {
            "title": "TABLE OF CONTENTS",
            "content": "1 Introduction 2 Off-manifold Phenomenon in Diffusion Models 3 Method"
        },
        {
            "title": "3.1 Temporal Alignment Guidance (TAG)",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . 3.2 Improving guidance with TAG . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Understanding TAG under Corrupted reverse process . . . . . . . . . . . . . . . .",
            "content": "4 Experiments 4.1 TFG Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Multi-conditional guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Few-Step Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Large-scale Text-to-Image Generation . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion and Future Works Broader Impact and Limitations Further background B.1 Diffusion models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Score based diffusion model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Training-free guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Manifold assumption . B.5 Few step generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Mathematical Derivations C.1 Upper bound by external drift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of Proposition B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Proof of Proposition B.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Continuous time limit of TAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 Proof of Proposition 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.7 Formal version of Theorem 3.5 with its proof . . . . . . . . . . . . . . . . . . . . Relation to Prior Works D.1 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Comparison with baseline methods . . . . . . . . . . . . . . . . . . . . . . . . . . 19 1 3 3 4 5 6 7 8 9 9 9 20 20 20 21 22 25 27 27 28 28 30 30 30 34"
        },
        {
            "title": "Preprint",
            "content": "E Implementation Details E.1 Toy experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Corrupted reverse process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Training-Free Guidance Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . E.3.1 Label guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.2 Guassian deblurring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.3 Super-resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.4 Multi-Conditional Guidance . . . . . . . . . . . . . . . . . . . . . . . . . E.3.5 Molecular generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.6 Audio generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.7 Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Time Predictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Large-scale Text-to-Image Generation . . . . . . . . . . . . . . . . . . . . . . . . Ablation Studies F.1 Few Step Unconditional Generation . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Time Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Visualizations of Generated Samples"
        },
        {
            "title": "A BROADER IMPACT AND LIMITATIONS",
            "content": "37 37 37 39 39 40 40 41 42 43 44 45 45 46 48 Broader impact Our algorithm improves the sample quality of diffusion models. While beneficial for applications like image generation or drug discovery, this also carries the risk of misuse common to generative models, potentially enabling harmful generation of images (e.g., disinformation), molecules (e.g., unsafe compounds), or audio. Developing stronger safeguard mechanisms within generative systems, including diffusion models, is essential to counteract such potential negative societal impacts. Limitations In our experiments, we noticed that once sample fidelity reaches high level, further narrowing the time-gap yields only marginal or no improvements in quality. Although our existing time-predictor training procedure is sufficient to demonstrate TAGs practical benefits (see Section 4), we anticipate that more sophisticated predictor architectures could unlock additional gains. We leave this exploration to future work. Usage of Large Language Models We utilized large language model to aid in polishing the writing and improving the clarity of this manuscript. The models role was strictly limited to assistance with grammar, phrasing, and style. All scientific ideas, methodologies, experimental results, and conclusions presented in this paper are the original work of the authors."
        },
        {
            "title": "B FURTHER BACKGROUND",
            "content": "In this section, we introduce more background of the key concepts used in this work. B.1 DIFFUSION MODELS Diffusion Models Diffusion models are generative models that sample from the data distribution, denoted as x0 qdata. Following the stochastic differential equation (SDE) framework (Song et al., 2021a), the forward diffusion process can be defined by the following SDE:"
        },
        {
            "title": "Preprint",
            "content": "dx = (x, t)dt + g(t)dwt, (17) where wt is standard wiener process (Øksendal, 2003). Ideally, if we denote qt(x) as the marginal distribution of the forward process in equation 17, it becomes close to (0, I) when goes to large enough . Then, diffusion model θ is trained to learn how to denoise noisy data by learning score function which is done by minimizing the following objective function (Song & Ermon, 2019; Song et al., 2021b): L(θ) = Et,x0λ(t)sθ(xt, t) xt log qt(xtx0)2 2, where is uniformly sampled from [0, ], xt denotes at timestep in equation 17, and λ(t) is weight parameter usually set to be constant Ho et al. (2020). (18) Conditional Diffusion Model The aim of conditional diffusion models is to sample from the conditional posterior p0(xc) with given condition c. This is achieved by learning conditional score function log qt(xc). Using Bayes rule the conditional score can be re-expressed as: log qt(xc) = log qt(x) + log qt(cx). (19) One could obtain log qt(cx) with auxiliary classifier (Dhariwal & Nichol, 2021) (classifier guidance), or train with condition-labeled data (Ho & Salimans, 2021) (classifier-free guidance) B.2 SCORE BASED DIFFUSION MODEL Here, we systematically present different forms of forward and reverse diffusion model processes and their types in the existing literature. Denoising score matching Learning score function log p(x) perfectly for all can ideally guide the sample towards high density region Hyvärinen & Dayan (2005). However, Song & Ermon (2019) suggests that neural network struggles to accurately model low density region. One alternative is use denoising score matchng (Vincent, 2011; Song & Ermon, 2019) where neural network instead models score function of perturbed dataset log pt(xt) where xt (x, σ(t)2 I). SDE framework Song et al. (2021b) define the forward and reverse process of diffusion model by the following form of stochastic differential equation (SDE). dx = (x, t)dt + g(t)dwt, (20) where wt is standard wiener process. Two types of SDE is widely used in current diffusion models, one is variance preserving SDE (VP-SDE) which has following form: dx = (cid:112)σ(t)σ(t) dwt, (21) where σ(t) is noise schedule as in Song & Ermon (2019). The other is variance exploding SDE (VE-SDE) which has following form: dxt = 1 β(t)x dt + (cid:112)β(t) dwt, (22) where β(t) is another noise schedule. ODE framework Reverse process of SDE in Eq. 1 has its corresponding ODE with same marginal probability density which is called probability flow ODE Song et al. (2021b): (cid:20) dx = (x) 1 2 g2(t)x log qt(x) (cid:21) dt. (23) discretized version of the PF-ODE sampler can be interpreted as DDIM sampling Song et al. (2021a). This ODE formulation can be leveraged to skip network evaluation, enabling faster inference time of diffusion models (Lu et al., 2022; Song et al., 2023)."
        },
        {
            "title": "Preprint",
            "content": "Connection to DDPM Here we offer the relationship between different frameworks for convenience. Song et al. (2021b) unified denoising score matching with DDPM Ho et al. (2020) by viewing forward process of DDPM as discretized version of VP-SDE in Eq. 21. In DDPM Ho et al. (2020), forward noise schedule is defined by xt = 1 αtϵ where ϵ is random noise from (0, I). This is discretized version of VP-SDE in Eq. 21 Song et al. (2021b), where notations have following relations: αtx0 + αt = exp( 1 (cid:90) 0 β(s) ds). (24) In DDPM, model output is denoted as ϵθ(x,t) which has following relationship with score function xt log pt(xt): xt log pt(xt) = 1 1 αt ϵθ(xt). (25) Unless otherwise stated, this work utilizes VP-SDE diffusion process with DDIM sampling. B.3 TRAINING-FREE GUIDANCE Training free guidance leverages clean estimates x0 during the reverse process. Specifically, Tweedies formula Efron (2011) is used to estimate original data during the reverse diffusion process. For VE-SDE, this can be represented as: ˆx0 := E[x0xt] = xt + (1 αt)xt log p(xt) αt . (26) (cid:82) 0 β(s)ds by Eq. 24. And for VE-SDE in Eq. 22, estimation of ˆx0 can be represented where αt = 1 as 2 ˆx0 := E[x0xt] = xt + σ2(t)xt log pt(xt). ˆx0, conditional probability for the target condition can be obtained as p(c ˆx0) exp(cid:0)ℓc(A(ˆx0), c)(cid:1), (27) (28) where denotes classifier or an analytic function that outputs condition given the clean estimate ˆx0 and ℓc : measures the discrepancy between the estimated property and the target property which is usually heuristically chosen function. Now conditional score function in Eq. 19 can be approximated by xt log pt(cxt) = xt log Ep(x0xt) (cid:2)exp(cid:0)ℓc(A(ˆx0))(cid:3) xt ˆx0 ˆx0 (ℓc(A(ˆx0)), (29) where we use chain-rule and the Tweedies formula. Extended view by TAG One can view applying TAG with Training Free Guidance as an extended framework. Denote ϕ : [0, ] as time predictor mapping noisy samples xt and conditions to plausible time indices [0, ]. The corresponding likelihood of having correct time becomes, p(t xt, c) exp(cid:0)ℓt(ϕ(xt, c), t)(cid:1), where ℓt : [0, ] is loss function that quantifies the difference between estimated time and the desired time. (30) With the extended view of adding time information as another condition, we can approximate the conditional distribution pt(xt c) as: pt(xt c) pt(xt) p(c xt) p(t xt, c), (31)"
        },
        {
            "title": "Preprint",
            "content": "where pt(xt) is from the pre-trained unconditional diffusion model. However, we only have access to p(c x0) and p(t xt, c). To bridge x0 and xt, we replace x0 with its denoised estimate ˆx0 = E[x0 xt]. This gives: p(c xt) exp(cid:0)ℓc(A(ˆx0), c)(cid:1). (32) To further align xt to the temporal manifold, we reparameterize xt as and write, xt ηtxtℓc(A(ˆx0), c) p(t xt, c) exp(cid:0)ℓt(ϕ(x t, c), t)(cid:1). Consequently, the approximated conditional distribution becomes, pt(xt c) pt(xt) exp(cid:0)ℓc(A(ˆx0), c)(cid:1) exp(cid:0)ℓt(ϕ(x t, c), t)(cid:1). (33) (34) If ϵθ(xt, t) σtxt log pt(xt) represents the unconditioned diffusion score, the new guided score for single-condition TAG is given by, ϵθ(xt, c, t) = ϵθ(xt, t) σtxtℓc(A(ˆx0), c) σtxtℓt(ϕ(x t, c), t). (35) In practice, one updates xt aligned with both the property and the correct time t, mitigating off-manifold drifting. before applying ℓt, ensuring that each sampling step remains Muti-conditional TAG Let c1 Y1, c2 Y2 be the target property value, and let A1, A2 : be property classifiers that map samples x0 to their respective predicted property values. To sample from the conditional distribution pt(xt c1, c2), we factorize, pt(xt c1, c2) pt(xt)p(c1 xt)p(c2 xt, c1)p(t xt, c1, c2), (36) where p(t xt, c1, c2) ensures ensures alignment of xt to the temporal manifold under c1 and c2. straightforward method is to directly model p(t xt, c1, c2) via multi-condition time predictor ϕ(xt, c1, c2): p(t xt, c1, c2) exp(cid:0)ℓt(ϕ(xt, c1, c2), t)(cid:1). While this method fully accounts for multi-condition effects, it requires training separate model for every condition combination, which becomes infeasible for complex or high-dimensional conditions. (37) To address this challenge, we employ single-condition time predictor ϕ(xt, c) that models p(t xt, c) for single condition c. In this case, we approximate p(t xt, c1, c2) by re-parameterizing xt to reflect c1. Proposition B.1. Let tribution p(xt be latent variable conditioned on xt and target property c1, with prior dist I). Given first-order approximation of the property likelihood: t, c1) (x t, η2 p(c1 t) exp (cid:0)ℓ1(A1(xt), c1) (x xt)xtℓ1(A1(xt), c1)(cid:1) , the posterior expectation of under p(x txt,c1)[x xt, c1) satisfies: t] = xt η2 tp(x Ex xtℓ1(A1(xt), c1). (38) (39) Proof. See Appendix C.2 Practically, Using Tweedies formula Efron (2011); Chung et al. (2023), we replace A1(xt) with A1(ˆx0), where ˆx0 = E[x0 xt] is the denoised estimate. Thus we have an approximation: xt η2 xtℓ1(A1(ˆx0), c1). (40) As result of Proposition B.1, the single-condition time predictor allows us to approximate p(t xt, c1, c2) by reparameterizing xt to reflect the influence of c1, yielding, p(txt, c1, c2) p(tx t, c2),"
        },
        {
            "title": "Preprint",
            "content": "where xtℓ1(A1(xt), c1). This reparameterization ensures that partially aligns with c1, reducing the approximation error when conditioning on c2 (see Algorithms 1 for implementation). = xt η2 We could further extend this framework to the case of an unconditional time predictor ϕ(xt), which models p(t xt) without explicit dependence on any condition. This extension significantly reduces the computational cost of training by requiring only single predictor for all possible conditions, relying on additional approximations of p(t xt, c1, c2) to capture the influence of c1 and c2 within the unconditional framework. Proposition B.2. Let priors: be latent variable conditioned on xt and target properties c1, c2, with p(xt p(x t, c1, c2) (x , c1) (x t, η2 , η2 I), I), (41) where satisfies: are intermediate samples reflecting c1 before updating c2. The posterior expectation Ex tp(x txt,c1,c2)[x t] = xt η2 ℓ1(A1(xt), c1) η2 ℓ2(A2(x ), c2). (42) Proof. See Appendix C.3 Again, in practical scenarios using Tweedies formula Efron (2011); Chung et al. (2023), we replace A1(xt) and A2(x t) with denoised estimates: (cid:0)A2 0 = E[x0 xt η2 ℓ where ˆx ℓ1(A1(xt), c1) ℓ1(A1(ˆx0), c1), η2 ℓ1(A1(xt), c1)(cid:1) , c2 (cid:0)x ℓ1(A1(ˆx0), c1)]. Substituting these approximations gives: (cid:1) ℓ2(A2(ˆx 0), c2), xt η2 ℓ1(A1(ˆx0), c1) η2 ℓ2(A2(ˆx 0), c2). (43) (44) The unconditional time predictor incorporates the influences of c1 and c2 by sequentially reparameterizing xt through iterative updates. This approach leverages reparameterization steps that align xt to the conditions c1 and c2, reducing the approximation gap to the true conditional distribution. The framework naturally extends to handle > 2 conditions, iteratively integrating each condition while maintaining computational efficiency (see Algorithms 2 for implementation). Pseudo-Code We provide the pseudo-code for implementing multi-conditional guidance using single-conditional (B.1) time predictor and an unconditional time predictor (B.2) in Alg. 1 and Alg. 2, respectively. B.4 MANIFOLD ASSUMPTION Ideally, even if original data manifold M0 can be low-dimensional object as pointed out in several works (Bortoli, 2022; He et al., 2024), with noise added from forward process in Eq. 17, pt(xt) > 0 for all xt where denotes the data domain. Since our motivation of off-manifold phenomenon happens in low-density region, we redefine the target data manifold for each timestep by the following definition. Definition B.3. Let ϵt > 0 be some threshold. The correct manifold at timestep is defined as Mt = {x : pt(x) ϵt}, (45) where is domain of the data. In other words, Mt consists of all points in whose probability density is at least ϵt. With above definition, we can formally define the off-manifold in diffusion models. Definition B.4. For given timestep in reverse diffusion process in Eq. 1, we define off-manifold phenomenon by xt becomes out of the correct manifold Mt defined in Definition B.3. In other words: xt / Mt. (46) We leave further theoretical understanding of off-manifold phenomenon from the above definition as future work."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1: DDIM Sampling with Single-Conditional Time Predictor Input :Unconditional score model xt log pt(xt), property classifier A1 : R, loss function ℓ1 : R, single-condition time predictor τ (xt, c), operator G, target properties c1, c2, guidance strength ρt, temporal alignment strength ωt, time steps . Output :Conditional sample x0. 1 Initialize xT (0, I); 2 for = T, . . . , 1 do to reflect c1: Compute ˆx0 xt+(1αt)xt log pt(xt) ; Reparameterize xt η2 Compute temporal alignment term using τ (x Define the generalized guidance operator G(xt, c1, c2) to compute joint or independent guidance contributions; ℓ1(A1(ˆx0), c1); t, c2): xtℓt(τ (x t, c2), t); αt (cid:16) xt 1αtxt log pt(xt) (cid:17) αt1 xt1 αt ρtG(xt, c1, c2) + ωtT + σtϵt. + (cid:112)1 αt1 σ xt log pt(xt) + 8 return x0; Algorithm 2: DDIM Sampling with Unconditional Time Predictor Input :Unconditional score model xt log pt(xt), property classifiers A1 : R, A2 : R, loss functions ℓ1, ℓ2 : R, unconditional time predictor τ (xt), operator G, target properties c1, c2, guidance strength ρt, temporal alignment strength ωt, time steps . Output :Conditional sample x0. 1 Initialize xT (0, I); 2 for = T, . . . , 1 do Compute ˆx0 xt+(1αt)xt log pt(xt) to reflect c1: Reparameterize x t+(1αt)x αt log pt(x t) αt ; xt η2 ; 0 to reflect c2: Compute ˆx Reparameterize η2 Compute temporal alignment term using τ (x Define the generalized guidance operator G(xt, c1, c2) to compute joint or independent guidance contributions; ℓ2(A2(ˆx ): xtℓt(τ (x 0), c2); ), t); ℓ1(A1(ˆx0), c1); (cid:16) xt 1αtxt log pt(xt) (cid:17) αt1 xt1 αt ρtG(xt, c1, c2) + ωtT + σtϵt. + (cid:112)1 αt1 σ2 xt log pt(xt) + 3 4 5 7 3 4 5 6 8 9 10 return x0; B.5 FEW STEP GENERATION As shown in Lu et al. (2022), PF-ODE in Eq. 23 sends xs at timestep to xt at timestep by solving, xt = (cid:82) (τ )dτ xs + (cid:90) (cid:82) τ (τ )dτ (e g2(τ ) 2στ ϵθ(xτ , τ ))dτ. Here, forward SDE is defined as follows. dxt = (t)xt dt + g2(t) 2σt ϵθ(xt, t) dt, xt (0, σ2 I), (47) (48) which incorporates both VP-SDE and VE-SDE scenarios (Appendix B.2) and (t), g(t) are defined as: (t) = log αt dt , g2(t) = dσ2 dt 2 log αt dt σ2 . (49) After using change of variable λ(t) := log( αt σt ), Lu et al. (2022) show following equation holds:"
        },
        {
            "title": "Preprint",
            "content": "xt = (cid:90) λt xs αt eλˆϵθ(ˆxλ, λ)dλ. (50) αt αs λs Now, from Eq. 50, one can observe how discretization error occurs if we skip the evaluation of the diffusion models for some of timesteps. Note that the discretization errors can be reduced by considering higher-order term in Eq. 50 (Karras et al., 2022; Lu et al., 2022; 2023) where we leave combining TAG with higher order diffusion solver as future work."
        },
        {
            "title": "C MATHEMATICAL DERIVATIONS",
            "content": "C.1 UPPER BOUND BY EXTERNAL DRIFT To analyze the error induced by the random shift, we compare how the samples follow original reverse SDE in equation 1, and the modified SDE in equation 2 differs by the following proposition: Proposition C.1 (Error bound by the drift). Let pt and pt be the probability distribution at time in the original reverse process in equation 1 and in the reverse process with external guidance v(x, c, t) in equation 2, respectively. The total variation distance p0 and p0 can be bounded as follows: (cid:90) (cid:90) d2 (p0, p0) KL(p0, p0) g(t)2pt(x)v(x, c, t)2 2 dx dt. (51) 1 2 0 Proposition C.1 provides an upper bound indicates that external guidance can induce distributional divergence in the worst case, even if the underlying score function for pt(x) is perfectly known. Proof of Proposition C.1 For the ease of analysis, we first redefine the notations. Suppose Yt and Yt be the random variable of backward process of original reverse diffusion process by satisfying YT = xt in Eq. 1 and reverse process with external guidance by satisfying YT = xt in Eq. 2, respectively. This can be restated with following formulations: dYt = (cid:2)f (Yt, t) + g(t)2 log qt(Yt)(cid:3) dt + g(t)dwt, Y0 (0, I) Yt = ( Yt, t) + g(t)2 (cid:16) (cid:104) log qt( Yt) + v( Yt, c, t) (cid:17)(cid:105) dt + g(t)d wt, Y0 (0, I). (52) Also, denote pt and pt be probability distributions of Yt and Yt, respectively and denote path measure of two process by P, P, respectively. Now, the goal is to bound the distance between pT and pT which are final output of two SDE processes. This can be proved by automatic consequence of Girsanovs Theorem (Karatzas & Shreve, 1991). To start, we first define the stochastic process (cid:32) Mt = exp (cid:90) σ(t)1v dwt 1 2 (cid:90) (cid:90) 0 (cid:33) σ(t)2v2dy dt (53) and assume Mt is Martingale. Then, Girsanovs Theorem states that the Radon-Nikodym derivative of with respect to becomes dP = MT dP, and this consequently bounds the KL divergence between two path measures as follows: (cid:90) (cid:90) KL(P, P) = pt(y)σ(t)2v2dydt. 1 2 0 Finally, using data processing inequality and Pinskers inequality together (Cover, 1999), one can obtain: (p0, p0) KL(p0, p0) KL(P, P) = EP d2 (cid:34) 1 2 (cid:90) (cid:90) 0 (cid:35) σ(t)2v2dy dt . (56) It is known that following is sufficient condition for for Mt to be Martingale (Novikovs condition): (cid:34) (cid:32) EP exp 1 2 (cid:90) (cid:90) 0 (cid:33)(cid:35) σ(t)2v2dy dt < , and this can be further relaxed by the following condition: (cid:90) pt(y)σ(t)2v2dy for all and some constant (Chen et al., 2023b). (57) (58) Note that similar analysis has been conducted to prove the convergence rate of diffusion models in (Chen et al., 2023b; Oko et al., 2023) while their analysis does not contain any additional guidance. 27 (54) (55)"
        },
        {
            "title": "Preprint",
            "content": "C.2 PROOF OF PROPOSITION B.1 Proposition B.1 Let tribution p(xt be latent variable conditioned on xt and target property c1, with prior dist I). Given first-order approximation of the property likelihood: t, c1) (x t, η2 p(c1 t) exp (cid:0)ℓ1(A1(xt), c1) (x xt)xtℓ1(A1(xt), c1)(cid:1) , the posterior expectation of under p(x txt,c1)[x xt, c1) satisfies: t] = xt η2 tp(x Ex xtℓ1(A1(xt), c1). (59) (60) Proof. Similar to Han et al. (2024a), which assumes prior on the clean sample estimate given latent variable and applies first-order expansion of the loss function, we assume prior on xt at each t. We model the temporal distribution p(t xt, c1, c2) via property loss function, whereas Han et al. (2024a) models p(c2 ˆx0, c1), with ˆx0 as the clean estimate. (61) (62) (63) (64) (65) (67) (68) The posterior distribution is derived via Bayes rule: xt, c1) p(xt t) 1, the posterior simplifies to: xt, c1) p(xt Assuming flat prior p(x p(x p(x t, c1)p(c1 t). t, c1)p(c1 t)p(x t). The Gaussian prior is given by: p(xt t, c1) exp (cid:18) t2 xt 2η2 (cid:19) . The likelihood p(c1 around xt: t) is approximated using first-order Taylor expansion of ℓ1(A1(x t), c1) ℓ1(A1(x t), c1) ℓ1(A1(xt), c1) + (x xt)xtℓ1(A1(xt), c1). Thus, the likelihood becomes: p(c1 t) exp (cid:0)ℓ1(A1(xt), c1) (x xt)xtℓ1(A1(xt), c1)(cid:1) . Combining the prior and likelihood, the log-posterior is: log p(x xt, c1) t2 xt 2η2 ℓ1(A1(xt), c1) (x xt)xtℓ1(A1(xt), c1). (66) Differentiating the log-posterior with respect to x log p(x xt, c1) = yields: xt η2 xtℓ1(A1(xt), c1). Setting the gradient to zero for the MAP estimate gives: = xt η2 xtℓ1(A1(xt), c1). For Gaussian posteriors, the MAP estimate coincides with the expectation. C.3 PROOF OF PROPOSITION B. Proposition B.2 Let be latent variable conditioned on xt and target properties c1, c2, with priors: t, η2 , η2 t, c1, c2) (x , c1) (x p(xt p(x I), I), (69) where satisfies: are intermediate samples reflecting c1 before updating c2. The posterior expectation Ex tp(x txt,c1,c2)[x t] = xt η ℓ1(A1(xt), c1) η2 ℓ2(A2(x ), c2). (70)"
        },
        {
            "title": "Preprint",
            "content": "p(x Assuming flat priors p(x Proof. The posterior distribution is derived via hierarchical Bayesian inference: t). t)p(x xt, c1, c2) p(xt t) 1 and p(x t, c1, c2)p(c1, c2 ) 1, the model simplifies to: t, c1, c2)p(c1 xt, c1, c2) p(xt p(x t)p(c2 t, c1). The Gaussian prior for p(xt t, c1, c2) is: p(xt t, c1, c2) exp (cid:18) t2 xt 2η2 (cid:19) . (71) (72) (73) The likelihood for c1 is approximated using first-order Taylor expansion of ℓ1(A1(x xt: ℓ1(A1(x t), c1) ℓ1(A1(xt), c1) + (x xt)ℓ1(A1(xt), c1). Thus, the likelihood becomes: p(c1 t) exp (cid:0)ℓ1(A1(xt), c1) (x xt)ℓ1(A1(xt), c1)(cid:1) . t), c1) around For p(c2 t, c1), we introduce an intermediate latent variable 2 (cid:18) p(x x , c1) exp (cid:19) . 2η2 t conditioned on and c1: The likelihood for c2 is approximated using first-order Taylor expansion of ℓ2(A2(x t: ), c2) around ℓ2(A2(x ), c2) ℓ2(A2(x t), c2) + (x t)ℓ2(A2(x t), c2). Substituting = t η2 p(c2 ℓ1(A1(xt), c1) (from Proposition C.2), the likelihood becomes: t, c1) exp (cid:0)ℓ2 ℓ1(A1(xt), c1)(cid:1) , c2 η2 (cid:0)A2 (cid:1)(cid:1) . (cid:0)x (74) (75) (76) (77) (78) Combining the Gaussian prior and the likelihood, the log-posterior is: t2 log p(x xt, c1, c2) ℓ1(A1(xt), c1)(x txt)ℓ1(A1(xt), c1)ℓ2(A2(x xt 2η2 t ), c2). (79) Differentiating with respect to and setting the gradient to zero for the MAP estimate gives: ℓ2(A2(x ℓ1(A1(xt), c1) η2 ), c2). = xt η2 (80) For Gaussian posteriors, the MAP estimate coincides with the expectation, completing the proof. C.4 PROOF OF THEOREM 3.3 For discretized diffusion timesteps [t1, t2, . . . , tn], and with denoting ptot := (cid:80) i-th timestep ti can be represented by rearranging the terms as follows: pj(x), TAG for log p(tix) = log = log (cid:18) p(xti)p(ti) (cid:19) p(xtk)p(tk) (cid:19) (cid:80) (cid:18) pi(x) ptot(x) = = pi(x) pi(x) pi(x) pi(x) ptot(x) ptot(x) xpk(x) ptot (x) (cid:80) = (1 pi(x) ptot(x) )x log pi(x) (cid:88) k=i pk(x) ptot(x) log pk(x) = (cid:88) k=i pk(x) ptot(x) (x log pi(x) log pk(x)) . (81)"
        },
        {
            "title": "Preprint",
            "content": "C.5 CONTINUOUS TIME LIMIT OF TAG Theorem C.2. (Continuous time TAG decomposition) For continuous time diffusion models, TLS score can be decomposed in the following way. log p(tx) = log pt(x) (cid:90) where γs = ps(x) (cid:82) pk(x)dk . Proof. log p(tx) = log = log (cid:82) (cid:18) p(xt)p(t) p(xs)p(s) (cid:19) (cid:18) pt(x) (cid:82) p(xs)ds γsx log ps(x)ds, (82) (cid:19) = xpt(x) pt(x) = log pt(x) (cid:82) xps(x)ds (cid:82) ps(x)ds (cid:90) ps(x) (cid:82) pk(x)dk (83) log ps(x)ds, gives the result. C.6 PROOF OF PROPOSITION 3.4 We restate Proposition 3.4 below for convenience. Proposition C.3. Applying TAG alters energy barrier map Uk(x) = log pk(x) at timestep tk to Φk(x) for any by: Φk(x) = Uk(x) γi Ui(x), (84) (cid:88) where γi = pi(x) ptot(x) for all i. Proof. Denote sk as new score term obtained by applying TAG at timestep tk. Then, from Theorem 3.3, one can see that: (cid:88) sk := γi (sk si) i=k = sk (1 (cid:88) i=k γi)sk (cid:88) i=k γisi, (85) where γi = pi(x) ptot(x) as before. From the definition of the potential Ui(x) = log pk(x) gradient of the Ui equals to the score function si for all i. Integrating both sides of the above equation and noting that the potential Uk is defined up to additive constants, we get the result. C.7 FORMAL VERSION OF THEOREM 3.5 WITH ITS PROOF JKO scheme (Jordan et al., 1998) establishes the foundational argument that the Fokker-Planck equation of the Langevin dynamic is the gradient flow of the KL divergence with respect to the Wasserstein-2 metric. We can leverage this to analyze the convergence guarantee of the modified correction sampling by TAG. We start by defining original and modified Langevin dynamics below. Modified Langevin dynamics Original Langevin dynamics at timestep tk can be stated as, When applying TAG, from Theorem 3.3, Langevin dynamics in each step can be modified as, dyt = sk(yt)dt + 2dWt. dxt = sk(xt) (cid:88) i=k γisi(xt) dt + 30 2dWt. (86) (87)"
        },
        {
            "title": "Preprint",
            "content": "Fokker-Plank equation and gradient flow Proposition C.4. (Fokker-Plank equation) For any smoothly evolving density qt driven by the Langevin dynamics of dxt = v(xt, t)dt + 2dWt, following equation holds: tqt = (qtv) + qt, where denotes Laplacian operator. (88) (89) Theorem C.5. For Langevin dynamics in eq. 88, gradient flow of the KL functional has the following form: dt KL(qtpk) = Eqt log qt pk 2 log qt pk (cid:20) (cid:21) (v log pk) . (90) Proof. Define the mismatch score rk(x, t) = log qt(x) pk(x) . One can observe that, dt KL(qtpk) = = = = (cid:90) (cid:90) (cid:90) (cid:90) (tqt) log qt pk dx [ (qtv) + qt] log qtv log qtv log (cid:90) (cid:90) dx dx qt pk qt pk qt pk dx qt log qt pk dx qt log qt log qt pk dx = Eqt [v rk log qt rk] , (91) where second equality comes from the Proposition C.4, third equality comes from the integration-byparts, and the last equality comes from the definition of rk. Now, from the definition of rk, we can rewrite, log qt = rk + log pk. (92) Putting this into the above result in Eq. 91, we get the result. Above theorem gives exact decreasing rate of KL divergence as shown by the following corollary: Corollary C.6. (Gradient flow of KL divergence) Applying Theorem C.5 to the original Langevin (Eq. 86), we can observe log pk = 0, and from this, the last term in Eq. 90 is canceled out which gives, dt KL(qtpk) = Eqtrk2, (93) Similarly, by applying Proposition C.3, we can obtain decreasing rate of modified Langevin (Eq. 87) as follows: dt KL(qtpk) = Eqt (cid:2)rk2 + A(t)(cid:3) , (94) as before and A(t) = (cid:80) γiEqt [rk(x, t) si(x)] is the extra term from the where rk = log qt pk TAG. Intuitively, if the expectation of A(t) in Eq. 94 is strictly positive, this helps escaping the low-density region faster compared to the original Langevin dynamics. To formalize this, we first define low-density region in the following way. Definition C.7. (Low density region) We say falls into low-density region whenever, Dk,ϵ Dk,ϵ = {pk(x) ϵ}, (95) for some constant ϵ > 0."
        },
        {
            "title": "Preprint",
            "content": "Definition C.8. (Escape time) Define stopping times τ, τ as follows: τ = inf{t 0 : yt = Dk.ϵ}, y0 q0, where qt follows from original Langevin (Eq. 86) and τ = inf{t 0 : xt = Dk.ϵ}, x0 q0, where is from the modified Langevin by TAG (Eq. 87). (96) (97) One can see that τ, τ is the escaping time of the low-desnity region. Thus, lower τ, τ implies faster convergence toward high-density region, meaning accelerated initial convergence speed. This is captured by the following theorem. Theorem C.9. Assume the support of initial distribution q0(x) is inside Dk,ϵ and for all < τ , following equation holds: Eqt (cid:88) γj rk(xt) sj(xt) (cid:12) (cid:12) Dk,ϵ β , β > 0. Moreover, assume the mixture score satisfies (cid:80) γiEqt [si] η for τ . Then, the expectation of the stopping time τ is bounded as, E[τ ] KL(q0pk) (β + β η2 ) , and consequently, tail bound of the escaping probability becomes: Pr(τ t) KL(q0pk) t(β + β η2 ) . Proof. First, from Cauchy-Schwarz inequality, one can observe: Eqt rk(x)2 Eqt (cid:80) Eqt (cid:80) γirk(x) si(x) γisi(x)2 β η2 . As result, gradient flow of KL divergence in Eq. 94 can be upper bounded by, dt KL(qtpk) = Eqt (cid:2)rk2 + A(t)(cid:3) = Eqtrk2 γiEqt[rk si] (cid:88) (β + β η2 ). Now, it is straightforward to see that for = τ and δ := β + β η2 , KL(qt pk) KL(q0pK) δt. From the positiveness of the KL, δt KL(q0pk). Now, taking expectation and sends gives E[τ ] = KL(q0pk) δ , (98) (99) (100) (101) (102) (103) (104) (105) which recovers Eq. 99. Now, form Markovs inequality, Eq. 100 holds. Corollary C.10. Applying TAG can accelerate convergence speed in sense that upper bound of E[τ ] is reduced compared to the upper bound of E[τ ] by the factor of 1 + η2. Moreover, continuous flow of the modified Langevin dynamics until time reduces KL divergence to the target measure by, KL(q0pk) KL(qtpk) tβ(1 + 1 η2 ) (106)"
        },
        {
            "title": "Preprint",
            "content": "The improvement factor 1 + η2 grows over increasing η which implies that if the expectation of the mixture score increases, faster convergence can be guaranteed. This agrees with our intuition that even the qt mostly resides in the low density region of the single timestep distribution pk, pj(x) can be high for some = and thereby contribute to the term η. Assumption C.11. Score approximation error is monotonically decreasing function of the density function pt(x). Specifically, assume for all in the diffusion process, there exist monotonic increasing function ht : R0 R0 with ht > 0 such that following relation holds: Exqtx log pk(x) sθ(x, tk)2 2 = ht (KL (qtpk)) (107) The above assumption implies that if particle deviates far from the true distribution pk, score approximation error increases. This is reasonable to assume in sense that neural network is trained only with the sample from pk and rarely sees the sample from pk(x) 0. With above assumptions, we provide the formal version of the Theorem 3.5. Theorem C.12. (Formal version of Theorem 3.5) Denote pt is reverse process of diffusion in Eq. 2. Given, Assumption C.11 and assumptions in Theorem C.5, the convergence guarantee for small t0 > 0 can be improved by simulating modified Langevin correction in Eq. 87 until time in the following way. dT (pt0, q0) dT (pt0, q0) 4 , where = (T t0) (cid:118) (cid:117) (cid:117) (cid:116)Expt (cid:34) 1 2 (cid:90) t0 g(t)2sθ(x) log pt(x) (cid:35) , 2 dt is the original score approximation error and = mβ(1 + 1 η2 )s (cid:90) g(t)2dt. (108) (109) (110) Proof. For path measure of forward process defined from t0 to and the path measure of the corresponding reverse process P, estimation error is decomposed as E[TV(x0, xt0 )] + E[TV(xT , (0, I)] + TV(P, Q) where first term is the truncation error, second term is initial noise mismatch between forward and reverse process, and the third term is KL divergence between path measures score approximation errors(for discrete sampling, additional discretization error is added as in (Chen et al., 2023b)). Chen et al. (2023b); Oko et al. (2023) show that the third term can be bounded by score approximation errors (please refer to Appendix C.1 and Section 5.2 of (Chen et al., 2023b) for details). Specifically, it can be shown from the Proposition C.1 and triangle inequality, (111) TV(P, Q) (cid:118) (cid:117) (cid:117) (cid:116)Expt (cid:34) 1 2 (cid:90) g(t)2sθ(x) log pt(x)2 (cid:35) 2 dt (cid:118) (cid:117) (cid:117) (cid:116)Expt (cid:34) + 1 2 (cid:90) t0 g(t)2v(x, c, t)2 (cid:35) 2 dt . (112) Now, one can observe from Corollary C.10, reduced score approximation error by simulating modified Langevin (Eq. 87) until time gives, Exqsx log pk(x) sθ(x, tk)2 2 Exq0x log pk(x) sθ(x, tk)2 2 mβ(1 + Now, observing that for two constants f, > 0 and > g, (cid:112)f (cid:112)f = + . 2 1 η2 )s. (113) (114) Putting = Exq0 log pk(x) sθ(x, tk)2 2, = mβ(1 + 1 Eq. 112 by applying Cauchy-Schwarz inequality gives the result. η2 )s into above and combining with"
        },
        {
            "title": "Preprint",
            "content": "Note that for the single discretized Langevin step can be also analyzed similarly for small step-size from the Girasonov theorem."
        },
        {
            "title": "D RELATION TO PRIOR WORKS",
            "content": "D.1 RELATED WORKS External Guidance in Diffusion Models Diffusion models can be leveraged in downstream applications by combining an unconditional diffusion process with external guidancewithout any additional training. Graikos et al. (2022) use off-the-shelf diffusion models to generate samples constrained to specific conditions, demonstrating applications in combinatorial optimization, while Chung et al. (2023) apply similar guidance to solve inverse problems. Bansal et al. (2024) extend this approach to user-specific conditioning in the image domain. TFG (Ye et al., 2024) provide unified training-free guidance framework by consolidating the design space of prior methods, searching for optimal hyperparameter combinations, and establishing benchmarks for training-free guidance. For scenarios involving multiple constraints, MultiDiffusion (Bar-Tal et al., 2023) achieves spatial control by fusing diffusion paths from different prompts. Off-Manifold Phenomenon Diffusion models exhibit exposure bias (Ning et al., 2024), as the reverse process does not match the learned forward process. Ning et al. (2023) reduce exposure bias by randomly perturbing the training data in diffusion models. Ning et al. (2024) show that scaling the vector norm of the diffusion model outputs can alleviate errors, while Li et al. (2024a) identify variance across sample batches to correct the time information in diffusion models. Song & Ermon (2019) explore Langevin dynamicsbased steps that utilize the learned score function for iterative refinement. Li & van der Schaar (2024) theoretically analyze how errors accumulate during the reverse process of diffusion models. Timestep in Diffusion Models Several studies have investigated the impact of timestep information in diffusion models. Xia et al. (2024) optimize timestep embeddings to correct the sampling direction, and San-Roman et al. (2021) demonstrate the effectiveness of adjusting the noise schedule. Kim & Ye (2023) and Kahouli et al. (2024) train neural networks to estimate accurate timestep information, while Jung et al. (2024) leverage time predictor to modify the reverse diffusion schedule and correct the reverse process. Sadat et al. (2024) and Li et al. (2024b) perturb time inputs in the primary score model to derive contrastive signals. Yadin et al. (2024) utilize time classifiers for score function approximation. In contrast, our TAG framework learns dedicated time predictor for p(t xt) and directly leverages its score log p(t x) to provably pull samples back onto their true temporal manifoldyielding both convergence guarantees and empirical gains for the first time. formal proof and discussion are provided in Appendix D. Comparison with other regularization techniques Recent works (Fan et al., 2023; Wallace et al., 2024) show fine-tune diffusion model using the reinforcement learning algorithm can boost the performance of diffusion models. To not diverge from the original diffusion process, they utilize KL regularization technique. However, fine-tuning diffusion models for practical downstream tasks is highly costly where target condition vary in real-time. Another option is to utilize control theory (Berner et al., 2024; Uehara et al., 2025) where the objective is to refine the sample trajectory to the desired reward weighted distribution with the KL regularization term added. However, this usually rely on generating multiple sample trajectories. In contrast to above techniques, our method does not require offline history nor multiple iterations, readily applicable even when target condition changes for every generation. D.2 COMPARISON WITH BASELINE METHODS Here, we elaborate on the detailed discussions of TAG with other relevant literatures that were briefly introduced in Sec. D.1. Early timestep and schedule optimizations Early works exploring the role of time include Xia et al. (2024) optimizing timestep embeddings, and San-Roman et al. (2021) focusing adjusting noise schedules. These approaches typically aim to find globally or locally optimal fixed schedules or input"
        },
        {
            "title": "Preprint",
            "content": "representations for time and generally do not offer sample-adaptive corrections at each step based on the evolving state of x. TAG, in contrast, provides such dynamic, sample-specific correction via its TLS (Eq. 12). This helps the sample adhere to the manifold implied by the schedule at each current timestep by considering the full posterior pϕ(x), thus acting as more flexible and adaptive generalized constraint than pre-defined temporal strategies. Time Correction Sampler TCS (Jung et al., 2024) also employs time predictor. However, TCS uses the predictors output, = arg max ϕ(xt), to directly modify the perceived timestep of the sample xt, subsequently altering the solver step to use sθ(xt, t) and adjusting the noise schedule. This constitutes \"hard\" temporal reassignment. TAG differs significantly by maintaining the solvers current timestep for sθ(xt, t) and instead adding the TLS as an additive correction to the sample itself. The TLS decomposition (Eq. 12) suggests TAGs correction is generalized constraint, as it considers attractive and repulsive forces from all potential timesteps based on pϕ(x), rather than singular reassignment, offering robust means to maintain manifold fidelity; our comparative experiments  (Table 2)  demonstrate TAGs superior performance over TCS. Time perturbation methods TSG (Sadat et al., 2024) and SG (Li et al., 2024b) leverage the score models (sθ) local sensitivity to time by perturbing its time input τ (e.g., τ δτ ) to derive contrastive guidance. In contrast, TAG employs its distinct TLS (Eq. 12) as an additive corrective term, without altering sθs time input. Theorem 3.3 shows that applying TAG has effect of getting negative guidance from all timesteps except the target timestep (i.e, current timestep) which potentially renders TAG more robust than the typically symmetric or single-perturbation strategies of TSG and SG, especially for significantly off-manifold scenarios. Exposure bias While methods like Epsilon Scaling (Ning et al., 2024) and the Time-Shift Sampler (Li et al., 2024a) act during inference, similar to TAG, they typically apply heuristic adjustments (e.g., scaling model outputs or shifting time inputs) to mitigate train-inference mismatch. Other approaches, such as that by Ning et al. (2023), modify the training data itself. TAG differs fundamentally by introducing learned score term the adaptive TLS, log pϕ(tx) which provides active, sample-specific correction based on learned temporal consistency, rather than relying on pre-defined heuristics or training data alterations. We compare TAG against (Ning et al., 2023)  (Table 7)  , (Ning et al., 2024) and (Li et al., 2024a) (in Table 8). TAG demonstrate consistent improvements against all baselines. Classification Diffusion Models While both TAG and CDM (Yadin et al., 2024) uses timestep classifier, the primary purpose of CDM is to estimate the log density of the generative output and approximate the score function in each timestep. Contrary to this, TAG leverages the gradient of time classifier whose purpose is to attract the sample to the desired timestep for reducing offmanifold phenomenon. We notice that Theorem 3.1 in CDM (Yadin et al., 2024) can be deduced from rearranging term in Theorem 3.3 of ours as follows: We begin by noting that in Theorem 3.3, ti is an arbitrary label in {t1, . . . , tn}. In CDM (Yadin et al., 2024) setting, we simply identify ti = and tT +1 = + 1. We now show that log(cid:0)pτ x(t x)(cid:1) log(cid:0)pτ x(T + 1 x)(cid:1) = log p(cid:0)ti x(cid:1) log p(cid:0)tT +1 x(cid:1) = log pi(x) log pT +1(x). Let γk = pk(x) ptot(x) . Then log(cid:0)pτ x(t x)(cid:1) = (cid:88) (cid:16) γk k=i log pi(x) log pk(x) (cid:17) = (cid:0)1 γi (cid:124) (cid:1) log pi(x) (cid:125) (cid:123)(cid:122) k=i γk log pi(x) (cid:80) (cid:88) (cid:124) γk log pk(x) + γi log pi(x), (cid:123)(cid:122) () (cid:125)"
        },
        {
            "title": "Preprint",
            "content": "because (cid:80) k=i γk = 1 γi. Similarly, log(cid:0)pτ x(T + 1 x)(cid:1) = (cid:88) (cid:16) γk k=T +1 log pT +1(x) log pk(x) (cid:17) = (cid:0)1 γT +1 (cid:1) log pT +1(x) + (cid:88) (cid:124) γk log pk(x) γT +1 log pT +1(x). (cid:123)(cid:122) () (cid:125) The terms (cid:80) γk log pk(x), labeled (), cancel out. What remains is log pi(x) log pT +1(x). Thus, log(cid:0)pτ x(t x)(cid:1) log(cid:0)pτ x(T + 1 x)(cid:1) = log pi(x) log pT +1(x) = log(cid:0)pxt(x)(cid:1) log(cid:0)pxT +1(x)(cid:1). As shown in Appendix B.1, Eq. (15) of (Yadin et al., 2024), combining this with the Gaussian prior argument and Tweedies formula Efron (2011) yields (cid:105) log(cid:0)pτ x(T + 1 x)(cid:1) log(cid:0)pτ x(t x)(cid:1) + , E(cid:2)εt xt = x(cid:3) = 1 αt (cid:104) which completes the derivation. Predictor-Corrector (PC) sampling Energy Diffusion (Du et al., 2024) and NCSN (Song & Ermon, 2019) rely on the score log pt(x) for generation and refinement. TAG, however, adds separate correction using the distinct TLS gradient, log pϕ(tx). Theorem 3.3 implies TAGs correction is uniquely adaptivestrengthening when samples are far from the manifold based on relative time probabilities. This adaptiveness may offer greater robustness than relying solely on log pt(x), which can be error-prone when off-manifold phenomena are present. Our direct experimental comparisons support this distinction; for instance, while some score-based correction sampling approaches (e.g., (Song et al., 2021b)) can degrade sampling quality under external guidance (such as with DPS), Table 8 demonstrates TAG outperforms over such baselines. Multi-condition generation MultiDiffusion (Bar-Tal et al., 2023) achieves spatial multi-condition control by fusing diffusion paths from multiple prompts, primarily targeting different image regions. Our approach to handling multiple conditions with TAG differs: we focus on combining multiple standard guidance terms and mitigating any resulting off-manifold drift by applying TAG. For efficiency in such scenarios, TAGs corrective temporal gradient, the TLS (x log pϕ(tx)), can be approximated using time predictors conditioned on single conditions or even an unconditional time predictor, as detailed in Sec. 3.2 and Appendix B.3. Thus, while MultiDiffusion employs path fusion mainly for spatial control objectives, TAG utilizes approximated temporal alignment to maintain manifold adherence when faced with combined guidance from multiple standard conditional inputs. Fine-tuning vs. TAG Standard approaches to adapt diffusion models for downstream taskssuch as adding spatial conditioning modules in ControlNet (Zhang et al., 2023), text-compatible prompt adapters in IP-Adapter (Ye et al., 2023), or rlbased reward tuning (Fan et al., 2023; Clark et al., 2024)require collecting task-specific labeled data, modifying model architectures, and performing hours of gradient-based optimization. In contrast, TAG trains only lightweight time predictor on noisy vs. clean timestep labels, completing in minutes on mimal computational resources (Jung et al., 2024). At inference, TAG injects temporally driven corrective gradient that steers samples back onto the appropriate diffusion manifold without altering the base models weights. This inference-time, training-free correction avoids fine-tunings cost and overfitting risks while supporting new guidance objectives such as reward alignment with DAS (Kim et al., 2025), multi-condition steering (Uehara et al., 2025), and style control via RB-Modulation (Rout et al., 2025). Moreover, by leveraging fundamental temporal consistency, TAG improves out-of-distribution robustness in tasks ranging from image and audio restoration to molecular generation (Ye et al., 2024; Bar-Tal et al., 2023). Thus, TAG offers general, low-overhead alternative to fine-tuning for mitigating off-manifold drift in guided diffusion."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Effect of Input perturbation on DPS, CIFAR-10. For fair comparison, we train diffusion models with different η from scratch following the official implementation code in [9]. No improvement over original diffusion model (η = 0) is observed in the presence of off-manifold phenomenon. We report the average value for 512 samples per each conditioning labels."
        },
        {
            "title": "Method",
            "content": "FID Acc. η = 0 η = 0.05 η = 0.10 η = 0.15 332.0 409.9 376.6 326.7 28.5 23.3 25.4 29.2 Table 8: Additional baselines when applying DPS on CIFAR-10. TAG improves the performance of DPS while other method struggles. FID Acc."
        },
        {
            "title": "Method",
            "content": "DPS TAG (ours) TCS [15] Timestep Guidance [16] Self-Guidance [17] Epsilon Scaling [10] Time Shift Sampler [11] 217.1 190.4 213.4 393.2 205.4 186.0 237. Langevin Dynamics [13] 226.8 57.5 63.2 29.4 9.4 51.6 53.0 60.8 58. Baseline experiments Here, we present experimental results comparing TAG against the baselines and prior works discussed throughout the section."
        },
        {
            "title": "E IMPLEMENTATION DETAILS",
            "content": "E.1 TOY EXPERIMENT Setup We construct the dataset from randomly generate 40,000 samples from the mixtures of two Gaussians as q0 1 2 ((10, 10), I). DDPM (VP-SDE) is utilized for diffusion process with total 100 diffusion timesteps. v(x, t) = 0.01 is applied as an external drift for every timestep. 2 ((10, 10), I) + 1 Training details For diffusion models, we use 3-layer MLP with 5000 training epochs with fullbatch size. For time predictor, 5-layer MLP is utilized with 5000 training epochs with full-batch size. We utilize single RTX 3090 GPU for the experiment. The predictor size in the toy experiment was not critical; TAG performed well even with predictor smaller than the score network, as shown in our ablation study Table 9. Importantly, in our main experiments  (Table 2)  , the effective SimpleCNN predictor is significantly smaller than the UNet diffusion backbone, demonstrating TAGs efficiency and lack of dependence on large predictor relative to the main model. See Appendix E.4 for further discussion on classifier robustness. E.2 CORRUPTED REVERSE PROCESS Setup We use the CIFAR-10 dataset and intentionally add random noise zt (0, σ2I) to the sample xt at each reverse timestep t. This simulates strong, non-physical perturbation pushing samples off-manifold. We generate 50,000 samples and evaluate using FID (Karras et al., 2018), IS (Salimans et al., 2016), and the Time-gap (Def. F.1). We utilize the pre-trained model CIFAR10DDPM Nichol & Dhariwal (2021) and use DDIM sampling with 50 diffusion timesteps. We run our our experiment on single A6000 GPU for the inference."
        },
        {
            "title": "Layers",
            "content": "W1 distance 0 (No TAG) 1 2 3 4 5 6 6.458 1.716 1.681 1.975 1.714 1.713 1.788 Table 9: Robustness of time classifier network on toy experiment. We measure Wasserstein distance (W1) for 10,000 samples. Consistent improvement compared to original reverse process when applying TAG independent of layer numbers. Algorithm In Algorithm 2, we provide pseudo-code of the corrupted reverse process setting conducted in Section 3.4. Algorithm 2 Corrupted reverse process with TAG Input: Diffusion model θ, time predictor ϕ, guidance strength schedule ω(t), number of total diffusion steps , Noise level σ. xT (0, I) for = T, , 1 do xt xt + σ ϵt where ϵt (0, I) Random noise with strength σ is added at each reverse diffusion timestep Obtain log p(x) from diffusion model θ xt1 xt from reverse diffusion step Calculate log pϕ(xt1) xt1 xt1 + ω(t) log pϕ(xt1). following Eq. 1 Calculating TLS score from the time predictor ϕ Applying TAG end for Output: Guidance schedule For the experiment, we use guidance schedule of wt = (1 αt) and where we refer ω as the guidance strength unless stated otherwise. Additional experimental results Here, to illustrate the correlation between TAG strength ω and performance metrics, we provide grid search results on the effect of guidance strength weight ω. For the experiment, we fix the noise schedule σ = 0.2 and follow the same setting in Section 3.4. The result is in Table 10 and one can observe that applying strong time guidance consistently increase the generation quality until performance is saturated. Table 10: Grid search result on the effects of TAG strength ω with σ = 0.2. ω 0.5 1.0 2.0 3.0 4.0 5. 6.0 7.0 8.0 9.0 TG 273.9 261.6 250.9 232.6 213.3 197.8 185.1 175.1 167.9 162.7 158.9 FID 410.1 408.5 406.7 390.3 376.2 361.2 350.6 344.3 339.1 335.6 334.6 IS 1.45 1. 1.29 1.27 1.28 1.38 1.41 1. 1.27 1.31 1.35 ω 10.0 15. 20.0 25.0 30.0 40.0 50.0 75. 100.0 150.0 200.0 156.0 143.2 129.1 116.2 107.5 TG 108.6 140.0 153.7 160.0 158.9 FID 345.6 318.8 291.4 277.1 270.7 257.6 247.1 240.8 236.7 229.5 223.2 IS 2.17 1.52 1.43 1. 1.63 1.90 2.01 2.14 1.60 98. 1.72 1.78 We also provide quantitative results of Figure 3 in Table 11. We measure FID (Karras et al., 2018), IS (Salimans et al., 2016), with time gap F.1 for the experiment and the best values for each noise strength σ where the guidance strength with the lowest FID value is reported. We find ="
        },
        {
            "title": "Preprint",
            "content": "0.2, 1.25, 4.5, 200.0 shows the lowest FID value for the noise strength level of σ = 0.05, 0.1, 0.2, 0.3 respectively. In Table 12, we compare FID values when applying TAG with original diffusion process which demonstrates applying TAG with right guidance strength can significantly improve the generation quality. Table 11: Comparison between original diffusion process and diffusion process with TAG across different noise strengths. σ = 0.05 σ = 0.1 σ = 0.2 σ = 0. TAG TG FID IS TG FID IS TG FID IS TG FID IS 43.0 78.9 5.29 104.1 193.6 2.37 229.6 351.4 1.50 274.0 410.1 1.28 42.1 62.5 5.73 41.6 115.6 3.80 97.3 230.9 1.67 158.9 223.2 2.17 Table 12: Comparison of FID values before and after applying TAG at different noise levels. Noise Level σ FID (before TAG) FID (after TAG) 0 0.05 0.1 0.2 0.3 11.799 78.882 193.653 351.318 410. 11.799 62.463 115.578 230.899 223.227 E.3 TRAINING-FREE GUIDANCE BENCHMARK Here, we provide experimental details that we follow in Section 4.1. We mainly follow TFG benchmark (Ye et al., 2024) for the fair comparison. All of the experiments in this subsection utilize training-free guidance as conditional guidance as stated in B.3. E.3.1 LABEL GUIDANCE Task description Label guidance target to generate designated label condition with only unconditional diffusion models. Dataset Two experiments are conducted using two image dataset: CIFAR10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015). Evaluation Following the image generation literature, we measure FID (Heusel et al., 2017) to assess fidelity and use accuracy to evaluate generation validity, defined as the proportion of generated samples classified as the target label. In other words, we measure: p(arg max ρ(x) = ctarget), (115) where ρ denotes classifier and ctarget refers to the target label. For CIFAR 10, we average the result across all 10 targets. For ImageNet, following (Ye et al., 2024), we randomly take different target values and report the average value across the selected target. We set the sample sizes to 512 for CIFAR10 and 256 for ImageNet in Table 2, while using 128 samples for ImageNet in the rest of the experiments. We note that the use of fewer evaluation samples is the primary reason for initially higher FIDs, which might consequently lag CFG SOTA. In Table 13, we present standard comparisons on CIFAR-10 using 50,000 samples that yielded improved and benchmark-consistent scores. Models For backbone diffusion models, DDPM in Nichol & Dhariwal (2021) is utilized for both CIFAR-10 and ImageNet."
        },
        {
            "title": "Preprint",
            "content": "Table 13: Originally, 512 samples were used for rapid, extensive experiments across various tasks. For more rigorous evaluation, we used 50,000 samples on CIFAR-10 with 100 inference steps. As expected, increasing the number of samples to match standard benchmark protocols led to improved FID scores."
        },
        {
            "title": "Method",
            "content": "512 samples FID Acc. TFG TFG + TAG (ours) 114.1 102.7 55.8 61.5 50000 samples TFG TFG + TAG (ours) 77.5 47.1 54.3 84.4 E.3.2 GUASSIAN DEBLURRING Task description Gaussian deblurring task aims to restore the noisy images which are blurred by Guassian process. This inverse problem has been extensively studied with diffusion models and notably, DPS (Chung et al., 2023) utilize training free guidance when given the blurring operator: We set the loss objective function ℓc in Eq. 29 as l2 norm between the blurred estimates and the target, ℓc = Ablur(x) y2. (117) = Ablur(x). (116) Dataset Cat images (Elson et al., 2007) is utilized for the diffusion model training with resolution 256256. Evaluation We measure FID score for the sample fidelity and LPIPIS (Zhang et al., 2018) for evaluating conditioning effects. E.3.3 SUPER-RESOLUTION Task description Super-resolution targets to upscale the originally lower-resolution images to the higher resolution images. Previous works (Saharia et al., 2022; Ho et al., 2022) show one can leverage diffusion models for this task. In super-resolution case, we assume having an downgrade operator Adown. With the operator we suppose low-resolution images is obtained from higher resolution image by Adown : R2562563 R64643, = Adown(x). Now, by setting following loss objective function in Eq. 29: ℓc = Adown(x y)2, we leverage training free guidance to restore the target high-resolution image. (118) (119) Dataset Cat images Elson et al. (2007) is utilized for the diffusion model training with resolution 256256. Evaluation FID is used for the sample fidelity and LIPIPS is used for evaluating conditioning effects. We set the sample size as 256 for the result in the Table 2 and 128 for all the other experiments. E.3.4 MULTI-CONDITIONAL GUIDANCE Task Description Multi-conditional guidance leverages multiple target functions to guide single sample towards multiple attribute-based targets. We explore two scenarios: (gender, hair color) and (gender, age). Each attribute has two labels: Gender: {male, female}, Age: {young, old}, Hair color: {black, blonde}."
        },
        {
            "title": "Preprint",
            "content": "Following Ye et al. (2024), we sampled images that maximize the marginal probability: pcombined(x0) = max x0 where ptarget(x0) is estimated using label guidance. However such naive approach of summing score functions for each condition, as in Eq. 5, can lead to off-manifold artifacts. ptarget1(x0)ptarget2(x0), max x0 (120) Dataset Experiments are conducted on CelebA-HQ (Karras et al., 2018) at resolution of 256256. Evaluation We assess sample fidelity using Kernel Inception Distance (KID) (Binkowski et al., 2018), with 1,000 randomly sampled CelebA-HQ images as references. The KID(log) scores are reported in Section 4. For validity, we compute classification accuracy using three independent attribute classifiers, evaluating the conjunction of target attributes: Accuracy = # (cid:86) target label(classified as target label) #generated samples . (121) We set the sample size as 256 across all experiments. Models We use the CelebA-DDPM model, trained on CelebA-HQ, as the base diffusion model. Binary classifiers are employed for attribute validation. E.3.5 MOLECULAR GENERATION Task description The goal of molecular generation in this work is to guide 3D molecules generated from unconditional diffusion models to the deisred quantum chemical properties (Hoogeboom et al., 2022). Utilizing the property predictor Aproperty is trained for each quantum chemical property, Aproperty : Rd R, Aproperty(x) = c. (122) Then, we set the training-free guidance objective function ℓc as square of l2 norm of the property gap as follows: ℓc = Aproperty(ˆx0) c2 2, (123) where ˆx0 is obtained from the Tweedies formula (Appendix B.3). Dataset We use QM-9 dataset (Ramakrishnan et al., 2014), which consists of 134k molecules with molecules having maximum 9 heavy atoms (C, N, O, F) labeled with 12 quantum chemical properties. The dataset is split into 130k / 18k / 13k molecules of training, valid, test data following (Hoogeboom et al., 2022). Following previous works (Hoogeboom et al., 2022; Bao et al., 2023; Xu et al., 2023), we take 6 quantum chemical properties as target property where we describe detils in the following. Polarizability (α): The extent to which molecules electron cloud can be distorted by an external electric field. HOMO-LUMO gap (ϵ): The energy difference between the highest occupied and lowest unoccupied molecular orbitals, signifying possible electronic transitions. HOMO energy (ϵHOMO): The energy of the highest occupied orbital, often linked to how easily molecule donates electrons. LUMO energy (ϵLUMO): The energy of the lowest unoccupied orbital, often linked to how readily molecule can accept electrons. Dipole moment (µ): numerical measure of charge separation within molecule, reflecting its polarity. Heat capacity (Cv): The amount of heat required to change the temperature of molecule by given amount. Models We utilize unconditional EDM from Hoogeboom et al. (2022) for the backbone diffusion model which consists of EGNN (Satorras et al., 2021). For the property prediction, we utilize EGNN backbone architecture as in (Bao et al., 2023) where each specialized prediction model which outputs scalar value is used."
        },
        {
            "title": "Preprint",
            "content": "Evaluation We evaluate Atom Stability (AS) which measures percentage of atoms within generated molecules that have right valencies. An atom is stable if its total bond count matches the expected valency for its atomic number (Hoogeboom et al., 2022). To measure conditioning effect, we calculate Mean Absolute Error (MAE) values between the target condition and predicted condition. We set the sample size as 4096 for the result in the Table 2 and 1024 for all the other experiments. E.3.6 AUDIO GENERATION Task description We conduct experiments for two types of tasks with audio diffusion models: Audio declipping and Audio inpainitng (Moliner & Välimäki, 2024; Moliner et al., 2023). Audio declipping is process that repairs distorted audio signals, specifically addressing the issue of clipping. Clipping occurs when the audio signals intensity surpasses the limits of the recording system, resulting in distorted sound with missing portions of the waveform. Audio inpainting is technique used to reconstruct missing or damaged parts of an audio signal. For the declipping test, we assume having clipping operator Ablur which corrupts mel spectrograms (Shen et al., 2018) as follows. Aclip : R256256 R256256, Aclip(x) = y, (124) where clipping operator is operated by zeroing out the 40 highest dimensions and zeroing out the lowest 40 dimensions in terms of the frequency values. For inpainting task, we assume we have blurring operator Ablur as Ablur : R256256 R256256, Ablur(x) = y, (125) where deblurring is conducted by zeroing out the values of middle 80 dimensions in the mel sepctrograms. For both tasks, we set the training-free guidance objective function ℓc with l2 norm. ℓc = Aclip(ˆx0) y2, ℓc = Ablur(ˆx0) y2. (126) Dataset We borrow open-source training data of Audio-DDPM 1 following Ye et al. (2024). Evaluation For both tasks, we use Frechet Audio Distance (FAD) (Kilgour et al., 2018) for measure how close the generated data is from the original distribution and Dynamic Time Warping (DTW) (Müller, 2007) for evaluating how generated samples are derived into the desired conditions. We set the sample size as 256 across all experiments. 1https://huggingface.co/teticio/audio-diffusion-"
        },
        {
            "title": "Preprint",
            "content": "E.3.7 HYPER-PARAMETERS In Table 14, we provide hyper-parameter settings for the DPS and TFG where we follow the optimal reported values in Ye et al. (2024). Table 14: Parameter table (ρ, µ, γ) DPS, TFG for all methods, tasks, and targets. DPS TFG Target ρ µ γ ρ µ γ CIFAR-10 label guidance 0 1 2 3 4 5 6 7 8 9 0 0 0 0 0.25 0 0 0 0 0 0 0 0 0 0 0.25 0 0 0 0 0 0 1 8 1 4 0.5 4 1 2 2 4 1 1 0.5 2 4 1 2 ImageNet label guidance 2 111 0.5 222 1 333 0.5 0 0 0 0 0 0 0 0 2 2 2 4 Fine-grained guidance 111 222 333 444 0.25 0 0 0.25 0 0 0.25 0 0 0.25 0 0 0.5 0.5 0.5 0.5 0.001 2 0.001 2 1 0.25 0.01 0.25 0.5 0.001 0.25 0.001 1 0.5 0.5 0.001 0.25 0.001 0. 2 0.5 1 4 2 0.5 0.5 0.5 0.5 0.1 0.1 1 0.1 0.01 0.01 0.01 0.01 Combined Guidance (gender & hair) (0,0) (0,1) (1,0) (1,1) 0 0 0 0 0 0 0 0 1 2 1 0.5 0.01 0.01 0.01 0.1 2 8 1 1 4 4 4 2 DPS Target ρ µ γ ρ TFG µ Combined Guidance (gender & age) 0 0 (0,0) 0 0 (0,1) (1,0) 0 0 0 0 (1,1) 2 8 2 0.5 1 0.5 0.5 1 8 1 4 2 Super-resolution 0 0 Gaussian Deblur 16 0 0 4 2 8 γ 0.01 1 0.01 0.1 0.01 0. Molecule Property α 0.005 0 0 0.016 0.001 0.0001 µ 0.02 0 0 0.001 0.002 Cv 0.005 0 0 0.004 0.001 ϵHOMO 0.005 0 0 0.002 0.004 ϵLUMO 0.1 0.001 0.001 0.005 0 0 0.016 0.002 0.0001 0.001 0.005 0 0 0.032 0.001 Audio Declipping 1 0 0 Audio Inpainting 16 0 0 0.25 1 0.1 0.1 E.4 TIME PREDICTOR Architecture Time Predictor is foundational component of our TAG framework, designed to estimate p(txt) or p(txt, c) for guiding noisy samples back to the desired data manifold. Its architecture is tailored to the input modality. For image and audio data, SimpleCNN is employed, comprising four convolutional layers with channel sizes (32, 64, 128, 256), each followed by ReLU activation and average pooling. This design is significantly lighter than the diffusion backbone. final linear layer produces logits for all timesteps. In conditional settings, learned embedding vectors for conditions are concatenated before the linear layer to model p(txt, c). For molecular data, modified Equivariant Graph Neural Network (EGNN) Satorras et al. (2021) processes node and edge features along with spatial coordinates. The concatenated node and spatial features are passed through feed-forward network to output logits representing the time distribution. These architectures are lightweight compared to the diffusion model backbone yet expressive enough to capture temporal and conditional relationships, involving minimal computational cost during sample generation. We present the performance analysis in next subsection. Following Jung et al. (2024), training involves minimizing cross-entropy loss between the true time step and the predicted distribution over timesteps. For each sample x0 from the data distribution, noisy version xt is generated at random using the forward process. The objective is: Ltime-predictor(ϕ) = Et,x0 [log (ˆpϕ(xt)t)] , (127)"
        },
        {
            "title": "Preprint",
            "content": "where ˆpϕ(xt)t is the predicted probability for t. Cross-entropy is chosen over regression due to overlapping supports of pt(x) and ps(x), ensuring ambiguity is handled probabilistically. The model is trained using the Adam optimizer (learning rate 1 104) for 300K iterations on most datasets, except for ImageNet, which uses 600K iterations. The batch sizes and GPU configurations for each dataset are summarized in Table 15. Table 15: Training Details for the Time Predictor Dataset Batch Size Training Iterations A100 GPUs ImageNet CIFAR10 CelebAHQ Cat Molecule Audio 1024 256 256 128 128 128 600K 300K 300K 300K 300K 300K 4 1 1 1 2 1 Performance We compare the performance of time predictors across diverse datasets and tasks. The time gap (Def. F.1) is presented in the Appendix F.2, where we evaluate its behavior across different datasets and tasks. Given true forward noise samples xt q(xtx0), we measure the time gap, where lower value indicates higher prediction accuracy. The results presented in Figure F.2 demonstrate that the time predictor achieves strong performance across most datasets and tasks, despite employing relatively simple CNN architecture. Notably, for timesteps < 600, nearly all models accurately predict the true timestep. However, for lowerdimensional datasets such as CIFAR-10 and molecular data, the prediction error increases as approaches the final timestep of the diffusion process, indicating degraded performance. This observation aligns with the findings of Kahouli et al. (2024), reported that higher data dimensionality enhances predictability, whereas overlapping distributions near impede accurate predictions. Consistent with these observations, our results indicate that the some model struggles in this regime, which we leave as an avenue for future work. The performance of TAG improves with better classifier, as it provides more accurate estimate of the true TLS. We conducted experiments using different training checkpoints (10K and 30K).Table 16 shows that performance on all metrics improved at the 30K checkpoint, correlating with the better performance of the more trained classifier. Table 16: Quantitative evaluation of TFG+TAG across varying training steps on CIFAR-10 confirms the relationship between classifier robustness and TAG performance. Training Steps 10 30 FID 116.0 Acc. 55.3 102.7 61. E.5 LARGE-SCALE TEXT-TO-IMAGE GENERATION Enhanced Reward Alignment All reward alignment experiments build on the DAS test-time sampler (Kim et al., 2025) with Stable Diffusion v1.5 (Rombach et al., 2022) as the base model. Unless stated otherwise, we follow the hyperparmeter setting in DAS (Kim et al., 2025). We evaluate with two settings: Single-objective alignment: We optimize two separate reward functions: the LAION Aesthetic predictor (Schuhmann et al., 2022) using 256 simple animal prompts from ImageNet (Russakovsky et al., 2015), and CLIPScore (Radford et al., 2021) using the HPSv2 prompt set (Wu et al., 2023). Multi-objective alignment: We combine aesthetic and CLIP rewards via rtotal(x) = rAesthetic(x) + (1 w) rCLIP(x), = 0.5."
        },
        {
            "title": "Preprint",
            "content": "In all experiments we use = 100 diffusion steps, single particle setting, and the tempering schedule λt = (1 + γ)t1 with γ = 0.008 following original setting for the fair comparison. Resampling is triggered when the effective sample size ESS < 0.5. We set the KL coefficient α = 0.01 for aesthetic alignment and α = 0.001 for CLIP alignment (single-objective), and α = 0.005 for multi-objective trials. For each prompt set, we sample 256 images and report the mean reward and mean Time-Gap (Def. F.1) over three independent runs. Improved Style Transfer We adopt the setup of (Ye et al., 2024). Our goal is to steer the latent diffusion model Stable-Diffusion-v-1-5 (Rombach et al., 2022) so that the generated images both match the input text prompts and reflect the style of given reference images. We achieve this by matching the Gram matrices (Johnson et al., 2016) of intermediate features from CLIP image encoder for the generated and reference images. Specifically, let xref be reference style image and D(z0t) be the decoded image obtained from the estimated latent z0t. We extract features from the third layer of the CLIP image encoder and compute their Gram matrices G(xref ), G(cid:0)D(z0t)(cid:1) following the methodology of MPGD (He et al., 2024) and FreeDoM (Yu et al., 2023). The styleguidance objective maximizes exp(cid:0) G(xref ) G(D(z0t)) (cid:1), where denotes the Frobenius norm. We measure style transfer quality using the Style Score and CLIP Score. As reference styles, we use the same four WikiArt images employed by MPGD (He et al., 2024), and for text prompts we select 64 samples from Partiprompts (Yu et al., 2022). For each style, we generate 64 images. To prevent inflated CLIP scores, we compute guidance and evaluation with two different CLIP models from the Hugging Face Hub, Guidance2 and Evaluation3. Throughout our experiments, we fix the guidance strength at ωt = 1. We leave exploring hyperparameter tuning to improve results as future work."
        },
        {
            "title": "F ABLATION STUDIES",
            "content": "F.1 FEW STEP UNCONDITIONAL GENERATION In few step generation experiments in Section 4.3, we study on the effect of TAG in unconditional generation scenario where no extra guidance is applied in reverse diffusion process. Specifically, we focus on few step generation scenario where discretization error happens as introduced in Appendix B.5. We further report the evaluation results with 50,000 samples in Table 17 where number of function evaluation (NFE) refers to how many times we evaluate with diffusion models during the reverse process. The result shows that TAG significantly improves FID and IS scores when less evaluation steps are used which alings with our intuition that fewer NFE induces more severe off-manifold phenomenon in reverse diffusion process. For the experiment, we utilize CIFAR-10 DDPM Song & Dhariwal (2024) as in Section 4.3 and use DDPM sampling. Table 17: Comparison of FID values before and after applying TAG in unconditional generation scenario. NFE 1 NFE NFE 5 TAG FID IS FID IS FID IS 449.8 1.26 194.5 2.04 116.5 3.08 232.9 2.26 124.2 3.55 97.4 3.66 2Guidance: openai/clip-vit-base-patch16 3Evaluation: openai/clip-vit-base-patch"
        },
        {
            "title": "Preprint",
            "content": "F.2 TIME GAP Time-Gap (TG) To quantify the temporal deviation during generation, we define the Time-Gap metric. Denoting the sample at timestep as xt and the time predictor as ϕ, the Time-Gap is the average absolute difference between the predicted timestep index and the true index: Definition F.1 (Time-Gap). Time-gap :="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 arg max ϕ(xt) t. (128) lower Time-gap indicates samples are closer to their expected temporal manifold. Time Gap across different timesteps To further identify how time gap varies across different diffusion timesteps, we conduct an ablation study where we measure time gap for every timestep in diffusion models. For each step, average time gap value over 512 samples are reported. (a) Unconditional (b) Conditional Figure 4: Time gap in CIFAR10. (a) Unconditional (b) Conditional Figure 5: Time gap in ImageNet."
        },
        {
            "title": "Preprint",
            "content": "(a) Unconditional (b) Deblur (c) Super-resolution Figure 6: Time gap in Cat. (a) Unconditional (b) Declipping (c) Inpainting Figure 7: Time gap in Audio. (a) Unconditional (b) Gender + Age (c) Gender + Hair Figure 8: Time gap in CelebA. (a) Unconditional (b) Polarizability α (c) Polarizability α and Dipole µ Figure 9: Time gap in Molecule."
        },
        {
            "title": "Preprint",
            "content": "(a) FID vs. TG (b) IS vs. TG Figure 10: Correlation between time gap and standard metrics for image generation quality. Network Architecture To assess the trade-off between model size and time-gap accuracy, we compare two backbones. Our SimpleCNN consists of four convolutional blocks with channel widths (32, 64, 128, 256). Each block uses 3 3 convolution, ReLU activation, and 2 2 average pooling. At just 1.48 parameters8.5 % of the 17.38 M-parameter UNet encoder (Dhariwal & Nichol, 2021)SimpleCNN matches its time-gap performance  (Table 18)  . This demonstrates that lightweight, single-path network can rival much larger UNet based classifiers. Table 18: FID on CIFAR-10 for time predictors using SimpleCNN (1.48 parameters) and UNet encoder (17.38 parameters), comparing unconditional and conditional models across training checkpoints. Checkpoint SimpleCNN UNet Unconditional Conditional Unconditional Conditional 50K 100K 200K 300K 24.19 23.24 23.11 22.93 23.64 27.33 22.64 21. 25.59 22.08 22.58 24.40 21.85 19.68 20.53 22.49 Correlation with other standard metrics We conduct ablation study on the correlation between Time Gap and standard metrics (FID and IS). Figure 10 illustrates how time gap and standard measures for image generation quality. We vary different number of function evaluation (NFE) in unconditional diffusion models. For the experiment, we generate 50,000 samples with DDIM sampling and utilize CIFAR10-DDPM model (Nichol & Dhariwal, 2021) with the NFE of 1, 5, 10, 20, 50. The result shows that as NFE increases, time gap reduces while FID decreases and IS increases. This demonstrates that Time Gap serves as good measure to evaluate the off-manifold phenomenon. Limitation Once the reverse diffusion process is good enough (i.e, time gap is already small), it often loses correlation with FID measure. We believe improving the performance of the time predictor network will reduce this problem and thereby further boost the effect of the TAG. We further note that the motivation of introducing Time Gap in this work is not to suggest new metric, but to quantify the amount of off-manifold phenomenon where applying TAG is intended to reduce the Time Gaps in each every timestep of the reverse diffusion process."
        },
        {
            "title": "G VISUALIZATIONS OF GENERATED SAMPLES",
            "content": "Here, we present qualitative examples corresponding to the experiments presented in Section 4.1. We provide visualizations for all four experimental settings: DPS, DPS+TAG, TFG, and TFG+TAG. Below, we detail the dataset configurations used for generating these qualitative examples."
        },
        {
            "title": "Preprint",
            "content": "CIFAR-10 We generate images conditioned on the target class 8 (corresponding to the Ship category). The images are produced using 250 inference steps with an TAG strength of ω = 0.15. ImageNet We present generated samples for target classes 111 (Worm) and 222 (Kuvasz), using 100 inference steps with an TAG strength of ω = 0.15. QM9 We show qualitative results for the target molecular properties polarizability α and dipole moment µ. In this setting, we employ 0.1 guidance strength for DPS, following the default configuration in Ye et al. (2024), with 100 inference steps. CelebA-HQ We provide qualitative examples for two specific conditions: Gender+Hair and Gender+Age. The target attributes in these cases are black hair, young age, and female gender, all represented as binary variables to be satisfied in our conditional generation. (a) DPS (b) DPS + TAG (c) TFG (d) TFG + TAG Figure 11: CIFAR10 with the target of 8 (Ship)."
        },
        {
            "title": "Preprint",
            "content": "(a) DPS (b) DPS + TAG (c) TFG (d) TFG + TAG Figure 12: ImageNet with the target of 111 (Worm)."
        },
        {
            "title": "Preprint",
            "content": "(a) DPS (b) DPS + TAG (c) TFG (d) TFG + TAG Figure 13: ImageNet with the target of 222 (Kuvasz)."
        },
        {
            "title": "Preprint",
            "content": "(a) DPS (b) DPS + TAG (c) TFG (d) TFG + TAG Figure 14: Molecule with condition of polarizability α."
        },
        {
            "title": "Preprint",
            "content": "(a) DPS (b) DPS + TAG (c) TFG (d) TFG + TAG Figure 15: Molecule with condition of dipole µ. (a) Without TAG (b) With TAG Figure 16: CelebA with condition of Gender (female) + Hair (black hair)."
        },
        {
            "title": "Preprint",
            "content": "(a) Without TAG (b) With TAG Figure 17: CelebA with condition of Gender (female) + Age (young)."
        }
    ],
    "affiliations": [
        "KAIST AI"
    ]
}