{
    "paper_title": "Soft Adaptive Policy Optimization",
    "authors": [
        "Chang Gao",
        "Chujie Zheng",
        "Xiong-Hui Chen",
        "Kai Dang",
        "Shixuan Liu",
        "Bowen Yu",
        "An Yang",
        "Shuai Bai",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs."
        },
        {
            "title": "Start",
            "content": "2025-12-02 Shixuan Liu"
        },
        {
            "title": "Soft Adaptive Policy Optimization",
            "content": "Chang Gao Chujie Zheng Xiong-Hui Chen Kai Dang Bowen Yu An Yang Shuai Bai Jingren Zhou Junyang Lin 5 2 0 D 1 ] . [ 2 7 4 3 0 2 . 1 1 5 2 : r Qwen Team, Alibaba Inc."
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variancea phenomenon exacerbated in Mixture-of-Experts modelsleading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms continuous trust region that avoids the brittle hard clipping band used in GSPO. When sequence contains few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides more reliable, scalable, and effective optimization strategy for RL training of LLMs."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become key driver of recent advances in large language models (LLMs), enabling deeper and longer reasoning for challenging tasks in mathematics, programming, and multimodal understanding (OpenAI, 2024; DeepSeek-AI, 2025; Qwen, 2025). Among RL methods, group-based policy optimization has emerged as practical recipe: multiple responses are sampled per query, sequence-level rewards are normalized within the group, and policy updates are weighted by importance ratios between the current and behavior policies (Shao et al., 2024; Zheng et al., 2025). central challenge in this setting is the high variance of token-level importance ratios, especially in Mixture-of-Experts (MoE) models where routing heterogeneity and long responses can amplify deviations across tokens. Such variance increases the likelihood of unstable updates. Hard clipping, as used in GRPO (Shao et al., 2024), constrains large deviations by zeroing gradients outside fixed band. While effective at curbing excessive steps, hard clipping makes it difficult to strike favorable trade-off: overly tight clipping limits the number of valid samples for gradient computation, while looser clipping introduces noisy gradients coming from off-policy samples. To address the brittleness of hard clipping in group-based policy optimization, we propose Soft Adaptive Policy Optimization (SAPO), smooth and adaptive policy-gradient method that replaces hard clipping with temperature-controlled soft gate, as shown in Figure 1. SAPO weights token-level updates by bounded, sigmoid-shaped function of the importance ratio, centered at the on-policy point. This implements continuous trust region: near on-policy, gradients are preserved to encourage useful updates and exploration; as the ratio deviates, gradients are attenuated smoothly rather than truncated, maintaining the learning signal for moderate deviations while reducing optimization noise. To further enhance robustness in large vocabularies, SAPO employs asymmetric temperatures for positive and negative tokens to make gradients on negative tokens decay more rapidly, reflecting their distinct stability Corresponding authors. 1 profiles: negative updates tend to increase the logits of many inappropriate tokens and are therefore more prone to introduce instability than positive updates. Conceptually, SAPO is designed to be sequence-coherent and token-adaptive. Under mild and empirically common conditionssmall on-policy steps and low dispersion of token log-ratios within sequencethe average token gate concentrates to smooth sequence-level gate, aligning optimization with sequencelevel rewards in the spirit of sequence-based methods such as GSPO (Zheng et al., 2025). When these conditions are violated due to heterogeneous or outlier tokens, SAPO selectively down-weights only the offending tokens while retaining informative gradients from near on-policy tokens within the same sequence. This selective attenuation mitigates the signal loss associated with hard clipping, improving sampling efficiency while maintaining stable updates. Empirically, SAPO provides improved stability and task performance compared with GSPO and GRPO. While all methods may ultimately exhibit signs of instability, SAPO sustains coherent learning for longer duration and reaches higher Pass@1 accuracy before divergence. This stems from SAPOs ability to preserve informative gradients beyond the hard-clip threshold while selectively suppressing highvariance token updates. Furthermore, our temperature ablations further reveal that the asymmetric designusing larger temperature for negative-token updatesis critical: it dampens high-variance negative gradients and significantly reduces the likelihood of early collapse. Beyond controlled settings, SAPO also proves effective in practical training of Qwen3-VL models across broad mixture of text and multimodal tasks and across different model scales and architectures. Together, these results demonstrate that SAPOs smooth gating and asymmetric temperature control enable more reliable and productive RL training of large language models."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notation. We model an autoregressive language model parameterized by θ as stochastic policy πθ over token sequences. Let denote query and the query set. For response to q, its likelihood under πθ factorizes as πθ(y q) = t=1 πθ(yt q, y<t), where is the number of tokens in y. Group Relative Policy Optimization (GRPO). For each query D, GRPO (Shao et al., 2024) samples group of responses {y1, . . . , yG} from the behavior policy πθold , computes their rewards {R1, . . . , RG}, and maximizes the following token-level objective: JGRPO(θ) = qD, {yi}G i= πθold (q) (cid:34) 1 i=1 1 yi yi t=1 (cid:16) min ri,t(θ) (cid:98)Ai,t, clip (ri,t(θ), 1 ε, 1 + ε) (cid:98)Ai,t (cid:35) (cid:17) , (1) where ri,t(θ) = πθ(yi,tq, yi,<t) (yi,tq, yi,<t) πθold , (cid:98)Ai,t = (cid:98)Ai = Ri mean({Rj}G j=1) (cid:17) (cid:16) std {Rj}G j= , (2) ε > 0 is the clipping range, is the number of responses in group, and (cid:98)Ai,t is the group-normalized advantage (shared across tokens within response). Group Sequence Policy Optimization (GSPO) GSPO (Zheng et al., 2025) employs the following sequence-level optimization objective: JGSPO(θ) = qD, {yi}G i=1 πθold (q) (cid:34) 1 i= (cid:16) min si(θ) (cid:98)Ai, clip (si(θ), 1 ε, 1 + ε) (cid:98)Ai (cid:35) (cid:17) , where si(θ) = (cid:18) πθ(yiq) (yiq) πθold (cid:19) 1 yi = exp (cid:32) 1 yi yi t=1 log πθ(yi,tq, yi,<t) (yi,tq, yi,<t) πθold (cid:33) , (cid:98)Ai = Ri mean({Rj}G j=1) (cid:17) (cid:16) std {Rj}G j= (3) (4) GSPO applies clipping at the sequence level rather than per token. The length normalization in si(θ) reduces variance and places it on consistent numerical scale across responses. 2 Figure 1: Comparison of policy-update objectives under positive advantage. The left panel shows the surrogate objective value; the right panel shows the corresponding gradient weight wi,t(θ) as function of the policy ratio ri,t(θ)."
        },
        {
            "title": "3 Soft Adaptive Policy Optimization",
            "content": "Soft Adaptive Policy Optimization (SAPO) is smooth and adaptive policy-gradient method for RL finetuning, which replaces hard clipping with temperature-controlled soft gate. Smooth gating functions have been explored in traditional RL settings (Chen et al., 2023). In SAPO, we incorporate this idea into the group-based RL paradigm for LLMs and extend it with two additional components that are essential for large-scale language model training: (1) token-level soft trust region that naturally yields sequence-level coherence, and (2) an asymmetric temperature design motivated by the distinct behaviors of positive and negative token updates. Specifically, SAPO maximizes the following objective: where (θ) = qD,{yi}G i=1 πθold (q) (cid:34) 1 i=1 1 yi yi t= (cid:35) fi,t(ri,t(θ)) (cid:98)Ai,t , fi,t(x) = σ (τi,t (x 1)) (cid:40) τi,t = 4 τi,t , if (cid:98)Ai,t > 0 τpos, τneg, otherwise , (5) (6) (cid:98)Ai,t and ri,t(θ) are computed as in Equation (2), τpos and τneg are the temperatures for positive and negative tokens, respectively, and σ(x) = 1/(1 + ex) is the sigmoid function. Differentiating (5) yields weighted log-policy gradient: θJ (θ) = qD,{yi}G i=1 πθold (q) (cid:34) 1 i=1 1 yi yi t= wi,t(θ) ri,t(θ) θ log πθ(yi,t q, yi,<t) (cid:98)Ai,t (7) (cid:35) where wi,t(θ) = 4 pi,t(θ) (cid:0)1 pi,t(θ)(cid:1), (8) which peaks at ri,t(θ) = 1 with value of 1 and decays smoothly and approximately exponentially as ri,t(θ) deviates from 1, implementing soft trust region and preventing both gradient vanishing and excessively large updates, as shown in Figure 1. Notably, at ri,t(θ) = 1, the soft-gated gradient equals that of the unclipped objective ri,t(θ) (cid:98)Ai,t, regardless of τi,t, preserving on-policy behavior. This also explains the presence of the 4/τi,t factor in fi,t. pi,t(θ) = σ (τi,t (ri,t(θ) 1)) , Compared with GSPO (Zheng et al., 2025) and GRPO (Shao et al., 2024), SAPO provides both sequencelevel coherence and token-level adaptivity: (1) Under mild assumptionssmall on-policy steps and low dispersion of token log-ratios within sequencethe average token gate concentrates to smooth sequence-level gate g(log si(θ)) = sech2(cid:0) τi 2 log si(θ)(cid:1). Thus, SAPO reduces to GSPO-like sequence formulation but with continuous trust region. Crucially, when few off-policy tokens push si beyond GSPOs hard band, GSPO suppresses gradients for the many near-on-policy tokens in that sequence, hurting sample efficiency. SAPO, in contrast, preserves informative gradients by down-weighting only the offending tokens while keeping near-on-policy tokens influential. (2) Relative to GRPO, SAPO avoids hard token-level clipping that zeroes gradients outside fixed range. Instead, SAPO scales updates smoothly, providing more balanced way to retain useful learning signals while preventing unstable policy shifts. See more details in Section 4. 3 Why Different Temperatures for Positive and Negative Advantages The hyperparameter τ controls the rate of attenuation: larger values produce faster decay. Although negative tokens are crucial for exploration and for preventing overfitting, they typically introduce greater instability than positive tokens. We justify this claim by analyzing how token-level gradients propagate through the logits. Let = [z1, z2, ..., zV ] denote the logits (with vocabulary size ), let denote token, and compute output probabilities via softmax operation, i.e., πθ(v q, yi,<t) = exp(zv)/ exp(zv ). We have log πθ(yi,t q, yi,<t) (cid:98)Ai,t zv πθ(yi,t q, yi,<t) zv = (cid:98)Ai,t πθ(yi,t q, yi,<t) 1(v = yi,t) exp(zyi,t ) vV exp(zv ) exp(zyi,t ) exp(zv) (vV exp(zv ))2 (cid:98)Ai,t πθ(yi,t q, yi,<t) = = (cid:40)(cid:0)1 πθ(yi,t q, yi,<t)(cid:1) (cid:98)Ai,t πθ(v q, yi,<t) (cid:98)Ai,t if = yi,t otherwise (sampled token) (unsampled token) (9) Positive advantages increase the sampled tokens logit and decrease all unsampled logits; negative advantages do the opposite, raising the logits of many unsampled tokens. In RL fine-tuning for LLMs, the action space is large vocabulary (often hundreds of thousands of tokens), whereas the number of desirable actions in given state is small. Consequently, negative gradients diffuse to numerous irrelevant tokensproviding some regularization but also inducing instability, especially in off-policy scenarios. Accordingly, we use distinct temperatures for positive and negative tokens and set τneg > τpos so that gradients on negative tokens decay more rapidly, thereby improving training stability and performance."
        },
        {
            "title": "4 A Gating-Function Perspective on SAPO’s Connections to GRPO and GSPO",
            "content": "Unified surrogate We consider unified surrogate of the form yi t=1 (θ) = qD, {yi}G 1 yi i=1 πθold 1 (q) i=1 (cid:34) (cid:0)ri,t(θ)(cid:1) fi,t (cid:35) (cid:98)Ai,t , (10) where fi,t() is an algorithm-specific gating function. We further define the length-normalized sequencelevel ratio as the geometric mean of token ratios: (cid:19) 1 yi (cid:32) (cid:33) = exp log ri,t(θ) , si,t(θ) = sg [si(θ)] πθ(yi,tq, yi,<t) sg [πθ(yi,tq, yi,<t)] , (11) si(θ) = (cid:18) πθ(yi q) (yi q) πθold 1 yi yi t=1 where sg[] denotes the stop gradient operation. Algorithm-specific fi,t The algorithms differ in the choice of fi,t: SAPO: SAPO i,t (ri,t(θ)) = σ(cid:0)τi (ri,t(θ) 1)(cid:1), τi = 4 τi (cid:40) τpos, τneg, (cid:98)Ai > 0, (cid:98)Ai 0, GRPO: GRPO i,t (ri,t(θ); (cid:98)Ai) = (cid:40) min(cid:0)ri,t(θ), 1 + ε(cid:1), max(cid:0)ri,t(θ), 1 ε(cid:1), (cid:40) (cid:98)Ai > 0, (cid:98)Ai 0, GSPO: GSPO i,t (ri,t(θ); (cid:98)Ai) seq i,t (cid:0)si,t(θ); (cid:98)Ai (cid:1) = min(cid:0)si,t(θ), 1 + ε(cid:1), max(cid:0)si,t(θ), 1 ε(cid:1), (cid:98)Ai > 0, (cid:98)Ai 0. (12) (13) (14) Note that GSPOs fi,t is token-invariant within sequence, while SAPO and GRPO are token-dependent. Gradient form for SAPO/GRPO Differentiating (10) and using θri,t(θ) = ri,t(θ)θ log πθ(yi,t q, yi,<t), we obtain θJ (θ) = (cid:34) 1 i=1 1 yi yi t=1 i,t (cid:0)ri,t(θ)(cid:1) ri,t(θ) θ log πθ(yi,t q, yi,<t) (cid:98)Ai (cid:35) . (15) 4.1 SAPOGSPO Connection: Reduction to Sequence-Level Soft Gate We show that SAPO reduces to GSPO-like sequence-level formulation under mild conditions, while retaining token-level adaptivity in heterogeneous sequences. 4 SAPOs token-level soft gate Using σ(x)(1 σ(x)) = SAPO i,t (ri,t(θ)) = 4 σ(cid:0)τi (ri,t(θ) 1)(cid:1)(cid:16) 1 σ(cid:0)τi (ri,t(θ) 1)(cid:1)(cid:17) Assumptions We invoke two common assumptions: 1 (ex/2+ex/2)2 = 1 4 sech2(x/2), we have (cid:17) = sech2(cid:16) τi (ri,t(θ) 1) , (16) (A1) Small-step/on-policy: ri,t(θ) 1. Thus, log ri,t(θ) ri,t(θ) 1. (A2) Low intra-sequence dispersion: letting zi,t(θ) := log ri,t(θ) and µi(θ) := 1 yi t(zi,t(θ) µi(θ))2 is small for most sequences. the variance Vari(θ) := 1 yi zi,t(θ) = log si(θ), Under (A1), we have SAPO i,t (ri,t(θ)) = sech2(cid:16) τi 2 (ri,t(θ) 1) (cid:17) sech2(cid:16) τi log ri,t(θ) (cid:17) =: gτi (cid:0)zi,t(θ)(cid:1). (17) Average token gates sequence gate By second-order Taylor expansion of the smooth function gτ(z) = sech2( τ 2 z) around µi(θ) = log si(θ), gτi (zi,t(θ)) = gτi (µi(θ)) + τi (µi(θ)) (zi,t(θ) µi(θ)) + 1 2 τi (ξi,t(θ)) (zi,t(θ) µi(θ))2, (18) for some ξi,t(θ) between zi,t(θ) and µi(θ). Averaging over tokens cancels the linear term: (cid:33) (cid:32) gτi (zi,t(θ)) = gτi (µi(θ)) + τi (ξi,t(θ)) (zi,t(θ) µi(θ)) . 1 2 1 yi yi t=1 1 yi yi t= For gτ(z) = sech2(αz) with α = τ 2 , direct calculation gives τ (z) = α2(cid:16) 4 sech2(αz) 6 sech4(αz) (cid:17) , sup τ (z) = 2α2 = τ2 2 . (19) (20) Hence, the average token gate is well-approximated by the sequence gate with uniform bound: Di(θ) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 yi gτi (zi,t(θ)) gτi (µi(θ)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 2 τi sup (z) Vari(θ) = τ2 4 Vari(θ). (21) Starting from (15) and applying ri,t(θ) 1 (A1), we have θJSAPO (cid:34) 1 i=1 1 yi yi t=1 gτi (zi,t(θ)) θ log πθ(yi,t q, yi,<t) (cid:98)Ai . (22) (cid:35) Using (21), we have θJSAPO = (cid:34) (cid:34) 1 1 i=1 i= (cid:0) log si(θ)(cid:1) gτi (cid:32) 1 yi yi t=1 θ log πθ(yi,t q, yi,<t) (cid:98)Ai (cid:33) (cid:35) (cid:0) log si(θ)(cid:1) θ log si(θ) (cid:98)Ai gτi (cid:35) . (23) Thus, under (A1)(A2), SAPO reduces to sequence-level update structurally similar to GSPO with smooth gate gτi (log si(θ)) = sech2( τi 2 log si(θ)). Do the two assumptions (A1) and (A2) hold? We empirically assess the small-step assumption (A1) and the low intra-sequence dispersion assumption (A2) by plotting the histograms of token ratios ri,t(θ) and per-sequence log-ratio variance Vari(θ) in Figures 2 and 3 for both MoE and dense models. The MoE model is cold-start checkpoint of Qwen3-30B-A3B, and the dense model is cold-start checkpoint of Qwen3-4B. The statistics are computed over more than 105 sequences and 109 tokens drawn from off-policy mini-batches. We observe that ri,t(θ) is sharply concentrated around 1, and Vari(θ) typically remains below 0.02, with relatively wider distribution for the MoE model (likely reflecting heterogeneity induced by expert routing) and tighter concentration for the dense model. These distributions indicate that (A1) and (A2) hold in the majority of cases, especially for dense architectures. Moreover, small Di(θ) directly implies that the average token gate is well-approximated by the sequence-level gate, supporting our reduction. Figure 2: Empirical validation of assumptions (A1)(A2) on the MoE model (Qwen3-30B-A3B). Left: histogram of token importance ratios ri,t(θ). Middle: histogram of per-sequence log-ratio variance Vari(θ). Right: scatter of Vari(θ) versus Di(θ). Figure 3: Empirical validation of assumptions (A1)(A2) on the dense model (Qwen3-4B). Left: histogram of token importance ratios ri,t(θ). Middle: histogram of per-sequence log-ratio variance Vari(θ). Right: scatter of Vari(θ) versus Di(θ). Advantages over GSPO Compared with GSPO, SAPO has the following advantages: (1) Smoothness and stability. The soft gate varies continuously with the sequence deviation, avoiding the discontinuities of hard clipping and reducing optimization noise. (2) Token-level adaptivity with sequence-level coherence. Under (A1) and (A2), SAPO acts like sequence-level method; when these conditions are violated (heterogeneous tokens or outliers), SAPO defaults to its token-level gating, selectively down-weighting outliers while preserving informative tokensan ability GSPO lacks. 4.2 SAPOGRPO Connection: Smooth Token Gates vs. Hard Token Clipping GRPOs piecewise-hard token gate For GRPO, GRPO the clipping band. Differentiating yields i,t (ri,t(θ); (cid:98)Ai) is piecewise constant with respect to GRPO i,t (ri,t(θ); (cid:98)Ai) = 1, 0, 1, 0, (cid:98)Ai > 0 and ri,t(θ) 1 + ε, (cid:98)Ai > 0 and ri,t(θ) > 1 + ε, (cid:98)Ai 0 and ri,t(θ) 1 ε, (cid:98)Ai 0 and ri,t(θ) < 1 ε. (24) Hence, GRPO employs binary trust region: tokens inside receive the same gradient as that of the unclipped objective; tokens outside receive zero gradient. i,t (ri,t(θ)) = sech2( τi Advantages over GRPO Compared with GRPO, SAPO replaces the hard indicator in (24) with the smooth kernel SAPO 2 (ri,t(θ) 1)), which avoids gradient vanishing and enables more stable update dynamics. When the policy change is small, gradients remain responsive and permit larger parameter updates; as the deviation grows, gradients shrink smoothly, resulting in more conservative adjustments. In contrast, GRPOs hard token clipping yields an all-or-nothing gate, often leading to brittle and unstable optimization behavior. Figure 4: Training reward and validation performance of cold-start model fine-tuned from Qwen3-30BA3B-Base under different RL algorithms. SAPO exhibits consistently stable learning and achieves higher final performance compared with GSPO and GRPO-R2, both of which experience early training collapse. 4.3 Summary These RL algorithms primarily differ in how they handle off-policy tokens for which ri,t(θ) deviates from 1. From token-level perspective, SAPO provides smooth downweighting mechanism; from sequence-level perspective, SAPO suppresses gradients from extreme off-policy tokens in sequences, thereby constructing more effective sequences for training. In contrast, GRPO and GSPO rely on hard clipping, which is less adaptive than SAPO for optimization."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Controlled Experiments We conduct experiments using cold-start model fine-tuned from Qwen3-30B-A3B-Base on mathematical reasoning queries. We report both the training reward and the validation performance (average Pass@1 over 16 samples) on the AIME25 (AIME, 2025), HMMT25 (HMMT, 2025), and BeyondAIME (Seed et al., 2025) benchmarks. During RL training, each batch of rollout data is divided into four mini-batches for gradient updates. For SAPO, we set τpos = 1.0 and τneg = 1.05 in Equation (6). We compare SAPO with GSPO and GRPO-R2 (i.e., GRPO equipped with routing replay), using the same hyperparameter configurations as in Zheng et al. (2025). Figure 4 shows that SAPO consistently improves model performance across all benchmarks, achieving both higher stability and stronger final performance compared with GSPO and GRPO-R2. While GSPO and GRPO-R2 exhibit early-stage training collapse, SAPO maintains stable training dynamics and ultimately attains superior performance. Notably, SAPO does not rely on routing replay for stabilization or strong performance, which improves exploration and reduces engineering overhead for RL systems. To empirically examine the effect of choosing τneg > τpos, we evaluate three configurations: τneg = 1.05 > τpos = 1.0, τneg = τpos = 1.0, and τneg = 0.95 < τpos = 1.0. As shown in Figure 5, training is most stable when negative tokens are assigned higher temperature (τneg = 1.05) and most unstable when they are assigned lower temperature (τneg = 0.95). These results suggest that gradients associated with negative tokens contribute more strongly to training instability, and that SAPOs asymmetric temperature design effectively mitigates this issue. 7 Figure 5: Training reward and validation performance of cold-start model fine-tuned from Qwen330B-A3B-Base using SAPO with different temperature settings. Using higher temperature for negative tokens (τneg > τpos) leads to the most stable training dynamics, whereas setting τneg < τpos causes significant instability. 5.2 Qwen3-VL Training We apply SAPO to train models in the Qwen3-VL family to assess its effectiveness in practical large-scale settings. Our experiments show that SAPO consistently improves performance across models of varying sizes and across both MoE and dense architectures. We train on broad collection of text and multimodal tasks, including mathematics, coding, and logical reasoning. To support multi-task learning, we maintain fixed sampling ratio for each task within each batch. We also employ large batch size, splitting each batch of rollout data into two mini-batches for gradient updates, ensuring that each mini-batch provides sufficient learning signal for all tasks. To highlight SAPOs advantages over GSPO and GRPO-R2, we evaluate all three reinforcement learning algorithms starting from preliminary cold-start checkpoint of Qwen3-VL-30B-A3B. We report training rewards and mean validation performance on four benchmarks: AIME25 (AIME, 2025) (Pass@1 with 32 samples), LiveCodeBench v6 (Jain et al., 2024) (Pass@1 with 8 samples), ZebraLogic (Lin et al., 2025), and MathVision (Wang et al., 2024). As shown in Figure 6, SAPO achieves steady performance gains throughout training and outperforms both baselines under equal compute budgets."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Soft Adaptive Policy Optimization (SAPO), smooth and token-adaptive reinforcement learning algorithm designed to address the instability and inefficiencies associated with hard-clipped policy optimization in large language models. By replacing discontinuous clipping with temperaturecontrolled soft gate and employing asymmetric temperatures to better regulate negative-token gradients, SAPO provides more stable and informative optimization signal. Empirical results on several mathematical reasoning benchmarks demonstrate that SAPO extends the duration of stable training and achieves higher Pass@1 performance under comparable budgets. Beyond controlled settings, large-scale experiments on Qwen3-VL models further demonstrate that SAPO delivers consistent improvements across diverse text and multimodal tasks and across different model sizes and architectures. These findings suggest that smooth and adaptive gating mechanisms offer promising direction for improving the robustness and effectiveness of RL training for large language models. 8 Figure 6: Training reward and validation performance of Qwen3-VL-30B-A3B from preliminary coldstart initialization, showing that SAPO achieves consistent improvements and outperforms GSPO and GRPO-R2 under the same compute budget."
        },
        {
            "title": "References",
            "content": "AIME. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/ AIMEProblemsandSolutions, 2025. Xing Chen, Dongcui Diao, Hechang Chen, Hengshuai Yao, Haiyin Piao, Zhixiao Sun, Zhiwei Yang, Randy Goebel, Bei Jiang, and Yi Chang. The sufficiency of off-policyness and soft clipping: Ppo is still insufficient according to an off-policy measure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 70787086, 2023. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. HMMT. Hmmt 2025. https://www.hmmt.org, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Team Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Inc."
    ]
}