{
    "paper_title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
    "authors": [
        "Mingzhe Zheng",
        "Yongqi Xu",
        "Haojian Huang",
        "Xuran Ma",
        "Yexin Liu",
        "Wenjie Shu",
        "Yatian Pang",
        "Feilong Tang",
        "Qifeng Chen",
        "Harry Yang",
        "Ser-Nam Lim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 9 5 2 2 0 . 2 1 4 2 : r VideoGen-of-Thought: Collaborative Framework for Multi-Shot Video Generation Mingzhe Zheng1,6, Yongqi Xu2,6, Haojian Huang3, Xuran Ma1,6, Yexin Liu1,6, Wenjie Shu1,6, Yatian Pang4,6, Feilong Tang1,6, Qifeng Chen1, Harry Yang1,6, Ser-Nam Lim5,6, 1 Hong Kong University of Science and Technology 4 National University of Singapore 2 Peking University 3 University of Hong Kong 5 University of Central Florida 6 Everlyn AI Figure 1. Illustration of VideoGen-of-Thought (VGoT). (a) Comparison of existing methods with VGoT in multi-shot video generation. Existing methods struggle with maintaining consistency and logical coherence across multiple shots, while VGoT effectively addresses these challenges through multi-shot generation approach. (b) Overview of our proposed framework VGoT, which is consist of the Script Module which generates detailed shot descriptions from five domains, the KeyFrame Module to create keyframes from scripts, the Shot-Level Video Module which synthesizes video latents on conditional with keyframes and scripts, and the Smooth Module ensures seamless transitions across shots, resulting in cohesive video narrative."
        },
        {
            "title": "Abstract",
            "content": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movielike videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining logical storyline and visual consistency across multiple shots of cohesive script since they are often trained with single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into structured, modular sequence, including (1) Script Generation, which translates curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures consistent multishot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate crossshot smoothing mechanism, which integrates reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos. Project page: https://cheliosoops.github.io/VGoT 1. Introduction Recent advancements in video generation techniques have yielded impressive results, particularly in creating short, visually appealing clips [6, 8, 9, 28, 69]. These advancements have been powered by increasingly sophisticated generative models, ranging from diffusion models [6, 29, 49, 56] to auto-regressive models [20, 39, 62, 65], supported by largescale datasets [31, 50, 51]. These methods have enabled the generation of high-quality, realistic short videos. However, generating multi-shot videos from brief user input script remains substantial challenge. Despite existing works successfully achieving long-term video generation, the primary focus has been on extending the duration of single-shot videos [19, 35, 40, 61]. In contrast, multi-shot video generation involves additional complexities in ensuring coherence across different shots. Moreover, while simply scaling up models or using larger datasets, as done in single-shot long video generation methods [45, 63], can enhance the visual quality of individual clips, it requires vast computational resources and extensive training times. More importantly, they fail to solve the challenges inherent in multishot video generation, which is more complex task that requires maintaining reasonable logic in the narrative, and visual consistency across different scenesall of which are essential for producing multi-shot, movie-like content. The limitations of existing approaches underscore fundamental gap in the field: increasing model size and dataset volume alone is insufficient for generating reasonable and consistent videos with multiple shots. The primary challenge is not solely to achieve high visual fidelity but also to ensure that each shot seamlessly connects to the next, thereby contributing to coherent and compelling narrative structure. When applied to multi-shot video generation, we observed that current methods often fall short in maintaining consistent storyline and visual dynamics across multiple shots [8, 9, 69]. The absence of structured approach for organizing and managing the generation process often results in breakdown of narrative flow, with logical transitions between scenes being lost. Therefore, there is pressing need for an approach that effectively balances visual quality with narrative consistency, managing the generation process without relying purely on computational scaling. Inspired by the thought process behind the creation of real movies, we propose VideoGen-of-Thought (VGoT)a collaborative framework designed to tackle these challenges and create multi-shot videos in step-by-step manner. An existing approach with similar manner called Chain of Thought [64] has been proven efficient in Large Language Models (LLMs) [1, 15] and some visual perception tasks [18]. In this paper, we extend this approach to video generation by breaking down the complex process into four distinct yet collaborative modules as shown in Fig. 1: Script Generation Module converts an simple and high-level story description into detailed prompts for each shot to provide solid narrative foundation; Keyframe Generation Module generates visually consistent keyframes based on the script, ensuring consistency in the appearance of characters, environments, and objects; Shot-Level Video Module produces video latents from corresponding keyframes and prompts at each shot that represents shot-level video clip; finally, the Smooth Module integrates cross-shot smoothing mechanism that processes information embeddings for the upcoming shot when the current one is nearing completion, ensuring seamless connection facilitated by reset boundary. This mechanism provides smooth transition between shots, resulting in an end-to-end shot-by-shot video generation process that maintains visual and narrative continuity across the entire video. As text-to-video [53, 57] (T2V) architecture, our method VGoT employs structured prompt design approach inspired by cinematic scriptwriting to ensure the generated videos are reasonable. Given one-sentence script as user input, our method first prepares it into short descriptions for each shot and then extends them into five domains including character, background, relations, camera pose, and High Dynamic Range (HDR) descriptionswhich together form comprehensive movie-like script, as shown in the left part of Fig 2. Reasonability here refers to the logical flow and narrative coherence of the generated video, ensuring that each shot contributes meaningfully to the storyline. Furthermore, our method also tackles the issue of consistency across shots by introducing mechanisms for both temporal and visual coherence. Specifically, we use identity-preserving (IP) embeddings from avatars automatically derived from the narrative to maintain consistent character portrayal throughout the video. Additionally, our cross-shot smoothing mechanism, which leverages latent features from adjacent shots, ensures seamless transitions, resulting in cohesive and consistent multi-shot video. Our experimental results demonstrate the efficacy of VGoT in generating high-quality, logically consistent, multi-shot videos. We evaluated VGoT using ten different stories, and fair comparison with existing baselines highlights the superior performance of our approach in terms of narrative coherence, visual quality, and cross-shot consistency. The main contributions of this paper are as follows: 1. We propose collaborative, training-free architecture VideoGen-of-Thought for multi-shot video generation, addressing the key challenges of reasonability and consistency across multiple shots. 2. We introduce structured prompt design that emulates cinematic scriptwriting, ensuring logical coherence and narrative depth in generated videos. 3. We develop cross-shot consistency mechanism using identity-preserving embeddings and cross-shot smoothing technique to maintain visual continuity and temporal coherence across different shots. require significant manual intervention to achieve consistent narrative, limiting scalability for fully automated storytelling. In contrast, our work provides detailed and integrated framework that not only ensures cross-shot coherence through novel smoothing mechanism but also employs rigorous evaluation methods to achieve both logical consistency and scalability, setting new paradigm for multi-shot video generation. 2. Related Work 3. Preliminaries and Problem Formulation Video generation has progressed significantly following the great success of diffusion models, leading to two important long video synthesis and multi-shot research categories: story generation. These areas focus on generating highquality, consistent videos either as extended single shots or as coherent sequences across multiple scenes. Long Video Synthesis. Long video synthesis has advanced through diffusion-based methods and autoregressive approaches. Diffusion models based on the Latent Diffusion Model [49] utilize iterative refinement to generate visually consistent frames and have been effective for short sequences and other domains [69, 16, 22, 25, 26, 32, 38, 41 44, 46, 54, 58, 66, 68, 71, 76, 7880, 82]. However, they struggle with maintaining coherence over extended video lengths. Autoregressive methods [20, 36, 73] predict frames sequentially but often face error accumulation, making long-term consistency challenging and computationally expensive. Additionally, training-free methods like FIFODiffusion [33] generate long sequences without training but lack mechanisms to manage transitions across shots, limiting their effectiveness in narrative-driven content. Overall, while these approaches achieve visual fidelity, they fail to ensure logical coherence across extended sequences. In contrast, VideoGen-of-Thought (VGOT) leverages modular approach that includes cross-shot smoothing mechanisms to ensure both visual consistency and narrative coherence, offering more holistic solution for generating longform videos. Multiple Shot Story Generation. Multiple Shot Story Generation focuses on maintaining narrative coherency across distinct scenes. Animate-a-Story [27] uses retrievalaugmented generation to ensure visual consistency but struggles with maintaining logical narrative transitions. DreamFactory [67] employs multi-agent system for collaboration across robust mechashots, but nisms for logical continuity, leading to inconsistencies. MovieDreamer [81] uses hierarchical framework to manage coherence but relies heavily on predefined structures, limiting adaptability to dynamic stories. Flexifilm [40] attempts to address scene adaptability by incorporating flexible conditions, but it still falls short in ensuring logical cohesion between shots and often requires manual adjustments for consistency. Collectively, these methods often lacks 3.1. Preliminaries Diffusion Models [29, 55, 56] are generative models that trained to approximate data distribution p(x) from random noise ϵ (0, I). In the forward process, noise scheduled by the variance βt is gradually added as q(xtxt1) = 1 βtxt1, βtI) over time steps, resulting in (xt; noisy latent variables xT . Then learnable model µθ parameterized by θ aims to denoise xT back to the original data sample x0, with parameterized distribution given by pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)) where µθ and Σθ are predicted mean and variance. The objective is to minimize the difference between predicted and true noise through the loss function: Luncond diff = Ex0,ϵN (0,I),t (cid:2)ϵ ϵθ(xt, t)2(cid:3) (1) In the reverse process, an overall sample queue is maintained as [xT ; xT 1; . . . ; x0] at the pixel level. Latent diffusion models [49] successfully map this process into lowerdimensional latent space using variational autoencoder (VAE) [10, 34, 37, 70]. This transformation significantly enhances flexibility, allowing additional conditions to be incorporated through cross-attention mechanisms [60], which can be formulated as: Attention(Q, K, ) = Softmax (cid:18) QK dk (cid:19) (2) where is derived from the noisy image embeddings ei, and and are derived from the text embeddings et in text-to-image tasks. This mechanism has been further extended to the temporal dimension in conditional diffusion models for video generation [6, 8, 26, 28, 74, 75, 77]. For video RF HW 3 with frames and spatial resolution , series of latent codes {zf }F 1 =0 , where zf Rhwd with channels will be denoise in the same level. These video diffusion models, denoted as ϵθ(zt, t, c), are capable of predicting ˆz, which can be decoded into video frames based on given conditions c. While most existing methods aim to achieve long video generation through enlarging the shape of latent code and the size of model parameters θ, FIFO-Diffusion [33] provides new approach through denoising queue of latents Figure 2. The FlowChart of VideoGen-of-Thought. Left: Shot descriptions are generated based on user prompts, describing various attributes including character details, background, relations, camera pose, and lighting HDR. Pre-shot descriptions provide broader context for the upcoming scenes. Middle Top: Keyframes are generated using text-to-image diffusion model conditioned with identitypreserving (IP) embeddings, which ensures consistent representation of characters throughout the shots. IP portraits help maintain visual identity consistency. Right: The shot-level video clips are generated from keyframes, followed by shot-by-shot smooth inference to ensure temporal consistency across different shots. This collaborative framework ultimately produces cohesive narrative-driven video. , updating with DDIM[55] sampler Φ Reasonability: Ensuring the generated video maintains Qk = {zf }fk+n as: fk Qk Φ(Qk, τk, c; ϵθ) (3) for each timestep τk [0, n] from different level, in effect implementing long-term frame-by-frame prediction. Despite FIFO excelling in frame-by-frame prediction, it fails to handle the quick transition for different shots in multi-shot video generation. Moreover, as text guided inference method, FIFO cant receive conditions like image. In our work, we use the generative capability of diffusion models and process cross-shot smoothness with FIFO-like mechanism at the shot level. 3.2. Problem Definition Given an one-sentence user input containing number of shots, e.g., set of one-sentence prompts, 30 shots, describe story of classic American woman Marys life, from birth to death, our method aims to generate multi-shot, reasonable, and consistent video with minimal manual intervention. The key challenges are defined as: Multi-Shot: Generating minute-level video containing plenty of shots. logical narrative and storyline. Consistency: Maintaining temporal and identity consistency across different shots. (VGoT), Our proposed solution, VideoGen-of-Thought starts from preparing the script {pi}N 1 for each shot, i=0 drafting keyframes {Ii}N 1 i=0 with identity-preserving, then generating video latents {zf }F 1 fi=0 with cross-shot smooth mechanism within shots. VGoT provides novel approach to overcoming the limitations of existing multiple shots video generation methods. 4. Method: VideoGen-of-Thought In this section, we introduce each module in our proposed framework, VideoGen-of-Thought (VGoT), including Script Generation Module, Keyframe Generation Module, ShotLevel Video Generation Module, and Cross-Shot Smooth Module. By breaking down the complex task of long video generation into series of smaller, well-defined modules, VGoT systematically addresses the key challenges of video length, logic, and cross-shot consistency. This modular decomposition allows each component to focus on specific aspects of the problem, resulting in long, reasonable, and consistent video results. 4.1. Module 1: Script Generation The script generation task is modeled as sequence generation problem using pretrained large language model (LLM) [1, 4, 15]. The process starts with short onesentence user prompt with the number of shots defined, e.g., set of one-sentence prompts, 30 shots, describe story about three avatars (two friends Mike and Jane, one bad guy Tom) and their Fantasy Adventure in an ancient pyramid. We leverage an LLM to prepare short shot descriptions = {si}N 1 for each shot, which builds the i=0 foundation logic for the whole story. The script pi for one shot can be fulfilled from si as: pi(pcha, pb, pr, pcam, ph) = MLLM(si, i) (4) where pcha represents the character description, pb is the background description, pr describes the relation between characters and elements in the scene, pcam describes the camera pose, and ph defines the HDR lighting details. An example is shown in the left top of Figure 3. The LLM is conditioned on both the current si and the previously generated prompts pi1 to maintain narrative coherence and avoid abrupt transitions between scenes. The algorithm for script generation is iterative, producing prompt for each video shot as shown in Algorithm 1. Algorithm 1 Script Generation for Long Video Require: User input S, total shot number 1: Initialize , ptmp 2: Initialize Story = {s1, s2, . . . , sN } with 3: for each short shot description si do 4: 5: 6: 7: end for 8: return Prompt Sequence = {p1, p2, . . . , pN } pi MLLM(si, i) Append pi to ptmp pi 4.2. Module 2: Keyframe Generation The keyframes generation task is based on the scripts prepared in the Script Module. For each shot, we use pretrained CLIP [47] encoder Etext to obtain text embedding eT = Etext(pi). We leverage the knowledge of LLM used in script module to create some avatar descriptions for this story based on short shot descriptions S. For instance, given the of Marys life provided in Sec 3.2, LLM provides the avatar prompts Pa = [Pa1, Pa2, . . . ] of Child Mary, Teenager Mary, Mid-aged Mary, Elder-aged Mary, and Old Mary from five domains defined in Eq 4, and specify each shot prompt with corresponding avatar. As introduced in section 3, we use pretrained text-toimage diffusion models MI [49, 59] to generate Ia = [MI (Pa1), MI (Pa2), . . . ] representing the avatar appearance from Pa through cross-attention [60]. Then we employ IP-Adapter [72] to decouple the key and value in the cross-attention in MI by introducing an additional block that involves the identity-preserving embedding from Ia. This mechanism ensures that each generated keyframe not only matches the given prompt but also maintains visual consistency with the given avatar through copied and . The generation of each keyframe can be formalized as: Ii = MI (eT , eI ) (5) where eT is the text embedding from pi as mentioned above, eI is the image embedding of the jth IP avatar selected for ith shot which is obtained by CLIP vision model [47]. This approach ensures keyframes {Ii}N 1 i=0 maintain visual consistency in the visual appearance of characters and backgrounds style for the same avatar in different shots. 4.3. Module 3: Shot-level Video Generation Through Script Module and KeyFrame Module, we have already got the scripts and keyframes required by each shotlevel video. We employ DynamiCrafter [68] as conditional video model MV that is capable of taking both the keyframes and the corresponding textual prompts as input to generate series of latents Zi = [z1, z2, ..., zk] representing the ith shot video clip Vi which includes frames, and this process can be formulated as: Zi = MV(eT , eI , ϵ), (6) and eI where the eT are the text embedding and image embedding of shot i, Zi Rf hwd is series of latent code that can be decoded into the video sequence Vi containing frames generated for shot i, and ϵi is noise map with same shape to initialize Zi . Note that we use short shot description si but not the detailed script pi to obtain the eT , because we found in our experiments that shot-level videos Vi will be less dynamic when given prompts that are too complex even though we obtain much better keyframe through pi. This phenomenon may have been caused by our video model MV receiving simpler textual descriptions during training. As future work, this may be improved if MV is trained with detailed scripts like Eq 4. By incorporating both the keyframe and the textual description to obtain latents in this video generation process, we secure the generation of high-quality, smooth video clips in the subsequent module. 4.4. Module 4: Smooth Mechanism The smoothing mechanism ensures seamless transition from the current shot to the next shot + 1, ultimately achieving overall coherence throughout the final video. Inspired by FIFO [33] that uses latent partitioning to divide Figure 3. Visual comparison of VGoT and baselines Figure 4. Visual showcases of VGoT generated multi-shot videos. latents into several parts to support different noise levels across frames, we implement simple but efficient strategy to secure cross-shot video generation. Since adjacent shots could be dissimilar, we consider it necessary to reset the Table 1. Quantitative comparison with state-of-the-art T2V baselines. We compare average CLIP scores from five domains defined in Eq 4, and the average FC and SC scores within and across shots between VGoT and baseline models. We use bold to highlight the highest and underline for the second high. Model CLIP (pcha) CLIP (pb) CLIP (pr) CLIP (pcam) CLIP (ph) FC (Within-Shot) FC (Cross-Shot) SC (Within-Shot) SC (Cross-Shot) EasyAnimate [69] CogVideo [71] VideoCrafter1 [8] VideoCrafter2 [9] VGoT *FC is denoted as Face Consistency, and SC is denoted as Style Consistency 0.1633 0.1632 0.1535 0.1654 0.1633 0.1130 0.1122 0.1032 0.1140 0.1130 0.4086 0.4113 0.4365 0.4000 0.4086 0.2429 0.2432 0.2417 0.2511 0.2429 0.0722 0.0701 0.0651 0.0694 0.0722 0.4705 0.6099 0.3706 0.5569 0. 0.0268 0.0222 0.0350 0.0686 0.2688 0.7969 0.7424 0.7623 0.7981 0.9717 0.2037 0.2069 0.1867 0.1798 0.4276 Table 2. Human Evaluation. We compare VGoT with baseline models in terms of Within-Shot Consistency, Cross-Shot Consistency, and Visual Quality. Within-Shot Consistency Cross-Shot Consistency Visual Quality Bad Normal Good Bad Normal Good Bad Normal Good EasyAnimate [69] CogVideo [71] VideoCrafter1 [8] VideoCrafter2 [9] VGoT 0.3333 0.1341 0.5446 0.1262 0.0889 0.3232 0.4146 0.2574 0.4854 0.2556 0.3434 0.4512 0.1980 0.3883 0.6556 0.3535 0.2927 0.6436 0.3495 0. 0.3535 0.5976 0.1881 0.3786 0.2444 0.3131 0.2317 0.1683 0.2718 0.6667 0.4646 0.1463 0.6535 0.1748 0.0889 0.2727 0.4512 0.1782 0.4951 0.2111 0.2828 0.5244 0.1683 0.3981 0.7000 noise ϵ (0, I) for {zf }(k(i+1) generated with Eq 6 at =ki shot i, and the length of the reset boundary should be equal to the frame number for each shot. Each reset boundary contains the conditional embedding eT , and series of noise ϵ set with time step scheduler βi as defined in Sec 3. Until the final unit is finished, we can decode the whole {zf }F 1 into final video with shots, and =0 visual demonstration of our generated results is shown in Fig 4. and eI 5. Experiments 5.1. Experiment Settings Datasets Since current datasets cant satisfy our requirements for cross-shot story, as defined in Sec 3.2, we use VGoT to create 10 stories, each containing 30 shots, with user input and 30 short shot scripts S, and detailed 30 shot scripts from five domains as Eq 4, making total of 300 video shots for comparison. Metrics In this work, we mainly utilize three metrics for quantitative evaluation: CLIP score [47], PSNR [17], IS [5], Face Consistency (FC) score and Style Consistency (SC) score. FC score is calculated by similarities between face features captured by InsightFace [2, 3, 11 14, 21, 23, 24, 48] in generated videos, and we obtain SC score by style feature similarities from VGG19 [52]. Specifically, we calculate FC score and SC score from both within one shot and cross shots to evaluate the face identity and visual style consistency. Implementation Details Our experiments were conducted on A800 machines. For module implementation, we employed GPT-4o [1] as LLM, Kolor [59] as text-to-image diffusion model MI , DynamiCrafter [68] as conditional video model MV . For comparison, we also implement EasyAnimate [69], CogVideo [30], VideoCrafter1 [8],VideoCrafter2 [9]. 5.2. Quantitative Evaluation In this section, we use the 10 stories defined in Sec 5.1 as dataset, and provide the one-sentence for VGoT and the detailed scripts of 30 shots for the baselines as input. Then we use the 300 shots for each method to calculate CLIP score from five domains, together with FC score and SC score within and across shots. The quantitative comparison is shown in Table 1. Our method significantly outperforms the baselines in terms of both FC and SC, particularly when evaluated across shots, demonstrating that VGoT effectively maintains character identity and style consistency throughout the entire video, providing seamless transitions. It is also worth noting that the CLIP Scores for our method are not obviously comparable with the baselines. This phenomena is due to the diversity of the generated results, as evidenced by our ablation studies in Sec 5.5, which indicate that VGoT tends to produce richer, more varied content that may not strictly match the prompt but adds to the overall narrative depth and coherence. 5.3. Qualitative Evaluation In this section, we use the one-sentence user input for VGoT, and detailed prompts generated by Script ModTable 3. Ablation Studies. We evaluate the impact of removing key modules from our proposed framework. Metrics include CLIP Score, PSNR, IS, FC score, and SC score CLIP average PSNR IS FC (Within-Shot) FC (Cross-Shot) SC (Within-Shot) SC (Cross-Shot) 24.3265 w/o EP w/o IP 24.3265 EP w/o IP 23.9228 w/o EP IP 25.7857 Full Model *FC is denoted as Face Consistency, and SC is denoted as Style Consistency 0.1146 0.1146 0.1223 0.1111 0.7364 0.7305 0.8745 0.8303 7.4624 7.5783 7.4521 7.5194 0.1129 0.1174 0.3291 0.2738 0.9406 0.9471 0.9486 0. 0.3650 0.3663 0.4186 0.3859 ule as input for baselines, to compare the visual effect of generated video results. Here we take the story set of one-sentence prompts, 30 shots, describe story of classic American woman Marys life, from birth to death. as an example, the comparison of results is demonstrated in Fig 3. Our method obviously achieves better visual consistency from style to identity over baselines. 5.4. Human Evaluation In this section, we present comprehensive user study comparing the generated results from VGoT and four baseline models. We use 10 stories, each consisting of 30 shots, as input and provide 10 users with total of 50 multi-shot videos. Each user is shown 10 multi-shot videos selected randomly from the generated content and asked to evaluate them based on three criteria: within-shot consistency, crossshot consistency, and visual quality, using rating scale of good, normal, or bad. The results, presented in Table 2, demonstrate that the videos produced by VGoT are preferred by users, particularly excelling in cross-shot consistency, highlighting the superiority of our method as acknowledged through human evaluation. 5.5. Ablation Studies In this section, we present ablation studies to evaluate the impact of two key components: (1) Enhanced Prompt (EP) Generation, and (2) Identity-Preserving (IP) Control for keyframe consistency. Both quantitative and qualitative comparisons of VGoT are conducted by systematically removing either EP or IP modules, or both, using the user input: set of one-sentence prompts, 30 shots, describing the journey of Carlos, from young boy on his first bike to becoming celebrated world champion cyclist. The qualitative results, shown in Figure 5, illustrate that the complete VGoT framework (top row) maintains both character identity and narrative depth, while systematically removing EP (second row), IP (third row), or both (bottom row) results in noticeable decline in visual diversity, narrative coherence, and character consistency. The quantitative results, as presented in Table 3, further support our visual findings, with the highest PSNR and IS values indicating superior quality for our complete model. Figure 5. Visual Demonstration of the ablation studies of VGoT Two key insights from Table 3 are worth noting: 1. The full model achieves the lowest CLIP Score across all ablations. This is because the EP module enriches textual descriptions and the IP module preserves consistent character portrayal, resulting in generated visuals that deviate from the original prompts but ultimately enhance the video quality. 2. The removal of EP leads to improvements in FC and SC scores. However, while these metrics reflect increased cross-shot consistency, they do not capture the overall logic and diversity of the storyline. Comparing the middle and right portions of the top and second rows in Figure 5 shows that the absence of EP results in diminished narrative diversity, with visually similar shots benefiting consistency metrics but compromising the storys richness. These ablation studies clearly demonstrate the importance of each component in producing high-quality, logically sound, and consistent multi-shot videos. 6. Conclusion In this paper, we introduce VideoGen-of-Thought (VGoT), modular framework that tackles the challenges of generating multi-shot videos with narrative coherence, visual consistency, and stylistic fidelity. VGoT breaks down video generation into four collaborative modules, ensuring high semantic alignment, keyframe consistency, smooth transitions, and scene-specific guidance. Our evaluations demonstrate that VGoT outperforms existing baselines in maintaining flow consistency, face coherence, and stylistic alignment, offering promising approach for scalable and coherent multi-shot video generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 5, 7 [2] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu. Partial fc: Training 10 million identities on single machine. In ICCVW, 2021. 7 [3] Xiang An, Jiangkang Deng, Jia Guo, Ziyong Feng, Xuhan Zhu, Yang Jing, and Liu Tongliang. Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc. In CVPR, 2022. 7 [4] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. 5 [5] Shane Barratt and Rishi Sharma. note on the inception score. arXiv preprint arXiv: 1801.01973, 2018. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv: 2311.15127, 2023. 2, 3 [7] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2, 3, 7 [9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 2, 3, 7 [10] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinhua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for improving latent video diffusion model. arXiv preprint arXiv: 2409.01199, 2024. 3 [11] Jiankang Deng, Anastasios Roussos, Grigorios Chrysos, Evangelos Ververas, Irene Kotsia, Jie Shen, and Stefanos Zafeiriou. The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking. IJCV, 2018. [12] Jiankang Deng, Jia Guo, Xue Niannan, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. [13] Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong, and Stefanos Zafeiriou. Sub-center arcface: Boosting face recognition by large-scale noisy web faces. In Proceedings of the IEEE Conference on European Conference on Computer Vision, 2020. [14] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In CVPR, 2020. 7 [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 5 [16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 3 [17] Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, and Paulo S. Rodrigues. formal evaluation of psnr as quality measurement parameter for image segmentation algorithms. arXiv preprint arXiv: 1605.07116, 2016. [18] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Forty-first International Conference on Machine Learning, 2024. 2 [19] Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, and Wenwu Zhu. Multi-sentence video grounding for long video generation. arXiv preprint arXiv: 2407.13219, 2024. 2 [20] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In European Conference on Computer Vision, pages 102118. Springer, 2022. 2, 3 [21] Baris Gecer, Jiankang Deng, and Stefanos Zafeiriou. OsIn Proceedings of the tec: One-shot texture completion. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 7 [22] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 3 [23] Jia Guo, Jiankang Deng, Niannan Xue, and Stefanos Zafeiriou. Stacked dense u-nets with dual transformers for robust face alignment. In BMVC, 2018. 7 [24] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. arXiv preprint arXiv:2105.04714, 2021. [25] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [26] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. 3 [27] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 3 [28] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 2, 3 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-tovideo generation via transformers. arXiv preprint arXiv: 2205.15868, 2022. [31] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. European Conference on Computer Vision, 2020. 2 [32] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 3 [33] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. In NeurIPS, 2024. 3, 5 [34] Diederik P. Kingma and M. Welling. Auto-encoding variational bayes. International Conference On Learning Representations, 2013. 3 [35] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. survey on long video generation: Challenges, methods, and prospects. arXiv preprint arXiv: 2403.16407, 2024. 2 [36] Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, and Furu Wei. Arlon: Boosting diffusion transformers with arXiv autoregressive models for long video generation. preprint arXiv: 2410.20502, 2024. [37] Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model. arXiv preprint arXiv:2411.17459, 2024. 3 [38] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, Tanghui Jia, Junwu Zhang, Zhenyu Tang, Yatian Pang, Bin She, Cen Yan, Zhiheng Hu, Xiaoyi Dong, Lin Chen, Zhang Pan, Xing Zhou, Shaoling Dong, Yonghong Tian, and Li Yuan. Open-sora plan: Open-source large video generation model, 2024. 3 [39] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan C. Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, Jui-Chieh Wu, Sen He, Tao Xiang, Jurgen Schmidhuber, and Juan-Manuel Perez-Rua. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv: 2410.20280, 2024. 2 [40] Yichen Ouyang, Hao Zhao, Gaoang Wang, et al. Flexifilm: Long video generation with flexible conditions. arXiv preprint arXiv:2404.18620, 2024. 2, 3 [41] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with auto-regressive latent diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 29202930, 2024. 3 [42] Yatian Pang, Tanghui Jia, Yujun Shi, Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Xing Zhou, Francis EH Tay, and Li Yuan. Envision3d: One image to 3d with anchor views interpolation. arXiv preprint arXiv:2403.08902, 2024. [43] Yatian Pang, Bin Zhu, Bin Lin, Mingzhe Zheng, Francis E. H. Tay, Ser-Nam Lim, Harry Yang, and Li Yuan. Dreamdance: Animating human images by enriching 3d geometry cues from 2d poses, 2024. [44] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 3 [45] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [46] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 3 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5, 7 [48] Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, and Xiaokang Yang. Facial geometric detail recovery via implicit representation. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), 2023. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 5 [50] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv: 2111.02114, 2021. 2 [51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 2 [52] K. Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. International Conference On Learning Representations, 2014. 7 [53] Uriel Singer, Adam Polyak, Thomas Hayes, Xiaoyue Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. International Conference on Learning Representations, 2022. 2 [54] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [55] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. International Conference on Learning Representations, 2020. 3, [56] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, S. Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. International Conference On Learning Representations, 2020. 2, 3 [57] Rui Sun, Yumin Zhang, Tejal Shah, Jiahao Sun, Shuoying Zhang, Wenqi Li, Haoran Duan, Bo Wei, and Rajiv Ranjan. From sora what we can see: survey of text-to-video generation. arXiv preprint arXiv: 2405.10674, 2024. 2 [58] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cyimage-to-3d generacle3d: High-quality and consistent arXiv preprint tion via generation-reconstruction cycle. arXiv:2407.19548, 2024. 3 [59] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 5, 7 [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3, 5 [61] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. [62] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint imagevideo tokenizer for visual generation. arXiv preprint arXiv: 2406.09399, 2024. 2 [63] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv: 2410.02757, 2024. 2 [64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv: 2201.11903, 2022. 2 [65] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, and Zhiwei Xiong. Artv: Auto-regressive text-to-video generation with diffusion models. arXiv preprint arXiv: 2311.18834, 2023. 2 [66] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [67] Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with multi-agent framework. arXiv preprint arXiv: 2408.11788, 2024. 3 [68] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 3, 5, 7 [69] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv: 2405.18991, 2024. 2, 7 [70] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Structured causal disentanglement in variational autoencoder. arXiv preprint arXiv: 2004.08697, 2020. 3 [71] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 7 [72] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 5 [73] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. [74] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 3 [75] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. 3 [76] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv: 2406.18522, 2024. 3 [77] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024. 3 [78] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 3 [79] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pages 115, 2024. [80] Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Xing Zhou, Munan Ning, and Li Yuan. Repaint123: Fast and high-quality one image to 3d generaIn European tion with progressive controllable repainting. Conference on Computer Vision, pages 303320. Springer, 2025. 3 [81] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 3 [82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. 3 VideoGen-of-Thought: Collaborative Framework for Multi-Shot Video Generation"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Preliminaries and Problem Formulation . . . 3.1. Preliminaries . 3.2. Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . 4. Method: VideoGen-of-Thought . . . . . 4.1. Module 1: Script Generation . 4.2. Module 2: Keyframe Generation . . . . . 4.3. Module 3: Shot-level Video Generation . . . . . . . 4.4. Module 4: Smooth Mechanism . . . . . . . . 5. Experiments . . 5.1. Experiment Settings . 5.2. Quantitative Evaluation . . 5.3. Qualitative Evaluation . . . 5.4. Human Evaluation . . . . 5.5. Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6. Conclusion A. Implementation Details of Script Module B. Additional Results C. User Study D. Limitations and Future Work 2 3 3 3 4 5 5 5 5 7 7 7 7 8 8 8 13 13 14 A. Implementation Details of Script Module The Script Module plays fundamental role in converting high-level user input into detailed series of prompts for each shot within the multi-shot video generation process. The specific process is to convert single sentence user input S, into more detailed and structured description S, which is then decomposed into set of prompts = {p1, p2, . . . , pN }, corresponding to each of the shots required for the complete video. This process uniformly adopts large language model (LLM) and uses prompt engineering to ensure the reasonableness of the generated video. This process enables the generation of logical, stepwise narrative structure that serves as the backbone for subsequent keyframe and shot-level generation. For example, consider the user input: e.g., set of onesentence prompts, 30 shots, describe story about three avatars (two friend Mike and Jane, one bad guy Tom) and their fantasy adventure in ancient pyramid.. The LLM would first transform this input into detailed version consisting of 30 one-sentence scripts like s1: Mikes Discovery: Mike examines an ancient map with intense focus, revealing the path to the pyramid. Subsequently, each si in is decomposed into prompts , such as p1: Character: Mike, holding an ancient map with Jane by his side. Background: dense jungle filled with mist and towering trees. Relation: Mike studies the map closely, pointing to pyramid. Camera Pose: Medium shot focusing on Mike and Jane. HDR Description: Soft light filters through the trees, creating dynamic shadows on the map and characters.. This structured approach ensures narrative coherence across multiple shots, laying the foundation for the generation process. B. Additional Results To further illustrate the effectiveness of our proposed VideoGen-of-Thought (VGoT) framework, we present additional qualitative results showcasing the generated videos. Besides the pyramid story with three avatars provided in Sec and materials, VGoT achieves better reasonable storyline and visual consistency in stories about one protagonists growth at different times. We prepared ten showcases in the materials, and large visual representation of VGoTs output is provided in Figure 6, spanning two full pages to ensure sufficient detail and clarity for each generated shot. This figure demonstrates the stepwise progression of narrative shots, showcasing the seamless visual consistency and logical coherence in wide variety of generated multi-shot videos. Moreover, we also provide additional comparison results to illustrate VGoTs advantages over existing stateof-the-art methods. These comparisons include visual comparisons with four baselines: EasyAnimate, CogVideo, VideoCrafter1, and VideoCrafter2. Each comparative example is analyzed in terms of visual consistency, narrative coherence, and overall quality. As shown in Figure 7, VGoT consistently outperforms the baselines in terms of character continuity, background stability, and logical flow across shots. These results highlight VGoTs ability to maintain coherent storytelling while also achieving high-quality visuals. We also prepared the original experiment data record for quantitative evaluation and ablation studies in our provided materials. C. User Study To evaluate the user-perceived quality of videos generated by our VGoT framework, we conducted an extensive user study involving 10 participants. The participants were given 50 accelerated multi-shot videos, each generated either by VGoT or one of four baseline methods. The 10 input stories, consisting of 30 shots each, were randomly assigned to ensure diverse feedback and minimize bias. Each user was presented with 10 videos from different sources and asked to evaluate them on scale of good, normal, or bad, based on three specific criteria: within-shot consistency, crossshot consistency, and visual quality. The results of the user study are summarized in Figure 8. The data indicates that users significantly preferred the videos generated by VGoT, especially regarding cross-shot consistency. Users found that VGoTs videos maintained logical transitions between shots and preserved character appearances across different scenes, reflecting the robustness of our approach. Compared to the baselines, VGoTs results were rated highly for narrative coherence and overall quality, demonstrating the effectiveness of our collaborative multi-shot framework in meeting user preferences. D. Limitations and Future Work While our VGoT framework offers significant advancements in multi-shot video generation, there are still limitations that need to be addressed, along with directions for future work. 1. Single Identity-Preserving Embedding per Shot: Currently, each shot only receives one identity-preserving (IP) embedding, which limits the flexibility to depict complex scenarios involving multiple characters or subjects. Future work will explore extending the framework to handle multi-subject driven shot-level video generation, thereby enhancing narrative richness and visual diversity. 2. Limited Evaluation Metrics: Our current evaluation focuses mainly on face and style consistency, which may not capture all aspects of multi-shot video quality comprehensively. There is lack of detailed multi-shot evaluation metrics to assess narrative flow and cross-shot coherence. Future work will involve researching advanced crossshot metrics and using these metrics to inform and guide model training, potentially improving overall video generation quality. Figure 6. VGoT Visual complement of the multi-camera video generated. Figure 7. Visual comparison of VGoT with baselines Supplement. Figure 8. Designed user study interface. Each participant was required to rate 50 videos by answering three sub-questions for each video. Due to page limitations, only two videos are shown here."
        }
    ],
    "affiliations": [
        "Everlyn AI",
        "Hong Kong University of Science and Technology",
        "National University of Singapore",
        "Peking University",
        "University of Central Florida",
        "University of Hong Kong"
    ]
}