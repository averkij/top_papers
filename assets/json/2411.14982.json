{
    "paper_title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
    "authors": [
        "Kaichen Zhang",
        "Yifei Shen",
        "Bo Li",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 2 8 9 4 1 . 1 1 4 2 : r Large Multi-modal Models Can Interpret Features in Large Multi-modal Models Kaichen Zhang1,2 Yifei Shen Bo Li1,2 Ziwei Liu1,2,(cid:0) 1LMMs-Lab Team 2S-Lab, NTU, Singapore {zhan0564, libo0013, ziwei.liu}@ntu.edu.sg Figure 1. a) The Sparse Autoencoder (SAE) is trained on LLaVA-NeXT data by integrating it into specific layer of the model, with all other components frozen. b) The features learned by the SAE are subsequently interpreted through the proposed auto-explanation pipeline, which analyzes the visual features based on their activation regions. c) It is demonstrated that these features can be employed to steer the models behavior by clamping them to high values."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply Sparse Autoencoder (SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model and demonstrate that these features can effectively steer the models behavior. Our results contribute to deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with *(cid:0)Corresponding author. potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with cognitive process of the human brain. We opensource our codebase and checkpoints at Github 1. Introduction Recently Large Multi-modal Models (LMMs) [3, 8, 20, 21, 36] have significantly advanced the field of computer vision, achieving remarkable success in various applications such as personal assistant, medical diagnosis, and embodied agents [24, 39]. These models have been integrated into commercial products to assist peoples daily life [2, 26] and hold large potential to transform the future. Despite their success, the opaque nature of LMMs often leads to unexpected behaviors, such as the hallucination of non-existent objects and relationships within images [14], as well as vulnerability to jailbreak attacks [7, 33]. These challenges underscore the critical importance of understanding and controlling the neural representations of LMMs. Interpreting LMMs presents several challenges compared to traditional models. One key challenge is the highdimensional, polysemantic nature of their representations. single neuron within these networks may encode multiple semantics, while single semantic can also be distributed across multiple neurons [11, 29]. For example, neuron in the vision features of Inception v1 can respond to both cat faces and car fronts [28]. The larger dimensionality of LMMs compared to conventional models adds more complexity. An efficient algorithm is needed to decompose neural representations into basic components. The second challenge is the vast and open-ended concepts in LMMs. Traditional models, which contain only hundreds of monosemantic concepts such as color, objects, attributes, and layout [4, 31, 34, 42], can be analyzed through extensive human labeling, enabling interpretation of neural representations based on these specific concepts. In contrast, LMMs contain hundreds of thousands of monosemantic concepts across open domains, making human analysis infeasible. This calls for zero-shot approach to detect the concepts, which minimizes human effort. Existing works, such as [5, 13], have demonstrated that larger models, like GPT-4, can be used to interpret neurons in smaller models, such as GPT-2. In this paper, we take an initial step toward exploring this approach in the domain of LMMs. We aim to dissect and understand open-semantic features by applying similar methods to analyze LLaVA-NeXT-8B with the larger LLaVA-OV-72B model. We employ sparse auto-encoders (SAEs) [10, 30], classic interpretability method, to address the first challenge of polysemantic neurons by disentangling them into human-understandable features. In previous works such as [6, 13, 18, 35], the learned features in SAE are proven to be more monosemantic and human-understandable than the neurons. For the second challenge, we develop pipeline for automatic feature discovery in SAEs, taking advantage of LMMs zero-shot abilities. Specifically, for specific learned feature in SAEs, we first identify Top-K images and the areas in those images that mostly activate on the feature. Then the images and patches will be fed into LLaVAOV-72B [17] to examine the common factors and generate explanations. In addition to methodological contributions, our case studies also offer unique insights into LMMs. Firstly, we identify emotional features in LMMs and demonstrate that these features enable LMMs to generate or share emotions. Previous works have highlighted the exceptional capabilities of LMMs in EQ assessments [39] and their ability to read emotions. We extend this investigation by exploring the emotions of LMMs and steering the model to express its own feelings. Secondly, we identify the causes of certain model behaviors and analyze potential reasons for undesired outcomes, such as hallucinations. Adjusting the relevant features can rectify the mistake. Thirdly, some features in LMMs align with those found in the human brain, suggesting that interpreting LMMs could be informative for understanding how the human brain integrates multisensory inputs. For example, we find features that are activated by an action and their corresponding OCR descriptions, which align with the invariant visual features found in the study by [32]. Specifically, our contributions can be summarized as follows: We introduce the use of Sparse Autoencoders (SAEs) to analyze the open-semantic features of Large Multimodal Models (LMMs) and demonstrate, for the first time in multimodal domain, that the features learned by SAE in smaller LMM can be effectively interpreted by larger LMM. We propose an automated pipeline for interpreting these features and further demonstrate that they can be used to steer model behaviors to address specific issues or induce desired outputs. Additionally, we propose method for identifying the underlying causes of model behaviors and offer an analysis of how to address these issues, highlighting the potential for developing more reliable LMMs. 2. Methodology In this section, we present our methodology to disentangle, interpret, and steer the internal representation of LMMs. 2.1. Sparse Auto-encoders for Disentanglement Architecture and loss function: We utilize the SAE architecture outlined in OpenAIs research [13], which consists of two-layer auto-encoder with TopK activation function. Lets denote the input by RT dl , where is the number of tokens and dl is the hidden dimension of Llava. The SAE operates as follows: = TopK(ReLU(W1(x b1) + b2)), ˆx = W2z + b3, (1) (2) where RT ds, where ds is the hidden dimension of SAE, represents the sparse data representation, ˆx is the reconstructed data, and the sets {Wi, bi} are the trainable parameters. The loss function combines the reconstruction error with an auxiliary loss used in [13] to prevent inactive features in z. To understand why SAEs yield monosemantic features, we draw parallels between the components in (1) and those in traditional sparse coding [10, 30]. In this context, W2 acts as an overcomplete dictionary [1] for the input data, with its rows forming the dictionary vectors, and serving as the sparse coefficients corresponding to these vectors. Due to the sparsity of z, the dictionary vectors tend Figure 2. The overview of the explanation pipeline, where images are forwarded through the LMM with the integrated SAE, and the activations of the top 256 most activated features are cached. For each feature, the top 5 images with the highest activations are selected, followed by the execution of zero-shot image explanations using large LMM. to be nearly orthogonal (or mutually incoherent) to minimize reconstruction error. This near orthogonality suggests that the dictionary vectors are almost independent and each coordinate of is expected to be monosemantic. Integrating SAEs into LLaVA: We incorporate SAE into specific layer of LLaVA, where the hidden representation corresponds to in (1). The SAE is trained using the LLaVA-NeXT [16, 20] sft dataset, which contains approximately 779k samples. During forward, we always use the Anyres [20] strategy to process the image tokens. 2.2. Zero-shot Identification of Concepts To identify the open-semantic features, we present pipeline leveraging open-source Large Multimodal Models (LMMs) to identify concepts within LMMs. In this subsection, we only consider the 576 base image features when preparing the exemplars. Identifying the Top Activated Images and Patches: Initially, we pinpoint the most activated images for each coordinate in the latent space vector z. Due to computational resource constraints, we cache subset of images from the LLaVA training dataset and augment it with images from additional datasets [12, 16, 19, 40], with total 46684 images, collectively denoted as D. These images are processed through LLaVA to yield the representation (cid:80) RD576d. The corresponding latent representation in the SAE is RD576ds. By averaging over the second dimension, we obtain the mean activation values = 1 Z[:, j, :] RDds . For each feature 576 in the SAE, we identify the top-5 influential image by selecting the top-5 activations along the first dimension of Z. To determine the specific patch that activates feature, we process the top-k most influential image through LMMs to obtain its representation R5576d and its corresponding SAE latent for feature Zi R5576 where represent one of the feature in ds. In real time, since we are using Top-K SAE, we can cache the using sparse vector RD576k by selecting the Top-K features from the last dimension and reduce the forward number. Automatic Feature Interpretation of LMMs by LMMs: We apply masks to the top activated images, using transparent masks for the most activated patches and black masks for the rest. These masked images are then input into LLaVA-NeXT-OV-72B [17] to detect common patterns. If no common patterns are discernible, the system will return message stating unable to produce explanations. We demonstrate the overall procedure of our explanation pipeline in Fig. 2. Reference Score Calculation: To quantify the relevance of features activation to given concept, we first refine all tokens by setting accordingly. This steering operation is further utilized in the subsequent subsection. 2.4. Localizing the Causes for Model Behaviors In scenarios where LMMs make decisions, it is often crucial to discern whether these decisions are influenced by visionrelated tokens and to determine which specific features are activated. We follow similar approaches in [27, 35, 37] and introduces the technique to identify such relationships in this subsection. We assume the input comprises tokens and the model begins outputting from the (T + 1)-th token, with the decision represented by single token. We denote the output logits for the (T + 1)-th token as u, the current output token id as vc = argmax(u), and baseline token id as vb. Our objective is to ascertain why the LMMs exhibit preference for vc over vb. The difference in logits is defined as: d(u, vc, vb) = u[vc] u[vb]. To locate the causes for the decision, we iterate over every patch and every hidden feature in the SAE. The process involves three steps for each token and each SAE feature j: 1. Apply Steer(i, j, 0) to negate the features impact. 2. Process the modified input through Llava to obtain new logits ˆu. 3. Calculate the influence of the j-th feature in the i-th token on the decision preference for vc over vb: I(i, j, vc, vb) = d( ˆu, vc, vb) d(u, vc, vb). Given the time-intensive nature of this method due to multiple forward passes, we employ linear approximation with the method in [27]: I(i, j, vc, vb) (cid:19)T (cid:18) d(u) ( ˆz z), where ˆz is the SAEs activation post-steering operation. This approximation allows us to estimate attribution of each token as illustrated in [27]. 3. Experiments 3.1. Scaling SAEs for LMMs Dataset and Model Setups: We choose the LLaVA-NeXTLLaMA3-8B [16] as our base model and hooked the SAE on the 25th layer and use the same fine-tuning data from LLaVA-NeXT [20] for training. During training, unlike previous works [6, 13, 35] that used pretrained format text, we format the text and image into ways that looks exactly the same as the supervised fine-tuning stage. We scale our sparse autoencoder with 217 with 8 batch size and 4 gradient accumulation steps. We later tries to scale the features into 218 but observe no loss decrease. We use the same Topk sparse autoencoder settings as Gao et al. [13], Makhzani Figure 3. An overview of the evaluation pipeline for calculating IOU scores. Initially, small LLM is used to refine the explanation into concise description, which is then employed to generate the segmentation mask. The IOU score is subsequently computed by comparing the mask to the binarized activated region. the descriptive text using language models to enhance conciseness. For instance, the verbose description The feature activates on the train tracks ... is condensed to Train tracks. This refinement is performed with smaller LLM to minimize computational expense and time. Following this, GroundingDino-SAM [22] is employed to generate segmentation mask based on the succinct interpretation. Subsequently, we construct composite mask incorporating every object detected in the image. The IoU score between the segmentation mask and our activation mask is then calculated, serving as the reference score for the features relevance. An example of the evaluation process is demonstrated in Fig. 3 where the refined explanation is being sent to the GroundingDino-SAM [22] to produce the segmentation ground truth and calculate the IOU with the activated region. 2.3. Steering the Neural Representation Having interpreted each representation in the SAEs, we explore how to influence the output by steering specific feature within the SAEs. Steering involves adjusting the features value, either increasing or decreasing it. Specifically, steering in SAEs entails setting the i-th hidden representation to predetermined value k. We define the steering operation Steer(C, k, i) in SAEs as follows: = ReLU(W1(x b1) + b2), z[C, j] = ˆz = TopK(z), ˆx = W2 ˆz + b3, (3) (4) (5) (6) where represents the set of tokens designated for steering, is the steering value, and is the index of the feature in the SAE to be steered. Following the steering operation, we input ˆx into the subsequent LLaVA layer instead of x. In the experiments discussed in Section 4.1, we apply steering to Concept Metric scene object part material texture colour Total IOU () CS () IOU () CS () IOU () CS () IOU () CS () IOU () CS () IOU () CS () IOU () CS () Random 0.007 1 103 18.1 6 102 0.005 5 104 18.2 2 102 0.007 2 103 18.1 5 102 0.01 8 103 18.1 1 101 0.007 2 103 18.4 6 102 0.005 2 103 19.6 7 102 0.005 2 104 18.2 1 102 V-Interp (Ours) 0.20 24.4 0.19 24.0 0.21 23.5 0.39 24.1 0.21 20.9 0.10 20.3 0.20 23.6 Table 1. The Intersection over Union (IoU) and CLIP scores for each concept are computed based on the top-5 most activated images. scene object part material texture colour Total GPT-4o Human 0.93 0.70 0.84 0.85 0.9 0.60 1.0 0.95 0.85 0.80 0.92 0. 0.89 0.75 Table 2. Consistency scores for our explanation, computed based on 100 samples per concept for GPT-4o consistency and 10 samples for human consistency, highlighting the agreement between GPT-4o-generated and human-generated explanations. and Frey [25] and select = 256 that similar to the activated features in [35]. Unless otherwise specified, we will use the settings of SAE with 217 features for the rest of this paper. The reason that we choose this large number of features is that we wish our feature to be more splited and informative as similar in the approaches in [1, 35]. 3.2. Interpretaion Pipeline Evaluation Results We present our result in Tab. 1. Due to the same limitation as illustrate in [6, 35], we report the result on 5000 subset of features with around 46684 images for caching the features activations. For the random result, we randomly sampled 5 images from the cache dataset and run 10 times for IOU and 30 times for the CLIP-Score. We then reported the average result of each run along with the 99% confidence interval. We followed the concepts used in [4] and utilize LLaMA-3.1-Instruct-70B [9] to help us label the concept according to its explanation. In Fig. 4, we also present examples that demonstrate the activated region for different concepts and report the IOU scores for each example. Consistency We present the consistency scores for each concept in Tab. 2. To evaluate the consistency of our explanations with the activated image regions, we employ GPT-4 as judge and conduct human study. For GPT consistency, we sample 100 test cases per concept, provided that the total number of samples for the concept exceeds 100. For human consistency, we manually label the correctness of 10 samples for each concept. 4. Probing into the Features In previous section, we demonstrate that we are able to locate and interpret the visual features in LMM using an automatic pipeline. However, what differs LMM from traditional vision model is its conversation, reasoning, and generalization ability across different modalities. On other words, we believe that the features inside LMM are opensemantic and should not be limited into the concepts in [4]. In this section, we probe into the features in our SAE and try to find out how different features contribute to the final result, and how can these features being used to steer models behavior in different scenarioes. 4.1. Case Studies of Emotion Feature When interacting with humans, it is essential for the model to demonstrate empathy and the ability to understand human emotions. In [39], it has been shown that large multimodal models (LMMs) can perceive emotions and Emotional Quotient (EQ), enabling them to understand and resonate with human feelings. Building on this, we specifically investigate the relevant features, exploring how the model comprehends these features and how they influence its reasoning processes. Through examples of various image features and their effects on model responses, we demonstrate that LMMs are capable of: 1) Connecting emotional concepts between text and visual features, such as actions and behaviors; 2) Engaging with human emotions by adjusting the corresponding features to intervene in the reasoning process manually. 3) Response to concept that with invariant features between modalities. Sad We present feature that may be related to the feeling of sadness and explore the potential for enabling the model to share emotional responses. After probing and confirming that the feature aligns with sadness, we investigate whether manipulating this feature could influence the models reasoning to simulate emotional responses. To test this, we use simple prompt, What is your feeling right now? and ask the assistant. Without steering, the model responds in neutral, standard AI assistant tone, showing no emotion. However, when we clamp the sad feature to high value, the model responds with sad as shown in Fig. Happy Beyond simply experiencing feeling, it is also essential for the LMM to interact and share emotions when presented with specific scenario. Toward this goal, we use Figure 4. comparison of several visual concepts and their activated areas. We compare several visual concepts and their corresponding activated areas, showcasing one example for each concept across different features. For each feature, we calculate the IOU by averaging the IOUs from the top-5 activated images. Although some features yield relatively low IOU scores, we find that the explanations are still semantically accurate with respect to the activated regions. Figure 5. The feature that relates to sad. We probe and find out the feature that activated on sad. By clamping this feature, we can enforce the model to share the feeling of sad the same method to probe the feature associated with happiness and provide an image depicting joyful scenario. When asked about its feelings without steering, the model responds that it does not experience emotions. However, similar to the sad feature, when we clamp the happy feature, the model responds with expressions of happiness, as shown in Fig. 6. This demonstrates that the models reasoning process can be effectively influenced. Hungry, Greedy We discovered an intriguing feature that links the text-based emotional concepts of greedy and hungry to visual representations of the action eating and the word hungry. We notice that the feature activates in response to the word hungry in the image, suggesting that it Figure 6. The feature that relates to happy. We find out that the feature is related with joy and celebrate action that relate to happiness. By clamping this feature, we can enforce the model to share the feeling happiness with others. connects not only to the action eat but can also extend to broader concepts. To test this, we clamp this feature and prompt the model with Tell me story about Alice and Bob; the generated response revolves around themes of greed as shown in Fig. 7. This demonstrates that the model can reason from the visual action eat to broader concept encompassing greedy and hungry with unified view. 4.2. Low Level Perception Features One important distinction we observed in our features, as compared to those found in Large Language Models in previous studies [6, 13, 35], is the presence of numerous lowlevel visual features. These features typically represent baFigure 7. feature that relates to the concept eat. We further investigate about the concept behind this feature and find out that model can reason from visual action eat into the concept concept and greedy Figure 8. An example of the hallucination on LLaVA. Bolivia is not shown on the image but the model still answer yes. sic concepts related to common visual elements and often exhibit high activation across most images. For example, these features may be associated with color, shape, or fundamental visual patterns, such as mesh structures or repetitive designs. In the example shown in Fig. 6, the feature associated with happy is only the 78th most activated feature, with many other low-level concepts present as well. We showcase some of these features in Sec. 12, highlighting key difference between LMMs and LLMs. 4.3. Localizing the Cause for Model Behaviors In Sec. 2.4, we mention the patching method that used to locate the cause for the models output. This has been treated as viewing the features as models intermediate steps in [35]. In this section, we use hallucination example to deeply study this process in LMM. As shown in Fig. 8, we provide an example from HallusionBench [14] that LLaVA hallucinates on the image and answer Yes even if the image does not shown anything about Bolivia. To study the cause for this output, we set the answer token vc = yes and vb = no and apply the algorithm with calculate per token contribution. By employing this strategy, we can filter out features that causing the model to be Figure 9. Feature that relates to the text Barcelona. By clamping this feature to high value, we intervene the reasoning steps and hallucination in Fig. 8 disappears. more favored to answer yes instead of answer no. To be specifically in this case, we mainly focus on two points: 1) How does the model reason from the image and is the model reason from the correct path? 2) If the model is paying the correct attention in the image, which part of the text part is causing the hallucination. To answer these two questions, we sort features by their attribution effect on image and text separately and observe their common part with high attribution. Image Attribution In Fig. 10, we visualize image patches for common areas with high attribution among the features. To do this, we first filtered out the top 10 features with the highest attribution towards the final output yes and visualized their attribution map. Examining these top features reveals that they primarily contribute to tokens associated with text elements, such as map legends, country names, and other key visual details. This observation demonstrates that the model is effectively focusing on relevant areas of the image and has the ability to accurately identifying where to extract necessary information. However, even with the correct visual perception ability, the model still fail to produce the final answer. Text Attribution To further investigate the source of the incorrect answer, we continue visualizing the attribution of text tokens in the question. As shown in Fig. 10, the token Bolivia contributes most to features with high attribution toward the answer yes. Additionally, tokens like to and the, along with concepts such as Amazon Basin, also have positive attribution to the hallucinated answer Figure 10. The high attribution area of different images and on the text. For images, we observe that features with high attribution mostly activate on positions that relate to key information about the question. For text, we observe that the Bolivia token contributes the most to the answer yes of reading the image as it can focus on the correct part of the image but it is being affected by the text tokens and approaches the question without answering the question on image. In this subsection, we focus on how can we utilize the steering effect to intervene the reasoning steps for the model to get the correct answer. To address this issue, we first consider which features can encourage the model to focus on text tokens in the image rather than the question text. We hypothesize that clamping activations for certain OCR features related to image text may prompt the model to prioritize image-based features over question text features. We identify two such features that reduce hallucinations. In Fig. 9, we locate feature linked to the word Barcelona and clamp it to high value, leading the model to rely on image information rather than general geographical knowledge. To validate this, we identify feature related to the word Los on traffic signs and similarly clamp it in Fig. 11, prompting the model to conclude that Bolivia is absent in the image. This shows that the model can use image information but may sometimes follow incorrect reasoning. With minimal intervention, we successfully guide the model to prioritize image information over question text. 5. Conclusion In summary, we present an in-depth examination on the internal structure of the LMM and introduce an automated pipeline for interpreting its open-semantic features. Furthermore, we propose methods for steering the models behavior through these features and for identifying the sources of errors within the LMM. By systematically analyzing both Figure 11. Feature that relates to the text Los. We validate our assumption by finding another feature that relates to text and mitigate the hallucination. yes. This partially explains why the model responds with yes instead of no, even after extracting useful information from the image. While reasoning from visual features, the model is also influenced by text, leading it to approach the question with its pretrained knowledge. 4.4. Application of Model Steering on Hallucination After identifying the cause for causing the hallucination, we start to wonder how can we fix this hallucination by using steering. We are now assure that the model has the ability the structural and functional aspects of the model, we aim to offer valuable insights into its interpretability and reliability. We hope that these findings will not only contribute to the advancement of research in this area but also stimulate further exploration into this area."
        },
        {
            "title": "References",
            "content": "[1] Alekh Agarwal, Animashree Anandkumar, Prateek Jain, and Praneeth Netrapalli. Learning sparsely used overcomplete dictionaries via alternating minimization, 2014. [2] Apple. Apple intelligence is available today on iphone, https : / / www . apple . com / sg / ipad and mac. newsroom/2024/10/appleintelligenceisavailable-today-on-iphone-ipad-and-mac/, 2024. [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 65416549, 2017. [5] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. https://openaipublic. blob.core.windows.net/neuronexplainer/ paper/index.html, 2023. [6] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html. [7] Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, and Jindong Gu. Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. [10] Michael Elad. Sparse and redundant representations: from theory to applications in signal and image processing. Springer Science & Business Media, 2010. [11] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac HatfieldDodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. [12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [13] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders, 2024. [14] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large visionlanguage models, 2023. [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [16] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. [18] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2, 2024. [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. [20] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [23] Sheng Liu, Haotian Ye, and James Zou. Reducing hallucinations in vision-language models via latent space steering. arXiv preprint arXiv:2410.15778, 2024. [24] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. [25] Alireza Makhzani and Brendan Frey. k-sparse autoencoders, 2014. [26] Meta. Introducing orion: Our first true augmented reality glasses. https://about.fb.com/news/2024/ 09 / introducing - orion - our - first - true - augmented-reality-glasses/, 2024. industrial scale. [27] Neel Nanda. Attribution patching: Activation patching https : / / www . neelnanda . at io / mechanistic - interpretability / attribution - patching, 2023. Accessed: 202409-30. [28] Chris Olah, Alexander Mordvintsev, Schubert. https://distill.pub/2017/feature-visualization. Feature visualization. and Ludwig Distill, 2017. [29] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024001, 2020. [30] Bruno Olshausen and David Field. Emergence of simplecell receptive field properties by learning sparse code for natural images. Nature, 381(6583):607609, 1996. [31] Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, and Matthieu Cord. concept-based explainability framework for large multimodal models, 2024. [32] Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Invariant visual representation by Koch, and Itzhak Fried. single neurons in the human brain. Nature, 435(7045):1102 1107, 2005. [33] Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, and Ethan Perez. When do universal image jailbreaks transfer between vision-language models?, 2024. [34] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15321540, 2021. [35] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. [36] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. [37] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in gpt-2 small, 2022. [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [39] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. [40] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. [41] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. [42] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856, 2014. Large Multi-modal Models Can Interpret Features in Large Multi-modal Models"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Related Works Dictionary Learning Dictionary learning is common approach for problems like ours, where we aim to extract set of features from collection of dense vectors. Sparse autoencoders (SAEs), proposed by [10, 30], have been used as classic interpretability method to address this challenge. SAEs are designed to identify mutually incoherent bases in data and represent the data as sparse linear combinations of these bases. Existing studies have applied SAEs to LLMs, finding that the bases represent monosemantic features in the data, with the coefficients indicating the activation of these features[13, 18, 35]. Large Multimodal Models With the development of large language models (LLMs), the performance of large multimodal models has also advanced rapidly, demonstrating strong results across various tasks [3, 16, 20, 38]. Studies such as [23, 31] have explored methods to understand or manipulate the internal structure of LMMs. In our work, we take an initial step toward evaluating and interpreting the open-semantic features within LMMs. 7. Limitations Our work primarily focuses on the LLaVA-NeXT-LLaMA8B model and specific layer within it. This focus on particular model and layer is based on the assumption of universality and disentanglement, as discussed in [6, 35]. However, this assumption may contribute to inaccuracies in interpretation and model steering. Due to limitations in computational complexity and storage, we were unable to prepare sufficiently large and diverse cached image dataset to accurately interpret the image features. Consequently, we present our results on subset of features and may have mistakenly classified some features as inactive. 8. Detail about Prompt We detail the prompts used in different stages of the automated pipeline in this section. The prompt for zero-shot identification of concepts is provided in Tab. 3. For this task, we utilize the LLaVA-NeXT-OV-72B model [17]. To refine labels and categorize explanations using large language models (LLMs), we use the prompts detailed in Tabs. 5 and 6. Specifically, LLaMA-3.1-Instruct-8B is used for label refinement, while LLaMA-3.1-Instruct-70B is employed for categorizing explanations. For high-throughput performance, the models are served using SGLang [41]. Figure 12. The feature related to money and its steering effect. Figure 13. The feature related to speech and its steering effect. Figure 14. The feature related to unix and its steering effect. 9. More Steering Examples In this section, we provide more steering examples that we discover during experiments. We perform large scale steering on the 5000 size features subset we choose and then filtered some interesting examples here. In Fig. 12, the feaPrompt : Zero-shot Identification of Concepts You are meticulous AI researcher conducting an important investigation into certain neuron in vision language model. (cid:44) Your task is to analyze the neuron and provide an explanation that thoroughly encapsulates its behavior. [REQUIREMENTS] 1. Focus only on the highlighted region in each image. If no region is highlighted or if the highlighted region is minimal (e.g., (cid:44) few bright spots), ignore the image. 2. Identify common visual patterns, objects, or concepts in the activated regions. For example, note if highlighted areas show (cid:44) consistent structures, such as mesh patterns or similar objects. [GUIDELINES] You will receive series of images where specific regions have been highlighted to indicate neuron activation. Nonhighlighted (cid:44) areas will be masked out or dimmed. Your analysis should consider only the highlighted regions and complete the (cid:44) following tasks: 1. Describe Only the Highlighted Regions: Generate captions solely based on the highlighted regions. If no meaningful pattern (cid:44) is visible, or if only few scattered spots are highlighted, output: [EXPLANATION]: Unable to produce (cid:44) descriptions. 2. Concise Description Only: Provide short, direct description of the common features within the highlighted regions. Avoid (cid:44) any interpretive languagesimply state what you see, such as meshlike structures or actions related to joy or (cid:44) happiness 3. Output Format: Begin each response with [EXPLANATION]: followed by your explanation, if applicable. Ensure the (cid:44) last line of your output follows this format. If unable to determine common visual features, output: [EXPLANATION]: Unable to produce descriptions Table 3. The prompt for zero-shot identification of concepts Prompt : GPT-consistency Evaluation [GUIDELINES] You are an AI assistant to help assessing whether the generated explanation is consistent with the activation area in the image. (cid:44) The activation area is being highlighted in the image and an explanation is provided for the activation area. You should output: 0 if the explanation is not consistent with the activation area in the image. 1 if the explanation is consistent with the activation area in the image. Please strictly follow the [GUIDELINES] and do not output anything other than the number 0 or Here is the explanation: {explanation} ANSWER : Table 4. The prompt to ask GPT to evaluate the correctness of the evaluation Prompt : Categorize explanation concept [GUIDELINES] You are an AI assistant tasked with assigning single label based on the given input text. Each input will contain description (cid:44) of visual feature, which you must categorize into one of the following classes: scene Describes scene or environment. object Describes an object or entity. part Describes part or aspect of an object. material Describes material or substance that constitutes other objects. texture Describes the texture of an object. color Describes the color of an object. Please provide only the class label from the list [scene, object, part, material, texture, color] with no additional text. Only one (cid:44) label should be chosen. Make sure you only choose from the classes listed above and do not output any other classes. Categorize the following description: {description} ANSWER: Table 5. The prompt that use to label concept for each description Figure 15. The feature related to chair and its steering effect. Figure 16. The feature related to money and its steering effect. ture activates on money and when this feature is clamped to 50, the model output story about saving funds and by house. In Fig. 13, when feature relates to feature that relate to speech, the model output story about man who is speech master. In Fig. 14, we found feature that relate to unix/linux and its steering effect would output story about Ubuntu. More interestingly, in Fig. 15, though the model response on visual chair object, when steering this feature, model would output story relates to chairman instead of chair. Another example is that in Fig. 16, when steering this feature related to key or keychain, the model output story about developing some analytic software. 10. CLIP-Score and IOU details We use Grounding DINO [22] as our grounding module and SAM Huge [15] as our segment module. The output from the interpretation pipeline is being refined into concise description by using the LLaMA-3.1-Instruct-8B [9]. We use ViT-B/32 CLIP model to generate embeddings and calculate the cosine similarity between the interpretations and the image. We calculate the IOU and the CLIP-Score using the top-5 activated images for each features. Due to the same limitation as illustrate in [6, 35], we report the result on 5000 subset of features with around 46684 images for caching the features activations. Prompt : Refine Interpretation [GUIDELINES] You are an AI assistant tasked with extracting meaningful labels from descriptions. You will receive description that may (cid:44) contain references to various entities, and your job is to rephrase and extract the key entities from the text. You will (cid:44) encounter several types of descriptions, and examples for each case are provided below. Please follow the given (cid:44) instructions carefully. When presenting your answer, first output [ANSWER], followed by the extracted entity. Thank you! Case 1: Good Description In this case, the description clearly identifies the entity. Examples: Description: The cell phone. Output: [ANSWER] The cell phone Description: The letters on the shipping containers. Output: [ANSWER] The letters on the shipping containers Case 2: Description includes additional words In this case, the description contains more information than needed. Extract only the key entity. Examples: Description: The images all display different models of Honda vehicles, suggesting the neuron is activated by the presence of (cid:44) Honda vehicles or the Honda logo. Output: [ANSWER] Honda vehicles Description: The neuron seems to be reacting to the word ORD on the billboard. It could be part of larger word or phrase, (cid:44) but the neuron specifically highlights the letters ORD. This suggests that the neuron might be specialized in (cid:44) recognizing or processing certain words or characters in images. The activation across the images indicates that the (cid:44) neuron is consistent in its response to textual elements, particularly those that include the ORD sequence. Output: [ANSWER] The word ORD Case 3: Bad Description In this case, the description does not provide sufficient or valid information. Examples: Description: Unable to produce descriptions. Output: Unable to produce descriptions [Description] {description} Table 6. The prompt that used to refine the explanations for grounding and segment visual objects 11. Feature Probing Due to the large number of features, identifying specific features of interest is challenging, and interpreting all available features before making selection is impractical. Following Templeton et al. [35], we also probe into the features of our SAE to identify several emotion-related features that may influence the models perceived emotional responses. We first prepare an image representing specific emotion, then select the top-k activated features for that image to run through our explanation and steering pipeline. From the output, we manually select the desired features and validate them through steering and activated examples. Unlike the approach in [35], which uses only the top 5 activated features, we found that higher value of is preferable Figure 17. Low level features in the LMM. These features activate in most of the images and showcase the models basic cognition and perception abilities. In many of our probing trials, these features exhibit high activation levels and respond to various aspects of the images. We believe these features function as universal elements in how language-vision models (LMMs) understand the world. Figure 18. The feature related to money and its steering effect. because single image can contain many low-level visual features and diverse semantic information. In practice, we select 30 100 and skip some of the top-activated values to exclude low-level visual features. 12. Low Level Perception Features Examples We identify many low-level visual features from the model that differ from the text-based features in large language models (LLMs). These visual features are strongly activated across most images and represent the models basic perceptual and cognitive abilities. In Fig. 17, we present examples of features activated by structure, shape, and color."
        }
    ],
    "affiliations": [
        "LMMs-Lab Team, S-Lab, NTU, Singapore"
    ]
}