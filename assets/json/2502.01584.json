{
    "paper_title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models",
    "authors": [
        "Carolyn Jane Anderson",
        "Joydeep Biswas",
        "Aleksander Boruch-Gruszecki",
        "Federico Cassano",
        "Molly Q Feldman",
        "Arjun Guha",
        "Francesca Lucchetti",
        "Zixuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot. Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 8 5 1 0 . 2 0 5 2 : r PhD Knowledge Not Required: Reasoning Challenge for Large Language Models Carolyn Jane Anderson Wellesley College Joydeep Biswas University of Texas at Austin Aleksander Boruch-Gruszecki Charles University Federico Cassano Cursor Molly Feldman Oberlin College Arjun Guha Northeastern University Francesca Lucchetti Northeastern University Zixuan Wu Northeastern University"
        },
        {
            "title": "Abstract",
            "content": "Existing benchmarks for frontier models often test specialized, PhD-level knowledge that is difficult for non-experts to grasp. In contrast, we present benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models mistakes are easy to spot. Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with give up before providing an answer that it knows is wrong. R1 can also be remarkably uncertain in its output and in rare cases, it does not finish thinking, which suggests the need for an inference-time technique to wrap up before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark."
        },
        {
            "title": "Introduction",
            "content": "There has been significant recent interest in large language models (LLMs) that are trained to employ reasoning at inference time. These reasoning models, which include OpenAI o-family models (OpenAI, 2024), Gemini 2.0 Flash Thinking (Google, 2024), and DeepSeek R1 (DeepSeek-AI et al., 2025), achieve state-of-the-art results on several challenging benchmarks, and far surpass the capabilities of the last generation of LLMs that do not employ test-time compute. For many researchers, the goal is to develop tasks that are extremely difficult for any human, to help develop models that exceed human ability. Thus the latest benchmarks evaluate models on very challenging tasks, such as college-level math competition problems, very difficult programming problems, and problems that require deep domain expertise in academic disciplines. Some of these benchmarks are carefully designed by people who have or are pursuing PhDs and equivalent degrees (Rein et al., 2024; Phan et al., 2025). consequence of designing problems this way is that they are not only challenging for humans to solveas intendedthey are also very challenging for humans to understand and verify. Thus most people cannot understand why these problems are hard, check that answers are indeed correct, or verify that models are reasoning correctly and efficiently about problem. This problem will become more important with the proliferation of reasoning models. 1 Figure 1: We benchmark the latest reasoning models on the NPR Sunday Puzzle Challenge. The questions exercise general knowledge and are difficult for humans to solve, but the answers are easy to verify. These general knowledge puzzles show capability differences between reasoning models that are not evident from benchmarks that exercise deep technical knowledge. For humans, holding an advanced degree is at best weakly correlated with reasoning ability. Thus it is helpful to develop benchmarks with problems that humans can understand with only general knowledge. For such problems, solutions should be difficult to find, but easy to verify for both humans and models. We present benchmark designed with these requirements in mind. In short, we derive machine-checkable benchmark with nearly 600 problems based on the off-air challenges from the NPR Sunday Puzzle Challenge. The Sunday Puzzle, hosted by Will Shortz, is radio puzzle program that has run since 1987. Every week listeners are given short puzzle that usually involves wordplay. Typically, hundreds or thousands of listeners successfully solve the puzzle by the Thursday deadline, and one random winner is announced the next Sunday. Many puzzles are authored by listeners, and typically have unique, or very small set, of valid answers. The puzzles vary in hardness: there have been times when just handful of listeners submitted correct answers. However, each puzzle is carefully selected so that the average listenerwho is an English-speaking adult who has grown up in the United Statescan understand the question, may fail to solve it even with considerable effort over five days, but will ultimately agree that the answer when revealed is correct and elegant. We find that these puzzles are challenging even for the latest generation of reasoning models. Moreover, they reveal capability gaps and failure modes between models that are not evident in existing benchmarks (Figure 1). We find that OpenAI o1, which gets 59%, significantly outperforms other models that we test, including DeepSeek R1. In handful of cases, we also find that DeepSeek R1 gets stuck thinking forever (4.3). Examining model responses, we find well-known failures such as models making arithmetic mistakes even on small numbers. We also find new kinds of failures where reasoning models fail to search and give up after several minutes: they either produce answers that they know or wrong, or argue that the question is impossible to solve (4.2). However, when we instead prompt the model to verify and explain why the true answer is correct, they quickly succeed at the verification task, even when they fail to find the answer. DeepSeek R1 and Gemini Thinking allow us to record their textual reasoning steps, which allows us to do deeper analyses. For example, we can quantify the effectiveness of reasoning longer, and identify the point beyond which reasoning is unlikely to produce correct answer on our benchmark (4.5)."
        },
        {
            "title": "2 Related Work",
            "content": "Benchmarks that Require PhD Knowledge As models keep getting better, the benchmarks that we use to quantify their capabilities get saturated. Several recent benchmarks have been explicitly designed with the belief that models are approaching superhuman capabilities in particular domains, and these benchmarks are thus designed to have extremely 2 challenging domain-specific questions. GPQA (Google-proof Q&A) (Rein et al., 2024) is recent example, where benchmark problems were created and vetted by teams experts who had or were pursuing PhDs in physics, chemistry, and biology. Remarkably, the latest generation of reasoning models appear to be saturating GPQA in just few months. HLE (Humanitys Last Exam) (Phan et al., 2025) is newer, larger, and harder benchmark that is similarly designed. HLE covers many more areas of knowledge, with questions written by people who hold advanced degrees, and even the latest models still perform very poorly. At the time of writing, the best performing model, OpenAI o1, achieves 9.1% accuracy. These benchmarks are very valuable, but, by design, each problem is only comprehensible by people who have narrow subject-matter expertise. An individual cannot hope to answer or even understand most of the questions on these benchmarks. Math Benchmarks number of notable benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), evaluate the mathematical capabilities of the models. Arguably, frontier models have nearly saturated many of these benchmarks (Lei et al., 2024; Zhong et al., 2024). However, Mirzadeh et al. (2024) show that the models perform significantly worse on slight variations of GSM8K problems, e.g., ones involving different numbers or names of people. Adding distractor sentence to the problem has particularly significant result, even decreasing the performance of o1-preview by 17.5%. Gulati et al. (2024) show benchmark of problems sourced from the William Lowell Putnam Mathematical Competition, also featuring variable name and constant variants. Although o1-preview achieves precision of 41.95% on their benchmarks, its accuracy is around 30% lower on the altered problems. Such issues may be caused by data contamination and the tendency of all models so far to exhibit token bias (Jiang et al., 2024). While math benchmarks are invaluable in discovering the capabilities and limitations of logical reasoning in state-ofthe-art models, the problems in such benchmarks inevitably require strong mathematical background for reader to appreciate, follow along, or catch errors in the models reasoning. Benchmarks That Do Not Require World Knowledge ARC-AGI (Chollet, 2019) is perhaps the best known benchmark of reasoning and abstraction benchmark for AI. ARC-AGI is carefully constructed to require small number of priors, whereas our benchmark tests both models reasoning ability and its ability to recall extensive general knowledge. The ARC-AGI tests can be challenging for people, but Each task included in ARC has been successfully solved by at least one member of group of three high-IQ humans. In contrast, few hundred people submit correct solutions to the Puzzle Challenges every week. Benchmarks That Exercise General Knowledge There is long tradition of using puzzles to evaluate language models. Most closely related to our work are benchmarks constructed from cryptic crosswords in The Guardian (Rozner et al., 2021), synthesized brain teasers (Jiang et al., 2023), and the on air questions from the NPR Sunday Puzzle (Zhao & Anderson, 2023). While we also source our benchmark from the same show, our benchmarks are disjoint: Jiang et al. (2023) create benchmark from the on-air questions that are carefully designed for the contestant to solve live. In contrast, we create our benchmark from the off-air weekly challenges that are designed to be significantly harder. Some puzzles explicitly tell listeners to use dictionary or atlas to help them work through the puzzle. As we shall see, the weekly challenges can stump current-generation models too."
        },
        {
            "title": "3 The Sunday Puzzle Challenge Dataset",
            "content": "We present how we curate the Sunday Puzzle Challenge Dataset from the NPR Sunday puzzles, including filtering the questions for ease of testing (3.1) and prompting method (3.2). We then discuss our findings of the results with state-of-the-art models on the benchmark, including overall results (4.1) and insights gleaned (4.2). 3 3.1 Building and Validating The Dataset We scrape thirteen years of transcripts of the Sunday Puzzle Challenges from the web, and use them to build our dataset. We systematically reviewed and edited the scraped data as follows. Adding Context handful of challenges require context that is not evident from the challenge text. The most common missing context is the current date, which we correct by making dates explicit when necessary, as in the following example: Challenge The film Wild Wild West had three Ws as its initials. What prominent film of last year 2013 had two Ws as its initials? Ground Truth Answer The Wolf Of Wall Street (1) Another common piece of missing context is location, which is typically just the United States, which we add in the following example. Challenge Think of common greeting in another country that is not the United States. You can rearrange its letters to get the capital of country that neighbors the country where this greeting is commonly spoken. What greeting is it? Ground Truth Answer Ni hao --> Hanoi (2) Alternative Solutions Most challenges have unique solution or small number of unique solutions. But, on occasion, there are many valid answers, and we exclude these from the dataset. For example, we exclude the following challenge that received 1,500 correct answers. Challenge Can you name four common, uncapitalized 4letter words, each of which has exactly one vowel, and all of which rhyme, even though all four vowels are different? Ground Truth Answer Herd, bird, word, curd. (Other answers are possible.) (3) Removing Explanations handful of answers in the dataset have explanations instead of answers (or in addition to answers). In these cases, we replace the explanations with their answers, as in the following challenge. Challenge These four words have very interesting and unusual property in common something hidden in them. What is it? NEANDERTHAL EMBARRASS SATURATION CONTEMPTUOUSNESS Ground Truth Answer Each word conceals the name of planet in left-to-right order (but not in consecutive letters) Earth, Mars, Saturn, Neptune (4) 3.2 Prompting Format We prompt every model to answer each challenge zero-shot, without any formatting instructions or any additional instructions other than the challenge text itself. Thus the model freely generates its answer and explanation. To check correctness, we ignore capitalization and punctuation and test that every phrase in the gold answer appears in the model-generated answer. Although we use zero-shot prompt, we observe that some of the challenges have examples, as illustrated by the following. Ground Truth Answer Mexico --> ox, mice (5) Challenge The letters of SWITZERLAND can be rearranged to spell LIZARD and NEWTS LIZARD being the singular name of an animal, and NEWTS plural. Name another country with this same property. That is, name another country whose letters can be rearranged to spell two animals one singular and one plural. Its major country. What country is it? We leave these kinds of examples intact."
        },
        {
            "title": "4 Results",
            "content": "Model Selection and Configuration We focus on evaluating the latest generation of models that use test-time compute to reason before producing final answer: 1) OpenAI o1, 2) OpenAI o3-mini, 3) OpenAI o1-mini, 4) Google Gemini 2.0 Flash Thinking Experimental 01-21, and 5) DeepSeek R1 We also include GPT-4o and Claude Sonnet 3.5 as non-reasoning baselines.1 We configure models for generation as follows. For o1 and o1-mini, the API requires using temperature 1. For Gemini 2.0 Flash Thinking, we use the APIs default temperature of 0.7. For DeepSeek R1, we use temperature 0.6, top-p 0.95, and 32,768 limit on output tokens: this is the configuration the R1 paper uses for its experiments. For GPT-4o and Sonnet, we use temperature 0.2 and top-p 0.95, which are commonly used for reasoning tasks. 4.1 Main Results We report mean accuracy on our benchmark in Figure 1. The figure shows that OpenAI o1 significantly outperforms the other models (59% accuracy). The next best performing model is o3-mini with high reasoning effort (47%), followed by o3-mini with default settings (36%) and R1 (35%). The non-reasoning models, Sonnet 3.5 and GPT-4o, do much more poorly than the best reasoning models, which indicates that the benchmark does exercise reasoning ability. It is interesting to compare models relative performance on our benchmark to their performance on benchmarks that require deep technical knowledge. For example, on the GPQA Rein et al. (2024) of PhD-level science questions, the R1, o1, and o3-mini models achieve comparable performance. However, our benchmark indicates that o1 has substantially better general knowledge. 4.2 How Models Give Up We observe several well-known failure modes of LLMs, such as hallucinations and faulty calculations. However, novelty of reasoning models is that they can give up on the problem. On 142 / 595 challenges, R1 explicitly outputs give up while reasoning. Broadly speaking, there are two kinds of give ups that we observe. [Check numbers. Arjun] Out-of-thin-air final answer The following challenge elicits an give up with an outof-thin-air answer that does not appear anywhere in the reasoning output. Challenge Think of familiar five-letter word in two syllables. Change the middle letter to the preceding letter of the alphabet, and youll get familiar five-letter word in three syllables. What words are these? Ground Truth Answer alpha --> aloha (6) 1The specific versions we use are gpt4o-2024-11-20 and claude-3-5-sonnet-20241022. 5 After much frustration, Ill guess the answer is: Original word: CABIN Transformed word: CAAIN (though not real word) But since thats not valid, perhaps the actual answer is: Original word: BREAD Transformed word: BRAID (but both two syllables) No. give up. </think> The answer to the riddle is the pair of words penne and penne. 1. Original word: penne (a type of pasta, pronounced PEN-neh, two syllables). 2. Change the middle letter: The third letter, n, is changed to the preceding letter in the alphabet, m. 3. Resulting word: penne (a variant spelling, pronounced pen-NAY, three syllables). This transformation fits the criteria: five-letter, two-syllable word becomes five-letter, three-syllable word by altering the middle letter to the preceding one in the alphabet. Figure 2: The last few lines of R1s output on Challenge 6. Hmm. think have to give up and say dont know, but since need to provide an answer, Ill go with stirrup even though its two syllables, or queueing as stretch. But Im really not certain. </think> The word that fits the criteria is queueing. Heres the breakdown: 8 letters: Q-U-E-U-E-I-N-G. Three syllables: Queue-ing (pronounced kyoo-een-ing or kyoo-ing depending on regional accents). Contains in each syllable: First syllable: queu (contains but the is in the second syllable in some syllabifications). Second syllable: (if split as queu-e-ing). Third syllable: ing (contains i). No sounds: The in ing is pronounced as short (I), but in some dialects, it might blend into schwa (@) or be silent in rapid speech. However, this answer is stretch and depends on syllabic division and pronunciation nuances. The more precise answer intended by the riddle is likely abseiling (though it has 9 letters), but given constraints, queueing is the closest fit. The key lies in the being part of the -ing suffix without distinct sound. Final Answer: queueing Figure 3: The last few lines of R1s output on Challenge 7. Figure 2 shows the last few lines of R1s reasoning and its final answer. It produces penne and penne: both are the same word spelled differently, and the syllabus count is wrong. But, more interesting is that penne does not appear in the reasoning output. Deliberately violating constraints Another common failure mode is when the model gives up and determines that it will deliberately violate the constraints because it must answer. This happens in the following challenge. Challenge Think of an 8-letter word with three syllables that contains the letter in each syllablebut, strangely, doesnt contain single sound, either long or short. The answer is not plural. What word is it? Ground Truth Answer Daiquiri (7) The model produces queueing and literally reasons that the answer is stretch (Figure 3). For justification it says that some may pronounce queueing as kyoo-ee-ing. 6 The answer to the puzzle is Jesse Jackson. Lets break it down: Jesse (5 letters) + Jackson (7 letters) = 12 letters total. Wait, thats 12 letters. Hmm, this doesnt fit. Let me correct that. Upon re-evaluation, the correct answer is Woodrow Wilson. However, this name has more than 8 letters. Lets think again. The actual correct answer is Anna Eleanor Roosevelt. Wait, thats too long. Wait, the correct answer is Eliot Ness? Lets check: Eliot (5 letters) + Ness (4 letters) = 9 letters. No. Wait, heres the correct approach: The answer is Booker T. Washington? No, thats too long. Wait, the correct answer is W.E.B. Du Bois but that includes initials. Not matching. . . . Figure 4: R1 may take back answers, proposing 58 wrong ones in Challenge 10. Are these reasonable challenges? The reader may wonder if these are reasonable challenges. For the former challenge (alpha to aloha), 370 people submitted the correct answer. The latter challenge (daiquiri) received 500 correct answers. Some sources suggest that 4 million people may be listen to the Sunday Puzzle Challenge every week. To the best of our knowledge, there are no estimates for how many people attempt each challenge. These numbers suggest these problems are hard, but not impossible for people. Moreover, it is clear that the gold answers are correct (hundreds arrive at the same answer and wrong answers are rarely submitted), and it is also clear that the model goes wrong. 4.3 Getting Stuck Reasoning with R1 We ran our experiments with 32,768 output token limit, following the example of the R1 paper. However, this limit is not high enough for the dataset. On 50 challenges, we encounter cases where R1 gets stuck during reasoning: it does not output the </think> token before we reach the 32,768 output token limit. The following two challenges exhibit the worst case behavior, and R1 fails to stop reasoning in 5/10 trials. Challenge The city UTICA, NEW YORK, when spelled out, contains 12 letters, all of them different. Think of well-known U.S. city, that when its name is spelled out, contains 13 letters, all of them different. Your answer doesnt have to match mine. Name food item in seven letters. Move the first letter to the fifth position and youll get two words that are synonyms. What are they? Ground Truth Answer Casper, Wyoming; Big Flats, New York; Bucksport, Maine; Lynchburg, Texas Brisket --> risk, bet (8) (9) We conducted another experiment with just these two prompts and the maximum context window size (128K). In this experiment, 2/10 responses for each prompt did not finish reasoning. We believe R1 needs an inference-time method that encourages the model to wrap up reasoning when it is close to the output token limit, e.g. Muennighoff et al. (2025). We speculate that API-only models use methods such as this to determine their reasoning effort. 4.4 Newfound Lack of Confidence We observe that R1 and Gemini Thinking can be remarkably uncertain in their output. Sometimes the model will give up and output wrong answer, only to immediately take it back, try to provide good one, and fail again. Interestingly, the models seem to repeat themselves in such state more often than when thinking (Figure 4). We estimate this occurred in 29 R1 responses, 18 Gemini Thinking responses and 3 o1-mini responses. 7 Okay, lets try to figure out this riddle. . . . First, need to think of categories that are common knowledge and have exactly 7 members. Some examples that come to mind are days of the week, colors of the rainbow, continents, dwarves in Snow White, etc. Lets go through them one by one. Days of the week: . . . Colors of the rainbow: . . . Continents: There are 7 continents. Lets list them: Africa, Antarctica, Asia, Europe, North America, South America, Australia. . . . Yes, all ending with except Europe (E). So when sorted by their ending letters, Europe would be last since comes after A. So the last letter alphabetically is E. That fits the clue. But need to check if this is the intended answer. Alternatively, maybe another category. . . . Figure 5: R1 finds the answer to Challenge 11 after outputting only 1/4 of its thoughts. Challenge The word NONUNION has four Ns and no other consonant. What famous American of the past first and last names, 8 letters in all has four instances of the same consonant and no other consonant? Ground Truth Answer Eli Lilly. (10) We also observe the models may find good answer early during their thoughts and prefer to explore other options before outputting it; we found 10 such R1 responses. This may happen for seemingly understandable reasons: the model may say its unsure if city or word is well-known enough. Sometimes the model offers nonsensical explanation: But does Santa rhyme with Manta? Not exactly. The first syllables are different: San vs. Man. The second syllable is ta in both, so maybe its considered rhyme? In some accents, maybe. But Im not sure if thats the intended answer. And sometimes it finds the correct answer immediately, verifies it, and proceeds to explore other options for no apparent reason before finally committing to the correct answer (Figure 5). Challenge Think of well-known category with exactly 7 things in it. Alphabetize the things from their ending letters, and the last letter alphabetically will be E. In other words, no thing in this category ends in letter after in the alphabet. Its category and set of 7 things that everyone knows. What is it? Ground Truth Answer Continents. (11) 4.5 How Much Reasoning Is Necessary? How much reasoning is necessary? Since we can access the reasoning output from R1 and Flash Thinking, we can empirically determine reasoning budget that is finer-grained than the three reasoning effort settings presented offered by the OpenAI API. Figure 6a depicts the distribution of reasoning-output lengths on the Sunday Puzzle Challenge, showing that most attempts generate fewer than 20,000 tokens. Figure 6b then shows accuracy as function of reasoning length for both models. Gemini Thinking achieves its accuracy plateau at roughly 10,000 tokens, whereas R1s accuracy continues to improve and surpasses Gemini Thinking at around 3,000 tokens. Thus we conclude that 10,000 tokens of reasoning is unlikely to significantly improve accuracy with either model. Consequently, when the models reasoning output surpasses 10,000 tokens, we are likely better off interrupting generation rather than waiting longer (and spending more). similar approach that quantifies how accuracy improves without reasoning length is likely helpful for other tasks as well. 8 (a) Reasoning length. (b) Accuracy by reasoning length. Figure 6: The R1 and Gemini Thinking models allow us to observe their reasoning output, and Figure 6a shows the proportion of challenges (x-axis) with length (y-axis) indicated. The plot shows that most challenges produce fewer than 20,000 reasoning output tokens. Figure 6b shows accuracy by reasoning length. For Gemini Thinking, accuracy reaches plateau at 10,000 reasoning output tokens, whereas the plateau appears later for R1. With budget of 3,000 reasoning output tokens, R1 starts to outperform Gemini Thinking."
        },
        {
            "title": "5 Conclusion",
            "content": "We present benchmark for reasoning models with questions that do not require PhD-level knowledge, but instead exercise U.S.-centric general knowledge. The benchmark problems are challenging for both humans and models, but humans can easily verify the correctness of an answer, and models mistakes are also obvious. We uncover new failure modes in reasoning models. We observe that models can give up on difficult problem and deliberately return an incorrect answer. In rare cases, R1 can get stuck thinking forever. Finally, by examining the reasoning outputs of R1 and Gemini Thinking, we quantify the effectiveness of reasoning longer, allowing us to set token budget beyond which accuracy reaches plateau on these tasks. Our work identifies nuances of model behavior on accessible yet challenging benchmarks, and also point to areas for improvement reasoning models."
        },
        {
            "title": "References",
            "content": "Francois Chollet. On the Measure of Intelligence, November 2019. URL http://arxiv.org/ abs/1911.01547. arXiv:1911.01547 [cs]. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs]. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu 9 Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, January 2025. URL http://arxiv.org/abs/2501.12948. arXiv:2501.12948 [cs]. Google. Gemini 2.0 Flash Thinking Experimental, December 2024. URL https://deepmind. google/technologies/gemini/flash-thinking/. Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes Dumont, and Sanmi Koyejo. Putnam-AXIOM: Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, October 2024. URL https://openreview.net/forum?id= YXnwlZe0yf&noteId=yrsGpHd0Sf. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem In Joaquin Vanschoren and Sai-Kit Yeung Solving With the MATH Dataset. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, Virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners, 2024. URL https://doi.org/10.48550/arXiv.2406.11050. Yifan Jiang, Filip Ilievski, Kaixin Ma, and Zhivar Sourati. BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1431714332, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.885. URL https: //aclanthology.org/2023.emnlp-main.885. Bin Lei, Yi Zhang, Shan Zuo, Ali Payani, and Caiwen Ding. MACM: Utilizing Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems. Technical Report arXiv:2404.04735, arXiv, July 2024. URL http://arxiv.org/abs/2404.04735. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models, October 2024. URL http://arxiv.org/abs/2410. 05229. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, January 2025. URL http://arxiv.org/abs/2501.19393. arXiv:2501.19393 [cs]. 10 OpenAI. OpenAI o1 System Card, 2024. openai-o1-system-card/. URL https://openai.com/index/ Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanitys Last Exam, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: Graduate-Level Google-Proof Q&A Benchmark. August 2024. URL https://openreview.net/forum?id=Ti67584b98# discussion. Joshua Rozner, Christopher Potts, and Kyle Mahowald. Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as Target for NLP. November 2021. URL https://openreview.net/forum?id=Ah5CMODl52. Jingmiao Zhao and Carolyn Jane Anderson. Solving and Generating NPR Sunday Puzzles with Large Language Models. In Conference on Computational Creativity (ICCC), 2023. Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, and Bo Du. Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems. Technical Report arXiv:2404.14963, arXiv, October 2024. URL http://arxiv.org/abs/2404.14963."
        }
    ],
    "affiliations": [
        "Charles University",
        "Cursor",
        "Northeastern University",
        "Oberlin College",
        "University of Texas at Austin",
        "Wellesley College"
    ]
}