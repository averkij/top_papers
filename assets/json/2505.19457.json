{
    "paper_title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs",
    "authors": [
        "Guilong Lu",
        "Xuntao Guo",
        "Rongjunchen Zhang",
        "Wenqiao Zhu",
        "Ji Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 5 4 9 1 . 5 0 5 2 : r BizFinBench: Business-Driven Real-World Financial Benchmark for Evaluating LLMs Guilong Lu1, Xuntao Guo1,2, Rongjunchen Zhang1, Wenqiao Zhu1, and Ji Liu1 1HiThink Research 2Harbin Institute of Technology"
        },
        {
            "title": "Abstract",
            "content": "Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed rapid advancements of Large Language Models (LLMs), which demonstrates remarkable capabilities across diverse domains, such as finance, law, healthcare and so on Chen et al. [2024], Liu et al. [2025], Zhang et al. [2023a], Lu et al. [2024], Xie et al. [2025]. In financial applications, LLMs are increasingly applied to complex tasks, including automated financial analysis, fraud detection, risk assessment, and investment strategy formulation Zhao et al. [2024], Gan et al. [2024]. However, evaluating the robustness and reliability of LLMs in finance domains remains significant challenge. denotes equal contribution. This paper was completed during Guilong Lu and Xuntao Guos internships at Hithink Research. denotes corresponding author, zhangrongjunchen@myhexin.com. Preprint. Under review. Figure 1: Comparison of numerical calculation questions in Fin-Eva Team [2023] and BizFinBench. The Fin-Eva example presents straightforward financial math problem, while the BizFinBench example requires multi-step reasoning: first analyzing the problem, then extracting and utilizing relevant data from provided markdown-formatted table for accurate computation. An Chinese version is included in the Appendix for clarity and ease of reference. Different from traditional Science, Technology, Engineering, and Mathematics (STEM) questions, where inputs are typically short, well-structured, and yield deterministic answers, financial tasks are more complex. They typically involve long context, structured inputs (e.g., tabular stock data, market news), require temporal reasoning, and demand fine-grained judgment under ambiguity. As illustrated in Figure 1, STEM-style questions usually have clear computational logic and single correct answer, while financial tasks call for multi-step reasoning over real-world data, generally with adversarial or noisy context Du et al. [2024]. Despite the emergence of financial benchmarks such as FinEval Zhang et al. [2023b], existing approaches treat financial tasks as general document Query-Answering (QA) Wang et al. [2024], lacking structured inputs and business-grounded reasoning required in practice. Thus, there emerges the gap between benchmark performance and real-world applicability. To address these limitations, we introduce BizFinBench, comprehensive benchmark designed to rigorously evaluate LLMs across broad spectrum of real-world financial tasks. In contrast to previous benchmarks, BizFinBench adopts business-driven data construction methodology and emphasizes contextual complexity and adversarial robustness. It encompasses five key dimensions: QA, prediction & recognition, reasoning, information extraction, and numerical calculation. Under these dimensions, BizFinBench comprises nine distinct categories: anomalous event attribution, financial numerical computation, financial time reasoning, financial tool usage, financial knowledge QA, financial data description, emotional value evaluation, stock price prediction, and financial named entity recognition. core characteristic of BizFinBench is the focus on business-contextual evaluation. For example, in the anomalous event attribution task, LLMs are required to identify the causes of stock price anomalies by analyzing time-sensitive news feeds, some of which are deliberately embedded with misleading positive or negative information. This setting challenges LLMs to perform fine-grained reasoning and signal discrimination under realistic noise and uncertainty. In addition to the benchmark design, critical component of BizFinBench is the design of reliable evaluation methodology. While constructing realistic tasks is essential, evaluating LLM outputs, particularly for open-ended, complex financial problems, remains significant challenge. Traditional human evaluation provides high-quality judgments but suffers from two major drawbacks: (1) the annotation cost increases exponentially with the scale and domain specificity of financial tasks, and (2) subjective inconsistencies among annotators can introduce substantial noise. Although 2 Data FLUE FLARE CF-Benchmark FinEval FinQA FinancelQ CGCE BizFinBench Table 1: Comparison Between BizFinBench and Other Financial Datasets Examples Language Year Source Task 2022 Multiple financial NLP tasks 2023 Multiple financial NLP tasks, financial prediction tasks Financial numerical reasoning 2024 Multiple financial NLP tasks 2023 Multiple financial NLP tasks 2021 2023 Multiple financial NLP tasks 2023 Multiple financial NLP tasks 2025 Multiple financial NLP tasks, financial prediction tasks 26292 English Aggregated from existing sources 19196 Chinese, English Aggregated from existing sources 3917 Chinese 8351 Chinese 8281 English 7137 Chinese 150 Chinese, English 7016 Chinese except Financial field examination & except except Financial field examination & except except except Business-based recent approaches like LLM-as-a-Judge Gu et al. [2024] attempt to automate evaluation through prompt-based simulations of human judgment, they are prone to prompt bias and generally lack alignment with expert-level assessments. These limitations are further magnified in the financial domain, where tasks demand multi-step reasoning, contextual interpretation, and robustness against adversarial or misleading signals. As such, existing evaluation paradigms are insufficient to capture the depth and nuance required for trustworthy assessment. To address this gap, we propose IteraJudge, an iterative calibration-based evaluation framework tailored for financial LLM benchmarks. Drawing inspiration from the RevisEval framework Zhang et al. [2024a], IteraJudge enhances evaluation accuracy and reliability through three core mechanisms: evaluation dimension disentanglement, sequential correction generation, and reference-aligned assessment. By integrating IteraJudge into BizFinBench, we establish rigorous and interpretable evaluation pipeline for LLM performance in high-stakes financial contexts. In summary, the major contributions of our work are as follows: We propose BizFinBench, the first evaluation benchmark in the financial domain that integrates business-oriented tasks, covering 5 dimensions and 9 categories. It is designed to assess the capacity of LLMs in real-world financial scenarios. We design novel evaluation method, i.e., IteraJudge, which enhances the capability of LLMs as judge by refining their decision boundaries in specific financial evaluation tasks. We conduct comprehensive evaluation with 25 LLMs based on BizFinBench, uncovering key insights into their strengths and limitations in financial applications."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we present existing evaluation benchmarks in financial domains. Then, we present the major LLMs specialized in financial domains. 2.1 Financial Evaluation Benchmarks FLUE Shah et al. [2022] is comprehensive suite of benchmarks covering five key financial tasks: sentiment analysis, news headline classification, named entity recognition, structural boundary detection, and question answering. Building on FLUE, FLARE Xie et al. [2023] expands the evaluation to include time-series processing capabilities, adding tasks such as stock price movement prediction. In addition to FLUE and FLARE, several specialized datasets focus on various aspects of financial evaluation. For example, FinQA Chen et al. [2022a] provides QA pairs annotated by financial experts, accompanied by earnings reports from S&P 500 companies. This dataset supports financial question answering, emphasizing detailed, factual responses based on corporate financial data. ConvFinQA Chen et al. [2022b] extends this by incorporating multi-turn dialogues, enabling more sophisticated interactions within the context of earnings reports, thus broadening the scope of financial evaluation to conversational contexts. FinEval Zhang et al. [2023b] adopts quantitative evaluation approach, combining long-term research insights with manual curation and featuring diverse question types. However, it primarily emphasizes static knowledge assessment and lacks coverage of dynamic, real-time financial tasks and fineFigure 2: Distribution of tasks in BizFinBench across five key dimensions. The benchmark is structured around five dimensions, each focusing on distinct capability of financial large language models. The figure also briefly illustrates the core focus of each dimension. grained capability diagnostics, which limits its effectiveness in benchmarking models under complex, business-driven financial scenarios. Expanding beyond traditional financial instruments such as stocks, bonds, and mutual funds, FinancelQ Duxiaoman DI Team [2023] introduces emerging topics such as cryptocurrencies and blockchain technologies. This dataset can be exploited for evaluating models in the rapidly evolving field of digital finance. In the context of Chinese financial benchmarks, several recent datasets have been released, including CFBenchmark Lei et al. [2023], which focuses on Chinese financial text analysis; DISC-FINSFT Chen et al. [2023], designed for financial sentiment analysis and forecasting; and CGCE Zhang et al. [2023c], which extends financial evaluation to include general knowledge and commonsense reasoning in Chinese financial documents. Table 1 provides comprehensive comparison of existing financial benchmarks, detailing key aspects such as the year of release, the number of samples, language coverage, data sources, and whether the dataset was constructed with real business scenarios in mind. From the comparison, it is evident that while several benchmarks focus on financial knowledge or specific task types, they often rely on synthetic data or public information without strong connection to actual business applications. In contrast, BizFinBench is the only benchmark explicitly designed around real-world financial operations and user interactions, making it uniquely positioned to evaluate the practical effectiveness of LLMs in authentic business environments. This business-centric design ensures higher relevance, realism, and applicability of the tasks included in the benchmark. 2.2 Financial Large Language Models By training on large corpus of financial data based on BERT, FinBERT Araci [2019] was proposed as pre-trained model for the financial domain, primarily used for sentiment analysis of financial texts. Subsequently, models such as FinMA Xie et al. [2023], InvestLM Yang et al. [2023a], and FinGPT Yang et al. [2023b] were fine-tuned on LLaMA Touvron et al. [2023] to further enhance their performance in the financial domain. The XuanYuan3-70B model, built on the LLaMA3-70B architecture and incrementally pre-trained with vast amount of Chinese and English corpora, focuses on the financial sector and is capable of handling complex tasks such as financial event interpretation, investment research applications, compliance, and risk management. BloombergGPT Wu et al. [2023] is 50-billion-parameter LLM based on the Bloom architecture, specifically designed for the financial industry, demonstrating strong adaptability in the financial domain. Meanwhile, Baichuan4Finance Zhang et al. [2024b] has achieved an accuracy rate of over 95% in various certification fields 4 Figure 3: Workflow of BizFinBench dataset construction. such as banking, insurance, funds, and securities, further proving its exceptional performance in the vertical financial sector. Dianjin-R1 Zhu et al. [2025] is designed for complex financial reasoning tasks and incorporates structured supervision along with dual-reward reinforcement learning, enabling it to outperform strong baselines across range of financial benchmarks. In addition, we also consider general-purpose models in our experiments, as many of them have undergone pre-training on datasets that contain financial texts, such as GPT-4o."
        },
        {
            "title": "3 BizFinBench",
            "content": "In this section, we detail the design of BizFinBench, comprehensive benchmark specialized for evaluating LLMs in financial domains. Compared to previous datasets, BizFinBench places strong emphasis on business practicality and real-world applicability, aiming to bridge the gap between academic evaluation and the complex challenges encountered in real-world financial scenarios. To capture the multifaceted nature of financial intelligence, we organize the benchmark into 10 distinct task types, which are further grouped into 5 overarching evaluation dimensions. As illustrated in Figure 2 1, these dimensions reflect key capabilities required in financial applications. For instance, the numerical computation dimension includes tasks that require models to perform financial computations and optimizations, calculate risk metrics, and solve portfolio allocation problems using quantitative methods. This dimension is designed to evaluate the capability of LLMs to apply precise mathematical reasoning in realistic financial contexts, where accuracy and analytical rigour are critical. This structured categorization not only facilitates fine-grained assessment of model strengths and weaknesses but also ensures that each component of the benchmark aligns with practical demands observed in financial services and business analytics. 3.1 Data Construction Our dataset is primarily sourced from real user queries on the iwencai APP 2, the APP serves broad user base of individual investors and financial professionals, offering functionalities such as stock screening, market analysis, and personalized investment assistance. Leveraging advanced Artificial Intelligence (AI) technologies, iwencai enables users to perform complex financial analyses through natural language queries, covering areas like A-shares, Hong Kong and U.S. stocks, ETFs, and macroeconomic indicators. Based on an extensive analysis of user queries from Platform A, our financial experts identified nine representative task categories that frequently appear in real-world financial scenarios. These include time reasoning, numerical computation, sentiment analysis, and so on. Notably, these categories collectively account for over 90% of the queries observed on the platform, making them highly representative of actual business needs in financial decision-making. To construct our dataset, we first aggregate large set of real user queries, then employ GPT4o OpenAI [2023] to clean noisy entries, filter out incomplete or invalid ones, and classify each valid query into the appropriate expert-defined category. For underrepresented categories, we further use GPT-4o to synthesize additional data, ensuring category balance and coverage. This process results in high-quality dataset tailored to practical financial applications. 1An English version is included in the Appendix 2https://www.iwencai.com/ 5 Table 2: Overview of BizFinBench Datasets Data Evaluation Dimensions Metrics Numbers Avg Len. Category Reasoning Anomalous Event Attribution (AEA) Financial Time Reasoning (FTR) Financial Tool Usage (FTU) Numerical calculation Financial Numerical Computation (FNC) Q&A Financial Knowledge QA (FQA) Financial Data Description (FDD) Prediction recognition Emotion Recognition (ER) Stock Price Prediction (SP) Causal consistency Information relevance Noise resistance Temporal reasoning correctness Tool selection appropriateness Parameter input accuracy Multi-tool coordination Computational accuracy Unit consistency Question comprehension Knowledge coverage Answer accuracy Trend accuracy Data consistency Emotion classification accuracy Implicit information extraction Trend judgment, Causal reasoning Accuracy 1064 939 Accuracy Judge Score 514 641 1162 4556 Accuracy 581 Judge Score 990 651 22 Judge Score 1461 311 Accuracy Accuracy Accuracy 497 435 2179 4498 533 Information extraction Financial Named Entity Recognition (FNER) Recognition accuracy Entity classification correctness Next, we collect relevant contextual data from internal financial databases and external sources based on the content of each user query. This includes stock prices, historical trading data, financial news, company disclosures, and so on. These sources are directly related to the query topic. For example, as illustrated in Figure 3, when user poses the question \"Why did Tesla stock soar?\", the raw query typically lacks explicit temporal markers. To address this problem, we automatically retrieve the timestamp, at which the query was originally issued and utilize it to construct temporally anchored version of the query. This allows us to retrieve the most relevant financial information and news surrounding that specific point in time. To enhance the discriminative power of the dataset, we carefully introduce designed distractor data into the context. These distractors are chosen to assess the reasoning capabilities of the model and include misleading but plausible information, such as news articles from unrelated companies, articles with opposing market sentiment (such as negative news during stock rally), or temporally misaligned events. This step ensures that answering the question correctly requires understanding both the financial context and the time-sensitive nature of the data, rather than relying on superficial keyword matches. Once the query and context are constructed, they are paired with task-specific prompt and submitted to large language model, such as GPT-4o, to generate candidate answers. These preliminary answers are not directly included in the dataset. Instead, each data point undergoes rigorous human annotation and validation process. To ensure high level of data quality and reliability, every entry is independently reviewed and annotated by three senior financial experts. Each expert has over five years of professional experience in roles such as equity research, investment analysis, or portfolio management, and has previously worked at top-tier financial institutions, including securities firms, asset management companies, or banks. During annotation, the experts assess the accuracy of the model-generated answers and verify whether the task category assigned to the query is appropriate. data point is accepted into the final dataset only when all three experts reach full consensus across all aspects, including answer validity, contextual consistency, and category correctness. If disagreements arise, the entry is subjected to further review and iterative refinement until unanimous agreement is reached. This multi-layered annotation process ensures that the dataset is not only factually accurate but also aligned with real-world financial reasoning and application standards. 3.2 Statistics The BizFinBench benchmark consists of total of 6,781 entries, encompassing wide variety of tasks designed to assess model performance across diverse financial challenges. By testing models on these tasks, we aim to evaluate not only their individual capabilities but also their ability to generalize across multiple facets of financial data analysis. 6 Figure 4: IteraJudge Pipeline. Table 2 provides detailed breakdown of the dataset, including the evaluation dimensions, corresponding metrics, the number of instances per task, and the average token length per entry 3. The dataset exhibits significant variability in input length, ranging from just 22 tokens to as many as 4,556 tokens. This broad range reflects the complexity and heterogeneity of real-world financial scenarios and presents meaningful challenge for models to demonstrate their ability to process both short and long financial texts effectively. 3.3 IteraJudge: An Incremental Multi-Dimensional Evaluation Framework As shown in Figure 4, IteraJudge evaluation framework performs dimension-decoupled assessment through three-phase pipeline: 1. Given question and initial answer pmodel(q), we sequentially refine the output across dimensions = {e1, . . . , eK} via prompted LLM transformations: yk = LLMrefine(yk1 P(ek, q)), where y0 = creating an interpretable improvement trajectory {yk}K k=0. 2. The fully refined yK serves as an auto - generated quality benchmark. 3. judge model computes the final score through contrastive evaluation: where the delta (yK y) quantitatively reveals the dimensional deficiencies of LLMs. score(y) = LLMjudge(q, y, yK, E) (1) (2) This question-anchored, iterative refinement process enables granular diagnosis while maintaining contextual consistency through explicit q-preservation in all steps."
        },
        {
            "title": "4 Experiments",
            "content": "This section summarizes the evaluated models (Section 4.1), including SOTA LLMs, inferenceoptimized models, and multimodal large language models. Section 4.2 describes the experimental setup. Section 4.3 presents key results across financial tasks, followed by the performance analysis of IteraJudge in Section 4.4. 4.1 Evaluated Models We conducted systematic evaluation of current mainstream LLMs on BizFinBench. For closedsource models, we selected five industry-recognized SOTA models: OpenAIs GPT-4o, o3 and o4-mini, Googles Gemini-2.0-Flash, and Anthropics Claude-3.5-Sonnet. For open-source models, our evaluation covered both general-purpose LLMs including the Qwen2.5 series, Llama-3.1 series and Llama-4-Scout, as well as the financial-specialized Xuanyuan3-70B model. To comprehensively assess model capability boundaries, we also incorporated the DeepSeek-R1 series (including the 3Detailed dataset information is provided in Appendix E. 7 R1-distill variant) which excels at complex reasoning tasks, the newly open-sourced reasoning model QwQ-32B and the recently released Qwen3 series with hybrid reasoning capabilities. Furthermore, to evaluate MLLMs on our benchmark, we extended our experiments to assess the performance of the Qwen-VL series of MLLMs4. Table 3: Performance Comparison of Large Language Models on BizFinBench. The models are evaluated across multiple tasks, with results color-coded to represent the top three performers for each task: golden indicates the top-performing model, silver represents the second-best result, and bronze denotes the third-best performance. Model AEA FNC FTR FTU FQA FDD ER SP FNER Average ChatGPT-o3 ChatGPT-o4-mini GPT-4o Gemini-2.0-Flash Claude-3.5-Sonnet 86.23 85.62 79.42 86.94 84.68 73.87 Qwen2.5-7B-Instruct 69.27 Qwen2.5-72B-Instruct 53.85 Qwen2.5-VL-3B 73.87 Qwen2.5-VL-7B 37.12 Qwen2.5-VL-14B 76.79 Qwen2.5-VL-32B 69.55 Qwen2.5-VL-72B 77.40 Qwen3-1.7B 83.60 Qwen3-4B 84.20 Qwen3-14B 83.80 Qwen3-32B 12.14 Xuanyuan3-70B 73.12 Llama-3.1-8B-Instruct 16.26 Llama-3.1-70B-Instruct 73.60 Llama 4 Scout 74.34 DeepSeek-V3 (671B) 80.36 DeepSeek-R1 (671B) QwQ-32B 84.02 DeepSeek-R1-Distill-Qwen-14B 71.33 DeepSeek-R1-Distill-Qwen-32B 73.68 4.2 Experiment Setting Propretary LLMs 75.36 71.23 76.20 73.97 42.81 61.30 60.10 56.51 62.67 63. 89.15 74.40 82.37 82.55 88.05 Open source LLMs 39.38 70.72 17.29 40.24 53.08 62.16 69.86 33.40 50.00 65.80 64.60 15.41 2.91 56.34 44.20 72.60 75.00 64.90 16.95 50.86 32.88 54.28 15.92 32.71 41.44 50.00 54.11 35.80 47.40 58.20 59.60 19.69 22.09 34.25 45.80 61.82 64.04 52.91 44.35 51.20 79.03 85.29 8.95 77.85 82.07 83.57 85.18 75.82 78.19 82.19 85.12 80.89 77.42 80.64 85.02 86.54 81.96 84.81 81.96 83.27 91.25 90.27 87.79 90.29 87.35 83.34 87.79 81.60 83.94 84.23 85.30 87.37 73.81 82.24 84.12 85.43 86.51 76.18 79.97 85.21 91.07 91.44 89.60 85.52 87. 98.55 95.73 98.84 98.62 96.85 78.93 97.43 59.44 77.41 7.97 95.95 97.34 78.62 80.16 92.91 95.37 83.90 69.09 86.90 92.32 98.11 98.41 94.20 92.81 97.81 44.48 47.67 45.33 22.17 16.67 37.50 35.33 39.50 38.83 37.33 40.50 35.00 22.40 42.20 33.00 39.00 29.83 29.00 33.33 25.60 32.67 39.67 34.50 39.50 41.50 53.27 52.32 54.33 56.14 47.60 51.91 55.13 52.49 51.91 54.93 54.93 54.94 48.53 50.51 52.31 52.26 52.62 54.21 62.16 55.76 55.73 55.13 56.68 50.20 53. 65.13 64.24 65.37 54.43 63.09 30.31 54.02 21.57 33.40 47.47 68.36 54.41 11.23 25.19 50.70 49.19 37.33 36.56 45.95 43.00 71.24 71.46 30.27 52.76 56.80 73.86 71.29 71.80 69.75 65.59 56.35 67.70 38.96 56.68 49.52 68.62 67.53 50.78 59.94 67.05 68.26 46.48 48.95 55.09 61.17 71.57 73.05 65.77 59.49 66.29 All LLMs were configured with maximum generation length of 1,024 tokens, temperature parameter = 0, and batch size = 1000. We employed GPT-4o as the unified evaluation judge. Opensource models were deployed on an 8NVIDIA H100 cluster, while closed-source models were accessed via their official APIs. The complete evaluation required approximately 10 hours with total computational cost of $21,000. To ensure standardized outputs and facilitate automated assessment, we constrained all models to produce strictly JSON-formatted responses containing two mandatory fields: ① Chain-of-Thought(cot): Detailed logic trace with intermediate steps; and ② Answer: Final conclusion derived after reasoning5. 4.3 Main Results Our evaluation on the BizFinBench benchmark reveals distinct capabilities of LLMs in the financial domain. All results as shown in Table 3. In the AEA task, Gemini-2.0-Flash achieves SOTA performance with score of 86.94, closely followed by ChatGPT-o3 (86.23) and ChatGPT-o4mini (85.62), demonstrating the strong and consistent performance of closed-source models in complex financial understanding. Moreover, proprietary models dominate knowledge-intensive tasksexemplified by the leading performance of GPT-4o in FDD with score of 98.84 and the leadership of ChatGPT-o3 in FTU with 89.15. However, open-source models like DeepSeek-V3 4The specific details of the relevant models can be found in the Appendix C. 5The formatting and style for each dataset are presented in Appendix F. Appendix provides the evaluation prompts used in our experiments. 8 (671B) show impressive competitiveness, particularly surpassing GPT-4o (65.37) in FNER with score of 71.46. Table 4: Comparative Evaluation of Judgment Methods Across Different LLM Judges Methods LLM as judge (GPT-4o) Ours (GPT-4o) LLM as judge (DeepSeek-V3) Ours (DeepSeek-V3) LLM as judge (Gemini-2.0-Flash) Ours (Gemini-2.0-Flash) LLM as judge (Qwen2.5-72B-Instruct) Ours (Qwen2.5-72B-Instruct) Financial Data Description Financial Tool Usage Spearman: 0.4848 Spearman: 0.5684 Spearman: 0.4685 Spearman: 0.4830 Spearman: 0.3763 Spearman: 0.4087 Spearman: 0.3112 Spearman: 0. Spearman: 0.8000 Spearman: 0.8667 Spearman: 0.7500 Spearman: 0.7833 Spearman: 0.7333 Spearman: 0.8167 Spearman: 0.7000 Spearman: 0.7500 Our evaluation leads to three key insights: First, model scale plays crucial role in numerical reasoning. Taking the Qwen3 series as an example, performance consistently improves with increasing model size, e.g., from the smallest 1.7B to the largest 32B, across nearly all tasks. Second, FTR emerges as particularly challenging task, with substantial score gap of 32.19 points between the top performer, GPT-4o (76.20), and lower-performing models like Llama-3.1-8B-Instruct (2.91), highlighting the need for targeted optimization in temporal reasoning. Third, while most models excel in structured data tasksachieving scores above 90 in cases like FDD. They underperform in more complex scenarios such as ER. Even GPT-4o scores only 45.33 in ER, with the best open-source model reaching just 41.50, underscoring significant room for improvement in financial sentiment analysis. Additionally, we identify two intriguing phenomena: (1) Llama-3.1-70B-Instruct demonstrates strong performance in Stock Prediction (62.16) but struggles with related reasoning tasks, and (2) distilled models (e.g., DeepSeek-R1-Distill-Qwen-32B) maintain competitive performance in FTU(83.27) but exhibit limitations in temporal reasoning (FTR, 50.86), highlighting considerable variation in knowledge retention across different capabilities. These findings suggest that while current LLMs are competent at handling basic financial tasks, they still face significant limitations when dealing with complex challenges that require integrated knowledge, particularly in cross-concept reasoning within financial contexts. 4.4 IteraJudge Ablation Experiments To rigorously validate the effectiveness of IteraJudge, we conducted ablation experiments on the FDD and FTU benchmark datasets. We selected Qwen2.5-7B-Instruct as the evaluated model and employed GPT-4o, DeepSeek-V3, Gemini-1.5-Flash, and Qwen2.5-72B-Instruct as judge models to evaluate its generated responses. Three sets of experiments were designed: (1) expert evaluation, (2) the vanilla LLM-as-a-Judge approach, and (3) the full IteraJudge framework. We take the Spearman correlation between the evaluation methods, i.e., vanilla LLM-as-a-Judge and IteraJudge. The experimental results are demonstrated in Table 4, compared to the LLM-as-a-Judge approach, IteraJudge achieves maximum improvement of 17.24% and minimum improvement of 3.09% in terms of Spearman correlation on the FDD benchmark dataset. In the FTU benchmark dataset, it shows maximum improvement of 11.37% and minimum improvement of 4.44%. These results confirm the effectiveness of IteraJudge in mitigating evaluation bias."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose BizFinBench, i.e., the first open-source benchmark dataset, which consists of the dataset deeply integrated with real-world financial business scenarios and iterative calibrationbased evaluation framework, i.e., IteraJudge. We conducted comprehensive evaluation of 25 SOTA LLMs, encompassing both closed-source and open-source models, across multiple task dimensions. Our results reveal significant performance gaps between existing LLMs and human-level expectations in several business-critical areas, highlighting the unique challenges of financial artificial intelligence. We find that no model dominates every task, while ChatGPT-o3, ChatGPT-o4-mini, GPT-4o, Gemini2.0-Flash, DeepSeek-R1, and Llama-3.1-70B-Instruct corresponds to the best performance in diverse metrics. In addition, experimental results also demonstrate that closed-source models place in the 9 top three on eight of nine subtasks. Furthermore, extensive experimental results reveal significant advantages of IteraJudge. BizFinBench serves not only as rigorous benchmark for evaluating financial reasoning capabilities, but also as practical guide for deploying LLMs in real-world financial applications. We believe this benchmark can accelerate progress in the development of trustworthy, high-performing financial language models."
        },
        {
            "title": "References",
            "content": "Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian McAuley, Linda Petzold, and William Yang Wang. survey on large language models for critical societal domains: Finance, healthcare, and law. arXiv preprint arXiv:2405.01769, 2024. Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Haohan Li, Yu Lu, Shilin Zhou, Yue Lu, Ziliang Gan, et al. Nexus-o: An omni-perceptive and-interactive model for language, audio, and vision. arXiv preprint arXiv:2503.01879, 2025. Rongjunchen Zhang, Tingmin Wu, Xiao Chen, Sheng Wen, Surya Nepal, Cecile Paris, and Yang Xiang. Dynalogue: transformer-based dialogue system with dynamic attention. In Proceedings of the ACM Web Conference 2023, pages 16041615, 2023a. Guilong Lu, Xiaolin Ju, Xiang Chen, Wenlong Pei, and Zhilong Cai. Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning. Journal of Systems and Software, 212:112031, 2024. Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, et al. Medical foundation large language models for comprehensive text analysis and beyond. npj Digital Medicine, 8(1):141, 2025. Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. Revolutionizing finance with llms: An overview of applications and insights. arXiv preprint arXiv:2401.11641, 2024. Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, et al. Mme-finance: multimodal finance benchmark for expert-level understanding and reasoning. arXiv preprint arXiv:2411.03314, 2024. Kelvin Du, Frank Xing, Rui Mao, and Erik Cambria. An evaluation of reasoning capabilities of large language models in financial sentiment analysis. In 2024 IEEE Conference on Artificial Intelligence (CAI), pages 189194. IEEE, 2024. Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, Zhoufan Zhu, Anbo Wu, Xin Guo, and Yun Chen. Fineval: chinese financial domain knowledge evaluation benchmark for large language models, 2023b. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. arXiv preprint arXiv:2406.17419, 2024. Fin-Eva Team. Fin-eva version 1.0, 2023. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, et al. Reviseval: Improving llm-as-a-judge via responseadapted references. arXiv preprint arXiv:2410.05193, 2024a. Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. When flue meets flang: Benchmarks and large In Proceedings of the 2022 Conference on pretrained language model for financial domain. Empirical Methods in Natural Language Processing, pages 23222335, 2022. 10 Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443, 2023. Zhiyu Chen, Wenhu Chen, Charese Smiley, and et al. Sameena Shah. Finqa: dataset of numerical reasoning over financial data, 2022a. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering, 2022b. Duxiaoman DI Team. FinanceIQ, 2023. URL https://github.com/Duxiaoman-DI/XuanYuan/ tree/main/FinanceIQ. Accessed: 2024-03-18. Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, and Changjun Jiang. Cfbenchmark: Chinese financial assistant benchmark for large language model, 2023. Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, et al. Disc-finllm: chinese financial large language model based on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205, 2023. Xuanyu Zhang, Bingbing Li, and Qing Yang. Cgce: chinese generative chat evaluation benchmark for general and financial domains, 2023c. Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models, 2019. Yi Yang, Yixuan Tang, and Kar Yan Tam. Investlm: large language model for investment using financial domain instruction tuning, 2023a. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models, 2023b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: large language model for finance, 2023. Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, and Jian Xie. Baichuan4-finance technical report. arXiv preprint arXiv:2412.15270, 2024b. Jie Zhu, Qian Chen, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. Dianjinr1: Evaluating and enhancing financial reasoning in large language models. arXiv preprint arXiv:2504.15716, 2025. OpenAI. Gpt-4 technical report, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Claude. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet. Jinze Bai, Shuai Bai, and et al. Yunfei Chu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. XuanYuan Team. Xuanyuan3-70b report, September 2024. URL https://github.com/ Duxiaoman-DI/XuanYuan. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/."
        },
        {
            "title": "A Reproducibility Statement",
            "content": "To ensure the reproducibility of our results, we have made substantial efforts to provide all necessary details and materials. Specifically, Section 3.1 presents the complete process of dataset construction, including data collection strategies. Furthermore, the benchmark setup and evaluation procedures are thoroughly described in Section 4. All evaluation metrics are clearly defined to facilitate independent verification and replication of our experiments by the research community."
        },
        {
            "title": "B Limitations",
            "content": "In this work, we propose novel benchmark and conduct comprehensive analysis of different LLMs capabilities in solving financial business problems. However, several limitations remain: (1) Our method for extracting final answers from model outputs is not yet perfect. In some cases, this method fails to locate an answer, leading to reported accuracy being an approximate lower bound. Additionally, due to potential formatting differences between the extracted answers and the ground truth, we employ rule-based approach to measure exact matches between the two, which may introduce an estimated 2% error in our experiments. (2) Our benchmark is primarily based on currently available financial data and task settings. Although it covers multiple key sub-tasks, some business scenarios may still be underrepresented. For example, highly specialized financial tasks such as complex derivatives pricing, risk management modeling, or decision support based on real-time market data are not yet fully reflected in our benchmark. This implies that our evaluation results may not completely capture LLMs real-world performance in more complex financial scenarios. (3) While we evaluate multiple SOTA LLMs under the same computational environment to ensure fairness, model performance may still be influenced by training data, inference strategies, and hyperparameter settings. Additionally, discrepancies between inference mechanisms in API-based and locally deployed models could introduce experimental biases. (4) Our evaluation primarily focuses on models abilities in single-turn question answering and task completion. However, in real-world applications, financial decision-making is often complex, multi-step process involving long-term reasoning, external tool utilization, and multi-turn interactions. The current evaluation framework does not fully cover these aspects, highlighting the need for further expansion to better reflect LLMs potential applications in financial business scenarios. For future work, we plan to optimize the answer extraction method to enhance evaluation accuracy and explore more advanced metrics to mitigate errors caused by format mismatches. Additionally, we aim to expand the benchmarks coverage by incorporating more challenging financial tasks and refining experimental settings to improve reproducibility and fairness."
        },
        {
            "title": "C Model detail",
            "content": "To better ensure the comprehensiveness and robustness of our evaluation, we selected wide range of models that differ in architecture, parameter size, training objectives, and domain specialization. Table 5 presents detailed information on the 25 evaluation models used in this study."
        },
        {
            "title": "D Instruction",
            "content": "Figure 5, Figure 6, and Figure 7 illustrate the instructions used for model evaluation on the open-ended answer dataset. 12 Table 5: Summary of Large Language Models Evaluated on BizFinBench. * indicates that the model is Mixture-of-Experts (MoE) model. Model Size Open source Evalation Release date Domain GPT-4o OpenAI [2023] ChatGPT-o3 ChatGPT-o4-mini Gemini-2.0-Flash Team et al. [2023] Claude-3.5-Sonnet Claude [2024] Qwen2.5-Instruct Bai et al. [2023] Qwen2.5-VL Qwen3 XuanYuan3-70B-Chat Team [2024] Llama-3.1-Instruct Dubey et al. [2024] Llama 4 DeepSeek-V3 Liu et al. [2024] DeepSeek-R1 Liu et al. [2024] QwQ-32B Team [2025] DeepSeek-R1-Distill-Qwen-14B Liu et al. [2024] 7B,72B 3B,7B,14B,32B,72B 8B,14B,32B,30B* 70B 8B,70B 109B* 671B* 671B* 32B 14B,32B API API API API API Local Local Local Local Local Local Local Local Local Local 01/29/2025 General 04/16/2025 General 04/16/2025 General 12/11/2024 General 06/20/2024 General 09/19/2024 General 01/28/2025 General 04/29/2025 General 09/06/2024 Finance 07/24/2024 General 04/05/2025 General 12/26/2024 General 12/26/2024 General 03/06/2025 General 12/26/2024 General Figure 5: The instructions utilized in the evaluation of the FDD dataset. 13 Figure 6: The instructions utilized in the evaluation of the FQA dataset. 14 Figure 7: The instructions utilized in the evaluation of the FTU dataset."
        },
        {
            "title": "E Dataset details",
            "content": "The details of each dataset type are as follows. Anomalous Event Attribution (AEA): This dataset evaluates the models ability to trace financial anomalies based on given information such as timestamps, news articles, financial reports, and stock movements. The model must identify the cause-and-effect relationships behind sudden market fluctuations and distinguish relevant factors from noise. Financial Numerical Computation (FNC): This dataset assesses the models ability to perform accurate numerical calculations in financial scenarios, including interest rate calculations, return on investment (ROI), and financial ratios. Financial Time Reasoning (FTR): This dataset tests the models ability to understand and reason about time-based financial events, such as predicting interest accruals, identifying the impact of quarterly reports, and assessing financial trends over different periods. Financial Tool Usage (FTU): This dataset evaluates the models ability to comprehend user queries and effectively use financial tools to solve real-world problems. It covers scenarios like investment analysis, market research, and information retrieval, requiring the model to select appropriate tools, input parameters accurately, and coordinate multiple tools when needed. Financial Knowledge QA (FQA): This dataset evaluates the models understanding and response capabilities regarding core knowledge in the financial domain. It spans wide range of financial topics, encompassing key areas such as fundamental financial concepts, financial markets, investment theory, macroeconomics, and finance. Financial Data Description (FDD): This dataset measures the models ability to analyze and describe structured and unstructured financial data, such as balance sheets, stock reports, and financial statements. 15 Emotion Recognition (ER): This dataset evaluates the models capability to recognize nuanced user emotions in complex financial market environments. The input data encompasses multiple dimensions, including market conditions, news articles, research reports, user portfolio information, and queries. The dataset covers six distinct emotional categories: optimism, anxiety, negativity, excitement, calmness, and regret. Stock Price Prediction (SP): This dataset evaluates the models ability to predict future stock prices based on historical trends, financial indicators, and market news. Financial Named Entity Recognition (FNER): This dataset focuses on evaluating the models ability to identify and classify financial entities such as company names, stock symbols, financial instruments, regulatory agencies, and economic indicators. Table 6 presents their maximum token length, minimum token length, and average length. Table 6: Financial Datasets Query Token Length Statistics. This table presents token length statistics for queries in financial datasets, including minimum (Min), maximum (Max), average (Avg) token counts, and total query count (Count). Dataset Min Max Avg Count NER FTU AEA ER FNC FDD FTR FQA SP 415 4,169 680 1,919 287 26 203 5 1, 1,194 6,289 1,396 2,569 2,698 645 8,265 45 5,532 533.1 4,555.5 938.7 2,178.5 650.5 310.9 1,162.0 21.7 4,498.1 433 641 1,064 600 581 1,461 514"
        },
        {
            "title": "G Other",
            "content": "16 Figure 8: An example instance from the Financial Numerical Computation dataset, in which the data table is formatted using Markdown syntax. 17 Figure 9: An example instance from the Financial Time Reasoning. Figure 10: An example instance from the Financial Knowledge QA dataset. Figure 11: An example instance from the Financial Data Description. 19 Figure 12: An example instance from the Financial Named Entity Recognition. 20 Figure 13: An example instance from the Financial Tool Usage. Figure 14: An example instance from the Anomalous Event Attribution. 22 Figure 15: An example instance from the Emotion Recognition. 23 Figure 16: An example instance from the Stock Price Prediction. Figure 17: An Chinese Version of the Comparison of Numerical Calculation Questions in FinEva Team [2023] and BizFinBench"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "HiThink Research"
    ]
}