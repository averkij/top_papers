{
    "paper_title": "SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning",
    "authors": [
        "Zewen Chen",
        "Juan Wang",
        "Wen Wang",
        "Sunhan Xu",
        "Hang Xiong",
        "Yun Zeng",
        "Jian Guo",
        "Shuxun Wang",
        "Chunfeng Yuan",
        "Bing Li",
        "Weiming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 1 6 1 0 1 . 1 1 4 2 : r a"
        },
        {
            "title": "SEAGULL",
            "content": ": No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning Zewen Chen1,2, Juan Wang1, Wen Wang3, Sunhan Xu4, Hang Xiong3, Yun Zeng5, Jian Guo4, Shuxun Wang1, Chunfeng Yuan1, Bing Li1,6(cid:66), Weiming Hu1,2,7 1 State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA 2 School of Artificial Intelligence, University of Chinese Academy of Sciences 3 Beijing Jiaotong University 4 Beijing Union University 5 China University of Petroleum 6 PeopleAI Inc. Beijing, China 7 School of Information Science and Technology, ShanghaiTech University {chenzewen2022, jun wang}@ia.ac.cn, bli@nlpr.ia.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "reference images are unavailable in real-word applications. Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from Large vision-Language model. SEAGULL incorporates visionlanguage model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the models ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the models ability to perceive real world distortions. After pre-training on SEAGULL-100w and finetuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the link. 1. Introduction Image quality assessment (IQA) is long-standing research in image processing fields. Compared to the full-reference IQA (FR-IQA) and the reduced-reference IQA, the noreference IQA (NR-IQA) receives more attention since the Figure 1. (A) Illustrations of the typical Vision-based and VLMbased IQA. Both of them are designed to analyze the quality of overall image. (B) Our SEAGULL has the capability in fine-grained quality assessment for specified ROI. The mask-based ROI is extracted by SAM [30]. Best viewed in color. As illustrated in Fig.1-(A), NR-IQA methods can be categorized into two main types based on their inputs: Visionbased IQA [8, 26, 48, 50, 62] and Vision-Language Model (VLM)-based IQA [55, 57, 66, 67]. Vision-based IQA predicts quality scores for the input images. This approach lacks of interpretability. VLM-based IQA provides detailed quality descriptions based on the input images and textual prompts. This approach has achieved significant success in analyzing quality for overall images, but overlooks the exploration of quality assessment for specified Regions of Interest (ROIs). Analyzing the quality for ROIs provides more refined guidance for image quality improvement, which has wide applications across various domains, such as video optimization for focused objects [28, 31, 32, 61], 1 image enhancement for ROIs [15, 25, 29, 52, 63] and image compression for interested regions [18, 24, 41]. One method to achieve the quality assessment for ROIs is cropping ROIs and then directly feeding cropped regions into existing vision-based IQA models. Alternatively, drawing bounding boxes (BBoxes) on images can also be used to indicate ROIs. However, both crop-based ROIs and BBoxbased ROIs include irrelevant background and struggle to precisely indicate interested objects, leading to inaccurate instructions to IQA models. In contrast, using masks to indicate ROIs (mask-based ROIs) accurately delineates the focused regions. Recently, numerous segmentation models [27, 30, 72] exhibit excellent zero-shot performance and many advanced VLMs [1, 38, 64] show impressive capabilities in visual understanding. These advancements make it potential to achieve IQA for ROIs by utilizing segmentation models to generate masks that indicate ROIs and VLMs for fine-grained ROI quality assessment. Nevertheless, there are two challenges: (1) The performance of these VLMs in quality analysis is limited [55, 57, 67], since they are designed and trained for high-level tasks and ignore to effectively extract low-level features. (2) Most existing IQA datasets [10, 20, 46, 57] only provide quality scores or descriptions for the overall images, which make them unsuitable for ROI-based IQA training. In this paper, we achieve fine-grained IQA for ROIs from the perspective of network and dataset. Firstly, we design novel network that consists of the Segment Anything Model (SAM) [30] to extract mask-based ROIs for accurately seeing and VLM to comprehend the ROIs quality for finegrained assessing. We name this network SEAGULL, which can SEe and Assess ROIs quality with GUidance from Large vision-Language model. Additionally, to enhance the capacity of SEAGULL in ROIs assessing, we meticulously design mask-based feature extractor (MFE). MFE extracts global and local view tokens using mask-based ROIs, providing SEAGULL with more perspectives to understand the quality of the ROIs. At last, by clicking any region on full image, SAM first extracts the mask to indicate the ROI, and SEAGULL subsequently performs fine-grained quality assessment on the ROI using the extracted mask. Secondly, we construct two ROI-based IQA datasets, namely SEAGULL-100w and SEAGULL-3K. Both of them provide labels for ROIs from three dimensions: ROI Quality Score, ROI Importance Score, and ROI Distortion Analysis. ROI Quality Scores quantitatively measure the quality for specific ROI. Importance Scores reflect the impact of an ROI on the overall image. Distortion Analysis provides distortion types and the severity degree. Specifically, SEAGULL-100w, consisting of about 100w synthetic distorimages and approximately 33 million masktion (Dist.) based ROIs, is constructed for pre-training to enhance the quality perception ability of models. Each ROI is carefully annotated with the three dimensions labels using reliable models. Specially, Dist. images in SEAGULL-100w are derived from the RAW images using an Image Signal Processor (ISP) under different distortions settings. Compared to RGB-derived Dist. images, RAW-derived Dist. images are more authentic, since more detailed information captured from the camera sensor is reserved in the RAW. However, the discrepancy between synthetic and authenimage hinders the robustness of models in realtic Dist. world images. Thus, we additionally construct the manually annotated SEAGULL-3k dataset for fine-tuning. This dataset consists of 3,261 ROIs for real-world authentic Dist. images and is annotated by 24 trained annotators. For each ROI, annotators are required to evaluate the distortion types, distortion severity degrees, quality score of the ROI and its importance on the overall image quality. To reduce bias, each ROI is annotated by at least 7 annotators, and the labels are determined based on their average results. As shown in Fig.1-(B), after pre-training on SEAGULL100w and fine-tuning on SEAGULL-3k, SEAGULL achieves fine-grained quality analysis for the specified ROI. Extensive experiments demonstrate the superiority of SEAGULL. Our contributions can be summarized as follows: 1. We propose SEAGULL, novel IQA framework that accomplishes fine-grained quality assessment for any specified ROIs from the aspects of Quality Score, Importance Score and Distortion Analysis. 2. We construct two ROI-based IQA datasets: SEAGULL100w and SEAGULL-3k. Compared to existing datasets, ours provide more detailed labels for ROI-based IQA. 3. Experimental results demonstrate that SEAGULL remarkably surpasses existing advanced IQA models and VLMs in ROI quality analysis. 2. Related Work 2.1. Vision-based IQA Vision-based IQA models solely rely on vision features extracted from input images to predict quality scores. Recently, numerous studies improve Vision-based IQA from various perspectives, such as proposing efficient network architectures [23, 26, 45, 60, 62], introducing relevant proxy tasks for pre-training [7, 36, 43, 53], proposing mixed training methods [8, 49, 50, 69] to extract universal information from diverse IQA datasets. Additionally, techniques such as negative sample mining [54], efficient loss function [34], quality ranking learning [14, 17, 39], and contrastive learning [42, 47, 71] have also been explored to further enhance the performance and robustness of Vision-based IQA models. Although these models show powerful performance, they only predict single overall quality score, which poses challenges to the interpretability of IQA. 2 Figure 2. The automatic pipeline for generating the SEAGULL-100w dataset. 2.2. Vision-Language-based IQA Recently, many VLMs [1, 4, 11, 37, 64, 68] achieve powerful performance on visual understanding and also show potential in quality perception [55, 57, 67]. For example, Wu et al. propose Q-Bench [55] and Q-Instruct[57] to evaluate and enhance the quality reasoning capabilities of VLMs. Zhang et al. [70] propose an all-in-one IQA model using language encoder. Moreover, Q-Align [56] integrates Image Aesthetic Assessment, IQA and Video Quality Assessment in one VLM. You et al. [66, 67] explore the ability of VLMs in IQA reasoning and pair-wise comparison. Although VLM-based IQA models demonstrate potential in overall quality analysis and scores predicting, they all overlook the exploration of assessing quality for ROIs. 2.3. IQA Datasets Most existing IQA datasets [10, 13, 16, 20, 33, 35, 44, 46] only provide overall quality scores as labels, lacking finegrained annotation for ROIs. FLIVE [65] provides quality scores for patches. However, these patches are randomly cropped and therefore do not contain practical semantics. Moreover, some reasoning IQA datasets [55, 57, 58, 67] have been proposed to improve the interpretability of IQA models. However, they only provide the coarse description on the overall images and lack the fine-grained description on the specified ROIs. Chen et al. construct QGround100K [6] with mask-based ROIs. The dataset provides single distortion label for each ROI, while ROIs in realworld images often exhibit mixed distortions or high quality. Additionally, it does not include other fine-grained labels about distortion severity degree and the importance of ROIs, which are essential for fine-grained assessment."
        },
        {
            "title": "Although existing methods improve IQA performance",
            "content": "and interpretability from various aspects, they mainly focus on overall IQA and neglect the exploration of ROI-based IQA. To address the challenges of unsuitable networks and datasets, we propose novel network (SEAGULL) and construct two ROI-based IQA datasets (SEAGULL-100w and SEAGULL-3k) to achieve IQA for ROIs. 3. The Proposed Datasets In this section, we introduce the construction process of the SEAGULL-100w and SEAGULL-3k. To make the two datasets suitable for VLMs tuning, we introduce the details of instruction-response design. 3.1. SEAGULL-100w Dataset Fig.2 shows three stages in the automatic construction pipeline for SEAGULL-100w: (1) Dist. image Collection for large-scale synthetic data generating, (2) Mask-based ROI Collection for rich and diverse ROIs obtaining, and (3) Label collection for ROIs fine-grained quality annotations generating. Next, we introduce the three stages in detail. 3.1.1. Distortion Image Collection Existing synthetic IQA datasets [33, 35, 44, 46] generate Dist. images using RGB images. However, RGB images significantly compress the sensor captured information, leading to the reduction of details. This makes RGBimages less authentic. For this reason, we derived Dist. take professional ISP, Adobe LightRoom (LR), to generate Dist. images using 8,146 RAW images in RAISE [12], since RAW images retain all sensor data, offering more details and higher dynamic ranges. Specifically, we firstly generate reference (Ref.) images with the default parameters of cameras. Then, by adjusting 3 the parameters of different ISP modules, we produce six types of distortions, namely exposure, noise, blur, contrast, colorfulness, and compression. We sample twenty different parameters for each of the six distortion types to generate Dist. images. Finally, we obtain about 100w Dist. images. 3.1.2. Mask-based ROI Collection With the advancements in open-world segmentation models such as SAM [30] and SEEM [72], it is efficient and reliable to obtain mask-based ROIs. As shown in Fig.2, we define object-level and component-level mask to assess the quality for an entire object (e.g., cat) or component of an object (e.g., the tail of cat), respectively. For object-level masks, we firstly employ SEEM [72] to detect objects in Ref. images. Subsequently, we employ SAM to extract object-level masks based on these detection BBoxes. For component-level masks, we directly adopt SAM to capture component-level masks with random points for each Ref. image and filter out small ROIs whose areas are smaller than 32 32 pixels. At last, we obtain about 10 million object-level and 23 million component-level masks. 3.1.3. Labels Collection We annotate three types of labels to indicate the ROIs quality, namely ROI Quality Scores, ROI Importance Scores and ROI Distortion Labels. A) ROI Quality Score. Firstly, we crop Dist. and Ref. images into patches using mask-based ROIs collected in Sec.3.1.2 and pad them into minimum rectangles. Then, we obtain quality scores for Dist. ROIs using TOPIQ [5], state-of-the-art (SOTA) FR-IQA that improves the performance by combining semantic and distortion features in top-down approach. B) ROI Importance Score. Different ROIs have significantly different impacts on the overall quality. Improving the quality of critical ROIs effectively enhances the overall quality of an image [22]. Thus, we introduce importance scores to assess the importance of ROIs to the overall image quality. Specifically, we directly replace the corresponding position of the Ref. image with the Dist. ROI to obtain the replaced Dist. image, and then evaluating the difference between the replaced Dist. image and the Ref. image using TOPIQ. If the ROI is important to the overall quality, there will be significant score difference between the two images. Hence, we adopt this method to measure the importance of ROIs to the overall image quality. C) ROI Distortion Labels. As described in Sec.3.1.1, we create the Dist. images with different distortion types. The applied single distortion is set as the distortion label for each ROI. Moreover, we find that ROIs with quality scores higher than 0.92 show no much difference from the Ref. images. Thus, we set the distortion labels of these ROIs with the scores above 0.92 to Without distortions. Otherwise, it is labeled as with the corresponding distortion. 3.2. SEAGULL-3k Dataset Although synthetic Dist. images with reliable annotations are easily accessible, they differ significantly from authentic images, which often display multiple and uneven distortions. This discrepancy hinders the robustness of models in real-world images. Thus, we additionally construct SEAGULL-3k dataset, comprising authentic Dist. images with ROI labels annotated by 24 trained annotators. Initially, we collect 968 Dist. images from four existing authentic IQA datasets: LIVEC [16], BID [10], SPAQ [13], and KonIQ [20]. Subsequently, following Sec.3.1.2, we obtain two or three component-level and object-level ROIs for each images and obtain 3,261 ROIs. For each ROI, at least 7 annotators assign the labels. Specifically, for the distortion label, annotators assess the six specified distortions and assign rating from six pre-defined distortion levels: extreme (0), severe (1), moderate (2), minor (3), trivial (4), or nonexistent (5). If majority of annotators rate non-existent, then the ROI is labeled as without that specific distortion; conversely, it is labeled as with the distortion and its severity degree is calculated by averaging the levels rated by annotators. For the quality label, annotators assign ROI quality from five levels: bad (0), poor (1), fair (2), good (3) and excellent (4). Similarly, ROI importance score is rated from unimportant (0), minor (1), normal (2), important (3) and essential (4). The final quality and importance score of ROI are the average of scores assigned by annotators. Subsequently, we discretize these scores into five categorical levels using the following equation [56]: , if (1) < st K(st) = kt (i + 1) 5 5 where st denotes scores for quality (sqs), importance (sis) and distortion severity degree (sdsd). kt , kdsd } represents categories [Bad, Poor, Fair, Good, Excellent] for quality scores (kqs ), [Unimportant, Minor, Normal, Important, Essential] for importance scores (kis ), and [Extreme, Severe, Moderate, Minor, Trivial] for distortion severity levels (kdsd ) and is the category index. {kqs , kis 3.3. Instruction-Response Design It is crucial to design reliable instruction-responses for VLM tuning. In this paper, we design two types of instruction-response namely Analysis Instruction-Response (AIR) and Judgment Instruction-Response (JIR). The AIR encourages VLMs to answer the questions about the ROI quality, importance and distortion analysis. The JIR queries the model whether the provided instructions are correct. A) Analysis Instruction-Response. We design three types of AIRs: 1) For ROI quality scores: Analyze the quality of this region. 2) For ROI importance scores: Consider the impact of this region on the overall image quality. Analyze its importance. 3) For distortion labels: Analyze the 4 Figure 3. Overview of the SEAGULL (left) and the Mask-based Feature Extractor (right). Best viewed in color. distortions of this region. B) Judgment Instruction-Response. To enhance the quality understanding and robustness of VLMs, we additionally design three JIRs, namely quality score judgment, importance score judgment and distortion type judgment, with queries like: Is the quality of this region good, Is this region unimportant to the overall quality or Is this region in blur distortion to judge whether ROI meets the asked conditions. The expected responses are Yes or No. 4. The Proposed SEAGULL Network As illustrated in Fig.3, different from most VLMs [38, 56] that primarily consist of an image encoder and LLM, SEAGULL additionally incorporates mask-based feature extractor (MFE) to extract fine-grained features for ROIs. In the following, we introduce these parts in detail. 4.1. Image Encoder For Multi-level Perception It is essential to extract robust image tokens. CNN-based models such as ResNet [19] and ConvNeXt [40] not only accelerate the training and inference processes, but also extract both global and local features. These features are useful to IQA tasks [7, 48]. In our work, we employe the ConvNeXt-Large model as the image encoder. Specifically, for an image R3HW , the ConvNeXt-Large model sequentially processes it through four layers to extract multilevel image tokens, denoted as Fj RCj Hj Wj , where Cj, Hj and Wj represent the respective dimensions of the feature for 1 4. In SEAGULL, the image tokens from the final layer (F4) are utilized as image tokens and fed into linear project to align with text tokens. 4.2. Mask-based Feature Extractor The mask-based feature extractor (MFE) extracts both global and local view tokens using mask-based ROIs. Global view tokens contains ROI-related quality and semantic features extracted from long distance contexts. And local view tokens provide detailed and direct representation of these ROIs. Both of them supply SEAGULL with different perspectives to understand the quality of ROIs. Specifically, for binary mask R1HW , we firstly take the mask-pooling MP() [59] to extract regional tokens from the multi-level image tokens that fall inside the mask region. Subsequently, these regional tokens are projected into uniform dimension using full connection (FC) layers. We denote them as multi-level basic tokens, which can be formulated as follows: Bj = FCj(MP(M, Fj)) R1P , (2) where FCj() represents the j-th FC layer and denotes the dimension and 1 4. However, it is challenging for SEAGULL to achieve accurate ROI quality analysis solely based on basic tokens, since these tokens lack global contexts. To this end, we use the cross-attention (CA) mechanism to assist SEAGULL to gain comprehensive understandings of ROIs by capturing long distance contexts. At first, we take selfattention (SA) mechanism on the multi-level basic tokens Bj to make SEAGULL comprehend basic information. For multi-level image tokens, we adopt the reshape operation to change the original dimensions from RCj Hj Wj to RCj (Hj Wj ). Then, another FC layers project these features into the same dimension as the multi-level basic tokens, i.e. Fj = FCj(Fj) RCj . Finally, we set the multi-level basic tokens as queries (Q = SA(Bj)) to capture the global attention tokens from these multi-level image tokens served as keys (K = Fj) and values (V = Fj) , which can be formulated as follows: Gj = CA(Q = SA(Bj), = Fj, = Fj) R1P , (3) where CA() and SA() represent the cross-attention and 5 self-attention operation. After summing up the 4 basic tokens Bj and 4 global tokens Gj, respectively, two additional FC layers are used to further fuse the summed tokens. Then, we concatenate the two fused tokens into one combined feature, and feed it into another FC layer to gain the final global view tokens. This process can be formulated as follows: (cid:18)(cid:88)4 (cid:18)(cid:88)4 (cid:19)(cid:21)(cid:27) (cid:26) (cid:19) Bj , FC3 i=1 Gj i=1 , (4) Fgvt = FC1 CAT (cid:20) FC where Ck=1,2,3() represents FC layers and CAT() denotes the concatenation operation. Fgvt represents the global information related to the ROIs. To make SEAGULL better comprehend the quality of the ROIs, it is also necessary to provide local view. Although basic tokens contain local features, they offer limited details for ROIs. That is because basic tokens are extracted from multi-level image features, which represent the full image rather than the details of specified ROIs. Thus, we additionally introduce local view token Flvt. Specifically, we employ two CNNs to capture the features from the cropped ROI (Ic) using masks. Then, we use SA to further fuse the features and adopt FC layer to project it into the same dimension as the Fgvt. Finally, we get the local view token Flvt = FC(SA(CNN(Ic))) R1P . 4.3. LLM For Multi-modal Features Understanding The LLM fuses different multi-modal features and complete tasks based on text sequences. We set the text sequences similar to <image>n. This is the overview of the image. Here is the region <global> <local>. Analyze the quality of the region. This sequence is tokenized into text tokens, where the three special placeholders (<image>, <global> and <local>) are replaced with image tokens (F4), global view tokens (Fgvt) and local view tokens (Flvt). 5. Experiment 5.1. Experiment Settings A) Training Stages. Firstly, we pre-train SEAGULL on SEAGULL-100w to enhance the quality feature extraction capabilities. Then, we fine-tune the pre-trained SEAGULL on SEAGULL-3k to adapt to authentic images. The settings for these two stages are as follows: Stage 1: Pre-training on the SEAGULL-100w. At this stage, we fix the image encoder and pre-train the linear projector, MFE and LLM on SEAGULL-100w. This pre-training enables SEAGULL to capture more robust ROI quality-related features, enhancing the efficiency of subsequent fine-tuning with smaller datasets. Stage 2: Fine-tuning on the SEAGULL-3k. At this stage, we load the pre-trained weights from Stage 1 and fix the visual encoder while fine-tuning the remaining modules on SEAGULL-3k dataset. This stage allows SEAGULL to effectively adapt to real-world ROI-based IQA tasks. 6 B) Implement Details. We use ConvNeXt large [40] as the image encoder and the Vicuna [9] as the LLM, with maximum token length of 2,048. We apply the AdamW for training, with images resized to 512 512. Both stages are trained using LoRA [21] with lora rank of 128 and lora alpha of 256 on 8 NVIDIA 3090 GPUs, and the learning rate is adjusted using the Cosine Annealing. At the stage 1, pre-training is conducted with batch size of 32 and learning rate of 4 105 for 1 epochs. AIRs and JIRs are generated randomly based on ground truth (GT). At the stage 2, fine-tuning uses the fixed AIRs and JIRs with batch size of 32 and learning rate of 1 105 for 5 epochs. The images in SEAGULL-3k are split into 80% for training and 20% for testing to ensure no image overlapping. Performance evaluations are based on the final models after 5 epochs on SEAGULL-3k. Following the Q-Align [56], we use closed-set probability calculations to translate discrete levels into continuous scores for all VLM-based methods on Quality Scores and Importance Scores. C) Evaluation Metrics. For ROI Quality Scores and Importance Scores, we use Spearmans Rank Order Correlation Coefficient (SROCC) and Pearsons Linear Correlation Coefficient (PLCC) for evaluation, both of which range from -1 to 1. For Distortion Severity Degrees and Distortion Types, we use Precision, Recall, and F1 score, all ranging from 0 to 1. The higher the values of all five metrics, the better the performance. 5.2. Compared with State-of-the-art Models A) ROI-based Assessment Comparison. To demonstrate the efficacy of SEAGULL in ROI quality analysis, we conduct experiments on four sub-tasks: ROI Quality Scores prediction, Importance Scores prediction, Distortion Severity Degrees prediction and Distortion Types identification. We compare the performance of SEAGULL with twelve SOTA models, namely four vision-based IQA models (HyperIQA [48], DBCNN [69], QualiCLIP [2], and PromptIQA [8]), five VLMs (Yi-VL [3], mPLUG-Owl2[64], Qwen2-VL [51], LLaVA-1.5 [38] and Osprey [68]) and three VLMs pre-trained on IQA related datasets (mPLUGOwl2-Q-Align [56], mPLUG-Owl2-Q-Instruct [57] and LLaVA-1.5-Q-Instruct [57]). Except for the HyperIQA, DBCNN and QualiCLIP, which train separate models for different sub-tasks, others train only one model for all subtasks (All-In-One). For the vision-based models, we adopt crop-based ROIs. For the VLMs, ROIs are indicated using BBoxes, except Osprey and SEAGULL using masks. The ROIs and text prompts are used to qurey the models for ROI quality analysis. We calculate Precision, Recall and F1 score between predictions and GTs for each ROI sample. Then, we average the results across all ROIs to obtain Sample-Average Precision, Sample-Average Recall and Sample-Average F1 score. Table 1. ROI-based assessment comparison on four sub-tasks on the test set of SEAGULL-3k in terms of SROCC, PLCC, Sample-Average Precision, Sample-Average Recall and Sample-Average F1 Score. Best and second-best scores are marked in bold and underline, respectively. * denotes all-in-one models. denotes pre-training on SEAGULL-100w. Models HyperIQA DBCNN QualiCLIP PromptIQA* Yi-VL(6B)* mPLUG-Owl2 (7B)* Qwen2-VL (7B)* LLaVA-1.5 (7B)* mPLUG-Owl2 (Q-Align)* mPLUG-Owl2 (Q-Instruct)* LLaVA-1.5 (Q-Instruct)* Osprey (7B)* Seagull (7B)* Inputs Crop-based ROI Importance Score Distortion Type Labels Distortion Severity Degree Quality Score SROCC PLCC SROCC PLCC Precision (%) Recall (%) F1 Score (%) Precision (%) Recall (%) F1 Score(%) 0.7120 0.7162 0.6645 0.6836 0.6721 0.3832 0.6166 0.6090 0.4902 0.7377 0.7112 0.6028 0.5315 0.5427 0.6697 0.6281 0.6321 0.7176 0.6539 0.6533 0.7153 0.5693 0.5774 0.7338 0.6562 0.6622 0.5339 0.6644 0.6559 0.5172 0.6606 0.6623 0.7667 0.7176 0.7173 0.8811 & Full Image & Text 0.7452 0.7465 0.8603 23.44% 53.86% 45.83% 54.95% 42.19% 46.75% 53.96% 56.25% 59.08% 23.44% 56.37% 45.03% 57.55% 39.77% 64.04% 56.77% 62.52% 66.87% 23.44% 57.52% 51.15% 59.33% 52.44% 40.80% 57.87% 58.17% 59.90% 21.07% 27.00% 24.50% 25.19% 12.20% 25.25% 26.52% 29.55% 32.51% 21.07% 28.35% 27.41% 25.10% 15.60% 16.96% 27.69% 27.17% 29.50% 21.07% 26.69% 25.02% 24.14% 13.02% 19.00% 26.02% 26.72% 29.03% 0.6636 0.3551 0.4915 0.5991 0.6926 0.7173 0.7161 0.7377 0.5127 0.5037 0.7605 0.8756 0.8468 BBox-based ROI & Full Image & Text Mask-based ROI As shown in Tab.1, SEAGULL outperforms most compared models on the four sub-tasks. For vision-based methods, which take the crop-based ROIs as the input, they show limited performance on Importance Score prediction. This is because the cropped ROIs discard global information, limiting the models ability to understand the ROIs impact on the overall image. Additionally, compared to VLMs that use BBoxes to indicate ROIs, SEAGULL surpasses them on all sub-tasks. The results suggest that BBox-based ROIs introduce irrelevant background information and fail to precisely indicate ROIs, leading to inaccurate guidance. Despite these methods have been pretrained on large-scale datasets, they still exhibit suboptimal performance. This is since that most datasets for VLM pretraining primarily focus on high-level tasks, resulting in misalignment with the low-level task. Furthermore, VLMs pre-trained on IQA-related datasets, such as mPLUG-Owl2 (Q-Align), mPLUG-Owl2 (Q-Instruct) and LLaVA-1.5 (QInstruct), still show limited performance. This is because these datasets are designed for overall quality analysis, which differs from the fine-grained ROI-based IQA. Compared to Osprey, which uses mask-based ROIs and is pretrained on SEAGULL-100w, SEAGULL demonstrates superior performance across most sub-tasks, excepting Importance Score prediction. This slight advantage for Osprey (0.02 in SROCC and 0.03 in PLCC) can be attributed to its design for image content comprehension, enabling better ability of semantic understanding. Since Importance Score prediction highly depends on the semantics of the ROI and the global context, Osprey performs marginally better in this specific sub-task, It worth noting that all models metrics in the Severity Degree identification task are generally low. In this task, prediction is considered correct only when both the Severity Degree and the Distortion Type align with the GT. Thus, this is multi-class task with 30 categories (5 severity degree for 6 distortion types), making it more challenging than other sub-tasks. Nevertheless, SEAGULL still surpasses the second-best model by approximately 4.05% to 10.00%. B) Distortion Type Identification Comparison. In this experiments, we compare the F1 scores of SEAGULL with other VLMs across each distortion type. The results in Tab.2 demonstrate that SEAGULL outperforms other VLMs on nearly all distortion types and the clean condition (Without Distortion). On average, the F1 Score of SEAGULL surpasses that of the second-best model (Osprey) by 5.39%. Some models which are not pre-trained on SEAGULL100w struggle with Compression identification, primarily due to the smaller number of related samples in SEAGULL-3k, making this distortion more challenge to learn. Additionally, VLMs pre-trained on other IQA datasets, such as mPLUG-Owl2 (Q-Align) and mPLUG-Owl2 (QInstruct), exhibit limited performance in distortion identification. This is because these IQA datasets focus on overall quality assessment rather than fine-grained ROI-based distortion understanding. In contrast, Osprey and SEAGULL, both pre-trained on SEAGULL-100w, demonstrate the capability to identify all distortion types effectively. These results further demonstrate the effectiveness of the SEAGULL-100w dataset and the efficiency of the SEAGULL. 5.3. More Discussion A) Pre-training Impact Discussion. To evaluate the impact of pre-training with SEAGULL-100w on SEAGULLs performance, we pre-train SEAGULL using various data scales (0%, 25%, 50% and 100%) and subsequently finetune it on SEAGULL-3k. The results in Tab.3 indicate that SEAGULL pre-trained with only 25% data significantly outperforms the model without pre-training (0%) on all subtasks. This validates that pre-training on the SEAGULL100w effectively enhances models ability of quality feature extracting. Additionally, with pre-training scales increasing, SEAGULLs performance on Importance Score, Distortion Severity Degree and Distortion Types consistently im7 Table 2. Distortion types identification accuracy comparison on the test set of SEAGULL-3k in terms of F1 Score. Best and second-best scores are highlighted in bold and underline, respectively. denotes pre-training on SEAGULL-100w. Models Qwen2-VL LLaVA-1.5 mPLUG-Owl2 mPLUG-Owl2 (Q-Align) mPLUG-Owl2 (Q-Instruct) Osprey Seagull ROI Type BBox-based ROI & Full Image & Text 0.00% 23.53% 9.52% 0.00% 5.26% 20.83% & Full Image & Text 83.33% 39.48% 52.20% 25.00% Blur Colorfulness Noise Compression Contrast Exposure Clean Average 17.24% 38.16% 53.33% 32.58% 23.91% 51.34% 49.56% 42.95% 20.93% 47.88% 44.71% 38.22% 18.39% 30.00% 37.78% 27.89% 10.81% 47.45% 1.58% 30.77% 28.57% 50.10% 45.83% 44.53% 24.00% 51.94% 52.58% 46.93% 14.93% 33.63% 22.70% 15.69% 33.02% 38.91% 37.30% 39.59% 42.38% 24.03% 38.58% 46.43% 67.14% 79.09% 79.41% 69.38% 78.70% 81.05% Mask-based ROI proves. However, as the pre-trianing scales up from 50% to 100%, the improvement of SEAGULL in Quality Score prediction slows down. This is due to the gap between the synthetic and authentic Dist. images, resulting in negligible performance gains even with more pre-training data. Table 3. The impact of pre-training scales on SEAGULL-100w in terms of SROCC, PLCC, Sample-Average Recall and SampleAverage F1 Score. Best scores are highlighted in bold. Table 4. Ablation studies on critical components of the SEAGULL in terms of SROCC, PLCC, Sample-Average Recall and SampleAverage F1 Score. Best scores are highlighted in bold. Variants Quality Score Importance Score Severity Degree Distortion Degree SROCC PLCC SROCC PLCC Recall F1 Score Recall F1 Score 28.09% 25.49% 55.94% 50.18% 31.37% 28.91% 63.59% 58.12% 31.49% 28.72% 65.44% 58.16% 27.04% 24.37% 62.14% 54.82% 32.51% 29.03% 66.87% 59.08% w/o Pre-train 0.6236 0.6238 0.7512 0.6954 0.7022 0.8020 0.7211 0.7331 0.8538 0.5671 0.5761 0.2475 0.7452 0.7465 0.8603 w/o JIR w/o Local w/o Global Full 0.7628 0.7874 0.8409 0.2503 0.8468 Scale Importance Score Distortion Degree Distortion Type Quality Score SROCC PLCC SROCC PLCC Recall F1 Score Recall F1 Score 28.09% 25.49% 55.94% 50.18% 28.10% 25.64% 61.64% 56.12% 30.34% 28.20% 64.79% 58.11% 32.51% 29.03% 66.87% 59.08% 0% 0.6236 0.6238 0.7512 25% 0.6892 0.6866 0.7760 50% 0.7441 0.7389 0.7878 100% 0.7452 0.7465 0.8603 0.7628 0.7776 0.7926 0.8468 B) Ablation Studies. To evaluate the efficiency of proposed components, we train four variants: I) w/o Pre-train, which is directly trained on SEAGULL-3k; II) w/o JIR, which is trained on SEAGULL-3k that excludes JIRs as described in Sec.3.3; III) w/o Local View Tokens and IV) w/o Global View Tokens, which remove the local and global tokens from the MEF, respectively. Except the w/o Pre-train, other variants are pre-trained on SEAGULL-100w. The results from Tab.4 demonstrate that removing any components negatively impacts model performance. Notably, the variant without pre-training shows remarkable decline in performance, validating the significance of pretraining on the proposed SEAGULL-100w. Additionally, removing the JIR during training also shows drop performance in the four sub-tasks, especially for the Quality Score and Importance Scores, suggesting that JIR enhances the SEAGULLs understanding of these aspects. Furthermore, the removal of global tokens significantly degrades the performance, particularly in Importance Score prediction. This is because global tokens contain rich quality and semantic features. Moreover, the results of removing the local tokens indicate that the local detail provided by local view tokens enhance SEAGULLs understanding of fine-grained details. C) Visualization. We visualize analysis results from SEAGULL, human and three open-access VLMs: LLaVA-1.5, GPT-4v [1], and Q-Instruct, using authentic images. As illustrated in Fig.4, all VLMs face challenges in analyzing the quality of ROIs. In contrast, SEAGULL demonstrates strong consistency with human perception, which further validates its reliability. Figure 4. ROI quality analysis results from Human, VLMs and SEAGULL. Best viewed in color. 6. Conclusion In this paper, we have achieved IQA for ROIs using the difficulty VLMs and addressed two key challenges: of VLMs in perceiving fine-grained quality and the lack of ROI-based IQA datasets. Firstly, we propose novel network, SEAGULL, which incorporates VLM, masks generated by SAM, and carefully designed MFE. Then, we construct large-scale synthetic Dist. dataset, SEAGULL-100w, for pre-training to make models effectively percept ROI quality. And we manually annotate an authentic Dist. dataset, SEAGULL-3k, for fine-tuning to improve models generalization on authentic Dist. images. Experiments demonstrate that after pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL exhibits outstanding ROI quality assessment capabilities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 8 [2] Lorenzo Agnolucci, Leonardo Galteri, and Marco Bertini. Quality-aware image-text alignment for real-world image quality assessment, 2024. 6 [3] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. 6 [4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 3 [5] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2024. 4 [6] Chaofeng Chen, Sensen Yang, Haoning Wu, Liang Liao, Zicheng Zhang, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Q-ground: Image quality grounding with large multi-modality models. arXiv preprint arXiv:2407.17035, 2024. [7] Zewen Chen, Juan Wang, Bing Li, Chunfeng Yuan, Weihua Xiong, Rui Cheng, and Weiming Hu. Teacher-guided learning for blind image quality assessment. In Proceedings of the Asian Conference on Computer Vision, pages 2457 2474, 2022. 2, 5 [8] Zewen Chen, Haina Qin, Juan Wang, Chunfeng Yuan, Bing Li, Weiming Hu, and Liang Wang. Promptiqa: Boosting the performance and generalization for no-reference image quality assessment via prompts. In European Conference on Computer Vision, pages 247264. Springer, 2025. 1, 2, 6 [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 6 [10] Alexandre Ciancio, Eduardo AB da Silva, Amir Said, Ramin Samadani, Pere Obrador, et al. No-reference blur assessment IEEE of digital pictures based on multifeature classifiers. Transactions on image processing, 20(1):6475, 2010. 2, 3, 4 [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. 3 [12] Duc-Tien Dang-Nguyen, Cecilia Pasquini, Valentina Conotter, and Giulia Boato. Raise: raw images dataset for digital image forensics. In Proceedings of the 6th ACM Multimedia Systems Conference, page 219224, New York, NY, USA, 2015. Association for Computing Machinery. [13] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36773686, 2020. 3, 4 [14] Fei Gao, Dacheng Tao, Xinbo Gao, and Xuelong Li. Learning to rank for blind image quality assessment. IEEE transactions on neural networks and learning systems, 26(10): 22752290, 2015. 2 [15] Guangyong Gao, Hui Zhang, Zhihua Xia, Xiangyang Luo, and Yun-Qing Shi. Reversible data hiding-based contrast enhancement with multi-group stretching for roi of medical image. IEEE Transactions on Multimedia, 2023. 2 [16] Deepti Ghadiyaram and Alan Bovik. Massive online crowdsourced study of subjective and objective picture quality. IEEE Transactions on Image Processing, 25(1):372387, 2015. 3, 4 [17] Alireza Golestaneh, Saba Dadsetan, and Kris Kitani. No-reference image quality assessment via transformers, relIn Proceedings of the ative ranking, and self-consistency. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 12201230, 2022. 2 [18] Sunhyoung Han and Nuno Vasconcelos. Image compression using object-based regions of interest. In 2006 International Conference on Image Processing, pages 30973100, 2006. 2 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [20] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning IEEE Transactions on of blind image quality assessment. Image Processing, 29:40414056, 2020. 2, 3, 4 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6 9 [22] Janaki. Enhanced roi (region of interest algorithms) for medical image compression. International Journal of Computer Applications, 38(2):3843, 2012. 4 [23] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolutional neural networks for no-reference image quality assessment. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 17331740, 2014. 2 [24] Chia-Hao Kao, Ying-Chieh Weng, Yi-Hsin Chen, Wei-Chen Chiu, and Wen-Hsiao Peng. Transformer-based variable-rate image compression with region-of-interest control. In 2023 IEEE International Conference on Image Processing (ICIP), pages 29602964. IEEE, 2023. [25] Anureet Kaur, Akshay Girdhar, and Navdeep Kanwal. Region of interest based contrast enhancement techniques for In 2016 Second International Conference on ct images. Computational Intelligence & Communication Technology (CICT), pages 6063, 2016. 2 [26] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 51485157, 2021. 1, 2 [27] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, ChiKeung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36, 2024. 2 [28] Dae-Yeol Kim, Kwangkee Lee, and Chae-Bong Sohn. Assessment of roi selection for facial video-based rppg. Sensors, 21(23), 2021. 1 [29] Dae-Yeol Kim, Kwangkee Lee, and Chae-Bong Sohn. Assessment of roi selection for facial video-based rppg. Sensors, 21(23):7923, 2021. 2 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 39924003, 2023. 1, 2, [31] Lingchao Kong, Rui Dai, and Yuchi Zhang. new quality model for object detection using compressed videos. In 2016 IEEE International Conference on Image Processing (ICIP), pages 37973801, 2016. 1 [32] Homin Kwon, Hagyong Han, Sungmok Lee, Wontae Choi, and Bongsoon Kang. New video enhancement preprocessor using the region-of-interest for the videoconferencing. IEEE Transactions on Consumer Electronics, 56(4):2644 2651, 2010. 1 [33] Eric Cooper Larson and Damon Michael Chandler. Most apparent distortion: full-reference image quality assessment and the role of strategy. Journal of electronic imaging, 19 (1):011006, 2010. 3 [34] Dingquan Li, Tingting Jiang, and Ming Jiang. Norm-innorm loss with faster convergence and better performance In Proceedings of the 28th for image quality assessment. ACM International Conference on Multimedia, pages 789 797, 2020. 2 [35] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: large-scale artificially distorted iqa database. In 2019 Tenth International Conference on Quality of Multimedia Experience (QoMEX), pages 13. IEEE, 2019. [36] Kwan-Yee Lin and Guanxiang Wang. Hallucinated-iqa: Noreference image quality assessment via adversarial learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 732741, 2018. 2 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, pages 3489234916. Curran Associates, Inc., 2023. 3 [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 5, 6 [39] Xialei Liu, Joost Van De Weijer, and Andrew Bagdanov. Rankiqa: Learning from rankings for no-reference image quality assessment. In Proceedings of the IEEE international conference on computer vision, pages 10401049, 2017. 2 [40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. 5, 6 [41] Yi Ma, Yongqi Zhai, Chunhui Yang, Jiayu Yang, Ruofan Wang, Jing Zhou, Kai Li, Ying Chen, and Ronggang Wang. Variable rate roi image compression optimized for visual quality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1936 1940, 2021. [42] Pavan Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, and Alan Bovik. Image quality assessment using contrastive learning. IEEE Transactions on Image Processing, 31:41494161, 2022. 2 [43] Zhaoqing Pan, Feng Yuan, Jianjun Lei, Yuming Fang, Xiao Shao, and Sam Kwong. Vcrnet: Visual compensation restoration network for no-reference image quality assessIEEE Transactions on Image Processing, 31:1613 ment. 1627, 2022. 2 [44] Nikolay Ponomarenko, Oleg Ieremeiev, Vladimir Lukin, Karen Egiazarian, Lina Jin, Jaakko Astola, Benoit Vozel, Kacem Chehdi, Marco Carli, Federica Battisti, et al. Color image database tid2013: Peculiarities and preliminary results. In european workshop on visual information processing (EUVIP), pages 106111. IEEE, 2013. 3 [45] Guanyi Qin, Runze Hu, Yutao Liu, Xiawu Zheng, Haotian Liu, Xiu Li, and Yan Zhang. Data-efficient image quality assessment with attention-panel decoder. Proceedings of the AAAI Conference on Artificial Intelligence, 37:20912100, 2023. 2 [46] Hamid Sheikh, Muhammad Sabir, and Alan Bovik. statistical evaluation of recent full reference image quality IEEE Transactions on image proassessment algorithms. cessing, 15(11):34403451, 2006. 2, 3 [47] Jinsong Shi, Pan Gao, and Jie Qin. Transformer-based no-reference image quality assessment via supervised contrastive learning, 2023. [48] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by self-adaptive hyper network. In 10 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36673676, 2020. 1, 5, 6 [49] Wei Sun, Xiongkuo Min, Danyang Tu, Siwei Ma, and Guangtao Zhai. Blind quality assessment for in-the-wild images via hierarchical feature fusion and iterative mixed database training. IEEE Journal of Selected Topics in Signal Processing, 2023. 2 [50] Juan Wang, Zewen Chen, Chunfeng Yuan, Bing Li, Wentao Ma, and Weiming Hu. Hierarchical curriculum learning for no-reference image quality assessment. International Journal of Computer Vision, pages 120, 2023. 1, 2 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [52] Yi Wang, Shixin Zheng, Xiao Sun, Dan Guo, and Junjie Lang. Micro-expression recognition with attention mechanism and region enhancement. Multimedia Systems, 29(5): 30953103, 2023. 2 [53] Zhihua Wang and Kede Ma. Active fine-tuning from gmad IEEE examples improves blind image quality assessment. Transactions on Pattern Analysis and Machine Intelligence, 44(9):45774590, 2021. 2 [54] Zhihua Wang, Qiuping Jiang, Shanshan Zhao, Wensen Feng, and Weisi Lin. Deep blind image quality assessment powered by online hard example mining. IEEE Transactions on Multimedia, 2023. 2 [55] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 1, 2, 3 [56] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 3, 4, 5, 6 [57] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visual abilities for multi-modality foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2549025500, 2024. 1, 2, 3, [58] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, and Weisi Lin. Towards open-ended visual quality comparison, 2024. 3 [59] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panopIn tic segmentation with text-to-image diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 5 [60] Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, and Weisi Lin. Local distortion aware efficient transformer adaptation for image quality assessment. arXiv preprint arXiv:2308.12001, 2023. 2 [61] Ling Yang, Li Zhang, Siwei Ma, and Debin Zhao. roi quality adjustable rate control scheme for low bitrate video coding. In 2009 Picture Coding Symposium, pages 14, 2009. 1 [62] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11911200, 2022. 1, 2 [63] Yang Yang, Weiming Zhang, Dong Liang, and Nenghai Yu. roi-based high capacity reversible data hiding scheme with contrast enhancement for medical images. Multimedia Tools and Applications, 77:1804318065, 2018. [64] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language arXiv preprint model with modality collaboration. arXiv:2311.04257, 2023. 2, 3, 6 [65] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3 [66] Zhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, and Chao Dong. Depicting beyond scores: Advancing image quality assessment through multi-modal language models. arXiv preprint arXiv:2312.08962, 2023. 1, 3 [67] Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Tianfan Xue, and Chao Dong. Descriptive image quality assessment in the wild. arXiv preprint arXiv:2405.18842, 2024. 1, 2, 3 [68] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning, 2024. 3, 6 [69] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment using deep bilinear IEEE Transactions on Circonvolutional neural network. cuits and Systems for Video Technology, 30(1):3647, 2018. 2, 6 [70] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via visionlanguage correspondence: multitask learning perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1407114081, 2023. 3 [71] Kai Zhao, Kun Yuan, Ming Sun, Mading Li, and Xing Wen. Quality-aware pre-trained models for blind image quality In Proceedings of the IEEE/CVF Conference assessment. on Computer Vision and Pattern Recognition, pages 22302 22313, 2023. 2 [72] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae 11 In AdLee. Segment everything everywhere all at once. vances in Neural Information Processing Systems, pages 1976919782. Curran Associates, Inc., 2023. 2,"
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "Beijing Union University",
        "China University of Petroleum",
        "PeopleAI Inc.",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "School of Information Science and Technology, ShanghaiTech University",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA"
    ]
}