{
    "paper_title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning",
    "authors": [
        "Yuqian Fu",
        "Tinghong Chen",
        "Jiajun Chai",
        "Xihuai Wang",
        "Songjun Tu",
        "Guojun Yin",
        "Wei Lin",
        "Qichao Zhang",
        "Yuanheng Zhu",
        "Dongbin Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 6 7 9 1 . 6 0 5 2 : r Supervised and Reinforcement Fine-Tuning SRFT: SINGLE-STAGE METHOD WITH SUPERVISED AND REINFORCEMENT FINE-TUNING FOR REASONING Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao Institute of Automation, Chinese Academy of Sciences School of Artificial Intelligence, University of Chinese Academy of Sciences Meituan Shanghai Jiao Tong University {fuyuqian2022,yuanheng.zhu}@ia.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through twostage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks. Project Website: https://anonymous.4open.science/w/SRFT2025 Model Website: https://huggingface.co/Yuqian-Fu/SRFT"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in Large Language Models (LLMs) for reasoning (OpenAI, 2025; Guo et al., 2025; Anthropic, 2025) have demonstrated remarkable capabilities in complex problem-solving tasks. Despite these remarkable achievements, fine-tuning strategies for enhancing reasoning capabilities remain an active area of research, presenting both opportunities and challenges. Initial approaches often treat Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) as distinct, sequential phases. For instance, SFT might be used for instruction-following, followed by RL for alignment. However, this separation presents challenges: SFT can lead to models that memorize patterns without developing true reasoning abilities, potentially overfitting the training dataset (Chu et al., 2025; Chen et al., 2025a). Conversely, RL methods, while promising for exploration and reward optimization, can be sample inefficient, struggle with effective exploration in vast solution spaces (Gao et al., 2025; Dou et al., 2025; Schmied et al., 2025), or suffer from issues like mode collapse, where the model repeatedly generates similar, suboptimal outputs (Cai et al., 2025). Instead of simple sequential approaches, recent work (Yan et al., 2025; Wu et al., 2025; Liu et al., 2025a; Chen et al., 2025b; Liu et al., 2025b) has shown movement towards integrated frameworks that unify SFT and RL paradigms, or dynamically switch between the two fine-tuning methods during the LLM training process. As illustrated in Figure 1(a), SFT guides LLM policies toward demonstration distributions, while RL enables policies to explore improved solutions in the neighborhood of the base policy. Our illustration demonstrates special case: when the base policy is positioned near suboptimal policy, the RL rollouts alone cannot effectively navigate to the optimal policy. Beyond applying SFT and RL individually, the unified integration of SFT and RL within single-stage method Equal contribution. Corresponding authors. Work in progress. 1 Supervised and Reinforcement Fine-Tuning (a) Toy illustration of SFT, RL, and SRFT for LLM reasoning training on single prompt. (b) Framework of SRFT. Our method effectively leverages demonstrations to improve reasoning capabilities. Figure 1: Overview of SRFTs motivation and framework. (e.g., our proposed SRFT) enables policies to directly optimize toward better solutions across an expanded space. However, challenge remains in determining the balance between SFTs knowledge distillation and RLs policy optimization: insufficient integration risks error propagation and limits RLs improvements, while excessive reliance on demonstrations leads to overfitting that constrains exploration beyond the base policy distribution. This trade-off creates confusion for practitioners in choosing between SFT for leveraging demonstrations and RL for policy exploration. To address these issues, in this work, we study how to build single-stage LLM fine-tuning algorithms that are not only effective for LLM reasoning from SFT datasets but also well-suited to continuous improvement with RL rollouts. We conduct comprehensive analysis of the roles that SFT and RL play in fine-tuning LLM reasoning. Through our analysis in Sec. 3, we obtain the following key findings that guide our subsequent algorithm design. Key Findings Policy distribution effects (Sec. 3.1.1 and Sec. 3.1.2): During fine-tuning, SFT induces coarse-grained global changes to the LLMs policy distribution, while RL performs finegrained selective modifications. Single-stage optimization (Sec. 3.1.2 and Sec. 3.2.2): Single-stage integration of SFT and RL enables direct optimization for reasoning capabilities and achieves superior training efficiency compared to sequential SFTRL approaches. Entropy as an indicator (Sec. 3.2.1): Entropy dynamics reveal the underlying mechanisms of training processes, enabling balanced weighting between the two paradigms. Based on these insights, we propose Supervised Reinforcement Fine-Tuning (SRFT), single-stage method for LLM reasoning. As shown in Figure 1(b), we integrate SFT into RL and use entropy as an indicator to control the balance between these two paradigms. Specifically, for samples generated by LLM policy rollouts, we employ different RL training losses based on whether sample rewards are positive or negative. For samples from demonstration datasets, we simultaneously apply both SFT and RL objectives. This unified approach enables stable learning from demonstrations at multiple granularities while effectively bridging the complementary strengths of SFT and RL. We evaluate our method on five competition-level mathematical reasoning benchmarks and three out-of-distribution (OOD) benchmarks. Our proposed SRFT achieves an accuracy of 59.1% based on Qwen-2.5-Math-7B (Yang et al., 2024), outperforming previous SFT and RL baselines by significant margins. Moreover, SRFT demonstrates superior generalization capability, achieving an average improvement of over 4.7% compared to other methods utilizing demonstrations. Overall, our key contributions are: We conduct comprehensive analysis of SFT and RL in LLM reasoning, examining their differential effects on policy distributions and learning dynamics. Besides, we analyze the integration of SFT and RL through an entropy-based lens. We propose SRFT, single-stage fine-tuning approach that combines supervised fine-tuning and reinforcement learning with entropy-aware weighting mechanisms, enabling effective utilization of demonstrations while maintaining stable exploration dynamics. We demonstrate SRFTs superior performance across eight challenging benchmarks, achieving substantial improvements of 9.0% and 10.9% over zero-RL baselines on mathematical reasoning and out-of-distribution tasks, respectively. 2 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "2.1 SFT AND RL FOR LLM REASONING",
            "content": "Supervised Fine-Tuning (SFT) is standard approach for adapting pre-trained language models to specific downstream tasks or imparting particular stylistic characteristics. Given dataset = {(xi, yi)}N i=1, where xi is an input prompt and yi is the corresponding target response generated by the behavior policy πβ, the objective is to train the language model policy πθ (with parameters θ) to maximize the conditional probability of generating the target response yi given xi. This is typically achieved by minimizing the negative log-likelihood over the dataset: LSFT(θ) = E(x,y)D[ log πθ(yx)], (1) where yj is the j-th token in the response y, and y<j denotes the sequence of tokens in before yj. Reinforcement Learning (RL) is typically applied after SFT to further align LLMs with complex human preferences or desired behaviors (e.g., reasoning abilities, harmlessness) that are challenging to specify exhaustively through static datasets. In RL training, the LLMs token generation process is modeled as Markov Decision Process (MDP) (Puterman, 2014). We define state st at step as the concatenation of the input prompt and all tokens generated so far y<t. This state serves as input to the policy model πθ(st). Specifically, the policy processes st = (x, y<t) = (x1, x2, . . . , xl, y1, y2, . . . , yt1), where xi denotes the i-th token of the input and yj represents the token generated by πθ at step j. An action at corresponds to the selection of the next output token yt. The LLM, acting as policy πθ(atst), generates trajectory (a sequence of tokens) in response to the prompt x. reward function R(x, y) = (cid:80)T t=1 r(x, yt) provides scalar score for the entire trajectory given prompt x, typically derived from human evaluations or automated metrics. In the context of RL, the behavior policy πβ(yx) refers to the model that generated the responses in the replay buffer. This policy is crucial for RL, particularly for off-policy learning, as it enables proper importance sampling corrections to account for the distribution shift between the data-generating model and the current training model. The MDP formulation in LLMs presents several notable characteristics: Sequential state representation: At each step t, the state st consists of the concatenation of the input prompt and all actions (tokens) generated so far y<t. This state serves as input to the policy model πθ(st). Sparse and delayed rewards: Rewards R(x, y) are typically sparse, provided only upon completion of sequence y. This dependency on the final outputs overall quality complicates credit assignment across the generation process. 2.2 POLICY OPTIMIZATION IN REINFORCEMENT LEARNING To optimize the LLM policy, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) offers different RL algorithm that is presented as memory-efficient variant of Proximal Policy Optimization (PPO) (Schulman et al., 2017). key characteristic is that GRPO typically operates without learned value function. Instead, for given prompt x, it often generates group of responses {y1, . . . , yG} using the current policy. The rewards {R(x, y1), . . . , R(x, yG)} for these responses are then used to compute relative advantage for each response: ˆAk = R(x, yk) mean({R(x, yk)k = 1, 2, . . . , G}) std({R(x, yk)k = 1, 2, . . . , G}) . (2) Then, GRPO maximizes clipped surrogate objective function to ensure stable updates. Let πθold be the policy before the update. For each token yk,t in trajectory yk (from state st), the importance sampling ratio is rk,t(θ) = πθ(yk,tst) (yk,tst) . The objective function for GRPO can then be expressed as: πθold JGRPO(θ) = 1 (cid:88) k=1 1 yk yk (cid:88) t=1 (cid:104) min (cid:110) rk,t(θ) ˆAk, clip {rk,t(θ), 1 ϵ, 1 + ϵ} ˆAk (cid:111)(cid:105) , (3) where ϵ is small hyperparameter that defines the clipping range. Through this mechanism, the LLM policy is updated while maintaining stable gradient constraints. 3 Supervised and Reinforcement Fine-Tuning Figure 2: Visualization of LLM distribution changes during fine-tuning. (a) Heatmap visualization comparing responses generated by fine-tuned and base models, where darker background colors indicate larger probability changes. (b) Distribution of token probability changes across five mathematical reasoning benchmarks."
        },
        {
            "title": "3 ANALYSIS OF SFT AND RL IN LLM REASONING",
            "content": "In this section, we provide comprehensive analysis of the roles of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in LLM reasoning. We first examine their differential effects on token distributions (Sec. 3.1.1) and learning dynamics (Sec. 3.1.2), then investigate their integration mechanisms through an entropy-based perspective (Sec. 3.2). All experiments are conducted across five mathematical reasoning benchmarks (AIME24, AMC, MATH500, Minerva, and Olympiad) with results averaged. We tune hyperparameters for all baseline methods to ensure fair and optimal performance comparisons. 3.1 SFT AND RL EFFECTS ON LLMS: SLEDGEHAMMER VS. SCALPEL 3.1.1 EFFECTS ON TOKEN DISTRIBUTIONS To understand the differential impact of SFT and RL for reasoning, we visualize probability changes across response tokens to identical prompts before and after fine-tuning the same base model (Qwen-2.5-Math-7B). As illustrated in Figure 2(a), the results reveal fundamental asymmetry: SFT substantially alters the probability distribution across the entire response sequence, while RL selectively modifies probabilities for only small subset of tokens, while leaving numerical content and mathematical proof statements largely unchanged. We further quantify these distribution shifts across five benchmarks, as shown in Figure 2(b). The results demonstrate that SFT produces more pronounced changes to policy distributions compared to RL, with token probability changes in RL clustering near zero while SFT exhibits substantially larger magnitude shifts. From theoretical perspective, this behavior can be understood through the gradient of the SFT objective function: θLSFT = E(x,y)D (cid:88) (cid:88) (πθ(vx, y<t) 1v=yt) θ log πθ(vx, y<t) , (4) t=1 vV where is the LLM vocabulary, and 1v=yt is an indicator function that equals 1 when token matches the target token yt and 0 otherwise. The detailed derivation is provided in Appendix D. This formulation reveals that SFT systematically sharpens the model distribution by increasing probabilities for target tokens while decreasing probabilities for all other tokens in the vocabulary, leading to more deterministic outputs. 4 Supervised and Reinforcement Fine-Tuning Table 1: Performance comparison of SFT and RL integration strategies across multiple benchmarks. Bold and underlined indicate the best and second-best performance, respectively. Model AIME24 AMC MATH500 Minerva Olympiad Avg. Qwen2.5-Math-7B SFT RL RLSFT RLSFTKL SFTRL 14.1 21.2 21.2 10.5 13.1 24.5 44.8 53.2 59.3 40.4 45.2 59. 64.8 83.0 83.6 73.6 70.2 86.4 16.5 37.1 36.4 32.0 26.5 39.3 29.6 42.2 46.6 30.7 36.3 53.1 34.0 47.3 49.4 37.4 38.3 52."
        },
        {
            "title": "3.1.2 VISUALIZATION OF LEARNING DYNAMICS",
            "content": "Beyond the token probability analysis, we analyze the training paradigms from the perspective of learning dynamics. Since directly measuring the LLM feature space is computationally intractable, we propose novel visualization approach that maps each model to point in the vocabulary probability space, treating models as functions that transform prompts to output probability distributions over the vocabulary. We establish three reference modelsthe base model (Qwen-2.5-Math-7B), DeepSeek-R1, and QwQ-32B (Team, 2025)as coordinate frames, enabling indirect measurement of model evolution at different fine-tuning steps through the distance in probability space (two models are considered close if they assign similar output probabilities to all tokens across all prompts). Detailed methodology for this visualization is provided in Appendix E. Figure 3: Learning dynamics during different fine-tuning paradigms in three-dimensional probability space. The number denotes the final performance of each training process. The visualization is shown in Figure 3, which demonstrates that all fine-tuning paradigms exhibit performance improvements while simultaneously moving away from the base model space (Qwen-2.5-Math7B). Specifically, SFT exhibits greater distribution changes from the base model compared to RL and achieves higher performance, which further validates our observations in Sec. 3.1.1 that SFT induces larger changes to model distributions while RL fine-tuning within neighborhood of the initialization point. We further analyze two integration approaches: the two-stage SFTRL method and our proposed single-stage SRFT approach detailed in the following section. The results reveal that the learning dynamics of the two-stage SFTRL method traverse from the post-SFT model toward higher-performance region that paradoxically lies closer to the base model, suggesting that the initial SFT phase may induce excessive distributional deviation from the base model, thereby compromising the effectiveness of subsequent RL. In contrast, our single-stage method demonstrates more constrained yet targeted changes in the probability space, enabling more precise optimization compared to sequential integration approaches. 3.2 INTEGRATION OF SFT AND RL: FROM TWO-STAGE TO SINGLE-STAGE 3.2.1 SEQUENTIAL INTEGRATION ANALYSIS In this section, we examine the integration of SFT and RL through the lens of entropy dynamics to understand their complementary roles in LLM fine-tuning. We begin by systematically analyzing two sequential integration approaches: SFTRL and RLSFT, as shown in Figure 4. As demonstrated in Table 1 and Figure 4(a), applying SFT after RL consistently yields suboptimal performance across all benchmarks. To mitigate the detrimental policy shifts induced by RLSFT, we introduced KL divergence constraint (SFTKL, detailed in Appendix B) to regularize the distribution changes. However, even with this constraint, the performance improvements remained limited, 5 Supervised and Reinforcement Fine-Tuning (a) Reasoning capability comparison between SFTRL and RLSFT. (b) Entropy dynamics of two different finetuning paradigms. Figure 4: Comparison between SFTRL and RLSFT. suggesting fundamental incompatibility in this ordering. In contrast, existing methods successfully achieve substantial performance gains through RL when applied after the base model SFT, as evidenced in Table 1. This asymmetric behavior reveals that the sequence of fine-tuning paradigms critically affects the final model performance, motivating our entropy-based analysis to understand the underlying mechanisms. To understand this asymmetric behavior, we analyze the training dynamics of SFT and RL from an entropy perspective. As illustrated in Figure 4(b), policies after RL exhibit significantly lower entropy, approaching deterministic outputs. However, the distribution shift introduced by subsequent SFT causes rapid increase in entropy (corresponding to the sharp performance drop in Figure 4(a)), followed by gradual decline. Moreover, models after RL demonstrate limited capacity for further learning through SFT, as evidenced by the entropy plateau occurring after approximately 90 training steps (Figure 4(b)). In contrast, base models undergoing SFT exhibit brief initial entropy increase followed by sustained decrease, ultimately yielding performance improvements. This distinct entropy trajectory suggests that while RL effectively enhances LLM performance, it simultaneously reduces the models plasticityits capacity to adapt through subsequent training. These findings establish entropy as crucial indicator for effective SFT and RL integration. 3.2.2 SINGLE-STAGE INTEGRATION ANALYSIS Building upon the analysis above, we establish that the SFTRL paradigm demonstrates superior suitability for LLM reasoning compared to RLSFT. However, beyond these sequential integration approaches, we investigate single-stage approach that directly unifies both paradigms (SFT+RL), with the combined objective LSFT+RL = LSFT + LRL. We conduct preliminary experiment comparing pure RL, sequential SFTRL with varying SFT steps, and single-stage SFT+RL, as illustrated in Figure 5. Our empirical findings reveal that the singlestage SFT+RL method achieves superior training efficiency compared to the sequential SFTRL approach. Notably, we observe an intriguing phenomenon in models with extensive SFT pre-training (350 steps SFT followed by 150 steps RL): transient performance degradation during the initial phases of RL. We attribute this behavior to two principal factors: First, SFT datasets derived from other models responses may not consistently represent optimal solutions, even when sourced from high-quality demonstrations, potentially leading to suboptimal policy learning during the SFT phase. Second, pure RL exhibits limited data efficiency due to its inability to effectively leverage demonstrations. In the sequential SFTRL training paradigm, the RL phase may induce catastrophic forgetting of knowledge acquired during SFT (Cai et al., 2025), resulting in transient performance deterioration. Figure 5: Preliminary comparison across pure RL, sequential SFTRL, and single-stage SFT+RL integration approaches. 6 Supervised and Reinforcement Fine-Tuning In contrast, the single-stage SFT+RL method effectively leverages demonstrations through unified optimization. This approach enables direct policy optimization toward the target objective while preserving the knowledge distillation benefits of supervised learning from datasets. Importantly, both datasets utilization methods significantly outperform pure RL across all performance metrics."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we present the Supervised and Reinforcement Fine-tuning (SRFT) algorithm, which integrates the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in single-stage approach. Building upon the RL framework described in Sec. 2.2, SRFT incorporates flexible guidance from demonstrations, enabling the algorithm to harness the complementary strengths of both fine-tuning paradigms. The core innovation of SRFT lies in its single-stage learning mechanism: coarse-grained behavior policy approximation through SFT and fine-grained policy refinement through RL, both applied to demonstration data and self-generated trial-and-error data."
        },
        {
            "title": "4.1 LEARNING FROM DEMONSTRATIONS",
            "content": "Given dataset containing demonstrations Ddemo. (e.g., reasoning responses generated by DeepSeekR1), SRFT employs dual-pronged strategy to effectively harness this valuable resource. First, we leverage SFT to perform coarse-grained approximation of the behavior policy underlying the experts responses. The behavior policy πβ(yx) captures the underlying generation patterns that produced these high-quality responses, which we seek to approximate through supervised learning: SFT = E(x,y)Ddemo.[ log πθ(yx)]. Ldemo. (5) Second, we adopt an off-policy RL approach similar to LUFFY (Yan et al., 2025) to perform finegrained learning of the behavior policy through RL. Specifically, we directly augment the LLMs on-policy rollout group with demonstrations, creating heterogeneous training batch: (6) where Groll. denotes the on-policy rollout group, Gdemo. denotes the demonstration group. The advantage estimation for the entire group is given by: Gaug. = {(xi, yi)}Groll. i=1 {(xj, yj)}Gdemo. j=1 , ˆAk = r(x, yk) mean({r(x, yk)k = 1, 2, . . . , Gaug.}) std({r(x, yk)k = 1, 2, . . . , Gaug.}) . (7) Since responses generated by expert LLMs typically exhibit higher rewards, their inclusion increases the advantage estimation for the entire group as shown in Eq. (2), promoting optimistic exploration in the LLM policy through this mechanism. To address the distribution mismatch between behavior policies πβ of demonstrations and the current training policy πθ identified in our analysis, we implement two key mitigation strategies: For SFT on demonstrations, our entropy analysis in Sec. 3.2 demonstrates that entropy serves as crucial indicator for effective SFT and RL integration. Motivated by this insight, we introduce an adaptive weighting mechanism that dynamically adjusts based on the current policy entropy, employing wSFT = 0.5 stop_grad(exp(H(πθ))) as the SFT weight, where stop_grad() denotes the gradient stopping operation. This entropy-aware mechanism ensures that when the policy exhibits high entropy (indicating uncertainty), the SFT training loss exerts diminished influence on the model updates, thereby mitigating performance degradation caused by distribution mismatch between the behavior policy of demonstrations and the current policy while still enabling effective behavior policy approximation: SFT (θ) = wSFT E(x,y)Ddemo.[ log πθ(yx)]. Ldemo. (8) For off-policy RL training, we introduce an importance sampling term similar to GRPO (Shao et al., 2024) and PPO (Schulman et al., 2017) to account for the distribution shift between the behavior policy and the current policy: (cid:110) rk,t(θ) ˆAk, clip {rk,t(θ), 1 ϵ, 1 + ϵ} ˆAk (θ) = E(x,y)Ddemo. Ldemo. RL min (cid:111)(cid:105) (9) (cid:104) , rk,t(θ) = πθ(yk,txt) πβ(yk,txt) . 7 (10) Supervised and Reinforcement Fine-Tuning Following practices established in recent work (Yan et al., 2025; Ma et al., 2025), we set the behavior policy πβ = 1 to avoid tokenization complexities that arise when aligning the current training policy with the behavior policy, thereby facilitating easy integration of off-the-shelf datasets without requiring recomputation of behavior policy probabilities. Additionally, we omit the clipping operation, as the standard clipping mechanism becomes imbalanced and potentially unstable when πβ = 1."
        },
        {
            "title": "4.2 LEARNING FROM SELF-EXPLORATION",
            "content": "In addition to leveraging the demonstration data, SRFT enables the LLM policy to learn simultaneously from its own exploration rollouts. While traditional RL methods learn from both positive and negative samples generated during rollouts, we observe that under on-policy RL with binary rewards {1, 1}, the basic RL objective function can be naturally decomposed into two distinct components: Lself-rollout RL = ExD,yπθ(x)[R(x, y) log πθ(yx)] = ExD,y+πθ(x)[ log πθ(y+x)] (cid:125) (cid:123)(cid:122) Positive Sample 1 (cid:124) (cid:124) + ExD,yπθ(x)[log πθ(yx)] , (cid:125) (cid:123)(cid:122) Negative Sample 2 (11) where denotes the RL training dataset, and y+ and represent the correct and incorrect responses, respectively. critical insight emerges from this decomposition: the positive sample objective 1 exhibits structural similarity to supervised fine-tuning, as it maximizes the likelihood of correct responses. However, these positive samples are generated on-policy by the current policy πθ rather than sourced from SFT datasets, distinguishing our approach from conventional supervised learning paradigms. The negative reward component 2 implements likelihood minimization, systematically reducing the probability mass assigned to incorrect responses. This structural correspondence suggests that learning from positive samples constitutes coarse-grained optimization strategy that necessitates careful balance. Moreover, in contrast to learning from demonstrations, self-exploration induces rapid entropy reduction as the model converges toward increasingly deterministic outputs, potentially compromising exploration capabilities. To mitigate this phenomenon and preserve training stability, inspired by our analysis in Sec. 3.1.1, we introduce an entropy-adaptive weighting mechanism wRL = 0.1 stop_grad(exp(H(πθ))) specifically for the positive sample objective. This mechanism is similar to our formulation in Eq. (8) but serves the complementary purpose of maintaining exploration diversity. The complete self-exploration objective is formulated as: Lself-rollout RL (θ) = wRL ExD,y+πθ(x)[ log πθ(y+x)] + ExD,yπθ(x)[log πθ(yx)]. (12) 4.3 INTEGRATING DEMONSTRATIONS WITH SELF-EXPLORATION ROLLOUTS IN SINGLE-STAGE APPROACH By leveraging both demonstrations and self-generated rollouts, SRFT effectively balances the coarsegrained adjustments of SFT with the fine-grained refinements of RL throughout the single-stage fine-tuning process. The total loss function combines all four components: LSRFT(θ) = Ldemo. SFT (θ) + Ldemo. RL (θ) + Lself-rollout RL (θ). (13) This objective enables SRFT to simultaneously benefit from demonstrations and self-exploration rollouts while maintaining stable training dynamics through two entropy-aware weighting mechanisms."
        },
        {
            "title": "5 EXPERIMENTS.",
            "content": "5.1 EXPERIMENTAL SETUPS Training Datasets. We employ OpenR1-Math-46k-81921 (Yan et al., 2025) as the training dataset for SRFT, which constitutes subset of OpenR1-Math-220k (Face, 2025) comprising 46,000 mathematical problems sourced from NuminaMath 1.5 (LI et al., 2024), accompanied by high-quality reasoning responses generated by DeepSeek-R1 (Guo et al., 2025). The dataset undergoes filtering through Math-Verify2 to exclude instances with unverifiable answers or responses exceeding 8,192 1https://huggingface.co/datasets/Elliott/Openr1-Math-46k-8192 2https://github.com/huggingface/Math-Verify 8 Supervised and Reinforcement Fine-Tuning tokens in length. This dataset serves multiple purposes in our framework: providing prompts for policy rollouts, ground-truth answers for reward computation, and high-quality demonstrations for SRFT. The dataset details are provided in Appendix C. Evaluation. We conduct comprehensive evaluation on several widely-adopted mathematical reasoning benchmarks, including AIME24 (Li et al., 2024), AMC (Li et al., 2024), Minerva (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), and MATH500 (Hendrycks et al., 2021). For datasets with limited sample sizes (AIME24 and AMC), we report the avg@32 metric; for the remaining three datasets, we adopt pass@1 as the evaluation criterion. Given that our method primarily focuses on mathematical reasoning capabilities, we further assess the models generalization ability on three out-of-distribution benchmarks: ARC-C (Clark et al., 2018) (open-domain reasoning), GPQA-Diamond (Rein et al., 2024) (graduate-level scientific knowledge, denoted as GPQA-D), and MMLU-Pro (Wang et al., 2024) (reasoning problems from academic examinations and textbooks). To mitigate potential information leakage, we randomly shuffle the option orders for all multiplechoice questions. During inference, we set the generation temperature to 0.6 with maximum response length of 8,192 tokens. We employ Math-verify as the verifier for training validation and the OAT-Grader (Liu et al., 2024) for final evaluation. Baseline Methods. We benchmark SRFT against the following baselines using Qwen2.5-Math-7B as the base model. SFT methods: (1) SFT on OpenR1-Math-46k-8192 dataset; (2) SFT training with KL divergence constraints incorporated into the loss function (SFTKL). RL methods: (3) RLGRPO (Shao et al., 2024), simplified PPO variant trained on the same 46k dataset; (4) SimpleRL-Zero (Zeng et al., 2025), applying GRPO to approximately 24k mathematical samples from GSM8K and MATH; (5) OpenReasoner-Zero (Hu et al., 2025), PPO-based approach trained on 129k multi-source samples including AIME; (6) PRIME-Zero (Cui et al., 2025), conducting policy rollouts on 150k NuminaMath queries with implicit process rewards and final labels. SFT and RL methods: (7) SFTRL, sequential training with SFT the same 46k dataset followed by GRPO; (8) ReLIFT (Ma et al., 2025), an approach that interleaves RL with online Fine-Tuning on the hardest questions; (9) LUFFY (Yan et al., 2025), mixed-policy GRPO approach using the same 46k dataset; (10) TAPO (Wu et al., 2025), dynamically integrating structured external knowledge within the GRPO framework, trained on 5.5k samples from the MATH dataset. Implementation Details. Following recent work (Yan et al., 2025; Wu et al., 2025; Cui et al., 2025), we use the Qwen2.5-Math-7B (Yang et al., 2024) model as the base model. In SRFT, we generate 8 rollout trajectories per prompt with maximum sequence length of 8,192 tokens. All experiments are conducted over 500 training steps. Comprehensive experimental details are provided in Appendix B. 5.2 EXPERIMENTAL RESULTS Reasoning Benchmark Performance. Our main results are shown in Table 2, where we compare SRFT with several zero-RL baselines, as well as direct SFT, and SFT+RL methods. Across five challenging competition-level reasoning benchmarks, SRFT achieves an average score of 59.1, significantly outperforming existing RL methods by margin of +9.0 points over the best baseline, clearly demonstrating the benefit of integrating demonstrations with self-exploration in LLM reasoning. We also observe that SRFT achieves +4.8 points improvement over the SFT methods, indicating that the self-exploration component can effectively refine the policy distribution learned from demonstrations. Compared to the SFT+RL methods, SRFT achieves +3.4 points improvement, demonstrating that the single-stage design and entropy-aware weighting mechanism can effectively balance the benefits of demonstrations and self-exploration. Out-of-Distribution Generalization. Regarding out-of-distribution performance, the results in Table 2 show that SRFT also achieves an average score of 62.5 and outperforms the best baseline by +4.7 points. These results highlight SRFTs effectiveness in combining demonstrations with self-exploration to improve generalization ability. Training Dynamics. Figure 6 shows the training dynamics of SRFT, including training rewards, response lengths, and training entropy. As shown in Figure 6(a), SRFT achieves faster performance improvement compared to RL, with both SRFT and RL exhibiting an increasing trend in training 9 Supervised and Reinforcement Fine-Tuning Table 2: Overall performance on five competition-level mathematical reasoning benchmarks and three out-of-distribution benchmarks based on Qwen2.5-Math-7B. Bold and underlined indicate the best and second-best performance, respectively. Model Qwen2.5-Math Qwen2.5-Math-Instruct Supervised Fine-Tuning SFT SFTKL Reinforcement Learning RLGRPO (Shao et al., 2024) SimpleRL-Zero (Zeng et al., 2025) OpenReasoner-Zero (Hu et al., 2025) PRIME-Zero (Cui et al., 2025) Oat-Zero (Zeng et al., 2025) SFT and RL SFT RL LUFFY (Yan et al., 2025) TAPO (Wu et al., 2025) ReLIFT (Ma et al., 2025) SRFT (ours) In-Distribution Performance Out-of-Distribution Performance AIME24 AMC MATH500 Minerva Olympiad Avg. ARC-C GPQA-D MMLU-Pro Avg. 11.4 12.9 31.1 13. 24.7 27.0 16.5 17.0 33.4 32. 29.4 33.3 28.2 35.3 32.6 48. 62.8 45.2 61.6 54.9 52.1 54. 61.2 67.1 65.6 77.5 64.8 74. 48.8 81.2 85.2 70.2 79.2 76. 82.4 81.4 78.0 84.2 87.6 83. 85.0 89.8 8.7 33.1 39.1 26. 33.7 25.0 33.1 39.0 34.6 34. 37.5 38.2 37.1 15.8 39.8 53. 36.3 47.1 34.7 47.1 40.3 43. 54.6 57.2 46.2 54.9 23.5 43. 54.3 38.2 49.3 43.5 46.2 46. 50.1 54.5 55.5 55.7 54.0 18. 70.3 76.2 33.3 75.6 30.2 66. 73.3 70.1 76.4 80.5 81.6 74. 11.1 24.7 25.8 22.2 31.3 23. 29.8 18.2 23.7 37.9 39.9 37. 40.9 16.9 34.1 45.7 30.4 42. 34.5 58.7 32.7 41.7 49.6 53. 49.6 51.9 15.4 43.0 49.2 28. 49.7 29.3 51.6 41.4 45.2 54. 57.8 56.4 55.9 39.7 This methods performance is taken from the corresponding paper. 85.3 59. 58.3 46.4 62.5 55.9 (a) Training Rewards (b) Response Lengths (c) Training Entropy Figure 6: Training dynamics during RL and SRFT training, including training rewards, response lengths, and training entropy. rewards. In terms of response length, as shown in Figure 6(b), when faced with challenging training data, RL exhibits tendency toward generating more concise responses, whereas SRFT shows progressive lengthening of responses, indicating the development of more thorough and detailed reasoning processes. From an entropy perspective in Figure 6(c), compared to the rapid entropy decline exhibited by RL, our method SRFT maintains more stable entropy, indicating that the policy can continue exploring during training, which also demonstrates the effectiveness of the entropy-aware weighting mechanism. Ablation Study. We conduct an ablation study to assess the effectiveness of each component. As shown in Table 3, we evaluate the impact of the two key entropy-aware weighting mechanisms: wSFT for demonstrations learning and wRL for positive self-exploration samples. We ablate these mechanisms by setting their values to fixed constant of 1.0 to evaluate the contribution of these two weighting components. Removing the SFT weighting mechanism (w/o wSFT) results in performance drop of -4.0 points, while removing the RL weighting (w/o wRL) leads to -2.9 point decrease, demonstrating that both components contribute significantly to overall performance. The ablation results validate our theoretical analysis, confirming that entropy-aware weighting mechanisms enable SRFT to dynamically balance supervised learning and reinforcement learning components, leading to more stable training and superior performance compared to fixed weighting schemes. 10 Supervised and Reinforcement Fine-Tuning Table 3: Ablation results on SRFT, including the impact of wSFT and wRL. Model AIME24 AMC MATH-500 Minerva Olympiad Avg. Qwen2.5-Math SRFT w/o wSFT SRFT w/o wRL SRFT 11.4 30. 32.6 35.3 32.6 65.8 67.2 72. 48.8 87.0 87.5 89.8 8.7 36. 37.4 39.7 15.8 55.8 56.5 58. 23.5 55.1 56.2 59."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Reinforcement Learning for LLM Reasoning. The pursuit of complex reasoning capabilities in LLMs has witnessed remarkable progress, with RL emerging as pivotal methodology for enhancing reasoning abilities beyond the limitations of SFT alone. Recent approaches such as GRPO (Shao et al., 2024; Guo et al., 2025), DAPO (Yu et al., 2025), DR.GRPO (Liu et al., 2025c), and VAPO (Yue et al., 2025a) have demonstrated substantial improvements in mathematical reasoning and complex problemsolving tasks. However, the precise mechanisms through which RL enhances reasoning capabilities remain incompletely understood. Several empirical investigations suggest that reinforcement learning primarily serves to elicit, refine, or improve the sampling of pre-existing reasoning abilities rather than instilling entirely novel fundamental reasoning skills from scratch. For instance, Yue et al. (2025b) question whether current reinforcement learning with verifiable rewards (RLVR) genuinely expands the reasoning boundary (pass@k) or primarily improves the sampling efficiency of already known solutions (pass@1). Similarly, Wang et al. (2025a) highlight that base models already possess substantial reasoning capabilities that reinforcement learning can effectively unlock or redirect. Nevertheless, ProRL (Liu et al., 2025d) demonstrates that RL-trained models can achieve improved success rates on tasks where base models completely fail, suggesting that sustained and stable reinforcement learning training can indeed expand the reasoning capability boundaries of LLMs. In this work, we design single-stage method that combines SFT and RL, maintaining stable entropy during training and achieving continuous performance improvement. Integrating Supervised Fine-Tuning and Reinforcement Learning. The synergistic interaction between SFT and RL represents critical area of investigation in modern LLM development. SFT on high-quality reasoning chains can establish robust initial policy foundation, which RL can subsequently optimize. Cai et al. (2025) explore the necessary extent of exploration following SFT, finding that RL continues to provide substantial benefits by enabling models to deviate from potentially suboptimal SFT trajectories. Recent research suggests that SFT may equip models with structured reasoning templates that RL subsequently validates and improves (Chen et al., 2025a). Nevertheless, determining the optimal strategy for combining these complementary paradigms remains an active area of debate. To enhance sample efficiency and provide structured guidance for RL exploration, researchers have investigated various approaches for integrating external supervision into the reinforcement learning framework. UFT (Liu et al., 2025a) proposes novel paradigm that merges SFT and RL into single process, using informative supervision signals like hints from partial solutions to guide exploration and accelerate convergence. Addressing the limitations of on-policy learning, LUFFY (Yan et al., 2025) augments RLVR by incorporating off-policy reasoning traces from stronger models, dynamically balancing imitation with on-policy exploration to improve capabilities. ReLIFT (Ma et al., 2025) addresses the limitations of pure RL by interleaving reinforcement learning with supervised fine-tuning on high-quality demonstrations collected during training, enabling models to acquire new knowledge beyond their original capabilities. TAPO (Wu et al., 2025) enhances RL by incorporating external high-level guidance in the form of \"thought patterns\" abstracted from prior samples, adaptively integrating these to balance model-internal exploration with external strategy exploitation. SASR (Chen et al., 2025b) offers hybrid framework that theoretically unifies SFT and RL, using SFT for initial warm-up and then adaptively blending it with an online RL method based on training dynamics to maintain core reasoning while exploring diverse paths, using high-quality SFT demonstrations as key external data source. Furthermore, the single-stage integration of SFT and RL helps mitigate the catastrophic forgetting problem that previous methods encountered when transitioning from SFT to RL (Chen et al., 2025b; Liu et al., 2025b). These approaches collectively underscore an emerging trend toward more sophisticated integrations of supervised signals within reinforcement learning frameworks to improve reasoning alignment and overall performance. 11 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we investigate the integration of SFT and RL for LLM reasoning. Through comprehensive analysis, we reveal that SFT performs coarse-grained global adjustments while RL conducts fine-grained selective optimizations, with entropy serving as crucial training indicator. Building on these observations, we propose SRFT, single-stage approach that unifies both paradigms through entropy-aware weighting mechanisms. Extensive experiments demonstrate SRFTs effectiveness, achieving 59.1% average accuracy and outperforming zero-RL baselines by 9.0% on reasoning tasks and 10.9% on out-of-distribution benchmarks. Limitations. While our work demonstrates the effectiveness of entropy-aware SFT-RL singlestage integration, our current utilization of entropy dynamics remains relatively simple with basic exponential weighting functions. The rich temporal patterns of entropy during training suggest opportunities for more sophisticated entropy-based control mechanisms. Future work could explore adaptive entropy scheduling or multi-timescale entropy analysis to better capture the interplay between SFT and RL signals, potentially leading to more principled hybrid training algorithms. Additionally, our approach assumes access to high-quality demonstrations, and future research could investigate the potential for training with imperfect demonstrations to enhance the methods applicability. 12 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "REFERENCES",
            "content": "OpenAI. Introducing OpenAI o3 and o4-mini, 2025. URL https://openai.com/index/introdu cing-o3-and-o4-mini/. 1 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 8, 11, 22 Anthropic. Introducing Claude 4, 2025. URL https://www.anthropic.com/news/claude-4. 1 Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 1 Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. SFT or RL? an early investigation into training R1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025a. 1, Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, and Xiangyu Zhao. Navigate the unknown: Enhancing LLM reasoning with intrinsic motivation guided exploration. arXiv preprint arXiv:2505.17621, 2025. 1 Shihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving RL exploration for LLM reasoning through retrospective replay. arXiv preprint arXiv:2504.14363, 2025. 1 Thomas Schmied, Jörg Bornschein, Jordi Grau-Moya, Markus Wulfmeier, and Razvan Pascanu. LLMs are greedy agents: Effects of RL fine-tuning on decision-making abilities. arXiv preprint arXiv:2504.16078, 2025. 1 Hongyi James Cai, Junlin Wang, Xiaoyin Chen, and Bhuwan Dhingra. How much backtracking is enough? exploring the interplay of SFT and RL in enhancing LLM reasoning. arXiv preprint arXiv:2505.24273, 2025. 1, 6, 11 Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. 1, 7, 8, 9, 10, 11, 18 Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, and Jianhua Tao. Thought-augmented policy optimization: Bridging external guidance and internal capabilities. arXiv preprint arXiv:2505.15692, 2025. 1, 9, 10, 11, Mingyang Liu, Gabriele Farina, and Asuman Ozdaglar. UFT: Unifying supervised and reinforcement fine-tuning. arXiv preprint arXiv:2505.16984, 2025a. 1, 11 Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, and Xiao Wang. Step-wise adaptive integration of supervised fine-tuning and reinforcement learning for task-specific LLMs. arXiv preprint arXiv:2505.13026, 2025b. 1, 11 Yihao Liu, Shuocheng Li, Lang Cao, Yuhang Xie, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, and Dongmei Zhang. SuperRL: Reinforcement learning with supervision to boost LLM reasoning. arXiv preprint arXiv:2506.01096, 2025b. 1, 11 An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. 2, 9, 22 Martin Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. 3 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3, 7, 9, 10, 11, 18 13 Supervised and Reinforcement Fine-Tuning John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3, Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. 5, 22 Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, et al. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025. 8, 9, 10, 11, 18 Hugging Face. Open R1: fully open reproduction of DeepSeek-R1, January 2025. 8, 18 Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath, 2024. 8 Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. 9 Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. 9 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 9 Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 9 David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. 9 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: more robust and challenging multitask language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Zichen Liu, Changyu Chen, Xinyi Wan, Chao Du, Wee Sun Lee, and Min Lin. OAT: researchfriendly framework for LLM online alignment. https://github.com/sail-sg/oat, 2024. 9, 18 Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRLZoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. 9, 10 Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-Reasoner-Zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. 9, 10 Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. 9, 10 14 Supervised and Reinforcement Fine-Tuning Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 11 Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding R1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025c. 11 Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. VAPO: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025a. 11 Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? arXiv preprint arXiv:2504.13837, 2025b. 11 Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025a. 11 Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. ProRL: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025d. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. arXiv preprint arXiv:2506.01939, 2025b. 17 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 18 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 18 15 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "A More Experimental Results",
            "content": "A.1 Token Probability Visualization of SRFT . . . . . . . . . . . . . . . . . . . . . . . A.2 Entropy-aware Gradient Clipping . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Dataset Details",
            "content": "C.1 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D SFT Gradient Derivation",
            "content": "D.1 Problem Setup . . . . D.2 Gradient Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Visualization of Learning Dynamics E.1 Theoretical Definition . E.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 17 18 19 19 21 21 21 22 22 16 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "A MORE EXPERIMENTAL RESULTS",
            "content": "A.1 TOKEN PROBABILITY VISUALIZATION OF SRFT We visualize the token probability distribution of SRFT after training, as shown in Figure A1. We observe that the token probability changes are moderate, achieving balanced point between SFT and RL that enhances the models reasoning capabilities while preserving its base abilities. Figure A1: Token probability distribution visualization for SRFT. A.2 ENTROPY-AWARE GRADIENT CLIPPING Our investigation into the entropy characters of tokens modified by fine-tuning reveals that reinforcement learning predominantly targets tokens with high entropy distributions, finding that aligns with recent work on selective optimization in language models (Wang et al., 2025b). To empirically validate this observation, we design controlled experiments implementing gradient truncation for highprobability tokens during RL training. As demonstrated in Figure A2, the models performance remains comparable to the original RL algorithm even when gradients are truncated for low-entropy tokens, providing strong empirical support for our hypothesis. This evidence confirms that RL operates with remarkable selectivity, precisely adjusting tokens with uncertain distributions while leaving confident predictions largely unchanged. In contrast, SFT applies broad modifications across the entire token space, fundamentally altering the models distributional characters in less discriminative manner. Figure A2: Performance of RL with gradient clipping for low-entropy tokens. 17 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "Training. We follow the SFT configuration of OpenR1-Qwen-7B (Face, 2025), performing full fine-tuning on DeepSeek-R1 generated reasoning traces and prompts. The training hyperparameters include batch size of 128, learning rate of 5 106, linear learning rate schedule with 10% warmup, and training for 3 epochs. For SFT with KL regularization, we use identical settings while adding KL divergence regularization between the current policy and the base model (Qwen2.5-Math-7B) with weight λ = 0.2. The SFTKL loss is: LSFTKL(θ) = E(x,y)D[ log πθ(yx)] + λLKL(θ, θbase), where LKL(θ, θbase) is the KL divergence between the current policy and the base model. For RL, we train for 500 steps with 8 rollouts per prompt. The learning rate is fixed at 1 106. For our method SRFT, we use the same training settings as RL. Since the maximum sequence length for Qwen2.5-Math-7B is 4096, which is insufficient for our tasks, we increase the RoPE theta from 10,000 to 40,000 and expand the window size to 16,384. For all experiments, we use verl3 (Sheng et al., 2024) as the implementation framework. All experiments are conducted on 64 A100 GPUs. (A1) Evaluation. All evaluations are conducted using VLLM (Kwon et al., 2023) with temperature set to 0.6 and maximum generation length of 8192 tokens. For datasets with limited sample sizes (AIME24 and AMC), we report the avg@32 metric; for the remaining three datasets, we adopt pass@1 as the evaluation criterion. We verify the correctness of generated solutions using Math-Verify and OAT-Grader (Liu et al., 2024). For baseline comparisons, we independently validate the results of base model, SFT-related baselines, GRPO (Shao et al., 2024), LUFFY (Yan et al., 2025), and ReLIFT (Ma et al., 2025), while results for TAPO (Wu et al., 2025) and other zero-shot reinforcement learning models are taken from the TAPO (because we cannot find the open-source code or model) and LUFFY papers. Reward Design. To evaluate the impact of our method, we adopt simple reward function as below. All training experiments employ the same reward function. R(x, y) = (cid:26)1, if is correct 0, otherwise . (A2) Chat Template. Following Yan et al. (2025); Ma et al. (2025), for all training paradigms (SFT, RL, SRFT), we employ unified system prompt that encourages systematic reasoning, as shown in Figure A3. We also experimented with alternative templates, as shown in Figure A4. Figure A3: Chat template for all training paradigms (SFT, RL, SRFT). Template Ablation. To minimize template influence, we evaluated the base Qwen-7B-Math model with different templates. Results are shown in Table A1, which indicates that our template design effectively guides the models reasoning process while maintaining consistency across different mathematical domains. 3https://github.com/volcengine/verl 18 Supervised and Reinforcement Fine-Tuning Figure A4: Chat template for Qwen-2.5-Math. Table A1: Template ablation results on mathematical reasoning benchmarks"
        },
        {
            "title": "Template",
            "content": "AIME24 AMC MATH500 Minerva Olympiad Average No Template Qwen2.5-Math SRFT 0.302 0.144 0.296 0.132 0.088 0.165 0.596 0.446 0.648 0.424 0.303 0. 0.134 0.111 0.141 0.318 0.218 0."
        },
        {
            "title": "C DATASET DETAILS",
            "content": "C.1 TRAINING DATASET OpenR1-Math-220k is comprehensive dataset designed for mathematical reasoning, comprising 220,000 math problems. Each problem is associated with two to four reasoning traces generated by the DeepSeek-R1 model. The traces have been verified using tools like Math Verify and Llama3.3-70B-Instruct, ensuring at least one correct reasoning path per problem. This dataset challenges models to understand and replicate complex reasoning processes across various mathematical domains including algebra, geometry, number theory, and calculus. C.2 EVALUATION BENCHMARKS To evaluate the models above, we use eight benchmarks categorized into mathematical reasoning benchmarks and out-of-distribution (OOD) benchmarks as described below. C.2.1 MATHEMATICAL REASONING BENCHMARKS AIME24 is benchmark dataset based on problems from the 2024 American Invitational Mathematics Examination, prestigious high school mathematics competition in the United States. The AIME24 benchmark tests models ability to solve challenging mathematics problems by generating step-bystep solutions and providing the correct answers. This dataset contains problems from the American Invitational Mathematics Examination (AIME) 2024, organized in JSONL format where each line represents complete problem. Concepts typically covered include topics in elementary algebra, geometry, trigonometry, as well as number theory, probability, and combinatorics. The examination consists of 15 problems with integer answers between 0 and 999, requiring advanced mathematical reasoning and problem-solving skills. AMC is validation dataset containing problems from the American Mathematics Competitions, specifically AMC12 from 2022 and 2023. All 83 problems come from AMC12 2022, AMC12 2023, and have been extracted from the AOPS wiki page. The AMC 10 is 25-question, 75-minute multiplechoice competition designed for students in grades 10 and below. The content covers mathematical topics such as elementary algebra, basic geometry, area and volume formulas, elementary number theory, and elementary probability. This dataset serves as an internal validation set and focuses on competition-level mathematical problems comparable in difficulty to AMC12 and AIME exams. MATH500 is carefully curated subset of mathematical problems designed for robust evaluation. MATH500 is subset of 500 randomly sampled questions from Hendrycks 2021 MATH dataset, created by OpenAI in late 2024 as consequence of their appropriation of 90% of the original 5000 MATH questions for training data for reinforcement learning on o1-series models. The dataset 19 Supervised and Reinforcement Fine-Tuning Table A2: Benchmarks used in this study. indicates the split is not officially provided. Dataset #Train #Test Task Type Domain License Source OPENR1-MATH-220K 220,000 Math reasoning Mathematics Apache 2. [Link] Training Dataset AIME24 AMC MATH500 MINERVA OLYMPIAD ARC-C GPQA-D MMLU-PRO Mathematical Reasoning Benchmarks 83 500 272 674 Math competition Math competition Mathematics Mathematics MIT Apache 2.0 Mathematical reasoning Mathematics - Mathematical reasoning Mathematics Math competition Mathematics Apache 2. Apache 2.0 Out-of-Distribution (OOD) Benchmarks 1,172 198 Science reasoning General science CC-BY-SA-4.0 Scientific reasoning Bio, Phys, Chem CC-BY-4.0 12,032 Multi-task understanding Multidisciplinary MIT [Link] [Link] [Link] [Link] [Link] [Link] [Link] [Link] maintains the diversity and complexity of the original MATH benchmark while providing clean evaluation set that avoids potential data contamination issues. Minerva is mathematical reasoning benchmark that encompasses wide range of mathematical domains and difficulty levels. The dataset is designed to evaluate models capabilities in advanced mathematical problem-solving, including topics from high school to undergraduate level mathematics. It includes problems requiring multi-step reasoning, symbolic manipulation, and deep mathematical understanding across various mathematical fields. Olympiad refers to mathematical olympiad-level problems that represent some of the most challenging mathematical reasoning tasks. Unlike existing Olympiad-related benchmarks, datasets in this category focus exclusively on mathematics and comprise vast collections of competition-level problems. These problems are meticulously categorized into 33+ sub-domains and span across 10+ distinct difficulty levels. These problems require exceptional mathematical insight, creativity, and advanced problem-solving techniques typically seen in international mathematical competitions. We specifically utilize the OE_TO_MATHS_EN_COMP subset for our evaluation. C.2.2 OUT-OF-DISTRIBUTION (OOD) BENCHMARKS ARC-C (AI2 Reasoning Challenge-Challenge) is dataset of grade-school level science questions that require commonsense reasoning and knowledge application. The dataset consists of multiplechoice questions that are designed to be easy for humans but challenging for AI systems, testing the models ability to apply scientific knowledge and reasoning in everyday contexts beyond pure mathematical domains. GPQA-D (Graduate-Level Google-Proof Q&A-Diamond) is challenging benchmark designed to evaluate advanced reasoning capabilities in scientific domains. GPQA consists of 448 multiple-choice questions designed to evaluate the capabilities of LLMs and scalable oversight mechanisms. This dataset provides \"Google-proof\" questions in biology, physics, and chemistry, designed to test deep domain expertise and reasoning under challenging conditions. The diamond subset contains 198 hard problems. The questions require graduate-level knowledge and are specifically designed to be difficult to answer even with internet search. MMLU-Pro (Massive Multitask Language Understanding-Professional) is an enhanced version of the original MMLU benchmark designed to be more challenging and robust. The MMLU-Pro dataset is an enhanced version of the Massive Multitask Language Understanding benchmark. Its designed to be more robust and challenging, aiming to rigorously evaluate language understanding capabilities. MMLU is comprehensive benchmark that covers 57 subjects across fields like mathematics, history, law, and medicine. It assesses not only factual knowledge but also the models capacity to apply this knowledge in context-specific scenarios. MMLU-Pro increases the difficulty and reduces potential shortcuts while maintaining broad coverage across academic disciplines. Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "D SFT GRADIENT DERIVATION",
            "content": "In this section, we provide the detailed mathematical derivation for the gradient of the Supervised Fine-Tuning (SFT) loss function, which was referenced in Sec. 3.1.1. D.1 PROBLEM SETUP Given dataset = {(xi, yi)}N corresponding target sequence, the SFT objective function is (the same as Eq. (1)): i=1 where xi is an input prompt and yi = (yi,1, yi,2, . . . , yi,Ti) is the Since the sequence probability is factorized as: LSFT(θ) = E(x,y)D[ log πθ(yx)]. The SFT loss becomes: πθ(yx) = (cid:89) t=1 (cid:34) LSFT(θ) = E(x,y)D πθ(ytx, y<t). (cid:35) log πθ(ytx, y<t) . (cid:88) t=1 (A3) (A4) (A5) D.2 GRADIENT DERIVATION To derive the gradient θLSFT(θ), we need to compute: θLSFT(θ) = E(x,y)Ddata (cid:34) (cid:88) t=1 θ log πθ(ytx, y<t) . (A6) (cid:35) Now, lets consider the key insight: at each time step t, the model produces probability distribution over the entire vocabulary V. The gradient of the log probability for the target token yt can be expressed in terms of the models prediction probabilities for all vocabulary tokens. For any token at position t, we have: θ log πθ(vx, y<t) = 1 πθ(vx, y<t) θπθ(vx, y<t) Since (cid:80) vV πθ(vx, y<t) = 1, we have the constraint: (cid:88) vV θπθ(vx, y<t) = 0. (A7) (A8) Using the chain rule and the fact that the softmax normalization affects all vocabulary tokens, the gradient can be written as: θ log πθ(ytx, y<t) = (cid:88) vV (1v=yt πθ(vx, y<t)) θ log πθ(vx, y<t), (A9) where 1v=yt is the indicator function that equals 1 when = yt and 0 otherwise. Substituting this back into the SFT gradient, we obtain: θLSFT = E(x,y)D (cid:88) (cid:88) (πθ(vx, y<t) 1v=yt) θ log πθ(vx, y<t) . (A10) t=1 vV This formulation reveals the fundamental mechanism of SFT: at each time step, the gradient encourages the model to increase the probability of the target token (1v=yt = 1) while decreasing the probabilities of all other tokens in the vocabulary (1v=yt = 0). The magnitude of the decrease for each non-target token is proportional to its current probability πθ(vx, y<t). This analysis confirms our empirical observations that SFT produces broad, coarse-grained changes to the models probability distributions across the entire vocabulary, systematically sharpening the distribution toward the target tokens in the training data. 21 Supervised and Reinforcement Fine-Tuning"
        },
        {
            "title": "E DETAILS OF VISUALIZATION OF LEARNING DYNAMICS",
            "content": "To characterize model dynamics during training, in Sec. 3.1.2, we formulate language model as mapping from input prompts to output probability distributions over the vocabulary. Two models are considered similar or close if they assign identical probabilities to every output token across all input prompts. Under this equivalence, each model can be theoretically represented as point in an infinite-dimensional probability space, where each dimension corresponds to the models assigned probability for specific token at specific position within all possible outputs. E.1 THEORETICAL DEFINITION Model Space Formalization. We define the model space as the set of all possible conditional probability distributions over token sequences. Each model can thus be represented as vector in an infinite-dimensional space, with each dimension corresponding to the conditional probability pM (vi x, y<t) for vocabulary token vi at position t. Measure Model Dynamics via Reference Models. To make this space tractable for analysis and visualization, we measure the model dynamics by the distance between each model and set of reference models = {R1, R2, . . . , Rk} (reference frames): dR(M ) = (dR1 (M ), dR2(M ), . . . , dRk (M )) , (A11) where dRi(M ) quantifies the distance between model and reference model Ri in terms of assigned sequence probabilities. E.2 EXPERIMENTAL SETUP To instantiate the above definition, we construct dataset comprising 1,024 response sequences generated by each of the reference models under identical prompts. These prompts are drawn from diverse mixture of mathematical reasoning benchmarks (including AIME24, Minerva, Olympiad, AMC, and MATH-500) to ensure broad coverage across problem domains and difficulty levels. For each response, we compute the distance between the probabilities assigned by model to the reference tokens, and then aggregate overall responses to obtain the final distance dRi (M ) for each Ri. These distances collectively define the models position in the projected subspace. We select three reference models to construct the projection basis: (1) DeepSeek-R1 (Guo et al., 2025), representing state-of-the-art reasoning performance; (2) QwQ-32B (Team, 2025), serving as high-performing but structurally distinct baseline; and (3) Qwen-2.5-Math-7B (Yang et al., 2024), which acts as the base model prior to fine-tuning. Together, they span spectrum from foundational to advanced capabilities, forming semantically meaningful coordinate system. For every model checkpoint throughout training, we evaluate its distance based on its probability assignments over the reference responses. This yields trajectory in the model space that traces how the model evolves over time. By comparing these trajectories across training paradigms (e.g., SFT and RL), we uncover distinct optimization dynamics and convergence behaviors. This three-dimensional distance framework provides both theoretical grounding and practical interpretability for analyzing training dynamics. It enables direct comparison of different model variants and reveals how specific training strategies influence progression through the reasoning capability landscape."
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "Meituan",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "Shanghai Jiao Tong University"
    ]
}