{
    "paper_title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "authors": [
        "Xinyu Zhang",
        "Yuxuan Dong",
        "Yanrui Wu",
        "Jiaxing Huang",
        "Chengyou Jia",
        "Basura Fernando",
        "Mike Zheng Shou",
        "Lingling Zhang",
        "Jun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark comprising knowledge-based (25%) and reasoning-based (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard requiring 15.6, reflecting the complexity of physics-based reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive step-level evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answer-level evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identified four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as a novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https:/dxzxy12138.github.io/PhysReason."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 5 0 2 1 . 2 0 5 2 : r PhysReason: Comprehensive Benchmark towards Physics-Based Reasoning Xinyu Zhang1, Yuxuan Dong1, Yanrui Wu1, Jiaxing Huang1, Chengyou Jia 1, Basura Fernando 3, Mike Zheng Shou 2, Lingling Zhang1, Jun Liu1 1Xian Jiaotong University 2Show Lab, National University of Singapore 3Institute of High-Performance Computing, A*STAR zhang1393869716@stu.xjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physicsbased reasoning, complex task requiring physics theorems and constraints. We present PhysReason, 1,200-problem benchmark comprising knowledge-based (25%) and reasoningbased (75%) problems, where the latter are divided into three difficulty levels (easy, medium, hard). Notably, problems require an average of 8.1 solution steps, with hard problems requiring 15.6, reflecting the complexity of physicsbased reasoning. We propose the Physics Solution Auto Scoring Framework, incorporating efficient answer-level and comprehensive steplevel evaluations. Top-performing models like Deepseek-R1, Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on answerlevel evaluation, with performance dropping from knowledge questions (75.11%) to hard problems (31.95%). Through step-level evaluation, we identify four key bottlenecks: Physics Theorem Application, Physics Process Understanding, Calculation, and Physics Condition Analysis. These findings position PhysReason as novel and comprehensive benchmark for evaluating physics-based reasoning capabilities in large language models. Our code and data will be published at https: //dxzxy12138.github.io/PhysReason/."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable performance across various domains, such as math (Lightman et al., 2024; Cobbe et al., 2021) and logical reasoning (Hendrycks et al.; Xu et al., 2025). However, current evaluations often overlook physics-based reasoning, limiting their applications in scenarios such as robotics (Chow et al., 2025) and autonomous driving (Huang et al., 2023). This is because physicsbased reasoning, integrating multiple theorems and 1 physics constraints, is more closely aligned with practical applications than math and logical reasoning. Consequently, developing comprehensive benchmark for evaluating LLMs physics-based reasoning capabilities is crucial for discovering current limitations and guiding future improvements. There are several pioneering physics benchmarks (K-12 level like ScienceQA (Lu et al., 2022), college-level SciBench (Wang et al.), and expertlevel GPQA (Rein et al., 2024)) encompassing progressively advanced knowledge domains. However, they exhibit two critical limitations: oversimplified reasoning processes and neglecting step-level evaluation. These problems typically involve only 3-4 physics formulas, focusing solely on final answers to measure model performance. Therefore, benchmark featuring in-depth reasoning processes and step-level evaluation is urgently needed to measure LLMs physics-based reasoning capabilities. To address these limitations, we present PhysReason, comprehensive benchmark comprising 1,200 problems designed to evaluate models physics-based reasoning capabilities. As illustrated in Figure 1, PhysReason features physics problems that require multi-step reasoning and precise application of physics theorems. The benchmark introduces several key characteristics: 1. Stratified difficulty: There are knowledgebased (25%) and reasoning-based (75%) problems, with reasoning problems categorized into easy, medium, and hard (25% each). 2. Complex reasoning: Solutions average 8.1 steps per problem, with hard problems reaching 15.6 steps, exceeding current physics benchmarks which typically only contain 3-4 steps. 3. Multi-modal design: 81% of problems include diagrams, evaluating models capabilities in comprehending visual and textual information. To evaluate performance on PhysReason comprehensively, we propose the Physics Solution Auto Scoring Framework (PSAS) based on current Figure 1: An illustration of example from our PhysReason benchmark. Due to space constraints, only key components are shown. Please refer to Appendix for complete annotations. LLMs capabilities in information extraction and formula comparison. This framework encompasses two answer-level and step-level evaluation methods, PSAS-A and PSAS-S. PSAS-A enables efficient evaluation through answer comparison, while PSAS-S facilitates comprehensive analysis through step-by-step reasoning verification. Experimental results demonstrate that PSAS significantly outperforms direct LLM-based evaluation approaches, achieving an evaluation accuracy exceeding 98%. We evaluate seven non-O-like models and eight O-like models on the PhysReason benchmark. Results show that while Deepseek-R1 (Guo et al., 2025), Gemini-2.0-Flash-Thinking-0121 (Deepmind), and o3-mini-high (OpenAI, 2025) demonstrate superior performance, their average scores remain below 60%. Moreover, models excel in basic physics concepts but consistently show performance degradation as problem difficulty and required solution steps increase (from 75.11% to 31.95%). This degradation stems from the models inability to maintain accuracy across consecutive solution steps, so maintaining the reliability of the reasoning process is crucial. Through step-level evaluation, we identify four critical bottlenecks limiting model performance: Physics Theorem Application, Physics Process Understanding, Calculation Process, and Physics Condition Analysis."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Large Language Model Evaluation LLMs have demonstrated remarkable performance across various domains, such as math reasoning (Jiang et al., 2024; Li et al., 2024; Imani et al., 2023), logical reasoning (Sun et al., 2024a; Xu et al., 2024), and text generation (Zhao et al., 2024; Liang et al., 2024). However, they struggle with physics world interactions, limiting their adoption in areas like autonomous driving and robotics (Gao et al., 2024b). Unlike mathematical and logical reasoning, physics-based reasoning requires the integration of multiple principles and physics-world constraints (Kline, 1981). Therefore, mastering physics-based reasoning is fundamental to unlocking LLMs potential in practical applications (Lai et al., 2024). Current evaluations primarily focus on mathematical and logical reasoning, revealing crucial gap in evaluating LLM capabilities based on physics-based reasoning. 2.2 Physics Benchmarks Existing physics benchmarks span three knowledge complexity levels: K-12 (ScienceQA (Lu et al., 2022), E-EVAL (Hou et al., 2024)), college-level (MMLU (Hendrycks et al.), AGIEval (Zhong et al., 2024), JEEBench (Arora et al.), TheoremQA (Chen et al., 2023), EMMA (Hao et al., 2025), SciEval (Sun et al., 2024b), C-Eval-STEM (Huang et al., 2024), SciBench (Wang et al.)), and expert-level (OlympiadBench(He et al., 2024), GPQA (Rein et al., 2024)). While these benchmarks showcase LLMs knowledge breadth, they simplify reasoning to 3-4 steps and emphasize only final answers. PhysReason addresses these gaps through complex reasoning process and step-level evaluation."
        },
        {
            "title": "3 Benchmark",
            "content": "3.1 Collection We describe our comprehensive data collection process that spans five key stages: Acquisition, Standardization, Translation, Search Prevention, and Difficulty Classification. 2 Table 1: Comparative analysis of our PhysReason with other physics-based reasoning benchmarks. For Knowledge, COMP: Competition, COL: College, CEE: College Entrance Examination, K1-K12: Elementary and High School, PH.D: Doctor of Philosophy; For question type, OE: Open-ended, MC: Multiple-choice, Avg. T: Average Tokens; For solution type, Avg. S: Average Steps. Benchmark Multi-modal Size Knowledge Question Solution JEEBench MMLU-Pro GPQA SciEval SciBench MMMU ScienceQA OlympiadBench EMMA Ours-Knowledge Ours-Easy Ours-Medium Ours-Hard Ours-Full CEE 123 COL 1299 PH.D. 227 - 1657 COL 295 COL 443 K1-K12 617 COMP 2334 - 156 CEE+COMP 300 CEE+COMP 300 CEE+COMP 300 300 CEE+COMP 1200 CEE+COMP Type OE,MC MC OE OE,MC OE OE,MC MC OE MC OE OE OE OE OE Avg. Step-by-step Avg. Avg. 169.7 52.1 111.4 154.5 80.5 53.8 13.3 222.0 109.5 163.7 171.2 229.2 340.9 226.3 - - 197.2 - 315.9 - 63.0 199.8 - 196.5 241.5 391.3 936.1 441.3 - - 3.6 - 2.8 - 2.4 3.7 - 3.3 5.0 8.4 15.6 8.1 - - - - - Acquisition: We collect public physics problems from global college entrance examinations, their associated practice tests, and international physics competitions. Our sources include Chinese, Indian, and Russian exams, as well as IPhO, APhO, EPhO, and so on. This comprehensive benchmark derives from 1,254 PDFs containing over 20,000 unique problems, ensuring diverse difficulty levels. Standardization: Using MinerU (Wang et al., 2024a) framework, we parse the content of these PDFs into structured problem information. Subsequently, all problems undergo rigorous deduplication, filtering, and formatting to ensure complete problem statements, precise physics terms, accurate expressions, and consistent presentation style. Translation: We implement two-phase translation process utilizing translators for initial conversion. Engineering Ph.D. candidates with physics expertise then verify the translations for accuracy and professionalism, especially physics terms. Search Prevention: Following (Rein et al., 2024), we exclude problems whose solutions and answers can be found through five-minute Google search to minimize potential data leakage. Difficulty Classification: Based on the time students typically need to solve problems and the theorems applied, questions are categorized into knowledge-based and reasoning-based types, with the latter subdivided into three difficulty levels (easy, medium, and hard). This classification enables the comprehension evaluation of physics concepts and physics-based reasoning capabilities. 3.2 Annotation As shown in Figure 1, our annotation framework consists of 8 key elements: Diagram, Context, Sub-questions, Solution, Step Analysis, Answer, Theorem, and Difficulty. Context presents the physics scenario and conditions. Diagram visualizes the physics scenario with concise illustrations complementing the Context. Sub-questions give questions to assess the understanding and application of the concept. Solution provides stepby-step reasoning process, and Answer gives the answer to each sub-question. Step Analysis explains the physics theorem used in each step and the physics quantities obtained. Theorem lists the physics theorems applied in the question, and Difficulty indicates the difficulty classification. 3.3 Characteristics PhysReason consists of 1,200 carefully curated physics problems as shown in Table 1, with strategic composition of 25% knowledge-based and 75% reasoning-based questions across various difficulty levels, collectively covering 147 physics theorems. The problems span Classic Mechanics, Quantum Mechanics, Fluid Mechanics, Thermodynamics, Electromagnetics, Optics, and Relativity. As shown in Figure 2, three critical solution metrics (theorem, step, and token) correlate positively with problem difficulty levels, validating the rationality of our difficulty classification. Notably, the medium and hard problems demonstrate higher complexity compared to existing benchmarks. PhysReason distin3 Figure 2: Analysis of solution theorems, solution steps, and solution tokens across different problem categories, with comparisons from SciBench, GPQA, and OlympiadBench. guishes itself through three key characteristics: 1. Stratified difficulty: The benchmark maintains carefully balanced composition of knowledgebased (25%) and reasoning-based (75%) problems. The reasoning-based problems are methodically distributed across three difficulty levels (easy, medium, and hard - 25% each), enabling comprehensive capability evaluation. 2. Complex reasoning: Detailed step-by-step solution annotations accompany each problem. These annotated solutions demonstrate complex reasoning chains averaging 8.1 steps per problem, with hard problems requiring up to 15.6 steps, significantly surpassing the complexity of existing physics-based reasoning benchmarks. 3. Multi-modal design: The benchmark features high proportion (81%) of problems with diagrams, authentically replicating physics-based reasoning scenarios while effectively evaluating both textual and visual reasoning capabilities."
        },
        {
            "title": "4 Evaluation Framework",
            "content": "4.1 Why LLMs Can Evaluate? Unlike multiple-choice problems, PhysReason contains open-ended answers and steps with diverse expressions but consistent semantics. Given that LLMs have demonstrated exceptional capabilities in both precise content extraction and formula consistency evaluation (Contributors, 2023; Gao et al., 2024a), they serve as practical tools for automated physics solution evaluation. Therefore, we propose automated answer-level and step-level evaluations, achieving comprehensive evaluation and avoiding labor-intensive manual assessment. 4.2 How Answer-Level Evaluation Works? We develop Physics Solution Auto Scoring Framework-Answer Level (PSAS-A), which evaluates based on sub-question answers. Given models reasoning process for problem with sub-questions {q1, q2, . . . , qn}, we first extract the models answers ˆai for each qi from with an LLM. Then, we employ the LLM to verify if ˆai is semantically consistent with the standard answer ai of sub-question qi. The comparison function C(ˆai, ai) returns 1 if consistent and 0 otherwise. Considering that the sub-questions with different steps should not carry equal weights in scoring, we use the length of annotation solution si of subquestion qi, i.e., (si) as weighting scalar. The models reasoning process answer-level score for each problem is calculated as follows: Score(M ) = (cid:80) qi (si) C(ˆai, ai) (si) (cid:80) qi (1) 4.3 How Step-Level Evaluation Works The current mainstream evaluation approach (He et al., 2024) with LLMs relies on answers, failing to reveal how and where models deviate from correct reasoning paths. To address this, we propose the Physics Solution Auto-Scoring Framework-Step Level (PSAS-S), which enables detailed assessment and analysis of each reasoning step. The framework is divided into four phases: Data Extraction, Scoring, First Error Step Detection, Error Analysis, as detailed in Algorithm 1. Data Extraction phase leverages LLMs using Target components from Step Analysis annotations (Figure 1) as prompts to locate and extract relevant content from model outputs for each annotated solution step si. This phase effectively handles redundant thinking processes in LLMs reasoning process while maintaining semantic equivalence. It obtains the mapping relationship between extracted relevant steps and annotated solution steps S. Scoring phase evaluates each step si through two complementary components of theorem assessment ScoreFormula( ˆfi, fi) and result verification ScoreValue(ˆvi, vi), each with weight of 0.5. The final score is calculated as shown in Algorithm 1. This ensures balanced assessment of theorem application and computational accuracy. First Error Step Detection phase identifies the earliest step of deviation from the correct soluAlgorithm 1 Physics Solution Auto Scoring Framework-Step Level (PSAS-S) Extract and normalize solution steps from model output 1: Phase 1: Data Extraction 2: Input: Model output , Annotation solution steps = {s1, s2, ..., sn}, Annotation step formulas = {f1, f2, ..., fn}, Information needed for step-level evaluation Annotation step values = {v1, v2, ..., vn} E: extracted relevant steps Ensure one-to-one mapping between extracted and annotated steps Evaluate formula application and numerical calculations Formula content Calculation target Get final score Get the final score with the step-level evaluation Identify the earliest point of solution deviation Initialization (cid:80)n i=1 scorei Ei LLM(ExtractTemplate(M, si)) ˆfi ExtractFormula(ei) ˆvi ExtractValue(ei) scorei 0.5 ScoreFormula( ˆfi, fi) + 0.5 ScoreValue(ˆvi, vi) 3: for si do 4: 5: end for 6: Assert = 7: Phase 2: Scoring 8: for (ei, si) (E, S) do 9: 10: 11: 12: end for 13: inal_score 14: Phase 3: First Error Step Detection 15: irst_error_step 16: for 1 to do 17: 18: 19: 20: 21: end for 22: Phase 4: Error Analysis 23: ErrorTypes = {DAE, PTAE, PCAE, PPUE, VRE, CPE, BCAE} 24: if irst_error_step < then 25: 26: 27: 28: end if 29: Output: inal_score, irst_error_step, error_type, error_analysis irst_error_step error_type LLM(ClassificationTemplate(ej, sj, )) error_analysis LLM(AnalysisTemplate(ej, sj)) if scorei < 1 then end if error_step FindOriStep(M, ei) with the relationship between and Find corresponding original step Get the minimum irst_error_step min(f irst_error_step, error_step) Analyze the first error step Error categories Identify error type Generate error analysis Table 2: Comparison between PSAS framework and direct use of LLM evaluation, where Answer Acc. denotes the accuracy of answer-level evaluation and Step Acc. indicates the precision in identifying the initial error step in the reasoning process. Model Gemini-2.0-Flash Deepseek-V3 Gemini-2.0-Flash-Thinking-0121 Deepseek-R1 Our (Gemini-2.0-Flash) Our (Deepseek-V3) Answer Acc. Step Acc. 87.81 89.78 91.24 93.31 98.96 99.35 33.18 34.45 35.74 37.54 97.23 98.04 ror (DAE), Physics Theorem Application Error (PTAE), Physics Condition Analysis Error (PCAE), Physics Process Understanding Error (PPUE), Variable Relationship Error (VRE), Calculation Process Error (CPE), and Boundary Condition Analysis Error (BCAE). Detailed error-type descriptions are available in the Appendix C. LLMs use structured prompts to identify the error type for the first error step. Then, comprehensive error analysis is generated to explain the reasoning behind the mistake. simplified example is shown in Figure 3. 4.4 Whether Evaluation Trustworthy? To validate the reliability of both our PSAS-A and PSAS-S, we compare our PSAS against conventional direct LLM evaluation approaches at both Figure 3: Step-level evaluation example obtained from PSAS-S framework. tion path. When any step is found with score below 1, FindOriStep function locates the corresponding original step in the models raw output based on the mapping relationship between and obtained from the Data Extraction phase, and updates irst_error_step to maintain the earliest error position. This enables precise identification of where the models reasoning first goes wrong. Error Analysis phase analyzes the first error step detected in the solution, with two components: error classification and error analysis. For error classification, PSAS-S considers seven types of common errors: Diagram Analysis Er5 answer-level and step-level, using the Chain-ofThought reasoning strategy. We implement experiments using Deepseek-V3 and Gemini-2.0-Flash as scoring models in the following experiments: 1. For answer-level evaluation, we employ scoring models to assess answer correctness by combining both model-generated outputs and annotation answers. We then compare these results with the judgments obtained from PSAS-A. 2. For step-level evaluation, inspired by previous work (Zheng et al., 2024), we design the task of identifying the first error step in reasoning processes containing errors, where higher accuracy indicates more precise evaluation of the reasoning process. Then, we submit both modelgenerated and annotated reasoning processes to scoring models to determine the location of the first error step, comparing with PSAS-S. Then, we collect 8,400 reasoning processes generated from multiple advanced models, including Deepseek-R1, Gemini-2.0-Flash, Gemini-2.0Flash-Thinking-0121, GLM-Zero, o1-mini, QwQ32B, and QvQ-72B. Subsequently, we randomly sample 1,000 reasoning processes and meticulously manually annotate them to determine the correctness of each answer and identify the location of the first error step. The results presented in Table 2 demonstrate that our frameworks achieve superior performance compared to direct LLM evaluation, highlighting the accuracy and reliability of PSAS evaluation results on PhysReason."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setting Baselines: We evaluate current mainstream opensource and closed-source LLMs, VLMs, and several o-like models. For models that cannot accept visual inputs, we use Gemini-2.0-Flash to generate captions for each image as supplementary information. We assess 15 advanced LLMs/VLMs under the zero-shot Chain-of-Thought (CoT) setting (encouraging models to think step by step), including 7 non-O-like models (Qwen2-VL-72B (Wang et al., 2024b), GPT-4o (OpenAI), Claude3.5-Sonnet (Anthropic), InternVL2.5-78B (Chen et al., 2024), Deepseek-v3 (DeepSeek-AI, 2024)), Gemini-2.0-Flash (Deepmind, 2024), Gemini-2.0Pro (Deepmind, 2025) and 8 O-like models (QwQ32B (Team, 2024b), QvQ-72B (Team, 2024a), o1-mini (OpenAI, 2024b), o1 (OpenAI, 2024a), o3-mini-high (OpenAI, 2025), Gemini-2.0-FlashThinking (Deepmind), Deepseek-R1 (Guo et al., 2025), GLM-Zero (ZhipuAI, 2024)). Note that Gemini-2.0-Flash-Thinking has two versions: 1206 and 0121. Due to API limitations, we do not experiment with o1 on the entire dataset. All other models are evaluated on the complete benchmark. Evaluation Workflow: We encourage models to generate reasoning processes step by step for all problems in PhysReason, with open-source models running on NVIDIA A800 GPUs. Please refer to Appendix-E for the detail prompt template. Then, we evaluate the models performance with the PSAS framework at both the answer and step levels, as described in Sections 4.2 and 4.3. Based on the experimental results in Section 4.4, considering both efficiency and performance, we select Deepseek-V3 as the final scoring model. PhysReason-mini: Considering that the complete PhysReason requires relative high evaluation costs, we create balanced PhysReason subset - PhysReason-mini. We randomly sample 200 questions from the whole benchmark (50 for each difficulty level), striving to achieve equal representation across categories wherever possible. 5.2 Main Results As demonstrated in Tables 3 and 4, the experimental results on the PhysReason and PhysReason-mini reveal several significant findings. Model Categories: O-like models exceed nonO-like ones, with multiple O-like models surpassing 50% answer-level accuracy compared to nonO-like models peak of 47.88%. Difficulty Level Analysis: As the difficulty increases, the required solution steps also increase, while model performance severely declines, indicating that models still perform inadequately on physics problems requiring deep reasoning. Step-level vs. Answer-level Evaluation: The two evaluation frameworks assess performance from different perspectives. Step-level scores consistently surpass answer-level scores, indicating that models can achieve some correct steps despite failing to reach the correct final answer. Moreover, the step-level score differences between models become more pronounced than those at the answer level as problem difficulty increases. This demonstrates that step-level evaluation proves more discriminative in distinguishing model capabilities, particularly in highly challenging problems. The distributions of these two evaluation methodologies exhibit non-perfect synchronization, indicating that 6 Table 3: Model performance comparisons on the PhysReason benchmark using answer-level (left of /) and step-level (right of /) evaluations across different input combinations of Questions (Q), Images (I), and Image Captions (IC). Gemini-2.0-T and represent Gemini-2.0-Flash-Thinking-1206 and 0121. Model Input Knowledge Medium Hard Avg. Q, Q, Q, 41.92/62.47 Qwen2VL-72B 28.34/64.71 InternVL2.5-78B GPT-4o 50.71/65.82 Deepseek-V3-671B Q, IC 55.86/66.14 54.14/66.45 Claude-3.5-Sonnet 65.08/75.04 Gemini-2.0-Flash 67.99/79.01 Gemini-2.0-Pro Q, Q, Q, o1-mini QvQ-72B Gemini-2.0-T QwQ-32B GLM-Zero o3-mini-high Gemini-2.0-T Deepseek-R1 Q, IC 53.90/65.74 Q, 62.44/70.92 65.35/77.20 Q, Q, IC 62.03/76.28 Q, IC 64.95/80.36 Q, IC 70.67/83.61 73.44/84.15 Q, Q, IC 75.11/85.91 Easy Non-O-like Models 24.04/45.26 24.16/50.69 33.87/51.98 40.06/52.77 41.35/55.85 54.84/68.60 55.43/71.47 O-like Models 35.21/52.26 53.74/64.65 51.89/67.49 54.92/71.08 54.11/71.54 67.20/81.95 63.17/75.94 65.08/79.81 15.97/36.13 17.72/38.56 22.73/42.36 26.63/44.02 28.14/44.86 39.79/55.67 44.29/57.74 22.24/40.19 28.18/54.88 44.43/58.95 43.64/62.14 41.32/63.67 45.31/64.57 50.41/66.60 54.84/72. 4.83/24.23 9.71/25.95 11.03/24.71 13.73/26.87 15.11/28.51 21.99/38.39 23.81/42.66 10.61/26.80 14.30/36.47 27.14/45.48 22.99/42.19 23.04/47.46 30.12/47.23 31.90/48.47 31.95/51.50 16.96/42.88 19.98/45.89 29.58/47.23 34.07/48.42 34.69/49.88 45.20/60.40 47.88/62.74 30.49/47.18 32.67/57.66 47.20/63.07 45.89/63.87 46.52/65.76 53.32/69.34 54.73/69.73 56.75/73.26 Table 4: Comparison on PhysReason-mini with PSASA, where Gemini-2.0-T and represent Gemini-2.0Flash-Thinking-1206 and 0121. And K., E., M. and H. represent knowledge, easy, medium and hard. Table 5: Test-Time Compute Scaling Performance Comparisons on PhysReason-mini with PSAS-A, where Flash and Think denote Gemini-2.0-Flash and Gemini2.0-Flash-Thinking-0121, and Tour. means Tournament. Model o1-mini QvQ-72B QwQ-32B Gemini-2.0-T GLM-Zero o1 o3-mini-high Gemini-2.0-T Deepseek-R1 K. 54.80 51.17 64.40 71.47 72.70 72.47 71.10 76.33 85.17 E. 30.33 37.10 50.07 49.97 50.17 53.37 63.20 56.87 60.77 M. 15.41 29.83 38.88 36.83 43.42 49.31 47.02 51.85 47.24 H. 7.92 22.13 27.45 22.97 24.70 25.32 31.93 32.61 33.23 Avg. 27.11 35.06 45.20 45.42 47.75 50.12 53.31 54.42 56. step-level evaluation provides comprehensive insights to answer-level assessment. Medium and Hard Problem Analysis: Performance on medium and hard reasoning problems can emerge as key differentiators of model physics-based reasoning ability. Among these models, those achieving scores of 40/60 and 30/50 on answer-level and step-level evaluations respectively serve as critical reference points. Knowledge-Reasoning Correlation Analysis: Results show positive correlation between physics knowledge and reasoning capabilities, with Deepseek-R1 and Gemini-2.0-Flash-Thinking0121 excelling in both aspects. Moreover, among models with similar scores on knowledge problems, O-like models tend to achieve higher scores on reasoning problems (as demonstrated by GeminiBase Method Reward N=1 N=2 N= N=8 Flash Think BoN Tour. BoN Tour. Flash Think Flash Think Think Think 46.52 46.52 46.52 46.52 54.42 54. 46.67 47.37 45.87 47.51 52.27 55.60 47.12 48.87 47.36 52.11 54.78 56. 47.81 50.94 49.58 53.06 55.13 56.57 2.0-Flash and Gemini-2.0-T). This suggests that reinforcement learning and training with thought chains help improve models reasoning capabilities. In conclusion, effective reasoning relies on knowledge capacity and model architecture. 5.3 Results with Test-Time Compute Scaling We evaluate Best-of-N (BoN) and TournamentStyle selection (Snell et al., 2025; Yang et al., 2024) test-time compute scaling methods on PhysReasonmini. Using Gemini-2.0-Flash and Gemini-2.0Flash-Thinking-0121 as base models, we test different reward model configurations: when Flash serves as base model, both itself and Thinking-0121 are evaluated as reward models, while Thinking0121 uses self-reward due to its superior reasoning. Both methods (Cobbe et al., 2021; Lightman et al., 2024; Son et al., 2024) select optimal responses from multiple Chain-of-Thought candidates (N = 1, 7 Table 6: Performance Comparison with PSAS-A after Directly Concatenation (D. Acc) and Guided Error Localization (G. Acc) on PhysReason-mini, where Acc. means the original performance of the model, Gemini2.0-T represents Gemini-2.0-Flash-Thinking-0121. Model Deepseek-V3 Gemini-2.0-Flash Gemini-2.0-T Deepseek-R1 Acc. D. Acc. G. Acc. 40.78 34.07 51.55 46.52 56.82 54.42 58.33 56.60 29.31 42.76 50.66 52.26 Figure 4: Error statistics with PSAS-S framwork in PhysReason-mini, where Gemini-T-1206 and GeminiT-0121 denote Gemini-2.0-Flash-Thinking-1206 and Gemini-2.0-Flash-Thinking-0121. 2, 4, 8), as shown in Table 5. These scaling methods demonstrate the potential to enhance model performance through strategic response selection and process reward modeling. 5.4 Performance Improving with PSAS-S Given PSAS-Ss capability to locate and analyze the first error step as presented in Section 4.3, we conduct experiments on PhysReason-mini to explore whether models can correct errors after being informed. The experiments are divided into Direct concatenation and Guided error localization. The former (D. Acc.) combines questions with the previous reasoning process for second attempt. For the latter (G. Acc.), PSAS-S is used to locate and analyze the first error in the reasoning process, then combines the question, previous reasoning, and the location and analysis of the first error for second attempt. As shown in Table 6, results show that direct concatenation decreased performance by 3-5%, while guided error localization improved performance by 3-6%. This suggests that guiding LLMs to identify reasoning errors is crucial for enhancing their reasoning capabilities and also proves the effectiveness of our PSAS framework. 5.5 Error Kind Distribution Analysis Discovering errors in reasoning processes is not equivalent to fully understanding them; its also Figure 5: Performance with PSAS-S framework in the hard problems from PhysReason-mini. crucial to understand the causes of errors. We analyze the error distributions of different models on PhysiReason-mini as shown in Figure 4. Four prevalent error types consistently challenge all models: Physics Theorem Application, Physics Process Understanding, Calculation Process, and Physics Condition Analysis. This reveals models limited intuitive physics understanding, highlighting the need for stronger physics-based reasoning capabilities. Notably, o1 and o3-mini-high show elevated Physics Process Understanding Errors but reduced Calculation Process Errors. This maybe suggest trade-off between conceptual comprehension and computational precision. 5.6 Hard Problem Analysis Our analysis of 50 hard reasoning problems from PhysReason-mini across 7 models reveals two key insights (Figure 5). Despite variations in overall performance, each model exhibits unique strengths in specific problem domains, demonstrating the diverse nature of their reasoning capabilities. The models achievement of some scores (below 1) is notable, indicating their ability to initiate correct solution paths but failing to maintain this accuracy throughout the reasoning process. These patterns suggest that while current models grasp basic physics concepts, they struggle to sustain accurate reasoning across extended solution steps."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce PhysReason, novel physics-based reasoning benchmark with stratified difficulty and Physics Solution Auto-Scoring Framework with answer and step level evaluation. Experimental results show consistent decline in performance as reasoning depth increases. This benchmark establishes new standards for evaluating and improving AI models physics-based reasoning abilities."
        },
        {
            "title": "Limitation",
            "content": "Despite the comprehensive nature of our benchmark, two key limitations warrant discussion, concerning both benchmark construction and evaluation methodology. First, they focus primarily on testing models ability to apply and reason with physics theorems under idealized conditions, rather than fully reflecting real-world physics scenarios. However, it is worth noting that applying physics theorems under idealized conditions serves as the foundation for real-world physics scenarios, as the latter is more complex. However, current LLMs performance even on idealized conditions remains unsatisfactory. Therefore, PhysReason remains valuable in evaluating models ability to apply physics theorems for physics-based reasoning. Moreover, through data synthesis, many problems in PhysReason can be adapted to create real-world physics reasoning scenarios, which will be direction for our future research. Second, our evaluation framework, though achieving over 98% accuracy using LLMs as assessment tools, is not without limitations. The PSAS-S framework, while demonstrating satisfactory performance, increases computational time for evaluation. In future work, we will explore ways to optimize evaluation time while maintaining assessment accuracy."
        },
        {
            "title": "Ethical Statement",
            "content": "In developing PhysReason, we carefully considered and addressed potential implications and risks. Our benchmark, sourced exclusively from public official materials (IPhO, Gaokao, JEE, and authorized mock exams), undergoes rigorous data cleansing, deduplication, and standardization to ensure reliability while minimizing bias and data leakage. Committed to environmental sustainability, we publicly release complete datasets and accompanying scripts under appropriate licenses (MIT and CC BY-NC-SA) to cut down on unnecessary carbon footprint, while optimizing processing pipelines to reduce computational overhead. In all experiments, we strictly comply with all licenses for models and data. Our benchmark is an important resource that drives AGIs strength in scientific reasoning, maintaining high standards for data quality and ethical considerations."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 sonnet. Daman Arora, Himanshu Gaurav Singh, et al. Have llms advanced enough? challenging problem solving benchmark for large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. 2025. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Google Deepmind. Gemini 2.0 flash thinking mode. Google Deepmind. 2024. Introducing gemini 2.0: Our new ai model for the agentic era. Google Deepmind. 2025. Introducing gemini 2.0 pro. DeepSeek-AI. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. 2024a. Omnimath: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. 2024b. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1246212469. IEEE. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. 9 Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. 2025. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations. Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, and Min Yang. 2024. E-EVAL: comprehensive Chinese k-12 education evaluation benchmark for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 77537774, Bangkok, Thailand. Association for Computational Linguistics. Yu Huang, Yue Chen, and Zhu Li. 2023. Applications of large scale foundation models for autonomous driving. arXiv preprint arXiv:2311.12144. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36. Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large In Proceedings of the 61st Anlanguage models. nual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 3742. Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James Kwok. 2024. Forward-backward reasoning in large language models for mathematical verification. In Findings of the Association for Computational Linguistics ACL 2024, pages 66476661. Morris Kline. 1981. Mathematics and the physical world. Courier Corporation. Wenqiang Lai, Tianwei Zhang, Tin Lun Lam, and Yuan Gao. 2024. Vision-language model-based physical reasoning for robot liquid perception. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 96529659. IEEE. Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, et al. 2024. Controllable text generation for large language models: survey. arXiv preprint arXiv:2408.12599. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. OpenAI. Hello gpt-4o. OpenAI. 2024a. Learning to reason with LLMs. OpenAI. 2024b. Openai o1-mini. OpenAI. 2025. Openai o3-mini. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling llm test-time compute optimally can be more effective than scaling model parameters. In International Conference on Learning Representations. Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, and Kuntae Kim. 2024. Varco arena: tournament approach to reference-free benchmarking large language models. arXiv preprint arXiv:2411.01281. Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. 2024a. Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98289862. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024b. Scieval: multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1905319061. Qwen Team. 2024a. Qvq: To see the world with wisdom. 10 Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2024. Agieval: human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314. Qwen Team. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. Zaharov Timur, Konstantin Korolev, and Aleksandr Nikolich. 2024. Physics big. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. 2024a. Mineru: An open-source solution for precise document content extraction. Preprint, arXiv:2409.18839. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. In Forty-first International Conference on Machine Learning. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2025. Are large language models really good logical reasoners? comprehensive evaluation and beyond. IEEE Transactions on Knowledge and Data Engineering. Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, and Jun Liu. 2024. Symbol-LLM: Towards foundational symbolcentric interface for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1309113116, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2024. Docmatheval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1610316120. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. ZhipuAI. 2024. Glm-zero mode."
        },
        {
            "title": "A Data Sources",
            "content": "Our dataset is derived from four distinct sources, each representing different levels and approaches to physics education and assessment. These sources have been carefully selected to ensure comprehensive coverage of physics problems across various difficulty levels and cultural contexts. The diversity of these sources helps in creating robust and wellrounded dataset that captures different pedagogical approaches and problem-solving methodologies. International Physics Olympiad (IPhO) Problems The Physics Olympiad problems are globally recognized for their complexity and quality. These problems typically require multiple solution approaches and the integration of capabilities across mathematics and physics subdisciplines. Participants in these competitions represent some of the worlds strongest talent in physics logical reasoning. The problems often combine theoretical understanding with practical applications, requiring students to demonstrate both analytical and creative problem-solving skills. The international nature of these competitions ensures diverse range of problem-solving approaches and cultural perspectives. among multiple high schools. These diverse sources provide rich spectrum of problemsolving scenarios and difficulty levels. The multi-tiered nature of these mock examinations reflects different regional interpretations of educational standards while maintaining alignment with national requirements. The variety in question sources ensures exposure to different testing styles and pedagogical approaches, making this dataset particularly valuable for understanding the breadth of physics education assessment in China. Indian Joint Entrance Examination (Advanced) This examination represents one of Indias most prestigious and challenging engineering entrance tests. The exam structure, consisting of two papers with 50-60 questions each, provides comprehensive assessment of physics knowledge alongside mathematics and chemistry. The questions are known for their analytical depth and often require multi-step problem-solving approaches. The exams high stakes nature and competitive environment ensure that the problems are both challenging and discriminating, making them valuable additions to our dataset. Chinese National College Entrance ExamiOthers nation (Gaokao) Physics Questions The Gaokao physics questions represent rigorous standardized assessment system that has been refined over decades. These questions are designed to test both fundamental understanding and advanced application of physics concepts at the high school level. The problems are carefully calibrated to discriminate between different levels of student ability while maintaining high reliability and validity. They often incorporate real-world scenarios and practical applications, making them particularly valuable for assessing applied physics knowledge. Chinese Mock Examinations at Various Levels Our collection includes comprehensive range of mock examination questions from multiple administrative levels in China. This includes provincial-level mock exams, citylevel assessment materials, and joint examination papers created through collaboration We also obtained some physics questions from non-Chinese and English sources on huggingface, such as Russian (Timur et al., 2024). This dataset consists of diverse collection of physics problems, categorized into different domains, including 1000 problems on Kinematics, 600 problems on Electricity and Circuits, and 500 problems on Thermodynamics. All data has been extracted from open sources, ensuring wide variety of problem types and difficulty levels. The PhysReason benchmark is derived from publicly available physics education materials including: International Physics Olympiad problems (2008-2021), Chinese National College Entrance Examination physics questions (2010-2024), Indian Joint Entrance Examination Advanced physics problems (2010-2024), Chinese provincial and municipal mock examination questions (2015-2024). We have collected more than 20,000 physics problems. All problems were collected in accordance with fair use principles for educational and research 12 purposes. The complete benchmark and associated code will be released under the MIT License for research use. The dataset contains no personally identifiable information. All problems are from standardized tests and competition materials with no individual student data. This documentation ensures reproducibility and proper usage of the benchmark while protecting privacy and intellectual property rights."
        },
        {
            "title": "B Benchmark",
            "content": "B.1 Collection B.1.1 Data Acquisition We systematically collected, curated, and processed physics problems from diverse sources to ensure comprehensive coverage of physics concepts and problem-solving scenarios. Our dataset comprises 1,254 PDF documents totaling 27,874 pages, yielding over 20,000 unique problems. This extensive collection provides rich foundation for developing comprehensive physics problem benchmark. B.1.2 Data Standardization We implemented systematic data processing pipeline utilizing MinerU (Wang et al., 2024a) for PDF parsing. The standardization process encompasses several critical phases: initial format conversion, rigorous deduplication, and comprehensive formatting standardization. Each question underwent rigorous quality assessment process with specific evaluation criteria: 1. Complete problem statements with welldefined variables and conditions 2. Clear and unambiguous wording 3. Accurate expressions and units Phase II: Verification Validation by Engineering Ph.D. candidates Verification of physics terminology accuracy Confirmation of semantic equivalence Review of mathematical expression consistency B.1.4 Search Prevention Following (Rein et al., 2024), we exclude problems whose answers could be found through fiveminute Google search to minimize data leakage. This step ensures that model evaluation reflects genuine physics problem-solving capabilities rather than information retrieval abilities. B.1.5 Difficulty Classification: Questions were systematically categorized using multi-dimensional classification framework: Primary Classification Knowledge-based questions: Focus on fundamental physics concepts Clear-cut application of specific theorems Direct calculation or concept identification Reasoning-based questions: Multiple-theorem integration Multi-step problem-solving approaches Complex analytical thinking 4. Consistent formatting of equations and symDifficulty Levels in Reasoning-based Questions bols B.1.3 Translation To standardize the multilingual dataset comprising Chinese, English, Hindi, and Russian content, we implement two-phase process: Phase I: Translation Initial translation by translators Easy: Total steps 5 Completion time: 0-5 minutes Medium: Total steps 10 Completion time: 5-15 minutes Strict adherence to standardized physics terminology Hard: Consistent mathematical notation and expressions Total steps > 10 Completion time: 15+ minutes Figure 6: Illustration of the data collection pipeline. B.2 Annotation Key Elements As shown in Figure 1, our annotation framework consists of 7 key elements: Context: Detailed physics scenario description: Describe the physics setup thoroughly, including objects, environment, and interactions. For example, specify angles, materials, initial conditions, and forces. Clear specification of conditions and constraints: Explicitly list all given conditions: initial conditions (e.g., initial velocity, position), boundary conditions, and constraints (e.g., inextensible string, frictionless surface). Standardized notation for physics quantities: Use consistent and standard symbols for physics quantities (e.g., for velocity, for acceleration, for mass) throughout the annotation. Sub-question: Hierarchical structure of related questions: Break down complex problem into smaller, logically connected subquestions. These should build upon each other. Clear progression of complexity: Subquestions should increase in difficulty, guiding the learner from basic concepts to more advanced analysis. If the formula can be solved to value, it should also have value: If steps formula yields numerical result, provide that result. Step Analysis: Explicit theorem application rationale: Clearly state which theorem, law, or principle is applied in each step and why its applicable. Example: Newtons Second Law. Physics quantity derivation explanation: Explain how unknown physics quantities are derived from known ones. Example: \"W = Ek\" Answer: Numerical results with appropriate units: Provide the correct numerical value and units for numerical answers (e.g., \"v = 5m/s\"). Formulaic results with appropriate symbols: For formulaic answers, use previously defined standard symbols and ensure the formulas correctness (e.g., \"v = 2gh\"). Difficulty: Reasoning difficulty metrics: Use qualitative descriptions (e.g., \"knowledge\" \"easy,\" \"medium,\" \"hard\") Solution: Theorem: Detailed step-by-step reasoning process: Provide comprehensive, step-by-step solution. Do not skip any crucial reasoning steps. Each step contains at least one formula: Each step in the solution should include at least one relevant physics formula (theorem, law, or derived equation). Comprehensive list of applicable theorems, laws, and formulas: Provide complete list of all the specific physics theorems, laws, and equations that are relevant to solving the problem. Examples include: Newtons Second Law, WorkEnergy Theorem, Conservation of Momentum, Kinematic Equations, etc. 14 Core Concepts: Identify the fundamental physics principles and ideas that underpin the solution, even if they arent expressed as single equation. Examples include: Wave-Particle Duality."
        },
        {
            "title": "C Error Type Details",
            "content": "The following is summary of the error types, categorized and with expanded explanations: 1. Diagram Analysis Errors: Description: Errors related to the comprehension, plotting, analysis, or extraction of data from graphical representations. This encompasses any mistake made when working with diagrams, charts, or graphs. Examples: Misreading the labels or units on the axes of graph. Misinterpreting the trend of curve (e.g., confusing linear relationship with an exponential one). Failing to identify key data points or features on the graph (e.g., maxima, minima, intercepts). Incorrectly extrapolating or interpolating data from the graph. Drawing an inaccurate graph based on given data. Misunderstanding the conditions under which particular law is valid. 3. Physics Condition Analysis Errors: Description: Errors related to the incorrect assessment of the physics systems boundaries, the forces acting on it, or its constituent components. This involves misunderstanding of what is happening in the system. Examples: Neglecting the force of friction in situation where it is significant. Incorrectly identifying the system boundary, leading to errors in applying conservation laws. Misjudging whether system is isolated (no external forces) or not. Failing to consider all relevant forces acting on an object. Misidentifying the components of system that are interacting. 4. Physics Process Understanding Errors: Description: Errors stemming from misunderstanding of how physics phenomenon develops, how states change, or the causal relationships between events. This involves misunderstanding of how things are happening. 2. Physics Theorem Application Errors: Examples: Description: Errors arising from the incorrect application of physics theorems or principles, or using them in situations where they are not valid. This includes both misremembering the law itself and misapplying correctly remembered law. Examples: Applying Newtons Laws of Motion to non-inertial reference frame without accounting for fictitious forces. Using the conservation of energy principle in system where nonconservative forces (like friction) are doing significant work. Applying formula outside of its valid range of applicability (e.g., using small-angle approximation when the angle is large). Incorrectly analyzing the motion of projectile, such as misunderstanding the independence of horizontal and vertical motion. Misunderstanding the mechanisms of energy transformation (e.g., confusing heat and temperature). Incorrectly predicting the direction of motion based on the forces involved. Having misconceptions about the nature of physics process (e.g., believing that continuous force is needed to maintain constant velocity). 5. Variable Relationship Errors: Description: Errors caused by misunderstanding the dependencies or functional relationships between different physics 15 quantities. This involves incorrectly relating variables. Examples: Incorrectly assuming that acceleration is directly proportional to velocity. Misunderstanding the relationship between force, mass, and acceleration (Newtons Second Law). Confusing the relationship between potential and kinetic energy. Failing to recognize an inverse relationship between two variables. 6. Calculation Process Errors: Description: Errors occurring during the mathematical manipulation of equations, the derivation of formulas, or the substitution of numerical values. These are purely mathematical mistakes. Examples: Making algebraic errors when rearranging equations. Incorrectly performing unit conversions (e.g., mixing up meters and centimeters). Making arithmetic errors (e.g., simple addition or multiplication mistakes). Incorrectly substituting values into formula. Errors in using calculator. 7. Boundary Condition Analysis Errors: Description: Errors resulting from neglecting or mishandling special cases, limiting conditions, or the applicable ranges of variables or equations. This involves not considering the \"edges\" of the problem. Examples: Failing to consider the behavior of system at extremely high or low temperatures. Neglecting the effects of air resistance when analyzing projectile motion at high speeds. Not considering the limitations of particular model or approximation. Applying formula outside its range of validity. Ignoring initial conditions or other constraints."
        },
        {
            "title": "D Example",
            "content": "We have provided representative example for each of the four question difficulty levelsknowledge (Figure 7), easy (Figure 8), medium (Figure 9), and hard (Figure 10) to serve as guide. The knowledge-level problem demonstrates the fundamental application of electromagnetic principles, requiring direct use of basic physics theorems without complex problem-solving steps. This type of question focuses on testing models understanding of core concepts and their ability to apply basic formulas. The easy-level problem involves straightforward mechanical system with clear physics conditions. It requires models to apply basic conservation laws and Newtons laws in sequential manner, with each step building logically on the previous one. The solution path is direct and requires minimal manipulation. The medium-level problem introduces multiple state changes and requires models to analyze system under different configurations. It combines several physics principles and demands more sophisticated understanding of how different variables interact. The solution requires models to track system changes systematically while maintaining consistency in their physics-based reasoning. The hard-level problem presents complex mechanical system with multiple connected components and sequential events. It requires models to analyze series of interactions, apply multiple physics principles simultaneously, and consider various constraints throughout the problem-solving process. The solution demands both careful physics insight and mathematical rigor, testing models ability to synthesize different concepts and handle multi-step calculations. These examples demonstrate the progressive complexity in physics problem-solving across different difficulty levels. From knowledge-level questions testing basic concept application, to hard problems requiring integration of multiple physics principles and sophisticated analysis, each level builds upon the previous one. This hierarchical structure effectively assesses models comprehension 16 and problem-solving abilities, ranging from fundamental understanding to advanced physics-based reasoning and mathematical manipulation. The gradual increase in complexity helps evaluate models mastery of both individual concepts and their ability to synthesize multiple physics principles in complex scenarios."
        },
        {
            "title": "E Evaluation Prompt",
            "content": "To systematically evaluate models mathematical reasoning capabilities, we designed structured prompt template that follows the zero-shot Chain-of-Thought (CoT) paradigm. This template adopts hierarchical structure comprising image information, problem context, and sequential subquestions, requiring models to provide standardized step-by-step solutions. The prompt structure consists of the following key components: E.1 Input Components Image Caption: For models without direct image processing capabilities, we utilize Gemini-2.0-flash to generate image descriptions as supplementary information Context: Provides the overall background and fundamental information of the problem Sub-questions: Decomposes complex problems into progressive sub-questions E.2 Output Specifications The template requests structured output format with the following requirements: Step-by-step reasoning for each sub-question Continuous step numbering across subquestions One formula and its solution process per step Mathematical formulas enclosed in LaTeX notation ($) This design adheres to the zero-shot Chain-ofThought paradigm, facilitating systematic thinking through explicit step division and standardized output format, which benefits both model reasoning and subsequent performance evaluation. The templates flexibility allows it to accommodate pjhysical problems of varying complexity, with adjustable numbers of sub-questions and solution steps based on specific problem requirements."
        },
        {
            "title": "F Details of Experimental Result",
            "content": "We previously presented only partial model performance benchmarks on PhysReason-mini. And we provide comprehensive performance evaluation across all models, as shown in Table 7."
        },
        {
            "title": "G Details of Scientific Artifacts",
            "content": "Our PhysReason benchmark dataset integrates problems from multiple sources: International Physics Olympiad (2008-2021), Chinese National College Entrance Examination (2010-2024), Indian Joint Entrance Examination Advanced (20102024), Chinese provincial and municipal mock examination questions (2015-2024), and additional physics problems from Russian sources, totaling over 20,000 unique physics problems from 1,254 PDF documents across 27,874 pages. The dataset has been carefully curated to ensure comprehensive coverage while respecting intellectual property rights - all problems are utilized under the CC BYNC-SA and MIT licenses, and all materials were collected in accordance with fair use principles for educational and research purposes. We maintain strict privacy standards with no personally identifiable information included, as all problems are sourced from standardized tests and competition materials. The complete benchmark and associated code are made available for research use, requiring users to comply with both the MIT License terms for our implementation and the respective original licenses (CC BY-NC-SA) for the educational materials, thereby ensuring proper attribution and usage rights while promoting academic accessibility."
        },
        {
            "title": "H Details of Computational Experiment",
            "content": "Our computational experiments were conducted across multiple Large Language Models (LLMs), Vision Language Models (VLMs), and other specialized models. The infrastructure primarily consisted of NVIDIA A800 GPUs for running opensource models. For model specifications, we evaluated seventeen models in total, including Qwen2VL-72B (72 billion parameters), QwQ-32B (32 billion parameters), QvQ-72B (72 billion parameters), InternVL2.5-78B (78 billion parameters), and various other commercial models like GPT-4, Claude-3.5-Sonnet, and Gemini series. All experiments were conducted under zero-shot Chain-ofThought (CoT) setting to encourage step-by-step reasoning. For the experimental setup, we utilized 17 Table 7: Model performance comparisons on the PhysReason-mini benchmark using answer-level evaluation across different input combinations of Questions (Q), Images (I), and Image Captions (IC). Gemini-2.0-T and represent Gemini-2.0-Flash-Thinking-1206 and 0121. Input Knowledge Model Non-O-like Models Q, Qwen2VL-72B Q, InternVL2.5-78B Q, GPT-4o Claude-3.5-Sonnet Q, Deepseek-V3-671B Q, IC Q, Gemini-2.0-Flash Q, Gemini-2.0-Pro O-like Models o1-mini QvQ-72B QwQ-32B Gemini-2.0-T GLM-Zero o1 o3-mini-high Gemini-2.0-T Deepseek-R1 Q, IC Q, Q, IC Q, Q, IC Q, Q, IC Q, Q, IC 25.40 37.90 51.12 49.00 56.60 67.80 69.32 54.80 51.17 64.4 71.47 72.70 72.47 71.10 76.33 85.17 Easy Medium Hard Avg. 27.00 20.60 31.95 40.43 40.97 52.10 53.67 30.33 37.10 50.07 49.97 50.17 53.37 63.20 56.87 60.77 11.4 18.14 20.75 23.45 22.22 40.00 44. 15.41 29.83 38.88 36.83 43.42 49.31 47.02 51.85 47.24 8.5 7.97 12.54 12.33 14.61 23.19 26.24 7.92 22.13 27.45 22.97 24.70 25.32 31.93 32.61 33.23 18.07 21.15 29.09 31.3 33.6 46.52 48.55 27.11 35.06 45.20 45.42 47.75 50.12 53.31 54.42 56.60 no formal recruitment process or compensation was required, and they were fully aware of how the data would be used in the study. The annotation process focused solely on physics content evaluation and did not involve collecting any personal identifying information or expose annotators to any risks. As this research involved co-authors analyzing academic content rather than external human subjects, it was determined to be exempt from formal ethics review board approval. The annotation work was conducted as part of regular academic research activities within our institution. No protected or sensitive demographic information was collected or used in this research."
        },
        {
            "title": "Writing",
            "content": "We used Claude-3.5-Sonnet, o1, o3-mini-high, and Deepseek-R1 to help us write code and polish the paper. specific prompts (detailed in supplementary materials) to maintain consistency across all evaluations. The models processed the complete PhysReason benchmark dataset, with the exception of O1 due to API limitations. For performance evaluation, we employed both PSAS-A and PSAS-S frameworks, with Deepseek-V3 ultimately selected as the scoring model based on efficiency and performance considerations. Regarding implementation details, models that couldnt process visual inputs were supplemented with image captions generated by Gemini-2.0-Flash. For reproducibility purposes, all prompt templates are provided in the supplementary materials. Due to the computational cost of the PSAS-S framework, some experiments were conducted using only the PSAS-A framework to maintain efficiency."
        },
        {
            "title": "I Details of human annotators",
            "content": "For data annotation and evaluation, we engaged four graduate students (including both PhD and Masters students) from engineering disciplines who are also co-authors of this paper. All annotators possessed strong backgrounds in both high school and undergraduate physics, making them well-qualified for this task. Since the annotators were co-authors actively involved in the research, 18 Figure 7: knowledge example in our benchmark. 19 Figure 8: An easy example in our benchmark. Figure 9: medium example in our benchmark. 21 Figure 10: hard example in our benchmark."
        }
    ],
    "affiliations": [
        "Institute of High-Performance Computing, A*STAR",
        "Show Lab, National University of Singapore",
        "Xian Jiaotong University"
    ]
}