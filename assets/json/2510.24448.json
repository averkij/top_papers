{
    "paper_title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
    "authors": [
        "Pablo Acuaviva",
        "Aram Davtyan",
        "Mariam Hassan",
        "Sebastian Stapf",
        "Ahmad Rahimi",
        "Alexandre Alahi",
        "Paolo Favaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 4 4 4 2 . 0 1 5 2 : r RETHINKING VISUAL INTELLIGENCE: FROM VIDEO PRETRAINING"
        },
        {
            "title": "INSIGHTS",
            "content": "Pablo Acuaviva Computer Vision Group University of Bern Bern, Switzerland pablo.acuavivahuertos@unibe.ch Mariam Hassan VITA Lab, EPFL Lausanne, Switzerland mariam.hassan@epfl.ch Ahmad Rahimi VITA Lab, EPFL Lausanne, Switzerland ahmad.rahimi@epfl.ch Paolo Favaro Computer Vision Group University of Bern Bern, Switzerland paolo.favaro@inf.unibe.ch Aram Davtyan Computer Vision Group University of Bern Bern, Switzerland aram.davtyan@unibe.ch Sebastian Stapf Computer Vision Group University of Bern Bern, Switzerland sebastian.stapf@unibe.ch Alexandre Alahi VITA Lab, EPFL Lausanne, Switzerland alexandre.alahi@epfl.ch"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problemsolving. We investigate Video Diffusion Models (VDMs) as promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design controlled evaluation in which both pretrained LLM and pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Foundation models have reshaped natural language processing by showing that large-scale pretraining can equip models with broad knowledge and strong inductive priors. This foundation allows models to adapt quickly and effectively to new tasks through techniques like in-context learning Computer Vision Group, University of Bern https://www.cvg.unibe.ch/people/ VITA Lab, EPFL https://www.epfl.ch/labs/vita/people/ Preprint. Under Review. Figure 1: Radar plot showing ConceptARC competencies between VDMs and LLMs , GPT-4 [IC] is added for additional reference. (Brown et al., 2020) and parameter-efficient fine-tuning (Liu et al., 2022), achieving strong performance with minimal supervision. The success of Large Language Models (LLMs) illustrates how scale and pretraining can create systems that generalize across diverse problems. Achieving similar level of versatility in vision, however, remains largely unexplored and major challenge. Despite recent breakthroughs in image and video generation (Labs, 2025; Polyak et al., 2024; Qin et al., 2024), vision models are not yet on par with LLMs when it comes to compositional skills, sample efficiency, and versatility in problem solving. Video Diffusion Models (VDMs) represent an exciting direction for narrowing this gap. Pretraining on rich spatiotemporal data endows them with strong inductive biases for spatial structure and temporal dynamics (Blattmann et al., 2023; Google DeepMind, 2025; Wu et al., 2025), which we hypothesize can be harnessed for structured visual understanding. We move beyond treating videos as mere generative artifacts and instead regard them as natural representation for problem solving, where tasks are expressed as transformations unfolding over time. Building on this perspective, we introduce simple and general framework for adapting VDMs to broad class of visual tasks and evaluate them head-to-head with equally adapted LLMs (see Figure 1). This setup allows us to test whether large-scale video pretraining offers complementary foundation for structured visual problem-solving, contrasting the strengths of visually grounded models with those of symbolically trained language models. Each task is represented consistently but adapted to each model familys modality: LLMs operate in text-to-text setting, where inputs and outputs are serialized into structured text, while VDMs receive an image-to-image formulation, where inputoutput pairs are rendered as short videos to model the task as temporal transformation. Both model families use identical LoRA-based Hu et al. (2022) adaptation: adapters are inserted at corresponding layers, pretrained backbones remain frozen, and only lightweight parameters are updated. This symmetry provides controlled basis for comparison and isolates the impact of video pretraining on structured visual understanding. Our contributions are as follows: 1. unified framework for adapting VDMs to image-to-image visual tasks by reframing examples as temporal sequences. 2. controlled evaluation setting where both VDMs and LLMs are fine-tuned with LoRA-based adaptation, enabling direct comparison. 3. Empirical evidence that VDMs benefit from video pretraining for visual intelligence, hinting at path toward flexible visual foundation models with both generative and problem-solving strengths. Preprint. Under Review."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Language Foundation Models. LLMs have demonstrated remarkable generalization and adaptability to new tasks with minimal supervision, mainly due to their large-scale pretraining on diverse text corpora Brown et al. (2020); Chowdhery et al. (2023). Their extensive pretraining equips LLMs with rich knowledge and strong inductive biases, enabling them to perform few-shot learning Brown et al. (2020) and in-context learning Coda-Forno et al. (2023), where models learn new tasks only by observing handful of examples. Parameter-efficient finetuning methods like LoRA Hu et al. (2022) extend this adaptability allowing LLMs to specialize to new domains while the backbone is completely frozen Liao et al. (2025). Together, these capabilities make LLMs highly flexible and scalable problem solvers. In this paper, we leverage this adaptability to compare the data efficiency of LLMs and VDMs across diverse visual tasks. Video Diffusion Models. Diffusion-based generative models have recently achieved remarkable progress in video synthesis. Pioneering approaches such as CogVideo Hong et al. (2022) and Villegas et al. (2022) introduced scalable architectures for text-to-video generation. More recent models like Sora Qin et al. (2024), MovieGen Polyak et al. (2024), Veo 3 Google DeepMind (2025), and CogVideoX Yang et al. (2024) set new standards for quality and realism. Recent work has investigated controllable video generation NVIDIA et al. (2025); Hassan et al. (2025); Kanervisto et al. (2025), with the goal of producing realistic, high-quality videos while allowing precise control over motion and dynamics. These methods emphasize modeling dynamic environments and predicting plausible future states conditioned on past observations and control inputs. Visual Foundation Models Recent work has investigated the use of generative models as generalist vision models. Methods such as image inpainting for visual prompting Bar et al. (2022) and image-based in-context learning Wang et al. (2023a) demonstrate that structured inputs can enable these models to solve diverse tasks. Diffusion models have further been extended to in-context learning Wang et al. (2023b), instruction following across heterogeneous tasks Geng et al. (2024), and broader computer vision problem solving Zhao et al. (2025). Sequential modeling has been proposed as unified interface for scaling vision models Bai et al. (2024). Building on this line of work, Lin et al. (2025) train CogVideoX1.5 with temporal in-context prompts for multi-task learning, but their focus remains on broad computer vision benchmarks rather than visual intelligence, and their method requires extensive training1. Our approach does not attempt to build foundation model from scratch. Instead, we investigate whether pretrained VDM, pretrained extensively on next-frame prediction, can begin to exhibit the properties expected of visual foundation models by leveraging inductive biases gained through spatiotemporal pretraining."
        },
        {
            "title": "3.1 SETUP AND COMPARISON PROTOCOL",
            "content": "We adopt the definition of intelligence proposed by Chollet (2019): The intelligence of system is measure of its skill acquisition efficiency over scope of tasks with respect to priors, experience, and generalization difficulty. This perspective motivates our evaluation design. We focus not only on absolute accuracy but also on how quickly models acquire new capabilities when exposed to limited supervision. To evaluate our hypothesis we curate diverse benchmark of visually grounded tasks that can be specified textually as grid-based problems, including ARC-AGI, Sudoku solving, and route planning. We now describe the evaluation setup in detail. Let denote task with dataset DT = {(xi, yi)}n Each sample is expressed in two complementary modalities: i=1, where each xi and yi is an input-output pair. 1We add qualitative results on standard computer vision tasks in the Appendix to show that our framework can also be extended to this setting. Preprint. Under Review. Image An image pair (I(xi), I(yi)), where I() deterministically renders RGB images of size (3 ). Text JSON pair (J(xi), J(yi)), where J() maps grid to compact JSON string. We serialize samples in neutral format that avoids domain-specific priors, requiring both models to infer task rules directly from raw representations. Training and evaluation splits are identical across all models to ensure fair and controlled comparison. VDMs are trained directly on the image modality using our approach, which we detail in the next section, while LLMs are trained on the text modality. We define accuracy as the proportion of test instances where the predicted output exactly matches the ground truth grid. For tasks where multiple valid solutions may exist (e.g., Sudoku, Sudoku Mini, Hitori), we filter datasets to ensure each instance has an unique solution. When unique solutions cannot easily be guaranteed, as in Shortest Path, we introduce complementary metrics to better capture solution quality (see Section 4.2.2). To evaluate efficiency of skill acquisition, we consider two complementary settings. ARC Family. We evaluate models on ARC-AGI and ConceptARC, where the challenge is to solve diverse tasks from only 25 demonstrations. Following prior work Moskvichev et al. (2023); Chollet (2019); Li et al. (2025), we measure how many tasks each model can solve under this minimal supervision regime. Structured Visual Tasks. We then turn to structured benchmarks. Here we systematically vary n, the number of training examples per task, to trace curves and quantify the rate of skill acquisition rather than focusing solely on endpoint accuracy. 3.2 ADAPTING VIDEO DIFFUSION MODELS FOR IMAGE-TO -IMAGE We adapt pretrained VDMs to image-to-image (I2I) prediction tasks by re-framing each inputoutput pair (Ixi , Iyi) as short transition video. This leverages the generative prior of VDMs, while requiring minimal supervision. Transition video construction vi = [vi,1, . . . , vi,F ], where Each pair (xi, yi) is converted into temporal sequence vi,1 = I(xi), vi,F = I(yi). Intermediate frames are generated with an interpolation function ϕ. For example, convex interpolation produces smooth transition vi,f = (1 α) I(xi) + αI(yi), where α = 1 , and = 1, . . . , F, while discrete interpolation simply holds the input frame for the first half of the sequence and afterwards switches to the output frame: vi,f = (cid:26)I(xi), I(yi), F/2, > F/2. This yields dataset VT of input-conditioned video trajectories. For our comparisons, we adopt the discrete interpolation to avoid introducing any biases. Fine-tuning We adapt pretrained VDM by conditioning on the first frame v0 fixed text embedding etext. Given noisy video vt at step t, the model predicts noise ϵθ via 1 and neutral LVDM = Ev0VT ,ϵN (0,I),t (cid:2)ϵ ϵθ(vt, t, c)2 (cid:3) , = {v0 1, etext}. We use LoRA modules for fine-tuning, updating only these additional weights while keeping the pretrained model frozen. Inference At test time, the model generates predictions through reverse diffusion. The procedure is detailed in Algorithm 1. This procedure reframes image-to-image prediction as conditional video generation problem, enabling efficient adaptation of pretrained VDMs to new tasks. Preprint. Under Review. Algorithm 1 Inference for VDM Algorithm 2 Inference for LLM 1: Encode input: ctest {I(xtest), etext} 2: Initialize noise: sample vT (0, I) 3: Reverse diffusion: recover v0 conditioned 1: Encode input: J(xtest) as JSON string 2: Tokenize and feed sequence into model 3: Autoregressively decode output until termion ctest 4: Output prediction: ˆy F (final frame) nation 4: Return prediction: ˆy as JSON string"
        },
        {
            "title": "3.3 ADAPTING LARGE LANGUAGE MODELS",
            "content": "We adapt pretrained LLMs to structured prediction tasks by framing each example as JSON-toJSON translation problem. Fine-tuning We adapt pretrained LLMs using standard sequence-to-sequence objective. Given tokenized inputoutput pairs, the model is trained to maximize the likelihood of the target sequence under teacher forcing: LLLM = 1 (cid:88) vi (cid:88) i=1 t=1 log pθ(vi,t ui, v<t ). We insert LoRA modules into the pretrained backbone, fine-tuning only these lightweight adapters while keeping the majority of parameters frozen. Inference At test time, predictions are generated autoregressively. The procedure is summarized in Algorithm 2."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 ARC FAMILY The ARC-AGI benchmark Chollet (2019) evaluates an agents ability to infer and apply abstract patterns through compositional understanding, few-shot learning, and inductive generalization. Each ARC task provides only handful of inputoutput examples (typically 25), requiring the model to discover the underlying transformation rule and apply it to novel test inputs. This benchmark is widely regarded as challenging measure of progress in abstraction and generalization. We follow the evaluation protocol of Chollet et al. (2024), which allows up to two attempts per test input and counts question as solved only if all predictions match the ground truth. Quantitative results appear in Table 1, with qualitative examples in Figure 3. For comparison, we also report single-attempt results of commercial LLMs from Chollet et al. (2024). Figure 2 illustrates the overlap between tasks solved by the VDM and the LLM, underscoring their complementary strengths. We evaluate models on ConceptARC Moskvichev et al. (2023), curated variant of ARC designed to systematically measure visual concept understanding and generalization. ConceptARC groups tasks into 16 concept categories (for example, Above and Below, Center, Count), with each category containing 10 tasks. Each task includes 3 distinct test inputs, creating controlled variation in visual patterns and object relationships while maintaining internal consistency within each concept group. Following the protocol of Moskvichev et al. (2023), we allow three attempts per test input and mark an input as solved if any attempt is correct. Performance is reported in Figure 1, where we further include as VDMs: Wan2.1-14B Wang et al. (2025), LTX-13B, LTX-2B HaCohen et al. (2025), CogVideoX1.5-5B Yang et al. (2024) and as LLMs: Qwen3-4B-Instruct-2057, Qwen3-8B Qwen3-4B-Instruct-2507 Team (2025), Llama3.1-8B Meta-AI (2024), and GPT-4 in an IC setting Moskvichev et al. (2023). Full table with results is included in the Appendix. These results highlight the importance of strong visual priors: by leveraging representations that capture spatial structure, compositionality, and low-level visual cues, the VDM is able to approach these abstract tasks in way that improves upon traditional text-centric approaches. Preprint. Under Review. Table 1: ARC-AGI test performance. Following the official evaluation protocol Chollet et al. (2024), models are evaluated with two attempts per test input. We also report single-attempt results for comparability with commercial LLMs, which are only available under this setting. Model Accuracy (%) Two-attempts setting CogVideoX1.5-5B Qwen3-4B-Instruct-2507 Single-attempt setting CogVideoX1.5-5B Qwen3-4B-Instruct-2507 OpenAI o1-preview Anthropic Claude 3.5 Sonnet OpenAI GPT-4o Google Gemini 1.5 16.75 8.00 12.50 6.75 21.00 21.00 9.00 8.00 Figure 2: Venn diagram of ARC-AGI tasks showing those solved exclusively by each model and those solved by both. Input Output Input Output Input Output p E i r 5 - 5 . 1 d C 7 0 5 2 - r I 4 - 3 Q Input Prediction Input Prediction Input Prediction Figure 3: Qualitative results on ARC-AGI for problems 0607ce86, 7ee1c6ea, and f45f5ca7. 4.2 STRUCTURED VISUAL TASKS from each family: From this point onward, we focus on one representative model CogVideoX1.5-5B Yang et al. (2024) for video diffusion models and Qwen3-4B-Instruct-2507 Qwen3-4B-Instruct-2507 Team (2025) for language models. This pairing aligns model scale while contrasting pretraining modalities, allowing us to examine how different priors influence adaptability to visually grounded tasks. Preprint. Under Review. Figure 4:"
        },
        {
            "title": "Accuracy as a function of",
            "content": "training set size for CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 on five visual games."
        },
        {
            "title": "4.2.1 VISUAL GAMES",
            "content": "As part of our broader evaluation, we examine performance on diverse set of five visual games that span both puzzle-solving and board play. These tasks provide an additional perspective on how the models handle structured visual inputs and varying interaction styles. The puzzle-based tasks, Hitori 5x5 and two versions of Sudoku (standard one and Mini), focus on solving constraint-based problems in structured grids, where success depends on extracting spatial patterns and enforcing global consistency from local information. The board games, Connect 4 and Chess Mate-in-1, shift attention to game scenarios where the goal is to identify the winning move in given configuration. Together, these games cover range of visual layouts and structured objectives, complementing the other tasks explored in this study. Figure 4 presents model performance as function of training samples. CogVideoX1.5-5B demonstrates strong scaling behavior across most tasks, surpassing Qwen3-4BInstruct-2507 in four of the five games. Its advantage is particularly clear in Sudoku and Hitori, which rely on interpreting complex grid layouts and visual compositions. This supports the view that VDMs capture compositional features in visual data more effectively than LLMs, which are primarily optimized for language. The only exception is chess, where Qwen3-4B-Instruct-2507 performs better, likely reflecting the abundance of chess material in textual corpora that LLMs can partially internalize during pretraining Kuo et al. (2023). the number of 4.2.2 ROUTE PLANNING We evaluate route planning in 2D grid environments through two tasks: Maze and Shortest Path. In Maze, the model must navigate from the top-left to the bottom-right corner of grid. In Shortest Path, the objective is to connect two arbitrary points with the shortest possible route. For Shortest Path, we report two complementary metrics to assess model performance: Path Success Rate (PSR) The percentage of evaluation examples where the predicted path forms continuous connection between the source and target locations. Relative Path Length (RPL) For cases where valid path is produced, we compute RPL = Predicted Path Length Ground-Truth Shortest Path Length . This value may increase even as overall performance improves, since better models tend to predict good paths for more challenging cases, potentially constructing longer yet valid paths. For Maze, we evaluate in two settings: matched-scale (Base Maze) scenario, where both training and evaluation are conducted on 2121 mazes to study performance as function of training sample size; and generalization scenario, where models are trained on smaller 13 13 grids and tested on larger 21 21 grids to assess cross-scale generalization (Maze Generalization). Accuracy results are shown in Figure 5. For Shortest Path, additional metrics are reported in Table 2. The VDM consistently constructs valid paths with far fewer supervised examples, achieving up to tenfold reduction in data requirements in low-sample regimes, which underscores its stronger inductive biases relative to the LLM. Moreover, it demonstrates the ability to generalize much quicker from limited training on smaller mazes to larger, more complex ones. Preprint. Under Review. Figure 5:"
        },
        {
            "title": "Accuracy as a function of",
            "content": "training set size for CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 on Base Maze, Maze Generalization, and Shortest Path. CogVideoX1.5-5B Qwen3-4B Instruct-2507 Table 2: Relative Path Length (RPL) and Path Success Rate (PSR) for both models across training sample sizes for Shortest Path. a a a e h Samples 3 5 10 30 50 100 300 500 1000 3000 5000 CogVideoX1.5-5B Qwen3-4B-Instruct-2507 RPL RPL 1.005 1.089 1.060 1.020 1.028 1.038 1.013 1.025 1.017 1.040 1.007 1.019 1.005 1.043 1.005 1.026 1.000 1.016 1. PSR 0.115 0.160 0.245 0.670 0.645 0.870 0.940 0.985 0.990 0.990 1.000 PSR 0.015 0.060 0.205 0.530 0.605 0.710 0.795 0.870 Figure 6: Qualitative examples for Base Maze and Shortest Path tasks, after fine-tuning with = 300 samples. 4.2.3 CELLULAR AUTOMATA We evaluate the capacity of both models to capture complex spatial patterns in cellular automata (CA). Our study spans one-dimensional Elementary Cellular Automata (ECA) Wolfram (1984), foundational class of binary-state systems, as well as two-dimensional Life-like Cellular Automata, including Conways Game of Life Gardner (1970), defined by various birth and survival (B/S) rules. Additionally, we consider Langtons ant Langton (1986), deterministic agent-based system, where the task is to predict the complete grid state after steps of evolution. For the 1D ECA experiments, we evaluate four representative rules from each of Wolframs four complexity classes. We measure task completion as achieving an accuracy above fixed threshold δ = 0.9. Figure 7 reports the number of training examples required to reach this performance for each rule. Across these rules, both models show broadly similar behavior, with the VDM being better in some cases and worse in others, though overall it remains competitive with the LLM. In two-dimensional settings, clearer differences emerge (see Figures 9, 10). For Life-like cellular automata, the VDM reaches threshold accuracy with far fewer examples, and similar advantage is observed in Langtons ant. In the case of Langtons ant, the gap grows larger as the number of steps to be predicted increases, indicating that the VDM scales more effectively on tasks that demand long-range spatial planning."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "Our study shows that VDMs pretrained on spatiotemporal data adapt effectively to structured visual tasks with fewer training examples than comparable LLMs. This demonstrates how modalityaligned pretraining and inductive biases support transfer: VDMs excel in tasks requiring spatial structure and temporal transformation, while LLMs retain strengths in symbol rich domains. LargePreprint. Under Review. Figure 7: Number of training examples required to achieve δ 0.9 accuracy for selected 1D ECA rules (lower is better). Input Output CogVideoX1.5-5B Qwen3-4B Instruct-2507 3 2 / 3 / 2 Figure 8: Qualitative examples for Life-like cellular automata with rules B3/S23 and B2/S tasks, after fine-tuning with = 30 samples. scale pretraining on spatiotemporal data with representations aligned to visual structure thus emerges as promising venue for advancing visual intelligence. The implications are twofold. For researchers, our benchmarks provide evidence that pretraining pipelines designed around modality-specific structure can unlock new capabilities, offering path toward more data-efficient models. For practitioners, the inclusion of navigation-style tasks such as mazes and route planning suggests that pretrained VDMs may hold potential for downstream domains like planning, simulation, or robotics. However, validating these capabilities in more realistic, embodied environments remains an important direction for future work. Overall, these results underline that modality-aligned pretraining plays central role in advancing visual intelligence."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was supported as part of the Swiss AI Initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project ID a03 on Alps. Pablo Acuaviva, Aram Davtyan and Sebastian Stapf were supported by SNSF Grant 10001278. Some of the calculations were performed on UBELIX (https://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern. Preprint. Under Review. Figure 9: Number of training examples required to achieve δ 0.9 accuracy for selected Lifelike cellular automata rules (lower is better). Figure 10: Accuracy as function of trainsize for CogVideoX1.5-5B and ing set Qwen3-4B-Instruct-2507 on Langtons Ant with prediction horizon of 2,3,5 and 10."
        },
        {
            "title": "REFERENCES",
            "content": "Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2286122872, 2024. Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei A. Efros. Visual prompting via image inpainting. CoRR, abs/2209.00647, 2022. URL https://doi.org/10. 48550/arXiv.2209.00647. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. URL https://doi.org/10.48550/arXiv.2311.15127. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Francois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and Eric Schulz. Metain-context learning in large language models. Advances in Neural Information Processing Systems, 36:6518965201, 2023. Matthew Cook. Universality in elementary cellular automata. Complex Systems, 15(1):140, 2004. Martin Gardner. Mathematical games: The fantastic combinations of john conways new solitaire game life. Scientific American, 223(4):120123, 1970. Gemma Team. Gemma 3: Technical report. arXiv preprint arXiv:2503.19786, 2025. URL https: //arxiv.org/abs/2503.19786. Preprint. Under Review. Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 1270912720, 2024. Google DeepMind. Veo 3. https://deepmind.google/models/veo/, September 2025. URL https://deepmind.google/models/veo/. Accessed: 2025-09-23. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. CoRR, abs/2501.00103, January 2025. URL https://doi.org/10.48550/arXiv. 2501.00103. Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro Rezende, Yasaman Haghighi, David Bruggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, Marco Cannici, Elie Aljalbout, Botao Ye, Xi Wang, Aram Davtyan, Mathieu Salzmann, Davide Scaramuzza, Marc Pollefeys, Paolo Favaro, and Alexandre Alahi. Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. CVPR, 2025. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. CoRR, abs/2205.15868, 2022. URL https://doi.org/10.48550/arXiv.2205.15868. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2024. URL https://arxiv.org/abs/2307.13269. Liqiang Jing, Hardy Chen, Ehsan Aghazadeh, Xin Eric Wang, and Xinya Du. comprehensive analysis for visual object hallucination in large vision-language models. In Knowledgeable Foundation Models at ACL 2025, 2025. URL https://openreview.net/forum?id= Ya4mqbhDP4. Anssi Kanervisto, Dave Bignell, Linda Yilin Wen, Martin Grayson, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Tabish Rashid, Tim Pearce, Yuhan Cao, et al. World and human action models towards gameplay ideation. Nature, 638(8051):656663, 2025. Mu-Tien Kuo, Chih-Chung Hsueh, and Richard Tzong-Han Tsai. Large language models on the chessboard: study on chatgpts formal language comprehension and complex reasoning skills. 2023. Preprint, arXiv. Black Forest Labs. Flux.1-dev. https://huggingface.co/black-forest-labs/ FLUX.1-dev, 2025. Christopher G. Langton. Studying artificial life with cellular automata. Physica D: Nonlinear Phenomena, 22(1-3):120149, 1986. Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Wei-Long Zheng, Yewen Pu, and Kevin Ellis. Combining induction and transduction for abstract reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=UmdotAAVDe. Xiaoxuan Liao, Chihang Wang, Shicheng Zhou, Jiacheng Hu, Hongye Zheng, and Jia Gao. Dynamic adaptation of lora fine-tuning for efficient and task-specific optimization of large language models. In Proceedings of the 2025 International Conference on Artificial Intelligence and Computational Intelligence, pp. 120125, 2025. Preprint. Under Review. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/ 1405.0312. Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. CoRR, abs/2205.05638, 2022. URL https://doi.org/10.48550/arXiv.2205. 05638. Meta-AI. Llama 3.1 models. https://ai.meta.com/blog/meta-llama-3-1 and https://huggingface.co/meta-llama/Llama-3.1-8B, 2024. Arsenii Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=8ykyGbtt2q. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. URL https://arxiv.org/abs/2501.03575. Adam Polyak et al. Movie gen: cast of media foundation models. CoRR, abs/2410.13720, 2024. URL https://doi.org/10.48550/arXiv.2410.13720. Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham M. Kakade, Eran Malach, and Samy Jelassi. Lora soups: Merging loras for practical skill composition tasks. CoRR, abs/2410.13025, 2024. URL https://doi.org/10.48550/arXiv.2410.13025. Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, Lei Bai, Wanli Ouyang, and Ruimao Zhang. Worldsimbench: Towards video generation models as world simulators. CoRR, abs/2410.18072, 2024. URL https: //doi.org/10.48550/arXiv.2410.18072. quantum24. Chess puzzles 10k in pgn san. https://huggingface.co/datasets/ quantum24/chess_puzzles_10k_in_pgn_san, Curated collection of checkmate-in-1, -2, and -3 puzzles derived from the Lichess community puzzle database. Licensed under CC0 1.0. 2023. Qwen3-4B-Instruct-2507 Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for In International Conference on Computer Vision (ICCV) holistic indoor scene understanding. 2021, 2021. Preprint. Under Review. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. CoRR, abs/2208.12242, 2022. URL https://doi.org/10.48550/arXiv.2208.12242. Mong Yuan Sim, Wei Emma Zhang, Xiang Dai, and Biaoyan Fang. Can vlms actually see and read? survey on modality collapse in vision-language models. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2445224470, 2025. Ruben Villegas et al. Phenaki: Variable length video generation from open domain textual description. CoRR, abs/2210.02399, 2022. URL https://doi.org/10.48550/arXiv.2210. 02399. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. CoRR, abs/2503.20314, March 2025. URL https://doi.org/10.48550/arXiv. 2503.20314. Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 68306839, 2023a. Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, In-context learning unlocked for diffusion models. Advances in NeuMingyuan Zhou, et al. ral Information Processing Systems, 36:85428562, 2023b. Stephen Wolfram. Physica D: Nonlinear Phenomena, 10(1):135, 1984. https://doi.org/10. 1016/0167-2789(84)90245-8. URL https://www.sciencedirect.com/science/ article/pii/0167278984902458. Universality and complexity in cellular automata. doi: ISSN 0167-2789. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report. Technical report, Qwen Team, August 2025. URL https://arxiv.org/abs/2508.02324. Accessed: 2025-09-23. Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks? The Thirteenth International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id=BgYbk6ZmeX. Zhuoyi Yang, Shuhong Wang, Jing Li, Haoran Zhang, Junpeng Chen, Zeyu Wang, Qian Liu, Jinzhe Li, Yifan Du, Kun Zhou, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, and Chunhua Shen. Diception: generalist diffusion model for visual perceptual tasks. arXiv preprint arXiv:2502.17157, 2025. URL https://arxiv.org/abs/2502.17157. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. Preprint. Under Review. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302321, 2019."
        },
        {
            "title": "LIMITATIONS AND FUTURE WORK",
            "content": "Our study focuses on grid-based benchmarks such as ARC-AGI, ConceptARC, and synthetic puzzles. This controlled setup provides systematic framework for comparing models under equivalent conditions, offering clear interface through which LLMs can demonstrate visual understanding. While these benchmarks do not capture the full diversity of real-world challenges, they remain valuable for isolating and analyzing the role of modality-aligned pretraining in visual intelligence. The iterative nature of diffusion sampling also adds significant computational overhead. While some tasks can perform well with only few sampling steps Xu et al. (2025), complex domains such as ARC-AGI often require longer sampling schedules to maintain structural consistency and coherence. Fine-tuning VDMs remains computationally demanding even when using parameter-efficient methods such as LoRAs. Future work could explore modular and composable LoRA strategies Huang et al. (2024); Prabhakar et al. (2024), potentially reducing retraining costs while enhancing crosstask generalization. Another promising direction is to extend these models toward in-context task adaptation. Just as LLMs have evolved from next-token prediction in their pretraining phase to incontext question answering, VDMs could benefit from fine-tuning approaches that enable flexible, context-dependent adaptation. Beyond improving adaptability, understanding the mechanisms that give rise to visual intelligence in these models is an equally important research direction. Inspired by ongoing advances in mechanistic interpretability for LLMs, future work could aim to uncover how VDMs internally represent and manipulate concepts."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "We report here the detailed computational costs and hyperparameter settings used in our experiments. Tables 3 and 4 summarize the GPU hours required across different tasks, while Tables 5 and 6 provide the LoRA fine-tuning configurations for both VDMs and LLMs. Table 3: GPU hours required for ConceptARC across VDMs and LLMs. Reported hours are wallclock time and depend on hardware. VDM Model (GPU) Wan2.1-14B (H100) LTX-13B (H100) CogVideoX1.5-5B (RTX4090) LTX-2B (H100) Hours LLM Model (GPU) 100 Llama3.1-8B (H100) 95 Qwen3-8B (2RTX4090) 130 Qwen3-4B-Instruct-2057 (RTX4090) 40 Hours 80 100 135 Table 4: GPU hours required for ARC-AGI and Structured Visual Tasks. Reported hours are wallclock time and depend on hardware. ARCAGI Model (GPU) CogVideoX1.5-5B (RTX4090) Qwen3-4B-Instruct-2057 (RTX4090) Hours Structured Task Model (GPU) 450 CogVideoX1.5-5B (RTX4090) 475 Qwen3-4B-Instruct-2057 (RTX4090) Hours 1650 2000 To ensure reproducibility, we also include the fine-tuning hyperparameters for each model. The following two tables detail the LoRA, training, and optimizer configurations used for VDMs  (Table 5)  and LLMs  (Table 6)  . Note. LoRA ranks differ slightly across model families (VDMs use rank 64, whereas LLMs use rank 32). We verified that performance is largely insensitive to this setting: Qwen3 models with rank 64 Preprint. Under Review. Table 5: LoRA finetuning configuration for VDM experiments. Parameter LTX-13B LTX-2B CogVideoX1.55B Wan2.1-14B LoRA Configuration Rank Alpha Target modules 64 64 to q, to v, ff.net.0.proj, ff.net.2 to k, to out.0, 64 64 to q, to v, ff.net.0.proj, ff.net. to k, to out.0, 64 32 QKVO Training Configuration Seed Batch size Gradient accumulation steps Optimizer Configuration Optimizer Learning rate Scheduler Max grad norm 42 2 2 AdamW 2e-4 Linear 1.0 42 4 AdamW 2e-4 Linear 1.0 42 2 1 AdamW 1e-4 Constant 1.0 64 32 42 1 1 AdamW 1e-4 Constant 0. Table 6: LoRA finetuning configuration for LLMs used. Parameter LoRA Configuration Rank Alpha Dropout Target modules Qwen3-4B-Instruct-2507 Qwen3-8B LLaMA-3.1-8B 32 32 0 proj, proj, proj, proj, gate proj, up proj, down proj 32 32 0 proj, proj, proj, proj, gate proj, up proj, down proj 32 64 0.05 proj, proj, proj, proj, gate proj, up proj, down proj, lm head Model Setup Max sequence length Random seed Training Configuration Batch size per device Effective batch size Gradient accumulation steps Learning rate Scheduler Warmup steps Weight decay Generation Configuration Max new tokens Temperature Top-p Top-k 8192 3407 2 8 4 2e-4 Linear 5 0.01 4096 0.7 0.8 8192 3407 1 8 8 2e-4 Linear 5 0.01 4096 0.7 0.8 20 4096 3407 1 8 8 2e-4 Linear 5 0.01 4096 0.7 0.8 perform comparably to rank 32, and CogVideoX1.5-5B models with rank 32 match the reported rank 64 results. In both cases, we report the configuration that yielded stronger results in our initial trials. All reported results in the paper correspond to the configurations shown in the tables."
        },
        {
            "title": "B TASK DETAILS",
            "content": "For completeness, we provide additional explanations of the tasks considered in our evaluation. Each subsection introduces task family and highlights the key rules and objectives, we further provide examples on how the task is encoded into image and text. B.1 VISUAL GAMES B.1.1 HITORI 5X5 Objective: Eliminate cells so that each number appears at most once per row and column. Rules: 1. number must not be repeated in any row or column. 2. Shaded cells cannot be orthogonally adjacent. Preprint. Under Review. 3. All unshaded cells must form single connected component. We add an example of the task in Figure 11. Input Output t e p g I [ a s e e ] [3, 3, 1, 2, 5], [2, 3, 4, 4, 1], [3, 4, 4, 5, 3], [1, 3, 3, 4, 3], [4, 1, 5, 3, 2], [ ] [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 0, 0, 0], Figure 11: Example input-output pair for task Hitori. B.1.2 SUDOKU Objective: Fill the grid so that all constraints are satisfied. Rules: 1. Each row must contain all required digits without repetition. 2. Each column must contain all required digits without repetition. 3. Each subgrid must contain all required digits without repetition. We evaluate two variants: Mini Sudoku (4x4 with 2x2 subgrids, see Figure 12) and Sudoku (9x9 with 3x3 subgrids, see Figure 13). B.1.3 CONNECT 4 Objective: Place tokens to align four in row. Rules: 1. Players alternate dropping tokens into one of the seven columns. 2. token occupies the lowest available cell in the chosen column. 3. player wins by forming horizontal, vertical, or diagonal line of four tokens. We restrict evaluation to single-move winning scenarios, see Figure 14. B.1.4 CHESS MATE-IN-1 Objective: Deliver checkmate in single move. Rules: 1. All standard chess movement rules apply. 2. move is correct only if it results in an immediate checkmate of the opposing king. Preprint. Under Review. Input Output [ ] [3, 0, 0, 2], [2, 0, 0, 0], [4, 2, 3, 1], [0, 3, 0, 0], [ ] [3, 4, 1, 2], [2, 1, 4, 3], [4, 2, 3, 1], [1, 3, 2, 4], Figure 12: Example input-output pair for task Sudoku Mini. Input Output t e p g I t e p x o t e R m [ a s e t ] [6, 0, 0, 0, 2, 0, 0, 8, 0], [0, 3, 2, 7, 0, 0, 6, 0, 5], [7, 0, 0, 6, 4, 5, 1, 2, 0], [4, 0, 0, 2, 9, 0, 0, 0, 7], [8, 7, 0, 0, 3, 6, 2, 9, 0], [0, 0, 0, 0, 0, 0, 4, 3, 6], [9, 0, 7, 0, 6, 8, 0, 1, 0], [0, 4, 1, 0, 0, 0, 5, 0, 8], [0, 0, 0, 3, 1, 0, 9, 0, 2], [ ] [6, 5, 4, 1, 2, 3, 7, 8, 9], [1, 3, 2, 7, 8, 9, 6, 4, 5], [7, 9, 8, 6, 4, 5, 1, 2, 3], [4, 6, 3, 2, 9, 1, 8, 5, 7], [8, 7, 5, 4, 3, 6, 2, 9, 1], [2, 1, 9, 8, 5, 7, 4, 3, 6], [9, 2, 7, 5, 6, 8, 3, 1, 4], [3, 4, 1, 9, 7, 2, 5, 6, 8], [5, 8, 6, 3, 1, 4, 9, 7, 2], Figure 13: Example input-output pair for task Sudoku. To ensure the task is well defined, we filter scenarios so that they always correspond to white moves. The original dataset is extracted from quantum24 (2023), and an illustrative example is shown in Figure 15. B.2 ROUTE PLANNING We evaluate route planning in two-dimensional grid environments. The objective across tasks is to construct valid paths that connect designated start and goal locations under different structural constraints. We consider two tasks: Maze and Shortest Path. Preprint. Under Review. Input Output t e p g n a s e e [ ] [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [2, 0, 2, 2, 0, 0, 0], [1, 0, 1, 1, 1, 0, 0], [1, 0, 2, 2, 2, 1, 0], [1, 0, 1, 2, 2, 2, 1], [ ] [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [2, 0, 2, 2, 0, 0, 0], [1, 0, 1, 1, 1, 1, 0], [1, 0, 2, 2, 2, 1, 0], [1, 0, 1, 2, 2, 2, 1], Figure 14: Example input-output pair for task Connect4. B.2.1 MAZE Objective: Navigate from the start cell to the goal cell through grid containing blocked and open positions. Rules: 1. The agent starts at the top-left cell and must reach the bottom-right cell. 2. Movement is allowed only through open cells. 3. Allowed moves are up, down, left, and right (no diagonal moves). 4. valid solution is continuous sequence of moves from start to goal. We evaluate two scenarios: Base Maze: Training and evaluation on 21 21 grids. Maze Generalization: Training on smaller 13 13 grids and testing on larger 21 21 grids. We illustrate sample 21 21 maze in Figure 17, which serves as training and evaluation data in the Base Maze setting and as evaluation data in the Maze Generalization setting. Figure 16 shows sample 13 13 maze, which is used as training data in the Maze Generalization setting. B.2.2 SHORTEST PATH Objective: Connect two arbitrary points with the shortest possible route. Rules: 1. Start and goal cells are specified anywhere on the grid. 2. Movement is allowed only through open cells. 3. Allowed moves are up, down, left, and right (no diagonal moves). 4. valid solution is continuous path from start to goal with minimal length among all possible paths. We provide an example in Figure 18. Preprint. Under Review. Input Output t e n a s e a o t e R T [ ] [10, 0, 0, 11, 12, 9, 0, 10], [7, 0, 0, 0, 0, 0, 7, 7], [0, 0, 7, 0, 0, 7, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0], [1, 8, 0, 7, 0, 0, 0, 0], [0, 5, 0, 1, 0, 8, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 3, 0, 0, 6, 2, 0], [ ] [10, 0, 0, 11, 12, 9, 0, 10], [7, 0, 0, 0, 0, 5, 7, 7], [0, 0, 7, 0, 0, 7, 0, 0], [0, 0, 0, 0, 2, 0, 0, 0], [1, 8, 0, 7, 0, 0, 0, 0], [0, 0, 0, 1, 0, 8, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 3, 0, 0, 6, 2, 0], Figure 15: Example input-output pair for task Chess Mate in 1. B.3 CELLULAR AUTOMATA B.3.1 ELEMENTARY CELLULAR AUTOMATA (ECA) Elementary Cellular Automata (ECA) are one-dimensional binary-state automata defined on line of cells. Each cell ct {0, 1} at time updates based on itself and its two neighbors: ct+1 = (ct i1, ct i, ct i+1), where is specified by rule number between 0 and 255. For example, Rule 110 is encoded by the binary string 01101110, which maps the eight possible neighborhoods (ct i+1) to the next state: i1, ct i, ct Neighborhood Next state 111 0 110 1 101 1 100 0 011 010 1 001 1 000 0 We evaluate four representative rules from each of Wolframs classes Wolfram (1984), summarized in Table 7. Rule 110 is well known for its complex localized structures and universality Cook (2004). We show an example in Figure 19. B.3.2 LIFE-LIKE CELLULAR AUTOMATA Life-like CA generalize Conways Game of Life Gardner (1970), using binary cells on twodimensional grid. Each cell updates according to the number of live neighbors in the Moore neighPreprint. Under Review. Input Output t e p g I i n r t n a s e a o t e R T [ ] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 3, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [ ] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 3, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 4, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 4, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 4, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 4, 4, 4, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 4, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 4, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 4, 0, 1, 0, 4, 4, 4, 4, 4, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 4, 0, 1, 0, 4, 0, 0, 0, 4, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 4, 4, 4, 4, 4, 1, 1, 0, 2, 0, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], Figure 16: Example input-output pair for task Maze Small. Input Output [ ] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0], [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0], [0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0], [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ ] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 1, 1, 1, 0, 1, 1, 1, 0], [0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 4, 4, 4, 0, 1, 0, 4, 4, 4, 0, 4, 4, 4, 0, 1, 1, 1, 0, 1, 0], [0, 0, 0, 4, 0, 1, 0, 4, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 1, 0], [0, 1, 0, 4, 0, 4, 4, 4, 1, 1, 0, 1, 1, 4, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 4, 0, 4, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0], [0, 1, 1, 4, 4, 4, 0, 1, 1, 1, 0, 4, 4, 4, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 4, 4, 4, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 4, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 4, 4, 4, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 4, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 4, 0, 0, 0, 1, 0], [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 4, 0, 1, 1, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 4, 4, 4, 4, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Figure 17: Example input-output pair for task Maze. borhood (eight adjacent cells). In standard Game of Life (B3/S23): ct+1 i,j = 1 1 0 if cell is dead and has exactly 3 live neighbors (birth), if cell is alive and has 2 or 3 live neighbors (survival), otherwise (death). We consider several well-known Life-like variants. These rules, summarized in Table 8, capture diverse behaviors ranging from explosive growth to symmetry under inversion. We shown an example in Figure 20 of the basic Game of Life. Preprint. Under Review. Input Output t e p g n a s e e [ ] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0], [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], [0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ ] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], [0, 1, 1, 0, 1, 1, 1, 4, 4, 4, 2, 1, 1, 1, 0], [0, 1, 0, 0, 0, 1, 1, 4, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 0, 1, 4, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 0, 1, 1, 4, 0, 0, 0, 1, 1, 1, 0], [0, 1, 0, 0, 1, 0, 1, 4, 1, 1, 1, 1, 0, 1, 0], [0, 1, 0, 0, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 0, 1, 4, 1, 1, 1, 0, 1, 1, 0], [0, 3, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], Figure 18: Example input-output pair for task Shortest Path. Table 7: Representative Elementary Cellular Automata rules by Wolfram class. Class Class 1 Class 2 Class 3 Class 4 Rules 8, 32, 128, 160 4, 108, 170, 250 30, 45, 90, 150 110, 54, 62, 106 B.3.3 LANGTONS ANT Langtons ant Langton (1986) is an agent-based CA where single agent moves on binary grid. At each step: (x, y), d, g(x, y) (x, y), d, g(x, y), where (x, y) is the current cell, is direction, and g(x, y) {0, 1} is the cell state. 1. If g(x, y) = 0, turn right; if g(x, y) = 1, turn left. 2. Flip the cell color: g(x, y) = 1 g(x, y). 3. Move forward one step. After many steps, chaotic behavior gives way to repeating highway structure. To make the task predictable, we always start with the ant facing on the same initial direction and being on top of 0 cell. For an example see Figure"
        },
        {
            "title": "C ADDITIONAL QUALITATIVE RESULTS",
            "content": "C.1 ARC-AGI To further illustrate the complementary strengths of VDMs and LLMs, we include qualitative examples of ARC-AGI tasks. In some cases, the LLM enables it to find the correct solution, while the VDM fails. Examples of this behavior is shown in Figure 24. In contrast, there are tasks where both models succeed, suggesting that the underlying structure can be captured through either symbolic reasoning or visual pattern learning. One such case is given in Figure 25. Preprint. Under Review. Input Output i n r e I t e p x o t e R m n a s e e [ ] [1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ ] [1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1], [1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1], [1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1], [2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1], [2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2], [1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1], [1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1], [1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1], Figure 19: Example input-output pair for task ECA rule 101. Input Output [ ] [0, 1, 1, 0, 0, 0, 0, 1], [1, 1, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 0], [0, 1, 1, 1, 0, 0, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 1, 1, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0], [ ] [1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0], [0, 1, 0, 1, 0, 1, 0, 0], [0, 1, 0, 1, 1, 1, 1, 0], [0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 1, 1, 1, 1, 0], Figure 20: Example input-output pair for task Game of Life step 1. Finally, we highlight situations where only the VDM solves the task correctly (Figures 22 and 23). These examples emphasize how visual inductive biases allow the VDM to generalize in settings where symbolic reasoning alone appears insufficient. C.2 STRUCTURED VISUAL TASKS We include additional qualitative examples from structured visual tasks such as mazes, route planning, and cellular automata, complementing the quantitative results in the main text. Preprint. Under Review. Input Output Input Output Input Output Input Prediction Input Prediction Input Prediction p E i r 5 - 5 . 1 d C 7 0 5 2 - r I 4 - 3 Q Figure 22: Qualitative results on ARC-AGI for problems 60a26a3e, 62b74c02, 8a371977. Preprint. Under Review. Input Output Input Output Input Output Input Prediction Input Prediction Input Prediction p E i a 5 - 5 . 1 d C 7 0 5 2 - r I 4 - 3 Q Figure 23: Qualitative results on ARC-AGI for problems 2072aba6, 4aab4007, 5207a7b5. Preprint. Under Review. Input Output Input Output Input Output Input Prediction Input Prediction Input Prediction p E n r 5 - 5 . 1 d C 7 0 5 2 - r I 4 - 3 Q Figure 24: Qualitative results on ARC-AGI for problems ca8de6ea, d37a1ef5, e95e3d8e. Preprint. Under Review. Input Output Input Output Input Output Input Prediction Input Prediction Input Prediction p E n T 5 - 5 . 1 d C 7 0 5 2 - r I 4 - 3 Q Figure 25: Qualitative results on ARC-AGI for problems 575b1a71, 68b67ca3, 8ee62060. Preprint. Under Review. Input CogVideoX1.5-5B Qwen3-4B Instruct-2507 Figure 26: Representative examples for the Shortest Path task, showing ground truth inputs (left) and model predictions (center and right) after finetuning with = 300 samples. Preprint. Under Review. Input Output CogVideoX1.5-5B Qwen3-4B Instruct-2507 Figure 27: Additional qualitative examples for the Maze task, showing inputs, ground truth outputs, and model predictions after finetuning with = 300 samples. Preprint. Under Review. Input Output CogVideoX1.5-5B Qwen3-4B Instruct-2507 Figure 28: Additional qualitative examples for the Sudoku task, showing inputs, ground truth outputs, and model predictions after finetuning with = 1000 samples. Preprint. Under Review. Input Output CogVideoX1.5-5B Qwen3-4B Instruct-2507 Figure 29: Additional qualitative examples for the Hitori task, showing inputs, ground truth outputs, and model predictions after finetuning with = 100 samples. Preprint. Under Review. Input Output CogVideoX1.5-5B Qwen3-4B Instruct-2507 Figure 30: Additional qualitative examples for the Langton Ant (horizon 10) task, showing inputs, ground truth outputs, and model predictions after finetuning with = 1000 samples. Preprint. Under Review. Table 8: Life-like cellular automata variants evaluated. Rule (B/S) Name Day & Night B3678/S34678 Maze Seeds Life B3/S12345 B2/S B3/S2 Description Symmetric under inversion; complex dynamics Generates labyrinth-like, maze-like growth All live cells die each step; explosive expansion Sparse survival; promotes small, mobile clusters Input Output i n r e I t e p x [ ] [1, 1, 1, 1, 1, 0, 0, 1], [1, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 1], [1, 1, 1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 2, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 1, 0, 1, 0, 1, 0], [ ] [1, 1, 1, 1, 1, 0, 0, 1], [1, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 1], [1, 1, 1, 0, 0, 1, 0, 0], [1, 0, 0, 1, 1, 1, 1, 0], [0, 0, 1, 0, 0, 2, 1, 0], [0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 1, 0, 1, 0, 1, 0], Figure 21: Example input-output pair for task Langton ant step 2."
        },
        {
            "title": "E ARC FAMILY",
            "content": "Here, we include the comparison table for ConceptARC, by including finetuned LLMs (Qwen3-4BInstruct, Qwen3-8B, LLama3.1-8B-Instruct) and GPT-4 [IC]2 Moskvichev et al. (2023), as well as VDMs (CogVideoX1.5-5B, Wan2.1-14B, LTX-2B/13B). These additional results provide broader context and help reinforce the trends observed in the main text. See Table 9. The relatively lower performance of LTX compared to other VDMs may stem from its aggressive VAE compression, which can discard structural information important for ConceptARC. This reflects design tradeoff of the LTX models, aimed at enabling much faster video generation HaCohen et al. (2025). E.1 PITFALLS OF VISION LANGUAGE MODELS VisionLanguage Models (VLMs) promise to bridge the gap between visual perception and language by training on vast datasets of paired images and text. In principle, this multimodal pretraining should enable these models to solve visually grounded tasks more effectively than language-only models. To test whether this promise holds in practice, we evaluate representative VLM, Gemma4B Gemma Team (2025), on structured visual task: Sudoku. We fine-tune the same model with = 1000 samples under three configurations: text-only, imageonly, and combined imagetext; keeping all other settings fixed. The results in Table 10 reveal striking limitation: adding image input offers no measurable improvement, and the image-only 2Added for reference with commercial models, this case is directly IC and not our finetune approach. Preprint. Under Review. Table 9: Concept-wise overall accuracy across models. Best values are highlighted for VDMs or LLMs . Concept LTX-13B LTX-2B Wan2.1-14B CogVideoX1.5-5B AboveBelow TopBottom2D TopBottom3D HorizontalVertical Center FilledNotFilled CompleteShape InsideOutside ExtractObjects Count SameDifferent Order MoveToBoundary ExtendToBoundary Copy CleanUp Average Accuracy 0.30 0.23 0.27 0.13 0.33 0.30 0.20 0.27 0.07 0.40 0.23 0.03 0.17 0.20 0.20 0.43 0. 0.17 0.17 0.17 0.20 0.30 0.27 0.10 0.27 0.07 0.43 0.23 0.03 0.00 0.23 0.03 0.40 0.19 0.37 0.63 0.47 0.53 0.57 0.50 0.40 0.37 0.23 0.83 0.33 0.00 0.13 0.50 0.17 0.60 0.41 0.40 0.37 0.33 0.47 0.37 0.37 0.37 0.33 0.07 0.57 0.37 0.07 0.17 0.40 0.13 0.53 0.33 Qwen3-4B Instruct-2507 0.40 0.50 0.13 0.43 0.20 0.27 0.23 0.13 0.10 0.13 0.27 0.27 0.23 0.13 0.17 0.27 0.24 Qwen3-8B Llama3.1-8B GPT-4 [IC] 0.40 0.50 0.20 0.47 0.20 0.23 0.30 0.20 0.10 0.13 0.23 0.27 0.10 0.17 0.10 0.30 0. 0.17 0.37 0.17 0.33 0.13 0.20 0.13 0.13 0.03 0.17 0.27 0.10 0.17 0.10 0.10 0.27 0.18 0.23 0.23 0.20 0.27 0.33 0.17 0.23 0.10 0.03 0.13 0.17 0.27 0.20 0.07 0.23 0.20 0.19 variant performs worse than trivial baseline. This suggests that the model is unable to extract meaningful information from visual inputs, even when explicitly trained to do so. Table 10: Relative Accuracy and Accuracy on Sudoku. Model Text-only Combined imagetext Image-only Relative Accuracy Accuracy 0.79 0.78 0.12 0.06 0.06 0.00 To investigate why, we train the image-only model on simplified task: reconstructing the textual grid representation of its own image input rather than predicting Sudoku solution. With small training sets (n = 3, 5, 10), the model fails to interpret the images and instead memorizes training samples, reproducing them verbatim regardless of input  (Table 11)  . The model learns little about the underlying structure of the visual input. Table 11: Distribution of outputs on the test set exactly matching training samples for different training set sizes. Training Set Size 5 10 Sample Sample 1 Sample 2 Sample 3 Sample 1 Sample 2 Sample 3 Sample 4 Sample 1 Sample 2 Sample 3 Sample 4 Sample 5 Sample 6 Sample 7 Sample 8 Proportion Total Proportion 1.00 0. 0.96 0.385 0.010 0.605 0.490 0.030 0.335 0.135 0.100 0.010 0.030 0.005 0.015 0.170 0.615 0.010 This experiment exposes deeper issue: despite their multimodal pretraining, current VLMs struggle to extract structured information from images Jing et al. (2025); Sim et al. (2025). They appear to rely primarily on semantics and basic pattern recognition rather than true visual understanding. Furthermore, VLMs inherit many of the limitations of LLMs, such as reliance on text-based outputs, without gaining meaningful visual understanding ability. Preprint. Under Review. Because VLMs provide no measurable advantage over language-only models for these structured visual tasks, we focus on LLMs as the primary baseline. LLMs already demonstrate strong capabilities in structured prediction and symbolic manipulation, making them fair and informative comparison point for VDMs. This framing keeps the evaluation focused on model families that offer complementary strengths. RESULTS - FULL TABLES We provide the complete set of experimental results, which constitute the underlying data for the figures reported in the main paper. Table 12: Comparison of CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 accuracy on structured games. Missing values are shown as -. CogVideoX1.5-5B Chess-Mate-in-1 Connect 4 Hitori 5x5 3 5 10 30 50 100 300 500 1000 3000 5000 0.00 0.00 0.00 0.02 0.04 0.08 0.14 0.20 0.22 0.44 0.62 0.74 0.78 0.80 0.85 0.84 0.89 0.90 0.92 0.90 0.01 0.02 0.62 0.72 0.84 0.92 0.94 0.94 0.96 0.98 0.99 Sudoku Mini 0.22 0.36 0.65 0.78 0.90 0.91 0.90 0.94 0.91 0.95 0.96 Qwen3-4B-Instruct-2507 Sudoku Chess-Mate-in-1 Connect 4 Hitori 5x5 0.00 0.00 0.00 0.20 0.34 0.60 0.55 0.60 0.79 0.86 0. 0.00 0.02 0.04 0.13 0.15 0.24 0.38 0.44 0.56 0.03 0.05 0.08 0.38 0.38 0.69 0.71 0.69 0.76 0.78 0.82 0.00 0.00 0.02 0.02 0.10 0.28 0.57 0.64 0.86 0.94 0.96 Sudoku Mini 0.18 0.22 0.48 0.64 0.68 0.78 0.80 0.86 0.90 0.92 0.96 Sudoku 0.00 0.00 0.01 0.01 0.06 0.14 0.32 0.55 Table 13: Comparison of CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 accuracy on Life-Like Cellular Automata variants. Missing values are shown as -. 10 30 50 100 300 500 CogVideoX1.5-5B Qwen3-4B-Instruct-2507 Life B3S2 DayAndNight Maze 0.00 0.87 0.91 0.96 0.00 0.81 0.95 1.00 0.00 1.00 1.00 1.00 Seeds Game of Life Life B3S2 DayAndNight Maze 0.00 1.00 1.00 0.61 1.00 1.00 0.81 0.80 0.87 1.00 1.00 0.63 0.64 0.70 1.00 1.00 0.00 0.96 0.97 1.00 Seeds Game Of Life 0.75 0.78 0.63 1.00 1.00 0.63 0.64 0.73 1.00 1.00 Table 14: Comparison of CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 accuracy on Langtons Ant with respect to number of steps into the future. Missing values are shown as -. 3 5 10 30 50 100 300 500 1000 3000 5000 Step 2 0.18 0.23 0.67 1.00 1.00 1.00 CogVideoX1.5-5B Step 3 0.03 0.07 0.29 0.76 0.99 1.000 Step 5 0.03 0.04 0.06 0.25 0.41 0.88 1.00 1.00 1.00 Step 10 0.00 0.01 0.01 0.01 0.08 0.42 0.83 0.98 0.99 Qwen3-4B-Instruct-2507 Step 2 0.32 0.21 0.51 0.79 0.950 0.99 1.00 1.00 1.00 Step 3 0.03 0.04 0.19 0.46 0.58 0.910 1.00 1.00 1.00 Step 5 0.06 0.14 0.39 0.98 1.00 1.00 Step 10 0.00 0.010 0.01 0.12 0.21 0.47 0.71 0.93 EXPLORING GENERALIZATION OF I2I-TUNED VDMS While the main text emphasizes grid-structured visual prediction tasks, our framework extends naturally to broad range of image-to-image problems. In this section, we briefly explore its applicability to classical computer vision tasks. Few-shot adaptation functions both as an efficient tuning strategy Preprint. Under Review. Table 15: Comparison of CogVideoX1.5 and Qwen3-4B-Instruct-2507 accuracy on Maze and Shortest Path tasks. Missing values are shown as -. 3 5 10 30 50 100 300 500 1000 3000 5000 CogVideoX1.5 Qwen3-4B-Instruct-2507 Base Maze Maze Generalization Shortest Path Base Maze Maze Generalization 0.015 0.010 0.070 0.550 0.760 0.940 1.000 1.000 0.050 0.175 0.355 0.590 0.755 0.885 0.865 0.815 0.940 0.010 0.025 0.040 0.330 0.420 0.700 0.860 0.910 0.945 0.960 0.975 0.000 0.005 0.005 0.115 0.195 0.500 0.710 0.925 0.000 0.000 0.020 0.060 0.335 0.375 0.525 Shortest Path 0.010 0.010 0.050 0.155 0.320 0.500 0.640 0. Table 16: Comparison of CogVideoX1.5-5B and Qwen3-4B-Instruct-2507 accuracy on cellular automata rules grouped by Wolfram classes. Missing values are shown as -. CogVideoX1.5-5B Qwen3-4B-Instruct-2507 R32 0.49 0.51 0.67 0.82 0.98 Class R128 R160 0.13 0.29 0.20 0.28 0.48 0.32 0.87 0.85 0.93 0.99 Class 2 R8 0.06 0.10 0.19 0.72 0.81 0.97 0.98 R32 0.02 0.06 0.21 0.67 0.96 0.93 R128 R160 0.04 0.04 0.04 0.06 0.12 0.08 0.81 0.65 0.84 0.77 0.99 0.90 R4 0.72 0.82 0.90 1.00 R108 R170 R250 0.17 0.07 0.155 0.19 0.27 0.310 0.27 0.87 0.415 0.59 1.00 0.640 0.90 1.00 0.785 Class 3 R150 0.00 0.00 0.00 0.00 0.01 0.65 0.86 0.98 Class 4 R106 R110 0.00 0.00 0.00 0.09 0.57 0.97 1. R30 0.18 0.83 0.97 1.00 R45 0.00 0.00 0.00 0.07 0.53 1.00 R90 0.00 0.00 0.00 0.10 0.25 0.99 R54 0.00 0.00 0.01 0.54 0.99 1.00 1.00 R62 0.02 0.02 0.03 0.31 0.53 0.97 0.87 0.95 1.00 1. R108 R170 R250 0.85 0.99 0.98 1.00 1.00 0.52 0.86 1.00 0.99 0.47 0.82 0.90 1.00 R45 0.03 0.71 0.98 1.00 R54 0.31 0.78 0.94 1.00 R90 0.03 0.08 0.27 0.90 R62 0.13 0.79 0.93 1.00 R150 0.01 0.97 0.99 1.00 R106 0.18 0.63 1.00 1.00 R8 0.75 0.71 0.74 0.77 0.72 1.00 R4 0.71 0.76 0.74 0.85 0.93 R30 0.00 0.00 0.00 0.07 0.55 0.97 R110 0.00 0.00 0.00 0.42 0.90 1.00 1.00 3 5 10 30 50 100 300 3 5 10 30 50 100 300 3 5 10 30 50 100 300 500 3 5 10 30 50 100 300 Preprint. Under Review. and as probe of model competence: if the model succeeds with very few paired examples, it indicates that the underlying ability was already internalized during pretraining. We fine-tune CogVideoX1.5-5B, across tasks using between one and thirty paired examples, maintaining the same architecture, optimization schedule, and hyperparameters as in the main experiments. No auxiliary losses or task-specific modifications are introduced, isolating the contribution of pretrained knowledge. We explore this setup on several established datasets spanning diverse visual domains, including NYUv2 Nathan Silberman & Fergus (2012), ADE20K Zhou et al. (2017; 2019), ML-Hypersim Roberts et al. (2021), COCO 2017 Lin et al. (2014), and DreamBooth Ruiz et al. (2022). These benchmarks cover wide range of classical computer vision problems, from structured scene understanding to generative image transformation. Figure 31 illustrates that the model can capture geometric transformations under extreme few-shot conditions. We further show one-shot style transfer in Figure 32. We also qualitative show this framework can be used to solve some classical computer vision tasks. In Figure 34 we show examples after training with only = 30 samples for Binary Segmentation for dogs and Pose estimation for humans. Zoom 1-shot Vertical Flip 1-shot Horizontal Flip 1-shot Input Rotation45 3-shot Rotation90 3-shot Shearing 3-shot Figure 31: Geometric transformations learned in few-shot setting. Input is shown on the left, with 1-shot results on the top row and 3-shot results on the bottom row. Preprint. Under Review. Input Starry Night Pixel Art Cubism Ukiyo-e Figure 32: 1-shot style transfer results. The model adapts the input images to distinct artistic styles (Starry Night, Pixel Art, Cubism, and Ukiyo-e) using only single reference example. Input = 1 = 3 = = 10 t p o z l a J Figure 33: Qualitative results for different tasks (Inpainting, Colorization, Jigsaw) with different numbers of training examples. Preprint. Under Review. Input Binary Segmentation Prediction Input Pose Prediction Figure 34: Predictions after finetuning with = 30 samples for Binary Segmentation and Pose. Preprint. Under Review. Input Depth Prediction Figure 35: Predictions after finetuning with = 30 samples for Depth. Input Image Segmentation Prediction Figure 36: Examples from the Image Segmentation in 1-shot setting for Chamber. Preprint. Under Review. Input Segmentation Image Prediction m C a d d Figure 37: Examples from the Segmentation Image task in the 1-shot setting. Each environment corresponds to separate 1-shot training: for Chamber we train on one chamber and test on others, while for Coast and Badlands the same protocol applies within their category. Preprint. Under Review."
        }
    ],
    "affiliations": [
        "Computer Vision Group University of Bern Bern, Switzerland",
        "VITA Lab, EPFL Lausanne, Switzerland"
    ]
}