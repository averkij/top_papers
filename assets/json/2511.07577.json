{
    "paper_title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
    "authors": [
        "Yining Lu",
        "Wenyi Tang",
        "Max Johnson",
        "Taeho Jung",
        "Meng Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 7 5 7 0 . 1 1 5 2 : r Yining Lu * 1 Wenyi Tang * 1 Max Johnson 1 Taeho Jung 1 Meng Jiang ABSTRACT Existing retrieval-augmented generation (RAG) systems typically use centralized architecture, causing high cost of data collection, integration, and management, as well as privacy concerns. There is great need for decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves +10.7% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG."
        },
        {
            "title": "INTRODUCTION",
            "content": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) has been popular technique for enhancing large language models (LLMs) by providing them access to external knowledge sources during inference. Most existing RAG systems adopt centralized architecture in which single administrator manages all data sources, including collection, cleaning, de-duplication, and indexing. However, as the number of data sources grows, this centralized architecture encounters challenges in data management costs (Gao et al., 2024; Barnett et al., 2024) and privacy concerns (Addison et al., 2024). To address these challenges, we build decentralized RAG system (dRAG) that adopts the similar definition of data providers introduced by Hecking et al. (2025). Our dRAG system enables LLM services to retrieve documents directly from sources maintained and controlled by individual data owners, giving them the flexibility to decide what information to share and what retrieval policies to implement. However, building dRAG is not trivial because data sources from different owners often vary notably in quality, mak- *Equal contribution 1Department of Computer Science and Engineering, University of Notre Dame, USA. Correspondence to: Yining Lu <ylu33@nd.edu>, Wenyi Tang <wtang3@nd.edu>. Preprint. Under review. ing it impractical to treat all sources equally reliable. For example, some data sources may contain outdated, misinformation, or incomplete data (as shown in Figure 1). The existing decentralized RAG systems (Hecking et al., 2025; Xu et al., 2025; Zhou et al., 2025) typically assume all data sources are fully reliable, which is unrealistic in practice. To overcome this limitation, dRAG has novel scoring mechanism that dynamically evaluates the reliability of data sources from the quality of responses they help generate. These reliability scores enable dRAG to prioritize high-quality sources during retrieval, thereby improving both retrieval accuracy and response quality in noisy data environments. Yet these reliability scores cannot be trusted if managed by centralized entity, as centralized administrator could easily manipulate scores to favor certain data sources, creating single point of failure and compromising system integrity. So, we leverage blockchain in dRAG to secure the reliability scores and relevant logs. The scoring mechanism is implemented within smart contract, whose execution and verification are collectively performed by the majority of nodes in the blockchain network. This ensures that all updates to the scores are executed properly, creating tamper-proof and transparent scoring records that all participants can verify without relying on central authority. dRAG can benefit many domains and applications where Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain effective dRAG system should demonstrate robustness in such unreliable environments by outperforming its centralized counterpart (see Centralized (unreliable) and Decentralized (unreliable) lines in Figure 1) and exhibit consistent growth as more user queries are processed. Remarkably, our results also show that dRAG can even approach the performance of centralized system with fully reliable data sources (see Centralized (reliable) with no injected noise), an ideal configuration that is difficult and costly to maintain in real-world settings. As previously discussed, centralized systems require administrators to invest substantial effort in data management to ensure data source reliability, whereas dRAG eliminates this burden while achieving comparable performance. In summary, our contributions are threefold: We introduce reliability scoring mechanism for decentralized RAG systems that dynamically evaluates source reliability to improve retrieval and answer quality. We build our dRAG on blockchain to establish decentralized, transparent, and trustworthy scoring management for data source reliability, using smart contracts to ensure full traceability of score updates. We open-source our system to facilitate future research. Through controlled experiments, we demonstrate that dRAG can improve retrieval and response quality over time, outperforming centralized systems under unreliable data environments and achieving performance comparable to those with ideal, fully reliable data sources."
        },
        {
            "title": "2 RELATED WORK AND BACKGROUND",
            "content": "2.1 Centralized Retrieval Augmented Generation Traditional RAG systems rely on centralized architectures in which knowledge bases and retrieval mechanisms are managed under unified control (Karpukhin et al., 2020; Lewis et al., 2020; Li et al., 2022; Gao et al., 2024). This centralized design introduces several fundamental limitations that hinder its scalability and practical deployment. Centralized RAG architectures face inherent scalability limitations stemming from their single-index design and monolithic control (Wang et al., 2024; Douze et al., 2025). Even with techniques such as query routing and hierarchical retrieval (Xu et al., 2025; Helmi, 2025), such systems struggle to efficiently scale as data volumes and domains increase. Beyond scalability, centralization leads to data management complexity across diverse data sources (Chong et al., 2025), high privacy risks (Zeng et al., 2024; 2025a), and governance challenges related to access control and compliance (Jayasundara et al., 2024; Zeng et al., 2025b). By contrast, our dRAG inherently mitigates these issues by distributing Figure 1. Overview of centralized RAG system (left) and our dRAG system (right), with their performance comparison (bottom) on LLAMA-3.2-3B-INSTRUCT. In the unreliable data environments, our dRAG significantly outperforms the centralized system with increasing query exposure, approaching the performance upper-bound from the centralized system achieved in fully reliable data environments. data cannot be centrally managed, such as regulated enterprises (e.g., healthcare and public sector), cross-institutional consortia, multi-tenant platforms, and open-source communities. dRAG offers reliable retrieval augmentation across independently owned sources through two mechanisms: (1) client-side reliability scoring for response quality, and (2) blockchain-based reliability management interface ensuring transparency and trustworthiness. dRAG can be easily deployed: data providers expose standards-compliant retrieval endpoint and register with the smart contract registry, while users install our lightweight, open-sourced client library, which connects to the deployed smart contract onchain for retrieval and reranking. Furthermore, the continuously updated reliability scores on blockchain provide each data owner with actionable feedback, encouraging proactive monitoring and improvement of their data sources. To demonstrate the effectiveness of dRAG, we simulate unreliable data environments in which multiple data sources provide Wikipedia documents of varying reliability to answer questions from the Natural Questions dataset (Kwiatkowski et al., 2019). Specifically, we inject noise into documents by replacing ground-truth tokens with incorrect answers, thereby polluting them to varying levels of reliability. An Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain retrieval across independent data providers for better scalability and data management. 2.2 Decentralized Retrieval Augmented Generation Recent studies have explored decentralized solutions to these limitations through different technical strategies (Hecking et al., 2025; Yang et al., 2025; Zhou et al., 2025; Chakraborty et al., 2025). The External Retrieval Interface framework aims to decentralize the RAG system from software perspective by decoupling retrieval, augmentation, and generation components through standardized protocols, enabling data providers to maintain full control and keep data at its source (Hecking et al., 2025). Another notable approach is federated learning, which enables collaborative training while preserving data locality. For example, FedRAG demonstrates privacy-preserving training of RAG components without raw data exchange (Mao et al., 2025). De-DSI applies federated learning to train differentiable search indexes without centralized coordination (Neague et al., 2024). Unlike these approaches, which require complex training and coordinated parameter sharing, our dRAG avoids training overhead by operating entirely at inference time and emphasizes traceability through blockchain logging of source contributions, providing lightweight yet trustworthy solution for quality control. Blockchain is considered natural choice for building decentralized applications, including decentralized RAG systems. It helps establish decentralized, immutable ledger through cryptographic hashing and peer-to-peer consensus mechanisms, ensuring data integrity and transparency without the need for central authority. Smart contracts are Turing-complete programs deployed on the ledger. It provides guaranteed execution integrity, as its code and state are verified and validated by the entire decentralized network via the underlying consensus mechanism, as long as the majority of the network is honest. The consensus ensures that the contract logic executes exactly as programmed, with the resulting state change being recorded on the tamperproof blockchain ledger only after achieving network-wide agreement. This allows the development of robust, transparent, and tamper-proof decentralized applications (dApps) that run on decentralized network with public verifiability, rather than relying on single/institutional trust. Existing blockchain-based RAG approaches employ consensus protocols for decentralized knowledge validation. For example, some frameworks introduce domain-specific validator networks (e.g., using distributed hash tables) (Yu & Sato, 2024) or permissioned blockchains where expert nodes must review and agree on new knowledge before it is integrated (Andersen et al., 2025). Unlike these mechanisms that rely on complex blockchain protocols to coordinate knowledge propagation, our approach uses smart contracts in more lightweight manner. It leverages smart contracts to dynamically evaluate and prioritize data sources based on their reliability, eliminating the need for complex protocols with specialized validator committees or centralized content reindexing. Essentially, we provide transparent, tamper-proof trust layer on top of decentralized retrieval, enabling robust performance even when some sources are unreliable."
        },
        {
            "title": "3 SYSTEM ARCHITECTURE",
            "content": "3.1 System Components Figure 2. System overview of dRAG. Our dRAG system can be abstracted into three components: Decentralized Data Sources, LLM Service, and Decentralized Blockchain Network (as shown in Figure 2). Decentralized Data Sources. The decentralized data sources in dRAG are maintained individually by each data owner, and each may provide standard-compatible API for the LLM service to query for documents. Decentralized Blockchain Network. The decentralized blockchain network serves as the infrastructure for dRAG to evaluate the reliability of each decentralized data source (detailed in Section 4). The scoring management is implemented via smart contract on the Ethereum blockchain, providing trustworthy external evaluation of decentralized data sources. The smart contract, which maintains the scoreboard for all data sources, is executed by public, peer-topeer blockchain network. The integrity of its execution is guaranteed by the decentralized nature of blockchain. LLM Service. The LLM service responds to user queries, interacting with other system components, including the smart contract on the blockchain, and the decentralized data sources to generate the answer. Notably, multiple LLM services can share the same dRAG infrastructure to serve different users by connecting and providing feedback to the same smart contract on-chain. 3.2 System Workflow Figure 2 presents the system overview of dRAG. Additional implementation details are provided in Section 5. We assume the LLM Service has direct access to individual data Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain sources, allowing it to retrieve the most relevant documents through standard-compliant interfaces. Similar to other RAG systems, users submit queries to the LLM service to obtain answers from dRAG. The LLM service will first sample subset of data sources based on the reliability evaluation from Section 4.1, and route the query to them. The sampled data sources then use the locally hosted retriever to find the documents most relevant to the query and return them to the LLM service. The data source sampling process can be customized to meet the query and user needs. In dRAG, we present an effective sampling process based on parameter that quantifies the usefulness of each data source in improving the LLM performance, as described in Section 4.2. After retrieving the documents from data sources, the documents will be reranked and concatenated with the query before being fed to the LLM. The reranking process incorporates reliability scores to identify the most reliable documents for answering the query. The LLM then generates an answer, and the user provides feedback on it to update the reliability scores on the blockchain. While our system accepts direct user feedback, we also support automatic evaluation via exact matching to ground-truth answers (if available). We provide detailed discussion of reliability scoring, data source sampling, and reranking in the next section."
        },
        {
            "title": "RELIABILITY",
            "content": "Algorithm 1 Reliability-Guided RAG 1: Inputs: user query q; data sources = {si} 2: Hyperparameters: number of retrievers ; per-source fetch ; reranked top-K 3: Initialization: usefulness Ui and reliability Ri for source si 4: for each query do 5: 6: Sample sources {si} proportionally to Ui Retrieve documents from each selected source with size Rerank with Equation 4; keep top-K documents X1:K Generate response using X1:K For each document X1:K and sentence d, compute (x) via either Equation 1 or Equation 2 Compute document score: (d) Aggxdf (x) Update Ui, Ri for each source si with documents in X1:K using (d) following Equation 3 7: 8: 9: 10: 11: 12: end for We introduce how dRAG evaluates (4.1) and updates (4.2) data source reliability to improve retrieval and response quality. We present the whole procedure in Algorithm 1. 4.1 Sentence Importance Evaluation Uddin et al., 2025). For instance, Wikipedia article documenting the 2020 U.S. presidential election provides limited value when addressing queries about the 2024 election. We consider data source reliable if it consistently provides informative content that improves LLM generation across diverse range of user queries. To quantify this notion of reliability, we first compute the importance of sentences from the retrieved document to LLM generation, which can be evaluated under two scenarios. Ground-truth answers are unavailable. We estimate the importance of each sentence in the retrieved document using Monte-Carlo Shapley (MC-Shapley) values (Goldshmidt & Horovicz, 2024). Formally, the reliability score is defined as the expected marginal contribution of to the model output across sampled subsets of sentences: fshapley(x) = Esd{x} (cid:2)F (s {x}) (s)(cid:3), (1) where () is the utility function, which in our setting is computed as the cosine similarity between the baseline response (generated using the full retrieved document) and the response conditioned on the sampled subset or {x}. This formulation quantifies the marginal contribution of each sentence to the models output, featuring greater generalizability but with increased computational overhead. Ground-truth answers are available. We adopt an information-theoretic formulation in which each sentence is treated as potential rationale. The informativeness of rationale, denoted as finfo(x), is defined as the conditional Vinformation (Hewitt et al., 2021) capturing the reduction in model predictive uncertainty conditioning on the rationale: finfo(x) = HV (y q) H(y x), (2) where and are the ground-truth answer and query. We train evaluators following RORA (Jiang et al., 2024) to estimate H() as multivariable predictive V-entropy. This approach improves computational efficiency but reduces generalizability to out-of-domain sentences. 4.2 Data Source Reliability Update Given sentence-level importance scores {f (x) : d} for document retrieved from data source si, we aggregate these scores to obtain document-level reliability estimate (d). The aggregation strategy depends on the score scale and the corresponding evaluation method: for Shapleybased scores, we compute the arithmetic mean (d) = 1 xd fshapley(x), while for entropy-based scores, we take the maximum (d) = maxxd finfo(x). (cid:80) The utility of retrieved documents varies across different information needs and temporal contexts (Qian et al., 2024; We maintain cumulative reliability score Ri for each data source i, which is updated based on user feedback regarding Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain the correctness of the generated response y. The update rule is straightforward: for document belonging to source si, Ri (cid:40) Ri + (d), Ri (d), if is correct if is incorrect (3) This additive update scheme allows the system to accumulate evidence about source reliability over multiple interactions, rewarding sources that contribute to correct answers while penalizing those with errors. While reliability measures source accuracy, our experiments show that LLMs occasionally generate responses without using the retrieved documents. In such cases, source may be inherently reliable yet fail to contribute usefully to the generation. To address this gap, we introduce complementary usefulness score Ui that quantifies the extent to which documents from source are actually used by the LLM to generate responses. We evaluate whether generated responses can be grounded in the retrieved documents and update Ui accordingly: (1) If the response is not grounded, we penalize the usefulness score as Ui Ui (d), while leaving Ri unchanged. (2) If the response is grounded, we reward usefulness Ui Ui + (d) and update reliability score Ri following Equation 3. We provide empirical justification for Ui in Section 6.2. Reliability-Aware Retrieval and Reranking. We incorporate these scores at different RAG stages to optimize retrieval quality. Following the standard RAG pipeline (Karpukhin et al., 2020; Li et al., 2025), we employ dense retrieval system implemented with FAISS (Douze et al., 2025) for initial candidate selection, followed by neural reranker for final document ordering. Specifically, at the retrieval stage, we use usefulness scores Ui to sample dense retrievers and their corresponding data sources. At the reranking stage, we leverage source reliability Ri to refine the final document ordering. For candidate document retrieved from data source si, its final reranking score is: scorefinal = (1 α) scorererank + α σ(Ri) (4) where scorererank denotes the original reranking score and σ() is normalization function (implemented as minmax normalization in our case). The hyperparameter α [0, 1] controls the influence of reliability on the final ranking."
        },
        {
            "title": "SYSTEM WITH SMART CONTRACT",
            "content": "In this section, we describe the detailed system implementation of dRAG with the reliability evaluation from Section 4. Figure 3. Detailed Workflow of dRAG, with feedback log to update the reliability scores. For brevity, state information linking the update to genuine query is omitted in the example. Ground truth is optional for sentence importance evaluations; if it is unavailable, user feedback (True/False) will be used to update the reliability scores. 5.1 On-Chain Trustworthy Reliability Management with Smart Contract By distributing the knowledge base and eliminating the need for centralized database manager, dRAG requires decentralized management of source scores. Since the reliability scores are used to evaluate the trustworthiness of data sources, they must be managed on public bulletin board where no single party can tamper with them. Additionally, the updating of these scores should allow public auditing to ensure transparency. This design makes the scoring mechanism in dRAG more trustworthy and transparent. dRAG utilizes blockchain as an infrastructure for decentralized score management and auditable logging. smart contract is deployed on the blockchain to facilitate the implementation of basic scoring functions, including initialization, retrieval, and update. Every update of the scores will leave log on-chain, with traceable information that showcases the reason for the modification. The blockchain serves as public bulletin board to keep the scores, and the smart contract provides the interface to manage and update the scores with decentralization-based execution integrity. Score Initialization. In dRAG, every data source owner will be registered as an account with an ECDSA (Elliptic Curve Digital Signature Algorithm) compatible public/secret key pair (pk, sk) on the blockchain. Every owner of the data source needs to use the deployed dRAG contract to initialize scoring record for their data source on the blockchain. The contract will create corresponding score records for each data source, each with an initial score, which will be stored as state variable on-chain and be publicly accessible. Reliability Score Update. After obtaining responses from Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain Algorithm 2 Trustworthy Reliability Update with Signature 1: Inputs: State digest state, signatures sig[1..n], source IDs Source[1..n], updated reliability scores R[1..n], updated usefulness scores [1..n], feedback info info if score record for Source[i] does not exist then abort with InvalidUpdateCount 2: current timestamp() 3: if lengths of sig, Source, R, differ then 4: 5: end if 6: for = 1 to do 7: 8: 9: 10: 11: 12: 13: 14: end if pk scoreRecords[Source[i]].sourceAddress if ECDSA.verify(state, sig[i], pk i) then abort with InvalidSignature abort with DataSourceNotExists end if Update scoreRecords[Source[i]]: reliabilityScore R[i], usefulnessScore [i], timestamp Emit ScoreRecordUpdated event for Source[i] with new scores, t, and info 15: 16: end for the LLM in dRAG, users can provide feedback on responses along with ground-truth answers, and update the reliability based on their contribution to the reliability records on the blockchain via smart contract. The LLM service will first perform sentence-level importance evaluation (described in Section 4) and calculate score changes based on the evaluation. Then, the LLM service needs to include the evaluation in feedback log transaction and submit it to the blockchain network so the deployed smart contract can update the scores for each source. To prevent arbitrary updates to reliability records, the smart contract will perform basic verification of the feedback log to ensure that updates can be traced back to legitimate query sent to the data sources. Trustworthy Reliability Update with Signature. Algorithm 2 presents the pseudocode for the signature verification and reliability score update logic of our smart contract. We also provide deployed smart contract instance of this algorithm at the public testnet Sepolia at 2at6kd.short.gy/4R9Tbm. When the users query is broadcast to the sampled data sources to retrieve potentially relevant documents, the LLM service also generates state information and attaches it to the query. The state is digital digest (secure hash) that includes the query and the reliability scores of all sampled data sources, which can be viewed as summary binding the current query to the current state of all sampled data source scores. The sampled data sources can obtain the same information (the query and the reliability scores of all data sources) to generate and verify if the state matches their current view. When returning the retrieved documents to the LLM service, each sampled data source must create digital signature sig using its private key sk, indicating its consent to acknowledge and confirm the state. This signature will be considered as consent to allow the LLM service to update the reliability score of the data sources. The LLM service creates the feedback log, and the signatures obtained from each data source will be included as part of the feedback log transaction updating the scores, as shown in Figure 3. The smart contract will retrieve all relevant reliability records associated with the owners public keys and perform batch verification of all signatures against the information included in the transaction, including the query prompt, the importance evaluation summary, and the reported scoring state hash from the data sources. The batched verification of the above information ensures that the feedback log transaction, which updates the scores, can be traced back to specific user query. For further security and tracing purposes, like preventing replay attacks by reusing the signature to maliciously update the data source reliability scores, one may consider leaving log recording the query and sampled data source list (marked as unused) on-chain before the LLM service broadcasts the query and retrieves the documents from data sources. When the data sources create the signature, the state should also include the query log. The first feedback on the query marks the query log as used and invalidates all subsequent updates that attempt to exploit the same query. 5.2 Auditing Reliability Updates for score update occurs any data Whenever the deployed smart contract will emit source, ScoreRecordUpdated event, including the necessary information to trace the update back to the original query included in the feedback log (as shown in line 14 of Algorithm 2). The event data will be part of the blockchain but not directly accessible to the smart contract, helping reduce the networks computational overhead. The event data can be queried and retrieved by any blockchain gateway node, linked to the specific transaction in the blockchain, allowing anyone to audit the entire history of data source reliability updates and ensure the update can be traced back to genuine query. This information can be used to reproduce the query for further investigation. Additionally, our system can also be extended to support asynchronous feedback and score updates from the users. This could be useful in cases where ground truth for certain queries is not immediately available, allowing users to provide evaluations and update scores at later time. Specifically, users still receive signatures from selected data sources and obtain responses from the LLM. The sentence-level importance evaluation will be conducted without ground truth using Equation 1. Users can submit Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain polluted environments, while ideally approaching the performance upper bound set by centralized systems in unpolluted conditions. We evaluate our dRAG system on LLAMA3.2-3BINSTRUCT and LLAMA-3.1-8B-INSTRUCT (Grattafiori et al., 2024). We train our evaluators using invariant learning with T5-base (Raffel et al., 2020) to compute Equation 2, following the RORA approach (Jiang et al., 2024). We use the following hyperparameters in our main experiments (for results shown in Table 1): reliability weight α = 0.5 for token-level pollution environments and α = 0.2 for document-level pollution environments.2 We set the sampling number to = 5, top-K = 2, and per-source fetch = 3. For LLM inference, we consistently set the temperature to 0 for better reproducibility. We initialize both usefulness and reliability scores to 10. We deploy the smart contract used in dRAG on the Sepolia network, public Ethereum testnet, together with an easyto-use package tool to test and evaluate the core functions of dRAG. We provide detailed cost evaluation for maintaining the dRAG infrastructure in Section 6.3. 6.1 Centralized versus Decentralized RAG System dRAG outperforms centralized RAG system in unreliable data environments. Figure 5a presents performance comparison between centralized and decentralized RAG systems using LLAMA-3.1-8B-INSTRUCT on token-level polluted data (left in Figure 4). Combined with the results shown in Figure 1 using LLAMA-3.2-3B-INSTRUCT, these findings demonstrate that dRAG exhibits self-improvement capabilities by progressively learning to prioritize the most reliable data sources across user queries. The underlying mechanism for this improvement is illustrated in Figure 5b. Initially, all sources start with identical reliability scores. However, data source consistently receives positive feedback as its high-quality documents are retrieved and contribute to generating correct answers. As result, its reliability score increases while other sources remain largely unchanged or slightly decline, making more likely to be retrieved in subsequent queries. This mechanism explains key behavioral pattern observed in dRAG when deployed in unreliable data environments: the system initially performs at lower-bound level, equivalent to centralized RAG system using polluted data, but steadily improves over time as reliable sources accumulate positive feedback and begin to dominate retrieval decisions. To further validate these improvements, we analyze the usage distribution of each data source across queries. Figure 5c 2The lower α in document-level settings accounts for the nonoverlapping document distribution, where single high α value would bias retrieval toward completely irrelevant data sources. Further analysis is provided in Section 6.1. Figure 4. Statistics of two simulated unreliable data environments. Left: Token-level pollution shows the percentage of ground-truth tokens replaced with random tokens across six data sources (A-F), where different sources contain overlapping ground-truth documents. Right: Document-level pollution shows how 3197 groundtruth documents are divided into six sources (A-F) and mixed with polluted documents to create varying reliability levels, with no document overlap between sources. feedback with non-determined importance report to the blockchain. Once ground truth becomes available, users can invoke the smart contract to report it on-chain and then update the reliability scores accordingly."
        },
        {
            "title": "6 EXPERIMENTS AND RESULTS",
            "content": "To demonstrate the effectiveness of dRAG, we evaluate it across two environmental configurations, each containing 6 data sources with varying pollution levels {0%, 20%, , 100%}, where = 0% indicates no pollution (i.e., the data source provides ground-truth documents for the original Natural Questions dataset (Kwiatkowski et al., 2019)), and higher values of indicate increasingly unreliable documents within the data source. For building these two environments, we (1) inject noise into the documents by replacing ground-truth tokens with randomly sampled tokens based on the given pollution level (tokenlevel pollution); (2) partition the original dataset into 6 disjoint data sources, where each source is populated with randomly sampled polluted documents, such that all sources maintain equal length and achieve their specified pollution levels without overlap (document-level pollution).1 Figure 4 shows statistics of these two data environments. For polluted data environments, an effective reliability evaluation should successfully identify and prioritize the least polluted (i.e., most reliable) data sources throughout the process, and therefore well-designed dRAG system should outperform its centralized counterpart under 1We use the words polluted and unreliable interchangeably hereafter. Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain (a) System Performance Comparison (b) Data Source Reliability (c) Data Source Usage Figure 5. (a) Performance comparison between dRAG and centralized RAG systems on LLAMA-3.1-8B-INSTRUCT across reliable and unreliable data environments; (b) Evolution of reliability score Ri (Equation 3) throughout the query process; (c) Percentage distribution of data sources (A-F) across sequential query bins of 100 queries each. Models Pollution Strategy Centralized (Unreliable) dRAG (Unreliable) MC-Shapley RORA Centralized (Reliable) LLAMA-3.2-3B-INSTRUCT LLAMA-3.1-8B-INSTRUCT Token-level Document-level Token-level Document-level 30.56 28.74 37.37 34.38 42.69 30.60 46.58 37.37 49.90 29. 43.30 37.24 44.24 49.03 Table 1. Average accuracy of centralized and decentralized (dRAG) systems on reliable and unreliable data environments. For dRAG, performance is reported after 500 warmup queries to ensure reliability score convergence. Reliability scores are updated using sentencelevel importance estimation via MC-Shapley (Goldshmidt & Horovicz, 2024) or RORA (Jiang et al., 2024) methods. Unreliable data was polluted using token-level or document-level strategies. confirms that as dRAG digests more queries (organized into bins of 100), it learns to prioritize more reliable sources. Specifically, the usage percentage of data source increases substantially from approximately 40% in the first query bin to over 80% in later bins (bins 6-13). This increase is accompanied by corresponding decrease in the utilization of less reliable data sources B, C, and D, whose combined usage drops from roughly 60% to under 20%. These results highlight that dRAG effectively learns to distinguish data quality and progressively favors the least polluted sources, thereby improving retrieval performance over time. dRAG achieves performance comparable to centralized systems running in ideal, reliable data environments. Table 1 presents the average accuracy of centralized and decentralized systems across various models, scoring methods, and data environments. Our findings demonstrate that dRAG consistently outperforms the centralized system under unreliable data environments across all experimental configurations. More remarkably, under conditions of token-level pollution, dRAG achieves performance levels comparable to those of centralized systems operating on fully reliable data sources. This aligns with the intuition that prioritizing high-quality data sources during retrieval fundamentally enhances performance, with the upper bound constrained by the models inherent capacity to process fully reliable data. However, we observe notable performance disparity between the two pollution strategies. Specifically, unreliable environments constructed using document-level pollution yield lower accuracy compared to those using tokenlevel pollution. This difference stems from different document distributions yielded by two strategies (Figure 4). Document-level pollution creates data sources that cover only subset of the original documents (1066 out of 3197 to answer queries). Consequently, when dRAG promotes the most reliable sources (e.g., A) during reranking, it may inadvertently favor irrelevant documents from these sources simply based on their high reliability scores. In contrast, token-level pollution creates more balanced data environment where all data sources maintain identical document coverage to the original corpus, with reliability varying at the granular token level rather than through selective document inclusion. We leave the challenge of handling these incomplete and unreliable data sources to future work. 6.2 Sensitivity Analysis and Score Validation Considering that dRAG uses reliability weights α for balancing rerank and reliability scores for each data source, and uses the number of retrievers for sampling from the usefulness score, we study dRAGs sensitivity towards the choice of these two hyperparameters. Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain (a) Reweight Weight Sensitivity (b) Retriever Number Sensitivity (c) Entropy of Polluted Data Sources Figure 6. (a-b): Sensitivity test results of dRAG on reliability weight α and number of retrievers . Results are obtained from token-level polluted environment where dRAG uses the MC-Shapley scoring method with LLAMA-3.2-3B-INSTRUCT. The indicates the baseline performance of centralized system on unreliable data (30.56, from Table 1), and the indicates the upper-bound performance of centralized system on reliable data (44.24). (c): Entropy in sentence importance estimation across different pollution levels. Question: Who got the first Nobel prize in physics? MC-SHAPLEY: The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen, of Germany, who received 150,782 SEK, which is equal to 7,731,004 SEK in December 2007. John Bardeen is the only laureate to win the prize twicein 1956 and 1972. Maria Sklodowska-Curie also won two Nobel Prizes, for physics in 1903 and chemistry in 1911. RORA: The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Rontgen, of Germany, who received 150,782 SEK, which is equal to 7,731,004 SEK in December 2007. John Bardeen is the only laureate to win the prize twicein 1956 and 1972. Maria Sklodowska-Curie also won two Nobel Prizes, for physics in 1903 and chemistry in 1911. Score 0.3626 0 0 1.0806 -1.2246 -2.6737 Table 2. Example sentence-level scores for the first three sentences of the retrieved document, computed by MC-Shapley (using LLAMA3.2-3B-INSTRUCT) and RORA for the given question. The scores in the right column correspond to each sentence. The ground-truth answer is Wilhelm Conrad Rontgen. Darker highlighting denotes sentences assigned higher scores by each method. Reliability weight α. We test dRAG using MC-Shapley across six reliability weight values on LLAMA-3.2-3BINSTRUCT under token-level polluted data, where higher reliability weight results in more aggressive reliabilitydriven filtering. Results shown in Figure 6a indicate that increasing the weight does induce stronger intervention (e.g., from 0.1 to 0.5), making the dRAG retrieves documents from less polluted data sources and improves its performance towards the upper bound. However, exceedingly large reliability weights, such as 1.0, can cause the dRAG to over-rely on reliability scores while overlooking the semantic relevance captured by neural rerankers, resulting in accuracy decline to approximately 34%. Notably, across all tested reliability weights, dRAG consistently outperforms the centralized system under identical conditions. This further demonstrates dRAGs robustness in unreliable and noisy data environments, because accidental retrievals from highly polluted sources can be overridden by documents from more reliable sources, which typically receive higher reranking scores according to Equation 4 and are consequently positioned later in the context window where they can receive more attention from LLMs. Number of retrievers . The number of retrievers determines how many data sources are sampled based on their usefulness scores. higher value of forces dRAG to retrieve documents from more data sources that have historically contributed meaningfully to LLM generation (see the usefulness definition in Section 4.2). We evaluate ranging from single data source to all six available sources, and the results are shown in Figure 6b. Clearly, increasing the number of retrievers consistently improves accuracy, elevating performance from below the lower bound to nearly reaching the upper bound. This demonstrates that data sources with higher reliability scores are not necessarily the most useful ones for LLM generation. By retrieving documents from multiple sources, dRAG learns from diverse information across different reliability levels, enabling more accurate and nuanced updates to the reliability scores. We also notice performance drop when we set = 6, where all data sources are included and usefulness-based sampling is practically disabled. Therefore, this drop can be interpreted as an ablation result, highlighting the practical importance of usefulness-based sampling for dRAG. Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain Sentence importance score validation. We validate that importance score estimation methods, MC-Shapley (Goldshmidt & Horovicz, 2024) in Equation 1 and RORA (Jiang et al., 2024) in Equation 2 can effectively capture the contribution of each retrieved sentence to LLM generation. While these methods have been previously evaluated against human annotations in their original works, we test their utility in our simulated unreliable data environments by analyzing the entropy of the resulting importance scores across sentences drawn from data sources with varying levels of pollution. As illustrated in Figure 6c, both MC-Shapley and RORA exhibit consistent decrease in score entropy as the data pollution level increases. This is because token-level pollution progressively replaces ground-truth tokens with incorrect ones, leading to uniformly low scores across sentences, thereby reducing entropy. Importantly, this pattern reflects the methods sensitivity to data quality degradation, confirming their viability as robust indicators for identifying unreliable information for our dRAG system. We further provide real example in Table 2 showing how the two methods capture the relative importance of each sentence. 6.3 System Cost Evaluation 1 2 5 10 15 20 Total Gas Used (Cost) Per-Update Gas (Cost) 71,277 ($0.0258) 96,352 ($0.0349) 211,492 ($0.0766) 376,899 ($0.1364) 502,565 ($0.1820) 628,048 ($0.2274) 71,277 ($0.0258) 48,176 ($0.0174) 42,298 ($0.0153) 37,690 ($0.0136) 33,504 ($0.0121) 31,402 ($0.0114) Table 3. Gas usage and USD cost for providing feedback and updating the reliability scores at 0.09 gwei/gas and ETH = $4,022.14 (on Oct 29, 2025). represents the number of data sources to update in the feedback. This mainly reflects the cost of the signature. Actual gas consumption also depends on the query length and other factors. Utilizing decentralized infrastructure like public blockchain is typically not free, as it consumes the computational power of the network. In the Ethereum network, gas is the unit that measures the computational effort required to execute operations, like running smart contract. Each operation incurs fixed amount of gas, and users pay fee based on this total, which compensates network participants for the resources they use. Table 3 provides the cost of the most important part of making dRAG trustworthy infrastructure, namely, on-chain verification of feedback and score updates. As the number of score updates increases (i.e., the number of queries the dRAG has seen), the per-update cost decreases, reflecting approximately 55.8% of gas efficiency gains from our batching operations, dropping from 2.6 cents per update (n=1) to 1.1 cents per update (n=20). This demonstrates the potential for dRAG to utilize batched queries and feedback to update the reliability evaluation while maintaining reasonable cost range."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "We built dRAG, decentralized RAG system that addresses data reliability challenges in real-world settings. Our dRAG dynamically computed and updated reliability scores for each data source, securing these scores on blockchain to ensure both quality control and trust among data source owners. Through controlled experiments, we demonstrated that dRAG consistently outperforms centralized systems in unreliable data environments across different models, data pollution strategies, and reliability scoring methods. Notably, our system can even achieve performance comparable to that obtained in fully reliable environments. To the best of our knowledge, dRAG is the first blockchain-based decentralized RAG system that incorporates quality control mechanisms to handle noisy, real world-like data. We hope this work opens new research directions at the intersection of decentralized systems, RAG, and trustworthy AI. While dRAG performs effectively when data sources provide full coverage for user queries (i.e., token-level pollution), it struggles under more challenging conditions. For example, when pollution occurs at the document level, its performance falls considerably short of the upper bound achieved by centralized systems in fully reliable environments, as we briefly discussed in Section 6.1. This gap arises from fundamental challenge in the data distribution: each data source provides only partial documents rather than complete coverage of the query space, such that the most reliable source may not contain the most relevant documents for given query. To address this limitation, straightforward solution is to make reliability scores work adaptively rather than through single fusion at the reranking stage. For instance, the system could first retrieve documents by relevance, then use learned gate (classifier) to determine whether to apply reliability-based reranking or preserve the original relevance ordering for each query. Such an adaptive, query-conditioned pipeline, paired with our iterative source reliability updates, may close the performance gap in challenging scenarios with fine-grained, non-overlapping data sources. Another promising direction for future research is to analyze the convergence rate of reliability scores in dRAG. Specifically, it remains unclear how many queries are required for the dRAG system to reach its performance upper bound, and which factors (e.g., query diversity, number of data sources, data source quality, etc.) most significantly influence the convergence rate of these reliability scores. systematic investigation of these convergence dynamics would provide valuable insights into the systems scalability and practical deployment. We leave this exploration to future work. Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was partially supported by NSF IIS-2119531, IIS-2137396, IIS-2142827, IIS-2234058, OAC-2312973, NASA ULI 80NSSC23M0058 and Open Philanthropy. We also appreciate the support from the Foundation Models and Applications Lab of Lucy Institute and ND-IBM Tech Ethics Lab."
        },
        {
            "title": "REFERENCES",
            "content": "Addison, P., Nguyen, M.-T. H., Medan, T., Shah, J., Manzari, M. T., McElrone, B., Lalwani, L., More, A., Sharma, S., Roth, H. R., Yang, I., Chen, C., Xu, D., Cheng, Y., Feng, A., and Xu, Z. C-fedrag: confidential federated retrieval-augmented generation system, 2024. URL https://arxiv.org/abs/2412.13163. Andersen, T. E., Avalos, A. M., Dagher, G. G., and Long, M. D-rag: privacy-preserving framework for decentralized rag using blockchain. Artificial Intelligence, Soft Computing And Application Trends 2025, 2025. URL https://api.semanticscholar. org/CorpusID:276622158. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. Seven failure points when engineering retrieval augmented generation system, 2024. URL https://arxiv.org/abs/2401.05856. Chakraborty, A., Dahal, C., and Gupta, V. Federated retrieval-augmented generation: systematic mapping study, 2025. URL https://arxiv.org/abs/ 2505.18906. Chong, Z.-K., Ohsaki, H., and Ng, B. Llm-net: Democratizing llms-as-a-service through blockchain-based expert networks, 2025. URL https://arxiv.org/abs/ 2501.07288. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazare, P.-E., Lomeli, M., Hosseini, L., and Jegou, H. The faiss library, 2025. URL https://arxiv.org/ abs/2401.08281. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M., and Wang, H. Retrieval-augmented generation for large language models: survey, 2024. URL https://arxiv.org/abs/2312.10997. Goldshmidt, R. and Horovicz, M. Tokenshap: Interpreting large language models with monte carlo shapley value estimation, 2024. URL https://arxiv.org/abs/ 2407.10114. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Hecking, T., Sommer, T., and Felderer, M. An architecture and protocol for decentralized retrieval augmented generation. In 2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C), pp. 3135, 2025. doi: 10.1109/ICSA-C65153.2025. 00012. URL https://ieeexplore.ieee.org/ document/11014986. Helmi, T. Decentralizing ai memory: Shimi, semantic hierarchical memory index for scalable agent reasoning, 2025. URL https://arxiv.org/abs/2504.06135. Hewitt, J., Ethayarajh, K., Liang, P., and Manning, C. Conditional probing: measuring usable information beyond baseline. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 16261639, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 122. URL https://aclanthology.org/2021. emnlp-main.122/. Jayasundara, S. H., Arachchilage, N. A. G., and Russello, G. Ragent: Retrieval-based access control policy generation, 2024. URL https://arxiv.org/abs/2409. 07489. Jiang, Z., Lu, Y., Chen, H., Khashabi, D., Van Durme, B., and Liu, A. RORA: Robust free-text rationale evaluation. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10701087, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.60. URL https: //aclanthology.org/2024.acl-long.60/. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In the 2020 Conference on Empirical Proceedings of Methods in Natural Language Processing (EMNLP), pp. 67696781, Online, November 2020. Association doi: 10.18653/v1/ for Computational Linguistics. Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain 2020.emnlp-main.550. URL https://www.aclweb. org/anthology/2020.emnlp-main.550. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10. 1162/tacl 00276. URL https://aclanthology. org/Q19-1026/. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 94599474. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper. pdf. Li, H., Su, Y., Cai, D., Wang, Y., and Liu, L. survey on retrieval-augmented text generation, 2022. URL https: //arxiv.org/abs/2202.01110. Li, S., Stenzel, L., Eickhoff, C., and Bahrainian, S. A. Enhancing retrieval-augmented generation: study of best practices. In Rambow, O., Wanner, L., Apidianaki, M., Al-Khalifa, H., Eugenio, B. D., and Schockaert, S. (eds.), Proceedings of the 31st International Conference on Computational Linguistics, pp. 67056717, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025. coling-main.449/. Mao, Q., Zhang, Q., Hao, H., Han, Z., Xu, R., Jiang, W., Hu, Q., Chen, Z., Zhou, T., Li, B., Song, Y., Dong, J., Li, J., and Yu, P. S. Privacy-preserving federated embedding learning for localized retrieval-augmented generation, 2025. URL https://arxiv.org/abs/2504. 19101. Neague, P., Gregoriadis, M., and Pouwelse, J. Dedsi: Decentralised differentiable search index. In Proceedings of the 4th Workshop on Machine Learning and Systems, EuroMLSys 24, pp. 134143, New York, NY, USA, 2024. Association for Computing Machinery. doi: 10.1145/ 3642970.3655837. URL https://doi.org/10. 1145/3642970.3655837. ISBN 9798400705410. Qian, X., Zhang, Y., Zhao, Y., Zhou, B., Sui, X., Zhang, L., and Song, K. TimeR4 : Time-aware retrievalaugmented large language models for temporal knowlIn Al-Onaizan, Y., edge graph question answering. Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 69426952, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 394. URL https://aclanthology.org/2024. emnlp-main.394/. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr. org/papers/v21/20-074.html. Uddin, M. N., Saeidi, A., Handa, D., Seth, A., Son, T. C., Blanco, E., Corman, S., and Baral, C. UnSeenTimeQA: Time-sensitive question-answering beyond LLMs memorization. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18731913, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/ 2025.acl-long.94. URL https://aclanthology. org/2025.acl-long.94/. Wang, Z., Teo, S. X., Ouyang, J., Xu, Y., and Shi, W. M-rag: Reinforcing large language model performance through retrieval-augmented generation with multiple partitions, 2024. URL https://arxiv.org/abs/ 2405.16420. Xu, C., Gao, L., Miao, Y., and Zheng, X. Distributed retrieval-augmented generation, 2025. URL https:// arxiv.org/abs/2505.00443. Yang, Y., Chai, H., Shao, S., Song, Y., Qi, S., Rui, R., and Zhang, W. Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems, 2025. URL https://arxiv.org/abs/2504.00587. Yu, J. and Sato, H. Derag: Decentralized multi-source rag system with optimized pyth network. In 2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA), pp. 106115, 2024. doi: 10.1109/ISPA63168.2024.00022. URL https:// ieeexplore.ieee.org/document/10885343. Zeng, S., Zhang, J., He, P., Liu, Y., Xing, Y., Xu, H., Ren, J., Chang, Y., Wang, S., Yin, D., and Tang, J. The good and the bad: Exploring privacy issues in retrievalaugmented generation (RAG). In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain for Computational Linguistics: ACL 2024, pp. 4505 4524, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.267. URL https://aclanthology. org/2024.findings-acl.267/. Zeng, S., Zhang, J., He, P., Ren, J., Zheng, T., Lu, H., Xu, H., Liu, H., Xing, Y., and Tang, J. Mitigating the privacy issues in retrieval-augmented generation (rag) via pure synthetic data, 2025a. URL https://arxiv.org/ abs/2406.14773. Zeng, Z., Liu, J., Chiang, M.-F., He, J., and Zhang, Z. S-RAG: novel audit framework for detecting unauIn thorized use of personal data in RAG systems. Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1037510385, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long. 512. URL https://aclanthology.org/2025. acl-long.512/. Zhou, W., Yan, Y., and Yang, Q. Dgrag: Distributed graph-based retrieval-augmented generation in edgecloud systems, 2025. URL https://arxiv.org/ abs/2505.19847."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, University of Notre Dame, USA"
    ]
}