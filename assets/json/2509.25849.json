{
    "paper_title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation",
    "authors": [
        "Ziniu Li",
        "Congliang Chen",
        "Tianyun Yang",
        "Tian Ding",
        "Ruoyu Sun",
        "Ge Zhang",
        "Wenhao Huang",
        "Zhi-Quan Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 9 4 8 5 2 . 9 0 5 2 : r Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation Ziniu Li1,2, Congliang Chen2, Tianyun Yang3, Ding Tian3, Ruoyu Sun2,3, Ge Zhang1, Wenhao Huang1, Zhi-Quan Luo2,3 1ByteDance Seed, 2The Chinese University of Hong Kong, Shenzhen, 3Shenzhen Research Institute of Big Data"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each tasks exploration as an \"item\" with distinct \"value\" and \"cost\", we establish connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the models current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources. Date: October 1, 2025 Correspondence: Ziniu Li at liziniu@bytedance.com and Ge Zhang at zhangge.eli@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "The remarkable capabilities of Large Language Models (LLMs) have led to their widespread application across various domains [2, 8, 26, 27, 31, 46]. While pre-training on vast text corpora endows LLMs with general knowledge and linguistic fluency, fine-tuning them for specialized tasks often necessitates more targeted optimization beyond pre-training. Reinforcement Learning (RL) has emerged as powerful paradigm for this purpose [10, 20, 28], enabling LLMs to iteratively self-improve by interacting with environments. popular instantiation is RL with verifiable rewards [18], where LLMs generate responses and receive binary (true/false) feedback based on their outcomes, iteratively refining their internal policies to search for optimal solutions. Initially pioneered in mathematical reasoning [13], this framework has since been extended to domains like 1 Figure 1 Illustration of our framework for allocating exploration budgets among tasks from computational resources. We model each task as an item with learning value and computational cost, then solve the allocation problem using Knapsack optimization. coding [23] and agentic tasks [39]. core challenge in these applications is explorationsampling diverse trajectories to find better solutions. This process is computationally expensive in practice due to sequential nature of autoregressive generation. As such, most RL pipelines use small number of rollouts per prompt (e.g., 8) for exploration. However, this uniform allocation strategy could lead to some problematic outcomes. For example, in the Group Relative Policy Optimization (GRPO) [32] algorithm, meaningful learning signals (gradients) only emerge when both successful and failed attempts are present in the same batch. With uniform budget, easy tasks often result in all-success outcomes, and hard tasks in all-failure outcomes, leading to zero gradients and stalled learning. This issue has been well-documented in previous research [5, 48], and we approach it from the broader perspective of strategic exploration budget allocation. We argue the fundamental problem is the mismatch between tasks difficulty and its assigned exploration budget. Hard tasks, which require extensive (could even require more than 100) exploration to find useful trajectories, receive too little effort under uniform rule. Easy tasks, which require minimal exploration, waste compute by being over-sampled. Thus, heterogeneous and customized exploration allocation strategy is preferred. To this end, we introduce knapsack-based formulation: each task, when assigned certain budget, can be conceptualized as an \"item\" with an associated value (learning potential) and cost (computational effort of exploration). The allocation problem is thus equivalent to the classical knapsack problem [25, 29], where the objective is to maximize total value under fixed global budget. We refer to this approach as Knapsack RL; see Figure 1 for illustration. When applied to the popular GRPO framework, our method enables dynamic, heterogeneous allocation of exploration budgets, which allows sufficient exploration on training tasks. Empirically, across Qwen series models [45, 46] sized from 1B to 7B, we observe 20-40% improvement in effective gradient ratios, translating into more reliable policy improvements and average performance gains of about 2-4 points on several challenging benchmarks. To get better sense of this improvement, we note that achieving comparable improvements with uniform allocation would require nearly 2x the computation. We present this as proof-of-concept, demonstrating promising direction to boost the effectiveness of RL."
        },
        {
            "title": "2 Preliminary",
            "content": "Following [28, 32], we model language generation as autoregressive sampling from conditional probability distribution πθ(yx), where represents the input prompt and represents the generated response. The parameter θ denotes the trainable parameters. Our goal is to improve the language model via RL by maximizing the expected performance of responses generated from the model distribution πθ: max θ Eyπθ(x)[r(x, y)]. (1) 2 In this paper, we focus on RL with verifiable rewards [18]. Specifically, let = (CoT, answer) denote the concatenation of Chain-of-Thought (CoT) [42] reasoning steps CoT and the final solution answer. The reward function r(x, y) is defined as: r(x, y) = I(answer is correct with respect to x), where I() is the indicator function and {0, 1} is binary (1 for correct, 0 for incorrect). This is equivalent to {1, 1} scheme, where incorrect responses incur negative reward. This outcome-based reward formulation has been widely adopted (see e.g., [10] and references therein) and has been shown to effectively incentivize reasoning abilities [43]. (2) Algorithm 1 RL with Classical Homogeneous Budget Allocation 1: for iteration = 1, 2, . . . do 2: 3: 4: 5: Sample mini-batch of prompts (x1, . . . , xM ) Generate responses for each prompt xi Evaluate the rewards (e.g., equation (2)) and compute the gradients (e.g., equation (3)) Update model parameters with estimated gradients Budget Allocation To optimize equation (1), policy gradient methods [38] are commonly employed. Among these, REINFORCE [44]-style stochastic policy gradient methods have become standard since the work of [20]. These methods stochastically sample responses from πθ and estimate gradients using direct reward feedback. Originally designed for single-task RL, this approach is typically extended to multi-task RL by employing homogeneous exploration budget allocation. Algorithm 1 summarizes this classical framework. In Algorithm 1, the sampling process in Line 3 corresponds to exploration in RL, where the model generates responses to search for optimal solutions. Line 5 corresponds to exploitation, updating the model to leverage feedback from data. We adopt the widely used gradient estimator from Group Relative Policy Optimization (GRPO) [32]: g(θ) = (cid:88) (cid:88) i=1 j= θ log πθ(yijxi) (r(xi, yij) bi) ci, (3) where yij denotes the j-th sampled response for prompt xi, and θ log πθ(yijxi) represents the gradient of the log-probability with respect to model parameters θ. The baseline bi and normalization factor ci are defined as: bi = 1/N (cid:80)N j=1(r(xi, yij) bi)2 is the standard deviation of rewards for prompt xi, and ϵ is small constant (106) preventing division by zero when σi = 0. Technically, GRPO computes relative advantages within each response group (prompt), increasing likelihood of positive responses and decreasing likelihood of negative ones. j=1 r(xi, yij) and ci = 1/(σi + ϵ) with σi = 1/N (cid:80)N (cid:113)"
        },
        {
            "title": "3 Diagnosing Exploration in Homogeneous Budget Allocation",
            "content": "In this section, we discuss the limitations of homogeneous budget allocation for GRPO and present empirical observations that motivate our work."
        },
        {
            "title": "3.1 Motivation",
            "content": "Exploration in RL is computationally expensive due to the sequential nature of autoregressive generation, often requiring substantial GPU memory and hours of computation, especially for reasoning tasks. From the sample efficiency perspective, it is critical to assess how much each collected sample actually contributes to gradient updates. For GRPO, we make the following observation. Observation 1. Let gi = (cid:80)N If σi = 0, meaning that all sampled responses for xi yield identical rewards (all correct or all incorrect), then (r(xi, yij) bi) = 0 for every sample in this group, leading to gi = 0. In this case, the model receives no learning signal from that prompt. j=1 θ log πθ(yijxi) (r(xi, yij) bi) ci be the gradient for prompt i. 3 This phenomenon is widely recognized as major bottleneck for GRPO in practice [5, 48]. To formally track it, we introduce the metric effective-gradient-ratio, which measures the proportion of individual samples that contribute non-zero gradients: effective-gradient-ratio ="
        },
        {
            "title": "1\nM · N",
            "content": "M (cid:88) (cid:88) i=1 j=1 I(gi,j = 0), (4) where gi,j = θ log πθ(yijxi) (r(xi, yij) bi) ci is the gradient contribution from the j-th sample of the i-th prompt. Higher values indicate that more samples provide useful gradient signals. We also define two complementary metrics: zero-gradient-ratio (by all positive rewards): proportion of prompts yielding zero gradients due to uniformly positive rewards; and zero-gradient-ratio (by all negative rewards): proportion of prompts yielding zero gradients due to uniformly negative (or zero) rewards. We visualize these dynamics in Figure 2 for the Qwen2.5-Math-7B model trained on the DAPO-MATH-17K dataset. Each mini-batch contains = 256 prompts with = 8 rollouts per prompt. The results reveal several concerning patterns: Low Overall Effectiveness: The effective gradient ratio consistently remains below 60%, meaning that over 40% of sampled data fails to contribute to model updatesa significant waste of computational resources. Dynamic Training Phases: The gradient dynamics exhibit three distinct phases: Early Training (0-70 iterations, approximately the first epoch): The model struggles with most tasks, leading to predominantly all-negative rewards (green line peaks near 95%). This results in minimal learning signals being generated. Mid Training (70-600 iterations): As the model improves, it begins solving some tasks while still failing others, creating the mixed outcomes necessary for effective gradients. The effective gradient ratio can maintain above 40% during this phase. Late Training (600+ iterations): Tasks become increasingly easy, leading to rise in all-positive rewards (orange line increases to 40%). Simultaneously, challenging tasks still result in all-negative rewards (the green line fluctuates around 20%). As result, the effective-gradient-ratio steadily decreases to about 20% by 1000 iterations. We provide theoretical analysis toward understanding the above empirical observations in the next section."
        },
        {
            "title": "3.2 Theoretical Analysis",
            "content": "We model reward outcomes as Bernoulli random variables to analyze the exploration budget required. Definition 1 (Success Rate). We define the success rate pi on prompt xi as the probability that the model generates correct response: pi p(xi) = Eyπθ(xi)[r(yxi)] = Pr[r(yxi) = 1]. This formulation allows statistical analysis of stochastic gradients. For sampled responses yi1, . . . , yiN on prompt xi, the probability that both correct and incorrect samples are observed is: Figure 2 The ratio of effective gradients and zero gradients during training. P(gi = 0) = 1 P[all rewards are the same] = 1 P[all rewards are 1s] P[all rewards are 0s] = 1 pN (1 pi)N . This raises the question: how large must the sampling budget be to obtain non-zero gradient? We answer this from two perspectives: high-probability guarantees and expected sample complexity. 4 Figure 3 Exploration budget required to ensure non-zero gradients based on success rate. Note that success rates with in the same bins are grouped from real samples, which may not be symmetry, rendering the exploration budget may not be symmetry as the theory suggests. Theorem 1 (Exploration Budget). Given prompt with the success rate pi (0, 1), we have that High probability bound: For any α (0, 1), to ensure P(gi = 0) α, it suffices to take ln(1α) ln(max{pi,1pi}) . Expected number of rollouts: Let first denote the number of independent rollouts required until gi = 0 is achieved for the first time. Its expectation is: E[N first ] = 1/pi + 1/(1 pi) 1. Please refer to Appendix for the proof. To illustrate, for example, if = 0.5, we need 3 rollouts on average to obtain non-zero gradient. For hard task with = 0.01, we require 100 rollouts on average, and to achieve 90% chance of non-zero gradient, we would need 229 rollouts. We show the theoretical predictions in Figure 3. We employ the Qwen2.5-Math-7B-Instruct model to generate 256 responses for 1,000 prompts from the DAPO-Math-17K dataset. Then we estimate and compute the minimal budget needed for gi = 0 from the data. We exclude prompts that with empirical success rate of 0.0 or 1.0, because our exploration budget 256 is not sufficient. The results show that typical budget of = 8 only covers tasks with [0.1, 0.9]. For tasks with 0 or 1, even increasing to 16 or 32 is insufficient. Overall, our analysis shows that the sampling budget required for meaningful gradients could be much larger than what is practically used. This also helps explain the low effective gradient ratio observed in Figure 2. Existing practices typically address this kind of insufficient exploration challenge in two ways: Increasing the exploration budget uniformly. This involves raising for example, from 8 to 16 or even 32which could help address exploration on extremely hard or easy tasks and improve the effective gradient ratio. However, setting very large value for , such as = 100, is often impractical due to prohibitive computational costs. Filtering hard and easy prompts. Tasks that are too easy or too hard are dropped. This kind of approach is leveraged in [39, 48]. However, as we have seen, the proportion of prompts yielding zero gradients due to all-negative rewards (the green line in Figure 2) is around 20% in late training, indicating many tasks are not yet fully solved. If we simply filter these prompts, we may close off crucial source for RL, where meaningful learning often comes from converting failures into successes. That is, removing hard prompts deprives the model of opportunities to practice on challenging examples, limiting the information available to LLMs. In this work, we favor addressing this issue by scaling exploration budgets, but recognize that the first approach presents fundamental computation-exploration dilemma. This tension motivates our pursuit of more principled solution for allocation of exploration budgets."
        },
        {
            "title": "4 Proposed Approach: Knapsack-based Budget Allocation",
            "content": "In this section, we introduce our approach to addressing the explorationcomputation dilemma. Our key principle is that computational resources are typically fixed by user constraints; thus, we do not assume access to additional resources. Instead, we treat the available compute as fixed pool and design centralized allocation strategy that distributes exploration budgets across tasks in more informed way. The central technical question is: given fixed total budget, what is the optimal allocation for RL exploration? The homogeneous allocation strategy fails to consider the following aspects to be optimal: Exploration cost: some tasks require more rollouts to make progress. Learning value: the potential benefit of improving performance on given task. We formalize the above idea as constrained optimization problem: max N1,...,NM subject to (cid:88) i=1 (cid:88) i=1 Value(Ni, pi) (5) Ni = Ntotal, Nlow Ni Nup, Ni Z+, where Ni is the number of trajectories allocated to prompt xi, and pi is the success rate. The bounds Nlow (e.g., 2) and Nup (e.g., 128) allow to enforce coverage and prevent degenerate allocations. The total budget Ntotal is usually set to to match the homogeneous allocation rule. This formulation is structurally identical to the classical knapsack problem [29]. In the analogy, task equipped with specific exploration budget corresponds to an item. The number of allocated rollout trajectories Ni is the items weight, and the resulting Value(Ni, pi) is the items value. This analogy is natural: increasing Ni requires more computation (longer generation time or more GPU memory), but it may also yield higher learning benefit by producing more informative gradients. The knapsacks capacity corresponds to the total exploration budget, which is physically determined by the available computational resources. The optimization objective is thus to maximize the total learning value subject to the fixed exploration budget. We summarize the correspondence in Table 1. Table 1 Connection between RL exploration and the knapsack problem. RL Exploration Setting task with an exploration budget Number of trajectories assigned to the task Potential learning benefit of allocating budget to the task Total exploration budget across all tasks Available computational resources (e.g., GPUs) Knapsack Analogy An item placed in the knapsack Weight of the item Value of the item Knapsack capacity The knapsack (the container itself) subtle but important point is that task difficulty alone does not determine value. In other words, task xi (an item) does not inherently possess value based solely on its difficulty pi. Instead, value emerges only when difficulty is paired with particular budget Ni, which governs its effective contribution. Consequently, the optimization problem is not defined over tasks directly but over all valid taskallocation pairs. This means the effective item set is larger, with size (Nup Nlow), since each task can correspond to multiple potential items depending on the allocated budget."
        },
        {
            "title": "4.1 Formulation of Task Value",
            "content": "In this section, we substantiate the above framework with the proposed idea. We recognize that homogeneous budget allocation fails to take the task value into consideration. For GRPO, we address this issue by defining the value of assigning Ni exploration budget units to prompt xi as Value(Ni, pi) = ProbNonZeroGradient(Ni, pi) InfoGain(pi), where ProbNonZeroGradient(Ni, pi) = 1 pNi (1 pi)Ni is the probability of obtaining non-zero gradient (see Section 3.2) for GRPO, and InfoGain(pi) quantifies the informativeness of gradient if one occurs. It can also be extended to other algorithms; see Appendix B. Our design emphasizes coverage of effective gradients across prompts: it accounts for whether non-zero gradient is likely to appear, but not for the exact balance of positive versus negative samples. In this work, we define InfoGain as measure of the expected increase in success probability after gradient update, while noting that alternative formulations could be explored in future work. Formally, let pt denote the success rate before the update and pt+1 InfoGain = pi = pt+1 the rate after the update. We define pt i. Directly computing this requires access to the post-update success probability, which is intractable. We address this issue by some approximation bounds. Proposition 1. With the Taylor expansion, the InfoGain can be approximated by pi(1 pi)2 . Please refer to Appendix for detailed derivation. This approximation relies only on the current success rate pi. Although idealized and not exact, it captures essential intuitions while remaining simple to compute. Its key properties are: InfoGain(pi) is maximized at pi = 1/3. This aligns with the intuition that uncertain-but-promising samples are most valuable. InfoGain(pi) is asymmetric: for equally distant values of pi from 1/3, harder tasks yield larger information gain than easier tasks. Furthermore, InfoGain(pi) 0 as pi 0 or pi 1, meaning extremely hard or extremely easy prompts provide diminishing value. We visualize our defined Value(Ni, pi) in Figure 4. This contour plot shows lines of equal value, highlighting the interplay between the success rate pi and the exploration budget Ni. For example, the three highlighted points demonstrate that different combinations of pi and Ni can yield comparable high values. task with the success rate pi = 0.35 (which is close to 1/3), requires relatively small exploration budget of Ni = 4 to achieve high value. However, for tasks with success rates further from this optimum, such as harder task with pi = 0.19 or an easier one with pi = 0.52, the required exploration budget is now specified as Ni = 16 or Ni = 8 respectively to reach the same value level."
        },
        {
            "title": "4.2 Algorithm Implementation\nEstimating success rates. In practice, the success rate pi\nis not directly available as a prior and must be estimated\nfrom collected samples. In this work, we employ a simple\nheuristic: using the success rates observed in the previous epoch as estimates for the current one. Specifically,\nthe first epoch may follow a homogeneous budget allocation rule, after which the proposed knapsack-based\n(cid:98)pi to guide allocation. Although this strategy introduces some\napproach leverages the estimated success rates\ndelay and noise, it has proven empirically effective. More sophisticated estimation techniques (e.g., online\nlogistic regression) that account for task correlations present promising directions for future improvement.",
            "content": "Figure 4 The interplay between success rate, exploration budget and the value. 7 (cid:98)pi values are directly used to formulate the discrete constrained optimization problem These estimated (Equation 5), which can be solved in polynomial time using standard dynamic programming techniques. With Numba [17] acceleration, it typically runs within 12 seconds. Handling Extreme Cases. Our value function defined in Section 4.1 assigns zero value to prompts with empirical success rates of 0 or 1, which would otherwise lead to zero budget allocation for these prompts. To prevent their complete exclusion and maintain coverage: For (cid:98)pi = 1.0 (prompts always solved correctly), the estimate may be not accurate from history samples, so we allocate small minimum budget (e.g., 2) to ensure they are still considered. This can be achieved by set Nlow in equation (5). For (cid:98)pi = 0.0 (prompts never solved correctly), we employ fallback allocation strategy. We first estimate the total budget required for prompts with pi (0, 1] according to Theorem 1 and the above rule. Any remaining budget is subsequently distributed among extremely hard tasks. This strategy is particularly beneficial in later training stages where many prompts become easy, thus freeing up capacity to focus on hard tasks. Rollout Balancing. In practice, the total number of trajectories (M ) is typically generated by parallel workers (where < ), often leveraging efficient inference engines like vLLMs [16]. While homogeneous allocation rule allows for simple division of prompts among workers (each performing rollouts per prompt), our knapsack-based approach can lead to significant imbalance in allocated rollouts per prompt. This occurs because certain prompts may be allocated disproportionately large exploration budgets, creating an uneven workload and potentially leading to GPU idles and inefficient resource utilization. To address this issue, we employ simple rollout balancing strategy: we treat each allocated rollout for prompt as an individual execution job. These execution jobs are then randomly dispatched to the available workers, with the inference engine generating one response per prompt. This approach is suitable for settings where prompts are not excessively long, thus not strictly requiring advanced techniques like prefix caching. For scenarios involving longer prompts, we would consider using the KarmarkarKarp bin-packing algorithm [14] to group prompts into approximately balanced batches based on their allocated budgets. Workers would then process these balanced groups of prompts, potentially utilizing prefix caching. Overall, our knapsack-based exploration method integrates seamlessly into large-scale RL training pipelines with minimal modifications (see Listing 1 in the Appendix). Computationally, it adds negligible overhead. Algorithmically, it introduces no additional hyperparameters to tune and does not bias policy gradients. From systems perspective, core components of inference (e.g., vLLM-based accelerated generation [16]) and training (e.g., FSDP [52] and Megatron [34]) remain unchanged, ensuring full compatibility with existing infrastructure."
        },
        {
            "title": "5.1 Main Results\nExperiment Setting. We implement Knapsack-RL and baseline methods using the large-scale RL training\nframework Verl [33]. Our primary focus is GRPO [32], a widely examined method, and we refer to our specific\nimplementation as Knapsack-GRPO. Training utilizes the DAPO-Math-17K dataset [48], which comprises\n17,917 prompts, each with a ground truth answer for verification.",
            "content": "We conduct experiments with both pre-trained and instruction-tuned models. The pre-trained models include Qwen3-4B-Base [46] and Qwen2.5-Math-7B [45]. For instruction-tuned models, we utilize DeepSeek-R1-DistillQwen-1.5B [10] (abbreviated as DPSK-R1-Distill-1.5B) and Qwen3-4B-Instruct-2507 [46] (abbreviated as Qwen3-4B-Instruct). In each iteration, we employ mini-batch size of = 256 prompts and generate = 8 rollouts. Our models are trained for 1,000 iterations. The extensive training duration of 1,000 iterations for Qwen2.5-Math-7B, for example, requires about 1,400 GPU hours with A100 GPUs. For evaluation, we follow [24] and assess our method on several mathematical reasoning benchmarks: AIME, AMC, MATH, MINERVA, and OLYMPIAD Bench (OLYMPIAD for short). Given AIMEs small sample size, 8 we combine its 2024 and 2025 editions into single dataset, hereafter referred to as AIME. Additionally, we include GPQA [30] as an out-of-domain evaluation, which tests scientific reasoning across physics, chemistry, and biology. All reported performance metrics are averaged over 16 generated responses. Table 2 Evaluation performance (avg@16) comparison across different models and benchmarks."
        },
        {
            "title": "AIME",
            "content": "34.0 6.6 20.7 DPSK-R1-Distill-1.5B 25.3 27.6 + GRPO + Knapsack-GRPO Qwen3-4B-Base + GRPO + Knapsack-GRPO Qwen3-4B-Instruct + GRPO + Knapsack-GRPO Qwen2.5-Math-7B + GRPO + Knapsack-GRPO 48.2 12.3 23.9 20.8 47.7 47.0 24."
        },
        {
            "title": "OLYMPIAD",
            "content": "81.4 84.0 86.7 48.0 80.6 81.0 92.4 92.5 92.5 61.2 81.7 83.9 25.8 27. 28.5 19.4 31.9 35.7 35.4 41.8 38.2 11.8 33.6 34.5 41.7 46.4 49.7 23.1 44. 46.2 61.6 61.8 63.5 26.1 41.9 44.1 AMC 62.1 71.1 75.1 29.9 56.9 66.0 82. 84.9 83.1 41.0 70.6 77."
        },
        {
            "title": "GPQA",
            "content": "39.1 36.7 40.3 26.4 46.6 45.5 43.0 54.4 59.9 22.0 40.8 43.8 Avg 42.9 45. 49.7 22.9 43.2 45.1 58.6 59.2 61.9 26.7 45.2 47.5 We report the evaluation performance in Table 2, observing consistent improvements across all tested models after applying our RL training. Specifically, Knapsack-GRPO consistently outperforms GRPO. For instance, in terms of average performance, it improves by 3.8 points for DPSK-R1-Distill-1.5B compared to GRPO. On specific benchmarks, the improvements are even more significant: for example, 6.4 points on AIME for DPSK-R1-Distill-1.5B, 9.1 points on AMC for Qwen3-4B-Base, 5.5 points on GPQA for Qwen3-4B-Instruct, and 6.8 points on AMC for Qwen2.5-Math-7B."
        },
        {
            "title": "5.2 Understanding Knapsack-based Exploration",
            "content": "This section delves into understanding the superiority of knapsack-based exploration. We visualize its exploration budget distribution and analyze its efficacy through gradient effectiveness and task status dynamics during training, primarily focusing on the Qwen2.5-Math-7B model. We provide additional results in Appendix D. Exploration Budgets. To illustrate the impact of knapsack-based exploration, we visualize the assigned exploration budgets. Specifically, we quantify the frequency with which different exploration budgets are allocated to individual prompts during the training. These results are presented in Figure 5. We observe that, even without introducing additional computational resources, our approach can dynamically assign up to 93 exploration budgets to certain tasks. This level of dynamic, high-budget allocation is impractical to achieve under conventional homogeneous budget allocation framework. Effective Gradient Ratio. Figure 6 shows the effective gradient ratio during training, as defined in equation (4). Knapsack-based budget allocation improves this ratio by approximately 20-40% across Figure 5 Distribution of exploration budgets allocated by knapsack-GRPO for Qwen2.5-Math-7B during training. 9 models. Unlike uniform allocation, the knapsack method avoids clear decreasing trend. This stems from dynamically distributing exploration budgets, targeting tasks with mixed successful and failed trajectories. These observations partially explain Knapsack-GRPOs policy improvements. (a) DPSK-R1-Distill-1.5B (b) Qwen3-4B (c) Qwen2.5-Math-7B Figure 6 Effective gradient ratio during training. Task Transition Dynamics. To understand our methods influence on learning, we analyze prompt evolution during training. We categorize training prompts into five performance statuses based on success rate (pi): extremely-hard (pi = 0, all failures), hard (0 < pi 0.2), medium (0.2 < pi < 0.8), easy (0.8 pi < 1.0), and extremely-easy (pi = 1.0, all successes). Our analysis covers two aspects: 1) prompt status transitions after training, and 2) final prompt status distribution. (a) GRPO (b) Knapsack-GRPO Figure 7 Prompt transition matrices for Qwen2.5-Math-7B during training. The cell (i, j) indicates the percentage of samples transitioning from status to status j. Figure 7 visualizes the 5 5 transition matrix for Qwen2.5-Math-7B training, illustrating prompt category transitions. Knapsack-GRPO demonstrates superior efficiency in learning challenging tasks. Specifically, the self-absorption frequency for extremely-hard samples (prompts remaining in that status) is 43.4% for Knapsack-GRPO, notably lower than GRPOs 47.1%. Furthermore, Knapsack-GRPO shows higher transition rate to extremely-easy tasks (last column in heatmap) than GRPO, indicating more effectively mastered samples. We also examine the final distribution of prompt statuses after training, specifically by counting the training samples in each status, as depicted in Figure 8. Knapsack-GRPO has 3,596 extremely-hard tasks, less than GRPOs 3,793. This 197-task reduction suggests Knapsack-GRPOs dynamic budget allocation makes them more tractable. Consistent with observed transitions, Knapsack-GRPO yields 9,274 extremely-easy tasks, surpassing GRPOs 8,676. 10 Despite these promising results, approximately 20% of prompts remain in the extremely-hard category even after 1,000 training iterations. We investigate if these are truly unsolvable: for Knapsack-GRPO, 577 of these challenging prompts recorded at least one positive trajectory during optimization, implying they are not inherently unsolvable. Future research could explore experience replay techniques to address these samples more effectively."
        },
        {
            "title": "5.3 Experiments with Different Exploration Bud-\ngets",
            "content": "Finally, we conduct experiments with varying total exploration budgets to assess performance under different computational resource constraints. In contrast to previous experiments, which used total budget of Ntotal = 256 8 = 2048, here we explore scenarios with Ntotal = 1024 and Ntotal = 4096. For the vanilla GRPO, this corresponds to using = 4 and = 16, respectively. Note that the total budget parameter does not impact Knapsack-GRPO in the same way, given its distinct allocation strategy. Figure 8 Distribution of sample statuses after training. The results for the Qwen2.5-Math-7B model are shown in Figure 9. Knapsack-GRPO clearly outperforms GRPO, particularly when computational resources are limited. In the low-budget scenario, Knapsack-GRPO improves performance from 39.8 to 45.5, while continuing to maintain its advantage even with higher exploration budgets. These findings demonstrate that Knapsack-GRPO achieves the same performance as standard GRPO with roughly 2x the computational resources, highlighting the efficiency of its exploration budget allocation."
        },
        {
            "title": "6 Related Work",
            "content": "Figure 9 Performance comparison under different exploration budgets. Data heterogeneity. central challenge in RL for LLMs arises from the heterogeneity of training data. Prompts vary substantially in difficulty, leading to diverse reward distributions, and these distributions evolve throughout training, further complicating learning. Prior work has recognized this issue: for example, Li et al. [20] observed substantial variations in reward distributions across prompts, which complicated stable gradient estimation. Their solution introduced refined baselines to reduce variance, thereby improving the exploitation stage of RL. Following this, many advanced policy optimization methods have been proposed (e.g., [1, 32, 48]). We refer readers to surveys [40, 50] for broader overview. By contrast, our work directly tackles the exploration challenge posed by heterogeneous data, focusing on how to allocate exploration resources more effectively to capture informative trajectories in the first place. Prompt selection and curriculum learning. Another line of research seeks to improve data efficiency through prompt selection and curriculum design [6, 21, 37, 49]. For example, Chen et al. [6] used advantage estimates as proxy for difficulty to construct curricula, while Sun et al. [37] employed perplexity as difficulty metric. Additionally, Yu et al. [48] presented the concept of \"dynamic sampling\" to address the issue of sparse gradients; however, it is crucial to clarify that their \"sampling\" refers to selecting prompts that yield effective gradients, rather than dynamically allocating exploration budgets. These works primarily operate along the axis of which prompts to train on, while maintaining homogeneous exploration per prompt. In contrast, our approach focuses on how much exploration to allocate to each prompt. Our work aims to addresses the need for more extensive exploration on challenging tasks directly. To underscore this fundamental difference, consider that prompt selection methods might prioritize subset of simple prompts to achieve high effective gradient ratio. Our work, however, aims to dynamically design the 11 exploration budget for all prompts to ensure that each receives sufficient exploration to generate effective gradients, especially those that are inherently harder. Resource allocation. Optimizing resource allocation has long been studied in operations research and systems [12, 15]. However, connections to RL have been more limited, partly because traditional RL often addresses single-task settings where computational budgets do not require explicit distribution. Theoretical works have considered online exploration under knapsack constraints [3, 7], but typically in the single-task formulation. By contrast, our formulation of Knapsack RL explicitly allocates exploration budgets across multiple tasks from centralized computational pool. The most closely related work is [47], which also investigates dynamic resource allocation but within rejection sampling and RAFT [9] frameworks, focusing on variance reduction. By contrast, our method directly targets online RL, formulating exploration allocation as knapsack optimization problem that explicitly balances computational cost with expected learning value. In addition, studies such as [41, 51] consider computation allocation during the inference stage, whereas our work addresses it in the training stage where learning signals must be actively generated. Scaling RL. Our method resonates with the principle of test-time scaling [4, 36], which allocates additional computational resources (e.g., best-of-N sampling, majority voting) to improve response quality. Similarly, our approach leverages extra compute to amplify exploration, thereby enhancing the quality of collected training signals. More broadly, our work aligns with recent efforts that scale compute in post-training to unlock stronger downstream performance [13, 22]."
        },
        {
            "title": "7 Conclusion",
            "content": "Motivated by the observation that RL agents require extensive exploration on challenging tasks to gather informative feedback and drive self-improvement, we investigate the problem of optimally allocating computational resources for exploration. We formulate this problem as knapsack optimization, where each task-budget pair is treated as an item with an associated cost and value. This framework enables us to prioritize harder tasks, thereby yielding more effective gradients and leading to superior policy improvements. This comes at no additional computational cost, effectively offering \"free lunch\". We view this work as an initial step toward scaling exploration as means to unlock RLs potential in LLM post-training. Several directions offer promising avenues for future research: Extending beyond rollout counts. In this work, we focus on exploration budgets measured by the number of rollouts. Future extensions could incorporate other computational factors, such as the token length of responses or the number of interaction turns required in agentic tasks. Designing richer value functions. We model the task value using first-order Taylor expansion of one-step policy improvement. Exploring alternative formulations of value functions could yield more accurate assessments of learning potential and further enhance allocation strategies. Incorporating advanced exploration strategies. Our study employs simple on-policy rollouts for tractability. While effective, some tasks remain unsolved under this paradigm. More sophisticated strategiessuch as tree-based exploration inspired by Monte Carlo Tree Search used in AlphaGo [35]offer compelling path forward. Recent work in Tree-RL [11] and TreePO [19] demonstrates the promise of techniques like state rollbacks to advantageous intermediates. Integrating such methods with our knapsack-based allocation framework is promising."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [2] Anthropic. System card: Claude opus 4 & claude sonnet 4. 2025. URL https://www-cdn.anthropic.com/ 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. 12 [3] Kianté Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. Advances in Neural Information Processing Systems, 33:1631516326, 2020. [4] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [5] Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, and Tianyi Lin. Spectral policy optimization: Coloring your incorrect reasoning in grpo. arXiv preprint arXiv:2505.11595, 2025. [6] Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint arXiv:2505.14970, 2025. [7] Xiaoyu Chen, Jiachen Hu, Lihong Li, and Liwei Wang. Efficient reinforcement learning in factored mdps with application to constrained rl. arXiv preprint arXiv:2008.13319, 2020. [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [9] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. Treerl: Llm reinforcement learning with on-policy tree search. arXiv preprint arXiv:2506.11902, 2025. [12] Hameed Hussain, Saif Ur Rehman Malik, Abdul Hameed, Samee Ullah Khan, Gage Bickler, Nasro Min-Allah, Muhammad Bilal Qureshi, Limin Zhang, Wang Yongji, Nasir Ghani, et al. survey on resource allocation in high performance distributed computing systems. Parallel Computing, 39(11):709736, 2013. [13] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [14] Narendra Karmarkar and Richard Karp. An efficient approximation scheme for the one-dimensional bin-packing problem. In 23rd Annual Symposium on Foundations of Computer Science (sfcs 1982), pages 312320. IEEE, 1982. [15] Naoki Katoh, Akiyoshi Shioura, and Toshihide Ibaraki. Resource allocation problems. Handbook of combinatorial optimization, pages 28972988, 2013. [16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. [17] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. Numba: llvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, pages 16, 2015. [18] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [19] Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, et al. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic tree-based modeling. arXiv preprint arXiv:2508.17445, 2025. [20] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 2912829163, 2024. 13 [21] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. [22] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. [23] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-atO3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. [24] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. [25] George Mathews. On the partition of numbers. Proceedings of the London Mathematical Society, 1(1):486490, 1896. [26] AI Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on, 4(7):2025, 2025. [27] OpenAI. Gpt-5 system card. https://openai.com/index/gpt-5-system-card/, 2025. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [29] David Pisinger and Paolo Toth. Knapsack problems. In Handbook of Combinatorial Optimization: Volume13, pages 299428. Springer, 1998. [30] Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman. Gpqa: graduatelevel google-proof q&a benchmark, nov. arXiv preprint arXiv:2311.12022, 2023. [31] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [33] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [34] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. arXiv preprint Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. [35] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [36] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [37] Yan Sun, Guo Jia, Stanley Kok, ZihWanao Wang, Zujie Wen, and Zhiqiang Zhang. Stretching the comfort zone: Boost data efficiency for rl training with prepo!, August 2025. URL https://yansun-x.notion.site/ data-efficiency-prepo. [38] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. 14 [39] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [40] Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, et al. survey on large language models for mathematical reasoning. arXiv preprint arXiv:2506.08446, 2025. [41] Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. Every rollout counts: Optimal resource allocation for efficient test-time scaling. arXiv preprint arXiv:2506.15707, 2025. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [43] Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025. [44] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. [45] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [46] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [47] Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, and Tong Zhang. Optimizing chain-of-thought reasoners via gradient variance minimization in rejection sampling and rl. arXiv preprint arXiv:2505.02391, 2025. [48] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [49] Chuheng Zhang, Wei Shen, Li Zhao, Xuyun Zhang, Xiaolong Xu, Wanchun Dou, and Jiang Bian. Policy filtration for rlhf to mitigate noise in reward models. arXiv preprint arXiv:2409.06957, 2024. [50] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. [51] Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, and Lei Li. Scaling llm inference efficiently with optimized sample compute allocation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 79597973, 2025. [52] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023."
        },
        {
            "title": "A Proof",
            "content": "Proof of Theorem 1. We prove both parts of the lemma. Part 1: High probability bound. We want to find the minimum such that P(gi = 0) α for given α (0, 1). From the problem setup, we have: For the condition P(gi = 0) α to hold, we require: P(gi = 0) = 1 pN (1 pi)N , (1 pi)N α 1 pN + (1 pi)N 1 α. pN Let = max{pi, 1 pi}. Since pi (0, 1), we have 1 2 and 1 pi pi. The case pi < 1 follows by symmetry. 2 Since (1 pi) pi, we have (1 pi)N pN for 1. Therefore: . Without loss of generality, assume pi 1 2 , so = pi For large , the term qN dominates (1 q)N since > 1 2 . More precisely, we have: + (1 pi)N 2pN pN = 2qN . lim (1 q)N qN = lim (cid:19)N (cid:18) 1 = 0, since (1 q)/q < 1. Therefore, for sufficiently large , the constraint (6) is dominated by the term qN : qN 1 α ln ln(1 α). Since < 1, we have ln < 0, which gives: ln(1 α) ln = ln(1 α) ln(max{pi, 1 pi}) . (6) (7) Part 2: Expected number of rollouts. Let X1, X2, . . . be i.i.d. Bernoulli random variables with Pr(Xi = 1) = (0, 1), where 1 denotes success and 0 denotes failure. Define first = min{n 1 : both 0 and 1 have appeared among X1, . . . , Xn}. We compute E[N ] by conditioning on the first trial X1. Case 1: X1 = 1 (probability p). After the first success, we still need to wait until the first failure occurs. The waiting time for the first failure follows geometric distribution with success probability 1 p, whose expectation is 1/(1 p). Thus E[N X1 = 1] = 1 + 1 1 . Case 2: X1 = 0 (probability 1 p). By symmetry, we wait for the first success; its waiting time has expectation 1/p, so Applying the law of total expectation: E[N X1 = 0] = 1 + 1 . E[N ] = (cid:16) 1 + (cid:17) 1 1 (cid:16) + (1 p) 1 + (cid:17) 1 = 1 + 1 + 1 16 = 1 + 1 1 1. Hence, the expected number of rollouts until we first observe both success and failure is This completes the proof of the second part of Lemma 1. E[N first] = 1 + 1 1 1 . Proof of Proposition 1. We provide rigorous derivation under the following assumptions: The policy follows softmax distribution: pk = exp(zk) (cid:80)K j=1 exp(zj ) for action k. The gradient update follows the policy gradient rule with advantage A: zk zk + η I[k = y] zk log py (8) where η is the learning rate and is the chosen action. We assume unit learning rate (η = 1) and unit advantage (A = 1) for simplicity. Step 1: Taylor expansion. For small parameter changes, the change in success probability can be approximated by: py (cid:88) k= py zk .zk Step 2: Computing partial derivatives. For the softmax probability py = exp(zy) (cid:80)K j=1 exp(zj ) , we have: py zy = py(1 py), and py zk = pypk, for = y. Step 3: Determining parameter updates. Under the policy gradient update rule, we have: Therefore, the parameter updates are: zk log py = I[k = y] pk. zy = I[y = y] py = 1 py, zk = I[k = y] pk = 0 pk = pk, for = Step 4: Computing InfoGain. Substituting the partial derivatives and parameter updates: py = py zy zy + (cid:88) k=y py zk zk = py(1 py) (1 py) + = py(1 py)2 + py (cid:88) p2 (pypk) (pk) (cid:88) k=y k=y Step 5: Simplification under first-order approximation. For the first-order Taylor approximation to be accurate, we require small parameter updates. Under this condition, the cross-terms (cid:80) are second-order in the update magnitude and can be neglected compared to the main term py(1 py)2. Therefore, we obtain: k=y p2 This completes the proof. InfoGain py(1 py)2 . 17 To validate this approximation, we conduct an empirical study with 100 actions, comparing the InfoGain computed through exact gradient updates against our theoretical approximation from Proposition 1. As shown in Figure 10, the two curves align closely across different success rates, demonstrating that our formula p(1 p)2 provides reliable approximation for practical use."
        },
        {
            "title": "B Extensions",
            "content": "In this work, we mainly focus on the widely used GRPO [32] algorithm to design the optimal allocation strategy. Here we discuss possible extensions for other RL algorithms by adapting the core framework while maintaining the same task value function structure: Figure 10 InfoGain and approximate formula. Comparison of exact The key difference lies in how we compute ProbNonZeroGradient(Ni, pi) for different algorithms: Value(Ni, pi) = ProbNonZeroGradient(Ni, pi) InfoGain(pi). RLOO [1]. RLOOs policy gradient estimator is equivalent to GRPO up to constants, thus we may not need fundamental changes. The probability of obtaining non-zero gradient remains: ProbNonZeroGradient(Ni, pi) = 1 pNi (1 pi)Ni. ReMax [20]. ReMax leverages the reward of greedy response as baseline, rather than the averaged reward used in GRPO. In this setting, gradient update occurs only when the sampled trajectory differs from the greedy response. If we denote the probability of the greedy response as α, then the probability of sampling trajectory different from the greedy response is 1 α. The probability of obtaining non-zero gradient with Ni samples becomes: ProbNonZeroGradient(Ni, α) = 1 αNi. This represents the probability that at least one of the Ni sampled trajectories differs from the greedy response, thereby producing gradient signal. REINFORCE [44]. There is no baseline design in vanilla REINFORCE. We can directly calculate the ProbNonZeroGradient to account for the case where at least one trajectory receives positive reward: ProbNonZeroGradient(Ni, pi) = 1 (1 pi)Ni. This formulation is simpler than GRPO since we only need to ensure at least one successful trajectory occurs, rather than balancing positive and negative samples. The proposed frameworks modularity allows for straightforward adaptation to other RL algorithms by: (1) identifying the algorithms gradient computation mechanism, (2) determining conditions for non-zero gradients, (3) calculating the corresponding ProbNonZeroGradient function, and (4) maintaining the same InfoGain(pi) = pi(1 pi)2 formulation across algorithms. This demonstrates the general applicability of our value-based budget allocation approach beyond the specific GRPO implementation."
        },
        {
            "title": "C Experiment Details",
            "content": "Our experiments utilized the large-scale RL training framework Verl, specifically version 0.5.0. No modifications were made to the core training and inference code, with the exception of the advantage calculation, where values were clipped between -5 and 5. This was implemented because, as rollout responses were scaled, we observed their values could become significantly large in extreme cases, thus requiring this additional clipping for numerical stability. Following recommendations from [48], the learning rate was set to 106, with importance sampling clipping ratios (high/low) of 0.28 and 0.2, respectively. Neither KL nor entropy regularization was employed. Models were trained with maximum sequence length of 4K tokens, with the exception of DPSK-R1-Distill-1.5B, 18 which utilized 8K tokens to accommodate its typically longer Chain-of-Thought (CoT) behaviors requiring more context. For evaluation results reported during training, models were assessed every 10 training iterations using 16 generated responses. To manage evaluation time, 100 evaluation samples were randomly selected from benchmarks when the total number of samples exceeded this number. For the final evaluation performance presented in Table 2, different maximum sequence lengths were used to prevent response truncation: 4K tokens for Qwen2.5-Math-7B, 8K tokens for Qwen3-4B and Qwen3-4BInstruct, and 16K tokens for DPSK-R1-Distill-1.5B. Consequently, these results may not perfectly align with those reported in the training curves. Listing 1 Python pseudo code implementation of knapsack RL. Two components are modified: (1) budget allocation is replaced with knapsack optimization for better resource distribution, and (2) task status is updated based on external feedback. def budget_allocation ( batch , total_budget , ** kwargs ) : - + budget = np . full ( len ( batch ) , total_budget // len ( batch [ prompt ]) ) budget = knapsack ( batch [ status ] , total_budget , ** kwargs ) indices = [] for task_id , task_budget in enumerate ( budget ) : if task_budget > 0: indices . extend ([ task_id ] * task_budget ) return batch . select_idxs ( indices ) gen_batch = budget_allocation(batch, total_budget, **kwargs) if rollout_balancing : indicies = np . random . shuffle ( np . arange ( len ( batch [ prompt ]) ) ) batch = batch . select_idxs ( indicies ) batch = actor . ge ner ate_sequences ( gen_batch ) batch = p _ a _ _ a g ( batch ) train_dataset.update_status(batch) actor . update ( batch ) 1 2 4 5 6 7 8 10 11 12 13 14"
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Visualization of Exploration Process Evolution of Prompts. To illustrate the impact of exploration budgets on individual prompt learning dynamics, we track and visualize the learning trajectories of several randomly selected prompts from the training data in Figure 11. Each subplot corresponds to unique prompt, identified by its index in the title. We observe that for several examples, our framework effectively allocates more exploration budget, leading to complete learning of the prompt (e.g., prompts in the first row, first column, and second row, first column). Conversely, some tasks remain highly challenging, where neither Knapsack-GRPO nor GRPO achieves satisfactory performance (e.g., the prompt in the third row, second column). D.2 Training curves As references, the training curves for all models are displayed in Figures 12, 13, 14, and 15. Compared with the final results in Table 2, these plots further show that Knapsack-GRPO delivers rapid performance improvement early in the training process. We also observe few cases of performance degeneration, which points to the need for exploring more stable policy optimization techniques in future research. D.3 Ablation Studies In 4.2, we introduced the fallback strategy, which reallocates excess exploration Without Fallback Strategy. budgets from already-solved prompts to those that remain unsolved. This prevents common failure mode: difficult prompts may otherwise receive too few resources, while easy prompts are oversampled. Figure 11 Learning dynamics of randomly selected prompts throughout training, comparing GRPO and KnapsackGRPO. Each subplot shows the success rate evolution for specific prompt. concrete example is shown in Table 3 with 8 prompts. Without the fallback strategy, the allocation assigns over 50 exploration units to task with success rate of 0.9, while the unsolved task (success rate 0.0) receives only 2 units. In contrast, with the fallback strategy, the unsolved task is assigned 29 unitssubstantially increasing its chance of making progress. Empirically, this design proves crucial (Figure 16). In our experiments with the Qwen2.5-Math-7B model, removing the fallback strategy led to unstable training, large performance fluctuations on benchmarks such as AMC and OlympiadBench, and overall degraded results. This result suggests that neglecting challenging examples during training weakens the reinforcement signal, ultimately harming the models ability to generalize. Low and Up Bounds. Our framework incorporates safeguards in the form of hyper-parameters Nlow and Nup, as defined in equation (5). Nup is set to 128 primarily to facilitate faster computation of the knapsack optimization using dynamic programming; its specific value does not critically impact performance. Conversely, Nlow is set to 2 to prevent degenerate allocation scenarios, particularly when success rates might be inaccurate, as elaborated in Section 4.2. We present ablation results for these bounds in Figure 17, which empirically support these design choices. D.4 Comparing with Dynamic Sampling in DAPO Dynamic sampling, technique introduced in the DAPO paper [48], selects prompts with mix of positive and negative rewards, filtering out those with exclusively positive or negative outcomes. This process is repeated until target number of prompts is accumulated, strategy that has been shown to be effective. While effective, dynamic sampling operates on different principle than our knapsack-based approach. Dynamic sampling aims to scale up effective prompts, while our method focuses on scaling up effective responses. Since these two approaches are parallel and can be combined, we conducted empirical studies to 20 Figure 12 Evaluation performance of DPSK-R1-Distill-1.5B across training iterations. Figure 13 Evaluation performance of Qwen3-4B-Base across training iterations. Table 3 Comparison of budget allocation with and without fallback strategy. With Fallback Strategy Without Fallback Strategy Success Rate Cost Assignment Success Rate Cost Assignment 0.0 0.9 1.0 1.0 1.0 1.0 1.0 1.0 22 0 0 0 0 0 0 29 23 2 2 2 2 2 0.0 0.9 1.0 1.0 1.0 1.0 1.0 1.0 22 0.0 0.0 0.0 0.0 0.0 0.0 2 50 2 2 2 2 2 2 Index 1 2 3 4 5 6 7 8 21 Figure 14 Evaluation performance of Qwen3-4B-Instruct across training iterations. Figure 15 Evaluation performance of Qwen2.5-Math-7B across training iterations. 22 Figure 16 Effect of the fallback strategy. Without it, exploration budgets are disproportionately allocated to prompts with at least one successful trial, while unsolved tasks are largely ignored. explore their synergy. We evaluated the performance of these methods using two different metrics, as shown in the training curves in Figure 18 and Figure 19. Because dynamic sampling requires multiple exploration steps to accumulate enough effective prompts for single gradient update, we can analyze performance in two ways: By exploration budget: Figure 18 shows performance relative to the total number of exploration iterations. This measures how effectively total computation budget is converted into performance gains. We found that dynamic sampling boosts GRPOs performance on benchmarks like AIME and OLYMPIAD, improving the score from 45.2 to 46.2. When we combined dynamic sampling with our knapsack-based exploration, performance on the AMC benchmark improved significantly (from 69.8 to 73.0), resulting in total performance of 46.5. This is slightly better than dynamic sampling alone but worse than our pure knapsack approach. We attribute this partially to the fact that knapsack-GRPO utilizes more gradient iterations, and therefore do not consider this negative result. By gradient update iterations: Figure 19 displays performance against the number of gradient updates. This metric assesses the value of each gradient update. The results clearly show that effective gradients, whether from dynamic sampling or our knapsack-based exploration, lead to greater performance gains for the same number of update iterations, which validates the core motivation behind both techniques. 23 Figure 17 Ablation study on the impact of Nlow and Nup constraints within the knapsack optimization framework. Figure 18 Performance of Qwen2.5-Math-7B relative to the number of exploration iterations, demonstrating how effectively the total computation budget is converted into performance gains. 24 Figure 19 Performance of Qwen2.5-Math-7B as function of the number of LLM gradient updates. This figure validates that effective gradients, derived from either dynamic sampling or the knapsack-based approach, lead to greater performance gains for the same number of updates."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Shenzhen Research Institute of Big Data",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}