{
    "paper_title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
    "authors": [
        "Enshu Liu",
        "Xuefei Ning",
        "Yu Wang",
        "Zinan Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 3 5 1 7 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DISTILLED DECODING 1: ONE-STEP SAMPLING OF IMAGE AUTO-REGRESSIVE MODELS WITH FLOW MATCHING Enshu Liu, Xuefei Ning, Yu Wang, Department of EE, Tsinghua University les23@mails.tsinghua.edu.cn foxdoraame@gmail.com yu-wang@mail.tsinghua.edu.cn Zinan Lin Microsoft Research zinanlin@microsoft.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive (AR) models have recently achieved state-of-the-art performance in text and image generation. However, their primary limitation is slow generation speed due to the token-by-token process. We ask an ambitious question: can pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that attempt to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To overcome this, we propose Distilled Decoding (DD), which leverages flow matching to create deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train network to distill this mapping, enabling few-step generation. The entire training process of DD does not need the training data of the original AR model (as opposed to some other methods), thus making DD more practical. We evaluate DD on state-of-the-art image AR models and present promising results. For VAR, which requires 10-step generation (680 tokens), DD enables onestep generation (6.3 speed-up), with an acceptable increase in FID from 4.19 to 9.96 on ImageNet-256. Similarly, for LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8 speed-up with comparable FID increase from 4.11 to 11.35 on ImageNet-256. In both cases, baseline methods completely fail with FID scores >100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of onestep generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The code and the pre-trained models will be released at https:// github.com/imagination-research/distilled-decoding. The project website is at https://imagination-research.github.io/ distilled-decoding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Autoregressive (AR) models (Van den Oord et al., 2016; Chen et al., 2018; Esser et al., 2021; Razavi et al., 2019; Lee et al., 2022; Yu et al., 2021; Chang et al., 2022; Li et al., 2023; 2024a; Touvron et al., 2023a;b; Ouyang et al., 2022) are the foundation of state-of-the-art (SOTA) models for text generation (e.g., GPT (Brown, 2020; Radford et al., 2019; Radford, 2018; Achiam et al., 2023)) and image generation (e.g., VAR (Tian et al., 2024), LlamaGen (Sun et al., 2024)). Despite their impressive performance, AR models suffer from slow generation speeds due to their sequential generation process. More specifically, AR models formulate data (e.g., text, images) as sequence of tokens and are trained to predict the conditional probability distribution of one token given all previous tokens. This means that AR models can only generate data in token-by-token Work mostly done during Enshu Lius internship at Microsoft Research Project advisor: Zinan Lin"
        },
        {
            "title": "Preprint",
            "content": "DD-1step (218) DD-2step (117) DD-42step (5.7) LlamaGen-256step Figure 1: Qualitative comparisons between DD and vanilla LlamaGen Sun et al. (2024) on ImageNet 256256. We show that the generated images of DD have small quality loss compared to the pre-trained AR model, while achieving 200 speedup. More examples are in App. F. Activate Upholstered Fabric Armchair Butterfly Womens T-Shirt pair of shoes on the floor bowl of fresh fruits Figure 2: Qualitative results of DD-2step on text-to-image task. The model is distilled from LlamaGen model with prompts from LAION-COCO dataset. The speedup is around 93 compared to the teacher model. More examples are in App. F. fashion, which is slow. For example, LlamaGen-343M-256256 requires 256 steps (5 seconds1) to generate one 256256 image. In this paper, we ask the ambitious question: Can pre-trained AR model be adapted to generate data in few (e.g., one or two) steps? If successful, this would greatly benefit both the model developers (e.g., reducing the testing and deployment cost) and the end users (e.g., reducing the latency). Apparently, this problem is challenging. While speeding up AR models by generating multiple tokens at time  (Fig. 4)  is extensively studied in literature (Ning et al., 2024a; Liu et al., 2024; Jin et al., 2024; Stern et al., 2018; Kou et al., 2024; Gloeckle et al., 2024; Cai et al., 2024; Santilli et al., 2023; Chang et al., 2022; Li et al., 2024a), none of these works were able to generate the entire sample (i.e., all tokens) in one step. Indeed, we find that there is fundamental reason for this limitation (Sec. 3.1). Sampling multiple tokens in parallel (given previous tokens) would have to assume that these tokens are conditionally independent with each other, which is incorrect in practice. As an extreme, generating all tokens in one step (i.e., assuming that all tokens are independent) would completely destroy the characteristics in data (Sec. 3.1). This insight suggest that few-step AR generation requires fundamentally different approach. In this paper, we introduce Distilled Decoding (DD)  (Fig. 4)  , novel method for distilling pre-trained AR model for few-step sampling. In each AR generation step, we use flow matching (FM) (Liu et al., 2022; Lipman et al., 2022) to transform random noisy token, sampled from an isotropic Gaussian distribution, into the generated token. FM ensures that the generated tokens distribution aligns with that of the AR model. However, this generation process would be even slower than vanilla AR due to the added FM overhead. The actual benefit is that, the mapping between noisy and generated token is determinstic. Therefore, we can train model that directly distills the mapping between the entire sequence of noisy tokens and the generated tokens. This enables one-step generation by inputting the sequence of noisy tokens into the distilled model. Moreover, this deep synergy between AR and FM allows for flexible use of additional generation steps to improve data quality without changing the model (see Sec. 3.3). Note that the training of DD 1Measured on one NVIDIA A100-80G GPU."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Comparison of DD models, pre-trained models, and other acceleration methods for pretrained models. DD achieves significant speedup compared to pre-trained models with comparable performance. In contrast, other methods performance degrades quickly as inference time decreases. Figure 4: High-level comparison between our Distilled Decoding (DD) and prior work. To generate sequence of tokens qi: (a) the vanilla AR model generates token-by-token, thus being slow; (b) parallel decoding generates multiple tokens in parallel (Sec. 4.1), which fundamentally cannot match the generated distribution of the original AR model with one-step generation (see Sec. 3.1); (c) our DD maps noise tokens Ïµi from Gaussian distribution to the whole sequence of generated tokens directly in one step and it is guaranteed that (in the optimal case) the distribution of generated tokens matches that of the original AR model. does not need the training data of the original AR model. This makes DD more practical as training data is often not released especially for SOTA LLMs. As the first work in this series, we focus on image AR models. We validate the effectiveness of DD on the latest and SOTA image AR models: VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024). Our key contributions are: We identify the fundamental limitations of existing methods that prevent them from achieving few-step sampling in AR models. We introduce DD to distill pre-trained AR models for few-step sampling. For the first time, we demonstrate the feasibility of 1-step sampling with SOTA image AR models. For example, on ImageNet, DD reduces the sampling of VAR from 10 steps to 1-step (6.3 speed-up) with an acceptable increase in FID from 4.19 to 9.96; for LlamaGen, DD cuts sampling from 256 steps to 1 (217.8 speed-up) with comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail on 1 step generation and achieve FID scores >100. For text-to-image generation with LlamaGen on LAION-COCO dataset, DD is able to reduce the sampling steps from 256 to 2 (92.9 speed-up) with minimal FID increase from 25.70 to 28.95. DD also supports more generation steps for better image quality  (Fig. 3)  . See Figs. 1 and 2 for visualization. This work challenges the assumption that AR models must be slow. We hope it paves the way for efficient AR models and inspires research into one-step AR models in other areas, such as text generation, where the task is more challenging due to the higher number of steps."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we introduce the preliminaries required to understand DD. 2.1 AR MODELS Given random variable arranged in sequence of tokens (q1, , qn), AR models learn the conditional distribution of tokens given all previous ones: p(qiq<i) = p(qiqi1, qi2, , q1). The data likelihood is given by p(z) = (cid:81)n i=1 p(qiq<i). At generation, AR models sample tokens one by one using the learned conditional distribution for each token, which is therefore slow."
        },
        {
            "title": "2.2\nImage tokenizer. To apply AR models to images, we need to represent continuous images as a\nsequence of discrete tokens. Early works of image AR models operate on quantized pixels (Van den\nOord et al., 2016; Chen et al., 2018). Later works propose the vector quantization (VQ) method, uti-\nlizing a encoder E, a quantizer Q, and a decoder D to quantize and reconstruct images. Specifically,\nafter [E, Q, D] is fully trained, the encoder will transform the original image x â R3ÃHÃW into\na more compact latent space: Z = E(x), where Z = {z1, z2, Â· Â· Â· , zhÃw} â RCÃhÃw consists of\nh Ã w embeddings, each has a dimension of C. Then, the quantizer will look up the closest token qi\nin the codebook V = (c1, c2, Â· Â· Â· , cV ) â RV ÃC for each embedding zi. AR works on the quantized\nsequence Zq = {q1, q2, Â· Â· Â· , qhÃw}. Finally, one can use the decoder to reconstruct the image from\nthe quantized sequence: Ëx = D(Zq). A distance loss l(Ëx, x) is used in training for accurate recon-\nstruction. The VQ scheme is used in many popular image AR models (Li et al., 2023; Chang et al.,\n2022; Tian et al., 2024; Sun et al., 2024; Li et al., 2024a).",
            "content": "The order of tokens. Another important design choice of image AR models is the order of tokens. typical and straightforward method of ordering tokens is following the raster order of the embeddings zi, such as LlamaGen (Sun et al., 2024). Random order is more general option (Chang et al., 2022; Li et al., 2023; 2024a). The SOTA method VAR (Tian et al., 2024) proposes novel method where the tokens are arranged according to the resolution. Specifically, give the latent RChw, VAR down-samples it to series of resolutions ((h1, w1), (h2, w2), , (hK, wK)), where hi < hj, wi < wj for < j. Then, VAR arranges the augmented tokens from these latents in the order of resolution. Tokens from the same resolution are sampled simultaneously. This more natural and human-aligned next scale prediction approach enables VAR to achieve excellent performance. However, it still needs 10 steps to generate 256256 image, making it inefficient. 2.3 FLOW MATCHING Given two random distribution Ï0(x), Ï1(x) with Rd, flow matching (FM) (Liu et al., 2022; Lipman et al., 2022) constructs probability path which connects the two distributions. Setting up continuous timestep axis and putting the two distribution at = 0 and = 1 respectively, such probability paths can be viewed as an ordinary differential equation (ODE) trajectory. Specifically, flow matching defines velocity field (x, t) at any timestep t, given by: (x, t) = Ex0,x1Ï0,1(x0,x1)( (1) where Ï0,1(x0, x1) is any joint distribution that satisfies both (cid:82) Ï0,1(x0, x1)dx0 = Ï1(x1) and (cid:82) Ï0,1(x0, x1)dx1 = Ï0(x0), and Ï(x, y, t) is trivariate bijection function for and y, which means that at any give timestep t, knowing any two of the Ï, x, will determine the third one. Ï(x, y, t) should also satisfy the boundary conditions: Ï(x0, x1, 0) = x0, Ï(x0, x1, 1) = x1. Ï(x0, x1, t) = x), Ï(x0, x1, t) Ï =0 (xÏ , Ï )dÏ . The flow matching ODE has marginal preserving property. Given x0 Ï0, we can obtain xt by stepping along the ODE trajectory: xt = x0 + (cid:82) It can be proved that the distribution of xt is equal to the distribution of Ï(x0, x1, t) (Liu et al., 2022; Lipman et al., 2022): Ï(xt) = Ï(Ï(x0, x1, t)), where x0, x1 Ï0,1(x0, x1). When we step to = 1, we will get sample from another distribution Ï1. In practice, Ï1 is usually set as the target distribution we want to sample from, while Ï0 is set as simple prior distribution, e.g., Gaussian distribution. Then flow matching enables generative process from the prior to the target. 3 METHOD 3.1 TRAINING FEW-STEP AR MODELS IS NON-TRIVIAL In this section, we explain why few-step generation is challenging for AR models and why existing approaches fundamentally cannot achieve it. To finish the sampling process of the whole token sequence (q1, , qn) in as few steps as possible, each step should generate as many tokens as possible. Assume our goal is to generate set of next tokens (qk+1, , qm) based on subsequence (q1, , qk) as prefix in one step, there are several straightforward ideas to solve this problem. We discuss each method and the reason why they do not work as below: (1) Train neural network with the prefix sequence as input and output the subsequent tokens in one run. Actually, the information in the prefix (q0, , qk) is not enough to deterministically specify the (qk+1, , qm), because there are many feasible possibilities for the subsequent tokens that can fit the prefix, leaving one-to-many mapping for the neural network to learn. For example, we can mask the face portion of portrait and then select another face from many choices to replace it. In this case, the neural network can only learn an average of all possibilities and output blurry im-"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: AR flow matching. Given all previous tokens, the teacher AR model gives probability vector for the next token, which defines mixture of Dirac delta distributions over all tokens in the codebook. We then construct deterministic mapping between the Gaussian distribution and the Dirac delta distribution with flow matching. The next noise token Ïµ4 is sampled from the Gaussian distribution, and its corresponding token in the codebook becomes the next token q4. age tokens, like the one in MAE (He et al., 2022). At the stage where the prefix token is insufficient to constrain the subsequent tokens (e.g., at the beginning of the AR generation process), feasible choices are even much more, leading to poor generation quality. further potential solution is to select the choice with the highest probability as the output target of the network (e.g., Song et al. (2021)). However, such method will destroy the distribution completely by collapsing into the most likely mode, which is impractical for image generation. (2) Model the distribution of all subsequent tokens. The neural network can be trained to output the probability of each subsequent token given the prefix as the input. This approach avoids the challenges caused by the one-to-many mapping and is used in mask-based AR models (Chang et al., 2022; Li et al., 2023; 2024a) and VAR (Tian et al., 2024). In this case, the sampling processes of each token are independent from each other, introducing gap between the modeled distribution (cid:81)m i=k+1 p(qiqk, , q1) and the ground truth p(qm, , qk+1qk, , q1). For image AR models, the gap is acceptable when is small. However, when the number of new tokens is large, the gap will increase exponentially, making the performance much worse. Consider the case where the model is trained to learn to sample all tokens in one run (which is our goal). In this case, the cross-entropy loss of every token is calculated separately. The final objective can be written as = 1 (cid:88) 1 n (cid:88) (cid:88) pijklogËpÎ¸jk, (2) i=1 k= j=1 where is the codebook size, is the dataset size, pijk is one-hot vector along the third dimension given any (i, j), indicating the ground-truth probability distribution, and ËpÎ¸jk is the modeled distribution. In the following proposition, we give the optimal solution of ËpÎ¸jk. Proposition 3.1. The optimal solution for Eq. (2) is ËpÎ¸jk = This solution equals to occurrence frequency of the k-th token at the j-th position within the dataset. The proof can be found in App. A. Based on this proposition, consider toy case where the dataset only contains 2 data samples: = {(0, 0), (1, 1)}. It is easy to find the optimal one-step sampling distribution is uniform distribution among {(0, 0), (1, 1), (0, 1), (1, 0)}, which is incorrect. It indicates that the widely used method which predicts the next group of tokens is fundamentally impossible to apply for few-step generation. We will see such experimental evidence in Sec. 5. i=1 pijk (cid:80)N Additionally, attempting to model the joint distribution by outputting the probability distribution over all possible next mk tokens is also impractical because of the large number mk of possible values, where is the codebook size, which is typically several thousands or even tens of thousands. 3.2 CONSTRUCTING DETERMINISTIC TRAJECTORIES FROM GAUSSIAN TO DATA THROUGH AR FLOW MATCHING As discussed in section Sec. 3.1, training model to generate all the tokens in single run is impossible due to the issue of one-to-many mapping. Therefore, constructing one-to-one mapping is the key for training model to generate more tokens simultaneously. The process is illustrated in Fig. 5. Construct mapping for single token. Consider the sampling process of single token qi RC given (q1, , qi1). Inspired by the knowledge distillation method for diffusion model (Luhman & Luhman, 2021; Salimans & Ho, 2022; Song et al., 2023; Song & Dhariwal, 2023; Kim et al., 2023) which map noise to data, we propose to use noise token as additional information to determine this single token. Specifically, we sample noise token from prior distribution and map it to the generated token. We hope that (1) every noise token will be transferred to deterministic data token in the codebook, and (2) the distribution of generated token equals to pÎ¸(qiqi1, , q0) given by the pre-trained AR model. We propose to use flow matching as the desired mapping for the single token. We operate on the continuous embedding space EC of each token given by the codebook = (c1, , cv) RV C. Since the distribution of the next token is discrete probabil-"
        },
        {
            "title": "Preprint",
            "content": "ity distribution among all tokens in the codebook: P(qi = cj) = pj, where pj 0, (cid:80)V j=1 pj = 1, it can also be viewed as weighted sum of point distributions in the continuous embedding space: p(qi) = (cid:80)V j=1 pjÎ´(qi cj), where Î´() is the Dirac function. Denoting Ït as the marginal distribution at timestep t, we set this weighted sum of point distributions as the target distribution Ï1 and apply standard Gaussian distribution as the source distribution Ï0. We further choose the linear interpolation used in Liu et al. (2022) as the perturb function: Ï(z0, z1, t) = (1 t)z0 + tz1. Then the velocity field in the flow matching framework is given as: (x, t) = (cid:80)V j=1 pj(cj x)e (xtcj )2 (1t)2 (1 t) (cid:80)V j=1 pje (xtcj )2 (1t)2 (3) In this way, we construct mapping from noise token embedding Ïµi RC to the generated token embedding qi with the ODE dx = (x, t)dt while keeping the distribution of qi unchanged. We denote this mapping as qi = (Ïµi, pÎ¸(q<i)) (4) In practice, we use numerical solvers (Lu et al., 2022a;b) to solve the ODE process, so qi will not have an exact match in the codebook. To tackle that, we will use the token closest to qi in the codebook. Construct the whole trajectory along the AR generation process. After constructing mapping from (q1, , qi1, Ïµi) to (q1, , qi1, qi), we can extend such mapping to situations with arbitrary length of subsequent noise tokens (q1, , qi1, Ïµi, , Ïµm) via an iterative way. After generating the current token qi from noise Ïµi, we can update the sequence with (q1, , qi, Ïµi+1, , Ïµn) and get the conditional probability p(qi+1qi, , q1). Then the same method can be applied to transfer the next noise token Ïµi+1 to data token qi+1, until all the subsequent noise tokens are mapped to the corresponding generated data. This process imposes no constraints on the length of the prefix, so we can start from pure noise sequence (Ïµ1, , Ïµn). In this way, we construct an AR trajectory {Xi}n+1 i=1 from pure noise to the final data using flow matching, given as: Xi = (q1, , qi1, Ïµi, , Ïµn) (5) The recurrence relation is given by Eq. (4). 3.3 DISTILLING AR ALONG THE TRAJECTORY The AR trajectory transfer the pure noise sequence progressively to the final data sequence, thus is suitable for neural network to learn. To enable trade-off between sample quality and sample step, we train the model to predict the final data not only at the beginning of the trajectory but also at intermediate points. Based on this, we first clarify the notation, discuss the model parameterization and training loss, and finally introduce the overall workflow of distillation and sampling. Notation. Suppose = (x1, , xn) and = (y1, , yn) are two arbitrary sequence. We denote X[: t] as sub-sequence with the first tokens in and X[t + 1 :] is the rest part: X[: t] = (x1, , xt), X[t + 1 :] = (xt+1, , xn). 2 Additionally, we define the Concat operation as concatenating two sequences: Concat(X, ) = (x1, , xn, y1, , yn). Model parameterization. The model takes an intermediate value Xt = (q1, , qt1, Ïµt, , Ïµn) of the trajectory and the position as input, and output the final data Xn+1 RnC corresponding to the Xt. Suppose we have neural network fÎ¸ and denote its output as fÎ¸(X) = (fÎ¸1, , fÎ¸n). Since the first 1 tokens of Xt is already correct tokens, they can be directly used as the first 1 tokens of the predicted results. We thus parameterize the model output as: FÎ¸(Xt, t) = Concat(Xt[: 1], fÎ¸(Xt)[t :]) = (q1, , qt1, fÎ¸t, , fÎ¸n) (6) Training loss. We train the model by minimizing the distance between its output and the target data sequence, which gives the following loss: = E[Î»(t)d(FÎ¸(Xt, t), Xn+1)] (7) Denote {t1, . . . , tL} as the set from which timestep can be sampled, where ti [1, n] and t1 = 1, and Xt = Concat(Xn[: 1], X1[t :]), where Xn is the final generated data by the pre-trained AR model corresponding to X1. The expectation of Eq. (7) is taken with respect to Uniform{ti}L i=1 and X1 (0, I). Î»() = 0 is step-wise weighted function. d(, ) is any distance function that satisfies d(x, y) 0 and d(x, y) = 0 if and only if = y. 2Note that this is different from Python indexing."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: The training and generation workflow of DD. Given X1 with noise tokens Ïµi, the whole trajectory X1, , X5 consists of data tokens qi and noise tokens Ïµi is uniquely determined (Sec. 3.2). Assuming the timesteps are set to {t1 = 1, t2 = 3}. During training (Sec. 3.3), we train DD model to reconstruct X5 given X1 or X3 as input. The DD will then have the capability of jumping from X1 and X3 to any point in the later trajectory (e.g., X1 to any of {X2, , X5}). During generation (Sec. 3.3), we can either do 1-step (X1 X5) or 2-step generation (X1 X3 X5). Additionally, we can do generation with more steps by incorporating the teacher AR model in part of the generation process, such as 3-step generation X1 X2 X3 X5 where X2 X3 utilizes the AR model and other steps use the DD model. Overall distillation workflow  (Fig. 6)  . Our overall workflow consists of two parts: generating the training set and training the model. First, we iteratively draw noises from standard Gaussian distribution and calculate the deterministic generated data through the method in Sec. 3.2, to construct dataset of noise-data pairs (see Alg. 1). Then we train the model with Eq. (7) using the dataset (see Alg. 2). In this case, the noise-data pair (X1, Xn) is directly drawn from the dataset. Generation  (Fig. 6)  . With one trained DD model using the above workflow, we can flexibly adjust the trade-off between sample quality and the number of generation timesteps during inference (Alg. 3). Specifically, any subset of the training timesteps {tk1 , , tkl } where tk1 = 1 can be chosen as timestep path from noise to data. We can choose only the first timestep {t1} to do one-step generation, or leverage more timesteps to improve sample quality. Utilizing the AR property of the trajectory, jumping from step tki to tki+1 can be implemented by predicting all tokens at timestep tki and only keeping the ones before tki+1. Additionally, we can utilize the pre-trained AR model to achieve more flexibility in the trade-off between sample quality and timestep. Specifically, we can use the pre-trained AR model to generate from any position along the sampling process until reaching the next trained timestep for few-step sampling. Alg. 4 is an example of 2 + tk2 ts step sampling with the pre-trained model inserted into the original two-step sampling process. We use the DD model on the first step, re-generate the last tk2 ts tokens with the pre-trained AR model, and then apply the DD model for the second step. Discussions. As deep synergy between AR and flow matching, DD possesses interesting properties that none of the flow matching (which is closely related to diffusion models) and AR have. Compared to AR, DD has the capability of adjusting the number of generation steps. Compared to flow matching (and diffusion models), DD uses AR to construct the trajectories, and therefore, DD has an easy way to jump back to any point in the trajectory by replacing the last few generated tokens with the original noise tokens, which we utilize in the sampling procedure above. This property enables DD to keep the trajectory unchanged when sampling with multi-steps, in contrast to other intermediate-to-final distillation methods for diffusion models (Song et al., 2023; Song & Dhariwal, 2023) which cannot maintain the trajectory during multi-step sampling. Another point to mention is that the DD does not require the training data of the original AR model, as opposed to some related work (Li et al., 2024b; Song et al., 2023). This makes DD more practical in cases where the training data of the pre-trained AR models is not released."
        },
        {
            "title": "4 RELATED WORK\n4.1 DECREASING THE NUMBER OF SAMPLING STEPS OF AR MODELS",
            "content": "Image generation. (1) Mask-based image AR methods (Chang et al., 2022; Li et al., 2023; 2024a) adopt random masking ratio when training, allowing them to directly perform trade-off between performance and number of steps. Specifically, these models are trained to predict an arbitrary number of tokens given some other arbitrary number of tokens. By increasing the number of tokens generated in each step, they can sample faster at the cost of loss in generation quality. However, the fewest number of generation steps evaluated in these works is only 8 and we show in Sec. 3."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Generate dataset Algorithm 2 Training Require: Î¸Î¦: The pre-trained AR model for = 1, , do Generation Process 1: 2: for = 1, , do 3: X1 (Ïµ1, , Ïµn) (0, I) 4: 5: 6: 7: Xn (q1, , qn) 8: 9: end for 10: return qt (Ïµt, pÎ¸Î¦ (q<t)) {(X1, Xn)} end for Require: :Generated dataset with noise-data pairs D, the pretrained AR model Î¸Î¦. Hyper-parameter : Available timesteps for training {t1, , tL}, learning rate Î·. Sample (X1, Xn) D, {t1, , tL} Training Process 1: Î¸ initialized from Î¸Î¦ 2: for = 1, , Iter do 3: 4: Xt Concat(Xn[: 1], X1[t :]) 5: 6: 7: 8: end for 9: return Î¸ ËXn FÎ¸(Xt, t) d( ËXn, Xn) Î¸ Î¸ Î·Î¸L Algorithm 3 Sampling Algorithm 4 Sampling with the pre-trained AR model Require: : The distilled few-step model Î¸, sampling timesteps {tk1 , , tkl }. Sampling Process 1: X1 (Ïµ1, , Ïµn) (0, I), X1 2: for in {tk1 , , tkl } do 3: Concat(X[: 1], X1[t :]) 4: FÎ¸(X, t) 5: end for 6: return Require: : The distilled few-step model Î¸, the pre-trained AR model Î¸Î¦, sampling timesteps {tk1 = 1, tk2 }, starting timestep of pre-trained model tk1 < ts < tk2 . Sampling Process 1: X1 (Ïµ1, , Ïµn) (0, I), X1 2: Concat(X[: tk1 1], X1[tk1 :]) 3: FÎ¸(X, tk1 ) 4: Concat(X[: ts 1], X1[ts :]) 5: for in {ts, , tk2 1} do 6: 7: X[t] qt 8: end for 9: FÎ¸(X, tk2 ) 10: return Sample qt pÎ¸Î¦ (X[: 1]) that they fundamentally cannot support very few steps (e.g., one or two steps). transformer-based methods, how to reduce their number of steps is still unknown. (2) For causalText generation (large language models). There are already many works focusing on speeding up the generation of LLMs (Zhou et al., 2024). (1) Speculative decoding first generates draft of multiple following tokens and then applies the target LLM to verify these tokens in parallel. The draft tokens can either be generated (a) sequentially by another (small) AR model (Chen et al., 2023; Leviathan et al., 2023; Li et al., 2024b) or (b) in parallel by specially trained models or heads (Cai et al., 2024; Gloeckle et al., 2024; Xia et al., 2023). Method (a) does not reduce the number of generation steps and therefore is irrelevant to our few-step generation goal. Method (b) fundamentally cannot match the distribution of the target AR model (Sec. 3.1). As result, even when generating small number of tokens at time, they rely on verification step to ensure correctness, let alone one-step generation. (2) In contrast to token-level parallelism exploited in speculative decoding, content-based parallelism is another paradigm that exploit the weak relation between different parts of the generated content (Ning et al., 2024a; Liu et al., 2024; Jin et al., 2024). These methods prompt or train the LLM to generate an answer outline, enabling the LLM to generate independent points in the outline in parallel. However, these methods change the output distribution (3) Conceptually similar to our methods, CLLMs (Kou et al., 2024) of the original AR model. utilize Jacobi decoding trajectory (Song et al., 2021; Santilli et al., 2023) and train student model to conduct one-step sampling along the trajectory. However, CLLMs only support greedy decoding, which deviates from the (non-deterministic) generated distribution of the original AR model. In contrast to all these methods, DD supports one-step sampling while theoretically having the potential to match the output distribution of the original AR model. 4.2 DIFFUSION DISTILLATION Similar to AR models, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) also suffer from large number of sampling steps. Knowledge distillation is promising approach to address this problem and has been widely studied. These methods leverage the trajec-"
        },
        {
            "title": "Preprint",
            "content": "tory constructed by the pre-trained diffusion model, and train neural networks to directly predict the value multiple steps ahead in the trajectory, rather than predicting just one step ahead as in the pre-trained model. One key difference between various knowledge distillation methods lies in how the starting and ending points of multi-step skipping are selected. The earliest method (Luhman & Luhman, 2021) trains the model to skip the entire trajectory (i.e., predicting the final data directly from the initial Gaussian noise), enabling one-step sampling. PD (Salimans & Ho, 2022) progressively merges two adjacent steps into one, which makes the optimization task smoother and provides trade-off between steps and quality. CM (Song et al., 2023; Song & Dhariwal, 2023) skips from any intermediate points along the trajectory to the final data. CTM (Kim et al., 2023) proposes more general framework which enables the transfer between any two points in the trajectory. Unlike diffusion models, AR models do not come with these deterministic trajectories, and therefore, doing knowledge distillation on AR is not straightforward. One of our key innovations is the construction of deterministic trajectories out from pre-trained AR (Sec. 3.2), thereby making knowledge distillation for AR possible. In this paper, we only explored simple knowledge distillation paradigm, and we hope that this work opens the door for more knowledge distillation paradigms (including the ones above) for AR in the future. See App. for more discussions on related works (Li et al., 2024a; Yin et al., 2024)"
        },
        {
            "title": "5 EXPERIMENTS\nIn this section, we use DD to accelerate pre-trained image AR models and demonstrate its effective-\nness by comparing with the original model and other baselines. More details are in App. C",
            "content": "5.1 SETUP Training. For pre-trained AR models, we choose VAR (Tian et al., 2024) and LlamaGen (Sun et al., 2024) for the following reasons: (1) Both methods are recently released, very popular, and have achieved excellent performance (e.g., LlamaGen claims to beat diffusion models on image generation). (2) Their token sequences are defined very differently (Sec. 2.2): VAR uses images of different resolutions to construct the sequence, while LlamaGen adopts traditional method where tokens are latent pixels and arranged in raster order. We want to test the universality of DD across different token sequence definitions. (3) Their number of generation steps vary significantly: VAR has 10 steps while LlamaGen uses 256 steps. We want to test whether DD is able to achieve few-step generation no matter whether the AR model has small or large number of steps. (4) They both provide different sizes of pre-trained models for us to study how DD scales with model sizes. Following these works, we choose the popular ImageNet 256256 (Deng et al., 2009) as the benchmark for our main results. We first generate 1.2M data-noise pairs use each of the two models. We set the number of available timesteps as 2 and use {1, 6}, {1, 81} as {tk1, tk2} for VAR, LlamaGen, respectively. Experimental results show that DD can compress both models to 1 or 2 steps with comparable quality. We further evaluate DD on text-to-image models to assess its capability in handling large-scale tasks. Specifically, we select the stage-1 text-to-image model from LlamaGen (Sun et al., 2024) as our base model, which uses 256 steps for generation. For training, we generate 3M data-noise pairs using prompts randomly sampled from the LAION-COCO dataset. For evaluation, we generate images based on 5k additional prompts from the LAION-COCO dataset and report the FID score between the generated images and their corresponding real images. All other settings are consistent with those used for the ImageNet dataset. Generation. We apply Alg. 3 with 1 and 2 steps as our main results in Sec. 5.2. We additionally use Alg. 4 in Sec. 5.3 for better quality with more generation steps. We use series of staring timesteps for the pre-trained model to get smooth trade-off between generation cost and quality. Baselines. Since there is no existing method for decreasing the generation steps of visual AR with causal transformers (Sec. 4), we design two baselines: (1) Directly skipping the last several steps, denoted as skip-n where is the number of skipped steps, and (2) predicting the distribution of all tokens in one step with the optimal solution in Prop. 3.1, denoted as onestep*. We also compare with the base pre-trained AR models and other generative models for comprehensiveness. 5.2 MAIN RESULTS The main results of DD are shown at Tab. 1. The key takeaways are:"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Generative performance on class-conditional ImageNet-256. #Step indicates the number of model inference to generate one image. Time is the wall-time of generating one image in the steady state. Results with are taken from the VAR paper (Tian et al., 2024). Type GAN Diff. Diff. Diff. Diff. Mask. AR AR AR AR AR AR AR AR AR AR Model StyleGan-XL (Sauer et al., 2022) ADM (Dhariwal & Nichol, 2021) LDM-4-G (Rombach et al., 2022) DiT-L/2 (Peebles & Xie, 2023) L-DiT-7B (Peebles & Xie, 2023) MaskGIT (Chang et al., 2022) VQVAE-2 (Razavi et al., 2019) VQGAN (Esser et al., 2021) VQGAN (Esser et al., 2021) ViTVQ (Yu et al., 2021) RQTran. (Lee et al., 2022) VAR-d16 (Tian et al., 2024) VAR-d20 (Tian et al., 2024) VAR-d24 (Tian et al., 2024) LlamaGen-B (Sun et al., 2024) LlamaGen-L (Sun et al., 2024) Baseline Baseline Baseline Baseline Baseline Baseline VAR-skip-1 VAR-skip-2 VAR-onestep* LlamaGen-skip-106 LlamaGen-skip-156 LlamaGen-onestep* Ours Ours Ours Ours Ours Ours Ours Ours Ours Ours VAR-d16-DD VAR-d16-DD VAR-d20-DD VAR-d20-DD VAR-d24-DD VAR-d24-DD LlamaGen-B-DD LlamaGen-B-DD LlamaGen-L-DD LlamaGen-L-DD FID 2.30 10.94 3.60 5.02 2.28 6.18 31.11 18.65 15.78 4.17 7.55 4.19 3.35 2.51 5.42 4.11 9.52 40.09 157.5 19.14 80.72 220. 9.94 7.82 9.55 7.33 8.92 6.95 15.50 11.17 11.35 7.58 IS Pre Rec #Para #Step Time 265.1 101.0 247.7 167.2 316.2 182.1 45 80.4 74.3 175.1 134.0 230.2 301.4 312.2 193.5 283. 178.9 56.8 80.39 12.13 193.6 197.0 197.2 204.5 202.8 222.5 135.4 154.8 193.6 237.5 0.78 0.69 0.75 0.83 0.80 0.36 0.78 0.84 0.84 0.82 0.83 0.85 0.68 0.46 0.42 0.17 0.80 0.80 0.78 0.82 0.78 0.83 0.76 0.80 0.81 0.84 0.53 0.63 0.57 0.58 0. 0.57 0.26 0.48 0.51 0.53 0.44 0.48 0.54 0.50 0.43 0.20 0.37 0.41 0.38 0.40 0.39 0.43 0.26 0.31 0.30 0.37 166M 554M 400M 458M 7.0B 227M 13.5B 227M 1.4B 1.7B 3.8B 310M 600M 1.03B 111M 343M 310M 310M 343M 343M 327M 327M 635M 635M 1.09B 1.09B 98.3M 98.3M 326M 326M 250 250 250 250 8 5120 256 256 1024 68 10 10 10 256 256 9 8 1 150 100 1 1 2 1 2 1 2 1 2 1 0.3 168 31 >45 0.5 19 24 >24 21 0.133 - - - 5.01 0.113 0.098 2.94 1.95 0.021 (6.3) 0.036 (3.7) - - - - - - 0.023 (217.8) 0.043 (116.5) DD enable few-step generation of AR models. By comparing the required generation step and time between the pre-trained models (VAR and LlamaGen) and our distilled models (VAR-DD and LlamaGen-DD), we can see that DD compresses the step and accelerates the pre-trained AR by very impressive ratio (6.3 on VAR and 217.8 on LlamaGen). Notably, DD decreases the generation step and time of LlamaGen by two orders. Baselines do not work for few-step generation. From Tab. 1, we can see that DD steadily surpasses the two types of baselines. For skip baseline, we find that the performance rapidly declines as the number of skipped steps increases. The onestep* indeed does not work as expected in Sec. 3.1, due to the lack of correlation between tokens. DD does not sacrifice quality too much. While achieving significant speedup, DD does not sacrifice the generation quality too much. For VAR, the FID increases of one-step and two-step generation are smaller than 6 and 4, respectively. For LlamaGen which has more original steps, the FID increase is around 3.5 for two-step generation. Note that as DD learns the mapping given by the pre-trained AR, its performance is bounded by the pre-trained model. Such results have already outperformed many other popular methods like ADM (Dhariwal & Nichol, 2021) with much faster generation speed. DD scales well with different model sizes. We experiment with two different sizes of LlamaGen and three different sizes of VAR. DD always achieves reasonable sample quality across different model sizes. In addition, for each model family, with larger model sizes, DD can achieve better FID. These results suggest that DD works and scales well with different model sizes. DD allows users to choose the desired trade-off between quality and speed. From Tab. 1, we can see that generation with two steps has better quality than generation with one step, indicating that DD offers trade-off between quality and step, which is property that AR models using causal transformer (such as VAR and LlamaGen) do not have."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Generation quality of involving the pre-trained AR model when sampling. The notation pre-trained-n-m means that the pre-trained AR model is used to re-generate the n-th to 1-th tokens in the sequence generated by the first step of DD. Type Model FID IS Pre Rec #Para #Step AR AR Ours Ours Ours Ours Ours Ours VAR (Tian et al., 2024) LlamaGen (Sun et al., 2024) VAR-pre-trained-1-6 VAR-pre-trained-4-6 VAR-pre-trained-5-6 LlamaGen-pre-trained-1-81 LlamaGen-pre-trained-41-81 LlamaGen-pre-trained-614.19 4.11 5.03 5.47 6.54 5.71 6.20 6.76 230.2 283.5 242.8 230.5 210.8 238.6 233.8 231.4 0.84 0.865 0.84 0.84 0.83 0.83 0.83 0. 0.48 0.48 0.45 0.43 0.42 0.43 0.41 0.40 310M 343M 327M 327M 327M 326M 326M 326M 10 256 6 4 3 81 42 Time 0.133 5.01 0.090 (1.5) 0.062 (2.1) 0.045 (2.6) 1.725 (2.9) 0.880 (5.7) 0.447 (11.2) Table 3: Generation results of DD on text-to-image task. Type AR Ours Ours Model FID #Param #Step LlamaGen LlamaGen-DD LlamaGen-DD 25.70 36.09 28.95 775M 756M 756M 1 2 Time 7.90 0.052 (151.9) 0.085 (92.9) 5.3 GENERATION INVOLVING THE PRE-TRAINED MODEL In this section, we test the method in Alg. 4 which utilizes both the distilled model from DD and the pre-trained AR model to provide more trade-off points between quality and step. As discussed in Sec. 3.3, we use the pre-trained AR model to re-generate token from ts to tk2 1, denoted as pre-trained-ts-tk2 . Results are shown at Tab. 2. We can see this generation method offers flexible and smooth trade-off between quality and step. For example, for VAR, by totally replacing the first step with the pre-trained model, DD reaches FID of 5.03 which is very close to the pre-trained model, while achieving 1.5 speedup. For LlamaGen, DD achieves FID 6.76 with 11.2 speedup. 5.4 TEXT-TO-IMAGE GENERATION In this section, we report the results of DD on text-to-image generation in Tab. 3. We can see that DD achieves performance comparable to the pre-trained model (i.e., 25.70 v.s. 28.95) with only 2 sampling steps. Generated examples are demonstrated in Fig. 2 and App. F. 5.5 ABLATION STUDY Effect of the training iteration. From Fig. 7, we can see that the few-step FID decreases rapidly in the first few epochs, demonstrating the high adaptability of the pre-trained AR model across different tasks, which contributes to the smaller distilling cost of DD than training AR models from scratch. Effect of the intermediate timestep. From Fig. 7, we find that the convergence speed and performance of different intermediate timesteps are similar, indicating that DD is not sensitive to it. Effect of the dataset size. From Fig. 8, we find that the DD can still work when there is only 0.6M (data, noise) pairs. Additionally, with more (data, noise) pairs, the performance improves. The results demonstrate DDs robustness to limited data and its data scaling ability."
        },
        {
            "title": "6 LIMITATIONS AND FUTURE WORK\nWe discuss the limitations of DD and provide several future work directions.",
            "content": "Training few-step AR models without teachers. In this work, we distill pre-trained teacher AR models to support few-step sampling. The sample quality is therefore bounded by the pre-trained AR model. It would be interesting to explore if it is possible to eliminate the need for teacher, which offers more flexibility and potentially can lead to better sample quality. As discussed in Sec. 4.2, the trajectory construction in DD opens up the opportunity to apply other diffusion distillation approaches to AR models. One potential direction is to apply the teacher-free consistency training approaches (Song et al., 2023; Song & Dhariwal, 2023) on AR models."
        },
        {
            "title": "Preprint",
            "content": "Applications on LLMs. Theoretically, DD can be directly applied to LLMs. However, different from visual AR models, the codebook of LLMs is much larger in both size and embedding dimensions. Additionally, the sequence length of LLMs may be much longer. Therefore, applying DD on LLMs introduces new challenges to be solved. Questioning the fundamental trade-off between inference cost and performance. It was recently believed that for AR models scaling up the inference compute and number of generation steps is important for getting better performance, as exemplified by various LLM prompting techniques (Wei et al., 2022; Nori et al., 2023; Ning et al., 2024b; Snell et al., 2024) and the inference scaling law presented in OpenAI o1 (OpenAI, 2024). However, as our experiments demonstrated, it is possible to significantly reduce the inference compute and generation steps without losing too much fidelity, at least for current image AR models. This suggests that while the inference compute is important, current models might be wasting compute in the wrong place. It would be interesting to study (1) where the optimal trade-off between inference cost and performance is, (2) how much current models are away from that optimal trade-off, and (3) how to modify current models or design new models to reach that optimal trade-off."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "The authors would like to thank Sergey Yekhanin and Arturs Backurs of Microsoft Research for their support and suggestions of this work, Qian Chen for his help with experiments, and the anonymous reviewers for their valuable feedback and suggestions."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In International conference on machine learning, pp. 864872. PMLR, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel SynarXiv preprint naeve. Better & faster large language models via multi-token prediction. arXiv:2404.19737, 2024."
        },
        {
            "title": "Preprint",
            "content": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Shuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Morley Mao, Atul Prakash, Feng Qian, and Danyang Zhuo. Adaptive skeleton graph decoding. arXiv preprint arXiv:2402.12280, 2024. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21422152, 2023. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024a. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. Apar: Llms can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-ofthought: Prompting llms for efficient parallel generation. In The Twelfth International Conference on Learning Representations, 2024a. Xuefei Ning, Zifu Wang, Shiyao Li, Zinan Lin, Peiran Yao, Tianyu Fu, Matthew Blaschko, Guohao Dai, Huazhong Yang, and Yu Wang. Can llms learn by teaching? preliminary study. arXiv preprint arXiv:2406.14629, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023. OpenAI,"
        },
        {
            "title": "Sep",
            "content": "2024. learning-to-reason-with-llms."
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Alec Radford. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pp. 97919800. PMLR, 2021. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024."
        },
        {
            "title": "Preprint",
            "content": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decodIn Findings of the ing: Exploiting speculative execution for accelerating seq2seq generation. Association for Computational Linguistics: EMNLP 2023, pp. 39093925, 2023. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024. PROOF OF PROP. 3. Proof. Since all positions in the sequence are symmetrical, we can just consider one certain position j. The loss on this position is Due to the normalization constraints of probability, we have Lj = 1 (cid:88) (cid:88) pijklogËpÎ¸jk i=1 k=1 Considering the Lagrange Multiplier Method, the Lagrange function is given as k=1 (cid:88) ËpÎ¸jk = 1 F(ËpÎ¸j1, , ËpÎ¸jV , Î») = 1 (cid:88) (cid:88) pijklogËpÎ¸jk Î»( (cid:88) ËpÎ¸jk 1) i=1 k=1 k=1 The optimal ËpÎ¸jk and the corresponding Î» satisfy Î» = 0, = 1, , ËpÎ¸jk Î» = = (cid:80)N i=1 pijk ËpÎ¸jk (cid:88) k=1 ËpÎ¸jk 1 = 0 15 (8) (9) (10) (11) (12)"
        },
        {
            "title": "Preprint",
            "content": "By solving the above equations, we can get the final solution k=1 pijk i=1 pijk ËpÎ¸jk = (cid:80)N (cid:80)N Î» = (cid:80)V i=1 = 1 (13) (14)"
        },
        {
            "title": "B MODEL ARCHITECTURE DESIGN",
            "content": "Since our method does not alter the AR property nature of the generation task, we can use the same architecture as the pre-trained model while slightly modify several modules as discussed below. Aside from these adjusted modules, all other modules inherit the weights from the teacher model for quicker convergence. Two final heads for logits and embedding prediction. The task of the model is to output the token in the codebook (with shape of C) which corresponds to the input noise. There are two approaches to achieve this goal: we can view the problem as classification problem among all tokens in the codebook, or we can treat it as regression task aimed at outputting the correct embedding. Therefore, we set two final heads after the transformer backbone. For each token, one outputs RV as the predicted logits among all tokens in the codebook, and the other outputs RC as the predicted embedding. For logits output, we use cross entropy as the distance function in Eq. (7), while we apply LPIPS loss (Zhang et al., 2018) for the embedding prediction. The overall objective is weighted sum of the two losses. We empirically find that the predicted logits perform well when is small and much worse when is large, while the predicted embeddings perform the opposite. Thus we simply set split point and use the predicted logits for small ts and the predicted embeddings for the rest. Additional embeddings for noise and data tokens. Since data tokens and noise tokens are fed in the model simultaneously, and their distribution differs significantly, it is necessary for the network to distinguish between them. Therefore, before the transformer block, We add two different learnable embeddings to the data tokens and noise tokens, respectively. The two embeddings are randomly initialized. Positional Embedding. Actually, the processed sequence of our model is one token longer than the pre-trained AR model. It is because the pre-trained model needs 1 class label token and 1 generated token to obtain the whole sequence, while our model needs 1 class label token and noise token to generate the whole sequence. Thus, for models who have positional embedding, we increase its length by 1 and randomly initialize this new part. Attention Mask. The attention mask of the pre-trained AR model also can not be used directly due to the mismatch in length. In our case, we let every token see all previous tokens. For VAR, all tokens generated at the same step can see each other as well."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "In this section, we introduce more details of the experiments in Sec. 5. Dataset Generation. In the dataset generation phase, we use Alg. 1 to construct the data-noise pairs. For the AR models, we set the classifier-free guidance scale to 2.0 for both VAR and LlamaGen, while other settings follow the default configuration of these two works. In the flow matching process, we employ the perturb function from Rectflow (Liu et al., 2022): Ï(x0, x1, t) = (1t)x0+tx1. We use the DPM-Solver (Lu et al., 2022a;b) to efficiently solve the FM ODE. Specifically, after calculating the velocity model given by Eq. (3), we wrap the model function (x, t) to get the noise prediction model Ïµ(x, t): Ïµ(x, t) = tV (x, t). Then we can directly use the official implementation of DPM-Solver (see https://github.com/LuChengTHU/dpm-solver) to conduct sampling with the noise prediction model and the Rectflow noise schedule. Our configuration includes 10 NFE multistep DPM-Solver++ with an order of 3. Training Configuration. We follow most of the training configuration for the pre-trained AR model. For VAR, we use batch size of 512, base learning rate 1e-4 per 256 batch size, and"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: The training curve of FID vs. epoch or iteration for different intermediate timesteps. FIDs are calculated with 5k generated sample. Figure 8: The training curve of FID vs. epoch for different dataset sizes. FIDs are calculated with 5k generated sample. an AdamW optimizer with Î²1 = 0.9, Î²2 = 0.95. For LlamaGen, all other settings are the same except for the learning rate, which is fixed at 1e-4 and doesnt vary with the batch size. We distill the VAR model for 120 epochs and LlamaGen model for 70 epochs. We additionally use exponential moving average (EMA) with rate of 0.9999. As discussed in App. B, our model has two types of prediction results, therefore two types of loss. We assign loss weight of 1.0 for the embedding loss (LPIPS) and weight of 0.1 for the logits loss (cross entropy) to maintain similar loss scale for them. For the timestep-wise loss weight, we use uniform one with Î»(t) = 1."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "In this section, we show the training curve of different experimental settings. Results are demonstrate in Fig. 7 and Fig. 8."
        },
        {
            "title": "E MORE RELATED WORK DISCUSSIONS",
            "content": "In this section, we provide expanded discussions on two related works: DMD (Yin et al., 2024) and MAR (Li et al., 2024a). DMD (Yin et al., 2024) uses distribution matching to distill diffusion models into few steps. Specifically, the main idea of DMD is to minimize the distance between the generated distribution and the teacher distribution at all timesteps. Additionally, it constructs data-noise pairs and conducts direct distillation as regularization loss term to avoid mode collapse. There are several key differences between DD and DMD: (1) Target: DMD focuses on diffusion models, whereas DD focuses on AR. These are two different problems; (2) Technique: In diffusion models, the diffusion process naturally defines the data-noise pairs, which can be directly used for distillation as in DMD. In contrast, AR models do not have pre-defined concept of noise. How to construct noise and the data-noise pair (Sec. 3.2) is one important and unique contribution of our work. That being said, the distribution matching idea in DMD is very insightful, and could potentially be combined with DD to achieve better approaches for few-step sampling of AR models. For example, from high-level perspective, the noisy image in DMD is similar to the partial data sequence in AR, while the next token logits given by the pre-trained AR model can be viewed as the score function in DMD. The objective can be set as minimizing the distance between the output of fake logits prediction network and the pre-trained AR at all timesteps. As long as the next-token conditional"
        },
        {
            "title": "Preprint",
            "content": "distribution of the generated distribution is the same as the distribution given by the pre-trained AR model, the modeled one-step distribution will be correct. There is no requirement for any data-noise pair in this case. MAR (Li et al., 2024a) proposes to replace the cross-entropy loss with diffusion loss in AR, which shares some similarities with DD at high level since both works view decoding as denoising process. The goals of these two works are different though. MAR targets removing the vector quantization in image AR models for better performance, while DD aims to compress the sampling steps of AR, without any modification to the codebook. In this sense, these two works are orthogonal and could potentially be combined to develop new method that leverages the strengths of both."
        },
        {
            "title": "F VISUALIZATION",
            "content": "In this section, we demonstrate samples generated by DD. Results of label conditional generation are demonstrated in Figs. 9 to 12. Examples of text conditional generation are demonstrated in Figs. 13 to 15, where most texts are taken from the LAION-COCO dataset."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Generation results with VAR model (Tian et al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-4-6, and the pre-trained VAR model."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Generation results with VAR model (Tian et al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-4-6, and the pre-trained VAR model."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Generation results with LlamaGen model (Sun et al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-41-81, and the pre-trained LlamaGen model."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Generation results with LlamaGen model (Sun et al., 2024). From left to right: one-step DD model, two-step DD model, DD-pre-trained-41-81, and the pre-trained LlamaGen model."
        },
        {
            "title": "Text Prompts",
            "content": "LlamaGen-256step (Sun et al., 2024) DD-2step (Ours) Red Celtic Heart Knot T-Shirt Diamanttavla Pink Skull Beauty 50x50 CHRYSLER Town Country 2 Drawer Nightstand by PoliVaz Relative Risks etc 1 Pottery Barn Emery Linen Drapes Panels Curtains Blackout Lining 50x63 Red Figure 13: Comparisons of text-to-image results between DD and the pre-trained LlamaGen model (Sun et al., 2024). Prompts without * are taken from the LAION-COCO dataset."
        },
        {
            "title": "Text Prompts",
            "content": "LlamaGen256step (Sun et al., 2024) DD-2step (Ours) WEEKLY or MONTHLY. Soft Touch Genuine Leather Sectional in BARK T-Shirt ToyWatch Mens Plasteramic Diver Watch Wood Wall Behind Tv Ask Designer Series Mistakes Made To Tv Walls Nesting With Grace Activate Upholstered Fabric Armchair Womens Ladies Studded Ankle Strap Espadrilles Platform Shoes Wedge Sandals Size pair of shoes on the floor* Figure 14: Comparisons of text-to-image results between DD and the pre-trained LlamaGen model Sun et al. (2024). Prompts without * are taken from the LAION-COCO dataset."
        },
        {
            "title": "Text Prompts",
            "content": "LlamaGen-256step (Sun et al., 2024) DD-2step (Ours) Butterfly Womens T-Shirt chanel-cruise-collection-fashion-show2016-16-colorful-dresses-outfit (58) tree on hill* Meeting Facility, Holiday Inn Munich-Unterhaching bridge over river* bowl of fresh fruits* Figure 15: Comparisons of text-to-image results between DD and the pre-trained LlamaGen model Sun et al. (2024). Prompts without * are taken from the LAION-COCO dataset."
        }
    ],
    "affiliations": [
        "Department of EE, Tsinghua University",
        "Microsoft Research"
    ]
}