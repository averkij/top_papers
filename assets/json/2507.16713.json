{
    "paper_title": "Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory",
    "authors": [
        "Guowei Lan",
        "Kaixian Qu",
        "René Zurbrügg",
        "Changan Chen",
        "Christopher E. Mower",
        "Haitham Bou-Ammar",
        "Marco Hutter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated experiences during this process are then summarized into a long-term memory, enabling retrieval of learned knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12 real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of ExpTeach."
        },
        {
            "title": "Start",
            "content": "Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory Guowei Lan1 * Kaixian Qu1 * Rene Zurbr ugg1 2 Changan Chen1 Christopher E. Mower3 Haitham Bou-Ammar3 4 Marco Hutter1 1Robotic Systems Lab, ETH Zurich 2ETH AI Center 3Huawei Noahs Ark Lab 4UCL Centre for AI 5 2 0 2 2 2 ] . [ 1 3 1 7 6 1 . 7 0 5 2 : r Figure 1: Robot completes new task guided by previously self-generated memory. We propose grounding VLMs through memory generated by the robot itself from direct hardware experiments, establishing awareness of the robots own capability. When executing novel task, the robot keeps short-term memory that helps the robot reflect and complete the task (illustrated in the grey clip). The experience is then stored as long-term memory and retrieved to guide the VLMs task planning whenever similar scenario is encountered (illustrated in the main figure). Abstract: Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains challenge. This paper presents EXPTEACH, framework that grounds VLMs to physical robots by building self-generated memory of real-world experiences. In EXPTEACH, the VLM autonomously plans actions, verifies outcomes, reflects on failures, and adapts robot behaviors in closed loop. The self-generated experiences during this process are then summarized into long-term memory, enabling retrieval of learned knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, EXPTEACH enhances the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12 real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of EXPTEACH. Keywords: robot memory, vision-language models, task planning *These authors contributed equally to this work. Correspondence to Kaixian Qu <kaixqu@ethz.ch>."
        },
        {
            "title": "Introduction",
            "content": "Recently, large language models (LLMs) have demonstrated near-human performance across range of reasoning tasks, showcasing emergent capabilities in diverse domains such as mathematics and programming [1, 2, 3, 4, 5, 6]. These broad competencies have enabled LLMs to move beyond traditional language tasks and play an increasingly important role in robotics. In particular, they are now widely used in task planning, where LLMs interpret natural language instructions and generate feasible action plans with common-sense reasoning [7, 8, 9, 10, 11]. To address the limitations of text-only input, research has increasingly shifted toward multimodal models, especially vision-language models (VLMs) that jointly process visual and textual data. Recent VLMs [12, 13, 14, 15] exhibit strong multimodal reasoning and high-resolution visual processing. Building on these capabilities, recent work has leveraged VLMs to enable robots to reason about visual inputs and develop closed-loop, autonomous systems [16, 17, 18, 19, 20]. This approach reduces reliance on manually designed components such as explicit scene descriptors [9, 10, 21, 22]. However, grounding VLMs, originally trained on internet data, to diverse real-world robots remains challenge. For example, in the scenario depicted in Figure 1, when seeing tennis ball partially occluded by fan, VLM often confidently instructs the robot to pick up the ball. While this aligns with human intuitionsince humans can often act successfully based on partial visibilityrobots typically struggle to grasp the ball due to imperfect object perception. This raises critical question: how can we make the VLM aware of the specific capabilities of the robot it is assisting? How can we effectively ground VLMs for robotics? One promising direction lies in augmenting VLMs with memory, core brain-based capability underpinning human cognition [23, 24, 25, 26]. Incorporating memory of past experience into LLM agents has shown potential for improving decision-making in complex tasks [27, 28, 29]. In robotics, memory has been used to help agents retain contextual information to support tasks such as navigating familiar environments [30, 31, 32], and to enable more natural and effective humanrobot interaction [21, 33, 34, 35, 36]. Building on these developments, we investigate whether VLMs, when deployed on real robots, can generate their own memory to ground themselves in the specific capabilities and limitations of the robot. This paper presents EXPTEACH, which grounds VLMs for robotics through self-generated memory of past experiences. The central idea behind EXPTEACH is that, even if initially poorly grounded, VLM can autonomously complete instructions efficiently and use the resulting selfgenerated experiences to progressively ground itself via memory mechanism. To realize this, EXPTEACH hinges on three pivotal components: (i) VLM success detector for autonomous feedback; (ii) short-term memory (STM) that enables intelligent adaptation; and (iii) long-term memory (LTM) that stores past experiences. Upon receiving new instruction, EXPTEACH retrieves relevant experiences from the LTM to ground the VLM through retrieval-augmented generation (RAG) [37, 38]. Additionally, EXPTEACH enhances the spatial understanding of VLMs with an on-demand image annotation module. We evaluate our approach on multiple challenging real-world tasks and show that this grounding technique significantly strengthens the robots performance. This paper presents the following key contributions: self-generated memory framework combining short-term and long-term memory to ground VLMs in robotic planning. memory retrieval strategy using RAG to access task-relevant prior experiences from LTM, enabling the robot to act correctly in future tasks with similar instructions and scenes. An on-demand image annotation module that enhances spatial reasoning across multiple skills, leading to more accurate and robust action execution with VLMs. Extensive real-world evaluations demonstrating that EXPTEACH significantly improves success rates through both STM and LTM, and generalizes effectively to unseen tasks."
        },
        {
            "title": "2 Related Work",
            "content": "VLMs in Robotics. LLMs have demonstrated strong capabilities in robotics [8, 7, 10, 9]. Recent developments in VLMs [12, 14] have further propelled the field by integrating visual perception into reasoning [39, 16, 11]. Prior research has explored the use of VLMs in various robotics tasks, such as generating action sequences from visual and language inputs [40, 18], assessing action outcomes and failure recovery [41, 18, 42, 20]. In this work, we build on these approaches by using VLM as both task planner and success detector, enabling closed-loop task execution. Image Annotation for Action Planning. Annotating images is an effective approach to help ground VLMs for precise action planning [43, 44, 45]. Recent works combines object-centric annotations (masks aligned directly with objects) with grasping tools to support more challenging tasks such as semantic object grasping [46, 47, 48]. However, these approaches are typically limited to planar tabletop manipulation or grasping alone. In contrast, EXPTEACH introduces an on-demand image annotation tool that enables grounded actions in 3D space for multiple skills beyond 6-DoF grasping. Self-Reflection and Robot Memory. Recent work has explored ways for LLM-enabled robots to reflect on their failures [22, 49, 50] and to learn from diverse range of experiences [51]. However, these systems typically require external supervision signals, as there is no visual feedback from the LLM itself. Recent advances in VLMs have enabled robots to plan, verify, and self-reflect using single VLM model with raw sensor input [18, 20]. Our approach, EXPTEACH, investigates whether reflection can lead to emergent intelligent object interactions (including tool use) and how such experiences enhance future performance. While aligning with recent work on autonomous memory generation for VLM agents [52], EXPTEACH specifically focuses on grounding VLMs to robots through real-world experiments. Table 1 summarizes the comparison with related approaches. Table 1: Comparison of EXPTEACH with prior LLM/VLM task planning work. LVLM stands for large vision-language model. Interactive Replan refers to interacting with non-target objects after failure to aid task completion. Emergent Tool Use denotes the robot autonomously deciding to use an unmentioned tool and is subset of interactive replanning. Short-Term Memory uses the current action log, while Long-Term Memory leverages past summarized experiences. LVLM Feedback CaP [7] SayCan [8] Inner Mono. [10] RoboTool [53] DROC [21] REFLECT [22] ComeRobot [18] ReplanVLM [20] EXPTEACH (ours) Short-Term Memory Long-Term Memory Interactive Replan Emergent Tool Use Robot Reflect"
        },
        {
            "title": "3 Problem Formulation",
            "content": "An embodied robotic agent is given high-level natural language instruction I, such as Move the screws to the toolbox. The robot is equipped with predefined library of parameterized skills πk Π and operates in closed-loop manner, executing actions step by step while receiving egocentric RGB-D observations. The task planner , implemented as pretrained VLM, is responsible for planning sequence of actions (skill instantiations at = πk(arg)) that fulfill the instruction I. At each timestep t, the planner observes the environment ot through RGB-D input and selects an action at for the robot to execute. In addition, can leverage contextual information to guide planning. Our work investigates how the planner , VLM originally trained on internet data and unaware of the specific embodiment, can be effectively and efficiently grounded to the robots capabilities through this contextual information C. Furthermore, we explore whether such grounding can be achieved autonomouslythat is, whether the VLM can generate on its own by self-correcting its behaviors and using that to ground itself. 3 Figure 2: EXPTEACH pipeline overview. At the start of each task, the system takes the user instruction and egocentric observation o0, which the VLM summarizes into scenario. RAG retrieves relevant experiences from long-term memory (section 4.3) and, together with the instruction and observation, feeds them into the VLM task planner (section 4.1) to generate the next action a. After execution, success is checked by the VLM. If the task is not completed, the action and its feedback are accumulated into short-term memory (section 4.2) and fed back into planning. Once the task is completed, the short-term memory is summarized and stored in long-term memory for future use."
        },
        {
            "title": "4 Method",
            "content": "EXPTEACH leverages the VLM task planner and success detector as the core components for closed-loop planning (section 4.1). The VLM task planner is enhanced with memory mechanism comprising short-term memory (section 4.2), which records actions taken for the current instruction I, and long-term memory (section 4.3), which stores past experiences. In addition, the VLM planner may activate an on-demand image annotation tool to enhance its spatial reasoning capabilities (section 4.4). An overview of the framework is provided in Figure 2."
        },
        {
            "title": "4.1 VLM Task Planning",
            "content": "We use the same VLM for both the task planner and the success detector D. Both components use the function-calling interface to enforce structured output format, such as push(object=\"apple\"). The VLM task planner interprets user instructions and generates action plans from RGB images. After each action is executed, the VLM success detector evaluates the feedback signal from visual observation o. This feedback includes scene description, action outcome (success or failure), failure analysis for recovery, and suggestions for the next action. The feedback is then passed to the task planner via the STM."
        },
        {
            "title": "4.2 Short-Term Memory",
            "content": "Short-term memory in our framework plays critical role in grounding VLMs in robots to dynamically reflect, replan, and recover from failures during task execution. As depicted in Figure 2, the short-term memory operates in an iterative pipeline, interacting closely with the task planner and success detector D. It stores the generated action log until time t. More precisely, each entry records the action and its associated feedback from D, = {(aτ , rτ +1)}t1 τ =0 = {(aτ , D(aτ , oτ +1))}t1 τ =0 (1) In cases of action failure, the short-term memory enables the robot to identify better strategies for achieving the taskfor example, interacting with non-target objects to facilitate grasping or using tools to manipulate small objects. STM can significantly improve success rates within the allotted attempts for challenging tasks."
        },
        {
            "title": "4.3 Long-Term Memory",
            "content": "The LTM plays pivotal role in grounding VLMs for robotics by autonomously accumulating knowledge and experience over time. Specifically, upon successful completion of task, the robots STM contents are summarized by VLM experience summarizer and stored into the LTM as key-value pair (K, E) where the key is the scenario description (which consists of the user instruction and the initial scene description from the RGB image o0), and the value is the associated summarized experience E, as illustrated in Figure 2. When presented with new task, the task planner retrieves relevant prior experiences from the LTM through RAG process. new retrieval key (scenario) is formed by combining the current user instruction and scene description o. This key is embedded into real-valued vector using OpenAIs text-embedding-3-large model [54]. In the LTM, each stored scenario maintains cached embedding from the same model. RAG then uses cosine similarity to retrieve the top-k most relevant past experiences E, which are incorporated into the task planning prompt. Through this retrieval and augmentation mechanism, the robot gains access to relevant prior experiences, allowing it to act In summary, the LTM correctly from the outset. grounds VLMs in robotic experience, providing relevant knowledge that enables the robot to plan correctly and even generalize to unseen but similar situations. Algorithm 1 illustrates this process. Algorithm 1 EXPTEACH Given: Instruction I, initial obs. o0 Internal: Long-term memory 1: EMBED(I, o0) 2: {(Ki, Ei)}k 3: 0, 4: repeat at (I, ot, m, {(Ki, Ei)}k 5: Execute at and receive ot+1 6: rt+1 D(at, ot+1) 7: (at, rt+1) 8: + 1 9: 10: until rt.completed 11: {(K, E(m))} i=1 RAG(K, M) i=1)"
        },
        {
            "title": "4.4 Enhanced Skillset with Image Annotations",
            "content": "We focus on three manipulation skillspick, place, and push. Identifying suitable location for each action is non-trivial: purely geometric grasp planning may seize an undesirable part (e.g., the meat on skewer or the ice-cream top rather than its cone), and effective placing or pushing likewise demands semantic scene understanding. To address these challenges, we introduce an on-demand image-annotation tool Figure 3: Illustration of image annotation tools. that is shared across all skills. Given user instruction and the current RGB frame ot, VLM first selects the skill to execute with its parameters (object name, whether image annotation is needed, etc.). The robot then performs open-vocabulary segmentation with Grounded SAM [55], which integrates Grounding DINO [56] and SAM [57], producing an initial object mask. If the VLM requests second annotation pass, our image overlays set of candidate location masks on the image, allowing the VLM to choose the most appropriate location for the current action a. Figure 3 illustrates this procedure for the pick and push actions. For placing, we apply farthest-point sampling (FPS) [58] on the segmented mask to generate candidate placement locations, whereas for pushing, we draw candidate goal masks that denote the end points. In both cases, the VLM evaluates the annotated options and selects the most suitable location, as illustrated in Figure 3. For grasping, the segmented point cloud is first passed to AnyGrasp [59], which returns set of grasp hypotheses accompanied by confidence scores sconf [0, 1]. Grasp poses that violate kinematic constraints are filtered out through inverse-kinematics checks using 5 Pinocchio [60], yielding the feasible subset G. If the VLM determines that image annotation is beneficial (typically not the case for simple objects like an apple), our annotation tool similarly performs FPS within the object mask to generate collection of numbered location masks for the VLM to select from. The final grasp is selected by maximizing the product of two scores: = arg maxgG sconf (g) sloc(g), where sloc(g) [0, 1] denotes the location score, computed based on the normalized Euclidean distance between grasp and the chosen location. (2)"
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "In our experiments, we use legged manipulator that combines ANYmal [61], quadrupedal robot, with 6-DoF arm. The arm is equipped with Robotiq 2F-140 gripper for object manipulation and ZED Mini Stereo Camera mounted on the elbow. For the VLM, we employ gpt-4o [12], with the prompt details provided in the Appendix. If the robot fails an action and alters the environment, human operator intervenes to reset the scene, after which the robot resumes execution."
        },
        {
            "title": "5.2 Evaluation of Short-Term Memory and Self-Reflection",
            "content": "To evaluate the effectiveness of the STM and reflection module at efficiently generating successful episodes, even after initial failures, we designed series of challenging object manipulation tasks. For the baseline, we use CaP-V, which enhances CaP [7] by incorporating visual feedback. This baseline does not have STM and selects the next action solely based on the current image and user instruction, without the ability to reflect on failed actions. Experimental results in Table 2 highlight the critical role of STM and reflection in achieving successful task completion. Without STM, the robot tends to repeat the same failures without adapting, leading to poor task performance. Table 2: Effect of STM on success rates. Each task is tested 510 times with two attempts allowed. CaP-V"
        },
        {
            "title": "Task",
            "content": "Put apple on plate (container obstructs) Move tiny candy (sponge/towel nearby) Move egg (open view) Pick up bowl (apple inside) 50% 86% 22% 67% 40% 100% 33% 83% Table 3: Effect of LTM on single-trial success rates. EXPTEACH Task ComeRobot [18] Put apple on plate (container obstructs) Move tiny candy (towel nearby) Move egg (open view) Pick up bowl (apple inside) Put tennis ball in box (mug obstructs) Put orange/ball on plate (fan blocks) Move crumpled paper (brush nearby) Move screw (towel nearby) Move sushi (open view) Move grape/cherry (open view) Pick up box (apple on top) Pick up towel (orange on top) 29% 11% 20% 17% 29% 10% 25% 0% 14% 20% 43% 50% 100% 78% 100% 83% 71% 80% 63% 86% 71% 70% 86% 75% With STM, the robot can successfully reflect on its failures, leading to emergent intelligent object interactions (including tool use) and ultimately task success. For example, when instructed to put the apple on the plate, and faced with partially obstructing container, the robot initially fails to grasp the apple due to poor perception. After detecting the failure, the VLM decides to push away the container and successfully retries the grasp (see Figure 1). Similarly, when told to move the candy to the banana, the robot fails to push the candy with its gripper due to insufficient contact. Upon reflection, the VLM autonomously chooses to use sponge as tool to push more effectively (see Figure 4). After cracking an egg while grasping, the VLM similarly learns to push rather than grasp fragile objects. In another case, when asked to collect the bowl, the robot initially drops an apple from the bowl during execution because it overlooked the object. After the user intervenes and rebuilds the scene, the robot reflects and modifies its strategy: should first move the apple to the table before picking up the bowl (see Figure 4). Note that this is the only instance where we let human judge success, as the robot technically picked up the bowl, but the apple dropping was unacceptable and should be treated as failure. 6 Figure 4: Examples of EXPTEACH on different scenarios. In the first STM example (top row), after failing to push the candy with its gripper, the robot reflects and uses sponge as tool. In the second STM example (middle row), after dropping the apple during bowl collection, the robot reflects with user feedback and learns to move the apple first. These experiences are then summarized and stored in the LTM, allowing the robot to generalize its learning to similar future scenarios (bottom row). For example, when asked to Move the screw to the toolbox, the robot immediately decides to use towel to push the screw successfully. Likewise, when tasked with Pick up the milk carton, it remembers to reposition the apple first before retrieving the target item."
        },
        {
            "title": "5.3 Evaluation of Grounding VLMs through Self-Generated Memory",
            "content": "Upon completing the task in section 5.2, the STM is autonomously transferred to the LTM for future use. Together with 96 unrelated experiences from simple tasks, we build an LTM containing 100 entries. For the baseline, we use ComeRobot [18], which does not have access to the LTM. We first evaluate the effectiveness of the LTM by reconstructing the exact same scene with the same objects. As shown in Table 3, the robot successfully recalls the correct action from the LTM with RAG, demonstrating improved performance compared to ComeRobot. We further investigate its ability to generalize. To do so, we modify the scene to create new but structurally similar scenarios, as illustrated in Figure 4. The results in Table 3 show that experience gained from one task successfully transfers to related tasks under similar conditions and significantly improves the success rate."
        },
        {
            "title": "5.4 Ablation Study of Memory Retrieval",
            "content": "To evaluate the effectiveness of our memory retrieval strategy, we measure the VLMs task planning success rate (without execution) across 12 tasks using three retrieval settings, as shown in Table 4. We re-use the self-generated LTM described in section 5.3. Randomly selecting memories (with = 5) yields the worst performance, with success rate of 27%. This is because the likelihood of retrieving task-relevant context is relatively low. Providing the entire LTM leads to better performance (success rate = 67%); however, unfiltered retrieval can overwhelm the model with irrelevant or distracting information, introducing unstable behavior. This aligns with prior findings that excessively long or noisy contexts can degrade model performance, as LLMs may struggle to focus on the most relevant content [62, 63]. In contrast, our retrieval strategy with RAG consistently improves accuracy, achieving success rate of 89%. Most remaining failures occur when the VLM judges the 7 retrieved memory to be misaligned with the current visual scene and thus chooses to rely less on the past experience stored in LTM. Table 4: Ablation study of memory retrieval module on task planning success rates. The success rate here only considers the correct action sequence (without execution). User instructions Entire LTM Random Top via RAG Put apple on the plate (container obstructs) Put tennis ball in the box (mug obstructs) Put tennis ball/orange in the plate (fan blocks) Move tiny candy to banana (sponge/towel nearby) Move tiny paper trash next to the box/banana (brush nearby) Move small screw next to the toolbox/banana (towel nearby) Move egg to sushi/banana (open view) Move sushi next to the plate (open view) Move grape/cherry close to banana (open view) Pick up the bowl (apple inside) Pick up the box (apple on top) Pick up the towel (orange on top) 100% 43% 60% 89% 38% 71% 80% 43% 50% 83% 71% 75% 43% 29% 20% 22% 25% 0% 20% 29% 20% 17% 43% 50% 100% 71% 80% 89% 86% 86% 100% 86% 80% 100% 86% 100%"
        },
        {
            "title": "5.5 Ablation Study of Image Annotation Module",
            "content": "To evaluate the effectiveness of image annotation on grasping tasks, we compared success rates with and without it across 7 objects. grasp was considered successful if it targeted the correct object section (e.g., the stick of meat skewer). As shown in Figure 5, annotation significantly improves success rates for objects with complex shapes that require grasping specific sections (e.g., drumsticks, skewers). Without annotation, AnyGrasp [59] often favors larger surfaces due to its reliance on geometric cues. The failures with annotation are mostly due to inaccurate 3D point clouds. We also evaluated pushing by measuring the distance error to the target location. As shown in the right panel of Figure 5, image annotation consistently reduces pushing errors, highlighting its benefits beyond grasping. Figure 5: Ablation study of the image annotation module. Left: success rates for picking objects (higher is better). Right: distance errors in pushing one object to another (lower is better). The results demonstrate that incorporating image annotation consistently enhances performance."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces EXPTEACH, novel method that equips VLMs with self-generated memory to better ground them in real-world robotic systems. Empirical results show that EXPTEACH enables robots to autonomously reflect and adapt through short-term memory, leading to substantial improvements in task success rates and enabling intelligent object interactions (such as creative tool use). Additionally, extensive experimental results show that aggregating these self-generated shortterm memories grounds the VLM in its own past experience, yielding significant improvements over the state of the art in task planning when faced with new scenarios. Our work can serve as general framework for integrating VLMs with robotic systems and is step further towards the development of general-purpose robotic systems."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "While EXPTEACH demonstrates strong potential in grounding VLMs within robotic systems by autonomously generating memory, it still presents several limitations. First, EXPTEACH currently focuses exclusively on manipulation tasks. However, its core approach is not inherently limited to this domain, and we plan to extend it to mobile manipulation tasks in future work [18]. Second, our current setup assumes that visual feedback alone is sufficient for understanding and decisionmaking. In practice, some real-world scenarios require feedback that cannot be captured by vision alone. Integrating additional modalitiessuch as tactile or auditory signalscould greatly improve the systems capacity to interpret complex, multimodal feedback [22, 49]. Third, while prior work has explored aligning robotic memory with user preferences, EXPTEACH does not yet support this capability. We intend to incorporate user-aligned memory into our framework, enabling more personalized and context-aware behavior [64, 21]. Finally, inspired by Self-Instruct [65], we are also interested in enabling VLMs to autonomously propose tasks, generate instructions, and learn from the outcomes, further reducing the amount of human effort required."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported by the Swiss National Science Foundation through the National Centre of Competence in Digital Fabrication (NCCR dfab), by Huawei Tech R&D (UK) through research funding agreement, by an ETH RobotX research grant funded through the ETH Zurich Foundation, and partially by the ETH AI Center. This work was also conducted as part of ANYmal Research, community to advance legged robotics. We would also like to thank Cesar Cadena for his support and helpful discussions."
        },
        {
            "title": "References",
            "content": "[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [3] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [4] J. Ahn, R. Verma, R. Lou, D. Liu, R. Zhang, and W. Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. [5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [6] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [7] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code In 2023 IEEE International as policies: Language model programs for embodied control. Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. [8] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 9 [9] C. E. Mower, Y. Wan, H. Yu, A. Grosnit, J. Gonzalez-Billandon, M. Zimmer, J. Wang, X. Zhang, Y. Zhao, A. Zhai, et al. Ros-llm: ros framework for embodied ai with task feedback and structured reasoning. arXiv preprint arXiv:2406.19741, 2024. [10] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, pages 17691782. PMLR, 2023. [11] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Conference on Robot Learning, pages 540562. PMLR, 2023. [12] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, arXiv preprint J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. [13] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [14] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [15] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y. Ma, C. Wu, B. Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [16] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, pages 84698488. PMLR, 2023. [17] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [18] P. Zhi, Z. Zhang, Y. Zhao, M. Han, Z. Zhang, Z. Li, Z. Jiao, B. Jia, and S. Huang. Closed-loop open-vocabulary mobile manipulation with gpt-4v. arXiv preprint arXiv:2404.10220, 2024. [19] G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [20] A. Mei, G.-N. Zhu, H. Zhang, and Z. Gan. Replanvlm: Replanning robotic tasks with visual language models. IEEE Robotics and Automation Letters, 2024. [21] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, and D. Sadigh. Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 15172 15179. IEEE, 2024. [22] Z. Liu, A. Bahety, and S. Song. Reflect: Summarizing robot experiences for failure explanation and correction. In Conference on Robot Learning, pages 34683484. PMLR, 2023. [23] J. Zhang. Cognitive functions of the brain: perception, attention and memory. arXiv preprint arXiv:1907.02863, 2019. [24] L. R. Squire. Memory and the hippocampus: synthesis from findings with rats, monkeys, and humans. Psychological review, 99(2):195, 1992. 10 [25] E. Tulving. Episodic memory: From mind to brain. Annual review of psychology, 53(1):125, 2002. [26] J. R. Anderson. The architecture of cognition. Psychology Press, 2013. [27] A. Madaan, N. Tandon, P. Clark, and Y. Yang. Memory-assisted prompt editing to improve gpt3 after deployment. In ACL 2022 Workshop on Commonsense Representation and Reasoning, 2022. [28] X. Tang, T. Hu, M. Ye, Y. Shao, X. Yin, S. Ouyang, W. Zhou, P. Lu, Z. Zhang, Y. Zhao, et al. Chemagent: Self-updating library in large language models improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025. [29] J. Zheng, C. Shi, X. Cai, Q. Li, D. Zhang, C. Li, D. Yu, and Q. Ma. Lifelong learning of large language model based agents: roadmap. arXiv preprint arXiv:2501.07278, 2025. [30] Z. Wang, B. Yu, J. Zhao, W. Sun, S. Hou, S. Liang, X. Hu, Y. Han, and Y. Gan. Karma: Augmenting embodied ai agents with long-and-short term memory systems. arXiv preprint arXiv:2409.14908, 2024. [31] Q. Xie, S. Y. Min, P. Ji, Y. Yang, T. Zhang, K. Xu, A. Bajaj, R. Salakhutdinov, M. JohnsonRoberson, and Y. Bisk. Embodied-rag: General non-parametric embodied memory for retrieval and generation. arXiv preprint arXiv:2409.18313, 2024. [32] M. F. Ginting, D.-K. Kim, S.-K. Kim, B. J. Krishna, M. J. Kochenderfer, S. Omidshafiei, and A.-a. Agha-mohammadi. Saycomply: Grounding field robotic tasks in operational compliance through retrieval-based language models. arXiv preprint arXiv:2411.11323, 2024. [33] R. Arora, N. Narendranath, A. Tambi, S. S. Zachariah, S. Chakraborty, and R. Paul. G2 tr: Generalized grounded temporal reasoning for robot instruction following by combining large pre-trained models. arXiv preprint arXiv:2410.07494, 2024. [34] S. Paplu, R. F. Navarro, and K. Berns. Harnessing long-term memory for personalized humanIn 2022 IEEE-RAS 21st International Conference on Humanoid Robots robot interactions. (Humanoids), pages 377382. IEEE, 2022. [35] I. Idrees, S. P. Reiss, and S. Tellex. Robomem: Giving long term memory to robots. arXiv preprint arXiv:2003.10553, 2020. [36] L. Barmann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour. Incremental learning of humanoid robot behavior from natural interaction and large language models. Frontiers in Robotics and AI, 11:1455375, 2024. [37] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [38] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, and H. Wang. Retrievalaugmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2, 2023. [39] R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu, S. Song, A. Kapoor, K. Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, page 02783649241281508, 2023. [40] Y. Hu, F. Lin, T. Zhang, L. Yi, and Y. Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023. 11 [41] Y. Guo, Y.-J. Wang, L. Zha, and J. Chen. Doremi: Grounding language model by detecting and recovering from plan-execution misalignment. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1212412131. IEEE, 2024. [42] M. Ahn, M. G. Arenas, M. Bennice, N. Brown, C. Chan, B. David, A. Francis, G. Gonzalez, R. Hessmer, T. Jackson, et al. Vader: Visual affordance detection and error recovery for multi robot human collaboration. arXiv preprint arXiv:2405.16021, 2024. [43] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [44] H. Liu, R. Yao, W. Liu, Z. Huang, S. Shen, and J. Ma. Codrivevlm: Vlm-enhanced urban cooperative dispatching and motion planning for future autonomous mobility on demand systems. arXiv preprint arXiv:2501.06132, 2025. [45] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid, Z. Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. In International Conference on Machine Learning, pages 3732137341. PMLR, 2024. [46] K. Fang, F. Liu, P. Abbeel, and S. Levine. Moka: Open-world robotic manipulation through mark-based visual prompting. Robotics: Science and Systems (RSS), 2024. [47] G. Tziafas and H. Kasaei. Towards open-world grasping with large vision-language models. In 8th Annual Conference on Robot Learning, 2024. [48] Y. Qian, X. Zhu, O. Biza, S. Jiang, L. Zhao, H. Huang, Y. Qi, and R. Platt. Thinkgrasp: vision-language system for strategic part grasping in clutter. arXiv preprint arXiv:2407.11298, 2024. [49] Z. Wang, B. Liang, V. Dhat, Z. Brumbaugh, N. Walker, R. Krishna, and M. Cakmak. can tell what am doing: Toward real-world natural language grounding of robot experiences. In 8th Annual Conference on Robot Learning, 2024. [50] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36: 86348652, 2023. [51] T. Kagaya, T. J. Yuan, Y. Lou, J. Karlekar, S. Pranata, A. Kinose, K. Oguri, F. Wick, and Y. You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. In NeurIPS 2024 Workshop on Open-World Agents, 2024. [52] G. Sarch, L. Jang, M. Tarr, W. W. Cohen, K. Marino, and K. Fragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs of thought. Advances in Neural Information Processing Systems, 37:7594275985, 2024. [53] M. Xu, P. Huang, W. Yu, S. Liu, X. Zhang, Y. Niu, T. Zhang, F. Xia, J. Tan, and D. Zhao. Creative robot tool use with large language models. arXiv preprint arXiv:2310.13065, 2023. [54] OpenAI. New embedding models and api updates, 2024. URL https://openai.com/ index/new-embedding-models-and-api-updates/. Accessed: 2025-05-01. [55] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, et al. arXiv preprint Grounded sam: Assembling open-world models for diverse visual tasks. arXiv:2401.14159, 2024. [56] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 12 [57] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [58] Y. Eldar, M. Lindenbaum, M. Porat, and Y. Y. Zeevi. The farthest point strategy for progressive image sampling. IEEE transactions on image processing, 6(9):13051315, 1997. [59] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 39(5):39293945, 2023. [60] J. Carpentier, G. Saurel, G. Buondonno, J. Mirabel, F. Lamiraux, O. Stasse, and N. Mansard. The pinocchio c++ library fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives. In IEEE International Symposium on System Integrations (SII), 2019. [61] M. Hutter, C. Gehring, D. Jud, A. Lauber, C. D. Bellicoso, V. Tsounis, J. Hwangbo, K. Bodie, P. Fankhauser, M. Bloesch, et al. Anymal-a highly mobile and dynamic quadrupedal robot. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 3844. IEEE, 2016. [62] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. [63] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021. [64] M. Abugurain and S. Park. Integrating disambiguation and user preferences into large language models for robot motion planning. arXiv preprint arXiv:2404.14547, 2024. [65] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. arXiv preprint Self-instruct: Aligning language models with self-generated instructions. arXiv:2212.10560, 2022."
        },
        {
            "title": "A Prompts for the VLM Modules",
            "content": "A.1 VLM Scene Describer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 The VLM Scene Describer generates an initial scene description, combining the users instruction as scenario, which is then used as key for memory retrival. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in generating accurate scene descriptions. You always apply chain-of-thought reasoning to ensure accurate and comprehensive scene understanding. The users instruction: {instruction} Based on your observation of the image, provide short scene description focusing on the spatial relationships between the target object and the nearby objects the robot may need to interact with. Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" describe_scene \" , \" description \" : \" Provide brief description of the environment surrounding the target object . \" , \" strict \" : True , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Describe what was observed in the image to generate the scene description . \" , } , \" cen e_d esc rip tio \" : { \" type \" : \" string \" , \" description \" : \" brief summary of the scene , focusing on relevant spatial relationships . \" , } , } , \" required \" : [ \" reasoning \" , \" sc ne_ es cri pt io \" ] , \" i n r r s \" : False , } , } , } A.2 VLM Action Planner The VLM Action Planner leverages the users instructions, current scene information, and memory to generate the next necessary action, such as picking, placing, or pushing an object, to successfully complete the given task. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in task planning and can learn or adapt from the previous experience. Always apply chain-of-thought reasoning and think step by step before making any final decision. The robot received this instruction from the user: {instruction}. Considering the given image, along with the robots capabilities and experience, what is the most appropriate next action to efficiently fulfill the users instruction? Here is the short-term memory for the current task so far for reference: {short term memory}. learn from this experience history, especially the suggestions for next action, to plan the next action."
        },
        {
            "title": "Please",
            "content": "The following lifelong memories represent the robots previous activities and are intended to showcase its capabilities and experience. Please first identify similar scenarios and learn from them to avoid similar failures: {long term memory} 14 For the push action, choose the most efficient direction to push. left in the image, the robot should prefer to push the object to the left if both directions are viable. Conversely, If the object is on the right side, the robot should prefer pushing it rightward if both directions are viable. For example, if the target is on the For pick or place actions, indicate whether the object must be grasped at specific section to ensure stable and proper grasp. achieve more precise grasping. Note that due to imperfect part segmentation, this should be activated if the object needs to be held by specific part, for example, to avoid contaminating food or to prevent damage to the object. This will enable the image annotation tool and trigger follow-up query to If you plan to use tool, first check whether its ready to use. axes on the table, you may need to grasp it first. For example, if you intend to use the For all actions, always pay attention to the spatial relationships between objects, and ensure the robot interacts with only one object at time. Avoid giving or parameters that could cause the robot to unintentionally interact with the wrong object. Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" pick_object \" , \" description \" : \" Pick specified object , providing details about the grasping area and surrounding environment . \" , \" strict \" : true , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" cen e_d esc rip tio \" : { \" type \" : \" string \" , \" description \" : \" Short description of the object surroundings , especially the spatial relationships with nearby objects . \" } , \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Provide reasoning for each parameter choice . \" } , \" target_object \" : { \" type \" : \" string \" , \" description \" : \" Specify the object the robot should pick . \" } , \" grasp_part \" : { \" type \" : \" string \" , \" description \" : \" Specify the part of the object to be grasped . Leave blank if no commonly recognized specific part is relevant to the action . \" } , \" c c _ s _ u d \" : { \" type \" : \" boolean \" , \" description \" : \" Indicate whether the object must be grasped at specific section to ensure stable and proper grasp . \" } , \" ac io _d es ri ti on \" : { \" type \" : \" string \" , \" description \" : \" Briefly describe the action to be performed , focusing only on what the robot should do . \" } } , \" required \" : [ \" scen e_d esc rip tio \" , \" reasoning \" , \" target_object \" , \" grasp_part \" , \" c c _ s _ u d \" , \" io _d sc ri ti \" ] , \" i n r r s \" : false } } } , { \" type \" : \" function \" , \" function \" : { \" name \" : \" place_object \" , \" description \" : \" Place specified object at designated location , including context about positioning and the surrounding environment . \" , \" strict \" : true , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" cen e_d esc rip tio \" : { \" type \" : \" string \" , 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 \" description \" : \" Detailed description of the surroundings where the object will be placed , including nearby objects and obstacles . \" } , \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Provide reasoning for each parameter choice . \" } , \" target_object \" : { \" type \" : \" string \" , \" description \" : \" Specify the name or type of the object that the robot should place . \" } , \" pl ce en t_ oc ti on \" : { \" type \" : \" string \" , \" description \" : \" The specific name of the location where the robot should place the object . \" } , \" c _ c n _ t _ u d \" : { \" type \" : \" boolean \" , \" description \" : \" Indicate whether the object must be placed in specific spot within the placement area . \" } , \" ac io _d es ri ti on \" : { \" type \" : \" string \" , \" description \" : \" Briefly describe the action to be performed , focusing only on what the robot should do . \" } } , \" required \" : [ \" cen e_d esc rip tio \" , \" reasoning \" , \" target_object \" , \" ce en _l oc ti \" , \" c _ c n _ t _ u d \" , \" ac io _ c pt on \" ] , \" i n r r s \" : false } } } , { \" type \" : \" function \" , \" function \" : { \" name \" : \" push_object \" , \" description \" : \" Push the specified object by the minimum required distance . \" , \" strict \" : true , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" cen e_d esc rip tio \" : { \" type \" : \" string \" , \" description \" : \" Detailed description of the scene , including spatial relationships with nearby objects . Also describe the object location in the image frame ( . . , left or right side ) . \" } , \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Provide reasoning for each parameter choice . \" } , \" object_to_push \" : { \" type \" : \" string \" , \" description \" : \" Specify the object to be pushed by the robot gripper . \" } , \" push_direction \" : { \" type \" : \" string \" , \" enum \" : [ \" left \" , \" right \" ] , \" description \" : \" The direction in which to push the object in the image view . \" } , \" ac io _d es ri ti on \" : { \" type \" : \" string \" , \" description \" : \" Briefly describe the action to be performed , focusing only on what the robot should do . \" } } , \" required \" : [ \" scen e_d esc rip tio \" , \" reasoning \" , \" bject_t o_push \" , \" push_direction \" , \" ac io _d es ri ti \" ] , \" i n r r s \" : false } } } 16 1 2 3 4 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 A.3 VLM Grasping Selector The VLM Grasping Selector determines the most stable and effective section of the object to grasp, providing reasoning and descriptions for each section to identify the optimal grasp section for the performing action. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in semantic object manipulation. You always apply chain-of-thought reasoning to thoroughly analyze each situation before making final decision. Based on your observation of the image, determine the optimal grasping section of the object to ensure stable handling and successfully fulfill the given action: {action}. Avoid contaminating food, damaging the object, or compromising safety. Select the most appropriate grasping section (by number) that best fulfills the action requirements, as human would. Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" choose_section \" , \" description \" : \" Select the most stable and effective section of the object to grasp in order to perform the action . \" , \" strict \" : True , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Explain the rationale behind selecting the chosen section . \" , } , \" e _ t _ c t \" : { \" type \" : \" string \" , \" description \" : \" Describe each numbered section and the corresponding part of the object . \" , } , \" s _ t _ b \" : { \" type \" : \" integer \" , \" enum \" : [ + 1 for in range ( len ( section_list ) ) ] , \" description \" : \" Choose the number corresponding to the best section for grasping the target object . \" , } , } , \" required \" : [ \" reasoning \" , \" e _ t _ c t \" , \" s _ t _ b \" ] , \" i n r r s \" : False , } , } , } A.4 VLM Placement Selector The VLM Placement Selector chooses the most stable and effective location for object placement, providing reasoning and description of each option to identify the best spot for the performing action. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in spatial analysis and object placement. You always apply chain-of-thought reasoning to thoroughly analyze each situation before making final decision. Based on your observation of the given image, select the optimal placement location for the object that ensures both stability and accessibility for performing the action: {action}. Function Definition: 17 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 25 26 27 28 1 2 3 4 5 6 7 8 9 10 11 12 { \" type \" : \" function \" , \" function \" : { \" name \" : \" choose_location \" , \" description \" : \" Select the most stable and effective location to place the object in order to fulfill the task . \" , \" strict \" : True , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Explain the rationale behind selecting each location , focusing on stability , accessibility , and suitability for the task . \" , } , \" c n _ t _ c t \" : { \" type \" : \" string \" , \" description \" : \" Describe each numbered option and its corresponding placement spot . \" , } , \" t _ c n _ a n \" : { \" type \" : \" integer \" , \" enum \" : [ + 1 for in range ( len ( c n _ t _ t ) ) ] , \" description \" : \" Choose the number corresponding to the most suitable placement location for the object . \" , } , } , \" required \" : [ \" reasoning \" , \" c n _ t _ c t \" , \" t _ c n _ a n \" ] , \" i n r r s \" : False , } , } , } A.5 VLM Pushing Spot Selector The VLM Pushing Spot Selector identifies the most effective final gripper position for completing the task, minimizing unnecessary movement while providing reasoning and descriptions for each potential position. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in semantic object pushing. You always apply chain-of-thought reasoning to thoroughly analyze each situation before making final decision. Based on the given image, determine the optimal final position of the gripper to complete the following action efficiently, ensuring stability and minimizing unnecessary pushing distance: {action}. The initial position (0) represents the grippers position before pushing. The provided numbers indicate These positions refer specifically to the grippers the possible final gripper positions after the push. movement, not the objects. Assume that the relative position between the gripper and the object remains unchanged before and after the push. Select the optimal final gripper position number. Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" select_position \" , \" description \" : \" Select the most effective final gripper position to successfully complete the action while minimizing unnecessary movement . \" , \" strict \" : True , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Explain the rationale behind selecting the final gripper position . \" , 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 } , \" p _ i n _ c t \" : { \" type \" : \" string \" , \" description \" : \" list of descriptions corresponding to each possible gripper position . \" , } , \" p _ i n _ b \" : { \" type \" : \" integer \" , \" enum \" : [ + 1 for in range ( len ( position_list ) ) ] , \" description \" : \" Select the number corresponding to the optimal final gripper position to complete the task . \" , } , } , \" required \" : [ \" reasoning \" , \" p _ i n _ c t \" , \" p _ i n _ b \" , ] , \" i n r r s \" : False , } , } , } A.6 VLM Success Detector The VLM Success Detector determines whether the current action or task has been successfully completed, diagnosing potential failure causes, and suggest next steps. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in detecting whether an action or task has been successfully completed, and you provide clear, constructive feedback or alternatives when needed. You always apply chain-of-thought reasoning to thoroughly analyze each situation before reaching conclusion. Please analyze the provided image to determine whether the given action and current task have been successfully completed. If an action has failed, analyze the spatial relationship between the target object and its surrounding objects to identify the cause of failure. Learn from the failure cause and consider another way to achieve the goal. This may involve interacting with other relevant objects in the environment if necessary. Considering the robot only has two-finger gripper, it might not be able to interact with things very precisely. If there are any tools or objects in the image that could help achieve the goal, please consider using Everything on the table could potentially be used as tool. them. Be creative! Action to detect: {action} The users instruction: {instruction} Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" l e _ i _ t _ _ u \" , \" description \" : \" Evaluate whether the current action or task was successfully completed , and identify any issues that may impact the task overall feasibility . \" , \" strict \" : true , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" reasoning \" : { \" type \" : \" string \" , \" description \" : \" Provide brief explanation of the reasoning behind the action status and issue identification . \" } , \" action_status \" : { \" type \" : \" string \" , \" enum \" : [ \" successful \" , \" uncertain \" , \" failed \" ] , \" description \" : \" Indicate whether the current action was completed successfully , failed , or had an uncertain outcome . \" 19 18 19 20 21 22 23 24 26 27 28 29 30 31 32 33 34 35 36 37 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 19 20 21 22 23 24 } , \" failure_cause \" : { \" type \" : \" string \" , \" description \" : \" Provide one short , specific reason for the action outcome . \" } , \" t _ p _ g i \" : { \" type \" : \" string \" , \" description \" : \" Provide one short and specific suggestion for the next action needed to fulfill the task . \" } , \" s_t ask _co mpl ete \" : { \" type \" : \" boolean \" , \" description \" : \" Set to true if the task has been successfully completed , that is , if the intended result has been achieved . \" } } , \" required \" : [ \" reasoning \" , \" action_status \" , \" failure_cause \" , \" t _ p _ g i \" , \" is _ta sk_ comp let ed \" ] , \" i n r r s \" : false } } } A.7 VLM Experience Summarizer The VLM Experience Summarizer generates lifelong memory summarizing the robots recent actions based on short-term memory logs. It captures key events such as successes, failures, behavioral adjustments. Prompt: You are helpful assistant for legged robot equipped with single arm and two-finger gripper. You specialize in converting the robots experiences into concise task summaries. You always apply chain-of-thought reasoning to thoroughly analyze each situation before performing the conversion. Please convert the following robot short-term memory into single, concise paragraph summary. Robot short-term memory: {short term memory} Function Definition: { \" type \" : \" function \" , \" function \" : { \" name \" : \" m z _ o _ e n \" , \" description \" : ( \" Summarize the robot short - term experience in paragraph using the given memory logs . \" \" Include reflections if the robot learned or adjusted its behavior , or any human observations . \" ) , \" strict \" : True , \" parameters \" : { \" type \" : \" object \" , \" properties \" : { \" summary \" : { \" type \" : \" string \" , \" description \" : ( \" paragraph summarizing the sequence of actions from the given memory log , including any failures , adjustments made by the robot , or observations by the operator . \" ) , } } , \" required \" : [ \" summary \" ] , \" i n r r s \" : False , } , } , }"
        },
        {
            "title": "B Additional Results",
            "content": "B.1 Short-Term Memory We evaluated robot memory generation across four challenging real-world scenarios. Each task was tested 510 times, with two attempts allowed per trial. Table 5 presents the complete response from the VLM modules for one task illustrated in Figure 4, showcasing memory generation through the robots self-reflection during the task. Table 5: Illustration of self-reflection for the instruction: Pick up the bowl."
        },
        {
            "title": "Scene",
            "content": "VLM Response Response from VLM Task Planner: { \"name\": \"pick object\", \"arguments\": { \"scene description\": \"A bowl with an apple inside is placed on table. The bowl is centered on the table with no other objects nearby.\", \"reasoning\": and unobstructed, allowing for straightforward approach. stability and avoids disturbing the apple inside.\", \"target object\": \"specific grasp required\": \"Approach the bowl from the middle and grasp it securely to pick it up.\" } }"
        },
        {
            "title": "Grasping the bowl from the middle ensures",
            "content": "\"bowl\", \"grasp part\": \"middle\", \"The bowl is centered true, \"action description\": Observation from human: the bowl cause the apple was dropped on the floor."
        },
        {
            "title": "The robot failed to pick up",
            "content": "\"The apple inside"
        },
        {
            "title": "The apple is centrally",
            "content": "To avoid dropping the apple, it Response from VLM Task Planner: { \"name\": \"pick object\", \"arguments\": { \"scene description\": \"A bowl with an apple inside is on table. The robots gripper is positioned in front of the bowl.\", \"reasoning\": the bowl may cause instability if the bowl is picked directly. should be removed first. placed, allowing for straightforward grasp from the middle.\", \"target object\": \"apple\", \"grasp part\": \"\", \"specific grasp required\": \"action description\": middle of the bowl and lift it out.\" } } Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": \"The robots gripper is holding the apple, indicating that the action of grasping the apple was successful.\", \"action status\": \"successful\", \"failure cause\": \"next step suggestions\": reposition the gripper to grasp the bowl directly.\", false } } \"is task completed\": \"Grasp the apple from the \"Release the apple and \"None\", false,"
        },
        {
            "title": "Scene",
            "content": "\"Place the apple on the table.\" \"The The table is clear, so placing the"
        },
        {
            "title": "The",
            "content": "false, \"successful\", The robots gripper is \"apple\", \"placement location\": \"None\", \"next step suggestions\": \"The image shows the apple placed"
        },
        {
            "title": "The bowl is positioned directly in",
            "content": "VLM Response Response from VLM Task Planner: { \"name\": \"place object\", \"arguments\": { \"scene description\": \"The robot is holding an apple in its gripper, and there is bowl on the table in front of it. table is clear of other objects.\", \"reasoning\": robot needs to place the apple down before it can pick up the bowl. apple directly on the table is safe and efficient.\", \"target object\": \"table\", \"precise placement spot required\": \"action description\": } } Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": on the table, which means the action of placing the apple is successful. open and not holding the bowl, indicating that the task of picking up the bowl has not been completed. front of the robot, making it accessible for the next attempt.\", \"action status\": \"failure cause\": \"Move the gripper forward to grasp the bowl and lift it.\", \"is task completed\": Response from VLM Task Planner: { \"name\": \"pick object\", \"arguments\": { \"scene description\": \"The bowl is on the table, centered in front of the robot. the table, but it is not obstructing the bowl.\", \"reasoning\": the robot, making it accessible for direct approach. robot can grasp the bowl from the middle without any obstruction.\", \"target object\": \"grasp part\": false, \"action description\": forward to grasp the bowl and lift it.\" } } Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": positioned above the edge of the bowl and appears to be holding it securely. obstructions or issues with the spatial relationship between the bowl and its surroundings. not obstructed by other objects, and the gripper is not holding any other object.\", \"action status\": \"successful\", \"failure cause\": successful.\", \"next step suggestions\": \"No further action is needed as the bowl has been successfully picked up.\", \"is task completed\":"
        },
        {
            "title": "There is an apple on the left side of",
            "content": "\"middle\", \"specific grasp required\": The apple is not in the way, so the \"The robots two-finger gripper is \"The bowl is directly in front of"
        },
        {
            "title": "There are no visible",
            "content": "\"Move the gripper"
        },
        {
            "title": "The bowl is",
            "content": "false } } true } } \"bowl\", \"N/A - The action was Once the task is marked as complete by the VLM Success Detector, EXPTEACH activates the VLM Experience Summarizer to summarize the self-generated short-term memory accumulated during the task, which is then stored in long-term memory. We show one example in Table 6. Table 6: STM summarization for the instruction: Pick up the bowl. Left: Sequence of STM entries. Right: summary generated by the VLM Experience Summarizer."
        },
        {
            "title": "After receiving",
            "content": "\"The robot initially \"summarize robot experience\", STM Summary Response from VLM Experience Summarizer: { \"name\": \"arguments\": { \"summary\": attempted to pick up bowl containing an apple from cluttered table but failed, resulting in the apple being dropped on the floor. feedback, the robot successfully grasped the apple from the bowl and placed it on the table. repositioned its gripper to directly grasp and lift the bowl. completed successfully with the bowl being picked up, and no further actions were needed. approach based on visual feedback and suggestions, demonstrating adaptability in handling the objects.\" } } Subsequently, the robot"
        },
        {
            "title": "The task was",
            "content": "STM Logs At time step 0, bowl with an apple The bowl inside is placed on table. is centered on the table with no other objects nearby. At time step 1, the human stated that the robot failed to pick up the bowl because the apple was dropped on the floor. At time step 2, VLM detected that the robot successfully grasped the apple from the middle of the bowl and lifted it out. Release the apple and reposition the gripper to grasp the bowl directly. Suggestions for next action: At time step 3, VLM detected that the robot placed the apple on the table. Suggestions for next action: gripper forward to grasp the bowl and lift it."
        },
        {
            "title": "Move the",
            "content": "At time step 4, VLM detected that the robot successfully moved the gripper forward to grasp the bowl and lift it. Suggestions for next action: action is needed as the bowl has been successfully picked up."
        },
        {
            "title": "No further",
            "content": "B.2 Long-Term Memory Upon receiving the users instruction, the VLM Scene Describer first generates description of the current scene. This scene description is then combined with the instruction to form retrieval key, which is used to retrieve relevant experiences from the long-term memory to guide this task. We show an example of the retrieval in Table 7. 23 Table 7: Past experiences retrieved via RAG. Scenario Instruction: Pick up the milk carton. Scene: Pick up the bowl. bowl with an apple inside is placed on table. Retrieved LTM long-term memory 1 Instruction: Scene: The bowl is centered on the table with no other objects nearby. Experience: The robot initially attempted to pick up bowl containing an apple from cluttered table but failed, resulting in the apple being dropped on the floor. grasped the apple from the bowl and placed it on the table. to directly grasp and lift the bowl. completed successfully with the bowl being picked up, and no further actions were needed. its approach based on visual feedback and suggestions, demonstrating adaptability in handling the objects. After receiving feedback, the robot successfully Subsequently, the robot repositioned its gripper"
        },
        {
            "title": "The task was",
            "content": "Place the spoon next to the plate. The spoon was already in the robots gripper, and long-term memory 2 Instruction: Scene: the plate was centered on the table. Experience: the plate."
        },
        {
            "title": "The robot placed the spoon in the middle of",
            "content": "Pick up the water bottle. long-term memory 3 Instruction: Scene: surface. Experience: middle of the bottle and lifted it successfully."
        },
        {
            "title": "The robot wrapped its gripper around the",
            "content": "Put the apple on the plate. The apple is next to can. long-term memory 4 Instruction: Scene: The robot initially attempted to grasp an Experience: apple that was positioned next to can on cluttered The first attempt to pick up the apple failed table. due to the can obstructing access. Recognizing this, the robot adjusted its approach by pushing the can to the right, successfully clearing the path to the apple. With the obstruction removed, the robot was able to pick up the apple from the top without disturbing the nearby items. Finally, the robot placed the apple gently onto the plate, completing the task without any further issues or need for human intervention. Place the remote on the table. long-term memory 5 Instruction: Scene: table surface was empty. Experience: and released the remote onto the surface."
        },
        {
            "title": "The robot moved its gripper over the table",
            "content": "The robot was already holding the remote, and the We evaluated the generalization capability of the LTM using set of challenging real-world scenarios. Each task is tested 510 times, with only one attempt allowed per trial. The following example illustrates the full response of the VLM modules, demonstrating how long-term memory could help ground VLMs to act according to past experiences. 24 Table 8: Illustration of the grounding with retrieved LTM based on the instruction: Pick up the milk carton."
        },
        {
            "title": "Scene",
            "content": "\"apple\", \"grasp part\":"
        },
        {
            "title": "The action specified was to grasp",
            "content": "\"The robots two-finger gripper is \"top\", false, \"action description\": Output Response from VLM Task Planner: { \"name\": \"pick object\", \"arguments\": { \"scene description\": \"A milk carton with an apple on top is placed on The carton is centered on the table with no table. other objects nearby.\", \"reasoning\": \"The apple on top of the milk carton needs to be removed first to avoid dropping it when picking up the carton. This is similar to the previous experience where the robot had to remove an apple from bowl before picking up the bowl.\", \"target object\": \"specific grasp required\": \"Grasp the apple from the top and lift it off the milk carton.\" } } Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": currently grasping the apple, which is positioned on top of the milk carton. the apple from the top and lift it off the milk carton. However, the image shows the apple still in the gripper and above the milk carton, suggesting the apple has been successfully grasped and lifted. to pick up the milk carton, which cannot be done while the apple is still in the gripper. placed elsewhere before the robot can attempt to grasp and lift the milk carton. the table that could be used as tools, so the next step should be to place the apple on the table, freeing the gripper to pick up the milk carton.\", \"action status\": \"successful\", \"failure cause\": \"Apple is still in the gripper, preventing the robot from grasping the milk carton.\", \"next step suggestions\": the table to free the gripper, then attempt to grasp and lift the milk carton.\", \"is task completed\": Response from VLM Task Planner: { \"name\": \"place object\", \"arguments\": { \"scene description\": \"The robot is holding an apple in its gripper, and there is milk carton on the table in front of it. The table is clear, providing space to place the apple down.\", \"reasoning\": \"The robots gripper is currently occupied with an apple, preventing it from picking up the milk carton. on the table will free the gripper, allowing the robot to proceed with picking up the milk carton.\", \"target object\": \"table\", \"precise placement spot required\": \"action description\": to free the gripper.\" } } \"apple\", \"placement location\":"
        },
        {
            "title": "There are no other objects on",
            "content": "\"Place the apple on the table The task, however, is \"Place the apple on"
        },
        {
            "title": "Placing the apple",
            "content": "false } } false,"
        },
        {
            "title": "Scene",
            "content": "\"No failure; the"
        },
        {
            "title": "There are no spatial obstructions between",
            "content": "Output Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": \"The image shows the robots gripper is open and empty, with the apple now placed on the table to the left and the milk carton in the center. The table is clear, and the apple is no longer in the gripper, indicating the action to place the apple down has been completed. interact with the milk carton, which is the next step for the task. the gripper and the milk carton, so the robot can proceed to pick up the milk carton.\", \"action status\": \"successful\", \"failure cause\": apple was placed on the table, freeing the gripper.\", \"Proceed to move the gripper \"next step suggestions\": towards the milk carton and attempt to grasp it.\", false } } \"is task completed\": Response from VLM Task Planner: { \"name\": \"pick object\", \"arguments\": { \"scene description\": \"The milk carton is on the table, directly in front of the robots gripper. There is an apple to the left of the milk carton, but it is not obstructing the path to the milk carton.\", \"reasoning\": \"The milk carton is directly in front of the robot, making it accessible for direct approach. the way, so the robot can grasp it from the middle. The milk carton should be grasped from the sides to ensure stable grip.\", \"target object\": \"grasp part\": \"action description\": carton and grasp it from the sides to lift it.\" } } Response from VLM Success Detector: { \"name\": \"evaluate action status and issues\", \"arguments\": { \"reasoning\": green carton labeled beleaf, which appears to be plant-based drink, possibly oat milk. to pick up the milk carton, and the gripper is securely around the carton, lifting it from the table. are no visible obstructions or issues with the grasp. The only other object on the table is an apple, which is not interfering with the action. gripper seems to have successfully grasped the carton, and the task appears to be completed.\", \"action status\": \"successful\", \"failure cause\": successful.\", \"next step suggestions\": action needed; the milk carton has been picked up.\", true } } \"is task completed\": \"milk carton\", true, \"Move the gripper towards the milk \"The robots gripper is currently holding \"sides\", \"specific grasp required\":"
        },
        {
            "title": "There are no obstacles in",
            "content": "\"N/A - The action was The two-finger"
        },
        {
            "title": "The action was",
            "content": "\"No further"
        },
        {
            "title": "There",
            "content": "B.3 Image Annotation Module To evaluate the image annotation module, we conducted 5 trials per scenario across 7 different scenarios for both picking and pushing actions. The evaluation measures the success rate of selecting the correct object sections for picking and analyzes the distance error between the object and the target position after pushing, as shown in Figure 5. The following figures (Figure 6, Figure 7, Figure 8) show testing scenarios in real-world environment, where the image annotation module plays key role in handling challenging objects that are difficult to grasp due to their complex shapes or strict grasping constraints. grasp is considered successful if the object is picked up from the correct region, similar to how human would grasp it. 26 Figure 6: Left: The robot picked up the meat section without VLM guidance through image annotation. Right: With image annotation, the robot successfully picked up the bone section of the drumstick. Figure 7: Left: The robot picked up the meat of the skewer without VLM guidance through image annotation. Right: With image annotation, the robot successfully picked up the stick of the skewer. Figure 8: Left: The robot picked up the body of the brush without VLM guidance through image annotation. Right: With image annotation, the robot successfully picked up the handle of the brush."
        }
    ],
    "affiliations": [
        "ETH AI Center",
        "Huawei Noahs Ark Lab",
        "Robotic Systems Lab, ETH Zurich",
        "UCL Centre for AI"
    ]
}