{
    "paper_title": "DepthLab: From Partial to Complete",
    "authors": [
        "Zhiheng Liu",
        "Ka Leong Cheng",
        "Qiuyu Wang",
        "Shuzhe Wang",
        "Hao Ouyang",
        "Bin Tan",
        "Kai Zhu",
        "Yujun Shen",
        "Qifeng Chen",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Missing values remain a common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, a foundation depth inpainting model powered by image diffusion priors. Our model features two notable strengths: (1) it demonstrates resilience to depth-deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https://johanan528.github.io/depthlab_web/."
        },
        {
            "title": "Start",
            "content": "DepthLab: From Partial to Complete Zhiheng Liu1, Ka Leong Cheng2,3, Qiuyu Wang3, Shuzhe Wang4, Hao Ouyang3, Bin Tan3, Kai Zhu5, Yujun Shen3, Qifeng Chen2, Ping Luo1 1HKU, 2HKUST, 3Ant Group, 4Aalto University, 5Tongyi Lab 4 2 0 2 4 2 ] . [ 1 3 5 1 8 1 . 2 1 4 2 : r Figure 1. DepthLab for diverse downstream tasks. Many tasks naturally contain partial depth information, such as (1) 3D Gaussian inpainting, (2) LiDAR depth completion, (3) sparse-view reconstruction with Dust3R, and (4) text-to-scene generation. Our model leverages this known information to achieve improved depth estimation, enhancing performance in downstream tasks. We hope to motivate more related tasks to adopt DepthLab."
        },
        {
            "title": "Abstract",
            "content": "Missing values remain common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, foundation depth inpainting model powered by image diffusion priors. Our model features two no- (1) it demonstrates resilience to depthtable strengths: deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, textto-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https: //johanan528.github.io/depthlab_web/. including 3D scene inpainting, 1 1. Introduction Depth inpainting, the task of reconstructing missing or occluded depth information in images, is pivotal in numerous domains, including 3D vision [25, 34, 91], robotics [1, 32], and augmented reality [53, 61]. As shown in Fig. 1, robust depth inpainting model can enable high-quality 3D scene completion, editing, reconstruction, and generation. Previous research in depth inpainting can be categorized into two primary approaches. The first approach [12, 26, 48, 65, 71, 89] focuses on completing globally sparse LiDAR depth [16] data to dense depth, typically trained and tested on fixed datasets. However, these models lack generalization and have limited applicability across diverse downstream tasks. The second approach [15, 31, 72, 75, 77] employs monocular depth estimators to infer the depth of single image, aligning the inpainted region with known depth. These methods often suffer from significant geometric inconsistencies, particularly along edges, due to misalignment between estimated depth and existing geometry. Recent research [41] incorporates RGB images into the UNet input as guidance to train depth inpainting model, but its performance remains suboptimal in complex scenes and when inpainting large missing regions. To this end, we propose DepthLab, foundation model for RGB image-conditioned depth inpainting, which introduces dual-branch depth inpainting diffusion framework. Specifically, the framework processes single reference image through Reference U-Net, which extracts RGB features as conditional input. Simultaneously, the known depth and the mask-indicating regions requiring inpainting are fed into the depth Estimation U-Net. The extracted RGB features are progressively integrated layer by layer into the depth Estimation U-Net to guide the inpainting process During training, we apply random scale normalization to the known depth to mitigate regularization overflow caused by non-global extrema in the known regions. Similar to Marigold [31], our model requires only synthetic RGBD data for training with just few GPU days. Benefiting from the powerful priors of the diffusion model, DepthLab demonstrates strong generalization capabilities across diverse scenarios. As shown in Fig. 1, thanks to accurate depth inpainting, DepthLab supports variety of downstream applications. (1) 3D scene inpainting [41]: In 3D scenes, we start by inpainting the depth of the image inpainted regions from the posed reference views, then unproject the points into the 3D space for optimal initialization, which significantly enhances the quality and speed of the 3D scene inpainting. (2) Text-to-scene generation [83]: Our method substantially improves the process of generating 3D scene from single image by eliminating the need for alignment. This advancement effectively mitigates issues of edge disjunction that previously arose from geometric inconsistencies between the inpainted and known depth, thereby significantly enhancing the coherence and quality of the generated (3) Sparse-View Reconstruction with DUST3R: scenes. InstantSplat [14] leverages point clouds from DUST3R [68] as an initialization for SfM-free reconstruction and novel view synthesis. By adding noise to DUST3R depth maps as latent input, our method refines depth in regions lacking cross-view correspondences, producing more precise, geometry-consistent depth maps. These refined depth maps further enhance the initial point clouds for InstantSplat. (4) LiDAR depth completion: Sensor depth completion [89] is an important task related to depth estimation. Unlike existing methods that are trained and tested on single dataset, such as NYUv2 [46], our approach achieves comparable results in zero-shot setting and can deliver even better outcomes with minimal fine-tuning. For more comprehensive evaluation, we assessed its effectiveness on depth estimation benchmarks by applying various types of random masks to regions requiring inpainting. 2. Related Work 2.1. Depth Completion Depth completion [22, 39, 42, 43, 70, 74] is key task in computer vision, especially with the rise of active depth sensors. It aims to estimate accurate depth maps from sparse ground-truth measurements provided by sensors. However, existing methods [12, 26, 48, 65, 70, 89, 90] are often trained and evaluated on fixed datasets, limiting their generalization abilities, and many of them rely on cumbersome processing pipelines. In contrast, this paper offers an efficient and robust foundational model for depth completion, capable of strong generalization across diverse scenarios in various downstream applications. 2.2. Monocular Depth Estimation Monocular depth estimation is crucial task in computer vision, approached mainly through discriminative and generative methods. Discriminative methods often predict depth as either metric depth [2, 3, 29, 36, 44, 86] or relative depth [13, 51, 52, 76, 77, 80, 81, 87]. Recently, generative models [4, 23, 88], particularly diffusion models, gained popularity in depth estimation. Methods like DDP [27], DepthGen [57], Marigold [31], and GeoWizard [15] leverage diffusion processes to produce high-quality depth maps, although often at the cost of slow sampling. Flow matchingbased models [18] emerge to mitigate these limitations, providing faster sampling. Despite advances in monocular depth estimation, aligning estimated depth with known data often causes geometric inconsistency. Our work addresses this by introducing robust foundational depth inpainting model for various tasks. 2 2.3. Downstream Tasks with Known Partial Depth Apart from the traditional depth completion task, many 3D vision tasks involve partially known depth information: (1) 3D scene inpainting focuses on filling in missing parts of 3D spaces, such as removing objects and generating plausible geometry and textures for the inpainted regions. Early works mainly addressed geometry completion [10, 11, 19, 30, 49, 62, 69], while recent methods focus on jointly inpainting both texuture and geometry [6, 24, 33, 40, 45, 79]. Such 3D scenes can be rendered to obtain depth, where unchanged areas serve as known partial depth information. (2) 3D scene generation involves creating 3D content from inputs such as single image or text prompt, utilizing strong generative priors and depth estimation. Recent methods [8, 20, 35, 47, 83, 84, 92] change the camera viewpoint, perform RGB inpainting on the warped image, and then estimate depth, aligning it with the warped depth. Conversely, our method considers the warped depth as known, formulating the existing twostep process as depth inpainting conditioned at known scale. (3) DUST3R [68] presents novel 3D reconstruction pipeline that operates on sparse views without any prior knowledge of camera calibration or poses. Recently, several works [38, 60, 64] leverage initial point cloud from COLMAP [58] or DUST3R for subsequent tasks such as 3D scene reconstruction [14], novel view synthesis [85], and 4D scene generation [7]. However, DUST3R struggles to produce accurate reconstructions in regions without viewpoint overlap. Our method leverages depth data from regions with pixel correspondences to enhance the depth in areas lacking matches. 3. Method Given an original (incomplete or distorted) depth map R1HW , binary mask R1HW indicating the target areas for inpainting, and conditional RGB image R3HW , our objective is to use the RGB image to predict complete depth map RHW . This involves preserving the depth values in the unmasked regions while accurately estimating the depth in the masked areas. This process naturally aligns the estimated depth with the existing geometry, eliminating inconsistencies between the known and target inpainting areas. To achieve this, we introduce dual-branch diffusionbased framework for depth inpainting, comprising Reference U-Net for RGB feature extraction and an Estimation U-Net that takes the original depth and inpainting mask as input. Instead of the commonly used text conditioning, we utilize cross-attention with the CLIP image encoder to capture rich semantic information. Layer-by-layer feature fusion through attention between the Reference U-Net and Estimation U-Net enables finer-grained visual guidance. 3 This design enables DepthLab to achieve remarkable results, even for large inpainting regions or complex RGB images. An overview is demonstrated in Fig. 2. 3.1. Network Design Both branches use Marigold [31] as the base model, finetuned from Stable Diffusion V2 [56]. This design eliminates the need to learn the domain transfer process from RGB to depth, improving training efficiency. The detailed architecture of each network is provided below. Depth encoder and decoder. We use fixed variational autoencoder (VAE) encoder to encode both RGB images and their corresponding depth maps into the latent Since the encoder is originally designed for space. three-channel RGB inputs, single-channel depth maps are replicated across three channels to match the RGB format. Notably, as the VAE encoder is intended for non-sparse inputs, we apply nearest neighbor interpolation to densify the sparse depth maps before encoding. During inference, the denoised depth latent z(d) 0 R4hw at step = 0 is decoded using the decoder D, and the average of the three channels is used as the predicted depth map. Unlike Marigold, which estimates relative depth and uses leastsquares optimization to obtain metric depth, our depth inpainting aims to directly estimate metric depth map based on the values and scale of the known depth regions. Details regarding depth preand post-processing, such as normalization, are provided in Sec. 3.2. Estimation U-Net. The input to the Estimation Denoising U-Net consists of three components: the noisy depth latent z(d) R4hw, the masked depth latent z(d) R4hw, and the encoded mask R4hw, which are concatenated together. The latent depth representations have 4 channels, resulting from the VAE encoding, and (h, w) are spatial dimensions downsampled by factor of 8 compared to the original input dimensions (H, ). Notably, to more accurately preserve mask information, instead of simply downsampling the mask as in traditional image inpainting methods [28, 93], we encode the mask R1HW using the VAE encoder to obtain R4hw, which effectively retains the sparse and fine-grained information. is obtained by encoding the initial depth map into the latent space and adding noise at step t. The masked depth latent z(d) is generated by applying random masking to the original groundtruth depth map, followed by nearest-neighbor interpolation in the inpainting regions and encoding via the VAE. Since the VAE of Stable Diffusion [56] excels at reconstructing dense information, this approach better preserves the known depth values at sparse points and complex edge boundaries. Reference U-Net. InFusion [41] feeds single reference image into the encoder, subsequently concatenating the image latent with noisy depth latent, masked depth latent, During training, the noisy latent depth z(d) Figure 2. The training process of DepthLab. First, we apply random masking to the ground truth depth to create the masked depth, followed by interpolation. Both the interpolated masked depth and the original depth undergo random scale normalization before being fed into the encoder. The Reference U-Net extracts RGB features, while the Estimation U-Net takes the noisy depth, masked depth, and encoded mask as input. Layer-by-layer feature fusion allows for finer-grained visual guidance, achieving high-quality depth predictions even in large or complex masked regions. and downsampled mask, resulting in total of 13 channels. However, this approach may lose regional depth information or struggle to generate clear depth edges, especially when inpainting large areas or using complex reference images. Recent studies [21, 73] demonstrate that an additional U-Net can extract more fine-grained features from the reference image. Inspired by these findings, we adopt similar architecture. We first obtain two feature maps, f1 Rchw and f2 Rchw, from the Reference UNet and Estimation Denoising U-Net, respectively. We then concatenate these feature maps along the width dimension, resulting in Rch2w. Next, we apply self-attention operation on the concatenated feature map and extract the first half of the resulting feature map as the output. This allows us to leverage the fine-grained semantic features from each layer of the base model during training. Furthermore, since the Reference U-Net and Estimation Denoising U-Net share the same architecture and initial weightsboth pretrained on Marigold [31]the Estimation Denoising U-Net can selectively learn relevant features from the Reference U-Net within the same feature space. 3.2. Training Protocol Depth normalization. Our goal is to maintain the original depth information in the known regions, predict the depth for unknown inpainting regions, and avoid geometric inconsistency. The final output is an absolute depth map. During inference, since the depth of the inpainting regions is unknown, we cannot determine the minimum and maximum depth values of the entire depth map. To simulate this scenario during training, we calculate the minimum and maximum values (dmin and dmax) for the known depth regions and linearly normalize them to the range of [1, 1]. Notably, local minimum and maximum values may not always correspond to the global minimum and maximum, which can lead to overflow during VAE decoding. To address this, we introduce random compression factor β in the range [0.2, 1.0] during normalization to handle these cases more effectively. The depth values are normalized using: = (cid:18) dmin dmax dmin (cid:19) 0.5 2 β. (1) This approach adheres to the convention of the Stable Diffusion VAE while also enforcing canonical, affineinvariant depth representation that is independent of data statistics. This ensures that depth values are constrained by the near and far planes, providing stability and reducing the influence of data distribution. Finally, we apply inverse normalization to the network output to restore the absolute depth scale and achieve depth inpainting. Masking strategy. To maximize coverage across vast array of downstream tasks, variety of masking strategies are employed. Initially, we randomly select from strokes, circles, squares, or random combination of these shapes to create the masks. to enhance the depth Secondly, completion taskrecovering full depth map from sparse depth data captured by sensorswe implement random point masking, where only 0.1-2% of points are set as known. Lastly, for improved object-level inpainting, we utilize the Grounded-SAM [54] to annotate training data, subsequently filtering the masks based on their confidence In general, the combined application of multiple scores. masking strategies further enhances the robustness of our method. 4 Table 1. Quantitative comparison of various methods on different datasets. Better: AbsRel , δ1 . The best results are marked in bold, and the second-best underlined. Our method incorporates known depth information, achieving optimal performance across all metrics. Training samples NYUv KITTI ETH3D ScanNet DIODE Method Real Synthetic AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 DiverseDepth [80] MiDaS [51] LeReS [81] Omnidata [13] HDN [87] DPT [52] DepthAnthing [76] DepthAnthingV2 [77] Marigold [31] DepthFM [18] GeoWizard [15] Ours 320K 2M 54K 300K 11.9M 310K 300K 1.2M 188K 63.5M 62M 595K 74k 63K 278K 74k 12.1 10.9 9.2 7.8 7.2 9.8 4.4 4. 5.6 6.5 5.2 2.5 86.8 88.9 91.5 94.0 94.6 90.1 97.6 98.0 96.4 95.6 96.5 98.8 18.8 24.2 14.9 14.7 11.2 10.2 7.6 7. 9.8 8.4 9.6 7.2 70.2 62.2 78.5 83.7 87.2 89.9 94.7 94.8 91.7 93.2 92.3 95.3 23.0 18.3 17.3 16.9 12.1 7.7 12.5 13. 6.6 6.4 3.1 69.9 75.4 77.7 77.8 94.2 94.6 88.5 86.6 95.9 96.3 97.9 11.1 13.2 9.6 7.2 8.0 8.4 4.2 4. 6.3 6.1 2.3 87.6 87.6 90.4 94.1 94.2 93.2 98.1 98.2 95.4 95.4 98.5 37.2 33.7 27.4 34.4 24.2 18.1 27.4 27. 30.9 22.4 29.5 17.6 63.8 70.6 77.0 73.1 78.3 75.8 76.1 76.4 77.2 79.8 79.5 85.6 Figure 3. Qualitative comparison of various methods on different datasets. In the second column, black represents the known regions, while white indicates the predicted areas. Notably, to emphasize the contrast, we reattach the known ground truth depth to the corresponding positions in the right-side visualizations of the depth maps. Other methods exhibit significant geometric inconsistency. 4. Experiments 4.1. Implementation Details and Datasets For training DepthLab, we utilize Training details. two synthetic datasets covering indoor and outdoor scenes. The first dataset, Hypersim [55], is photorealistic dataset incomplete samples. with 461 indoor scenes, from which we select 365 scenes, totaling approximately 54K training samples after filtering out The second dataset, Virtual is synthetic street-scene dataset comprisKITTI [5], ing five scenes with varying conditions, such as weather and camera perspectives. We employ 4 of these scenes, Figure 4. Visualization of gaussian inpainting. By projecting depth directly into three-dimensional space as initial points, natural 3D consistency is maintained, enabling texture editing and object addition. Please zoom in to view more details. amounting to approximately 20K training samples. We initialize the Reference U-Net and Estimation U-Net with pre-trained weights from the Marigold [31]. We also test other pre-trained weights, and the results are presented in Supplementary Materials. The training process spans 200 epochs, starting with an initial learning rate of 1e-3. This learning rate is scheduled to decay after every 50 epochs. We only use random flips as data augmentation. Utilizing eight A100-80G GPUs, the training process is completed within two days. Evaluation datasets. We assess our performance across five zero-shot benchmarks, including NYUv2 [46], KITTI [17], ETH3D [59], ScanNet [9], and DIODE [66]. During inference, we randomly select strokes, circles, squares, or combinations of the three across the entire image to represent unknown regions. In addition, we include another type of mask, where we randomly set only 0.5% to 1% of sparse pixels across the entire image as known depth. 4.2. Comparisons Given an image with ground truth depth, we first mask some regions of the GT depth to simulate missing data and perform depth inpainting using our model. For other methods, we utilize their pretrained models to estimate the depth map from the corresponding RGB image. Metric calculations are restricted to the masked regions to assess the inpainting performance while excluding the known regions from the evaluation. Baselines and metrics. We compare DepthLab with both discriminative and generative methods. The discriminative methods include DiverseDepth [80], MiDaS [51], LeReS [81], Omnidata [13], HDN [87], DPT [52], DepthAnything [76], and DepthAnythingV2 [77]. The generative methods include Marigold [31], DepthFM [18], and GeoWizard [15]. Following prior research [82], we evaluate the performance of depth estimation methods using absolute relative error (AbsRel) and accuracy within threshold of δ1 = 1.25. Qualitative comparison. We present the quantitative evaluations in Tab. 1. Notably, unlike discriminative methods such as DepthAnything V2 [77], which is trained on dataset of 63M samples, our approach relies solely on 74K synthetic data. Compared to other generative methods, our approach not only inherits the rich priors of the pre-trained diffusion model but also leverages its ability to incorporate local priors for conditionally generating coherent images. This enables us to effectively utilize partial known depth and align with the known scale, achieving optimal performance across all datasets and metrics. Quantitative comparison. As shown in Fig. 3, we set the black areas in the mask as known depth and the white areas as regions for prediction. Notably, for ease of comparison, we replace the known regions with ground truth in the depth maps shown in the three rightmost columns. The depth estimated by our method exhibits better geometric consistency, while other monocular methods, even after least-squares optimization, still show noticeable disjunctions with the known regions. Please zoom in to observe the details. 4.3. Applications In this section, we provide detailed overview for utilizing DepthLab in downstream tasks and present the corresponding results. 3D gaussian inpainting. Initially, we employ Gaussian Grouping [78] to segment and remove partial Gaussians. The SDXL Inpainting model [50] is then applied to the 6 Figure 5. Visualization of 3d scene generation. Left: Depth comparison. Align represents the least-square method and shows clear geometric inconsistencies at boundaries. While LucidDreamer reduces these inconsistencies, it compromises the accuracy of the newly estimated depth. In contrast, our model produces consistent and accurate depth. Right: The improved depth estimation from our model leads to superior 3D scene generation results. Figure 6. Visualization of sparse-view reconstruction with DUST3R. Left: Compared to InstantSplat [14], which directly uses point cloud from DUST3R as initialization, our method produces sharper and clearer depth in non-matching regions. Right: Using our method for improved initialization results in higher-quality Gaussian splatting rendering. Please zoom in for details. rendered image at the reference view. The inpainted RGB image subsequently serves as guidance to complete the depth information for that reference view. The points are then unprojected into three-dimensional space for optimal initialization. As shown in Fig. 4, due to the geometric consistency between the inpainted and original Gaussians and the alignment between pixels and inpainted Gaussians, simple edits to the inpainted image enable texture modification and object insertion in the inpainted regions. Text to 3D scene generation. Recent methods [8, 47, 92] begin by projecting single-view depth estimation onto 3D scene to create an initial point cloud from given viewpoint. The camera is then rotated to compute the warped image and warped depth. Following inpainting on the warped RGB image, monocular depth estimation is applied, and the estimated depth is aligned with the previously warped depth. The aligned depth data is then unprojected back into the original point cloud. This process repeats with changes in viewpoint. However, as shown in Fig. 5, using LucidDreamer [8] as an example, this approach suffers from geometric inconsistency during depth alignment across different scales, adversely impacting depth accuracy in the inpainted regions. In contrast, our model can directly take the inpainted image and warped depth as input, producing geometrically consistent depth maps without requiring alignment. Reconstruction with DUST3R. DUST3R [68] introduces the new 3D reconstruction pipeline from sparse views without prior information about camera calibration or poses. This method can provide dense pixel-to-pixel and pixelto-point correspondences, making it applicable to various 3D vision tasks, including depth prediction. However, we observe that DUST3R delivers high-quality depth predictions primarily at points with pixel correspondences while struggling to generate clear depth edges for points without correspondence across different views. To overcome this limitation, we introduce refinement framework that improves depth estimation for regions with weak or absent correspondences. Our approach begins by generating mask for pixels without matches from 7 Table 2. Quantitative comparison of depth completion.Ours* represents the zero-shot capability of our model, while Ours represents its performance after fine-tuning. NLSPN [48] DSN [12] Struct-MDC [26] ACMNet [90] CFormer [89] BP-Net [65] LRRU [70] Ours* Ours 0.104 0.090 0.245 0. 0.092 0.105 0.090 0.091 0.089 RMSE Table 3. Analysis of known depth ratios. We assess our models performance at known depths of 2%, 5%, 10%, 30%, and 50% ratio. 2% 5% 10% 30% 50% Dataset AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 NYUv2 ETH3D 3.3 3.1 98.2 97.4 3.0 2.9 98.3 98.0 2.8 2. 98.4 98.3 2.5 2.3 98.8 98.5 2.2 2.0 98.8 98.6 any source images. These non-matching regions are then refined through our proposed DepthLab. Specifically, we employ variational autoencoder (VAE) to encode the initial depth estimates from DUST3R into the latent space, adding noise to produce noisy latents. Depths at matched points are encoded as masked depth latents. Both the noisy and masked latent representations, along with their respective masks, are fed into our model to generate refined depth maps with enhanced accuracy and spatial consistency. We evaluate our method on InstanSplat [14], sparse-view, SfM-free Gaussian splatting method that uses the predicted point cloud from DUST3R for novel view synthesis. By unprojecting our enhanced depth maps into 3D space, we replace DUST3Rs original point cloud with our refined data as input for InstanSplat. As shown in Fig. 6, our approach effectively sharpens initial depth from DUST3R, substantially improving Gaussian splatting rendering quality. We also provide quantitative comparisons in the supplementary materials. Sensor depth completion. Sensor depth completion is an important task related to depth estimation, with widespread applications in robotics and autonomous navigation. Due to hardware limitations of depth sensors, only partial depth image is available, making it necessary to fill in the missing depth values while preserving the known depth values. We obtain the corresponding masks based on the coordinates of the sparse depth and perform comparisons on the NYU Depth v2 [46]. Following CompletionFormer [89], during the original frames of size 640 480 are evaluation, then downsampled by half using bilinear interpolation, center-cropped to 304 228, with only 500 groundtruth pixels available. As shown in Tab. 2, our model differs from traditional depth completion methods, which are typically trained and tested on fixed datasets, leading In contrast, our to limited generalization capabilities. approach exhibits better generalization without requiring complex designs, and with fine-tuning of only 10, 000 steps, it achieves performance comparable to state-of-theart methods, demonstrating its potential as foundational model for depth completion tasks. Additionally, one major limitation is the need for downsampling the mask in the latent space, where the VAE of Stable Diffusion 2.1 struggles to finely reconstruct extremely sparse data. Further analysis is provided in the supplementary materials. 4.4. Analysis of Known Depth Ratios. To investigate the dependency of our models performance on known depth ratios, we randomly select varying proportions of pixels with ground truth depth information to serve as known conditions for inference. As shown in Tab. 3, our model demonstrates outstanding performance even with very limited known conditions. We will provide more analysis on the known depth in the supplementary materials. 5. Conclusion and Future Work Conclusion. In this work, we introduced DepthLab, robust depth inpainting framework designed to handle complex scenarios by leveraging RGB images and known depth as conditional inputs. Our approach maintains geometric consistency, ensuring that estimated depth aligns seamlessly with existing structures. By leveraging the priors in pre-trained diffusion models, DepthLab demonstrates significant generalization across various depth inpainting tasks. This is evidenced by its superior performance on multiple benchmarks and applications, including 3D scene generation, Gaussian inpainting, LiDAR depth completion, and sparse-view reconstruction. Our experiments showcase the robustness of our method, highlighting its ability to improve both depth accuracy and downstream 3D scene quality. Moving forward, we envision DepthLab serving as foundational model for wider range of tasks. Future work. We expect three promising directions for future research: (1) employing consistency model [63] or flow-based approaches [37, 67] to accelerate sampling (2) further fine-tuning existing image VAEs to speed; more effectively encode sparse information, such as sparse masks and depth; and (3) extending this inpainting technique to normal estimation models, which could facilitate more versatile editing of different 3D assets."
        },
        {
            "title": "References",
            "content": "[1] James Andrew Bagnell, David Bradley, David Silver, Boris Sofman, and Anthony Stentz. Learning for autonomous navigation. IEEE Robotics & Automation Magazine, 17(2):7484, 2010. 2 [2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive In Computer Vision and Pattern Recognition, bins. pages 40094018, 2021. 2 [3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Localbins: Improving depth estimation by learning local distributions. In European Conference on Computer Vision, pages 480496. Springer, 2022. 2 [4] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 2 [5] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2, 2020. [6] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. arXiv preprint arXiv:2311.14521, 2023. 3 [7] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene genarXiv preprint eration from monocular videos. arXiv:2405.02280, 2024. 3 [8] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 3, 7 [9] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 6 [10] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Jurgen Sturm, and Matthias Nießner. Scancomplete: Large-scale scene completion and semantic In Computer Vision and segmentation for 3d scans. Pattern Recognition, pages 45784587, 2018. 3 [11] Angela Dai, Christian Diller, and Matthias Nießner. SG-NN: sparse generative neural networks for selfsupervised scene completion of RGB-D scans. In Computer Vision and Pattern Recognition, pages 846 855, 2020. 3 [12] Raul de Queiroz Mendes, Eduardo Godinho Ribeiro, Nicolas dos Santos Rosa, and Valdir Grassi Jr. On deep learning techniques to boost monocular depth estimation for autonomous navigation. Robotics and Autonomous Systems, 136:103701, 2021. 2, [13] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d In International Conference on Computer scans. Vision, pages 1078610796, 2021. 2, 5, 6 [14] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, and Yue Wang. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds, 2024. 2, 3, 7, 8 [15] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. arXiv preprint arXiv:2403.12013, 2024. 2, 5, 6 [16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision In Computer Vision and Pattern benchmark suite. Recognition, pages 33543361. IEEE, 2012. 2 [17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 6 [18] Ming Gui, Johannes Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast monocular depth estimation with flow matching. arXiv preprint arXiv:2403.13788, 2024. 2, 5, [19] Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, and Yizhou Yu. High-resolution shape completion using deep neural networks for global In Internastructure and local geometry inference. tional Conference on Computer Vision, pages 8593, 2017. 3 [20] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 79097920, 2023. 3 [21] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. 4 [22] Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiaojin Gong. Penet: Towards precise and efficient image guided depth completion. In 2021 IEEE 9 International Conference on Robotics and Automation (ICRA), pages 1365613662. IEEE, 2021. [23] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth arXiv preprint sequences for open-world videos. arXiv:2409.02095, 2024. 2 [24] Jiajun Huang and Hongchuan Yu. Pointn move: Interactive scene object manipulation on gaussian splatting radiance fields. CoRR, abs/2311.16737, 2023. 3 [25] Shahram Izadi, David Kim, Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 559568, 2011. 2 [26] Jinwoo Jeon, Hyunjun Lim, Dong-Uk Seo, and Hyun Myung. Struct-mdc: Mesh-refined unsupervised depth completion leveraging structural regularities IEEE Robotics and Automation from visual slam. Letters, 7(3):63916398, 2022. 2, 8 [27] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In International Conference on Computer Vision, pages 2174121752, 2023. 2 [28] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976, 2024. 3 [29] Jinyoung Jun, Jae-Han Lee, Chul Lee, and Chang-Su Kim. Depth map decomposition for monocular depth In European Conference on Computer estimation. Vision, pages 1834. Springer, 2022. 2 [30] Michael M. Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of SGP, pages 6170, 2006. [31] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for In Proceedings of the monocular depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 4, 5, 6 [34] Jianwei Li, Wei Gao, and Yihong Wu. High-quality 3d reconstruction with depth super-resolution and completion. IEEE Access, 7:1937019381, 2019. 2 [35] Pengzhi Li, Chengshuai Tang, Qinxuan Huang, and Zhiheng Li. Art3d: 3d gaussian splatting for textarXiv preprint guided artistic scenes generation. arXiv:2405.10508, 2024. 3 [36] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Binsformer: Revisiting adaptive bins for IEEE Transactions on Jiang. monocular depth estimation. Image Processing, 2024. 2 [37] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Flow matcharXiv preprint Maximilian Nickel, and Matt Le. ing for generative modeling. arXiv:2210.02747, 2022. 8 [38] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from arXiv sparse views with video diffusion model. preprint arXiv:2408.16767, 2024. 3 [39] Lina Liu, Xibin Song, Xiaoyang Lyu, Junwei Diao, Mengmeng Wang, Yong Liu, and Liangjun Zhang. Fcfr-net: Feature fusion based coarse-to-fine residual In Assoc. Adv. Artif. learning for depth completion. Intell., pages 21362144, 2021. 2 [40] Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Instance neural radiance In International Conference on Computer ViTai, and Chi-Keung Tang. field. sion, pages 787796, 2023. 3 [41] Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, and Yang Cao. Infusion: Inpainting 3d gaussians via learning depth completion from diffusion prior. arXiv preprint arXiv:2404.11613, 2024. 2, 3 [42] Fangchang Ma and Sertac Karaman. Sparse-to-dense: Depth prediction from sparse depth samples and single image. In 2018 IEEE international conference on robotics and automation (ICRA), pages 4796 4803. IEEE, 2018. [43] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. In 2019 International Conference on Robotics and Automation (ICRA), pages 3288 3295. IEEE, 2019. 2 [32] Dong Ki Kim and Tsuhan Chen. Deep neural network for real-time autonomous indoor navigation. arXiv preprint arXiv:1511.04668, 2015. 2 [44] Alican Mertan, Damien Jade Duff, and Gozde Unal. Single image depth estimation: An overview. Digital Signal Processing, 123:103441, 2022. 2 [33] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In Advances in Neural Information Processing Systems, 2022. 3 [45] Ashkan Mirzaei, Yash Kant, Jonathan Kelly, and Igor Gilitschenski. Laterf: Label and text driven object radiance fields. In European Conference on Computer Vision, pages 2036, 2022. 3 [46] Pushmeet Kohli Nathan Silberman, Derek Hoiem Indoor segmentation and support In ECCV, 2012. 2, 6, and Rob Fergus. inference from rgbd images. 8 [47] Hao Ouyang, Kathryn Heal, Stephen Lombardi, and Tiancheng Sun. Text2immersion: Generative imarXiv preprint mersive scene with 3d gaussians. arXiv:2312.09242, 2023. 3, 7 [48] Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. In European Conference on Computer Vision, pages 120136. Springer, 2020. 2, 8 [49] Jeong Joon Park, Peter R. Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Computer Vision and Pattern Recognition, pages 165174, 2019. 3 [50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR, abs/2307.01952, 2023. [51] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for IEEE transactions zero-shot cross-dataset transfer. on pattern analysis and machine intelligence, 44(3): 16231637, 2020. 2, 5, 6 [52] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In International Conference on Computer Vision, pages 1217912188, 2021. 2, 5, 6 [53] Alex Rasla and Michael Beyeler. The relative importance of depth cues and semantic edges for indoor mobility using simulated prosthetic vision in immersive virtual reality. In Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology, pages 111, 2022. 2 [54] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 4 [55] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 5 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion modIn Computer Vision and Pattern Recognition, els. 2022. 3 [57] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023. 2 [58] Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Computer Vision and Pattern Recognition, pages 41044113, 2016. 3 [59] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multicamera videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32603269, 2017. 6 [60] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, ChiehHsin Lai, Seungryong Kim, and Yuki Mitsufuji. Genwarp: Single image to novel views with semanticarXiv preprint preserving generative warping. arXiv:2405.17251, 2024. 3 [61] Mel Slater and Sylvia Wilbur. framework for immersive virtual environments (five): Speculations on the role of presence in virtual environments. Presence: Teleoperators & Virtual Environments, 6(6):603616, 1997. [62] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, and Thomas A. Funkhouser. Semantic scene completion from single depth image. In Computer Vision and Pattern Recognition, pages 190 198, 2017. 3 [63] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 8 [64] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. 3 [65] Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, and Ping Tan. Bilateral propagation network for depth completion. In Computer Vision and Pattern Recognition, pages 97639772, 2024. 2, 8 [66] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 6 [67] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Rectified diffusion: Wang, and Hongsheng Li. 11 Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 8 [68] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Computer Vision and Pattern Recognition, 2024. 2, 3, 7 [69] Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, and Ulrich Neumann. Shape inpainting using 3d generative adversarial network and recurrent convoIn International Conference on lutional networks. Computer Vision, pages 23172325, 2017. 3 [70] Yufei Wang, Bo Li, Ge Zhang, Qi Liu, Tao Gao, and Yuchao Dai. Lrru: Long-short range recurrent updating networks for depth completion. In International Conference on Computer Vision, pages 94229432, 2023. 2, [71] Yufei Wang, Ge Zhang, Shaoqian Wang, Bo Li, Qi Liu, Le Hui, and Yuchao Dai. Improving depth comIn Computer pletion via depth feature upsampling. Vision and Pattern Recognition, pages 2110421113, 2024. 2 [72] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. 2 [73] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In arXiv, 2023. 4 [74] Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, and Jian Yang. Tri-perspective view decomposition for In Computer geometry-aware depth completion. Vision and Pattern Recognition, pages 48744884, 2024. 2 [75] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled In Computer Vision and Pattern Recognition, data. 2024. 2 [76] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled In Computer Vision and Pattern Recognition, data. pages 1037110381, 2024. 2, 5, [77] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 2, 5, 6 [78] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. arXiv preprint arXiv:2312.00732, 2023. 6 [79] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. arXiv preprint arXiv:2312.00732, 2023. 3 [80] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth arXiv preprint prediction using diverse data. arXiv:2002.00569, 2020. 2, 5, 6 [81] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Computer Vision and Pattern Recognition, pages 204213, 2021. 2, 5, 6 [82] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In International Conference on Computer Vision, pages 90439053, 2023. 6 [83] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, and Charles Herrmann. Wonderjourney: GoarXiv preprint ing from anywhere to everywhere. arXiv:2312.03884, 2023. 2, [84] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv preprint arXiv:2406.09394, 2024. 3 [85] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [86] Yuan, Gu, Dai, Zhu, and Tan. New crfs: Neural window fully-connected crfs for monocarXiv preprint ular depth estimation. arxiv 2022. arXiv:2203.01502, 2022. 2 [87] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. Advances in Neural Information Processing Systems, 35:14128 14139, 2022. 2, 5, 6 [88] Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, and Christopher Schroers. Betterdepth: Plug-and-play diffusion refiner for zeroarXiv preprint shot monocular depth estimation. arXiv:2407.17952, 2024. 2 [89] Youmin Zhang, Xianda Guo, Matteo Poggi, Zheng Zhu, Guan Huang, and Stefano Mattoccia. Completionformer: Depth completion with convolutions and vision transformers. In Computer Vision and Pattern Recognition, pages 1852718536, 2023. 2, 8 [90] Shanshan Zhao, Mingming Gong, Huan Fu, and Dacheng Tao. Adaptive context-aware multi-modal network for depth completion. IEEE Transactions on Image Processing, 30:52645276, 2021. 2, 8 [91] Qian-Yi Zhou and Vladlen Koltun. Color map optimization for 3d reconstruction with consumer depth cameras. ACM Trans. Graph., 33(4):110, 2014. 2 [92] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2025. 3, 7 [93] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023."
        }
    ],
    "affiliations": [
        "Aalto University",
        "Ant Group",
        "HKU",
        "HKUST",
        "Tongyi Lab"
    ]
}