{
    "paper_title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs",
    "authors": [
        "Wei Zhou",
        "Jun Zhou",
        "Haoyu Wang",
        "Zhenghao Li",
        "Qikang He",
        "Shaokun Han",
        "Guoliang Li",
        "Xuanhe Zhou",
        "Yeye He",
        "Chunwei Liu",
        "Zirui Tang",
        "Bin Wang",
        "Shen Tang",
        "Kai Zuo",
        "Yuyu Luo",
        "Zhenzhe Zheng",
        "Conghui He",
        "Jingren Zhou",
        "Fan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation. By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 1 Can LLMs Clean Up Your Mess? Survey of Application-Ready Data Preparation with LLMs Wei Zhou, Jun Zhou, Haoyu Wang, Zhenghao Li, Qikang He, Shaokun Han, Guoliang Li Fellow, IEEE, Xuanhe Zhou, Yeye He, Chunwei Liu, Zirui Tang, Bin Wang, Shen Tang, Kai Zuo, Yuyu Luo, Zhenzhe Zheng, Conghui He, Jingren Zhou Fellow, IEEE, Fan Wu Awesome-Data-LLM: https:// github.com/ weAIDB/ awesome-data-llm 6 2 0 2 J 2 2 ] . [ 1 8 5 0 7 1 . 1 0 6 2 : r AbstractData preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming transformative and potentially dominant paradigm for data preparation. By investigating hundreds of recent literature works, this paper presents systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols. Index TermsData Preparation, Data Cleaning, Data Integration, Data Enrichment, LLMs, Agents I. INTRODUCTION"
        },
        {
            "title": "D ATA preparation refers to the process of transforming",
            "content": "trustworthy raw datasets into high-quality ones (e.g., and comprehensive) by denoising corrupted inputs, identifying cross-dataset relationships, and extracting meaningful insights. Despite its foundational role in downstream applications such as business intelligence (BI) analytics [1], [2], training [3], [4], and data machine learning (ML) model Wei Zhou, Jun Zhou, Haoyu Wang, Zhenghao Li, Qikang He, Shaokun Han, Xuanhe Zhou, Zhenzhe Zheng, and Fan Wu are with Shanghai Jiao Tong University, Shanghai, China. Guoliang Li is with Tsinghua University, Beijing, China. Yeye He is with Microsoft Research. Chunwei Liu is with MIT CSAIL, USA. Bin Wang and Conghui He are with Shanghai AI Laboratory. Shen Tang and Kai Zuo are with Xiaohongshu Inc. Yuyu Luo is with the Hong Kong University of Science and Technology (Guangzhou), China. Jingren Zhou is with Alibaba Group. Corresponding author: Xuanhe Zhou (zhouxuanhe@sjtu.edu.cn). Fig. 1. Application-Ready DATA PREPARATION Three core tasks (i.e., Data Cleaning, Integration, and Enrichment) address key sources of data inefficiency: quality issues, integration barriers, and semantic gaps. sharing [5], [6], data preparation remains critical bottleneck in real scenarios. For instance, an estimated 20%30% of enterprise revenue is lost due to data inefficiencies [7]. As illustrated in Figure 1, real-world data inefficiencies primarily arise from three sources: (1) Consistency & Quality Issues (e.g., non-standard formats, noise, and incompleteness); (2) Isolation & Integration Barriers (e.g., disparate systems, entity ambiguity, and schema conflicts); and (3) Semantic & Context Limitations (e.g., missing metadata and unlabeled data). To these challenges, data preparation [8], [9] involves three main tasks: Data Cleaning, Data Integration, and Data Enrichment, which transform raw inputs into unified, reliable, and enriched datasets. As the volume and heterogeneity of data continue to surge (e.g., global data volume is forecast to triple from 2025 to 2029 [10]), the imperative for effective data preparation has never been greater. However, traditional data preparation methods rely heavily on static rules [11], [12], manual interventions, or narrowly scoped models [13], [14], motivating the need for more intelligent, adaptive solutions. A. Limitations of Traditional DATA PREPARATION As discussed above, traditional preparation techniques, ranging from heuristic rule-based systems [15], [16], [12] IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 2 Fig. 2. Overview of Application-Ready DATA PREPARATION through LLM-Enhanced Methods. to domain-specific machine-learning models [17], [18], [13], [14], face several fundamental limitations. (Limitation ❶) High Manual Effort and Expertise Dependence. Traditional data preparation methods largely depend on fixed rules and domain-specific configurations, such as regular expressions and validation constraints [19], [20]. This reliance demands substantial manual effort and specialized expert knowledge, introducing significant development and maintenance barriers. For instance, data standardization typically requires complex, handcrafted scripts (e.g., userdefined functions) or manual constraints (e.g., date formatting rules) [11], [21]. Similarly, data error processing pipelines often rely on fixed detect-then-correct workflows defined by manually crafted rules, which are not only labor-intensive to maintain but also prone to introducing new errors (e.g., incorrectly repaired values) during correction [22]. (Limitation ❷) Limited Semantic Awareness in Preparation Enforcement. Conventional rule-based approaches predominantly rely on statistical patterns (e.g., computing missing value percentages) or syntactic matching, which fundamentally limit their ability to accurately identify complex inconsistencies that require semantic reasoning. For example, in data integration, traditional similarity-based matching techniques struggle to resolve semantic ambiguities (such as abbreviations, synonyms, or domain-specific terminology) due to the lack of commonsense or domain-specific knowledge [23]. Moreover, keyword-based search mechanisms in data enrichment frequently fail to capture user intent, creating semantic gap that leaves relevant datasets undiscovered [24], [25]. (Limitation ❸) Poor Generalization across Diverse Preparation Tasks and Data Modalities. Traditional deep learning models typically require specialized feature engiIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 3 neering [14] or domain-specific training [13], which severely restricts their generalizability across diverse domains and data modalities. For example, fine-tuned entity-matching models exhibit significant performance degradation when applied to out-of-distribution entities [26]. Similarly, supervised data annotation models struggle to perform well on data from underrepresented subgroups or domains with limited labeled examples [27], [28]. Furthermore, methods designed for structured tabular data often fail to effectively process semi-structured text or other modalities [29], limiting their applicability in heterogeneous data environments. (Limitation ❹) Preparation Reliance on Labeled Data and Limited Knowledge Integration. Small-model-based approaches typically require large volumes of high-quality and accurately labeled examples, which can be expensive to obtain at scale [30]. For instance, in data annotation, the prohibitively high cost of expert labeling limits the scale of reliable datasets, whereas crowdsourced alternatives often exhibit unstable quality [31]. Moreover, existing methods often lack the flexibility to integrate diverse contexts. For example, general retrievalbased systems [24] face challenges in effectively integrating structured table data with unstructured free-text context. B. LLM-Enhanced DATA PREPARATION: Driving Forces And Opportunities To overcome these limitations, recent advances in large language models (LLMs) have catalyzed paradigm shift in data preparation [32], [33]. This transformation is fueled by three converging forces. First, the increasing demand for application-ready data, which is essential for scenarios such as personalizing customer experiences [34] and enabling the methodological shift from real-time analytics. Second, static, rule-based pipelines to LLM agent frameworks that can autonomously plan (e.g., interpret ambiguous data patterns), execute (e.g., adapt to heterogeneous formats), and reflect infrastructure advances on data preparation actions. Third, that support flexible and cost-effective LLM technique usage, such as the API integrations for LLM agent construction in Databricks Unity Catalog [35] and the proliferation of opensource LLMs. By leveraging generative capabilities, semantic reasoning, and extensive pretraining, LLMS introduce paradigm shift that offers opportunities in four aspects. (Opportunity ❶) From Manual Preparation to Instruction-Driven and Agentic Automation. To address the high manual effort and expertise dependence in data preparation, LLM-enhanced techniques facilitate natural-language interactions and automated workflow generation [36], [19]. For instance, in data cleaning, users can directly define transformation logic using textual prompts rather than writing complex user-defined functions [11]. Moreover, advanced data cleaning frameworks (e.g., Clean Agent [36], AutoDCWorkflow [19]) have integrated LLM-enhanced agents to orchestrate cleaning workflows, in which agents plan and execute pipelines by identifying quality issues and invoking external tools to achieve effective data cleaning with minimal human intervention. (Opportunity ❷) Semantic Reasoning for Consistent Preparation Enforcement. Unlike traditional methods that rely on syntactic similarity or heuristics, LLM-enhanced approaches incorporate semantic reasoning into preparation workflows [20], [22]. For example, in data integration, LLMs utilize pretrained semantic knowledge to resolve ambiguities of abbreviations, synonyms, and domain-specific terminology [23]. In data enrichment, LLMs infer semantic column groups and generate human-aligned dataset descriptions, enabling more accurate dataset understanding and enrichment beyond keyword-based or statistical profiling [37], [38]. (Opportunity ❸) From Domain-Specific Preparation Training to Cross-Modal Generalization. LLM-enhanced techniques reduce reliance on domain-specific feature engineering and task-specific training, demonstrating strong adaptability across data modalities [39]. For example, in data cleaning, LLMs handle heterogeneous schemas and formats by following instructions via few-shot, similarity-based in-context prompting without fine-tuning [40]. For tabular data integration, specialized encoders (e.g., TableGPT2 [41]) bridge the modality gap between tabular structures and textual queries, ensuring robust performance without extensive domain-specific feature engineering. (Opportunity ❹) Knowledge-Augmented Preparation with Minimal Labeling. LLMs alleviate the need for large volumes of high-quality labels by exploiting pretrained knowledge and dynamically integrating external context [42]. For example, in entity matching, some methods incorporate external domain knowledge (e.g., from Wikidata) and structured pseudo-code into prompts to reduce reliance on task-specific training pairs [43]. In data cleaning and data enrichment, Retrieval-Augmented Generation (RAG) based frameworks retrieve relevant external information from data lakes, enabling accurate value restoration and metadata generation without requiring fully observed training data [44], [25]. C. Contributions and Differences with Existing Surveys data (e.g., preparation application-ready We comprehensively review recent advances in LLMenhanced for decision-making, analytics, or other applications) with focused scope. Instead of covering all possible preparation tasks, we concentrate on three core tasks that appear most in existing studies [8], [9] and real-world pipelines [45] (i.e., data cleaning, data integration, and data enrichment in Figure 2). Within this scope, we present task-centered taxonomy, summarize representative methods and their technical characteristics, and discuss open problems and future research directions. Data Cleaning. Targeting the Consistency & Quality Issues this task aims to produce standardized and in Figure 1, denoised data. We focus on three main subtasks: (1) Data Standardization, which transforms diverse representations into unified formats using specific prompts [11], [21] or agents that automatically generate cleaning workflows [36], [19]; (2) Data Error Processing, which detects and repairs erroneous values (e.g., spelling mistakes, invalid values, outlier values) through direct LLM prompting [46], [37], [20], methods that add context to the model [40], [47], or fine-tuning models for specific error types [20]; and (3) Data Imputation, which IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 fills missing values using clear instructions and retrievalaugmented generation to find relevant information [44]. Data Integration. Addressing the Isolation & Integration Barriers in Figure 1, this task aims to identify and combine related data from different sources. We review two core subtasks: (1) Entity Matching, which links records referring to the same real-world entity using structured prompts [26], [48], sometimes supported by code-based reasoning [43]; and (2) Schema Matching, which matches columns or attributes between datasets using direct prompting [49], RAG techniques with multiple models [50], knowledge graph-based methods [23], or agent-based systems that plan the matching process [51], [52]. Data Enrichment. Focusing on the Semantic & Context Limitations, this task augments datasets with semantic insights. We cover two key subtasks: (1) Data Annotation, which assigns data labels or types using various prompting strategies [53], [54], [28], supported by retrieval-based [55] and LLM-generated context [24]; and (2) Data Profiling, which generates semantic profiles and summaries (e.g., metadata) using task-specific prompts [38], [56], often enhanced with external context via retrieval-augmented generation [25]. Compared with existing LLM and data preparation surveys [95], [96], [97], [32], [98], [99], [33], [100], [101], [102], our survey differs in several significant aspects. Holistic vs. Limited Task Scope. We provide comprehensive review of three fundamental data preparation tasks (cleaning, integration, enrichment) across diverse data modalities, including table and text. In contrast, existing surveys typically limit their scope to specific tasks [103], [95] or only the tabular modality [33], [100] Systematic Taxonomy vs. Coarse or Narrow Method Category. We propose unified taxonomy that systematically organizes existing LLM-enhanced methods by underlying techniques, including prompt-based and LLM agent-based frameworks. In contrast, prior surveys either classifies works using coarse, general categories [8] or limit their focus to specific methods, such as agent-based systems [9]. Paradigm Shift Summary vs. Static Description. We systematically examine how data preparation has evolved from rule-based systems to LLM agent frameworks, summarizing the corresponding advantages and limitations. In contrast, prior studies [8] present works individually, offering limited analysis of paradigm shifts and little discussion of the fields evolution. Emerging Challenges and Roadmap vs. Conventional Perspectives. We summarize challenges in the LLM era, including inference costs, hallucinations, and cross-modal consistency, and outline forward-looking roadmap. This distinguishes our work from existing surveys that focus primarily on typical issues (e.g., scalability) or offer generic conclusions, providing guidance for the next-generation data preparation. Moreover, we have the following observations on the evolution of methodology across data preparation tasks. Shift Toward Cost-Efficient Hybrid Methods. Recent work moves beyond exclusive reliance on LLM inference and instead adopts hybrid approaches. Among them, LLMs either generate executable preparation programs or transfer their reasoning capabilities to smaller language models (SLMs), thereby reducing execution cost and improving scalability. Reduced Emphasis on Task-Specific Fine-Tuning. The focus has shifted away from maintaining heavily fine-tuned, task-specific LLMs toward methods that optimize other aspects, such as the input construction. Techniques such as retrieval augmentation and structured serialization are used to adapt general-purpose models to new tasks, enabling greater flexibility and lower maintenance overhead. Limited Attempts of Agentic Implementations. Although agent-based orchestration supports more autonomous data preparation workflows, relatively few systems have been fully studied and implemented in practice. This gap indicates that reliable and robust agentic deployment remains to be explored. Task-Specific Methodology Difference. Data cleaning employs hybrid LLM-ML approach for accurate error detection and repair; data integration emphasizes multi-model collaboration to scale matching and alignment; and data enrichment integrates retrieval-augmented and hybrid prompting techniques to enhance the semantic understanding of data and metadata. Cross-Modal Generalization with Unified Representations. Recent methods increasingly support multiple data modalities within single architecture. By using shared semantic representations, text, and other data uniformly, reducing the reliance on modalityspecific feature engineering. these methods process tables, II. DATA PREPARATION: DEFINITION AND SCOPE In this section, we provide clear definition of three fundamental data preparation tasks, including Data Cleaning to remove errors and inconsistencies from raw data, Data Integration to combine and harmonize data from multiple sources, and Data Enrichment to identify patterns, relationships, and knowledge that support informed decisions. Data Cleaning aims to convert corrupted or low-quality data within dataset into trustworthy form suitable for downstream tasks (e.g., statistical analysis). It involves tasks such as fixing typographical errors, resolving formatting inconsistencies, and handling violations of data dependencies. Recent LLM-enhanced studies primarily focus on three critical tasks including data standardization, data error detection and correction of data errors, and data imputation. (1) Data Standardization [104], [105] aims to transform heterogeneous, inconsistent, or non-conforming data into unified representation that satisfies predefined consistency requirements. Formally, given dataset and consistency criteria C, it applies or learns standardization function fstd such that the output dataset Dstd = fstd(D, C) satisfies C. Typical tasks include format normalization (e.g., converting dates from 7th April 2021 to 20210407), case normalization (e.g., SCHOOL to school), and symbol or delimiter cleanup (e.g., removing redundant separators 1000 . to obtain 1000). LLM-enhanced methods leverage context-aware prompting and reasoning-driven code synthesis to produce automated, semantically consistent transformations, reducing reliance on manual pattern definition and improving generalization across heterogeneous data formats. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 TABLE TECHNIQUE OVERVIEW OF LLM-ENHANCED DATA PREPARATION METHODS. Task Category Modality Year Work Data Standadization Data Error Processing Data Imputation Entity Matching Schema Matching Data Annotation Prompt-Based End-to-End Standardization Automatic Code-Synthesis Standardization Tool-Assisted Agent-Based Standardization Prompt-Based End-to-End Error Processing Function-Synthesis-Oriented Error Processing Task Adaptive Fine-Tuned Model-Based Error Processing Hybrid LLM-ML Enhanced Error Processing Tabular Text 2023 2024 2023 Tabular Tabular Text Other Tabular 2024 2025 2024 2024 Tabular 2024 Tabular 2025 Prompt-Based End-to-End Imputation Tabular 2025 Context-Retrieval Guided Imputation Model-Optimized Adaptive Imputation Prompt-Based End-to-End Matching Task-Adaptive-Tuned Matching Multi-Model Collaborative Matching Prompt-Based End-to-End Matching Retrieval-Enriched Contextual Matching Model-Optimized Adaptive Matching Multi-Model Collaborative Matching Agent-Guided Orchestration-based Matching Prompt-based End-to-End Annotation RAG-Assisted Contextual Annotation Fine-tuned Augmented Annotation Tabular Tabular Tabular Text Tabular Tabular Text Tabular Tabular Tabular Tabular Other Tabular Tabular Text Tabular Text Tabular Text Hybrid LLM-ML Annotation Text Tool-Assisted Agent-based Annotation Tabular Other 2024 2025 2024 2024 2025 2024 2023 2025 2025 2025 2024 2025 2024 2025 2025 2024 2025 2024 2025 2023 2024 2024 2025 2025 2025 2025 2025 2025 2025 2024 Data Profiling Prompt-Based End-to-End Profiling RAG-Assisted Contextual Profiling Tabular 2025 2026 2025 2025 2025 Other Tabular Other LLMGDO [11] LLM-Preprocessor [57] EVAPORATE [21] AutoDCWorkflow [19] CleanAgent [36] Cocoon-Cleaner [37] IterClean [22] LLMErrorBench [47] Multi-News+ [46] LLM-SSDC [29] LLMClean [40] LLM-TabAD [58] GIDCL [20] ForestED [59] ZeroED [60] LDI [61] CRILM [62] LLM-PromptImp [63] LLM-Forest [64] RetClean [44] LakeFill [42] LLM-REC [65] UnIMP [66] Quantum-UnIMP [67] BATCHER [48] KCMF [43] MatchGPT [26] LLM-CER [68] ChatEL [69] Jellyfish [39] LLM-CDEM [70] FTEM-LLM [71] COMEM [72] LLMaEL [73] LLMSchemaBench [49] GLaVLLM [74] Matchmaker [75] KG-RAG4SM [23] TableLlama [76] TableGPT2 [41] Magneto [50] Agent-OM [51] Harmonia [52] ArcheType [27] CHORUS [53] Goby [54] Columbo [77] LLMCTA [28] EAGLE [78] AutoLabel [79] LLMLog [80] Anno-Lexical [31] RACOON [55] Birdie [24] LLMAnno [81] PACTA [82] OpenLLMAnno [83] CanDist [84] AutoAnnotator [85] STA Agent [86] TESSA [87] DynoClass [88] Cocoon [89] AutoDDG [38] LEDD [56] LLM-HTS [90] HyperJoin [91] OCTOPUS [92] LLMCodeProfiling [93] Pneuma [25] LLMDap [94] ICL CoT - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Prompting Ensemble - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - RAG Model Adaptation Agentic Workflow Keyword Semantic Other SFT RL - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - SelfReflect - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 5 Hybrid - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Output Strategy LLM Inference Program Synthesis - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (2) Data Error Processing [15], [106], [107] refers to the two-stage process of detecting erroneous values and subsequently repairing them to restore data reliability. Formally, given dataset and set of error types K, an detection function fid(D, K) identifies an error set Derr, after which repair function ffix produces refined dataset Dfix = ffix(D, Derr) such that fid(Dfix, K) = . Typical tasks include identifying data irregularities (e.g., constraint violations) and performing data corrections (e.g., resolving encoding errors) to uphold data correctness. LLM-enhanced techniques employ hybrid LLMML architectures and executable code generation to deliver accurate, scalable error identification and correction, thereby lowering dependence on hand-crafted rules and boosting adaptability across varied, noisy datasets. (3) Data Imputation [108], [109], [110] refers to the task of detecting missing data entries and estimating plausible values for them, with the goal of restoring datasets structural completeness and logical coherence. More formally, given dataset containing missing entries, the objective is to learn or apply an imputation function fimp that yields completed dataset Dimp = fimp(D), in which all previously missing entries are filled with inferred, plausible values. Typical tasks include predicting absent columns based on correlated attributes (e.g., deducing missing city from phone area code) or exploiting auxiliary sources (e.g., inferring missing product attributes using relevant tuples from data lake). LLMenhanced approaches use semantic reasoning and external knowledge to generate accurate, context-aware replacements, lessening dependence on fully observed training data and enhancing generalization across heterogeneous datasets. Data Integration aims to align elements across diverse datasets so that they can be accessed and analyzed in unified, consistent manner. Instead of exhaustively enumerating all integration task, this survey focuses on entity matching and schema matching, as these are key steps in real-world data integration workflows and have received the most attention in IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 6 recent LLM-based research. (1) Entity Matching [111], [112] refers to the task of deciding whether two records correspond to the same real-world entity, facilitating data alignment within single dataset or across multiple datasets. More formally, given two collections R1 and R2 and record pair (r1, r2) with r1 R1 and r2 R2, the objective is to estimate and assign score to the likelihood that the two records describe the same entity. Typical subtasks include mapping product listings across different e-commerce sites (e.g., associating the same item on Amazon and eBay) and detecting duplicate customer entries. LLMenhanced entity matching leverages structured prompting and collaboration among multiple models to deliver robust and interpretable matching, reducing dependence on task-specific training and enhancing generalization across diverse schemas. (2) Schema Matching [113], [114], [115] aims to identify semantic correspondences between columns or tables across heterogeneous schemas, thereby supporting integrated data access and analysis. Formally, given source schema Ss and target schema St, each represented as collection of tables with their respective column sets, the goal is to learn mapping function fsm that maps every source column As to semantically equivalent target column At (or to if no suitable counterpart exists). Common subtasks involve matching columns whose names with synonymous meanings (e.g., linking price in one table with cost in another) and detecting correspondences between tables (e.g., aligning CustomerInfo with ClientDetails). LLM-enhanced schema matching leverages prompt-based reasoning, retrievalaugmented information, and multi-agent coordination to handle semantic ambiguity and structural variation, thereby lowering reliance on hand-crafted rules and improving alignment quality across heterogeneous domains. Data Enrichment focuses on augmenting datasets by adding semantic labels and descriptive metadata, or by discovering complementary datasets that increase their value for downstream tasks (e.g., data analysis). It involves subtasks such as classifying column types and producing dataset-level descriptions. This survey concentrates on data annotation and data profiling, which represent the predominant enrichment operations in existing LLM-enhanced studies. (1) Data Annotation [116], [117] aims to attach semantic or structural labels to elements in raw data so that they can be understood and utilized by downstream applications. Formally, given dataset D, the objective is to define labeling function fann that maps each data element to one or more labels in L, such as its semantic role or data type. Typical subtasks include semantic column-type annotation (e.g., identifying column as CustomerID or birthDate), table-class detection (e.g., determining that table is an Enterprise Sales Record), and cell entity annotation (e.g., linking the cell Apple to the entity Apple_Inc). LLM-enhanced annotation leverages instruction-based prompting, retrievalaugmented context, and fine-tuning to deliver precise, scalable, and domain-sensitive labeling, substantially decreasing manual workload and reducing manual effort and mitigating hallucination compared to traditional task-specific models. (2) Data Profiling [118], [119] refers to the task of systematically analyzing dataset to derive its structural, statistical, and semantic properties, as well as identifying associations with relevant datasets, thereby producing rich metadata that facilitates data comprehension and quality evaluation. Formally, for dataset D, profiling function fpro generates metadata collection = {m1, . . . , mk}, where each metadata element mi encodes characteristics such as distributional statistics, structural regularities, semantic categories, or connections to semantically related datasets. Common subtasks include semantic metadata generation (e.g., summarizing the contents of tables and assigning domain-aware descriptions to columns) and structural relationship extraction (e.g., clustering related columns and inferring hierarchical dependencies). LLMenhanced profiling combines prompt-based analysis, retrievalaugmented contextualization, and layered semantic reasoning to yield accurate, interpretable metadata that improves data exploration, enables quality assurance, and offers reliable foundation for downstream applications. Unlike data preparation pipelines designed specifically for training, fine-tuning, or directly prompting LLMS themselves [8], this survey focuses on LLM-enhanced data preparation methods that aim to refine the quality, consistency, and semantic coherence of data used in downstream analytical and machine-learning applications, as summarized in Table I. III. LLM FOR DATA CLEANING Traditional data cleaning methods rely on rigid rules and constraints (e.g., ZIP code validation), which demand substantial manual effort and domain expertise (e.g., schema knowledge in financial data) [19], [20]. Moreover, they often require task-specific training, which limits their generalization across different scenarios [21]. Recent studies show that LLMS can address these limitations by reducing manual and programming effort (e.g., offering natural language interfaces), and supporting the seamless integration of domain knowledge for the following tasks. Data Standardization. Data standardization refers to transforming heterogeneous or non-uniform values into unified format, enabling dependable analysis and efficient downstream processing. Existing LLM-enhanced standardization techniques can be classified into three main categories. ❶ Prompt-Based End-to-End Standardization. As shown in Figure 3, this method uses structured prompts that specify detailed standardization rules (e.g., normalization criteria) or provide stepwise reasoning instructions, guiding LLMS to generate data outputs in standardized format. Instruction-Guided Standardization Prompting. This category relies on manually crafted prompts, together with incontext or labeled standardization examples, to guide LLMS in performing data standardization across diverse tasks. For instance, LLM-GDO [11] employs user-specified prompts with parameterized templates to encode data standardization rules as textual instructions (e.g., convert dates into YYYYMMDD) and to substitute user-defined functions (e.g., executable formatting code implementations). This Reasoning-Enhanced Batch Standardization Prompting. and reasoning step-wise leverages category IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 7 batch-wise processing prompting to enhance both the standardization robustness and efficiency. For instance, LLMPreprocessor [57] proposes unified prompting framework that tackles hallucinations, domain shifts, and computational costs through: (1) zero-shot Chain-of-Thought prompting, which elicits step-by-step reasoning to first verify the correct target column and then to guide LLMS in producing the standardized output; and (2) batch-wise prompting, which feeds multiple items into single prompt so they can be processed simultaneously. ❷ Automatic Code-Synthesis Standardization. This approach standardizes data by instructing LLMS to generate executable code that performs the standardization. The generated code is then executed to ensure uniform data handling and improve efficiency. For instance, Evaporate [21] prompts LLMS to produce code that derives structured representations from semi-structured documents; results from multiple candidate functions are then combined to boost accuracy while preserving low computational overhead. ❸ Tool-Assisted Agent-Based Standardization. As shown in Figure 3, this approach overcomes the challenges of complex prompt design by employing LLM agents to coordinate and execute standardization pipelines. For instance, CleanAgent [36] maps specific standardization operations with domain-specific APIs, and relies on agents to execute standardization pipeline, which involves generating API calls (e.g., clean_date(df, \"Admission Date\", \"MM/DD/YYYY\")) and executing them iteratively. Similarly, AutoDCWorkflow [19] leverages LLM agents to assemble pipelines and carry out stepwise reasoning to locate relevant columns, evaluate data quality, and apply appropriate operations (e.g., upper() and trim()), while leveraging tools such as OpenRefine [12] for execution and feedback. Discussion. (1) Prompt-Based Standardization for Heterogeneous Modalities. This paradigm leverages structured instructions and in-context examples to flexibly convert diverse inputs into unified format, enabling rapid, trainingfree deployment [21]. Nonetheless, its dependence on direct LLM inference leads to high token consumption and constrains scalability for large-scale or frequently repeated tasks. (2) Code-Based Standardization for Scalable Execution. This paradigm enhances efficiency by using reusable transformation functions with fixed execution cost, making it well-suited for processing large datasets [21], [57]. However, it is vulnerable to errors because LLMS may produce faulty code, requiring the aggregation of multiple candidate functions to maintain reliability. (3) Agentic-Based Standardization for Automated Pipelines. This paradigm constructs automated pipelines by translating natural-language specifications into executable workflows, thereby increasing usability and transparency [11], [36], [19]. However, coordinating numerous tools and APIs introduces additional maintenance overhead and can increase latency relative to direct prompt-based approaches. Data Error Processing. Given data item, data error processing typically involves two stages: detecting errors and then Fig. 3. Example of LLM-Enhanced Data Standardization. correcting them. Common error types include typographical mistakes (typos), anomalous numeric values, and violations of data dependencies. Existing approaches to error processing can generally be grouped into four major categories. ❶ Prompt-Based End-to-End Error Processing. This approach relies on structured prompts that describe explicit error detection and correction instructions, organize processing steps into iterative workflows, or incorporate illustrative examples and reasoning guidance, to instruct LLMS to identify and repair data errors directly. Instruction-Based Processing Prompting. This category pairs explicit prompting instructions with serialized tabular rows to guide LLMS in performing error detection and correction. For instance, Cocoon-Cleaner [37] uses batch-style prompting by serializing sampled values from each column (e.g., 1,000 entries per column) and grouping them by their corresponding subject column. It allows LLMS to iteratively identify and fix issues such as typos and inconsistent formats, with minimal supervision (e.g., five labeled tuples). Workflow-Based Iterative Processing Prompting. This category encompasses iterative, multi-step processing workflows (e.g., the detectverifyrepair loop), in which LLM repeatedly executes, evaluates, and refines processing operations. For instance, LLMErrorBench [47] guides LLMS through an iterative sequence of dataset examination, targeted correction (e.g., value substitution), and automated quality evaluation, using prompts enriched with contextual cues such as error locations. To address newly introduced errors and the dependence on rigid, predefined rules in sequential pipelines, IterClean [22] introduces an integrated prompting framework in which LLMS simultaneously serve as error detector, self-verifier, and data repairer within continuous feedback loop. Exampleand Reasoning-Enhanced Processing Prompting. This category incorporates few-shot examples and explicit reasoning steps into error-handling pipelines. For instance, Multi-News+ [46] employs Chain-of-Thought prompting in conjunction with majority voting and self-consistency verification, thereby mimicking human decision-making to enhance both the accuracy and interpretability of noisy document clasIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 8 sification. To alleviate the need for manually crafting intricate parsing rules for semi-structured data errors, LLM-SSDC [29] recasts the problem as text correction task, using oneshot prompt that includes general instructions and single input-output example. This allows LLMS to automatically fix structural misplacements (e.g., relocating paragraph indices from <content> tag to <num> tag). ❷ Function-Synthesis-Oriented Error Processing. To address the scalability of manually crafting rules, this approach leverages LLMS to synthesize executable processing functions that explicitly encode table semantics and data dependencies. For instance, LLMClean [40] instructs LLMS to derive collection of ontological functional dependencies (OFDs) from the dataset schema, the data, and domain ontology, which together define validation rules within context model. Each OFD represents concrete rule, such as ZipCode City in postal ontology. These OFDs are subsequently used to detect errors (e.g., inconsistent values) and to steer iterative data repair via integrated tools such as Baran [120]. ❸ Task-Adaptive Fine-Tuned Error Processing. As shown in Figure 4, this method fine-tunes LLMS to learn datasetspecific error patterns that are hard to capture via prompting alone, leveraging synthetic noise or contextual augmentation to enhance both error detection and correction performance. Synthetic Noise-Augmented Fine-Tuning. This category finetunes LLMS using synthetic datasets augmented with different noises, such as Gaussian or multinomial, to learn error detection. For instance, LLM-TabAD [58] adapts base LLMS (e.g., Llama 2 [121]) for error detection by constructing synthetic datasets where each example is small batch of rows together with the indices of the abnormal rows. Continuous columns in the rows are drawn from mixture of narrow Gaussian (normal values) and wide Gaussian (anomalous extremes), while categorical columns are sampled from two multinomial distributions with different probability patterns. Each batch is then serialized into natural-language description, and the LLM is fine-tuned to predict the anomaly row indices. LLM-Based Context Augmentation Fine-Tuning. In this category, LLMS are fine-tuned using prompts that are enriched information, such as serialized with additional contextual neighboring cells and retrieved similar examples. As an illustration, GIDCL [20] constructs fine-tuning data by combining labeled tuples with pseudo-labeled tuples produced via LLMbased augmentation. Each training instance is represented as context-enriched prompt that includes: (1) an instruction (e.g., Correct the ProviderID to valid numeric format), (2) serialized erroneous cell along with its row and column context (e.g., <COL>ProviderID<VAL>1x1303...), (3) in-context learning examples (e.g., bxrmxngham birmingham), and (4) retrieval-augmented examples drawn from the same cluster (e.g., clean tuples obtained via k-means). ❹ Hybrid LLM-ML Enhanced Error Processing. As shown in Figure 4, this approach integrates LLMS with machine learning models to strike balance between accuracy and computational efficiency in handling errors. In practical deployments, LLMS are either employed to create labeled datasets that train ML models, or to derive structural representations that guide ML-based error processing. Fig. 4. Example of LLM-Enhanced Data Error Processing. LLM-Labeled ML Processing Training. In this category, LLM is employed as data labeler to create pseudo-labels and synthetic examples of correctly identified errors, which are then used to train lightweight ML model that serves as an efficient detector. As an illustrative instance, ZeroED [60] uses LLMS to annotate features and subsequently trains lightweight ML classifier (e.g., an MLP) for end-to-end error detection. The training dataset is obtained via zeroshot pipeline: representative values are first chosen through clustering, then labeled by the LLM, and these labels are propagated to nearby values. The dataset is further enriched with LLM-generated synthetic corruptions (e.g., substituting valid ages with impossible values such as 999) to better capture rare error patterns. LLM-Induced Structure for ML Processing. In this category, LLM is employed as logical blueprint to construct interpretable error-detection programs, which are later run and combined by machine-learning models. As an illustration, to enhance both explainability and robustness in data processing, ForestED [59] restructures the processing pipeline by leveraging the LLM to produce transparent decision structures (e.g., trees whose nodes apply rule-based format or range checks, along with relational nodes that encode cross-column dependencies), while downstream ML models execute and aggregate these structures to generate the final predictions. Discussion. (1) Prompt-Based Processing for End-toEnd Workflows. This approach reframes error processing as generative modeling problem through data serialization [29], [46], and couples decomposed pipelines with iterative verification loops to ensure robust reasoning [37], [22], [47]. However, direct LLM inference remains constrained by token limits when operating on large tables, and iterative self-correction cycles can compound hallucinations or introduce new errors. (2) Function-Synthesis Processing for Automatic Rule Discovery. This paradigm leverages LLMS to autonomously identify hidden dependencies and synthesize explicit, executable cleaning routines directly from raw data [40]. However, deriving strict validation rules from already corrupted inputs risks overfitting to noise, causing the LLM to synthesize invalid IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 9 rules that effectively encode errors as valid rules. (3) TaskAdaptive Error Processing for Domain Specificity. This strategy addresses the texttable modality discrepancy by fine-tuning LLMS on synthetic noise or context-enriched datasets to capture complex, dataset-specific error patterns [58], [20]. Nonetheless, it requires significant cold start investment to curate or generate sufficiently highquality training data. (4) Hybrid LLM-ML Error Processing for Scalable Detection. This approach uses LLMS to produce pseudo-labels [60] or to derive interpretable decision structures [59] that guide lightweight, scalable ML classifiers. However, the ultimate detection performance is tightly constrained by both the fidelity of the initial LLMgenerated labels and the capabilities of the induced models. Data Imputation. For data record that contains missing entries (e.g., null values), data imputation aims to estimate these unknown values using the surrounding contextual information. Existing LLM-enhanced approaches can be grouped into three main categories. ❶ Prompt-Based End-to-End Imputation. As shown in Figure 5, this approach uses structured prompts to direct LLMS to fill in missing values in single step. Existing methods either arranges imputation prompts via heuristic formatting schemes or selectively augments prompts with relevant context. Heuristic-Structured Imputation Prompting. This category organizes imputation prompts using heuristic rules that aim to optimize the formatting of instructions for missing value imputation. For instance, CRILM [62] employs rule-based prompt design by converting feature names into natural language phrases (e.g., turning alcohol into alcohol content), retaining the observed values (e.g., 12.47), and adding domain-specific context (e.g., wine). These components are then combined into explicit natural language statements such as The alcohol content in the wine is 12.47. The resulting descriptions are supplied as prompts to LLMS, along with detailed instructions for producing descriptions for the missing values. Selective Imputation Context Prompting. This category focuses on including only the most relevant information in the imputation context, thereby reducing redundancy and token usage. For instance, LLM-PromptImp [63] refines the context by choosing the columns that are most relevant to the target missing attribute, where relevance is determined using correlation metrics (e.g., Pearson correlation, Cramers V, and η correlation) tailored to different data types. LDI [61] narrows the imputation context by first detecting columns that exhibit explicit dependency relationships with the target column, and then selecting small number of representative tuples whose values are among the top-k most similar to the incomplete tuple, measured by the normalized length of the longest common substring across these dependent columns. LLMForest [64] enables selective construction of the imputation context by converting tabular data into hierarchically merged bipartite information graphs and then retrieving neighboring nodes that are both correlated and diverse for tuples containing missing entries. ❷ Context-Retrieval Guided Imputation. This approach enables LLMS to handle previously unseen, domain-specific, Fig. 5. Example of LLM-Enhanced Data Imputation. or private datasets by dynamically enriching the input with supplemental context retrieved from external sources. For instance, RetClean [44] builds an index over data lake using both syntactic and semantic retrieval, selects pool of candidate tuples, reranks them with learned ranking model, and then presents the dirty tuple together with the top-k retrieved tuples to LLMS for imputation. Similarly, LakeFill [42] adopts two-stage retrieverreranker architecture: an initial vectorbased retriever assembles broad candidate set from the data lake, followed by reranker that filters this down to small set of highly relevant tuples that form the imputation context. ❸ Model-Optimized Adaptive Imputation. As shown in Figure 5, this approach improves imputation quality by adjusting either the LLMs training procedure or its architecture to better capture complex relationships in mixed-type tabular data. Adaptive Model Fine-Tuning Optimization. This category improves imputation by fine-tuning LLMS on task-specific datasets through parameter-efficient methods. For example, LLM-REC [65] adopts data-partitioned fine-tuning framework that divides the dataset into complete and incomplete portions. It then leverages the complete portion to partially fine-tune the LLM using LoRA, thereby enabling the model to impute missing values based on the observed data. Module-Augmented Architecture Optimization. This class of methods incorporates dedicated modules into LLMS to model structural or feature-level dependencies that standard LLMS may overlook. For instance, UnIMP [66] augments the LLM with two lightweight components that capture interactions among numerical, categorical, and textual cells: (1) high-order message-passing module that aggregates both local and global relational information, and (2) an attention-based fusion module that merges these features with prompt embeddings prior to decoding the final imputed values. Building on UnIMP, Quantum-UnIMP [67] adds quantum featureencoding module that maps mixed-type inputs into classical vectors used to parameterize an Instantaneous Quantum Polynomial (IQP) circuit. The resulting quantum embeddings serve as the initial node representations in the UnIMP hypergraph. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 10 Discussion. (1) Prompt-Based Imputation for Balanced Efficiency. This line of work leverages structured prompts and targeted context removal to reduce token consumption while mitigating class imbalance [63]. However, aggressive pruning can omit subtle cross-column relationships that are crucial for inferring missing values in complex, highdimensional tables. (2) Retrieval-Guided Imputation for Verifiable Privacy. This paradigm relies on RAG to ground imputation in external data lakes, enabling explicit source attribution and secure, on-premise deployment [44]. However, its performance is tightly constrained by the coverage and fidelity of relevant tuples in the data lake, and retrieval noise can further impair imputation accuracy. (3) ModelOptimized Imputation for Structural Complexity. This strategy incorporates tailored architectural components or incremental training schemes to model global and local dependencies in heterogeneous, mixed-type datasets [66]. Nonetheless, these specialized components introduce additional architectural complexity and higher computational costs compared to standard, general-purpose LLMS. IV. LLM FOR DATA INTEGRATION Traditional integration methods often struggle with semantic ambiguities and inconsistencies, especially in complex settings where domain-specific knowledge is unavailable [23]. Moreover, pretrained language models generally demand substantial task-specific training data and often suffer from performance degradation when dealing with out-of-distribution entities [26]. By contrast, recent work has demonstrated that LLMS exhibit strong semantic understanding, allowing them to detect relationships across datasets and integrate domain knowledge, thereby achieving robust generalization across wide range of integration tasks. Entity Matching. Entity matching aims to decide whether pair of data records corresponds to the same real-world entity. Existing LLM-enhanced approaches can be broadly grouped into three main categories. ❶ Prompt-Based End-to-End Matching. This approach relies on structured prompts to guide LLMS in performing entity matching directly. Existing methods either include explicit guidance via detailed instructions and in-context examples or organize candidate tuples into batches to enhance efficiency. Guidance-Driven In-Context Matching Prompting. This category enhances entity matching through carefully structured in-context guidance, including strategically selected demonstrations, expert-defined logical rules, and multi-step prompting pipelines. For example, MatchGPT [26] prepares guidance by selecting in-context demonstrations via various strategies (e.g., similarity-based vs. manual) and automatically generating textual matching rules from handwritten examples. ChatEL [69] further follows the guidance of multi-step pipeline to first retrieve candidates, then generate task-oriented auxiliary descriptions, and finally perform instruction-guided multiple-choice selection to identify matches. To mitigate hallucination and reliance on the LLMs internal knowledge, KcMF [43] incorporates expert-designed pseudo-code of ifthen-else logic enriched with external domain knowledge, and Fig. 6. Example of LLM-Enhanced Entity Matching. employs an ensemble voting mechanism to aggregate multisource outputs. Batch-Clustering Matching Prompting. This category enhances matching efficiency by packing multiple entities or entity pairs into single prompt, allowing LLMS to jointly reason about them. For instance, BATCHER [48] groups multiple entity pairs into one prompt via greedy, coverbased selection strategy that clusters pairs exhibiting similar matching semantics (e.g., relying on the same matching rules or patterns). Similarly, LLM-CER [68] employs list-wise prompting approach that processes batch of tuples at once, using in-context examples to cluster related entities in single pass and thereby lowering the cost associated with sequential pairwise matching. ❷ Task-Adaptive-Tuned Matching. As shown in Figure 6, this approach fine-tunes LLMS for entity matching using task-specific supervision, either by distilling reasoning traces from stronger models or by improving training data quality to enhance matching adaptability and generalization. Reasoning-Distilled Matching Tuning. This category finetunes local small LLMS using Chain-of-Thought traces distilled from larger models. For example, Jellyfish [39] performs parameter-efficient instruction tuning on small models (ranging 7B-13B) using reasoning traces (derived from CoT prompting over serialized data) distilled from larger mixtureof-experts LLM (e.g., Mixtral-8x7B) to improve reasoning consistency and task transferability. Data-Centric Matching Tuning. This category optimizes the fine-tuning process by improving the quality of training data via enriched information. For example, FTEM-LLM [71] adds clear explanations to the training data that describe why two items are the same or different (e.g., comparing specific columns). It also cleans the data by removing mislabeled examples and generating hard negatives via embedding-space neighbor selection. Similarly, LLM-CDEM [70] demonstrates that data-centric strategies (e.g., Anymatch [122] uses an AutoML-based strategy to identify and add hard examples to the training set, and uses attribute-level augmentation to inIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 11 crease the training sets granularity), which focus on improving training data quality, significantly outperform model-centric approaches in achieving robust cross-domain generalization. ❸ Multi-Model Collaborative Matching. As shown in Figure 6, this approach enhances entity matching by coordinating multiple models to exploit their complementary strengths. For instance, COMEM [72] proposes LLM collaboration in combined local and global matching strategy, where mediumsized LLM (3B-11B) ranks top-k candidates via bubble sort to mitigate position bias and context-length dependency, and stronger LLM (e.g., GPT-4o) refines these candidates by modeling inter-tuple interactions to ensure globally consistent and accurate matching. To effectively resolve long-tail entity ambiguity and maintain computational efficiency, LLMaEL [73] leverages LLMS as context augmenters to generate entity descriptions as additional input for small entity matching models. The augmented context is integrated via concatenation, fine-tuning, or ensemble methods to guide small entity matching models to produce accurate results. Discussion. (1) Prompt-Based Matching for End-to-End Resolution. This approach utilizes structured guidance (e.g., logical rules, multi-step pipelines) [26], [69], [43] and batching strategies [48], [68] to perform matching directly, facilitating explainable decisions and improved efficiency. However, reliance on the LLMS internal knowledge makes it sensitive to input phrasing and incurs significant token costs for large-scale candidate lists. (2) TaskAdaptive Matching for Robust Adaptation. This approach bridges the gap between security and generalization by fine-tuning local models [39] or prioritizing data-centric training strategies to handle unseen schemas [71], [70]. However, it faces significant cold start challenge, requiring high-quality, diverse training data to prevent overfitting or performance regression on out-of-distribution domains. (3) Multi-Model Collaborative Matching for Scalable Consistency. This approach leverages lightweight rankers for preliminary blocking [72] or context augmentation [73] to address position bias and global consistency the pipelines overall accuracy is violations. However, strictly bounded by the recall of the preliminary blocking stage, as early filtering errors cannot be recovered by the LLM. Schema Matching. The objective of schema matching is to identify correspondences between elements across different database schemas (e.g., matching column names such as employee ID and staff number). Existing LLM-enhanced approaches can be divided into five categories. ❶ Prompt-Based End-to-End Matching. This approach uses structured prompts to enable LLMS to perform schema matching without explicit code implementations. For example, LLMSchemaBench [49] designs prompts for different tasks across varying contexts and adopts prompting patterns such as persona specification (e.g., instructing LLMS to act as schema matcher), match-criteria definition, Chain-ofThought reasoning instructions, and structured output formats. GLaVLLM [74] further optimizes matching prompts by three Fig. 7. Example of LLM-Enhanced Schema Matching. strategies: (1) it improves output consistency by applying symmetric transformations to the input schemas and aggregating multiple outputs; (2) it increases matching expressiveness through structured prompting and rule decomposition, supporting complex matching patterns such as Global-and-Local-asView, where multiple source relations jointly define multiple target relations; and (3) it reduces token usage by filtering tasks based on data types and grouping similar tasks before prompting LLMS. ❷ Retrieval-Enriched Contextual Matching. As shown in Figure 7, this approach improves schema matching by augmenting LLM inputs with context obtained from external retrieval components. For instance, Matchmaker [75] integrates pretrained retrieval models (such as ColBERTv2 [123]) with LLMS by encoding columns at the token level for vector-based semantic retrieval, and then using an LLM to score and rank the retrieved candidates. KG-RAG4SM [23] extends this idea by employing multiple retrieval strategies, including vectorbased, graph traversal, and query-driven searchto extract relevant subgraphs from knowledge graphs, which are then ranked and injected into LLM prompts to provide richer context for matching. ❸ Model-Optimized Adaptive Matching. As shown in Figure 7, this approach enhances matching effectiveness through modality-aware fine-tuning, complemented by specialized module designs. For example, TableLlama [76] applies instruction tuning over wide range of table-centric tasks, allowing the model to learn alignment strategies and column semantics implicitly, without changing its core architecture. Building on this, TableGPT2 [41] adopts an architecture-augmented optimization scheme by incorporating two-dimensional table encoder that generates permutation-invariant representations, thereby enhancing the stability and accuracy of cross-table column alignment and candidate match ranking. ❹ Multi-Model Collaborative Matching. This approach improves schema matching by coordinating multiple models with complementary capabilities. For example, Magneto [50] IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 12 adopts retrieve-and-rerank framework in which small pretrained language models first produce candidate match rankings for each input column, and LLMS subsequently refine these candidates through reranking to achieve higher matching accuracy and efficiency. ❺ Agent-Guided Orchestration-Based Matching. In this paradigm, LLM agents are used to manage and coordinate the entire schema matching pipeline. Existing methods either designate distinct agents to handle and carry out specific matching subtasks or depend on agent-based planning mechanisms to orchestrate set of predefined tools. Role-Based Matching Orchestration. In this category, the workflow is partitioned into specialized agents, each responsible for different operations. For instance, Agent-OM [51] uses two LLM agents (a Retrieval Agent and Matching Agent) to coordinate the matching process, breaking tasks down via Chain-of-Thought prompting, calling specialized tools (such as syntactic, lexical, and semantic retrievers and matchers), and relying on hybrid memory architecture (relational + vector database) for storage and retrieval. Tool-Planning Matching Orchestration. This category uses LLM agents to coordinate predefined tools through dynamic planning to solve complex matching problems. For example, Harmonia [52] employs LLM agents to orchestrate and integrate set of predefined data integration tools (i.e., modular algorithms tailored to specific matching subtasks, such as top_matches for retrieving the top-k most suitable candidates), and complements them with on-demand code generation when the available tools are inadequate. At the same time, it incorporates mechanisms such as ReAct [124] for joint reasoning and action planning, interactive user feedback for correcting errors, and declarative pipeline specifications to guarantee reproducibility. Discussion. (1) Prompt-Based Matching for Stable Alignment. This paradigm employs one-to-many comparisons and symmetric transformations to promote consistency and reduce sensitivity to inputs constrained by privacy [49], [74]. However, when it relies exclusively on metadata, the model is unable to interpret semantically opaque column names, and its exhaustive verification strategy leads to prohibitive token consumption for large schemas. (2) Retrieval-Enriched Matching for Hallucination Resistance. This approach grounds the alignment in verifiable semantic subgraphs by retrieving contextual information from external knowledge graphs [23]. However, its performance can be constrained by the domain coverage of the external knowledge source and the added retrieval overhead (e.g., graph traversal). (3) Model-Optimized Matching for Structural Semantics. This approach integrates specialized architectural components (e.g., table encoders) or taskoriented fine-tuning to encode table-specific alignment regularities [76], [41]. However, it relocates the bottleneck to training data acquisition, demanding high-quality or large-scale datasets to achieve robust generalization across heterogeneous domains. (4) Multi-Model Matching for Cost-Efficient Scale. This paradigm relies on LLMS to generate training instances for lightweight scorers, forming scalable filter-then-rank pipeline [50]. However, the ultimate matching quality is tightly constrained by the fidelity of the synthetic training data and the loss of reasoning capability transferred to the smaller model. (5) Agent-Guided Matching for Autonomous Workflows. This approach leverages chain-based reasoning and selfrefinement strategies to coordinate complex, multi-stage alignment procedures [51]. HoweverNonetheless, the complex orchestration of tools and iterative reasoning cycles can introduce additional latency and maintenance overhead compared with static LLMS. V. LLM FOR DATA ENRICHMENT Existing data enrichment techniques suffer from two main drawbacks. First, they limited interactions between queries and tables [24]. Second, many such methods depend strongly on large labeled corpora, are brittle under distribution shifts, and do not generalize well to rare or highly specialized domains [27], [28]. Recent studies have shown that LLMS can mitigate these issues by producing high-quality metadata, enhancing the contextual information of datasets, and enabling natural language interfaces for performing enrichment tasks. Data Annotation. Data annotation is the process of assigning semantic or structural labels to data instances, such as identifying column types (e.g., Manufacturer or birthDate in the DBPedia ontology). Recent LLM-enhanced methods typically can be divided into five main categories. ❶ Prompt-Based End-to-End Annotation. This approach utilizes carefully crafted prompts to guide LLMS in performing diverse annotation tasks. It involves methods that supply explicit annotation guidelines and contextual information, while also leveraging reasoning and iterative self-refinement to improve annotation accuracy. Instruction-Guided Annotation Prompting. This category uses structured prompts with explicit instructions to guide LLMS in performing data annotation tasks. For example, CHORUS [53] designs prompts that combine correct annotation demonstrations, serialized data samples, metadata, domain knowledge, and output formatting guidance. Similarly, EAGLE [78] employs task-specific prompts to selectively label critical or uncertain samples (identified via prediction disagreement), combining zero-shot LLM annotation with active learning to enhance generalization in low-data settings. ArcheType [27] adopts column-at-once serialization strategy that includes only representative column samples for zero-shot column type annotation. To handle abbreviated column names, Columbo [77] defines prompt instructions over three modules: (1) summarizer module generates concise group and table summaries from context to provide annotation guidance, (2) generator module expands tokenized column names into meaningful phrases, and (3) reviser module evaluates and refines the consistency of these expanded phrases. Reasoning-Enhanced Iterative Annotation Prompting. This category enhances annotation quality by using structured prompts that guide models through step-by-step reasoning and iterative self-assessment to produce more accurate labels. For IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 example, Goby [54] applies tree-structured serialization and Chain-of-Thought prompting for enterprise column type annotation. AutoLabel [79] performs automated text annotation on representative samples (selected via DBSCAN [125] clustering and stratified sampling) using domain-optimized CoT reasoning templates that decompose complex labeling tasks into stepwise instructions (e.g., First classify entity types, then assess confidence levels), while human feedback loop iteratively validates low-confidence outputs. Anno-lexical [31] further adopts majority voting mechanism that aggregates annotations from multiple open-source LLMS to enhance annotation robustness and reduce bias. LLMCTA [28] produces and iteratively improves label definitions using promptdriven methods, such as self-refinement (progressively enhancing definitions by learning from errors) and self-correction (a two-stage process involving separate reviewer model). LLMLog [80] tackles ambiguity in log template generation via multi-round annotation, leveraging self-evaluation metrics like prediction confidence to identify uncertain or representative logs, and repeatedly updating in-context examples to refine prompt instructions and boost annotation accuracy. ❷ RAG-Assisted Contextual Annotation. This approach enriches LLM prompts to enhance annotation by retrieving relevant context, either from semantically similar examples or from external knowledge graphs. Semantic-Based Annotation Example Retrieval. This category enhances annotation accuracy by retrieving semantically relevant examples to enrich the prompt context. For instance, LLMAnno [81] addresses the inefficiency of manually selecting examples for large-scale named entity recognition (e.g., annotating 10,000 resumes) by retrieving the most relevant training examples and constructing context-enriched prompts for LLMS. Experiments show that retrieval based on appropriate embeddings (e.g., text-embedding-3-large [126]) outperforms zero-shot and in-context learning across multiple LLMS (7B-70B parameters) and datasets. Graph-Based Annotation Knowledge Retrieval. This category enhances annotation by retrieving relevant entity triples from external knowledge graphs to enrich the prompt context. For example, RACOON [55] extracts entity-related knowledge (e.g., labels and triples) from knowledge graph, converting it into concise contextual representations, and incorporating it into prompts to enhance semantic type annotation accuracy. ❸ Fine-Tuned Augmented Annotation. This approach improves annotation in specialized domains by fine-tuning LLMS on task-specific datasets. For example, PACTA [82] combines low-rank adaptation with prompt augmentation, decomposing prompts into reusable patterns and training across diverse contexts to reduce prompt sensitivity in column type annotation. OpenLLMAnno [83] demonstrates that fine-tuned local LLMS (e.g., Llama 2, FLAN-T5) outperform proprietary models like GPT-3.5 in specialized text annotation tasks, achieving substantial accuracy gains even with small number of labeled samples (e.g., 12.4% improvement with 100 samples for FLAN-T5-XL). ❹ Hybrid LLM-ML Annotation. As shown in Figure 8, this approach combines LLMS with ML models to improve annotation accuracy and robustness through knowledge distillation Fig. 8. Example of LLM-Enhanced Data Annotation. and collaborative orchestration. For instance, CanDist [84] employs distillation-based framework where LLMS uses task-specific prompts to generate multiple candidate annotations, and SLMs (e.g., RoBERTa-Base) then distill and filter them. distribution refinement mechanism updates the SLMs distribution, gradually correcting false positives and improving robustness to noisy data. AutoAnnotator [85] uses two-layer collaboration: (1) LLMS act as meta-controllers, selecting suitable SLMs, generating annotation, and verifying hard samples, while (2) SLMs perform bulk annotation, produce high-confidence labels via majority voting, and iteratively finetune on LLM-verified hard samples to enhance generalization. ❺ Tool-Assisted Agent-Based Annotation. As shown in Figure 8, this approach uses LLM agents augmented with specialized tools to handle complex annotation tasks. For example, STA Agent [86] leverages ReAct-based LLM agent for semantic table annotation, combining preprocessing (e.g., spelling correction, abbreviation expansion) with tools for column topic detection, knowledge graph enrichment, and context-aware selection, while reducing redundant outputs via Levenshtein distance. TESSA [87] employs multi-agent system for cross-domain time series annotation, integrating general and domain-specific agents with multi-modal feature extraction toolbox for intraand inter-variable analysis and reviewer module to ensure consistent and accurate annotations. Discussion. (1) Prompt-Based Annotation for Complex Reasoning. This approach uses structured prompts to capture iterative feedback [28], [53] or multi-step reasoning [54], [77], progressively refining annotation guidelines to clarify ambiguous schemas. However, the reliance on lengthy, complex instructions and repeated interactions might lead to high token consumption and latency. (2) Retrieval-Enriched Annotation for Factual Accuracy. This approach fetches context from external knowledge bases to ground annotations in verifiable information, enabling more reliable handling of specialized domains where the models internal knowledge may be obsolete [55], [81]. However, its accuracy is tightly constrained by the IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 14 reliability of the external resources and by noise from irrelevant or low-quality retrieved content. (3) Fine-Tuned Annotation for Domain Specificity. This approach adapts open-source models for specific domains (e.g., law, politics) via parameter-efficient fine-tuning, reaching high accuracy with lower deployment costs [83]. However, this merely shifts the primary bottleneck to data acquisition, since it requires extensive, high-quality instruction data to avoid overfitting. (4) Hybrid LLM-ML Annotation for Scalable Deployment. This approach trains lightweight ML models on weighted label distributions generated by LLMS, ensuring cost-effective inference [84]. However, the ML models performance is fundamentally limited by the LLMSs upper bound, and the distillation step often results in loss of the reasoning depth required for subtle edge cases. (5) Agent-Based Annotation for ToolAssisted Tasks. This approach uses autonomous agents that call external tools (e.g., search engines) for resolving hard-to-label entities [86]. However, the sequential use of multiple tools might introduce significant delays, making it impractical for real-time or high-volume annotation. Data Profiling. Data profiling involves characterizing given dataset by generating additional information (e.g., dataset descriptions, schema summaries, or hierarchical organization) or associating relevant datasets that enrich its structural and semantic understanding. Recent LLM-enhanced methods can be classified into two categories. ❶ Prompt-Based End-to-End Profiling. As shown in Figure 9, this approach uses carefully designed prompts to guide LLMS in profiling datasets, combining explicit instructions or constraints with few-shot examples and reasoning to handle complex, heterogeneous, and structured data effectively. Instruction and Constraint-Based Profiling Prompting. This category guides dataset profiling by incorporating explicit instructions or usage constraints in prompts to cover various aspects of the data. For example, AutoDDG [38] instructs LLMS to generate both user-oriented and search-optimized descriptions based on dataset content and intended usage. LEDD [56] employs prompts with task-specific instructions for data lake profiling, including summarizing clusters into hierarchical categories and refining natural language queries for semantic search. DynoClass [88] specifies instructions in the prompt to synthesize detailed table descriptions from sampled rows and existing documentation, integrating them into coherent global hierarchy. LLM-HTS [90] instructs LLMS to infer open-set semantic types for tables and columns, which are then used to build hierarchical semantic trees via embeddingbased clustering. Cocoon-Profiler [89] describes instructions at three levels: (1) table-level prompts constrain summarization using initial rows and documentation, (2) schema-level prompts guide hierarchical column grouping in JSON format, and (3) column-level prompts generate descriptions based on example rows and global context. HyperJoin [91] instructs LLMS to create semantically equivalent column name variants using table context and naming conventions, producing structured JSON outputs to construct inter-table hyperedges. Fig. 9. Example of LLM-Enhanced Data Profiling. (CoT) OCTOPUS [92] specifies strict constraints in the prompts to output only column names separated by specific delimiters and SQL sketch, enabling lightweight entity-aware profiling. Example and Reasoning-Enhanced Profiling Prompting. This category combines few-shot example prompts with Chain-of-Thought reasoning to support structured profiling of complex and heterogeneous data. For instance, LLMCodeProfiling [93] uses two-stage, prompt-based framework for cross-language code profiling. In the syntactic abstraction stage, few-shot CoT prompts demonstrate how abstract syntax tree (AST) nodes from different languages can be converted into unified tabular representation, guiding the LLM to infer deterministic mappings that align languagespecific constructs to common schema. In the semantic assignment stage, instructional classification prompts direct the LLM to assign imported packages to functional categories (e.g., labeling scikit-learn as machine learning). ❷ RAG-Assisted Contextual Profiling. As shown in Figure 9, this approach combines multiple retrieval techniques with LLM reasoning to improve profiling accuracy and consistency, especially when metadata is sparse or incomplete. For example, LLMDap [94] employs vector search to gather relevant textual evidence, including scientific articles, documentation, and metadata fragments, to generate semantically consistent dataset-level profiles (e.g., dataset descriptions, variable definitions, and structured metadata). Pneuma [25] integrates hybrid retrieval methods, such as full-text and vector search, to identify relevant tables from databases or data lakes, using LLMS to generate semantic column descriptions and to refine and rerank the retrieved results. Discussion. (1) Prompt-Based Profiling for Descriptive Summarization. This approach integrates structural and statistical metadata into prompts to generate faithful dataset descriptions, overcoming context window limits [38]. However, relying solely on summary statistics is lossy compression, potentially causing the model to miss finegrained semantic anomalies hidden in the raw data. (2) IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 15 TABLE II SUMMARY OF REPRESENTATIVE DATA PREPARATION DATASETS. Category Data Cleaning Data Integration Data Enrichment Dataset Chicago Food Inspection [127], [19] Paycheck Protection Program [128], [19] Enron Emails [129], [21] SWDE Movie / University / NBA [130], [21] Hospital [106], [57], [37], [19], [20], [59], [60], [22] Flights [131], [36], [19], [37], [20], [59], [60], [22] Beers [132], [37], [20], [59], [60], [22] Meat Consumption [133], [47] Hotel Booking [134], [47] Adult Income [135], [57], [63] Travel datasets [136], [63] Buy [137], [138], [57], [66] Restaurant [139], [138], [57], [66] Walmart [140], [138], [66] abt-buy [137], [39], [70], [72], [26], [48] Amazon-Google [137], [57], [39], [70], [72], [26], [48] Walmart-Amazon [140], [57], [39], [70], [72], [26], [48] DBLP-Scholar / ACM [137], [57], [39], [70], [72], [26], [48] WDC Products [141], [26], [70] OMOP [142], [23] Synthea [143], [57], [23], [43], [75], [74] MIMIC [144], [23], [43], [75], [74] GDC-SM [145], [50] ChEMBL-SM [146], [50] NQ-Tables [147] / OpenWikiTable [148], [24] DBpedia Ontology Dataset [149], [84] AGNews [150], [84] CoNLL-2003 [151], [81] WNUT-17 [152], [81] ChEMBL-DP [146], [25], [92] Adventure Works [153], [25], [92] Public BI Benchmark [154], [25], [92] Chicago Open Data [127], [25], [92] FetaQA [155], [25], [92] Task DS DS DS DS DS, DEP DS, DEP DEP DEP DEP DEP, DI DI DI DI DI EM EM EM EM EM SM SM SM SM SM DA DA DA DA DA DP DP DP DP DP Modality Tabular Tabular Text Other Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Tabular Text Text Text Text Tabular Tabular Tabular Tabular Tabular Granularity column column tuple document / page cell / tuple / column cell / tuple / column cell / tuple / column table table cell / tuple / tuple tuple cell cell cell tuple pair tuple pair tuple pair tuple pair tuple pair attribute-pair attribute-pair attribute-pair attribute-pair attribute pair table document document document document table table table table table Data Volume (Unit) 298,345 (rows) 661,218 (rows) 517,401 (emails) 20,000 / 16,705 / 4,405 (pages) 1,000 (rows) 2,377 (rows) 2,410 (rows) 12,140 (rows) 119,390 (rows) 48,842 (rows) 954 (rows) 651 (rows) 864 (rows) 4,654 (rows) 1,097 (pairs) 1300 (pairs) 1154 (pairs) 5347 / 2224 (pairs) 40,500 (pairs) 37 (tables), 394 (attributes) 12 (tables), 111 (attributes) 25 (tables), 240 (attributes) 20 (tables) 8 (datasets) 952 / 6,602 (tables) 70,000 (documents) 7,600 (documents) 386 (documents) 1,287 (documents) 78 (tables) 88 (tables) 203 (tables) 802 (tables) 10,330 (tables) Evaluation Metric Precision, Recall, F1-score Precision, Recall, F1-score F1-score F1-score Precision, Recall, F1-score Precision, Recall, F1-score, Matching Rate Precision, Recall, F1-score F1-score F1-score Precision, Recall, F1-score, Accuracy, ROC-AUC Precision, Recall, F1-score, Accuracy, ROC-AUC Accuracy, ROUGE-1, Cos-Sim Accuracy, ROUGE-1, Cos-Sim ROUGE-1, Cos-Sim F1-score F1-score F1-score F1-score F1-score Precision, Recall, F1-score, Accuracy Precision, Recall, F1-score, Accuracy Precision, Recall, F1-score, Accuracy MRR, Recall@GT MRR, Recall@GT P@k Accuracy, 1-α, F1-score Accuracy, 1-α, F1-score F1-score F1-score Precision, Recall, F1-score, Hit Rate Precision, Recall, F1-score, Hit Rate Precision, Recall, F1-score, Hit Rate Precision, Recall, F1-score, Hit Rate Precision, Recall, F1-score, Hit Rate Abbreviations: DS Data Standardization; DEP Data Error Processing; DI Data Imputation; EM Entity Matching; SM Schema Matching; DP Data Profiling; DA Data Annotation. Iterative Profiling for Hierarchical Structure. This approach utilizes LLM-driven clustering and summarization to build hierarchical views of data lakes, enabling semantic search across disparate tables [56]. However, the iterative abstraction process risks accumulating information loss, resulting in vague or generic descriptions at higher levels of the hierarchy. (3) Hybrid Profiling for Quality Assurance. This approach augments statistical profiling with LLM-driven reasoning and human verification to identify complex structural anomalies and disguised missing values [37]. However, the reliance on humanin-the-loop intervention creates scalability bottleneck, making it unsuitable for fully automated, real-time data pipelines. (4) Retrieval-Enriched Profiling for Contextual Grounding. This approach retrieves external context (e.g., similar tables or text) to ground the generation of evidencebased schema descriptions [25], [94]. However, the final profiling accuracy is strictly bounded by the relevance of the retrieved corpus, where noisy or outdated external context can induce hallucinations. VI. EVALUATION A. DATA PREPARATION Datasets To support systematic evaluation of LLM-enhanced data preparation, we summarize representative datasets in Table II, providing detailed information across multiple dimensions, including category, task, modality, granularity, data volume, and evaluation metrics. It allows researchers to compare and select benchmarks tailored to their specific use cases. For instance, we present granularity-driven perspective below that groups benchmarks by their fundamental processing unit (i.e., records, schemas, or entire objects). (1) Record-Level. This category treats individual tuples, cells, or tuple pairs as the analysis unit. It covers most data cleaning, error processing, data imputation, and entity matching tasks, including detecting erroneous values, standardizing attributes, imputing missing cells, and identifying coreference across records. Representative tuple-level benchmarks include Adult Income [135], Hospital [106], Beers [132], Flights [131], and text-based datasets such as Enron Emails [129]. Columnlevel benchmarks include the Paycheck Protection Program [128] and Chicago Food Inspection [127]. Cell-level benchmarks include Buy [137], Restaurant [139], and Walmart [140]. Conversely, tuple-pair benchmarks, including abtbuy [137], AmazonGoogle [137], WalmartAmazon [140], DBLPScholar [137], DBLPACM [137], and WDC Products [141], focus on pairwise comparisons across heterogeneous sources for record-level alignment. (2) Schema-Level. This category focuses on attribute pairs or schema elements, aiming to align columns and conceptual entities across heterogeneous schemas. The challenge shifts from validating individual values to matching semantic meanings and structural roles. Benchmarks such as OMOP [142], Synthea [143], and MIMIC [144] focus on clinical attribute alignment. Moreover, datasets such as GDC-SM [145] and ChEMBL-SM [146] evaluate cross-source attribute alignment within complex scientific and biomedical schemas. (3) Object-Level. This category deals with entire tables or documents as the fundamental processing unit. Unlike recordor schema-level tasks, these benchmarks require reasoning over global structure and broader context. Tablelevel datasets supporting data profiling and annotation include Public BI [154], Adventure Works [153], ChEMBLDP [146], Chicago Open Data [127], NQ-Tables [147], and FetaQA [155]. Document-level benchmarks, such as AGNews [150], DBpedia [149], CoNLL-2003 [151], and WNUT17 [152], require combining evidence across full texts for semantic grounding and annotation. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 16 B. DATA PREPARATION Metrics In real deployments, data preparation methods are evaluated across multiple dimensions. Therefore, we organize evaluation metrics in Table II by the aspects they measure, including correctness, robustness, ranking quality, and semantic consistency, rather than only by the tasks. ❶ Preparation Correctness Assessment. This category evaluates the correctness of preparation methods by measuring how accurately they process target data elements relative to ground-truth references. Operation Precision. These metrics quantify the reliability of predictions from preparation methods. For example, (1) Accuracy [156] measures the proportion of correctly classified elements across relevant and irrelevant elements, commonly used in classification tasks such as error identification in data error processing. (2) Precision [156] measures the fraction of correctly identified matches or errors among all elements flagged by the method, reflecting output reliability in tasks like entity or schema matching. (3) F1-score [156] extends precision to penalize both incorrect identifications and missed detections within single measure, making it suitable for applications where both erroneous outputs and overlooked cases are significant. Operation Coverage. These metrics reflect whether preparation methods comprehensively address all required elements. For example, (1) Recall [156] measures the proportion of correctly identified matches or errors among all groundtruth elements, reflecting methods ability to avoid missed detections in tasks such as entity matching. (2) Matching Rate [137] quantifies the proportion of target elements that are successfully aligned to valid representation, commonly used in tasks such as entity matching. ❷ Preparation Robustness Assessment. This category evaluates the stability and reliability of preparation methods over diverse datasets. These metrics measure how consistently method maintains its effectiveness across varying data distributions and structural complexity. For example, (1) ROC [157] characterizes the trade-off between correctly identifying target elements (e.g., valid matches) and incorrectly flagging nontarget elements as the decision threshold varies, providing global view of method behavior in tasks such as data error processing. (2) AUC [157] summarizes this behavior into single measure that reflects methods ability to distinguish relevant from irrelevant elements across all thresholds and is commonly used in tasks such as data error processing. ❸ Enrichment and Ranking Quality Assessment. This category evaluates the quality of preparation methods by measuring how effectively they retrieve and prioritize relevant information over ground-truth results. Retrieval Ranking Quality. These metrics assess the relevance of top-ranked candidates in retrieval-based preparation tasks. For example, (1) P@k [158] measures the fraction of queries where correct result is found within the topk elements, reflecting retrieval utility in data profiling. (2) MRR [158] measures the average rank position of the first correct result across queries, indicating how quickly relevant elements are placed at the top of the list. Enrichment Completeness. These metrics measure how comprehensively preparation methods find all relevant information during data enrichment. For example, (1) Recall@GT [158] measures the fraction of correctly identified elements among the top-k results, where is the total number of true elements, assessing coverage in tasks such as entity or schema matching. (2) 1 α [159] measures the fraction of data elements for which the correct label is present in the set of candidates, evaluating label coverage in tasks such as data annotation. (3) Hit Rate [158] measures the fraction of search queries that return at least one correct result, evaluating basic retrieval success in tasks such as data annotation. ❹ Semantic Preservation Assessment. This category evaluates the ability of preparation methods to preserve semantic meaning in the generated outputs. These metrics measure how consistently method maintains semantics between its outputs and the reference content. For example, (1) ROUGE [160] assesses semantic consistency at the lexical level by measuring n-gram overlap between the output and the reference text, commonly used to evaluate whether the outputs retain key terms in tasks such as data standardization. (2) Cosine Similarity [161] measures semantic alignment in an embedding space by comparing vector representations of the generated and reference texts with continuous measure in tasks such as data profiling. VII. CHALLENGES AND FUTURE DIRECTIONS A. Data Cleaning ❶ Global-Aware and Semantically Flexible Cleaning. Most existing prompt-based cleaning methods operate on restrictive local contexts, such as individual rows or small batches [57], [37]. While retrieval-augmented methods expand this scope by fetching external evidence [44], [42], they remain centered on instance-level context and cannot capture dataset-level properties (e.g., uniqueness constraints or aggregate correlations) essential for issues requiring holistic views. Future work should explore hybrid systems that integrate LLMS with external analysis engines capable of providing global statistics and constraints, enabling joint reasoning over local instances and dataset-level signals while preserving the semantic flexibility. ❷ Robust and Error-Controlled Cleaning. Agent-based data cleaning mimics human-style workflows and can improve cleaning coverage [36], [19], but current systems lack effective safeguards against error accumulation and hallucinated cleaning. Although recent general-purpose frameworks introduce uncertainty estimation [162] and self-correction strategies [163] to improve agent reliability, these techniques are mostly heuristic and cannot be directly applied to data cleaning tasks that require strict correctness guarantees. An important open direction is to design uncertainty-aware agent-based cleaning frameworks that use conservative decision strategies, formal validation mechanisms, and explicit risk control, allowing systems to balance cleaning coverage with measurable error risk and move toward provably robust cleaning pipelines. ❹ Efficient and Scalable Collaborative Cleaning. Promptbased data cleaning methods struggle to scale to large tables limits [47], [60], while agent-based workdue to context flows often incur high computational cost and latency [36]. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 17 Although smaller, locally deployable models and federated learning frameworks enable privacy-preserving cleaning deployments [164], existing systems lack principled strategies for coordinating models with different capabilities. An important future direction is to design hierarchical cleaning frameworks that assign routine cleaning tasks to small local models and reserve LLMS for complex reasoning, combined with efficient table partitioning and selective context management to reduce cost and latency without sacrificing cleaning quality. B. Data Integration ❶ Universal and Cross-Domain Integration. Recent structure-aware matching methods [165] and cross-dataset integration studies [70] have shown encouraging results, but they generally assume the presence of reasonably informative schemas. In practice, many integration scenarios involve extreme heterogeneity, including unclear or abbreviated attribute names, substantial structural mismatches (e.g., nested data mapped to flat tables), and datasets with little or no usable metadata. These conditions remain difficult for current methods to handle reliably. An key future direction is to develop techniques that rely less on schema descriptions and prompts, and instead infer semantic correspondences directly from data instances (e.g., value distributions and cooccurrence patterns), enabling robust integration even when schema information is missing or misleading. ❷ Universal Integration in Diverse Realistic Datasets. Despite recent progress, LLM-enhanced integration methods often require curated examples [26], [69] or domain-specific fine-tuning [39], [71] to achieve high performance. Although zero-shot cross-domain integration has received increasing attention [166], it remains limited in realistic integration with varying schema design, value formats, or domain-specific semantics. Thus, building single matcher that can reliably transfer integration behaviors across diverse datasets remains major challenge. We should explore research in meta-learning and synthetic data generation to create universal integration models that generalize to new domains without requiring expensive, domain-specific training data. ❸ Rule-Constrained and Globally Valid Integration. Recent in-context clustering methods [48] for entity matching can efficiently enforce simple global properties, such as transitivity, during matching [68], [43]. In practice, however, data integration often requires satisfying more complex and domainspecific constraints, including multi-entity relationships, temporal ordering, and business rules. These constraints are difficult to express and enforce using prompt-based approaches. future direction is to augment LLM-based An important integration pipelines with explicit reasoning components, such as constraint solvers and graph-based inference modules, that can be invoked by LLM agents to ensure that integration results respect complex, domain-specific constraints. C. Data Enrichment ❶ Interactive Human-in-the-Loop Enrichment. Fully auis often impractical, especially tomated data enrichment when enrichment decisions are ambiguous or domain dependent [38], [94]. In practice, effective workflows require close collaboration between human experts and LLM-enhanced systems. However, most existing methods are designed for oneshot automation and provide limited support for interactive refinement, where users can guide decisions, verify results, and correct errors during the enrichment process. We need to develop novel interactive frameworks where LLMS can explain their reasoning, solicit feedback on ambiguous cases, and incrementally refine enrichment tasks based on human guidance, treating the user as core component of the system. ❷ Multi-Aspect and Open-Ended Enrichment. Evaluating LLM-enhanced data enrichment remains challenging in two aspects. First, enrichment often involves multiple aspects, such as annotating column types [53], [28], expanding textual descriptions [38], [56], which are difficult to assess with single task-level metric. Second, many enrichment outputs are free-form text, where quality cannot be judged using simple binary or precision-based measures. As result, existing benchmark is largely designed for structured or closed-form tasks and fail to reflect the quality and usefulness of realworld enrichment results. key future direction is to develop standardized enrichment benchmarks that support multi-aspect evaluation and richer assessment criteria, combining automatic metrics with reference-based, model-based, or humanin-the-loop evaluation to better capture enrichment quality, usefulness, and cost in realistic scenarios. ❸ Faithful and Evidence-Grounded Enrichment. Generative data enrichment using LLMS can produce fluent but unsupported outputs, such as inferred constraints, textual summaries, or data profiles, particularly when the input data is noisy or incomplete [54], [92]. Although retrieval-augmented generation provides useful grounding mechanisms [81], [55], [25], existing approaches are primarily designed for structured tables and do not directly meet the needs of unstructured data enrichment. As result, enriched content often lacks clear links to the data or knowledge sources that justify it. An important future direction is to design faithfulness-aware enrichment methods in which every generated output is explicitly grounded in verifiable evidence, such as supporting data samples, query execution results, or cited external knowledge, so that enriched information is both useful and trustworthy. VIII. CONCLUSION In this survey, we present task-centric review of recent advances in LLM-enhanced data preparation, covering data cleaning, data integration, and data enrichment. We systematically analyze how LLMS reshape traditional data preparation workflows by enabling capabilities such as instruction-driven automation, semantic-aware reasoning, cross-domain generalization, and knowledge-augmented processing. Through unified taxonomy, we organize representative methods, distill their design principles, and discuss the limitations of existing LLM-enhanced methods. We also summarize representative datasets and metrics to facilitate comprehensive evaluations of these methods. Finally, we identify open challenges and outline future research directions. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY"
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. S. S. Neeli, Ensuring data quality: critical aspect of business intelligence success, International Journal of Leading Research Publication, vol. 2, no. 9, pp. 17, 2021. [2] R. Ali and D. Darmawan, Big data management optimization for managerial decision making and business strategy, Journal of Social Science Studies, vol. 3, no. 2, p. 139144, Jul. 2023. [3] A. N. Prasad, Introduction to Data Governance for Machine Learning Systems: Fundamental Principles, Critical Practices, and Future Trends, 1st ed. Berkeley, CA: Apress, 2024. [4] J. Hausenloy, D. McClements, and M. Thakur, Towards data governance of frontier AI models, CoRR, vol. abs/2412.03824, 2024. [5] J. Guan, Data Sharing Governance and Management Framework. Singapore: Springer Nature Singapore, 2026, pp. 3768. [6] H. A. Davis et al., Establishing data governance for sharing and access to real-world data: case study, JAMIA Open, vol. 8, no. 3, p. ooaf041, 06 2025. [7] R. Chen, The hidden cost of poor data quality & governance: Adm turns risk into revenue, 2025, online. Acceldata Blog. Accessed: 202601-05. [8] X. Zhou et al., survey of LLM DATA, CoRR, vol. abs/2505.18458, 2025. [9] Y. Zhu et al., survey of data agents: Emerging paradigm or overstated hype? CoRR, vol. abs/2510.23587, 2025. [10] Statista, Worldwide data created, captured, copied, and consumed, 2025, online. Statista. Accessed: 2026-01-05. [11] L. Ma et al., Llms with user-defined prompts as generic data operators IEEE, 2023, pp. for reliable data processing, in IEEE Big Data. 31443148. [12] OpenRefine Community, Openrefine: power tool for working with messy data, https://openrefine.org, accessed: 2025-11-05. [13] Y. Li, J. Li, Y. Suhara, A. Doan, and W. Tan, Deep entity matching with pre-trained language models, Proc. VLDB Endow., vol. 14, no. 1, pp. 5060, 2020. [14] X. Deng, H. Sun, A. Lees, Y. Wu, and C. Yu, TURL: table understanding through representation learning, Proc. VLDB Endow., vol. 14, no. 3, pp. 307319, 2020. [15] P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, Conditional functional dependencies for data cleaning, in ICDE. IEEE Computer Society, 2007, pp. 746755. [16] M. Dallachiesa et al., NADEEF: commodity data cleaning system, in SIGMOD Conference. ACM, 2013, pp. 541552. [17] M. Ebraheem, S. Thirumuruganathan, S. R. Joty, M. Ouzzani, and N. Tang, Distributed representations of tuples for entity resolution, Proc. VLDB Endow., vol. 11, no. 11, pp. 14541467, 2018. [18] S. Thirumuruganathan et al., Deep learning for blocking in entity matching: design space exploration, Proc. VLDB Endow., vol. 14, no. 11, pp. 24592472, 2021. [19] L. Li, L. Fang, and V. I. Torvik, Autodcworkflow: Llm-based data cleaning workflow auto-generation and benchmark, CoRR, vol. abs/2412.06724, 2024. [20] M. Yan, Y. Wang, Y. Wang, X. Miao, and J. Li, GIDCL: graphenhanced interpretable data cleaning framework with large language models, Proc. ACM Manag. Data, vol. 2, no. 6, pp. 236:1236:29, 2024. [21] S. Arora et al., Language models enable simple systems for generating structured views of heterogeneous data lakes, Proc. VLDB Endow., vol. 17, no. 2, pp. 92105, 2023. [22] W. Ni, K. Zhang, X. Miao, X. Zhao, Y. Wu, and J. Yin, Iterclean: An iterative data cleaning framework with large language models, in ACM TUR-C, 2024. [23] C. Ma, S. Chakrabarti, A. Khan, and B. Molnar, Knowledge graphbased retrieval-augmented generation for schema matching, CoRR, vol. abs/2501.08686, 2025. [24] Y. Guo, Z. Hu, Y. Mao, B. Zheng, Y. Gao, and M. Zhou, BIRDIE: language-driven table discovery using differentiable search natural index, Proc. VLDB Endow., vol. 18, no. 7, pp. 20702083, 2025. [25] M. I. L. Balaka, D. Alexander, Q. Wang, Y. Gong, A. Krisnadhi, and R. C. Fernandez, Pneuma: Leveraging llms for tabular data in an end-to-end system, Proc. ACM representation and retrieval Manag. Data, vol. 3, no. 3, pp. 200:1200:28, 2025. [26] R. Peeters, A. Steiner, and C. Bizer, Entity matching using large language models, in EDBT, 2025, pp. 529541. [27] B. Feuer, Y. Liu, C. Hegde, and J. Freire, Archetype: novel framework for open-source column type annotation using large language models, Proc. VLDB Endow., vol. 17, no. 9, pp. 22792292, 2024. [28] K. Korini and C. Bizer, Evaluating knowledge generation and selfrefinement strategies for llm-based column type annotation, in ADBIS, ser. Lecture Notes in Computer Science, vol. 16043. Springer, 2025, pp. 111127. [29] M. Mondal, J. Audiffren, L. Dolamic, G. Bovet, and P. Cudre-Mauroux, Cleaning semi-structured errors in open data using large language models, in SDS. IEEE, 2024, pp. 258261. [30] J. Tu et al., Unicorn: unified multi-tasking model for supporting matching tasks in data integration, Proc. ACM Manag. Data, vol. 1, no. 1, pp. 84:184:26, 2023. [31] T. Horych et al., The promises and pitfalls of LLM annotations in dataset labeling: case study on media bias detection, in NAACL (Findings), 2025, pp. 13701386. [32] S. Wang et al., Large language models for data science: survey, https://openreview.net/forum?id=PiBQUGagoi, 06 2025, under review at ACL Rolling Review. [33] M. Chen et al., Empowering tabular data preparation with language models: Why and how? CoRR, vol. abs/2508.01556, 2025. [34] M. Sood and V. Venkatraman, Is your enterprise data strategy ready for the age of intelligence? Sponsored Content, Harvard Business Review, Sep. 2025. [35] A. Dogra, V. Kolovski, and S. Murching, Introducing new governance capabilities to scale ai agents with confidence: Unified governance across models, tools, and data, 2025, online. Databricks Blog. Accessed: 2026-01-05. [36] D. Qi and J. Wang, Cleanagent: Automating data standardization with llm-based agents, CoRR, vol. abs/2403.08291, 2024. [37] S. Zhang, Z. Huang, and E. Wu, Data cleaning using large language models, in ICDEW. IEEE, 2025, pp. 2832. [38] H. Zhang, Y. Liu, W. Hung, A. S. R. Santos, and J. Freire, Autoddg: Automated dataset description generation using large language models, CoRR, vol. abs/2502.01050, 2025. [39] H. Zhang, Y. Dong, C. Xiao, and M. Oyamada, Jellyfish: large language model for data preprocessing, CoRR, vol. abs/2312.01678, 2023. [40] F. Biester, M. Abdelaal, and D. D. Gaudio, Llmclean: Context-aware tabular data cleaning via llm-generated ofds, in ADBIS, vol. 2186. Springer, 2024, pp. 6878. [41] A. Su et al., Tablegpt2: large multimodal model with tabular data integration, CoRR, vol. abs/2411.02059, 2024. [42] C. Yang, Y. Luo, C. Cui, J. Fan, C. Chai, and N. Tang, Data imputation with limited data redundancy using data lakes, Proc. VLDB Endow., vol. 18, no. 10, pp. 33543367, 2025. [43] Y. Xu, H. Li, K. Chen, and L. Shou, Kcmf: knowledge-compliant framework for schema and entity matching with fine-tuning-free llms, CoRR, vol. abs/2410.12480, 2024. [44] M. Y. Eltabakh, Z. A. Naeem, M. S. Ahmad, M. Ouzzani, and N. Tang, Retclean: Retrieval-based tabular data cleaning using llms and data lakes, Proc. VLDB Endow., vol. 17, no. 12, pp. 44214424, 2024. [45] M. Hameed and F. Naumann, Data preparation: survey of commercial tools, SIGMOD Rec., vol. 49, no. 3, pp. 1829, 2020. [46] J. Choi, J. Yun, K. Jin, and Y. Kim, Multi-news+: Cost-efficient dataset cleansing via llm-based data annotation, in EMNLP, 2024, pp. 1529. [47] T. Bendinelli, A. Dox, and C. Holz, Exploring LLM agents for cleaning tabular machine learning datasets, CoRR, vol. abs/2503.06664, 2025. design space exploration, in ICDE. [48] M. Fan et al., Cost-effective in-context learning for entity resolution: IEEE, 2024, pp. 36963709. [49] M. Parciak, B. Vandevoort, F. Neven, L. M. Peeters, and S. Vansummeren, Schema matching with large language models: an experimental study, in VLDB Workshops, 2024. [50] Y. Liu, E. Pena, A. S. R. Santos, E. Wu, and J. Freire, Magneto: Combining small and large language models for schema matching, Proc. VLDB Endow., vol. 18, no. 8, pp. 26812694, 2025. [51] Z. Qiang, W. Wang, and K. Taylor, Agent-om: Leveraging LLM agents for ontology matching, Proc. VLDB Endow., vol. 18, no. 3, pp. 516 529, 2024. [52] A. S. R. Santos, E. H. M. Pena, R. Lopez, and J. Freire, Interactive data harmonization with LLM agents, CoRR, vol. abs/2502.07132, 2025. [53] M. Kayali, A. Lykov, I. Fountalis, N. Vasiloglou, D. Olteanu, and D. Suciu, CHORUS: foundation models for unified data discovery and exploration, Proc. VLDB Endow., vol. 17, no. 8, pp. 21042114, 2024. [54] M. Kayali, F. Wenz, N. Tatbul, and . Demiralp, Mind the data gap: Bridging llms to enterprise data integration, CoRR, vol. abs/2412.20331, 2024. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 19 [55] L. Wei, G. Xiao, and M. Balazinska, RACOON: an llm-based framework for retrieval-augmented column type annotation with knowledge graph, CoRR, vol. abs/2409.14556, 2024. [56] Q. An, C. Ying, Y. Zhu, Y. Xu, M. Zhang, and J. Wang, LEDD: large language model-empowered data discovery in data lakes, CoRR, vol. abs/2502.15182, 2025. [57] H. Zhang, Y. Dong, C. Xiao, and M. Oyamada, Large language models as data preprocessors, in VLDB Workshops, 2024. [58] A. Li et al., Anomaly detection of tabular data using llms, CoRR, vol. abs/2406.16308, 2024. [59] M. Wang et al., Ensembling llm-induced decision trees for explainable and robust error detection, arXiv preprint arXiv:2512.07246, 2025. [60] W. Ni et al., Zeroed: Hybrid zero-shot error detection through large language model reasoning, in ICDE. IEEE, 2025, pp. 31263139. [61] S. Omidvartehrani and D. Rafiei, Ldi: Localized data imputation for text-rich tables, arXiv preprint arXiv:2506.16616, 2025. [62] A. Hayat and M. R. Hasan, context-aware approach for enhancing data imputation with pre-trained language models, in COLING, 2025, pp. 56685685. [63] S. Srinivasan and L. Manikonda, Does prompt design impact quality of data imputation by llms? CoRR, vol. abs/2506.04172, 2025. [64] X. He, Y. Ban, J. Zou, T. Wei, C. B. Cook, and J. He, LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation, in ACL (Findings), 2025, pp. 69216936. [65] Z. Ding, J. Tian, Z. Wang, J. Zhao, and S. Li, Data imputation using large language model to accelerate recommendation system, arXiv preprint arXiv:2407.10078, 2024. [66] J. Wang, K. Wang, Y. Zhang, W. Zhang, X. Xu, and X. Lin, On llm-enhanced mixed-type data imputation with high-order message passing, Proc. VLDB Endow., vol. 18, no. 10, pp. 34213434, 2025. [67] H. Jamali, Quantum-accelerated neural imputation with large language models (llms), CoRR, vol. abs/2507.08255, 2025. [68] J. Fu, H. Tang, A. Khan, S. Mehrotra, X. Ke, and Y. Gao, In-context clustering-based entity resolution with large language models: design space exploration, Proc. ACM Manag. Data, vol. 3, no. 4, pp. 252:1 252:28, 2025. [69] Y. Ding, Q. Zeng, and T. Weninger, Chatel: Entity linking with chatbots, in LREC/COLING, 2024, pp. 30863097. [70] Z. Zhang, P. Groth, I. Calixto, and S. Schelter, deep dive into cross-dataset entity matching with large and small language models, in EDBT. OpenProceedings.org, 2025, pp. 922934. [71] Q. Ruan, D. Shi, and T. Bauernhansl, Fine-tuning large language models with contrastive margin ranking loss for selective entity matching in product data integration, Adv. Eng. Informatics, vol. 67, p. 103538, 2025. [72] T. Wang et al., Match, compare, or select? an investigation of large language models for entity matching, in COLING, 2025, pp. 96109. [73] A. Xin et al., LLMAEL: large language models are good context augmenters for entity linking, in CIKM. ACM, 2025, pp. 35503559. [74] C. Buss, M. Safari, A. Termehchy, S. Lee, and D. Maier, Towards scalable schema mapping using large language models, CoRR, vol. abs/2505.24716, 2025. [75] N. Seedat and M. van der Schaar, Matchmaker: Self-improving large language model programs for schema matching, CoRR, vol. abs/2410.24105, 2024. [76] T. Zhang, X. Yue, Y. Li, and H. Sun, Tablellama: Towards open large generalist models for tables, in NAACL-HLT. Association for Computational Linguistics, 2024, pp. 60246044. [77] T. Cai, S. Sheen, and A. Doan, Columbo: Expanding abbreviated column names for tabular data using large language models, CoRR, vol. abs/2508.09403, 2025. [78] P. Bansal and A. Sharma, Large language models as annotators: Enhancing generalization of NLP models at minimal cost, CoRR, vol. abs/2306.15766, 2023. [79] X. Ming, S. Li, M. Li, L. He, and Q. Wang, Autolabel: Automated textual data annotation method based on active learning and large language model, in KSEM (4), vol. 14887. Springer, 2024, pp. 400 411. [80] F. Teng, H. Li, and L. Chen, Llmlog: Advanced log template generation via llm-driven multi-round annotation, Proc. VLDB Endow., vol. 18, no. 9, pp. 31343148, 2025. [81] M. Uzair-Ul-Haq, D. Rigoni, and A. Sperduti, Llms as data annotators: How close are we to human performance, CoRR, vol. abs/2504.15022, 2025. [82] H. Meng, J. Cao, and R. Pottinger, Robust llm-based column type annotation via prompt augmentation with lora tuning, arXiv preprint arXiv:2512.22742, 2025. [83] M. Alizadeh et al., Open-source llms for text annotation: practical guide for model setting and fine-tuning, J. Comput. Soc. Sci., vol. 8, no. 1, p. 17, 2025. [84] M. Xia et al., Prompt candidates, then distill: teacher-student framework for llm-driven data annotation, in ACL (1), 2025, pp. 2750 2770. [85] Y. Lu, Z. Ji, J. Du, S. Yu, Q. Xuan, and T. Zhou, From llm-anation to llm-orchestrator: Coordinating small models for data labeling, CoRR, vol. abs/2506.16393, 2025. [86] Y. Geng et al., An LLM agent-based complex semantic table annotation approach, in ADMA (2), vol. 16198. Springer, 2025, pp. 223238. [87] M. Lin et al., Decoding time series with llms: multi-agent framework for cross-domain annotation, CoRR, vol. abs/2410.17462, 2024. [88] H. Wang, E. Wu, K. Liu, and J. Liu, Dynoclass: dynamic tableclass detection system without the need for predefined ontologies, in TRL @ NeurIPS 2024, 2024. [89] Z. Huang and E. Wu, Cocoon: Semantic table profiling using large language models, in HILDA@SIGMOD. ACM, 2024, pp. 17. [90] G. Fan and J. Freire, Hierarchical table semantics for exploratory table discovery, in HILDA@SIGMOD. ACM, 2025, pp. 5:15:7. [91] S. Liu, J. Wang, X. Lin, L. Qin, W. Zhang, and Y. Zhang, Hyperjoin: Llm-augmented hypergraph link prediction for joinable table discovery, arXiv preprint arXiv:2601.01015, 2026. [92] W.-Z. Li and S. Galhotra, Octopus: lightweight entity-aware system for multi-table data discovery and cell-level retrieval, arXiv preprint arXiv:2601.02304, 2026. [93] P. Thorat et al., Llm-aided customizable profiling of code data based on programming language concepts, CoRR, vol. abs/2503.15571, 2025. [94] S. Jiang, S. Sørbø, P. Tinn, S. F. Karim, and D. Roman, Llmdap: Llm-based data profiling and sharing, in VLDB 2025 Workshop: 3rd Data EConomy Workshop (DEC), 2025. [95] Z. Tan et al., Large language models for data annotation and synthesis: survey, in EMNLP, 2024, pp. 930957. [96] B. Ding et al., Data augmentation using llms: Data perspectives, learning paradigms and challenges, in ACL (Findings), 2024, pp. 16791705. [97] M. Cheng et al., survey on table mining with large language models: Challenges, advancements and prospects, Authorea Preprints, 2025. [98] Z. Qin et al., The synergy between data and multi-modal large language models: survey from co-development perspective, IEEE Trans. Pattern Anal. Mach. Intell., vol. 47, no. 10, pp. 84158434, 2025. [99] M. Nadas, L. Diosan, and A. Tomescu, Synthetic data generation using large language models: Advances in text and code, IEEE Access, vol. 13, pp. 134 615134 633, 2025. [100] R. Shi, Y. Wang, M. Du, X. Shen, and X. Wang, comprehensive survey of synthetic tabular data generation, CoRR, vol. abs/2504.16506, 2025. [101] J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim, survey on large language models for code generation, CoRR, vol. abs/2406.00515, 2024. [102] L. Long et al., On llms-driven synthetic data generation, curation, and evaluation: survey, in ACL (Findings), 2024, pp. 11 06511 082. [103] M. H. Moslemi, A. Mousavi, B. Behkamal, and M. Milani, Heterogeneity in entity matching: survey and experimental analysis, CoRR, vol. abs/2508.08076, 2025. [104] E. Rahm and H. H. Do, Data cleaning: Problems and current approaches, IEEE Data Eng. Bull., vol. 23, no. 4, pp. 313, 2000. [105] M. A. Hernandez and S. J. Stolfo, Real-world data is dirty: Data cleansing and the merge/purge problem, Data Min. Knowl. Discov., vol. 2, no. 1, pp. 937, 1998. [106] X. Chu, I. F. Ilyas, and P. Papotti, Holistic data cleaning: Putting IEEE Computer Society, 2013, pp. violations into context, in ICDE. 458469. [107] W. Fan and F. Geerts, Foundations of Data Quality Management, ser. Synthesis Lectures on Data Management. Morgan & Claypool Publishers, 2012. [108] R. J. Little and D. B. Rubin, Statistical Analysis with Missing Data, 3rd ed. Wiley, 2019. [109] D. B. RUBIN, Inference and missing data, Biometrika, vol. 63, no. 3, pp. 581592, 12 1976. [110] J. L. Schafer and J. W. Graham, Missing data: Our view of the state of the art, Psychological Methods, vol. 7, no. 2, pp. 147177, 2002. [111] I. P. Fellegi and A. B. Sunter, theory for record linkage, Journal of the American Statistical Association, vol. 64, no. 328, pp. 11831210, 1969. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, JANUARY 2025 20 [112] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios, Duplicate record detection: survey, IEEE Trans. Knowl. Data Eng., vol. 19, no. 1, pp. 116, 2007. [113] E. Rahm and P. A. Bernstein, survey of approaches to automatic schema matching, VLDB J., vol. 10, no. 4, pp. 334350, 2001. [114] A. Doan, P. M. Domingos, and A. Y. Halevy, Reconciling schemas of disparate data sources: machine-learning approach, in SIGMOD Conference. ACM, 2001, pp. 509520. [115] J. Madhavan, P. A. Bernstein, and E. Rahm, Generic schema matching with cupid, in VLDB, 2001, pp. 4958. [116] G. Limaye, S. Sarawagi, and S. Chakrabarti, Annotating and searching web tables using entities, types and relationships, Proc. VLDB Endow., vol. 3, no. 1, pp. 13381347, 2010. [117] M. Hulsebos et al., Sherlock: deep learning approach to semantic data type detection, in KDD. ACM, 2019, pp. 15001508. [118] Z. Abedjan, L. Golab, and F. Naumann, Profiling relational data: survey, VLDB J., vol. 24, no. 4, pp. 557581, 2015. [119] Y. Huhtala, J. Karkkainen, P. Porkka, and H. Toivonen, TANE: an efficient algorithm for discovering functional and approximate dependencies, Comput. J., vol. 42, no. 2, pp. 100111, 1999. [120] M. Mahdavi et al., Raha: configuration-free error detection system, in SIGMOD Conference. ACM, 2019, pp. 865882. [121] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, CoRR, vol. abs/2307.09288, 2023. [122] Z. Zhang, P. Groth, I. Calixto, and S. Schelter, Anymatch - efficient zero-shot entity matching with small language model, CoRR, vol. abs/2409.04073, 2024. [123] O. Khattab and M. Zaharia, Colbert: Efficient and effective passage search via contextualized late interaction over BERT, in SIGIR. ACM, 2020, pp. 3948. [124] S. Yao et al., React: Synergizing reasoning and acting in language models, in ICLR, 2023. [125] E. Schubert, J. Sander, M. Ester, H. Kriegel, and X. Xu, DBSCAN revisited, revisited: Why and how you should (still) use DBSCAN, ACM Trans. Database Syst., vol. 42, no. 3, pp. 19:119:21, 2017. [126] OpenAI, Embeddings, https://platform.openai.com/docs/guides/ embeddings, 2024, accessed: 2024-07-20. [127] Chicago open data portal. https://data.cityofchicago.org/. Accessed: 2026-01-14. [128] U.S. Small Business Administration, PPP FOIA, https://data.sba.gov/ dataset/ppp-foia, 2021, [Data set]. Accessed: 2026-01-14. [129] B. Klimt and Y. Yang, Introducing the enron corpus, in CEAS, 2004. [130] Q. Hao, R. Cai, Y. Pang, and L. Zhang, From one tree to forest: unified solution for structured web data extraction, in SIGIR. ACM, 2011, pp. 775784. [131] X. Li, X. L. Dong, K. Lyons, W. Meng, and D. Srivastava, Truth finding on the deep web: is the problem solved? Proc. VLDB Endow., vol. 6, no. 2, p. 97108, Dec. 2012. [132] Jean-NicholasHould, Craft beers dataset, https://www.kaggle.com/ nickhould/craft-cans, 2017, kaggle dataset, Accessed: 2026-01-14. [133] Scibearia, Meat consumption per capita dataset, https: //www.kaggle.com/datasets/scibearia/meat-consumption-per-capita, 2024, kaggle dataset, Accessed: 2026-01-14. [134] Mojtaba, Hotel booking dataset, https://www.kaggle.com/datasets/ mojtaba142/hotel-booking, 2021, kaggle dataset, Accessed: 2026-0114. [135] wenruliu, Adult income dataset, https://www.kaggle.com/datasets/ wenruliu/adult-income-dataset, 2016, kaggle dataset, Accessed: 202601-14. Tour [136] Tejashvi, diction, tour-travels-customer-churn-prediction, Accessed: 2026-01-15. customer prehttps://www.kaggle.com/datasets/tejashvi14/ dataset. kaggle travels churn 2021, & [137] H. Kopcke, A. Thor, and E. Rahm, Evaluation of entity resolution approaches on real-world match problems, Proc. VLDB Endow., vol. 3, no. 1, pp. 484493, 2010. [138] Y. Mei, S. Song, C. Fang, H. Yang, J. Fang, and J. Long, Capturing semantics for imputation with pre-trained language models, in 2021 IEEE 37th International Conference on Data Engineering (ICDE), 2021, pp. 6172. [139] U. of Texas at Austin Machine Learning Research Group, Duplicate detection, record linkage, and identity uncertainty: Datasets, https:// www.cs.utexas.edu/ml/riddle/data.html, 2003, last modified: August 25, 2003. [140] S. Das et al., The magellan data repository, https://sites.google.com/ site/anhaidgroup/useful-stuff/the-magellan-data-repository. [141] R. Peeters, R. C. Der, and C. Bizer, WDC products: multidimensional entity matching benchmark, in EDBT. OpenProceedings.org, 2024, pp. 2233. [142] G. Hripcsak et al., Observational health data sciences and informatics (OHDSI): opportunities for observational researchers, in MedInfo, ser. Studies in Health Technology and Informatics, vol. 216. IOS Press, 2015, pp. 574578. [143] J. A. Walonoski et al., Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record, J. Am. Medical Informatics Assoc., vol. 25, no. 3, pp. 230238, 2018. [144] A. E. W. Johnson, D. J. Stone, L. A. Celi, and T. J. Pollard, The mimic code repository: enabling reproducibility in critical care research, Journal of the American Medical Informatics Association, vol. 25, no. 1, pp. 3239, 2018. [145] A. S. R. Santos et al., GDC-SM: the GDC schema matching benchmark (version 1.0), https://doi.org/10.5281/zenodo.14963588, Apr. 2025, accessed: 2026-01-15. [146] A. Gaulton et al., Chembl: large-scale bioactivity database for drug discovery, Nucleic Acids Res., vol. 40, no. Database-Issue, pp. 1100 1107, 2012. [147] J. Herzig, P. K. Nowak, T. Muller, F. Piccinno, and J. M. Eisenschlos, Tapas: Weakly supervised table parsing via pre-training, in ACL, 2020, pp. 43204333. [148] S. Kweon, Y. Kwon, S. Cho, Y. Jo, and E. Choi, Open-wikitable : Dataset for open domain question answering with complex reasoning over table, in ACL (Findings), 2023, pp. 82858297. [149] J. Lehmann et al., Dbpedia - large-scale, multilingual knowledge base extracted from wikipedia, Semantic Web, vol. 6, no. 2, pp. 167 195, 2015. [150] X. Zhang, J. J. Zhao, and Y. LeCun, Character-level convolutional networks for text classification, in NIPS, 2015, pp. 649657. [151] E. F. T. K. Sang and F. D. Meulder, Introduction to the conll2003 shared task: Language-independent named entity recognition, in CoNLL. ACL, 2003, pp. 142147. [152] L. Derczynski, E. Nichols, M. van Erp, and N. Limsopatham, Results of the WNUT2017 shared task on novel and emerging entity recognition, in NUT@EMNLP, 2017, pp. 140147. [153] AdventureWorks, Adventure works sample databases, https://learn. microsoft.com/en-us/sql/samples/adventureworks-install-configure, 2026, accessed: 2026-01-14. [154] Public bi benchmark. https://github.com/cwida/public bi benchmark. Accessed: 2026-01-14. [155] L. Nan et al., Fetaqa: Free-form table question answering, CoRR, vol. abs/2104.00369, 2021. [156] C. Van Rijsbergen, Information Retrieval. Butterworths, 1979. [157] T. Fawcett, An introduction to ROC analysis, Pattern Recognit. Lett., vol. 27, no. 8, pp. 861874, 2006. [158] C. D. Manning, P. Raghavan, and H. Schutze, Introduction to Information Retrieval. Cambridge University Press, 2008. [159] S. He, C. Wang, G. Yang, and L. Feng, Candidate label set pruning: data-centric perspective for deep partial-label learning, in The 12th International Conference on Learning Representations (ICLR), 2024. [160] C.-Y. Lin, Rouge: package for automatic evaluation of summaries, in in Text Summarization Branches Out. ACL, 2004, pp. 7481. [161] G. Salton, A. Wong, and C.-S. Yang, vector space model for automatic indexing, Commun. ACM, vol. 18, no. 11, pp. 613620, 1975. [162] Q. Zhao et al., Uncertainty propagation on LLM agent, in ACL (1). Association for Computational Linguistics, 2025, pp. 60646073. [163] S. V. Vuddanti et al., PALADIN: self-correcting language model agents to cure tool-failure cases, CoRR, vol. abs/2509.25238, 2025. [164] W. Kuang et al., Federatedscope-llm: comprehensive package for fine-tuning large language models in federated learning, in KDD. ACM, 2024, pp. 52605271. [165] M. Parciak, B. Vandevoort, F. Neven, L. M. Peeters, and S. Vansummeren, Llm-matcher: name-based schema matching tool using large language models, in SIGMOD Conference Companion. ACM, 2025, pp. 203206. [166] A. Cocchieri, M. M. Galindo, G. Frisoni, G. Moro, C. Sartori, and G. Tagliavini, Zeroner: Fueling zero-shot named entity recognition via entity type descriptions, in ACL (Findings). Association for Computational Linguistics, 2025, pp. 15 59415 616."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "MIT CSAIL",
        "Microsoft Research",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "Xiaohongshu Inc."
    ]
}